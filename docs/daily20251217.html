<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251216.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image", "author": "Sicheng Xu and Guojun Chen and Jiaolong Yang and Yizhong Zhang and Yu Deng and Steve Lin and Baining Guo", "abstract": "We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.", "link": "http://arxiv.org/abs/2512.14677v1", "date": "2025-12-16", "relevancy": 3.5642, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7552}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7552}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VASA-3D%3A%20Lifelike%20Audio-Driven%20Gaussian%20Head%20Avatars%20from%20a%20Single%20Image&body=Title%3A%20VASA-3D%3A%20Lifelike%20Audio-Driven%20Gaussian%20Head%20Avatars%20from%20a%20Single%20Image%0AAuthor%3A%20Sicheng%20Xu%20and%20Guojun%20Chen%20and%20Jiaolong%20Yang%20and%20Yizhong%20Zhang%20and%20Yu%20Deng%20and%20Steve%20Lin%20and%20Baining%20Guo%0AAbstract%3A%20We%20propose%20VASA-3D%2C%20an%20audio-driven%2C%20single-shot%203D%20head%20avatar%20generator.%20This%20research%20tackles%20two%20major%20challenges%3A%20capturing%20the%20subtle%20expression%20details%20present%20in%20real%20human%20faces%2C%20and%20reconstructing%20an%20intricate%203D%20head%20avatar%20from%20a%20single%20portrait%20image.%20To%20accurately%20model%20expression%20details%2C%20VASA-3D%20leverages%20the%20motion%20latent%20of%20VASA-1%2C%20a%20method%20that%20yields%20exceptional%20realism%20and%20vividness%20in%202D%20talking%20heads.%20A%20critical%20element%20of%20our%20work%20is%20translating%20this%20motion%20latent%20to%203D%2C%20which%20is%20accomplished%20by%20devising%20a%203D%20head%20model%20that%20is%20conditioned%20on%20the%20motion%20latent.%20Customization%20of%20this%20model%20to%20a%20single%20image%20is%20achieved%20through%20an%20optimization%20framework%20that%20employs%20numerous%20video%20frames%20of%20the%20reference%20head%20synthesized%20from%20the%20input%20image.%20The%20optimization%20takes%20various%20training%20losses%20robust%20to%20artifacts%20and%20limited%20pose%20coverage%20in%20the%20generated%20training%20data.%20Our%20experiment%20shows%20that%20VASA-3D%20produces%20realistic%203D%20talking%20heads%20that%20cannot%20be%20achieved%20by%20prior%20art%2C%20and%20it%20supports%20the%20online%20generation%20of%20512x512%20free-viewpoint%20videos%20at%20up%20to%2075%20FPS%2C%20facilitating%20more%20immersive%20engagements%20with%20lifelike%203D%20avatars.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVASA-3D%253A%2520Lifelike%2520Audio-Driven%2520Gaussian%2520Head%2520Avatars%2520from%2520a%2520Single%2520Image%26entry.906535625%3DSicheng%2520Xu%2520and%2520Guojun%2520Chen%2520and%2520Jiaolong%2520Yang%2520and%2520Yizhong%2520Zhang%2520and%2520Yu%2520Deng%2520and%2520Steve%2520Lin%2520and%2520Baining%2520Guo%26entry.1292438233%3DWe%2520propose%2520VASA-3D%252C%2520an%2520audio-driven%252C%2520single-shot%25203D%2520head%2520avatar%2520generator.%2520This%2520research%2520tackles%2520two%2520major%2520challenges%253A%2520capturing%2520the%2520subtle%2520expression%2520details%2520present%2520in%2520real%2520human%2520faces%252C%2520and%2520reconstructing%2520an%2520intricate%25203D%2520head%2520avatar%2520from%2520a%2520single%2520portrait%2520image.%2520To%2520accurately%2520model%2520expression%2520details%252C%2520VASA-3D%2520leverages%2520the%2520motion%2520latent%2520of%2520VASA-1%252C%2520a%2520method%2520that%2520yields%2520exceptional%2520realism%2520and%2520vividness%2520in%25202D%2520talking%2520heads.%2520A%2520critical%2520element%2520of%2520our%2520work%2520is%2520translating%2520this%2520motion%2520latent%2520to%25203D%252C%2520which%2520is%2520accomplished%2520by%2520devising%2520a%25203D%2520head%2520model%2520that%2520is%2520conditioned%2520on%2520the%2520motion%2520latent.%2520Customization%2520of%2520this%2520model%2520to%2520a%2520single%2520image%2520is%2520achieved%2520through%2520an%2520optimization%2520framework%2520that%2520employs%2520numerous%2520video%2520frames%2520of%2520the%2520reference%2520head%2520synthesized%2520from%2520the%2520input%2520image.%2520The%2520optimization%2520takes%2520various%2520training%2520losses%2520robust%2520to%2520artifacts%2520and%2520limited%2520pose%2520coverage%2520in%2520the%2520generated%2520training%2520data.%2520Our%2520experiment%2520shows%2520that%2520VASA-3D%2520produces%2520realistic%25203D%2520talking%2520heads%2520that%2520cannot%2520be%2520achieved%2520by%2520prior%2520art%252C%2520and%2520it%2520supports%2520the%2520online%2520generation%2520of%2520512x512%2520free-viewpoint%2520videos%2520at%2520up%2520to%252075%2520FPS%252C%2520facilitating%2520more%2520immersive%2520engagements%2520with%2520lifelike%25203D%2520avatars.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VASA-3D%3A%20Lifelike%20Audio-Driven%20Gaussian%20Head%20Avatars%20from%20a%20Single%20Image&entry.906535625=Sicheng%20Xu%20and%20Guojun%20Chen%20and%20Jiaolong%20Yang%20and%20Yizhong%20Zhang%20and%20Yu%20Deng%20and%20Steve%20Lin%20and%20Baining%20Guo&entry.1292438233=We%20propose%20VASA-3D%2C%20an%20audio-driven%2C%20single-shot%203D%20head%20avatar%20generator.%20This%20research%20tackles%20two%20major%20challenges%3A%20capturing%20the%20subtle%20expression%20details%20present%20in%20real%20human%20faces%2C%20and%20reconstructing%20an%20intricate%203D%20head%20avatar%20from%20a%20single%20portrait%20image.%20To%20accurately%20model%20expression%20details%2C%20VASA-3D%20leverages%20the%20motion%20latent%20of%20VASA-1%2C%20a%20method%20that%20yields%20exceptional%20realism%20and%20vividness%20in%202D%20talking%20heads.%20A%20critical%20element%20of%20our%20work%20is%20translating%20this%20motion%20latent%20to%203D%2C%20which%20is%20accomplished%20by%20devising%20a%203D%20head%20model%20that%20is%20conditioned%20on%20the%20motion%20latent.%20Customization%20of%20this%20model%20to%20a%20single%20image%20is%20achieved%20through%20an%20optimization%20framework%20that%20employs%20numerous%20video%20frames%20of%20the%20reference%20head%20synthesized%20from%20the%20input%20image.%20The%20optimization%20takes%20various%20training%20losses%20robust%20to%20artifacts%20and%20limited%20pose%20coverage%20in%20the%20generated%20training%20data.%20Our%20experiment%20shows%20that%20VASA-3D%20produces%20realistic%203D%20talking%20heads%20that%20cannot%20be%20achieved%20by%20prior%20art%2C%20and%20it%20supports%20the%20online%20generation%20of%20512x512%20free-viewpoint%20videos%20at%20up%20to%2075%20FPS%2C%20facilitating%20more%20immersive%20engagements%20with%20lifelike%203D%20avatars.&entry.1838667208=http%3A//arxiv.org/abs/2512.14677v1&entry.124074799=Read"},
{"title": "GASPACHO: Gaussian Splatting for Controllable Humans and Objects", "author": "Aymen Mir and Arthur Moreau and Helisa Dhamo and Zhensong Zhang and Gerard Pons-Moll and Eduardo P\u00e9rez-Pellitero", "abstract": "We present GASPACHO, a method for generating photorealistic, controllable renderings of human-object interactions from multi-view RGB video. Unlike prior work that reconstructs only the human and treats objects as background, GASPACHO simultaneously recovers animatable templates for both the human and the interacting object as distinct sets of Gaussians, thereby allowing for controllable renderings of novel human object interactions in different poses from novel-camera viewpoints. We introduce a novel formulation that learns object Gaussians on an underlying 2D surface manifold rather than in 3D volume, yielding sharper, fine-grained object details for dynamic object reconstruction. We further propose a contact constraint in Gaussian space that regularizes human-object relations and enables natural, physically plausible animation. Across three benchmarks - BEHAVE, NeuralDome, and DNA-Rendering - GASPACHO achieves high-quality reconstructions under heavy occlusion and supports controllable synthesis of novel human-object interactions. We also demonstrate that our method allows for composition of humans and objects in 3D scenes and for the first time showcase that neural rendering can be used for the controllable generation of photoreal humans interacting with dynamic objects in diverse scenes. Our results are available at: https://miraymen.github.io/gaspacho/", "link": "http://arxiv.org/abs/2503.09342v2", "date": "2025-12-16", "relevancy": 3.4605, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7115}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6948}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.67}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GASPACHO%3A%20Gaussian%20Splatting%20for%20Controllable%20Humans%20and%20Objects&body=Title%3A%20GASPACHO%3A%20Gaussian%20Splatting%20for%20Controllable%20Humans%20and%20Objects%0AAuthor%3A%20Aymen%20Mir%20and%20Arthur%20Moreau%20and%20Helisa%20Dhamo%20and%20Zhensong%20Zhang%20and%20Gerard%20Pons-Moll%20and%20Eduardo%20P%C3%A9rez-Pellitero%0AAbstract%3A%20We%20present%20GASPACHO%2C%20a%20method%20for%20generating%20photorealistic%2C%20controllable%20renderings%20of%20human-object%20interactions%20from%20multi-view%20RGB%20video.%20Unlike%20prior%20work%20that%20reconstructs%20only%20the%20human%20and%20treats%20objects%20as%20background%2C%20GASPACHO%20simultaneously%20recovers%20animatable%20templates%20for%20both%20the%20human%20and%20the%20interacting%20object%20as%20distinct%20sets%20of%20Gaussians%2C%20thereby%20allowing%20for%20controllable%20renderings%20of%20novel%20human%20object%20interactions%20in%20different%20poses%20from%20novel-camera%20viewpoints.%20We%20introduce%20a%20novel%20formulation%20that%20learns%20object%20Gaussians%20on%20an%20underlying%202D%20surface%20manifold%20rather%20than%20in%203D%20volume%2C%20yielding%20sharper%2C%20fine-grained%20object%20details%20for%20dynamic%20object%20reconstruction.%20We%20further%20propose%20a%20contact%20constraint%20in%20Gaussian%20space%20that%20regularizes%20human-object%20relations%20and%20enables%20natural%2C%20physically%20plausible%20animation.%20Across%20three%20benchmarks%20-%20BEHAVE%2C%20NeuralDome%2C%20and%20DNA-Rendering%20-%20GASPACHO%20achieves%20high-quality%20reconstructions%20under%20heavy%20occlusion%20and%20supports%20controllable%20synthesis%20of%20novel%20human-object%20interactions.%20We%20also%20demonstrate%20that%20our%20method%20allows%20for%20composition%20of%20humans%20and%20objects%20in%203D%20scenes%20and%20for%20the%20first%20time%20showcase%20that%20neural%20rendering%20can%20be%20used%20for%20the%20controllable%20generation%20of%20photoreal%20humans%20interacting%20with%20dynamic%20objects%20in%20diverse%20scenes.%20Our%20results%20are%20available%20at%3A%20https%3A//miraymen.github.io/gaspacho/%0ALink%3A%20http%3A//arxiv.org/abs/2503.09342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGASPACHO%253A%2520Gaussian%2520Splatting%2520for%2520Controllable%2520Humans%2520and%2520Objects%26entry.906535625%3DAymen%2520Mir%2520and%2520Arthur%2520Moreau%2520and%2520Helisa%2520Dhamo%2520and%2520Zhensong%2520Zhang%2520and%2520Gerard%2520Pons-Moll%2520and%2520Eduardo%2520P%25C3%25A9rez-Pellitero%26entry.1292438233%3DWe%2520present%2520GASPACHO%252C%2520a%2520method%2520for%2520generating%2520photorealistic%252C%2520controllable%2520renderings%2520of%2520human-object%2520interactions%2520from%2520multi-view%2520RGB%2520video.%2520Unlike%2520prior%2520work%2520that%2520reconstructs%2520only%2520the%2520human%2520and%2520treats%2520objects%2520as%2520background%252C%2520GASPACHO%2520simultaneously%2520recovers%2520animatable%2520templates%2520for%2520both%2520the%2520human%2520and%2520the%2520interacting%2520object%2520as%2520distinct%2520sets%2520of%2520Gaussians%252C%2520thereby%2520allowing%2520for%2520controllable%2520renderings%2520of%2520novel%2520human%2520object%2520interactions%2520in%2520different%2520poses%2520from%2520novel-camera%2520viewpoints.%2520We%2520introduce%2520a%2520novel%2520formulation%2520that%2520learns%2520object%2520Gaussians%2520on%2520an%2520underlying%25202D%2520surface%2520manifold%2520rather%2520than%2520in%25203D%2520volume%252C%2520yielding%2520sharper%252C%2520fine-grained%2520object%2520details%2520for%2520dynamic%2520object%2520reconstruction.%2520We%2520further%2520propose%2520a%2520contact%2520constraint%2520in%2520Gaussian%2520space%2520that%2520regularizes%2520human-object%2520relations%2520and%2520enables%2520natural%252C%2520physically%2520plausible%2520animation.%2520Across%2520three%2520benchmarks%2520-%2520BEHAVE%252C%2520NeuralDome%252C%2520and%2520DNA-Rendering%2520-%2520GASPACHO%2520achieves%2520high-quality%2520reconstructions%2520under%2520heavy%2520occlusion%2520and%2520supports%2520controllable%2520synthesis%2520of%2520novel%2520human-object%2520interactions.%2520We%2520also%2520demonstrate%2520that%2520our%2520method%2520allows%2520for%2520composition%2520of%2520humans%2520and%2520objects%2520in%25203D%2520scenes%2520and%2520for%2520the%2520first%2520time%2520showcase%2520that%2520neural%2520rendering%2520can%2520be%2520used%2520for%2520the%2520controllable%2520generation%2520of%2520photoreal%2520humans%2520interacting%2520with%2520dynamic%2520objects%2520in%2520diverse%2520scenes.%2520Our%2520results%2520are%2520available%2520at%253A%2520https%253A//miraymen.github.io/gaspacho/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GASPACHO%3A%20Gaussian%20Splatting%20for%20Controllable%20Humans%20and%20Objects&entry.906535625=Aymen%20Mir%20and%20Arthur%20Moreau%20and%20Helisa%20Dhamo%20and%20Zhensong%20Zhang%20and%20Gerard%20Pons-Moll%20and%20Eduardo%20P%C3%A9rez-Pellitero&entry.1292438233=We%20present%20GASPACHO%2C%20a%20method%20for%20generating%20photorealistic%2C%20controllable%20renderings%20of%20human-object%20interactions%20from%20multi-view%20RGB%20video.%20Unlike%20prior%20work%20that%20reconstructs%20only%20the%20human%20and%20treats%20objects%20as%20background%2C%20GASPACHO%20simultaneously%20recovers%20animatable%20templates%20for%20both%20the%20human%20and%20the%20interacting%20object%20as%20distinct%20sets%20of%20Gaussians%2C%20thereby%20allowing%20for%20controllable%20renderings%20of%20novel%20human%20object%20interactions%20in%20different%20poses%20from%20novel-camera%20viewpoints.%20We%20introduce%20a%20novel%20formulation%20that%20learns%20object%20Gaussians%20on%20an%20underlying%202D%20surface%20manifold%20rather%20than%20in%203D%20volume%2C%20yielding%20sharper%2C%20fine-grained%20object%20details%20for%20dynamic%20object%20reconstruction.%20We%20further%20propose%20a%20contact%20constraint%20in%20Gaussian%20space%20that%20regularizes%20human-object%20relations%20and%20enables%20natural%2C%20physically%20plausible%20animation.%20Across%20three%20benchmarks%20-%20BEHAVE%2C%20NeuralDome%2C%20and%20DNA-Rendering%20-%20GASPACHO%20achieves%20high-quality%20reconstructions%20under%20heavy%20occlusion%20and%20supports%20controllable%20synthesis%20of%20novel%20human-object%20interactions.%20We%20also%20demonstrate%20that%20our%20method%20allows%20for%20composition%20of%20humans%20and%20objects%20in%203D%20scenes%20and%20for%20the%20first%20time%20showcase%20that%20neural%20rendering%20can%20be%20used%20for%20the%20controllable%20generation%20of%20photoreal%20humans%20interacting%20with%20dynamic%20objects%20in%20diverse%20scenes.%20Our%20results%20are%20available%20at%3A%20https%3A//miraymen.github.io/gaspacho/&entry.1838667208=http%3A//arxiv.org/abs/2503.09342v2&entry.124074799=Read"},
{"title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis", "author": "Kaizhe Zhang and Yijie Zhou and Weizhan Zhang and Caixia Yan and Haipeng Du and yugui xie and Yu-Hui Wen and Yong-Jin Liu", "abstract": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.", "link": "http://arxiv.org/abs/2512.14352v1", "date": "2025-12-16", "relevancy": 3.3476, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6989}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6738}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGS%3A%20Hybrid%20Gaussian%20Splatting%20with%20Static-Dynamic%20Decomposition%20for%20Compact%20Dynamic%20View%20Synthesis&body=Title%3A%20HGS%3A%20Hybrid%20Gaussian%20Splatting%20with%20Static-Dynamic%20Decomposition%20for%20Compact%20Dynamic%20View%20Synthesis%0AAuthor%3A%20Kaizhe%20Zhang%20and%20Yijie%20Zhou%20and%20Weizhan%20Zhang%20and%20Caixia%20Yan%20and%20Haipeng%20Du%20and%20yugui%20xie%20and%20Yu-Hui%20Wen%20and%20Yong-Jin%20Liu%0AAbstract%3A%20Dynamic%20novel%20view%20synthesis%20%28NVS%29%20is%20essential%20for%20creating%20immersive%20experiences.%20Existing%20approaches%20have%20advanced%20dynamic%20NVS%20by%20introducing%203D%20Gaussian%20Splatting%20%283DGS%29%20with%20implicit%20deformation%20fields%20or%20indiscriminately%20assigned%20time-varying%20parameters%2C%20surpassing%20NeRF-based%20methods.%20However%2C%20due%20to%20excessive%20model%20complexity%20and%20parameter%20redundancy%2C%20they%20incur%20large%20model%20sizes%20and%20slow%20rendering%20speeds%2C%20making%20them%20inefficient%20for%20real-time%20applications%2C%20particularly%20on%20resource-constrained%20devices.%20To%20obtain%20a%20more%20efficient%20model%20with%20fewer%20redundant%20parameters%2C%20in%20this%20paper%2C%20we%20propose%20Hybrid%20Gaussian%20Splatting%20%28HGS%29%2C%20a%20compact%20and%20efficient%20framework%20explicitly%20designed%20to%20disentangle%20static%20and%20dynamic%20regions%20of%20a%20scene%20within%20a%20unified%20representation.%20The%20core%20innovation%20of%20HGS%20lies%20in%20our%20Static-Dynamic%20Decomposition%20%28SDD%29%20strategy%2C%20which%20leverages%20Radial%20Basis%20Function%20%28RBF%29%20modeling%20for%20Gaussian%20primitives.%20Specifically%2C%20for%20dynamic%20regions%2C%20we%20employ%20time-dependent%20RBFs%20to%20effectively%20capture%20temporal%20variations%20and%20handle%20abrupt%20scene%20changes%2C%20while%20for%20static%20regions%2C%20we%20reduce%20redundancy%20by%20sharing%20temporally%20invariant%20parameters.%20Additionally%2C%20we%20introduce%20a%20two-stage%20training%20strategy%20tailored%20for%20explicit%20models%20to%20enhance%20temporal%20coherence%20at%20static-dynamic%20boundaries.%20Experimental%20results%20demonstrate%20that%20our%20method%20reduces%20model%20size%20by%20up%20to%2098%25%20and%20achieves%20real-time%20rendering%20at%20up%20to%20125%20FPS%20at%204K%20resolution%20on%20a%20single%20RTX%203090%20GPU.%20It%20further%20sustains%20160%20FPS%20at%201352%20%2A%201014%20on%20an%20RTX%203050%20and%20has%20been%20integrated%20into%20the%20VR%20system.%20Moreover%2C%20HGS%20achieves%20comparable%20rendering%20quality%20to%20state-of-the-art%20methods%20while%20providing%20significantly%20improved%20visual%20fidelity%20for%20high-frequency%20details%20and%20abrupt%20scene%20changes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGS%253A%2520Hybrid%2520Gaussian%2520Splatting%2520with%2520Static-Dynamic%2520Decomposition%2520for%2520Compact%2520Dynamic%2520View%2520Synthesis%26entry.906535625%3DKaizhe%2520Zhang%2520and%2520Yijie%2520Zhou%2520and%2520Weizhan%2520Zhang%2520and%2520Caixia%2520Yan%2520and%2520Haipeng%2520Du%2520and%2520yugui%2520xie%2520and%2520Yu-Hui%2520Wen%2520and%2520Yong-Jin%2520Liu%26entry.1292438233%3DDynamic%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520is%2520essential%2520for%2520creating%2520immersive%2520experiences.%2520Existing%2520approaches%2520have%2520advanced%2520dynamic%2520NVS%2520by%2520introducing%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520with%2520implicit%2520deformation%2520fields%2520or%2520indiscriminately%2520assigned%2520time-varying%2520parameters%252C%2520surpassing%2520NeRF-based%2520methods.%2520However%252C%2520due%2520to%2520excessive%2520model%2520complexity%2520and%2520parameter%2520redundancy%252C%2520they%2520incur%2520large%2520model%2520sizes%2520and%2520slow%2520rendering%2520speeds%252C%2520making%2520them%2520inefficient%2520for%2520real-time%2520applications%252C%2520particularly%2520on%2520resource-constrained%2520devices.%2520To%2520obtain%2520a%2520more%2520efficient%2520model%2520with%2520fewer%2520redundant%2520parameters%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520Hybrid%2520Gaussian%2520Splatting%2520%2528HGS%2529%252C%2520a%2520compact%2520and%2520efficient%2520framework%2520explicitly%2520designed%2520to%2520disentangle%2520static%2520and%2520dynamic%2520regions%2520of%2520a%2520scene%2520within%2520a%2520unified%2520representation.%2520The%2520core%2520innovation%2520of%2520HGS%2520lies%2520in%2520our%2520Static-Dynamic%2520Decomposition%2520%2528SDD%2529%2520strategy%252C%2520which%2520leverages%2520Radial%2520Basis%2520Function%2520%2528RBF%2529%2520modeling%2520for%2520Gaussian%2520primitives.%2520Specifically%252C%2520for%2520dynamic%2520regions%252C%2520we%2520employ%2520time-dependent%2520RBFs%2520to%2520effectively%2520capture%2520temporal%2520variations%2520and%2520handle%2520abrupt%2520scene%2520changes%252C%2520while%2520for%2520static%2520regions%252C%2520we%2520reduce%2520redundancy%2520by%2520sharing%2520temporally%2520invariant%2520parameters.%2520Additionally%252C%2520we%2520introduce%2520a%2520two-stage%2520training%2520strategy%2520tailored%2520for%2520explicit%2520models%2520to%2520enhance%2520temporal%2520coherence%2520at%2520static-dynamic%2520boundaries.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520reduces%2520model%2520size%2520by%2520up%2520to%252098%2525%2520and%2520achieves%2520real-time%2520rendering%2520at%2520up%2520to%2520125%2520FPS%2520at%25204K%2520resolution%2520on%2520a%2520single%2520RTX%25203090%2520GPU.%2520It%2520further%2520sustains%2520160%2520FPS%2520at%25201352%2520%252A%25201014%2520on%2520an%2520RTX%25203050%2520and%2520has%2520been%2520integrated%2520into%2520the%2520VR%2520system.%2520Moreover%252C%2520HGS%2520achieves%2520comparable%2520rendering%2520quality%2520to%2520state-of-the-art%2520methods%2520while%2520providing%2520significantly%2520improved%2520visual%2520fidelity%2520for%2520high-frequency%2520details%2520and%2520abrupt%2520scene%2520changes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGS%3A%20Hybrid%20Gaussian%20Splatting%20with%20Static-Dynamic%20Decomposition%20for%20Compact%20Dynamic%20View%20Synthesis&entry.906535625=Kaizhe%20Zhang%20and%20Yijie%20Zhou%20and%20Weizhan%20Zhang%20and%20Caixia%20Yan%20and%20Haipeng%20Du%20and%20yugui%20xie%20and%20Yu-Hui%20Wen%20and%20Yong-Jin%20Liu&entry.1292438233=Dynamic%20novel%20view%20synthesis%20%28NVS%29%20is%20essential%20for%20creating%20immersive%20experiences.%20Existing%20approaches%20have%20advanced%20dynamic%20NVS%20by%20introducing%203D%20Gaussian%20Splatting%20%283DGS%29%20with%20implicit%20deformation%20fields%20or%20indiscriminately%20assigned%20time-varying%20parameters%2C%20surpassing%20NeRF-based%20methods.%20However%2C%20due%20to%20excessive%20model%20complexity%20and%20parameter%20redundancy%2C%20they%20incur%20large%20model%20sizes%20and%20slow%20rendering%20speeds%2C%20making%20them%20inefficient%20for%20real-time%20applications%2C%20particularly%20on%20resource-constrained%20devices.%20To%20obtain%20a%20more%20efficient%20model%20with%20fewer%20redundant%20parameters%2C%20in%20this%20paper%2C%20we%20propose%20Hybrid%20Gaussian%20Splatting%20%28HGS%29%2C%20a%20compact%20and%20efficient%20framework%20explicitly%20designed%20to%20disentangle%20static%20and%20dynamic%20regions%20of%20a%20scene%20within%20a%20unified%20representation.%20The%20core%20innovation%20of%20HGS%20lies%20in%20our%20Static-Dynamic%20Decomposition%20%28SDD%29%20strategy%2C%20which%20leverages%20Radial%20Basis%20Function%20%28RBF%29%20modeling%20for%20Gaussian%20primitives.%20Specifically%2C%20for%20dynamic%20regions%2C%20we%20employ%20time-dependent%20RBFs%20to%20effectively%20capture%20temporal%20variations%20and%20handle%20abrupt%20scene%20changes%2C%20while%20for%20static%20regions%2C%20we%20reduce%20redundancy%20by%20sharing%20temporally%20invariant%20parameters.%20Additionally%2C%20we%20introduce%20a%20two-stage%20training%20strategy%20tailored%20for%20explicit%20models%20to%20enhance%20temporal%20coherence%20at%20static-dynamic%20boundaries.%20Experimental%20results%20demonstrate%20that%20our%20method%20reduces%20model%20size%20by%20up%20to%2098%25%20and%20achieves%20real-time%20rendering%20at%20up%20to%20125%20FPS%20at%204K%20resolution%20on%20a%20single%20RTX%203090%20GPU.%20It%20further%20sustains%20160%20FPS%20at%201352%20%2A%201014%20on%20an%20RTX%203050%20and%20has%20been%20integrated%20into%20the%20VR%20system.%20Moreover%2C%20HGS%20achieves%20comparable%20rendering%20quality%20to%20state-of-the-art%20methods%20while%20providing%20significantly%20improved%20visual%20fidelity%20for%20high-frequency%20details%20and%20abrupt%20scene%20changes.&entry.1838667208=http%3A//arxiv.org/abs/2512.14352v1&entry.124074799=Read"},
{"title": "Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling", "author": "Maik Dannecker and Steven Jia and Nil Stolt-Ans\u00f3 and Nadine Girard and Guillaume Auzias and Fran\u00e7ois Rousseau and Daniel Rueckert", "abstract": "Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \\textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\\mathbf\u03a3_{obs} = \\mathbf\u03a3_{HR} + \\mathbf\u03a3_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\\times$--10$\\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.", "link": "http://arxiv.org/abs/2512.11624v2", "date": "2025-12-16", "relevancy": 3.2258, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6705}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6581}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Explicit%3A%20Slice-to-Volume%20Reconstruction%20via%203D%20Gaussian%20Primitives%20with%20Analytic%20Point%20Spread%20Function%20Modeling&body=Title%3A%20Fast%20and%20Explicit%3A%20Slice-to-Volume%20Reconstruction%20via%203D%20Gaussian%20Primitives%20with%20Analytic%20Point%20Spread%20Function%20Modeling%0AAuthor%3A%20Maik%20Dannecker%20and%20Steven%20Jia%20and%20Nil%20Stolt-Ans%C3%B3%20and%20Nadine%20Girard%20and%20Guillaume%20Auzias%20and%20Fran%C3%A7ois%20Rousseau%20and%20Daniel%20Rueckert%0AAbstract%3A%20Recovering%20high-fidelity%203D%20images%20from%20sparse%20or%20degraded%202D%20images%20is%20a%20fundamental%20challenge%20in%20medical%20imaging%2C%20with%20broad%20applications%20ranging%20from%203D%20ultrasound%20reconstruction%20to%20MRI%20super-resolution.%20In%20the%20context%20of%20fetal%20MRI%2C%20high-resolution%203D%20reconstruction%20of%20the%20brain%20from%20motion-corrupted%20low-resolution%202D%20acquisitions%20is%20a%20prerequisite%20for%20accurate%20neurodevelopmental%20diagnosis.%20While%20implicit%20neural%20representations%20%28INRs%29%20have%20recently%20established%20state-of-the-art%20performance%20in%20self-supervised%20slice-to-volume%20reconstruction%20%28SVR%29%2C%20they%20suffer%20from%20a%20critical%20computational%20bottleneck%3A%20accurately%20modeling%20the%20image%20acquisition%20physics%20requires%20expensive%20stochastic%20Monte%20Carlo%20sampling%20to%20approximate%20the%20point%20spread%20function%20%28PSF%29.%20In%20this%20work%2C%20we%20propose%20a%20shift%20from%20neural%20network%20based%20implicit%20representations%20to%20Gaussian%20based%20explicit%20representations.%20By%20parameterizing%20the%20HR%203D%20image%20volume%20as%20a%20field%20of%20anisotropic%20Gaussian%20primitives%2C%20we%20leverage%20the%20property%20of%20Gaussians%20being%20closed%20under%20convolution%20and%20thus%20derive%20a%20%5Ctextit%7Bclosed-form%20analytical%20solution%7D%20for%20the%20forward%20model.%20This%20formulation%20reduces%20the%20previously%20intractable%20acquisition%20integral%20to%20an%20exact%20covariance%20addition%20%28%24%5Cmathbf%CE%A3_%7Bobs%7D%20%3D%20%5Cmathbf%CE%A3_%7BHR%7D%20%2B%20%5Cmathbf%CE%A3_%7BPSF%7D%24%29%2C%20effectively%20bypassing%20the%20need%20for%20compute-intensive%20stochastic%20sampling%20while%20ensuring%20exact%20gradient%20propagation.%20We%20demonstrate%20that%20our%20approach%20matches%20the%20reconstruction%20quality%20of%20self-supervised%20state-of-the-art%20SVR%20frameworks%20while%20delivering%20a%205%24%5Ctimes%24--10%24%5Ctimes%24%20speed-up%20on%20neonatal%20and%20fetal%20data.%20With%20convergence%20often%20reached%20in%20under%2030%20seconds%2C%20our%20framework%20paves%20the%20way%20towards%20translation%20into%20clinical%20routine%20of%20real-time%20fetal%203D%20MRI.%20Code%20will%20be%20public%20at%20%7Bhttps%3A//github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Explicit%253A%2520Slice-to-Volume%2520Reconstruction%2520via%25203D%2520Gaussian%2520Primitives%2520with%2520Analytic%2520Point%2520Spread%2520Function%2520Modeling%26entry.906535625%3DMaik%2520Dannecker%2520and%2520Steven%2520Jia%2520and%2520Nil%2520Stolt-Ans%25C3%25B3%2520and%2520Nadine%2520Girard%2520and%2520Guillaume%2520Auzias%2520and%2520Fran%25C3%25A7ois%2520Rousseau%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3DRecovering%2520high-fidelity%25203D%2520images%2520from%2520sparse%2520or%2520degraded%25202D%2520images%2520is%2520a%2520fundamental%2520challenge%2520in%2520medical%2520imaging%252C%2520with%2520broad%2520applications%2520ranging%2520from%25203D%2520ultrasound%2520reconstruction%2520to%2520MRI%2520super-resolution.%2520In%2520the%2520context%2520of%2520fetal%2520MRI%252C%2520high-resolution%25203D%2520reconstruction%2520of%2520the%2520brain%2520from%2520motion-corrupted%2520low-resolution%25202D%2520acquisitions%2520is%2520a%2520prerequisite%2520for%2520accurate%2520neurodevelopmental%2520diagnosis.%2520While%2520implicit%2520neural%2520representations%2520%2528INRs%2529%2520have%2520recently%2520established%2520state-of-the-art%2520performance%2520in%2520self-supervised%2520slice-to-volume%2520reconstruction%2520%2528SVR%2529%252C%2520they%2520suffer%2520from%2520a%2520critical%2520computational%2520bottleneck%253A%2520accurately%2520modeling%2520the%2520image%2520acquisition%2520physics%2520requires%2520expensive%2520stochastic%2520Monte%2520Carlo%2520sampling%2520to%2520approximate%2520the%2520point%2520spread%2520function%2520%2528PSF%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520shift%2520from%2520neural%2520network%2520based%2520implicit%2520representations%2520to%2520Gaussian%2520based%2520explicit%2520representations.%2520By%2520parameterizing%2520the%2520HR%25203D%2520image%2520volume%2520as%2520a%2520field%2520of%2520anisotropic%2520Gaussian%2520primitives%252C%2520we%2520leverage%2520the%2520property%2520of%2520Gaussians%2520being%2520closed%2520under%2520convolution%2520and%2520thus%2520derive%2520a%2520%255Ctextit%257Bclosed-form%2520analytical%2520solution%257D%2520for%2520the%2520forward%2520model.%2520This%2520formulation%2520reduces%2520the%2520previously%2520intractable%2520acquisition%2520integral%2520to%2520an%2520exact%2520covariance%2520addition%2520%2528%2524%255Cmathbf%25CE%25A3_%257Bobs%257D%2520%253D%2520%255Cmathbf%25CE%25A3_%257BHR%257D%2520%252B%2520%255Cmathbf%25CE%25A3_%257BPSF%257D%2524%2529%252C%2520effectively%2520bypassing%2520the%2520need%2520for%2520compute-intensive%2520stochastic%2520sampling%2520while%2520ensuring%2520exact%2520gradient%2520propagation.%2520We%2520demonstrate%2520that%2520our%2520approach%2520matches%2520the%2520reconstruction%2520quality%2520of%2520self-supervised%2520state-of-the-art%2520SVR%2520frameworks%2520while%2520delivering%2520a%25205%2524%255Ctimes%2524--10%2524%255Ctimes%2524%2520speed-up%2520on%2520neonatal%2520and%2520fetal%2520data.%2520With%2520convergence%2520often%2520reached%2520in%2520under%252030%2520seconds%252C%2520our%2520framework%2520paves%2520the%2520way%2520towards%2520translation%2520into%2520clinical%2520routine%2520of%2520real-time%2520fetal%25203D%2520MRI.%2520Code%2520will%2520be%2520public%2520at%2520%257Bhttps%253A//github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Explicit%3A%20Slice-to-Volume%20Reconstruction%20via%203D%20Gaussian%20Primitives%20with%20Analytic%20Point%20Spread%20Function%20Modeling&entry.906535625=Maik%20Dannecker%20and%20Steven%20Jia%20and%20Nil%20Stolt-Ans%C3%B3%20and%20Nadine%20Girard%20and%20Guillaume%20Auzias%20and%20Fran%C3%A7ois%20Rousseau%20and%20Daniel%20Rueckert&entry.1292438233=Recovering%20high-fidelity%203D%20images%20from%20sparse%20or%20degraded%202D%20images%20is%20a%20fundamental%20challenge%20in%20medical%20imaging%2C%20with%20broad%20applications%20ranging%20from%203D%20ultrasound%20reconstruction%20to%20MRI%20super-resolution.%20In%20the%20context%20of%20fetal%20MRI%2C%20high-resolution%203D%20reconstruction%20of%20the%20brain%20from%20motion-corrupted%20low-resolution%202D%20acquisitions%20is%20a%20prerequisite%20for%20accurate%20neurodevelopmental%20diagnosis.%20While%20implicit%20neural%20representations%20%28INRs%29%20have%20recently%20established%20state-of-the-art%20performance%20in%20self-supervised%20slice-to-volume%20reconstruction%20%28SVR%29%2C%20they%20suffer%20from%20a%20critical%20computational%20bottleneck%3A%20accurately%20modeling%20the%20image%20acquisition%20physics%20requires%20expensive%20stochastic%20Monte%20Carlo%20sampling%20to%20approximate%20the%20point%20spread%20function%20%28PSF%29.%20In%20this%20work%2C%20we%20propose%20a%20shift%20from%20neural%20network%20based%20implicit%20representations%20to%20Gaussian%20based%20explicit%20representations.%20By%20parameterizing%20the%20HR%203D%20image%20volume%20as%20a%20field%20of%20anisotropic%20Gaussian%20primitives%2C%20we%20leverage%20the%20property%20of%20Gaussians%20being%20closed%20under%20convolution%20and%20thus%20derive%20a%20%5Ctextit%7Bclosed-form%20analytical%20solution%7D%20for%20the%20forward%20model.%20This%20formulation%20reduces%20the%20previously%20intractable%20acquisition%20integral%20to%20an%20exact%20covariance%20addition%20%28%24%5Cmathbf%CE%A3_%7Bobs%7D%20%3D%20%5Cmathbf%CE%A3_%7BHR%7D%20%2B%20%5Cmathbf%CE%A3_%7BPSF%7D%24%29%2C%20effectively%20bypassing%20the%20need%20for%20compute-intensive%20stochastic%20sampling%20while%20ensuring%20exact%20gradient%20propagation.%20We%20demonstrate%20that%20our%20approach%20matches%20the%20reconstruction%20quality%20of%20self-supervised%20state-of-the-art%20SVR%20frameworks%20while%20delivering%20a%205%24%5Ctimes%24--10%24%5Ctimes%24%20speed-up%20on%20neonatal%20and%20fetal%20data.%20With%20convergence%20often%20reached%20in%20under%2030%20seconds%2C%20our%20framework%20paves%20the%20way%20towards%20translation%20into%20clinical%20routine%20of%20real-time%20fetal%203D%20MRI.%20Code%20will%20be%20public%20at%20%7Bhttps%3A//github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.11624v2&entry.124074799=Read"},
{"title": "RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection", "author": "Yongyang Zhou and Fang-Lue Zhang and Zichen Wang and Lei Zhang", "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in novel view synthesis. However, rendering reflective objects remains a significant challenge, particularly in inverse rendering and relighting. We introduce RTR-GS, a novel inverse rendering framework capable of robustly rendering objects with arbitrary reflectance properties, decomposing BRDF and lighting, and delivering credible relighting results. Given a collection of multi-view images, our method effectively recovers geometric structure through a hybrid rendering model that combines forward rendering for radiance transfer with deferred rendering for reflections. This approach successfully separates high-frequency and low-frequency appearances, mitigating floating artifacts caused by spherical harmonic overfitting when handling high-frequency details. We further refine BRDF and lighting decomposition using an additional physically-based deferred rendering branch. Experimental results show that our method enhances novel view synthesis, normal estimation, decomposition, and relighting while maintaining efficient training inference process.", "link": "http://arxiv.org/abs/2507.07733v2", "date": "2025-12-16", "relevancy": 3.2229, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7053}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6413}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RTR-GS%3A%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering%20with%20Radiance%20Transfer%20and%20Reflection&body=Title%3A%20RTR-GS%3A%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering%20with%20Radiance%20Transfer%20and%20Reflection%0AAuthor%3A%20Yongyang%20Zhou%20and%20Fang-Lue%20Zhang%20and%20Zichen%20Wang%20and%20Lei%20Zhang%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20impressive%20capabilities%20in%20novel%20view%20synthesis.%20However%2C%20rendering%20reflective%20objects%20remains%20a%20significant%20challenge%2C%20particularly%20in%20inverse%20rendering%20and%20relighting.%20We%20introduce%20RTR-GS%2C%20a%20novel%20inverse%20rendering%20framework%20capable%20of%20robustly%20rendering%20objects%20with%20arbitrary%20reflectance%20properties%2C%20decomposing%20BRDF%20and%20lighting%2C%20and%20delivering%20credible%20relighting%20results.%20Given%20a%20collection%20of%20multi-view%20images%2C%20our%20method%20effectively%20recovers%20geometric%20structure%20through%20a%20hybrid%20rendering%20model%20that%20combines%20forward%20rendering%20for%20radiance%20transfer%20with%20deferred%20rendering%20for%20reflections.%20This%20approach%20successfully%20separates%20high-frequency%20and%20low-frequency%20appearances%2C%20mitigating%20floating%20artifacts%20caused%20by%20spherical%20harmonic%20overfitting%20when%20handling%20high-frequency%20details.%20We%20further%20refine%20BRDF%20and%20lighting%20decomposition%20using%20an%20additional%20physically-based%20deferred%20rendering%20branch.%20Experimental%20results%20show%20that%20our%20method%20enhances%20novel%20view%20synthesis%2C%20normal%20estimation%2C%20decomposition%2C%20and%20relighting%20while%20maintaining%20efficient%20training%20inference%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2507.07733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRTR-GS%253A%25203D%2520Gaussian%2520Splatting%2520for%2520Inverse%2520Rendering%2520with%2520Radiance%2520Transfer%2520and%2520Reflection%26entry.906535625%3DYongyang%2520Zhou%2520and%2520Fang-Lue%2520Zhang%2520and%2520Zichen%2520Wang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520demonstrated%2520impressive%2520capabilities%2520in%2520novel%2520view%2520synthesis.%2520However%252C%2520rendering%2520reflective%2520objects%2520remains%2520a%2520significant%2520challenge%252C%2520particularly%2520in%2520inverse%2520rendering%2520and%2520relighting.%2520We%2520introduce%2520RTR-GS%252C%2520a%2520novel%2520inverse%2520rendering%2520framework%2520capable%2520of%2520robustly%2520rendering%2520objects%2520with%2520arbitrary%2520reflectance%2520properties%252C%2520decomposing%2520BRDF%2520and%2520lighting%252C%2520and%2520delivering%2520credible%2520relighting%2520results.%2520Given%2520a%2520collection%2520of%2520multi-view%2520images%252C%2520our%2520method%2520effectively%2520recovers%2520geometric%2520structure%2520through%2520a%2520hybrid%2520rendering%2520model%2520that%2520combines%2520forward%2520rendering%2520for%2520radiance%2520transfer%2520with%2520deferred%2520rendering%2520for%2520reflections.%2520This%2520approach%2520successfully%2520separates%2520high-frequency%2520and%2520low-frequency%2520appearances%252C%2520mitigating%2520floating%2520artifacts%2520caused%2520by%2520spherical%2520harmonic%2520overfitting%2520when%2520handling%2520high-frequency%2520details.%2520We%2520further%2520refine%2520BRDF%2520and%2520lighting%2520decomposition%2520using%2520an%2520additional%2520physically-based%2520deferred%2520rendering%2520branch.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520enhances%2520novel%2520view%2520synthesis%252C%2520normal%2520estimation%252C%2520decomposition%252C%2520and%2520relighting%2520while%2520maintaining%2520efficient%2520training%2520inference%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTR-GS%3A%203D%20Gaussian%20Splatting%20for%20Inverse%20Rendering%20with%20Radiance%20Transfer%20and%20Reflection&entry.906535625=Yongyang%20Zhou%20and%20Fang-Lue%20Zhang%20and%20Zichen%20Wang%20and%20Lei%20Zhang&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20has%20demonstrated%20impressive%20capabilities%20in%20novel%20view%20synthesis.%20However%2C%20rendering%20reflective%20objects%20remains%20a%20significant%20challenge%2C%20particularly%20in%20inverse%20rendering%20and%20relighting.%20We%20introduce%20RTR-GS%2C%20a%20novel%20inverse%20rendering%20framework%20capable%20of%20robustly%20rendering%20objects%20with%20arbitrary%20reflectance%20properties%2C%20decomposing%20BRDF%20and%20lighting%2C%20and%20delivering%20credible%20relighting%20results.%20Given%20a%20collection%20of%20multi-view%20images%2C%20our%20method%20effectively%20recovers%20geometric%20structure%20through%20a%20hybrid%20rendering%20model%20that%20combines%20forward%20rendering%20for%20radiance%20transfer%20with%20deferred%20rendering%20for%20reflections.%20This%20approach%20successfully%20separates%20high-frequency%20and%20low-frequency%20appearances%2C%20mitigating%20floating%20artifacts%20caused%20by%20spherical%20harmonic%20overfitting%20when%20handling%20high-frequency%20details.%20We%20further%20refine%20BRDF%20and%20lighting%20decomposition%20using%20an%20additional%20physically-based%20deferred%20rendering%20branch.%20Experimental%20results%20show%20that%20our%20method%20enhances%20novel%20view%20synthesis%2C%20normal%20estimation%2C%20decomposition%2C%20and%20relighting%20while%20maintaining%20efficient%20training%20inference%20process.&entry.1838667208=http%3A//arxiv.org/abs/2507.07733v2&entry.124074799=Read"},
{"title": "GT2-GS: Geometry-aware Texture Transfer for Gaussian Splatting", "author": "Wenjie Liu and Zhongliang Liu and Junwei Shu and Changbo Wang and Yang Li", "abstract": "Transferring 2D textures to 3D modalities is of great significance for improving the efficiency of multimedia content creation. Existing approaches have rarely focused on transferring image textures onto 3D representations. 3D style transfer methods are capable of transferring abstract artistic styles to 3D scenes. However, these methods often overlook the geometric information of the scene, which makes it challenging to achieve high-quality 3D texture transfer results. In this paper, we present GT^2-GS, a geometry-aware texture transfer framework for gaussian splitting. From the perspective of matching texture features with geometric information in rendered views, we identify the issue of insufficient texture features and propose a geometry-aware texture augmentation module to expand the texture feature set. Moreover, a geometry-consistent texture loss is proposed to optimize texture features into the scene representation. This loss function incorporates both camera pose and 3D geometric information of the scene, enabling controllable texture-oriented appearance editing. Finally, a geometry preservation strategy is introduced. By alternating between the texture transfer and geometry correction stages over multiple iterations, this strategy achieves a balance between learning texture features and preserving geometric integrity. Extensive experiments demonstrate the effectiveness and controllability of our method. Through geometric awareness, our approach achieves texture transfer results that better align with human visual perception. Our homepage is available at https://vpx-ecnu.github.io/GT2-GS-website.", "link": "http://arxiv.org/abs/2505.15208v2", "date": "2025-12-16", "relevancy": 3.2077, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6729}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6542}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GT2-GS%3A%20Geometry-aware%20Texture%20Transfer%20for%20Gaussian%20Splatting&body=Title%3A%20GT2-GS%3A%20Geometry-aware%20Texture%20Transfer%20for%20Gaussian%20Splatting%0AAuthor%3A%20Wenjie%20Liu%20and%20Zhongliang%20Liu%20and%20Junwei%20Shu%20and%20Changbo%20Wang%20and%20Yang%20Li%0AAbstract%3A%20Transferring%202D%20textures%20to%203D%20modalities%20is%20of%20great%20significance%20for%20improving%20the%20efficiency%20of%20multimedia%20content%20creation.%20Existing%20approaches%20have%20rarely%20focused%20on%20transferring%20image%20textures%20onto%203D%20representations.%203D%20style%20transfer%20methods%20are%20capable%20of%20transferring%20abstract%20artistic%20styles%20to%203D%20scenes.%20However%2C%20these%20methods%20often%20overlook%20the%20geometric%20information%20of%20the%20scene%2C%20which%20makes%20it%20challenging%20to%20achieve%20high-quality%203D%20texture%20transfer%20results.%20In%20this%20paper%2C%20we%20present%20GT%5E2-GS%2C%20a%20geometry-aware%20texture%20transfer%20framework%20for%20gaussian%20splitting.%20From%20the%20perspective%20of%20matching%20texture%20features%20with%20geometric%20information%20in%20rendered%20views%2C%20we%20identify%20the%20issue%20of%20insufficient%20texture%20features%20and%20propose%20a%20geometry-aware%20texture%20augmentation%20module%20to%20expand%20the%20texture%20feature%20set.%20Moreover%2C%20a%20geometry-consistent%20texture%20loss%20is%20proposed%20to%20optimize%20texture%20features%20into%20the%20scene%20representation.%20This%20loss%20function%20incorporates%20both%20camera%20pose%20and%203D%20geometric%20information%20of%20the%20scene%2C%20enabling%20controllable%20texture-oriented%20appearance%20editing.%20Finally%2C%20a%20geometry%20preservation%20strategy%20is%20introduced.%20By%20alternating%20between%20the%20texture%20transfer%20and%20geometry%20correction%20stages%20over%20multiple%20iterations%2C%20this%20strategy%20achieves%20a%20balance%20between%20learning%20texture%20features%20and%20preserving%20geometric%20integrity.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%20controllability%20of%20our%20method.%20Through%20geometric%20awareness%2C%20our%20approach%20achieves%20texture%20transfer%20results%20that%20better%20align%20with%20human%20visual%20perception.%20Our%20homepage%20is%20available%20at%20https%3A//vpx-ecnu.github.io/GT2-GS-website.%0ALink%3A%20http%3A//arxiv.org/abs/2505.15208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGT2-GS%253A%2520Geometry-aware%2520Texture%2520Transfer%2520for%2520Gaussian%2520Splatting%26entry.906535625%3DWenjie%2520Liu%2520and%2520Zhongliang%2520Liu%2520and%2520Junwei%2520Shu%2520and%2520Changbo%2520Wang%2520and%2520Yang%2520Li%26entry.1292438233%3DTransferring%25202D%2520textures%2520to%25203D%2520modalities%2520is%2520of%2520great%2520significance%2520for%2520improving%2520the%2520efficiency%2520of%2520multimedia%2520content%2520creation.%2520Existing%2520approaches%2520have%2520rarely%2520focused%2520on%2520transferring%2520image%2520textures%2520onto%25203D%2520representations.%25203D%2520style%2520transfer%2520methods%2520are%2520capable%2520of%2520transferring%2520abstract%2520artistic%2520styles%2520to%25203D%2520scenes.%2520However%252C%2520these%2520methods%2520often%2520overlook%2520the%2520geometric%2520information%2520of%2520the%2520scene%252C%2520which%2520makes%2520it%2520challenging%2520to%2520achieve%2520high-quality%25203D%2520texture%2520transfer%2520results.%2520In%2520this%2520paper%252C%2520we%2520present%2520GT%255E2-GS%252C%2520a%2520geometry-aware%2520texture%2520transfer%2520framework%2520for%2520gaussian%2520splitting.%2520From%2520the%2520perspective%2520of%2520matching%2520texture%2520features%2520with%2520geometric%2520information%2520in%2520rendered%2520views%252C%2520we%2520identify%2520the%2520issue%2520of%2520insufficient%2520texture%2520features%2520and%2520propose%2520a%2520geometry-aware%2520texture%2520augmentation%2520module%2520to%2520expand%2520the%2520texture%2520feature%2520set.%2520Moreover%252C%2520a%2520geometry-consistent%2520texture%2520loss%2520is%2520proposed%2520to%2520optimize%2520texture%2520features%2520into%2520the%2520scene%2520representation.%2520This%2520loss%2520function%2520incorporates%2520both%2520camera%2520pose%2520and%25203D%2520geometric%2520information%2520of%2520the%2520scene%252C%2520enabling%2520controllable%2520texture-oriented%2520appearance%2520editing.%2520Finally%252C%2520a%2520geometry%2520preservation%2520strategy%2520is%2520introduced.%2520By%2520alternating%2520between%2520the%2520texture%2520transfer%2520and%2520geometry%2520correction%2520stages%2520over%2520multiple%2520iterations%252C%2520this%2520strategy%2520achieves%2520a%2520balance%2520between%2520learning%2520texture%2520features%2520and%2520preserving%2520geometric%2520integrity.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520and%2520controllability%2520of%2520our%2520method.%2520Through%2520geometric%2520awareness%252C%2520our%2520approach%2520achieves%2520texture%2520transfer%2520results%2520that%2520better%2520align%2520with%2520human%2520visual%2520perception.%2520Our%2520homepage%2520is%2520available%2520at%2520https%253A//vpx-ecnu.github.io/GT2-GS-website.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GT2-GS%3A%20Geometry-aware%20Texture%20Transfer%20for%20Gaussian%20Splatting&entry.906535625=Wenjie%20Liu%20and%20Zhongliang%20Liu%20and%20Junwei%20Shu%20and%20Changbo%20Wang%20and%20Yang%20Li&entry.1292438233=Transferring%202D%20textures%20to%203D%20modalities%20is%20of%20great%20significance%20for%20improving%20the%20efficiency%20of%20multimedia%20content%20creation.%20Existing%20approaches%20have%20rarely%20focused%20on%20transferring%20image%20textures%20onto%203D%20representations.%203D%20style%20transfer%20methods%20are%20capable%20of%20transferring%20abstract%20artistic%20styles%20to%203D%20scenes.%20However%2C%20these%20methods%20often%20overlook%20the%20geometric%20information%20of%20the%20scene%2C%20which%20makes%20it%20challenging%20to%20achieve%20high-quality%203D%20texture%20transfer%20results.%20In%20this%20paper%2C%20we%20present%20GT%5E2-GS%2C%20a%20geometry-aware%20texture%20transfer%20framework%20for%20gaussian%20splitting.%20From%20the%20perspective%20of%20matching%20texture%20features%20with%20geometric%20information%20in%20rendered%20views%2C%20we%20identify%20the%20issue%20of%20insufficient%20texture%20features%20and%20propose%20a%20geometry-aware%20texture%20augmentation%20module%20to%20expand%20the%20texture%20feature%20set.%20Moreover%2C%20a%20geometry-consistent%20texture%20loss%20is%20proposed%20to%20optimize%20texture%20features%20into%20the%20scene%20representation.%20This%20loss%20function%20incorporates%20both%20camera%20pose%20and%203D%20geometric%20information%20of%20the%20scene%2C%20enabling%20controllable%20texture-oriented%20appearance%20editing.%20Finally%2C%20a%20geometry%20preservation%20strategy%20is%20introduced.%20By%20alternating%20between%20the%20texture%20transfer%20and%20geometry%20correction%20stages%20over%20multiple%20iterations%2C%20this%20strategy%20achieves%20a%20balance%20between%20learning%20texture%20features%20and%20preserving%20geometric%20integrity.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20and%20controllability%20of%20our%20method.%20Through%20geometric%20awareness%2C%20our%20approach%20achieves%20texture%20transfer%20results%20that%20better%20align%20with%20human%20visual%20perception.%20Our%20homepage%20is%20available%20at%20https%3A//vpx-ecnu.github.io/GT2-GS-website.&entry.1838667208=http%3A//arxiv.org/abs/2505.15208v2&entry.124074799=Read"},
{"title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting", "author": "Shilong Jin and Haoran Duan and Litao Hua and Wentao Huang and Yuan Zhou", "abstract": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.", "link": "http://arxiv.org/abs/2512.07345v2", "date": "2025-12-16", "relevancy": 3.0854, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6311}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6128}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Debiasing%20Diffusion%20Priors%20via%203D%20Attention%20for%20Consistent%20Gaussian%20Splatting&body=Title%3A%20Debiasing%20Diffusion%20Priors%20via%203D%20Attention%20for%20Consistent%20Gaussian%20Splatting%0AAuthor%3A%20Shilong%20Jin%20and%20Haoran%20Duan%20and%20Litao%20Hua%20and%20Wentao%20Huang%20and%20Yuan%20Zhou%0AAbstract%3A%20Versatile%203D%20tasks%20%28e.g.%2C%20generation%20or%20editing%29%20that%20distill%20from%20Text-to-Image%20%28T2I%29%20diffusion%20models%20have%20attracted%20significant%20research%20interest%20for%20not%20relying%20on%20extensive%203D%20training%20data.%20However%2C%20T2I%20models%20exhibit%20limitations%20resulting%20from%20prior%20view%20bias%2C%20which%20produces%20conflicting%20appearances%20between%20different%20views%20of%20an%20object.%20This%20bias%20causes%20subject-words%20to%20preferentially%20activate%20prior%20view%20features%20during%20cross-attention%20%28CA%29%20computation%2C%20regardless%20of%20the%20target%20view%20condition.%20To%20overcome%20this%20limitation%2C%20we%20conduct%20a%20comprehensive%20mathematical%20analysis%20to%20reveal%20the%20root%20cause%20of%20the%20prior%20view%20bias%20in%20T2I%20models.%20Moreover%2C%20we%20find%20different%20UNet%20layers%20show%20different%20effects%20of%20prior%20view%20in%20CA.%20Therefore%2C%20we%20propose%20a%20novel%20framework%2C%20TD-Attn%2C%20which%20addresses%20multi-view%20inconsistency%20via%20two%20key%20components%3A%20%281%29%20the%203D-Aware%20Attention%20Guidance%20Module%20%283D-AAG%29%20constructs%20a%20view-consistent%203D%20attention%20Gaussian%20for%20subject-words%20to%20enforce%20spatial%20consistency%20across%20attention-focused%20regions%2C%20thereby%20compensating%20for%20the%20limited%20spatial%20information%20in%202D%20individual%20view%20CA%20maps%3B%20%282%29%20the%20Hierarchical%20Attention%20Modulation%20Module%20%28HAM%29%20utilizes%20a%20Semantic%20Guidance%20Tree%20%28SGT%29%20to%20direct%20the%20Semantic%20Response%20Profiler%20%28SRP%29%20in%20localizing%20and%20modulating%20CA%20layers%20that%20are%20highly%20responsive%20to%20view%20conditions%2C%20where%20the%20enhanced%20CA%20maps%20further%20support%20the%20construction%20of%20more%20consistent%203D%20attention%20Gaussians.%20Notably%2C%20HAM%20facilitates%20semantic-specific%20interventions%2C%20enabling%20controllable%20and%20precise%203D%20editing.%20Extensive%20experiments%20firmly%20establish%20that%20TD-Attn%20has%20the%20potential%20to%20serve%20as%20a%20universal%20plugin%2C%20significantly%20enhancing%20multi-view%20consistency%20across%203D%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebiasing%2520Diffusion%2520Priors%2520via%25203D%2520Attention%2520for%2520Consistent%2520Gaussian%2520Splatting%26entry.906535625%3DShilong%2520Jin%2520and%2520Haoran%2520Duan%2520and%2520Litao%2520Hua%2520and%2520Wentao%2520Huang%2520and%2520Yuan%2520Zhou%26entry.1292438233%3DVersatile%25203D%2520tasks%2520%2528e.g.%252C%2520generation%2520or%2520editing%2529%2520that%2520distill%2520from%2520Text-to-Image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520attracted%2520significant%2520research%2520interest%2520for%2520not%2520relying%2520on%2520extensive%25203D%2520training%2520data.%2520However%252C%2520T2I%2520models%2520exhibit%2520limitations%2520resulting%2520from%2520prior%2520view%2520bias%252C%2520which%2520produces%2520conflicting%2520appearances%2520between%2520different%2520views%2520of%2520an%2520object.%2520This%2520bias%2520causes%2520subject-words%2520to%2520preferentially%2520activate%2520prior%2520view%2520features%2520during%2520cross-attention%2520%2528CA%2529%2520computation%252C%2520regardless%2520of%2520the%2520target%2520view%2520condition.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520conduct%2520a%2520comprehensive%2520mathematical%2520analysis%2520to%2520reveal%2520the%2520root%2520cause%2520of%2520the%2520prior%2520view%2520bias%2520in%2520T2I%2520models.%2520Moreover%252C%2520we%2520find%2520different%2520UNet%2520layers%2520show%2520different%2520effects%2520of%2520prior%2520view%2520in%2520CA.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520TD-Attn%252C%2520which%2520addresses%2520multi-view%2520inconsistency%2520via%2520two%2520key%2520components%253A%2520%25281%2529%2520the%25203D-Aware%2520Attention%2520Guidance%2520Module%2520%25283D-AAG%2529%2520constructs%2520a%2520view-consistent%25203D%2520attention%2520Gaussian%2520for%2520subject-words%2520to%2520enforce%2520spatial%2520consistency%2520across%2520attention-focused%2520regions%252C%2520thereby%2520compensating%2520for%2520the%2520limited%2520spatial%2520information%2520in%25202D%2520individual%2520view%2520CA%2520maps%253B%2520%25282%2529%2520the%2520Hierarchical%2520Attention%2520Modulation%2520Module%2520%2528HAM%2529%2520utilizes%2520a%2520Semantic%2520Guidance%2520Tree%2520%2528SGT%2529%2520to%2520direct%2520the%2520Semantic%2520Response%2520Profiler%2520%2528SRP%2529%2520in%2520localizing%2520and%2520modulating%2520CA%2520layers%2520that%2520are%2520highly%2520responsive%2520to%2520view%2520conditions%252C%2520where%2520the%2520enhanced%2520CA%2520maps%2520further%2520support%2520the%2520construction%2520of%2520more%2520consistent%25203D%2520attention%2520Gaussians.%2520Notably%252C%2520HAM%2520facilitates%2520semantic-specific%2520interventions%252C%2520enabling%2520controllable%2520and%2520precise%25203D%2520editing.%2520Extensive%2520experiments%2520firmly%2520establish%2520that%2520TD-Attn%2520has%2520the%2520potential%2520to%2520serve%2520as%2520a%2520universal%2520plugin%252C%2520significantly%2520enhancing%2520multi-view%2520consistency%2520across%25203D%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debiasing%20Diffusion%20Priors%20via%203D%20Attention%20for%20Consistent%20Gaussian%20Splatting&entry.906535625=Shilong%20Jin%20and%20Haoran%20Duan%20and%20Litao%20Hua%20and%20Wentao%20Huang%20and%20Yuan%20Zhou&entry.1292438233=Versatile%203D%20tasks%20%28e.g.%2C%20generation%20or%20editing%29%20that%20distill%20from%20Text-to-Image%20%28T2I%29%20diffusion%20models%20have%20attracted%20significant%20research%20interest%20for%20not%20relying%20on%20extensive%203D%20training%20data.%20However%2C%20T2I%20models%20exhibit%20limitations%20resulting%20from%20prior%20view%20bias%2C%20which%20produces%20conflicting%20appearances%20between%20different%20views%20of%20an%20object.%20This%20bias%20causes%20subject-words%20to%20preferentially%20activate%20prior%20view%20features%20during%20cross-attention%20%28CA%29%20computation%2C%20regardless%20of%20the%20target%20view%20condition.%20To%20overcome%20this%20limitation%2C%20we%20conduct%20a%20comprehensive%20mathematical%20analysis%20to%20reveal%20the%20root%20cause%20of%20the%20prior%20view%20bias%20in%20T2I%20models.%20Moreover%2C%20we%20find%20different%20UNet%20layers%20show%20different%20effects%20of%20prior%20view%20in%20CA.%20Therefore%2C%20we%20propose%20a%20novel%20framework%2C%20TD-Attn%2C%20which%20addresses%20multi-view%20inconsistency%20via%20two%20key%20components%3A%20%281%29%20the%203D-Aware%20Attention%20Guidance%20Module%20%283D-AAG%29%20constructs%20a%20view-consistent%203D%20attention%20Gaussian%20for%20subject-words%20to%20enforce%20spatial%20consistency%20across%20attention-focused%20regions%2C%20thereby%20compensating%20for%20the%20limited%20spatial%20information%20in%202D%20individual%20view%20CA%20maps%3B%20%282%29%20the%20Hierarchical%20Attention%20Modulation%20Module%20%28HAM%29%20utilizes%20a%20Semantic%20Guidance%20Tree%20%28SGT%29%20to%20direct%20the%20Semantic%20Response%20Profiler%20%28SRP%29%20in%20localizing%20and%20modulating%20CA%20layers%20that%20are%20highly%20responsive%20to%20view%20conditions%2C%20where%20the%20enhanced%20CA%20maps%20further%20support%20the%20construction%20of%20more%20consistent%203D%20attention%20Gaussians.%20Notably%2C%20HAM%20facilitates%20semantic-specific%20interventions%2C%20enabling%20controllable%20and%20precise%203D%20editing.%20Extensive%20experiments%20firmly%20establish%20that%20TD-Attn%20has%20the%20potential%20to%20serve%20as%20a%20universal%20plugin%2C%20significantly%20enhancing%20multi-view%20consistency%20across%203D%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.07345v2&entry.124074799=Read"},
{"title": "MMGR: Multi-Modal Generative Reasoning", "author": "Zefan Cai and Haoyi Qiu and Tianyi Ma and Haozhe Zhao and Gengze Zhou and Kung-Hsiang Huang and Parisa Kordjamshidi and Minjia Zhang and Xiao Wen and Jiuxiang Gu and Nanyun Peng and Junjie Hu", "abstract": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "link": "http://arxiv.org/abs/2512.14691v1", "date": "2025-12-16", "relevancy": 3.0388, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6224}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6035}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMGR%3A%20Multi-Modal%20Generative%20Reasoning&body=Title%3A%20MMGR%3A%20Multi-Modal%20Generative%20Reasoning%0AAuthor%3A%20Zefan%20Cai%20and%20Haoyi%20Qiu%20and%20Tianyi%20Ma%20and%20Haozhe%20Zhao%20and%20Gengze%20Zhou%20and%20Kung-Hsiang%20Huang%20and%20Parisa%20Kordjamshidi%20and%20Minjia%20Zhang%20and%20Xiao%20Wen%20and%20Jiuxiang%20Gu%20and%20Nanyun%20Peng%20and%20Junjie%20Hu%0AAbstract%3A%20Video%20foundation%20models%20generate%20visually%20realistic%20and%20temporally%20coherent%20content%2C%20but%20their%20reliability%20as%20world%20simulators%20depends%20on%20whether%20they%20capture%20physical%2C%20logical%2C%20and%20spatial%20constraints.%20Existing%20metrics%20such%20as%20Frechet%20Video%20Distance%20%28FVD%29%20emphasize%20perceptual%20quality%20and%20overlook%20reasoning%20failures%2C%20including%20violations%20of%20causality%2C%20physics%2C%20and%20global%20consistency.%20We%20introduce%20MMGR%20%28Multi-Modal%20Generative%20Reasoning%20Evaluation%20and%20Benchmark%29%2C%20a%20principled%20evaluation%20framework%20based%20on%20five%20reasoning%20abilities%3A%20Physical%2C%20Logical%2C%203D%20Spatial%2C%202D%20Spatial%2C%20and%20Temporal.%20MMGR%20evaluates%20generative%20reasoning%20across%20three%20domains%3A%20Abstract%20Reasoning%20%28ARC-AGI%2C%20Sudoku%29%2C%20Embodied%20Navigation%20%28real-world%203D%20navigation%20and%20localization%29%2C%20and%20Physical%20Commonsense%20%28sports%20and%20compositional%20interactions%29.%20MMGR%20applies%20fine-grained%20metrics%20that%20require%20holistic%20correctness%20across%20both%20video%20and%20image%20generation.%20We%20benchmark%20leading%20video%20models%20%28Veo-3%2C%20Sora-2%2C%20Wan-2.2%29%20and%20image%20models%20%28Nano-banana%2C%20Nano-banana%20Pro%2C%20GPT-4o-image%2C%20Qwen-image%29%2C%20revealing%20strong%20performance%20gaps%20across%20domains.%20Models%20show%20moderate%20success%20on%20Physical%20Commonsense%20tasks%20but%20perform%20poorly%20on%20Abstract%20Reasoning%20%28below%2010%20percent%20accuracy%20on%20ARC-AGI%29%20and%20struggle%20with%20long-horizon%20spatial%20planning%20in%20embodied%20settings.%20Our%20analysis%20highlights%20key%20limitations%20in%20current%20models%2C%20including%20overreliance%20on%20perceptual%20data%2C%20weak%20global%20state%20consistency%2C%20and%20objectives%20that%20reward%20visual%20plausibility%20over%20causal%20correctness.%20MMGR%20offers%20a%20unified%20diagnostic%20benchmark%20and%20a%20path%20toward%20reasoning-aware%20generative%20world%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMGR%253A%2520Multi-Modal%2520Generative%2520Reasoning%26entry.906535625%3DZefan%2520Cai%2520and%2520Haoyi%2520Qiu%2520and%2520Tianyi%2520Ma%2520and%2520Haozhe%2520Zhao%2520and%2520Gengze%2520Zhou%2520and%2520Kung-Hsiang%2520Huang%2520and%2520Parisa%2520Kordjamshidi%2520and%2520Minjia%2520Zhang%2520and%2520Xiao%2520Wen%2520and%2520Jiuxiang%2520Gu%2520and%2520Nanyun%2520Peng%2520and%2520Junjie%2520Hu%26entry.1292438233%3DVideo%2520foundation%2520models%2520generate%2520visually%2520realistic%2520and%2520temporally%2520coherent%2520content%252C%2520but%2520their%2520reliability%2520as%2520world%2520simulators%2520depends%2520on%2520whether%2520they%2520capture%2520physical%252C%2520logical%252C%2520and%2520spatial%2520constraints.%2520Existing%2520metrics%2520such%2520as%2520Frechet%2520Video%2520Distance%2520%2528FVD%2529%2520emphasize%2520perceptual%2520quality%2520and%2520overlook%2520reasoning%2520failures%252C%2520including%2520violations%2520of%2520causality%252C%2520physics%252C%2520and%2520global%2520consistency.%2520We%2520introduce%2520MMGR%2520%2528Multi-Modal%2520Generative%2520Reasoning%2520Evaluation%2520and%2520Benchmark%2529%252C%2520a%2520principled%2520evaluation%2520framework%2520based%2520on%2520five%2520reasoning%2520abilities%253A%2520Physical%252C%2520Logical%252C%25203D%2520Spatial%252C%25202D%2520Spatial%252C%2520and%2520Temporal.%2520MMGR%2520evaluates%2520generative%2520reasoning%2520across%2520three%2520domains%253A%2520Abstract%2520Reasoning%2520%2528ARC-AGI%252C%2520Sudoku%2529%252C%2520Embodied%2520Navigation%2520%2528real-world%25203D%2520navigation%2520and%2520localization%2529%252C%2520and%2520Physical%2520Commonsense%2520%2528sports%2520and%2520compositional%2520interactions%2529.%2520MMGR%2520applies%2520fine-grained%2520metrics%2520that%2520require%2520holistic%2520correctness%2520across%2520both%2520video%2520and%2520image%2520generation.%2520We%2520benchmark%2520leading%2520video%2520models%2520%2528Veo-3%252C%2520Sora-2%252C%2520Wan-2.2%2529%2520and%2520image%2520models%2520%2528Nano-banana%252C%2520Nano-banana%2520Pro%252C%2520GPT-4o-image%252C%2520Qwen-image%2529%252C%2520revealing%2520strong%2520performance%2520gaps%2520across%2520domains.%2520Models%2520show%2520moderate%2520success%2520on%2520Physical%2520Commonsense%2520tasks%2520but%2520perform%2520poorly%2520on%2520Abstract%2520Reasoning%2520%2528below%252010%2520percent%2520accuracy%2520on%2520ARC-AGI%2529%2520and%2520struggle%2520with%2520long-horizon%2520spatial%2520planning%2520in%2520embodied%2520settings.%2520Our%2520analysis%2520highlights%2520key%2520limitations%2520in%2520current%2520models%252C%2520including%2520overreliance%2520on%2520perceptual%2520data%252C%2520weak%2520global%2520state%2520consistency%252C%2520and%2520objectives%2520that%2520reward%2520visual%2520plausibility%2520over%2520causal%2520correctness.%2520MMGR%2520offers%2520a%2520unified%2520diagnostic%2520benchmark%2520and%2520a%2520path%2520toward%2520reasoning-aware%2520generative%2520world%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMGR%3A%20Multi-Modal%20Generative%20Reasoning&entry.906535625=Zefan%20Cai%20and%20Haoyi%20Qiu%20and%20Tianyi%20Ma%20and%20Haozhe%20Zhao%20and%20Gengze%20Zhou%20and%20Kung-Hsiang%20Huang%20and%20Parisa%20Kordjamshidi%20and%20Minjia%20Zhang%20and%20Xiao%20Wen%20and%20Jiuxiang%20Gu%20and%20Nanyun%20Peng%20and%20Junjie%20Hu&entry.1292438233=Video%20foundation%20models%20generate%20visually%20realistic%20and%20temporally%20coherent%20content%2C%20but%20their%20reliability%20as%20world%20simulators%20depends%20on%20whether%20they%20capture%20physical%2C%20logical%2C%20and%20spatial%20constraints.%20Existing%20metrics%20such%20as%20Frechet%20Video%20Distance%20%28FVD%29%20emphasize%20perceptual%20quality%20and%20overlook%20reasoning%20failures%2C%20including%20violations%20of%20causality%2C%20physics%2C%20and%20global%20consistency.%20We%20introduce%20MMGR%20%28Multi-Modal%20Generative%20Reasoning%20Evaluation%20and%20Benchmark%29%2C%20a%20principled%20evaluation%20framework%20based%20on%20five%20reasoning%20abilities%3A%20Physical%2C%20Logical%2C%203D%20Spatial%2C%202D%20Spatial%2C%20and%20Temporal.%20MMGR%20evaluates%20generative%20reasoning%20across%20three%20domains%3A%20Abstract%20Reasoning%20%28ARC-AGI%2C%20Sudoku%29%2C%20Embodied%20Navigation%20%28real-world%203D%20navigation%20and%20localization%29%2C%20and%20Physical%20Commonsense%20%28sports%20and%20compositional%20interactions%29.%20MMGR%20applies%20fine-grained%20metrics%20that%20require%20holistic%20correctness%20across%20both%20video%20and%20image%20generation.%20We%20benchmark%20leading%20video%20models%20%28Veo-3%2C%20Sora-2%2C%20Wan-2.2%29%20and%20image%20models%20%28Nano-banana%2C%20Nano-banana%20Pro%2C%20GPT-4o-image%2C%20Qwen-image%29%2C%20revealing%20strong%20performance%20gaps%20across%20domains.%20Models%20show%20moderate%20success%20on%20Physical%20Commonsense%20tasks%20but%20perform%20poorly%20on%20Abstract%20Reasoning%20%28below%2010%20percent%20accuracy%20on%20ARC-AGI%29%20and%20struggle%20with%20long-horizon%20spatial%20planning%20in%20embodied%20settings.%20Our%20analysis%20highlights%20key%20limitations%20in%20current%20models%2C%20including%20overreliance%20on%20perceptual%20data%2C%20weak%20global%20state%20consistency%2C%20and%20objectives%20that%20reward%20visual%20plausibility%20over%20causal%20correctness.%20MMGR%20offers%20a%20unified%20diagnostic%20benchmark%20and%20a%20path%20toward%20reasoning-aware%20generative%20world%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.14691v1&entry.124074799=Read"},
{"title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "author": "Heyi Chen and Siyan Chen and Xin Chen and Yanfei Chen and Ying Chen and Zhuo Chen and Feng Cheng and Tianheng Cheng and Xinqi Cheng and Xuyan Chi and Jian Cong and Jing Cui and Qinpeng Cui and Qide Dong and Junliang Fan and Jing Fang and Zetao Fang and Chengjian Feng and Han Feng and Mingyuan Gao and Yu Gao and Dong Guo and Qiushan Guo and Boyang Hao and Qingkai Hao and Bibo He and Qian He and Tuyen Hoang and Ruoqing Hu and Xi Hu and Weilin Huang and Zhaoyang Huang and Zhongyi Huang and Donglei Ji and Siqi Jiang and Wei Jiang and Yunpu Jiang and Zhuo Jiang and Ashley Kim and Jianan Kong and Zhichao Lai and Shanshan Lao and Yichong Leng and Ai Li and Feiya Li and Gen Li and Huixia Li and JiaShi Li and Liang Li and Ming Li and Shanshan Li and Tao Li and Xian Li and Xiaojie Li and Xiaoyang Li and Xingxing Li and Yameng Li and Yifu Li and Yiying Li and Chao Liang and Han Liang and Jianzhong Liang and Ying Liang and Zhiqiang Liang and Wang Liao and Yalin Liao and Heng Lin and Kengyu Lin and Shanchuan Lin and Xi Lin and Zhijie Lin and Feng Ling and Fangfang Liu and Gaohong Liu and Jiawei Liu and Jie Liu and Jihao Liu and Shouda Liu and Shu Liu and Sichao Liu and Songwei Liu and Xin Liu and Xue Liu and Yibo Liu and Zikun Liu and Zuxi Liu and Junlin Lyu and Lecheng Lyu and Qian Lyu and Han Mu and Xiaonan Nie and Jingzhe Ning and Xitong Pan and Yanghua Peng and Lianke Qin and Xueqiong Qu and Yuxi Ren and Kai Shen and Guang Shi and Lei Shi and Yan Song and Yinglong Song and Fan Sun and Li Sun and Renfei Sun and Yan Sun and Zeyu Sun and Wenjing Tang and Yaxue Tang and Zirui Tao and Feng Wang and Furui Wang and Jinran Wang and Junkai Wang and Ke Wang and Kexin Wang and Qingyi Wang and Rui Wang and Sen Wang and Shuai Wang and Tingru Wang and Weichen Wang and Xin Wang and Yanhui Wang and Yue Wang and Yuping Wang and Yuxuan Wang and Ziyu Wang and Guoqiang Wei and Wanru Wei and Di Wu and Guohong Wu and Hanjie Wu and Jian Wu and Jie Wu and Ruolan Wu and Xinglong Wu and Yonghui Wu and Ruiqi Xia and Liang Xiang and Fei Xiao and XueFeng Xiao and Pan Xie and Shuangyi Xie and Shuang Xu and Jinlan Xue and Shen Yan and Bangbang Yang and Ceyuan Yang and Jiaqi Yang and Runkai Yang and Tao Yang and Yang Yang and Yihang Yang and ZhiXian Yang and Ziyan Yang and Songting Yao and Yifan Yao and Zilyu Ye and Bowen Yu and Jian Yu and Chujie Yuan and Linxiao Yuan and Sichun Zeng and Weihong Zeng and Xuejiao Zeng and Yan Zeng and Chuntao Zhang and Heng Zhang and Jingjie Zhang and Kuo Zhang and Liang Zhang and Liying Zhang and Manlin Zhang and Ting Zhang and Weida Zhang and Xiaohe Zhang and Xinyan Zhang and Yan Zhang and Yuan Zhang and Zixiang Zhang and Fengxuan Zhao and Huating Zhao and Yang Zhao and Hao Zheng and Jianbin Zheng and Xiaozheng Zheng and Yangyang Zheng and Yijie Zheng and Jiexin Zhou and Jiahui Zhu and Kuan Zhu and Shenhan Zhu and Wenjia Zhu and Benhui Zou and Feilong Zuo", "abstract": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "link": "http://arxiv.org/abs/2512.13507v2", "date": "2025-12-16", "relevancy": 2.951, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6032}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5855}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model&body=Title%3A%20Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model%0AAuthor%3A%20Heyi%20Chen%20and%20Siyan%20Chen%20and%20Xin%20Chen%20and%20Yanfei%20Chen%20and%20Ying%20Chen%20and%20Zhuo%20Chen%20and%20Feng%20Cheng%20and%20Tianheng%20Cheng%20and%20Xinqi%20Cheng%20and%20Xuyan%20Chi%20and%20Jian%20Cong%20and%20Jing%20Cui%20and%20Qinpeng%20Cui%20and%20Qide%20Dong%20and%20Junliang%20Fan%20and%20Jing%20Fang%20and%20Zetao%20Fang%20and%20Chengjian%20Feng%20and%20Han%20Feng%20and%20Mingyuan%20Gao%20and%20Yu%20Gao%20and%20Dong%20Guo%20and%20Qiushan%20Guo%20and%20Boyang%20Hao%20and%20Qingkai%20Hao%20and%20Bibo%20He%20and%20Qian%20He%20and%20Tuyen%20Hoang%20and%20Ruoqing%20Hu%20and%20Xi%20Hu%20and%20Weilin%20Huang%20and%20Zhaoyang%20Huang%20and%20Zhongyi%20Huang%20and%20Donglei%20Ji%20and%20Siqi%20Jiang%20and%20Wei%20Jiang%20and%20Yunpu%20Jiang%20and%20Zhuo%20Jiang%20and%20Ashley%20Kim%20and%20Jianan%20Kong%20and%20Zhichao%20Lai%20and%20Shanshan%20Lao%20and%20Yichong%20Leng%20and%20Ai%20Li%20and%20Feiya%20Li%20and%20Gen%20Li%20and%20Huixia%20Li%20and%20JiaShi%20Li%20and%20Liang%20Li%20and%20Ming%20Li%20and%20Shanshan%20Li%20and%20Tao%20Li%20and%20Xian%20Li%20and%20Xiaojie%20Li%20and%20Xiaoyang%20Li%20and%20Xingxing%20Li%20and%20Yameng%20Li%20and%20Yifu%20Li%20and%20Yiying%20Li%20and%20Chao%20Liang%20and%20Han%20Liang%20and%20Jianzhong%20Liang%20and%20Ying%20Liang%20and%20Zhiqiang%20Liang%20and%20Wang%20Liao%20and%20Yalin%20Liao%20and%20Heng%20Lin%20and%20Kengyu%20Lin%20and%20Shanchuan%20Lin%20and%20Xi%20Lin%20and%20Zhijie%20Lin%20and%20Feng%20Ling%20and%20Fangfang%20Liu%20and%20Gaohong%20Liu%20and%20Jiawei%20Liu%20and%20Jie%20Liu%20and%20Jihao%20Liu%20and%20Shouda%20Liu%20and%20Shu%20Liu%20and%20Sichao%20Liu%20and%20Songwei%20Liu%20and%20Xin%20Liu%20and%20Xue%20Liu%20and%20Yibo%20Liu%20and%20Zikun%20Liu%20and%20Zuxi%20Liu%20and%20Junlin%20Lyu%20and%20Lecheng%20Lyu%20and%20Qian%20Lyu%20and%20Han%20Mu%20and%20Xiaonan%20Nie%20and%20Jingzhe%20Ning%20and%20Xitong%20Pan%20and%20Yanghua%20Peng%20and%20Lianke%20Qin%20and%20Xueqiong%20Qu%20and%20Yuxi%20Ren%20and%20Kai%20Shen%20and%20Guang%20Shi%20and%20Lei%20Shi%20and%20Yan%20Song%20and%20Yinglong%20Song%20and%20Fan%20Sun%20and%20Li%20Sun%20and%20Renfei%20Sun%20and%20Yan%20Sun%20and%20Zeyu%20Sun%20and%20Wenjing%20Tang%20and%20Yaxue%20Tang%20and%20Zirui%20Tao%20and%20Feng%20Wang%20and%20Furui%20Wang%20and%20Jinran%20Wang%20and%20Junkai%20Wang%20and%20Ke%20Wang%20and%20Kexin%20Wang%20and%20Qingyi%20Wang%20and%20Rui%20Wang%20and%20Sen%20Wang%20and%20Shuai%20Wang%20and%20Tingru%20Wang%20and%20Weichen%20Wang%20and%20Xin%20Wang%20and%20Yanhui%20Wang%20and%20Yue%20Wang%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Ziyu%20Wang%20and%20Guoqiang%20Wei%20and%20Wanru%20Wei%20and%20Di%20Wu%20and%20Guohong%20Wu%20and%20Hanjie%20Wu%20and%20Jian%20Wu%20and%20Jie%20Wu%20and%20Ruolan%20Wu%20and%20Xinglong%20Wu%20and%20Yonghui%20Wu%20and%20Ruiqi%20Xia%20and%20Liang%20Xiang%20and%20Fei%20Xiao%20and%20XueFeng%20Xiao%20and%20Pan%20Xie%20and%20Shuangyi%20Xie%20and%20Shuang%20Xu%20and%20Jinlan%20Xue%20and%20Shen%20Yan%20and%20Bangbang%20Yang%20and%20Ceyuan%20Yang%20and%20Jiaqi%20Yang%20and%20Runkai%20Yang%20and%20Tao%20Yang%20and%20Yang%20Yang%20and%20Yihang%20Yang%20and%20ZhiXian%20Yang%20and%20Ziyan%20Yang%20and%20Songting%20Yao%20and%20Yifan%20Yao%20and%20Zilyu%20Ye%20and%20Bowen%20Yu%20and%20Jian%20Yu%20and%20Chujie%20Yuan%20and%20Linxiao%20Yuan%20and%20Sichun%20Zeng%20and%20Weihong%20Zeng%20and%20Xuejiao%20Zeng%20and%20Yan%20Zeng%20and%20Chuntao%20Zhang%20and%20Heng%20Zhang%20and%20Jingjie%20Zhang%20and%20Kuo%20Zhang%20and%20Liang%20Zhang%20and%20Liying%20Zhang%20and%20Manlin%20Zhang%20and%20Ting%20Zhang%20and%20Weida%20Zhang%20and%20Xiaohe%20Zhang%20and%20Xinyan%20Zhang%20and%20Yan%20Zhang%20and%20Yuan%20Zhang%20and%20Zixiang%20Zhang%20and%20Fengxuan%20Zhao%20and%20Huating%20Zhao%20and%20Yang%20Zhao%20and%20Hao%20Zheng%20and%20Jianbin%20Zheng%20and%20Xiaozheng%20Zheng%20and%20Yangyang%20Zheng%20and%20Yijie%20Zheng%20and%20Jiexin%20Zhou%20and%20Jiahui%20Zhu%20and%20Kuan%20Zhu%20and%20Shenhan%20Zhu%20and%20Wenjia%20Zhu%20and%20Benhui%20Zou%20and%20Feilong%20Zuo%0AAbstract%3A%20Recent%20strides%20in%20video%20generation%20have%20paved%20the%20way%20for%20unified%20audio-visual%20generation.%20In%20this%20work%2C%20we%20present%20Seedance%201.5%20pro%2C%20a%20foundational%20model%20engineered%20specifically%20for%20native%2C%20joint%20audio-video%20generation.%20Leveraging%20a%20dual-branch%20Diffusion%20Transformer%20architecture%2C%20the%20model%20integrates%20a%20cross-modal%20joint%20module%20with%20a%20specialized%20multi-stage%20data%20pipeline%2C%20achieving%20exceptional%20audio-visual%20synchronization%20and%20superior%20generation%20quality.%20To%20ensure%20practical%20utility%2C%20we%20implement%20meticulous%20post-training%20optimizations%2C%20including%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20high-quality%20datasets%20and%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20with%20multi-dimensional%20reward%20models.%20Furthermore%2C%20we%20introduce%20an%20acceleration%20framework%20that%20boosts%20inference%20speed%20by%20over%2010X.%20Seedance%201.5%20pro%20distinguishes%20itself%20through%20precise%20multilingual%20and%20dialect%20lip-syncing%2C%20dynamic%20cinematic%20camera%20control%2C%20and%20enhanced%20narrative%20coherence%2C%20positioning%20it%20as%20a%20robust%20engine%20for%20professional-grade%20content%20creation.%20Seedance%201.5%20pro%20is%20now%20accessible%20on%20Volcano%20Engine%20at%20https%3A//console.volcengine.com/ark/region%3Aark%2Bcn-beijing/experience/vision%3Ftype%3DGenVideo.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13507v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeedance%25201.5%2520pro%253A%2520A%2520Native%2520Audio-Visual%2520Joint%2520Generation%2520Foundation%2520Model%26entry.906535625%3DHeyi%2520Chen%2520and%2520Siyan%2520Chen%2520and%2520Xin%2520Chen%2520and%2520Yanfei%2520Chen%2520and%2520Ying%2520Chen%2520and%2520Zhuo%2520Chen%2520and%2520Feng%2520Cheng%2520and%2520Tianheng%2520Cheng%2520and%2520Xinqi%2520Cheng%2520and%2520Xuyan%2520Chi%2520and%2520Jian%2520Cong%2520and%2520Jing%2520Cui%2520and%2520Qinpeng%2520Cui%2520and%2520Qide%2520Dong%2520and%2520Junliang%2520Fan%2520and%2520Jing%2520Fang%2520and%2520Zetao%2520Fang%2520and%2520Chengjian%2520Feng%2520and%2520Han%2520Feng%2520and%2520Mingyuan%2520Gao%2520and%2520Yu%2520Gao%2520and%2520Dong%2520Guo%2520and%2520Qiushan%2520Guo%2520and%2520Boyang%2520Hao%2520and%2520Qingkai%2520Hao%2520and%2520Bibo%2520He%2520and%2520Qian%2520He%2520and%2520Tuyen%2520Hoang%2520and%2520Ruoqing%2520Hu%2520and%2520Xi%2520Hu%2520and%2520Weilin%2520Huang%2520and%2520Zhaoyang%2520Huang%2520and%2520Zhongyi%2520Huang%2520and%2520Donglei%2520Ji%2520and%2520Siqi%2520Jiang%2520and%2520Wei%2520Jiang%2520and%2520Yunpu%2520Jiang%2520and%2520Zhuo%2520Jiang%2520and%2520Ashley%2520Kim%2520and%2520Jianan%2520Kong%2520and%2520Zhichao%2520Lai%2520and%2520Shanshan%2520Lao%2520and%2520Yichong%2520Leng%2520and%2520Ai%2520Li%2520and%2520Feiya%2520Li%2520and%2520Gen%2520Li%2520and%2520Huixia%2520Li%2520and%2520JiaShi%2520Li%2520and%2520Liang%2520Li%2520and%2520Ming%2520Li%2520and%2520Shanshan%2520Li%2520and%2520Tao%2520Li%2520and%2520Xian%2520Li%2520and%2520Xiaojie%2520Li%2520and%2520Xiaoyang%2520Li%2520and%2520Xingxing%2520Li%2520and%2520Yameng%2520Li%2520and%2520Yifu%2520Li%2520and%2520Yiying%2520Li%2520and%2520Chao%2520Liang%2520and%2520Han%2520Liang%2520and%2520Jianzhong%2520Liang%2520and%2520Ying%2520Liang%2520and%2520Zhiqiang%2520Liang%2520and%2520Wang%2520Liao%2520and%2520Yalin%2520Liao%2520and%2520Heng%2520Lin%2520and%2520Kengyu%2520Lin%2520and%2520Shanchuan%2520Lin%2520and%2520Xi%2520Lin%2520and%2520Zhijie%2520Lin%2520and%2520Feng%2520Ling%2520and%2520Fangfang%2520Liu%2520and%2520Gaohong%2520Liu%2520and%2520Jiawei%2520Liu%2520and%2520Jie%2520Liu%2520and%2520Jihao%2520Liu%2520and%2520Shouda%2520Liu%2520and%2520Shu%2520Liu%2520and%2520Sichao%2520Liu%2520and%2520Songwei%2520Liu%2520and%2520Xin%2520Liu%2520and%2520Xue%2520Liu%2520and%2520Yibo%2520Liu%2520and%2520Zikun%2520Liu%2520and%2520Zuxi%2520Liu%2520and%2520Junlin%2520Lyu%2520and%2520Lecheng%2520Lyu%2520and%2520Qian%2520Lyu%2520and%2520Han%2520Mu%2520and%2520Xiaonan%2520Nie%2520and%2520Jingzhe%2520Ning%2520and%2520Xitong%2520Pan%2520and%2520Yanghua%2520Peng%2520and%2520Lianke%2520Qin%2520and%2520Xueqiong%2520Qu%2520and%2520Yuxi%2520Ren%2520and%2520Kai%2520Shen%2520and%2520Guang%2520Shi%2520and%2520Lei%2520Shi%2520and%2520Yan%2520Song%2520and%2520Yinglong%2520Song%2520and%2520Fan%2520Sun%2520and%2520Li%2520Sun%2520and%2520Renfei%2520Sun%2520and%2520Yan%2520Sun%2520and%2520Zeyu%2520Sun%2520and%2520Wenjing%2520Tang%2520and%2520Yaxue%2520Tang%2520and%2520Zirui%2520Tao%2520and%2520Feng%2520Wang%2520and%2520Furui%2520Wang%2520and%2520Jinran%2520Wang%2520and%2520Junkai%2520Wang%2520and%2520Ke%2520Wang%2520and%2520Kexin%2520Wang%2520and%2520Qingyi%2520Wang%2520and%2520Rui%2520Wang%2520and%2520Sen%2520Wang%2520and%2520Shuai%2520Wang%2520and%2520Tingru%2520Wang%2520and%2520Weichen%2520Wang%2520and%2520Xin%2520Wang%2520and%2520Yanhui%2520Wang%2520and%2520Yue%2520Wang%2520and%2520Yuping%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Ziyu%2520Wang%2520and%2520Guoqiang%2520Wei%2520and%2520Wanru%2520Wei%2520and%2520Di%2520Wu%2520and%2520Guohong%2520Wu%2520and%2520Hanjie%2520Wu%2520and%2520Jian%2520Wu%2520and%2520Jie%2520Wu%2520and%2520Ruolan%2520Wu%2520and%2520Xinglong%2520Wu%2520and%2520Yonghui%2520Wu%2520and%2520Ruiqi%2520Xia%2520and%2520Liang%2520Xiang%2520and%2520Fei%2520Xiao%2520and%2520XueFeng%2520Xiao%2520and%2520Pan%2520Xie%2520and%2520Shuangyi%2520Xie%2520and%2520Shuang%2520Xu%2520and%2520Jinlan%2520Xue%2520and%2520Shen%2520Yan%2520and%2520Bangbang%2520Yang%2520and%2520Ceyuan%2520Yang%2520and%2520Jiaqi%2520Yang%2520and%2520Runkai%2520Yang%2520and%2520Tao%2520Yang%2520and%2520Yang%2520Yang%2520and%2520Yihang%2520Yang%2520and%2520ZhiXian%2520Yang%2520and%2520Ziyan%2520Yang%2520and%2520Songting%2520Yao%2520and%2520Yifan%2520Yao%2520and%2520Zilyu%2520Ye%2520and%2520Bowen%2520Yu%2520and%2520Jian%2520Yu%2520and%2520Chujie%2520Yuan%2520and%2520Linxiao%2520Yuan%2520and%2520Sichun%2520Zeng%2520and%2520Weihong%2520Zeng%2520and%2520Xuejiao%2520Zeng%2520and%2520Yan%2520Zeng%2520and%2520Chuntao%2520Zhang%2520and%2520Heng%2520Zhang%2520and%2520Jingjie%2520Zhang%2520and%2520Kuo%2520Zhang%2520and%2520Liang%2520Zhang%2520and%2520Liying%2520Zhang%2520and%2520Manlin%2520Zhang%2520and%2520Ting%2520Zhang%2520and%2520Weida%2520Zhang%2520and%2520Xiaohe%2520Zhang%2520and%2520Xinyan%2520Zhang%2520and%2520Yan%2520Zhang%2520and%2520Yuan%2520Zhang%2520and%2520Zixiang%2520Zhang%2520and%2520Fengxuan%2520Zhao%2520and%2520Huating%2520Zhao%2520and%2520Yang%2520Zhao%2520and%2520Hao%2520Zheng%2520and%2520Jianbin%2520Zheng%2520and%2520Xiaozheng%2520Zheng%2520and%2520Yangyang%2520Zheng%2520and%2520Yijie%2520Zheng%2520and%2520Jiexin%2520Zhou%2520and%2520Jiahui%2520Zhu%2520and%2520Kuan%2520Zhu%2520and%2520Shenhan%2520Zhu%2520and%2520Wenjia%2520Zhu%2520and%2520Benhui%2520Zou%2520and%2520Feilong%2520Zuo%26entry.1292438233%3DRecent%2520strides%2520in%2520video%2520generation%2520have%2520paved%2520the%2520way%2520for%2520unified%2520audio-visual%2520generation.%2520In%2520this%2520work%252C%2520we%2520present%2520Seedance%25201.5%2520pro%252C%2520a%2520foundational%2520model%2520engineered%2520specifically%2520for%2520native%252C%2520joint%2520audio-video%2520generation.%2520Leveraging%2520a%2520dual-branch%2520Diffusion%2520Transformer%2520architecture%252C%2520the%2520model%2520integrates%2520a%2520cross-modal%2520joint%2520module%2520with%2520a%2520specialized%2520multi-stage%2520data%2520pipeline%252C%2520achieving%2520exceptional%2520audio-visual%2520synchronization%2520and%2520superior%2520generation%2520quality.%2520To%2520ensure%2520practical%2520utility%252C%2520we%2520implement%2520meticulous%2520post-training%2520optimizations%252C%2520including%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520on%2520high-quality%2520datasets%2520and%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520with%2520multi-dimensional%2520reward%2520models.%2520Furthermore%252C%2520we%2520introduce%2520an%2520acceleration%2520framework%2520that%2520boosts%2520inference%2520speed%2520by%2520over%252010X.%2520Seedance%25201.5%2520pro%2520distinguishes%2520itself%2520through%2520precise%2520multilingual%2520and%2520dialect%2520lip-syncing%252C%2520dynamic%2520cinematic%2520camera%2520control%252C%2520and%2520enhanced%2520narrative%2520coherence%252C%2520positioning%2520it%2520as%2520a%2520robust%2520engine%2520for%2520professional-grade%2520content%2520creation.%2520Seedance%25201.5%2520pro%2520is%2520now%2520accessible%2520on%2520Volcano%2520Engine%2520at%2520https%253A//console.volcengine.com/ark/region%253Aark%252Bcn-beijing/experience/vision%253Ftype%253DGenVideo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13507v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seedance%201.5%20pro%3A%20A%20Native%20Audio-Visual%20Joint%20Generation%20Foundation%20Model&entry.906535625=Heyi%20Chen%20and%20Siyan%20Chen%20and%20Xin%20Chen%20and%20Yanfei%20Chen%20and%20Ying%20Chen%20and%20Zhuo%20Chen%20and%20Feng%20Cheng%20and%20Tianheng%20Cheng%20and%20Xinqi%20Cheng%20and%20Xuyan%20Chi%20and%20Jian%20Cong%20and%20Jing%20Cui%20and%20Qinpeng%20Cui%20and%20Qide%20Dong%20and%20Junliang%20Fan%20and%20Jing%20Fang%20and%20Zetao%20Fang%20and%20Chengjian%20Feng%20and%20Han%20Feng%20and%20Mingyuan%20Gao%20and%20Yu%20Gao%20and%20Dong%20Guo%20and%20Qiushan%20Guo%20and%20Boyang%20Hao%20and%20Qingkai%20Hao%20and%20Bibo%20He%20and%20Qian%20He%20and%20Tuyen%20Hoang%20and%20Ruoqing%20Hu%20and%20Xi%20Hu%20and%20Weilin%20Huang%20and%20Zhaoyang%20Huang%20and%20Zhongyi%20Huang%20and%20Donglei%20Ji%20and%20Siqi%20Jiang%20and%20Wei%20Jiang%20and%20Yunpu%20Jiang%20and%20Zhuo%20Jiang%20and%20Ashley%20Kim%20and%20Jianan%20Kong%20and%20Zhichao%20Lai%20and%20Shanshan%20Lao%20and%20Yichong%20Leng%20and%20Ai%20Li%20and%20Feiya%20Li%20and%20Gen%20Li%20and%20Huixia%20Li%20and%20JiaShi%20Li%20and%20Liang%20Li%20and%20Ming%20Li%20and%20Shanshan%20Li%20and%20Tao%20Li%20and%20Xian%20Li%20and%20Xiaojie%20Li%20and%20Xiaoyang%20Li%20and%20Xingxing%20Li%20and%20Yameng%20Li%20and%20Yifu%20Li%20and%20Yiying%20Li%20and%20Chao%20Liang%20and%20Han%20Liang%20and%20Jianzhong%20Liang%20and%20Ying%20Liang%20and%20Zhiqiang%20Liang%20and%20Wang%20Liao%20and%20Yalin%20Liao%20and%20Heng%20Lin%20and%20Kengyu%20Lin%20and%20Shanchuan%20Lin%20and%20Xi%20Lin%20and%20Zhijie%20Lin%20and%20Feng%20Ling%20and%20Fangfang%20Liu%20and%20Gaohong%20Liu%20and%20Jiawei%20Liu%20and%20Jie%20Liu%20and%20Jihao%20Liu%20and%20Shouda%20Liu%20and%20Shu%20Liu%20and%20Sichao%20Liu%20and%20Songwei%20Liu%20and%20Xin%20Liu%20and%20Xue%20Liu%20and%20Yibo%20Liu%20and%20Zikun%20Liu%20and%20Zuxi%20Liu%20and%20Junlin%20Lyu%20and%20Lecheng%20Lyu%20and%20Qian%20Lyu%20and%20Han%20Mu%20and%20Xiaonan%20Nie%20and%20Jingzhe%20Ning%20and%20Xitong%20Pan%20and%20Yanghua%20Peng%20and%20Lianke%20Qin%20and%20Xueqiong%20Qu%20and%20Yuxi%20Ren%20and%20Kai%20Shen%20and%20Guang%20Shi%20and%20Lei%20Shi%20and%20Yan%20Song%20and%20Yinglong%20Song%20and%20Fan%20Sun%20and%20Li%20Sun%20and%20Renfei%20Sun%20and%20Yan%20Sun%20and%20Zeyu%20Sun%20and%20Wenjing%20Tang%20and%20Yaxue%20Tang%20and%20Zirui%20Tao%20and%20Feng%20Wang%20and%20Furui%20Wang%20and%20Jinran%20Wang%20and%20Junkai%20Wang%20and%20Ke%20Wang%20and%20Kexin%20Wang%20and%20Qingyi%20Wang%20and%20Rui%20Wang%20and%20Sen%20Wang%20and%20Shuai%20Wang%20and%20Tingru%20Wang%20and%20Weichen%20Wang%20and%20Xin%20Wang%20and%20Yanhui%20Wang%20and%20Yue%20Wang%20and%20Yuping%20Wang%20and%20Yuxuan%20Wang%20and%20Ziyu%20Wang%20and%20Guoqiang%20Wei%20and%20Wanru%20Wei%20and%20Di%20Wu%20and%20Guohong%20Wu%20and%20Hanjie%20Wu%20and%20Jian%20Wu%20and%20Jie%20Wu%20and%20Ruolan%20Wu%20and%20Xinglong%20Wu%20and%20Yonghui%20Wu%20and%20Ruiqi%20Xia%20and%20Liang%20Xiang%20and%20Fei%20Xiao%20and%20XueFeng%20Xiao%20and%20Pan%20Xie%20and%20Shuangyi%20Xie%20and%20Shuang%20Xu%20and%20Jinlan%20Xue%20and%20Shen%20Yan%20and%20Bangbang%20Yang%20and%20Ceyuan%20Yang%20and%20Jiaqi%20Yang%20and%20Runkai%20Yang%20and%20Tao%20Yang%20and%20Yang%20Yang%20and%20Yihang%20Yang%20and%20ZhiXian%20Yang%20and%20Ziyan%20Yang%20and%20Songting%20Yao%20and%20Yifan%20Yao%20and%20Zilyu%20Ye%20and%20Bowen%20Yu%20and%20Jian%20Yu%20and%20Chujie%20Yuan%20and%20Linxiao%20Yuan%20and%20Sichun%20Zeng%20and%20Weihong%20Zeng%20and%20Xuejiao%20Zeng%20and%20Yan%20Zeng%20and%20Chuntao%20Zhang%20and%20Heng%20Zhang%20and%20Jingjie%20Zhang%20and%20Kuo%20Zhang%20and%20Liang%20Zhang%20and%20Liying%20Zhang%20and%20Manlin%20Zhang%20and%20Ting%20Zhang%20and%20Weida%20Zhang%20and%20Xiaohe%20Zhang%20and%20Xinyan%20Zhang%20and%20Yan%20Zhang%20and%20Yuan%20Zhang%20and%20Zixiang%20Zhang%20and%20Fengxuan%20Zhao%20and%20Huating%20Zhao%20and%20Yang%20Zhao%20and%20Hao%20Zheng%20and%20Jianbin%20Zheng%20and%20Xiaozheng%20Zheng%20and%20Yangyang%20Zheng%20and%20Yijie%20Zheng%20and%20Jiexin%20Zhou%20and%20Jiahui%20Zhu%20and%20Kuan%20Zhu%20and%20Shenhan%20Zhu%20and%20Wenjia%20Zhu%20and%20Benhui%20Zou%20and%20Feilong%20Zuo&entry.1292438233=Recent%20strides%20in%20video%20generation%20have%20paved%20the%20way%20for%20unified%20audio-visual%20generation.%20In%20this%20work%2C%20we%20present%20Seedance%201.5%20pro%2C%20a%20foundational%20model%20engineered%20specifically%20for%20native%2C%20joint%20audio-video%20generation.%20Leveraging%20a%20dual-branch%20Diffusion%20Transformer%20architecture%2C%20the%20model%20integrates%20a%20cross-modal%20joint%20module%20with%20a%20specialized%20multi-stage%20data%20pipeline%2C%20achieving%20exceptional%20audio-visual%20synchronization%20and%20superior%20generation%20quality.%20To%20ensure%20practical%20utility%2C%20we%20implement%20meticulous%20post-training%20optimizations%2C%20including%20Supervised%20Fine-Tuning%20%28SFT%29%20on%20high-quality%20datasets%20and%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20with%20multi-dimensional%20reward%20models.%20Furthermore%2C%20we%20introduce%20an%20acceleration%20framework%20that%20boosts%20inference%20speed%20by%20over%2010X.%20Seedance%201.5%20pro%20distinguishes%20itself%20through%20precise%20multilingual%20and%20dialect%20lip-syncing%2C%20dynamic%20cinematic%20camera%20control%2C%20and%20enhanced%20narrative%20coherence%2C%20positioning%20it%20as%20a%20robust%20engine%20for%20professional-grade%20content%20creation.%20Seedance%201.5%20pro%20is%20now%20accessible%20on%20Volcano%20Engine%20at%20https%3A//console.volcengine.com/ark/region%3Aark%2Bcn-beijing/experience/vision%3Ftype%3DGenVideo.&entry.1838667208=http%3A//arxiv.org/abs/2512.13507v2&entry.124074799=Read"},
{"title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking", "author": "Lihong Wang and Liangqi Li and Weiwei Feng and Jiamin Wu and Changtao Miao and Tieru Wu and Rui Ma and Bo Zhang and Zhe Li", "abstract": "CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.", "link": "http://arxiv.org/abs/2512.14654v1", "date": "2025-12-16", "relevancy": 2.9365, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViRC%3A%20Enhancing%20Visual%20Interleaved%20Mathematical%20CoT%20with%20Reason%20Chunking&body=Title%3A%20ViRC%3A%20Enhancing%20Visual%20Interleaved%20Mathematical%20CoT%20with%20Reason%20Chunking%0AAuthor%3A%20Lihong%20Wang%20and%20Liangqi%20Li%20and%20Weiwei%20Feng%20and%20Jiamin%20Wu%20and%20Changtao%20Miao%20and%20Tieru%20Wu%20and%20Rui%20Ma%20and%20Bo%20Zhang%20and%20Zhe%20Li%0AAbstract%3A%20CoT%20has%20significantly%20enhanced%20the%20reasoning%20ability%20of%20LLMs%20while%20it%20faces%20challenges%20when%20extended%20to%20multimodal%20domains%2C%20particularly%20in%20mathematical%20tasks.%20Existing%20MLLMs%20typically%20perform%20textual%20reasoning%20solely%20from%20a%20single%20static%20mathematical%20image%2C%20overlooking%20dynamic%20visual%20acquisition%20during%20reasoning.%20In%20contrast%2C%20humans%20repeatedly%20examine%20visual%20image%20and%20employ%20step-by-step%20reasoning%20to%20prove%20intermediate%20propositions.%20This%20strategy%20of%20decomposing%20the%20problem-solving%20process%20into%20key%20logical%20nodes%20adheres%20to%20Miller%27s%20Law%20in%20cognitive%20science.%20Inspired%20by%20this%20insight%2C%20we%20propose%20a%20ViRC%20framework%20for%20multimodal%20mathematical%20tasks%2C%20introducing%20a%20Reason%20Chunking%20mechanism%20that%20structures%20multimodal%20mathematical%20CoT%20into%20consecutive%20Critical%20Reasoning%20Units%20%28CRUs%29%20to%20simulate%20human%20expert%20problem-solving%20patterns.%20CRUs%20ensure%20intra-unit%20textual%20coherence%20for%20intermediate%20proposition%20verification%20while%20integrating%20visual%20information%20across%20units%20to%20generate%20subsequent%20propositions%20and%20support%20structured%20reasoning.%20To%20this%20end%2C%20we%20present%20CRUX%20dataset%20by%20using%20three%20visual%20tools%20and%20four%20reasoning%20patterns%20to%20provide%20explicitly%20annotated%20CRUs%20across%20multiple%20reasoning%20paths%20for%20each%20mathematical%20problem.%20Leveraging%20the%20CRUX%20dataset%2C%20we%20propose%20a%20progressive%20training%20strategy%20inspired%20by%20human%20cognitive%20learning%2C%20which%20includes%20Instructional%20SFT%2C%20Practice%20SFT%2C%20and%20Strategic%20RL%2C%20aimed%20at%20further%20strengthening%20the%20Reason%20Chunking%20ability%20of%20the%20model.The%20resulting%20ViRC-7B%20model%20achieves%20a%2018.8%5C%25%20average%20improvement%20over%20baselines%20across%20multiple%20mathematical%20benchmarks.%20Code%20is%20available%20at%20https%3A//github.com/Leon-LihongWang/ViRC.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViRC%253A%2520Enhancing%2520Visual%2520Interleaved%2520Mathematical%2520CoT%2520with%2520Reason%2520Chunking%26entry.906535625%3DLihong%2520Wang%2520and%2520Liangqi%2520Li%2520and%2520Weiwei%2520Feng%2520and%2520Jiamin%2520Wu%2520and%2520Changtao%2520Miao%2520and%2520Tieru%2520Wu%2520and%2520Rui%2520Ma%2520and%2520Bo%2520Zhang%2520and%2520Zhe%2520Li%26entry.1292438233%3DCoT%2520has%2520significantly%2520enhanced%2520the%2520reasoning%2520ability%2520of%2520LLMs%2520while%2520it%2520faces%2520challenges%2520when%2520extended%2520to%2520multimodal%2520domains%252C%2520particularly%2520in%2520mathematical%2520tasks.%2520Existing%2520MLLMs%2520typically%2520perform%2520textual%2520reasoning%2520solely%2520from%2520a%2520single%2520static%2520mathematical%2520image%252C%2520overlooking%2520dynamic%2520visual%2520acquisition%2520during%2520reasoning.%2520In%2520contrast%252C%2520humans%2520repeatedly%2520examine%2520visual%2520image%2520and%2520employ%2520step-by-step%2520reasoning%2520to%2520prove%2520intermediate%2520propositions.%2520This%2520strategy%2520of%2520decomposing%2520the%2520problem-solving%2520process%2520into%2520key%2520logical%2520nodes%2520adheres%2520to%2520Miller%2527s%2520Law%2520in%2520cognitive%2520science.%2520Inspired%2520by%2520this%2520insight%252C%2520we%2520propose%2520a%2520ViRC%2520framework%2520for%2520multimodal%2520mathematical%2520tasks%252C%2520introducing%2520a%2520Reason%2520Chunking%2520mechanism%2520that%2520structures%2520multimodal%2520mathematical%2520CoT%2520into%2520consecutive%2520Critical%2520Reasoning%2520Units%2520%2528CRUs%2529%2520to%2520simulate%2520human%2520expert%2520problem-solving%2520patterns.%2520CRUs%2520ensure%2520intra-unit%2520textual%2520coherence%2520for%2520intermediate%2520proposition%2520verification%2520while%2520integrating%2520visual%2520information%2520across%2520units%2520to%2520generate%2520subsequent%2520propositions%2520and%2520support%2520structured%2520reasoning.%2520To%2520this%2520end%252C%2520we%2520present%2520CRUX%2520dataset%2520by%2520using%2520three%2520visual%2520tools%2520and%2520four%2520reasoning%2520patterns%2520to%2520provide%2520explicitly%2520annotated%2520CRUs%2520across%2520multiple%2520reasoning%2520paths%2520for%2520each%2520mathematical%2520problem.%2520Leveraging%2520the%2520CRUX%2520dataset%252C%2520we%2520propose%2520a%2520progressive%2520training%2520strategy%2520inspired%2520by%2520human%2520cognitive%2520learning%252C%2520which%2520includes%2520Instructional%2520SFT%252C%2520Practice%2520SFT%252C%2520and%2520Strategic%2520RL%252C%2520aimed%2520at%2520further%2520strengthening%2520the%2520Reason%2520Chunking%2520ability%2520of%2520the%2520model.The%2520resulting%2520ViRC-7B%2520model%2520achieves%2520a%252018.8%255C%2525%2520average%2520improvement%2520over%2520baselines%2520across%2520multiple%2520mathematical%2520benchmarks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Leon-LihongWang/ViRC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViRC%3A%20Enhancing%20Visual%20Interleaved%20Mathematical%20CoT%20with%20Reason%20Chunking&entry.906535625=Lihong%20Wang%20and%20Liangqi%20Li%20and%20Weiwei%20Feng%20and%20Jiamin%20Wu%20and%20Changtao%20Miao%20and%20Tieru%20Wu%20and%20Rui%20Ma%20and%20Bo%20Zhang%20and%20Zhe%20Li&entry.1292438233=CoT%20has%20significantly%20enhanced%20the%20reasoning%20ability%20of%20LLMs%20while%20it%20faces%20challenges%20when%20extended%20to%20multimodal%20domains%2C%20particularly%20in%20mathematical%20tasks.%20Existing%20MLLMs%20typically%20perform%20textual%20reasoning%20solely%20from%20a%20single%20static%20mathematical%20image%2C%20overlooking%20dynamic%20visual%20acquisition%20during%20reasoning.%20In%20contrast%2C%20humans%20repeatedly%20examine%20visual%20image%20and%20employ%20step-by-step%20reasoning%20to%20prove%20intermediate%20propositions.%20This%20strategy%20of%20decomposing%20the%20problem-solving%20process%20into%20key%20logical%20nodes%20adheres%20to%20Miller%27s%20Law%20in%20cognitive%20science.%20Inspired%20by%20this%20insight%2C%20we%20propose%20a%20ViRC%20framework%20for%20multimodal%20mathematical%20tasks%2C%20introducing%20a%20Reason%20Chunking%20mechanism%20that%20structures%20multimodal%20mathematical%20CoT%20into%20consecutive%20Critical%20Reasoning%20Units%20%28CRUs%29%20to%20simulate%20human%20expert%20problem-solving%20patterns.%20CRUs%20ensure%20intra-unit%20textual%20coherence%20for%20intermediate%20proposition%20verification%20while%20integrating%20visual%20information%20across%20units%20to%20generate%20subsequent%20propositions%20and%20support%20structured%20reasoning.%20To%20this%20end%2C%20we%20present%20CRUX%20dataset%20by%20using%20three%20visual%20tools%20and%20four%20reasoning%20patterns%20to%20provide%20explicitly%20annotated%20CRUs%20across%20multiple%20reasoning%20paths%20for%20each%20mathematical%20problem.%20Leveraging%20the%20CRUX%20dataset%2C%20we%20propose%20a%20progressive%20training%20strategy%20inspired%20by%20human%20cognitive%20learning%2C%20which%20includes%20Instructional%20SFT%2C%20Practice%20SFT%2C%20and%20Strategic%20RL%2C%20aimed%20at%20further%20strengthening%20the%20Reason%20Chunking%20ability%20of%20the%20model.The%20resulting%20ViRC-7B%20model%20achieves%20a%2018.8%5C%25%20average%20improvement%20over%20baselines%20across%20multiple%20mathematical%20benchmarks.%20Code%20is%20available%20at%20https%3A//github.com/Leon-LihongWang/ViRC.&entry.1838667208=http%3A//arxiv.org/abs/2512.14654v1&entry.124074799=Read"},
{"title": "TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks", "author": "Johannes Kiechle and Stefan M. Fischer and Daniel M. Lang and Cosmin I. Bercea and Matthew J. Nyflot and Lina Felsner and Julia A. Schnabel and Jan C. Peeken", "abstract": "The sharp rise in medical tomography examinations has created a demand for automated systems that can reliably extract informative features for downstream tasks such as tumor characterization. Although 3D volumes contain richer information than individual slices, effective 3D classification remains difficult: volumetric data encode complex spatial dependencies, and the scarcity of large-scale 3D datasets has constrained progress toward 3D foundation models. As a result, many recent approaches rely on 2D vision foundation models trained on natural images, repurposing them as feature extractors for medical scans with surprisingly strong performance. Despite their practical success, current methods that apply 2D foundation models to 3D scans via slice-based decomposition remain fundamentally limited. Standard slicing along axial, sagittal, and coronal planes often fails to capture the true spatial extent of a structure when its orientation does not align with these canonical views. More critically, most approaches aggregate slice features independently, ignoring the underlying 3D geometry and losing spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. Instead of restricting the model to axial, sagittal, or coronal planes, our method samples both canonical and non-canonical cross-sections generated from uniformly distributed points on a sphere enclosing the volume. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.", "link": "http://arxiv.org/abs/2511.09605v3", "date": "2025-12-16", "relevancy": 2.9264, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TomoGraphView%3A%203D%20Medical%20Image%20Classification%20with%20Omnidirectional%20Slice%20Representations%20and%20Graph%20Neural%20Networks&body=Title%3A%20TomoGraphView%3A%203D%20Medical%20Image%20Classification%20with%20Omnidirectional%20Slice%20Representations%20and%20Graph%20Neural%20Networks%0AAuthor%3A%20Johannes%20Kiechle%20and%20Stefan%20M.%20Fischer%20and%20Daniel%20M.%20Lang%20and%20Cosmin%20I.%20Bercea%20and%20Matthew%20J.%20Nyflot%20and%20Lina%20Felsner%20and%20Julia%20A.%20Schnabel%20and%20Jan%20C.%20Peeken%0AAbstract%3A%20The%20sharp%20rise%20in%20medical%20tomography%20examinations%20has%20created%20a%20demand%20for%20automated%20systems%20that%20can%20reliably%20extract%20informative%20features%20for%20downstream%20tasks%20such%20as%20tumor%20characterization.%20Although%203D%20volumes%20contain%20richer%20information%20than%20individual%20slices%2C%20effective%203D%20classification%20remains%20difficult%3A%20volumetric%20data%20encode%20complex%20spatial%20dependencies%2C%20and%20the%20scarcity%20of%20large-scale%203D%20datasets%20has%20constrained%20progress%20toward%203D%20foundation%20models.%20As%20a%20result%2C%20many%20recent%20approaches%20rely%20on%202D%20vision%20foundation%20models%20trained%20on%20natural%20images%2C%20repurposing%20them%20as%20feature%20extractors%20for%20medical%20scans%20with%20surprisingly%20strong%20performance.%20Despite%20their%20practical%20success%2C%20current%20methods%20that%20apply%202D%20foundation%20models%20to%203D%20scans%20via%20slice-based%20decomposition%20remain%20fundamentally%20limited.%20Standard%20slicing%20along%20axial%2C%20sagittal%2C%20and%20coronal%20planes%20often%20fails%20to%20capture%20the%20true%20spatial%20extent%20of%20a%20structure%20when%20its%20orientation%20does%20not%20align%20with%20these%20canonical%20views.%20More%20critically%2C%20most%20approaches%20aggregate%20slice%20features%20independently%2C%20ignoring%20the%20underlying%203D%20geometry%20and%20losing%20spatial%20coherence%20across%20slices.%20To%20overcome%20these%20limitations%2C%20we%20propose%20TomoGraphView%2C%20a%20novel%20framework%20that%20integrates%20omnidirectional%20volume%20slicing%20with%20spherical%20graph-based%20feature%20aggregation.%20Instead%20of%20restricting%20the%20model%20to%20axial%2C%20sagittal%2C%20or%20coronal%20planes%2C%20our%20method%20samples%20both%20canonical%20and%20non-canonical%20cross-sections%20generated%20from%20uniformly%20distributed%20points%20on%20a%20sphere%20enclosing%20the%20volume.%20We%20publicly%20share%20our%20accessible%20code%20base%20at%20http%3A//github.com/compai-lab/2025-MedIA-kiechle%20and%20provide%20a%20user-friendly%20library%20for%20omnidirectional%20volume%20slicing%20at%20https%3A//pypi.org/project/OmniSlicer.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09605v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTomoGraphView%253A%25203D%2520Medical%2520Image%2520Classification%2520with%2520Omnidirectional%2520Slice%2520Representations%2520and%2520Graph%2520Neural%2520Networks%26entry.906535625%3DJohannes%2520Kiechle%2520and%2520Stefan%2520M.%2520Fischer%2520and%2520Daniel%2520M.%2520Lang%2520and%2520Cosmin%2520I.%2520Bercea%2520and%2520Matthew%2520J.%2520Nyflot%2520and%2520Lina%2520Felsner%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Jan%2520C.%2520Peeken%26entry.1292438233%3DThe%2520sharp%2520rise%2520in%2520medical%2520tomography%2520examinations%2520has%2520created%2520a%2520demand%2520for%2520automated%2520systems%2520that%2520can%2520reliably%2520extract%2520informative%2520features%2520for%2520downstream%2520tasks%2520such%2520as%2520tumor%2520characterization.%2520Although%25203D%2520volumes%2520contain%2520richer%2520information%2520than%2520individual%2520slices%252C%2520effective%25203D%2520classification%2520remains%2520difficult%253A%2520volumetric%2520data%2520encode%2520complex%2520spatial%2520dependencies%252C%2520and%2520the%2520scarcity%2520of%2520large-scale%25203D%2520datasets%2520has%2520constrained%2520progress%2520toward%25203D%2520foundation%2520models.%2520As%2520a%2520result%252C%2520many%2520recent%2520approaches%2520rely%2520on%25202D%2520vision%2520foundation%2520models%2520trained%2520on%2520natural%2520images%252C%2520repurposing%2520them%2520as%2520feature%2520extractors%2520for%2520medical%2520scans%2520with%2520surprisingly%2520strong%2520performance.%2520Despite%2520their%2520practical%2520success%252C%2520current%2520methods%2520that%2520apply%25202D%2520foundation%2520models%2520to%25203D%2520scans%2520via%2520slice-based%2520decomposition%2520remain%2520fundamentally%2520limited.%2520Standard%2520slicing%2520along%2520axial%252C%2520sagittal%252C%2520and%2520coronal%2520planes%2520often%2520fails%2520to%2520capture%2520the%2520true%2520spatial%2520extent%2520of%2520a%2520structure%2520when%2520its%2520orientation%2520does%2520not%2520align%2520with%2520these%2520canonical%2520views.%2520More%2520critically%252C%2520most%2520approaches%2520aggregate%2520slice%2520features%2520independently%252C%2520ignoring%2520the%2520underlying%25203D%2520geometry%2520and%2520losing%2520spatial%2520coherence%2520across%2520slices.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520TomoGraphView%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520omnidirectional%2520volume%2520slicing%2520with%2520spherical%2520graph-based%2520feature%2520aggregation.%2520Instead%2520of%2520restricting%2520the%2520model%2520to%2520axial%252C%2520sagittal%252C%2520or%2520coronal%2520planes%252C%2520our%2520method%2520samples%2520both%2520canonical%2520and%2520non-canonical%2520cross-sections%2520generated%2520from%2520uniformly%2520distributed%2520points%2520on%2520a%2520sphere%2520enclosing%2520the%2520volume.%2520We%2520publicly%2520share%2520our%2520accessible%2520code%2520base%2520at%2520http%253A//github.com/compai-lab/2025-MedIA-kiechle%2520and%2520provide%2520a%2520user-friendly%2520library%2520for%2520omnidirectional%2520volume%2520slicing%2520at%2520https%253A//pypi.org/project/OmniSlicer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09605v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TomoGraphView%3A%203D%20Medical%20Image%20Classification%20with%20Omnidirectional%20Slice%20Representations%20and%20Graph%20Neural%20Networks&entry.906535625=Johannes%20Kiechle%20and%20Stefan%20M.%20Fischer%20and%20Daniel%20M.%20Lang%20and%20Cosmin%20I.%20Bercea%20and%20Matthew%20J.%20Nyflot%20and%20Lina%20Felsner%20and%20Julia%20A.%20Schnabel%20and%20Jan%20C.%20Peeken&entry.1292438233=The%20sharp%20rise%20in%20medical%20tomography%20examinations%20has%20created%20a%20demand%20for%20automated%20systems%20that%20can%20reliably%20extract%20informative%20features%20for%20downstream%20tasks%20such%20as%20tumor%20characterization.%20Although%203D%20volumes%20contain%20richer%20information%20than%20individual%20slices%2C%20effective%203D%20classification%20remains%20difficult%3A%20volumetric%20data%20encode%20complex%20spatial%20dependencies%2C%20and%20the%20scarcity%20of%20large-scale%203D%20datasets%20has%20constrained%20progress%20toward%203D%20foundation%20models.%20As%20a%20result%2C%20many%20recent%20approaches%20rely%20on%202D%20vision%20foundation%20models%20trained%20on%20natural%20images%2C%20repurposing%20them%20as%20feature%20extractors%20for%20medical%20scans%20with%20surprisingly%20strong%20performance.%20Despite%20their%20practical%20success%2C%20current%20methods%20that%20apply%202D%20foundation%20models%20to%203D%20scans%20via%20slice-based%20decomposition%20remain%20fundamentally%20limited.%20Standard%20slicing%20along%20axial%2C%20sagittal%2C%20and%20coronal%20planes%20often%20fails%20to%20capture%20the%20true%20spatial%20extent%20of%20a%20structure%20when%20its%20orientation%20does%20not%20align%20with%20these%20canonical%20views.%20More%20critically%2C%20most%20approaches%20aggregate%20slice%20features%20independently%2C%20ignoring%20the%20underlying%203D%20geometry%20and%20losing%20spatial%20coherence%20across%20slices.%20To%20overcome%20these%20limitations%2C%20we%20propose%20TomoGraphView%2C%20a%20novel%20framework%20that%20integrates%20omnidirectional%20volume%20slicing%20with%20spherical%20graph-based%20feature%20aggregation.%20Instead%20of%20restricting%20the%20model%20to%20axial%2C%20sagittal%2C%20or%20coronal%20planes%2C%20our%20method%20samples%20both%20canonical%20and%20non-canonical%20cross-sections%20generated%20from%20uniformly%20distributed%20points%20on%20a%20sphere%20enclosing%20the%20volume.%20We%20publicly%20share%20our%20accessible%20code%20base%20at%20http%3A//github.com/compai-lab/2025-MedIA-kiechle%20and%20provide%20a%20user-friendly%20library%20for%20omnidirectional%20volume%20slicing%20at%20https%3A//pypi.org/project/OmniSlicer.&entry.1838667208=http%3A//arxiv.org/abs/2511.09605v3&entry.124074799=Read"},
{"title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation", "author": "Leon Sick and Lukas Hoyer and Dominik Engel and Pedro Hermosilla and Timo Ropinski", "abstract": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.", "link": "http://arxiv.org/abs/2512.14440v1", "date": "2025-12-16", "relevancy": 2.9017, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5818}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5815}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S2D%3A%20Sparse-To-Dense%20Keymask%20Distillation%20for%20Unsupervised%20Video%20Instance%20Segmentation&body=Title%3A%20S2D%3A%20Sparse-To-Dense%20Keymask%20Distillation%20for%20Unsupervised%20Video%20Instance%20Segmentation%0AAuthor%3A%20Leon%20Sick%20and%20Lukas%20Hoyer%20and%20Dominik%20Engel%20and%20Pedro%20Hermosilla%20and%20Timo%20Ropinski%0AAbstract%3A%20In%20recent%20years%2C%20the%20state-of-the-art%20in%20unsupervised%20video%20instance%20segmentation%20has%20heavily%20relied%20on%20synthetic%20video%20data%2C%20generated%20from%20object-centric%20image%20datasets%20such%20as%20ImageNet.%20However%2C%20video%20synthesis%20by%20artificially%20shifting%20and%20scaling%20image%20instance%20masks%20fails%20to%20accurately%20model%20realistic%20motion%20in%20videos%2C%20such%20as%20perspective%20changes%2C%20movement%20by%20parts%20of%20one%20or%20multiple%20instances%2C%20or%20camera%20motion.%20To%20tackle%20this%20issue%2C%20we%20propose%20an%20unsupervised%20video%20instance%20segmentation%20model%20trained%20exclusively%20on%20real%20video%20data.%20We%20start%20from%20unsupervised%20instance%20segmentation%20masks%20on%20individual%20video%20frames.%20However%2C%20these%20single-frame%20segmentations%20exhibit%20temporal%20noise%20and%20their%20quality%20varies%20through%20the%20video.%20Therefore%2C%20we%20establish%20temporal%20coherence%20by%20identifying%20high-quality%20keymasks%20in%20the%20video%20by%20leveraging%20deep%20motion%20priors.%20The%20sparse%20keymask%20pseudo-annotations%20are%20then%20used%20to%20train%20a%20segmentation%20model%20for%20implicit%20mask%20propagation%2C%20for%20which%20we%20propose%20a%20Sparse-To-Dense%20Distillation%20approach%20aided%20by%20a%20Temporal%20DropLoss.%20After%20training%20the%20final%20model%20on%20the%20resulting%20dense%20labelset%2C%20our%20approach%20outperforms%20the%20current%20state-of-the-art%20across%20various%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS2D%253A%2520Sparse-To-Dense%2520Keymask%2520Distillation%2520for%2520Unsupervised%2520Video%2520Instance%2520Segmentation%26entry.906535625%3DLeon%2520Sick%2520and%2520Lukas%2520Hoyer%2520and%2520Dominik%2520Engel%2520and%2520Pedro%2520Hermosilla%2520and%2520Timo%2520Ropinski%26entry.1292438233%3DIn%2520recent%2520years%252C%2520the%2520state-of-the-art%2520in%2520unsupervised%2520video%2520instance%2520segmentation%2520has%2520heavily%2520relied%2520on%2520synthetic%2520video%2520data%252C%2520generated%2520from%2520object-centric%2520image%2520datasets%2520such%2520as%2520ImageNet.%2520However%252C%2520video%2520synthesis%2520by%2520artificially%2520shifting%2520and%2520scaling%2520image%2520instance%2520masks%2520fails%2520to%2520accurately%2520model%2520realistic%2520motion%2520in%2520videos%252C%2520such%2520as%2520perspective%2520changes%252C%2520movement%2520by%2520parts%2520of%2520one%2520or%2520multiple%2520instances%252C%2520or%2520camera%2520motion.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520an%2520unsupervised%2520video%2520instance%2520segmentation%2520model%2520trained%2520exclusively%2520on%2520real%2520video%2520data.%2520We%2520start%2520from%2520unsupervised%2520instance%2520segmentation%2520masks%2520on%2520individual%2520video%2520frames.%2520However%252C%2520these%2520single-frame%2520segmentations%2520exhibit%2520temporal%2520noise%2520and%2520their%2520quality%2520varies%2520through%2520the%2520video.%2520Therefore%252C%2520we%2520establish%2520temporal%2520coherence%2520by%2520identifying%2520high-quality%2520keymasks%2520in%2520the%2520video%2520by%2520leveraging%2520deep%2520motion%2520priors.%2520The%2520sparse%2520keymask%2520pseudo-annotations%2520are%2520then%2520used%2520to%2520train%2520a%2520segmentation%2520model%2520for%2520implicit%2520mask%2520propagation%252C%2520for%2520which%2520we%2520propose%2520a%2520Sparse-To-Dense%2520Distillation%2520approach%2520aided%2520by%2520a%2520Temporal%2520DropLoss.%2520After%2520training%2520the%2520final%2520model%2520on%2520the%2520resulting%2520dense%2520labelset%252C%2520our%2520approach%2520outperforms%2520the%2520current%2520state-of-the-art%2520across%2520various%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S2D%3A%20Sparse-To-Dense%20Keymask%20Distillation%20for%20Unsupervised%20Video%20Instance%20Segmentation&entry.906535625=Leon%20Sick%20and%20Lukas%20Hoyer%20and%20Dominik%20Engel%20and%20Pedro%20Hermosilla%20and%20Timo%20Ropinski&entry.1292438233=In%20recent%20years%2C%20the%20state-of-the-art%20in%20unsupervised%20video%20instance%20segmentation%20has%20heavily%20relied%20on%20synthetic%20video%20data%2C%20generated%20from%20object-centric%20image%20datasets%20such%20as%20ImageNet.%20However%2C%20video%20synthesis%20by%20artificially%20shifting%20and%20scaling%20image%20instance%20masks%20fails%20to%20accurately%20model%20realistic%20motion%20in%20videos%2C%20such%20as%20perspective%20changes%2C%20movement%20by%20parts%20of%20one%20or%20multiple%20instances%2C%20or%20camera%20motion.%20To%20tackle%20this%20issue%2C%20we%20propose%20an%20unsupervised%20video%20instance%20segmentation%20model%20trained%20exclusively%20on%20real%20video%20data.%20We%20start%20from%20unsupervised%20instance%20segmentation%20masks%20on%20individual%20video%20frames.%20However%2C%20these%20single-frame%20segmentations%20exhibit%20temporal%20noise%20and%20their%20quality%20varies%20through%20the%20video.%20Therefore%2C%20we%20establish%20temporal%20coherence%20by%20identifying%20high-quality%20keymasks%20in%20the%20video%20by%20leveraging%20deep%20motion%20priors.%20The%20sparse%20keymask%20pseudo-annotations%20are%20then%20used%20to%20train%20a%20segmentation%20model%20for%20implicit%20mask%20propagation%2C%20for%20which%20we%20propose%20a%20Sparse-To-Dense%20Distillation%20approach%20aided%20by%20a%20Temporal%20DropLoss.%20After%20training%20the%20final%20model%20on%20the%20resulting%20dense%20labelset%2C%20our%20approach%20outperforms%20the%20current%20state-of-the-art%20across%20various%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.14440v1&entry.124074799=Read"},
{"title": "OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control", "author": "Xilong Zhou and Jianchun Chen and Pramod Rao and Timo Teufel and Linjie Lyu and Tigran Minasian and Oleksandr Sotnychenko and Xiao-Xiao Long and Marc Habermann and Christian Theobalt", "abstract": "We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.", "link": "http://arxiv.org/abs/2511.02483v3", "date": "2025-12-16", "relevancy": 2.8815, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5816}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5816}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OLATverse%3A%20A%20Large-scale%20Real-world%20Object%20Dataset%20with%20Precise%20Lighting%20Control&body=Title%3A%20OLATverse%3A%20A%20Large-scale%20Real-world%20Object%20Dataset%20with%20Precise%20Lighting%20Control%0AAuthor%3A%20Xilong%20Zhou%20and%20Jianchun%20Chen%20and%20Pramod%20Rao%20and%20Timo%20Teufel%20and%20Linjie%20Lyu%20and%20Tigran%20Minasian%20and%20Oleksandr%20Sotnychenko%20and%20Xiao-Xiao%20Long%20and%20Marc%20Habermann%20and%20Christian%20Theobalt%0AAbstract%3A%20We%20introduce%20OLATverse%2C%20a%20large-scale%20dataset%20comprising%20around%209M%20images%20of%20765%20real-world%20objects%2C%20captured%20from%20multiple%20viewpoints%20under%20a%20diverse%20set%20of%20precisely%20controlled%20lighting%20conditions.%20While%20recent%20advances%20in%20object-centric%20inverse%20rendering%2C%20novel%20view%20synthesis%20and%20relighting%20have%20shown%20promising%20results%2C%20most%20techniques%20still%20heavily%20rely%20on%20the%20synthetic%20datasets%20for%20training%20and%20small-scale%20real-world%20datasets%20for%20benchmarking%2C%20which%20limits%20their%20realism%20and%20generalization.%20To%20address%20this%20gap%2C%20OLATverse%20offers%20two%20key%20advantages%20over%20existing%20datasets%3A%20large-scale%20coverage%20of%20real%20objects%20and%20high-fidelity%20appearance%20under%20precisely%20controlled%20illuminations.%20Specifically%2C%20OLATverse%20contains%20765%20common%20and%20uncommon%20real-world%20objects%2C%20spanning%20a%20wide%20range%20of%20material%20categories.%20Each%20object%20is%20captured%20using%2035%20DSLR%20cameras%20and%20331%20individually%20controlled%20light%20sources%2C%20enabling%20the%20simulation%20of%20diverse%20illumination%20conditions.%20In%20addition%2C%20for%20each%20object%2C%20we%20provide%20well-calibrated%20camera%20parameters%2C%20accurate%20object%20masks%2C%20photometric%20surface%20normals%2C%20and%20diffuse%20albedo%20as%20auxiliary%20resources.%20We%20also%20construct%20an%20extensive%20evaluation%20set%2C%20establishing%20the%20first%20comprehensive%20real-world%20object-centric%20benchmark%20for%20inverse%20rendering%20and%20normal%20estimation.%20We%20believe%20that%20OLATverse%20represents%20a%20pivotal%20step%20toward%20integrating%20the%20next%20generation%20of%20inverse%20rendering%20and%20relighting%20methods%20with%20real-world%20data.%20The%20full%20dataset%2C%20along%20with%20all%20post-processing%20workflows%2C%20will%20be%20publicly%20released%20at%20https%3A//vcai.mpi-inf.mpg.de/projects/OLATverse/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.02483v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOLATverse%253A%2520A%2520Large-scale%2520Real-world%2520Object%2520Dataset%2520with%2520Precise%2520Lighting%2520Control%26entry.906535625%3DXilong%2520Zhou%2520and%2520Jianchun%2520Chen%2520and%2520Pramod%2520Rao%2520and%2520Timo%2520Teufel%2520and%2520Linjie%2520Lyu%2520and%2520Tigran%2520Minasian%2520and%2520Oleksandr%2520Sotnychenko%2520and%2520Xiao-Xiao%2520Long%2520and%2520Marc%2520Habermann%2520and%2520Christian%2520Theobalt%26entry.1292438233%3DWe%2520introduce%2520OLATverse%252C%2520a%2520large-scale%2520dataset%2520comprising%2520around%25209M%2520images%2520of%2520765%2520real-world%2520objects%252C%2520captured%2520from%2520multiple%2520viewpoints%2520under%2520a%2520diverse%2520set%2520of%2520precisely%2520controlled%2520lighting%2520conditions.%2520While%2520recent%2520advances%2520in%2520object-centric%2520inverse%2520rendering%252C%2520novel%2520view%2520synthesis%2520and%2520relighting%2520have%2520shown%2520promising%2520results%252C%2520most%2520techniques%2520still%2520heavily%2520rely%2520on%2520the%2520synthetic%2520datasets%2520for%2520training%2520and%2520small-scale%2520real-world%2520datasets%2520for%2520benchmarking%252C%2520which%2520limits%2520their%2520realism%2520and%2520generalization.%2520To%2520address%2520this%2520gap%252C%2520OLATverse%2520offers%2520two%2520key%2520advantages%2520over%2520existing%2520datasets%253A%2520large-scale%2520coverage%2520of%2520real%2520objects%2520and%2520high-fidelity%2520appearance%2520under%2520precisely%2520controlled%2520illuminations.%2520Specifically%252C%2520OLATverse%2520contains%2520765%2520common%2520and%2520uncommon%2520real-world%2520objects%252C%2520spanning%2520a%2520wide%2520range%2520of%2520material%2520categories.%2520Each%2520object%2520is%2520captured%2520using%252035%2520DSLR%2520cameras%2520and%2520331%2520individually%2520controlled%2520light%2520sources%252C%2520enabling%2520the%2520simulation%2520of%2520diverse%2520illumination%2520conditions.%2520In%2520addition%252C%2520for%2520each%2520object%252C%2520we%2520provide%2520well-calibrated%2520camera%2520parameters%252C%2520accurate%2520object%2520masks%252C%2520photometric%2520surface%2520normals%252C%2520and%2520diffuse%2520albedo%2520as%2520auxiliary%2520resources.%2520We%2520also%2520construct%2520an%2520extensive%2520evaluation%2520set%252C%2520establishing%2520the%2520first%2520comprehensive%2520real-world%2520object-centric%2520benchmark%2520for%2520inverse%2520rendering%2520and%2520normal%2520estimation.%2520We%2520believe%2520that%2520OLATverse%2520represents%2520a%2520pivotal%2520step%2520toward%2520integrating%2520the%2520next%2520generation%2520of%2520inverse%2520rendering%2520and%2520relighting%2520methods%2520with%2520real-world%2520data.%2520The%2520full%2520dataset%252C%2520along%2520with%2520all%2520post-processing%2520workflows%252C%2520will%2520be%2520publicly%2520released%2520at%2520https%253A//vcai.mpi-inf.mpg.de/projects/OLATverse/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02483v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OLATverse%3A%20A%20Large-scale%20Real-world%20Object%20Dataset%20with%20Precise%20Lighting%20Control&entry.906535625=Xilong%20Zhou%20and%20Jianchun%20Chen%20and%20Pramod%20Rao%20and%20Timo%20Teufel%20and%20Linjie%20Lyu%20and%20Tigran%20Minasian%20and%20Oleksandr%20Sotnychenko%20and%20Xiao-Xiao%20Long%20and%20Marc%20Habermann%20and%20Christian%20Theobalt&entry.1292438233=We%20introduce%20OLATverse%2C%20a%20large-scale%20dataset%20comprising%20around%209M%20images%20of%20765%20real-world%20objects%2C%20captured%20from%20multiple%20viewpoints%20under%20a%20diverse%20set%20of%20precisely%20controlled%20lighting%20conditions.%20While%20recent%20advances%20in%20object-centric%20inverse%20rendering%2C%20novel%20view%20synthesis%20and%20relighting%20have%20shown%20promising%20results%2C%20most%20techniques%20still%20heavily%20rely%20on%20the%20synthetic%20datasets%20for%20training%20and%20small-scale%20real-world%20datasets%20for%20benchmarking%2C%20which%20limits%20their%20realism%20and%20generalization.%20To%20address%20this%20gap%2C%20OLATverse%20offers%20two%20key%20advantages%20over%20existing%20datasets%3A%20large-scale%20coverage%20of%20real%20objects%20and%20high-fidelity%20appearance%20under%20precisely%20controlled%20illuminations.%20Specifically%2C%20OLATverse%20contains%20765%20common%20and%20uncommon%20real-world%20objects%2C%20spanning%20a%20wide%20range%20of%20material%20categories.%20Each%20object%20is%20captured%20using%2035%20DSLR%20cameras%20and%20331%20individually%20controlled%20light%20sources%2C%20enabling%20the%20simulation%20of%20diverse%20illumination%20conditions.%20In%20addition%2C%20for%20each%20object%2C%20we%20provide%20well-calibrated%20camera%20parameters%2C%20accurate%20object%20masks%2C%20photometric%20surface%20normals%2C%20and%20diffuse%20albedo%20as%20auxiliary%20resources.%20We%20also%20construct%20an%20extensive%20evaluation%20set%2C%20establishing%20the%20first%20comprehensive%20real-world%20object-centric%20benchmark%20for%20inverse%20rendering%20and%20normal%20estimation.%20We%20believe%20that%20OLATverse%20represents%20a%20pivotal%20step%20toward%20integrating%20the%20next%20generation%20of%20inverse%20rendering%20and%20relighting%20methods%20with%20real-world%20data.%20The%20full%20dataset%2C%20along%20with%20all%20post-processing%20workflows%2C%20will%20be%20publicly%20released%20at%20https%3A//vcai.mpi-inf.mpg.de/projects/OLATverse/.&entry.1838667208=http%3A//arxiv.org/abs/2511.02483v3&entry.124074799=Read"},
{"title": "Order Matters: 3D Shape Generation from Sequential VR Sketches", "author": "Yizi Chen and Sidi Wu and Tianyi Xiao and Nina Wiedemann and Loic Landrieu", "abstract": "VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.", "link": "http://arxiv.org/abs/2512.04761v2", "date": "2025-12-16", "relevancy": 2.8752, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5768}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5768}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Order%20Matters%3A%203D%20Shape%20Generation%20from%20Sequential%20VR%20Sketches&body=Title%3A%20Order%20Matters%3A%203D%20Shape%20Generation%20from%20Sequential%20VR%20Sketches%0AAuthor%3A%20Yizi%20Chen%20and%20Sidi%20Wu%20and%20Tianyi%20Xiao%20and%20Nina%20Wiedemann%20and%20Loic%20Landrieu%0AAbstract%3A%20VR%20sketching%20lets%20users%20explore%20and%20iterate%20on%20ideas%20directly%20in%203D%2C%20offering%20a%20faster%20and%20more%20intuitive%20alternative%20to%20conventional%20CAD%20tools.%20However%2C%20existing%20sketch-to-shape%20models%20ignore%20the%20temporal%20ordering%20of%20strokes%2C%20discarding%20crucial%20cues%20about%20structure%20and%20design%20intent.%20We%20introduce%20VRSketch2Shape%2C%20the%20first%20framework%20and%20multi-category%20dataset%20for%20generating%203D%20shapes%20from%20sequential%20VR%20sketches.%20Our%20contributions%20are%20threefold%3A%20%28i%29%20an%20automated%20pipeline%20that%20generates%20sequential%20VR%20sketches%20from%20arbitrary%20shapes%2C%20%28ii%29%20a%20dataset%20of%20over%2020k%20synthetic%20and%20900%20hand-drawn%20sketch-shape%20pairs%20across%20four%20categories%2C%20and%20%28iii%29%20an%20order-aware%20sketch%20encoder%20coupled%20with%20a%20diffusion-based%203D%20generator.%20Our%20approach%20yields%20higher%20geometric%20fidelity%20than%20prior%20work%2C%20generalizes%20effectively%20from%20synthetic%20to%20real%20sketches%20with%20minimal%20supervision%2C%20and%20performs%20well%20even%20on%20partial%20sketches.%20All%20data%20and%20models%20will%20be%20released%20open-source%20at%20https%3A//chenyizi086.github.io/VRSketch2Shape_website.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrder%2520Matters%253A%25203D%2520Shape%2520Generation%2520from%2520Sequential%2520VR%2520Sketches%26entry.906535625%3DYizi%2520Chen%2520and%2520Sidi%2520Wu%2520and%2520Tianyi%2520Xiao%2520and%2520Nina%2520Wiedemann%2520and%2520Loic%2520Landrieu%26entry.1292438233%3DVR%2520sketching%2520lets%2520users%2520explore%2520and%2520iterate%2520on%2520ideas%2520directly%2520in%25203D%252C%2520offering%2520a%2520faster%2520and%2520more%2520intuitive%2520alternative%2520to%2520conventional%2520CAD%2520tools.%2520However%252C%2520existing%2520sketch-to-shape%2520models%2520ignore%2520the%2520temporal%2520ordering%2520of%2520strokes%252C%2520discarding%2520crucial%2520cues%2520about%2520structure%2520and%2520design%2520intent.%2520We%2520introduce%2520VRSketch2Shape%252C%2520the%2520first%2520framework%2520and%2520multi-category%2520dataset%2520for%2520generating%25203D%2520shapes%2520from%2520sequential%2520VR%2520sketches.%2520Our%2520contributions%2520are%2520threefold%253A%2520%2528i%2529%2520an%2520automated%2520pipeline%2520that%2520generates%2520sequential%2520VR%2520sketches%2520from%2520arbitrary%2520shapes%252C%2520%2528ii%2529%2520a%2520dataset%2520of%2520over%252020k%2520synthetic%2520and%2520900%2520hand-drawn%2520sketch-shape%2520pairs%2520across%2520four%2520categories%252C%2520and%2520%2528iii%2529%2520an%2520order-aware%2520sketch%2520encoder%2520coupled%2520with%2520a%2520diffusion-based%25203D%2520generator.%2520Our%2520approach%2520yields%2520higher%2520geometric%2520fidelity%2520than%2520prior%2520work%252C%2520generalizes%2520effectively%2520from%2520synthetic%2520to%2520real%2520sketches%2520with%2520minimal%2520supervision%252C%2520and%2520performs%2520well%2520even%2520on%2520partial%2520sketches.%2520All%2520data%2520and%2520models%2520will%2520be%2520released%2520open-source%2520at%2520https%253A//chenyizi086.github.io/VRSketch2Shape_website.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Order%20Matters%3A%203D%20Shape%20Generation%20from%20Sequential%20VR%20Sketches&entry.906535625=Yizi%20Chen%20and%20Sidi%20Wu%20and%20Tianyi%20Xiao%20and%20Nina%20Wiedemann%20and%20Loic%20Landrieu&entry.1292438233=VR%20sketching%20lets%20users%20explore%20and%20iterate%20on%20ideas%20directly%20in%203D%2C%20offering%20a%20faster%20and%20more%20intuitive%20alternative%20to%20conventional%20CAD%20tools.%20However%2C%20existing%20sketch-to-shape%20models%20ignore%20the%20temporal%20ordering%20of%20strokes%2C%20discarding%20crucial%20cues%20about%20structure%20and%20design%20intent.%20We%20introduce%20VRSketch2Shape%2C%20the%20first%20framework%20and%20multi-category%20dataset%20for%20generating%203D%20shapes%20from%20sequential%20VR%20sketches.%20Our%20contributions%20are%20threefold%3A%20%28i%29%20an%20automated%20pipeline%20that%20generates%20sequential%20VR%20sketches%20from%20arbitrary%20shapes%2C%20%28ii%29%20a%20dataset%20of%20over%2020k%20synthetic%20and%20900%20hand-drawn%20sketch-shape%20pairs%20across%20four%20categories%2C%20and%20%28iii%29%20an%20order-aware%20sketch%20encoder%20coupled%20with%20a%20diffusion-based%203D%20generator.%20Our%20approach%20yields%20higher%20geometric%20fidelity%20than%20prior%20work%2C%20generalizes%20effectively%20from%20synthetic%20to%20real%20sketches%20with%20minimal%20supervision%2C%20and%20performs%20well%20even%20on%20partial%20sketches.%20All%20data%20and%20models%20will%20be%20released%20open-source%20at%20https%3A//chenyizi086.github.io/VRSketch2Shape_website.&entry.1838667208=http%3A//arxiv.org/abs/2512.04761v2&entry.124074799=Read"},
{"title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs", "author": "Jun Zhang and Teng Wang and Yuying Ge and Yixiao Ge and Xinhao Li and Ying Shan and Limin Wang", "abstract": "This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.", "link": "http://arxiv.org/abs/2512.14698v1", "date": "2025-12-16", "relevancy": 2.8267, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeLens%3A%20Rethinking%20Video%20Temporal%20Grounding%20with%20Multimodal%20LLMs&body=Title%3A%20TimeLens%3A%20Rethinking%20Video%20Temporal%20Grounding%20with%20Multimodal%20LLMs%0AAuthor%3A%20Jun%20Zhang%20and%20Teng%20Wang%20and%20Yuying%20Ge%20and%20Yixiao%20Ge%20and%20Xinhao%20Li%20and%20Ying%20Shan%20and%20Limin%20Wang%0AAbstract%3A%20This%20paper%20does%20not%20introduce%20a%20novel%20method%20but%20instead%20establishes%20a%20straightforward%2C%20incremental%2C%20yet%20essential%20baseline%20for%20video%20temporal%20grounding%20%28VTG%29%2C%20a%20core%20capability%20in%20video%20understanding.%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20excel%20at%20various%20video%20understanding%20tasks%2C%20the%20recipes%20for%20optimizing%20them%20for%20VTG%20remain%20under-explored.%20In%20this%20paper%2C%20we%20present%20TimeLens%2C%20a%20systematic%20investigation%20into%20building%20MLLMs%20with%20strong%20VTG%20ability%2C%20along%20two%20primary%20dimensions%3A%20data%20quality%20and%20algorithmic%20design.%20We%20first%20expose%20critical%20quality%20issues%20in%20existing%20VTG%20benchmarks%20and%20introduce%20TimeLens-Bench%2C%20comprising%20meticulously%20re-annotated%20versions%20of%20three%20popular%20benchmarks%20with%20strict%20quality%20criteria.%20Our%20analysis%20reveals%20dramatic%20model%20re-rankings%20compared%20to%20legacy%20benchmarks%2C%20confirming%20the%20unreliability%20of%20prior%20evaluation%20standards.%20We%20also%20address%20noisy%20training%20data%20through%20an%20automated%20re-annotation%20pipeline%2C%20yielding%20TimeLens-100K%2C%20a%20large-scale%2C%20high-quality%20training%20dataset.%20Building%20on%20our%20data%20foundation%2C%20we%20conduct%20in-depth%20explorations%20of%20algorithmic%20design%20principles%2C%20yielding%20a%20series%20of%20meaningful%20insights%20and%20effective%20yet%20efficient%20practices.%20These%20include%20interleaved%20textual%20encoding%20for%20time%20representation%2C%20a%20thinking-free%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20approach%20as%20the%20training%20paradigm%2C%20and%20carefully%20designed%20recipes%20for%20RLVR%20training.%20These%20efforts%20culminate%20in%20TimeLens%20models%2C%20a%20family%20of%20MLLMs%20with%20state-of-the-art%20VTG%20performance%20among%20open-source%20models%20and%20even%20surpass%20proprietary%20models%20such%20as%20GPT-5%20and%20Gemini-2.5-Flash.%20All%20codes%2C%20data%2C%20and%20models%20will%20be%20released%20to%20facilitate%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeLens%253A%2520Rethinking%2520Video%2520Temporal%2520Grounding%2520with%2520Multimodal%2520LLMs%26entry.906535625%3DJun%2520Zhang%2520and%2520Teng%2520Wang%2520and%2520Yuying%2520Ge%2520and%2520Yixiao%2520Ge%2520and%2520Xinhao%2520Li%2520and%2520Ying%2520Shan%2520and%2520Limin%2520Wang%26entry.1292438233%3DThis%2520paper%2520does%2520not%2520introduce%2520a%2520novel%2520method%2520but%2520instead%2520establishes%2520a%2520straightforward%252C%2520incremental%252C%2520yet%2520essential%2520baseline%2520for%2520video%2520temporal%2520grounding%2520%2528VTG%2529%252C%2520a%2520core%2520capability%2520in%2520video%2520understanding.%2520While%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520excel%2520at%2520various%2520video%2520understanding%2520tasks%252C%2520the%2520recipes%2520for%2520optimizing%2520them%2520for%2520VTG%2520remain%2520under-explored.%2520In%2520this%2520paper%252C%2520we%2520present%2520TimeLens%252C%2520a%2520systematic%2520investigation%2520into%2520building%2520MLLMs%2520with%2520strong%2520VTG%2520ability%252C%2520along%2520two%2520primary%2520dimensions%253A%2520data%2520quality%2520and%2520algorithmic%2520design.%2520We%2520first%2520expose%2520critical%2520quality%2520issues%2520in%2520existing%2520VTG%2520benchmarks%2520and%2520introduce%2520TimeLens-Bench%252C%2520comprising%2520meticulously%2520re-annotated%2520versions%2520of%2520three%2520popular%2520benchmarks%2520with%2520strict%2520quality%2520criteria.%2520Our%2520analysis%2520reveals%2520dramatic%2520model%2520re-rankings%2520compared%2520to%2520legacy%2520benchmarks%252C%2520confirming%2520the%2520unreliability%2520of%2520prior%2520evaluation%2520standards.%2520We%2520also%2520address%2520noisy%2520training%2520data%2520through%2520an%2520automated%2520re-annotation%2520pipeline%252C%2520yielding%2520TimeLens-100K%252C%2520a%2520large-scale%252C%2520high-quality%2520training%2520dataset.%2520Building%2520on%2520our%2520data%2520foundation%252C%2520we%2520conduct%2520in-depth%2520explorations%2520of%2520algorithmic%2520design%2520principles%252C%2520yielding%2520a%2520series%2520of%2520meaningful%2520insights%2520and%2520effective%2520yet%2520efficient%2520practices.%2520These%2520include%2520interleaved%2520textual%2520encoding%2520for%2520time%2520representation%252C%2520a%2520thinking-free%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520approach%2520as%2520the%2520training%2520paradigm%252C%2520and%2520carefully%2520designed%2520recipes%2520for%2520RLVR%2520training.%2520These%2520efforts%2520culminate%2520in%2520TimeLens%2520models%252C%2520a%2520family%2520of%2520MLLMs%2520with%2520state-of-the-art%2520VTG%2520performance%2520among%2520open-source%2520models%2520and%2520even%2520surpass%2520proprietary%2520models%2520such%2520as%2520GPT-5%2520and%2520Gemini-2.5-Flash.%2520All%2520codes%252C%2520data%252C%2520and%2520models%2520will%2520be%2520released%2520to%2520facilitate%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeLens%3A%20Rethinking%20Video%20Temporal%20Grounding%20with%20Multimodal%20LLMs&entry.906535625=Jun%20Zhang%20and%20Teng%20Wang%20and%20Yuying%20Ge%20and%20Yixiao%20Ge%20and%20Xinhao%20Li%20and%20Ying%20Shan%20and%20Limin%20Wang&entry.1292438233=This%20paper%20does%20not%20introduce%20a%20novel%20method%20but%20instead%20establishes%20a%20straightforward%2C%20incremental%2C%20yet%20essential%20baseline%20for%20video%20temporal%20grounding%20%28VTG%29%2C%20a%20core%20capability%20in%20video%20understanding.%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20excel%20at%20various%20video%20understanding%20tasks%2C%20the%20recipes%20for%20optimizing%20them%20for%20VTG%20remain%20under-explored.%20In%20this%20paper%2C%20we%20present%20TimeLens%2C%20a%20systematic%20investigation%20into%20building%20MLLMs%20with%20strong%20VTG%20ability%2C%20along%20two%20primary%20dimensions%3A%20data%20quality%20and%20algorithmic%20design.%20We%20first%20expose%20critical%20quality%20issues%20in%20existing%20VTG%20benchmarks%20and%20introduce%20TimeLens-Bench%2C%20comprising%20meticulously%20re-annotated%20versions%20of%20three%20popular%20benchmarks%20with%20strict%20quality%20criteria.%20Our%20analysis%20reveals%20dramatic%20model%20re-rankings%20compared%20to%20legacy%20benchmarks%2C%20confirming%20the%20unreliability%20of%20prior%20evaluation%20standards.%20We%20also%20address%20noisy%20training%20data%20through%20an%20automated%20re-annotation%20pipeline%2C%20yielding%20TimeLens-100K%2C%20a%20large-scale%2C%20high-quality%20training%20dataset.%20Building%20on%20our%20data%20foundation%2C%20we%20conduct%20in-depth%20explorations%20of%20algorithmic%20design%20principles%2C%20yielding%20a%20series%20of%20meaningful%20insights%20and%20effective%20yet%20efficient%20practices.%20These%20include%20interleaved%20textual%20encoding%20for%20time%20representation%2C%20a%20thinking-free%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20approach%20as%20the%20training%20paradigm%2C%20and%20carefully%20designed%20recipes%20for%20RLVR%20training.%20These%20efforts%20culminate%20in%20TimeLens%20models%2C%20a%20family%20of%20MLLMs%20with%20state-of-the-art%20VTG%20performance%20among%20open-source%20models%20and%20even%20surpass%20proprietary%20models%20such%20as%20GPT-5%20and%20Gemini-2.5-Flash.%20All%20codes%2C%20data%2C%20and%20models%20will%20be%20released%20to%20facilitate%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.14698v1&entry.124074799=Read"},
{"title": "LWGANet: Addressing Spatial and Channel Redundancy in Remote Sensing Visual Tasks with Light-Weight Grouped Attention", "author": "Wei Lu and Xue Yang and Si-Bao Chen", "abstract": "Light-weight neural networks for remote sensing (RS) visual analysis must overcome two inherent redundancies: spatial redundancy from vast, homogeneous backgrounds, and channel redundancy, where extreme scale variations render a single feature space inefficient. Existing models, often designed for natural images, fail to address this dual challenge in RS scenarios. To bridge this gap, we propose LWGANet, a light-weight backbone engineered for RS-specific properties. LWGANet introduces two core innovations: a Top-K Global Feature Interaction (TGFI) module that mitigates spatial redundancy by focusing computation on salient regions, and a Light-Weight Grouped Attention (LWGA) module that resolves channel redundancy by partitioning channels into specialized, scale-specific pathways. By synergistically resolving these core inefficiencies, LWGANet achieves a superior trade-off between feature representation quality and computational cost. Extensive experiments on twelve diverse datasets across four major RS tasks--scene classification, oriented object detection, semantic segmentation, and change detection--demonstrate that LWGANet consistently outperforms state-of-the-art light-weight backbones in both accuracy and efficiency. Our work establishes a new, robust baseline for efficient visual analysis in RS images.", "link": "http://arxiv.org/abs/2501.10040v3", "date": "2025-12-16", "relevancy": 2.7935, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5853}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5571}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LWGANet%3A%20Addressing%20Spatial%20and%20Channel%20Redundancy%20in%20Remote%20Sensing%20Visual%20Tasks%20with%20Light-Weight%20Grouped%20Attention&body=Title%3A%20LWGANet%3A%20Addressing%20Spatial%20and%20Channel%20Redundancy%20in%20Remote%20Sensing%20Visual%20Tasks%20with%20Light-Weight%20Grouped%20Attention%0AAuthor%3A%20Wei%20Lu%20and%20Xue%20Yang%20and%20Si-Bao%20Chen%0AAbstract%3A%20Light-weight%20neural%20networks%20for%20remote%20sensing%20%28RS%29%20visual%20analysis%20must%20overcome%20two%20inherent%20redundancies%3A%20spatial%20redundancy%20from%20vast%2C%20homogeneous%20backgrounds%2C%20and%20channel%20redundancy%2C%20where%20extreme%20scale%20variations%20render%20a%20single%20feature%20space%20inefficient.%20Existing%20models%2C%20often%20designed%20for%20natural%20images%2C%20fail%20to%20address%20this%20dual%20challenge%20in%20RS%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20propose%20LWGANet%2C%20a%20light-weight%20backbone%20engineered%20for%20RS-specific%20properties.%20LWGANet%20introduces%20two%20core%20innovations%3A%20a%20Top-K%20Global%20Feature%20Interaction%20%28TGFI%29%20module%20that%20mitigates%20spatial%20redundancy%20by%20focusing%20computation%20on%20salient%20regions%2C%20and%20a%20Light-Weight%20Grouped%20Attention%20%28LWGA%29%20module%20that%20resolves%20channel%20redundancy%20by%20partitioning%20channels%20into%20specialized%2C%20scale-specific%20pathways.%20By%20synergistically%20resolving%20these%20core%20inefficiencies%2C%20LWGANet%20achieves%20a%20superior%20trade-off%20between%20feature%20representation%20quality%20and%20computational%20cost.%20Extensive%20experiments%20on%20twelve%20diverse%20datasets%20across%20four%20major%20RS%20tasks--scene%20classification%2C%20oriented%20object%20detection%2C%20semantic%20segmentation%2C%20and%20change%20detection--demonstrate%20that%20LWGANet%20consistently%20outperforms%20state-of-the-art%20light-weight%20backbones%20in%20both%20accuracy%20and%20efficiency.%20Our%20work%20establishes%20a%20new%2C%20robust%20baseline%20for%20efficient%20visual%20analysis%20in%20RS%20images.%0ALink%3A%20http%3A//arxiv.org/abs/2501.10040v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLWGANet%253A%2520Addressing%2520Spatial%2520and%2520Channel%2520Redundancy%2520in%2520Remote%2520Sensing%2520Visual%2520Tasks%2520with%2520Light-Weight%2520Grouped%2520Attention%26entry.906535625%3DWei%2520Lu%2520and%2520Xue%2520Yang%2520and%2520Si-Bao%2520Chen%26entry.1292438233%3DLight-weight%2520neural%2520networks%2520for%2520remote%2520sensing%2520%2528RS%2529%2520visual%2520analysis%2520must%2520overcome%2520two%2520inherent%2520redundancies%253A%2520spatial%2520redundancy%2520from%2520vast%252C%2520homogeneous%2520backgrounds%252C%2520and%2520channel%2520redundancy%252C%2520where%2520extreme%2520scale%2520variations%2520render%2520a%2520single%2520feature%2520space%2520inefficient.%2520Existing%2520models%252C%2520often%2520designed%2520for%2520natural%2520images%252C%2520fail%2520to%2520address%2520this%2520dual%2520challenge%2520in%2520RS%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520LWGANet%252C%2520a%2520light-weight%2520backbone%2520engineered%2520for%2520RS-specific%2520properties.%2520LWGANet%2520introduces%2520two%2520core%2520innovations%253A%2520a%2520Top-K%2520Global%2520Feature%2520Interaction%2520%2528TGFI%2529%2520module%2520that%2520mitigates%2520spatial%2520redundancy%2520by%2520focusing%2520computation%2520on%2520salient%2520regions%252C%2520and%2520a%2520Light-Weight%2520Grouped%2520Attention%2520%2528LWGA%2529%2520module%2520that%2520resolves%2520channel%2520redundancy%2520by%2520partitioning%2520channels%2520into%2520specialized%252C%2520scale-specific%2520pathways.%2520By%2520synergistically%2520resolving%2520these%2520core%2520inefficiencies%252C%2520LWGANet%2520achieves%2520a%2520superior%2520trade-off%2520between%2520feature%2520representation%2520quality%2520and%2520computational%2520cost.%2520Extensive%2520experiments%2520on%2520twelve%2520diverse%2520datasets%2520across%2520four%2520major%2520RS%2520tasks--scene%2520classification%252C%2520oriented%2520object%2520detection%252C%2520semantic%2520segmentation%252C%2520and%2520change%2520detection--demonstrate%2520that%2520LWGANet%2520consistently%2520outperforms%2520state-of-the-art%2520light-weight%2520backbones%2520in%2520both%2520accuracy%2520and%2520efficiency.%2520Our%2520work%2520establishes%2520a%2520new%252C%2520robust%2520baseline%2520for%2520efficient%2520visual%2520analysis%2520in%2520RS%2520images.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10040v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LWGANet%3A%20Addressing%20Spatial%20and%20Channel%20Redundancy%20in%20Remote%20Sensing%20Visual%20Tasks%20with%20Light-Weight%20Grouped%20Attention&entry.906535625=Wei%20Lu%20and%20Xue%20Yang%20and%20Si-Bao%20Chen&entry.1292438233=Light-weight%20neural%20networks%20for%20remote%20sensing%20%28RS%29%20visual%20analysis%20must%20overcome%20two%20inherent%20redundancies%3A%20spatial%20redundancy%20from%20vast%2C%20homogeneous%20backgrounds%2C%20and%20channel%20redundancy%2C%20where%20extreme%20scale%20variations%20render%20a%20single%20feature%20space%20inefficient.%20Existing%20models%2C%20often%20designed%20for%20natural%20images%2C%20fail%20to%20address%20this%20dual%20challenge%20in%20RS%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20propose%20LWGANet%2C%20a%20light-weight%20backbone%20engineered%20for%20RS-specific%20properties.%20LWGANet%20introduces%20two%20core%20innovations%3A%20a%20Top-K%20Global%20Feature%20Interaction%20%28TGFI%29%20module%20that%20mitigates%20spatial%20redundancy%20by%20focusing%20computation%20on%20salient%20regions%2C%20and%20a%20Light-Weight%20Grouped%20Attention%20%28LWGA%29%20module%20that%20resolves%20channel%20redundancy%20by%20partitioning%20channels%20into%20specialized%2C%20scale-specific%20pathways.%20By%20synergistically%20resolving%20these%20core%20inefficiencies%2C%20LWGANet%20achieves%20a%20superior%20trade-off%20between%20feature%20representation%20quality%20and%20computational%20cost.%20Extensive%20experiments%20on%20twelve%20diverse%20datasets%20across%20four%20major%20RS%20tasks--scene%20classification%2C%20oriented%20object%20detection%2C%20semantic%20segmentation%2C%20and%20change%20detection--demonstrate%20that%20LWGANet%20consistently%20outperforms%20state-of-the-art%20light-weight%20backbones%20in%20both%20accuracy%20and%20efficiency.%20Our%20work%20establishes%20a%20new%2C%20robust%20baseline%20for%20efficient%20visual%20analysis%20in%20RS%20images.&entry.1838667208=http%3A//arxiv.org/abs/2501.10040v3&entry.124074799=Read"},
{"title": "LCMem: A Universal Model for Robust Image Memorization Detection", "author": "Mischa Dombrowski and Felix N\u00fctzel and Bernhard Kainz", "abstract": "Recent advances in generative image modeling have achieved visual realism sufficient to deceive human experts, yet their potential for privacy preserving data sharing remains insufficiently understood. A central obstacle is the absence of reliable memorization detection mechanisms, limited quantitative evaluation, and poor generalization of existing privacy auditing methods across domains. To address this, we propose to view memorization detection as a unified problem at the intersection of re-identification and copy detection, whose complementary goals cover both identity consistency and augmentation-robust duplication, and introduce Latent Contrastive Memorization Network (LCMem), a cross-domain model evaluated jointly on both tasks. LCMem achieves this through a two-stage training strategy that first learns identity consistency before incorporating augmentation-robust copy detection. Across six benchmark datasets, LCMem achieves improvements of up to 16 percentage points on re-identification and 30 percentage points on copy detection, enabling substantially more reliable memorization detection at scale. Our results show that existing privacy filters provide limited performance and robustness, highlighting the need for stronger protection mechanisms. We show that LCMem sets a new standard for cross-domain privacy auditing, offering reliable and scalable memorization detection. Code and model is publicly available at https://github.com/MischaD/LCMem.", "link": "http://arxiv.org/abs/2512.14421v1", "date": "2025-12-16", "relevancy": 2.7726, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCMem%3A%20A%20Universal%20Model%20for%20Robust%20Image%20Memorization%20Detection&body=Title%3A%20LCMem%3A%20A%20Universal%20Model%20for%20Robust%20Image%20Memorization%20Detection%0AAuthor%3A%20Mischa%20Dombrowski%20and%20Felix%20N%C3%BCtzel%20and%20Bernhard%20Kainz%0AAbstract%3A%20Recent%20advances%20in%20generative%20image%20modeling%20have%20achieved%20visual%20realism%20sufficient%20to%20deceive%20human%20experts%2C%20yet%20their%20potential%20for%20privacy%20preserving%20data%20sharing%20remains%20insufficiently%20understood.%20A%20central%20obstacle%20is%20the%20absence%20of%20reliable%20memorization%20detection%20mechanisms%2C%20limited%20quantitative%20evaluation%2C%20and%20poor%20generalization%20of%20existing%20privacy%20auditing%20methods%20across%20domains.%20To%20address%20this%2C%20we%20propose%20to%20view%20memorization%20detection%20as%20a%20unified%20problem%20at%20the%20intersection%20of%20re-identification%20and%20copy%20detection%2C%20whose%20complementary%20goals%20cover%20both%20identity%20consistency%20and%20augmentation-robust%20duplication%2C%20and%20introduce%20Latent%20Contrastive%20Memorization%20Network%20%28LCMem%29%2C%20a%20cross-domain%20model%20evaluated%20jointly%20on%20both%20tasks.%20LCMem%20achieves%20this%20through%20a%20two-stage%20training%20strategy%20that%20first%20learns%20identity%20consistency%20before%20incorporating%20augmentation-robust%20copy%20detection.%20Across%20six%20benchmark%20datasets%2C%20LCMem%20achieves%20improvements%20of%20up%20to%2016%20percentage%20points%20on%20re-identification%20and%2030%20percentage%20points%20on%20copy%20detection%2C%20enabling%20substantially%20more%20reliable%20memorization%20detection%20at%20scale.%20Our%20results%20show%20that%20existing%20privacy%20filters%20provide%20limited%20performance%20and%20robustness%2C%20highlighting%20the%20need%20for%20stronger%20protection%20mechanisms.%20We%20show%20that%20LCMem%20sets%20a%20new%20standard%20for%20cross-domain%20privacy%20auditing%2C%20offering%20reliable%20and%20scalable%20memorization%20detection.%20Code%20and%20model%20is%20publicly%20available%20at%20https%3A//github.com/MischaD/LCMem.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCMem%253A%2520A%2520Universal%2520Model%2520for%2520Robust%2520Image%2520Memorization%2520Detection%26entry.906535625%3DMischa%2520Dombrowski%2520and%2520Felix%2520N%25C3%25BCtzel%2520and%2520Bernhard%2520Kainz%26entry.1292438233%3DRecent%2520advances%2520in%2520generative%2520image%2520modeling%2520have%2520achieved%2520visual%2520realism%2520sufficient%2520to%2520deceive%2520human%2520experts%252C%2520yet%2520their%2520potential%2520for%2520privacy%2520preserving%2520data%2520sharing%2520remains%2520insufficiently%2520understood.%2520A%2520central%2520obstacle%2520is%2520the%2520absence%2520of%2520reliable%2520memorization%2520detection%2520mechanisms%252C%2520limited%2520quantitative%2520evaluation%252C%2520and%2520poor%2520generalization%2520of%2520existing%2520privacy%2520auditing%2520methods%2520across%2520domains.%2520To%2520address%2520this%252C%2520we%2520propose%2520to%2520view%2520memorization%2520detection%2520as%2520a%2520unified%2520problem%2520at%2520the%2520intersection%2520of%2520re-identification%2520and%2520copy%2520detection%252C%2520whose%2520complementary%2520goals%2520cover%2520both%2520identity%2520consistency%2520and%2520augmentation-robust%2520duplication%252C%2520and%2520introduce%2520Latent%2520Contrastive%2520Memorization%2520Network%2520%2528LCMem%2529%252C%2520a%2520cross-domain%2520model%2520evaluated%2520jointly%2520on%2520both%2520tasks.%2520LCMem%2520achieves%2520this%2520through%2520a%2520two-stage%2520training%2520strategy%2520that%2520first%2520learns%2520identity%2520consistency%2520before%2520incorporating%2520augmentation-robust%2520copy%2520detection.%2520Across%2520six%2520benchmark%2520datasets%252C%2520LCMem%2520achieves%2520improvements%2520of%2520up%2520to%252016%2520percentage%2520points%2520on%2520re-identification%2520and%252030%2520percentage%2520points%2520on%2520copy%2520detection%252C%2520enabling%2520substantially%2520more%2520reliable%2520memorization%2520detection%2520at%2520scale.%2520Our%2520results%2520show%2520that%2520existing%2520privacy%2520filters%2520provide%2520limited%2520performance%2520and%2520robustness%252C%2520highlighting%2520the%2520need%2520for%2520stronger%2520protection%2520mechanisms.%2520We%2520show%2520that%2520LCMem%2520sets%2520a%2520new%2520standard%2520for%2520cross-domain%2520privacy%2520auditing%252C%2520offering%2520reliable%2520and%2520scalable%2520memorization%2520detection.%2520Code%2520and%2520model%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/MischaD/LCMem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCMem%3A%20A%20Universal%20Model%20for%20Robust%20Image%20Memorization%20Detection&entry.906535625=Mischa%20Dombrowski%20and%20Felix%20N%C3%BCtzel%20and%20Bernhard%20Kainz&entry.1292438233=Recent%20advances%20in%20generative%20image%20modeling%20have%20achieved%20visual%20realism%20sufficient%20to%20deceive%20human%20experts%2C%20yet%20their%20potential%20for%20privacy%20preserving%20data%20sharing%20remains%20insufficiently%20understood.%20A%20central%20obstacle%20is%20the%20absence%20of%20reliable%20memorization%20detection%20mechanisms%2C%20limited%20quantitative%20evaluation%2C%20and%20poor%20generalization%20of%20existing%20privacy%20auditing%20methods%20across%20domains.%20To%20address%20this%2C%20we%20propose%20to%20view%20memorization%20detection%20as%20a%20unified%20problem%20at%20the%20intersection%20of%20re-identification%20and%20copy%20detection%2C%20whose%20complementary%20goals%20cover%20both%20identity%20consistency%20and%20augmentation-robust%20duplication%2C%20and%20introduce%20Latent%20Contrastive%20Memorization%20Network%20%28LCMem%29%2C%20a%20cross-domain%20model%20evaluated%20jointly%20on%20both%20tasks.%20LCMem%20achieves%20this%20through%20a%20two-stage%20training%20strategy%20that%20first%20learns%20identity%20consistency%20before%20incorporating%20augmentation-robust%20copy%20detection.%20Across%20six%20benchmark%20datasets%2C%20LCMem%20achieves%20improvements%20of%20up%20to%2016%20percentage%20points%20on%20re-identification%20and%2030%20percentage%20points%20on%20copy%20detection%2C%20enabling%20substantially%20more%20reliable%20memorization%20detection%20at%20scale.%20Our%20results%20show%20that%20existing%20privacy%20filters%20provide%20limited%20performance%20and%20robustness%2C%20highlighting%20the%20need%20for%20stronger%20protection%20mechanisms.%20We%20show%20that%20LCMem%20sets%20a%20new%20standard%20for%20cross-domain%20privacy%20auditing%2C%20offering%20reliable%20and%20scalable%20memorization%20detection.%20Code%20and%20model%20is%20publicly%20available%20at%20https%3A//github.com/MischaD/LCMem.&entry.1838667208=http%3A//arxiv.org/abs/2512.14421v1&entry.124074799=Read"},
{"title": "DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning", "author": "Nakamasa Inoue and Kanoko Goto and Masanari Oi and Martyna Gruszka and Mahiro Ukai and Takumi Hirose and Yusuke Sekikawa", "abstract": "Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.", "link": "http://arxiv.org/abs/2512.14420v1", "date": "2025-12-16", "relevancy": 2.7688, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DISCODE%3A%20Distribution-Aware%20Score%20Decoder%20for%20Robust%20Automatic%20Evaluation%20of%20Image%20Captioning&body=Title%3A%20DISCODE%3A%20Distribution-Aware%20Score%20Decoder%20for%20Robust%20Automatic%20Evaluation%20of%20Image%20Captioning%0AAuthor%3A%20Nakamasa%20Inoue%20and%20Kanoko%20Goto%20and%20Masanari%20Oi%20and%20Martyna%20Gruszka%20and%20Mahiro%20Ukai%20and%20Takumi%20Hirose%20and%20Yusuke%20Sekikawa%0AAbstract%3A%20Large%20vision-language%20models%20%28LVLMs%29%20have%20shown%20impressive%20performance%20across%20a%20broad%20range%20of%20multimodal%20tasks.%20However%2C%20robust%20image%20caption%20evaluation%20using%20LVLMs%20remains%20challenging%2C%20particularly%20under%20domain-shift%20scenarios.%20To%20address%20this%20issue%2C%20we%20introduce%20the%20Distribution-Aware%20Score%20Decoder%20%28DISCODE%29%2C%20a%20novel%20finetuning-free%20method%20that%20generates%20robust%20evaluation%20scores%20better%20aligned%20with%20human%20judgments%20across%20diverse%20domains.%20The%20core%20idea%20behind%20DISCODE%20lies%20in%20its%20test-time%20adaptive%20evaluation%20approach%2C%20which%20introduces%20the%20Adaptive%20Test-Time%20%28ATT%29%20loss%2C%20leveraging%20a%20Gaussian%20prior%20distribution%20to%20improve%20robustness%20in%20evaluation%20score%20estimation.%20This%20loss%20is%20efficiently%20minimized%20at%20test%20time%20using%20an%20analytical%20solution%20that%20we%20derive.%20Furthermore%2C%20we%20introduce%20the%20Multi-domain%20Caption%20Evaluation%20%28MCEval%29%20benchmark%2C%20a%20new%20image%20captioning%20evaluation%20benchmark%20covering%20six%20distinct%20domains%2C%20designed%20to%20assess%20the%20robustness%20of%20evaluation%20metrics.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20DISCODE%20achieves%20state-of-the-art%20performance%20as%20a%20reference-free%20evaluation%20metric%20across%20MCEval%20and%20four%20representative%20existing%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDISCODE%253A%2520Distribution-Aware%2520Score%2520Decoder%2520for%2520Robust%2520Automatic%2520Evaluation%2520of%2520Image%2520Captioning%26entry.906535625%3DNakamasa%2520Inoue%2520and%2520Kanoko%2520Goto%2520and%2520Masanari%2520Oi%2520and%2520Martyna%2520Gruszka%2520and%2520Mahiro%2520Ukai%2520and%2520Takumi%2520Hirose%2520and%2520Yusuke%2520Sekikawa%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520shown%2520impressive%2520performance%2520across%2520a%2520broad%2520range%2520of%2520multimodal%2520tasks.%2520However%252C%2520robust%2520image%2520caption%2520evaluation%2520using%2520LVLMs%2520remains%2520challenging%252C%2520particularly%2520under%2520domain-shift%2520scenarios.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520the%2520Distribution-Aware%2520Score%2520Decoder%2520%2528DISCODE%2529%252C%2520a%2520novel%2520finetuning-free%2520method%2520that%2520generates%2520robust%2520evaluation%2520scores%2520better%2520aligned%2520with%2520human%2520judgments%2520across%2520diverse%2520domains.%2520The%2520core%2520idea%2520behind%2520DISCODE%2520lies%2520in%2520its%2520test-time%2520adaptive%2520evaluation%2520approach%252C%2520which%2520introduces%2520the%2520Adaptive%2520Test-Time%2520%2528ATT%2529%2520loss%252C%2520leveraging%2520a%2520Gaussian%2520prior%2520distribution%2520to%2520improve%2520robustness%2520in%2520evaluation%2520score%2520estimation.%2520This%2520loss%2520is%2520efficiently%2520minimized%2520at%2520test%2520time%2520using%2520an%2520analytical%2520solution%2520that%2520we%2520derive.%2520Furthermore%252C%2520we%2520introduce%2520the%2520Multi-domain%2520Caption%2520Evaluation%2520%2528MCEval%2529%2520benchmark%252C%2520a%2520new%2520image%2520captioning%2520evaluation%2520benchmark%2520covering%2520six%2520distinct%2520domains%252C%2520designed%2520to%2520assess%2520the%2520robustness%2520of%2520evaluation%2520metrics.%2520In%2520our%2520experiments%252C%2520we%2520demonstrate%2520that%2520DISCODE%2520achieves%2520state-of-the-art%2520performance%2520as%2520a%2520reference-free%2520evaluation%2520metric%2520across%2520MCEval%2520and%2520four%2520representative%2520existing%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DISCODE%3A%20Distribution-Aware%20Score%20Decoder%20for%20Robust%20Automatic%20Evaluation%20of%20Image%20Captioning&entry.906535625=Nakamasa%20Inoue%20and%20Kanoko%20Goto%20and%20Masanari%20Oi%20and%20Martyna%20Gruszka%20and%20Mahiro%20Ukai%20and%20Takumi%20Hirose%20and%20Yusuke%20Sekikawa&entry.1292438233=Large%20vision-language%20models%20%28LVLMs%29%20have%20shown%20impressive%20performance%20across%20a%20broad%20range%20of%20multimodal%20tasks.%20However%2C%20robust%20image%20caption%20evaluation%20using%20LVLMs%20remains%20challenging%2C%20particularly%20under%20domain-shift%20scenarios.%20To%20address%20this%20issue%2C%20we%20introduce%20the%20Distribution-Aware%20Score%20Decoder%20%28DISCODE%29%2C%20a%20novel%20finetuning-free%20method%20that%20generates%20robust%20evaluation%20scores%20better%20aligned%20with%20human%20judgments%20across%20diverse%20domains.%20The%20core%20idea%20behind%20DISCODE%20lies%20in%20its%20test-time%20adaptive%20evaluation%20approach%2C%20which%20introduces%20the%20Adaptive%20Test-Time%20%28ATT%29%20loss%2C%20leveraging%20a%20Gaussian%20prior%20distribution%20to%20improve%20robustness%20in%20evaluation%20score%20estimation.%20This%20loss%20is%20efficiently%20minimized%20at%20test%20time%20using%20an%20analytical%20solution%20that%20we%20derive.%20Furthermore%2C%20we%20introduce%20the%20Multi-domain%20Caption%20Evaluation%20%28MCEval%29%20benchmark%2C%20a%20new%20image%20captioning%20evaluation%20benchmark%20covering%20six%20distinct%20domains%2C%20designed%20to%20assess%20the%20robustness%20of%20evaluation%20metrics.%20In%20our%20experiments%2C%20we%20demonstrate%20that%20DISCODE%20achieves%20state-of-the-art%20performance%20as%20a%20reference-free%20evaluation%20metric%20across%20MCEval%20and%20four%20representative%20existing%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.14420v1&entry.124074799=Read"},
{"title": "VIBE: Can a VLM Read the Room?", "author": "Tania Chakraborty and Eylon Caplan and Dan Goldwasser", "abstract": "Understanding human social behavior such as recognizing emotions and the social dynamics causing them is an important and challenging problem. While LLMs have made remarkable advances, they are limited to the textual domain and cannot account for the major role that non-verbal cues play in understanding social situations. Vision Language Models (VLMs) can potentially account for this gap, however their ability to make correct inferences over such social cues has received little attention. In this paper, we explore the capabilities of VLMs at social reasoning. We identify a previously overlooked limitation in VLMs: the Visual Social-Pragmatic Inference gap. To target this gap, we propose a new task for VLMs: Visual Social-Pragmatic Inference. We construct a high quality dataset to test the abilities of a VLM for this task and benchmark the performance of several VLMs on it.", "link": "http://arxiv.org/abs/2506.11162v2", "date": "2025-12-16", "relevancy": 2.7531, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIBE%3A%20Can%20a%20VLM%20Read%20the%20Room%3F&body=Title%3A%20VIBE%3A%20Can%20a%20VLM%20Read%20the%20Room%3F%0AAuthor%3A%20Tania%20Chakraborty%20and%20Eylon%20Caplan%20and%20Dan%20Goldwasser%0AAbstract%3A%20Understanding%20human%20social%20behavior%20such%20as%20recognizing%20emotions%20and%20the%20social%20dynamics%20causing%20them%20is%20an%20important%20and%20challenging%20problem.%20While%20LLMs%20have%20made%20remarkable%20advances%2C%20they%20are%20limited%20to%20the%20textual%20domain%20and%20cannot%20account%20for%20the%20major%20role%20that%20non-verbal%20cues%20play%20in%20understanding%20social%20situations.%20Vision%20Language%20Models%20%28VLMs%29%20can%20potentially%20account%20for%20this%20gap%2C%20however%20their%20ability%20to%20make%20correct%20inferences%20over%20such%20social%20cues%20has%20received%20little%20attention.%20In%20this%20paper%2C%20we%20explore%20the%20capabilities%20of%20VLMs%20at%20social%20reasoning.%20We%20identify%20a%20previously%20overlooked%20limitation%20in%20VLMs%3A%20the%20Visual%20Social-Pragmatic%20Inference%20gap.%20To%20target%20this%20gap%2C%20we%20propose%20a%20new%20task%20for%20VLMs%3A%20Visual%20Social-Pragmatic%20Inference.%20We%20construct%20a%20high%20quality%20dataset%20to%20test%20the%20abilities%20of%20a%20VLM%20for%20this%20task%20and%20benchmark%20the%20performance%20of%20several%20VLMs%20on%20it.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIBE%253A%2520Can%2520a%2520VLM%2520Read%2520the%2520Room%253F%26entry.906535625%3DTania%2520Chakraborty%2520and%2520Eylon%2520Caplan%2520and%2520Dan%2520Goldwasser%26entry.1292438233%3DUnderstanding%2520human%2520social%2520behavior%2520such%2520as%2520recognizing%2520emotions%2520and%2520the%2520social%2520dynamics%2520causing%2520them%2520is%2520an%2520important%2520and%2520challenging%2520problem.%2520While%2520LLMs%2520have%2520made%2520remarkable%2520advances%252C%2520they%2520are%2520limited%2520to%2520the%2520textual%2520domain%2520and%2520cannot%2520account%2520for%2520the%2520major%2520role%2520that%2520non-verbal%2520cues%2520play%2520in%2520understanding%2520social%2520situations.%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520can%2520potentially%2520account%2520for%2520this%2520gap%252C%2520however%2520their%2520ability%2520to%2520make%2520correct%2520inferences%2520over%2520such%2520social%2520cues%2520has%2520received%2520little%2520attention.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520capabilities%2520of%2520VLMs%2520at%2520social%2520reasoning.%2520We%2520identify%2520a%2520previously%2520overlooked%2520limitation%2520in%2520VLMs%253A%2520the%2520Visual%2520Social-Pragmatic%2520Inference%2520gap.%2520To%2520target%2520this%2520gap%252C%2520we%2520propose%2520a%2520new%2520task%2520for%2520VLMs%253A%2520Visual%2520Social-Pragmatic%2520Inference.%2520We%2520construct%2520a%2520high%2520quality%2520dataset%2520to%2520test%2520the%2520abilities%2520of%2520a%2520VLM%2520for%2520this%2520task%2520and%2520benchmark%2520the%2520performance%2520of%2520several%2520VLMs%2520on%2520it.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIBE%3A%20Can%20a%20VLM%20Read%20the%20Room%3F&entry.906535625=Tania%20Chakraborty%20and%20Eylon%20Caplan%20and%20Dan%20Goldwasser&entry.1292438233=Understanding%20human%20social%20behavior%20such%20as%20recognizing%20emotions%20and%20the%20social%20dynamics%20causing%20them%20is%20an%20important%20and%20challenging%20problem.%20While%20LLMs%20have%20made%20remarkable%20advances%2C%20they%20are%20limited%20to%20the%20textual%20domain%20and%20cannot%20account%20for%20the%20major%20role%20that%20non-verbal%20cues%20play%20in%20understanding%20social%20situations.%20Vision%20Language%20Models%20%28VLMs%29%20can%20potentially%20account%20for%20this%20gap%2C%20however%20their%20ability%20to%20make%20correct%20inferences%20over%20such%20social%20cues%20has%20received%20little%20attention.%20In%20this%20paper%2C%20we%20explore%20the%20capabilities%20of%20VLMs%20at%20social%20reasoning.%20We%20identify%20a%20previously%20overlooked%20limitation%20in%20VLMs%3A%20the%20Visual%20Social-Pragmatic%20Inference%20gap.%20To%20target%20this%20gap%2C%20we%20propose%20a%20new%20task%20for%20VLMs%3A%20Visual%20Social-Pragmatic%20Inference.%20We%20construct%20a%20high%20quality%20dataset%20to%20test%20the%20abilities%20of%20a%20VLM%20for%20this%20task%20and%20benchmark%20the%20performance%20of%20several%20VLMs%20on%20it.&entry.1838667208=http%3A//arxiv.org/abs/2506.11162v2&entry.124074799=Read"},
{"title": "SAM3-I: Segment Anything with Instructions", "author": "Jingjing Li and Yue Feng and Yuchen Guo and Jincai Huang and Yongri Piao and Qi Bi and Miao Zhang and Xiaoqi Zhao and Qiang Chen and Shihao Zou and Wei Ji and Huchuan Lu and Li Cheng", "abstract": "Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.", "link": "http://arxiv.org/abs/2512.04585v2", "date": "2025-12-16", "relevancy": 2.7439, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5535}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5535}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM3-I%3A%20Segment%20Anything%20with%20Instructions&body=Title%3A%20SAM3-I%3A%20Segment%20Anything%20with%20Instructions%0AAuthor%3A%20Jingjing%20Li%20and%20Yue%20Feng%20and%20Yuchen%20Guo%20and%20Jincai%20Huang%20and%20Yongri%20Piao%20and%20Qi%20Bi%20and%20Miao%20Zhang%20and%20Xiaoqi%20Zhao%20and%20Qiang%20Chen%20and%20Shihao%20Zou%20and%20Wei%20Ji%20and%20Huchuan%20Lu%20and%20Li%20Cheng%0AAbstract%3A%20Segment%20Anything%20Model%203%20%28SAM3%29%20has%20advanced%20open-vocabulary%20segmentation%20through%20promptable%20concept%20segmentation%2C%20allowing%20users%20to%20segment%20all%20instances%20corresponding%20to%20a%20given%20concept%2C%20typically%20specified%20with%20short%20noun-phrase%20%28NP%29%20prompts.%20While%20this%20marks%20the%20first%20integration%20of%20language-level%20concepts%20within%20the%20SAM%20family%2C%20real-world%20usage%20typically%20requires%20far%20richer%20expressions%20that%20include%20attributes%2C%20spatial%20relations%2C%20functionalities%2C%20actions%2C%20states%2C%20and%20even%20implicit%20reasoning%20over%20instances.%20Currently%2C%20SAM3%20relies%20on%20external%20multi-modal%20agents%20to%20convert%20complex%20instructions%20into%20NPs%20and%20then%20conduct%20iterative%20mask%20filtering.%20However%2C%20these%20NP-level%20concepts%20remain%20overly%20coarse%2C%20often%20failing%20to%20precisely%20represent%20a%20specific%20instance.%20In%20this%20work%2C%20we%20present%20SAM3-I%2C%20an%20enhanced%20framework%20that%20unifies%20concept-level%20understanding%20and%20instruction-level%20reasoning%20within%20the%20SAM%20family.%20SAM3-I%20introduces%20an%20instruction-aware%20cascaded%20adaptation%20mechanism%20that%20progressively%20aligns%20expressive%20instruction%20semantics%20with%20SAM3%27s%20existing%20vision-language%20representations%2C%20enabling%20direct%20instruction-following%20segmentation%20without%20sacrificing%20its%20original%20concept-driven%20capabilities.%20Furthermore%2C%20we%20design%20a%20structured%20instruction%20taxonomy%20spanning%20concept%2C%20simple%2C%20and%20complex%20levels%2C%20and%20develop%20a%20scalable%20data%20engine%20to%20construct%20a%20dataset%20with%20diverse%20instruction-mask%20pairs.%20Experiments%20show%20that%20SAM3-I%20delivers%20appealing%20performance%2C%20demonstrating%20that%20SAM3%20can%20be%20effectively%20extended%20to%20follow%20natural-language%20instructions%20while%20preserving%20its%20strong%20concept%20grounding.%20We%20open-source%20SAM3-I%20and%20provide%20practical%20fine-tuning%20workflows%2C%20enabling%20researchers%20to%20adapt%20it%20to%20domain-specific%20applications.%20The%20source%20code%20is%20available%20here.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM3-I%253A%2520Segment%2520Anything%2520with%2520Instructions%26entry.906535625%3DJingjing%2520Li%2520and%2520Yue%2520Feng%2520and%2520Yuchen%2520Guo%2520and%2520Jincai%2520Huang%2520and%2520Yongri%2520Piao%2520and%2520Qi%2520Bi%2520and%2520Miao%2520Zhang%2520and%2520Xiaoqi%2520Zhao%2520and%2520Qiang%2520Chen%2520and%2520Shihao%2520Zou%2520and%2520Wei%2520Ji%2520and%2520Huchuan%2520Lu%2520and%2520Li%2520Cheng%26entry.1292438233%3DSegment%2520Anything%2520Model%25203%2520%2528SAM3%2529%2520has%2520advanced%2520open-vocabulary%2520segmentation%2520through%2520promptable%2520concept%2520segmentation%252C%2520allowing%2520users%2520to%2520segment%2520all%2520instances%2520corresponding%2520to%2520a%2520given%2520concept%252C%2520typically%2520specified%2520with%2520short%2520noun-phrase%2520%2528NP%2529%2520prompts.%2520While%2520this%2520marks%2520the%2520first%2520integration%2520of%2520language-level%2520concepts%2520within%2520the%2520SAM%2520family%252C%2520real-world%2520usage%2520typically%2520requires%2520far%2520richer%2520expressions%2520that%2520include%2520attributes%252C%2520spatial%2520relations%252C%2520functionalities%252C%2520actions%252C%2520states%252C%2520and%2520even%2520implicit%2520reasoning%2520over%2520instances.%2520Currently%252C%2520SAM3%2520relies%2520on%2520external%2520multi-modal%2520agents%2520to%2520convert%2520complex%2520instructions%2520into%2520NPs%2520and%2520then%2520conduct%2520iterative%2520mask%2520filtering.%2520However%252C%2520these%2520NP-level%2520concepts%2520remain%2520overly%2520coarse%252C%2520often%2520failing%2520to%2520precisely%2520represent%2520a%2520specific%2520instance.%2520In%2520this%2520work%252C%2520we%2520present%2520SAM3-I%252C%2520an%2520enhanced%2520framework%2520that%2520unifies%2520concept-level%2520understanding%2520and%2520instruction-level%2520reasoning%2520within%2520the%2520SAM%2520family.%2520SAM3-I%2520introduces%2520an%2520instruction-aware%2520cascaded%2520adaptation%2520mechanism%2520that%2520progressively%2520aligns%2520expressive%2520instruction%2520semantics%2520with%2520SAM3%2527s%2520existing%2520vision-language%2520representations%252C%2520enabling%2520direct%2520instruction-following%2520segmentation%2520without%2520sacrificing%2520its%2520original%2520concept-driven%2520capabilities.%2520Furthermore%252C%2520we%2520design%2520a%2520structured%2520instruction%2520taxonomy%2520spanning%2520concept%252C%2520simple%252C%2520and%2520complex%2520levels%252C%2520and%2520develop%2520a%2520scalable%2520data%2520engine%2520to%2520construct%2520a%2520dataset%2520with%2520diverse%2520instruction-mask%2520pairs.%2520Experiments%2520show%2520that%2520SAM3-I%2520delivers%2520appealing%2520performance%252C%2520demonstrating%2520that%2520SAM3%2520can%2520be%2520effectively%2520extended%2520to%2520follow%2520natural-language%2520instructions%2520while%2520preserving%2520its%2520strong%2520concept%2520grounding.%2520We%2520open-source%2520SAM3-I%2520and%2520provide%2520practical%2520fine-tuning%2520workflows%252C%2520enabling%2520researchers%2520to%2520adapt%2520it%2520to%2520domain-specific%2520applications.%2520The%2520source%2520code%2520is%2520available%2520here.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM3-I%3A%20Segment%20Anything%20with%20Instructions&entry.906535625=Jingjing%20Li%20and%20Yue%20Feng%20and%20Yuchen%20Guo%20and%20Jincai%20Huang%20and%20Yongri%20Piao%20and%20Qi%20Bi%20and%20Miao%20Zhang%20and%20Xiaoqi%20Zhao%20and%20Qiang%20Chen%20and%20Shihao%20Zou%20and%20Wei%20Ji%20and%20Huchuan%20Lu%20and%20Li%20Cheng&entry.1292438233=Segment%20Anything%20Model%203%20%28SAM3%29%20has%20advanced%20open-vocabulary%20segmentation%20through%20promptable%20concept%20segmentation%2C%20allowing%20users%20to%20segment%20all%20instances%20corresponding%20to%20a%20given%20concept%2C%20typically%20specified%20with%20short%20noun-phrase%20%28NP%29%20prompts.%20While%20this%20marks%20the%20first%20integration%20of%20language-level%20concepts%20within%20the%20SAM%20family%2C%20real-world%20usage%20typically%20requires%20far%20richer%20expressions%20that%20include%20attributes%2C%20spatial%20relations%2C%20functionalities%2C%20actions%2C%20states%2C%20and%20even%20implicit%20reasoning%20over%20instances.%20Currently%2C%20SAM3%20relies%20on%20external%20multi-modal%20agents%20to%20convert%20complex%20instructions%20into%20NPs%20and%20then%20conduct%20iterative%20mask%20filtering.%20However%2C%20these%20NP-level%20concepts%20remain%20overly%20coarse%2C%20often%20failing%20to%20precisely%20represent%20a%20specific%20instance.%20In%20this%20work%2C%20we%20present%20SAM3-I%2C%20an%20enhanced%20framework%20that%20unifies%20concept-level%20understanding%20and%20instruction-level%20reasoning%20within%20the%20SAM%20family.%20SAM3-I%20introduces%20an%20instruction-aware%20cascaded%20adaptation%20mechanism%20that%20progressively%20aligns%20expressive%20instruction%20semantics%20with%20SAM3%27s%20existing%20vision-language%20representations%2C%20enabling%20direct%20instruction-following%20segmentation%20without%20sacrificing%20its%20original%20concept-driven%20capabilities.%20Furthermore%2C%20we%20design%20a%20structured%20instruction%20taxonomy%20spanning%20concept%2C%20simple%2C%20and%20complex%20levels%2C%20and%20develop%20a%20scalable%20data%20engine%20to%20construct%20a%20dataset%20with%20diverse%20instruction-mask%20pairs.%20Experiments%20show%20that%20SAM3-I%20delivers%20appealing%20performance%2C%20demonstrating%20that%20SAM3%20can%20be%20effectively%20extended%20to%20follow%20natural-language%20instructions%20while%20preserving%20its%20strong%20concept%20grounding.%20We%20open-source%20SAM3-I%20and%20provide%20practical%20fine-tuning%20workflows%2C%20enabling%20researchers%20to%20adapt%20it%20to%20domain-specific%20applications.%20The%20source%20code%20is%20available%20here.&entry.1838667208=http%3A//arxiv.org/abs/2512.04585v2&entry.124074799=Read"},
{"title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in", "author": "Xiaoqian Shen and Min-Hung Chen and Yu-Chiang Frank Wang and Mohamed Elhoseiny and Ryo Hachiuma", "abstract": "Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\\% on NExT-GQA and 4.6\\% on ReXTime, while also enhancing average answer accuracy by 2.4\\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\\% on long-video benchmarks.", "link": "http://arxiv.org/abs/2512.14273v1", "date": "2025-12-16", "relevancy": 2.7303, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zoom-Zero%3A%20Reinforced%20Coarse-to-Fine%20Video%20Understanding%20via%20Temporal%20Zoom-in&body=Title%3A%20Zoom-Zero%3A%20Reinforced%20Coarse-to-Fine%20Video%20Understanding%20via%20Temporal%20Zoom-in%0AAuthor%3A%20Xiaoqian%20Shen%20and%20Min-Hung%20Chen%20and%20Yu-Chiang%20Frank%20Wang%20and%20Mohamed%20Elhoseiny%20and%20Ryo%20Hachiuma%0AAbstract%3A%20Grounded%20video%20question%20answering%20%28GVQA%29%20aims%20to%20localize%20relevant%20temporal%20segments%20in%20videos%20and%20generate%20accurate%20answers%20to%20a%20given%20question%3B%20however%2C%20large%20video-language%20models%20%28LVLMs%29%20exhibit%20limited%20temporal%20awareness.%20Although%20existing%20approaches%20based%20on%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20attempt%20to%20improve%20temporal%20grounding%2C%20they%20still%20struggle%20to%20faithfully%20ground%20their%20answers%20in%20the%20relevant%20video%20evidence%2C%20leading%20to%20temporal%20mislocalization%20and%20hallucinations.%20In%20this%20work%2C%20we%20present%20Zoom-Zero%2C%20a%20coarse-to-fine%20framework%20that%20first%20localizes%20query-relevant%20segments%20and%20then%20temporally%20zooms%20into%20the%20most%20salient%20frames%20for%20finer-grained%20visual%20verification.%20Our%20method%20addresses%20the%20limits%20of%20GRPO%20for%20the%20GVQA%20task%20with%20two%20key%20innovations%3A%20%28i%29%20a%20zoom-in%20accuracy%20reward%20that%20validates%20the%20fidelity%20of%20temporal%20grounding%20prediction%20and%20facilitates%20fine-grained%20visual%20verification%20on%20grounded%20frames%3B%20%28ii%29%20token-selective%20credit%20assignment%2C%20which%20attributes%20rewards%20to%20the%20tokens%20responsible%20for%20temporal%20localization%20or%20answer%20generation%2C%20mitigating%20GRPO%27s%20issue%20in%20handling%20multi-faceted%20reward%20signals.%20Our%20proposed%20method%20advances%20grounded%20video%20question%20answering%2C%20improving%20temporal%20grounding%20by%205.2%5C%25%20on%20NExT-GQA%20and%204.6%5C%25%20on%20ReXTime%2C%20while%20also%20enhancing%20average%20answer%20accuracy%20by%202.4%5C%25.%20Additionally%2C%20the%20coarse-to-fine%20zoom-in%20during%20inference%20further%20benefits%20long-form%20video%20understanding%20by%20preserving%20critical%20visual%20details%20without%20compromising%20global%20context%2C%20yielding%20an%20average%20improvement%20of%206.4%5C%25%20on%20long-video%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZoom-Zero%253A%2520Reinforced%2520Coarse-to-Fine%2520Video%2520Understanding%2520via%2520Temporal%2520Zoom-in%26entry.906535625%3DXiaoqian%2520Shen%2520and%2520Min-Hung%2520Chen%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Mohamed%2520Elhoseiny%2520and%2520Ryo%2520Hachiuma%26entry.1292438233%3DGrounded%2520video%2520question%2520answering%2520%2528GVQA%2529%2520aims%2520to%2520localize%2520relevant%2520temporal%2520segments%2520in%2520videos%2520and%2520generate%2520accurate%2520answers%2520to%2520a%2520given%2520question%253B%2520however%252C%2520large%2520video-language%2520models%2520%2528LVLMs%2529%2520exhibit%2520limited%2520temporal%2520awareness.%2520Although%2520existing%2520approaches%2520based%2520on%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520attempt%2520to%2520improve%2520temporal%2520grounding%252C%2520they%2520still%2520struggle%2520to%2520faithfully%2520ground%2520their%2520answers%2520in%2520the%2520relevant%2520video%2520evidence%252C%2520leading%2520to%2520temporal%2520mislocalization%2520and%2520hallucinations.%2520In%2520this%2520work%252C%2520we%2520present%2520Zoom-Zero%252C%2520a%2520coarse-to-fine%2520framework%2520that%2520first%2520localizes%2520query-relevant%2520segments%2520and%2520then%2520temporally%2520zooms%2520into%2520the%2520most%2520salient%2520frames%2520for%2520finer-grained%2520visual%2520verification.%2520Our%2520method%2520addresses%2520the%2520limits%2520of%2520GRPO%2520for%2520the%2520GVQA%2520task%2520with%2520two%2520key%2520innovations%253A%2520%2528i%2529%2520a%2520zoom-in%2520accuracy%2520reward%2520that%2520validates%2520the%2520fidelity%2520of%2520temporal%2520grounding%2520prediction%2520and%2520facilitates%2520fine-grained%2520visual%2520verification%2520on%2520grounded%2520frames%253B%2520%2528ii%2529%2520token-selective%2520credit%2520assignment%252C%2520which%2520attributes%2520rewards%2520to%2520the%2520tokens%2520responsible%2520for%2520temporal%2520localization%2520or%2520answer%2520generation%252C%2520mitigating%2520GRPO%2527s%2520issue%2520in%2520handling%2520multi-faceted%2520reward%2520signals.%2520Our%2520proposed%2520method%2520advances%2520grounded%2520video%2520question%2520answering%252C%2520improving%2520temporal%2520grounding%2520by%25205.2%255C%2525%2520on%2520NExT-GQA%2520and%25204.6%255C%2525%2520on%2520ReXTime%252C%2520while%2520also%2520enhancing%2520average%2520answer%2520accuracy%2520by%25202.4%255C%2525.%2520Additionally%252C%2520the%2520coarse-to-fine%2520zoom-in%2520during%2520inference%2520further%2520benefits%2520long-form%2520video%2520understanding%2520by%2520preserving%2520critical%2520visual%2520details%2520without%2520compromising%2520global%2520context%252C%2520yielding%2520an%2520average%2520improvement%2520of%25206.4%255C%2525%2520on%2520long-video%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zoom-Zero%3A%20Reinforced%20Coarse-to-Fine%20Video%20Understanding%20via%20Temporal%20Zoom-in&entry.906535625=Xiaoqian%20Shen%20and%20Min-Hung%20Chen%20and%20Yu-Chiang%20Frank%20Wang%20and%20Mohamed%20Elhoseiny%20and%20Ryo%20Hachiuma&entry.1292438233=Grounded%20video%20question%20answering%20%28GVQA%29%20aims%20to%20localize%20relevant%20temporal%20segments%20in%20videos%20and%20generate%20accurate%20answers%20to%20a%20given%20question%3B%20however%2C%20large%20video-language%20models%20%28LVLMs%29%20exhibit%20limited%20temporal%20awareness.%20Although%20existing%20approaches%20based%20on%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20attempt%20to%20improve%20temporal%20grounding%2C%20they%20still%20struggle%20to%20faithfully%20ground%20their%20answers%20in%20the%20relevant%20video%20evidence%2C%20leading%20to%20temporal%20mislocalization%20and%20hallucinations.%20In%20this%20work%2C%20we%20present%20Zoom-Zero%2C%20a%20coarse-to-fine%20framework%20that%20first%20localizes%20query-relevant%20segments%20and%20then%20temporally%20zooms%20into%20the%20most%20salient%20frames%20for%20finer-grained%20visual%20verification.%20Our%20method%20addresses%20the%20limits%20of%20GRPO%20for%20the%20GVQA%20task%20with%20two%20key%20innovations%3A%20%28i%29%20a%20zoom-in%20accuracy%20reward%20that%20validates%20the%20fidelity%20of%20temporal%20grounding%20prediction%20and%20facilitates%20fine-grained%20visual%20verification%20on%20grounded%20frames%3B%20%28ii%29%20token-selective%20credit%20assignment%2C%20which%20attributes%20rewards%20to%20the%20tokens%20responsible%20for%20temporal%20localization%20or%20answer%20generation%2C%20mitigating%20GRPO%27s%20issue%20in%20handling%20multi-faceted%20reward%20signals.%20Our%20proposed%20method%20advances%20grounded%20video%20question%20answering%2C%20improving%20temporal%20grounding%20by%205.2%5C%25%20on%20NExT-GQA%20and%204.6%5C%25%20on%20ReXTime%2C%20while%20also%20enhancing%20average%20answer%20accuracy%20by%202.4%5C%25.%20Additionally%2C%20the%20coarse-to-fine%20zoom-in%20during%20inference%20further%20benefits%20long-form%20video%20understanding%20by%20preserving%20critical%20visual%20details%20without%20compromising%20global%20context%2C%20yielding%20an%20average%20improvement%20of%206.4%5C%25%20on%20long-video%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.14273v1&entry.124074799=Read"},
{"title": "Training Multi-Image Vision Agents via End2End Reinforcement Learning", "author": "Chengqi Dong and Chuhuai Yue and Hang He and Rongge Mao and Fenghe Tang and S Kevin Zhou and Zekun Xu and Xiaohan Wang and Jiajun Chai and Wei Lin and Guojun Yin", "abstract": "Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images\" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.", "link": "http://arxiv.org/abs/2512.08980v2", "date": "2025-12-16", "relevancy": 2.7273, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5432}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Multi-Image%20Vision%20Agents%20via%20End2End%20Reinforcement%20Learning&body=Title%3A%20Training%20Multi-Image%20Vision%20Agents%20via%20End2End%20Reinforcement%20Learning%0AAuthor%3A%20Chengqi%20Dong%20and%20Chuhuai%20Yue%20and%20Hang%20He%20and%20Rongge%20Mao%20and%20Fenghe%20Tang%20and%20S%20Kevin%20Zhou%20and%20Zekun%20Xu%20and%20Xiaohan%20Wang%20and%20Jiajun%20Chai%20and%20Wei%20Lin%20and%20Guojun%20Yin%0AAbstract%3A%20Recent%20VLM-based%20agents%20aim%20to%20replicate%20OpenAI%20O3%27s%20%60%60thinking%20with%20images%22%20via%20tool%20use%2C%20but%20most%20open-source%20methods%20limit%20input%20to%20a%20single%20image%2C%20falling%20short%20on%20real-world%20multi-image%20QA%20tasks.%20To%20address%20this%2C%20we%20propose%20IMAgent%2C%20an%20open-source%20vision%20agent%20trained%20via%20end-to-end%20reinforcement%20learning%20dedicated%20for%20complex%20multi-image%20tasks.%20By%20leveraging%20a%20multi-agent%20system%2C%20we%20generate%20challenging%20and%20visually-rich%20multi-image%20QA%20pairs%20to%20fully%20activate%20the%20tool-use%20potential%20of%20the%20base%20VLM.%20Through%20manual%20verification%2C%20we%20obtain%20MIFG-QA%2C%20comprising%2010k%20samples%20for%20training%20and%20evaluation.%20With%20deeper%20reasoning%20steps%2C%20VLMs%20may%20increasingly%20ignore%20visual%20inputs.%20We%20therefore%20develop%20two%20specialized%20tools%20for%20visual%20reflection%20and%20confirmation%2C%20allowing%20the%20model%20to%20proactively%20reallocate%20its%20attention%20to%20image%20content%20during%20inference.%20Benefiting%20from%20our%20well-designed%20action-trajectory%20two-level%20mask%20strategy%2C%20IMAgent%20achieves%20stable%20tool%20use%20behavior%20via%20pure%20RL%20training%20without%20requiring%20costly%20supervised%20fine-tuning%20data.%20Extensive%20experiments%20demonstrate%20that%20IMAgent%20maintains%20strong%20performance%20on%20existing%20single-image%20benchmarks%20while%20achieving%20substantial%20improvements%20on%20our%20proposed%20multi-image%20dataset%2C%20with%20our%20analysis%20providing%20actionable%20insights%20for%20the%20research%20community.%20Codes%20and%20data%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Multi-Image%2520Vision%2520Agents%2520via%2520End2End%2520Reinforcement%2520Learning%26entry.906535625%3DChengqi%2520Dong%2520and%2520Chuhuai%2520Yue%2520and%2520Hang%2520He%2520and%2520Rongge%2520Mao%2520and%2520Fenghe%2520Tang%2520and%2520S%2520Kevin%2520Zhou%2520and%2520Zekun%2520Xu%2520and%2520Xiaohan%2520Wang%2520and%2520Jiajun%2520Chai%2520and%2520Wei%2520Lin%2520and%2520Guojun%2520Yin%26entry.1292438233%3DRecent%2520VLM-based%2520agents%2520aim%2520to%2520replicate%2520OpenAI%2520O3%2527s%2520%2560%2560thinking%2520with%2520images%2522%2520via%2520tool%2520use%252C%2520but%2520most%2520open-source%2520methods%2520limit%2520input%2520to%2520a%2520single%2520image%252C%2520falling%2520short%2520on%2520real-world%2520multi-image%2520QA%2520tasks.%2520To%2520address%2520this%252C%2520we%2520propose%2520IMAgent%252C%2520an%2520open-source%2520vision%2520agent%2520trained%2520via%2520end-to-end%2520reinforcement%2520learning%2520dedicated%2520for%2520complex%2520multi-image%2520tasks.%2520By%2520leveraging%2520a%2520multi-agent%2520system%252C%2520we%2520generate%2520challenging%2520and%2520visually-rich%2520multi-image%2520QA%2520pairs%2520to%2520fully%2520activate%2520the%2520tool-use%2520potential%2520of%2520the%2520base%2520VLM.%2520Through%2520manual%2520verification%252C%2520we%2520obtain%2520MIFG-QA%252C%2520comprising%252010k%2520samples%2520for%2520training%2520and%2520evaluation.%2520With%2520deeper%2520reasoning%2520steps%252C%2520VLMs%2520may%2520increasingly%2520ignore%2520visual%2520inputs.%2520We%2520therefore%2520develop%2520two%2520specialized%2520tools%2520for%2520visual%2520reflection%2520and%2520confirmation%252C%2520allowing%2520the%2520model%2520to%2520proactively%2520reallocate%2520its%2520attention%2520to%2520image%2520content%2520during%2520inference.%2520Benefiting%2520from%2520our%2520well-designed%2520action-trajectory%2520two-level%2520mask%2520strategy%252C%2520IMAgent%2520achieves%2520stable%2520tool%2520use%2520behavior%2520via%2520pure%2520RL%2520training%2520without%2520requiring%2520costly%2520supervised%2520fine-tuning%2520data.%2520Extensive%2520experiments%2520demonstrate%2520that%2520IMAgent%2520maintains%2520strong%2520performance%2520on%2520existing%2520single-image%2520benchmarks%2520while%2520achieving%2520substantial%2520improvements%2520on%2520our%2520proposed%2520multi-image%2520dataset%252C%2520with%2520our%2520analysis%2520providing%2520actionable%2520insights%2520for%2520the%2520research%2520community.%2520Codes%2520and%2520data%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Multi-Image%20Vision%20Agents%20via%20End2End%20Reinforcement%20Learning&entry.906535625=Chengqi%20Dong%20and%20Chuhuai%20Yue%20and%20Hang%20He%20and%20Rongge%20Mao%20and%20Fenghe%20Tang%20and%20S%20Kevin%20Zhou%20and%20Zekun%20Xu%20and%20Xiaohan%20Wang%20and%20Jiajun%20Chai%20and%20Wei%20Lin%20and%20Guojun%20Yin&entry.1292438233=Recent%20VLM-based%20agents%20aim%20to%20replicate%20OpenAI%20O3%27s%20%60%60thinking%20with%20images%22%20via%20tool%20use%2C%20but%20most%20open-source%20methods%20limit%20input%20to%20a%20single%20image%2C%20falling%20short%20on%20real-world%20multi-image%20QA%20tasks.%20To%20address%20this%2C%20we%20propose%20IMAgent%2C%20an%20open-source%20vision%20agent%20trained%20via%20end-to-end%20reinforcement%20learning%20dedicated%20for%20complex%20multi-image%20tasks.%20By%20leveraging%20a%20multi-agent%20system%2C%20we%20generate%20challenging%20and%20visually-rich%20multi-image%20QA%20pairs%20to%20fully%20activate%20the%20tool-use%20potential%20of%20the%20base%20VLM.%20Through%20manual%20verification%2C%20we%20obtain%20MIFG-QA%2C%20comprising%2010k%20samples%20for%20training%20and%20evaluation.%20With%20deeper%20reasoning%20steps%2C%20VLMs%20may%20increasingly%20ignore%20visual%20inputs.%20We%20therefore%20develop%20two%20specialized%20tools%20for%20visual%20reflection%20and%20confirmation%2C%20allowing%20the%20model%20to%20proactively%20reallocate%20its%20attention%20to%20image%20content%20during%20inference.%20Benefiting%20from%20our%20well-designed%20action-trajectory%20two-level%20mask%20strategy%2C%20IMAgent%20achieves%20stable%20tool%20use%20behavior%20via%20pure%20RL%20training%20without%20requiring%20costly%20supervised%20fine-tuning%20data.%20Extensive%20experiments%20demonstrate%20that%20IMAgent%20maintains%20strong%20performance%20on%20existing%20single-image%20benchmarks%20while%20achieving%20substantial%20improvements%20on%20our%20proposed%20multi-image%20dataset%2C%20with%20our%20analysis%20providing%20actionable%20insights%20for%20the%20research%20community.%20Codes%20and%20data%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2512.08980v2&entry.124074799=Read"},
{"title": "SuperCLIP: CLIP with Simple Classification Supervision", "author": "Weiheng Zhao and Zilong Huang and Jiashi Feng and Xinggang Wang", "abstract": "Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.", "link": "http://arxiv.org/abs/2512.14480v1", "date": "2025-12-16", "relevancy": 2.6921, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5793}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperCLIP%3A%20CLIP%20with%20Simple%20Classification%20Supervision&body=Title%3A%20SuperCLIP%3A%20CLIP%20with%20Simple%20Classification%20Supervision%0AAuthor%3A%20Weiheng%20Zhao%20and%20Zilong%20Huang%20and%20Jiashi%20Feng%20and%20Xinggang%20Wang%0AAbstract%3A%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20achieves%20strong%20generalization%20in%20vision-language%20tasks%20by%20aligning%20images%20and%20texts%20in%20a%20shared%20embedding%20space.%20However%2C%20recent%20findings%20show%20that%20CLIP-like%20models%20still%20underutilize%20fine-grained%20semantic%20signals%20in%20text%2C%20and%20this%20issue%20becomes%20even%20more%20pronounced%20when%20dealing%20with%20long%20and%20detailed%20captions.%20This%20stems%20from%20CLIP%27s%20training%20objective%2C%20which%20optimizes%20only%20global%20image-text%20similarity%20and%20overlooks%20token-level%20supervision%20-%20limiting%20its%20ability%20to%20achieve%20fine-grained%20visual-text%20alignment.%20To%20address%20this%2C%20we%20propose%20SuperCLIP%2C%20a%20simple%20yet%20effective%20framework%20that%20augments%20contrastive%20learning%20with%20classification-based%20supervision.%20By%20adding%20only%20a%20lightweight%20linear%20layer%20to%20the%20vision%20encoder%2C%20SuperCLIP%20leverages%20token-level%20cues%20to%20enhance%20visual-textual%20alignment%20-%20with%20just%20a%200.077%25%20increase%20in%20total%20FLOPs%2C%20and%20no%20need%20for%20additional%20annotated%20data.%20Experiments%20show%20that%20SuperCLIP%20consistently%20improves%20zero-shot%20classification%2C%20image-text%20retrieval%2C%20and%20purely%20visual%20tasks.%20These%20gains%20hold%20regardless%20of%20whether%20the%20model%20is%20trained%20on%20original%20web%20data%20or%20rich%20re-captioned%20data%2C%20demonstrating%20SuperCLIP%27s%20ability%20to%20recover%20textual%20supervision%20in%20both%20cases.%20Furthermore%2C%20SuperCLIP%20alleviates%20CLIP%27s%20small-batch%20performance%20drop%20through%20classification-based%20supervision%20that%20avoids%20reliance%20on%20large%20batch%20sizes.%20Code%20and%20models%20will%20be%20made%20open%20source.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperCLIP%253A%2520CLIP%2520with%2520Simple%2520Classification%2520Supervision%26entry.906535625%3DWeiheng%2520Zhao%2520and%2520Zilong%2520Huang%2520and%2520Jiashi%2520Feng%2520and%2520Xinggang%2520Wang%26entry.1292438233%3DContrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520achieves%2520strong%2520generalization%2520in%2520vision-language%2520tasks%2520by%2520aligning%2520images%2520and%2520texts%2520in%2520a%2520shared%2520embedding%2520space.%2520However%252C%2520recent%2520findings%2520show%2520that%2520CLIP-like%2520models%2520still%2520underutilize%2520fine-grained%2520semantic%2520signals%2520in%2520text%252C%2520and%2520this%2520issue%2520becomes%2520even%2520more%2520pronounced%2520when%2520dealing%2520with%2520long%2520and%2520detailed%2520captions.%2520This%2520stems%2520from%2520CLIP%2527s%2520training%2520objective%252C%2520which%2520optimizes%2520only%2520global%2520image-text%2520similarity%2520and%2520overlooks%2520token-level%2520supervision%2520-%2520limiting%2520its%2520ability%2520to%2520achieve%2520fine-grained%2520visual-text%2520alignment.%2520To%2520address%2520this%252C%2520we%2520propose%2520SuperCLIP%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520that%2520augments%2520contrastive%2520learning%2520with%2520classification-based%2520supervision.%2520By%2520adding%2520only%2520a%2520lightweight%2520linear%2520layer%2520to%2520the%2520vision%2520encoder%252C%2520SuperCLIP%2520leverages%2520token-level%2520cues%2520to%2520enhance%2520visual-textual%2520alignment%2520-%2520with%2520just%2520a%25200.077%2525%2520increase%2520in%2520total%2520FLOPs%252C%2520and%2520no%2520need%2520for%2520additional%2520annotated%2520data.%2520Experiments%2520show%2520that%2520SuperCLIP%2520consistently%2520improves%2520zero-shot%2520classification%252C%2520image-text%2520retrieval%252C%2520and%2520purely%2520visual%2520tasks.%2520These%2520gains%2520hold%2520regardless%2520of%2520whether%2520the%2520model%2520is%2520trained%2520on%2520original%2520web%2520data%2520or%2520rich%2520re-captioned%2520data%252C%2520demonstrating%2520SuperCLIP%2527s%2520ability%2520to%2520recover%2520textual%2520supervision%2520in%2520both%2520cases.%2520Furthermore%252C%2520SuperCLIP%2520alleviates%2520CLIP%2527s%2520small-batch%2520performance%2520drop%2520through%2520classification-based%2520supervision%2520that%2520avoids%2520reliance%2520on%2520large%2520batch%2520sizes.%2520Code%2520and%2520models%2520will%2520be%2520made%2520open%2520source.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperCLIP%3A%20CLIP%20with%20Simple%20Classification%20Supervision&entry.906535625=Weiheng%20Zhao%20and%20Zilong%20Huang%20and%20Jiashi%20Feng%20and%20Xinggang%20Wang&entry.1292438233=Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20achieves%20strong%20generalization%20in%20vision-language%20tasks%20by%20aligning%20images%20and%20texts%20in%20a%20shared%20embedding%20space.%20However%2C%20recent%20findings%20show%20that%20CLIP-like%20models%20still%20underutilize%20fine-grained%20semantic%20signals%20in%20text%2C%20and%20this%20issue%20becomes%20even%20more%20pronounced%20when%20dealing%20with%20long%20and%20detailed%20captions.%20This%20stems%20from%20CLIP%27s%20training%20objective%2C%20which%20optimizes%20only%20global%20image-text%20similarity%20and%20overlooks%20token-level%20supervision%20-%20limiting%20its%20ability%20to%20achieve%20fine-grained%20visual-text%20alignment.%20To%20address%20this%2C%20we%20propose%20SuperCLIP%2C%20a%20simple%20yet%20effective%20framework%20that%20augments%20contrastive%20learning%20with%20classification-based%20supervision.%20By%20adding%20only%20a%20lightweight%20linear%20layer%20to%20the%20vision%20encoder%2C%20SuperCLIP%20leverages%20token-level%20cues%20to%20enhance%20visual-textual%20alignment%20-%20with%20just%20a%200.077%25%20increase%20in%20total%20FLOPs%2C%20and%20no%20need%20for%20additional%20annotated%20data.%20Experiments%20show%20that%20SuperCLIP%20consistently%20improves%20zero-shot%20classification%2C%20image-text%20retrieval%2C%20and%20purely%20visual%20tasks.%20These%20gains%20hold%20regardless%20of%20whether%20the%20model%20is%20trained%20on%20original%20web%20data%20or%20rich%20re-captioned%20data%2C%20demonstrating%20SuperCLIP%27s%20ability%20to%20recover%20textual%20supervision%20in%20both%20cases.%20Furthermore%2C%20SuperCLIP%20alleviates%20CLIP%27s%20small-batch%20performance%20drop%20through%20classification-based%20supervision%20that%20avoids%20reliance%20on%20large%20batch%20sizes.%20Code%20and%20models%20will%20be%20made%20open%20source.&entry.1838667208=http%3A//arxiv.org/abs/2512.14480v1&entry.124074799=Read"},
{"title": "Enhancing Interpretability for Vision Models via Shapley Value Optimization", "author": "Kanglong Fan and Yunqiao Yang and Chen Ma", "abstract": "Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.", "link": "http://arxiv.org/abs/2512.14354v1", "date": "2025-12-16", "relevancy": 2.6867, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Interpretability%20for%20Vision%20Models%20via%20Shapley%20Value%20Optimization&body=Title%3A%20Enhancing%20Interpretability%20for%20Vision%20Models%20via%20Shapley%20Value%20Optimization%0AAuthor%3A%20Kanglong%20Fan%20and%20Yunqiao%20Yang%20and%20Chen%20Ma%0AAbstract%3A%20Deep%20neural%20networks%20have%20demonstrated%20remarkable%20performance%20across%20various%20domains%2C%20yet%20their%20decision-making%20processes%20remain%20opaque.%20Although%20many%20explanation%20methods%20are%20dedicated%20to%20bringing%20the%20obscurity%20of%20DNNs%20to%20light%2C%20they%20exhibit%20significant%20limitations%3A%20post-hoc%20explanation%20methods%20often%20struggle%20to%20faithfully%20reflect%20model%20behaviors%2C%20while%20self-explaining%20neural%20networks%20sacrifice%20performance%20and%20compatibility%20due%20to%20their%20specialized%20architectural%20designs.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20self-explaining%20framework%20that%20integrates%20Shapley%20value%20estimation%20as%20an%20auxiliary%20task%20during%20training%2C%20which%20achieves%20two%20key%20advancements%3A%201%29%20a%20fair%20allocation%20of%20the%20model%20prediction%20scores%20to%20image%20patches%2C%20ensuring%20explanations%20inherently%20align%20with%20the%20model%27s%20decision%20logic%2C%20and%202%29%20enhanced%20interpretability%20with%20minor%20structural%20modifications%2C%20preserving%20model%20performance%20and%20compatibility.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Interpretability%2520for%2520Vision%2520Models%2520via%2520Shapley%2520Value%2520Optimization%26entry.906535625%3DKanglong%2520Fan%2520and%2520Yunqiao%2520Yang%2520and%2520Chen%2520Ma%26entry.1292438233%3DDeep%2520neural%2520networks%2520have%2520demonstrated%2520remarkable%2520performance%2520across%2520various%2520domains%252C%2520yet%2520their%2520decision-making%2520processes%2520remain%2520opaque.%2520Although%2520many%2520explanation%2520methods%2520are%2520dedicated%2520to%2520bringing%2520the%2520obscurity%2520of%2520DNNs%2520to%2520light%252C%2520they%2520exhibit%2520significant%2520limitations%253A%2520post-hoc%2520explanation%2520methods%2520often%2520struggle%2520to%2520faithfully%2520reflect%2520model%2520behaviors%252C%2520while%2520self-explaining%2520neural%2520networks%2520sacrifice%2520performance%2520and%2520compatibility%2520due%2520to%2520their%2520specialized%2520architectural%2520designs.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520self-explaining%2520framework%2520that%2520integrates%2520Shapley%2520value%2520estimation%2520as%2520an%2520auxiliary%2520task%2520during%2520training%252C%2520which%2520achieves%2520two%2520key%2520advancements%253A%25201%2529%2520a%2520fair%2520allocation%2520of%2520the%2520model%2520prediction%2520scores%2520to%2520image%2520patches%252C%2520ensuring%2520explanations%2520inherently%2520align%2520with%2520the%2520model%2527s%2520decision%2520logic%252C%2520and%25202%2529%2520enhanced%2520interpretability%2520with%2520minor%2520structural%2520modifications%252C%2520preserving%2520model%2520performance%2520and%2520compatibility.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Interpretability%20for%20Vision%20Models%20via%20Shapley%20Value%20Optimization&entry.906535625=Kanglong%20Fan%20and%20Yunqiao%20Yang%20and%20Chen%20Ma&entry.1292438233=Deep%20neural%20networks%20have%20demonstrated%20remarkable%20performance%20across%20various%20domains%2C%20yet%20their%20decision-making%20processes%20remain%20opaque.%20Although%20many%20explanation%20methods%20are%20dedicated%20to%20bringing%20the%20obscurity%20of%20DNNs%20to%20light%2C%20they%20exhibit%20significant%20limitations%3A%20post-hoc%20explanation%20methods%20often%20struggle%20to%20faithfully%20reflect%20model%20behaviors%2C%20while%20self-explaining%20neural%20networks%20sacrifice%20performance%20and%20compatibility%20due%20to%20their%20specialized%20architectural%20designs.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20self-explaining%20framework%20that%20integrates%20Shapley%20value%20estimation%20as%20an%20auxiliary%20task%20during%20training%2C%20which%20achieves%20two%20key%20advancements%3A%201%29%20a%20fair%20allocation%20of%20the%20model%20prediction%20scores%20to%20image%20patches%2C%20ensuring%20explanations%20inherently%20align%20with%20the%20model%27s%20decision%20logic%2C%20and%202%29%20enhanced%20interpretability%20with%20minor%20structural%20modifications%2C%20preserving%20model%20performance%20and%20compatibility.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2512.14354v1&entry.124074799=Read"},
{"title": "SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome", "author": "Dabin Jeong and Amirhossein Vahidi and Ciro Ram\u00edrez-Su\u00e1stegui and Marie Moullet and Kevin Ly and Mohammad Vali Sanian and Sebastian Birk and Yinshui Chang and Adam Boxall and Daniyal Jafree and Lloyd Steele and Vijaya Baskar MS and Muzlifah Haniffa and Mohammad Lotfollahi", "abstract": "Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\\% in the gene-expression prediction task and avg. 26.93\\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.", "link": "http://arxiv.org/abs/2511.15464v3", "date": "2025-12-16", "relevancy": 2.6851, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5839}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5151}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIGMMA%3A%20Hierarchical%20Graph-Based%20Multi-Scale%20Multi-modal%20Contrastive%20Alignment%20of%20Histopathology%20Image%20and%20Spatial%20Transcriptome&body=Title%3A%20SIGMMA%3A%20Hierarchical%20Graph-Based%20Multi-Scale%20Multi-modal%20Contrastive%20Alignment%20of%20Histopathology%20Image%20and%20Spatial%20Transcriptome%0AAuthor%3A%20Dabin%20Jeong%20and%20Amirhossein%20Vahidi%20and%20Ciro%20Ram%C3%ADrez-Su%C3%A1stegui%20and%20Marie%20Moullet%20and%20Kevin%20Ly%20and%20Mohammad%20Vali%20Sanian%20and%20Sebastian%20Birk%20and%20Yinshui%20Chang%20and%20Adam%20Boxall%20and%20Daniyal%20Jafree%20and%20Lloyd%20Steele%20and%20Vijaya%20Baskar%20MS%20and%20Muzlifah%20Haniffa%20and%20Mohammad%20Lotfollahi%0AAbstract%3A%20Recent%20advances%20in%20computational%20pathology%20have%20leveraged%20vision-language%20models%20to%20learn%20joint%20representations%20of%20Hematoxylin%20and%20Eosin%20%28HE%29%20images%20with%20spatial%20transcriptomic%20%28ST%29%20profiles.%20However%2C%20existing%20approaches%20typically%20align%20HE%20tiles%20with%20their%20corresponding%20ST%20profiles%20at%20a%20single%20scale%2C%20overlooking%20fine-grained%20cellular%20structures%20and%20their%20spatial%20organization.%20To%20address%20this%2C%20we%20propose%20Sigmma%2C%20a%20multi-modal%20contrastive%20alignment%20framework%20for%20learning%20hierarchical%20representations%20of%20HE%20images%20and%20spatial%20transcriptome%20profiles%20across%20multiple%20scales.%20Sigmma%20introduces%20multi-scale%20contrastive%20alignment%2C%20ensuring%20that%20representations%20learned%20at%20different%20scales%20remain%20coherent%20across%20modalities.%20Furthermore%2C%20by%20representing%20cell%20interactions%20as%20a%20graph%20and%20integrating%20inter-%20and%20intra-subgraph%20relationships%2C%20our%20approach%20effectively%20captures%20cell-cell%20interactions%2C%20ranging%20from%20fine%20to%20coarse%2C%20within%20the%20tissue%20microenvironment.%20We%20demonstrate%20that%20Sigmm%20learns%20representations%20that%20better%20capture%20cross-modal%20correspondences%2C%20leading%20to%20an%20improvement%20of%20avg.%209.78%5C%25%20in%20the%20gene-expression%20prediction%20task%20and%20avg.%2026.93%5C%25%20in%20the%20cross-modal%20retrieval%20task%20across%20datasets.%20We%20further%20show%20that%20it%20learns%20meaningful%20multi-tissue%20organization%20in%20downstream%20analyses.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15464v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIGMMA%253A%2520Hierarchical%2520Graph-Based%2520Multi-Scale%2520Multi-modal%2520Contrastive%2520Alignment%2520of%2520Histopathology%2520Image%2520and%2520Spatial%2520Transcriptome%26entry.906535625%3DDabin%2520Jeong%2520and%2520Amirhossein%2520Vahidi%2520and%2520Ciro%2520Ram%25C3%25ADrez-Su%25C3%25A1stegui%2520and%2520Marie%2520Moullet%2520and%2520Kevin%2520Ly%2520and%2520Mohammad%2520Vali%2520Sanian%2520and%2520Sebastian%2520Birk%2520and%2520Yinshui%2520Chang%2520and%2520Adam%2520Boxall%2520and%2520Daniyal%2520Jafree%2520and%2520Lloyd%2520Steele%2520and%2520Vijaya%2520Baskar%2520MS%2520and%2520Muzlifah%2520Haniffa%2520and%2520Mohammad%2520Lotfollahi%26entry.1292438233%3DRecent%2520advances%2520in%2520computational%2520pathology%2520have%2520leveraged%2520vision-language%2520models%2520to%2520learn%2520joint%2520representations%2520of%2520Hematoxylin%2520and%2520Eosin%2520%2528HE%2529%2520images%2520with%2520spatial%2520transcriptomic%2520%2528ST%2529%2520profiles.%2520However%252C%2520existing%2520approaches%2520typically%2520align%2520HE%2520tiles%2520with%2520their%2520corresponding%2520ST%2520profiles%2520at%2520a%2520single%2520scale%252C%2520overlooking%2520fine-grained%2520cellular%2520structures%2520and%2520their%2520spatial%2520organization.%2520To%2520address%2520this%252C%2520we%2520propose%2520Sigmma%252C%2520a%2520multi-modal%2520contrastive%2520alignment%2520framework%2520for%2520learning%2520hierarchical%2520representations%2520of%2520HE%2520images%2520and%2520spatial%2520transcriptome%2520profiles%2520across%2520multiple%2520scales.%2520Sigmma%2520introduces%2520multi-scale%2520contrastive%2520alignment%252C%2520ensuring%2520that%2520representations%2520learned%2520at%2520different%2520scales%2520remain%2520coherent%2520across%2520modalities.%2520Furthermore%252C%2520by%2520representing%2520cell%2520interactions%2520as%2520a%2520graph%2520and%2520integrating%2520inter-%2520and%2520intra-subgraph%2520relationships%252C%2520our%2520approach%2520effectively%2520captures%2520cell-cell%2520interactions%252C%2520ranging%2520from%2520fine%2520to%2520coarse%252C%2520within%2520the%2520tissue%2520microenvironment.%2520We%2520demonstrate%2520that%2520Sigmm%2520learns%2520representations%2520that%2520better%2520capture%2520cross-modal%2520correspondences%252C%2520leading%2520to%2520an%2520improvement%2520of%2520avg.%25209.78%255C%2525%2520in%2520the%2520gene-expression%2520prediction%2520task%2520and%2520avg.%252026.93%255C%2525%2520in%2520the%2520cross-modal%2520retrieval%2520task%2520across%2520datasets.%2520We%2520further%2520show%2520that%2520it%2520learns%2520meaningful%2520multi-tissue%2520organization%2520in%2520downstream%2520analyses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15464v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIGMMA%3A%20Hierarchical%20Graph-Based%20Multi-Scale%20Multi-modal%20Contrastive%20Alignment%20of%20Histopathology%20Image%20and%20Spatial%20Transcriptome&entry.906535625=Dabin%20Jeong%20and%20Amirhossein%20Vahidi%20and%20Ciro%20Ram%C3%ADrez-Su%C3%A1stegui%20and%20Marie%20Moullet%20and%20Kevin%20Ly%20and%20Mohammad%20Vali%20Sanian%20and%20Sebastian%20Birk%20and%20Yinshui%20Chang%20and%20Adam%20Boxall%20and%20Daniyal%20Jafree%20and%20Lloyd%20Steele%20and%20Vijaya%20Baskar%20MS%20and%20Muzlifah%20Haniffa%20and%20Mohammad%20Lotfollahi&entry.1292438233=Recent%20advances%20in%20computational%20pathology%20have%20leveraged%20vision-language%20models%20to%20learn%20joint%20representations%20of%20Hematoxylin%20and%20Eosin%20%28HE%29%20images%20with%20spatial%20transcriptomic%20%28ST%29%20profiles.%20However%2C%20existing%20approaches%20typically%20align%20HE%20tiles%20with%20their%20corresponding%20ST%20profiles%20at%20a%20single%20scale%2C%20overlooking%20fine-grained%20cellular%20structures%20and%20their%20spatial%20organization.%20To%20address%20this%2C%20we%20propose%20Sigmma%2C%20a%20multi-modal%20contrastive%20alignment%20framework%20for%20learning%20hierarchical%20representations%20of%20HE%20images%20and%20spatial%20transcriptome%20profiles%20across%20multiple%20scales.%20Sigmma%20introduces%20multi-scale%20contrastive%20alignment%2C%20ensuring%20that%20representations%20learned%20at%20different%20scales%20remain%20coherent%20across%20modalities.%20Furthermore%2C%20by%20representing%20cell%20interactions%20as%20a%20graph%20and%20integrating%20inter-%20and%20intra-subgraph%20relationships%2C%20our%20approach%20effectively%20captures%20cell-cell%20interactions%2C%20ranging%20from%20fine%20to%20coarse%2C%20within%20the%20tissue%20microenvironment.%20We%20demonstrate%20that%20Sigmm%20learns%20representations%20that%20better%20capture%20cross-modal%20correspondences%2C%20leading%20to%20an%20improvement%20of%20avg.%209.78%5C%25%20in%20the%20gene-expression%20prediction%20task%20and%20avg.%2026.93%5C%25%20in%20the%20cross-modal%20retrieval%20task%20across%20datasets.%20We%20further%20show%20that%20it%20learns%20meaningful%20multi-tissue%20organization%20in%20downstream%20analyses.&entry.1838667208=http%3A//arxiv.org/abs/2511.15464v3&entry.124074799=Read"},
{"title": "Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification", "author": "Cheng-Han Lu and Pei-Hsuan Tsai", "abstract": "Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.", "link": "http://arxiv.org/abs/2512.14491v1", "date": "2025-12-16", "relevancy": 2.675, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5838}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5127}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Multi-Modal%20Transformer%20with%20Masking%20for%20Alzheimer%27s%20Disease%20Classification&body=Title%3A%20Sparse%20Multi-Modal%20Transformer%20with%20Masking%20for%20Alzheimer%27s%20Disease%20Classification%0AAuthor%3A%20Cheng-Han%20Lu%20and%20Pei-Hsuan%20Tsai%0AAbstract%3A%20Transformer-based%20multi-modal%20intelligent%20systems%20often%20suffer%20from%20high%20computational%20and%20energy%20costs%20due%20to%20dense%20self-attention%2C%20limiting%20their%20scalability%20under%20resource%20constraints.%20This%20paper%20presents%20SMMT%2C%20a%20sparse%20multi-modal%20transformer%20architecture%20designed%20to%20improve%20efficiency%20and%20robustness.%20Building%20upon%20a%20cascaded%20multi-modal%20transformer%20framework%2C%20SMMT%20introduces%20cluster-based%20sparse%20attention%20to%20achieve%20near%20linear%20computational%20complexity%20and%20modality-wise%20masking%20to%20enhance%20robustness%20against%20incomplete%20inputs.%20The%20architecture%20is%20evaluated%20using%20Alzheimer%27s%20Disease%20classification%20on%20the%20ADNI%20dataset%20as%20a%20representative%20multi-modal%20case%20study.%20Experimental%20results%20show%20that%20SMMT%20maintains%20competitive%20predictive%20performance%20while%20significantly%20reducing%20training%20time%2C%20memory%20usage%2C%20and%20energy%20consumption%20compared%20to%20dense%20attention%20baselines%2C%20demonstrating%20its%20suitability%20as%20a%20resource-aware%20architectural%20component%20for%20scalable%20intelligent%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Multi-Modal%2520Transformer%2520with%2520Masking%2520for%2520Alzheimer%2527s%2520Disease%2520Classification%26entry.906535625%3DCheng-Han%2520Lu%2520and%2520Pei-Hsuan%2520Tsai%26entry.1292438233%3DTransformer-based%2520multi-modal%2520intelligent%2520systems%2520often%2520suffer%2520from%2520high%2520computational%2520and%2520energy%2520costs%2520due%2520to%2520dense%2520self-attention%252C%2520limiting%2520their%2520scalability%2520under%2520resource%2520constraints.%2520This%2520paper%2520presents%2520SMMT%252C%2520a%2520sparse%2520multi-modal%2520transformer%2520architecture%2520designed%2520to%2520improve%2520efficiency%2520and%2520robustness.%2520Building%2520upon%2520a%2520cascaded%2520multi-modal%2520transformer%2520framework%252C%2520SMMT%2520introduces%2520cluster-based%2520sparse%2520attention%2520to%2520achieve%2520near%2520linear%2520computational%2520complexity%2520and%2520modality-wise%2520masking%2520to%2520enhance%2520robustness%2520against%2520incomplete%2520inputs.%2520The%2520architecture%2520is%2520evaluated%2520using%2520Alzheimer%2527s%2520Disease%2520classification%2520on%2520the%2520ADNI%2520dataset%2520as%2520a%2520representative%2520multi-modal%2520case%2520study.%2520Experimental%2520results%2520show%2520that%2520SMMT%2520maintains%2520competitive%2520predictive%2520performance%2520while%2520significantly%2520reducing%2520training%2520time%252C%2520memory%2520usage%252C%2520and%2520energy%2520consumption%2520compared%2520to%2520dense%2520attention%2520baselines%252C%2520demonstrating%2520its%2520suitability%2520as%2520a%2520resource-aware%2520architectural%2520component%2520for%2520scalable%2520intelligent%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Multi-Modal%20Transformer%20with%20Masking%20for%20Alzheimer%27s%20Disease%20Classification&entry.906535625=Cheng-Han%20Lu%20and%20Pei-Hsuan%20Tsai&entry.1292438233=Transformer-based%20multi-modal%20intelligent%20systems%20often%20suffer%20from%20high%20computational%20and%20energy%20costs%20due%20to%20dense%20self-attention%2C%20limiting%20their%20scalability%20under%20resource%20constraints.%20This%20paper%20presents%20SMMT%2C%20a%20sparse%20multi-modal%20transformer%20architecture%20designed%20to%20improve%20efficiency%20and%20robustness.%20Building%20upon%20a%20cascaded%20multi-modal%20transformer%20framework%2C%20SMMT%20introduces%20cluster-based%20sparse%20attention%20to%20achieve%20near%20linear%20computational%20complexity%20and%20modality-wise%20masking%20to%20enhance%20robustness%20against%20incomplete%20inputs.%20The%20architecture%20is%20evaluated%20using%20Alzheimer%27s%20Disease%20classification%20on%20the%20ADNI%20dataset%20as%20a%20representative%20multi-modal%20case%20study.%20Experimental%20results%20show%20that%20SMMT%20maintains%20competitive%20predictive%20performance%20while%20significantly%20reducing%20training%20time%2C%20memory%20usage%2C%20and%20energy%20consumption%20compared%20to%20dense%20attention%20baselines%2C%20demonstrating%20its%20suitability%20as%20a%20resource-aware%20architectural%20component%20for%20scalable%20intelligent%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.14491v1&entry.124074799=Read"},
{"title": "Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting", "author": "Yuxuan Yang and Dalin Zhang and Yuxuan Liang and Hua Lu and Gang Chen and Huan Li", "abstract": "Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning. The code is available at https://github.com/SuDIS-ZJU/SCAM.", "link": "http://arxiv.org/abs/2502.14704v4", "date": "2025-12-16", "relevancy": 2.6613, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5582}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5252}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Data%20are%20Good%20Labels%3A%20On%20the%20Self-supervised%20Labeling%20for%20Time%20Series%20Forecasting&body=Title%3A%20Not%20All%20Data%20are%20Good%20Labels%3A%20On%20the%20Self-supervised%20Labeling%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Yuxuan%20Yang%20and%20Dalin%20Zhang%20and%20Yuxuan%20Liang%20and%20Hua%20Lu%20and%20Gang%20Chen%20and%20Huan%20Li%0AAbstract%3A%20Time%20Series%20Forecasting%20%28TSF%29%20is%20a%20crucial%20task%20in%20various%20domains%2C%20yet%20existing%20TSF%20models%20rely%20heavily%20on%20high-quality%20data%20and%20insufficiently%20exploit%20all%20available%20data.%20This%20paper%20explores%20a%20novel%20self-supervised%20approach%20to%20re-label%20time%20series%20datasets%20by%20inherently%20constructing%20candidate%20datasets.%20During%20the%20optimization%20of%20a%20simple%20reconstruction%20network%2C%20intermediates%20are%20used%20as%20pseudo%20labels%20in%20a%20self-supervised%20paradigm%2C%20improving%20generalization%20for%20any%20predictor.%20We%20introduce%20the%20Self-Correction%20with%20Adaptive%20Mask%20%28SCAM%29%2C%20which%20discards%20overfitted%20components%20and%20selectively%20replaces%20them%20with%20pseudo%20labels%20generated%20from%20reconstructions.%20Additionally%2C%20we%20incorporate%20Spectral%20Norm%20Regularization%20%28SNR%29%20to%20further%20suppress%20overfitting%20from%20a%20loss%20landscape%20perspective.%20Our%20experiments%20on%20eleven%20real-world%20datasets%20demonstrate%20that%20SCAM%20consistently%20improves%20the%20performance%20of%20various%20backbone%20models.%20This%20work%20offers%20a%20new%20perspective%20on%20constructing%20datasets%20and%20enhancing%20the%20generalization%20of%20TSF%20models%20through%20self-supervised%20learning.%20The%20code%20is%20available%20at%20https%3A//github.com/SuDIS-ZJU/SCAM.%0ALink%3A%20http%3A//arxiv.org/abs/2502.14704v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Data%2520are%2520Good%2520Labels%253A%2520On%2520the%2520Self-supervised%2520Labeling%2520for%2520Time%2520Series%2520Forecasting%26entry.906535625%3DYuxuan%2520Yang%2520and%2520Dalin%2520Zhang%2520and%2520Yuxuan%2520Liang%2520and%2520Hua%2520Lu%2520and%2520Gang%2520Chen%2520and%2520Huan%2520Li%26entry.1292438233%3DTime%2520Series%2520Forecasting%2520%2528TSF%2529%2520is%2520a%2520crucial%2520task%2520in%2520various%2520domains%252C%2520yet%2520existing%2520TSF%2520models%2520rely%2520heavily%2520on%2520high-quality%2520data%2520and%2520insufficiently%2520exploit%2520all%2520available%2520data.%2520This%2520paper%2520explores%2520a%2520novel%2520self-supervised%2520approach%2520to%2520re-label%2520time%2520series%2520datasets%2520by%2520inherently%2520constructing%2520candidate%2520datasets.%2520During%2520the%2520optimization%2520of%2520a%2520simple%2520reconstruction%2520network%252C%2520intermediates%2520are%2520used%2520as%2520pseudo%2520labels%2520in%2520a%2520self-supervised%2520paradigm%252C%2520improving%2520generalization%2520for%2520any%2520predictor.%2520We%2520introduce%2520the%2520Self-Correction%2520with%2520Adaptive%2520Mask%2520%2528SCAM%2529%252C%2520which%2520discards%2520overfitted%2520components%2520and%2520selectively%2520replaces%2520them%2520with%2520pseudo%2520labels%2520generated%2520from%2520reconstructions.%2520Additionally%252C%2520we%2520incorporate%2520Spectral%2520Norm%2520Regularization%2520%2528SNR%2529%2520to%2520further%2520suppress%2520overfitting%2520from%2520a%2520loss%2520landscape%2520perspective.%2520Our%2520experiments%2520on%2520eleven%2520real-world%2520datasets%2520demonstrate%2520that%2520SCAM%2520consistently%2520improves%2520the%2520performance%2520of%2520various%2520backbone%2520models.%2520This%2520work%2520offers%2520a%2520new%2520perspective%2520on%2520constructing%2520datasets%2520and%2520enhancing%2520the%2520generalization%2520of%2520TSF%2520models%2520through%2520self-supervised%2520learning.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/SuDIS-ZJU/SCAM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14704v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Data%20are%20Good%20Labels%3A%20On%20the%20Self-supervised%20Labeling%20for%20Time%20Series%20Forecasting&entry.906535625=Yuxuan%20Yang%20and%20Dalin%20Zhang%20and%20Yuxuan%20Liang%20and%20Hua%20Lu%20and%20Gang%20Chen%20and%20Huan%20Li&entry.1292438233=Time%20Series%20Forecasting%20%28TSF%29%20is%20a%20crucial%20task%20in%20various%20domains%2C%20yet%20existing%20TSF%20models%20rely%20heavily%20on%20high-quality%20data%20and%20insufficiently%20exploit%20all%20available%20data.%20This%20paper%20explores%20a%20novel%20self-supervised%20approach%20to%20re-label%20time%20series%20datasets%20by%20inherently%20constructing%20candidate%20datasets.%20During%20the%20optimization%20of%20a%20simple%20reconstruction%20network%2C%20intermediates%20are%20used%20as%20pseudo%20labels%20in%20a%20self-supervised%20paradigm%2C%20improving%20generalization%20for%20any%20predictor.%20We%20introduce%20the%20Self-Correction%20with%20Adaptive%20Mask%20%28SCAM%29%2C%20which%20discards%20overfitted%20components%20and%20selectively%20replaces%20them%20with%20pseudo%20labels%20generated%20from%20reconstructions.%20Additionally%2C%20we%20incorporate%20Spectral%20Norm%20Regularization%20%28SNR%29%20to%20further%20suppress%20overfitting%20from%20a%20loss%20landscape%20perspective.%20Our%20experiments%20on%20eleven%20real-world%20datasets%20demonstrate%20that%20SCAM%20consistently%20improves%20the%20performance%20of%20various%20backbone%20models.%20This%20work%20offers%20a%20new%20perspective%20on%20constructing%20datasets%20and%20enhancing%20the%20generalization%20of%20TSF%20models%20through%20self-supervised%20learning.%20The%20code%20is%20available%20at%20https%3A//github.com/SuDIS-ZJU/SCAM.&entry.1838667208=http%3A//arxiv.org/abs/2502.14704v4&entry.124074799=Read"},
{"title": "RePo: Language Models with Context Re-Positioning", "author": "Huayang Li and Tianyu Zhao and Richard Sproat", "abstract": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_\u03c6$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.", "link": "http://arxiv.org/abs/2512.14391v1", "date": "2025-12-16", "relevancy": 2.6459, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RePo%3A%20Language%20Models%20with%20Context%20Re-Positioning&body=Title%3A%20RePo%3A%20Language%20Models%20with%20Context%20Re-Positioning%0AAuthor%3A%20Huayang%20Li%20and%20Tianyu%20Zhao%20and%20Richard%20Sproat%0AAbstract%3A%20In-context%20learning%20is%20fundamental%20to%20modern%20Large%20Language%20Models%20%28LLMs%29%3B%20however%2C%20prevailing%20architectures%20impose%20a%20rigid%20and%20fixed%20contextual%20structure%20by%20assigning%20linear%20or%20constant%20positional%20indices.%20Drawing%20on%20Cognitive%20Load%20Theory%20%28CLT%29%2C%20we%20argue%20that%20this%20uninformative%20structure%20increases%20extraneous%20cognitive%20load%2C%20consuming%20finite%20working%20memory%20capacity%20that%20should%20be%20allocated%20to%20deep%20reasoning%20and%20attention%20allocation.%20To%20address%20this%2C%20we%20propose%20RePo%2C%20a%20novel%20mechanism%20that%20reduces%20extraneous%20load%20via%20context%20re-positioning.%20Unlike%20standard%20approaches%2C%20RePo%20utilizes%20a%20differentiable%20module%2C%20%24f_%CF%86%24%2C%20to%20assign%20token%20positions%20that%20capture%20contextual%20dependencies%2C%20rather%20than%20replying%20on%20pre-defined%20integer%20range.%20By%20continually%20pre-training%20on%20the%20OLMo-2%201B%20backbone%2C%20we%20demonstrate%20that%20RePo%20significantly%20enhances%20performance%20on%20tasks%20involving%20noisy%20contexts%2C%20structured%20data%2C%20and%20longer%20context%20length%2C%20while%20maintaining%20competitive%20performance%20on%20general%20short-context%20tasks.%20Detailed%20analysis%20reveals%20that%20RePo%20successfully%20allocate%20higher%20attention%20to%20distant%20but%20relevant%20information%2C%20assign%20positions%20in%20dense%20and%20non-linear%20space%2C%20and%20capture%20the%20intrinsic%20structure%20of%20the%20input%20context.%20Our%20code%20is%20available%20at%20https%3A//github.com/SakanaAI/repo.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRePo%253A%2520Language%2520Models%2520with%2520Context%2520Re-Positioning%26entry.906535625%3DHuayang%2520Li%2520and%2520Tianyu%2520Zhao%2520and%2520Richard%2520Sproat%26entry.1292438233%3DIn-context%2520learning%2520is%2520fundamental%2520to%2520modern%2520Large%2520Language%2520Models%2520%2528LLMs%2529%253B%2520however%252C%2520prevailing%2520architectures%2520impose%2520a%2520rigid%2520and%2520fixed%2520contextual%2520structure%2520by%2520assigning%2520linear%2520or%2520constant%2520positional%2520indices.%2520Drawing%2520on%2520Cognitive%2520Load%2520Theory%2520%2528CLT%2529%252C%2520we%2520argue%2520that%2520this%2520uninformative%2520structure%2520increases%2520extraneous%2520cognitive%2520load%252C%2520consuming%2520finite%2520working%2520memory%2520capacity%2520that%2520should%2520be%2520allocated%2520to%2520deep%2520reasoning%2520and%2520attention%2520allocation.%2520To%2520address%2520this%252C%2520we%2520propose%2520RePo%252C%2520a%2520novel%2520mechanism%2520that%2520reduces%2520extraneous%2520load%2520via%2520context%2520re-positioning.%2520Unlike%2520standard%2520approaches%252C%2520RePo%2520utilizes%2520a%2520differentiable%2520module%252C%2520%2524f_%25CF%2586%2524%252C%2520to%2520assign%2520token%2520positions%2520that%2520capture%2520contextual%2520dependencies%252C%2520rather%2520than%2520replying%2520on%2520pre-defined%2520integer%2520range.%2520By%2520continually%2520pre-training%2520on%2520the%2520OLMo-2%25201B%2520backbone%252C%2520we%2520demonstrate%2520that%2520RePo%2520significantly%2520enhances%2520performance%2520on%2520tasks%2520involving%2520noisy%2520contexts%252C%2520structured%2520data%252C%2520and%2520longer%2520context%2520length%252C%2520while%2520maintaining%2520competitive%2520performance%2520on%2520general%2520short-context%2520tasks.%2520Detailed%2520analysis%2520reveals%2520that%2520RePo%2520successfully%2520allocate%2520higher%2520attention%2520to%2520distant%2520but%2520relevant%2520information%252C%2520assign%2520positions%2520in%2520dense%2520and%2520non-linear%2520space%252C%2520and%2520capture%2520the%2520intrinsic%2520structure%2520of%2520the%2520input%2520context.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/SakanaAI/repo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RePo%3A%20Language%20Models%20with%20Context%20Re-Positioning&entry.906535625=Huayang%20Li%20and%20Tianyu%20Zhao%20and%20Richard%20Sproat&entry.1292438233=In-context%20learning%20is%20fundamental%20to%20modern%20Large%20Language%20Models%20%28LLMs%29%3B%20however%2C%20prevailing%20architectures%20impose%20a%20rigid%20and%20fixed%20contextual%20structure%20by%20assigning%20linear%20or%20constant%20positional%20indices.%20Drawing%20on%20Cognitive%20Load%20Theory%20%28CLT%29%2C%20we%20argue%20that%20this%20uninformative%20structure%20increases%20extraneous%20cognitive%20load%2C%20consuming%20finite%20working%20memory%20capacity%20that%20should%20be%20allocated%20to%20deep%20reasoning%20and%20attention%20allocation.%20To%20address%20this%2C%20we%20propose%20RePo%2C%20a%20novel%20mechanism%20that%20reduces%20extraneous%20load%20via%20context%20re-positioning.%20Unlike%20standard%20approaches%2C%20RePo%20utilizes%20a%20differentiable%20module%2C%20%24f_%CF%86%24%2C%20to%20assign%20token%20positions%20that%20capture%20contextual%20dependencies%2C%20rather%20than%20replying%20on%20pre-defined%20integer%20range.%20By%20continually%20pre-training%20on%20the%20OLMo-2%201B%20backbone%2C%20we%20demonstrate%20that%20RePo%20significantly%20enhances%20performance%20on%20tasks%20involving%20noisy%20contexts%2C%20structured%20data%2C%20and%20longer%20context%20length%2C%20while%20maintaining%20competitive%20performance%20on%20general%20short-context%20tasks.%20Detailed%20analysis%20reveals%20that%20RePo%20successfully%20allocate%20higher%20attention%20to%20distant%20but%20relevant%20information%2C%20assign%20positions%20in%20dense%20and%20non-linear%20space%2C%20and%20capture%20the%20intrinsic%20structure%20of%20the%20input%20context.%20Our%20code%20is%20available%20at%20https%3A//github.com/SakanaAI/repo.&entry.1838667208=http%3A//arxiv.org/abs/2512.14391v1&entry.124074799=Read"},
{"title": "CrossPT-EEG: A Benchmark for Cross-Participant and Cross-Time Generalization of EEG-based Visual Decoding", "author": "Shuqi Zhu and Ziyi Ye and Qingyao Ai and Yiqun Liu", "abstract": "Exploring brain activity in relation to visual perception provides insights into the biological representation of the world. While functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) have enabled effective image classification and reconstruction, their high cost and bulk limit practical use. Electroencephalography (EEG), by contrast, offers low cost and excellent temporal resolution, but its potential has been limited by the scarcity of large, high-quality datasets and by block-design experiments that introduce temporal confounds. To fill this gap, we present CrossPT-EEG, a benchmark for cross-participant and cross-time generalization of visual decoding from EEG. We collected EEG data from 16 participants while they viewed 4,000 images sampled from ImageNet, with image stimuli annotated at multiple levels of granularity. Our design includes two stages separated in time to allow cross-time generalization and avoid block-design artifacts. We also introduce benchmarks tailored to non-block design classification, as well as pre-training experiments to assess cross-time and cross-participant generalization. These findings highlight the dataset's potential to enhance EEG-based visual brain-computer interfaces, deepen our understanding of visual perception in biological systems, and suggest promising applications for improving machine vision models.", "link": "http://arxiv.org/abs/2406.07151v2", "date": "2025-12-16", "relevancy": 2.6318, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrossPT-EEG%3A%20A%20Benchmark%20for%20Cross-Participant%20and%20Cross-Time%20Generalization%20of%20EEG-based%20Visual%20Decoding&body=Title%3A%20CrossPT-EEG%3A%20A%20Benchmark%20for%20Cross-Participant%20and%20Cross-Time%20Generalization%20of%20EEG-based%20Visual%20Decoding%0AAuthor%3A%20Shuqi%20Zhu%20and%20Ziyi%20Ye%20and%20Qingyao%20Ai%20and%20Yiqun%20Liu%0AAbstract%3A%20Exploring%20brain%20activity%20in%20relation%20to%20visual%20perception%20provides%20insights%20into%20the%20biological%20representation%20of%20the%20world.%20While%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20and%20magnetoencephalography%20%28MEG%29%20have%20enabled%20effective%20image%20classification%20and%20reconstruction%2C%20their%20high%20cost%20and%20bulk%20limit%20practical%20use.%20Electroencephalography%20%28EEG%29%2C%20by%20contrast%2C%20offers%20low%20cost%20and%20excellent%20temporal%20resolution%2C%20but%20its%20potential%20has%20been%20limited%20by%20the%20scarcity%20of%20large%2C%20high-quality%20datasets%20and%20by%20block-design%20experiments%20that%20introduce%20temporal%20confounds.%20To%20fill%20this%20gap%2C%20we%20present%20CrossPT-EEG%2C%20a%20benchmark%20for%20cross-participant%20and%20cross-time%20generalization%20of%20visual%20decoding%20from%20EEG.%20We%20collected%20EEG%20data%20from%2016%20participants%20while%20they%20viewed%204%2C000%20images%20sampled%20from%20ImageNet%2C%20with%20image%20stimuli%20annotated%20at%20multiple%20levels%20of%20granularity.%20Our%20design%20includes%20two%20stages%20separated%20in%20time%20to%20allow%20cross-time%20generalization%20and%20avoid%20block-design%20artifacts.%20We%20also%20introduce%20benchmarks%20tailored%20to%20non-block%20design%20classification%2C%20as%20well%20as%20pre-training%20experiments%20to%20assess%20cross-time%20and%20cross-participant%20generalization.%20These%20findings%20highlight%20the%20dataset%27s%20potential%20to%20enhance%20EEG-based%20visual%20brain-computer%20interfaces%2C%20deepen%20our%20understanding%20of%20visual%20perception%20in%20biological%20systems%2C%20and%20suggest%20promising%20applications%20for%20improving%20machine%20vision%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2406.07151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrossPT-EEG%253A%2520A%2520Benchmark%2520for%2520Cross-Participant%2520and%2520Cross-Time%2520Generalization%2520of%2520EEG-based%2520Visual%2520Decoding%26entry.906535625%3DShuqi%2520Zhu%2520and%2520Ziyi%2520Ye%2520and%2520Qingyao%2520Ai%2520and%2520Yiqun%2520Liu%26entry.1292438233%3DExploring%2520brain%2520activity%2520in%2520relation%2520to%2520visual%2520perception%2520provides%2520insights%2520into%2520the%2520biological%2520representation%2520of%2520the%2520world.%2520While%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%2520and%2520magnetoencephalography%2520%2528MEG%2529%2520have%2520enabled%2520effective%2520image%2520classification%2520and%2520reconstruction%252C%2520their%2520high%2520cost%2520and%2520bulk%2520limit%2520practical%2520use.%2520Electroencephalography%2520%2528EEG%2529%252C%2520by%2520contrast%252C%2520offers%2520low%2520cost%2520and%2520excellent%2520temporal%2520resolution%252C%2520but%2520its%2520potential%2520has%2520been%2520limited%2520by%2520the%2520scarcity%2520of%2520large%252C%2520high-quality%2520datasets%2520and%2520by%2520block-design%2520experiments%2520that%2520introduce%2520temporal%2520confounds.%2520To%2520fill%2520this%2520gap%252C%2520we%2520present%2520CrossPT-EEG%252C%2520a%2520benchmark%2520for%2520cross-participant%2520and%2520cross-time%2520generalization%2520of%2520visual%2520decoding%2520from%2520EEG.%2520We%2520collected%2520EEG%2520data%2520from%252016%2520participants%2520while%2520they%2520viewed%25204%252C000%2520images%2520sampled%2520from%2520ImageNet%252C%2520with%2520image%2520stimuli%2520annotated%2520at%2520multiple%2520levels%2520of%2520granularity.%2520Our%2520design%2520includes%2520two%2520stages%2520separated%2520in%2520time%2520to%2520allow%2520cross-time%2520generalization%2520and%2520avoid%2520block-design%2520artifacts.%2520We%2520also%2520introduce%2520benchmarks%2520tailored%2520to%2520non-block%2520design%2520classification%252C%2520as%2520well%2520as%2520pre-training%2520experiments%2520to%2520assess%2520cross-time%2520and%2520cross-participant%2520generalization.%2520These%2520findings%2520highlight%2520the%2520dataset%2527s%2520potential%2520to%2520enhance%2520EEG-based%2520visual%2520brain-computer%2520interfaces%252C%2520deepen%2520our%2520understanding%2520of%2520visual%2520perception%2520in%2520biological%2520systems%252C%2520and%2520suggest%2520promising%2520applications%2520for%2520improving%2520machine%2520vision%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrossPT-EEG%3A%20A%20Benchmark%20for%20Cross-Participant%20and%20Cross-Time%20Generalization%20of%20EEG-based%20Visual%20Decoding&entry.906535625=Shuqi%20Zhu%20and%20Ziyi%20Ye%20and%20Qingyao%20Ai%20and%20Yiqun%20Liu&entry.1292438233=Exploring%20brain%20activity%20in%20relation%20to%20visual%20perception%20provides%20insights%20into%20the%20biological%20representation%20of%20the%20world.%20While%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20and%20magnetoencephalography%20%28MEG%29%20have%20enabled%20effective%20image%20classification%20and%20reconstruction%2C%20their%20high%20cost%20and%20bulk%20limit%20practical%20use.%20Electroencephalography%20%28EEG%29%2C%20by%20contrast%2C%20offers%20low%20cost%20and%20excellent%20temporal%20resolution%2C%20but%20its%20potential%20has%20been%20limited%20by%20the%20scarcity%20of%20large%2C%20high-quality%20datasets%20and%20by%20block-design%20experiments%20that%20introduce%20temporal%20confounds.%20To%20fill%20this%20gap%2C%20we%20present%20CrossPT-EEG%2C%20a%20benchmark%20for%20cross-participant%20and%20cross-time%20generalization%20of%20visual%20decoding%20from%20EEG.%20We%20collected%20EEG%20data%20from%2016%20participants%20while%20they%20viewed%204%2C000%20images%20sampled%20from%20ImageNet%2C%20with%20image%20stimuli%20annotated%20at%20multiple%20levels%20of%20granularity.%20Our%20design%20includes%20two%20stages%20separated%20in%20time%20to%20allow%20cross-time%20generalization%20and%20avoid%20block-design%20artifacts.%20We%20also%20introduce%20benchmarks%20tailored%20to%20non-block%20design%20classification%2C%20as%20well%20as%20pre-training%20experiments%20to%20assess%20cross-time%20and%20cross-participant%20generalization.%20These%20findings%20highlight%20the%20dataset%27s%20potential%20to%20enhance%20EEG-based%20visual%20brain-computer%20interfaces%2C%20deepen%20our%20understanding%20of%20visual%20perception%20in%20biological%20systems%2C%20and%20suggest%20promising%20applications%20for%20improving%20machine%20vision%20models.&entry.1838667208=http%3A//arxiv.org/abs/2406.07151v2&entry.124074799=Read"},
{"title": "High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models", "author": "Tristan S. W. Stevens and Ois\u00edn Nolan and Oudom Somphone and Jean-Luc Robert and Ruud J. G. van Sloun", "abstract": "Three-dimensional ultrasound enables real-time volumetric visualization of anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces reliance on precise probe orientation, potentially making ultrasound more accessible to clinicians with varying levels of experience and improving automated measurements and post-exam analysis. However, achieving both high volume rates and high image quality remains a significant challenge. While 3D diverging waves can provide high volume rates, they suffer from limited tissue harmonic generation and increased multipath effects, which degrade image quality. One compromise is to retain focus in elevation while leveraging unfocused diverging waves in the lateral direction to reduce the number of transmissions per elevation plane. Reaching the volume rates achieved by full 3D diverging waves, however, requires dramatically undersampling the number of elevation planes. Subsequently, to render the full volume, simple interpolation techniques are applied. This paper introduces a novel approach to 3D ultrasound reconstruction from a reduced set of elevation planes by employing diffusion models (DMs) to achieve increased spatial and temporal resolution. We compare both traditional and supervised deep learning-based interpolation methods on a 3D cardiac ultrasound dataset. Our results show that DM-based reconstruction consistently outperforms the baselines in image quality and downstream task performance. Additionally, we accelerate inference by leveraging the temporal consistency inherent to ultrasound sequences. Finally, we explore the robustness of the proposed method by exploiting the probabilistic nature of diffusion posterior sampling to quantify reconstruction uncertainty and demonstrate improved recall on out-of-distribution data with synthetic anomalies under strong subsampling.", "link": "http://arxiv.org/abs/2505.22090v2", "date": "2025-12-16", "relevancy": 2.6173, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6612}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6612}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High%20Volume%20Rate%203D%20Ultrasound%20Reconstruction%20with%20Diffusion%20Models&body=Title%3A%20High%20Volume%20Rate%203D%20Ultrasound%20Reconstruction%20with%20Diffusion%20Models%0AAuthor%3A%20Tristan%20S.%20W.%20Stevens%20and%20Ois%C3%ADn%20Nolan%20and%20Oudom%20Somphone%20and%20Jean-Luc%20Robert%20and%20Ruud%20J.%20G.%20van%20Sloun%0AAbstract%3A%20Three-dimensional%20ultrasound%20enables%20real-time%20volumetric%20visualization%20of%20anatomical%20structures.%20Unlike%20traditional%202D%20ultrasound%2C%203D%20imaging%20reduces%20reliance%20on%20precise%20probe%20orientation%2C%20potentially%20making%20ultrasound%20more%20accessible%20to%20clinicians%20with%20varying%20levels%20of%20experience%20and%20improving%20automated%20measurements%20and%20post-exam%20analysis.%20However%2C%20achieving%20both%20high%20volume%20rates%20and%20high%20image%20quality%20remains%20a%20significant%20challenge.%20While%203D%20diverging%20waves%20can%20provide%20high%20volume%20rates%2C%20they%20suffer%20from%20limited%20tissue%20harmonic%20generation%20and%20increased%20multipath%20effects%2C%20which%20degrade%20image%20quality.%20One%20compromise%20is%20to%20retain%20focus%20in%20elevation%20while%20leveraging%20unfocused%20diverging%20waves%20in%20the%20lateral%20direction%20to%20reduce%20the%20number%20of%20transmissions%20per%20elevation%20plane.%20Reaching%20the%20volume%20rates%20achieved%20by%20full%203D%20diverging%20waves%2C%20however%2C%20requires%20dramatically%20undersampling%20the%20number%20of%20elevation%20planes.%20Subsequently%2C%20to%20render%20the%20full%20volume%2C%20simple%20interpolation%20techniques%20are%20applied.%20This%20paper%20introduces%20a%20novel%20approach%20to%203D%20ultrasound%20reconstruction%20from%20a%20reduced%20set%20of%20elevation%20planes%20by%20employing%20diffusion%20models%20%28DMs%29%20to%20achieve%20increased%20spatial%20and%20temporal%20resolution.%20We%20compare%20both%20traditional%20and%20supervised%20deep%20learning-based%20interpolation%20methods%20on%20a%203D%20cardiac%20ultrasound%20dataset.%20Our%20results%20show%20that%20DM-based%20reconstruction%20consistently%20outperforms%20the%20baselines%20in%20image%20quality%20and%20downstream%20task%20performance.%20Additionally%2C%20we%20accelerate%20inference%20by%20leveraging%20the%20temporal%20consistency%20inherent%20to%20ultrasound%20sequences.%20Finally%2C%20we%20explore%20the%20robustness%20of%20the%20proposed%20method%20by%20exploiting%20the%20probabilistic%20nature%20of%20diffusion%20posterior%20sampling%20to%20quantify%20reconstruction%20uncertainty%20and%20demonstrate%20improved%20recall%20on%20out-of-distribution%20data%20with%20synthetic%20anomalies%20under%20strong%20subsampling.%0ALink%3A%20http%3A//arxiv.org/abs/2505.22090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh%2520Volume%2520Rate%25203D%2520Ultrasound%2520Reconstruction%2520with%2520Diffusion%2520Models%26entry.906535625%3DTristan%2520S.%2520W.%2520Stevens%2520and%2520Ois%25C3%25ADn%2520Nolan%2520and%2520Oudom%2520Somphone%2520and%2520Jean-Luc%2520Robert%2520and%2520Ruud%2520J.%2520G.%2520van%2520Sloun%26entry.1292438233%3DThree-dimensional%2520ultrasound%2520enables%2520real-time%2520volumetric%2520visualization%2520of%2520anatomical%2520structures.%2520Unlike%2520traditional%25202D%2520ultrasound%252C%25203D%2520imaging%2520reduces%2520reliance%2520on%2520precise%2520probe%2520orientation%252C%2520potentially%2520making%2520ultrasound%2520more%2520accessible%2520to%2520clinicians%2520with%2520varying%2520levels%2520of%2520experience%2520and%2520improving%2520automated%2520measurements%2520and%2520post-exam%2520analysis.%2520However%252C%2520achieving%2520both%2520high%2520volume%2520rates%2520and%2520high%2520image%2520quality%2520remains%2520a%2520significant%2520challenge.%2520While%25203D%2520diverging%2520waves%2520can%2520provide%2520high%2520volume%2520rates%252C%2520they%2520suffer%2520from%2520limited%2520tissue%2520harmonic%2520generation%2520and%2520increased%2520multipath%2520effects%252C%2520which%2520degrade%2520image%2520quality.%2520One%2520compromise%2520is%2520to%2520retain%2520focus%2520in%2520elevation%2520while%2520leveraging%2520unfocused%2520diverging%2520waves%2520in%2520the%2520lateral%2520direction%2520to%2520reduce%2520the%2520number%2520of%2520transmissions%2520per%2520elevation%2520plane.%2520Reaching%2520the%2520volume%2520rates%2520achieved%2520by%2520full%25203D%2520diverging%2520waves%252C%2520however%252C%2520requires%2520dramatically%2520undersampling%2520the%2520number%2520of%2520elevation%2520planes.%2520Subsequently%252C%2520to%2520render%2520the%2520full%2520volume%252C%2520simple%2520interpolation%2520techniques%2520are%2520applied.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%25203D%2520ultrasound%2520reconstruction%2520from%2520a%2520reduced%2520set%2520of%2520elevation%2520planes%2520by%2520employing%2520diffusion%2520models%2520%2528DMs%2529%2520to%2520achieve%2520increased%2520spatial%2520and%2520temporal%2520resolution.%2520We%2520compare%2520both%2520traditional%2520and%2520supervised%2520deep%2520learning-based%2520interpolation%2520methods%2520on%2520a%25203D%2520cardiac%2520ultrasound%2520dataset.%2520Our%2520results%2520show%2520that%2520DM-based%2520reconstruction%2520consistently%2520outperforms%2520the%2520baselines%2520in%2520image%2520quality%2520and%2520downstream%2520task%2520performance.%2520Additionally%252C%2520we%2520accelerate%2520inference%2520by%2520leveraging%2520the%2520temporal%2520consistency%2520inherent%2520to%2520ultrasound%2520sequences.%2520Finally%252C%2520we%2520explore%2520the%2520robustness%2520of%2520the%2520proposed%2520method%2520by%2520exploiting%2520the%2520probabilistic%2520nature%2520of%2520diffusion%2520posterior%2520sampling%2520to%2520quantify%2520reconstruction%2520uncertainty%2520and%2520demonstrate%2520improved%2520recall%2520on%2520out-of-distribution%2520data%2520with%2520synthetic%2520anomalies%2520under%2520strong%2520subsampling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High%20Volume%20Rate%203D%20Ultrasound%20Reconstruction%20with%20Diffusion%20Models&entry.906535625=Tristan%20S.%20W.%20Stevens%20and%20Ois%C3%ADn%20Nolan%20and%20Oudom%20Somphone%20and%20Jean-Luc%20Robert%20and%20Ruud%20J.%20G.%20van%20Sloun&entry.1292438233=Three-dimensional%20ultrasound%20enables%20real-time%20volumetric%20visualization%20of%20anatomical%20structures.%20Unlike%20traditional%202D%20ultrasound%2C%203D%20imaging%20reduces%20reliance%20on%20precise%20probe%20orientation%2C%20potentially%20making%20ultrasound%20more%20accessible%20to%20clinicians%20with%20varying%20levels%20of%20experience%20and%20improving%20automated%20measurements%20and%20post-exam%20analysis.%20However%2C%20achieving%20both%20high%20volume%20rates%20and%20high%20image%20quality%20remains%20a%20significant%20challenge.%20While%203D%20diverging%20waves%20can%20provide%20high%20volume%20rates%2C%20they%20suffer%20from%20limited%20tissue%20harmonic%20generation%20and%20increased%20multipath%20effects%2C%20which%20degrade%20image%20quality.%20One%20compromise%20is%20to%20retain%20focus%20in%20elevation%20while%20leveraging%20unfocused%20diverging%20waves%20in%20the%20lateral%20direction%20to%20reduce%20the%20number%20of%20transmissions%20per%20elevation%20plane.%20Reaching%20the%20volume%20rates%20achieved%20by%20full%203D%20diverging%20waves%2C%20however%2C%20requires%20dramatically%20undersampling%20the%20number%20of%20elevation%20planes.%20Subsequently%2C%20to%20render%20the%20full%20volume%2C%20simple%20interpolation%20techniques%20are%20applied.%20This%20paper%20introduces%20a%20novel%20approach%20to%203D%20ultrasound%20reconstruction%20from%20a%20reduced%20set%20of%20elevation%20planes%20by%20employing%20diffusion%20models%20%28DMs%29%20to%20achieve%20increased%20spatial%20and%20temporal%20resolution.%20We%20compare%20both%20traditional%20and%20supervised%20deep%20learning-based%20interpolation%20methods%20on%20a%203D%20cardiac%20ultrasound%20dataset.%20Our%20results%20show%20that%20DM-based%20reconstruction%20consistently%20outperforms%20the%20baselines%20in%20image%20quality%20and%20downstream%20task%20performance.%20Additionally%2C%20we%20accelerate%20inference%20by%20leveraging%20the%20temporal%20consistency%20inherent%20to%20ultrasound%20sequences.%20Finally%2C%20we%20explore%20the%20robustness%20of%20the%20proposed%20method%20by%20exploiting%20the%20probabilistic%20nature%20of%20diffusion%20posterior%20sampling%20to%20quantify%20reconstruction%20uncertainty%20and%20demonstrate%20improved%20recall%20on%20out-of-distribution%20data%20with%20synthetic%20anomalies%20under%20strong%20subsampling.&entry.1838667208=http%3A//arxiv.org/abs/2505.22090v2&entry.124074799=Read"},
{"title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition", "author": "Abdullah Al Mamun and Miaohua Zhang and David Ahmedt-Aristizabal and Zeeshan Hayder and Mohammad Awrangjeb", "abstract": "Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.", "link": "http://arxiv.org/abs/2512.14309v1", "date": "2025-12-16", "relevancy": 2.5913, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.543}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5113}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSMamba%3A%20Progressive%20Self-supervised%20Vision%20Mamba%20for%20Plant%20Disease%20Recognition&body=Title%3A%20PSMamba%3A%20Progressive%20Self-supervised%20Vision%20Mamba%20for%20Plant%20Disease%20Recognition%0AAuthor%3A%20Abdullah%20Al%20Mamun%20and%20Miaohua%20Zhang%20and%20David%20Ahmedt-Aristizabal%20and%20Zeeshan%20Hayder%20and%20Mohammad%20Awrangjeb%0AAbstract%3A%20Self-supervised%20Learning%20%28SSL%29%20has%20become%20a%20powerful%20paradigm%20for%20representation%20learning%20without%20manual%20annotations.%20However%2C%20most%20existing%20frameworks%20focus%20on%20global%20alignment%20and%20struggle%20to%20capture%20the%20hierarchical%2C%20multi-scale%20lesion%20patterns%20characteristic%20of%20plant%20disease%20imagery.%20To%20address%20this%20gap%2C%20we%20propose%20PSMamba%2C%20a%20progressive%20self-supervised%20framework%20that%20integrates%20the%20efficient%20sequence%20modelling%20of%20Vision%20Mamba%20%28VM%29%20with%20a%20dual-student%20hierarchical%20distillation%20strategy.%20Unlike%20conventional%20single%20teacher-student%20designs%2C%20PSMamba%20employs%20a%20shared%20global%20teacher%20and%20two%20specialised%20students%3A%20one%20processes%20mid-scale%20views%20to%20capture%20lesion%20distributions%20and%20vein%20structures%2C%20while%20the%20other%20focuses%20on%20local%20views%20to%20capture%20fine-grained%20cues%20such%20as%20texture%20irregularities%20and%20early-stage%20lesions.%20This%20multi-granular%20supervision%20facilitates%20the%20joint%20learning%20of%20contextual%20and%20detailed%20representations%2C%20with%20consistency%20losses%20ensuring%20coherent%20cross-scale%20alignment.%20Experiments%20on%20three%20benchmark%20datasets%20show%20that%20PSMamba%20consistently%20outperforms%20state-of-the-art%20SSL%20methods%2C%20delivering%20superior%20accuracy%20and%20robustness%20in%20both%20domain-shifted%20and%20fine-grained%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSMamba%253A%2520Progressive%2520Self-supervised%2520Vision%2520Mamba%2520for%2520Plant%2520Disease%2520Recognition%26entry.906535625%3DAbdullah%2520Al%2520Mamun%2520and%2520Miaohua%2520Zhang%2520and%2520David%2520Ahmedt-Aristizabal%2520and%2520Zeeshan%2520Hayder%2520and%2520Mohammad%2520Awrangjeb%26entry.1292438233%3DSelf-supervised%2520Learning%2520%2528SSL%2529%2520has%2520become%2520a%2520powerful%2520paradigm%2520for%2520representation%2520learning%2520without%2520manual%2520annotations.%2520However%252C%2520most%2520existing%2520frameworks%2520focus%2520on%2520global%2520alignment%2520and%2520struggle%2520to%2520capture%2520the%2520hierarchical%252C%2520multi-scale%2520lesion%2520patterns%2520characteristic%2520of%2520plant%2520disease%2520imagery.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520PSMamba%252C%2520a%2520progressive%2520self-supervised%2520framework%2520that%2520integrates%2520the%2520efficient%2520sequence%2520modelling%2520of%2520Vision%2520Mamba%2520%2528VM%2529%2520with%2520a%2520dual-student%2520hierarchical%2520distillation%2520strategy.%2520Unlike%2520conventional%2520single%2520teacher-student%2520designs%252C%2520PSMamba%2520employs%2520a%2520shared%2520global%2520teacher%2520and%2520two%2520specialised%2520students%253A%2520one%2520processes%2520mid-scale%2520views%2520to%2520capture%2520lesion%2520distributions%2520and%2520vein%2520structures%252C%2520while%2520the%2520other%2520focuses%2520on%2520local%2520views%2520to%2520capture%2520fine-grained%2520cues%2520such%2520as%2520texture%2520irregularities%2520and%2520early-stage%2520lesions.%2520This%2520multi-granular%2520supervision%2520facilitates%2520the%2520joint%2520learning%2520of%2520contextual%2520and%2520detailed%2520representations%252C%2520with%2520consistency%2520losses%2520ensuring%2520coherent%2520cross-scale%2520alignment.%2520Experiments%2520on%2520three%2520benchmark%2520datasets%2520show%2520that%2520PSMamba%2520consistently%2520outperforms%2520state-of-the-art%2520SSL%2520methods%252C%2520delivering%2520superior%2520accuracy%2520and%2520robustness%2520in%2520both%2520domain-shifted%2520and%2520fine-grained%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSMamba%3A%20Progressive%20Self-supervised%20Vision%20Mamba%20for%20Plant%20Disease%20Recognition&entry.906535625=Abdullah%20Al%20Mamun%20and%20Miaohua%20Zhang%20and%20David%20Ahmedt-Aristizabal%20and%20Zeeshan%20Hayder%20and%20Mohammad%20Awrangjeb&entry.1292438233=Self-supervised%20Learning%20%28SSL%29%20has%20become%20a%20powerful%20paradigm%20for%20representation%20learning%20without%20manual%20annotations.%20However%2C%20most%20existing%20frameworks%20focus%20on%20global%20alignment%20and%20struggle%20to%20capture%20the%20hierarchical%2C%20multi-scale%20lesion%20patterns%20characteristic%20of%20plant%20disease%20imagery.%20To%20address%20this%20gap%2C%20we%20propose%20PSMamba%2C%20a%20progressive%20self-supervised%20framework%20that%20integrates%20the%20efficient%20sequence%20modelling%20of%20Vision%20Mamba%20%28VM%29%20with%20a%20dual-student%20hierarchical%20distillation%20strategy.%20Unlike%20conventional%20single%20teacher-student%20designs%2C%20PSMamba%20employs%20a%20shared%20global%20teacher%20and%20two%20specialised%20students%3A%20one%20processes%20mid-scale%20views%20to%20capture%20lesion%20distributions%20and%20vein%20structures%2C%20while%20the%20other%20focuses%20on%20local%20views%20to%20capture%20fine-grained%20cues%20such%20as%20texture%20irregularities%20and%20early-stage%20lesions.%20This%20multi-granular%20supervision%20facilitates%20the%20joint%20learning%20of%20contextual%20and%20detailed%20representations%2C%20with%20consistency%20losses%20ensuring%20coherent%20cross-scale%20alignment.%20Experiments%20on%20three%20benchmark%20datasets%20show%20that%20PSMamba%20consistently%20outperforms%20state-of-the-art%20SSL%20methods%2C%20delivering%20superior%20accuracy%20and%20robustness%20in%20both%20domain-shifted%20and%20fine-grained%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.14309v1&entry.124074799=Read"},
{"title": "LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning", "author": "Jiaqi Wang and Binquan Ji and Haibo Luo and Yiyang Qi and Ruiting Li and Huiyan Wang and Yuantao Han and Cangyi Yang and jiaxu Zhang and Feiliang Ren", "abstract": "Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Framework--LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects.", "link": "http://arxiv.org/abs/2509.12875v3", "date": "2025-12-16", "relevancy": 2.5759, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LTA-thinker%3A%20Latent%20Thought-Augmented%20Training%20Framework%20for%20Large%20Language%20Models%20on%20Complex%20Reasoning&body=Title%3A%20LTA-thinker%3A%20Latent%20Thought-Augmented%20Training%20Framework%20for%20Large%20Language%20Models%20on%20Complex%20Reasoning%0AAuthor%3A%20Jiaqi%20Wang%20and%20Binquan%20Ji%20and%20Haibo%20Luo%20and%20Yiyang%20Qi%20and%20Ruiting%20Li%20and%20Huiyan%20Wang%20and%20Yuantao%20Han%20and%20Cangyi%20Yang%20and%20jiaxu%20Zhang%20and%20Feiliang%20Ren%0AAbstract%3A%20Complex%20Reasoning%20in%20Large%20Language%20Models%20can%20be%20dynamically%20optimized%20using%20Test-Time%20Scaling%20%28TTS%29%20to%20mitigate%20Overthinking.%20Methods%20such%20as%20Coconut%2C%20SoftCoT%20and%20its%20variant%20are%20effective%20in%20continuous%20latent%20space%20inference%2C%20the%20core%20bottleneck%20still%20lies%20in%20the%20efficient%20generation%20and%20utilization%20of%20high-quality%20Latent%20Thought.%20Drawing%20from%20the%20theory%20of%20SoftCoT%2B%2B%20that%20a%20larger%20variance%20in%20the%20generated%20Latent%20Thought%20distribution%20more%20closely%20approximates%20the%20golden%20truth%20distribution%2C%20we%20propose%20a%20Latent%20Thought-Augmented%20Training%20Framework--LTA-Thinker%2C%20which%20improves%20distributional%20variance%20and%20enhances%20reasoning%20performance%20from%20two%20perspectives.%20First%2C%20LTA-Thinker%20constructs%20a%20Latent%20Thought%20generation%20architecture%20based%20on%20a%20learnable%20prior.%20This%20architecture%20aims%20to%20increase%20the%20variance%20distribution%20of%20generated%20Latent%20Thought%20Vectors%20in%20order%20to%20simplify%20the%20overall%20structure%20and%20raise%20the%20performance%20ceiling.%20Second%2C%20LTA-Thinker%20introduces%20a%20distribution-based%20directional%20optimization%20paradigm%20that%20jointly%20constrains%20both%20distribution%20locality%20and%20distribution%20scale.%20This%20mechanism%20improves%20information%20efficiency%20and%20computational%20cost%20through%20a%20multi-objective%20co-training%20strategy%2C%20which%20combines%20standard%20Supervised%20Fine-Tuning%20%28SFT%29%20loss%20with%20two%20novel%20losses%3A%20Semantic%20Alignment%20Loss%2C%20which%20utilizes%20KL%20divergence%20to%20ensure%20that%20the%20Latent%20Thought%20is%20highly%20relevant%20to%20the%20semantics%20of%20the%20question%3B%20Reasoning%20Focus%20Loss%2C%20which%20utilizes%20a%20contrastive%20learning%20mechanism%20to%20guide%20the%20model%20to%20focus%20on%20the%20most%20critical%20reasoning%20steps.%20Experiments%20show%20that%20LTA-thinker%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20among%20various%20baselines%20and%20demonstrates%20a%20higher%20performance%20ceiling%20and%20better%20scaling%20effects.%0ALink%3A%20http%3A//arxiv.org/abs/2509.12875v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLTA-thinker%253A%2520Latent%2520Thought-Augmented%2520Training%2520Framework%2520for%2520Large%2520Language%2520Models%2520on%2520Complex%2520Reasoning%26entry.906535625%3DJiaqi%2520Wang%2520and%2520Binquan%2520Ji%2520and%2520Haibo%2520Luo%2520and%2520Yiyang%2520Qi%2520and%2520Ruiting%2520Li%2520and%2520Huiyan%2520Wang%2520and%2520Yuantao%2520Han%2520and%2520Cangyi%2520Yang%2520and%2520jiaxu%2520Zhang%2520and%2520Feiliang%2520Ren%26entry.1292438233%3DComplex%2520Reasoning%2520in%2520Large%2520Language%2520Models%2520can%2520be%2520dynamically%2520optimized%2520using%2520Test-Time%2520Scaling%2520%2528TTS%2529%2520to%2520mitigate%2520Overthinking.%2520Methods%2520such%2520as%2520Coconut%252C%2520SoftCoT%2520and%2520its%2520variant%2520are%2520effective%2520in%2520continuous%2520latent%2520space%2520inference%252C%2520the%2520core%2520bottleneck%2520still%2520lies%2520in%2520the%2520efficient%2520generation%2520and%2520utilization%2520of%2520high-quality%2520Latent%2520Thought.%2520Drawing%2520from%2520the%2520theory%2520of%2520SoftCoT%252B%252B%2520that%2520a%2520larger%2520variance%2520in%2520the%2520generated%2520Latent%2520Thought%2520distribution%2520more%2520closely%2520approximates%2520the%2520golden%2520truth%2520distribution%252C%2520we%2520propose%2520a%2520Latent%2520Thought-Augmented%2520Training%2520Framework--LTA-Thinker%252C%2520which%2520improves%2520distributional%2520variance%2520and%2520enhances%2520reasoning%2520performance%2520from%2520two%2520perspectives.%2520First%252C%2520LTA-Thinker%2520constructs%2520a%2520Latent%2520Thought%2520generation%2520architecture%2520based%2520on%2520a%2520learnable%2520prior.%2520This%2520architecture%2520aims%2520to%2520increase%2520the%2520variance%2520distribution%2520of%2520generated%2520Latent%2520Thought%2520Vectors%2520in%2520order%2520to%2520simplify%2520the%2520overall%2520structure%2520and%2520raise%2520the%2520performance%2520ceiling.%2520Second%252C%2520LTA-Thinker%2520introduces%2520a%2520distribution-based%2520directional%2520optimization%2520paradigm%2520that%2520jointly%2520constrains%2520both%2520distribution%2520locality%2520and%2520distribution%2520scale.%2520This%2520mechanism%2520improves%2520information%2520efficiency%2520and%2520computational%2520cost%2520through%2520a%2520multi-objective%2520co-training%2520strategy%252C%2520which%2520combines%2520standard%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520loss%2520with%2520two%2520novel%2520losses%253A%2520Semantic%2520Alignment%2520Loss%252C%2520which%2520utilizes%2520KL%2520divergence%2520to%2520ensure%2520that%2520the%2520Latent%2520Thought%2520is%2520highly%2520relevant%2520to%2520the%2520semantics%2520of%2520the%2520question%253B%2520Reasoning%2520Focus%2520Loss%252C%2520which%2520utilizes%2520a%2520contrastive%2520learning%2520mechanism%2520to%2520guide%2520the%2520model%2520to%2520focus%2520on%2520the%2520most%2520critical%2520reasoning%2520steps.%2520Experiments%2520show%2520that%2520LTA-thinker%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520among%2520various%2520baselines%2520and%2520demonstrates%2520a%2520higher%2520performance%2520ceiling%2520and%2520better%2520scaling%2520effects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12875v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTA-thinker%3A%20Latent%20Thought-Augmented%20Training%20Framework%20for%20Large%20Language%20Models%20on%20Complex%20Reasoning&entry.906535625=Jiaqi%20Wang%20and%20Binquan%20Ji%20and%20Haibo%20Luo%20and%20Yiyang%20Qi%20and%20Ruiting%20Li%20and%20Huiyan%20Wang%20and%20Yuantao%20Han%20and%20Cangyi%20Yang%20and%20jiaxu%20Zhang%20and%20Feiliang%20Ren&entry.1292438233=Complex%20Reasoning%20in%20Large%20Language%20Models%20can%20be%20dynamically%20optimized%20using%20Test-Time%20Scaling%20%28TTS%29%20to%20mitigate%20Overthinking.%20Methods%20such%20as%20Coconut%2C%20SoftCoT%20and%20its%20variant%20are%20effective%20in%20continuous%20latent%20space%20inference%2C%20the%20core%20bottleneck%20still%20lies%20in%20the%20efficient%20generation%20and%20utilization%20of%20high-quality%20Latent%20Thought.%20Drawing%20from%20the%20theory%20of%20SoftCoT%2B%2B%20that%20a%20larger%20variance%20in%20the%20generated%20Latent%20Thought%20distribution%20more%20closely%20approximates%20the%20golden%20truth%20distribution%2C%20we%20propose%20a%20Latent%20Thought-Augmented%20Training%20Framework--LTA-Thinker%2C%20which%20improves%20distributional%20variance%20and%20enhances%20reasoning%20performance%20from%20two%20perspectives.%20First%2C%20LTA-Thinker%20constructs%20a%20Latent%20Thought%20generation%20architecture%20based%20on%20a%20learnable%20prior.%20This%20architecture%20aims%20to%20increase%20the%20variance%20distribution%20of%20generated%20Latent%20Thought%20Vectors%20in%20order%20to%20simplify%20the%20overall%20structure%20and%20raise%20the%20performance%20ceiling.%20Second%2C%20LTA-Thinker%20introduces%20a%20distribution-based%20directional%20optimization%20paradigm%20that%20jointly%20constrains%20both%20distribution%20locality%20and%20distribution%20scale.%20This%20mechanism%20improves%20information%20efficiency%20and%20computational%20cost%20through%20a%20multi-objective%20co-training%20strategy%2C%20which%20combines%20standard%20Supervised%20Fine-Tuning%20%28SFT%29%20loss%20with%20two%20novel%20losses%3A%20Semantic%20Alignment%20Loss%2C%20which%20utilizes%20KL%20divergence%20to%20ensure%20that%20the%20Latent%20Thought%20is%20highly%20relevant%20to%20the%20semantics%20of%20the%20question%3B%20Reasoning%20Focus%20Loss%2C%20which%20utilizes%20a%20contrastive%20learning%20mechanism%20to%20guide%20the%20model%20to%20focus%20on%20the%20most%20critical%20reasoning%20steps.%20Experiments%20show%20that%20LTA-thinker%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20among%20various%20baselines%20and%20demonstrates%20a%20higher%20performance%20ceiling%20and%20better%20scaling%20effects.&entry.1838667208=http%3A//arxiv.org/abs/2509.12875v3&entry.124074799=Read"},
{"title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations", "author": "Aaron Kurda and Simon Steuernagel and Lukas Jung and Marcus Baum", "abstract": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .", "link": "http://arxiv.org/abs/2512.14428v1", "date": "2025-12-16", "relevancy": 2.5741, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5007}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Odyssey%3A%20An%20Automotive%20Lidar-Inertial%20Odometry%20Dataset%20for%20GNSS-denied%20situations&body=Title%3A%20Odyssey%3A%20An%20Automotive%20Lidar-Inertial%20Odometry%20Dataset%20for%20GNSS-denied%20situations%0AAuthor%3A%20Aaron%20Kurda%20and%20Simon%20Steuernagel%20and%20Lukas%20Jung%20and%20Marcus%20Baum%0AAbstract%3A%20The%20development%20and%20evaluation%20of%20Lidar-Inertial%20Odometry%20%28LIO%29%20and%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20systems%20requires%20a%20precise%20ground%20truth.%20The%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%20is%20often%20used%20as%20a%20foundation%20for%20this%2C%20but%20its%20signals%20can%20be%20unreliable%20in%20obstructed%20environments%20due%20to%20multi-path%20effects%20or%20loss-of-signal.%20While%20existing%20datasets%20compensate%20for%20the%20sporadic%20loss%20of%20GNSS%20signals%20by%20incorporating%20Inertial%20Measurement%20Unit%20%28IMU%29%20measurements%2C%20the%20commonly%20used%20Micro-Electro-Mechanical%20Systems%20%28MEMS%29%20or%20Fiber%20Optic%20Gyroscope%20%28FOG%29-based%20systems%20do%20not%20permit%20the%20prolonged%20study%20of%20GNSS-denied%20environments.%20To%20close%20this%20gap%2C%20we%20present%20Odyssey%2C%20a%20LIO%20dataset%20with%20a%20focus%20on%20GNSS-denied%20environments%20such%20as%20tunnels%20and%20parking%20garages%20as%20well%20as%20other%20underrepresented%2C%20yet%20ubiquitous%20situations%20such%20as%20stop-and-go-traffic%2C%20bumpy%20roads%20and%20wide%20open%20fields.%20Our%20ground%20truth%20is%20derived%20from%20a%20navigation-grade%20Inertial%20Navigation%20System%20%28INS%29%20equipped%20with%20a%20Ring%20Laser%20Gyroscope%20%28RLG%29%2C%20offering%20exceptional%20bias%20stability%20characteristics%20compared%20to%20IMUs%20used%20in%20existing%20datasets%20and%20enabling%20the%20prolonged%20and%20accurate%20study%20of%20GNSS-denied%20environments.%20This%20makes%20Odyssey%20the%20first%20publicly%20available%20dataset%20featuring%20a%20RLG-based%20INS.%20Besides%20providing%20data%20for%20LIO%2C%20we%20also%20support%20other%20tasks%2C%20such%20as%20place%20recognition%2C%20through%20the%20threefold%20repetition%20of%20all%20trajectories%20as%20well%20as%20the%20integration%20of%20external%20mapping%20data%20by%20providing%20precise%20geodetic%20coordinates.%20All%20data%2C%20dataloader%20and%20other%20material%20is%20available%20online%20at%20https%3A//odyssey.uni-goettingen.de/%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOdyssey%253A%2520An%2520Automotive%2520Lidar-Inertial%2520Odometry%2520Dataset%2520for%2520GNSS-denied%2520situations%26entry.906535625%3DAaron%2520Kurda%2520and%2520Simon%2520Steuernagel%2520and%2520Lukas%2520Jung%2520and%2520Marcus%2520Baum%26entry.1292438233%3DThe%2520development%2520and%2520evaluation%2520of%2520Lidar-Inertial%2520Odometry%2520%2528LIO%2529%2520and%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520systems%2520requires%2520a%2520precise%2520ground%2520truth.%2520The%2520Global%2520Navigation%2520Satellite%2520System%2520%2528GNSS%2529%2520is%2520often%2520used%2520as%2520a%2520foundation%2520for%2520this%252C%2520but%2520its%2520signals%2520can%2520be%2520unreliable%2520in%2520obstructed%2520environments%2520due%2520to%2520multi-path%2520effects%2520or%2520loss-of-signal.%2520While%2520existing%2520datasets%2520compensate%2520for%2520the%2520sporadic%2520loss%2520of%2520GNSS%2520signals%2520by%2520incorporating%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%2520measurements%252C%2520the%2520commonly%2520used%2520Micro-Electro-Mechanical%2520Systems%2520%2528MEMS%2529%2520or%2520Fiber%2520Optic%2520Gyroscope%2520%2528FOG%2529-based%2520systems%2520do%2520not%2520permit%2520the%2520prolonged%2520study%2520of%2520GNSS-denied%2520environments.%2520To%2520close%2520this%2520gap%252C%2520we%2520present%2520Odyssey%252C%2520a%2520LIO%2520dataset%2520with%2520a%2520focus%2520on%2520GNSS-denied%2520environments%2520such%2520as%2520tunnels%2520and%2520parking%2520garages%2520as%2520well%2520as%2520other%2520underrepresented%252C%2520yet%2520ubiquitous%2520situations%2520such%2520as%2520stop-and-go-traffic%252C%2520bumpy%2520roads%2520and%2520wide%2520open%2520fields.%2520Our%2520ground%2520truth%2520is%2520derived%2520from%2520a%2520navigation-grade%2520Inertial%2520Navigation%2520System%2520%2528INS%2529%2520equipped%2520with%2520a%2520Ring%2520Laser%2520Gyroscope%2520%2528RLG%2529%252C%2520offering%2520exceptional%2520bias%2520stability%2520characteristics%2520compared%2520to%2520IMUs%2520used%2520in%2520existing%2520datasets%2520and%2520enabling%2520the%2520prolonged%2520and%2520accurate%2520study%2520of%2520GNSS-denied%2520environments.%2520This%2520makes%2520Odyssey%2520the%2520first%2520publicly%2520available%2520dataset%2520featuring%2520a%2520RLG-based%2520INS.%2520Besides%2520providing%2520data%2520for%2520LIO%252C%2520we%2520also%2520support%2520other%2520tasks%252C%2520such%2520as%2520place%2520recognition%252C%2520through%2520the%2520threefold%2520repetition%2520of%2520all%2520trajectories%2520as%2520well%2520as%2520the%2520integration%2520of%2520external%2520mapping%2520data%2520by%2520providing%2520precise%2520geodetic%2520coordinates.%2520All%2520data%252C%2520dataloader%2520and%2520other%2520material%2520is%2520available%2520online%2520at%2520https%253A//odyssey.uni-goettingen.de/%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Odyssey%3A%20An%20Automotive%20Lidar-Inertial%20Odometry%20Dataset%20for%20GNSS-denied%20situations&entry.906535625=Aaron%20Kurda%20and%20Simon%20Steuernagel%20and%20Lukas%20Jung%20and%20Marcus%20Baum&entry.1292438233=The%20development%20and%20evaluation%20of%20Lidar-Inertial%20Odometry%20%28LIO%29%20and%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20systems%20requires%20a%20precise%20ground%20truth.%20The%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%20is%20often%20used%20as%20a%20foundation%20for%20this%2C%20but%20its%20signals%20can%20be%20unreliable%20in%20obstructed%20environments%20due%20to%20multi-path%20effects%20or%20loss-of-signal.%20While%20existing%20datasets%20compensate%20for%20the%20sporadic%20loss%20of%20GNSS%20signals%20by%20incorporating%20Inertial%20Measurement%20Unit%20%28IMU%29%20measurements%2C%20the%20commonly%20used%20Micro-Electro-Mechanical%20Systems%20%28MEMS%29%20or%20Fiber%20Optic%20Gyroscope%20%28FOG%29-based%20systems%20do%20not%20permit%20the%20prolonged%20study%20of%20GNSS-denied%20environments.%20To%20close%20this%20gap%2C%20we%20present%20Odyssey%2C%20a%20LIO%20dataset%20with%20a%20focus%20on%20GNSS-denied%20environments%20such%20as%20tunnels%20and%20parking%20garages%20as%20well%20as%20other%20underrepresented%2C%20yet%20ubiquitous%20situations%20such%20as%20stop-and-go-traffic%2C%20bumpy%20roads%20and%20wide%20open%20fields.%20Our%20ground%20truth%20is%20derived%20from%20a%20navigation-grade%20Inertial%20Navigation%20System%20%28INS%29%20equipped%20with%20a%20Ring%20Laser%20Gyroscope%20%28RLG%29%2C%20offering%20exceptional%20bias%20stability%20characteristics%20compared%20to%20IMUs%20used%20in%20existing%20datasets%20and%20enabling%20the%20prolonged%20and%20accurate%20study%20of%20GNSS-denied%20environments.%20This%20makes%20Odyssey%20the%20first%20publicly%20available%20dataset%20featuring%20a%20RLG-based%20INS.%20Besides%20providing%20data%20for%20LIO%2C%20we%20also%20support%20other%20tasks%2C%20such%20as%20place%20recognition%2C%20through%20the%20threefold%20repetition%20of%20all%20trajectories%20as%20well%20as%20the%20integration%20of%20external%20mapping%20data%20by%20providing%20precise%20geodetic%20coordinates.%20All%20data%2C%20dataloader%20and%20other%20material%20is%20available%20online%20at%20https%3A//odyssey.uni-goettingen.de/%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.14428v1&entry.124074799=Read"},
{"title": "Unified Semantic Transformer for 3D Scene Understanding", "author": "Sebastian Koch and Johanna Wald and Hide Matsuki and Pedro Hermosilla and Timo Ropinski and Federico Tombari", "abstract": "Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io", "link": "http://arxiv.org/abs/2512.14364v1", "date": "2025-12-16", "relevancy": 2.5548, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6422}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6422}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Semantic%20Transformer%20for%203D%20Scene%20Understanding&body=Title%3A%20Unified%20Semantic%20Transformer%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Sebastian%20Koch%20and%20Johanna%20Wald%20and%20Hide%20Matsuki%20and%20Pedro%20Hermosilla%20and%20Timo%20Ropinski%20and%20Federico%20Tombari%0AAbstract%3A%20Holistic%203D%20scene%20understanding%20involves%20capturing%20and%20parsing%20unstructured%203D%20environments.%20Due%20to%20the%20inherent%20complexity%20of%20the%20real%20world%2C%20existing%20models%20have%20predominantly%20been%20developed%20and%20limited%20to%20be%20task-specific.%20We%20introduce%20UNITE%2C%20a%20Unified%20Semantic%20Transformer%20for%203D%20scene%20understanding%2C%20a%20novel%20feed-forward%20neural%20network%20that%20unifies%20a%20diverse%20set%20of%203D%20semantic%20tasks%20within%20a%20single%20model.%20Our%20model%20operates%20on%20unseen%20scenes%20in%20a%20fully%20end-to-end%20manner%20and%20only%20takes%20a%20few%20seconds%20to%20infer%20the%20full%203D%20semantic%20geometry.%20Our%20approach%20is%20capable%20of%20directly%20predicting%20multiple%20semantic%20attributes%2C%20including%203D%20scene%20segmentation%2C%20instance%20embeddings%2C%20open-vocabulary%20features%2C%20as%20well%20as%20affordance%20and%20articulations%2C%20solely%20from%20RGB%20images.%20The%20method%20is%20trained%20using%20a%20combination%20of%202D%20distillation%2C%20heavily%20relying%20on%20self-supervision%20and%20leverages%20novel%20multi-view%20losses%20designed%20to%20ensure%203D%20view%20consistency.%20We%20demonstrate%20that%20UNITE%20achieves%20state-of-the-art%20performance%20on%20several%20different%20semantic%20tasks%20and%20even%20outperforms%20task-specific%20models%2C%20in%20many%20cases%2C%20surpassing%20methods%20that%20operate%20on%20ground%20truth%203D%20geometry.%20See%20the%20project%20website%20at%20unite-page.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2512.14364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Semantic%2520Transformer%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DSebastian%2520Koch%2520and%2520Johanna%2520Wald%2520and%2520Hide%2520Matsuki%2520and%2520Pedro%2520Hermosilla%2520and%2520Timo%2520Ropinski%2520and%2520Federico%2520Tombari%26entry.1292438233%3DHolistic%25203D%2520scene%2520understanding%2520involves%2520capturing%2520and%2520parsing%2520unstructured%25203D%2520environments.%2520Due%2520to%2520the%2520inherent%2520complexity%2520of%2520the%2520real%2520world%252C%2520existing%2520models%2520have%2520predominantly%2520been%2520developed%2520and%2520limited%2520to%2520be%2520task-specific.%2520We%2520introduce%2520UNITE%252C%2520a%2520Unified%2520Semantic%2520Transformer%2520for%25203D%2520scene%2520understanding%252C%2520a%2520novel%2520feed-forward%2520neural%2520network%2520that%2520unifies%2520a%2520diverse%2520set%2520of%25203D%2520semantic%2520tasks%2520within%2520a%2520single%2520model.%2520Our%2520model%2520operates%2520on%2520unseen%2520scenes%2520in%2520a%2520fully%2520end-to-end%2520manner%2520and%2520only%2520takes%2520a%2520few%2520seconds%2520to%2520infer%2520the%2520full%25203D%2520semantic%2520geometry.%2520Our%2520approach%2520is%2520capable%2520of%2520directly%2520predicting%2520multiple%2520semantic%2520attributes%252C%2520including%25203D%2520scene%2520segmentation%252C%2520instance%2520embeddings%252C%2520open-vocabulary%2520features%252C%2520as%2520well%2520as%2520affordance%2520and%2520articulations%252C%2520solely%2520from%2520RGB%2520images.%2520The%2520method%2520is%2520trained%2520using%2520a%2520combination%2520of%25202D%2520distillation%252C%2520heavily%2520relying%2520on%2520self-supervision%2520and%2520leverages%2520novel%2520multi-view%2520losses%2520designed%2520to%2520ensure%25203D%2520view%2520consistency.%2520We%2520demonstrate%2520that%2520UNITE%2520achieves%2520state-of-the-art%2520performance%2520on%2520several%2520different%2520semantic%2520tasks%2520and%2520even%2520outperforms%2520task-specific%2520models%252C%2520in%2520many%2520cases%252C%2520surpassing%2520methods%2520that%2520operate%2520on%2520ground%2520truth%25203D%2520geometry.%2520See%2520the%2520project%2520website%2520at%2520unite-page.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Semantic%20Transformer%20for%203D%20Scene%20Understanding&entry.906535625=Sebastian%20Koch%20and%20Johanna%20Wald%20and%20Hide%20Matsuki%20and%20Pedro%20Hermosilla%20and%20Timo%20Ropinski%20and%20Federico%20Tombari&entry.1292438233=Holistic%203D%20scene%20understanding%20involves%20capturing%20and%20parsing%20unstructured%203D%20environments.%20Due%20to%20the%20inherent%20complexity%20of%20the%20real%20world%2C%20existing%20models%20have%20predominantly%20been%20developed%20and%20limited%20to%20be%20task-specific.%20We%20introduce%20UNITE%2C%20a%20Unified%20Semantic%20Transformer%20for%203D%20scene%20understanding%2C%20a%20novel%20feed-forward%20neural%20network%20that%20unifies%20a%20diverse%20set%20of%203D%20semantic%20tasks%20within%20a%20single%20model.%20Our%20model%20operates%20on%20unseen%20scenes%20in%20a%20fully%20end-to-end%20manner%20and%20only%20takes%20a%20few%20seconds%20to%20infer%20the%20full%203D%20semantic%20geometry.%20Our%20approach%20is%20capable%20of%20directly%20predicting%20multiple%20semantic%20attributes%2C%20including%203D%20scene%20segmentation%2C%20instance%20embeddings%2C%20open-vocabulary%20features%2C%20as%20well%20as%20affordance%20and%20articulations%2C%20solely%20from%20RGB%20images.%20The%20method%20is%20trained%20using%20a%20combination%20of%202D%20distillation%2C%20heavily%20relying%20on%20self-supervision%20and%20leverages%20novel%20multi-view%20losses%20designed%20to%20ensure%203D%20view%20consistency.%20We%20demonstrate%20that%20UNITE%20achieves%20state-of-the-art%20performance%20on%20several%20different%20semantic%20tasks%20and%20even%20outperforms%20task-specific%20models%2C%20in%20many%20cases%2C%20surpassing%20methods%20that%20operate%20on%20ground%20truth%203D%20geometry.%20See%20the%20project%20website%20at%20unite-page.github.io&entry.1838667208=http%3A//arxiv.org/abs/2512.14364v1&entry.124074799=Read"},
{"title": "Spherical Leech Quantization for Visual Tokenization and Generation", "author": "Yue Zhao and Hanwen Jiang and Zhenlin Xu and Chutong Yang and Ehsan Adeli and Philipp Kr\u00e4henb\u00fchl", "abstract": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($\u039b_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.", "link": "http://arxiv.org/abs/2512.14697v1", "date": "2025-12-16", "relevancy": 2.5508, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spherical%20Leech%20Quantization%20for%20Visual%20Tokenization%20and%20Generation&body=Title%3A%20Spherical%20Leech%20Quantization%20for%20Visual%20Tokenization%20and%20Generation%0AAuthor%3A%20Yue%20Zhao%20and%20Hanwen%20Jiang%20and%20Zhenlin%20Xu%20and%20Chutong%20Yang%20and%20Ehsan%20Adeli%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%0AAbstract%3A%20Non-parametric%20quantization%20has%20received%20much%20attention%20due%20to%20its%20efficiency%20on%20parameters%20and%20scalability%20to%20a%20large%20codebook.%20In%20this%20paper%2C%20we%20present%20a%20unified%20formulation%20of%20different%20non-parametric%20quantization%20methods%20through%20the%20lens%20of%20lattice%20coding.%20The%20geometry%20of%20lattice%20codes%20explains%20the%20necessity%20of%20auxiliary%20loss%20terms%20when%20training%20auto-encoders%20with%20certain%20existing%20lookup-free%20quantization%20variants%20such%20as%20BSQ.%20As%20a%20step%20forward%2C%20we%20explore%20a%20few%20possible%20candidates%2C%20including%20random%20lattices%2C%20generalized%20Fibonacci%20lattices%2C%20and%20densest%20sphere%20packing%20lattices.%20Among%20all%2C%20we%20find%20the%20Leech%20lattice-based%20quantization%20method%2C%20which%20is%20dubbed%20as%20Spherical%20Leech%20Quantization%20%28%24%CE%9B_%7B24%7D%24-SQ%29%2C%20leads%20to%20both%20a%20simplified%20training%20recipe%20and%20an%20improved%20reconstruction-compression%20tradeoff%20thanks%20to%20its%20high%20symmetry%20and%20even%20distribution%20on%20the%20hypersphere.%20In%20image%20tokenization%20and%20compression%20tasks%2C%20this%20quantization%20approach%20achieves%20better%20reconstruction%20quality%20across%20all%20metrics%20than%20BSQ%2C%20the%20best%20prior%20art%2C%20while%20consuming%20slightly%20fewer%20bits.%20The%20improvement%20also%20extends%20to%20state-of-the-art%20auto-regressive%20image%20generation%20frameworks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpherical%2520Leech%2520Quantization%2520for%2520Visual%2520Tokenization%2520and%2520Generation%26entry.906535625%3DYue%2520Zhao%2520and%2520Hanwen%2520Jiang%2520and%2520Zhenlin%2520Xu%2520and%2520Chutong%2520Yang%2520and%2520Ehsan%2520Adeli%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%26entry.1292438233%3DNon-parametric%2520quantization%2520has%2520received%2520much%2520attention%2520due%2520to%2520its%2520efficiency%2520on%2520parameters%2520and%2520scalability%2520to%2520a%2520large%2520codebook.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520unified%2520formulation%2520of%2520different%2520non-parametric%2520quantization%2520methods%2520through%2520the%2520lens%2520of%2520lattice%2520coding.%2520The%2520geometry%2520of%2520lattice%2520codes%2520explains%2520the%2520necessity%2520of%2520auxiliary%2520loss%2520terms%2520when%2520training%2520auto-encoders%2520with%2520certain%2520existing%2520lookup-free%2520quantization%2520variants%2520such%2520as%2520BSQ.%2520As%2520a%2520step%2520forward%252C%2520we%2520explore%2520a%2520few%2520possible%2520candidates%252C%2520including%2520random%2520lattices%252C%2520generalized%2520Fibonacci%2520lattices%252C%2520and%2520densest%2520sphere%2520packing%2520lattices.%2520Among%2520all%252C%2520we%2520find%2520the%2520Leech%2520lattice-based%2520quantization%2520method%252C%2520which%2520is%2520dubbed%2520as%2520Spherical%2520Leech%2520Quantization%2520%2528%2524%25CE%259B_%257B24%257D%2524-SQ%2529%252C%2520leads%2520to%2520both%2520a%2520simplified%2520training%2520recipe%2520and%2520an%2520improved%2520reconstruction-compression%2520tradeoff%2520thanks%2520to%2520its%2520high%2520symmetry%2520and%2520even%2520distribution%2520on%2520the%2520hypersphere.%2520In%2520image%2520tokenization%2520and%2520compression%2520tasks%252C%2520this%2520quantization%2520approach%2520achieves%2520better%2520reconstruction%2520quality%2520across%2520all%2520metrics%2520than%2520BSQ%252C%2520the%2520best%2520prior%2520art%252C%2520while%2520consuming%2520slightly%2520fewer%2520bits.%2520The%2520improvement%2520also%2520extends%2520to%2520state-of-the-art%2520auto-regressive%2520image%2520generation%2520frameworks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spherical%20Leech%20Quantization%20for%20Visual%20Tokenization%20and%20Generation&entry.906535625=Yue%20Zhao%20and%20Hanwen%20Jiang%20and%20Zhenlin%20Xu%20and%20Chutong%20Yang%20and%20Ehsan%20Adeli%20and%20Philipp%20Kr%C3%A4henb%C3%BChl&entry.1292438233=Non-parametric%20quantization%20has%20received%20much%20attention%20due%20to%20its%20efficiency%20on%20parameters%20and%20scalability%20to%20a%20large%20codebook.%20In%20this%20paper%2C%20we%20present%20a%20unified%20formulation%20of%20different%20non-parametric%20quantization%20methods%20through%20the%20lens%20of%20lattice%20coding.%20The%20geometry%20of%20lattice%20codes%20explains%20the%20necessity%20of%20auxiliary%20loss%20terms%20when%20training%20auto-encoders%20with%20certain%20existing%20lookup-free%20quantization%20variants%20such%20as%20BSQ.%20As%20a%20step%20forward%2C%20we%20explore%20a%20few%20possible%20candidates%2C%20including%20random%20lattices%2C%20generalized%20Fibonacci%20lattices%2C%20and%20densest%20sphere%20packing%20lattices.%20Among%20all%2C%20we%20find%20the%20Leech%20lattice-based%20quantization%20method%2C%20which%20is%20dubbed%20as%20Spherical%20Leech%20Quantization%20%28%24%CE%9B_%7B24%7D%24-SQ%29%2C%20leads%20to%20both%20a%20simplified%20training%20recipe%20and%20an%20improved%20reconstruction-compression%20tradeoff%20thanks%20to%20its%20high%20symmetry%20and%20even%20distribution%20on%20the%20hypersphere.%20In%20image%20tokenization%20and%20compression%20tasks%2C%20this%20quantization%20approach%20achieves%20better%20reconstruction%20quality%20across%20all%20metrics%20than%20BSQ%2C%20the%20best%20prior%20art%2C%20while%20consuming%20slightly%20fewer%20bits.%20The%20improvement%20also%20extends%20to%20state-of-the-art%20auto-regressive%20image%20generation%20frameworks.&entry.1838667208=http%3A//arxiv.org/abs/2512.14697v1&entry.124074799=Read"},
{"title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives", "author": "Zihan Wang and Jiashun Wang and Jeff Tan and Yiwen Zhao and Jessica Hodgins and Shubham Tulsiani and Deva Ramanan", "abstract": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.", "link": "http://arxiv.org/abs/2512.14696v1", "date": "2025-12-16", "relevancy": 2.5497, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.672}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.621}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRISP%3A%20Contact-Guided%20Real2Sim%20from%20Monocular%20Video%20with%20Planar%20Scene%20Primitives&body=Title%3A%20CRISP%3A%20Contact-Guided%20Real2Sim%20from%20Monocular%20Video%20with%20Planar%20Scene%20Primitives%0AAuthor%3A%20Zihan%20Wang%20and%20Jiashun%20Wang%20and%20Jeff%20Tan%20and%20Yiwen%20Zhao%20and%20Jessica%20Hodgins%20and%20Shubham%20Tulsiani%20and%20Deva%20Ramanan%0AAbstract%3A%20We%20introduce%20CRISP%2C%20a%20method%20that%20recovers%20simulatable%20human%20motion%20and%20scene%20geometry%20from%20monocular%20video.%20Prior%20work%20on%20joint%20human-scene%20reconstruction%20relies%20on%20data-driven%20priors%20and%20joint%20optimization%20with%20no%20physics%20in%20the%20loop%2C%20or%20recovers%20noisy%20geometry%20with%20artifacts%20that%20cause%20motion%20tracking%20policies%20with%20scene%20interactions%20to%20fail.%20In%20contrast%2C%20our%20key%20insight%20is%20to%20recover%20convex%2C%20clean%2C%20and%20simulation-ready%20geometry%20by%20fitting%20planar%20primitives%20to%20a%20point%20cloud%20reconstruction%20of%20the%20scene%2C%20via%20a%20simple%20clustering%20pipeline%20over%20depth%2C%20normals%2C%20and%20flow.%20To%20reconstruct%20scene%20geometry%20that%20might%20be%20occluded%20during%20interactions%2C%20we%20make%20use%20of%20human-scene%20contact%20modeling%20%28e.g.%2C%20we%20use%20human%20posture%20to%20reconstruct%20the%20occluded%20seat%20of%20a%20chair%29.%20Finally%2C%20we%20ensure%20that%20human%20and%20scene%20reconstructions%20are%20physically-plausible%20by%20using%20them%20to%20drive%20a%20humanoid%20controller%20via%20reinforcement%20learning.%20Our%20approach%20reduces%20motion%20tracking%20failure%20rates%20from%2055.2%5C%25%20to%206.9%5C%25%20on%20human-centric%20video%20benchmarks%20%28EMDB%2C%20PROX%29%2C%20while%20delivering%20a%2043%5C%25%20faster%20RL%20simulation%20throughput.%20We%20further%20validate%20it%20on%20in-the-wild%20videos%20including%20casually-captured%20videos%2C%20Internet%20videos%2C%20and%20even%20Sora-generated%20videos.%20This%20demonstrates%20CRISP%27s%20ability%20to%20generate%20physically-valid%20human%20motion%20and%20interaction%20environments%20at%20scale%2C%20greatly%20advancing%20real-to-sim%20applications%20for%20robotics%20and%20AR/VR.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRISP%253A%2520Contact-Guided%2520Real2Sim%2520from%2520Monocular%2520Video%2520with%2520Planar%2520Scene%2520Primitives%26entry.906535625%3DZihan%2520Wang%2520and%2520Jiashun%2520Wang%2520and%2520Jeff%2520Tan%2520and%2520Yiwen%2520Zhao%2520and%2520Jessica%2520Hodgins%2520and%2520Shubham%2520Tulsiani%2520and%2520Deva%2520Ramanan%26entry.1292438233%3DWe%2520introduce%2520CRISP%252C%2520a%2520method%2520that%2520recovers%2520simulatable%2520human%2520motion%2520and%2520scene%2520geometry%2520from%2520monocular%2520video.%2520Prior%2520work%2520on%2520joint%2520human-scene%2520reconstruction%2520relies%2520on%2520data-driven%2520priors%2520and%2520joint%2520optimization%2520with%2520no%2520physics%2520in%2520the%2520loop%252C%2520or%2520recovers%2520noisy%2520geometry%2520with%2520artifacts%2520that%2520cause%2520motion%2520tracking%2520policies%2520with%2520scene%2520interactions%2520to%2520fail.%2520In%2520contrast%252C%2520our%2520key%2520insight%2520is%2520to%2520recover%2520convex%252C%2520clean%252C%2520and%2520simulation-ready%2520geometry%2520by%2520fitting%2520planar%2520primitives%2520to%2520a%2520point%2520cloud%2520reconstruction%2520of%2520the%2520scene%252C%2520via%2520a%2520simple%2520clustering%2520pipeline%2520over%2520depth%252C%2520normals%252C%2520and%2520flow.%2520To%2520reconstruct%2520scene%2520geometry%2520that%2520might%2520be%2520occluded%2520during%2520interactions%252C%2520we%2520make%2520use%2520of%2520human-scene%2520contact%2520modeling%2520%2528e.g.%252C%2520we%2520use%2520human%2520posture%2520to%2520reconstruct%2520the%2520occluded%2520seat%2520of%2520a%2520chair%2529.%2520Finally%252C%2520we%2520ensure%2520that%2520human%2520and%2520scene%2520reconstructions%2520are%2520physically-plausible%2520by%2520using%2520them%2520to%2520drive%2520a%2520humanoid%2520controller%2520via%2520reinforcement%2520learning.%2520Our%2520approach%2520reduces%2520motion%2520tracking%2520failure%2520rates%2520from%252055.2%255C%2525%2520to%25206.9%255C%2525%2520on%2520human-centric%2520video%2520benchmarks%2520%2528EMDB%252C%2520PROX%2529%252C%2520while%2520delivering%2520a%252043%255C%2525%2520faster%2520RL%2520simulation%2520throughput.%2520We%2520further%2520validate%2520it%2520on%2520in-the-wild%2520videos%2520including%2520casually-captured%2520videos%252C%2520Internet%2520videos%252C%2520and%2520even%2520Sora-generated%2520videos.%2520This%2520demonstrates%2520CRISP%2527s%2520ability%2520to%2520generate%2520physically-valid%2520human%2520motion%2520and%2520interaction%2520environments%2520at%2520scale%252C%2520greatly%2520advancing%2520real-to-sim%2520applications%2520for%2520robotics%2520and%2520AR/VR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRISP%3A%20Contact-Guided%20Real2Sim%20from%20Monocular%20Video%20with%20Planar%20Scene%20Primitives&entry.906535625=Zihan%20Wang%20and%20Jiashun%20Wang%20and%20Jeff%20Tan%20and%20Yiwen%20Zhao%20and%20Jessica%20Hodgins%20and%20Shubham%20Tulsiani%20and%20Deva%20Ramanan&entry.1292438233=We%20introduce%20CRISP%2C%20a%20method%20that%20recovers%20simulatable%20human%20motion%20and%20scene%20geometry%20from%20monocular%20video.%20Prior%20work%20on%20joint%20human-scene%20reconstruction%20relies%20on%20data-driven%20priors%20and%20joint%20optimization%20with%20no%20physics%20in%20the%20loop%2C%20or%20recovers%20noisy%20geometry%20with%20artifacts%20that%20cause%20motion%20tracking%20policies%20with%20scene%20interactions%20to%20fail.%20In%20contrast%2C%20our%20key%20insight%20is%20to%20recover%20convex%2C%20clean%2C%20and%20simulation-ready%20geometry%20by%20fitting%20planar%20primitives%20to%20a%20point%20cloud%20reconstruction%20of%20the%20scene%2C%20via%20a%20simple%20clustering%20pipeline%20over%20depth%2C%20normals%2C%20and%20flow.%20To%20reconstruct%20scene%20geometry%20that%20might%20be%20occluded%20during%20interactions%2C%20we%20make%20use%20of%20human-scene%20contact%20modeling%20%28e.g.%2C%20we%20use%20human%20posture%20to%20reconstruct%20the%20occluded%20seat%20of%20a%20chair%29.%20Finally%2C%20we%20ensure%20that%20human%20and%20scene%20reconstructions%20are%20physically-plausible%20by%20using%20them%20to%20drive%20a%20humanoid%20controller%20via%20reinforcement%20learning.%20Our%20approach%20reduces%20motion%20tracking%20failure%20rates%20from%2055.2%5C%25%20to%206.9%5C%25%20on%20human-centric%20video%20benchmarks%20%28EMDB%2C%20PROX%29%2C%20while%20delivering%20a%2043%5C%25%20faster%20RL%20simulation%20throughput.%20We%20further%20validate%20it%20on%20in-the-wild%20videos%20including%20casually-captured%20videos%2C%20Internet%20videos%2C%20and%20even%20Sora-generated%20videos.%20This%20demonstrates%20CRISP%27s%20ability%20to%20generate%20physically-valid%20human%20motion%20and%20interaction%20environments%20at%20scale%2C%20greatly%20advancing%20real-to-sim%20applications%20for%20robotics%20and%20AR/VR.&entry.1838667208=http%3A//arxiv.org/abs/2512.14696v1&entry.124074799=Read"},
{"title": "DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models", "author": "Ruofan Zhang and Bin Xia and Zhen Cheng and Cairen Jian and Minglun Yang and Ngai Wong and Yuan Cheng", "abstract": "Adaptive reasoning is essential for aligning the computational effort of large language models (LLMs) with the intrinsic difficulty of problems. Current chain-of-thought methods boost reasoning ability but indiscriminately generate long explanations, leading to evident inefficiency. However, existing reinforcement learning approaches to adaptive thinking remain unstable and heavily reward-dependent. Here we propose \\textbf{DART}, a supervised \\textbf{D}ifficulty-\\textbf{A}daptive \\textbf{R}easoning \\textbf{T}runcation framework that adjusts thinking length according to problem difficulty. By distilling concise reasoning patterns from stronger models, interpolating them into a continuum of reasoning styles, and curating optimal training data that balances correctness and compactness, DART learns when to ``stop thinking''. Across multiple mathematical benchmarks, experimental results demonstrate its remarkable efficiency while preserving or improving accuracy, achieving a significant 81.2\\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K dataset) with 5.33$\\times$ computational acceleration. DART provides a stable and general paradigm for efficient reasoning, advancing the development of adaptive intelligence in LLMs.", "link": "http://arxiv.org/abs/2511.01170v2", "date": "2025-12-16", "relevancy": 2.5339, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5233}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DART%3A%20Difficulty-Adaptive%20Reasoning%20Truncation%20for%20Efficient%20Large%20Language%20Models&body=Title%3A%20DART%3A%20Difficulty-Adaptive%20Reasoning%20Truncation%20for%20Efficient%20Large%20Language%20Models%0AAuthor%3A%20Ruofan%20Zhang%20and%20Bin%20Xia%20and%20Zhen%20Cheng%20and%20Cairen%20Jian%20and%20Minglun%20Yang%20and%20Ngai%20Wong%20and%20Yuan%20Cheng%0AAbstract%3A%20Adaptive%20reasoning%20is%20essential%20for%20aligning%20the%20computational%20effort%20of%20large%20language%20models%20%28LLMs%29%20with%20the%20intrinsic%20difficulty%20of%20problems.%20Current%20chain-of-thought%20methods%20boost%20reasoning%20ability%20but%20indiscriminately%20generate%20long%20explanations%2C%20leading%20to%20evident%20inefficiency.%20However%2C%20existing%20reinforcement%20learning%20approaches%20to%20adaptive%20thinking%20remain%20unstable%20and%20heavily%20reward-dependent.%20Here%20we%20propose%20%5Ctextbf%7BDART%7D%2C%20a%20supervised%20%5Ctextbf%7BD%7Difficulty-%5Ctextbf%7BA%7Ddaptive%20%5Ctextbf%7BR%7Deasoning%20%5Ctextbf%7BT%7Druncation%20framework%20that%20adjusts%20thinking%20length%20according%20to%20problem%20difficulty.%20By%20distilling%20concise%20reasoning%20patterns%20from%20stronger%20models%2C%20interpolating%20them%20into%20a%20continuum%20of%20reasoning%20styles%2C%20and%20curating%20optimal%20training%20data%20that%20balances%20correctness%20and%20compactness%2C%20DART%20learns%20when%20to%20%60%60stop%20thinking%27%27.%20Across%20multiple%20mathematical%20benchmarks%2C%20experimental%20results%20demonstrate%20its%20remarkable%20efficiency%20while%20preserving%20or%20improving%20accuracy%2C%20achieving%20a%20significant%2081.2%5C%25%20reasoning%20truncation%20%28DeepSeek-R1-Distill-Qwen-7B%20on%20GSM8K%20dataset%29%20with%205.33%24%5Ctimes%24%20computational%20acceleration.%20DART%20provides%20a%20stable%20and%20general%20paradigm%20for%20efficient%20reasoning%2C%20advancing%20the%20development%20of%20adaptive%20intelligence%20in%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.01170v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDART%253A%2520Difficulty-Adaptive%2520Reasoning%2520Truncation%2520for%2520Efficient%2520Large%2520Language%2520Models%26entry.906535625%3DRuofan%2520Zhang%2520and%2520Bin%2520Xia%2520and%2520Zhen%2520Cheng%2520and%2520Cairen%2520Jian%2520and%2520Minglun%2520Yang%2520and%2520Ngai%2520Wong%2520and%2520Yuan%2520Cheng%26entry.1292438233%3DAdaptive%2520reasoning%2520is%2520essential%2520for%2520aligning%2520the%2520computational%2520effort%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520the%2520intrinsic%2520difficulty%2520of%2520problems.%2520Current%2520chain-of-thought%2520methods%2520boost%2520reasoning%2520ability%2520but%2520indiscriminately%2520generate%2520long%2520explanations%252C%2520leading%2520to%2520evident%2520inefficiency.%2520However%252C%2520existing%2520reinforcement%2520learning%2520approaches%2520to%2520adaptive%2520thinking%2520remain%2520unstable%2520and%2520heavily%2520reward-dependent.%2520Here%2520we%2520propose%2520%255Ctextbf%257BDART%257D%252C%2520a%2520supervised%2520%255Ctextbf%257BD%257Difficulty-%255Ctextbf%257BA%257Ddaptive%2520%255Ctextbf%257BR%257Deasoning%2520%255Ctextbf%257BT%257Druncation%2520framework%2520that%2520adjusts%2520thinking%2520length%2520according%2520to%2520problem%2520difficulty.%2520By%2520distilling%2520concise%2520reasoning%2520patterns%2520from%2520stronger%2520models%252C%2520interpolating%2520them%2520into%2520a%2520continuum%2520of%2520reasoning%2520styles%252C%2520and%2520curating%2520optimal%2520training%2520data%2520that%2520balances%2520correctness%2520and%2520compactness%252C%2520DART%2520learns%2520when%2520to%2520%2560%2560stop%2520thinking%2527%2527.%2520Across%2520multiple%2520mathematical%2520benchmarks%252C%2520experimental%2520results%2520demonstrate%2520its%2520remarkable%2520efficiency%2520while%2520preserving%2520or%2520improving%2520accuracy%252C%2520achieving%2520a%2520significant%252081.2%255C%2525%2520reasoning%2520truncation%2520%2528DeepSeek-R1-Distill-Qwen-7B%2520on%2520GSM8K%2520dataset%2529%2520with%25205.33%2524%255Ctimes%2524%2520computational%2520acceleration.%2520DART%2520provides%2520a%2520stable%2520and%2520general%2520paradigm%2520for%2520efficient%2520reasoning%252C%2520advancing%2520the%2520development%2520of%2520adaptive%2520intelligence%2520in%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.01170v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DART%3A%20Difficulty-Adaptive%20Reasoning%20Truncation%20for%20Efficient%20Large%20Language%20Models&entry.906535625=Ruofan%20Zhang%20and%20Bin%20Xia%20and%20Zhen%20Cheng%20and%20Cairen%20Jian%20and%20Minglun%20Yang%20and%20Ngai%20Wong%20and%20Yuan%20Cheng&entry.1292438233=Adaptive%20reasoning%20is%20essential%20for%20aligning%20the%20computational%20effort%20of%20large%20language%20models%20%28LLMs%29%20with%20the%20intrinsic%20difficulty%20of%20problems.%20Current%20chain-of-thought%20methods%20boost%20reasoning%20ability%20but%20indiscriminately%20generate%20long%20explanations%2C%20leading%20to%20evident%20inefficiency.%20However%2C%20existing%20reinforcement%20learning%20approaches%20to%20adaptive%20thinking%20remain%20unstable%20and%20heavily%20reward-dependent.%20Here%20we%20propose%20%5Ctextbf%7BDART%7D%2C%20a%20supervised%20%5Ctextbf%7BD%7Difficulty-%5Ctextbf%7BA%7Ddaptive%20%5Ctextbf%7BR%7Deasoning%20%5Ctextbf%7BT%7Druncation%20framework%20that%20adjusts%20thinking%20length%20according%20to%20problem%20difficulty.%20By%20distilling%20concise%20reasoning%20patterns%20from%20stronger%20models%2C%20interpolating%20them%20into%20a%20continuum%20of%20reasoning%20styles%2C%20and%20curating%20optimal%20training%20data%20that%20balances%20correctness%20and%20compactness%2C%20DART%20learns%20when%20to%20%60%60stop%20thinking%27%27.%20Across%20multiple%20mathematical%20benchmarks%2C%20experimental%20results%20demonstrate%20its%20remarkable%20efficiency%20while%20preserving%20or%20improving%20accuracy%2C%20achieving%20a%20significant%2081.2%5C%25%20reasoning%20truncation%20%28DeepSeek-R1-Distill-Qwen-7B%20on%20GSM8K%20dataset%29%20with%205.33%24%5Ctimes%24%20computational%20acceleration.%20DART%20provides%20a%20stable%20and%20general%20paradigm%20for%20efficient%20reasoning%2C%20advancing%20the%20development%20of%20adaptive%20intelligence%20in%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2511.01170v2&entry.124074799=Read"},
{"title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure", "author": "Jooyeol Yun and Jaegul Choo", "abstract": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.", "link": "http://arxiv.org/abs/2512.14336v1", "date": "2025-12-16", "relevancy": 2.5287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vector%20Prism%3A%20Animating%20Vector%20Graphics%20by%20Stratifying%20Semantic%20Structure&body=Title%3A%20Vector%20Prism%3A%20Animating%20Vector%20Graphics%20by%20Stratifying%20Semantic%20Structure%0AAuthor%3A%20Jooyeol%20Yun%20and%20Jaegul%20Choo%0AAbstract%3A%20Scalable%20Vector%20Graphics%20%28SVG%29%20are%20central%20to%20modern%20web%20design%2C%20and%20the%20demand%20to%20animate%20them%20continues%20to%20grow%20as%20web%20environments%20become%20increasingly%20dynamic.%20Yet%20automating%20the%20animation%20of%20vector%20graphics%20remains%20challenging%20for%20vision-language%20models%20%28VLMs%29%20despite%20recent%20progress%20in%20code%20generation%20and%20motion%20planning.%20VLMs%20routinely%20mis-handle%20SVGs%2C%20since%20visually%20coherent%20parts%20are%20often%20fragmented%20into%20low-level%20shapes%20that%20offer%20little%20guidance%20of%20which%20elements%20should%20move%20together.%20In%20this%20paper%2C%20we%20introduce%20a%20framework%20that%20recovers%20the%20semantic%20structure%20required%20for%20reliable%20SVG%20animation%20and%20reveals%20the%20missing%20layer%20that%20current%20VLM%20systems%20overlook.%20This%20is%20achieved%20through%20a%20statistical%20aggregation%20of%20multiple%20weak%20part%20predictions%2C%20allowing%20the%20system%20to%20stably%20infer%20semantics%20from%20noisy%20predictions.%20By%20reorganizing%20SVGs%20into%20semantic%20groups%2C%20our%20approach%20enables%20VLMs%20to%20produce%20animations%20with%20far%20greater%20coherence.%20Our%20experiments%20demonstrate%20substantial%20gains%20over%20existing%20approaches%2C%20suggesting%20that%20semantic%20recovery%20is%20the%20key%20step%20that%20unlocks%20robust%20SVG%20animation%20and%20supports%20more%20interpretable%20interactions%20between%20VLMs%20and%20vector%20graphics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVector%2520Prism%253A%2520Animating%2520Vector%2520Graphics%2520by%2520Stratifying%2520Semantic%2520Structure%26entry.906535625%3DJooyeol%2520Yun%2520and%2520Jaegul%2520Choo%26entry.1292438233%3DScalable%2520Vector%2520Graphics%2520%2528SVG%2529%2520are%2520central%2520to%2520modern%2520web%2520design%252C%2520and%2520the%2520demand%2520to%2520animate%2520them%2520continues%2520to%2520grow%2520as%2520web%2520environments%2520become%2520increasingly%2520dynamic.%2520Yet%2520automating%2520the%2520animation%2520of%2520vector%2520graphics%2520remains%2520challenging%2520for%2520vision-language%2520models%2520%2528VLMs%2529%2520despite%2520recent%2520progress%2520in%2520code%2520generation%2520and%2520motion%2520planning.%2520VLMs%2520routinely%2520mis-handle%2520SVGs%252C%2520since%2520visually%2520coherent%2520parts%2520are%2520often%2520fragmented%2520into%2520low-level%2520shapes%2520that%2520offer%2520little%2520guidance%2520of%2520which%2520elements%2520should%2520move%2520together.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520framework%2520that%2520recovers%2520the%2520semantic%2520structure%2520required%2520for%2520reliable%2520SVG%2520animation%2520and%2520reveals%2520the%2520missing%2520layer%2520that%2520current%2520VLM%2520systems%2520overlook.%2520This%2520is%2520achieved%2520through%2520a%2520statistical%2520aggregation%2520of%2520multiple%2520weak%2520part%2520predictions%252C%2520allowing%2520the%2520system%2520to%2520stably%2520infer%2520semantics%2520from%2520noisy%2520predictions.%2520By%2520reorganizing%2520SVGs%2520into%2520semantic%2520groups%252C%2520our%2520approach%2520enables%2520VLMs%2520to%2520produce%2520animations%2520with%2520far%2520greater%2520coherence.%2520Our%2520experiments%2520demonstrate%2520substantial%2520gains%2520over%2520existing%2520approaches%252C%2520suggesting%2520that%2520semantic%2520recovery%2520is%2520the%2520key%2520step%2520that%2520unlocks%2520robust%2520SVG%2520animation%2520and%2520supports%2520more%2520interpretable%2520interactions%2520between%2520VLMs%2520and%2520vector%2520graphics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vector%20Prism%3A%20Animating%20Vector%20Graphics%20by%20Stratifying%20Semantic%20Structure&entry.906535625=Jooyeol%20Yun%20and%20Jaegul%20Choo&entry.1292438233=Scalable%20Vector%20Graphics%20%28SVG%29%20are%20central%20to%20modern%20web%20design%2C%20and%20the%20demand%20to%20animate%20them%20continues%20to%20grow%20as%20web%20environments%20become%20increasingly%20dynamic.%20Yet%20automating%20the%20animation%20of%20vector%20graphics%20remains%20challenging%20for%20vision-language%20models%20%28VLMs%29%20despite%20recent%20progress%20in%20code%20generation%20and%20motion%20planning.%20VLMs%20routinely%20mis-handle%20SVGs%2C%20since%20visually%20coherent%20parts%20are%20often%20fragmented%20into%20low-level%20shapes%20that%20offer%20little%20guidance%20of%20which%20elements%20should%20move%20together.%20In%20this%20paper%2C%20we%20introduce%20a%20framework%20that%20recovers%20the%20semantic%20structure%20required%20for%20reliable%20SVG%20animation%20and%20reveals%20the%20missing%20layer%20that%20current%20VLM%20systems%20overlook.%20This%20is%20achieved%20through%20a%20statistical%20aggregation%20of%20multiple%20weak%20part%20predictions%2C%20allowing%20the%20system%20to%20stably%20infer%20semantics%20from%20noisy%20predictions.%20By%20reorganizing%20SVGs%20into%20semantic%20groups%2C%20our%20approach%20enables%20VLMs%20to%20produce%20animations%20with%20far%20greater%20coherence.%20Our%20experiments%20demonstrate%20substantial%20gains%20over%20existing%20approaches%2C%20suggesting%20that%20semantic%20recovery%20is%20the%20key%20step%20that%20unlocks%20robust%20SVG%20animation%20and%20supports%20more%20interpretable%20interactions%20between%20VLMs%20and%20vector%20graphics.&entry.1838667208=http%3A//arxiv.org/abs/2512.14336v1&entry.124074799=Read"},
{"title": "Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs", "author": "Youngkyu Lee and Francesc Levrero Florencio and Jay Pathak and George Em Karniadakis", "abstract": "The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.", "link": "http://arxiv.org/abs/2512.14596v1", "date": "2025-12-16", "relevancy": 2.5133, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5258}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4955}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Iterative%20Solvers%20with%20Geometry-Aware%20Neural%20Preconditioners%20for%20Parametric%20PDEs&body=Title%3A%20Hybrid%20Iterative%20Solvers%20with%20Geometry-Aware%20Neural%20Preconditioners%20for%20Parametric%20PDEs%0AAuthor%3A%20Youngkyu%20Lee%20and%20Francesc%20Levrero%20Florencio%20and%20Jay%20Pathak%20and%20George%20Em%20Karniadakis%0AAbstract%3A%20The%20convergence%20behavior%20of%20classical%20iterative%20solvers%20for%20parametric%20partial%20differential%20equations%20%28PDEs%29%20is%20often%20highly%20sensitive%20to%20the%20domain%20and%20specific%20discretization%20of%20PDEs.%20Previously%2C%20we%20introduced%20hybrid%20solvers%20by%20combining%20the%20classical%20solvers%20with%20neural%20operators%20for%20a%20specific%20geometry%201%2C%20but%20they%20tend%20to%20under-perform%20in%20geometries%20not%20encountered%20during%20training.%20To%20address%20this%20challenge%2C%20we%20introduce%20Geo-DeepONet%2C%20a%20geometry-aware%20deep%20operator%20network%20that%20incorporates%20domain%20information%20extracted%20from%20finite%20element%20discretizations.%20Geo-DeepONet%20enables%20accurate%20operator%20learning%20across%20arbitrary%20unstructured%20meshes%20without%20requiring%20retraining.%20Building%20on%20this%2C%20we%20develop%20a%20class%20of%20geometry-aware%20hybrid%20preconditioned%20iterative%20solvers%20by%20coupling%20Geo-DeepONet%20with%20traditional%20methods%20such%20as%20relaxation%20schemes%20and%20Krylov%20subspace%20algorithms.%20Through%20numerical%20experiments%20on%20parametric%20PDEs%20posed%20over%20diverse%20unstructured%20domains%2C%20we%20demonstrate%20the%20enhanced%20robustness%20and%20efficiency%20of%20the%20proposed%20hybrid%20solvers%20for%20multiple%20real-world%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Iterative%2520Solvers%2520with%2520Geometry-Aware%2520Neural%2520Preconditioners%2520for%2520Parametric%2520PDEs%26entry.906535625%3DYoungkyu%2520Lee%2520and%2520Francesc%2520Levrero%2520Florencio%2520and%2520Jay%2520Pathak%2520and%2520George%2520Em%2520Karniadakis%26entry.1292438233%3DThe%2520convergence%2520behavior%2520of%2520classical%2520iterative%2520solvers%2520for%2520parametric%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520is%2520often%2520highly%2520sensitive%2520to%2520the%2520domain%2520and%2520specific%2520discretization%2520of%2520PDEs.%2520Previously%252C%2520we%2520introduced%2520hybrid%2520solvers%2520by%2520combining%2520the%2520classical%2520solvers%2520with%2520neural%2520operators%2520for%2520a%2520specific%2520geometry%25201%252C%2520but%2520they%2520tend%2520to%2520under-perform%2520in%2520geometries%2520not%2520encountered%2520during%2520training.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520Geo-DeepONet%252C%2520a%2520geometry-aware%2520deep%2520operator%2520network%2520that%2520incorporates%2520domain%2520information%2520extracted%2520from%2520finite%2520element%2520discretizations.%2520Geo-DeepONet%2520enables%2520accurate%2520operator%2520learning%2520across%2520arbitrary%2520unstructured%2520meshes%2520without%2520requiring%2520retraining.%2520Building%2520on%2520this%252C%2520we%2520develop%2520a%2520class%2520of%2520geometry-aware%2520hybrid%2520preconditioned%2520iterative%2520solvers%2520by%2520coupling%2520Geo-DeepONet%2520with%2520traditional%2520methods%2520such%2520as%2520relaxation%2520schemes%2520and%2520Krylov%2520subspace%2520algorithms.%2520Through%2520numerical%2520experiments%2520on%2520parametric%2520PDEs%2520posed%2520over%2520diverse%2520unstructured%2520domains%252C%2520we%2520demonstrate%2520the%2520enhanced%2520robustness%2520and%2520efficiency%2520of%2520the%2520proposed%2520hybrid%2520solvers%2520for%2520multiple%2520real-world%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Iterative%20Solvers%20with%20Geometry-Aware%20Neural%20Preconditioners%20for%20Parametric%20PDEs&entry.906535625=Youngkyu%20Lee%20and%20Francesc%20Levrero%20Florencio%20and%20Jay%20Pathak%20and%20George%20Em%20Karniadakis&entry.1292438233=The%20convergence%20behavior%20of%20classical%20iterative%20solvers%20for%20parametric%20partial%20differential%20equations%20%28PDEs%29%20is%20often%20highly%20sensitive%20to%20the%20domain%20and%20specific%20discretization%20of%20PDEs.%20Previously%2C%20we%20introduced%20hybrid%20solvers%20by%20combining%20the%20classical%20solvers%20with%20neural%20operators%20for%20a%20specific%20geometry%201%2C%20but%20they%20tend%20to%20under-perform%20in%20geometries%20not%20encountered%20during%20training.%20To%20address%20this%20challenge%2C%20we%20introduce%20Geo-DeepONet%2C%20a%20geometry-aware%20deep%20operator%20network%20that%20incorporates%20domain%20information%20extracted%20from%20finite%20element%20discretizations.%20Geo-DeepONet%20enables%20accurate%20operator%20learning%20across%20arbitrary%20unstructured%20meshes%20without%20requiring%20retraining.%20Building%20on%20this%2C%20we%20develop%20a%20class%20of%20geometry-aware%20hybrid%20preconditioned%20iterative%20solvers%20by%20coupling%20Geo-DeepONet%20with%20traditional%20methods%20such%20as%20relaxation%20schemes%20and%20Krylov%20subspace%20algorithms.%20Through%20numerical%20experiments%20on%20parametric%20PDEs%20posed%20over%20diverse%20unstructured%20domains%2C%20we%20demonstrate%20the%20enhanced%20robustness%20and%20efficiency%20of%20the%20proposed%20hybrid%20solvers%20for%20multiple%20real-world%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.14596v1&entry.124074799=Read"},
{"title": "Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection", "author": "Tejaswani Dash and Gautam Datla and Anudeep Vurity and Tazeem Ahmad and Mohd Adnan and Saima Rafi and Saisha Patro and Saina Patro", "abstract": "Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.", "link": "http://arxiv.org/abs/2512.14563v1", "date": "2025-12-16", "relevancy": 2.5118, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5215}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4953}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20GRU%2BMHSA%3A%20A%20Lightweight%20Hybrid%20Recurrent%20Attention%20Model%20for%20Cardiovascular%20Disease%20Detection&body=Title%3A%20Residual%20GRU%2BMHSA%3A%20A%20Lightweight%20Hybrid%20Recurrent%20Attention%20Model%20for%20Cardiovascular%20Disease%20Detection%0AAuthor%3A%20Tejaswani%20Dash%20and%20Gautam%20Datla%20and%20Anudeep%20Vurity%20and%20Tazeem%20Ahmad%20and%20Mohd%20Adnan%20and%20Saima%20Rafi%20and%20Saisha%20Patro%20and%20Saina%20Patro%0AAbstract%3A%20Cardiovascular%20disease%20%28CVD%29%20remains%20the%20leading%20cause%20of%20mortality%20worldwide%2C%20underscoring%20the%20need%20for%20reliable%20and%20efficient%20predictive%20tools%20that%20support%20early%20intervention.%20Traditional%20diagnostic%20approaches%20rely%20on%20handcrafted%20features%20and%20clinician%20expertise%2C%20while%20machine%20learning%20methods%20improve%20reproducibility%20but%20often%20struggle%20to%20generalize%20across%20noisy%20and%20heterogeneous%20clinical%20data.%20In%20this%20work%2C%20we%20propose%20Residual%20GRU%20with%20Multi-Head%20Self-Attention%2C%20a%20compact%20deep%20learning%20architecture%20designed%20for%20tabular%20clinical%20records.%20The%20model%20integrates%20residual%20bidirectional%20gated%20recurrent%20units%20for%20sequential%20modeling%20of%20feature%20columns%2C%20a%20channel%20reweighting%20block%2C%20and%20multi-head%20self-attention%20pooling%20with%20a%20learnable%20classification%20token%20to%20capture%20global%20context.%20We%20evaluate%20the%20model%20on%20the%20UCI%20Heart%20Disease%20dataset%20using%205-fold%20stratified%20cross-validation%20and%20compare%20it%20against%20classical%20methods%20such%20as%20Logistic%20Regression%2C%20Random%20Forest%2C%20and%20Support%20Vector%20Machines%2C%20as%20well%20as%20modern%20deep%20learning%20baselines%20including%20DeepMLP%2C%20convolutional%20networks%2C%20recurrent%20networks%2C%20and%20Transformers.%20The%20proposed%20model%20achieves%20an%20accuracy%20of%200.861%2C%20macro-F1%20of%200.860%2C%20ROC-AUC%20of%200.908%2C%20and%20PR-AUC%20of%200.904%2C%20outperforming%20all%20baselines.%20Ablation%20studies%20confirm%20the%20individual%20contributions%20of%20residual%20recurrence%2C%20channel%20gating%2C%20and%20attention%20pooling.%20t-SNE%20visualizations%20further%20indicate%20that%20the%20learned%20embeddings%20exhibit%20clearer%20separation%20between%20disease%20and%20non-disease%20classes%20compared%20to%20raw%20features.%20These%20results%20demonstrate%20that%20lightweight%20hybrid%20recurrent%20and%20attention-based%20architectures%20provide%20a%20strong%20balance%20between%20accuracy%20and%20efficiency%20for%20clinical%20risk%20prediction%2C%20supporting%20deployment%20in%20resource-constrained%20healthcare%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520GRU%252BMHSA%253A%2520A%2520Lightweight%2520Hybrid%2520Recurrent%2520Attention%2520Model%2520for%2520Cardiovascular%2520Disease%2520Detection%26entry.906535625%3DTejaswani%2520Dash%2520and%2520Gautam%2520Datla%2520and%2520Anudeep%2520Vurity%2520and%2520Tazeem%2520Ahmad%2520and%2520Mohd%2520Adnan%2520and%2520Saima%2520Rafi%2520and%2520Saisha%2520Patro%2520and%2520Saina%2520Patro%26entry.1292438233%3DCardiovascular%2520disease%2520%2528CVD%2529%2520remains%2520the%2520leading%2520cause%2520of%2520mortality%2520worldwide%252C%2520underscoring%2520the%2520need%2520for%2520reliable%2520and%2520efficient%2520predictive%2520tools%2520that%2520support%2520early%2520intervention.%2520Traditional%2520diagnostic%2520approaches%2520rely%2520on%2520handcrafted%2520features%2520and%2520clinician%2520expertise%252C%2520while%2520machine%2520learning%2520methods%2520improve%2520reproducibility%2520but%2520often%2520struggle%2520to%2520generalize%2520across%2520noisy%2520and%2520heterogeneous%2520clinical%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520Residual%2520GRU%2520with%2520Multi-Head%2520Self-Attention%252C%2520a%2520compact%2520deep%2520learning%2520architecture%2520designed%2520for%2520tabular%2520clinical%2520records.%2520The%2520model%2520integrates%2520residual%2520bidirectional%2520gated%2520recurrent%2520units%2520for%2520sequential%2520modeling%2520of%2520feature%2520columns%252C%2520a%2520channel%2520reweighting%2520block%252C%2520and%2520multi-head%2520self-attention%2520pooling%2520with%2520a%2520learnable%2520classification%2520token%2520to%2520capture%2520global%2520context.%2520We%2520evaluate%2520the%2520model%2520on%2520the%2520UCI%2520Heart%2520Disease%2520dataset%2520using%25205-fold%2520stratified%2520cross-validation%2520and%2520compare%2520it%2520against%2520classical%2520methods%2520such%2520as%2520Logistic%2520Regression%252C%2520Random%2520Forest%252C%2520and%2520Support%2520Vector%2520Machines%252C%2520as%2520well%2520as%2520modern%2520deep%2520learning%2520baselines%2520including%2520DeepMLP%252C%2520convolutional%2520networks%252C%2520recurrent%2520networks%252C%2520and%2520Transformers.%2520The%2520proposed%2520model%2520achieves%2520an%2520accuracy%2520of%25200.861%252C%2520macro-F1%2520of%25200.860%252C%2520ROC-AUC%2520of%25200.908%252C%2520and%2520PR-AUC%2520of%25200.904%252C%2520outperforming%2520all%2520baselines.%2520Ablation%2520studies%2520confirm%2520the%2520individual%2520contributions%2520of%2520residual%2520recurrence%252C%2520channel%2520gating%252C%2520and%2520attention%2520pooling.%2520t-SNE%2520visualizations%2520further%2520indicate%2520that%2520the%2520learned%2520embeddings%2520exhibit%2520clearer%2520separation%2520between%2520disease%2520and%2520non-disease%2520classes%2520compared%2520to%2520raw%2520features.%2520These%2520results%2520demonstrate%2520that%2520lightweight%2520hybrid%2520recurrent%2520and%2520attention-based%2520architectures%2520provide%2520a%2520strong%2520balance%2520between%2520accuracy%2520and%2520efficiency%2520for%2520clinical%2520risk%2520prediction%252C%2520supporting%2520deployment%2520in%2520resource-constrained%2520healthcare%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20GRU%2BMHSA%3A%20A%20Lightweight%20Hybrid%20Recurrent%20Attention%20Model%20for%20Cardiovascular%20Disease%20Detection&entry.906535625=Tejaswani%20Dash%20and%20Gautam%20Datla%20and%20Anudeep%20Vurity%20and%20Tazeem%20Ahmad%20and%20Mohd%20Adnan%20and%20Saima%20Rafi%20and%20Saisha%20Patro%20and%20Saina%20Patro&entry.1292438233=Cardiovascular%20disease%20%28CVD%29%20remains%20the%20leading%20cause%20of%20mortality%20worldwide%2C%20underscoring%20the%20need%20for%20reliable%20and%20efficient%20predictive%20tools%20that%20support%20early%20intervention.%20Traditional%20diagnostic%20approaches%20rely%20on%20handcrafted%20features%20and%20clinician%20expertise%2C%20while%20machine%20learning%20methods%20improve%20reproducibility%20but%20often%20struggle%20to%20generalize%20across%20noisy%20and%20heterogeneous%20clinical%20data.%20In%20this%20work%2C%20we%20propose%20Residual%20GRU%20with%20Multi-Head%20Self-Attention%2C%20a%20compact%20deep%20learning%20architecture%20designed%20for%20tabular%20clinical%20records.%20The%20model%20integrates%20residual%20bidirectional%20gated%20recurrent%20units%20for%20sequential%20modeling%20of%20feature%20columns%2C%20a%20channel%20reweighting%20block%2C%20and%20multi-head%20self-attention%20pooling%20with%20a%20learnable%20classification%20token%20to%20capture%20global%20context.%20We%20evaluate%20the%20model%20on%20the%20UCI%20Heart%20Disease%20dataset%20using%205-fold%20stratified%20cross-validation%20and%20compare%20it%20against%20classical%20methods%20such%20as%20Logistic%20Regression%2C%20Random%20Forest%2C%20and%20Support%20Vector%20Machines%2C%20as%20well%20as%20modern%20deep%20learning%20baselines%20including%20DeepMLP%2C%20convolutional%20networks%2C%20recurrent%20networks%2C%20and%20Transformers.%20The%20proposed%20model%20achieves%20an%20accuracy%20of%200.861%2C%20macro-F1%20of%200.860%2C%20ROC-AUC%20of%200.908%2C%20and%20PR-AUC%20of%200.904%2C%20outperforming%20all%20baselines.%20Ablation%20studies%20confirm%20the%20individual%20contributions%20of%20residual%20recurrence%2C%20channel%20gating%2C%20and%20attention%20pooling.%20t-SNE%20visualizations%20further%20indicate%20that%20the%20learned%20embeddings%20exhibit%20clearer%20separation%20between%20disease%20and%20non-disease%20classes%20compared%20to%20raw%20features.%20These%20results%20demonstrate%20that%20lightweight%20hybrid%20recurrent%20and%20attention-based%20architectures%20provide%20a%20strong%20balance%20between%20accuracy%20and%20efficiency%20for%20clinical%20risk%20prediction%2C%20supporting%20deployment%20in%20resource-constrained%20healthcare%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.14563v1&entry.124074799=Read"},
{"title": "Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees", "author": "Jason Lunder", "abstract": "In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.", "link": "http://arxiv.org/abs/2512.00204v2", "date": "2025-12-16", "relevancy": 2.5116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree%20Matching%20Networks%20for%20Natural%20Language%20Inference%3A%20Parameter-Efficient%20Semantic%20Understanding%20via%20Dependency%20Parse%20Trees&body=Title%3A%20Tree%20Matching%20Networks%20for%20Natural%20Language%20Inference%3A%20Parameter-Efficient%20Semantic%20Understanding%20via%20Dependency%20Parse%20Trees%0AAuthor%3A%20Jason%20Lunder%0AAbstract%3A%20In%20creating%20sentence%20embeddings%20for%20Natural%20Language%20Inference%20%28NLI%29%20tasks%2C%20using%20transformer-based%20models%20like%20BERT%20leads%20to%20high%20accuracy%2C%20but%20require%20hundreds%20of%20millions%20of%20parameters.%20These%20models%20take%20in%20sentences%20as%20a%20sequence%20of%20tokens%2C%20and%20learn%20to%20encode%20the%20meaning%20of%20the%20sequence%20into%20embeddings%20such%20that%20those%20embeddings%20can%20be%20used%20reliably%20for%20NLI%20tasks.%20Essentially%2C%20every%20word%20is%20considered%20against%20every%20other%20word%20in%20the%20sequence%2C%20and%20the%20transformer%20model%20is%20able%20to%20determine%20the%20relationships%20between%20them%2C%20entirely%20from%20scratch.%20However%2C%20a%20model%20that%20accepts%20explicit%20linguistic%20structures%20like%20dependency%20parse%20trees%20may%20be%20able%20to%20leverage%20prior%20encoded%20information%20about%20these%20relationships%2C%20without%20having%20to%20learn%20them%20from%20scratch%2C%20thus%20improving%20learning%20efficiency.%20To%20investigate%20this%2C%20we%20adapt%20Graph%20Matching%20Networks%20%28GMN%29%20to%20operate%20on%20dependency%20parse%20trees%2C%20creating%20Tree%20Matching%20Networks%20%28TMN%29.%20We%20compare%20TMN%20to%20a%20BERT%20based%20model%20on%20the%20SNLI%20entailment%20task%20and%20on%20the%20SemEval%20similarity%20task.%20TMN%20is%20able%20to%20achieve%20significantly%20better%20results%20with%20a%20significantly%20reduced%20memory%20footprint%20and%20much%20less%20training%20time%20than%20the%20BERT%20based%20model%20on%20the%20SNLI%20task%2C%20while%20both%20models%20struggled%20to%20preform%20well%20on%20the%20SemEval.%20Explicit%20structural%20representations%20significantly%20outperform%20sequence-based%20models%20at%20comparable%20scales%2C%20but%20current%20aggregation%20methods%20limit%20scalability.%20We%20propose%20multi-headed%20attention%20aggregation%20to%20address%20this%20limitation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree%2520Matching%2520Networks%2520for%2520Natural%2520Language%2520Inference%253A%2520Parameter-Efficient%2520Semantic%2520Understanding%2520via%2520Dependency%2520Parse%2520Trees%26entry.906535625%3DJason%2520Lunder%26entry.1292438233%3DIn%2520creating%2520sentence%2520embeddings%2520for%2520Natural%2520Language%2520Inference%2520%2528NLI%2529%2520tasks%252C%2520using%2520transformer-based%2520models%2520like%2520BERT%2520leads%2520to%2520high%2520accuracy%252C%2520but%2520require%2520hundreds%2520of%2520millions%2520of%2520parameters.%2520These%2520models%2520take%2520in%2520sentences%2520as%2520a%2520sequence%2520of%2520tokens%252C%2520and%2520learn%2520to%2520encode%2520the%2520meaning%2520of%2520the%2520sequence%2520into%2520embeddings%2520such%2520that%2520those%2520embeddings%2520can%2520be%2520used%2520reliably%2520for%2520NLI%2520tasks.%2520Essentially%252C%2520every%2520word%2520is%2520considered%2520against%2520every%2520other%2520word%2520in%2520the%2520sequence%252C%2520and%2520the%2520transformer%2520model%2520is%2520able%2520to%2520determine%2520the%2520relationships%2520between%2520them%252C%2520entirely%2520from%2520scratch.%2520However%252C%2520a%2520model%2520that%2520accepts%2520explicit%2520linguistic%2520structures%2520like%2520dependency%2520parse%2520trees%2520may%2520be%2520able%2520to%2520leverage%2520prior%2520encoded%2520information%2520about%2520these%2520relationships%252C%2520without%2520having%2520to%2520learn%2520them%2520from%2520scratch%252C%2520thus%2520improving%2520learning%2520efficiency.%2520To%2520investigate%2520this%252C%2520we%2520adapt%2520Graph%2520Matching%2520Networks%2520%2528GMN%2529%2520to%2520operate%2520on%2520dependency%2520parse%2520trees%252C%2520creating%2520Tree%2520Matching%2520Networks%2520%2528TMN%2529.%2520We%2520compare%2520TMN%2520to%2520a%2520BERT%2520based%2520model%2520on%2520the%2520SNLI%2520entailment%2520task%2520and%2520on%2520the%2520SemEval%2520similarity%2520task.%2520TMN%2520is%2520able%2520to%2520achieve%2520significantly%2520better%2520results%2520with%2520a%2520significantly%2520reduced%2520memory%2520footprint%2520and%2520much%2520less%2520training%2520time%2520than%2520the%2520BERT%2520based%2520model%2520on%2520the%2520SNLI%2520task%252C%2520while%2520both%2520models%2520struggled%2520to%2520preform%2520well%2520on%2520the%2520SemEval.%2520Explicit%2520structural%2520representations%2520significantly%2520outperform%2520sequence-based%2520models%2520at%2520comparable%2520scales%252C%2520but%2520current%2520aggregation%2520methods%2520limit%2520scalability.%2520We%2520propose%2520multi-headed%2520attention%2520aggregation%2520to%2520address%2520this%2520limitation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree%20Matching%20Networks%20for%20Natural%20Language%20Inference%3A%20Parameter-Efficient%20Semantic%20Understanding%20via%20Dependency%20Parse%20Trees&entry.906535625=Jason%20Lunder&entry.1292438233=In%20creating%20sentence%20embeddings%20for%20Natural%20Language%20Inference%20%28NLI%29%20tasks%2C%20using%20transformer-based%20models%20like%20BERT%20leads%20to%20high%20accuracy%2C%20but%20require%20hundreds%20of%20millions%20of%20parameters.%20These%20models%20take%20in%20sentences%20as%20a%20sequence%20of%20tokens%2C%20and%20learn%20to%20encode%20the%20meaning%20of%20the%20sequence%20into%20embeddings%20such%20that%20those%20embeddings%20can%20be%20used%20reliably%20for%20NLI%20tasks.%20Essentially%2C%20every%20word%20is%20considered%20against%20every%20other%20word%20in%20the%20sequence%2C%20and%20the%20transformer%20model%20is%20able%20to%20determine%20the%20relationships%20between%20them%2C%20entirely%20from%20scratch.%20However%2C%20a%20model%20that%20accepts%20explicit%20linguistic%20structures%20like%20dependency%20parse%20trees%20may%20be%20able%20to%20leverage%20prior%20encoded%20information%20about%20these%20relationships%2C%20without%20having%20to%20learn%20them%20from%20scratch%2C%20thus%20improving%20learning%20efficiency.%20To%20investigate%20this%2C%20we%20adapt%20Graph%20Matching%20Networks%20%28GMN%29%20to%20operate%20on%20dependency%20parse%20trees%2C%20creating%20Tree%20Matching%20Networks%20%28TMN%29.%20We%20compare%20TMN%20to%20a%20BERT%20based%20model%20on%20the%20SNLI%20entailment%20task%20and%20on%20the%20SemEval%20similarity%20task.%20TMN%20is%20able%20to%20achieve%20significantly%20better%20results%20with%20a%20significantly%20reduced%20memory%20footprint%20and%20much%20less%20training%20time%20than%20the%20BERT%20based%20model%20on%20the%20SNLI%20task%2C%20while%20both%20models%20struggled%20to%20preform%20well%20on%20the%20SemEval.%20Explicit%20structural%20representations%20significantly%20outperform%20sequence-based%20models%20at%20comparable%20scales%2C%20but%20current%20aggregation%20methods%20limit%20scalability.%20We%20propose%20multi-headed%20attention%20aggregation%20to%20address%20this%20limitation.&entry.1838667208=http%3A//arxiv.org/abs/2512.00204v2&entry.124074799=Read"},
{"title": "Scalable Formal Verification via Autoencoder Latent Space Abstraction", "author": "Robert Reed and Luca Laurenti and Morteza Lahijanian", "abstract": "Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.", "link": "http://arxiv.org/abs/2512.13593v2", "date": "2025-12-16", "relevancy": 2.5055, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Formal%20Verification%20via%20Autoencoder%20Latent%20Space%20Abstraction&body=Title%3A%20Scalable%20Formal%20Verification%20via%20Autoencoder%20Latent%20Space%20Abstraction%0AAuthor%3A%20Robert%20Reed%20and%20Luca%20Laurenti%20and%20Morteza%20Lahijanian%0AAbstract%3A%20Finite%20Abstraction%20methods%20provide%20a%20powerful%20formal%20framework%20for%20proving%20that%20systems%20satisfy%20their%20specifications.%20However%2C%20these%20techniques%20face%20scalability%20challenges%20for%20high-dimensional%20systems%2C%20as%20they%20rely%20on%20state-space%20discretization%20which%20grows%20exponentially%20with%20dimension.%20Learning-based%20approaches%20to%20dimensionality%20reduction%2C%20utilizing%20neural%20networks%20and%20autoencoders%2C%20have%20shown%20great%20potential%20to%20alleviate%20this%20problem.%20However%2C%20ensuring%20the%20correctness%20of%20the%20resulting%20verification%20results%20remains%20an%20open%20question.%20In%20this%20work%2C%20we%20provide%20a%20formal%20approach%20to%20reduce%20the%20dimensionality%20of%20systems%20via%20convex%20autoencoders%20and%20learn%20the%20dynamics%20in%20the%20latent%20space%20through%20a%20kernel-based%20method.%20We%20then%20construct%20a%20finite%20abstraction%20from%20the%20learned%20model%20in%20the%20latent%20space%20and%20guarantee%20that%20the%20abstraction%20contains%20the%20true%20behaviors%20of%20the%20original%20system.%20We%20show%20that%20the%20verification%20results%20in%20the%20latent%20space%20can%20be%20mapped%20back%20to%20the%20original%20system.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20multiple%20systems%2C%20including%20a%2026D%20system%20controlled%20by%20a%20neural%20network%2C%20showing%20significant%20scalability%20improvements%20without%20loss%20of%20rigor.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Formal%2520Verification%2520via%2520Autoencoder%2520Latent%2520Space%2520Abstraction%26entry.906535625%3DRobert%2520Reed%2520and%2520Luca%2520Laurenti%2520and%2520Morteza%2520Lahijanian%26entry.1292438233%3DFinite%2520Abstraction%2520methods%2520provide%2520a%2520powerful%2520formal%2520framework%2520for%2520proving%2520that%2520systems%2520satisfy%2520their%2520specifications.%2520However%252C%2520these%2520techniques%2520face%2520scalability%2520challenges%2520for%2520high-dimensional%2520systems%252C%2520as%2520they%2520rely%2520on%2520state-space%2520discretization%2520which%2520grows%2520exponentially%2520with%2520dimension.%2520Learning-based%2520approaches%2520to%2520dimensionality%2520reduction%252C%2520utilizing%2520neural%2520networks%2520and%2520autoencoders%252C%2520have%2520shown%2520great%2520potential%2520to%2520alleviate%2520this%2520problem.%2520However%252C%2520ensuring%2520the%2520correctness%2520of%2520the%2520resulting%2520verification%2520results%2520remains%2520an%2520open%2520question.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%2520formal%2520approach%2520to%2520reduce%2520the%2520dimensionality%2520of%2520systems%2520via%2520convex%2520autoencoders%2520and%2520learn%2520the%2520dynamics%2520in%2520the%2520latent%2520space%2520through%2520a%2520kernel-based%2520method.%2520We%2520then%2520construct%2520a%2520finite%2520abstraction%2520from%2520the%2520learned%2520model%2520in%2520the%2520latent%2520space%2520and%2520guarantee%2520that%2520the%2520abstraction%2520contains%2520the%2520true%2520behaviors%2520of%2520the%2520original%2520system.%2520We%2520show%2520that%2520the%2520verification%2520results%2520in%2520the%2520latent%2520space%2520can%2520be%2520mapped%2520back%2520to%2520the%2520original%2520system.%2520Finally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520multiple%2520systems%252C%2520including%2520a%252026D%2520system%2520controlled%2520by%2520a%2520neural%2520network%252C%2520showing%2520significant%2520scalability%2520improvements%2520without%2520loss%2520of%2520rigor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Formal%20Verification%20via%20Autoencoder%20Latent%20Space%20Abstraction&entry.906535625=Robert%20Reed%20and%20Luca%20Laurenti%20and%20Morteza%20Lahijanian&entry.1292438233=Finite%20Abstraction%20methods%20provide%20a%20powerful%20formal%20framework%20for%20proving%20that%20systems%20satisfy%20their%20specifications.%20However%2C%20these%20techniques%20face%20scalability%20challenges%20for%20high-dimensional%20systems%2C%20as%20they%20rely%20on%20state-space%20discretization%20which%20grows%20exponentially%20with%20dimension.%20Learning-based%20approaches%20to%20dimensionality%20reduction%2C%20utilizing%20neural%20networks%20and%20autoencoders%2C%20have%20shown%20great%20potential%20to%20alleviate%20this%20problem.%20However%2C%20ensuring%20the%20correctness%20of%20the%20resulting%20verification%20results%20remains%20an%20open%20question.%20In%20this%20work%2C%20we%20provide%20a%20formal%20approach%20to%20reduce%20the%20dimensionality%20of%20systems%20via%20convex%20autoencoders%20and%20learn%20the%20dynamics%20in%20the%20latent%20space%20through%20a%20kernel-based%20method.%20We%20then%20construct%20a%20finite%20abstraction%20from%20the%20learned%20model%20in%20the%20latent%20space%20and%20guarantee%20that%20the%20abstraction%20contains%20the%20true%20behaviors%20of%20the%20original%20system.%20We%20show%20that%20the%20verification%20results%20in%20the%20latent%20space%20can%20be%20mapped%20back%20to%20the%20original%20system.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20multiple%20systems%2C%20including%20a%2026D%20system%20controlled%20by%20a%20neural%20network%2C%20showing%20significant%20scalability%20improvements%20without%20loss%20of%20rigor.&entry.1838667208=http%3A//arxiv.org/abs/2512.13593v2&entry.124074799=Read"},
{"title": "Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models", "author": "Gabriele Prato and Shagun Sodhani and Alessandro Sordoni and Sarath Chandar", "abstract": "The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.", "link": "http://arxiv.org/abs/2512.14427v1", "date": "2025-12-16", "relevancy": 2.5048, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effect%20of%20Document%20Packing%20on%20the%20Latent%20Multi-Hop%20Reasoning%20Capabilities%20of%20Large%20Language%20Models&body=Title%3A%20Effect%20of%20Document%20Packing%20on%20the%20Latent%20Multi-Hop%20Reasoning%20Capabilities%20of%20Large%20Language%20Models%0AAuthor%3A%20Gabriele%20Prato%20and%20Shagun%20Sodhani%20and%20Alessandro%20Sordoni%20and%20Sarath%20Chandar%0AAbstract%3A%20The%20standard%20practice%20for%20training%20large%20language%20models%20involves%20packing%20multiple%20documents%20together%20to%20optimize%20computational%20efficiency.%20However%2C%20the%20impact%20of%20this%20process%20on%20the%20models%27%20capabilities%20remains%20largely%20unexplored.%20To%20address%20this%20gap%2C%20we%20investigate%20how%20different%20document-packing%20strategies%20influence%20the%20latent%20multi-hop%20reasoning%20abilities%20of%20LLMs.%20Our%20findings%20indicate%20that%20packing%20can%20improve%20model%20performance%20compared%20to%20training%20on%20individual%20documents%2C%20at%20the%20expense%20of%20more%20compute.%20To%20further%20understand%20the%20underlying%20mechanisms%2C%20we%20conduct%20an%20ablation%20study%2C%20identifying%20key%20factors%20that%20explain%20the%20advantages%20of%20packing.%20Ultimately%2C%20our%20research%20deepens%20the%20understanding%20of%20LLM%20training%20dynamics%20and%20provides%20practical%20insights%20for%20optimizing%20model%20development.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffect%2520of%2520Document%2520Packing%2520on%2520the%2520Latent%2520Multi-Hop%2520Reasoning%2520Capabilities%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DGabriele%2520Prato%2520and%2520Shagun%2520Sodhani%2520and%2520Alessandro%2520Sordoni%2520and%2520Sarath%2520Chandar%26entry.1292438233%3DThe%2520standard%2520practice%2520for%2520training%2520large%2520language%2520models%2520involves%2520packing%2520multiple%2520documents%2520together%2520to%2520optimize%2520computational%2520efficiency.%2520However%252C%2520the%2520impact%2520of%2520this%2520process%2520on%2520the%2520models%2527%2520capabilities%2520remains%2520largely%2520unexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520investigate%2520how%2520different%2520document-packing%2520strategies%2520influence%2520the%2520latent%2520multi-hop%2520reasoning%2520abilities%2520of%2520LLMs.%2520Our%2520findings%2520indicate%2520that%2520packing%2520can%2520improve%2520model%2520performance%2520compared%2520to%2520training%2520on%2520individual%2520documents%252C%2520at%2520the%2520expense%2520of%2520more%2520compute.%2520To%2520further%2520understand%2520the%2520underlying%2520mechanisms%252C%2520we%2520conduct%2520an%2520ablation%2520study%252C%2520identifying%2520key%2520factors%2520that%2520explain%2520the%2520advantages%2520of%2520packing.%2520Ultimately%252C%2520our%2520research%2520deepens%2520the%2520understanding%2520of%2520LLM%2520training%2520dynamics%2520and%2520provides%2520practical%2520insights%2520for%2520optimizing%2520model%2520development.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effect%20of%20Document%20Packing%20on%20the%20Latent%20Multi-Hop%20Reasoning%20Capabilities%20of%20Large%20Language%20Models&entry.906535625=Gabriele%20Prato%20and%20Shagun%20Sodhani%20and%20Alessandro%20Sordoni%20and%20Sarath%20Chandar&entry.1292438233=The%20standard%20practice%20for%20training%20large%20language%20models%20involves%20packing%20multiple%20documents%20together%20to%20optimize%20computational%20efficiency.%20However%2C%20the%20impact%20of%20this%20process%20on%20the%20models%27%20capabilities%20remains%20largely%20unexplored.%20To%20address%20this%20gap%2C%20we%20investigate%20how%20different%20document-packing%20strategies%20influence%20the%20latent%20multi-hop%20reasoning%20abilities%20of%20LLMs.%20Our%20findings%20indicate%20that%20packing%20can%20improve%20model%20performance%20compared%20to%20training%20on%20individual%20documents%2C%20at%20the%20expense%20of%20more%20compute.%20To%20further%20understand%20the%20underlying%20mechanisms%2C%20we%20conduct%20an%20ablation%20study%2C%20identifying%20key%20factors%20that%20explain%20the%20advantages%20of%20packing.%20Ultimately%2C%20our%20research%20deepens%20the%20understanding%20of%20LLM%20training%20dynamics%20and%20provides%20practical%20insights%20for%20optimizing%20model%20development.&entry.1838667208=http%3A//arxiv.org/abs/2512.14427v1&entry.124074799=Read"},
{"title": "Enhancing Visual Sentiment Analysis via Semiotic Isotopy-Guided Dataset Construction", "author": "Marco Blanchini and Giovanna Maria Dimitri and Benedetta Tondi and Tarcisio Lancioni and Mauro Barni", "abstract": "Visual Sentiment Analysis (VSA) is a challenging task due to the vast diversity of emotionally salient images and the inherent difficulty of acquiring sufficient data to capture this variability comprehensively. Key obstacles include building large-scale VSA datasets and developing effective methodologies that enable algorithms to identify emotionally significant elements within an image. These challenges are reflected in the limited generalization performance of VSA algorithms and models when trained and tested across different datasets. Starting from a pool of existing data collections, our approach enables the creation of a new larger dataset that not only contains a wider variety of images than the original ones, but also permits training new models with improved capability to focus on emotionally relevant combinations of image elements. This is achieved through the integration of the semiotic isotopy concept within the dataset creation process, providing deeper insights into the emotional content of images. Empirical evaluations show that models trained on a dataset generated with our method consistently outperform those trained on the original data collections, achieving superior generalization across major VSA benchmarks", "link": "http://arxiv.org/abs/2512.14665v1", "date": "2025-12-16", "relevancy": 2.4999, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Visual%20Sentiment%20Analysis%20via%20Semiotic%20Isotopy-Guided%20Dataset%20Construction&body=Title%3A%20Enhancing%20Visual%20Sentiment%20Analysis%20via%20Semiotic%20Isotopy-Guided%20Dataset%20Construction%0AAuthor%3A%20Marco%20Blanchini%20and%20Giovanna%20Maria%20Dimitri%20and%20Benedetta%20Tondi%20and%20Tarcisio%20Lancioni%20and%20Mauro%20Barni%0AAbstract%3A%20Visual%20Sentiment%20Analysis%20%28VSA%29%20is%20a%20challenging%20task%20due%20to%20the%20vast%20diversity%20of%20emotionally%20salient%20images%20and%20the%20inherent%20difficulty%20of%20acquiring%20sufficient%20data%20to%20capture%20this%20variability%20comprehensively.%20Key%20obstacles%20include%20building%20large-scale%20VSA%20datasets%20and%20developing%20effective%20methodologies%20that%20enable%20algorithms%20to%20identify%20emotionally%20significant%20elements%20within%20an%20image.%20These%20challenges%20are%20reflected%20in%20the%20limited%20generalization%20performance%20of%20VSA%20algorithms%20and%20models%20when%20trained%20and%20tested%20across%20different%20datasets.%20Starting%20from%20a%20pool%20of%20existing%20data%20collections%2C%20our%20approach%20enables%20the%20creation%20of%20a%20new%20larger%20dataset%20that%20not%20only%20contains%20a%20wider%20variety%20of%20images%20than%20the%20original%20ones%2C%20but%20also%20permits%20training%20new%20models%20with%20improved%20capability%20to%20focus%20on%20emotionally%20relevant%20combinations%20of%20image%20elements.%20This%20is%20achieved%20through%20the%20integration%20of%20the%20semiotic%20isotopy%20concept%20within%20the%20dataset%20creation%20process%2C%20providing%20deeper%20insights%20into%20the%20emotional%20content%20of%20images.%20Empirical%20evaluations%20show%20that%20models%20trained%20on%20a%20dataset%20generated%20with%20our%20method%20consistently%20outperform%20those%20trained%20on%20the%20original%20data%20collections%2C%20achieving%20superior%20generalization%20across%20major%20VSA%20benchmarks%0ALink%3A%20http%3A//arxiv.org/abs/2512.14665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Visual%2520Sentiment%2520Analysis%2520via%2520Semiotic%2520Isotopy-Guided%2520Dataset%2520Construction%26entry.906535625%3DMarco%2520Blanchini%2520and%2520Giovanna%2520Maria%2520Dimitri%2520and%2520Benedetta%2520Tondi%2520and%2520Tarcisio%2520Lancioni%2520and%2520Mauro%2520Barni%26entry.1292438233%3DVisual%2520Sentiment%2520Analysis%2520%2528VSA%2529%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%2520vast%2520diversity%2520of%2520emotionally%2520salient%2520images%2520and%2520the%2520inherent%2520difficulty%2520of%2520acquiring%2520sufficient%2520data%2520to%2520capture%2520this%2520variability%2520comprehensively.%2520Key%2520obstacles%2520include%2520building%2520large-scale%2520VSA%2520datasets%2520and%2520developing%2520effective%2520methodologies%2520that%2520enable%2520algorithms%2520to%2520identify%2520emotionally%2520significant%2520elements%2520within%2520an%2520image.%2520These%2520challenges%2520are%2520reflected%2520in%2520the%2520limited%2520generalization%2520performance%2520of%2520VSA%2520algorithms%2520and%2520models%2520when%2520trained%2520and%2520tested%2520across%2520different%2520datasets.%2520Starting%2520from%2520a%2520pool%2520of%2520existing%2520data%2520collections%252C%2520our%2520approach%2520enables%2520the%2520creation%2520of%2520a%2520new%2520larger%2520dataset%2520that%2520not%2520only%2520contains%2520a%2520wider%2520variety%2520of%2520images%2520than%2520the%2520original%2520ones%252C%2520but%2520also%2520permits%2520training%2520new%2520models%2520with%2520improved%2520capability%2520to%2520focus%2520on%2520emotionally%2520relevant%2520combinations%2520of%2520image%2520elements.%2520This%2520is%2520achieved%2520through%2520the%2520integration%2520of%2520the%2520semiotic%2520isotopy%2520concept%2520within%2520the%2520dataset%2520creation%2520process%252C%2520providing%2520deeper%2520insights%2520into%2520the%2520emotional%2520content%2520of%2520images.%2520Empirical%2520evaluations%2520show%2520that%2520models%2520trained%2520on%2520a%2520dataset%2520generated%2520with%2520our%2520method%2520consistently%2520outperform%2520those%2520trained%2520on%2520the%2520original%2520data%2520collections%252C%2520achieving%2520superior%2520generalization%2520across%2520major%2520VSA%2520benchmarks%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Visual%20Sentiment%20Analysis%20via%20Semiotic%20Isotopy-Guided%20Dataset%20Construction&entry.906535625=Marco%20Blanchini%20and%20Giovanna%20Maria%20Dimitri%20and%20Benedetta%20Tondi%20and%20Tarcisio%20Lancioni%20and%20Mauro%20Barni&entry.1292438233=Visual%20Sentiment%20Analysis%20%28VSA%29%20is%20a%20challenging%20task%20due%20to%20the%20vast%20diversity%20of%20emotionally%20salient%20images%20and%20the%20inherent%20difficulty%20of%20acquiring%20sufficient%20data%20to%20capture%20this%20variability%20comprehensively.%20Key%20obstacles%20include%20building%20large-scale%20VSA%20datasets%20and%20developing%20effective%20methodologies%20that%20enable%20algorithms%20to%20identify%20emotionally%20significant%20elements%20within%20an%20image.%20These%20challenges%20are%20reflected%20in%20the%20limited%20generalization%20performance%20of%20VSA%20algorithms%20and%20models%20when%20trained%20and%20tested%20across%20different%20datasets.%20Starting%20from%20a%20pool%20of%20existing%20data%20collections%2C%20our%20approach%20enables%20the%20creation%20of%20a%20new%20larger%20dataset%20that%20not%20only%20contains%20a%20wider%20variety%20of%20images%20than%20the%20original%20ones%2C%20but%20also%20permits%20training%20new%20models%20with%20improved%20capability%20to%20focus%20on%20emotionally%20relevant%20combinations%20of%20image%20elements.%20This%20is%20achieved%20through%20the%20integration%20of%20the%20semiotic%20isotopy%20concept%20within%20the%20dataset%20creation%20process%2C%20providing%20deeper%20insights%20into%20the%20emotional%20content%20of%20images.%20Empirical%20evaluations%20show%20that%20models%20trained%20on%20a%20dataset%20generated%20with%20our%20method%20consistently%20outperform%20those%20trained%20on%20the%20original%20data%20collections%2C%20achieving%20superior%20generalization%20across%20major%20VSA%20benchmarks&entry.1838667208=http%3A//arxiv.org/abs/2512.14665v1&entry.124074799=Read"},
{"title": "AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection", "author": "Lorenzo Pellegrini and Davide Cozzolino and Serafino Pandolfini and Davide Maltoni and Matteo Ferrara and Luisa Verdoliva and Marco Prati and Marco Ramilli", "abstract": "The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators.", "link": "http://arxiv.org/abs/2504.20865v3", "date": "2025-12-16", "relevancy": 2.4823, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6506}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6096}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection&body=Title%3A%20AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Lorenzo%20Pellegrini%20and%20Davide%20Cozzolino%20and%20Serafino%20Pandolfini%20and%20Davide%20Maltoni%20and%20Matteo%20Ferrara%20and%20Luisa%20Verdoliva%20and%20Marco%20Prati%20and%20Marco%20Ramilli%0AAbstract%3A%20The%20rapid%20advancement%20of%20generative%20AI%20has%20revolutionized%20image%20creation%2C%20enabling%20high-quality%20synthesis%20from%20text%20prompts%20while%20raising%20critical%20challenges%20for%20media%20authenticity.%20We%20present%20Ai-GenBench%2C%20a%20novel%20benchmark%20designed%20to%20address%20the%20urgent%20need%20for%20robust%20detection%20of%20AI-generated%20images%20in%20real-world%20scenarios.%20Unlike%20existing%20solutions%20that%20evaluate%20models%20on%20static%20datasets%2C%20Ai-GenBench%20introduces%20a%20temporal%20evaluation%20framework%20where%20detection%20methods%20are%20incrementally%20trained%20on%20synthetic%20images%2C%20historically%20ordered%20by%20their%20generative%20models%2C%20to%20test%20their%20ability%20to%20generalize%20to%20new%20generative%20models%2C%20such%20as%20the%20transition%20from%20GANs%20to%20diffusion%20models.%20Our%20benchmark%20focuses%20on%20high-quality%2C%20diverse%20visual%20content%20and%20overcomes%20key%20limitations%20of%20current%20approaches%2C%20including%20arbitrary%20dataset%20splits%2C%20unfair%20comparisons%2C%20and%20excessive%20computational%20demands.%20Ai-GenBench%20provides%20a%20comprehensive%20dataset%2C%20a%20standardized%20evaluation%20protocol%2C%20and%20accessible%20tools%20for%20both%20researchers%20and%20non-experts%20%28e.g.%2C%20journalists%2C%20fact-checkers%29%2C%20ensuring%20reproducibility%20while%20maintaining%20practical%20training%20requirements.%20By%20establishing%20clear%20evaluation%20rules%20and%20controlled%20augmentation%20strategies%2C%20Ai-GenBench%20enables%20meaningful%20comparison%20of%20detection%20methods%20and%20scalable%20solutions.%20Code%20and%20data%20are%20publicly%20available%20to%20ensure%20reproducibility%20and%20to%20support%20the%20development%20of%20robust%20forensic%20detectors%20to%20keep%20pace%20with%20the%20rise%20of%20new%20synthetic%20generators.%0ALink%3A%20http%3A//arxiv.org/abs/2504.20865v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-GenBench%253A%2520A%2520New%2520Ongoing%2520Benchmark%2520for%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DLorenzo%2520Pellegrini%2520and%2520Davide%2520Cozzolino%2520and%2520Serafino%2520Pandolfini%2520and%2520Davide%2520Maltoni%2520and%2520Matteo%2520Ferrara%2520and%2520Luisa%2520Verdoliva%2520and%2520Marco%2520Prati%2520and%2520Marco%2520Ramilli%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520generative%2520AI%2520has%2520revolutionized%2520image%2520creation%252C%2520enabling%2520high-quality%2520synthesis%2520from%2520text%2520prompts%2520while%2520raising%2520critical%2520challenges%2520for%2520media%2520authenticity.%2520We%2520present%2520Ai-GenBench%252C%2520a%2520novel%2520benchmark%2520designed%2520to%2520address%2520the%2520urgent%2520need%2520for%2520robust%2520detection%2520of%2520AI-generated%2520images%2520in%2520real-world%2520scenarios.%2520Unlike%2520existing%2520solutions%2520that%2520evaluate%2520models%2520on%2520static%2520datasets%252C%2520Ai-GenBench%2520introduces%2520a%2520temporal%2520evaluation%2520framework%2520where%2520detection%2520methods%2520are%2520incrementally%2520trained%2520on%2520synthetic%2520images%252C%2520historically%2520ordered%2520by%2520their%2520generative%2520models%252C%2520to%2520test%2520their%2520ability%2520to%2520generalize%2520to%2520new%2520generative%2520models%252C%2520such%2520as%2520the%2520transition%2520from%2520GANs%2520to%2520diffusion%2520models.%2520Our%2520benchmark%2520focuses%2520on%2520high-quality%252C%2520diverse%2520visual%2520content%2520and%2520overcomes%2520key%2520limitations%2520of%2520current%2520approaches%252C%2520including%2520arbitrary%2520dataset%2520splits%252C%2520unfair%2520comparisons%252C%2520and%2520excessive%2520computational%2520demands.%2520Ai-GenBench%2520provides%2520a%2520comprehensive%2520dataset%252C%2520a%2520standardized%2520evaluation%2520protocol%252C%2520and%2520accessible%2520tools%2520for%2520both%2520researchers%2520and%2520non-experts%2520%2528e.g.%252C%2520journalists%252C%2520fact-checkers%2529%252C%2520ensuring%2520reproducibility%2520while%2520maintaining%2520practical%2520training%2520requirements.%2520By%2520establishing%2520clear%2520evaluation%2520rules%2520and%2520controlled%2520augmentation%2520strategies%252C%2520Ai-GenBench%2520enables%2520meaningful%2520comparison%2520of%2520detection%2520methods%2520and%2520scalable%2520solutions.%2520Code%2520and%2520data%2520are%2520publicly%2520available%2520to%2520ensure%2520reproducibility%2520and%2520to%2520support%2520the%2520development%2520of%2520robust%2520forensic%2520detectors%2520to%2520keep%2520pace%2520with%2520the%2520rise%2520of%2520new%2520synthetic%2520generators.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20865v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection&entry.906535625=Lorenzo%20Pellegrini%20and%20Davide%20Cozzolino%20and%20Serafino%20Pandolfini%20and%20Davide%20Maltoni%20and%20Matteo%20Ferrara%20and%20Luisa%20Verdoliva%20and%20Marco%20Prati%20and%20Marco%20Ramilli&entry.1292438233=The%20rapid%20advancement%20of%20generative%20AI%20has%20revolutionized%20image%20creation%2C%20enabling%20high-quality%20synthesis%20from%20text%20prompts%20while%20raising%20critical%20challenges%20for%20media%20authenticity.%20We%20present%20Ai-GenBench%2C%20a%20novel%20benchmark%20designed%20to%20address%20the%20urgent%20need%20for%20robust%20detection%20of%20AI-generated%20images%20in%20real-world%20scenarios.%20Unlike%20existing%20solutions%20that%20evaluate%20models%20on%20static%20datasets%2C%20Ai-GenBench%20introduces%20a%20temporal%20evaluation%20framework%20where%20detection%20methods%20are%20incrementally%20trained%20on%20synthetic%20images%2C%20historically%20ordered%20by%20their%20generative%20models%2C%20to%20test%20their%20ability%20to%20generalize%20to%20new%20generative%20models%2C%20such%20as%20the%20transition%20from%20GANs%20to%20diffusion%20models.%20Our%20benchmark%20focuses%20on%20high-quality%2C%20diverse%20visual%20content%20and%20overcomes%20key%20limitations%20of%20current%20approaches%2C%20including%20arbitrary%20dataset%20splits%2C%20unfair%20comparisons%2C%20and%20excessive%20computational%20demands.%20Ai-GenBench%20provides%20a%20comprehensive%20dataset%2C%20a%20standardized%20evaluation%20protocol%2C%20and%20accessible%20tools%20for%20both%20researchers%20and%20non-experts%20%28e.g.%2C%20journalists%2C%20fact-checkers%29%2C%20ensuring%20reproducibility%20while%20maintaining%20practical%20training%20requirements.%20By%20establishing%20clear%20evaluation%20rules%20and%20controlled%20augmentation%20strategies%2C%20Ai-GenBench%20enables%20meaningful%20comparison%20of%20detection%20methods%20and%20scalable%20solutions.%20Code%20and%20data%20are%20publicly%20available%20to%20ensure%20reproducibility%20and%20to%20support%20the%20development%20of%20robust%20forensic%20detectors%20to%20keep%20pace%20with%20the%20rise%20of%20new%20synthetic%20generators.&entry.1838667208=http%3A//arxiv.org/abs/2504.20865v3&entry.124074799=Read"},
{"title": "Synthetic Electrogram Generation with Variational Autoencoders for ECGI", "author": "Miriam Guti\u00e9rrez Fern\u00e1ndez and Karen L\u00f3pez-Linares and Carlos Fambuena Santos and Mar\u00eda S. Guillem and Andreu M. Climent and \u00d3scar Barquero P\u00e9rez", "abstract": "Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.", "link": "http://arxiv.org/abs/2512.14537v1", "date": "2025-12-16", "relevancy": 2.4797, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5165}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4874}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Electrogram%20Generation%20with%20Variational%20Autoencoders%20for%20ECGI&body=Title%3A%20Synthetic%20Electrogram%20Generation%20with%20Variational%20Autoencoders%20for%20ECGI%0AAuthor%3A%20Miriam%20Guti%C3%A9rrez%20Fern%C3%A1ndez%20and%20Karen%20L%C3%B3pez-Linares%20and%20Carlos%20Fambuena%20Santos%20and%20Mar%C3%ADa%20S.%20Guillem%20and%20Andreu%20M.%20Climent%20and%20%C3%93scar%20Barquero%20P%C3%A9rez%0AAbstract%3A%20Atrial%20fibrillation%20%28AF%29%20is%20the%20most%20prevalent%20sustained%20cardiac%20arrhythmia%2C%20and%20its%20clinical%20assessment%20requires%20accurate%20characterization%20of%20atrial%20electrical%20activity.%20Noninvasive%20electrocardiographic%20imaging%20%28ECGI%29%20combined%20with%20deep%20learning%20%28DL%29%20approaches%20for%20estimating%20intracardiac%20electrograms%20%28EGMs%29%20from%20body%20surface%20potentials%20%28BSPMs%29%20has%20shown%20promise%2C%20but%20progress%20is%20hindered%20by%20the%20limited%20availability%20of%20paired%20BSPM-EGM%20datasets.%20To%20address%20this%20limitation%2C%20we%20investigate%20variational%20autoencoders%20%28VAEs%29%20for%20the%20generation%20of%20synthetic%20multichannel%20atrial%20EGMs.%20Two%20models%20are%20proposed%3A%20a%20sinus%20rhythm-specific%20VAE%20%28VAE-S%29%20and%20a%20class-conditioned%20VAE%20%28VAE-C%29%20trained%20on%20both%20sinus%20rhythm%20and%20AF%20signals.%20Generated%20EGMs%20are%20evaluated%20using%20morphological%2C%20spectral%2C%20and%20distributional%20similarity%20metrics.%20VAE-S%20achieves%20higher%20fidelity%20with%20respect%20to%20in%20silico%20EGMs%2C%20while%20VAE-C%20enables%20rhythm-specific%20generation%20at%20the%20expense%20of%20reduced%20sinus%20reconstruction%20quality.%20As%20a%20proof%20of%20concept%2C%20the%20generated%20EGMs%20are%20used%20for%20data%20augmentation%20in%20a%20downstream%20noninvasive%20EGM%20reconstruction%20task%2C%20where%20moderate%20augmentation%20improves%20estimation%20performance.%20These%20results%20demonstrate%20the%20potential%20of%20VAE-based%20generative%20modeling%20to%20alleviate%20data%20scarcity%20and%20enhance%20deep%20learning-based%20ECGI%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Electrogram%2520Generation%2520with%2520Variational%2520Autoencoders%2520for%2520ECGI%26entry.906535625%3DMiriam%2520Guti%25C3%25A9rrez%2520Fern%25C3%25A1ndez%2520and%2520Karen%2520L%25C3%25B3pez-Linares%2520and%2520Carlos%2520Fambuena%2520Santos%2520and%2520Mar%25C3%25ADa%2520S.%2520Guillem%2520and%2520Andreu%2520M.%2520Climent%2520and%2520%25C3%2593scar%2520Barquero%2520P%25C3%25A9rez%26entry.1292438233%3DAtrial%2520fibrillation%2520%2528AF%2529%2520is%2520the%2520most%2520prevalent%2520sustained%2520cardiac%2520arrhythmia%252C%2520and%2520its%2520clinical%2520assessment%2520requires%2520accurate%2520characterization%2520of%2520atrial%2520electrical%2520activity.%2520Noninvasive%2520electrocardiographic%2520imaging%2520%2528ECGI%2529%2520combined%2520with%2520deep%2520learning%2520%2528DL%2529%2520approaches%2520for%2520estimating%2520intracardiac%2520electrograms%2520%2528EGMs%2529%2520from%2520body%2520surface%2520potentials%2520%2528BSPMs%2529%2520has%2520shown%2520promise%252C%2520but%2520progress%2520is%2520hindered%2520by%2520the%2520limited%2520availability%2520of%2520paired%2520BSPM-EGM%2520datasets.%2520To%2520address%2520this%2520limitation%252C%2520we%2520investigate%2520variational%2520autoencoders%2520%2528VAEs%2529%2520for%2520the%2520generation%2520of%2520synthetic%2520multichannel%2520atrial%2520EGMs.%2520Two%2520models%2520are%2520proposed%253A%2520a%2520sinus%2520rhythm-specific%2520VAE%2520%2528VAE-S%2529%2520and%2520a%2520class-conditioned%2520VAE%2520%2528VAE-C%2529%2520trained%2520on%2520both%2520sinus%2520rhythm%2520and%2520AF%2520signals.%2520Generated%2520EGMs%2520are%2520evaluated%2520using%2520morphological%252C%2520spectral%252C%2520and%2520distributional%2520similarity%2520metrics.%2520VAE-S%2520achieves%2520higher%2520fidelity%2520with%2520respect%2520to%2520in%2520silico%2520EGMs%252C%2520while%2520VAE-C%2520enables%2520rhythm-specific%2520generation%2520at%2520the%2520expense%2520of%2520reduced%2520sinus%2520reconstruction%2520quality.%2520As%2520a%2520proof%2520of%2520concept%252C%2520the%2520generated%2520EGMs%2520are%2520used%2520for%2520data%2520augmentation%2520in%2520a%2520downstream%2520noninvasive%2520EGM%2520reconstruction%2520task%252C%2520where%2520moderate%2520augmentation%2520improves%2520estimation%2520performance.%2520These%2520results%2520demonstrate%2520the%2520potential%2520of%2520VAE-based%2520generative%2520modeling%2520to%2520alleviate%2520data%2520scarcity%2520and%2520enhance%2520deep%2520learning-based%2520ECGI%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Electrogram%20Generation%20with%20Variational%20Autoencoders%20for%20ECGI&entry.906535625=Miriam%20Guti%C3%A9rrez%20Fern%C3%A1ndez%20and%20Karen%20L%C3%B3pez-Linares%20and%20Carlos%20Fambuena%20Santos%20and%20Mar%C3%ADa%20S.%20Guillem%20and%20Andreu%20M.%20Climent%20and%20%C3%93scar%20Barquero%20P%C3%A9rez&entry.1292438233=Atrial%20fibrillation%20%28AF%29%20is%20the%20most%20prevalent%20sustained%20cardiac%20arrhythmia%2C%20and%20its%20clinical%20assessment%20requires%20accurate%20characterization%20of%20atrial%20electrical%20activity.%20Noninvasive%20electrocardiographic%20imaging%20%28ECGI%29%20combined%20with%20deep%20learning%20%28DL%29%20approaches%20for%20estimating%20intracardiac%20electrograms%20%28EGMs%29%20from%20body%20surface%20potentials%20%28BSPMs%29%20has%20shown%20promise%2C%20but%20progress%20is%20hindered%20by%20the%20limited%20availability%20of%20paired%20BSPM-EGM%20datasets.%20To%20address%20this%20limitation%2C%20we%20investigate%20variational%20autoencoders%20%28VAEs%29%20for%20the%20generation%20of%20synthetic%20multichannel%20atrial%20EGMs.%20Two%20models%20are%20proposed%3A%20a%20sinus%20rhythm-specific%20VAE%20%28VAE-S%29%20and%20a%20class-conditioned%20VAE%20%28VAE-C%29%20trained%20on%20both%20sinus%20rhythm%20and%20AF%20signals.%20Generated%20EGMs%20are%20evaluated%20using%20morphological%2C%20spectral%2C%20and%20distributional%20similarity%20metrics.%20VAE-S%20achieves%20higher%20fidelity%20with%20respect%20to%20in%20silico%20EGMs%2C%20while%20VAE-C%20enables%20rhythm-specific%20generation%20at%20the%20expense%20of%20reduced%20sinus%20reconstruction%20quality.%20As%20a%20proof%20of%20concept%2C%20the%20generated%20EGMs%20are%20used%20for%20data%20augmentation%20in%20a%20downstream%20noninvasive%20EGM%20reconstruction%20task%2C%20where%20moderate%20augmentation%20improves%20estimation%20performance.%20These%20results%20demonstrate%20the%20potential%20of%20VAE-based%20generative%20modeling%20to%20alleviate%20data%20scarcity%20and%20enhance%20deep%20learning-based%20ECGI%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2512.14537v1&entry.124074799=Read"},
{"title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models", "author": "Zefang Liu and Nam H. Nguyen and Yinzhu Quan and Shi-Xiong Zhang", "abstract": "Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.", "link": "http://arxiv.org/abs/2512.13618v2", "date": "2025-12-16", "relevancy": 2.4756, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4991}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Tokenization%20Strategies%20for%20Event%20Sequence%20Modeling%20with%20Large%20Language%20Models&body=Title%3A%20Temporal%20Tokenization%20Strategies%20for%20Event%20Sequence%20Modeling%20with%20Large%20Language%20Models%0AAuthor%3A%20Zefang%20Liu%20and%20Nam%20H.%20Nguyen%20and%20Yinzhu%20Quan%20and%20Shi-Xiong%20Zhang%0AAbstract%3A%20Representing%20continuous%20time%20is%20a%20critical%20and%20under-explored%20challenge%20in%20modeling%20temporal%20event%20sequences%20with%20large%20language%20models%20%28LLMs%29.%20Various%20strategies%20like%20byte-level%20representations%20or%20calendar%20tokens%20have%20been%20proposed.%20However%2C%20the%20optimal%20approach%20remains%20unclear%2C%20especially%20given%20the%20diverse%20statistical%20distributions%20of%20real-world%20event%20data%2C%20which%20range%20from%20smooth%20log-normal%20to%20discrete%2C%20spiky%20patterns.%20This%20paper%20presents%20the%20first%20empirical%20study%20of%20temporal%20tokenization%20for%20event%20sequences%2C%20comparing%20distinct%20encoding%20strategies%3A%20naive%20numeric%20strings%2C%20high-precision%20byte-level%20representations%2C%20human-semantic%20calendar%20tokens%2C%20classic%20uniform%20binning%2C%20and%20adaptive%20residual%20scalar%20quantization.%20We%20evaluate%20these%20strategies%20by%20fine-tuning%20LLMs%20on%20real-world%20datasets%20that%20exemplify%20these%20diverse%20distributions.%20Our%20analysis%20reveals%20that%20no%20single%20strategy%20is%20universally%20superior%3B%20instead%2C%20prediction%20performance%20depends%20heavily%20on%20aligning%20the%20tokenizer%20with%20the%20data%27s%20statistical%20properties%2C%20with%20log-based%20strategies%20excelling%20on%20skewed%20distributions%20and%20human-centric%20formats%20proving%20robust%20for%20mixed%20modalities.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Tokenization%2520Strategies%2520for%2520Event%2520Sequence%2520Modeling%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DZefang%2520Liu%2520and%2520Nam%2520H.%2520Nguyen%2520and%2520Yinzhu%2520Quan%2520and%2520Shi-Xiong%2520Zhang%26entry.1292438233%3DRepresenting%2520continuous%2520time%2520is%2520a%2520critical%2520and%2520under-explored%2520challenge%2520in%2520modeling%2520temporal%2520event%2520sequences%2520with%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Various%2520strategies%2520like%2520byte-level%2520representations%2520or%2520calendar%2520tokens%2520have%2520been%2520proposed.%2520However%252C%2520the%2520optimal%2520approach%2520remains%2520unclear%252C%2520especially%2520given%2520the%2520diverse%2520statistical%2520distributions%2520of%2520real-world%2520event%2520data%252C%2520which%2520range%2520from%2520smooth%2520log-normal%2520to%2520discrete%252C%2520spiky%2520patterns.%2520This%2520paper%2520presents%2520the%2520first%2520empirical%2520study%2520of%2520temporal%2520tokenization%2520for%2520event%2520sequences%252C%2520comparing%2520distinct%2520encoding%2520strategies%253A%2520naive%2520numeric%2520strings%252C%2520high-precision%2520byte-level%2520representations%252C%2520human-semantic%2520calendar%2520tokens%252C%2520classic%2520uniform%2520binning%252C%2520and%2520adaptive%2520residual%2520scalar%2520quantization.%2520We%2520evaluate%2520these%2520strategies%2520by%2520fine-tuning%2520LLMs%2520on%2520real-world%2520datasets%2520that%2520exemplify%2520these%2520diverse%2520distributions.%2520Our%2520analysis%2520reveals%2520that%2520no%2520single%2520strategy%2520is%2520universally%2520superior%253B%2520instead%252C%2520prediction%2520performance%2520depends%2520heavily%2520on%2520aligning%2520the%2520tokenizer%2520with%2520the%2520data%2527s%2520statistical%2520properties%252C%2520with%2520log-based%2520strategies%2520excelling%2520on%2520skewed%2520distributions%2520and%2520human-centric%2520formats%2520proving%2520robust%2520for%2520mixed%2520modalities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Tokenization%20Strategies%20for%20Event%20Sequence%20Modeling%20with%20Large%20Language%20Models&entry.906535625=Zefang%20Liu%20and%20Nam%20H.%20Nguyen%20and%20Yinzhu%20Quan%20and%20Shi-Xiong%20Zhang&entry.1292438233=Representing%20continuous%20time%20is%20a%20critical%20and%20under-explored%20challenge%20in%20modeling%20temporal%20event%20sequences%20with%20large%20language%20models%20%28LLMs%29.%20Various%20strategies%20like%20byte-level%20representations%20or%20calendar%20tokens%20have%20been%20proposed.%20However%2C%20the%20optimal%20approach%20remains%20unclear%2C%20especially%20given%20the%20diverse%20statistical%20distributions%20of%20real-world%20event%20data%2C%20which%20range%20from%20smooth%20log-normal%20to%20discrete%2C%20spiky%20patterns.%20This%20paper%20presents%20the%20first%20empirical%20study%20of%20temporal%20tokenization%20for%20event%20sequences%2C%20comparing%20distinct%20encoding%20strategies%3A%20naive%20numeric%20strings%2C%20high-precision%20byte-level%20representations%2C%20human-semantic%20calendar%20tokens%2C%20classic%20uniform%20binning%2C%20and%20adaptive%20residual%20scalar%20quantization.%20We%20evaluate%20these%20strategies%20by%20fine-tuning%20LLMs%20on%20real-world%20datasets%20that%20exemplify%20these%20diverse%20distributions.%20Our%20analysis%20reveals%20that%20no%20single%20strategy%20is%20universally%20superior%3B%20instead%2C%20prediction%20performance%20depends%20heavily%20on%20aligning%20the%20tokenizer%20with%20the%20data%27s%20statistical%20properties%2C%20with%20log-based%20strategies%20excelling%20on%20skewed%20distributions%20and%20human-centric%20formats%20proving%20robust%20for%20mixed%20modalities.&entry.1838667208=http%3A//arxiv.org/abs/2512.13618v2&entry.124074799=Read"},
{"title": "Polypersona: Persona-Grounded LLM for Synthetic Survey Responses", "author": "Tejaswani Dash and Dinesh Karri and Anudeep Vurity and Gautam Datla and Tazeem Ahmad and Saima Rafi and Rohith Tangudu", "abstract": "This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.", "link": "http://arxiv.org/abs/2512.14562v1", "date": "2025-12-16", "relevancy": 2.4542, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4983}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.489}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polypersona%3A%20Persona-Grounded%20LLM%20for%20Synthetic%20Survey%20Responses&body=Title%3A%20Polypersona%3A%20Persona-Grounded%20LLM%20for%20Synthetic%20Survey%20Responses%0AAuthor%3A%20Tejaswani%20Dash%20and%20Dinesh%20Karri%20and%20Anudeep%20Vurity%20and%20Gautam%20Datla%20and%20Tazeem%20Ahmad%20and%20Saima%20Rafi%20and%20Rohith%20Tangudu%0AAbstract%3A%20This%20paper%20introduces%20PolyPersona%2C%20a%20generative%20framework%20for%20synthesizing%20persona-conditioned%20survey%20responses%20across%20multiple%20domains.%20The%20framework%20instruction-tunes%20compact%20chat%20models%20using%20parameter-efficient%20LoRA%20adapters%20with%204-bit%20quantization%20under%20a%20resource-adaptive%20training%20setup.%20A%20dialogue-based%20data%20pipeline%20explicitly%20preserves%20persona%20cues%2C%20ensuring%20consistent%20behavioral%20alignment%20across%20generated%20responses.%20Using%20this%20pipeline%2C%20we%20construct%20a%20dataset%20of%203%2C568%20synthetic%20survey%20responses%20spanning%20ten%20domains%20and%20433%20distinct%20personas%2C%20enabling%20controlled%20instruction%20tuning%20and%20systematic%20multi-domain%20evaluation.%20We%20evaluate%20the%20generated%20responses%20using%20a%20multi-metric%20evaluation%20suite%20that%20combines%20standard%20text%20generation%20metrics%2C%20including%20BLEU%2C%20ROUGE%2C%20and%20BERTScore%2C%20with%20survey-specific%20metrics%20designed%20to%20assess%20structural%20coherence%2C%20stylistic%20consistency%2C%20and%20sentiment%20alignment.Experimental%20results%20show%20that%20compact%20models%20such%20as%20TinyLlama%201.1B%20and%20Phi-2%20achieve%20performance%20comparable%20to%20larger%207B%20to%208B%20baselines%2C%20with%20a%20highest%20BLEU%20score%20of%200.090%20and%20ROUGE-1%20of%200.429.%20These%20findings%20demonstrate%20that%20persona-conditioned%20fine-tuning%20enables%20small%20language%20models%20to%20generate%20reliable%20and%20coherent%20synthetic%20survey%20data.%20The%20proposed%20framework%20provides%20an%20efficient%20and%20reproducible%20approach%20for%20survey%20data%20generation%2C%20supporting%20scalable%20evaluation%20while%20facilitating%20bias%20analysis%20through%20transparent%20and%20open%20protocols.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolypersona%253A%2520Persona-Grounded%2520LLM%2520for%2520Synthetic%2520Survey%2520Responses%26entry.906535625%3DTejaswani%2520Dash%2520and%2520Dinesh%2520Karri%2520and%2520Anudeep%2520Vurity%2520and%2520Gautam%2520Datla%2520and%2520Tazeem%2520Ahmad%2520and%2520Saima%2520Rafi%2520and%2520Rohith%2520Tangudu%26entry.1292438233%3DThis%2520paper%2520introduces%2520PolyPersona%252C%2520a%2520generative%2520framework%2520for%2520synthesizing%2520persona-conditioned%2520survey%2520responses%2520across%2520multiple%2520domains.%2520The%2520framework%2520instruction-tunes%2520compact%2520chat%2520models%2520using%2520parameter-efficient%2520LoRA%2520adapters%2520with%25204-bit%2520quantization%2520under%2520a%2520resource-adaptive%2520training%2520setup.%2520A%2520dialogue-based%2520data%2520pipeline%2520explicitly%2520preserves%2520persona%2520cues%252C%2520ensuring%2520consistent%2520behavioral%2520alignment%2520across%2520generated%2520responses.%2520Using%2520this%2520pipeline%252C%2520we%2520construct%2520a%2520dataset%2520of%25203%252C568%2520synthetic%2520survey%2520responses%2520spanning%2520ten%2520domains%2520and%2520433%2520distinct%2520personas%252C%2520enabling%2520controlled%2520instruction%2520tuning%2520and%2520systematic%2520multi-domain%2520evaluation.%2520We%2520evaluate%2520the%2520generated%2520responses%2520using%2520a%2520multi-metric%2520evaluation%2520suite%2520that%2520combines%2520standard%2520text%2520generation%2520metrics%252C%2520including%2520BLEU%252C%2520ROUGE%252C%2520and%2520BERTScore%252C%2520with%2520survey-specific%2520metrics%2520designed%2520to%2520assess%2520structural%2520coherence%252C%2520stylistic%2520consistency%252C%2520and%2520sentiment%2520alignment.Experimental%2520results%2520show%2520that%2520compact%2520models%2520such%2520as%2520TinyLlama%25201.1B%2520and%2520Phi-2%2520achieve%2520performance%2520comparable%2520to%2520larger%25207B%2520to%25208B%2520baselines%252C%2520with%2520a%2520highest%2520BLEU%2520score%2520of%25200.090%2520and%2520ROUGE-1%2520of%25200.429.%2520These%2520findings%2520demonstrate%2520that%2520persona-conditioned%2520fine-tuning%2520enables%2520small%2520language%2520models%2520to%2520generate%2520reliable%2520and%2520coherent%2520synthetic%2520survey%2520data.%2520The%2520proposed%2520framework%2520provides%2520an%2520efficient%2520and%2520reproducible%2520approach%2520for%2520survey%2520data%2520generation%252C%2520supporting%2520scalable%2520evaluation%2520while%2520facilitating%2520bias%2520analysis%2520through%2520transparent%2520and%2520open%2520protocols.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polypersona%3A%20Persona-Grounded%20LLM%20for%20Synthetic%20Survey%20Responses&entry.906535625=Tejaswani%20Dash%20and%20Dinesh%20Karri%20and%20Anudeep%20Vurity%20and%20Gautam%20Datla%20and%20Tazeem%20Ahmad%20and%20Saima%20Rafi%20and%20Rohith%20Tangudu&entry.1292438233=This%20paper%20introduces%20PolyPersona%2C%20a%20generative%20framework%20for%20synthesizing%20persona-conditioned%20survey%20responses%20across%20multiple%20domains.%20The%20framework%20instruction-tunes%20compact%20chat%20models%20using%20parameter-efficient%20LoRA%20adapters%20with%204-bit%20quantization%20under%20a%20resource-adaptive%20training%20setup.%20A%20dialogue-based%20data%20pipeline%20explicitly%20preserves%20persona%20cues%2C%20ensuring%20consistent%20behavioral%20alignment%20across%20generated%20responses.%20Using%20this%20pipeline%2C%20we%20construct%20a%20dataset%20of%203%2C568%20synthetic%20survey%20responses%20spanning%20ten%20domains%20and%20433%20distinct%20personas%2C%20enabling%20controlled%20instruction%20tuning%20and%20systematic%20multi-domain%20evaluation.%20We%20evaluate%20the%20generated%20responses%20using%20a%20multi-metric%20evaluation%20suite%20that%20combines%20standard%20text%20generation%20metrics%2C%20including%20BLEU%2C%20ROUGE%2C%20and%20BERTScore%2C%20with%20survey-specific%20metrics%20designed%20to%20assess%20structural%20coherence%2C%20stylistic%20consistency%2C%20and%20sentiment%20alignment.Experimental%20results%20show%20that%20compact%20models%20such%20as%20TinyLlama%201.1B%20and%20Phi-2%20achieve%20performance%20comparable%20to%20larger%207B%20to%208B%20baselines%2C%20with%20a%20highest%20BLEU%20score%20of%200.090%20and%20ROUGE-1%20of%200.429.%20These%20findings%20demonstrate%20that%20persona-conditioned%20fine-tuning%20enables%20small%20language%20models%20to%20generate%20reliable%20and%20coherent%20synthetic%20survey%20data.%20The%20proposed%20framework%20provides%20an%20efficient%20and%20reproducible%20approach%20for%20survey%20data%20generation%2C%20supporting%20scalable%20evaluation%20while%20facilitating%20bias%20analysis%20through%20transparent%20and%20open%20protocols.&entry.1838667208=http%3A//arxiv.org/abs/2512.14562v1&entry.124074799=Read"},
{"title": "TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines", "author": "David Schulmeister and Valentin Hartmann and Lars Klein and Robert West", "abstract": "Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.", "link": "http://arxiv.org/abs/2512.14645v1", "date": "2025-12-16", "relevancy": 2.4541, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiME%3A%20Tiny%20Monolingual%20Encoders%20for%20Efficient%20NLP%20Pipelines&body=Title%3A%20TiME%3A%20Tiny%20Monolingual%20Encoders%20for%20Efficient%20NLP%20Pipelines%0AAuthor%3A%20David%20Schulmeister%20and%20Valentin%20Hartmann%20and%20Lars%20Klein%20and%20Robert%20West%0AAbstract%3A%20Today%2C%20a%20lot%20of%20research%20on%20language%20models%20is%20focused%20on%20large%2C%20general-purpose%20models.%20However%2C%20many%20NLP%20pipelines%20only%20require%20models%20with%20a%20well-defined%2C%20small%20set%20of%20capabilities.%20While%20large%20models%20are%20capable%20of%20performing%20the%20tasks%20of%20those%20smaller%20models%2C%20they%20are%20simply%20not%20fast%20enough%20to%20process%20large%20amounts%20of%20data%20or%20offer%20real-time%20responses.%20Furthermore%2C%20they%20often%20use%20unnecessarily%20large%20amounts%20of%20energy%2C%20leading%20to%20sustainability%20concerns%20and%20problems%20when%20deploying%20them%20on%20battery-powered%20devices.%20In%20our%20work%2C%20we%20show%20how%20to%20train%20small%20models%20for%20such%20efficiency-critical%20applications.%20As%20opposed%20to%20many%20off-the-shelf%20NLP%20pipelines%2C%20our%20models%20use%20modern%20training%20techniques%20such%20as%20distillation%2C%20and%20offer%20support%20for%20low-resource%20languages.%20We%20call%20our%20models%20TiME%20%28Tiny%20Monolingual%20Encoders%29%20and%20comprehensively%20evaluate%20them%20on%20a%20range%20of%20common%20NLP%20tasks%2C%20observing%20an%20improved%20trade-off%20between%20benchmark%20performance%20on%20one%20hand%2C%20and%20throughput%2C%20latency%20and%20energy%20consumption%20on%20the%20other.%20Along%20the%20way%2C%20we%20show%20that%20distilling%20monolingual%20models%20from%20multilingual%20teachers%20is%20possible%2C%20and%20likewise%20distilling%20models%20with%20absolute%20positional%20embeddings%20from%20teachers%20with%20relative%20positional%20embeddings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiME%253A%2520Tiny%2520Monolingual%2520Encoders%2520for%2520Efficient%2520NLP%2520Pipelines%26entry.906535625%3DDavid%2520Schulmeister%2520and%2520Valentin%2520Hartmann%2520and%2520Lars%2520Klein%2520and%2520Robert%2520West%26entry.1292438233%3DToday%252C%2520a%2520lot%2520of%2520research%2520on%2520language%2520models%2520is%2520focused%2520on%2520large%252C%2520general-purpose%2520models.%2520However%252C%2520many%2520NLP%2520pipelines%2520only%2520require%2520models%2520with%2520a%2520well-defined%252C%2520small%2520set%2520of%2520capabilities.%2520While%2520large%2520models%2520are%2520capable%2520of%2520performing%2520the%2520tasks%2520of%2520those%2520smaller%2520models%252C%2520they%2520are%2520simply%2520not%2520fast%2520enough%2520to%2520process%2520large%2520amounts%2520of%2520data%2520or%2520offer%2520real-time%2520responses.%2520Furthermore%252C%2520they%2520often%2520use%2520unnecessarily%2520large%2520amounts%2520of%2520energy%252C%2520leading%2520to%2520sustainability%2520concerns%2520and%2520problems%2520when%2520deploying%2520them%2520on%2520battery-powered%2520devices.%2520In%2520our%2520work%252C%2520we%2520show%2520how%2520to%2520train%2520small%2520models%2520for%2520such%2520efficiency-critical%2520applications.%2520As%2520opposed%2520to%2520many%2520off-the-shelf%2520NLP%2520pipelines%252C%2520our%2520models%2520use%2520modern%2520training%2520techniques%2520such%2520as%2520distillation%252C%2520and%2520offer%2520support%2520for%2520low-resource%2520languages.%2520We%2520call%2520our%2520models%2520TiME%2520%2528Tiny%2520Monolingual%2520Encoders%2529%2520and%2520comprehensively%2520evaluate%2520them%2520on%2520a%2520range%2520of%2520common%2520NLP%2520tasks%252C%2520observing%2520an%2520improved%2520trade-off%2520between%2520benchmark%2520performance%2520on%2520one%2520hand%252C%2520and%2520throughput%252C%2520latency%2520and%2520energy%2520consumption%2520on%2520the%2520other.%2520Along%2520the%2520way%252C%2520we%2520show%2520that%2520distilling%2520monolingual%2520models%2520from%2520multilingual%2520teachers%2520is%2520possible%252C%2520and%2520likewise%2520distilling%2520models%2520with%2520absolute%2520positional%2520embeddings%2520from%2520teachers%2520with%2520relative%2520positional%2520embeddings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiME%3A%20Tiny%20Monolingual%20Encoders%20for%20Efficient%20NLP%20Pipelines&entry.906535625=David%20Schulmeister%20and%20Valentin%20Hartmann%20and%20Lars%20Klein%20and%20Robert%20West&entry.1292438233=Today%2C%20a%20lot%20of%20research%20on%20language%20models%20is%20focused%20on%20large%2C%20general-purpose%20models.%20However%2C%20many%20NLP%20pipelines%20only%20require%20models%20with%20a%20well-defined%2C%20small%20set%20of%20capabilities.%20While%20large%20models%20are%20capable%20of%20performing%20the%20tasks%20of%20those%20smaller%20models%2C%20they%20are%20simply%20not%20fast%20enough%20to%20process%20large%20amounts%20of%20data%20or%20offer%20real-time%20responses.%20Furthermore%2C%20they%20often%20use%20unnecessarily%20large%20amounts%20of%20energy%2C%20leading%20to%20sustainability%20concerns%20and%20problems%20when%20deploying%20them%20on%20battery-powered%20devices.%20In%20our%20work%2C%20we%20show%20how%20to%20train%20small%20models%20for%20such%20efficiency-critical%20applications.%20As%20opposed%20to%20many%20off-the-shelf%20NLP%20pipelines%2C%20our%20models%20use%20modern%20training%20techniques%20such%20as%20distillation%2C%20and%20offer%20support%20for%20low-resource%20languages.%20We%20call%20our%20models%20TiME%20%28Tiny%20Monolingual%20Encoders%29%20and%20comprehensively%20evaluate%20them%20on%20a%20range%20of%20common%20NLP%20tasks%2C%20observing%20an%20improved%20trade-off%20between%20benchmark%20performance%20on%20one%20hand%2C%20and%20throughput%2C%20latency%20and%20energy%20consumption%20on%20the%20other.%20Along%20the%20way%2C%20we%20show%20that%20distilling%20monolingual%20models%20from%20multilingual%20teachers%20is%20possible%2C%20and%20likewise%20distilling%20models%20with%20absolute%20positional%20embeddings%20from%20teachers%20with%20relative%20positional%20embeddings.&entry.1838667208=http%3A//arxiv.org/abs/2512.14645v1&entry.124074799=Read"},
{"title": "LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts", "author": "Prasanjit Dubey and Aritra Guha and Zhengyi Zhou and Qiong Wu and Xiaoming Huo and Paromita Dubey", "abstract": "Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.", "link": "http://arxiv.org/abs/2512.14604v1", "date": "2025-12-16", "relevancy": 2.454, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5107}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.483}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLmFPCA-detect%3A%20LLM-powered%20Multivariate%20Functional%20PCA%20for%20Anomaly%20Detection%20in%20Sparse%20Longitudinal%20Texts&body=Title%3A%20LLmFPCA-detect%3A%20LLM-powered%20Multivariate%20Functional%20PCA%20for%20Anomaly%20Detection%20in%20Sparse%20Longitudinal%20Texts%0AAuthor%3A%20Prasanjit%20Dubey%20and%20Aritra%20Guha%20and%20Zhengyi%20Zhou%20and%20Qiong%20Wu%20and%20Xiaoming%20Huo%20and%20Paromita%20Dubey%0AAbstract%3A%20Sparse%20longitudinal%20%28SL%29%20textual%20data%20arises%20when%20individuals%20generate%20text%20repeatedly%20over%20time%20%28e.g.%2C%20customer%20reviews%2C%20occasional%20social%20media%20posts%2C%20electronic%20medical%20records%20across%20visits%29%2C%20but%20the%20frequency%20and%20timing%20of%20observations%20vary%20across%20individuals.%20These%20complex%20textual%20data%20sets%20have%20immense%20potential%20to%20inform%20future%20policy%20and%20targeted%20recommendations.%20However%2C%20because%20SL%20text%20data%20lack%20dedicated%20methods%20and%20are%20noisy%2C%20heterogeneous%2C%20and%20prone%20to%20anomalies%2C%20detecting%20and%20inferring%20key%20patterns%20is%20challenging.%20We%20introduce%20LLmFPCA-detect%2C%20a%20flexible%20framework%20that%20pairs%20LLM-based%20text%20embeddings%20with%20functional%20data%20analysis%20to%20detect%20clusters%20and%20infer%20anomalies%20in%20large%20SL%20text%20datasets.%20First%2C%20LLmFPCA-detect%20embeds%20each%20piece%20of%20text%20into%20an%20application-specific%20numeric%20space%20using%20LLM%20prompts.%20Sparse%20multivariate%20functional%20principal%20component%20analysis%20%28mFPCA%29%20conducted%20in%20the%20numeric%20space%20forms%20the%20workhorse%20to%20recover%20primary%20population%20characteristics%2C%20and%20produces%20subject-level%20scores%20which%2C%20together%20with%20baseline%20static%20covariates%2C%20facilitate%20data%20segmentation%2C%20unsupervised%20anomaly%20detection%20and%20inference%2C%20and%20enable%20other%20downstream%20tasks.%20In%20particular%2C%20we%20leverage%20LLMs%20to%20perform%20dynamic%20keyword%20profiling%20guided%20by%20the%20data%20segments%20and%20anomalies%20discovered%20by%20LLmFPCA-detect%2C%20and%20we%20show%20that%20cluster-specific%20functional%20PC%20scores%20from%20LLmFPCA-detect%2C%20used%20as%20features%20in%20existing%20pipelines%2C%20help%20boost%20prediction%20performance.%20We%20support%20the%20stability%20of%20LLmFPCA-detect%20with%20experiments%20and%20evaluate%20it%20on%20two%20different%20applications%20using%20public%20datasets%2C%20Amazon%20customer-review%20trajectories%2C%20and%20Wikipedia%20talk-page%20comment%20streams%2C%20demonstrating%20utility%20across%20domains%20and%20outperforming%20state-of-the-art%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLmFPCA-detect%253A%2520LLM-powered%2520Multivariate%2520Functional%2520PCA%2520for%2520Anomaly%2520Detection%2520in%2520Sparse%2520Longitudinal%2520Texts%26entry.906535625%3DPrasanjit%2520Dubey%2520and%2520Aritra%2520Guha%2520and%2520Zhengyi%2520Zhou%2520and%2520Qiong%2520Wu%2520and%2520Xiaoming%2520Huo%2520and%2520Paromita%2520Dubey%26entry.1292438233%3DSparse%2520longitudinal%2520%2528SL%2529%2520textual%2520data%2520arises%2520when%2520individuals%2520generate%2520text%2520repeatedly%2520over%2520time%2520%2528e.g.%252C%2520customer%2520reviews%252C%2520occasional%2520social%2520media%2520posts%252C%2520electronic%2520medical%2520records%2520across%2520visits%2529%252C%2520but%2520the%2520frequency%2520and%2520timing%2520of%2520observations%2520vary%2520across%2520individuals.%2520These%2520complex%2520textual%2520data%2520sets%2520have%2520immense%2520potential%2520to%2520inform%2520future%2520policy%2520and%2520targeted%2520recommendations.%2520However%252C%2520because%2520SL%2520text%2520data%2520lack%2520dedicated%2520methods%2520and%2520are%2520noisy%252C%2520heterogeneous%252C%2520and%2520prone%2520to%2520anomalies%252C%2520detecting%2520and%2520inferring%2520key%2520patterns%2520is%2520challenging.%2520We%2520introduce%2520LLmFPCA-detect%252C%2520a%2520flexible%2520framework%2520that%2520pairs%2520LLM-based%2520text%2520embeddings%2520with%2520functional%2520data%2520analysis%2520to%2520detect%2520clusters%2520and%2520infer%2520anomalies%2520in%2520large%2520SL%2520text%2520datasets.%2520First%252C%2520LLmFPCA-detect%2520embeds%2520each%2520piece%2520of%2520text%2520into%2520an%2520application-specific%2520numeric%2520space%2520using%2520LLM%2520prompts.%2520Sparse%2520multivariate%2520functional%2520principal%2520component%2520analysis%2520%2528mFPCA%2529%2520conducted%2520in%2520the%2520numeric%2520space%2520forms%2520the%2520workhorse%2520to%2520recover%2520primary%2520population%2520characteristics%252C%2520and%2520produces%2520subject-level%2520scores%2520which%252C%2520together%2520with%2520baseline%2520static%2520covariates%252C%2520facilitate%2520data%2520segmentation%252C%2520unsupervised%2520anomaly%2520detection%2520and%2520inference%252C%2520and%2520enable%2520other%2520downstream%2520tasks.%2520In%2520particular%252C%2520we%2520leverage%2520LLMs%2520to%2520perform%2520dynamic%2520keyword%2520profiling%2520guided%2520by%2520the%2520data%2520segments%2520and%2520anomalies%2520discovered%2520by%2520LLmFPCA-detect%252C%2520and%2520we%2520show%2520that%2520cluster-specific%2520functional%2520PC%2520scores%2520from%2520LLmFPCA-detect%252C%2520used%2520as%2520features%2520in%2520existing%2520pipelines%252C%2520help%2520boost%2520prediction%2520performance.%2520We%2520support%2520the%2520stability%2520of%2520LLmFPCA-detect%2520with%2520experiments%2520and%2520evaluate%2520it%2520on%2520two%2520different%2520applications%2520using%2520public%2520datasets%252C%2520Amazon%2520customer-review%2520trajectories%252C%2520and%2520Wikipedia%2520talk-page%2520comment%2520streams%252C%2520demonstrating%2520utility%2520across%2520domains%2520and%2520outperforming%2520state-of-the-art%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLmFPCA-detect%3A%20LLM-powered%20Multivariate%20Functional%20PCA%20for%20Anomaly%20Detection%20in%20Sparse%20Longitudinal%20Texts&entry.906535625=Prasanjit%20Dubey%20and%20Aritra%20Guha%20and%20Zhengyi%20Zhou%20and%20Qiong%20Wu%20and%20Xiaoming%20Huo%20and%20Paromita%20Dubey&entry.1292438233=Sparse%20longitudinal%20%28SL%29%20textual%20data%20arises%20when%20individuals%20generate%20text%20repeatedly%20over%20time%20%28e.g.%2C%20customer%20reviews%2C%20occasional%20social%20media%20posts%2C%20electronic%20medical%20records%20across%20visits%29%2C%20but%20the%20frequency%20and%20timing%20of%20observations%20vary%20across%20individuals.%20These%20complex%20textual%20data%20sets%20have%20immense%20potential%20to%20inform%20future%20policy%20and%20targeted%20recommendations.%20However%2C%20because%20SL%20text%20data%20lack%20dedicated%20methods%20and%20are%20noisy%2C%20heterogeneous%2C%20and%20prone%20to%20anomalies%2C%20detecting%20and%20inferring%20key%20patterns%20is%20challenging.%20We%20introduce%20LLmFPCA-detect%2C%20a%20flexible%20framework%20that%20pairs%20LLM-based%20text%20embeddings%20with%20functional%20data%20analysis%20to%20detect%20clusters%20and%20infer%20anomalies%20in%20large%20SL%20text%20datasets.%20First%2C%20LLmFPCA-detect%20embeds%20each%20piece%20of%20text%20into%20an%20application-specific%20numeric%20space%20using%20LLM%20prompts.%20Sparse%20multivariate%20functional%20principal%20component%20analysis%20%28mFPCA%29%20conducted%20in%20the%20numeric%20space%20forms%20the%20workhorse%20to%20recover%20primary%20population%20characteristics%2C%20and%20produces%20subject-level%20scores%20which%2C%20together%20with%20baseline%20static%20covariates%2C%20facilitate%20data%20segmentation%2C%20unsupervised%20anomaly%20detection%20and%20inference%2C%20and%20enable%20other%20downstream%20tasks.%20In%20particular%2C%20we%20leverage%20LLMs%20to%20perform%20dynamic%20keyword%20profiling%20guided%20by%20the%20data%20segments%20and%20anomalies%20discovered%20by%20LLmFPCA-detect%2C%20and%20we%20show%20that%20cluster-specific%20functional%20PC%20scores%20from%20LLmFPCA-detect%2C%20used%20as%20features%20in%20existing%20pipelines%2C%20help%20boost%20prediction%20performance.%20We%20support%20the%20stability%20of%20LLmFPCA-detect%20with%20experiments%20and%20evaluate%20it%20on%20two%20different%20applications%20using%20public%20datasets%2C%20Amazon%20customer-review%20trajectories%2C%20and%20Wikipedia%20talk-page%20comment%20streams%2C%20demonstrating%20utility%20across%20domains%20and%20outperforming%20state-of-the-art%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.14604v1&entry.124074799=Read"},
{"title": "CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer", "author": "Xianwei Cao and Dou Quan and Shuang Wang and Ning Huyan and Wei Wang and Yunan Li and Licheng Jiao", "abstract": "Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.", "link": "http://arxiv.org/abs/2512.14560v1", "date": "2025-12-16", "relevancy": 2.4504, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6612}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5838}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLNet%3A%20Cross-View%20Correspondence%20Makes%20a%20Stronger%20Geo-Localizationer&body=Title%3A%20CLNet%3A%20Cross-View%20Correspondence%20Makes%20a%20Stronger%20Geo-Localizationer%0AAuthor%3A%20Xianwei%20Cao%20and%20Dou%20Quan%20and%20Shuang%20Wang%20and%20Ning%20Huyan%20and%20Wei%20Wang%20and%20Yunan%20Li%20and%20Licheng%20Jiao%0AAbstract%3A%20Image%20retrieval-based%20cross-view%20geo-localization%20%28IRCVGL%29%20aims%20to%20match%20images%20captured%20from%20significantly%20different%20viewpoints%2C%20such%20as%20satellite%20and%20street-level%20images.%20Existing%20methods%20predominantly%20rely%20on%20learning%20robust%20global%20representations%20or%20implicit%20feature%20alignment%2C%20which%20often%20fail%20to%20model%20explicit%20spatial%20correspondences%20crucial%20for%20accurate%20localization.%20In%20this%20work%2C%20we%20propose%20a%20novel%20correspondence-aware%20feature%20refinement%20framework%2C%20termed%20CLNet%2C%20that%20explicitly%20bridges%20the%20semantic%20and%20geometric%20gaps%20between%20different%20views.%20CLNet%20decomposes%20the%20view%20alignment%20process%20into%20three%20learnable%20and%20complementary%20modules%3A%20a%20Neural%20Correspondence%20Map%20%28NCM%29%20that%20spatially%20aligns%20cross-view%20features%20via%20latent%20correspondence%20fields%3B%20a%20Nonlinear%20Embedding%20Converter%20%28NEC%29%20that%20remaps%20features%20across%20perspectives%20using%20an%20MLP-based%20transformation%3B%20and%20a%20Global%20Feature%20Recalibration%20%28GFR%29%20module%20that%20reweights%20informative%20feature%20channels%20guided%20by%20learned%20spatial%20cues.%20The%20proposed%20CLNet%20can%20jointly%20capture%20both%20high-level%20semantics%20and%20fine-grained%20alignments.%20Extensive%20experiments%20on%20four%20public%20benchmarks%2C%20CVUSA%2C%20CVACT%2C%20VIGOR%2C%20and%20University-1652%2C%20demonstrate%20that%20our%20proposed%20CLNet%20achieves%20state-of-the-art%20performance%20while%20offering%20better%20interpretability%20and%20generalizability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLNet%253A%2520Cross-View%2520Correspondence%2520Makes%2520a%2520Stronger%2520Geo-Localizationer%26entry.906535625%3DXianwei%2520Cao%2520and%2520Dou%2520Quan%2520and%2520Shuang%2520Wang%2520and%2520Ning%2520Huyan%2520and%2520Wei%2520Wang%2520and%2520Yunan%2520Li%2520and%2520Licheng%2520Jiao%26entry.1292438233%3DImage%2520retrieval-based%2520cross-view%2520geo-localization%2520%2528IRCVGL%2529%2520aims%2520to%2520match%2520images%2520captured%2520from%2520significantly%2520different%2520viewpoints%252C%2520such%2520as%2520satellite%2520and%2520street-level%2520images.%2520Existing%2520methods%2520predominantly%2520rely%2520on%2520learning%2520robust%2520global%2520representations%2520or%2520implicit%2520feature%2520alignment%252C%2520which%2520often%2520fail%2520to%2520model%2520explicit%2520spatial%2520correspondences%2520crucial%2520for%2520accurate%2520localization.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520correspondence-aware%2520feature%2520refinement%2520framework%252C%2520termed%2520CLNet%252C%2520that%2520explicitly%2520bridges%2520the%2520semantic%2520and%2520geometric%2520gaps%2520between%2520different%2520views.%2520CLNet%2520decomposes%2520the%2520view%2520alignment%2520process%2520into%2520three%2520learnable%2520and%2520complementary%2520modules%253A%2520a%2520Neural%2520Correspondence%2520Map%2520%2528NCM%2529%2520that%2520spatially%2520aligns%2520cross-view%2520features%2520via%2520latent%2520correspondence%2520fields%253B%2520a%2520Nonlinear%2520Embedding%2520Converter%2520%2528NEC%2529%2520that%2520remaps%2520features%2520across%2520perspectives%2520using%2520an%2520MLP-based%2520transformation%253B%2520and%2520a%2520Global%2520Feature%2520Recalibration%2520%2528GFR%2529%2520module%2520that%2520reweights%2520informative%2520feature%2520channels%2520guided%2520by%2520learned%2520spatial%2520cues.%2520The%2520proposed%2520CLNet%2520can%2520jointly%2520capture%2520both%2520high-level%2520semantics%2520and%2520fine-grained%2520alignments.%2520Extensive%2520experiments%2520on%2520four%2520public%2520benchmarks%252C%2520CVUSA%252C%2520CVACT%252C%2520VIGOR%252C%2520and%2520University-1652%252C%2520demonstrate%2520that%2520our%2520proposed%2520CLNet%2520achieves%2520state-of-the-art%2520performance%2520while%2520offering%2520better%2520interpretability%2520and%2520generalizability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLNet%3A%20Cross-View%20Correspondence%20Makes%20a%20Stronger%20Geo-Localizationer&entry.906535625=Xianwei%20Cao%20and%20Dou%20Quan%20and%20Shuang%20Wang%20and%20Ning%20Huyan%20and%20Wei%20Wang%20and%20Yunan%20Li%20and%20Licheng%20Jiao&entry.1292438233=Image%20retrieval-based%20cross-view%20geo-localization%20%28IRCVGL%29%20aims%20to%20match%20images%20captured%20from%20significantly%20different%20viewpoints%2C%20such%20as%20satellite%20and%20street-level%20images.%20Existing%20methods%20predominantly%20rely%20on%20learning%20robust%20global%20representations%20or%20implicit%20feature%20alignment%2C%20which%20often%20fail%20to%20model%20explicit%20spatial%20correspondences%20crucial%20for%20accurate%20localization.%20In%20this%20work%2C%20we%20propose%20a%20novel%20correspondence-aware%20feature%20refinement%20framework%2C%20termed%20CLNet%2C%20that%20explicitly%20bridges%20the%20semantic%20and%20geometric%20gaps%20between%20different%20views.%20CLNet%20decomposes%20the%20view%20alignment%20process%20into%20three%20learnable%20and%20complementary%20modules%3A%20a%20Neural%20Correspondence%20Map%20%28NCM%29%20that%20spatially%20aligns%20cross-view%20features%20via%20latent%20correspondence%20fields%3B%20a%20Nonlinear%20Embedding%20Converter%20%28NEC%29%20that%20remaps%20features%20across%20perspectives%20using%20an%20MLP-based%20transformation%3B%20and%20a%20Global%20Feature%20Recalibration%20%28GFR%29%20module%20that%20reweights%20informative%20feature%20channels%20guided%20by%20learned%20spatial%20cues.%20The%20proposed%20CLNet%20can%20jointly%20capture%20both%20high-level%20semantics%20and%20fine-grained%20alignments.%20Extensive%20experiments%20on%20four%20public%20benchmarks%2C%20CVUSA%2C%20CVACT%2C%20VIGOR%2C%20and%20University-1652%2C%20demonstrate%20that%20our%20proposed%20CLNet%20achieves%20state-of-the-art%20performance%20while%20offering%20better%20interpretability%20and%20generalizability.&entry.1838667208=http%3A//arxiv.org/abs/2512.14560v1&entry.124074799=Read"},
{"title": "SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition", "author": "Alessia Micieli and Giovanni Maria Farinella and Francesco Ragusa", "abstract": "In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.", "link": "http://arxiv.org/abs/2512.14489v1", "date": "2025-12-16", "relevancy": 2.4476, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SignIT%3A%20A%20Comprehensive%20Dataset%20and%20Multimodal%20Analysis%20for%20Italian%20Sign%20Language%20Recognition&body=Title%3A%20SignIT%3A%20A%20Comprehensive%20Dataset%20and%20Multimodal%20Analysis%20for%20Italian%20Sign%20Language%20Recognition%0AAuthor%3A%20Alessia%20Micieli%20and%20Giovanni%20Maria%20Farinella%20and%20Francesco%20Ragusa%0AAbstract%3A%20In%20this%20work%20we%20present%20SignIT%2C%20a%20new%20dataset%20to%20study%20the%20task%20of%20Italian%20Sign%20Language%20%28LIS%29%20recognition.%20The%20dataset%20is%20composed%20of%20644%20videos%20covering%203.33%20hours.%20We%20manually%20annotated%20videos%20considering%20a%20taxonomy%20of%2094%20distinct%20sign%20classes%20belonging%20to%205%20macro-categories%3A%20Animals%2C%20Food%2C%20Colors%2C%20Emotions%20and%20Family.%20We%20also%20extracted%202D%20keypoints%20related%20to%20the%20hands%2C%20face%20and%20body%20of%20the%20users.%20With%20the%20dataset%2C%20we%20propose%20a%20benchmark%20for%20the%20sign%20recognition%20task%2C%20adopting%20several%20state-of-the-art%20models%20showing%20how%20temporal%20information%2C%202D%20keypoints%20and%20RGB%20frames%20can%20be%20influence%20the%20performance%20of%20these%20models.%20Results%20show%20the%20limitations%20of%20these%20models%20on%20this%20challenging%20LIS%20dataset.%20We%20release%20data%20and%20annotations%20at%20the%20following%20link%3A%20https%3A//fpv-iplab.github.io/SignIT/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignIT%253A%2520A%2520Comprehensive%2520Dataset%2520and%2520Multimodal%2520Analysis%2520for%2520Italian%2520Sign%2520Language%2520Recognition%26entry.906535625%3DAlessia%2520Micieli%2520and%2520Giovanni%2520Maria%2520Farinella%2520and%2520Francesco%2520Ragusa%26entry.1292438233%3DIn%2520this%2520work%2520we%2520present%2520SignIT%252C%2520a%2520new%2520dataset%2520to%2520study%2520the%2520task%2520of%2520Italian%2520Sign%2520Language%2520%2528LIS%2529%2520recognition.%2520The%2520dataset%2520is%2520composed%2520of%2520644%2520videos%2520covering%25203.33%2520hours.%2520We%2520manually%2520annotated%2520videos%2520considering%2520a%2520taxonomy%2520of%252094%2520distinct%2520sign%2520classes%2520belonging%2520to%25205%2520macro-categories%253A%2520Animals%252C%2520Food%252C%2520Colors%252C%2520Emotions%2520and%2520Family.%2520We%2520also%2520extracted%25202D%2520keypoints%2520related%2520to%2520the%2520hands%252C%2520face%2520and%2520body%2520of%2520the%2520users.%2520With%2520the%2520dataset%252C%2520we%2520propose%2520a%2520benchmark%2520for%2520the%2520sign%2520recognition%2520task%252C%2520adopting%2520several%2520state-of-the-art%2520models%2520showing%2520how%2520temporal%2520information%252C%25202D%2520keypoints%2520and%2520RGB%2520frames%2520can%2520be%2520influence%2520the%2520performance%2520of%2520these%2520models.%2520Results%2520show%2520the%2520limitations%2520of%2520these%2520models%2520on%2520this%2520challenging%2520LIS%2520dataset.%2520We%2520release%2520data%2520and%2520annotations%2520at%2520the%2520following%2520link%253A%2520https%253A//fpv-iplab.github.io/SignIT/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SignIT%3A%20A%20Comprehensive%20Dataset%20and%20Multimodal%20Analysis%20for%20Italian%20Sign%20Language%20Recognition&entry.906535625=Alessia%20Micieli%20and%20Giovanni%20Maria%20Farinella%20and%20Francesco%20Ragusa&entry.1292438233=In%20this%20work%20we%20present%20SignIT%2C%20a%20new%20dataset%20to%20study%20the%20task%20of%20Italian%20Sign%20Language%20%28LIS%29%20recognition.%20The%20dataset%20is%20composed%20of%20644%20videos%20covering%203.33%20hours.%20We%20manually%20annotated%20videos%20considering%20a%20taxonomy%20of%2094%20distinct%20sign%20classes%20belonging%20to%205%20macro-categories%3A%20Animals%2C%20Food%2C%20Colors%2C%20Emotions%20and%20Family.%20We%20also%20extracted%202D%20keypoints%20related%20to%20the%20hands%2C%20face%20and%20body%20of%20the%20users.%20With%20the%20dataset%2C%20we%20propose%20a%20benchmark%20for%20the%20sign%20recognition%20task%2C%20adopting%20several%20state-of-the-art%20models%20showing%20how%20temporal%20information%2C%202D%20keypoints%20and%20RGB%20frames%20can%20be%20influence%20the%20performance%20of%20these%20models.%20Results%20show%20the%20limitations%20of%20these%20models%20on%20this%20challenging%20LIS%20dataset.%20We%20release%20data%20and%20annotations%20at%20the%20following%20link%3A%20https%3A//fpv-iplab.github.io/SignIT/.&entry.1838667208=http%3A//arxiv.org/abs/2512.14489v1&entry.124074799=Read"},
{"title": "Renal Cell Carcinoma subtyping: learning from multi-resolution localization", "author": "Mohamad Mohamad and Francesco Ponzio and Santa Di Cataldo and Damien Ambrosetti and Xavier Descombes", "abstract": "Renal Cell Carcinoma is typically asymptomatic at the early stages for many patients. This leads to a late diagnosis of the tumor, where the curability likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high, with respect to its incidence rate. To increase the survival chance, a fast and correct categorization of the tumor subtype is paramount. Nowadays, computerized methods, based on artificial intelligence, represent an interesting opportunity to improve the productivity and the objectivity of the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their exploitation is hampered by the paucity of annotated dataset, essential for a proficient training of supervised machine learning technologies. This study sets out to investigate a novel self supervised training strategy for machine learning diagnostic tools, based on the multi-resolution nature of the histological samples. We aim at reducing the need of annotated dataset, without significantly reducing the accuracy of the tool. We demonstrate the classification capability of our tool on a whole slide imaging dataset for Renal Cancer subtyping, and we compare our solution with several state-of-the-art classification counterparts.", "link": "http://arxiv.org/abs/2411.09471v2", "date": "2025-12-16", "relevancy": 2.4441, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4924}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Renal%20Cell%20Carcinoma%20subtyping%3A%20learning%20from%20multi-resolution%20localization&body=Title%3A%20Renal%20Cell%20Carcinoma%20subtyping%3A%20learning%20from%20multi-resolution%20localization%0AAuthor%3A%20Mohamad%20Mohamad%20and%20Francesco%20Ponzio%20and%20Santa%20Di%20Cataldo%20and%20Damien%20Ambrosetti%20and%20Xavier%20Descombes%0AAbstract%3A%20Renal%20Cell%20Carcinoma%20is%20typically%20asymptomatic%20at%20the%20early%20stages%20for%20many%20patients.%20This%20leads%20to%20a%20late%20diagnosis%20of%20the%20tumor%2C%20where%20the%20curability%20likelihood%20is%20lower%2C%20and%20makes%20the%20mortality%20rate%20of%20Renal%20Cell%20Carcinoma%20high%2C%20with%20respect%20to%20its%20incidence%20rate.%20To%20increase%20the%20survival%20chance%2C%20a%20fast%20and%20correct%20categorization%20of%20the%20tumor%20subtype%20is%20paramount.%20Nowadays%2C%20computerized%20methods%2C%20based%20on%20artificial%20intelligence%2C%20represent%20an%20interesting%20opportunity%20to%20improve%20the%20productivity%20and%20the%20objectivity%20of%20the%20microscopy-based%20Renal%20Cell%20Carcinoma%20diagnosis.%20Nonetheless%2C%20much%20of%20their%20exploitation%20is%20hampered%20by%20the%20paucity%20of%20annotated%20dataset%2C%20essential%20for%20a%20proficient%20training%20of%20supervised%20machine%20learning%20technologies.%20This%20study%20sets%20out%20to%20investigate%20a%20novel%20self%20supervised%20training%20strategy%20for%20machine%20learning%20diagnostic%20tools%2C%20based%20on%20the%20multi-resolution%20nature%20of%20the%20histological%20samples.%20We%20aim%20at%20reducing%20the%20need%20of%20annotated%20dataset%2C%20without%20significantly%20reducing%20the%20accuracy%20of%20the%20tool.%20We%20demonstrate%20the%20classification%20capability%20of%20our%20tool%20on%20a%20whole%20slide%20imaging%20dataset%20for%20Renal%20Cancer%20subtyping%2C%20and%20we%20compare%20our%20solution%20with%20several%20state-of-the-art%20classification%20counterparts.%0ALink%3A%20http%3A//arxiv.org/abs/2411.09471v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRenal%2520Cell%2520Carcinoma%2520subtyping%253A%2520learning%2520from%2520multi-resolution%2520localization%26entry.906535625%3DMohamad%2520Mohamad%2520and%2520Francesco%2520Ponzio%2520and%2520Santa%2520Di%2520Cataldo%2520and%2520Damien%2520Ambrosetti%2520and%2520Xavier%2520Descombes%26entry.1292438233%3DRenal%2520Cell%2520Carcinoma%2520is%2520typically%2520asymptomatic%2520at%2520the%2520early%2520stages%2520for%2520many%2520patients.%2520This%2520leads%2520to%2520a%2520late%2520diagnosis%2520of%2520the%2520tumor%252C%2520where%2520the%2520curability%2520likelihood%2520is%2520lower%252C%2520and%2520makes%2520the%2520mortality%2520rate%2520of%2520Renal%2520Cell%2520Carcinoma%2520high%252C%2520with%2520respect%2520to%2520its%2520incidence%2520rate.%2520To%2520increase%2520the%2520survival%2520chance%252C%2520a%2520fast%2520and%2520correct%2520categorization%2520of%2520the%2520tumor%2520subtype%2520is%2520paramount.%2520Nowadays%252C%2520computerized%2520methods%252C%2520based%2520on%2520artificial%2520intelligence%252C%2520represent%2520an%2520interesting%2520opportunity%2520to%2520improve%2520the%2520productivity%2520and%2520the%2520objectivity%2520of%2520the%2520microscopy-based%2520Renal%2520Cell%2520Carcinoma%2520diagnosis.%2520Nonetheless%252C%2520much%2520of%2520their%2520exploitation%2520is%2520hampered%2520by%2520the%2520paucity%2520of%2520annotated%2520dataset%252C%2520essential%2520for%2520a%2520proficient%2520training%2520of%2520supervised%2520machine%2520learning%2520technologies.%2520This%2520study%2520sets%2520out%2520to%2520investigate%2520a%2520novel%2520self%2520supervised%2520training%2520strategy%2520for%2520machine%2520learning%2520diagnostic%2520tools%252C%2520based%2520on%2520the%2520multi-resolution%2520nature%2520of%2520the%2520histological%2520samples.%2520We%2520aim%2520at%2520reducing%2520the%2520need%2520of%2520annotated%2520dataset%252C%2520without%2520significantly%2520reducing%2520the%2520accuracy%2520of%2520the%2520tool.%2520We%2520demonstrate%2520the%2520classification%2520capability%2520of%2520our%2520tool%2520on%2520a%2520whole%2520slide%2520imaging%2520dataset%2520for%2520Renal%2520Cancer%2520subtyping%252C%2520and%2520we%2520compare%2520our%2520solution%2520with%2520several%2520state-of-the-art%2520classification%2520counterparts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09471v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Renal%20Cell%20Carcinoma%20subtyping%3A%20learning%20from%20multi-resolution%20localization&entry.906535625=Mohamad%20Mohamad%20and%20Francesco%20Ponzio%20and%20Santa%20Di%20Cataldo%20and%20Damien%20Ambrosetti%20and%20Xavier%20Descombes&entry.1292438233=Renal%20Cell%20Carcinoma%20is%20typically%20asymptomatic%20at%20the%20early%20stages%20for%20many%20patients.%20This%20leads%20to%20a%20late%20diagnosis%20of%20the%20tumor%2C%20where%20the%20curability%20likelihood%20is%20lower%2C%20and%20makes%20the%20mortality%20rate%20of%20Renal%20Cell%20Carcinoma%20high%2C%20with%20respect%20to%20its%20incidence%20rate.%20To%20increase%20the%20survival%20chance%2C%20a%20fast%20and%20correct%20categorization%20of%20the%20tumor%20subtype%20is%20paramount.%20Nowadays%2C%20computerized%20methods%2C%20based%20on%20artificial%20intelligence%2C%20represent%20an%20interesting%20opportunity%20to%20improve%20the%20productivity%20and%20the%20objectivity%20of%20the%20microscopy-based%20Renal%20Cell%20Carcinoma%20diagnosis.%20Nonetheless%2C%20much%20of%20their%20exploitation%20is%20hampered%20by%20the%20paucity%20of%20annotated%20dataset%2C%20essential%20for%20a%20proficient%20training%20of%20supervised%20machine%20learning%20technologies.%20This%20study%20sets%20out%20to%20investigate%20a%20novel%20self%20supervised%20training%20strategy%20for%20machine%20learning%20diagnostic%20tools%2C%20based%20on%20the%20multi-resolution%20nature%20of%20the%20histological%20samples.%20We%20aim%20at%20reducing%20the%20need%20of%20annotated%20dataset%2C%20without%20significantly%20reducing%20the%20accuracy%20of%20the%20tool.%20We%20demonstrate%20the%20classification%20capability%20of%20our%20tool%20on%20a%20whole%20slide%20imaging%20dataset%20for%20Renal%20Cancer%20subtyping%2C%20and%20we%20compare%20our%20solution%20with%20several%20state-of-the-art%20classification%20counterparts.&entry.1838667208=http%3A//arxiv.org/abs/2411.09471v2&entry.124074799=Read"},
{"title": "Language Self-Play For Data-Free Training", "author": "Jakub Grudzien Kuba and Mengting Gu and Qi Ma and Yuandong Tian and Vijai Mohan and Jason Chen", "abstract": "Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself-a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following, mathematics, and coding benchmarks show that pretrained models can be effectively improved with self-play alone.", "link": "http://arxiv.org/abs/2509.07414v2", "date": "2025-12-16", "relevancy": 2.4231, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4926}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4857}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Self-Play%20For%20Data-Free%20Training&body=Title%3A%20Language%20Self-Play%20For%20Data-Free%20Training%0AAuthor%3A%20Jakub%20Grudzien%20Kuba%20and%20Mengting%20Gu%20and%20Qi%20Ma%20and%20Yuandong%20Tian%20and%20Vijai%20Mohan%20and%20Jason%20Chen%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20advanced%20rapidly%20in%20recent%20years%2C%20driven%20by%20scale%2C%20abundant%20high-quality%20training%20data%2C%20and%20reinforcement%20learning.%20Yet%20this%20progress%20faces%20a%20fundamental%20bottleneck%3A%20the%20need%20for%20ever%20more%20data%20from%20which%20models%20can%20continue%20to%20learn.%20In%20this%20work%2C%20we%20propose%20a%20reinforcement%20learning%20approach%20that%20removes%20this%20dependency%20by%20enabling%20models%20to%20improve%20without%20additional%20data.%20Our%20method%20leverages%20a%20game-theoretic%20framework%20of%20self-play%2C%20where%20a%20model%27s%20capabilities%20are%20cast%20as%20performance%20in%20a%20competitive%20game%20and%20stronger%20policies%20emerge%20by%20having%20the%20model%20play%20against%20itself-a%20process%20we%20call%20Language%20Self-Play%20%28LSP%29.%20Experiments%20with%20Llama-3.2-3B-Instruct%20on%20instruction-following%2C%20mathematics%2C%20and%20coding%20benchmarks%20show%20that%20pretrained%20models%20can%20be%20effectively%20improved%20with%20self-play%20alone.%0ALink%3A%20http%3A//arxiv.org/abs/2509.07414v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Self-Play%2520For%2520Data-Free%2520Training%26entry.906535625%3DJakub%2520Grudzien%2520Kuba%2520and%2520Mengting%2520Gu%2520and%2520Qi%2520Ma%2520and%2520Yuandong%2520Tian%2520and%2520Vijai%2520Mohan%2520and%2520Jason%2520Chen%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520advanced%2520rapidly%2520in%2520recent%2520years%252C%2520driven%2520by%2520scale%252C%2520abundant%2520high-quality%2520training%2520data%252C%2520and%2520reinforcement%2520learning.%2520Yet%2520this%2520progress%2520faces%2520a%2520fundamental%2520bottleneck%253A%2520the%2520need%2520for%2520ever%2520more%2520data%2520from%2520which%2520models%2520can%2520continue%2520to%2520learn.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520reinforcement%2520learning%2520approach%2520that%2520removes%2520this%2520dependency%2520by%2520enabling%2520models%2520to%2520improve%2520without%2520additional%2520data.%2520Our%2520method%2520leverages%2520a%2520game-theoretic%2520framework%2520of%2520self-play%252C%2520where%2520a%2520model%2527s%2520capabilities%2520are%2520cast%2520as%2520performance%2520in%2520a%2520competitive%2520game%2520and%2520stronger%2520policies%2520emerge%2520by%2520having%2520the%2520model%2520play%2520against%2520itself-a%2520process%2520we%2520call%2520Language%2520Self-Play%2520%2528LSP%2529.%2520Experiments%2520with%2520Llama-3.2-3B-Instruct%2520on%2520instruction-following%252C%2520mathematics%252C%2520and%2520coding%2520benchmarks%2520show%2520that%2520pretrained%2520models%2520can%2520be%2520effectively%2520improved%2520with%2520self-play%2520alone.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.07414v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Self-Play%20For%20Data-Free%20Training&entry.906535625=Jakub%20Grudzien%20Kuba%20and%20Mengting%20Gu%20and%20Qi%20Ma%20and%20Yuandong%20Tian%20and%20Vijai%20Mohan%20and%20Jason%20Chen&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20advanced%20rapidly%20in%20recent%20years%2C%20driven%20by%20scale%2C%20abundant%20high-quality%20training%20data%2C%20and%20reinforcement%20learning.%20Yet%20this%20progress%20faces%20a%20fundamental%20bottleneck%3A%20the%20need%20for%20ever%20more%20data%20from%20which%20models%20can%20continue%20to%20learn.%20In%20this%20work%2C%20we%20propose%20a%20reinforcement%20learning%20approach%20that%20removes%20this%20dependency%20by%20enabling%20models%20to%20improve%20without%20additional%20data.%20Our%20method%20leverages%20a%20game-theoretic%20framework%20of%20self-play%2C%20where%20a%20model%27s%20capabilities%20are%20cast%20as%20performance%20in%20a%20competitive%20game%20and%20stronger%20policies%20emerge%20by%20having%20the%20model%20play%20against%20itself-a%20process%20we%20call%20Language%20Self-Play%20%28LSP%29.%20Experiments%20with%20Llama-3.2-3B-Instruct%20on%20instruction-following%2C%20mathematics%2C%20and%20coding%20benchmarks%20show%20that%20pretrained%20models%20can%20be%20effectively%20improved%20with%20self-play%20alone.&entry.1838667208=http%3A//arxiv.org/abs/2509.07414v2&entry.124074799=Read"},
{"title": "Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset", "author": "Waqas Ahmed", "abstract": "The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.", "link": "http://arxiv.org/abs/2512.14422v1", "date": "2025-12-16", "relevancy": 2.4072, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4929}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4829}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Ensemble%20Method%20for%20Detecting%20Cyber-Attacks%20in%20Water%20Distribution%20Systems%20Using%20the%20BATADAL%20Dataset&body=Title%3A%20Hybrid%20Ensemble%20Method%20for%20Detecting%20Cyber-Attacks%20in%20Water%20Distribution%20Systems%20Using%20the%20BATADAL%20Dataset%0AAuthor%3A%20Waqas%20Ahmed%0AAbstract%3A%20The%20cybersecurity%20of%20Industrial%20Control%20Systems%20that%20manage%20critical%20infrastructure%20such%20as%20Water%20Distribution%20Systems%20has%20become%20increasingly%20important%20as%20digital%20connectivity%20expands.%20BATADAL%20benchmark%20data%20is%20a%20good%20source%20of%20testing%20intrusion%20detection%20techniques%2C%20but%20it%20presents%20several%20important%20problems%2C%20such%20as%20imbalance%20in%20the%20number%20of%20classes%2C%20multivariate%20time%20dependence%2C%20and%20stealthy%20attacks.%20We%20consider%20a%20hybrid%20ensemble%20learning%20model%20that%20will%20enhance%20the%20detection%20ability%20of%20cyber-attacks%20in%20WDS%20by%20using%20the%20complementary%20capabilities%20of%20machine%20learning%20and%20deep%20learning%20models.%20Three%20base%20learners%2C%20namely%2C%20Random%20Forest%20%2C%20eXtreme%20Gradient%20Boosting%20%2C%20and%20Long%20Short-Term%20Memory%20network%2C%20have%20been%20strictly%20compared%20and%20seven%20ensemble%20types%20using%20simple%20averaged%20and%20stacked%20learning%20with%20a%20logistic%20regression%20meta-learner.%20Random%20Forest%20analysis%20identified%20top%20predictors%20turned%20into%20temporal%20and%20statistical%20features%2C%20and%20Synthetic%20Minority%20Oversampling%20Technique%20%28SMOTE%29%20was%20used%20to%20overcome%20the%20class%20imbalance%20issue.%20The%20analyics%20indicates%20that%20the%20single%20Long%20Short-Term%20Memory%20network%20model%20is%20of%20poor%20performance%20%28F1%20%3D%200.000%2C%20AUC%20%3D%200.4460%29%2C%20but%20tree-based%20models%2C%20especially%20eXtreme%20Gradient%20Boosting%2C%20perform%20well%20%28F1%20%3D%200.7470%2C%20AUC%3D0.9684%29.%20The%20hybrid%20stacked%20ensemble%20of%20Random%20Forest%20%2C%20eXtreme%20Gradient%20Boosting%20%2C%20and%20Long%20Short-Term%20Memory%20network%20scored%20the%20highest%2C%20with%20the%20attack%20class%20of%200.7205%20with%20an%20F1-score%20and%20a%20AUC%20of%200.9826%20indicating%20that%20the%20heterogeneous%20stacking%20between%20model%20precision%20and%20generalization%20can%20work.%20The%20proposed%20framework%20establishes%20a%20robust%20and%20scalable%20solution%20for%20cyber-attack%20detection%20in%20time-dependent%20industrial%20systems%2C%20integrating%20temporal%20learning%20and%20ensemble%20diversity%20to%20support%20the%20secure%20operation%20of%20critical%20infrastructure.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Ensemble%2520Method%2520for%2520Detecting%2520Cyber-Attacks%2520in%2520Water%2520Distribution%2520Systems%2520Using%2520the%2520BATADAL%2520Dataset%26entry.906535625%3DWaqas%2520Ahmed%26entry.1292438233%3DThe%2520cybersecurity%2520of%2520Industrial%2520Control%2520Systems%2520that%2520manage%2520critical%2520infrastructure%2520such%2520as%2520Water%2520Distribution%2520Systems%2520has%2520become%2520increasingly%2520important%2520as%2520digital%2520connectivity%2520expands.%2520BATADAL%2520benchmark%2520data%2520is%2520a%2520good%2520source%2520of%2520testing%2520intrusion%2520detection%2520techniques%252C%2520but%2520it%2520presents%2520several%2520important%2520problems%252C%2520such%2520as%2520imbalance%2520in%2520the%2520number%2520of%2520classes%252C%2520multivariate%2520time%2520dependence%252C%2520and%2520stealthy%2520attacks.%2520We%2520consider%2520a%2520hybrid%2520ensemble%2520learning%2520model%2520that%2520will%2520enhance%2520the%2520detection%2520ability%2520of%2520cyber-attacks%2520in%2520WDS%2520by%2520using%2520the%2520complementary%2520capabilities%2520of%2520machine%2520learning%2520and%2520deep%2520learning%2520models.%2520Three%2520base%2520learners%252C%2520namely%252C%2520Random%2520Forest%2520%252C%2520eXtreme%2520Gradient%2520Boosting%2520%252C%2520and%2520Long%2520Short-Term%2520Memory%2520network%252C%2520have%2520been%2520strictly%2520compared%2520and%2520seven%2520ensemble%2520types%2520using%2520simple%2520averaged%2520and%2520stacked%2520learning%2520with%2520a%2520logistic%2520regression%2520meta-learner.%2520Random%2520Forest%2520analysis%2520identified%2520top%2520predictors%2520turned%2520into%2520temporal%2520and%2520statistical%2520features%252C%2520and%2520Synthetic%2520Minority%2520Oversampling%2520Technique%2520%2528SMOTE%2529%2520was%2520used%2520to%2520overcome%2520the%2520class%2520imbalance%2520issue.%2520The%2520analyics%2520indicates%2520that%2520the%2520single%2520Long%2520Short-Term%2520Memory%2520network%2520model%2520is%2520of%2520poor%2520performance%2520%2528F1%2520%253D%25200.000%252C%2520AUC%2520%253D%25200.4460%2529%252C%2520but%2520tree-based%2520models%252C%2520especially%2520eXtreme%2520Gradient%2520Boosting%252C%2520perform%2520well%2520%2528F1%2520%253D%25200.7470%252C%2520AUC%253D0.9684%2529.%2520The%2520hybrid%2520stacked%2520ensemble%2520of%2520Random%2520Forest%2520%252C%2520eXtreme%2520Gradient%2520Boosting%2520%252C%2520and%2520Long%2520Short-Term%2520Memory%2520network%2520scored%2520the%2520highest%252C%2520with%2520the%2520attack%2520class%2520of%25200.7205%2520with%2520an%2520F1-score%2520and%2520a%2520AUC%2520of%25200.9826%2520indicating%2520that%2520the%2520heterogeneous%2520stacking%2520between%2520model%2520precision%2520and%2520generalization%2520can%2520work.%2520The%2520proposed%2520framework%2520establishes%2520a%2520robust%2520and%2520scalable%2520solution%2520for%2520cyber-attack%2520detection%2520in%2520time-dependent%2520industrial%2520systems%252C%2520integrating%2520temporal%2520learning%2520and%2520ensemble%2520diversity%2520to%2520support%2520the%2520secure%2520operation%2520of%2520critical%2520infrastructure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Ensemble%20Method%20for%20Detecting%20Cyber-Attacks%20in%20Water%20Distribution%20Systems%20Using%20the%20BATADAL%20Dataset&entry.906535625=Waqas%20Ahmed&entry.1292438233=The%20cybersecurity%20of%20Industrial%20Control%20Systems%20that%20manage%20critical%20infrastructure%20such%20as%20Water%20Distribution%20Systems%20has%20become%20increasingly%20important%20as%20digital%20connectivity%20expands.%20BATADAL%20benchmark%20data%20is%20a%20good%20source%20of%20testing%20intrusion%20detection%20techniques%2C%20but%20it%20presents%20several%20important%20problems%2C%20such%20as%20imbalance%20in%20the%20number%20of%20classes%2C%20multivariate%20time%20dependence%2C%20and%20stealthy%20attacks.%20We%20consider%20a%20hybrid%20ensemble%20learning%20model%20that%20will%20enhance%20the%20detection%20ability%20of%20cyber-attacks%20in%20WDS%20by%20using%20the%20complementary%20capabilities%20of%20machine%20learning%20and%20deep%20learning%20models.%20Three%20base%20learners%2C%20namely%2C%20Random%20Forest%20%2C%20eXtreme%20Gradient%20Boosting%20%2C%20and%20Long%20Short-Term%20Memory%20network%2C%20have%20been%20strictly%20compared%20and%20seven%20ensemble%20types%20using%20simple%20averaged%20and%20stacked%20learning%20with%20a%20logistic%20regression%20meta-learner.%20Random%20Forest%20analysis%20identified%20top%20predictors%20turned%20into%20temporal%20and%20statistical%20features%2C%20and%20Synthetic%20Minority%20Oversampling%20Technique%20%28SMOTE%29%20was%20used%20to%20overcome%20the%20class%20imbalance%20issue.%20The%20analyics%20indicates%20that%20the%20single%20Long%20Short-Term%20Memory%20network%20model%20is%20of%20poor%20performance%20%28F1%20%3D%200.000%2C%20AUC%20%3D%200.4460%29%2C%20but%20tree-based%20models%2C%20especially%20eXtreme%20Gradient%20Boosting%2C%20perform%20well%20%28F1%20%3D%200.7470%2C%20AUC%3D0.9684%29.%20The%20hybrid%20stacked%20ensemble%20of%20Random%20Forest%20%2C%20eXtreme%20Gradient%20Boosting%20%2C%20and%20Long%20Short-Term%20Memory%20network%20scored%20the%20highest%2C%20with%20the%20attack%20class%20of%200.7205%20with%20an%20F1-score%20and%20a%20AUC%20of%200.9826%20indicating%20that%20the%20heterogeneous%20stacking%20between%20model%20precision%20and%20generalization%20can%20work.%20The%20proposed%20framework%20establishes%20a%20robust%20and%20scalable%20solution%20for%20cyber-attack%20detection%20in%20time-dependent%20industrial%20systems%2C%20integrating%20temporal%20learning%20and%20ensemble%20diversity%20to%20support%20the%20secure%20operation%20of%20critical%20infrastructure.&entry.1838667208=http%3A//arxiv.org/abs/2512.14422v1&entry.124074799=Read"},
{"title": "A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images", "author": "Rao Muhammad Umer and Daniel Sens and Jonathan Noll and Christian Matek and Lukas Wolfseher and Rainer Spang and Ralf Huss and Johannes Raffler and Sarah Reinke and Wolfram Klapper and Katja Steiger and Kristina Schwamborn and Carsten Marr", "abstract": "Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.", "link": "http://arxiv.org/abs/2512.14640v1", "date": "2025-12-16", "relevancy": 2.3978, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multicenter%20Benchmark%20of%20Multiple%20Instance%20Learning%20Models%20for%20Lymphoma%20Subtyping%20from%20HE-stained%20Whole%20Slide%20Images&body=Title%3A%20A%20Multicenter%20Benchmark%20of%20Multiple%20Instance%20Learning%20Models%20for%20Lymphoma%20Subtyping%20from%20HE-stained%20Whole%20Slide%20Images%0AAuthor%3A%20Rao%20Muhammad%20Umer%20and%20Daniel%20Sens%20and%20Jonathan%20Noll%20and%20Christian%20Matek%20and%20Lukas%20Wolfseher%20and%20Rainer%20Spang%20and%20Ralf%20Huss%20and%20Johannes%20Raffler%20and%20Sarah%20Reinke%20and%20Wolfram%20Klapper%20and%20Katja%20Steiger%20and%20Kristina%20Schwamborn%20and%20Carsten%20Marr%0AAbstract%3A%20Timely%20and%20accurate%20lymphoma%20diagnosis%20is%20essential%20for%20guiding%20cancer%20treatment.%20Standard%20diagnostic%20practice%20combines%20hematoxylin%20and%20eosin%20%28HE%29-stained%20whole%20slide%20images%20with%20immunohistochemistry%2C%20flow%20cytometry%2C%20and%20molecular%20genetic%20tests%20to%20determine%20lymphoma%20subtypes%2C%20a%20process%20requiring%20costly%20equipment%2C%20skilled%20personnel%2C%20and%20causing%20treatment%20delays.%20Deep%20learning%20methods%20could%20assist%20pathologists%20by%20extracting%20diagnostic%20information%20from%20routinely%20available%20HE-stained%20slides%2C%20yet%20comprehensive%20benchmarks%20for%20lymphoma%20subtyping%20on%20multicenter%20data%20are%20lacking.%20In%20this%20work%2C%20we%20present%20the%20first%20multicenter%20lymphoma%20benchmarking%20dataset%20covering%20four%20common%20lymphoma%20subtypes%20and%20healthy%20control%20tissue.%20We%20systematically%20evaluate%20five%20publicly%20available%20pathology%20foundation%20models%20%28H-optimus-1%2C%20H0-mini%2C%20Virchow2%2C%20UNI2%2C%20Titan%29%20combined%20with%20attention-based%20%28AB-MIL%29%20and%20transformer-based%20%28TransMIL%29%20multiple%20instance%20learning%20aggregators%20across%20three%20magnifications%20%2810x%2C%2020x%2C%2040x%29.%20On%20in-distribution%20test%20sets%2C%20models%20achieve%20multiclass%20balanced%20accuracies%20exceeding%2080%25%20across%20all%20magnifications%2C%20with%20all%20foundation%20models%20performing%20similarly%20and%20both%20aggregation%20methods%20showing%20comparable%20results.%20The%20magnification%20study%20reveals%20that%2040x%20resolution%20is%20sufficient%2C%20with%20no%20performance%20gains%20from%20higher%20resolutions%20or%20cross-magnification%20aggregation.%20However%2C%20on%20out-of-distribution%20test%20sets%2C%20performance%20drops%20substantially%20to%20around%2060%25%2C%20highlighting%20significant%20generalization%20challenges.%20To%20advance%20the%20field%2C%20larger%20multicenter%20studies%20covering%20additional%20rare%20lymphoma%20subtypes%20are%20needed.%20We%20provide%20an%20automated%20benchmarking%20pipeline%20to%20facilitate%20such%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multicenter%2520Benchmark%2520of%2520Multiple%2520Instance%2520Learning%2520Models%2520for%2520Lymphoma%2520Subtyping%2520from%2520HE-stained%2520Whole%2520Slide%2520Images%26entry.906535625%3DRao%2520Muhammad%2520Umer%2520and%2520Daniel%2520Sens%2520and%2520Jonathan%2520Noll%2520and%2520Christian%2520Matek%2520and%2520Lukas%2520Wolfseher%2520and%2520Rainer%2520Spang%2520and%2520Ralf%2520Huss%2520and%2520Johannes%2520Raffler%2520and%2520Sarah%2520Reinke%2520and%2520Wolfram%2520Klapper%2520and%2520Katja%2520Steiger%2520and%2520Kristina%2520Schwamborn%2520and%2520Carsten%2520Marr%26entry.1292438233%3DTimely%2520and%2520accurate%2520lymphoma%2520diagnosis%2520is%2520essential%2520for%2520guiding%2520cancer%2520treatment.%2520Standard%2520diagnostic%2520practice%2520combines%2520hematoxylin%2520and%2520eosin%2520%2528HE%2529-stained%2520whole%2520slide%2520images%2520with%2520immunohistochemistry%252C%2520flow%2520cytometry%252C%2520and%2520molecular%2520genetic%2520tests%2520to%2520determine%2520lymphoma%2520subtypes%252C%2520a%2520process%2520requiring%2520costly%2520equipment%252C%2520skilled%2520personnel%252C%2520and%2520causing%2520treatment%2520delays.%2520Deep%2520learning%2520methods%2520could%2520assist%2520pathologists%2520by%2520extracting%2520diagnostic%2520information%2520from%2520routinely%2520available%2520HE-stained%2520slides%252C%2520yet%2520comprehensive%2520benchmarks%2520for%2520lymphoma%2520subtyping%2520on%2520multicenter%2520data%2520are%2520lacking.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520multicenter%2520lymphoma%2520benchmarking%2520dataset%2520covering%2520four%2520common%2520lymphoma%2520subtypes%2520and%2520healthy%2520control%2520tissue.%2520We%2520systematically%2520evaluate%2520five%2520publicly%2520available%2520pathology%2520foundation%2520models%2520%2528H-optimus-1%252C%2520H0-mini%252C%2520Virchow2%252C%2520UNI2%252C%2520Titan%2529%2520combined%2520with%2520attention-based%2520%2528AB-MIL%2529%2520and%2520transformer-based%2520%2528TransMIL%2529%2520multiple%2520instance%2520learning%2520aggregators%2520across%2520three%2520magnifications%2520%252810x%252C%252020x%252C%252040x%2529.%2520On%2520in-distribution%2520test%2520sets%252C%2520models%2520achieve%2520multiclass%2520balanced%2520accuracies%2520exceeding%252080%2525%2520across%2520all%2520magnifications%252C%2520with%2520all%2520foundation%2520models%2520performing%2520similarly%2520and%2520both%2520aggregation%2520methods%2520showing%2520comparable%2520results.%2520The%2520magnification%2520study%2520reveals%2520that%252040x%2520resolution%2520is%2520sufficient%252C%2520with%2520no%2520performance%2520gains%2520from%2520higher%2520resolutions%2520or%2520cross-magnification%2520aggregation.%2520However%252C%2520on%2520out-of-distribution%2520test%2520sets%252C%2520performance%2520drops%2520substantially%2520to%2520around%252060%2525%252C%2520highlighting%2520significant%2520generalization%2520challenges.%2520To%2520advance%2520the%2520field%252C%2520larger%2520multicenter%2520studies%2520covering%2520additional%2520rare%2520lymphoma%2520subtypes%2520are%2520needed.%2520We%2520provide%2520an%2520automated%2520benchmarking%2520pipeline%2520to%2520facilitate%2520such%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multicenter%20Benchmark%20of%20Multiple%20Instance%20Learning%20Models%20for%20Lymphoma%20Subtyping%20from%20HE-stained%20Whole%20Slide%20Images&entry.906535625=Rao%20Muhammad%20Umer%20and%20Daniel%20Sens%20and%20Jonathan%20Noll%20and%20Christian%20Matek%20and%20Lukas%20Wolfseher%20and%20Rainer%20Spang%20and%20Ralf%20Huss%20and%20Johannes%20Raffler%20and%20Sarah%20Reinke%20and%20Wolfram%20Klapper%20and%20Katja%20Steiger%20and%20Kristina%20Schwamborn%20and%20Carsten%20Marr&entry.1292438233=Timely%20and%20accurate%20lymphoma%20diagnosis%20is%20essential%20for%20guiding%20cancer%20treatment.%20Standard%20diagnostic%20practice%20combines%20hematoxylin%20and%20eosin%20%28HE%29-stained%20whole%20slide%20images%20with%20immunohistochemistry%2C%20flow%20cytometry%2C%20and%20molecular%20genetic%20tests%20to%20determine%20lymphoma%20subtypes%2C%20a%20process%20requiring%20costly%20equipment%2C%20skilled%20personnel%2C%20and%20causing%20treatment%20delays.%20Deep%20learning%20methods%20could%20assist%20pathologists%20by%20extracting%20diagnostic%20information%20from%20routinely%20available%20HE-stained%20slides%2C%20yet%20comprehensive%20benchmarks%20for%20lymphoma%20subtyping%20on%20multicenter%20data%20are%20lacking.%20In%20this%20work%2C%20we%20present%20the%20first%20multicenter%20lymphoma%20benchmarking%20dataset%20covering%20four%20common%20lymphoma%20subtypes%20and%20healthy%20control%20tissue.%20We%20systematically%20evaluate%20five%20publicly%20available%20pathology%20foundation%20models%20%28H-optimus-1%2C%20H0-mini%2C%20Virchow2%2C%20UNI2%2C%20Titan%29%20combined%20with%20attention-based%20%28AB-MIL%29%20and%20transformer-based%20%28TransMIL%29%20multiple%20instance%20learning%20aggregators%20across%20three%20magnifications%20%2810x%2C%2020x%2C%2040x%29.%20On%20in-distribution%20test%20sets%2C%20models%20achieve%20multiclass%20balanced%20accuracies%20exceeding%2080%25%20across%20all%20magnifications%2C%20with%20all%20foundation%20models%20performing%20similarly%20and%20both%20aggregation%20methods%20showing%20comparable%20results.%20The%20magnification%20study%20reveals%20that%2040x%20resolution%20is%20sufficient%2C%20with%20no%20performance%20gains%20from%20higher%20resolutions%20or%20cross-magnification%20aggregation.%20However%2C%20on%20out-of-distribution%20test%20sets%2C%20performance%20drops%20substantially%20to%20around%2060%25%2C%20highlighting%20significant%20generalization%20challenges.%20To%20advance%20the%20field%2C%20larger%20multicenter%20studies%20covering%20additional%20rare%20lymphoma%20subtypes%20are%20needed.%20We%20provide%20an%20automated%20benchmarking%20pipeline%20to%20facilitate%20such%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.14640v1&entry.124074799=Read"},
{"title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "author": "Wenqiang Sun and Haiyu Zhang and Haoyuan Wang and Junta Wu and Zehan Wang and Zhenwei Wang and Yunhong Wang and Jun Zhang and Tengfei Wang and Chunchao Guo", "abstract": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "link": "http://arxiv.org/abs/2512.14614v1", "date": "2025-12-16", "relevancy": 2.3929, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6785}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5822}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldPlay%3A%20Towards%20Long-Term%20Geometric%20Consistency%20for%20Real-Time%20Interactive%20World%20Modeling&body=Title%3A%20WorldPlay%3A%20Towards%20Long-Term%20Geometric%20Consistency%20for%20Real-Time%20Interactive%20World%20Modeling%0AAuthor%3A%20Wenqiang%20Sun%20and%20Haiyu%20Zhang%20and%20Haoyuan%20Wang%20and%20Junta%20Wu%20and%20Zehan%20Wang%20and%20Zhenwei%20Wang%20and%20Yunhong%20Wang%20and%20Jun%20Zhang%20and%20Tengfei%20Wang%20and%20Chunchao%20Guo%0AAbstract%3A%20This%20paper%20presents%20WorldPlay%2C%20a%20streaming%20video%20diffusion%20model%20that%20enables%20real-time%2C%20interactive%20world%20modeling%20with%20long-term%20geometric%20consistency%2C%20resolving%20the%20trade-off%20between%20speed%20and%20memory%20that%20limits%20current%20methods.%20WorldPlay%20draws%20power%20from%20three%20key%20innovations.%201%29%20We%20use%20a%20Dual%20Action%20Representation%20to%20enable%20robust%20action%20control%20in%20response%20to%20the%20user%27s%20keyboard%20and%20mouse%20inputs.%202%29%20To%20enforce%20long-term%20consistency%2C%20our%20Reconstituted%20Context%20Memory%20dynamically%20rebuilds%20context%20from%20past%20frames%20and%20uses%20temporal%20reframing%20to%20keep%20geometrically%20important%20but%20long-past%20frames%20accessible%2C%20effectively%20alleviating%20memory%20attenuation.%203%29%20We%20also%20propose%20Context%20Forcing%2C%20a%20novel%20distillation%20method%20designed%20for%20memory-aware%20model.%20Aligning%20memory%20context%20between%20the%20teacher%20and%20student%20preserves%20the%20student%27s%20capacity%20to%20use%20long-range%20information%2C%20enabling%20real-time%20speeds%20while%20preventing%20error%20drift.%20Taken%20together%2C%20WorldPlay%20generates%20long-horizon%20streaming%20720p%20video%20at%2024%20FPS%20with%20superior%20consistency%2C%20comparing%20favorably%20with%20existing%20techniques%20and%20showing%20strong%20generalization%20across%20diverse%20scenes.%20Project%20page%20and%20online%20demo%20can%20be%20found%3A%20https%3A//3d-models.hunyuan.tencent.com/world/%20and%20https%3A//3d.hunyuan.tencent.com/sceneTo3D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldPlay%253A%2520Towards%2520Long-Term%2520Geometric%2520Consistency%2520for%2520Real-Time%2520Interactive%2520World%2520Modeling%26entry.906535625%3DWenqiang%2520Sun%2520and%2520Haiyu%2520Zhang%2520and%2520Haoyuan%2520Wang%2520and%2520Junta%2520Wu%2520and%2520Zehan%2520Wang%2520and%2520Zhenwei%2520Wang%2520and%2520Yunhong%2520Wang%2520and%2520Jun%2520Zhang%2520and%2520Tengfei%2520Wang%2520and%2520Chunchao%2520Guo%26entry.1292438233%3DThis%2520paper%2520presents%2520WorldPlay%252C%2520a%2520streaming%2520video%2520diffusion%2520model%2520that%2520enables%2520real-time%252C%2520interactive%2520world%2520modeling%2520with%2520long-term%2520geometric%2520consistency%252C%2520resolving%2520the%2520trade-off%2520between%2520speed%2520and%2520memory%2520that%2520limits%2520current%2520methods.%2520WorldPlay%2520draws%2520power%2520from%2520three%2520key%2520innovations.%25201%2529%2520We%2520use%2520a%2520Dual%2520Action%2520Representation%2520to%2520enable%2520robust%2520action%2520control%2520in%2520response%2520to%2520the%2520user%2527s%2520keyboard%2520and%2520mouse%2520inputs.%25202%2529%2520To%2520enforce%2520long-term%2520consistency%252C%2520our%2520Reconstituted%2520Context%2520Memory%2520dynamically%2520rebuilds%2520context%2520from%2520past%2520frames%2520and%2520uses%2520temporal%2520reframing%2520to%2520keep%2520geometrically%2520important%2520but%2520long-past%2520frames%2520accessible%252C%2520effectively%2520alleviating%2520memory%2520attenuation.%25203%2529%2520We%2520also%2520propose%2520Context%2520Forcing%252C%2520a%2520novel%2520distillation%2520method%2520designed%2520for%2520memory-aware%2520model.%2520Aligning%2520memory%2520context%2520between%2520the%2520teacher%2520and%2520student%2520preserves%2520the%2520student%2527s%2520capacity%2520to%2520use%2520long-range%2520information%252C%2520enabling%2520real-time%2520speeds%2520while%2520preventing%2520error%2520drift.%2520Taken%2520together%252C%2520WorldPlay%2520generates%2520long-horizon%2520streaming%2520720p%2520video%2520at%252024%2520FPS%2520with%2520superior%2520consistency%252C%2520comparing%2520favorably%2520with%2520existing%2520techniques%2520and%2520showing%2520strong%2520generalization%2520across%2520diverse%2520scenes.%2520Project%2520page%2520and%2520online%2520demo%2520can%2520be%2520found%253A%2520https%253A//3d-models.hunyuan.tencent.com/world/%2520and%2520https%253A//3d.hunyuan.tencent.com/sceneTo3D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldPlay%3A%20Towards%20Long-Term%20Geometric%20Consistency%20for%20Real-Time%20Interactive%20World%20Modeling&entry.906535625=Wenqiang%20Sun%20and%20Haiyu%20Zhang%20and%20Haoyuan%20Wang%20and%20Junta%20Wu%20and%20Zehan%20Wang%20and%20Zhenwei%20Wang%20and%20Yunhong%20Wang%20and%20Jun%20Zhang%20and%20Tengfei%20Wang%20and%20Chunchao%20Guo&entry.1292438233=This%20paper%20presents%20WorldPlay%2C%20a%20streaming%20video%20diffusion%20model%20that%20enables%20real-time%2C%20interactive%20world%20modeling%20with%20long-term%20geometric%20consistency%2C%20resolving%20the%20trade-off%20between%20speed%20and%20memory%20that%20limits%20current%20methods.%20WorldPlay%20draws%20power%20from%20three%20key%20innovations.%201%29%20We%20use%20a%20Dual%20Action%20Representation%20to%20enable%20robust%20action%20control%20in%20response%20to%20the%20user%27s%20keyboard%20and%20mouse%20inputs.%202%29%20To%20enforce%20long-term%20consistency%2C%20our%20Reconstituted%20Context%20Memory%20dynamically%20rebuilds%20context%20from%20past%20frames%20and%20uses%20temporal%20reframing%20to%20keep%20geometrically%20important%20but%20long-past%20frames%20accessible%2C%20effectively%20alleviating%20memory%20attenuation.%203%29%20We%20also%20propose%20Context%20Forcing%2C%20a%20novel%20distillation%20method%20designed%20for%20memory-aware%20model.%20Aligning%20memory%20context%20between%20the%20teacher%20and%20student%20preserves%20the%20student%27s%20capacity%20to%20use%20long-range%20information%2C%20enabling%20real-time%20speeds%20while%20preventing%20error%20drift.%20Taken%20together%2C%20WorldPlay%20generates%20long-horizon%20streaming%20720p%20video%20at%2024%20FPS%20with%20superior%20consistency%2C%20comparing%20favorably%20with%20existing%20techniques%20and%20showing%20strong%20generalization%20across%20diverse%20scenes.%20Project%20page%20and%20online%20demo%20can%20be%20found%3A%20https%3A//3d-models.hunyuan.tencent.com/world/%20and%20https%3A//3d.hunyuan.tencent.com/sceneTo3D.&entry.1838667208=http%3A//arxiv.org/abs/2512.14614v1&entry.124074799=Read"},
{"title": "VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models", "author": "Nguyen Tien Dong and Minh-Anh Nguyen and Thanh Dat Hoang and Nguyen Tuan Ngoc and Dao Xuan Quang Minh and Phan Phi Hai and Nguyen Thi Ngoc Anh and Dang Van Tu and Binh Vu", "abstract": "The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.", "link": "http://arxiv.org/abs/2512.14554v1", "date": "2025-12-16", "relevancy": 2.3753, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLegal-Bench%3A%20Cognitively%20Grounded%20Benchmark%20for%20Vietnamese%20Legal%20Reasoning%20of%20Large%20Language%20Models&body=Title%3A%20VLegal-Bench%3A%20Cognitively%20Grounded%20Benchmark%20for%20Vietnamese%20Legal%20Reasoning%20of%20Large%20Language%20Models%0AAuthor%3A%20Nguyen%20Tien%20Dong%20and%20Minh-Anh%20Nguyen%20and%20Thanh%20Dat%20Hoang%20and%20Nguyen%20Tuan%20Ngoc%20and%20Dao%20Xuan%20Quang%20Minh%20and%20Phan%20Phi%20Hai%20and%20Nguyen%20Thi%20Ngoc%20Anh%20and%20Dang%20Van%20Tu%20and%20Binh%20Vu%0AAbstract%3A%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20enabled%20new%20possibilities%20for%20applying%20artificial%20intelligence%20within%20the%20legal%20domain.%20Nonetheless%2C%20the%20complexity%2C%20hierarchical%20organization%2C%20and%20frequent%20revisions%20of%20Vietnamese%20legislation%20pose%20considerable%20challenges%20for%20evaluating%20how%20well%20these%20models%20interpret%20and%20utilize%20legal%20knowledge.%20To%20address%20this%20gap%2C%20Vietnamese%20Legal%20Benchmark%20%28VLegal-Bench%29%20is%20introduced%2C%20the%20first%20comprehensive%20benchmark%20designed%20to%20systematically%20assess%20LLMs%20on%20Vietnamese%20legal%20tasks.%20Informed%20by%20Bloom%27s%20cognitive%20taxonomy%2C%20VLegal-Bench%20encompasses%20multiple%20levels%20of%20legal%20understanding%20through%20tasks%20designed%20to%20reflect%20practical%20usage%20scenarios.%20The%20benchmark%20comprises%2010%2C450%20samples%20generated%20through%20a%20rigorous%20annotation%20pipeline%2C%20where%20legal%20experts%20label%20and%20cross-validate%20each%20instance%20using%20our%20annotation%20system%20to%20ensure%20every%20sample%20is%20grounded%20in%20authoritative%20legal%20documents%20and%20mirrors%20real-world%20legal%20assistant%20workflows%2C%20including%20general%20legal%20questions%20and%20answers%2C%20retrieval-augmented%20generation%2C%20multi-step%20reasoning%2C%20and%20scenario-based%20problem%20solving%20tailored%20to%20Vietnamese%20law.%20By%20providing%20a%20standardized%2C%20transparent%2C%20and%20cognitively%20informed%20evaluation%20framework%2C%20VLegal-Bench%20establishes%20a%20solid%20foundation%20for%20assessing%20LLM%20performance%20in%20Vietnamese%20legal%20contexts%20and%20supports%20the%20development%20of%20more%20reliable%2C%20interpretable%2C%20and%20ethically%20aligned%20AI-assisted%20legal%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLegal-Bench%253A%2520Cognitively%2520Grounded%2520Benchmark%2520for%2520Vietnamese%2520Legal%2520Reasoning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DNguyen%2520Tien%2520Dong%2520and%2520Minh-Anh%2520Nguyen%2520and%2520Thanh%2520Dat%2520Hoang%2520and%2520Nguyen%2520Tuan%2520Ngoc%2520and%2520Dao%2520Xuan%2520Quang%2520Minh%2520and%2520Phan%2520Phi%2520Hai%2520and%2520Nguyen%2520Thi%2520Ngoc%2520Anh%2520and%2520Dang%2520Van%2520Tu%2520and%2520Binh%2520Vu%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520enabled%2520new%2520possibilities%2520for%2520applying%2520artificial%2520intelligence%2520within%2520the%2520legal%2520domain.%2520Nonetheless%252C%2520the%2520complexity%252C%2520hierarchical%2520organization%252C%2520and%2520frequent%2520revisions%2520of%2520Vietnamese%2520legislation%2520pose%2520considerable%2520challenges%2520for%2520evaluating%2520how%2520well%2520these%2520models%2520interpret%2520and%2520utilize%2520legal%2520knowledge.%2520To%2520address%2520this%2520gap%252C%2520Vietnamese%2520Legal%2520Benchmark%2520%2528VLegal-Bench%2529%2520is%2520introduced%252C%2520the%2520first%2520comprehensive%2520benchmark%2520designed%2520to%2520systematically%2520assess%2520LLMs%2520on%2520Vietnamese%2520legal%2520tasks.%2520Informed%2520by%2520Bloom%2527s%2520cognitive%2520taxonomy%252C%2520VLegal-Bench%2520encompasses%2520multiple%2520levels%2520of%2520legal%2520understanding%2520through%2520tasks%2520designed%2520to%2520reflect%2520practical%2520usage%2520scenarios.%2520The%2520benchmark%2520comprises%252010%252C450%2520samples%2520generated%2520through%2520a%2520rigorous%2520annotation%2520pipeline%252C%2520where%2520legal%2520experts%2520label%2520and%2520cross-validate%2520each%2520instance%2520using%2520our%2520annotation%2520system%2520to%2520ensure%2520every%2520sample%2520is%2520grounded%2520in%2520authoritative%2520legal%2520documents%2520and%2520mirrors%2520real-world%2520legal%2520assistant%2520workflows%252C%2520including%2520general%2520legal%2520questions%2520and%2520answers%252C%2520retrieval-augmented%2520generation%252C%2520multi-step%2520reasoning%252C%2520and%2520scenario-based%2520problem%2520solving%2520tailored%2520to%2520Vietnamese%2520law.%2520By%2520providing%2520a%2520standardized%252C%2520transparent%252C%2520and%2520cognitively%2520informed%2520evaluation%2520framework%252C%2520VLegal-Bench%2520establishes%2520a%2520solid%2520foundation%2520for%2520assessing%2520LLM%2520performance%2520in%2520Vietnamese%2520legal%2520contexts%2520and%2520supports%2520the%2520development%2520of%2520more%2520reliable%252C%2520interpretable%252C%2520and%2520ethically%2520aligned%2520AI-assisted%2520legal%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLegal-Bench%3A%20Cognitively%20Grounded%20Benchmark%20for%20Vietnamese%20Legal%20Reasoning%20of%20Large%20Language%20Models&entry.906535625=Nguyen%20Tien%20Dong%20and%20Minh-Anh%20Nguyen%20and%20Thanh%20Dat%20Hoang%20and%20Nguyen%20Tuan%20Ngoc%20and%20Dao%20Xuan%20Quang%20Minh%20and%20Phan%20Phi%20Hai%20and%20Nguyen%20Thi%20Ngoc%20Anh%20and%20Dang%20Van%20Tu%20and%20Binh%20Vu&entry.1292438233=The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20has%20enabled%20new%20possibilities%20for%20applying%20artificial%20intelligence%20within%20the%20legal%20domain.%20Nonetheless%2C%20the%20complexity%2C%20hierarchical%20organization%2C%20and%20frequent%20revisions%20of%20Vietnamese%20legislation%20pose%20considerable%20challenges%20for%20evaluating%20how%20well%20these%20models%20interpret%20and%20utilize%20legal%20knowledge.%20To%20address%20this%20gap%2C%20Vietnamese%20Legal%20Benchmark%20%28VLegal-Bench%29%20is%20introduced%2C%20the%20first%20comprehensive%20benchmark%20designed%20to%20systematically%20assess%20LLMs%20on%20Vietnamese%20legal%20tasks.%20Informed%20by%20Bloom%27s%20cognitive%20taxonomy%2C%20VLegal-Bench%20encompasses%20multiple%20levels%20of%20legal%20understanding%20through%20tasks%20designed%20to%20reflect%20practical%20usage%20scenarios.%20The%20benchmark%20comprises%2010%2C450%20samples%20generated%20through%20a%20rigorous%20annotation%20pipeline%2C%20where%20legal%20experts%20label%20and%20cross-validate%20each%20instance%20using%20our%20annotation%20system%20to%20ensure%20every%20sample%20is%20grounded%20in%20authoritative%20legal%20documents%20and%20mirrors%20real-world%20legal%20assistant%20workflows%2C%20including%20general%20legal%20questions%20and%20answers%2C%20retrieval-augmented%20generation%2C%20multi-step%20reasoning%2C%20and%20scenario-based%20problem%20solving%20tailored%20to%20Vietnamese%20law.%20By%20providing%20a%20standardized%2C%20transparent%2C%20and%20cognitively%20informed%20evaluation%20framework%2C%20VLegal-Bench%20establishes%20a%20solid%20foundation%20for%20assessing%20LLM%20performance%20in%20Vietnamese%20legal%20contexts%20and%20supports%20the%20development%20of%20more%20reliable%2C%20interpretable%2C%20and%20ethically%20aligned%20AI-assisted%20legal%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.14554v1&entry.124074799=Read"},
{"title": "The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy", "author": "Zhuo Chen and Fanyue Wei and Runze Xu and Jingjing Li and Lixin Duan and Angela Yao and Wen Li", "abstract": "Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.", "link": "http://arxiv.org/abs/2512.14423v1", "date": "2025-12-16", "relevancy": 2.351, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.598}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.591}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Devil%20is%20in%20Attention%20Sharing%3A%20Improving%20Complex%20Non-rigid%20Image%20Editing%20Faithfulness%20via%20Attention%20Synergy&body=Title%3A%20The%20Devil%20is%20in%20Attention%20Sharing%3A%20Improving%20Complex%20Non-rigid%20Image%20Editing%20Faithfulness%20via%20Attention%20Synergy%0AAuthor%3A%20Zhuo%20Chen%20and%20Fanyue%20Wei%20and%20Runze%20Xu%20and%20Jingjing%20Li%20and%20Lixin%20Duan%20and%20Angela%20Yao%20and%20Wen%20Li%0AAbstract%3A%20Training-free%20image%20editing%20with%20large%20diffusion%20models%20has%20become%20practical%2C%20yet%20faithfully%20performing%20complex%20non-rigid%20edits%20%28e.g.%2C%20pose%20or%20shape%20changes%29%20remains%20highly%20challenging.%20We%20identify%20a%20key%20underlying%20cause%3A%20attention%20collapse%20in%20existing%20attention%20sharing%20mechanisms%2C%20where%20either%20positional%20embeddings%20or%20semantic%20features%20dominate%20visual%20content%20retrieval%2C%20leading%20to%20over-editing%20or%20under-editing.To%20address%20this%20issue%2C%20we%20introduce%20SynPS%2C%20a%20method%20that%20Synergistically%20leverages%20Positional%20embeddings%20and%20Semantic%20information%20for%20faithful%20non-rigid%20image%20editing.%20We%20first%20propose%20an%20editing%20measurement%20that%20quantifies%20the%20required%20editing%20magnitude%20at%20each%20denoising%20step.%20Based%20on%20this%20measurement%2C%20we%20design%20an%20attention%20synergy%20pipeline%20that%20dynamically%20modulates%20the%20influence%20of%20positional%20embeddings%2C%20enabling%20SynPS%20to%20balance%20semantic%20modifications%20and%20fidelity%20preservation.By%20adaptively%20integrating%20positional%20and%20semantic%20cues%2C%20SynPS%20effectively%20avoids%20both%20over-%20and%20under-editing.%20Extensive%20experiments%20on%20public%20and%20newly%20curated%20benchmarks%20demonstrate%20the%20superior%20performance%20and%20faithfulness%20of%20our%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Devil%2520is%2520in%2520Attention%2520Sharing%253A%2520Improving%2520Complex%2520Non-rigid%2520Image%2520Editing%2520Faithfulness%2520via%2520Attention%2520Synergy%26entry.906535625%3DZhuo%2520Chen%2520and%2520Fanyue%2520Wei%2520and%2520Runze%2520Xu%2520and%2520Jingjing%2520Li%2520and%2520Lixin%2520Duan%2520and%2520Angela%2520Yao%2520and%2520Wen%2520Li%26entry.1292438233%3DTraining-free%2520image%2520editing%2520with%2520large%2520diffusion%2520models%2520has%2520become%2520practical%252C%2520yet%2520faithfully%2520performing%2520complex%2520non-rigid%2520edits%2520%2528e.g.%252C%2520pose%2520or%2520shape%2520changes%2529%2520remains%2520highly%2520challenging.%2520We%2520identify%2520a%2520key%2520underlying%2520cause%253A%2520attention%2520collapse%2520in%2520existing%2520attention%2520sharing%2520mechanisms%252C%2520where%2520either%2520positional%2520embeddings%2520or%2520semantic%2520features%2520dominate%2520visual%2520content%2520retrieval%252C%2520leading%2520to%2520over-editing%2520or%2520under-editing.To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520SynPS%252C%2520a%2520method%2520that%2520Synergistically%2520leverages%2520Positional%2520embeddings%2520and%2520Semantic%2520information%2520for%2520faithful%2520non-rigid%2520image%2520editing.%2520We%2520first%2520propose%2520an%2520editing%2520measurement%2520that%2520quantifies%2520the%2520required%2520editing%2520magnitude%2520at%2520each%2520denoising%2520step.%2520Based%2520on%2520this%2520measurement%252C%2520we%2520design%2520an%2520attention%2520synergy%2520pipeline%2520that%2520dynamically%2520modulates%2520the%2520influence%2520of%2520positional%2520embeddings%252C%2520enabling%2520SynPS%2520to%2520balance%2520semantic%2520modifications%2520and%2520fidelity%2520preservation.By%2520adaptively%2520integrating%2520positional%2520and%2520semantic%2520cues%252C%2520SynPS%2520effectively%2520avoids%2520both%2520over-%2520and%2520under-editing.%2520Extensive%2520experiments%2520on%2520public%2520and%2520newly%2520curated%2520benchmarks%2520demonstrate%2520the%2520superior%2520performance%2520and%2520faithfulness%2520of%2520our%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Devil%20is%20in%20Attention%20Sharing%3A%20Improving%20Complex%20Non-rigid%20Image%20Editing%20Faithfulness%20via%20Attention%20Synergy&entry.906535625=Zhuo%20Chen%20and%20Fanyue%20Wei%20and%20Runze%20Xu%20and%20Jingjing%20Li%20and%20Lixin%20Duan%20and%20Angela%20Yao%20and%20Wen%20Li&entry.1292438233=Training-free%20image%20editing%20with%20large%20diffusion%20models%20has%20become%20practical%2C%20yet%20faithfully%20performing%20complex%20non-rigid%20edits%20%28e.g.%2C%20pose%20or%20shape%20changes%29%20remains%20highly%20challenging.%20We%20identify%20a%20key%20underlying%20cause%3A%20attention%20collapse%20in%20existing%20attention%20sharing%20mechanisms%2C%20where%20either%20positional%20embeddings%20or%20semantic%20features%20dominate%20visual%20content%20retrieval%2C%20leading%20to%20over-editing%20or%20under-editing.To%20address%20this%20issue%2C%20we%20introduce%20SynPS%2C%20a%20method%20that%20Synergistically%20leverages%20Positional%20embeddings%20and%20Semantic%20information%20for%20faithful%20non-rigid%20image%20editing.%20We%20first%20propose%20an%20editing%20measurement%20that%20quantifies%20the%20required%20editing%20magnitude%20at%20each%20denoising%20step.%20Based%20on%20this%20measurement%2C%20we%20design%20an%20attention%20synergy%20pipeline%20that%20dynamically%20modulates%20the%20influence%20of%20positional%20embeddings%2C%20enabling%20SynPS%20to%20balance%20semantic%20modifications%20and%20fidelity%20preservation.By%20adaptively%20integrating%20positional%20and%20semantic%20cues%2C%20SynPS%20effectively%20avoids%20both%20over-%20and%20under-editing.%20Extensive%20experiments%20on%20public%20and%20newly%20curated%20benchmarks%20demonstrate%20the%20superior%20performance%20and%20faithfulness%20of%20our%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2512.14423v1&entry.124074799=Read"},
{"title": "Context-Picker: Dynamic context selection using multi-stage reinforcement learning", "author": "Siyuan Zhu and Chengdong Xu and Kaiqiang Ke and Chao Yu", "abstract": "In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \\emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \\emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \\emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines \"minimal sufficient sets\" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.", "link": "http://arxiv.org/abs/2512.14465v1", "date": "2025-12-16", "relevancy": 2.3431, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Picker%3A%20Dynamic%20context%20selection%20using%20multi-stage%20reinforcement%20learning&body=Title%3A%20Context-Picker%3A%20Dynamic%20context%20selection%20using%20multi-stage%20reinforcement%20learning%0AAuthor%3A%20Siyuan%20Zhu%20and%20Chengdong%20Xu%20and%20Kaiqiang%20Ke%20and%20Chao%20Yu%0AAbstract%3A%20In%20long-context%20question%20answering%20%28LCQA%29%2C%20determining%20the%20optimal%20amount%20of%20context%20for%20a%20given%20query%20is%20a%20significant%20challenge.%20Including%20too%20few%20passages%20may%20omit%20critical%20information%2C%20while%20including%20too%20many%20can%20introduce%20noise%20and%20reduce%20the%20quality%20of%20the%20answer.%20Traditional%20approaches%2C%20such%20as%20fixed%20Top-%24K%24%20retrieval%20and%20single-stage%20reranking%2C%20face%20the%20dilemma%20of%20selecting%20the%20right%20number%20of%20passages.%20This%20problem%20is%20particularly%20pronounced%20for%20factoid%20questions%2C%20which%20often%20require%20only%20a%20few%20specific%20pieces%20of%20evidence.%20To%20address%20this%20issue%2C%20we%20introduce%20%5Cemph%7BContext-Picker%7D%2C%20a%20reasoning-aware%20framework%20that%20shifts%20the%20paradigm%20from%20similarity-based%20ranking%20to%20minimal%20sufficient%20subset%20selection.%20Context-Picker%20treats%20context%20selection%20as%20a%20decision-making%20process%20optimized%20via%20a%20human-inspired%2C%20two-stage%20reinforcement%20learning%20schedule%3A%20a%20%5Cemph%7Brecall-oriented%7D%20stage%20that%20prioritizes%20the%20coverage%20of%20reasoning%20chains%2C%20followed%20by%20a%20%5Cemph%7Bprecision-oriented%7D%20stage%20that%20aggressively%20prunes%20redundancy%20to%20distill%20a%20compact%20evidence%20set.%20To%20resolve%20reward%20sparsity%2C%20we%20propose%20an%20offline%20evidence%20distillation%20pipeline%20that%20mines%20%22minimal%20sufficient%20sets%22%20via%20a%20Leave-One-Out%20%28LOO%29%20procedure%2C%20providing%20dense%2C%20task-aligned%20supervision.%20Experiments%20on%20five%20long-context%20and%20multi-hop%20QA%20benchmarks%20demonstrate%20that%20Context-Picker%20significantly%20outperforms%20strong%20RAG%20baselines%2C%20achieving%20superior%20answer%20accuracy%20with%20comparable%20or%20reduced%20context%20lengths.%20Ablation%20studies%20indicate%20that%20the%20coarse-to-fine%20optimization%20schedule%2C%20the%20redundancy-aware%20reward%20shaping%2C%20and%20the%20rationale-guided%20format%20all%20contribute%20substantially%20to%20these%20gains.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Picker%253A%2520Dynamic%2520context%2520selection%2520using%2520multi-stage%2520reinforcement%2520learning%26entry.906535625%3DSiyuan%2520Zhu%2520and%2520Chengdong%2520Xu%2520and%2520Kaiqiang%2520Ke%2520and%2520Chao%2520Yu%26entry.1292438233%3DIn%2520long-context%2520question%2520answering%2520%2528LCQA%2529%252C%2520determining%2520the%2520optimal%2520amount%2520of%2520context%2520for%2520a%2520given%2520query%2520is%2520a%2520significant%2520challenge.%2520Including%2520too%2520few%2520passages%2520may%2520omit%2520critical%2520information%252C%2520while%2520including%2520too%2520many%2520can%2520introduce%2520noise%2520and%2520reduce%2520the%2520quality%2520of%2520the%2520answer.%2520Traditional%2520approaches%252C%2520such%2520as%2520fixed%2520Top-%2524K%2524%2520retrieval%2520and%2520single-stage%2520reranking%252C%2520face%2520the%2520dilemma%2520of%2520selecting%2520the%2520right%2520number%2520of%2520passages.%2520This%2520problem%2520is%2520particularly%2520pronounced%2520for%2520factoid%2520questions%252C%2520which%2520often%2520require%2520only%2520a%2520few%2520specific%2520pieces%2520of%2520evidence.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520%255Cemph%257BContext-Picker%257D%252C%2520a%2520reasoning-aware%2520framework%2520that%2520shifts%2520the%2520paradigm%2520from%2520similarity-based%2520ranking%2520to%2520minimal%2520sufficient%2520subset%2520selection.%2520Context-Picker%2520treats%2520context%2520selection%2520as%2520a%2520decision-making%2520process%2520optimized%2520via%2520a%2520human-inspired%252C%2520two-stage%2520reinforcement%2520learning%2520schedule%253A%2520a%2520%255Cemph%257Brecall-oriented%257D%2520stage%2520that%2520prioritizes%2520the%2520coverage%2520of%2520reasoning%2520chains%252C%2520followed%2520by%2520a%2520%255Cemph%257Bprecision-oriented%257D%2520stage%2520that%2520aggressively%2520prunes%2520redundancy%2520to%2520distill%2520a%2520compact%2520evidence%2520set.%2520To%2520resolve%2520reward%2520sparsity%252C%2520we%2520propose%2520an%2520offline%2520evidence%2520distillation%2520pipeline%2520that%2520mines%2520%2522minimal%2520sufficient%2520sets%2522%2520via%2520a%2520Leave-One-Out%2520%2528LOO%2529%2520procedure%252C%2520providing%2520dense%252C%2520task-aligned%2520supervision.%2520Experiments%2520on%2520five%2520long-context%2520and%2520multi-hop%2520QA%2520benchmarks%2520demonstrate%2520that%2520Context-Picker%2520significantly%2520outperforms%2520strong%2520RAG%2520baselines%252C%2520achieving%2520superior%2520answer%2520accuracy%2520with%2520comparable%2520or%2520reduced%2520context%2520lengths.%2520Ablation%2520studies%2520indicate%2520that%2520the%2520coarse-to-fine%2520optimization%2520schedule%252C%2520the%2520redundancy-aware%2520reward%2520shaping%252C%2520and%2520the%2520rationale-guided%2520format%2520all%2520contribute%2520substantially%2520to%2520these%2520gains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Picker%3A%20Dynamic%20context%20selection%20using%20multi-stage%20reinforcement%20learning&entry.906535625=Siyuan%20Zhu%20and%20Chengdong%20Xu%20and%20Kaiqiang%20Ke%20and%20Chao%20Yu&entry.1292438233=In%20long-context%20question%20answering%20%28LCQA%29%2C%20determining%20the%20optimal%20amount%20of%20context%20for%20a%20given%20query%20is%20a%20significant%20challenge.%20Including%20too%20few%20passages%20may%20omit%20critical%20information%2C%20while%20including%20too%20many%20can%20introduce%20noise%20and%20reduce%20the%20quality%20of%20the%20answer.%20Traditional%20approaches%2C%20such%20as%20fixed%20Top-%24K%24%20retrieval%20and%20single-stage%20reranking%2C%20face%20the%20dilemma%20of%20selecting%20the%20right%20number%20of%20passages.%20This%20problem%20is%20particularly%20pronounced%20for%20factoid%20questions%2C%20which%20often%20require%20only%20a%20few%20specific%20pieces%20of%20evidence.%20To%20address%20this%20issue%2C%20we%20introduce%20%5Cemph%7BContext-Picker%7D%2C%20a%20reasoning-aware%20framework%20that%20shifts%20the%20paradigm%20from%20similarity-based%20ranking%20to%20minimal%20sufficient%20subset%20selection.%20Context-Picker%20treats%20context%20selection%20as%20a%20decision-making%20process%20optimized%20via%20a%20human-inspired%2C%20two-stage%20reinforcement%20learning%20schedule%3A%20a%20%5Cemph%7Brecall-oriented%7D%20stage%20that%20prioritizes%20the%20coverage%20of%20reasoning%20chains%2C%20followed%20by%20a%20%5Cemph%7Bprecision-oriented%7D%20stage%20that%20aggressively%20prunes%20redundancy%20to%20distill%20a%20compact%20evidence%20set.%20To%20resolve%20reward%20sparsity%2C%20we%20propose%20an%20offline%20evidence%20distillation%20pipeline%20that%20mines%20%22minimal%20sufficient%20sets%22%20via%20a%20Leave-One-Out%20%28LOO%29%20procedure%2C%20providing%20dense%2C%20task-aligned%20supervision.%20Experiments%20on%20five%20long-context%20and%20multi-hop%20QA%20benchmarks%20demonstrate%20that%20Context-Picker%20significantly%20outperforms%20strong%20RAG%20baselines%2C%20achieving%20superior%20answer%20accuracy%20with%20comparable%20or%20reduced%20context%20lengths.%20Ablation%20studies%20indicate%20that%20the%20coarse-to-fine%20optimization%20schedule%2C%20the%20redundancy-aware%20reward%20shaping%2C%20and%20the%20rationale-guided%20format%20all%20contribute%20substantially%20to%20these%20gains.&entry.1838667208=http%3A//arxiv.org/abs/2512.14465v1&entry.124074799=Read"},
{"title": "Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence", "author": "Shreyas Subramanian and Bala Krishnamoorthy and Pranav Murthy", "abstract": "Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \\emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.", "link": "http://arxiv.org/abs/2512.14527v1", "date": "2025-12-16", "relevancy": 2.3403, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5097}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4584}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Learning%20Rate%20Scheduling%20based%20on%20Loss%20Changes%20Leads%20to%20Faster%20Convergence&body=Title%3A%20Dynamic%20Learning%20Rate%20Scheduling%20based%20on%20Loss%20Changes%20Leads%20to%20Faster%20Convergence%0AAuthor%3A%20Shreyas%20Subramanian%20and%20Bala%20Krishnamoorthy%20and%20Pranav%20Murthy%0AAbstract%3A%20Despite%20significant%20advances%20in%20optimizers%20for%20training%2C%20most%20research%20works%20use%20common%20scheduler%20choices%20like%20Cosine%20or%20exponential%20decay.%20In%20this%20paper%2C%20we%20study%20%5Cemph%7BGreedyLR%7D%2C%20a%20novel%20scheduler%20that%20adaptively%20adjusts%20the%20learning%20rate%20during%20training%20based%20on%20the%20current%20loss.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%20scheduler%2C%20we%20conduct%20experiments%20on%20several%20NLP%2C%20CV%2C%20and%20LLM%20tasks%20with%20up%20to%20%247B%24%20parameters%2C%20including%20both%20fine-tuning%20and%20pre-training%20experiments.%20The%20results%20show%20that%20our%20approach%20outperforms%20several%20state-of-the-art%20schedulers%20in%20terms%20of%20accuracy%2C%20speed%2C%20and%20convergence.%20We%20also%20provide%20a%20theoretical%20analysis%20of%20the%20GreedyLR%20algorithm%2C%20including%20a%20proof%20of%20convergence%20and%20derivation%20of%20the%20optimal%20scaling%20factor%20%24F%24%20that%20maximizes%20the%20convergence%20rate%2C%20along%20with%20experiments%20to%20show%20robustness%20of%20the%20algorithm%20to%20realistic%20noisy%20landscapes.%20Our%20scheduler%20is%20easy%20to%20implement%2C%20computationally%20efficient%2C%20and%20could%20be%20considered%20a%20good%20default%20scheduler%20for%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Learning%2520Rate%2520Scheduling%2520based%2520on%2520Loss%2520Changes%2520Leads%2520to%2520Faster%2520Convergence%26entry.906535625%3DShreyas%2520Subramanian%2520and%2520Bala%2520Krishnamoorthy%2520and%2520Pranav%2520Murthy%26entry.1292438233%3DDespite%2520significant%2520advances%2520in%2520optimizers%2520for%2520training%252C%2520most%2520research%2520works%2520use%2520common%2520scheduler%2520choices%2520like%2520Cosine%2520or%2520exponential%2520decay.%2520In%2520this%2520paper%252C%2520we%2520study%2520%255Cemph%257BGreedyLR%257D%252C%2520a%2520novel%2520scheduler%2520that%2520adaptively%2520adjusts%2520the%2520learning%2520rate%2520during%2520training%2520based%2520on%2520the%2520current%2520loss.%2520To%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520scheduler%252C%2520we%2520conduct%2520experiments%2520on%2520several%2520NLP%252C%2520CV%252C%2520and%2520LLM%2520tasks%2520with%2520up%2520to%2520%25247B%2524%2520parameters%252C%2520including%2520both%2520fine-tuning%2520and%2520pre-training%2520experiments.%2520The%2520results%2520show%2520that%2520our%2520approach%2520outperforms%2520several%2520state-of-the-art%2520schedulers%2520in%2520terms%2520of%2520accuracy%252C%2520speed%252C%2520and%2520convergence.%2520We%2520also%2520provide%2520a%2520theoretical%2520analysis%2520of%2520the%2520GreedyLR%2520algorithm%252C%2520including%2520a%2520proof%2520of%2520convergence%2520and%2520derivation%2520of%2520the%2520optimal%2520scaling%2520factor%2520%2524F%2524%2520that%2520maximizes%2520the%2520convergence%2520rate%252C%2520along%2520with%2520experiments%2520to%2520show%2520robustness%2520of%2520the%2520algorithm%2520to%2520realistic%2520noisy%2520landscapes.%2520Our%2520scheduler%2520is%2520easy%2520to%2520implement%252C%2520computationally%2520efficient%252C%2520and%2520could%2520be%2520considered%2520a%2520good%2520default%2520scheduler%2520for%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Learning%20Rate%20Scheduling%20based%20on%20Loss%20Changes%20Leads%20to%20Faster%20Convergence&entry.906535625=Shreyas%20Subramanian%20and%20Bala%20Krishnamoorthy%20and%20Pranav%20Murthy&entry.1292438233=Despite%20significant%20advances%20in%20optimizers%20for%20training%2C%20most%20research%20works%20use%20common%20scheduler%20choices%20like%20Cosine%20or%20exponential%20decay.%20In%20this%20paper%2C%20we%20study%20%5Cemph%7BGreedyLR%7D%2C%20a%20novel%20scheduler%20that%20adaptively%20adjusts%20the%20learning%20rate%20during%20training%20based%20on%20the%20current%20loss.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%20scheduler%2C%20we%20conduct%20experiments%20on%20several%20NLP%2C%20CV%2C%20and%20LLM%20tasks%20with%20up%20to%20%247B%24%20parameters%2C%20including%20both%20fine-tuning%20and%20pre-training%20experiments.%20The%20results%20show%20that%20our%20approach%20outperforms%20several%20state-of-the-art%20schedulers%20in%20terms%20of%20accuracy%2C%20speed%2C%20and%20convergence.%20We%20also%20provide%20a%20theoretical%20analysis%20of%20the%20GreedyLR%20algorithm%2C%20including%20a%20proof%20of%20convergence%20and%20derivation%20of%20the%20optimal%20scaling%20factor%20%24F%24%20that%20maximizes%20the%20convergence%20rate%2C%20along%20with%20experiments%20to%20show%20robustness%20of%20the%20algorithm%20to%20realistic%20noisy%20landscapes.%20Our%20scheduler%20is%20easy%20to%20implement%2C%20computationally%20efficient%2C%20and%20could%20be%20considered%20a%20good%20default%20scheduler%20for%20training.&entry.1838667208=http%3A//arxiv.org/abs/2512.14527v1&entry.124074799=Read"},
{"title": "TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation", "author": "Qizhi Wang", "abstract": "Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.", "link": "http://arxiv.org/abs/2512.14358v1", "date": "2025-12-16", "relevancy": 2.3361, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4883}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4577}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiCard%3A%20Deployable%20EXPLAIN-only%20Residual%20Learning%20for%20Cardinality%20Estimation&body=Title%3A%20TiCard%3A%20Deployable%20EXPLAIN-only%20Residual%20Learning%20for%20Cardinality%20Estimation%0AAuthor%3A%20Qizhi%20Wang%0AAbstract%3A%20Cardinality%20estimation%20is%20a%20key%20bottleneck%20for%20cost-based%20query%20optimization%2C%20yet%20deployable%20improvements%20remain%20difficult%3A%20classical%20estimators%20miss%20correlations%2C%20while%20learned%20estimators%20often%20require%20workload-specific%20training%20pipelines%20and%20invasive%20integration%20into%20the%20optimizer.%20This%20paper%20presents%20TiCard%2C%20a%20low%20intrusion%2C%20correction-based%20framework%20that%20augments%20%28rather%20than%20replaces%29%20a%20database%27s%20native%20estimator.%20TiCard%20learns%20multiplicative%20residual%20corrections%20using%20EXPLAIN-only%20features%2C%20and%20uses%20EXPLAIN%20ANALYZE%20only%20for%20offline%20labels.%20We%20study%20two%20practical%20instantiations%3A%20%28i%29%20a%20Gradient%20Boosting%20Regressor%20for%20sub-millisecond%20inference%2C%20and%20%28ii%29%20TabPFN%2C%20an%20in-context%20tabular%20foundation%20model%20that%20adapts%20by%20refreshing%20a%20small%20reference%20set%20without%20gradient%20retraining.%20On%20TiDB%20with%20TPCH%20and%20the%20Join%20Order%20Benchmark%2C%20in%20a%20low-trace%20setting%20%28263%20executions%20total%3B%20157%20used%20for%20learning%29%2C%20TiCard%20improves%20operator-level%20tail%20accuracy%20substantially%3A%20P90%20Q-error%20drops%20from%20312.85%20%28native%29%20to%2013.69%20%28TiCard-GBR%29%2C%20and%20P99%20drops%20from%2037%2C974.37%20to%203%2C416.50%20%28TiCard-TabPFN%29%2C%20while%20a%20join-only%20policy%20preserves%20near-perfect%20median%20behavior.%20We%20position%20TiCard%20as%20an%20AI4DB%20building%20block%20focused%20on%20deployability%3A%20explicit%20scope%2C%20conservative%20integration%20policies%2C%20and%20an%20integration%20roadmap%20from%20offline%20correction%20to%20in-optimizer%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiCard%253A%2520Deployable%2520EXPLAIN-only%2520Residual%2520Learning%2520for%2520Cardinality%2520Estimation%26entry.906535625%3DQizhi%2520Wang%26entry.1292438233%3DCardinality%2520estimation%2520is%2520a%2520key%2520bottleneck%2520for%2520cost-based%2520query%2520optimization%252C%2520yet%2520deployable%2520improvements%2520remain%2520difficult%253A%2520classical%2520estimators%2520miss%2520correlations%252C%2520while%2520learned%2520estimators%2520often%2520require%2520workload-specific%2520training%2520pipelines%2520and%2520invasive%2520integration%2520into%2520the%2520optimizer.%2520This%2520paper%2520presents%2520TiCard%252C%2520a%2520low%2520intrusion%252C%2520correction-based%2520framework%2520that%2520augments%2520%2528rather%2520than%2520replaces%2529%2520a%2520database%2527s%2520native%2520estimator.%2520TiCard%2520learns%2520multiplicative%2520residual%2520corrections%2520using%2520EXPLAIN-only%2520features%252C%2520and%2520uses%2520EXPLAIN%2520ANALYZE%2520only%2520for%2520offline%2520labels.%2520We%2520study%2520two%2520practical%2520instantiations%253A%2520%2528i%2529%2520a%2520Gradient%2520Boosting%2520Regressor%2520for%2520sub-millisecond%2520inference%252C%2520and%2520%2528ii%2529%2520TabPFN%252C%2520an%2520in-context%2520tabular%2520foundation%2520model%2520that%2520adapts%2520by%2520refreshing%2520a%2520small%2520reference%2520set%2520without%2520gradient%2520retraining.%2520On%2520TiDB%2520with%2520TPCH%2520and%2520the%2520Join%2520Order%2520Benchmark%252C%2520in%2520a%2520low-trace%2520setting%2520%2528263%2520executions%2520total%253B%2520157%2520used%2520for%2520learning%2529%252C%2520TiCard%2520improves%2520operator-level%2520tail%2520accuracy%2520substantially%253A%2520P90%2520Q-error%2520drops%2520from%2520312.85%2520%2528native%2529%2520to%252013.69%2520%2528TiCard-GBR%2529%252C%2520and%2520P99%2520drops%2520from%252037%252C974.37%2520to%25203%252C416.50%2520%2528TiCard-TabPFN%2529%252C%2520while%2520a%2520join-only%2520policy%2520preserves%2520near-perfect%2520median%2520behavior.%2520We%2520position%2520TiCard%2520as%2520an%2520AI4DB%2520building%2520block%2520focused%2520on%2520deployability%253A%2520explicit%2520scope%252C%2520conservative%2520integration%2520policies%252C%2520and%2520an%2520integration%2520roadmap%2520from%2520offline%2520correction%2520to%2520in-optimizer%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiCard%3A%20Deployable%20EXPLAIN-only%20Residual%20Learning%20for%20Cardinality%20Estimation&entry.906535625=Qizhi%20Wang&entry.1292438233=Cardinality%20estimation%20is%20a%20key%20bottleneck%20for%20cost-based%20query%20optimization%2C%20yet%20deployable%20improvements%20remain%20difficult%3A%20classical%20estimators%20miss%20correlations%2C%20while%20learned%20estimators%20often%20require%20workload-specific%20training%20pipelines%20and%20invasive%20integration%20into%20the%20optimizer.%20This%20paper%20presents%20TiCard%2C%20a%20low%20intrusion%2C%20correction-based%20framework%20that%20augments%20%28rather%20than%20replaces%29%20a%20database%27s%20native%20estimator.%20TiCard%20learns%20multiplicative%20residual%20corrections%20using%20EXPLAIN-only%20features%2C%20and%20uses%20EXPLAIN%20ANALYZE%20only%20for%20offline%20labels.%20We%20study%20two%20practical%20instantiations%3A%20%28i%29%20a%20Gradient%20Boosting%20Regressor%20for%20sub-millisecond%20inference%2C%20and%20%28ii%29%20TabPFN%2C%20an%20in-context%20tabular%20foundation%20model%20that%20adapts%20by%20refreshing%20a%20small%20reference%20set%20without%20gradient%20retraining.%20On%20TiDB%20with%20TPCH%20and%20the%20Join%20Order%20Benchmark%2C%20in%20a%20low-trace%20setting%20%28263%20executions%20total%3B%20157%20used%20for%20learning%29%2C%20TiCard%20improves%20operator-level%20tail%20accuracy%20substantially%3A%20P90%20Q-error%20drops%20from%20312.85%20%28native%29%20to%2013.69%20%28TiCard-GBR%29%2C%20and%20P99%20drops%20from%2037%2C974.37%20to%203%2C416.50%20%28TiCard-TabPFN%29%2C%20while%20a%20join-only%20policy%20preserves%20near-perfect%20median%20behavior.%20We%20position%20TiCard%20as%20an%20AI4DB%20building%20block%20focused%20on%20deployability%3A%20explicit%20scope%2C%20conservative%20integration%20policies%2C%20and%20an%20integration%20roadmap%20from%20offline%20correction%20to%20in-optimizer%20use.&entry.1838667208=http%3A//arxiv.org/abs/2512.14358v1&entry.124074799=Read"},
{"title": "Native and Compact Structured Latents for 3D Generation", "author": "Jianfeng Xiang and Xiaoxue Chen and Sicheng Xu and Ruicheng Wang and Zelong Lv and Yu Deng and Hongyuan Zhu and Yue Dong and Hao Zhao and Nicholas Jing Yuan and Jiaolong Yang", "abstract": "Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.", "link": "http://arxiv.org/abs/2512.14692v1", "date": "2025-12-16", "relevancy": 2.335, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5909}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5823}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Native%20and%20Compact%20Structured%20Latents%20for%203D%20Generation&body=Title%3A%20Native%20and%20Compact%20Structured%20Latents%20for%203D%20Generation%0AAuthor%3A%20Jianfeng%20Xiang%20and%20Xiaoxue%20Chen%20and%20Sicheng%20Xu%20and%20Ruicheng%20Wang%20and%20Zelong%20Lv%20and%20Yu%20Deng%20and%20Hongyuan%20Zhu%20and%20Yue%20Dong%20and%20Hao%20Zhao%20and%20Nicholas%20Jing%20Yuan%20and%20Jiaolong%20Yang%0AAbstract%3A%20Recent%20advancements%20in%203D%20generative%20modeling%20have%20significantly%20improved%20the%20generation%20realism%2C%20yet%20the%20field%20is%20still%20hampered%20by%20existing%20representations%2C%20which%20struggle%20to%20capture%20assets%20with%20complex%20topologies%20and%20detailed%20appearance.%20This%20paper%20present%20an%20approach%20for%20learning%20a%20structured%20latent%20representation%20from%20native%203D%20data%20to%20address%20this%20challenge.%20At%20its%20core%20is%20a%20new%20sparse%20voxel%20structure%20called%20O-Voxel%2C%20an%20omni-voxel%20representation%20that%20encodes%20both%20geometry%20and%20appearance.%20O-Voxel%20can%20robustly%20model%20arbitrary%20topology%2C%20including%20open%2C%20non-manifold%2C%20and%20fully-enclosed%20surfaces%2C%20while%20capturing%20comprehensive%20surface%20attributes%20beyond%20texture%20color%2C%20such%20as%20physically-based%20rendering%20parameters.%20Based%20on%20O-Voxel%2C%20we%20design%20a%20Sparse%20Compression%20VAE%20which%20provides%20a%20high%20spatial%20compression%20rate%20and%20a%20compact%20latent%20space.%20We%20train%20large-scale%20flow-matching%20models%20comprising%204B%20parameters%20for%203D%20generation%20using%20diverse%20public%203D%20asset%20datasets.%20Despite%20their%20scale%2C%20inference%20remains%20highly%20efficient.%20Meanwhile%2C%20the%20geometry%20and%20material%20quality%20of%20our%20generated%20assets%20far%20exceed%20those%20of%20existing%20models.%20We%20believe%20our%20approach%20offers%20a%20significant%20advancement%20in%203D%20generative%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNative%2520and%2520Compact%2520Structured%2520Latents%2520for%25203D%2520Generation%26entry.906535625%3DJianfeng%2520Xiang%2520and%2520Xiaoxue%2520Chen%2520and%2520Sicheng%2520Xu%2520and%2520Ruicheng%2520Wang%2520and%2520Zelong%2520Lv%2520and%2520Yu%2520Deng%2520and%2520Hongyuan%2520Zhu%2520and%2520Yue%2520Dong%2520and%2520Hao%2520Zhao%2520and%2520Nicholas%2520Jing%2520Yuan%2520and%2520Jiaolong%2520Yang%26entry.1292438233%3DRecent%2520advancements%2520in%25203D%2520generative%2520modeling%2520have%2520significantly%2520improved%2520the%2520generation%2520realism%252C%2520yet%2520the%2520field%2520is%2520still%2520hampered%2520by%2520existing%2520representations%252C%2520which%2520struggle%2520to%2520capture%2520assets%2520with%2520complex%2520topologies%2520and%2520detailed%2520appearance.%2520This%2520paper%2520present%2520an%2520approach%2520for%2520learning%2520a%2520structured%2520latent%2520representation%2520from%2520native%25203D%2520data%2520to%2520address%2520this%2520challenge.%2520At%2520its%2520core%2520is%2520a%2520new%2520sparse%2520voxel%2520structure%2520called%2520O-Voxel%252C%2520an%2520omni-voxel%2520representation%2520that%2520encodes%2520both%2520geometry%2520and%2520appearance.%2520O-Voxel%2520can%2520robustly%2520model%2520arbitrary%2520topology%252C%2520including%2520open%252C%2520non-manifold%252C%2520and%2520fully-enclosed%2520surfaces%252C%2520while%2520capturing%2520comprehensive%2520surface%2520attributes%2520beyond%2520texture%2520color%252C%2520such%2520as%2520physically-based%2520rendering%2520parameters.%2520Based%2520on%2520O-Voxel%252C%2520we%2520design%2520a%2520Sparse%2520Compression%2520VAE%2520which%2520provides%2520a%2520high%2520spatial%2520compression%2520rate%2520and%2520a%2520compact%2520latent%2520space.%2520We%2520train%2520large-scale%2520flow-matching%2520models%2520comprising%25204B%2520parameters%2520for%25203D%2520generation%2520using%2520diverse%2520public%25203D%2520asset%2520datasets.%2520Despite%2520their%2520scale%252C%2520inference%2520remains%2520highly%2520efficient.%2520Meanwhile%252C%2520the%2520geometry%2520and%2520material%2520quality%2520of%2520our%2520generated%2520assets%2520far%2520exceed%2520those%2520of%2520existing%2520models.%2520We%2520believe%2520our%2520approach%2520offers%2520a%2520significant%2520advancement%2520in%25203D%2520generative%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Native%20and%20Compact%20Structured%20Latents%20for%203D%20Generation&entry.906535625=Jianfeng%20Xiang%20and%20Xiaoxue%20Chen%20and%20Sicheng%20Xu%20and%20Ruicheng%20Wang%20and%20Zelong%20Lv%20and%20Yu%20Deng%20and%20Hongyuan%20Zhu%20and%20Yue%20Dong%20and%20Hao%20Zhao%20and%20Nicholas%20Jing%20Yuan%20and%20Jiaolong%20Yang&entry.1292438233=Recent%20advancements%20in%203D%20generative%20modeling%20have%20significantly%20improved%20the%20generation%20realism%2C%20yet%20the%20field%20is%20still%20hampered%20by%20existing%20representations%2C%20which%20struggle%20to%20capture%20assets%20with%20complex%20topologies%20and%20detailed%20appearance.%20This%20paper%20present%20an%20approach%20for%20learning%20a%20structured%20latent%20representation%20from%20native%203D%20data%20to%20address%20this%20challenge.%20At%20its%20core%20is%20a%20new%20sparse%20voxel%20structure%20called%20O-Voxel%2C%20an%20omni-voxel%20representation%20that%20encodes%20both%20geometry%20and%20appearance.%20O-Voxel%20can%20robustly%20model%20arbitrary%20topology%2C%20including%20open%2C%20non-manifold%2C%20and%20fully-enclosed%20surfaces%2C%20while%20capturing%20comprehensive%20surface%20attributes%20beyond%20texture%20color%2C%20such%20as%20physically-based%20rendering%20parameters.%20Based%20on%20O-Voxel%2C%20we%20design%20a%20Sparse%20Compression%20VAE%20which%20provides%20a%20high%20spatial%20compression%20rate%20and%20a%20compact%20latent%20space.%20We%20train%20large-scale%20flow-matching%20models%20comprising%204B%20parameters%20for%203D%20generation%20using%20diverse%20public%203D%20asset%20datasets.%20Despite%20their%20scale%2C%20inference%20remains%20highly%20efficient.%20Meanwhile%2C%20the%20geometry%20and%20material%20quality%20of%20our%20generated%20assets%20far%20exceed%20those%20of%20existing%20models.%20We%20believe%20our%20approach%20offers%20a%20significant%20advancement%20in%203D%20generative%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.14692v1&entry.124074799=Read"},
{"title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors", "author": "Yiheng Huang and Junhong Chen and Anqi Ning and Zhanhong Liang and Nick Michiels and Luc Claesen and Wenyin Liu", "abstract": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.", "link": "http://arxiv.org/abs/2512.14536v1", "date": "2025-12-16", "relevancy": 2.3106, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5965}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5662}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DASP%3A%20Self-supervised%20Nighttime%20Monocular%20Depth%20Estimation%20with%20Domain%20Adaptation%20of%20Spatiotemporal%20Priors&body=Title%3A%20DASP%3A%20Self-supervised%20Nighttime%20Monocular%20Depth%20Estimation%20with%20Domain%20Adaptation%20of%20Spatiotemporal%20Priors%0AAuthor%3A%20Yiheng%20Huang%20and%20Junhong%20Chen%20and%20Anqi%20Ning%20and%20Zhanhong%20Liang%20and%20Nick%20Michiels%20and%20Luc%20Claesen%20and%20Wenyin%20Liu%0AAbstract%3A%20Self-supervised%20monocular%20depth%20estimation%20has%20achieved%20notable%20success%20under%20daytime%20conditions.%20However%2C%20its%20performance%20deteriorates%20markedly%20at%20night%20due%20to%20low%20visibility%20and%20varying%20illumination%2C%20e.g.%2C%20insufficient%20light%20causes%20textureless%20areas%2C%20and%20moving%20objects%20bring%20blurry%20regions.%20To%20this%20end%2C%20we%20propose%20a%20self-supervised%20framework%20named%20DASP%20that%20leverages%20spatiotemporal%20priors%20for%20nighttime%20depth%20estimation.%20Specifically%2C%20DASP%20consists%20of%20an%20adversarial%20branch%20for%20extracting%20spatiotemporal%20priors%20and%20a%20self-supervised%20branch%20for%20learning.%20In%20the%20adversarial%20branch%2C%20we%20first%20design%20an%20adversarial%20network%20where%20the%20discriminator%20is%20composed%20of%20four%20devised%20spatiotemporal%20priors%20learning%20blocks%20%28SPLB%29%20to%20exploit%20the%20daytime%20priors.%20In%20particular%2C%20the%20SPLB%20contains%20a%20spatial-based%20temporal%20learning%20module%20%28STLM%29%20that%20uses%20orthogonal%20differencing%20to%20extract%20motion-related%20variations%20along%20the%20time%20axis%20and%20an%20axial%20spatial%20learning%20module%20%28ASLM%29%20that%20adopts%20local%20asymmetric%20convolutions%20with%20global%20axial%20attention%20to%20capture%20the%20multiscale%20structural%20information.%20By%20combining%20STLM%20and%20ASLM%2C%20our%20model%20can%20acquire%20sufficient%20spatiotemporal%20features%20to%20restore%20textureless%20areas%20and%20estimate%20the%20blurry%20regions%20caused%20by%20dynamic%20objects.%20In%20the%20self-supervised%20branch%2C%20we%20propose%20a%203D%20consistency%20projection%20loss%20to%20bilaterally%20project%20the%20target%20frame%20and%20source%20frame%20into%20a%20shared%203D%20space%2C%20and%20calculate%20the%203D%20discrepancy%20between%20the%20two%20projected%20frames%20as%20a%20loss%20to%20optimize%20the%203D%20structural%20consistency%20and%20daytime%20priors.%20Extensive%20experiments%20on%20the%20Oxford%20RobotCar%20and%20nuScenes%20datasets%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20for%20nighttime%20depth%20estimation.%20Ablation%20studies%20further%20validate%20the%20effectiveness%20of%20each%20component.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDASP%253A%2520Self-supervised%2520Nighttime%2520Monocular%2520Depth%2520Estimation%2520with%2520Domain%2520Adaptation%2520of%2520Spatiotemporal%2520Priors%26entry.906535625%3DYiheng%2520Huang%2520and%2520Junhong%2520Chen%2520and%2520Anqi%2520Ning%2520and%2520Zhanhong%2520Liang%2520and%2520Nick%2520Michiels%2520and%2520Luc%2520Claesen%2520and%2520Wenyin%2520Liu%26entry.1292438233%3DSelf-supervised%2520monocular%2520depth%2520estimation%2520has%2520achieved%2520notable%2520success%2520under%2520daytime%2520conditions.%2520However%252C%2520its%2520performance%2520deteriorates%2520markedly%2520at%2520night%2520due%2520to%2520low%2520visibility%2520and%2520varying%2520illumination%252C%2520e.g.%252C%2520insufficient%2520light%2520causes%2520textureless%2520areas%252C%2520and%2520moving%2520objects%2520bring%2520blurry%2520regions.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520self-supervised%2520framework%2520named%2520DASP%2520that%2520leverages%2520spatiotemporal%2520priors%2520for%2520nighttime%2520depth%2520estimation.%2520Specifically%252C%2520DASP%2520consists%2520of%2520an%2520adversarial%2520branch%2520for%2520extracting%2520spatiotemporal%2520priors%2520and%2520a%2520self-supervised%2520branch%2520for%2520learning.%2520In%2520the%2520adversarial%2520branch%252C%2520we%2520first%2520design%2520an%2520adversarial%2520network%2520where%2520the%2520discriminator%2520is%2520composed%2520of%2520four%2520devised%2520spatiotemporal%2520priors%2520learning%2520blocks%2520%2528SPLB%2529%2520to%2520exploit%2520the%2520daytime%2520priors.%2520In%2520particular%252C%2520the%2520SPLB%2520contains%2520a%2520spatial-based%2520temporal%2520learning%2520module%2520%2528STLM%2529%2520that%2520uses%2520orthogonal%2520differencing%2520to%2520extract%2520motion-related%2520variations%2520along%2520the%2520time%2520axis%2520and%2520an%2520axial%2520spatial%2520learning%2520module%2520%2528ASLM%2529%2520that%2520adopts%2520local%2520asymmetric%2520convolutions%2520with%2520global%2520axial%2520attention%2520to%2520capture%2520the%2520multiscale%2520structural%2520information.%2520By%2520combining%2520STLM%2520and%2520ASLM%252C%2520our%2520model%2520can%2520acquire%2520sufficient%2520spatiotemporal%2520features%2520to%2520restore%2520textureless%2520areas%2520and%2520estimate%2520the%2520blurry%2520regions%2520caused%2520by%2520dynamic%2520objects.%2520In%2520the%2520self-supervised%2520branch%252C%2520we%2520propose%2520a%25203D%2520consistency%2520projection%2520loss%2520to%2520bilaterally%2520project%2520the%2520target%2520frame%2520and%2520source%2520frame%2520into%2520a%2520shared%25203D%2520space%252C%2520and%2520calculate%2520the%25203D%2520discrepancy%2520between%2520the%2520two%2520projected%2520frames%2520as%2520a%2520loss%2520to%2520optimize%2520the%25203D%2520structural%2520consistency%2520and%2520daytime%2520priors.%2520Extensive%2520experiments%2520on%2520the%2520Oxford%2520RobotCar%2520and%2520nuScenes%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520for%2520nighttime%2520depth%2520estimation.%2520Ablation%2520studies%2520further%2520validate%2520the%2520effectiveness%2520of%2520each%2520component.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DASP%3A%20Self-supervised%20Nighttime%20Monocular%20Depth%20Estimation%20with%20Domain%20Adaptation%20of%20Spatiotemporal%20Priors&entry.906535625=Yiheng%20Huang%20and%20Junhong%20Chen%20and%20Anqi%20Ning%20and%20Zhanhong%20Liang%20and%20Nick%20Michiels%20and%20Luc%20Claesen%20and%20Wenyin%20Liu&entry.1292438233=Self-supervised%20monocular%20depth%20estimation%20has%20achieved%20notable%20success%20under%20daytime%20conditions.%20However%2C%20its%20performance%20deteriorates%20markedly%20at%20night%20due%20to%20low%20visibility%20and%20varying%20illumination%2C%20e.g.%2C%20insufficient%20light%20causes%20textureless%20areas%2C%20and%20moving%20objects%20bring%20blurry%20regions.%20To%20this%20end%2C%20we%20propose%20a%20self-supervised%20framework%20named%20DASP%20that%20leverages%20spatiotemporal%20priors%20for%20nighttime%20depth%20estimation.%20Specifically%2C%20DASP%20consists%20of%20an%20adversarial%20branch%20for%20extracting%20spatiotemporal%20priors%20and%20a%20self-supervised%20branch%20for%20learning.%20In%20the%20adversarial%20branch%2C%20we%20first%20design%20an%20adversarial%20network%20where%20the%20discriminator%20is%20composed%20of%20four%20devised%20spatiotemporal%20priors%20learning%20blocks%20%28SPLB%29%20to%20exploit%20the%20daytime%20priors.%20In%20particular%2C%20the%20SPLB%20contains%20a%20spatial-based%20temporal%20learning%20module%20%28STLM%29%20that%20uses%20orthogonal%20differencing%20to%20extract%20motion-related%20variations%20along%20the%20time%20axis%20and%20an%20axial%20spatial%20learning%20module%20%28ASLM%29%20that%20adopts%20local%20asymmetric%20convolutions%20with%20global%20axial%20attention%20to%20capture%20the%20multiscale%20structural%20information.%20By%20combining%20STLM%20and%20ASLM%2C%20our%20model%20can%20acquire%20sufficient%20spatiotemporal%20features%20to%20restore%20textureless%20areas%20and%20estimate%20the%20blurry%20regions%20caused%20by%20dynamic%20objects.%20In%20the%20self-supervised%20branch%2C%20we%20propose%20a%203D%20consistency%20projection%20loss%20to%20bilaterally%20project%20the%20target%20frame%20and%20source%20frame%20into%20a%20shared%203D%20space%2C%20and%20calculate%20the%203D%20discrepancy%20between%20the%20two%20projected%20frames%20as%20a%20loss%20to%20optimize%20the%203D%20structural%20consistency%20and%20daytime%20priors.%20Extensive%20experiments%20on%20the%20Oxford%20RobotCar%20and%20nuScenes%20datasets%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20for%20nighttime%20depth%20estimation.%20Ablation%20studies%20further%20validate%20the%20effectiveness%20of%20each%20component.&entry.1838667208=http%3A//arxiv.org/abs/2512.14536v1&entry.124074799=Read"},
{"title": "A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation", "author": "Xianping Ma and Xiaokang Zhang and Man-On Pun and Bo Huang", "abstract": "Multimodal remote sensing data, acquired from diverse sensors, offer a comprehensive and integrated perspective of the Earth's surface. Leveraging multimodal fusion techniques, semantic segmentation enables detailed and accurate analysis of geographic scenes, surpassing single-modality approaches. Building on advancements in vision foundation models, particularly the Segment Anything Model (SAM), this study proposes a unified framework incorporating a novel Multimodal Fine-tuning Network (MFNet) for remote sensing semantic segmentation. The proposed framework is designed to seamlessly integrate with various fine-tuning mechanisms, demonstrated through the inclusion of Adapter and Low-Rank Adaptation (LoRA) as representative examples. This extensibility ensures the framework's adaptability to other emerging fine-tuning strategies, allowing models to retain SAM's general knowledge while effectively leveraging multimodal data. Additionally, a pyramid-based Deep Fusion Module (DFM) is introduced to integrate high-level geographic features across multiple scales, enhancing feature representation prior to decoding. This work also highlights SAM's robust generalization capabilities with Digital Surface Model (DSM) data, a novel application. Extensive experiments on three benchmark multimodal remote sensing datasets, ISPRS Vaihingen, ISPRS Potsdam and MMHunan, demonstrate that the proposed MFNet significantly outperforms existing methods in multimodal semantic segmentation, setting a new standard in the field while offering a versatile foundation for future research and applications. The source code for this work is accessible at https://github.com/sstary/SSRS.", "link": "http://arxiv.org/abs/2410.11160v2", "date": "2025-12-16", "relevancy": 2.2991, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5788}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5764}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20with%20Multimodal%20Fine-tuning%20for%20Remote%20Sensing%20Semantic%20Segmentation&body=Title%3A%20A%20Unified%20Framework%20with%20Multimodal%20Fine-tuning%20for%20Remote%20Sensing%20Semantic%20Segmentation%0AAuthor%3A%20Xianping%20Ma%20and%20Xiaokang%20Zhang%20and%20Man-On%20Pun%20and%20Bo%20Huang%0AAbstract%3A%20Multimodal%20remote%20sensing%20data%2C%20acquired%20from%20diverse%20sensors%2C%20offer%20a%20comprehensive%20and%20integrated%20perspective%20of%20the%20Earth%27s%20surface.%20Leveraging%20multimodal%20fusion%20techniques%2C%20semantic%20segmentation%20enables%20detailed%20and%20accurate%20analysis%20of%20geographic%20scenes%2C%20surpassing%20single-modality%20approaches.%20Building%20on%20advancements%20in%20vision%20foundation%20models%2C%20particularly%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20this%20study%20proposes%20a%20unified%20framework%20incorporating%20a%20novel%20Multimodal%20Fine-tuning%20Network%20%28MFNet%29%20for%20remote%20sensing%20semantic%20segmentation.%20The%20proposed%20framework%20is%20designed%20to%20seamlessly%20integrate%20with%20various%20fine-tuning%20mechanisms%2C%20demonstrated%20through%20the%20inclusion%20of%20Adapter%20and%20Low-Rank%20Adaptation%20%28LoRA%29%20as%20representative%20examples.%20This%20extensibility%20ensures%20the%20framework%27s%20adaptability%20to%20other%20emerging%20fine-tuning%20strategies%2C%20allowing%20models%20to%20retain%20SAM%27s%20general%20knowledge%20while%20effectively%20leveraging%20multimodal%20data.%20Additionally%2C%20a%20pyramid-based%20Deep%20Fusion%20Module%20%28DFM%29%20is%20introduced%20to%20integrate%20high-level%20geographic%20features%20across%20multiple%20scales%2C%20enhancing%20feature%20representation%20prior%20to%20decoding.%20This%20work%20also%20highlights%20SAM%27s%20robust%20generalization%20capabilities%20with%20Digital%20Surface%20Model%20%28DSM%29%20data%2C%20a%20novel%20application.%20Extensive%20experiments%20on%20three%20benchmark%20multimodal%20remote%20sensing%20datasets%2C%20ISPRS%20Vaihingen%2C%20ISPRS%20Potsdam%20and%20MMHunan%2C%20demonstrate%20that%20the%20proposed%20MFNet%20significantly%20outperforms%20existing%20methods%20in%20multimodal%20semantic%20segmentation%2C%20setting%20a%20new%20standard%20in%20the%20field%20while%20offering%20a%20versatile%20foundation%20for%20future%20research%20and%20applications.%20The%20source%20code%20for%20this%20work%20is%20accessible%20at%20https%3A//github.com/sstary/SSRS.%0ALink%3A%20http%3A//arxiv.org/abs/2410.11160v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520with%2520Multimodal%2520Fine-tuning%2520for%2520Remote%2520Sensing%2520Semantic%2520Segmentation%26entry.906535625%3DXianping%2520Ma%2520and%2520Xiaokang%2520Zhang%2520and%2520Man-On%2520Pun%2520and%2520Bo%2520Huang%26entry.1292438233%3DMultimodal%2520remote%2520sensing%2520data%252C%2520acquired%2520from%2520diverse%2520sensors%252C%2520offer%2520a%2520comprehensive%2520and%2520integrated%2520perspective%2520of%2520the%2520Earth%2527s%2520surface.%2520Leveraging%2520multimodal%2520fusion%2520techniques%252C%2520semantic%2520segmentation%2520enables%2520detailed%2520and%2520accurate%2520analysis%2520of%2520geographic%2520scenes%252C%2520surpassing%2520single-modality%2520approaches.%2520Building%2520on%2520advancements%2520in%2520vision%2520foundation%2520models%252C%2520particularly%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520this%2520study%2520proposes%2520a%2520unified%2520framework%2520incorporating%2520a%2520novel%2520Multimodal%2520Fine-tuning%2520Network%2520%2528MFNet%2529%2520for%2520remote%2520sensing%2520semantic%2520segmentation.%2520The%2520proposed%2520framework%2520is%2520designed%2520to%2520seamlessly%2520integrate%2520with%2520various%2520fine-tuning%2520mechanisms%252C%2520demonstrated%2520through%2520the%2520inclusion%2520of%2520Adapter%2520and%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520as%2520representative%2520examples.%2520This%2520extensibility%2520ensures%2520the%2520framework%2527s%2520adaptability%2520to%2520other%2520emerging%2520fine-tuning%2520strategies%252C%2520allowing%2520models%2520to%2520retain%2520SAM%2527s%2520general%2520knowledge%2520while%2520effectively%2520leveraging%2520multimodal%2520data.%2520Additionally%252C%2520a%2520pyramid-based%2520Deep%2520Fusion%2520Module%2520%2528DFM%2529%2520is%2520introduced%2520to%2520integrate%2520high-level%2520geographic%2520features%2520across%2520multiple%2520scales%252C%2520enhancing%2520feature%2520representation%2520prior%2520to%2520decoding.%2520This%2520work%2520also%2520highlights%2520SAM%2527s%2520robust%2520generalization%2520capabilities%2520with%2520Digital%2520Surface%2520Model%2520%2528DSM%2529%2520data%252C%2520a%2520novel%2520application.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520multimodal%2520remote%2520sensing%2520datasets%252C%2520ISPRS%2520Vaihingen%252C%2520ISPRS%2520Potsdam%2520and%2520MMHunan%252C%2520demonstrate%2520that%2520the%2520proposed%2520MFNet%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520multimodal%2520semantic%2520segmentation%252C%2520setting%2520a%2520new%2520standard%2520in%2520the%2520field%2520while%2520offering%2520a%2520versatile%2520foundation%2520for%2520future%2520research%2520and%2520applications.%2520The%2520source%2520code%2520for%2520this%2520work%2520is%2520accessible%2520at%2520https%253A//github.com/sstary/SSRS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11160v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20with%20Multimodal%20Fine-tuning%20for%20Remote%20Sensing%20Semantic%20Segmentation&entry.906535625=Xianping%20Ma%20and%20Xiaokang%20Zhang%20and%20Man-On%20Pun%20and%20Bo%20Huang&entry.1292438233=Multimodal%20remote%20sensing%20data%2C%20acquired%20from%20diverse%20sensors%2C%20offer%20a%20comprehensive%20and%20integrated%20perspective%20of%20the%20Earth%27s%20surface.%20Leveraging%20multimodal%20fusion%20techniques%2C%20semantic%20segmentation%20enables%20detailed%20and%20accurate%20analysis%20of%20geographic%20scenes%2C%20surpassing%20single-modality%20approaches.%20Building%20on%20advancements%20in%20vision%20foundation%20models%2C%20particularly%20the%20Segment%20Anything%20Model%20%28SAM%29%2C%20this%20study%20proposes%20a%20unified%20framework%20incorporating%20a%20novel%20Multimodal%20Fine-tuning%20Network%20%28MFNet%29%20for%20remote%20sensing%20semantic%20segmentation.%20The%20proposed%20framework%20is%20designed%20to%20seamlessly%20integrate%20with%20various%20fine-tuning%20mechanisms%2C%20demonstrated%20through%20the%20inclusion%20of%20Adapter%20and%20Low-Rank%20Adaptation%20%28LoRA%29%20as%20representative%20examples.%20This%20extensibility%20ensures%20the%20framework%27s%20adaptability%20to%20other%20emerging%20fine-tuning%20strategies%2C%20allowing%20models%20to%20retain%20SAM%27s%20general%20knowledge%20while%20effectively%20leveraging%20multimodal%20data.%20Additionally%2C%20a%20pyramid-based%20Deep%20Fusion%20Module%20%28DFM%29%20is%20introduced%20to%20integrate%20high-level%20geographic%20features%20across%20multiple%20scales%2C%20enhancing%20feature%20representation%20prior%20to%20decoding.%20This%20work%20also%20highlights%20SAM%27s%20robust%20generalization%20capabilities%20with%20Digital%20Surface%20Model%20%28DSM%29%20data%2C%20a%20novel%20application.%20Extensive%20experiments%20on%20three%20benchmark%20multimodal%20remote%20sensing%20datasets%2C%20ISPRS%20Vaihingen%2C%20ISPRS%20Potsdam%20and%20MMHunan%2C%20demonstrate%20that%20the%20proposed%20MFNet%20significantly%20outperforms%20existing%20methods%20in%20multimodal%20semantic%20segmentation%2C%20setting%20a%20new%20standard%20in%20the%20field%20while%20offering%20a%20versatile%20foundation%20for%20future%20research%20and%20applications.%20The%20source%20code%20for%20this%20work%20is%20accessible%20at%20https%3A//github.com/sstary/SSRS.&entry.1838667208=http%3A//arxiv.org/abs/2410.11160v2&entry.124074799=Read"},
{"title": "Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training", "author": "Tidiane Camaret Ndir and Alexander Pfefferle and Robin Tibor Schirrmeister", "abstract": "Interactive 3D biomedical image segmentation requires efficient models that can iteratively refine predictions based on user prompts. Current foundation models either lack volumetric awareness or suffer from limited interactive capabilities. We propose a training strategy that combines dynamic volumetric prompt generation with content-aware adaptive cropping to optimize the use of the image encoder. Our method simulates realistic user interaction patterns during training while addressing the computational challenges of learning from sequential refinement feedback on a single GPU. For efficient training, we initialize our network using the publicly available weights from the nnInteractive segmentation model. Evaluation on the \\textbf{Foundation Models for Interactive 3D Biomedical Image Segmentation} competition demonstrates strong performance with an average final Dice score of 0.6385, normalized surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice) and 2.5671 (NSD).", "link": "http://arxiv.org/abs/2510.03189v3", "date": "2025-12-16", "relevancy": 2.2807, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5888}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5666}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Prompt%20Generation%20for%20Interactive%203D%20Medical%20Image%20Segmentation%20Training&body=Title%3A%20Dynamic%20Prompt%20Generation%20for%20Interactive%203D%20Medical%20Image%20Segmentation%20Training%0AAuthor%3A%20Tidiane%20Camaret%20Ndir%20and%20Alexander%20Pfefferle%20and%20Robin%20Tibor%20Schirrmeister%0AAbstract%3A%20Interactive%203D%20biomedical%20image%20segmentation%20requires%20efficient%20models%20that%20can%20iteratively%20refine%20predictions%20based%20on%20user%20prompts.%20Current%20foundation%20models%20either%20lack%20volumetric%20awareness%20or%20suffer%20from%20limited%20interactive%20capabilities.%20We%20propose%20a%20training%20strategy%20that%20combines%20dynamic%20volumetric%20prompt%20generation%20with%20content-aware%20adaptive%20cropping%20to%20optimize%20the%20use%20of%20the%20image%20encoder.%20Our%20method%20simulates%20realistic%20user%20interaction%20patterns%20during%20training%20while%20addressing%20the%20computational%20challenges%20of%20learning%20from%20sequential%20refinement%20feedback%20on%20a%20single%20GPU.%20For%20efficient%20training%2C%20we%20initialize%20our%20network%20using%20the%20publicly%20available%20weights%20from%20the%20nnInteractive%20segmentation%20model.%20Evaluation%20on%20the%20%5Ctextbf%7BFoundation%20Models%20for%20Interactive%203D%20Biomedical%20Image%20Segmentation%7D%20competition%20demonstrates%20strong%20performance%20with%20an%20average%20final%20Dice%20score%20of%200.6385%2C%20normalized%20surface%20distance%20of%200.6614%2C%20and%20area-under-the-curve%20metrics%20of%202.4799%20%28Dice%29%20and%202.5671%20%28NSD%29.%0ALink%3A%20http%3A//arxiv.org/abs/2510.03189v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Prompt%2520Generation%2520for%2520Interactive%25203D%2520Medical%2520Image%2520Segmentation%2520Training%26entry.906535625%3DTidiane%2520Camaret%2520Ndir%2520and%2520Alexander%2520Pfefferle%2520and%2520Robin%2520Tibor%2520Schirrmeister%26entry.1292438233%3DInteractive%25203D%2520biomedical%2520image%2520segmentation%2520requires%2520efficient%2520models%2520that%2520can%2520iteratively%2520refine%2520predictions%2520based%2520on%2520user%2520prompts.%2520Current%2520foundation%2520models%2520either%2520lack%2520volumetric%2520awareness%2520or%2520suffer%2520from%2520limited%2520interactive%2520capabilities.%2520We%2520propose%2520a%2520training%2520strategy%2520that%2520combines%2520dynamic%2520volumetric%2520prompt%2520generation%2520with%2520content-aware%2520adaptive%2520cropping%2520to%2520optimize%2520the%2520use%2520of%2520the%2520image%2520encoder.%2520Our%2520method%2520simulates%2520realistic%2520user%2520interaction%2520patterns%2520during%2520training%2520while%2520addressing%2520the%2520computational%2520challenges%2520of%2520learning%2520from%2520sequential%2520refinement%2520feedback%2520on%2520a%2520single%2520GPU.%2520For%2520efficient%2520training%252C%2520we%2520initialize%2520our%2520network%2520using%2520the%2520publicly%2520available%2520weights%2520from%2520the%2520nnInteractive%2520segmentation%2520model.%2520Evaluation%2520on%2520the%2520%255Ctextbf%257BFoundation%2520Models%2520for%2520Interactive%25203D%2520Biomedical%2520Image%2520Segmentation%257D%2520competition%2520demonstrates%2520strong%2520performance%2520with%2520an%2520average%2520final%2520Dice%2520score%2520of%25200.6385%252C%2520normalized%2520surface%2520distance%2520of%25200.6614%252C%2520and%2520area-under-the-curve%2520metrics%2520of%25202.4799%2520%2528Dice%2529%2520and%25202.5671%2520%2528NSD%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03189v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Prompt%20Generation%20for%20Interactive%203D%20Medical%20Image%20Segmentation%20Training&entry.906535625=Tidiane%20Camaret%20Ndir%20and%20Alexander%20Pfefferle%20and%20Robin%20Tibor%20Schirrmeister&entry.1292438233=Interactive%203D%20biomedical%20image%20segmentation%20requires%20efficient%20models%20that%20can%20iteratively%20refine%20predictions%20based%20on%20user%20prompts.%20Current%20foundation%20models%20either%20lack%20volumetric%20awareness%20or%20suffer%20from%20limited%20interactive%20capabilities.%20We%20propose%20a%20training%20strategy%20that%20combines%20dynamic%20volumetric%20prompt%20generation%20with%20content-aware%20adaptive%20cropping%20to%20optimize%20the%20use%20of%20the%20image%20encoder.%20Our%20method%20simulates%20realistic%20user%20interaction%20patterns%20during%20training%20while%20addressing%20the%20computational%20challenges%20of%20learning%20from%20sequential%20refinement%20feedback%20on%20a%20single%20GPU.%20For%20efficient%20training%2C%20we%20initialize%20our%20network%20using%20the%20publicly%20available%20weights%20from%20the%20nnInteractive%20segmentation%20model.%20Evaluation%20on%20the%20%5Ctextbf%7BFoundation%20Models%20for%20Interactive%203D%20Biomedical%20Image%20Segmentation%7D%20competition%20demonstrates%20strong%20performance%20with%20an%20average%20final%20Dice%20score%20of%200.6385%2C%20normalized%20surface%20distance%20of%200.6614%2C%20and%20area-under-the-curve%20metrics%20of%202.4799%20%28Dice%29%20and%202.5671%20%28NSD%29.&entry.1838667208=http%3A//arxiv.org/abs/2510.03189v3&entry.124074799=Read"},
{"title": "C-ing Clearly: Enhanced Binary Code Explanations using C code", "author": "Teodor Poncu and Ioana Pintilie and Marius Dragoi and Dragos Tantaru and Florin Brad", "abstract": "Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.", "link": "http://arxiv.org/abs/2512.14500v1", "date": "2025-12-16", "relevancy": 2.2806, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-ing%20Clearly%3A%20Enhanced%20Binary%20Code%20Explanations%20using%20C%20code&body=Title%3A%20C-ing%20Clearly%3A%20Enhanced%20Binary%20Code%20Explanations%20using%20C%20code%0AAuthor%3A%20Teodor%20Poncu%20and%20Ioana%20Pintilie%20and%20Marius%20Dragoi%20and%20Dragos%20Tantaru%20and%20Florin%20Brad%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20typically%20excel%20at%20coding%20tasks%20involving%20high-level%20programming%20languages%2C%20as%20opposed%20to%20lower-level%20programming%20languages%2C%20such%20as%20assembly.%20We%20propose%20a%20synthetic%20data%20generation%20method%20named%20C-ing%20Clearly%2C%20which%20leverages%20the%20corresponding%20C%20code%20to%20enhance%20an%20LLM%27s%20understanding%20of%20assembly.%20By%20fine-tuning%20on%20data%20generated%20through%20our%20method%2C%20we%20demonstrate%20improved%20LLM%20performance%20for%20binary%20code%20summarization%20and%20vulnerability%20detection.%20Our%20approach%20demonstrates%20consistent%20gains%20across%20different%20LLM%20families%20and%20model%20sizes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-ing%2520Clearly%253A%2520Enhanced%2520Binary%2520Code%2520Explanations%2520using%2520C%2520code%26entry.906535625%3DTeodor%2520Poncu%2520and%2520Ioana%2520Pintilie%2520and%2520Marius%2520Dragoi%2520and%2520Dragos%2520Tantaru%2520and%2520Florin%2520Brad%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520typically%2520excel%2520at%2520coding%2520tasks%2520involving%2520high-level%2520programming%2520languages%252C%2520as%2520opposed%2520to%2520lower-level%2520programming%2520languages%252C%2520such%2520as%2520assembly.%2520We%2520propose%2520a%2520synthetic%2520data%2520generation%2520method%2520named%2520C-ing%2520Clearly%252C%2520which%2520leverages%2520the%2520corresponding%2520C%2520code%2520to%2520enhance%2520an%2520LLM%2527s%2520understanding%2520of%2520assembly.%2520By%2520fine-tuning%2520on%2520data%2520generated%2520through%2520our%2520method%252C%2520we%2520demonstrate%2520improved%2520LLM%2520performance%2520for%2520binary%2520code%2520summarization%2520and%2520vulnerability%2520detection.%2520Our%2520approach%2520demonstrates%2520consistent%2520gains%2520across%2520different%2520LLM%2520families%2520and%2520model%2520sizes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-ing%20Clearly%3A%20Enhanced%20Binary%20Code%20Explanations%20using%20C%20code&entry.906535625=Teodor%20Poncu%20and%20Ioana%20Pintilie%20and%20Marius%20Dragoi%20and%20Dragos%20Tantaru%20and%20Florin%20Brad&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20typically%20excel%20at%20coding%20tasks%20involving%20high-level%20programming%20languages%2C%20as%20opposed%20to%20lower-level%20programming%20languages%2C%20such%20as%20assembly.%20We%20propose%20a%20synthetic%20data%20generation%20method%20named%20C-ing%20Clearly%2C%20which%20leverages%20the%20corresponding%20C%20code%20to%20enhance%20an%20LLM%27s%20understanding%20of%20assembly.%20By%20fine-tuning%20on%20data%20generated%20through%20our%20method%2C%20we%20demonstrate%20improved%20LLM%20performance%20for%20binary%20code%20summarization%20and%20vulnerability%20detection.%20Our%20approach%20demonstrates%20consistent%20gains%20across%20different%20LLM%20families%20and%20model%20sizes.&entry.1838667208=http%3A//arxiv.org/abs/2512.14500v1&entry.124074799=Read"},
{"title": "HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion", "author": "Yifang Xu and Benxiang Zhai and Yunzhuo Sun and Ming Li and Yang Li and Sidan Du", "abstract": "Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.", "link": "http://arxiv.org/abs/2512.14542v1", "date": "2025-12-16", "relevancy": 2.2793, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6122}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5696}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiFi-Portrait%3A%20Zero-shot%20Identity-preserved%20Portrait%20Generation%20with%20High-fidelity%20Multi-face%20Fusion&body=Title%3A%20HiFi-Portrait%3A%20Zero-shot%20Identity-preserved%20Portrait%20Generation%20with%20High-fidelity%20Multi-face%20Fusion%0AAuthor%3A%20Yifang%20Xu%20and%20Benxiang%20Zhai%20and%20Yunzhuo%20Sun%20and%20Ming%20Li%20and%20Yang%20Li%20and%20Sidan%20Du%0AAbstract%3A%20Recent%20advancements%20in%20diffusion-based%20technologies%20have%20made%20significant%20strides%2C%20particularly%20in%20identity-preserved%20portrait%20generation%20%28IPG%29.%20However%2C%20when%20using%20multiple%20reference%20images%20from%20the%20same%20ID%2C%20existing%20methods%20typically%20produce%20lower-fidelity%20portraits%20and%20struggle%20to%20customize%20face%20attributes%20precisely.%20To%20address%20these%20issues%2C%20this%20paper%20presents%20HiFi-Portrait%2C%20a%20high-fidelity%20method%20for%20zero-shot%20portrait%20generation.%20Specifically%2C%20we%20first%20introduce%20the%20face%20refiner%20and%20landmark%20generator%20to%20obtain%20fine-grained%20multi-face%20features%20and%203D-aware%20face%20landmarks.%20The%20landmarks%20include%20the%20reference%20ID%20and%20the%20target%20attributes.%20Then%2C%20we%20design%20HiFi-Net%20to%20fuse%20multi-face%20features%20and%20align%20them%20with%20landmarks%2C%20which%20improves%20ID%20fidelity%20and%20face%20control.%20In%20addition%2C%20we%20devise%20an%20automated%20pipeline%20to%20construct%20an%20ID-based%20dataset%20for%20training%20HiFi-Portrait.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20method%20surpasses%20the%20SOTA%20approaches%20in%20face%20similarity%20and%20controllability.%20Furthermore%2C%20our%20method%20is%20also%20compatible%20with%20previous%20SDXL-based%20works.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiFi-Portrait%253A%2520Zero-shot%2520Identity-preserved%2520Portrait%2520Generation%2520with%2520High-fidelity%2520Multi-face%2520Fusion%26entry.906535625%3DYifang%2520Xu%2520and%2520Benxiang%2520Zhai%2520and%2520Yunzhuo%2520Sun%2520and%2520Ming%2520Li%2520and%2520Yang%2520Li%2520and%2520Sidan%2520Du%26entry.1292438233%3DRecent%2520advancements%2520in%2520diffusion-based%2520technologies%2520have%2520made%2520significant%2520strides%252C%2520particularly%2520in%2520identity-preserved%2520portrait%2520generation%2520%2528IPG%2529.%2520However%252C%2520when%2520using%2520multiple%2520reference%2520images%2520from%2520the%2520same%2520ID%252C%2520existing%2520methods%2520typically%2520produce%2520lower-fidelity%2520portraits%2520and%2520struggle%2520to%2520customize%2520face%2520attributes%2520precisely.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520presents%2520HiFi-Portrait%252C%2520a%2520high-fidelity%2520method%2520for%2520zero-shot%2520portrait%2520generation.%2520Specifically%252C%2520we%2520first%2520introduce%2520the%2520face%2520refiner%2520and%2520landmark%2520generator%2520to%2520obtain%2520fine-grained%2520multi-face%2520features%2520and%25203D-aware%2520face%2520landmarks.%2520The%2520landmarks%2520include%2520the%2520reference%2520ID%2520and%2520the%2520target%2520attributes.%2520Then%252C%2520we%2520design%2520HiFi-Net%2520to%2520fuse%2520multi-face%2520features%2520and%2520align%2520them%2520with%2520landmarks%252C%2520which%2520improves%2520ID%2520fidelity%2520and%2520face%2520control.%2520In%2520addition%252C%2520we%2520devise%2520an%2520automated%2520pipeline%2520to%2520construct%2520an%2520ID-based%2520dataset%2520for%2520training%2520HiFi-Portrait.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520surpasses%2520the%2520SOTA%2520approaches%2520in%2520face%2520similarity%2520and%2520controllability.%2520Furthermore%252C%2520our%2520method%2520is%2520also%2520compatible%2520with%2520previous%2520SDXL-based%2520works.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiFi-Portrait%3A%20Zero-shot%20Identity-preserved%20Portrait%20Generation%20with%20High-fidelity%20Multi-face%20Fusion&entry.906535625=Yifang%20Xu%20and%20Benxiang%20Zhai%20and%20Yunzhuo%20Sun%20and%20Ming%20Li%20and%20Yang%20Li%20and%20Sidan%20Du&entry.1292438233=Recent%20advancements%20in%20diffusion-based%20technologies%20have%20made%20significant%20strides%2C%20particularly%20in%20identity-preserved%20portrait%20generation%20%28IPG%29.%20However%2C%20when%20using%20multiple%20reference%20images%20from%20the%20same%20ID%2C%20existing%20methods%20typically%20produce%20lower-fidelity%20portraits%20and%20struggle%20to%20customize%20face%20attributes%20precisely.%20To%20address%20these%20issues%2C%20this%20paper%20presents%20HiFi-Portrait%2C%20a%20high-fidelity%20method%20for%20zero-shot%20portrait%20generation.%20Specifically%2C%20we%20first%20introduce%20the%20face%20refiner%20and%20landmark%20generator%20to%20obtain%20fine-grained%20multi-face%20features%20and%203D-aware%20face%20landmarks.%20The%20landmarks%20include%20the%20reference%20ID%20and%20the%20target%20attributes.%20Then%2C%20we%20design%20HiFi-Net%20to%20fuse%20multi-face%20features%20and%20align%20them%20with%20landmarks%2C%20which%20improves%20ID%20fidelity%20and%20face%20control.%20In%20addition%2C%20we%20devise%20an%20automated%20pipeline%20to%20construct%20an%20ID-based%20dataset%20for%20training%20HiFi-Portrait.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20method%20surpasses%20the%20SOTA%20approaches%20in%20face%20similarity%20and%20controllability.%20Furthermore%2C%20our%20method%20is%20also%20compatible%20with%20previous%20SDXL-based%20works.&entry.1838667208=http%3A//arxiv.org/abs/2512.14542v1&entry.124074799=Read"},
{"title": "A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data", "author": "Yanning Dai and Chenyu Tang and Ruizhi Zhang and Wenyu Yang and Yilan Zhang and Yuhui Wang and Junliang Chen and Xuhang Chen and Ruimou Xie and Yangyue Cao and Qiaoying Li and Jin Cao and Tao Li and Hubin Zhao and Yu Pan and Arokia Nathan and Xin Gao and Peter Smielewski and Shuo Gao", "abstract": "Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.", "link": "http://arxiv.org/abs/2512.14329v1", "date": "2025-12-16", "relevancy": 2.2604, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5954}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5614}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20data-physics%20hybrid%20generative%20model%20for%20patient-specific%20post-stroke%20motor%20rehabilitation%20using%20wearable%20sensor%20data&body=Title%3A%20A%20data-physics%20hybrid%20generative%20model%20for%20patient-specific%20post-stroke%20motor%20rehabilitation%20using%20wearable%20sensor%20data%0AAuthor%3A%20Yanning%20Dai%20and%20Chenyu%20Tang%20and%20Ruizhi%20Zhang%20and%20Wenyu%20Yang%20and%20Yilan%20Zhang%20and%20Yuhui%20Wang%20and%20Junliang%20Chen%20and%20Xuhang%20Chen%20and%20Ruimou%20Xie%20and%20Yangyue%20Cao%20and%20Qiaoying%20Li%20and%20Jin%20Cao%20and%20Tao%20Li%20and%20Hubin%20Zhao%20and%20Yu%20Pan%20and%20Arokia%20Nathan%20and%20Xin%20Gao%20and%20Peter%20Smielewski%20and%20Shuo%20Gao%0AAbstract%3A%20Dynamic%20prediction%20of%20locomotor%20capacity%20after%20stroke%20is%20crucial%20for%20tailoring%20rehabilitation%2C%20yet%20current%20assessments%20provide%20only%20static%20impairment%20scores%20and%20do%20not%20indicate%20whether%20patients%20can%20safely%20perform%20specific%20tasks%20such%20as%20slope%20walking%20or%20stair%20climbing.%20Here%2C%20we%20develop%20a%20data-physics%20hybrid%20generative%20framework%20that%20reconstructs%20an%20individual%20stroke%20survivor%27s%20neuromuscular%20control%20from%20a%20single%2020%20m%20level-ground%20walking%20trial%20and%20predicts%20task-conditioned%20locomotion%20across%20rehabilitation%20scenarios.%20The%20system%20combines%20wearable-sensor%20kinematics%2C%20a%20proportional-derivative%20physics%20controller%2C%20a%20population%20Healthy%20Motion%20Atlas%2C%20and%20goal-conditioned%20deep%20reinforcement%20learning%20with%20behaviour%20cloning%20and%20generative%20adversarial%20imitation%20learning%20to%20generate%20physically%20plausible%2C%20patient-specific%20gait%20simulations%20for%20slopes%20and%20stairs.%20In%2011%20stroke%20survivors%2C%20the%20personalized%20controllers%20preserved%20idiosyncratic%20gait%20patterns%20while%20improving%20joint-angle%20and%20endpoint%20fidelity%20by%204.73%25%20and%2012.10%25%2C%20respectively%2C%20and%20reducing%20training%20time%20to%2025.56%25%20relative%20to%20a%20physics-only%20baseline.%20In%20a%20multicentre%20pilot%20involving%2021%20inpatients%2C%20clinicians%20who%20used%20our%20locomotion%20predictions%20to%20guide%20task%20selection%20and%20difficulty%20obtained%20larger%20gains%20in%20Fugl-Meyer%20lower-extremity%20scores%20over%2028%20days%20of%20standard%20rehabilitation%20than%20control%20clinicians%20%28mean%20change%206.0%20versus%203.7%20points%29.%20These%20findings%20indicate%20that%20our%20generative%2C%20task-predictive%20framework%20can%20augment%20clinical%20decision-making%20in%20post-stroke%20gait%20rehabilitation%20and%20provide%20a%20template%20for%20dynamically%20personalized%20motor%20recovery%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520data-physics%2520hybrid%2520generative%2520model%2520for%2520patient-specific%2520post-stroke%2520motor%2520rehabilitation%2520using%2520wearable%2520sensor%2520data%26entry.906535625%3DYanning%2520Dai%2520and%2520Chenyu%2520Tang%2520and%2520Ruizhi%2520Zhang%2520and%2520Wenyu%2520Yang%2520and%2520Yilan%2520Zhang%2520and%2520Yuhui%2520Wang%2520and%2520Junliang%2520Chen%2520and%2520Xuhang%2520Chen%2520and%2520Ruimou%2520Xie%2520and%2520Yangyue%2520Cao%2520and%2520Qiaoying%2520Li%2520and%2520Jin%2520Cao%2520and%2520Tao%2520Li%2520and%2520Hubin%2520Zhao%2520and%2520Yu%2520Pan%2520and%2520Arokia%2520Nathan%2520and%2520Xin%2520Gao%2520and%2520Peter%2520Smielewski%2520and%2520Shuo%2520Gao%26entry.1292438233%3DDynamic%2520prediction%2520of%2520locomotor%2520capacity%2520after%2520stroke%2520is%2520crucial%2520for%2520tailoring%2520rehabilitation%252C%2520yet%2520current%2520assessments%2520provide%2520only%2520static%2520impairment%2520scores%2520and%2520do%2520not%2520indicate%2520whether%2520patients%2520can%2520safely%2520perform%2520specific%2520tasks%2520such%2520as%2520slope%2520walking%2520or%2520stair%2520climbing.%2520Here%252C%2520we%2520develop%2520a%2520data-physics%2520hybrid%2520generative%2520framework%2520that%2520reconstructs%2520an%2520individual%2520stroke%2520survivor%2527s%2520neuromuscular%2520control%2520from%2520a%2520single%252020%2520m%2520level-ground%2520walking%2520trial%2520and%2520predicts%2520task-conditioned%2520locomotion%2520across%2520rehabilitation%2520scenarios.%2520The%2520system%2520combines%2520wearable-sensor%2520kinematics%252C%2520a%2520proportional-derivative%2520physics%2520controller%252C%2520a%2520population%2520Healthy%2520Motion%2520Atlas%252C%2520and%2520goal-conditioned%2520deep%2520reinforcement%2520learning%2520with%2520behaviour%2520cloning%2520and%2520generative%2520adversarial%2520imitation%2520learning%2520to%2520generate%2520physically%2520plausible%252C%2520patient-specific%2520gait%2520simulations%2520for%2520slopes%2520and%2520stairs.%2520In%252011%2520stroke%2520survivors%252C%2520the%2520personalized%2520controllers%2520preserved%2520idiosyncratic%2520gait%2520patterns%2520while%2520improving%2520joint-angle%2520and%2520endpoint%2520fidelity%2520by%25204.73%2525%2520and%252012.10%2525%252C%2520respectively%252C%2520and%2520reducing%2520training%2520time%2520to%252025.56%2525%2520relative%2520to%2520a%2520physics-only%2520baseline.%2520In%2520a%2520multicentre%2520pilot%2520involving%252021%2520inpatients%252C%2520clinicians%2520who%2520used%2520our%2520locomotion%2520predictions%2520to%2520guide%2520task%2520selection%2520and%2520difficulty%2520obtained%2520larger%2520gains%2520in%2520Fugl-Meyer%2520lower-extremity%2520scores%2520over%252028%2520days%2520of%2520standard%2520rehabilitation%2520than%2520control%2520clinicians%2520%2528mean%2520change%25206.0%2520versus%25203.7%2520points%2529.%2520These%2520findings%2520indicate%2520that%2520our%2520generative%252C%2520task-predictive%2520framework%2520can%2520augment%2520clinical%2520decision-making%2520in%2520post-stroke%2520gait%2520rehabilitation%2520and%2520provide%2520a%2520template%2520for%2520dynamically%2520personalized%2520motor%2520recovery%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20data-physics%20hybrid%20generative%20model%20for%20patient-specific%20post-stroke%20motor%20rehabilitation%20using%20wearable%20sensor%20data&entry.906535625=Yanning%20Dai%20and%20Chenyu%20Tang%20and%20Ruizhi%20Zhang%20and%20Wenyu%20Yang%20and%20Yilan%20Zhang%20and%20Yuhui%20Wang%20and%20Junliang%20Chen%20and%20Xuhang%20Chen%20and%20Ruimou%20Xie%20and%20Yangyue%20Cao%20and%20Qiaoying%20Li%20and%20Jin%20Cao%20and%20Tao%20Li%20and%20Hubin%20Zhao%20and%20Yu%20Pan%20and%20Arokia%20Nathan%20and%20Xin%20Gao%20and%20Peter%20Smielewski%20and%20Shuo%20Gao&entry.1292438233=Dynamic%20prediction%20of%20locomotor%20capacity%20after%20stroke%20is%20crucial%20for%20tailoring%20rehabilitation%2C%20yet%20current%20assessments%20provide%20only%20static%20impairment%20scores%20and%20do%20not%20indicate%20whether%20patients%20can%20safely%20perform%20specific%20tasks%20such%20as%20slope%20walking%20or%20stair%20climbing.%20Here%2C%20we%20develop%20a%20data-physics%20hybrid%20generative%20framework%20that%20reconstructs%20an%20individual%20stroke%20survivor%27s%20neuromuscular%20control%20from%20a%20single%2020%20m%20level-ground%20walking%20trial%20and%20predicts%20task-conditioned%20locomotion%20across%20rehabilitation%20scenarios.%20The%20system%20combines%20wearable-sensor%20kinematics%2C%20a%20proportional-derivative%20physics%20controller%2C%20a%20population%20Healthy%20Motion%20Atlas%2C%20and%20goal-conditioned%20deep%20reinforcement%20learning%20with%20behaviour%20cloning%20and%20generative%20adversarial%20imitation%20learning%20to%20generate%20physically%20plausible%2C%20patient-specific%20gait%20simulations%20for%20slopes%20and%20stairs.%20In%2011%20stroke%20survivors%2C%20the%20personalized%20controllers%20preserved%20idiosyncratic%20gait%20patterns%20while%20improving%20joint-angle%20and%20endpoint%20fidelity%20by%204.73%25%20and%2012.10%25%2C%20respectively%2C%20and%20reducing%20training%20time%20to%2025.56%25%20relative%20to%20a%20physics-only%20baseline.%20In%20a%20multicentre%20pilot%20involving%2021%20inpatients%2C%20clinicians%20who%20used%20our%20locomotion%20predictions%20to%20guide%20task%20selection%20and%20difficulty%20obtained%20larger%20gains%20in%20Fugl-Meyer%20lower-extremity%20scores%20over%2028%20days%20of%20standard%20rehabilitation%20than%20control%20clinicians%20%28mean%20change%206.0%20versus%203.7%20points%29.%20These%20findings%20indicate%20that%20our%20generative%2C%20task-predictive%20framework%20can%20augment%20clinical%20decision-making%20in%20post-stroke%20gait%20rehabilitation%20and%20provide%20a%20template%20for%20dynamically%20personalized%20motor%20recovery%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2512.14329v1&entry.124074799=Read"},
{"title": "DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance", "author": "Shreedhar Govil and Didier Stricker and Jason Rambach", "abstract": "Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\\circ$ field of view driver attention dataset, containing $\\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.", "link": "http://arxiv.org/abs/2512.14266v1", "date": "2025-12-16", "relevancy": 2.2578, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5688}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriverGaze360%3A%20OmniDirectional%20Driver%20Attention%20with%20Object-Level%20Guidance&body=Title%3A%20DriverGaze360%3A%20OmniDirectional%20Driver%20Attention%20with%20Object-Level%20Guidance%0AAuthor%3A%20Shreedhar%20Govil%20and%20Didier%20Stricker%20and%20Jason%20Rambach%0AAbstract%3A%20Predicting%20driver%20attention%20is%20a%20critical%20problem%20for%20developing%20explainable%20autonomous%20driving%20systems%20and%20understanding%20driver%20behavior%20in%20mixed%20human-autonomous%20vehicle%20traffic%20scenarios.%20Although%20significant%20progress%20has%20been%20made%20through%20large-scale%20driver%20attention%20datasets%20and%20deep%20learning%20architectures%2C%20existing%20works%20are%20constrained%20by%20narrow%20frontal%20field-of-view%20and%20limited%20driving%20diversity.%20Consequently%2C%20they%20fail%20to%20capture%20the%20full%20spatial%20context%20of%20driving%20environments%2C%20especially%20during%20lane%20changes%2C%20turns%2C%20and%20interactions%20involving%20peripheral%20objects%20such%20as%20pedestrians%20or%20cyclists.%20In%20this%20paper%2C%20we%20introduce%20DriverGaze360%2C%20a%20large-scale%20360%24%5E%5Ccirc%24%20field%20of%20view%20driver%20attention%20dataset%2C%20containing%20%24%5Csim%241%20million%20gaze-labeled%20frames%20collected%20from%2019%20human%20drivers%2C%20enabling%20comprehensive%20omnidirectional%20modeling%20of%20driver%20gaze%20behavior.%20Moreover%2C%20our%20panoramic%20attention%20prediction%20approach%2C%20DriverGaze360-Net%2C%20jointly%20learns%20attention%20maps%20and%20attended%20objects%20by%20employing%20an%20auxiliary%20semantic%20segmentation%20head.%20This%20improves%20spatial%20awareness%20and%20attention%20prediction%20across%20wide%20panoramic%20inputs.%20Extensive%20experiments%20demonstrate%20that%20DriverGaze360-Net%20achieves%20state-of-the-art%20attention%20prediction%20performance%20on%20multiple%20metrics%20on%20panoramic%20driving%20images.%20Dataset%20and%20method%20available%20at%20https%3A//av.dfki.de/drivergaze360.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriverGaze360%253A%2520OmniDirectional%2520Driver%2520Attention%2520with%2520Object-Level%2520Guidance%26entry.906535625%3DShreedhar%2520Govil%2520and%2520Didier%2520Stricker%2520and%2520Jason%2520Rambach%26entry.1292438233%3DPredicting%2520driver%2520attention%2520is%2520a%2520critical%2520problem%2520for%2520developing%2520explainable%2520autonomous%2520driving%2520systems%2520and%2520understanding%2520driver%2520behavior%2520in%2520mixed%2520human-autonomous%2520vehicle%2520traffic%2520scenarios.%2520Although%2520significant%2520progress%2520has%2520been%2520made%2520through%2520large-scale%2520driver%2520attention%2520datasets%2520and%2520deep%2520learning%2520architectures%252C%2520existing%2520works%2520are%2520constrained%2520by%2520narrow%2520frontal%2520field-of-view%2520and%2520limited%2520driving%2520diversity.%2520Consequently%252C%2520they%2520fail%2520to%2520capture%2520the%2520full%2520spatial%2520context%2520of%2520driving%2520environments%252C%2520especially%2520during%2520lane%2520changes%252C%2520turns%252C%2520and%2520interactions%2520involving%2520peripheral%2520objects%2520such%2520as%2520pedestrians%2520or%2520cyclists.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DriverGaze360%252C%2520a%2520large-scale%2520360%2524%255E%255Ccirc%2524%2520field%2520of%2520view%2520driver%2520attention%2520dataset%252C%2520containing%2520%2524%255Csim%25241%2520million%2520gaze-labeled%2520frames%2520collected%2520from%252019%2520human%2520drivers%252C%2520enabling%2520comprehensive%2520omnidirectional%2520modeling%2520of%2520driver%2520gaze%2520behavior.%2520Moreover%252C%2520our%2520panoramic%2520attention%2520prediction%2520approach%252C%2520DriverGaze360-Net%252C%2520jointly%2520learns%2520attention%2520maps%2520and%2520attended%2520objects%2520by%2520employing%2520an%2520auxiliary%2520semantic%2520segmentation%2520head.%2520This%2520improves%2520spatial%2520awareness%2520and%2520attention%2520prediction%2520across%2520wide%2520panoramic%2520inputs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DriverGaze360-Net%2520achieves%2520state-of-the-art%2520attention%2520prediction%2520performance%2520on%2520multiple%2520metrics%2520on%2520panoramic%2520driving%2520images.%2520Dataset%2520and%2520method%2520available%2520at%2520https%253A//av.dfki.de/drivergaze360.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriverGaze360%3A%20OmniDirectional%20Driver%20Attention%20with%20Object-Level%20Guidance&entry.906535625=Shreedhar%20Govil%20and%20Didier%20Stricker%20and%20Jason%20Rambach&entry.1292438233=Predicting%20driver%20attention%20is%20a%20critical%20problem%20for%20developing%20explainable%20autonomous%20driving%20systems%20and%20understanding%20driver%20behavior%20in%20mixed%20human-autonomous%20vehicle%20traffic%20scenarios.%20Although%20significant%20progress%20has%20been%20made%20through%20large-scale%20driver%20attention%20datasets%20and%20deep%20learning%20architectures%2C%20existing%20works%20are%20constrained%20by%20narrow%20frontal%20field-of-view%20and%20limited%20driving%20diversity.%20Consequently%2C%20they%20fail%20to%20capture%20the%20full%20spatial%20context%20of%20driving%20environments%2C%20especially%20during%20lane%20changes%2C%20turns%2C%20and%20interactions%20involving%20peripheral%20objects%20such%20as%20pedestrians%20or%20cyclists.%20In%20this%20paper%2C%20we%20introduce%20DriverGaze360%2C%20a%20large-scale%20360%24%5E%5Ccirc%24%20field%20of%20view%20driver%20attention%20dataset%2C%20containing%20%24%5Csim%241%20million%20gaze-labeled%20frames%20collected%20from%2019%20human%20drivers%2C%20enabling%20comprehensive%20omnidirectional%20modeling%20of%20driver%20gaze%20behavior.%20Moreover%2C%20our%20panoramic%20attention%20prediction%20approach%2C%20DriverGaze360-Net%2C%20jointly%20learns%20attention%20maps%20and%20attended%20objects%20by%20employing%20an%20auxiliary%20semantic%20segmentation%20head.%20This%20improves%20spatial%20awareness%20and%20attention%20prediction%20across%20wide%20panoramic%20inputs.%20Extensive%20experiments%20demonstrate%20that%20DriverGaze360-Net%20achieves%20state-of-the-art%20attention%20prediction%20performance%20on%20multiple%20metrics%20on%20panoramic%20driving%20images.%20Dataset%20and%20method%20available%20at%20https%3A//av.dfki.de/drivergaze360.&entry.1838667208=http%3A//arxiv.org/abs/2512.14266v1&entry.124074799=Read"},
{"title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus", "author": "Antonio Guillen-Perez", "abstract": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40% ccompared to the best single scout models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.", "link": "http://arxiv.org/abs/2512.12012v2", "date": "2025-12-16", "relevancy": 2.2545, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.586}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5622}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic-Drive%3A%20Democratizing%20Long-Tail%20Data%20Curation%20via%20Open-Vocabulary%20Grounding%20and%20Neuro-Symbolic%20VLM%20Consensus&body=Title%3A%20Semantic-Drive%3A%20Democratizing%20Long-Tail%20Data%20Curation%20via%20Open-Vocabulary%20Grounding%20and%20Neuro-Symbolic%20VLM%20Consensus%0AAuthor%3A%20Antonio%20Guillen-Perez%0AAbstract%3A%20The%20development%20of%20robust%20Autonomous%20Vehicles%20%28AVs%29%20is%20bottlenecked%20by%20the%20scarcity%20of%20%22Long-Tail%22%20training%20data.%20While%20fleets%20collect%20petabytes%20of%20video%20logs%2C%20identifying%20rare%20safety-critical%20events%20%28e.g.%2C%20erratic%20jaywalking%2C%20construction%20diversions%29%20remains%20a%20manual%2C%20cost-prohibitive%20process.%20Existing%20solutions%20rely%20on%20coarse%20metadata%20search%2C%20which%20lacks%20precision%2C%20or%20cloud-based%20VLMs%2C%20which%20are%20privacy-invasive%20and%20expensive.%20We%20introduce%20Semantic-Drive%2C%20a%20local-first%2C%20neuro-symbolic%20framework%20for%20semantic%20data%20mining.%20Our%20approach%20decouples%20perception%20into%20two%20stages%3A%20%281%29%20Symbolic%20Grounding%20via%20a%20real-time%20open-vocabulary%20detector%20%28YOLOE%29%20to%20anchor%20attention%2C%20and%20%282%29%20Cognitive%20Analysis%20via%20a%20Reasoning%20VLM%20that%20performs%20forensic%20scene%20analysis.%20To%20mitigate%20hallucination%2C%20we%20implement%20a%20%22System%202%22%20inference-time%20alignment%20strategy%2C%20utilizing%20a%20multi-model%20%22Judge-Scout%22%20consensus%20mechanism.%20Benchmarked%20on%20the%20nuScenes%20dataset%20against%20the%20Waymo%20Open%20Dataset%20%28WOD-E2E%29%20taxonomy%2C%20Semantic-Drive%20achieves%20a%20Recall%20of%200.966%20%28vs.%200.475%20for%20CLIP%29%20and%20reduces%20Risk%20Assessment%20Error%20by%2040%25%20ccompared%20to%20the%20best%20single%20scout%20models.%20The%20system%20runs%20entirely%20on%20consumer%20hardware%20%28NVIDIA%20RTX%203090%29%2C%20offering%20a%20privacy-preserving%20alternative%20to%20the%20cloud.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic-Drive%253A%2520Democratizing%2520Long-Tail%2520Data%2520Curation%2520via%2520Open-Vocabulary%2520Grounding%2520and%2520Neuro-Symbolic%2520VLM%2520Consensus%26entry.906535625%3DAntonio%2520Guillen-Perez%26entry.1292438233%3DThe%2520development%2520of%2520robust%2520Autonomous%2520Vehicles%2520%2528AVs%2529%2520is%2520bottlenecked%2520by%2520the%2520scarcity%2520of%2520%2522Long-Tail%2522%2520training%2520data.%2520While%2520fleets%2520collect%2520petabytes%2520of%2520video%2520logs%252C%2520identifying%2520rare%2520safety-critical%2520events%2520%2528e.g.%252C%2520erratic%2520jaywalking%252C%2520construction%2520diversions%2529%2520remains%2520a%2520manual%252C%2520cost-prohibitive%2520process.%2520Existing%2520solutions%2520rely%2520on%2520coarse%2520metadata%2520search%252C%2520which%2520lacks%2520precision%252C%2520or%2520cloud-based%2520VLMs%252C%2520which%2520are%2520privacy-invasive%2520and%2520expensive.%2520We%2520introduce%2520Semantic-Drive%252C%2520a%2520local-first%252C%2520neuro-symbolic%2520framework%2520for%2520semantic%2520data%2520mining.%2520Our%2520approach%2520decouples%2520perception%2520into%2520two%2520stages%253A%2520%25281%2529%2520Symbolic%2520Grounding%2520via%2520a%2520real-time%2520open-vocabulary%2520detector%2520%2528YOLOE%2529%2520to%2520anchor%2520attention%252C%2520and%2520%25282%2529%2520Cognitive%2520Analysis%2520via%2520a%2520Reasoning%2520VLM%2520that%2520performs%2520forensic%2520scene%2520analysis.%2520To%2520mitigate%2520hallucination%252C%2520we%2520implement%2520a%2520%2522System%25202%2522%2520inference-time%2520alignment%2520strategy%252C%2520utilizing%2520a%2520multi-model%2520%2522Judge-Scout%2522%2520consensus%2520mechanism.%2520Benchmarked%2520on%2520the%2520nuScenes%2520dataset%2520against%2520the%2520Waymo%2520Open%2520Dataset%2520%2528WOD-E2E%2529%2520taxonomy%252C%2520Semantic-Drive%2520achieves%2520a%2520Recall%2520of%25200.966%2520%2528vs.%25200.475%2520for%2520CLIP%2529%2520and%2520reduces%2520Risk%2520Assessment%2520Error%2520by%252040%2525%2520ccompared%2520to%2520the%2520best%2520single%2520scout%2520models.%2520The%2520system%2520runs%2520entirely%2520on%2520consumer%2520hardware%2520%2528NVIDIA%2520RTX%25203090%2529%252C%2520offering%2520a%2520privacy-preserving%2520alternative%2520to%2520the%2520cloud.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic-Drive%3A%20Democratizing%20Long-Tail%20Data%20Curation%20via%20Open-Vocabulary%20Grounding%20and%20Neuro-Symbolic%20VLM%20Consensus&entry.906535625=Antonio%20Guillen-Perez&entry.1292438233=The%20development%20of%20robust%20Autonomous%20Vehicles%20%28AVs%29%20is%20bottlenecked%20by%20the%20scarcity%20of%20%22Long-Tail%22%20training%20data.%20While%20fleets%20collect%20petabytes%20of%20video%20logs%2C%20identifying%20rare%20safety-critical%20events%20%28e.g.%2C%20erratic%20jaywalking%2C%20construction%20diversions%29%20remains%20a%20manual%2C%20cost-prohibitive%20process.%20Existing%20solutions%20rely%20on%20coarse%20metadata%20search%2C%20which%20lacks%20precision%2C%20or%20cloud-based%20VLMs%2C%20which%20are%20privacy-invasive%20and%20expensive.%20We%20introduce%20Semantic-Drive%2C%20a%20local-first%2C%20neuro-symbolic%20framework%20for%20semantic%20data%20mining.%20Our%20approach%20decouples%20perception%20into%20two%20stages%3A%20%281%29%20Symbolic%20Grounding%20via%20a%20real-time%20open-vocabulary%20detector%20%28YOLOE%29%20to%20anchor%20attention%2C%20and%20%282%29%20Cognitive%20Analysis%20via%20a%20Reasoning%20VLM%20that%20performs%20forensic%20scene%20analysis.%20To%20mitigate%20hallucination%2C%20we%20implement%20a%20%22System%202%22%20inference-time%20alignment%20strategy%2C%20utilizing%20a%20multi-model%20%22Judge-Scout%22%20consensus%20mechanism.%20Benchmarked%20on%20the%20nuScenes%20dataset%20against%20the%20Waymo%20Open%20Dataset%20%28WOD-E2E%29%20taxonomy%2C%20Semantic-Drive%20achieves%20a%20Recall%20of%200.966%20%28vs.%200.475%20for%20CLIP%29%20and%20reduces%20Risk%20Assessment%20Error%20by%2040%25%20ccompared%20to%20the%20best%20single%20scout%20models.%20The%20system%20runs%20entirely%20on%20consumer%20hardware%20%28NVIDIA%20RTX%203090%29%2C%20offering%20a%20privacy-preserving%20alternative%20to%20the%20cloud.&entry.1838667208=http%3A//arxiv.org/abs/2512.12012v2&entry.124074799=Read"},
{"title": "Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding", "author": "Haoyu Zhang and Qiaohui Chu and Meng Liu and Haoxiang Shi and Yaowei Wang and Liqiang Nie", "abstract": "AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. However, current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique challenges of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D, together with the instruction-tuning dataset EgoIT, which is collected from multiple sources to enhance the model's instruction-following capabilities. Building upon the datasets, we propose a migration strategy and further design a progressive mapping learning pipeline with three stages: Demonstrator Self-Preparation, Demonstrator-Learner Guidance, and Learner Self-Practice. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models.", "link": "http://arxiv.org/abs/2503.09143v2", "date": "2025-12-16", "relevancy": 2.2502, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5771}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exo2Ego%3A%20Exocentric%20Knowledge%20Guided%20MLLM%20for%20Egocentric%20Video%20Understanding&body=Title%3A%20Exo2Ego%3A%20Exocentric%20Knowledge%20Guided%20MLLM%20for%20Egocentric%20Video%20Understanding%0AAuthor%3A%20Haoyu%20Zhang%20and%20Qiaohui%20Chu%20and%20Meng%20Liu%20and%20Haoxiang%20Shi%20and%20Yaowei%20Wang%20and%20Liqiang%20Nie%0AAbstract%3A%20AI%20personal%20assistants%2C%20deployed%20through%20robots%20or%20wearables%2C%20require%20embodied%20understanding%20to%20collaborate%20effectively%20with%20humans.%20However%2C%20current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20primarily%20focus%20on%20third-person%20%28exocentric%29%20vision%2C%20overlooking%20the%20unique%20challenges%20of%20first-person%20%28egocentric%29%20videos.%20Additionally%2C%20high%20acquisition%20costs%20limit%20data%20size%2C%20impairing%20MLLM%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20learning%20the%20mapping%20between%20exocentric%20and%20egocentric%20domains%2C%20leveraging%20the%20extensive%20exocentric%20knowledge%20within%20existing%20MLLMs%20to%20enhance%20egocentric%20video%20understanding.%20To%20this%20end%2C%20we%20introduce%20Ego-ExoClip%2C%20a%20pre-training%20dataset%20comprising%201.1M%20synchronized%20ego-exo%20clip-text%20pairs%20derived%20from%20Ego-Exo4D%2C%20together%20with%20the%20instruction-tuning%20dataset%20EgoIT%2C%20which%20is%20collected%20from%20multiple%20sources%20to%20enhance%20the%20model%27s%20instruction-following%20capabilities.%20Building%20upon%20the%20datasets%2C%20we%20propose%20a%20migration%20strategy%20and%20further%20design%20a%20progressive%20mapping%20learning%20pipeline%20with%20three%20stages%3A%20Demonstrator%20Self-Preparation%2C%20Demonstrator-Learner%20Guidance%2C%20and%20Learner%20Self-Practice.%20Extensive%20experiments%20across%20diverse%20egocentric%20tasks%20reveal%20that%20existing%20MLLMs%20perform%20inadequately%20in%20egocentric%20video%20understanding%2C%20while%20our%20model%20significantly%20outperforms%20these%20leading%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2503.09143v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExo2Ego%253A%2520Exocentric%2520Knowledge%2520Guided%2520MLLM%2520for%2520Egocentric%2520Video%2520Understanding%26entry.906535625%3DHaoyu%2520Zhang%2520and%2520Qiaohui%2520Chu%2520and%2520Meng%2520Liu%2520and%2520Haoxiang%2520Shi%2520and%2520Yaowei%2520Wang%2520and%2520Liqiang%2520Nie%26entry.1292438233%3DAI%2520personal%2520assistants%252C%2520deployed%2520through%2520robots%2520or%2520wearables%252C%2520require%2520embodied%2520understanding%2520to%2520collaborate%2520effectively%2520with%2520humans.%2520However%252C%2520current%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520primarily%2520focus%2520on%2520third-person%2520%2528exocentric%2529%2520vision%252C%2520overlooking%2520the%2520unique%2520challenges%2520of%2520first-person%2520%2528egocentric%2529%2520videos.%2520Additionally%252C%2520high%2520acquisition%2520costs%2520limit%2520data%2520size%252C%2520impairing%2520MLLM%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520learning%2520the%2520mapping%2520between%2520exocentric%2520and%2520egocentric%2520domains%252C%2520leveraging%2520the%2520extensive%2520exocentric%2520knowledge%2520within%2520existing%2520MLLMs%2520to%2520enhance%2520egocentric%2520video%2520understanding.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Ego-ExoClip%252C%2520a%2520pre-training%2520dataset%2520comprising%25201.1M%2520synchronized%2520ego-exo%2520clip-text%2520pairs%2520derived%2520from%2520Ego-Exo4D%252C%2520together%2520with%2520the%2520instruction-tuning%2520dataset%2520EgoIT%252C%2520which%2520is%2520collected%2520from%2520multiple%2520sources%2520to%2520enhance%2520the%2520model%2527s%2520instruction-following%2520capabilities.%2520Building%2520upon%2520the%2520datasets%252C%2520we%2520propose%2520a%2520migration%2520strategy%2520and%2520further%2520design%2520a%2520progressive%2520mapping%2520learning%2520pipeline%2520with%2520three%2520stages%253A%2520Demonstrator%2520Self-Preparation%252C%2520Demonstrator-Learner%2520Guidance%252C%2520and%2520Learner%2520Self-Practice.%2520Extensive%2520experiments%2520across%2520diverse%2520egocentric%2520tasks%2520reveal%2520that%2520existing%2520MLLMs%2520perform%2520inadequately%2520in%2520egocentric%2520video%2520understanding%252C%2520while%2520our%2520model%2520significantly%2520outperforms%2520these%2520leading%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09143v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exo2Ego%3A%20Exocentric%20Knowledge%20Guided%20MLLM%20for%20Egocentric%20Video%20Understanding&entry.906535625=Haoyu%20Zhang%20and%20Qiaohui%20Chu%20and%20Meng%20Liu%20and%20Haoxiang%20Shi%20and%20Yaowei%20Wang%20and%20Liqiang%20Nie&entry.1292438233=AI%20personal%20assistants%2C%20deployed%20through%20robots%20or%20wearables%2C%20require%20embodied%20understanding%20to%20collaborate%20effectively%20with%20humans.%20However%2C%20current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20primarily%20focus%20on%20third-person%20%28exocentric%29%20vision%2C%20overlooking%20the%20unique%20challenges%20of%20first-person%20%28egocentric%29%20videos.%20Additionally%2C%20high%20acquisition%20costs%20limit%20data%20size%2C%20impairing%20MLLM%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20learning%20the%20mapping%20between%20exocentric%20and%20egocentric%20domains%2C%20leveraging%20the%20extensive%20exocentric%20knowledge%20within%20existing%20MLLMs%20to%20enhance%20egocentric%20video%20understanding.%20To%20this%20end%2C%20we%20introduce%20Ego-ExoClip%2C%20a%20pre-training%20dataset%20comprising%201.1M%20synchronized%20ego-exo%20clip-text%20pairs%20derived%20from%20Ego-Exo4D%2C%20together%20with%20the%20instruction-tuning%20dataset%20EgoIT%2C%20which%20is%20collected%20from%20multiple%20sources%20to%20enhance%20the%20model%27s%20instruction-following%20capabilities.%20Building%20upon%20the%20datasets%2C%20we%20propose%20a%20migration%20strategy%20and%20further%20design%20a%20progressive%20mapping%20learning%20pipeline%20with%20three%20stages%3A%20Demonstrator%20Self-Preparation%2C%20Demonstrator-Learner%20Guidance%2C%20and%20Learner%20Self-Practice.%20Extensive%20experiments%20across%20diverse%20egocentric%20tasks%20reveal%20that%20existing%20MLLMs%20perform%20inadequately%20in%20egocentric%20video%20understanding%2C%20while%20our%20model%20significantly%20outperforms%20these%20leading%20models.&entry.1838667208=http%3A//arxiv.org/abs/2503.09143v2&entry.124074799=Read"},
{"title": "An Unsupervised Deep Explainable AI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals", "author": "Konstantinos Vasili and Zachery T. Dahm and Stylianos Chatzidakis", "abstract": "Next generation advanced nuclear reactors are expected to be smaller both in size and power output, relying extensively on fully digital instrumentation and control systems. These reactors will generate a large flow of information in the form of multivariate time series data, conveying simultaneously various non linear cyber physical, process, control, sensor, and operational states. Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data. Here, we propose an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm to fully characterize real-time replay attacks, i.e., detection, source identification, timing and type, of increasing complexity during a dynamic time evolving reactor process. The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.", "link": "http://arxiv.org/abs/2508.09162v3", "date": "2025-12-16", "relevancy": 2.2465, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4681}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Unsupervised%20Deep%20Explainable%20AI%20Framework%20for%20Localization%20of%20Concurrent%20Replay%20Attacks%20in%20Nuclear%20Reactor%20Signals&body=Title%3A%20An%20Unsupervised%20Deep%20Explainable%20AI%20Framework%20for%20Localization%20of%20Concurrent%20Replay%20Attacks%20in%20Nuclear%20Reactor%20Signals%0AAuthor%3A%20Konstantinos%20Vasili%20and%20Zachery%20T.%20Dahm%20and%20Stylianos%20Chatzidakis%0AAbstract%3A%20Next%20generation%20advanced%20nuclear%20reactors%20are%20expected%20to%20be%20smaller%20both%20in%20size%20and%20power%20output%2C%20relying%20extensively%20on%20fully%20digital%20instrumentation%20and%20control%20systems.%20These%20reactors%20will%20generate%20a%20large%20flow%20of%20information%20in%20the%20form%20of%20multivariate%20time%20series%20data%2C%20conveying%20simultaneously%20various%20non%20linear%20cyber%20physical%2C%20process%2C%20control%2C%20sensor%2C%20and%20operational%20states.%20Ensuring%20data%20integrity%20against%20deception%20attacks%20is%20becoming%20increasingly%20important%20for%20networked%20communication%20and%20a%20requirement%20for%20safe%20and%20reliable%20operation.%20Current%20efforts%20to%20address%20replay%20attacks%2C%20almost%20universally%20focus%20on%20watermarking%20or%20supervised%20anomaly%20detection%20approaches%20without%20further%20identifying%20and%20characterizing%20the%20root%20cause%20of%20the%20anomaly.%20In%20addition%2C%20these%20approaches%20rely%20mostly%20on%20synthetic%20data%20with%20uncorrelated%20Gaussian%20process%20and%20measurement%20noise%20and%20full%20state%20feedback%20or%20are%20limited%20to%20univariate%20signals%2C%20signal%20stationarity%2C%20linear%20quadratic%20regulators%2C%20or%20other%20linear-time%20invariant%20state-space%20which%20may%20fail%20to%20capture%20any%20unmodeled%20system%20dynamics.%20In%20the%20realm%20of%20regulated%20nuclear%20cyber-physical%20systems%2C%20additional%20work%20is%20needed%20on%20characterization%20of%20replay%20attacks%20and%20explainability%20of%20predictions%20using%20real%20data.%20Here%2C%20we%20propose%20an%20unsupervised%20explainable%20AI%20framework%20based%20on%20a%20combination%20of%20autoencoder%20and%20customized%20windowSHAP%20algorithm%20to%20fully%20characterize%20real-time%20replay%20attacks%2C%20i.e.%2C%20detection%2C%20source%20identification%2C%20timing%20and%20type%2C%20of%20increasing%20complexity%20during%20a%20dynamic%20time%20evolving%20reactor%20process.%20The%20proposed%20XAI%20framework%20was%20benchmarked%20on%20several%20real%20world%20datasets%20from%20Purdue%27s%20nuclear%20reactor%20PUR-1%20with%20up%20to%20six%20signals%20concurrently%20being%20replayed.%20In%20all%20cases%2C%20the%20XAI%20framework%20was%20able%20to%20detect%20and%20identify%20the%20source%20and%20number%20of%20signals%20being%20replayed%20and%20the%20duration%20of%20the%20falsification%20with%2095%20percent%20or%20better%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09162v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Unsupervised%2520Deep%2520Explainable%2520AI%2520Framework%2520for%2520Localization%2520of%2520Concurrent%2520Replay%2520Attacks%2520in%2520Nuclear%2520Reactor%2520Signals%26entry.906535625%3DKonstantinos%2520Vasili%2520and%2520Zachery%2520T.%2520Dahm%2520and%2520Stylianos%2520Chatzidakis%26entry.1292438233%3DNext%2520generation%2520advanced%2520nuclear%2520reactors%2520are%2520expected%2520to%2520be%2520smaller%2520both%2520in%2520size%2520and%2520power%2520output%252C%2520relying%2520extensively%2520on%2520fully%2520digital%2520instrumentation%2520and%2520control%2520systems.%2520These%2520reactors%2520will%2520generate%2520a%2520large%2520flow%2520of%2520information%2520in%2520the%2520form%2520of%2520multivariate%2520time%2520series%2520data%252C%2520conveying%2520simultaneously%2520various%2520non%2520linear%2520cyber%2520physical%252C%2520process%252C%2520control%252C%2520sensor%252C%2520and%2520operational%2520states.%2520Ensuring%2520data%2520integrity%2520against%2520deception%2520attacks%2520is%2520becoming%2520increasingly%2520important%2520for%2520networked%2520communication%2520and%2520a%2520requirement%2520for%2520safe%2520and%2520reliable%2520operation.%2520Current%2520efforts%2520to%2520address%2520replay%2520attacks%252C%2520almost%2520universally%2520focus%2520on%2520watermarking%2520or%2520supervised%2520anomaly%2520detection%2520approaches%2520without%2520further%2520identifying%2520and%2520characterizing%2520the%2520root%2520cause%2520of%2520the%2520anomaly.%2520In%2520addition%252C%2520these%2520approaches%2520rely%2520mostly%2520on%2520synthetic%2520data%2520with%2520uncorrelated%2520Gaussian%2520process%2520and%2520measurement%2520noise%2520and%2520full%2520state%2520feedback%2520or%2520are%2520limited%2520to%2520univariate%2520signals%252C%2520signal%2520stationarity%252C%2520linear%2520quadratic%2520regulators%252C%2520or%2520other%2520linear-time%2520invariant%2520state-space%2520which%2520may%2520fail%2520to%2520capture%2520any%2520unmodeled%2520system%2520dynamics.%2520In%2520the%2520realm%2520of%2520regulated%2520nuclear%2520cyber-physical%2520systems%252C%2520additional%2520work%2520is%2520needed%2520on%2520characterization%2520of%2520replay%2520attacks%2520and%2520explainability%2520of%2520predictions%2520using%2520real%2520data.%2520Here%252C%2520we%2520propose%2520an%2520unsupervised%2520explainable%2520AI%2520framework%2520based%2520on%2520a%2520combination%2520of%2520autoencoder%2520and%2520customized%2520windowSHAP%2520algorithm%2520to%2520fully%2520characterize%2520real-time%2520replay%2520attacks%252C%2520i.e.%252C%2520detection%252C%2520source%2520identification%252C%2520timing%2520and%2520type%252C%2520of%2520increasing%2520complexity%2520during%2520a%2520dynamic%2520time%2520evolving%2520reactor%2520process.%2520The%2520proposed%2520XAI%2520framework%2520was%2520benchmarked%2520on%2520several%2520real%2520world%2520datasets%2520from%2520Purdue%2527s%2520nuclear%2520reactor%2520PUR-1%2520with%2520up%2520to%2520six%2520signals%2520concurrently%2520being%2520replayed.%2520In%2520all%2520cases%252C%2520the%2520XAI%2520framework%2520was%2520able%2520to%2520detect%2520and%2520identify%2520the%2520source%2520and%2520number%2520of%2520signals%2520being%2520replayed%2520and%2520the%2520duration%2520of%2520the%2520falsification%2520with%252095%2520percent%2520or%2520better%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09162v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Unsupervised%20Deep%20Explainable%20AI%20Framework%20for%20Localization%20of%20Concurrent%20Replay%20Attacks%20in%20Nuclear%20Reactor%20Signals&entry.906535625=Konstantinos%20Vasili%20and%20Zachery%20T.%20Dahm%20and%20Stylianos%20Chatzidakis&entry.1292438233=Next%20generation%20advanced%20nuclear%20reactors%20are%20expected%20to%20be%20smaller%20both%20in%20size%20and%20power%20output%2C%20relying%20extensively%20on%20fully%20digital%20instrumentation%20and%20control%20systems.%20These%20reactors%20will%20generate%20a%20large%20flow%20of%20information%20in%20the%20form%20of%20multivariate%20time%20series%20data%2C%20conveying%20simultaneously%20various%20non%20linear%20cyber%20physical%2C%20process%2C%20control%2C%20sensor%2C%20and%20operational%20states.%20Ensuring%20data%20integrity%20against%20deception%20attacks%20is%20becoming%20increasingly%20important%20for%20networked%20communication%20and%20a%20requirement%20for%20safe%20and%20reliable%20operation.%20Current%20efforts%20to%20address%20replay%20attacks%2C%20almost%20universally%20focus%20on%20watermarking%20or%20supervised%20anomaly%20detection%20approaches%20without%20further%20identifying%20and%20characterizing%20the%20root%20cause%20of%20the%20anomaly.%20In%20addition%2C%20these%20approaches%20rely%20mostly%20on%20synthetic%20data%20with%20uncorrelated%20Gaussian%20process%20and%20measurement%20noise%20and%20full%20state%20feedback%20or%20are%20limited%20to%20univariate%20signals%2C%20signal%20stationarity%2C%20linear%20quadratic%20regulators%2C%20or%20other%20linear-time%20invariant%20state-space%20which%20may%20fail%20to%20capture%20any%20unmodeled%20system%20dynamics.%20In%20the%20realm%20of%20regulated%20nuclear%20cyber-physical%20systems%2C%20additional%20work%20is%20needed%20on%20characterization%20of%20replay%20attacks%20and%20explainability%20of%20predictions%20using%20real%20data.%20Here%2C%20we%20propose%20an%20unsupervised%20explainable%20AI%20framework%20based%20on%20a%20combination%20of%20autoencoder%20and%20customized%20windowSHAP%20algorithm%20to%20fully%20characterize%20real-time%20replay%20attacks%2C%20i.e.%2C%20detection%2C%20source%20identification%2C%20timing%20and%20type%2C%20of%20increasing%20complexity%20during%20a%20dynamic%20time%20evolving%20reactor%20process.%20The%20proposed%20XAI%20framework%20was%20benchmarked%20on%20several%20real%20world%20datasets%20from%20Purdue%27s%20nuclear%20reactor%20PUR-1%20with%20up%20to%20six%20signals%20concurrently%20being%20replayed.%20In%20all%20cases%2C%20the%20XAI%20framework%20was%20able%20to%20detect%20and%20identify%20the%20source%20and%20number%20of%20signals%20being%20replayed%20and%20the%20duration%20of%20the%20falsification%20with%2095%20percent%20or%20better%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2508.09162v3&entry.124074799=Read"},
{"title": "ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning", "author": "Rishabh Dev Yadav and Avirup Das and Hongyu Song and Samuel Kaski and Wei Pan", "abstract": "Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.", "link": "http://arxiv.org/abs/2512.14331v1", "date": "2025-12-16", "relevancy": 2.2374, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6397}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5645}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARCADE%3A%20Adaptive%20Robot%20Control%20with%20Online%20Changepoint-Aware%20Bayesian%20Dynamics%20Learning&body=Title%3A%20ARCADE%3A%20Adaptive%20Robot%20Control%20with%20Online%20Changepoint-Aware%20Bayesian%20Dynamics%20Learning%0AAuthor%3A%20Rishabh%20Dev%20Yadav%20and%20Avirup%20Das%20and%20Hongyu%20Song%20and%20Samuel%20Kaski%20and%20Wei%20Pan%0AAbstract%3A%20Real-world%20robots%20must%20operate%20under%20evolving%20dynamics%20caused%20by%20changing%20operating%20conditions%2C%20external%20disturbances%2C%20and%20unmodeled%20effects.%20These%20may%20appear%20as%20gradual%20drifts%2C%20transient%20fluctuations%2C%20or%20abrupt%20shifts%2C%20demanding%20real-time%20adaptation%20that%20is%20robust%20to%20short-term%20variation%20yet%20responsive%20to%20lasting%20change.%20We%20propose%20a%20framework%20for%20modeling%20the%20nonlinear%20dynamics%20of%20robotic%20systems%20that%20can%20be%20updated%20in%20real%20time%20from%20streaming%20data.%20The%20method%20decouples%20representation%20learning%20from%20online%20adaptation%2C%20using%20latent%20representations%20learned%20offline%20to%20support%20online%20closed-form%20Bayesian%20updates.%20To%20handle%20evolving%20conditions%2C%20we%20introduce%20a%20changepoint-aware%20mechanism%20with%20a%20latent%20variable%20inferred%20from%20data%20likelihoods%20that%20indicates%20continuity%20or%20shift.%20When%20continuity%20is%20likely%2C%20evidence%20accumulates%20to%20refine%20predictions%3B%20when%20a%20shift%20is%20detected%2C%20past%20information%20is%20tempered%20to%20enable%20rapid%20re-learning.%20This%20maintains%20calibrated%20uncertainty%20and%20supports%20probabilistic%20reasoning%20about%20transient%2C%20gradual%2C%20or%20structural%20change.%20We%20prove%20that%20the%20adaptive%20regret%20of%20the%20framework%20grows%20only%20logarithmically%20in%20time%20and%20linearly%20with%20the%20number%20of%20shifts%2C%20competitive%20with%20an%20oracle%20that%20knows%20timings%20of%20shift.%20We%20validate%20on%20cartpole%20simulations%20and%20real%20quadrotor%20flights%20with%20swinging%20payloads%20and%20mid-flight%20drops%2C%20showing%20improved%20predictive%20accuracy%2C%20faster%20recovery%2C%20and%20more%20accurate%20closed-loop%20tracking%20than%20relevant%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARCADE%253A%2520Adaptive%2520Robot%2520Control%2520with%2520Online%2520Changepoint-Aware%2520Bayesian%2520Dynamics%2520Learning%26entry.906535625%3DRishabh%2520Dev%2520Yadav%2520and%2520Avirup%2520Das%2520and%2520Hongyu%2520Song%2520and%2520Samuel%2520Kaski%2520and%2520Wei%2520Pan%26entry.1292438233%3DReal-world%2520robots%2520must%2520operate%2520under%2520evolving%2520dynamics%2520caused%2520by%2520changing%2520operating%2520conditions%252C%2520external%2520disturbances%252C%2520and%2520unmodeled%2520effects.%2520These%2520may%2520appear%2520as%2520gradual%2520drifts%252C%2520transient%2520fluctuations%252C%2520or%2520abrupt%2520shifts%252C%2520demanding%2520real-time%2520adaptation%2520that%2520is%2520robust%2520to%2520short-term%2520variation%2520yet%2520responsive%2520to%2520lasting%2520change.%2520We%2520propose%2520a%2520framework%2520for%2520modeling%2520the%2520nonlinear%2520dynamics%2520of%2520robotic%2520systems%2520that%2520can%2520be%2520updated%2520in%2520real%2520time%2520from%2520streaming%2520data.%2520The%2520method%2520decouples%2520representation%2520learning%2520from%2520online%2520adaptation%252C%2520using%2520latent%2520representations%2520learned%2520offline%2520to%2520support%2520online%2520closed-form%2520Bayesian%2520updates.%2520To%2520handle%2520evolving%2520conditions%252C%2520we%2520introduce%2520a%2520changepoint-aware%2520mechanism%2520with%2520a%2520latent%2520variable%2520inferred%2520from%2520data%2520likelihoods%2520that%2520indicates%2520continuity%2520or%2520shift.%2520When%2520continuity%2520is%2520likely%252C%2520evidence%2520accumulates%2520to%2520refine%2520predictions%253B%2520when%2520a%2520shift%2520is%2520detected%252C%2520past%2520information%2520is%2520tempered%2520to%2520enable%2520rapid%2520re-learning.%2520This%2520maintains%2520calibrated%2520uncertainty%2520and%2520supports%2520probabilistic%2520reasoning%2520about%2520transient%252C%2520gradual%252C%2520or%2520structural%2520change.%2520We%2520prove%2520that%2520the%2520adaptive%2520regret%2520of%2520the%2520framework%2520grows%2520only%2520logarithmically%2520in%2520time%2520and%2520linearly%2520with%2520the%2520number%2520of%2520shifts%252C%2520competitive%2520with%2520an%2520oracle%2520that%2520knows%2520timings%2520of%2520shift.%2520We%2520validate%2520on%2520cartpole%2520simulations%2520and%2520real%2520quadrotor%2520flights%2520with%2520swinging%2520payloads%2520and%2520mid-flight%2520drops%252C%2520showing%2520improved%2520predictive%2520accuracy%252C%2520faster%2520recovery%252C%2520and%2520more%2520accurate%2520closed-loop%2520tracking%2520than%2520relevant%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARCADE%3A%20Adaptive%20Robot%20Control%20with%20Online%20Changepoint-Aware%20Bayesian%20Dynamics%20Learning&entry.906535625=Rishabh%20Dev%20Yadav%20and%20Avirup%20Das%20and%20Hongyu%20Song%20and%20Samuel%20Kaski%20and%20Wei%20Pan&entry.1292438233=Real-world%20robots%20must%20operate%20under%20evolving%20dynamics%20caused%20by%20changing%20operating%20conditions%2C%20external%20disturbances%2C%20and%20unmodeled%20effects.%20These%20may%20appear%20as%20gradual%20drifts%2C%20transient%20fluctuations%2C%20or%20abrupt%20shifts%2C%20demanding%20real-time%20adaptation%20that%20is%20robust%20to%20short-term%20variation%20yet%20responsive%20to%20lasting%20change.%20We%20propose%20a%20framework%20for%20modeling%20the%20nonlinear%20dynamics%20of%20robotic%20systems%20that%20can%20be%20updated%20in%20real%20time%20from%20streaming%20data.%20The%20method%20decouples%20representation%20learning%20from%20online%20adaptation%2C%20using%20latent%20representations%20learned%20offline%20to%20support%20online%20closed-form%20Bayesian%20updates.%20To%20handle%20evolving%20conditions%2C%20we%20introduce%20a%20changepoint-aware%20mechanism%20with%20a%20latent%20variable%20inferred%20from%20data%20likelihoods%20that%20indicates%20continuity%20or%20shift.%20When%20continuity%20is%20likely%2C%20evidence%20accumulates%20to%20refine%20predictions%3B%20when%20a%20shift%20is%20detected%2C%20past%20information%20is%20tempered%20to%20enable%20rapid%20re-learning.%20This%20maintains%20calibrated%20uncertainty%20and%20supports%20probabilistic%20reasoning%20about%20transient%2C%20gradual%2C%20or%20structural%20change.%20We%20prove%20that%20the%20adaptive%20regret%20of%20the%20framework%20grows%20only%20logarithmically%20in%20time%20and%20linearly%20with%20the%20number%20of%20shifts%2C%20competitive%20with%20an%20oracle%20that%20knows%20timings%20of%20shift.%20We%20validate%20on%20cartpole%20simulations%20and%20real%20quadrotor%20flights%20with%20swinging%20payloads%20and%20mid-flight%20drops%2C%20showing%20improved%20predictive%20accuracy%2C%20faster%20recovery%2C%20and%20more%20accurate%20closed-loop%20tracking%20than%20relevant%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.14331v1&entry.124074799=Read"},
{"title": "ART: Articulated Reconstruction Transformer", "author": "Zizhang Li and Cheng Zhang and Zhengqin Li and Henry Howard-Jenkins and Zhaoyang Lv and Chen Geng and Jiajun Wu and Richard Newcombe and Jakob Engel and Zhao Dong", "abstract": "We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.", "link": "http://arxiv.org/abs/2512.14671v1", "date": "2025-12-16", "relevancy": 2.2321, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5634}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5553}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ART%3A%20Articulated%20Reconstruction%20Transformer&body=Title%3A%20ART%3A%20Articulated%20Reconstruction%20Transformer%0AAuthor%3A%20Zizhang%20Li%20and%20Cheng%20Zhang%20and%20Zhengqin%20Li%20and%20Henry%20Howard-Jenkins%20and%20Zhaoyang%20Lv%20and%20Chen%20Geng%20and%20Jiajun%20Wu%20and%20Richard%20Newcombe%20and%20Jakob%20Engel%20and%20Zhao%20Dong%0AAbstract%3A%20We%20introduce%20ART%2C%20Articulated%20Reconstruction%20Transformer%20--%20a%20category-agnostic%2C%20feed-forward%20model%20that%20reconstructs%20complete%203D%20articulated%20objects%20from%20only%20sparse%2C%20multi-state%20RGB%20images.%20Previous%20methods%20for%20articulated%20object%20reconstruction%20either%20rely%20on%20slow%20optimization%20with%20fragile%20cross-state%20correspondences%20or%20use%20feed-forward%20models%20limited%20to%20specific%20object%20categories.%20In%20contrast%2C%20ART%20treats%20articulated%20objects%20as%20assemblies%20of%20rigid%20parts%2C%20formulating%20reconstruction%20as%20part-based%20prediction.%20Our%20newly%20designed%20transformer%20architecture%20maps%20sparse%20image%20inputs%20to%20a%20set%20of%20learnable%20part%20slots%2C%20from%20which%20ART%20jointly%20decodes%20unified%20representations%20for%20individual%20parts%2C%20including%20their%203D%20geometry%2C%20texture%2C%20and%20explicit%20articulation%20parameters.%20The%20resulting%20reconstructions%20are%20physically%20interpretable%20and%20readily%20exportable%20for%20simulation.%20Trained%20on%20a%20large-scale%2C%20diverse%20dataset%20with%20per-part%20supervision%2C%20and%20evaluated%20across%20diverse%20benchmarks%2C%20ART%20achieves%20significant%20improvements%20over%20existing%20baselines%20and%20establishes%20a%20new%20state%20of%20the%20art%20for%20articulated%20object%20reconstruction%20from%20image%20inputs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DART%253A%2520Articulated%2520Reconstruction%2520Transformer%26entry.906535625%3DZizhang%2520Li%2520and%2520Cheng%2520Zhang%2520and%2520Zhengqin%2520Li%2520and%2520Henry%2520Howard-Jenkins%2520and%2520Zhaoyang%2520Lv%2520and%2520Chen%2520Geng%2520and%2520Jiajun%2520Wu%2520and%2520Richard%2520Newcombe%2520and%2520Jakob%2520Engel%2520and%2520Zhao%2520Dong%26entry.1292438233%3DWe%2520introduce%2520ART%252C%2520Articulated%2520Reconstruction%2520Transformer%2520--%2520a%2520category-agnostic%252C%2520feed-forward%2520model%2520that%2520reconstructs%2520complete%25203D%2520articulated%2520objects%2520from%2520only%2520sparse%252C%2520multi-state%2520RGB%2520images.%2520Previous%2520methods%2520for%2520articulated%2520object%2520reconstruction%2520either%2520rely%2520on%2520slow%2520optimization%2520with%2520fragile%2520cross-state%2520correspondences%2520or%2520use%2520feed-forward%2520models%2520limited%2520to%2520specific%2520object%2520categories.%2520In%2520contrast%252C%2520ART%2520treats%2520articulated%2520objects%2520as%2520assemblies%2520of%2520rigid%2520parts%252C%2520formulating%2520reconstruction%2520as%2520part-based%2520prediction.%2520Our%2520newly%2520designed%2520transformer%2520architecture%2520maps%2520sparse%2520image%2520inputs%2520to%2520a%2520set%2520of%2520learnable%2520part%2520slots%252C%2520from%2520which%2520ART%2520jointly%2520decodes%2520unified%2520representations%2520for%2520individual%2520parts%252C%2520including%2520their%25203D%2520geometry%252C%2520texture%252C%2520and%2520explicit%2520articulation%2520parameters.%2520The%2520resulting%2520reconstructions%2520are%2520physically%2520interpretable%2520and%2520readily%2520exportable%2520for%2520simulation.%2520Trained%2520on%2520a%2520large-scale%252C%2520diverse%2520dataset%2520with%2520per-part%2520supervision%252C%2520and%2520evaluated%2520across%2520diverse%2520benchmarks%252C%2520ART%2520achieves%2520significant%2520improvements%2520over%2520existing%2520baselines%2520and%2520establishes%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520articulated%2520object%2520reconstruction%2520from%2520image%2520inputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ART%3A%20Articulated%20Reconstruction%20Transformer&entry.906535625=Zizhang%20Li%20and%20Cheng%20Zhang%20and%20Zhengqin%20Li%20and%20Henry%20Howard-Jenkins%20and%20Zhaoyang%20Lv%20and%20Chen%20Geng%20and%20Jiajun%20Wu%20and%20Richard%20Newcombe%20and%20Jakob%20Engel%20and%20Zhao%20Dong&entry.1292438233=We%20introduce%20ART%2C%20Articulated%20Reconstruction%20Transformer%20--%20a%20category-agnostic%2C%20feed-forward%20model%20that%20reconstructs%20complete%203D%20articulated%20objects%20from%20only%20sparse%2C%20multi-state%20RGB%20images.%20Previous%20methods%20for%20articulated%20object%20reconstruction%20either%20rely%20on%20slow%20optimization%20with%20fragile%20cross-state%20correspondences%20or%20use%20feed-forward%20models%20limited%20to%20specific%20object%20categories.%20In%20contrast%2C%20ART%20treats%20articulated%20objects%20as%20assemblies%20of%20rigid%20parts%2C%20formulating%20reconstruction%20as%20part-based%20prediction.%20Our%20newly%20designed%20transformer%20architecture%20maps%20sparse%20image%20inputs%20to%20a%20set%20of%20learnable%20part%20slots%2C%20from%20which%20ART%20jointly%20decodes%20unified%20representations%20for%20individual%20parts%2C%20including%20their%203D%20geometry%2C%20texture%2C%20and%20explicit%20articulation%20parameters.%20The%20resulting%20reconstructions%20are%20physically%20interpretable%20and%20readily%20exportable%20for%20simulation.%20Trained%20on%20a%20large-scale%2C%20diverse%20dataset%20with%20per-part%20supervision%2C%20and%20evaluated%20across%20diverse%20benchmarks%2C%20ART%20achieves%20significant%20improvements%20over%20existing%20baselines%20and%20establishes%20a%20new%20state%20of%20the%20art%20for%20articulated%20object%20reconstruction%20from%20image%20inputs.&entry.1838667208=http%3A//arxiv.org/abs/2512.14671v1&entry.124074799=Read"},
{"title": "Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning", "author": "Salvatore Romano and Marco Grassia and Giuseppe Mangioni", "abstract": "Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.", "link": "http://arxiv.org/abs/2512.14241v1", "date": "2025-12-16", "relevancy": 2.215, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5698}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5485}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20MMD%3A%20Evaluating%20Graph%20Generative%20Models%20with%20Geometric%20Deep%20Learning&body=Title%3A%20Beyond%20MMD%3A%20Evaluating%20Graph%20Generative%20Models%20with%20Geometric%20Deep%20Learning%0AAuthor%3A%20Salvatore%20Romano%20and%20Marco%20Grassia%20and%20Giuseppe%20Mangioni%0AAbstract%3A%20Graph%20generation%20is%20a%20crucial%20task%20in%20many%20fields%2C%20including%20network%20science%20and%20bioinformatics%2C%20as%20it%20enables%20the%20creation%20of%20synthetic%20graphs%20that%20mimic%20the%20properties%20of%20real-world%20networks%20for%20various%20applications.%20Graph%20Generative%20Models%20%28GGMs%29%20have%20emerged%20as%20a%20promising%20solution%20to%20this%20problem%2C%20leveraging%20deep%20learning%20techniques%20to%20learn%20the%20underlying%20distribution%20of%20real-world%20graphs%20and%20generate%20new%20samples%20that%20closely%20resemble%20them.%20Examples%20include%20approaches%20based%20on%20Variational%20Auto-Encoders%2C%20Recurrent%20Neural%20Networks%2C%20and%20more%20recently%2C%20diffusion-based%20models.%20However%2C%20the%20main%20limitation%20often%20lies%20in%20the%20evaluation%20process%2C%20which%20typically%20relies%20on%20Maximum%20Mean%20Discrepancy%20%28MMD%29%20as%20a%20metric%20to%20assess%20the%20distribution%20of%20graph%20properties%20in%20the%20generated%20ensemble.%20This%20paper%20introduces%20a%20novel%20methodology%20for%20evaluating%20GGMs%20that%20overcomes%20the%20limitations%20of%20MMD%2C%20which%20we%20call%20RGM%20%28Representation-aware%20Graph-generation%20Model%20evaluation%29.%20As%20a%20practical%20demonstration%20of%20our%20methodology%2C%20we%20present%20a%20comprehensive%20evaluation%20of%20two%20state-of-the-art%20Graph%20Generative%20Models%3A%20Graph%20Recurrent%20Attention%20Networks%20%28GRAN%29%20and%20Efficient%20and%20Degree-guided%20graph%20GEnerative%20model%20%28EDGE%29.%20We%20investigate%20their%20performance%20in%20generating%20realistic%20graphs%20and%20compare%20them%20using%20a%20Geometric%20Deep%20Learning%20model%20trained%20on%20a%20custom%20dataset%20of%20synthetic%20and%20real-world%20graphs%2C%20specifically%20designed%20for%20graph%20classification%20tasks.%20Our%20findings%20reveal%20that%20while%20both%20models%20can%20generate%20graphs%20with%20certain%20topological%20properties%2C%20they%20exhibit%20significant%20limitations%20in%20preserving%20the%20structural%20characteristics%20that%20distinguish%20different%20graph%20domains.%20We%20also%20highlight%20the%20inadequacy%20of%20Maximum%20Mean%20Discrepancy%20as%20an%20evaluation%20metric%20for%20GGMs%20and%20suggest%20alternative%20approaches%20for%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520MMD%253A%2520Evaluating%2520Graph%2520Generative%2520Models%2520with%2520Geometric%2520Deep%2520Learning%26entry.906535625%3DSalvatore%2520Romano%2520and%2520Marco%2520Grassia%2520and%2520Giuseppe%2520Mangioni%26entry.1292438233%3DGraph%2520generation%2520is%2520a%2520crucial%2520task%2520in%2520many%2520fields%252C%2520including%2520network%2520science%2520and%2520bioinformatics%252C%2520as%2520it%2520enables%2520the%2520creation%2520of%2520synthetic%2520graphs%2520that%2520mimic%2520the%2520properties%2520of%2520real-world%2520networks%2520for%2520various%2520applications.%2520Graph%2520Generative%2520Models%2520%2528GGMs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520this%2520problem%252C%2520leveraging%2520deep%2520learning%2520techniques%2520to%2520learn%2520the%2520underlying%2520distribution%2520of%2520real-world%2520graphs%2520and%2520generate%2520new%2520samples%2520that%2520closely%2520resemble%2520them.%2520Examples%2520include%2520approaches%2520based%2520on%2520Variational%2520Auto-Encoders%252C%2520Recurrent%2520Neural%2520Networks%252C%2520and%2520more%2520recently%252C%2520diffusion-based%2520models.%2520However%252C%2520the%2520main%2520limitation%2520often%2520lies%2520in%2520the%2520evaluation%2520process%252C%2520which%2520typically%2520relies%2520on%2520Maximum%2520Mean%2520Discrepancy%2520%2528MMD%2529%2520as%2520a%2520metric%2520to%2520assess%2520the%2520distribution%2520of%2520graph%2520properties%2520in%2520the%2520generated%2520ensemble.%2520This%2520paper%2520introduces%2520a%2520novel%2520methodology%2520for%2520evaluating%2520GGMs%2520that%2520overcomes%2520the%2520limitations%2520of%2520MMD%252C%2520which%2520we%2520call%2520RGM%2520%2528Representation-aware%2520Graph-generation%2520Model%2520evaluation%2529.%2520As%2520a%2520practical%2520demonstration%2520of%2520our%2520methodology%252C%2520we%2520present%2520a%2520comprehensive%2520evaluation%2520of%2520two%2520state-of-the-art%2520Graph%2520Generative%2520Models%253A%2520Graph%2520Recurrent%2520Attention%2520Networks%2520%2528GRAN%2529%2520and%2520Efficient%2520and%2520Degree-guided%2520graph%2520GEnerative%2520model%2520%2528EDGE%2529.%2520We%2520investigate%2520their%2520performance%2520in%2520generating%2520realistic%2520graphs%2520and%2520compare%2520them%2520using%2520a%2520Geometric%2520Deep%2520Learning%2520model%2520trained%2520on%2520a%2520custom%2520dataset%2520of%2520synthetic%2520and%2520real-world%2520graphs%252C%2520specifically%2520designed%2520for%2520graph%2520classification%2520tasks.%2520Our%2520findings%2520reveal%2520that%2520while%2520both%2520models%2520can%2520generate%2520graphs%2520with%2520certain%2520topological%2520properties%252C%2520they%2520exhibit%2520significant%2520limitations%2520in%2520preserving%2520the%2520structural%2520characteristics%2520that%2520distinguish%2520different%2520graph%2520domains.%2520We%2520also%2520highlight%2520the%2520inadequacy%2520of%2520Maximum%2520Mean%2520Discrepancy%2520as%2520an%2520evaluation%2520metric%2520for%2520GGMs%2520and%2520suggest%2520alternative%2520approaches%2520for%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20MMD%3A%20Evaluating%20Graph%20Generative%20Models%20with%20Geometric%20Deep%20Learning&entry.906535625=Salvatore%20Romano%20and%20Marco%20Grassia%20and%20Giuseppe%20Mangioni&entry.1292438233=Graph%20generation%20is%20a%20crucial%20task%20in%20many%20fields%2C%20including%20network%20science%20and%20bioinformatics%2C%20as%20it%20enables%20the%20creation%20of%20synthetic%20graphs%20that%20mimic%20the%20properties%20of%20real-world%20networks%20for%20various%20applications.%20Graph%20Generative%20Models%20%28GGMs%29%20have%20emerged%20as%20a%20promising%20solution%20to%20this%20problem%2C%20leveraging%20deep%20learning%20techniques%20to%20learn%20the%20underlying%20distribution%20of%20real-world%20graphs%20and%20generate%20new%20samples%20that%20closely%20resemble%20them.%20Examples%20include%20approaches%20based%20on%20Variational%20Auto-Encoders%2C%20Recurrent%20Neural%20Networks%2C%20and%20more%20recently%2C%20diffusion-based%20models.%20However%2C%20the%20main%20limitation%20often%20lies%20in%20the%20evaluation%20process%2C%20which%20typically%20relies%20on%20Maximum%20Mean%20Discrepancy%20%28MMD%29%20as%20a%20metric%20to%20assess%20the%20distribution%20of%20graph%20properties%20in%20the%20generated%20ensemble.%20This%20paper%20introduces%20a%20novel%20methodology%20for%20evaluating%20GGMs%20that%20overcomes%20the%20limitations%20of%20MMD%2C%20which%20we%20call%20RGM%20%28Representation-aware%20Graph-generation%20Model%20evaluation%29.%20As%20a%20practical%20demonstration%20of%20our%20methodology%2C%20we%20present%20a%20comprehensive%20evaluation%20of%20two%20state-of-the-art%20Graph%20Generative%20Models%3A%20Graph%20Recurrent%20Attention%20Networks%20%28GRAN%29%20and%20Efficient%20and%20Degree-guided%20graph%20GEnerative%20model%20%28EDGE%29.%20We%20investigate%20their%20performance%20in%20generating%20realistic%20graphs%20and%20compare%20them%20using%20a%20Geometric%20Deep%20Learning%20model%20trained%20on%20a%20custom%20dataset%20of%20synthetic%20and%20real-world%20graphs%2C%20specifically%20designed%20for%20graph%20classification%20tasks.%20Our%20findings%20reveal%20that%20while%20both%20models%20can%20generate%20graphs%20with%20certain%20topological%20properties%2C%20they%20exhibit%20significant%20limitations%20in%20preserving%20the%20structural%20characteristics%20that%20distinguish%20different%20graph%20domains.%20We%20also%20highlight%20the%20inadequacy%20of%20Maximum%20Mean%20Discrepancy%20as%20an%20evaluation%20metric%20for%20GGMs%20and%20suggest%20alternative%20approaches%20for%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.14241v1&entry.124074799=Read"},
{"title": "VICTOR: Dataset Copyright Auditing in Video Recognition Systems", "author": "Quan Yuan and Zhikun Zhang and Linkang Du and Min Chen and Mingyang Sun and Yunjun Gao and Shibo He and Jiming Chen", "abstract": "Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods.\n  In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.", "link": "http://arxiv.org/abs/2512.14439v1", "date": "2025-12-16", "relevancy": 2.2117, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5645}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5515}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VICTOR%3A%20Dataset%20Copyright%20Auditing%20in%20Video%20Recognition%20Systems&body=Title%3A%20VICTOR%3A%20Dataset%20Copyright%20Auditing%20in%20Video%20Recognition%20Systems%0AAuthor%3A%20Quan%20Yuan%20and%20Zhikun%20Zhang%20and%20Linkang%20Du%20and%20Min%20Chen%20and%20Mingyang%20Sun%20and%20Yunjun%20Gao%20and%20Shibo%20He%20and%20Jiming%20Chen%0AAbstract%3A%20Video%20recognition%20systems%20are%20increasingly%20being%20deployed%20in%20daily%20life%2C%20such%20as%20content%20recommendation%20and%20security%20monitoring.%20To%20enhance%20video%20recognition%20development%2C%20many%20institutions%20have%20released%20high-quality%20public%20datasets%20with%20open-source%20licenses%20for%20training%20advanced%20models.%20At%20the%20same%20time%2C%20these%20datasets%20are%20also%20susceptible%20to%20misuse%20and%20infringement.%20Dataset%20copyright%20auditing%20is%20an%20effective%20solution%20to%20identify%20such%20unauthorized%20use.%20However%2C%20existing%20dataset%20copyright%20solutions%20primarily%20focus%20on%20the%20image%20domain%3B%20the%20complex%20nature%20of%20video%20data%20leaves%20dataset%20copyright%20auditing%20in%20the%20video%20domain%20unexplored.%20Specifically%2C%20video%20data%20introduces%20an%20additional%20temporal%20dimension%2C%20which%20poses%20significant%20challenges%20to%20the%20effectiveness%20and%20stealthiness%20of%20existing%20methods.%0A%20%20In%20this%20paper%2C%20we%20propose%20VICTOR%2C%20the%20first%20dataset%20copyright%20auditing%20approach%20for%20video%20recognition%20systems.%20We%20develop%20a%20general%20and%20stealthy%20sample%20modification%20strategy%20that%20enhances%20the%20output%20discrepancy%20of%20the%20target%20model.%20By%20modifying%20only%20a%20small%20proportion%20of%20samples%20%28e.g.%2C%201%25%29%2C%20VICTOR%20amplifies%20the%20impact%20of%20published%20modified%20samples%20on%20the%20prediction%20behavior%20of%20the%20target%20models.%20Then%2C%20the%20difference%20in%20the%20model%27s%20behavior%20for%20published%20modified%20and%20unpublished%20original%20samples%20can%20serve%20as%20a%20key%20basis%20for%20dataset%20auditing.%20Extensive%20experiments%20on%20multiple%20models%20and%20datasets%20highlight%20the%20superiority%20of%20VICTOR.%20Finally%2C%20we%20show%20that%20VICTOR%20is%20robust%20in%20the%20presence%20of%20several%20perturbation%20mechanisms%20to%20the%20training%20videos%20or%20the%20target%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVICTOR%253A%2520Dataset%2520Copyright%2520Auditing%2520in%2520Video%2520Recognition%2520Systems%26entry.906535625%3DQuan%2520Yuan%2520and%2520Zhikun%2520Zhang%2520and%2520Linkang%2520Du%2520and%2520Min%2520Chen%2520and%2520Mingyang%2520Sun%2520and%2520Yunjun%2520Gao%2520and%2520Shibo%2520He%2520and%2520Jiming%2520Chen%26entry.1292438233%3DVideo%2520recognition%2520systems%2520are%2520increasingly%2520being%2520deployed%2520in%2520daily%2520life%252C%2520such%2520as%2520content%2520recommendation%2520and%2520security%2520monitoring.%2520To%2520enhance%2520video%2520recognition%2520development%252C%2520many%2520institutions%2520have%2520released%2520high-quality%2520public%2520datasets%2520with%2520open-source%2520licenses%2520for%2520training%2520advanced%2520models.%2520At%2520the%2520same%2520time%252C%2520these%2520datasets%2520are%2520also%2520susceptible%2520to%2520misuse%2520and%2520infringement.%2520Dataset%2520copyright%2520auditing%2520is%2520an%2520effective%2520solution%2520to%2520identify%2520such%2520unauthorized%2520use.%2520However%252C%2520existing%2520dataset%2520copyright%2520solutions%2520primarily%2520focus%2520on%2520the%2520image%2520domain%253B%2520the%2520complex%2520nature%2520of%2520video%2520data%2520leaves%2520dataset%2520copyright%2520auditing%2520in%2520the%2520video%2520domain%2520unexplored.%2520Specifically%252C%2520video%2520data%2520introduces%2520an%2520additional%2520temporal%2520dimension%252C%2520which%2520poses%2520significant%2520challenges%2520to%2520the%2520effectiveness%2520and%2520stealthiness%2520of%2520existing%2520methods.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520VICTOR%252C%2520the%2520first%2520dataset%2520copyright%2520auditing%2520approach%2520for%2520video%2520recognition%2520systems.%2520We%2520develop%2520a%2520general%2520and%2520stealthy%2520sample%2520modification%2520strategy%2520that%2520enhances%2520the%2520output%2520discrepancy%2520of%2520the%2520target%2520model.%2520By%2520modifying%2520only%2520a%2520small%2520proportion%2520of%2520samples%2520%2528e.g.%252C%25201%2525%2529%252C%2520VICTOR%2520amplifies%2520the%2520impact%2520of%2520published%2520modified%2520samples%2520on%2520the%2520prediction%2520behavior%2520of%2520the%2520target%2520models.%2520Then%252C%2520the%2520difference%2520in%2520the%2520model%2527s%2520behavior%2520for%2520published%2520modified%2520and%2520unpublished%2520original%2520samples%2520can%2520serve%2520as%2520a%2520key%2520basis%2520for%2520dataset%2520auditing.%2520Extensive%2520experiments%2520on%2520multiple%2520models%2520and%2520datasets%2520highlight%2520the%2520superiority%2520of%2520VICTOR.%2520Finally%252C%2520we%2520show%2520that%2520VICTOR%2520is%2520robust%2520in%2520the%2520presence%2520of%2520several%2520perturbation%2520mechanisms%2520to%2520the%2520training%2520videos%2520or%2520the%2520target%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VICTOR%3A%20Dataset%20Copyright%20Auditing%20in%20Video%20Recognition%20Systems&entry.906535625=Quan%20Yuan%20and%20Zhikun%20Zhang%20and%20Linkang%20Du%20and%20Min%20Chen%20and%20Mingyang%20Sun%20and%20Yunjun%20Gao%20and%20Shibo%20He%20and%20Jiming%20Chen&entry.1292438233=Video%20recognition%20systems%20are%20increasingly%20being%20deployed%20in%20daily%20life%2C%20such%20as%20content%20recommendation%20and%20security%20monitoring.%20To%20enhance%20video%20recognition%20development%2C%20many%20institutions%20have%20released%20high-quality%20public%20datasets%20with%20open-source%20licenses%20for%20training%20advanced%20models.%20At%20the%20same%20time%2C%20these%20datasets%20are%20also%20susceptible%20to%20misuse%20and%20infringement.%20Dataset%20copyright%20auditing%20is%20an%20effective%20solution%20to%20identify%20such%20unauthorized%20use.%20However%2C%20existing%20dataset%20copyright%20solutions%20primarily%20focus%20on%20the%20image%20domain%3B%20the%20complex%20nature%20of%20video%20data%20leaves%20dataset%20copyright%20auditing%20in%20the%20video%20domain%20unexplored.%20Specifically%2C%20video%20data%20introduces%20an%20additional%20temporal%20dimension%2C%20which%20poses%20significant%20challenges%20to%20the%20effectiveness%20and%20stealthiness%20of%20existing%20methods.%0A%20%20In%20this%20paper%2C%20we%20propose%20VICTOR%2C%20the%20first%20dataset%20copyright%20auditing%20approach%20for%20video%20recognition%20systems.%20We%20develop%20a%20general%20and%20stealthy%20sample%20modification%20strategy%20that%20enhances%20the%20output%20discrepancy%20of%20the%20target%20model.%20By%20modifying%20only%20a%20small%20proportion%20of%20samples%20%28e.g.%2C%201%25%29%2C%20VICTOR%20amplifies%20the%20impact%20of%20published%20modified%20samples%20on%20the%20prediction%20behavior%20of%20the%20target%20models.%20Then%2C%20the%20difference%20in%20the%20model%27s%20behavior%20for%20published%20modified%20and%20unpublished%20original%20samples%20can%20serve%20as%20a%20key%20basis%20for%20dataset%20auditing.%20Extensive%20experiments%20on%20multiple%20models%20and%20datasets%20highlight%20the%20superiority%20of%20VICTOR.%20Finally%2C%20we%20show%20that%20VICTOR%20is%20robust%20in%20the%20presence%20of%20several%20perturbation%20mechanisms%20to%20the%20training%20videos%20or%20the%20target%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.14439v1&entry.124074799=Read"},
{"title": "Continual Learning at the Edge: An Agnostic IIoT Architecture", "author": "Pablo Garc\u00eda-Santaclara and Bruno Fern\u00e1ndez-Castro and Rebeca P. D\u00edaz-Redondo and Carlos Calvo-Moa and Henar Mari\u00f1o-Bodel\u00f3n", "abstract": "The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.", "link": "http://arxiv.org/abs/2512.14311v1", "date": "2025-12-16", "relevancy": 2.2046, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4602}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4363}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20at%20the%20Edge%3A%20An%20Agnostic%20IIoT%20Architecture&body=Title%3A%20Continual%20Learning%20at%20the%20Edge%3A%20An%20Agnostic%20IIoT%20Architecture%0AAuthor%3A%20Pablo%20Garc%C3%ADa-Santaclara%20and%20Bruno%20Fern%C3%A1ndez-Castro%20and%20Rebeca%20P.%20D%C3%ADaz-Redondo%20and%20Carlos%20Calvo-Moa%20and%20Henar%20Mari%C3%B1o-Bodel%C3%B3n%0AAbstract%3A%20The%20exponential%20growth%20of%20Internet-connected%20devices%20has%20presented%20challenges%20to%20traditional%20centralized%20computing%20systems%20due%20to%20latency%20and%20bandwidth%20limitations.%20Edge%20computing%20has%20evolved%20to%20address%20these%20difficulties%20by%20bringing%20computations%20closer%20to%20the%20data%20source.%20Additionally%2C%20traditional%20machine%20learning%20algorithms%20are%20not%20suitable%20for%20edge-computing%20systems%2C%20where%20data%20usually%20arrives%20in%20a%20dynamic%20and%20continual%20way.%20However%2C%20incremental%20learning%20offers%20a%20good%20solution%20for%20these%20settings.%20We%20introduce%20a%20new%20approach%20that%20applies%20the%20incremental%20learning%20philosophy%20within%20an%20edge-computing%20scenario%20for%20the%20industrial%20sector%20with%20a%20specific%20purpose%3A%20real%20time%20quality%20control%20in%20a%20manufacturing%20system.%20Applying%20continual%20learning%20we%20reduce%20the%20impact%20of%20catastrophic%20forgetting%20and%20provide%20an%20efficient%20and%20effective%20solution.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Learning%2520at%2520the%2520Edge%253A%2520An%2520Agnostic%2520IIoT%2520Architecture%26entry.906535625%3DPablo%2520Garc%25C3%25ADa-Santaclara%2520and%2520Bruno%2520Fern%25C3%25A1ndez-Castro%2520and%2520Rebeca%2520P.%2520D%25C3%25ADaz-Redondo%2520and%2520Carlos%2520Calvo-Moa%2520and%2520Henar%2520Mari%25C3%25B1o-Bodel%25C3%25B3n%26entry.1292438233%3DThe%2520exponential%2520growth%2520of%2520Internet-connected%2520devices%2520has%2520presented%2520challenges%2520to%2520traditional%2520centralized%2520computing%2520systems%2520due%2520to%2520latency%2520and%2520bandwidth%2520limitations.%2520Edge%2520computing%2520has%2520evolved%2520to%2520address%2520these%2520difficulties%2520by%2520bringing%2520computations%2520closer%2520to%2520the%2520data%2520source.%2520Additionally%252C%2520traditional%2520machine%2520learning%2520algorithms%2520are%2520not%2520suitable%2520for%2520edge-computing%2520systems%252C%2520where%2520data%2520usually%2520arrives%2520in%2520a%2520dynamic%2520and%2520continual%2520way.%2520However%252C%2520incremental%2520learning%2520offers%2520a%2520good%2520solution%2520for%2520these%2520settings.%2520We%2520introduce%2520a%2520new%2520approach%2520that%2520applies%2520the%2520incremental%2520learning%2520philosophy%2520within%2520an%2520edge-computing%2520scenario%2520for%2520the%2520industrial%2520sector%2520with%2520a%2520specific%2520purpose%253A%2520real%2520time%2520quality%2520control%2520in%2520a%2520manufacturing%2520system.%2520Applying%2520continual%2520learning%2520we%2520reduce%2520the%2520impact%2520of%2520catastrophic%2520forgetting%2520and%2520provide%2520an%2520efficient%2520and%2520effective%2520solution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20at%20the%20Edge%3A%20An%20Agnostic%20IIoT%20Architecture&entry.906535625=Pablo%20Garc%C3%ADa-Santaclara%20and%20Bruno%20Fern%C3%A1ndez-Castro%20and%20Rebeca%20P.%20D%C3%ADaz-Redondo%20and%20Carlos%20Calvo-Moa%20and%20Henar%20Mari%C3%B1o-Bodel%C3%B3n&entry.1292438233=The%20exponential%20growth%20of%20Internet-connected%20devices%20has%20presented%20challenges%20to%20traditional%20centralized%20computing%20systems%20due%20to%20latency%20and%20bandwidth%20limitations.%20Edge%20computing%20has%20evolved%20to%20address%20these%20difficulties%20by%20bringing%20computations%20closer%20to%20the%20data%20source.%20Additionally%2C%20traditional%20machine%20learning%20algorithms%20are%20not%20suitable%20for%20edge-computing%20systems%2C%20where%20data%20usually%20arrives%20in%20a%20dynamic%20and%20continual%20way.%20However%2C%20incremental%20learning%20offers%20a%20good%20solution%20for%20these%20settings.%20We%20introduce%20a%20new%20approach%20that%20applies%20the%20incremental%20learning%20philosophy%20within%20an%20edge-computing%20scenario%20for%20the%20industrial%20sector%20with%20a%20specific%20purpose%3A%20real%20time%20quality%20control%20in%20a%20manufacturing%20system.%20Applying%20continual%20learning%20we%20reduce%20the%20impact%20of%20catastrophic%20forgetting%20and%20provide%20an%20efficient%20and%20effective%20solution.&entry.1838667208=http%3A//arxiv.org/abs/2512.14311v1&entry.124074799=Read"},
{"title": "Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs", "author": "Wentao Wan and Kaiyu Wu and Qingyang Ma and Nan Kang and Yunjie Chen and Liang Lin and Keze Wang", "abstract": "Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.", "link": "http://arxiv.org/abs/2512.14257v1", "date": "2025-12-16", "relevancy": 2.204, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Visual%20Programming%20for%20Visual%20Reasoning%20via%20Probabilistic%20Graphs&body=Title%3A%20Enhancing%20Visual%20Programming%20for%20Visual%20Reasoning%20via%20Probabilistic%20Graphs%0AAuthor%3A%20Wentao%20Wan%20and%20Kaiyu%20Wu%20and%20Qingyang%20Ma%20and%20Nan%20Kang%20and%20Yunjie%20Chen%20and%20Liang%20Lin%20and%20Keze%20Wang%0AAbstract%3A%20Recently%2C%20Visual%20Programming%20%28VP%29%20based%20on%20large%20language%20models%20%28LLMs%29%20has%20rapidly%20developed%20and%20demonstrated%20significant%20potential%20in%20complex%20Visual%20Reasoning%20%28VR%29%20tasks.%20Previous%20works%20to%20enhance%20VP%20have%20primarily%20focused%20on%20improving%20the%20quality%20of%20LLM-generated%20visual%20programs.%20However%2C%20they%20have%20neglected%20to%20optimize%20the%20VP-invoked%20pre-trained%20models%2C%20which%20serve%20as%20modules%20for%20the%20visual%20sub-tasks%20decomposed%20from%20the%20targeted%20tasks%20by%20VP.%20The%20difficulty%20is%20that%20there%20are%20only%20final%20labels%20of%20targeted%20VR%20tasks%20rather%20than%20labels%20of%20sub-tasks.%20Besides%2C%20the%20non-differentiable%20nature%20of%20VP%20impedes%20the%20direct%20use%20of%20efficient%20gradient-based%20optimization%20methods%20to%20leverage%20final%20labels%20for%20end-to-end%20learning%20of%20the%20entire%20VP%20framework.%20To%20overcome%20these%20issues%2C%20we%20propose%20EVPG%2C%20a%20method%20to%20Enhance%20Visual%20Programming%20for%20visual%20reasoning%20via%20Probabilistic%20Graphs.%20Specifically%2C%20we%20creatively%20build%20a%20directed%20probabilistic%20graph%20according%20to%20the%20variable%20dependency%20relationships%20during%20the%20VP%20executing%20process%2C%20which%20reconstructs%20the%20non-differentiable%20VP%20executing%20process%20into%20a%20differentiable%20exact%20probability%20inference%20process%20on%20this%20directed%20probabilistic%20graph.%20As%20a%20result%2C%20this%20enables%20the%20VP%20framework%20to%20utilize%20the%20final%20labels%20for%20efficient%2C%20gradient-based%20optimization%20in%20end-to-end%20supervised%20learning%20on%20targeted%20VR%20tasks.%20Extensive%20and%20comprehensive%20experiments%20demonstrate%20the%20effectiveness%20and%20advantages%20of%20our%20EVPG%2C%20showing%20significant%20performance%20improvements%20for%20VP%20on%20three%20classical%20complex%20VR%20tasks%3A%20GQA%2C%20NLVRv2%2C%20and%20Open%20Images.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Visual%2520Programming%2520for%2520Visual%2520Reasoning%2520via%2520Probabilistic%2520Graphs%26entry.906535625%3DWentao%2520Wan%2520and%2520Kaiyu%2520Wu%2520and%2520Qingyang%2520Ma%2520and%2520Nan%2520Kang%2520and%2520Yunjie%2520Chen%2520and%2520Liang%2520Lin%2520and%2520Keze%2520Wang%26entry.1292438233%3DRecently%252C%2520Visual%2520Programming%2520%2528VP%2529%2520based%2520on%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520rapidly%2520developed%2520and%2520demonstrated%2520significant%2520potential%2520in%2520complex%2520Visual%2520Reasoning%2520%2528VR%2529%2520tasks.%2520Previous%2520works%2520to%2520enhance%2520VP%2520have%2520primarily%2520focused%2520on%2520improving%2520the%2520quality%2520of%2520LLM-generated%2520visual%2520programs.%2520However%252C%2520they%2520have%2520neglected%2520to%2520optimize%2520the%2520VP-invoked%2520pre-trained%2520models%252C%2520which%2520serve%2520as%2520modules%2520for%2520the%2520visual%2520sub-tasks%2520decomposed%2520from%2520the%2520targeted%2520tasks%2520by%2520VP.%2520The%2520difficulty%2520is%2520that%2520there%2520are%2520only%2520final%2520labels%2520of%2520targeted%2520VR%2520tasks%2520rather%2520than%2520labels%2520of%2520sub-tasks.%2520Besides%252C%2520the%2520non-differentiable%2520nature%2520of%2520VP%2520impedes%2520the%2520direct%2520use%2520of%2520efficient%2520gradient-based%2520optimization%2520methods%2520to%2520leverage%2520final%2520labels%2520for%2520end-to-end%2520learning%2520of%2520the%2520entire%2520VP%2520framework.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520EVPG%252C%2520a%2520method%2520to%2520Enhance%2520Visual%2520Programming%2520for%2520visual%2520reasoning%2520via%2520Probabilistic%2520Graphs.%2520Specifically%252C%2520we%2520creatively%2520build%2520a%2520directed%2520probabilistic%2520graph%2520according%2520to%2520the%2520variable%2520dependency%2520relationships%2520during%2520the%2520VP%2520executing%2520process%252C%2520which%2520reconstructs%2520the%2520non-differentiable%2520VP%2520executing%2520process%2520into%2520a%2520differentiable%2520exact%2520probability%2520inference%2520process%2520on%2520this%2520directed%2520probabilistic%2520graph.%2520As%2520a%2520result%252C%2520this%2520enables%2520the%2520VP%2520framework%2520to%2520utilize%2520the%2520final%2520labels%2520for%2520efficient%252C%2520gradient-based%2520optimization%2520in%2520end-to-end%2520supervised%2520learning%2520on%2520targeted%2520VR%2520tasks.%2520Extensive%2520and%2520comprehensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520and%2520advantages%2520of%2520our%2520EVPG%252C%2520showing%2520significant%2520performance%2520improvements%2520for%2520VP%2520on%2520three%2520classical%2520complex%2520VR%2520tasks%253A%2520GQA%252C%2520NLVRv2%252C%2520and%2520Open%2520Images.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Visual%20Programming%20for%20Visual%20Reasoning%20via%20Probabilistic%20Graphs&entry.906535625=Wentao%20Wan%20and%20Kaiyu%20Wu%20and%20Qingyang%20Ma%20and%20Nan%20Kang%20and%20Yunjie%20Chen%20and%20Liang%20Lin%20and%20Keze%20Wang&entry.1292438233=Recently%2C%20Visual%20Programming%20%28VP%29%20based%20on%20large%20language%20models%20%28LLMs%29%20has%20rapidly%20developed%20and%20demonstrated%20significant%20potential%20in%20complex%20Visual%20Reasoning%20%28VR%29%20tasks.%20Previous%20works%20to%20enhance%20VP%20have%20primarily%20focused%20on%20improving%20the%20quality%20of%20LLM-generated%20visual%20programs.%20However%2C%20they%20have%20neglected%20to%20optimize%20the%20VP-invoked%20pre-trained%20models%2C%20which%20serve%20as%20modules%20for%20the%20visual%20sub-tasks%20decomposed%20from%20the%20targeted%20tasks%20by%20VP.%20The%20difficulty%20is%20that%20there%20are%20only%20final%20labels%20of%20targeted%20VR%20tasks%20rather%20than%20labels%20of%20sub-tasks.%20Besides%2C%20the%20non-differentiable%20nature%20of%20VP%20impedes%20the%20direct%20use%20of%20efficient%20gradient-based%20optimization%20methods%20to%20leverage%20final%20labels%20for%20end-to-end%20learning%20of%20the%20entire%20VP%20framework.%20To%20overcome%20these%20issues%2C%20we%20propose%20EVPG%2C%20a%20method%20to%20Enhance%20Visual%20Programming%20for%20visual%20reasoning%20via%20Probabilistic%20Graphs.%20Specifically%2C%20we%20creatively%20build%20a%20directed%20probabilistic%20graph%20according%20to%20the%20variable%20dependency%20relationships%20during%20the%20VP%20executing%20process%2C%20which%20reconstructs%20the%20non-differentiable%20VP%20executing%20process%20into%20a%20differentiable%20exact%20probability%20inference%20process%20on%20this%20directed%20probabilistic%20graph.%20As%20a%20result%2C%20this%20enables%20the%20VP%20framework%20to%20utilize%20the%20final%20labels%20for%20efficient%2C%20gradient-based%20optimization%20in%20end-to-end%20supervised%20learning%20on%20targeted%20VR%20tasks.%20Extensive%20and%20comprehensive%20experiments%20demonstrate%20the%20effectiveness%20and%20advantages%20of%20our%20EVPG%2C%20showing%20significant%20performance%20improvements%20for%20VP%20on%20three%20classical%20complex%20VR%20tasks%3A%20GQA%2C%20NLVRv2%2C%20and%20Open%20Images.&entry.1838667208=http%3A//arxiv.org/abs/2512.14257v1&entry.124074799=Read"},
{"title": "Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization", "author": "Yen-Ju Lu and Kunxiao Gao and Mingrui Liang and Helin Wang and Thomas Thebaud and Laureano Moro-Velazquez and Najim Dehak and Jesus Villalba", "abstract": "Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.", "link": "http://arxiv.org/abs/2512.14687v1", "date": "2025-12-16", "relevancy": 2.1966, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spoken%20DialogSum%3A%20An%20Emotion-Rich%20Conversational%20Dataset%20for%20Spoken%20Dialogue%20Summarization&body=Title%3A%20Spoken%20DialogSum%3A%20An%20Emotion-Rich%20Conversational%20Dataset%20for%20Spoken%20Dialogue%20Summarization%0AAuthor%3A%20Yen-Ju%20Lu%20and%20Kunxiao%20Gao%20and%20Mingrui%20Liang%20and%20Helin%20Wang%20and%20Thomas%20Thebaud%20and%20Laureano%20Moro-Velazquez%20and%20Najim%20Dehak%20and%20Jesus%20Villalba%0AAbstract%3A%20Recent%20audio%20language%20models%20can%20follow%20long%20conversations.%20However%2C%20research%20on%20emotion-aware%20or%20spoken%20dialogue%20summarization%20is%20constrained%20by%20the%20lack%20of%20data%20that%20links%20speech%2C%20summaries%2C%20and%20paralinguistic%20cues.%20We%20introduce%20Spoken%20DialogSum%2C%20the%20first%20corpus%20aligning%20raw%20conversational%20audio%20with%20factual%20summaries%2C%20emotion-rich%20summaries%2C%20and%20utterance-level%20labels%20for%20speaker%20age%2C%20gender%2C%20and%20emotion.%20The%20dataset%20is%20built%20in%20two%20stages%3A%20first%2C%20an%20LLM%20rewrites%20DialogSum%20scripts%20with%20Switchboard-style%20fillers%20and%20back-channels%2C%20then%20tags%20each%20utterance%20with%20emotion%2C%20pitch%2C%20and%20speaking%20rate.%20Second%2C%20an%20expressive%20TTS%20engine%20synthesizes%20speech%20from%20the%20tagged%20scripts%2C%20aligned%20with%20paralinguistic%20labels.%20Spoken%20DialogSum%20comprises%2013%2C460%20emotion-diverse%20dialogues%2C%20each%20paired%20with%20both%20a%20factual%20and%20an%20emotion-focused%20summary.%20The%20dataset%20is%20available%20online%20at%20https%3A//fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/.%20Baselines%20show%20that%20an%20Audio-LLM%20raises%20emotional-summary%20ROUGE-L%20by%2028%25%20relative%20to%20a%20cascaded%20ASR-LLM%20system%2C%20confirming%20the%20value%20of%20end-to-end%20speech%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpoken%2520DialogSum%253A%2520An%2520Emotion-Rich%2520Conversational%2520Dataset%2520for%2520Spoken%2520Dialogue%2520Summarization%26entry.906535625%3DYen-Ju%2520Lu%2520and%2520Kunxiao%2520Gao%2520and%2520Mingrui%2520Liang%2520and%2520Helin%2520Wang%2520and%2520Thomas%2520Thebaud%2520and%2520Laureano%2520Moro-Velazquez%2520and%2520Najim%2520Dehak%2520and%2520Jesus%2520Villalba%26entry.1292438233%3DRecent%2520audio%2520language%2520models%2520can%2520follow%2520long%2520conversations.%2520However%252C%2520research%2520on%2520emotion-aware%2520or%2520spoken%2520dialogue%2520summarization%2520is%2520constrained%2520by%2520the%2520lack%2520of%2520data%2520that%2520links%2520speech%252C%2520summaries%252C%2520and%2520paralinguistic%2520cues.%2520We%2520introduce%2520Spoken%2520DialogSum%252C%2520the%2520first%2520corpus%2520aligning%2520raw%2520conversational%2520audio%2520with%2520factual%2520summaries%252C%2520emotion-rich%2520summaries%252C%2520and%2520utterance-level%2520labels%2520for%2520speaker%2520age%252C%2520gender%252C%2520and%2520emotion.%2520The%2520dataset%2520is%2520built%2520in%2520two%2520stages%253A%2520first%252C%2520an%2520LLM%2520rewrites%2520DialogSum%2520scripts%2520with%2520Switchboard-style%2520fillers%2520and%2520back-channels%252C%2520then%2520tags%2520each%2520utterance%2520with%2520emotion%252C%2520pitch%252C%2520and%2520speaking%2520rate.%2520Second%252C%2520an%2520expressive%2520TTS%2520engine%2520synthesizes%2520speech%2520from%2520the%2520tagged%2520scripts%252C%2520aligned%2520with%2520paralinguistic%2520labels.%2520Spoken%2520DialogSum%2520comprises%252013%252C460%2520emotion-diverse%2520dialogues%252C%2520each%2520paired%2520with%2520both%2520a%2520factual%2520and%2520an%2520emotion-focused%2520summary.%2520The%2520dataset%2520is%2520available%2520online%2520at%2520https%253A//fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/.%2520Baselines%2520show%2520that%2520an%2520Audio-LLM%2520raises%2520emotional-summary%2520ROUGE-L%2520by%252028%2525%2520relative%2520to%2520a%2520cascaded%2520ASR-LLM%2520system%252C%2520confirming%2520the%2520value%2520of%2520end-to-end%2520speech%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spoken%20DialogSum%3A%20An%20Emotion-Rich%20Conversational%20Dataset%20for%20Spoken%20Dialogue%20Summarization&entry.906535625=Yen-Ju%20Lu%20and%20Kunxiao%20Gao%20and%20Mingrui%20Liang%20and%20Helin%20Wang%20and%20Thomas%20Thebaud%20and%20Laureano%20Moro-Velazquez%20and%20Najim%20Dehak%20and%20Jesus%20Villalba&entry.1292438233=Recent%20audio%20language%20models%20can%20follow%20long%20conversations.%20However%2C%20research%20on%20emotion-aware%20or%20spoken%20dialogue%20summarization%20is%20constrained%20by%20the%20lack%20of%20data%20that%20links%20speech%2C%20summaries%2C%20and%20paralinguistic%20cues.%20We%20introduce%20Spoken%20DialogSum%2C%20the%20first%20corpus%20aligning%20raw%20conversational%20audio%20with%20factual%20summaries%2C%20emotion-rich%20summaries%2C%20and%20utterance-level%20labels%20for%20speaker%20age%2C%20gender%2C%20and%20emotion.%20The%20dataset%20is%20built%20in%20two%20stages%3A%20first%2C%20an%20LLM%20rewrites%20DialogSum%20scripts%20with%20Switchboard-style%20fillers%20and%20back-channels%2C%20then%20tags%20each%20utterance%20with%20emotion%2C%20pitch%2C%20and%20speaking%20rate.%20Second%2C%20an%20expressive%20TTS%20engine%20synthesizes%20speech%20from%20the%20tagged%20scripts%2C%20aligned%20with%20paralinguistic%20labels.%20Spoken%20DialogSum%20comprises%2013%2C460%20emotion-diverse%20dialogues%2C%20each%20paired%20with%20both%20a%20factual%20and%20an%20emotion-focused%20summary.%20The%20dataset%20is%20available%20online%20at%20https%3A//fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/.%20Baselines%20show%20that%20an%20Audio-LLM%20raises%20emotional-summary%20ROUGE-L%20by%2028%25%20relative%20to%20a%20cascaded%20ASR-LLM%20system%2C%20confirming%20the%20value%20of%20end-to-end%20speech%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2512.14687v1&entry.124074799=Read"},
{"title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review", "author": "C\u00e9line Finet and Stephane Da Silva Martins and Jean-Bernard Hayet and Ioannis Karamouzas and Javad Amirian and Sylvie Le H\u00e9garat-Mascle and Julien Pettr\u00e9 and Emanuel Aldea", "abstract": "With the emergence of powerful data-driven methods in human trajectory prediction (HTP), gaining a finer understanding of multi-agent interactions lies within hand's reach, with important implications in areas such as social robot navigation, autonomous navigation, and crowd modeling. This survey reviews some of the most recent advancements in deep learning-based multi-agent trajectory prediction, focusing on studies published between 2020 and 2025. We categorize the existing methods based on their architectural design, their input representations, and their overall prediction strategies, placing a particular emphasis on models evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges and future research directions in the field of multi-agent HTP.", "link": "http://arxiv.org/abs/2506.14831v2", "date": "2025-12-16", "relevancy": 2.1941, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6058}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5555}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20Multi-Agent%20Human%20Trajectory%20Prediction%3A%20A%20Comprehensive%20Review&body=Title%3A%20Recent%20Advances%20in%20Multi-Agent%20Human%20Trajectory%20Prediction%3A%20A%20Comprehensive%20Review%0AAuthor%3A%20C%C3%A9line%20Finet%20and%20Stephane%20Da%20Silva%20Martins%20and%20Jean-Bernard%20Hayet%20and%20Ioannis%20Karamouzas%20and%20Javad%20Amirian%20and%20Sylvie%20Le%20H%C3%A9garat-Mascle%20and%20Julien%20Pettr%C3%A9%20and%20Emanuel%20Aldea%0AAbstract%3A%20With%20the%20emergence%20of%20powerful%20data-driven%20methods%20in%20human%20trajectory%20prediction%20%28HTP%29%2C%20gaining%20a%20finer%20understanding%20of%20multi-agent%20interactions%20lies%20within%20hand%27s%20reach%2C%20with%20important%20implications%20in%20areas%20such%20as%20social%20robot%20navigation%2C%20autonomous%20navigation%2C%20and%20crowd%20modeling.%20This%20survey%20reviews%20some%20of%20the%20most%20recent%20advancements%20in%20deep%20learning-based%20multi-agent%20trajectory%20prediction%2C%20focusing%20on%20studies%20published%20between%202020%20and%202025.%20We%20categorize%20the%20existing%20methods%20based%20on%20their%20architectural%20design%2C%20their%20input%20representations%2C%20and%20their%20overall%20prediction%20strategies%2C%20placing%20a%20particular%20emphasis%20on%20models%20evaluated%20using%20the%20ETH/UCY%20benchmark.%20Furthermore%2C%20we%20highlight%20key%20challenges%20and%20future%20research%20directions%20in%20the%20field%20of%20multi-agent%20HTP.%0ALink%3A%20http%3A//arxiv.org/abs/2506.14831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecent%2520Advances%2520in%2520Multi-Agent%2520Human%2520Trajectory%2520Prediction%253A%2520A%2520Comprehensive%2520Review%26entry.906535625%3DC%25C3%25A9line%2520Finet%2520and%2520Stephane%2520Da%2520Silva%2520Martins%2520and%2520Jean-Bernard%2520Hayet%2520and%2520Ioannis%2520Karamouzas%2520and%2520Javad%2520Amirian%2520and%2520Sylvie%2520Le%2520H%25C3%25A9garat-Mascle%2520and%2520Julien%2520Pettr%25C3%25A9%2520and%2520Emanuel%2520Aldea%26entry.1292438233%3DWith%2520the%2520emergence%2520of%2520powerful%2520data-driven%2520methods%2520in%2520human%2520trajectory%2520prediction%2520%2528HTP%2529%252C%2520gaining%2520a%2520finer%2520understanding%2520of%2520multi-agent%2520interactions%2520lies%2520within%2520hand%2527s%2520reach%252C%2520with%2520important%2520implications%2520in%2520areas%2520such%2520as%2520social%2520robot%2520navigation%252C%2520autonomous%2520navigation%252C%2520and%2520crowd%2520modeling.%2520This%2520survey%2520reviews%2520some%2520of%2520the%2520most%2520recent%2520advancements%2520in%2520deep%2520learning-based%2520multi-agent%2520trajectory%2520prediction%252C%2520focusing%2520on%2520studies%2520published%2520between%25202020%2520and%25202025.%2520We%2520categorize%2520the%2520existing%2520methods%2520based%2520on%2520their%2520architectural%2520design%252C%2520their%2520input%2520representations%252C%2520and%2520their%2520overall%2520prediction%2520strategies%252C%2520placing%2520a%2520particular%2520emphasis%2520on%2520models%2520evaluated%2520using%2520the%2520ETH/UCY%2520benchmark.%2520Furthermore%252C%2520we%2520highlight%2520key%2520challenges%2520and%2520future%2520research%2520directions%2520in%2520the%2520field%2520of%2520multi-agent%2520HTP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20Multi-Agent%20Human%20Trajectory%20Prediction%3A%20A%20Comprehensive%20Review&entry.906535625=C%C3%A9line%20Finet%20and%20Stephane%20Da%20Silva%20Martins%20and%20Jean-Bernard%20Hayet%20and%20Ioannis%20Karamouzas%20and%20Javad%20Amirian%20and%20Sylvie%20Le%20H%C3%A9garat-Mascle%20and%20Julien%20Pettr%C3%A9%20and%20Emanuel%20Aldea&entry.1292438233=With%20the%20emergence%20of%20powerful%20data-driven%20methods%20in%20human%20trajectory%20prediction%20%28HTP%29%2C%20gaining%20a%20finer%20understanding%20of%20multi-agent%20interactions%20lies%20within%20hand%27s%20reach%2C%20with%20important%20implications%20in%20areas%20such%20as%20social%20robot%20navigation%2C%20autonomous%20navigation%2C%20and%20crowd%20modeling.%20This%20survey%20reviews%20some%20of%20the%20most%20recent%20advancements%20in%20deep%20learning-based%20multi-agent%20trajectory%20prediction%2C%20focusing%20on%20studies%20published%20between%202020%20and%202025.%20We%20categorize%20the%20existing%20methods%20based%20on%20their%20architectural%20design%2C%20their%20input%20representations%2C%20and%20their%20overall%20prediction%20strategies%2C%20placing%20a%20particular%20emphasis%20on%20models%20evaluated%20using%20the%20ETH/UCY%20benchmark.%20Furthermore%2C%20we%20highlight%20key%20challenges%20and%20future%20research%20directions%20in%20the%20field%20of%20multi-agent%20HTP.&entry.1838667208=http%3A//arxiv.org/abs/2506.14831v2&entry.124074799=Read"},
{"title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition", "author": "Yiqing Zhou and Yu Lei and Shuzheng Si and Qingyan Sun and Wei Wang and Yifei Wu and Hao Wen and Gang Chen and Fanchao Qi and Maosong Sun", "abstract": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.", "link": "http://arxiv.org/abs/2512.14244v1", "date": "2025-12-16", "relevancy": 2.1829, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition&body=Title%3A%20From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition%0AAuthor%3A%20Yiqing%20Zhou%20and%20Yu%20Lei%20and%20Shuzheng%20Si%20and%20Qingyan%20Sun%20and%20Wei%20Wang%20and%20Yifei%20Wu%20and%20Hao%20Wen%20and%20Gang%20Chen%20and%20Fanchao%20Qi%20and%20Maosong%20Sun%0AAbstract%3A%20Managing%20extensive%20context%20remains%20a%20critical%20bottleneck%20for%20Large%20Language%20Models%20%28LLMs%29%2C%20particularly%20in%20applications%20like%20long-document%20question%20answering%20and%20autonomous%20agents%20where%20lengthy%20inputs%20incur%20high%20computational%20costs%20and%20introduce%20noise.%20Existing%20compression%20techniques%20often%20disrupt%20local%20coherence%20through%20discrete%20token%20removal%20or%20rely%20on%20implicit%20latent%20encoding%20that%20suffers%20from%20positional%20bias%20and%20incompatibility%20with%20closed-source%20APIs.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20EDU-based%20Context%20Compressor%2C%20a%20novel%20explicit%20compression%20framework%20designed%20to%20preserve%20both%20global%20structure%20and%20fine-grained%20details.%20Our%20approach%20reformulates%20context%20compression%20as%20a%20structure-then-select%20process.%20First%2C%20our%20LingoEDU%20transforms%20linear%20text%20into%20a%20structural%20relation%20tree%20of%20Elementary%20Discourse%20Units%20%28EDUs%29%20which%20are%20anchored%20strictly%20to%20source%20indices%20to%20eliminate%20hallucination.%20Second%2C%20a%20lightweight%20ranking%20module%20selects%20query-relevant%20sub-trees%20for%20linearization.%20To%20rigorously%20evaluate%20structural%20understanding%2C%20we%20release%20StructBench%2C%20a%20manually%20annotated%20dataset%20of%20248%20diverse%20documents.%20Empirical%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20structural%20prediction%20accuracy%20and%20significantly%20outperforms%20frontier%20LLMs%20while%20reducing%20costs.%20Furthermore%2C%20our%20structure-aware%20compression%20substantially%20enhances%20performance%20across%20downstream%20tasks%20ranging%20from%20long-context%20tasks%20to%20complex%20Deep%20Search%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Context%2520to%2520EDUs%253A%2520Faithful%2520and%2520Structured%2520Context%2520Compression%2520via%2520Elementary%2520Discourse%2520Unit%2520Decomposition%26entry.906535625%3DYiqing%2520Zhou%2520and%2520Yu%2520Lei%2520and%2520Shuzheng%2520Si%2520and%2520Qingyan%2520Sun%2520and%2520Wei%2520Wang%2520and%2520Yifei%2520Wu%2520and%2520Hao%2520Wen%2520and%2520Gang%2520Chen%2520and%2520Fanchao%2520Qi%2520and%2520Maosong%2520Sun%26entry.1292438233%3DManaging%2520extensive%2520context%2520remains%2520a%2520critical%2520bottleneck%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520particularly%2520in%2520applications%2520like%2520long-document%2520question%2520answering%2520and%2520autonomous%2520agents%2520where%2520lengthy%2520inputs%2520incur%2520high%2520computational%2520costs%2520and%2520introduce%2520noise.%2520Existing%2520compression%2520techniques%2520often%2520disrupt%2520local%2520coherence%2520through%2520discrete%2520token%2520removal%2520or%2520rely%2520on%2520implicit%2520latent%2520encoding%2520that%2520suffers%2520from%2520positional%2520bias%2520and%2520incompatibility%2520with%2520closed-source%2520APIs.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520EDU-based%2520Context%2520Compressor%252C%2520a%2520novel%2520explicit%2520compression%2520framework%2520designed%2520to%2520preserve%2520both%2520global%2520structure%2520and%2520fine-grained%2520details.%2520Our%2520approach%2520reformulates%2520context%2520compression%2520as%2520a%2520structure-then-select%2520process.%2520First%252C%2520our%2520LingoEDU%2520transforms%2520linear%2520text%2520into%2520a%2520structural%2520relation%2520tree%2520of%2520Elementary%2520Discourse%2520Units%2520%2528EDUs%2529%2520which%2520are%2520anchored%2520strictly%2520to%2520source%2520indices%2520to%2520eliminate%2520hallucination.%2520Second%252C%2520a%2520lightweight%2520ranking%2520module%2520selects%2520query-relevant%2520sub-trees%2520for%2520linearization.%2520To%2520rigorously%2520evaluate%2520structural%2520understanding%252C%2520we%2520release%2520StructBench%252C%2520a%2520manually%2520annotated%2520dataset%2520of%2520248%2520diverse%2520documents.%2520Empirical%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520structural%2520prediction%2520accuracy%2520and%2520significantly%2520outperforms%2520frontier%2520LLMs%2520while%2520reducing%2520costs.%2520Furthermore%252C%2520our%2520structure-aware%2520compression%2520substantially%2520enhances%2520performance%2520across%2520downstream%2520tasks%2520ranging%2520from%2520long-context%2520tasks%2520to%2520complex%2520Deep%2520Search%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Context%20to%20EDUs%3A%20Faithful%20and%20Structured%20Context%20Compression%20via%20Elementary%20Discourse%20Unit%20Decomposition&entry.906535625=Yiqing%20Zhou%20and%20Yu%20Lei%20and%20Shuzheng%20Si%20and%20Qingyan%20Sun%20and%20Wei%20Wang%20and%20Yifei%20Wu%20and%20Hao%20Wen%20and%20Gang%20Chen%20and%20Fanchao%20Qi%20and%20Maosong%20Sun&entry.1292438233=Managing%20extensive%20context%20remains%20a%20critical%20bottleneck%20for%20Large%20Language%20Models%20%28LLMs%29%2C%20particularly%20in%20applications%20like%20long-document%20question%20answering%20and%20autonomous%20agents%20where%20lengthy%20inputs%20incur%20high%20computational%20costs%20and%20introduce%20noise.%20Existing%20compression%20techniques%20often%20disrupt%20local%20coherence%20through%20discrete%20token%20removal%20or%20rely%20on%20implicit%20latent%20encoding%20that%20suffers%20from%20positional%20bias%20and%20incompatibility%20with%20closed-source%20APIs.%20To%20address%20these%20limitations%2C%20we%20introduce%20the%20EDU-based%20Context%20Compressor%2C%20a%20novel%20explicit%20compression%20framework%20designed%20to%20preserve%20both%20global%20structure%20and%20fine-grained%20details.%20Our%20approach%20reformulates%20context%20compression%20as%20a%20structure-then-select%20process.%20First%2C%20our%20LingoEDU%20transforms%20linear%20text%20into%20a%20structural%20relation%20tree%20of%20Elementary%20Discourse%20Units%20%28EDUs%29%20which%20are%20anchored%20strictly%20to%20source%20indices%20to%20eliminate%20hallucination.%20Second%2C%20a%20lightweight%20ranking%20module%20selects%20query-relevant%20sub-trees%20for%20linearization.%20To%20rigorously%20evaluate%20structural%20understanding%2C%20we%20release%20StructBench%2C%20a%20manually%20annotated%20dataset%20of%20248%20diverse%20documents.%20Empirical%20results%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20structural%20prediction%20accuracy%20and%20significantly%20outperforms%20frontier%20LLMs%20while%20reducing%20costs.%20Furthermore%2C%20our%20structure-aware%20compression%20substantially%20enhances%20performance%20across%20downstream%20tasks%20ranging%20from%20long-context%20tasks%20to%20complex%20Deep%20Search%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.14244v1&entry.124074799=Read"},
{"title": "UnCageNet: Tracking and Pose Estimation of Caged Animal", "author": "Sayak Dutta and Harish Katti and Shashikant Verma and Shanmuganathan Raman", "abstract": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.", "link": "http://arxiv.org/abs/2512.07712v2", "date": "2025-12-16", "relevancy": 2.1785, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5969}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnCageNet%3A%20Tracking%20and%20Pose%20Estimation%20of%20Caged%20Animal&body=Title%3A%20UnCageNet%3A%20Tracking%20and%20Pose%20Estimation%20of%20Caged%20Animal%0AAuthor%3A%20Sayak%20Dutta%20and%20Harish%20Katti%20and%20Shashikant%20Verma%20and%20Shanmuganathan%20Raman%0AAbstract%3A%20Animal%20tracking%20and%20pose%20estimation%20systems%2C%20such%20as%20STEP%20%28Simultaneous%20Tracking%20and%20Pose%20Estimation%29%20and%20ViTPose%2C%20experience%20substantial%20performance%20drops%20when%20processing%20images%20and%20videos%20with%20cage%20structures%20and%20systematic%20occlusions.%20We%20present%20a%20three-stage%20preprocessing%20pipeline%20that%20addresses%20this%20limitation%20through%3A%20%281%29%20cage%20segmentation%20using%20a%20Gabor-enhanced%20ResNet-UNet%20architecture%20with%20tunable%20orientation%20filters%2C%20%282%29%20cage%20inpainting%20using%20CRFill%20for%20content-aware%20reconstruction%20of%20occluded%20regions%2C%20and%20%283%29%20evaluation%20of%20pose%20estimation%20and%20tracking%20on%20the%20uncaged%20frames.%20Our%20Gabor-enhanced%20segmentation%20model%20leverages%20orientation-aware%20features%20with%2072%20directional%20kernels%20to%20accurately%20identify%20and%20segment%20cage%20structures%20that%20severely%20impair%20the%20performance%20of%20existing%20methods.%20Experimental%20validation%20demonstrates%20that%20removing%20cage%20occlusions%20through%20our%20pipeline%20enables%20pose%20estimation%20and%20tracking%20performance%20comparable%20to%20that%20in%20environments%20without%20occlusions.%20We%20also%20observe%20significant%20improvements%20in%20keypoint%20detection%20accuracy%20and%20trajectory%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnCageNet%253A%2520Tracking%2520and%2520Pose%2520Estimation%2520of%2520Caged%2520Animal%26entry.906535625%3DSayak%2520Dutta%2520and%2520Harish%2520Katti%2520and%2520Shashikant%2520Verma%2520and%2520Shanmuganathan%2520Raman%26entry.1292438233%3DAnimal%2520tracking%2520and%2520pose%2520estimation%2520systems%252C%2520such%2520as%2520STEP%2520%2528Simultaneous%2520Tracking%2520and%2520Pose%2520Estimation%2529%2520and%2520ViTPose%252C%2520experience%2520substantial%2520performance%2520drops%2520when%2520processing%2520images%2520and%2520videos%2520with%2520cage%2520structures%2520and%2520systematic%2520occlusions.%2520We%2520present%2520a%2520three-stage%2520preprocessing%2520pipeline%2520that%2520addresses%2520this%2520limitation%2520through%253A%2520%25281%2529%2520cage%2520segmentation%2520using%2520a%2520Gabor-enhanced%2520ResNet-UNet%2520architecture%2520with%2520tunable%2520orientation%2520filters%252C%2520%25282%2529%2520cage%2520inpainting%2520using%2520CRFill%2520for%2520content-aware%2520reconstruction%2520of%2520occluded%2520regions%252C%2520and%2520%25283%2529%2520evaluation%2520of%2520pose%2520estimation%2520and%2520tracking%2520on%2520the%2520uncaged%2520frames.%2520Our%2520Gabor-enhanced%2520segmentation%2520model%2520leverages%2520orientation-aware%2520features%2520with%252072%2520directional%2520kernels%2520to%2520accurately%2520identify%2520and%2520segment%2520cage%2520structures%2520that%2520severely%2520impair%2520the%2520performance%2520of%2520existing%2520methods.%2520Experimental%2520validation%2520demonstrates%2520that%2520removing%2520cage%2520occlusions%2520through%2520our%2520pipeline%2520enables%2520pose%2520estimation%2520and%2520tracking%2520performance%2520comparable%2520to%2520that%2520in%2520environments%2520without%2520occlusions.%2520We%2520also%2520observe%2520significant%2520improvements%2520in%2520keypoint%2520detection%2520accuracy%2520and%2520trajectory%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnCageNet%3A%20Tracking%20and%20Pose%20Estimation%20of%20Caged%20Animal&entry.906535625=Sayak%20Dutta%20and%20Harish%20Katti%20and%20Shashikant%20Verma%20and%20Shanmuganathan%20Raman&entry.1292438233=Animal%20tracking%20and%20pose%20estimation%20systems%2C%20such%20as%20STEP%20%28Simultaneous%20Tracking%20and%20Pose%20Estimation%29%20and%20ViTPose%2C%20experience%20substantial%20performance%20drops%20when%20processing%20images%20and%20videos%20with%20cage%20structures%20and%20systematic%20occlusions.%20We%20present%20a%20three-stage%20preprocessing%20pipeline%20that%20addresses%20this%20limitation%20through%3A%20%281%29%20cage%20segmentation%20using%20a%20Gabor-enhanced%20ResNet-UNet%20architecture%20with%20tunable%20orientation%20filters%2C%20%282%29%20cage%20inpainting%20using%20CRFill%20for%20content-aware%20reconstruction%20of%20occluded%20regions%2C%20and%20%283%29%20evaluation%20of%20pose%20estimation%20and%20tracking%20on%20the%20uncaged%20frames.%20Our%20Gabor-enhanced%20segmentation%20model%20leverages%20orientation-aware%20features%20with%2072%20directional%20kernels%20to%20accurately%20identify%20and%20segment%20cage%20structures%20that%20severely%20impair%20the%20performance%20of%20existing%20methods.%20Experimental%20validation%20demonstrates%20that%20removing%20cage%20occlusions%20through%20our%20pipeline%20enables%20pose%20estimation%20and%20tracking%20performance%20comparable%20to%20that%20in%20environments%20without%20occlusions.%20We%20also%20observe%20significant%20improvements%20in%20keypoint%20detection%20accuracy%20and%20trajectory%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2512.07712v2&entry.124074799=Read"},
{"title": "Improving Slow Transfer Predictions: Generative Methods Compared", "author": "Jacob Taegon Kim and Alex Sim and Kesheng Wu and Jinoh Kim", "abstract": "Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.", "link": "http://arxiv.org/abs/2512.14522v1", "date": "2025-12-16", "relevancy": 2.1696, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5481}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5468}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Slow%20Transfer%20Predictions%3A%20Generative%20Methods%20Compared&body=Title%3A%20Improving%20Slow%20Transfer%20Predictions%3A%20Generative%20Methods%20Compared%0AAuthor%3A%20Jacob%20Taegon%20Kim%20and%20Alex%20Sim%20and%20Kesheng%20Wu%20and%20Jinoh%20Kim%0AAbstract%3A%20Monitoring%20data%20transfer%20performance%20is%20a%20crucial%20task%20in%20scientific%20computing%20networks.%20By%20predicting%20performance%20early%20in%20the%20communication%20phase%2C%20potentially%20sluggish%20transfers%20can%20be%20identified%20and%20selectively%20monitored%2C%20optimizing%20network%20usage%20and%20overall%20performance.%20A%20key%20bottleneck%20to%20improving%20the%20predictive%20power%20of%20machine%20learning%20%28ML%29%20models%20in%20this%20context%20is%20the%20issue%20of%20class%20imbalance.%20This%20project%20focuses%20on%20addressing%20the%20class%20imbalance%20problem%20to%20enhance%20the%20accuracy%20of%20performance%20predictions.%20In%20this%20study%2C%20we%20analyze%20and%20compare%20various%20augmentation%20strategies%2C%20including%20traditional%20oversampling%20methods%20and%20generative%20techniques.%20Additionally%2C%20we%20adjust%20the%20class%20imbalance%20ratios%20in%20training%20datasets%20to%20evaluate%20their%20impact%20on%20model%20performance.%20While%20augmentation%20may%20improve%20performance%2C%20as%20the%20imbalance%20ratio%20increases%2C%20the%20performance%20does%20not%20significantly%20improve.%20We%20conclude%20that%20even%20the%20most%20advanced%20technique%2C%20such%20as%20CTGAN%2C%20does%20not%20significantly%20improve%20over%20simple%20stratified%20sampling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Slow%2520Transfer%2520Predictions%253A%2520Generative%2520Methods%2520Compared%26entry.906535625%3DJacob%2520Taegon%2520Kim%2520and%2520Alex%2520Sim%2520and%2520Kesheng%2520Wu%2520and%2520Jinoh%2520Kim%26entry.1292438233%3DMonitoring%2520data%2520transfer%2520performance%2520is%2520a%2520crucial%2520task%2520in%2520scientific%2520computing%2520networks.%2520By%2520predicting%2520performance%2520early%2520in%2520the%2520communication%2520phase%252C%2520potentially%2520sluggish%2520transfers%2520can%2520be%2520identified%2520and%2520selectively%2520monitored%252C%2520optimizing%2520network%2520usage%2520and%2520overall%2520performance.%2520A%2520key%2520bottleneck%2520to%2520improving%2520the%2520predictive%2520power%2520of%2520machine%2520learning%2520%2528ML%2529%2520models%2520in%2520this%2520context%2520is%2520the%2520issue%2520of%2520class%2520imbalance.%2520This%2520project%2520focuses%2520on%2520addressing%2520the%2520class%2520imbalance%2520problem%2520to%2520enhance%2520the%2520accuracy%2520of%2520performance%2520predictions.%2520In%2520this%2520study%252C%2520we%2520analyze%2520and%2520compare%2520various%2520augmentation%2520strategies%252C%2520including%2520traditional%2520oversampling%2520methods%2520and%2520generative%2520techniques.%2520Additionally%252C%2520we%2520adjust%2520the%2520class%2520imbalance%2520ratios%2520in%2520training%2520datasets%2520to%2520evaluate%2520their%2520impact%2520on%2520model%2520performance.%2520While%2520augmentation%2520may%2520improve%2520performance%252C%2520as%2520the%2520imbalance%2520ratio%2520increases%252C%2520the%2520performance%2520does%2520not%2520significantly%2520improve.%2520We%2520conclude%2520that%2520even%2520the%2520most%2520advanced%2520technique%252C%2520such%2520as%2520CTGAN%252C%2520does%2520not%2520significantly%2520improve%2520over%2520simple%2520stratified%2520sampling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Slow%20Transfer%20Predictions%3A%20Generative%20Methods%20Compared&entry.906535625=Jacob%20Taegon%20Kim%20and%20Alex%20Sim%20and%20Kesheng%20Wu%20and%20Jinoh%20Kim&entry.1292438233=Monitoring%20data%20transfer%20performance%20is%20a%20crucial%20task%20in%20scientific%20computing%20networks.%20By%20predicting%20performance%20early%20in%20the%20communication%20phase%2C%20potentially%20sluggish%20transfers%20can%20be%20identified%20and%20selectively%20monitored%2C%20optimizing%20network%20usage%20and%20overall%20performance.%20A%20key%20bottleneck%20to%20improving%20the%20predictive%20power%20of%20machine%20learning%20%28ML%29%20models%20in%20this%20context%20is%20the%20issue%20of%20class%20imbalance.%20This%20project%20focuses%20on%20addressing%20the%20class%20imbalance%20problem%20to%20enhance%20the%20accuracy%20of%20performance%20predictions.%20In%20this%20study%2C%20we%20analyze%20and%20compare%20various%20augmentation%20strategies%2C%20including%20traditional%20oversampling%20methods%20and%20generative%20techniques.%20Additionally%2C%20we%20adjust%20the%20class%20imbalance%20ratios%20in%20training%20datasets%20to%20evaluate%20their%20impact%20on%20model%20performance.%20While%20augmentation%20may%20improve%20performance%2C%20as%20the%20imbalance%20ratio%20increases%2C%20the%20performance%20does%20not%20significantly%20improve.%20We%20conclude%20that%20even%20the%20most%20advanced%20technique%2C%20such%20as%20CTGAN%2C%20does%20not%20significantly%20improve%20over%20simple%20stratified%20sampling.&entry.1838667208=http%3A//arxiv.org/abs/2512.14522v1&entry.124074799=Read"},
{"title": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing", "author": "Haoyang He and Jie Wang and Jiangning Zhang and Zhucun Xue and Xingyuan Bu and Qiangpeng Yang and Shilei Wen and Lei Xie", "abstract": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://lewandofskee.github.io/projects/OpenVE.", "link": "http://arxiv.org/abs/2512.07826v2", "date": "2025-12-16", "relevancy": 2.1682, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5705}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenVE-3M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Instruction-Guided%20Video%20Editing&body=Title%3A%20OpenVE-3M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Instruction-Guided%20Video%20Editing%0AAuthor%3A%20Haoyang%20He%20and%20Jie%20Wang%20and%20Jiangning%20Zhang%20and%20Zhucun%20Xue%20and%20Xingyuan%20Bu%20and%20Qiangpeng%20Yang%20and%20Shilei%20Wen%20and%20Lei%20Xie%0AAbstract%3A%20The%20quality%20and%20diversity%20of%20instruction-based%20image%20editing%20datasets%20are%20continuously%20increasing%2C%20yet%20large-scale%2C%20high-quality%20datasets%20for%20instruction-based%20video%20editing%20remain%20scarce.%20To%20address%20this%20gap%2C%20we%20introduce%20OpenVE-3M%2C%20an%20open-source%2C%20large-scale%2C%20and%20high-quality%20dataset%20for%20instruction-based%20video%20editing.%20It%20comprises%20two%20primary%20categories%3A%20spatially-aligned%20edits%20%28Global%20Style%2C%20Background%20Change%2C%20Local%20Change%2C%20Local%20Remove%2C%20Local%20Add%2C%20and%20Subtitles%20Edit%29%20and%20non-spatially-aligned%20edits%20%28Camera%20Multi-Shot%20Edit%20and%20Creative%20Edit%29.%20All%20edit%20types%20are%20generated%20via%20a%20meticulously%20designed%20data%20pipeline%20with%20rigorous%20quality%20filtering.%20OpenVE-3M%20surpasses%20existing%20open-source%20datasets%20in%20terms%20of%20scale%2C%20diversity%20of%20edit%20types%2C%20instruction%20length%2C%20and%20overall%20quality.%20Furthermore%2C%20to%20address%20the%20lack%20of%20a%20unified%20benchmark%20in%20the%20field%2C%20we%20construct%20OpenVE-Bench%2C%20containing%20431%20video-edit%20pairs%20that%20cover%20a%20diverse%20range%20of%20editing%20tasks%20with%20three%20key%20metrics%20highly%20aligned%20with%20human%20judgment.%20We%20present%20OpenVE-Edit%2C%20a%205B%20model%20trained%20on%20our%20dataset%20that%20demonstrates%20remarkable%20efficiency%20and%20effectiveness%20by%20setting%20a%20new%20state-of-the-art%20on%20OpenVE-Bench%2C%20outperforming%20all%20prior%20open-source%20models%20including%20a%2014B%20baseline.%20Project%20page%20is%20at%20https%3A//lewandofskee.github.io/projects/OpenVE.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07826v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenVE-3M%253A%2520A%2520Large-Scale%2520High-Quality%2520Dataset%2520for%2520Instruction-Guided%2520Video%2520Editing%26entry.906535625%3DHaoyang%2520He%2520and%2520Jie%2520Wang%2520and%2520Jiangning%2520Zhang%2520and%2520Zhucun%2520Xue%2520and%2520Xingyuan%2520Bu%2520and%2520Qiangpeng%2520Yang%2520and%2520Shilei%2520Wen%2520and%2520Lei%2520Xie%26entry.1292438233%3DThe%2520quality%2520and%2520diversity%2520of%2520instruction-based%2520image%2520editing%2520datasets%2520are%2520continuously%2520increasing%252C%2520yet%2520large-scale%252C%2520high-quality%2520datasets%2520for%2520instruction-based%2520video%2520editing%2520remain%2520scarce.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520OpenVE-3M%252C%2520an%2520open-source%252C%2520large-scale%252C%2520and%2520high-quality%2520dataset%2520for%2520instruction-based%2520video%2520editing.%2520It%2520comprises%2520two%2520primary%2520categories%253A%2520spatially-aligned%2520edits%2520%2528Global%2520Style%252C%2520Background%2520Change%252C%2520Local%2520Change%252C%2520Local%2520Remove%252C%2520Local%2520Add%252C%2520and%2520Subtitles%2520Edit%2529%2520and%2520non-spatially-aligned%2520edits%2520%2528Camera%2520Multi-Shot%2520Edit%2520and%2520Creative%2520Edit%2529.%2520All%2520edit%2520types%2520are%2520generated%2520via%2520a%2520meticulously%2520designed%2520data%2520pipeline%2520with%2520rigorous%2520quality%2520filtering.%2520OpenVE-3M%2520surpasses%2520existing%2520open-source%2520datasets%2520in%2520terms%2520of%2520scale%252C%2520diversity%2520of%2520edit%2520types%252C%2520instruction%2520length%252C%2520and%2520overall%2520quality.%2520Furthermore%252C%2520to%2520address%2520the%2520lack%2520of%2520a%2520unified%2520benchmark%2520in%2520the%2520field%252C%2520we%2520construct%2520OpenVE-Bench%252C%2520containing%2520431%2520video-edit%2520pairs%2520that%2520cover%2520a%2520diverse%2520range%2520of%2520editing%2520tasks%2520with%2520three%2520key%2520metrics%2520highly%2520aligned%2520with%2520human%2520judgment.%2520We%2520present%2520OpenVE-Edit%252C%2520a%25205B%2520model%2520trained%2520on%2520our%2520dataset%2520that%2520demonstrates%2520remarkable%2520efficiency%2520and%2520effectiveness%2520by%2520setting%2520a%2520new%2520state-of-the-art%2520on%2520OpenVE-Bench%252C%2520outperforming%2520all%2520prior%2520open-source%2520models%2520including%2520a%252014B%2520baseline.%2520Project%2520page%2520is%2520at%2520https%253A//lewandofskee.github.io/projects/OpenVE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07826v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenVE-3M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Instruction-Guided%20Video%20Editing&entry.906535625=Haoyang%20He%20and%20Jie%20Wang%20and%20Jiangning%20Zhang%20and%20Zhucun%20Xue%20and%20Xingyuan%20Bu%20and%20Qiangpeng%20Yang%20and%20Shilei%20Wen%20and%20Lei%20Xie&entry.1292438233=The%20quality%20and%20diversity%20of%20instruction-based%20image%20editing%20datasets%20are%20continuously%20increasing%2C%20yet%20large-scale%2C%20high-quality%20datasets%20for%20instruction-based%20video%20editing%20remain%20scarce.%20To%20address%20this%20gap%2C%20we%20introduce%20OpenVE-3M%2C%20an%20open-source%2C%20large-scale%2C%20and%20high-quality%20dataset%20for%20instruction-based%20video%20editing.%20It%20comprises%20two%20primary%20categories%3A%20spatially-aligned%20edits%20%28Global%20Style%2C%20Background%20Change%2C%20Local%20Change%2C%20Local%20Remove%2C%20Local%20Add%2C%20and%20Subtitles%20Edit%29%20and%20non-spatially-aligned%20edits%20%28Camera%20Multi-Shot%20Edit%20and%20Creative%20Edit%29.%20All%20edit%20types%20are%20generated%20via%20a%20meticulously%20designed%20data%20pipeline%20with%20rigorous%20quality%20filtering.%20OpenVE-3M%20surpasses%20existing%20open-source%20datasets%20in%20terms%20of%20scale%2C%20diversity%20of%20edit%20types%2C%20instruction%20length%2C%20and%20overall%20quality.%20Furthermore%2C%20to%20address%20the%20lack%20of%20a%20unified%20benchmark%20in%20the%20field%2C%20we%20construct%20OpenVE-Bench%2C%20containing%20431%20video-edit%20pairs%20that%20cover%20a%20diverse%20range%20of%20editing%20tasks%20with%20three%20key%20metrics%20highly%20aligned%20with%20human%20judgment.%20We%20present%20OpenVE-Edit%2C%20a%205B%20model%20trained%20on%20our%20dataset%20that%20demonstrates%20remarkable%20efficiency%20and%20effectiveness%20by%20setting%20a%20new%20state-of-the-art%20on%20OpenVE-Bench%2C%20outperforming%20all%20prior%20open-source%20models%20including%20a%2014B%20baseline.%20Project%20page%20is%20at%20https%3A//lewandofskee.github.io/projects/OpenVE.&entry.1838667208=http%3A//arxiv.org/abs/2512.07826v2&entry.124074799=Read"},
{"title": "G\u00f6del's Poetry", "author": "Kelly J. Davis", "abstract": "Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.", "link": "http://arxiv.org/abs/2512.14252v1", "date": "2025-12-16", "relevancy": 2.1664, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4423}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4374}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G%C3%B6del%27s%20Poetry&body=Title%3A%20G%C3%B6del%27s%20Poetry%0AAuthor%3A%20Kelly%20J.%20Davis%0AAbstract%3A%20Formal%2C%20automated%20theorem%20proving%20has%20long%20been%20viewed%20as%20a%20challenge%20to%20artificial%20intelligence.%20We%20introduce%20here%20a%20new%20approach%20to%20computer%20theorem%20proving%2C%20one%20that%20employs%20specialized%20language%20models%20for%20Lean4%20proof%20generation%20combined%20with%20recursive%20decomposition%20of%20difficult%20theorems%20into%20simpler%20entailing%20propositions.%20These%20models%20are%20coordinated%20through%20a%20multi-agent%20architecture%20that%20orchestrates%20autoformalization%20%28if%20required%29%2C%20proof%20generation%2C%20decomposition%20of%20difficult%20theorems%20into%20simpler%20entailing%20propositions%2C%20and%20recursive%20proof%20%28and/or%20decomposition%29%20of%20these%20propositions.%20Without%20decomposition%2C%20we%20achieve%20a%2090.4%25%20pass%20rate%20on%20miniF2F.%20With%20decomposition%2C%20this%20is%20significantly%20improved.%20A%20key%20technical%20contribution%20lies%20in%20our%20extension%20of%20the%20Kimina%20Lean%20Server%20with%20abstract%20syntax%20tree%20%28AST%29%20parsing%20capabilities%20to%20facilitate%20automated%2C%20recursive%20proof%20decomposition.%20The%20system%20is%20made%20available%20on%20PyPI%20as%20goedels-poetry%20%28at%20https%3A//pypi.org/project/goedels-poetry%20%29%2C%20and%20the%20open-source%20implementation%20KellyJDavis/goedels-poetry%20%28at%20https%3A//github.com/KellyJDavis/goedels-poetry%20%29%20facilitates%20both%20adaptation%20to%20alternative%20language%20models%20and%20extension%20with%20custom%20functionality.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG%25C3%25B6del%2527s%2520Poetry%26entry.906535625%3DKelly%2520J.%2520Davis%26entry.1292438233%3DFormal%252C%2520automated%2520theorem%2520proving%2520has%2520long%2520been%2520viewed%2520as%2520a%2520challenge%2520to%2520artificial%2520intelligence.%2520We%2520introduce%2520here%2520a%2520new%2520approach%2520to%2520computer%2520theorem%2520proving%252C%2520one%2520that%2520employs%2520specialized%2520language%2520models%2520for%2520Lean4%2520proof%2520generation%2520combined%2520with%2520recursive%2520decomposition%2520of%2520difficult%2520theorems%2520into%2520simpler%2520entailing%2520propositions.%2520These%2520models%2520are%2520coordinated%2520through%2520a%2520multi-agent%2520architecture%2520that%2520orchestrates%2520autoformalization%2520%2528if%2520required%2529%252C%2520proof%2520generation%252C%2520decomposition%2520of%2520difficult%2520theorems%2520into%2520simpler%2520entailing%2520propositions%252C%2520and%2520recursive%2520proof%2520%2528and/or%2520decomposition%2529%2520of%2520these%2520propositions.%2520Without%2520decomposition%252C%2520we%2520achieve%2520a%252090.4%2525%2520pass%2520rate%2520on%2520miniF2F.%2520With%2520decomposition%252C%2520this%2520is%2520significantly%2520improved.%2520A%2520key%2520technical%2520contribution%2520lies%2520in%2520our%2520extension%2520of%2520the%2520Kimina%2520Lean%2520Server%2520with%2520abstract%2520syntax%2520tree%2520%2528AST%2529%2520parsing%2520capabilities%2520to%2520facilitate%2520automated%252C%2520recursive%2520proof%2520decomposition.%2520The%2520system%2520is%2520made%2520available%2520on%2520PyPI%2520as%2520goedels-poetry%2520%2528at%2520https%253A//pypi.org/project/goedels-poetry%2520%2529%252C%2520and%2520the%2520open-source%2520implementation%2520KellyJDavis/goedels-poetry%2520%2528at%2520https%253A//github.com/KellyJDavis/goedels-poetry%2520%2529%2520facilitates%2520both%2520adaptation%2520to%2520alternative%2520language%2520models%2520and%2520extension%2520with%2520custom%2520functionality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G%C3%B6del%27s%20Poetry&entry.906535625=Kelly%20J.%20Davis&entry.1292438233=Formal%2C%20automated%20theorem%20proving%20has%20long%20been%20viewed%20as%20a%20challenge%20to%20artificial%20intelligence.%20We%20introduce%20here%20a%20new%20approach%20to%20computer%20theorem%20proving%2C%20one%20that%20employs%20specialized%20language%20models%20for%20Lean4%20proof%20generation%20combined%20with%20recursive%20decomposition%20of%20difficult%20theorems%20into%20simpler%20entailing%20propositions.%20These%20models%20are%20coordinated%20through%20a%20multi-agent%20architecture%20that%20orchestrates%20autoformalization%20%28if%20required%29%2C%20proof%20generation%2C%20decomposition%20of%20difficult%20theorems%20into%20simpler%20entailing%20propositions%2C%20and%20recursive%20proof%20%28and/or%20decomposition%29%20of%20these%20propositions.%20Without%20decomposition%2C%20we%20achieve%20a%2090.4%25%20pass%20rate%20on%20miniF2F.%20With%20decomposition%2C%20this%20is%20significantly%20improved.%20A%20key%20technical%20contribution%20lies%20in%20our%20extension%20of%20the%20Kimina%20Lean%20Server%20with%20abstract%20syntax%20tree%20%28AST%29%20parsing%20capabilities%20to%20facilitate%20automated%2C%20recursive%20proof%20decomposition.%20The%20system%20is%20made%20available%20on%20PyPI%20as%20goedels-poetry%20%28at%20https%3A//pypi.org/project/goedels-poetry%20%29%2C%20and%20the%20open-source%20implementation%20KellyJDavis/goedels-poetry%20%28at%20https%3A//github.com/KellyJDavis/goedels-poetry%20%29%20facilitates%20both%20adaptation%20to%20alternative%20language%20models%20and%20extension%20with%20custom%20functionality.&entry.1838667208=http%3A//arxiv.org/abs/2512.14252v1&entry.124074799=Read"},
{"title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration", "author": "Zhiwen Yang and Jiaju Zhang and Yang Yi and Jian Liang and Bingzheng Wei and Yan Xu", "abstract": "Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.", "link": "http://arxiv.org/abs/2512.14550v1", "date": "2025-12-16", "relevancy": 2.1651, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5558}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5484}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAT%3A%20Task-Adaptive%20Transformer%20for%20All-in-One%20Medical%20Image%20Restoration&body=Title%3A%20TAT%3A%20Task-Adaptive%20Transformer%20for%20All-in-One%20Medical%20Image%20Restoration%0AAuthor%3A%20Zhiwen%20Yang%20and%20Jiaju%20Zhang%20and%20Yang%20Yi%20and%20Jian%20Liang%20and%20Bingzheng%20Wei%20and%20Yan%20Xu%0AAbstract%3A%20Medical%20image%20restoration%20%28MedIR%29%20aims%20to%20recover%20high-quality%20medical%20images%20from%20their%20low-quality%20counterparts.%20Recent%20advancements%20in%20MedIR%20have%20focused%20on%20All-in-One%20models%20capable%20of%20simultaneously%20addressing%20multiple%20different%20MedIR%20tasks.%20However%2C%20due%20to%20significant%20differences%20in%20both%20modality%20and%20degradation%20types%2C%20using%20a%20shared%20model%20for%20these%20diverse%20tasks%20requires%20careful%20consideration%20of%20two%20critical%20inter-task%20relationships%3A%20task%20interference%2C%20which%20occurs%20when%20conflicting%20gradient%20update%20directions%20arise%20across%20tasks%20on%20the%20same%20parameter%2C%20and%20task%20imbalance%2C%20which%20refers%20to%20uneven%20optimization%20caused%20by%20varying%20learning%20difficulties%20inherent%20to%20each%20task.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20task-adaptive%20Transformer%20%28TAT%29%2C%20a%20novel%20framework%20that%20dynamically%20adapts%20to%20different%20tasks%20through%20two%20key%20innovations.%20First%2C%20a%20task-adaptive%20weight%20generation%20strategy%20is%20introduced%20to%20mitigate%20task%20interference%20by%20generating%20task-specific%20weight%20parameters%20for%20each%20task%2C%20thereby%20eliminating%20potential%20gradient%20conflicts%20on%20shared%20weight%20parameters.%20Second%2C%20a%20task-adaptive%20loss%20balancing%20strategy%20is%20introduced%20to%20dynamically%20adjust%20loss%20weights%20based%20on%20task-specific%20learning%20difficulties%2C%20preventing%20task%20domination%20or%20undertraining.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20TAT%20achieves%20state-of-the-art%20performance%20in%20three%20MedIR%20tasks--PET%20synthesis%2C%20CT%20denoising%2C%20and%20MRI%20super-resolution--both%20in%20task-specific%20and%20All-in-One%20settings.%20Code%20is%20available%20at%20https%3A//github.com/Yaziwel/TAT.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAT%253A%2520Task-Adaptive%2520Transformer%2520for%2520All-in-One%2520Medical%2520Image%2520Restoration%26entry.906535625%3DZhiwen%2520Yang%2520and%2520Jiaju%2520Zhang%2520and%2520Yang%2520Yi%2520and%2520Jian%2520Liang%2520and%2520Bingzheng%2520Wei%2520and%2520Yan%2520Xu%26entry.1292438233%3DMedical%2520image%2520restoration%2520%2528MedIR%2529%2520aims%2520to%2520recover%2520high-quality%2520medical%2520images%2520from%2520their%2520low-quality%2520counterparts.%2520Recent%2520advancements%2520in%2520MedIR%2520have%2520focused%2520on%2520All-in-One%2520models%2520capable%2520of%2520simultaneously%2520addressing%2520multiple%2520different%2520MedIR%2520tasks.%2520However%252C%2520due%2520to%2520significant%2520differences%2520in%2520both%2520modality%2520and%2520degradation%2520types%252C%2520using%2520a%2520shared%2520model%2520for%2520these%2520diverse%2520tasks%2520requires%2520careful%2520consideration%2520of%2520two%2520critical%2520inter-task%2520relationships%253A%2520task%2520interference%252C%2520which%2520occurs%2520when%2520conflicting%2520gradient%2520update%2520directions%2520arise%2520across%2520tasks%2520on%2520the%2520same%2520parameter%252C%2520and%2520task%2520imbalance%252C%2520which%2520refers%2520to%2520uneven%2520optimization%2520caused%2520by%2520varying%2520learning%2520difficulties%2520inherent%2520to%2520each%2520task.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520task-adaptive%2520Transformer%2520%2528TAT%2529%252C%2520a%2520novel%2520framework%2520that%2520dynamically%2520adapts%2520to%2520different%2520tasks%2520through%2520two%2520key%2520innovations.%2520First%252C%2520a%2520task-adaptive%2520weight%2520generation%2520strategy%2520is%2520introduced%2520to%2520mitigate%2520task%2520interference%2520by%2520generating%2520task-specific%2520weight%2520parameters%2520for%2520each%2520task%252C%2520thereby%2520eliminating%2520potential%2520gradient%2520conflicts%2520on%2520shared%2520weight%2520parameters.%2520Second%252C%2520a%2520task-adaptive%2520loss%2520balancing%2520strategy%2520is%2520introduced%2520to%2520dynamically%2520adjust%2520loss%2520weights%2520based%2520on%2520task-specific%2520learning%2520difficulties%252C%2520preventing%2520task%2520domination%2520or%2520undertraining.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520TAT%2520achieves%2520state-of-the-art%2520performance%2520in%2520three%2520MedIR%2520tasks--PET%2520synthesis%252C%2520CT%2520denoising%252C%2520and%2520MRI%2520super-resolution--both%2520in%2520task-specific%2520and%2520All-in-One%2520settings.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Yaziwel/TAT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAT%3A%20Task-Adaptive%20Transformer%20for%20All-in-One%20Medical%20Image%20Restoration&entry.906535625=Zhiwen%20Yang%20and%20Jiaju%20Zhang%20and%20Yang%20Yi%20and%20Jian%20Liang%20and%20Bingzheng%20Wei%20and%20Yan%20Xu&entry.1292438233=Medical%20image%20restoration%20%28MedIR%29%20aims%20to%20recover%20high-quality%20medical%20images%20from%20their%20low-quality%20counterparts.%20Recent%20advancements%20in%20MedIR%20have%20focused%20on%20All-in-One%20models%20capable%20of%20simultaneously%20addressing%20multiple%20different%20MedIR%20tasks.%20However%2C%20due%20to%20significant%20differences%20in%20both%20modality%20and%20degradation%20types%2C%20using%20a%20shared%20model%20for%20these%20diverse%20tasks%20requires%20careful%20consideration%20of%20two%20critical%20inter-task%20relationships%3A%20task%20interference%2C%20which%20occurs%20when%20conflicting%20gradient%20update%20directions%20arise%20across%20tasks%20on%20the%20same%20parameter%2C%20and%20task%20imbalance%2C%20which%20refers%20to%20uneven%20optimization%20caused%20by%20varying%20learning%20difficulties%20inherent%20to%20each%20task.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20task-adaptive%20Transformer%20%28TAT%29%2C%20a%20novel%20framework%20that%20dynamically%20adapts%20to%20different%20tasks%20through%20two%20key%20innovations.%20First%2C%20a%20task-adaptive%20weight%20generation%20strategy%20is%20introduced%20to%20mitigate%20task%20interference%20by%20generating%20task-specific%20weight%20parameters%20for%20each%20task%2C%20thereby%20eliminating%20potential%20gradient%20conflicts%20on%20shared%20weight%20parameters.%20Second%2C%20a%20task-adaptive%20loss%20balancing%20strategy%20is%20introduced%20to%20dynamically%20adjust%20loss%20weights%20based%20on%20task-specific%20learning%20difficulties%2C%20preventing%20task%20domination%20or%20undertraining.%20Extensive%20experiments%20demonstrate%20that%20our%20proposed%20TAT%20achieves%20state-of-the-art%20performance%20in%20three%20MedIR%20tasks--PET%20synthesis%2C%20CT%20denoising%2C%20and%20MRI%20super-resolution--both%20in%20task-specific%20and%20All-in-One%20settings.%20Code%20is%20available%20at%20https%3A//github.com/Yaziwel/TAT.&entry.1838667208=http%3A//arxiv.org/abs/2512.14550v1&entry.124074799=Read"},
{"title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids", "author": "Mohammed Ayman Habib and Aldo Petruzzelli", "abstract": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.", "link": "http://arxiv.org/abs/2512.14411v1", "date": "2025-12-16", "relevancy": 2.1648, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5684}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Data%20Pipelines%20for%20Adaptive%2C%20Mission-Ready%20Militarized%20Humanoids&body=Title%3A%20Synthetic%20Data%20Pipelines%20for%20Adaptive%2C%20Mission-Ready%20Militarized%20Humanoids%0AAuthor%3A%20Mohammed%20Ayman%20Habib%20and%20Aldo%20Petruzzelli%0AAbstract%3A%20Omnia%20presents%20a%20synthetic%20data%20driven%20pipeline%20to%20accelerate%20the%20training%2C%20validation%2C%20and%20deployment%20readiness%20of%20militarized%20humanoids.%20The%20approach%20converts%20first-person%20spatial%20observations%20captured%20from%20point-of-view%20recordings%2C%20smart%20glasses%2C%20augmented%20reality%20headsets%2C%20and%20spatial%20browsing%20workflows%20into%20scalable%2C%20mission-specific%20synthetic%20datasets%20for%20humanoid%20autonomy.%20By%20generating%20large%20volumes%20of%20high-fidelity%20simulated%20scenarios%20and%20pairing%20them%20with%20automated%20labeling%20and%20model%20training%2C%20the%20pipeline%20enables%20rapid%20iteration%20on%20perception%2C%20navigation%2C%20and%20decision-making%20capabilities%20without%20the%20cost%2C%20risk%2C%20or%20time%20constraints%20of%20extensive%20field%20trials.%20The%20resulting%20datasets%20can%20be%20tuned%20quickly%20for%20new%20operational%20environments%20and%20threat%20conditions%2C%20supporting%20both%20baseline%20humanoid%20performance%20and%20advanced%20subsystems%20such%20as%20multimodal%20sensing%2C%20counter-detection%20survivability%2C%20and%20CBRNE-relevant%20reconnaissance%20behaviors.%20This%20work%20targets%20faster%20development%20cycles%20and%20improved%20robustness%20in%20complex%2C%20contested%20settings%20by%20exposing%20humanoid%20systems%20to%20broad%20scenario%20diversity%20early%20in%20the%20development%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Data%2520Pipelines%2520for%2520Adaptive%252C%2520Mission-Ready%2520Militarized%2520Humanoids%26entry.906535625%3DMohammed%2520Ayman%2520Habib%2520and%2520Aldo%2520Petruzzelli%26entry.1292438233%3DOmnia%2520presents%2520a%2520synthetic%2520data%2520driven%2520pipeline%2520to%2520accelerate%2520the%2520training%252C%2520validation%252C%2520and%2520deployment%2520readiness%2520of%2520militarized%2520humanoids.%2520The%2520approach%2520converts%2520first-person%2520spatial%2520observations%2520captured%2520from%2520point-of-view%2520recordings%252C%2520smart%2520glasses%252C%2520augmented%2520reality%2520headsets%252C%2520and%2520spatial%2520browsing%2520workflows%2520into%2520scalable%252C%2520mission-specific%2520synthetic%2520datasets%2520for%2520humanoid%2520autonomy.%2520By%2520generating%2520large%2520volumes%2520of%2520high-fidelity%2520simulated%2520scenarios%2520and%2520pairing%2520them%2520with%2520automated%2520labeling%2520and%2520model%2520training%252C%2520the%2520pipeline%2520enables%2520rapid%2520iteration%2520on%2520perception%252C%2520navigation%252C%2520and%2520decision-making%2520capabilities%2520without%2520the%2520cost%252C%2520risk%252C%2520or%2520time%2520constraints%2520of%2520extensive%2520field%2520trials.%2520The%2520resulting%2520datasets%2520can%2520be%2520tuned%2520quickly%2520for%2520new%2520operational%2520environments%2520and%2520threat%2520conditions%252C%2520supporting%2520both%2520baseline%2520humanoid%2520performance%2520and%2520advanced%2520subsystems%2520such%2520as%2520multimodal%2520sensing%252C%2520counter-detection%2520survivability%252C%2520and%2520CBRNE-relevant%2520reconnaissance%2520behaviors.%2520This%2520work%2520targets%2520faster%2520development%2520cycles%2520and%2520improved%2520robustness%2520in%2520complex%252C%2520contested%2520settings%2520by%2520exposing%2520humanoid%2520systems%2520to%2520broad%2520scenario%2520diversity%2520early%2520in%2520the%2520development%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Data%20Pipelines%20for%20Adaptive%2C%20Mission-Ready%20Militarized%20Humanoids&entry.906535625=Mohammed%20Ayman%20Habib%20and%20Aldo%20Petruzzelli&entry.1292438233=Omnia%20presents%20a%20synthetic%20data%20driven%20pipeline%20to%20accelerate%20the%20training%2C%20validation%2C%20and%20deployment%20readiness%20of%20militarized%20humanoids.%20The%20approach%20converts%20first-person%20spatial%20observations%20captured%20from%20point-of-view%20recordings%2C%20smart%20glasses%2C%20augmented%20reality%20headsets%2C%20and%20spatial%20browsing%20workflows%20into%20scalable%2C%20mission-specific%20synthetic%20datasets%20for%20humanoid%20autonomy.%20By%20generating%20large%20volumes%20of%20high-fidelity%20simulated%20scenarios%20and%20pairing%20them%20with%20automated%20labeling%20and%20model%20training%2C%20the%20pipeline%20enables%20rapid%20iteration%20on%20perception%2C%20navigation%2C%20and%20decision-making%20capabilities%20without%20the%20cost%2C%20risk%2C%20or%20time%20constraints%20of%20extensive%20field%20trials.%20The%20resulting%20datasets%20can%20be%20tuned%20quickly%20for%20new%20operational%20environments%20and%20threat%20conditions%2C%20supporting%20both%20baseline%20humanoid%20performance%20and%20advanced%20subsystems%20such%20as%20multimodal%20sensing%2C%20counter-detection%20survivability%2C%20and%20CBRNE-relevant%20reconnaissance%20behaviors.%20This%20work%20targets%20faster%20development%20cycles%20and%20improved%20robustness%20in%20complex%2C%20contested%20settings%20by%20exposing%20humanoid%20systems%20to%20broad%20scenario%20diversity%20early%20in%20the%20development%20process.&entry.1838667208=http%3A//arxiv.org/abs/2512.14411v1&entry.124074799=Read"},
{"title": "Test Time Optimized Generalized AI-based Medical Image Registration Method", "author": "Sneha Sree C. and Dattesh Shanbhag and Sudhanya Chatterjee", "abstract": "Medical image registration is critical for aligning anatomical structures across imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound. Among existing techniques, non-rigid registration (NRR) is particularly challenging due to the need to capture complex anatomical deformations caused by physiological processes like respiration or contrast-induced signal variations. Traditional NRR methods, while theoretically robust, often require extensive parameter tuning and incur high computational costs, limiting their use in real-time clinical workflows. Recent deep learning (DL)-based approaches have shown promise; however, their dependence on task-specific retraining restricts scalability and adaptability in practice. These limitations underscore the need for efficient, generalizable registration frameworks capable of handling heterogeneous imaging contexts. In this work, we introduce a novel AI-driven framework for 3D non-rigid registration that generalizes across multiple imaging modalities and anatomical regions. Unlike conventional methods that rely on application-specific models, our approach eliminates anatomy- or modality-specific customization, enabling streamlined integration into diverse clinical environments.", "link": "http://arxiv.org/abs/2512.14556v1", "date": "2025-12-16", "relevancy": 2.1553, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5581}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5524}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test%20Time%20Optimized%20Generalized%20AI-based%20Medical%20Image%20Registration%20Method&body=Title%3A%20Test%20Time%20Optimized%20Generalized%20AI-based%20Medical%20Image%20Registration%20Method%0AAuthor%3A%20Sneha%20Sree%20C.%20and%20Dattesh%20Shanbhag%20and%20Sudhanya%20Chatterjee%0AAbstract%3A%20Medical%20image%20registration%20is%20critical%20for%20aligning%20anatomical%20structures%20across%20imaging%20modalities%20such%20as%20computed%20tomography%20%28CT%29%2C%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20ultrasound.%20Among%20existing%20techniques%2C%20non-rigid%20registration%20%28NRR%29%20is%20particularly%20challenging%20due%20to%20the%20need%20to%20capture%20complex%20anatomical%20deformations%20caused%20by%20physiological%20processes%20like%20respiration%20or%20contrast-induced%20signal%20variations.%20Traditional%20NRR%20methods%2C%20while%20theoretically%20robust%2C%20often%20require%20extensive%20parameter%20tuning%20and%20incur%20high%20computational%20costs%2C%20limiting%20their%20use%20in%20real-time%20clinical%20workflows.%20Recent%20deep%20learning%20%28DL%29-based%20approaches%20have%20shown%20promise%3B%20however%2C%20their%20dependence%20on%20task-specific%20retraining%20restricts%20scalability%20and%20adaptability%20in%20practice.%20These%20limitations%20underscore%20the%20need%20for%20efficient%2C%20generalizable%20registration%20frameworks%20capable%20of%20handling%20heterogeneous%20imaging%20contexts.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20AI-driven%20framework%20for%203D%20non-rigid%20registration%20that%20generalizes%20across%20multiple%20imaging%20modalities%20and%20anatomical%20regions.%20Unlike%20conventional%20methods%20that%20rely%20on%20application-specific%20models%2C%20our%20approach%20eliminates%20anatomy-%20or%20modality-specific%20customization%2C%20enabling%20streamlined%20integration%20into%20diverse%20clinical%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest%2520Time%2520Optimized%2520Generalized%2520AI-based%2520Medical%2520Image%2520Registration%2520Method%26entry.906535625%3DSneha%2520Sree%2520C.%2520and%2520Dattesh%2520Shanbhag%2520and%2520Sudhanya%2520Chatterjee%26entry.1292438233%3DMedical%2520image%2520registration%2520is%2520critical%2520for%2520aligning%2520anatomical%2520structures%2520across%2520imaging%2520modalities%2520such%2520as%2520computed%2520tomography%2520%2528CT%2529%252C%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%252C%2520and%2520ultrasound.%2520Among%2520existing%2520techniques%252C%2520non-rigid%2520registration%2520%2528NRR%2529%2520is%2520particularly%2520challenging%2520due%2520to%2520the%2520need%2520to%2520capture%2520complex%2520anatomical%2520deformations%2520caused%2520by%2520physiological%2520processes%2520like%2520respiration%2520or%2520contrast-induced%2520signal%2520variations.%2520Traditional%2520NRR%2520methods%252C%2520while%2520theoretically%2520robust%252C%2520often%2520require%2520extensive%2520parameter%2520tuning%2520and%2520incur%2520high%2520computational%2520costs%252C%2520limiting%2520their%2520use%2520in%2520real-time%2520clinical%2520workflows.%2520Recent%2520deep%2520learning%2520%2528DL%2529-based%2520approaches%2520have%2520shown%2520promise%253B%2520however%252C%2520their%2520dependence%2520on%2520task-specific%2520retraining%2520restricts%2520scalability%2520and%2520adaptability%2520in%2520practice.%2520These%2520limitations%2520underscore%2520the%2520need%2520for%2520efficient%252C%2520generalizable%2520registration%2520frameworks%2520capable%2520of%2520handling%2520heterogeneous%2520imaging%2520contexts.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520AI-driven%2520framework%2520for%25203D%2520non-rigid%2520registration%2520that%2520generalizes%2520across%2520multiple%2520imaging%2520modalities%2520and%2520anatomical%2520regions.%2520Unlike%2520conventional%2520methods%2520that%2520rely%2520on%2520application-specific%2520models%252C%2520our%2520approach%2520eliminates%2520anatomy-%2520or%2520modality-specific%2520customization%252C%2520enabling%2520streamlined%2520integration%2520into%2520diverse%2520clinical%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test%20Time%20Optimized%20Generalized%20AI-based%20Medical%20Image%20Registration%20Method&entry.906535625=Sneha%20Sree%20C.%20and%20Dattesh%20Shanbhag%20and%20Sudhanya%20Chatterjee&entry.1292438233=Medical%20image%20registration%20is%20critical%20for%20aligning%20anatomical%20structures%20across%20imaging%20modalities%20such%20as%20computed%20tomography%20%28CT%29%2C%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20ultrasound.%20Among%20existing%20techniques%2C%20non-rigid%20registration%20%28NRR%29%20is%20particularly%20challenging%20due%20to%20the%20need%20to%20capture%20complex%20anatomical%20deformations%20caused%20by%20physiological%20processes%20like%20respiration%20or%20contrast-induced%20signal%20variations.%20Traditional%20NRR%20methods%2C%20while%20theoretically%20robust%2C%20often%20require%20extensive%20parameter%20tuning%20and%20incur%20high%20computational%20costs%2C%20limiting%20their%20use%20in%20real-time%20clinical%20workflows.%20Recent%20deep%20learning%20%28DL%29-based%20approaches%20have%20shown%20promise%3B%20however%2C%20their%20dependence%20on%20task-specific%20retraining%20restricts%20scalability%20and%20adaptability%20in%20practice.%20These%20limitations%20underscore%20the%20need%20for%20efficient%2C%20generalizable%20registration%20frameworks%20capable%20of%20handling%20heterogeneous%20imaging%20contexts.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20AI-driven%20framework%20for%203D%20non-rigid%20registration%20that%20generalizes%20across%20multiple%20imaging%20modalities%20and%20anatomical%20regions.%20Unlike%20conventional%20methods%20that%20rely%20on%20application-specific%20models%2C%20our%20approach%20eliminates%20anatomy-%20or%20modality-specific%20customization%2C%20enabling%20streamlined%20integration%20into%20diverse%20clinical%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.14556v1&entry.124074799=Read"},
{"title": "Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies", "author": "Ekaterina Artemova and Laurie Burchell and Daryna Dementieva and Shu Okabe and Mariya Shmatova and Pedro Ortiz Suarez", "abstract": "This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.", "link": "http://arxiv.org/abs/2512.14576v1", "date": "2025-12-16", "relevancy": 2.1536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4316}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Resource%2C%20High-Impact%3A%20Building%20Corpora%20for%20Inclusive%20Language%20Technologies&body=Title%3A%20Low-Resource%2C%20High-Impact%3A%20Building%20Corpora%20for%20Inclusive%20Language%20Technologies%0AAuthor%3A%20Ekaterina%20Artemova%20and%20Laurie%20Burchell%20and%20Daryna%20Dementieva%20and%20Shu%20Okabe%20and%20Mariya%20Shmatova%20and%20Pedro%20Ortiz%20Suarez%0AAbstract%3A%20This%20tutorial%20%28https%3A//tum-nlp.github.io/low-resource-tutorial%29%20is%20designed%20for%20NLP%20practitioners%2C%20researchers%2C%20and%20developers%20working%20with%20multilingual%20and%20low-resource%20languages%20who%20seek%20to%20create%20more%20equitable%20and%20socially%20impactful%20language%20technologies.%20Participants%20will%20walk%20away%20with%20a%20practical%20toolkit%20for%20building%20end-to-end%20NLP%20pipelines%20for%20underrepresented%20languages%20--%20from%20data%20collection%20and%20web%20crawling%20to%20parallel%20sentence%20mining%2C%20machine%20translation%2C%20and%20downstream%20applications%20such%20as%20text%20classification%20and%20multimodal%20reasoning.%20The%20tutorial%20presents%20strategies%20for%20tackling%20the%20challenges%20of%20data%20scarcity%20and%20cultural%20variance%2C%20offering%20hands-on%20methods%20and%20modeling%20frameworks.%20We%20will%20focus%20on%20fair%2C%20reproducible%2C%20and%20community-informed%20development%20approaches%2C%20grounded%20in%20real-world%20scenarios.%20We%20will%20showcase%20a%20diverse%20set%20of%20use%20cases%20covering%20over%2010%20languages%20from%20different%20language%20families%20and%20geopolitical%20contexts%2C%20including%20both%20digitally%20resource-rich%20and%20severely%20underrepresented%20languages.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Resource%252C%2520High-Impact%253A%2520Building%2520Corpora%2520for%2520Inclusive%2520Language%2520Technologies%26entry.906535625%3DEkaterina%2520Artemova%2520and%2520Laurie%2520Burchell%2520and%2520Daryna%2520Dementieva%2520and%2520Shu%2520Okabe%2520and%2520Mariya%2520Shmatova%2520and%2520Pedro%2520Ortiz%2520Suarez%26entry.1292438233%3DThis%2520tutorial%2520%2528https%253A//tum-nlp.github.io/low-resource-tutorial%2529%2520is%2520designed%2520for%2520NLP%2520practitioners%252C%2520researchers%252C%2520and%2520developers%2520working%2520with%2520multilingual%2520and%2520low-resource%2520languages%2520who%2520seek%2520to%2520create%2520more%2520equitable%2520and%2520socially%2520impactful%2520language%2520technologies.%2520Participants%2520will%2520walk%2520away%2520with%2520a%2520practical%2520toolkit%2520for%2520building%2520end-to-end%2520NLP%2520pipelines%2520for%2520underrepresented%2520languages%2520--%2520from%2520data%2520collection%2520and%2520web%2520crawling%2520to%2520parallel%2520sentence%2520mining%252C%2520machine%2520translation%252C%2520and%2520downstream%2520applications%2520such%2520as%2520text%2520classification%2520and%2520multimodal%2520reasoning.%2520The%2520tutorial%2520presents%2520strategies%2520for%2520tackling%2520the%2520challenges%2520of%2520data%2520scarcity%2520and%2520cultural%2520variance%252C%2520offering%2520hands-on%2520methods%2520and%2520modeling%2520frameworks.%2520We%2520will%2520focus%2520on%2520fair%252C%2520reproducible%252C%2520and%2520community-informed%2520development%2520approaches%252C%2520grounded%2520in%2520real-world%2520scenarios.%2520We%2520will%2520showcase%2520a%2520diverse%2520set%2520of%2520use%2520cases%2520covering%2520over%252010%2520languages%2520from%2520different%2520language%2520families%2520and%2520geopolitical%2520contexts%252C%2520including%2520both%2520digitally%2520resource-rich%2520and%2520severely%2520underrepresented%2520languages.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Resource%2C%20High-Impact%3A%20Building%20Corpora%20for%20Inclusive%20Language%20Technologies&entry.906535625=Ekaterina%20Artemova%20and%20Laurie%20Burchell%20and%20Daryna%20Dementieva%20and%20Shu%20Okabe%20and%20Mariya%20Shmatova%20and%20Pedro%20Ortiz%20Suarez&entry.1292438233=This%20tutorial%20%28https%3A//tum-nlp.github.io/low-resource-tutorial%29%20is%20designed%20for%20NLP%20practitioners%2C%20researchers%2C%20and%20developers%20working%20with%20multilingual%20and%20low-resource%20languages%20who%20seek%20to%20create%20more%20equitable%20and%20socially%20impactful%20language%20technologies.%20Participants%20will%20walk%20away%20with%20a%20practical%20toolkit%20for%20building%20end-to-end%20NLP%20pipelines%20for%20underrepresented%20languages%20--%20from%20data%20collection%20and%20web%20crawling%20to%20parallel%20sentence%20mining%2C%20machine%20translation%2C%20and%20downstream%20applications%20such%20as%20text%20classification%20and%20multimodal%20reasoning.%20The%20tutorial%20presents%20strategies%20for%20tackling%20the%20challenges%20of%20data%20scarcity%20and%20cultural%20variance%2C%20offering%20hands-on%20methods%20and%20modeling%20frameworks.%20We%20will%20focus%20on%20fair%2C%20reproducible%2C%20and%20community-informed%20development%20approaches%2C%20grounded%20in%20real-world%20scenarios.%20We%20will%20showcase%20a%20diverse%20set%20of%20use%20cases%20covering%20over%2010%20languages%20from%20different%20language%20families%20and%20geopolitical%20contexts%2C%20including%20both%20digitally%20resource-rich%20and%20severely%20underrepresented%20languages.&entry.1838667208=http%3A//arxiv.org/abs/2512.14576v1&entry.124074799=Read"},
{"title": "Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin", "author": "David Wagmann and Matti Kr\u00fcger and Chao Wang and J\u00fcrgen Steimle", "abstract": "Effective communication of robotic touch intent is a key factor in promoting safe and predictable physical human-robot interaction (pHRI). While intent communication has been widely studied, existing approaches lack the spatial specificity and semantic depth necessary to convey robot touch actions. We present Mirror Skin, a cephalopod-inspired concept that utilizes high-resolution, mirror-like visual feedback on robotic skin. By mapping in-situ visual representations of a human's body parts onto the corresponding robot's touch region, Mirror Skin communicates who shall initiate touch, where it will occur, and when it is imminent. To inform the design of Mirror Skin, we conducted a structured design exploration with experts in virtual reality (VR), iteratively refining six key dimensions. A subsequent controlled user study demonstrated that Mirror Skin significantly enhances accuracy and reduces response times for interpreting touch intent. These findings highlight the potential of visual feedback on robotic skin to communicate human-robot touch interactions.", "link": "http://arxiv.org/abs/2512.11472v2", "date": "2025-12-16", "relevancy": 1.7017, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5867}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5478}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mirror%20Skin%3A%20In%20Situ%20Visualization%20of%20Robot%20Touch%20Intent%20on%20Robotic%20Skin&body=Title%3A%20Mirror%20Skin%3A%20In%20Situ%20Visualization%20of%20Robot%20Touch%20Intent%20on%20Robotic%20Skin%0AAuthor%3A%20David%20Wagmann%20and%20Matti%20Kr%C3%BCger%20and%20Chao%20Wang%20and%20J%C3%BCrgen%20Steimle%0AAbstract%3A%20Effective%20communication%20of%20robotic%20touch%20intent%20is%20a%20key%20factor%20in%20promoting%20safe%20and%20predictable%20physical%20human-robot%20interaction%20%28pHRI%29.%20While%20intent%20communication%20has%20been%20widely%20studied%2C%20existing%20approaches%20lack%20the%20spatial%20specificity%20and%20semantic%20depth%20necessary%20to%20convey%20robot%20touch%20actions.%20We%20present%20Mirror%20Skin%2C%20a%20cephalopod-inspired%20concept%20that%20utilizes%20high-resolution%2C%20mirror-like%20visual%20feedback%20on%20robotic%20skin.%20By%20mapping%20in-situ%20visual%20representations%20of%20a%20human%27s%20body%20parts%20onto%20the%20corresponding%20robot%27s%20touch%20region%2C%20Mirror%20Skin%20communicates%20who%20shall%20initiate%20touch%2C%20where%20it%20will%20occur%2C%20and%20when%20it%20is%20imminent.%20To%20inform%20the%20design%20of%20Mirror%20Skin%2C%20we%20conducted%20a%20structured%20design%20exploration%20with%20experts%20in%20virtual%20reality%20%28VR%29%2C%20iteratively%20refining%20six%20key%20dimensions.%20A%20subsequent%20controlled%20user%20study%20demonstrated%20that%20Mirror%20Skin%20significantly%20enhances%20accuracy%20and%20reduces%20response%20times%20for%20interpreting%20touch%20intent.%20These%20findings%20highlight%20the%20potential%20of%20visual%20feedback%20on%20robotic%20skin%20to%20communicate%20human-robot%20touch%20interactions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMirror%2520Skin%253A%2520In%2520Situ%2520Visualization%2520of%2520Robot%2520Touch%2520Intent%2520on%2520Robotic%2520Skin%26entry.906535625%3DDavid%2520Wagmann%2520and%2520Matti%2520Kr%25C3%25BCger%2520and%2520Chao%2520Wang%2520and%2520J%25C3%25BCrgen%2520Steimle%26entry.1292438233%3DEffective%2520communication%2520of%2520robotic%2520touch%2520intent%2520is%2520a%2520key%2520factor%2520in%2520promoting%2520safe%2520and%2520predictable%2520physical%2520human-robot%2520interaction%2520%2528pHRI%2529.%2520While%2520intent%2520communication%2520has%2520been%2520widely%2520studied%252C%2520existing%2520approaches%2520lack%2520the%2520spatial%2520specificity%2520and%2520semantic%2520depth%2520necessary%2520to%2520convey%2520robot%2520touch%2520actions.%2520We%2520present%2520Mirror%2520Skin%252C%2520a%2520cephalopod-inspired%2520concept%2520that%2520utilizes%2520high-resolution%252C%2520mirror-like%2520visual%2520feedback%2520on%2520robotic%2520skin.%2520By%2520mapping%2520in-situ%2520visual%2520representations%2520of%2520a%2520human%2527s%2520body%2520parts%2520onto%2520the%2520corresponding%2520robot%2527s%2520touch%2520region%252C%2520Mirror%2520Skin%2520communicates%2520who%2520shall%2520initiate%2520touch%252C%2520where%2520it%2520will%2520occur%252C%2520and%2520when%2520it%2520is%2520imminent.%2520To%2520inform%2520the%2520design%2520of%2520Mirror%2520Skin%252C%2520we%2520conducted%2520a%2520structured%2520design%2520exploration%2520with%2520experts%2520in%2520virtual%2520reality%2520%2528VR%2529%252C%2520iteratively%2520refining%2520six%2520key%2520dimensions.%2520A%2520subsequent%2520controlled%2520user%2520study%2520demonstrated%2520that%2520Mirror%2520Skin%2520significantly%2520enhances%2520accuracy%2520and%2520reduces%2520response%2520times%2520for%2520interpreting%2520touch%2520intent.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520visual%2520feedback%2520on%2520robotic%2520skin%2520to%2520communicate%2520human-robot%2520touch%2520interactions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mirror%20Skin%3A%20In%20Situ%20Visualization%20of%20Robot%20Touch%20Intent%20on%20Robotic%20Skin&entry.906535625=David%20Wagmann%20and%20Matti%20Kr%C3%BCger%20and%20Chao%20Wang%20and%20J%C3%BCrgen%20Steimle&entry.1292438233=Effective%20communication%20of%20robotic%20touch%20intent%20is%20a%20key%20factor%20in%20promoting%20safe%20and%20predictable%20physical%20human-robot%20interaction%20%28pHRI%29.%20While%20intent%20communication%20has%20been%20widely%20studied%2C%20existing%20approaches%20lack%20the%20spatial%20specificity%20and%20semantic%20depth%20necessary%20to%20convey%20robot%20touch%20actions.%20We%20present%20Mirror%20Skin%2C%20a%20cephalopod-inspired%20concept%20that%20utilizes%20high-resolution%2C%20mirror-like%20visual%20feedback%20on%20robotic%20skin.%20By%20mapping%20in-situ%20visual%20representations%20of%20a%20human%27s%20body%20parts%20onto%20the%20corresponding%20robot%27s%20touch%20region%2C%20Mirror%20Skin%20communicates%20who%20shall%20initiate%20touch%2C%20where%20it%20will%20occur%2C%20and%20when%20it%20is%20imminent.%20To%20inform%20the%20design%20of%20Mirror%20Skin%2C%20we%20conducted%20a%20structured%20design%20exploration%20with%20experts%20in%20virtual%20reality%20%28VR%29%2C%20iteratively%20refining%20six%20key%20dimensions.%20A%20subsequent%20controlled%20user%20study%20demonstrated%20that%20Mirror%20Skin%20significantly%20enhances%20accuracy%20and%20reduces%20response%20times%20for%20interpreting%20touch%20intent.%20These%20findings%20highlight%20the%20potential%20of%20visual%20feedback%20on%20robotic%20skin%20to%20communicate%20human-robot%20touch%20interactions.&entry.1838667208=http%3A//arxiv.org/abs/2512.11472v2&entry.124074799=Read"},
{"title": "Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer", "author": "Adarsha Shrestha and Basanta Pokharel and Binit Shrestha and Smriti Adhikari and Dinesh Gothe", "abstract": "Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.", "link": "http://arxiv.org/abs/2512.14585v1", "date": "2025-12-16", "relevancy": 2.1322, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4444}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4271}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Nepali-language%20LLMs%3A%20Efficient%20GPT%20training%20with%20a%20Nepali%20BPE%20tokenizer&body=Title%3A%20Towards%20Nepali-language%20LLMs%3A%20Efficient%20GPT%20training%20with%20a%20Nepali%20BPE%20tokenizer%0AAuthor%3A%20Adarsha%20Shrestha%20and%20Basanta%20Pokharel%20and%20Binit%20Shrestha%20and%20Smriti%20Adhikari%20and%20Dinesh%20Gothe%0AAbstract%3A%20Nepali%2C%20a%20low-resource%20language%20spoken%20by%20over%2032%20million%20people%2C%20continues%20to%20face%20challenges%20in%20natural%20language%20processing%20%28NLP%29%20due%20to%20its%20complex%20grammar%2C%20agglutinative%20morphology%2C%20and%20limited%20availability%20of%20high-quality%20corpora.%20Most%20efforts%20to%20date%20have%20centered%20on%20basic%20encoder%20architectures%3B%20they%20remain%20insufficient%20for%20Nepali-specific%20text%20generation.%20This%20study%20presents%20a%20GPT-2-based%20Nepali%20language%20model%20trained%20using%20several%20training%20strategies%20inspired%20by%20GPT-3%2C%20including%20optimized%20learning%20rate%20schedules%2C%20batch%20scaling%2C%20and%20architectural%20refinements.%20A%20custom%2016k%20Byte-Pair%20Encoding%20%28BPE%29%20tokenizer%20was%20trained%20exclusively%20on%20Nepali%20text%20to%20ensure%20more%20consistent%20segmentation%20and%20improved%20input%20representation.%20The%20model%20was%20pretrained%20on%20a%20combined%20dataset%20comprising%20a%2010.75GB%20cleaned%20NepBERTa%20corpus%20and%20additional%20web-scraped%20Nepali%20news%20articles.%20FlashAttention%20was%20integrated%20to%20reduce%20memory%20usage%20and%20stabilize%20training.%20After%20two%20epochs%2C%20the%20model%20achieved%20a%20training%20loss%20of%203.168177%2C%20a%20validation%20loss%20of%203.081982%2C%20and%20a%20final%20perplexity%20of%2021.80%2C%20demonstrating%20its%20capability%20to%20generate%20coherent%20Nepali%20news-style%20text.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Nepali-language%2520LLMs%253A%2520Efficient%2520GPT%2520training%2520with%2520a%2520Nepali%2520BPE%2520tokenizer%26entry.906535625%3DAdarsha%2520Shrestha%2520and%2520Basanta%2520Pokharel%2520and%2520Binit%2520Shrestha%2520and%2520Smriti%2520Adhikari%2520and%2520Dinesh%2520Gothe%26entry.1292438233%3DNepali%252C%2520a%2520low-resource%2520language%2520spoken%2520by%2520over%252032%2520million%2520people%252C%2520continues%2520to%2520face%2520challenges%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%2520due%2520to%2520its%2520complex%2520grammar%252C%2520agglutinative%2520morphology%252C%2520and%2520limited%2520availability%2520of%2520high-quality%2520corpora.%2520Most%2520efforts%2520to%2520date%2520have%2520centered%2520on%2520basic%2520encoder%2520architectures%253B%2520they%2520remain%2520insufficient%2520for%2520Nepali-specific%2520text%2520generation.%2520This%2520study%2520presents%2520a%2520GPT-2-based%2520Nepali%2520language%2520model%2520trained%2520using%2520several%2520training%2520strategies%2520inspired%2520by%2520GPT-3%252C%2520including%2520optimized%2520learning%2520rate%2520schedules%252C%2520batch%2520scaling%252C%2520and%2520architectural%2520refinements.%2520A%2520custom%252016k%2520Byte-Pair%2520Encoding%2520%2528BPE%2529%2520tokenizer%2520was%2520trained%2520exclusively%2520on%2520Nepali%2520text%2520to%2520ensure%2520more%2520consistent%2520segmentation%2520and%2520improved%2520input%2520representation.%2520The%2520model%2520was%2520pretrained%2520on%2520a%2520combined%2520dataset%2520comprising%2520a%252010.75GB%2520cleaned%2520NepBERTa%2520corpus%2520and%2520additional%2520web-scraped%2520Nepali%2520news%2520articles.%2520FlashAttention%2520was%2520integrated%2520to%2520reduce%2520memory%2520usage%2520and%2520stabilize%2520training.%2520After%2520two%2520epochs%252C%2520the%2520model%2520achieved%2520a%2520training%2520loss%2520of%25203.168177%252C%2520a%2520validation%2520loss%2520of%25203.081982%252C%2520and%2520a%2520final%2520perplexity%2520of%252021.80%252C%2520demonstrating%2520its%2520capability%2520to%2520generate%2520coherent%2520Nepali%2520news-style%2520text.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Nepali-language%20LLMs%3A%20Efficient%20GPT%20training%20with%20a%20Nepali%20BPE%20tokenizer&entry.906535625=Adarsha%20Shrestha%20and%20Basanta%20Pokharel%20and%20Binit%20Shrestha%20and%20Smriti%20Adhikari%20and%20Dinesh%20Gothe&entry.1292438233=Nepali%2C%20a%20low-resource%20language%20spoken%20by%20over%2032%20million%20people%2C%20continues%20to%20face%20challenges%20in%20natural%20language%20processing%20%28NLP%29%20due%20to%20its%20complex%20grammar%2C%20agglutinative%20morphology%2C%20and%20limited%20availability%20of%20high-quality%20corpora.%20Most%20efforts%20to%20date%20have%20centered%20on%20basic%20encoder%20architectures%3B%20they%20remain%20insufficient%20for%20Nepali-specific%20text%20generation.%20This%20study%20presents%20a%20GPT-2-based%20Nepali%20language%20model%20trained%20using%20several%20training%20strategies%20inspired%20by%20GPT-3%2C%20including%20optimized%20learning%20rate%20schedules%2C%20batch%20scaling%2C%20and%20architectural%20refinements.%20A%20custom%2016k%20Byte-Pair%20Encoding%20%28BPE%29%20tokenizer%20was%20trained%20exclusively%20on%20Nepali%20text%20to%20ensure%20more%20consistent%20segmentation%20and%20improved%20input%20representation.%20The%20model%20was%20pretrained%20on%20a%20combined%20dataset%20comprising%20a%2010.75GB%20cleaned%20NepBERTa%20corpus%20and%20additional%20web-scraped%20Nepali%20news%20articles.%20FlashAttention%20was%20integrated%20to%20reduce%20memory%20usage%20and%20stabilize%20training.%20After%20two%20epochs%2C%20the%20model%20achieved%20a%20training%20loss%20of%203.168177%2C%20a%20validation%20loss%20of%203.081982%2C%20and%20a%20final%20perplexity%20of%2021.80%2C%20demonstrating%20its%20capability%20to%20generate%20coherent%20Nepali%20news-style%20text.&entry.1838667208=http%3A//arxiv.org/abs/2512.14585v1&entry.124074799=Read"},
{"title": "Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean", "author": "Chuan He", "abstract": "Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $\u03b1$ of the noise. Nonetheless, existing complexity results often cover only the case $\u03b1\\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $\u03b1$ approaches $1$. This paper tackles the general case of noise with tail index $\u03b1\\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $\u03b1\\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.", "link": "http://arxiv.org/abs/2512.14686v1", "date": "2025-12-16", "relevancy": 1.7705, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4395}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias-Variance%20Trade-off%20for%20Clipped%20Stochastic%20First-Order%20Methods%3A%20From%20Bounded%20Variance%20to%20Infinite%20Mean&body=Title%3A%20Bias-Variance%20Trade-off%20for%20Clipped%20Stochastic%20First-Order%20Methods%3A%20From%20Bounded%20Variance%20to%20Infinite%20Mean%0AAuthor%3A%20Chuan%20He%0AAbstract%3A%20Stochastic%20optimization%20is%20fundamental%20to%20modern%20machine%20learning.%20Recent%20research%20has%20extended%20the%20study%20of%20stochastic%20first-order%20methods%20%28SFOMs%29%20from%20light-tailed%20to%20heavy-tailed%20noise%2C%20which%20frequently%20arises%20in%20practice%2C%20with%20clipping%20emerging%20as%20a%20key%20technique%20for%20controlling%20heavy-tailed%20gradients.%20Extensive%20theoretical%20advances%20have%20further%20shown%20that%20the%20oracle%20complexity%20of%20SFOMs%20depends%20on%20the%20tail%20index%20%24%CE%B1%24%20of%20the%20noise.%20Nonetheless%2C%20existing%20complexity%20results%20often%20cover%20only%20the%20case%20%24%CE%B1%5Cin%20%281%2C2%5D%24%2C%20that%20is%2C%20the%20regime%20where%20the%20noise%20has%20a%20finite%20mean%2C%20while%20the%20complexity%20bounds%20tend%20to%20infinity%20as%20%24%CE%B1%24%20approaches%20%241%24.%20This%20paper%20tackles%20the%20general%20case%20of%20noise%20with%20tail%20index%20%24%CE%B1%5Cin%280%2C2%5D%24%2C%20covering%20regimes%20ranging%20from%20noise%20with%20bounded%20variance%20to%20noise%20with%20an%20infinite%20mean%2C%20where%20the%20latter%20case%20has%20been%20scarcely%20studied.%20Through%20a%20novel%20analysis%20of%20the%20bias-variance%20trade-off%20in%20gradient%20clipping%2C%20we%20show%20that%20when%20a%20symmetry%20measure%20of%20the%20noise%20tail%20is%20controlled%2C%20clipped%20SFOMs%20achieve%20improved%20complexity%20guarantees%20in%20the%20presence%20of%20heavy-tailed%20noise%20for%20any%20tail%20index%20%24%CE%B1%5Cin%20%280%2C2%5D%24.%20Our%20analysis%20of%20the%20bias-variance%20trade-off%20not%20only%20yields%20new%20unified%20complexity%20guarantees%20for%20clipped%20SFOMs%20across%20this%20full%20range%20of%20tail%20indices%2C%20but%20is%20also%20straightforward%20to%20apply%20and%20can%20be%20combined%20with%20classical%20analyses%20under%20light-tailed%20noise%20to%20establish%20oracle%20complexity%20guarantees%20under%20heavy-tailed%20noise.%20Finally%2C%20numerical%20experiments%20validate%20our%20theoretical%20findings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias-Variance%2520Trade-off%2520for%2520Clipped%2520Stochastic%2520First-Order%2520Methods%253A%2520From%2520Bounded%2520Variance%2520to%2520Infinite%2520Mean%26entry.906535625%3DChuan%2520He%26entry.1292438233%3DStochastic%2520optimization%2520is%2520fundamental%2520to%2520modern%2520machine%2520learning.%2520Recent%2520research%2520has%2520extended%2520the%2520study%2520of%2520stochastic%2520first-order%2520methods%2520%2528SFOMs%2529%2520from%2520light-tailed%2520to%2520heavy-tailed%2520noise%252C%2520which%2520frequently%2520arises%2520in%2520practice%252C%2520with%2520clipping%2520emerging%2520as%2520a%2520key%2520technique%2520for%2520controlling%2520heavy-tailed%2520gradients.%2520Extensive%2520theoretical%2520advances%2520have%2520further%2520shown%2520that%2520the%2520oracle%2520complexity%2520of%2520SFOMs%2520depends%2520on%2520the%2520tail%2520index%2520%2524%25CE%25B1%2524%2520of%2520the%2520noise.%2520Nonetheless%252C%2520existing%2520complexity%2520results%2520often%2520cover%2520only%2520the%2520case%2520%2524%25CE%25B1%255Cin%2520%25281%252C2%255D%2524%252C%2520that%2520is%252C%2520the%2520regime%2520where%2520the%2520noise%2520has%2520a%2520finite%2520mean%252C%2520while%2520the%2520complexity%2520bounds%2520tend%2520to%2520infinity%2520as%2520%2524%25CE%25B1%2524%2520approaches%2520%25241%2524.%2520This%2520paper%2520tackles%2520the%2520general%2520case%2520of%2520noise%2520with%2520tail%2520index%2520%2524%25CE%25B1%255Cin%25280%252C2%255D%2524%252C%2520covering%2520regimes%2520ranging%2520from%2520noise%2520with%2520bounded%2520variance%2520to%2520noise%2520with%2520an%2520infinite%2520mean%252C%2520where%2520the%2520latter%2520case%2520has%2520been%2520scarcely%2520studied.%2520Through%2520a%2520novel%2520analysis%2520of%2520the%2520bias-variance%2520trade-off%2520in%2520gradient%2520clipping%252C%2520we%2520show%2520that%2520when%2520a%2520symmetry%2520measure%2520of%2520the%2520noise%2520tail%2520is%2520controlled%252C%2520clipped%2520SFOMs%2520achieve%2520improved%2520complexity%2520guarantees%2520in%2520the%2520presence%2520of%2520heavy-tailed%2520noise%2520for%2520any%2520tail%2520index%2520%2524%25CE%25B1%255Cin%2520%25280%252C2%255D%2524.%2520Our%2520analysis%2520of%2520the%2520bias-variance%2520trade-off%2520not%2520only%2520yields%2520new%2520unified%2520complexity%2520guarantees%2520for%2520clipped%2520SFOMs%2520across%2520this%2520full%2520range%2520of%2520tail%2520indices%252C%2520but%2520is%2520also%2520straightforward%2520to%2520apply%2520and%2520can%2520be%2520combined%2520with%2520classical%2520analyses%2520under%2520light-tailed%2520noise%2520to%2520establish%2520oracle%2520complexity%2520guarantees%2520under%2520heavy-tailed%2520noise.%2520Finally%252C%2520numerical%2520experiments%2520validate%2520our%2520theoretical%2520findings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias-Variance%20Trade-off%20for%20Clipped%20Stochastic%20First-Order%20Methods%3A%20From%20Bounded%20Variance%20to%20Infinite%20Mean&entry.906535625=Chuan%20He&entry.1292438233=Stochastic%20optimization%20is%20fundamental%20to%20modern%20machine%20learning.%20Recent%20research%20has%20extended%20the%20study%20of%20stochastic%20first-order%20methods%20%28SFOMs%29%20from%20light-tailed%20to%20heavy-tailed%20noise%2C%20which%20frequently%20arises%20in%20practice%2C%20with%20clipping%20emerging%20as%20a%20key%20technique%20for%20controlling%20heavy-tailed%20gradients.%20Extensive%20theoretical%20advances%20have%20further%20shown%20that%20the%20oracle%20complexity%20of%20SFOMs%20depends%20on%20the%20tail%20index%20%24%CE%B1%24%20of%20the%20noise.%20Nonetheless%2C%20existing%20complexity%20results%20often%20cover%20only%20the%20case%20%24%CE%B1%5Cin%20%281%2C2%5D%24%2C%20that%20is%2C%20the%20regime%20where%20the%20noise%20has%20a%20finite%20mean%2C%20while%20the%20complexity%20bounds%20tend%20to%20infinity%20as%20%24%CE%B1%24%20approaches%20%241%24.%20This%20paper%20tackles%20the%20general%20case%20of%20noise%20with%20tail%20index%20%24%CE%B1%5Cin%280%2C2%5D%24%2C%20covering%20regimes%20ranging%20from%20noise%20with%20bounded%20variance%20to%20noise%20with%20an%20infinite%20mean%2C%20where%20the%20latter%20case%20has%20been%20scarcely%20studied.%20Through%20a%20novel%20analysis%20of%20the%20bias-variance%20trade-off%20in%20gradient%20clipping%2C%20we%20show%20that%20when%20a%20symmetry%20measure%20of%20the%20noise%20tail%20is%20controlled%2C%20clipped%20SFOMs%20achieve%20improved%20complexity%20guarantees%20in%20the%20presence%20of%20heavy-tailed%20noise%20for%20any%20tail%20index%20%24%CE%B1%5Cin%20%280%2C2%5D%24.%20Our%20analysis%20of%20the%20bias-variance%20trade-off%20not%20only%20yields%20new%20unified%20complexity%20guarantees%20for%20clipped%20SFOMs%20across%20this%20full%20range%20of%20tail%20indices%2C%20but%20is%20also%20straightforward%20to%20apply%20and%20can%20be%20combined%20with%20classical%20analyses%20under%20light-tailed%20noise%20to%20establish%20oracle%20complexity%20guarantees%20under%20heavy-tailed%20noise.%20Finally%2C%20numerical%20experiments%20validate%20our%20theoretical%20findings.&entry.1838667208=http%3A//arxiv.org/abs/2512.14686v1&entry.124074799=Read"},
{"title": "A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems", "author": "Georg Volk and J\u00f6rg Gamerdinger and Alexander von Bernuth and Oliver Bringmann", "abstract": "Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.", "link": "http://arxiv.org/abs/2512.14367v1", "date": "2025-12-16", "relevancy": 2.1285, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5616}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5411}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Safety%20Metric%20to%20Evaluate%20Perception%20in%20Autonomous%20Systems&body=Title%3A%20A%20Comprehensive%20Safety%20Metric%20to%20Evaluate%20Perception%20in%20Autonomous%20Systems%0AAuthor%3A%20Georg%20Volk%20and%20J%C3%B6rg%20Gamerdinger%20and%20Alexander%20von%20Bernuth%20and%20Oliver%20Bringmann%0AAbstract%3A%20Complete%20perception%20of%20the%20environment%20and%20its%20correct%20interpretation%20is%20crucial%20for%20autonomous%20vehicles.%20Object%20perception%20is%20the%20main%20component%20of%20automotive%20surround%20sensing.%20Various%20metrics%20already%20exist%20for%20the%20evaluation%20of%20object%20perception.%20However%2C%20objects%20can%20be%20of%20different%20importance%20depending%20on%20their%20velocity%2C%20orientation%2C%20distance%2C%20size%2C%20or%20the%20potential%20damage%20that%20could%20be%20caused%20by%20a%20collision%20due%20to%20a%20missed%20detection.%20Thus%2C%20these%20additional%20parameters%20have%20to%20be%20considered%20for%20safety%20evaluation.%20We%20propose%20a%20new%20safety%20metric%20that%20incorporates%20all%20these%20parameters%20and%20returns%20a%20single%20easily%20interpretable%20safety%20assessment%20score%20for%20object%20perception.%20This%20new%20metric%20is%20evaluated%20with%20both%20real%20world%20and%20virtual%20data%20sets%20and%20compared%20to%20state%20of%20the%20art%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Safety%2520Metric%2520to%2520Evaluate%2520Perception%2520in%2520Autonomous%2520Systems%26entry.906535625%3DGeorg%2520Volk%2520and%2520J%25C3%25B6rg%2520Gamerdinger%2520and%2520Alexander%2520von%2520Bernuth%2520and%2520Oliver%2520Bringmann%26entry.1292438233%3DComplete%2520perception%2520of%2520the%2520environment%2520and%2520its%2520correct%2520interpretation%2520is%2520crucial%2520for%2520autonomous%2520vehicles.%2520Object%2520perception%2520is%2520the%2520main%2520component%2520of%2520automotive%2520surround%2520sensing.%2520Various%2520metrics%2520already%2520exist%2520for%2520the%2520evaluation%2520of%2520object%2520perception.%2520However%252C%2520objects%2520can%2520be%2520of%2520different%2520importance%2520depending%2520on%2520their%2520velocity%252C%2520orientation%252C%2520distance%252C%2520size%252C%2520or%2520the%2520potential%2520damage%2520that%2520could%2520be%2520caused%2520by%2520a%2520collision%2520due%2520to%2520a%2520missed%2520detection.%2520Thus%252C%2520these%2520additional%2520parameters%2520have%2520to%2520be%2520considered%2520for%2520safety%2520evaluation.%2520We%2520propose%2520a%2520new%2520safety%2520metric%2520that%2520incorporates%2520all%2520these%2520parameters%2520and%2520returns%2520a%2520single%2520easily%2520interpretable%2520safety%2520assessment%2520score%2520for%2520object%2520perception.%2520This%2520new%2520metric%2520is%2520evaluated%2520with%2520both%2520real%2520world%2520and%2520virtual%2520data%2520sets%2520and%2520compared%2520to%2520state%2520of%2520the%2520art%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Safety%20Metric%20to%20Evaluate%20Perception%20in%20Autonomous%20Systems&entry.906535625=Georg%20Volk%20and%20J%C3%B6rg%20Gamerdinger%20and%20Alexander%20von%20Bernuth%20and%20Oliver%20Bringmann&entry.1292438233=Complete%20perception%20of%20the%20environment%20and%20its%20correct%20interpretation%20is%20crucial%20for%20autonomous%20vehicles.%20Object%20perception%20is%20the%20main%20component%20of%20automotive%20surround%20sensing.%20Various%20metrics%20already%20exist%20for%20the%20evaluation%20of%20object%20perception.%20However%2C%20objects%20can%20be%20of%20different%20importance%20depending%20on%20their%20velocity%2C%20orientation%2C%20distance%2C%20size%2C%20or%20the%20potential%20damage%20that%20could%20be%20caused%20by%20a%20collision%20due%20to%20a%20missed%20detection.%20Thus%2C%20these%20additional%20parameters%20have%20to%20be%20considered%20for%20safety%20evaluation.%20We%20propose%20a%20new%20safety%20metric%20that%20incorporates%20all%20these%20parameters%20and%20returns%20a%20single%20easily%20interpretable%20safety%20assessment%20score%20for%20object%20perception.%20This%20new%20metric%20is%20evaluated%20with%20both%20real%20world%20and%20virtual%20data%20sets%20and%20compared%20to%20state%20of%20the%20art%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2512.14367v1&entry.124074799=Read"},
{"title": "SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions", "author": "Panayiotis Smeros and Vincent Emonet and Ruijie Wang and Ana-Claudia Sima and Tarcisio Mendes de Farias", "abstract": "The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.", "link": "http://arxiv.org/abs/2512.14277v1", "date": "2025-12-16", "relevancy": 1.8873, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARQL-LLM%3A%20Real-Time%20SPARQL%20Query%20Generation%20from%20Natural%20Language%20Questions&body=Title%3A%20SPARQL-LLM%3A%20Real-Time%20SPARQL%20Query%20Generation%20from%20Natural%20Language%20Questions%0AAuthor%3A%20Panayiotis%20Smeros%20and%20Vincent%20Emonet%20and%20Ruijie%20Wang%20and%20Ana-Claudia%20Sima%20and%20Tarcisio%20Mendes%20de%20Farias%0AAbstract%3A%20The%20advent%20of%20large%20language%20models%20is%20contributing%20to%20the%20emergence%20of%20novel%20approaches%20that%20promise%20to%20better%20tackle%20the%20challenge%20of%20generating%20structured%20queries%2C%20such%20as%20SPARQL%20queries%2C%20from%20natural%20language.%20However%2C%20these%20new%20approaches%20mostly%20focus%20on%20response%20accuracy%20over%20a%20single%20source%20while%20ignoring%20other%20evaluation%20criteria%2C%20such%20as%20federated%20query%20capability%20over%20distributed%20data%20stores%2C%20as%20well%20as%20runtime%20and%20cost%20to%20generate%20SPARQL%20queries.%20Consequently%2C%20they%20are%20often%20not%20production-ready%20or%20easy%20to%20deploy%20over%20%28potentially%20federated%29%20knowledge%20graphs%20with%20good%20accuracy.%20To%20mitigate%20these%20issues%2C%20in%20this%20paper%2C%20we%20extend%20our%20previous%20work%20and%20describe%20and%20systematically%20evaluate%20SPARQL-LLM%2C%20an%20open-source%20and%20triplestore-agnostic%20approach%2C%20powered%20by%20lightweight%20metadata%2C%20that%20generates%20SPARQL%20queries%20from%20natural%20language%20text.%20First%2C%20we%20describe%20its%20architecture%2C%20which%20consists%20of%20dedicated%20components%20for%20metadata%20indexing%2C%20prompt%20building%2C%20and%20query%20generation%20and%20execution.%20Then%2C%20we%20evaluate%20it%20based%20on%20a%20state-of-the-art%20challenge%20with%20multilingual%20questions%2C%20and%20a%20collection%20of%20questions%20from%20three%20of%20the%20most%20prevalent%20knowledge%20graphs%20within%20the%20field%20of%20bioinformatics.%20Our%20results%20demonstrate%20a%20substantial%20increase%20of%2024%25%20in%20the%20F1%20Score%20on%20the%20state-of-the-art%20challenge%2C%20adaptability%20to%20high-resource%20languages%20such%20as%20English%20and%20Spanish%2C%20as%20well%20as%20ability%20to%20form%20complex%20and%20federated%20bioinformatics%20queries.%20Furthermore%2C%20we%20show%20that%20SPARQL-LLM%20is%20up%20to%2036x%20faster%20than%20other%20systems%20participating%20in%20the%20challenge%2C%20while%20costing%20a%20maximum%20of%20%240.01%20per%20question%2C%20making%20it%20suitable%20for%20real-time%2C%20low-cost%20text-to-SPARQL%20applications.%20One%20such%20application%20deployed%20over%20real-world%20decentralized%20knowledge%20graphs%20can%20be%20found%20at%20https%3A//www.expasy.org/chat.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARQL-LLM%253A%2520Real-Time%2520SPARQL%2520Query%2520Generation%2520from%2520Natural%2520Language%2520Questions%26entry.906535625%3DPanayiotis%2520Smeros%2520and%2520Vincent%2520Emonet%2520and%2520Ruijie%2520Wang%2520and%2520Ana-Claudia%2520Sima%2520and%2520Tarcisio%2520Mendes%2520de%2520Farias%26entry.1292438233%3DThe%2520advent%2520of%2520large%2520language%2520models%2520is%2520contributing%2520to%2520the%2520emergence%2520of%2520novel%2520approaches%2520that%2520promise%2520to%2520better%2520tackle%2520the%2520challenge%2520of%2520generating%2520structured%2520queries%252C%2520such%2520as%2520SPARQL%2520queries%252C%2520from%2520natural%2520language.%2520However%252C%2520these%2520new%2520approaches%2520mostly%2520focus%2520on%2520response%2520accuracy%2520over%2520a%2520single%2520source%2520while%2520ignoring%2520other%2520evaluation%2520criteria%252C%2520such%2520as%2520federated%2520query%2520capability%2520over%2520distributed%2520data%2520stores%252C%2520as%2520well%2520as%2520runtime%2520and%2520cost%2520to%2520generate%2520SPARQL%2520queries.%2520Consequently%252C%2520they%2520are%2520often%2520not%2520production-ready%2520or%2520easy%2520to%2520deploy%2520over%2520%2528potentially%2520federated%2529%2520knowledge%2520graphs%2520with%2520good%2520accuracy.%2520To%2520mitigate%2520these%2520issues%252C%2520in%2520this%2520paper%252C%2520we%2520extend%2520our%2520previous%2520work%2520and%2520describe%2520and%2520systematically%2520evaluate%2520SPARQL-LLM%252C%2520an%2520open-source%2520and%2520triplestore-agnostic%2520approach%252C%2520powered%2520by%2520lightweight%2520metadata%252C%2520that%2520generates%2520SPARQL%2520queries%2520from%2520natural%2520language%2520text.%2520First%252C%2520we%2520describe%2520its%2520architecture%252C%2520which%2520consists%2520of%2520dedicated%2520components%2520for%2520metadata%2520indexing%252C%2520prompt%2520building%252C%2520and%2520query%2520generation%2520and%2520execution.%2520Then%252C%2520we%2520evaluate%2520it%2520based%2520on%2520a%2520state-of-the-art%2520challenge%2520with%2520multilingual%2520questions%252C%2520and%2520a%2520collection%2520of%2520questions%2520from%2520three%2520of%2520the%2520most%2520prevalent%2520knowledge%2520graphs%2520within%2520the%2520field%2520of%2520bioinformatics.%2520Our%2520results%2520demonstrate%2520a%2520substantial%2520increase%2520of%252024%2525%2520in%2520the%2520F1%2520Score%2520on%2520the%2520state-of-the-art%2520challenge%252C%2520adaptability%2520to%2520high-resource%2520languages%2520such%2520as%2520English%2520and%2520Spanish%252C%2520as%2520well%2520as%2520ability%2520to%2520form%2520complex%2520and%2520federated%2520bioinformatics%2520queries.%2520Furthermore%252C%2520we%2520show%2520that%2520SPARQL-LLM%2520is%2520up%2520to%252036x%2520faster%2520than%2520other%2520systems%2520participating%2520in%2520the%2520challenge%252C%2520while%2520costing%2520a%2520maximum%2520of%2520%25240.01%2520per%2520question%252C%2520making%2520it%2520suitable%2520for%2520real-time%252C%2520low-cost%2520text-to-SPARQL%2520applications.%2520One%2520such%2520application%2520deployed%2520over%2520real-world%2520decentralized%2520knowledge%2520graphs%2520can%2520be%2520found%2520at%2520https%253A//www.expasy.org/chat.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARQL-LLM%3A%20Real-Time%20SPARQL%20Query%20Generation%20from%20Natural%20Language%20Questions&entry.906535625=Panayiotis%20Smeros%20and%20Vincent%20Emonet%20and%20Ruijie%20Wang%20and%20Ana-Claudia%20Sima%20and%20Tarcisio%20Mendes%20de%20Farias&entry.1292438233=The%20advent%20of%20large%20language%20models%20is%20contributing%20to%20the%20emergence%20of%20novel%20approaches%20that%20promise%20to%20better%20tackle%20the%20challenge%20of%20generating%20structured%20queries%2C%20such%20as%20SPARQL%20queries%2C%20from%20natural%20language.%20However%2C%20these%20new%20approaches%20mostly%20focus%20on%20response%20accuracy%20over%20a%20single%20source%20while%20ignoring%20other%20evaluation%20criteria%2C%20such%20as%20federated%20query%20capability%20over%20distributed%20data%20stores%2C%20as%20well%20as%20runtime%20and%20cost%20to%20generate%20SPARQL%20queries.%20Consequently%2C%20they%20are%20often%20not%20production-ready%20or%20easy%20to%20deploy%20over%20%28potentially%20federated%29%20knowledge%20graphs%20with%20good%20accuracy.%20To%20mitigate%20these%20issues%2C%20in%20this%20paper%2C%20we%20extend%20our%20previous%20work%20and%20describe%20and%20systematically%20evaluate%20SPARQL-LLM%2C%20an%20open-source%20and%20triplestore-agnostic%20approach%2C%20powered%20by%20lightweight%20metadata%2C%20that%20generates%20SPARQL%20queries%20from%20natural%20language%20text.%20First%2C%20we%20describe%20its%20architecture%2C%20which%20consists%20of%20dedicated%20components%20for%20metadata%20indexing%2C%20prompt%20building%2C%20and%20query%20generation%20and%20execution.%20Then%2C%20we%20evaluate%20it%20based%20on%20a%20state-of-the-art%20challenge%20with%20multilingual%20questions%2C%20and%20a%20collection%20of%20questions%20from%20three%20of%20the%20most%20prevalent%20knowledge%20graphs%20within%20the%20field%20of%20bioinformatics.%20Our%20results%20demonstrate%20a%20substantial%20increase%20of%2024%25%20in%20the%20F1%20Score%20on%20the%20state-of-the-art%20challenge%2C%20adaptability%20to%20high-resource%20languages%20such%20as%20English%20and%20Spanish%2C%20as%20well%20as%20ability%20to%20form%20complex%20and%20federated%20bioinformatics%20queries.%20Furthermore%2C%20we%20show%20that%20SPARQL-LLM%20is%20up%20to%2036x%20faster%20than%20other%20systems%20participating%20in%20the%20challenge%2C%20while%20costing%20a%20maximum%20of%20%240.01%20per%20question%2C%20making%20it%20suitable%20for%20real-time%2C%20low-cost%20text-to-SPARQL%20applications.%20One%20such%20application%20deployed%20over%20real-world%20decentralized%20knowledge%20graphs%20can%20be%20found%20at%20https%3A//www.expasy.org/chat.&entry.1838667208=http%3A//arxiv.org/abs/2512.14277v1&entry.124074799=Read"},
{"title": "Dual Attention Guided Defense Against Malicious Edits", "author": "Jie Zhang and Shuai Dong and Shiguang Shan and Xilin Chen", "abstract": "Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2512.14333v1", "date": "2025-12-16", "relevancy": 1.1249, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5964}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5468}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Attention%20Guided%20Defense%20Against%20Malicious%20Edits&body=Title%3A%20Dual%20Attention%20Guided%20Defense%20Against%20Malicious%20Edits%0AAuthor%3A%20Jie%20Zhang%20and%20Shuai%20Dong%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20Recent%20progress%20in%20text-to-image%20diffusion%20models%20has%20transformed%20image%20editing%20via%20text%20prompts%2C%20yet%20this%20also%20introduces%20significant%20ethical%20challenges%20from%20potential%20misuse%20in%20creating%20deceptive%20or%20harmful%20content.%20While%20current%20defenses%20seek%20to%20mitigate%20this%20risk%20by%20embedding%20imperceptible%20perturbations%2C%20their%20effectiveness%20is%20limited%20against%20malicious%20tampering.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Dual%20Attention-Guided%20Noise%20Perturbation%20%28DANP%29%20immunization%20method%20that%20adds%20imperceptible%20perturbations%20to%20disrupt%20the%20model%27s%20semantic%20understanding%20and%20generation%20process.%20DANP%20functions%20over%20multiple%20timesteps%20to%20manipulate%20both%20cross-attention%20maps%20and%20the%20noise%20prediction%20process%2C%20using%20a%20dynamic%20threshold%20to%20generate%20masks%20that%20identify%20text-relevant%20and%20irrelevant%20regions.%20It%20then%20reduces%20attention%20in%20relevant%20areas%20while%20increasing%20it%20in%20irrelevant%20ones%2C%20thereby%20misguides%20the%20edit%20towards%20incorrect%20regions%20and%20preserves%20the%20intended%20targets.%20Additionally%2C%20our%20method%20maximizes%20the%20discrepancy%20between%20the%20injected%20noise%20and%20the%20model%27s%20predicted%20noise%20to%20further%20interfere%20with%20the%20generation.%20By%20targeting%20both%20attention%20and%20noise%20prediction%20mechanisms%2C%20DANP%20exhibits%20impressive%20immunity%20against%20malicious%20edits%2C%20and%20extensive%20experiments%20confirm%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Attention%2520Guided%2520Defense%2520Against%2520Malicious%2520Edits%26entry.906535625%3DJie%2520Zhang%2520and%2520Shuai%2520Dong%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3DRecent%2520progress%2520in%2520text-to-image%2520diffusion%2520models%2520has%2520transformed%2520image%2520editing%2520via%2520text%2520prompts%252C%2520yet%2520this%2520also%2520introduces%2520significant%2520ethical%2520challenges%2520from%2520potential%2520misuse%2520in%2520creating%2520deceptive%2520or%2520harmful%2520content.%2520While%2520current%2520defenses%2520seek%2520to%2520mitigate%2520this%2520risk%2520by%2520embedding%2520imperceptible%2520perturbations%252C%2520their%2520effectiveness%2520is%2520limited%2520against%2520malicious%2520tampering.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Dual%2520Attention-Guided%2520Noise%2520Perturbation%2520%2528DANP%2529%2520immunization%2520method%2520that%2520adds%2520imperceptible%2520perturbations%2520to%2520disrupt%2520the%2520model%2527s%2520semantic%2520understanding%2520and%2520generation%2520process.%2520DANP%2520functions%2520over%2520multiple%2520timesteps%2520to%2520manipulate%2520both%2520cross-attention%2520maps%2520and%2520the%2520noise%2520prediction%2520process%252C%2520using%2520a%2520dynamic%2520threshold%2520to%2520generate%2520masks%2520that%2520identify%2520text-relevant%2520and%2520irrelevant%2520regions.%2520It%2520then%2520reduces%2520attention%2520in%2520relevant%2520areas%2520while%2520increasing%2520it%2520in%2520irrelevant%2520ones%252C%2520thereby%2520misguides%2520the%2520edit%2520towards%2520incorrect%2520regions%2520and%2520preserves%2520the%2520intended%2520targets.%2520Additionally%252C%2520our%2520method%2520maximizes%2520the%2520discrepancy%2520between%2520the%2520injected%2520noise%2520and%2520the%2520model%2527s%2520predicted%2520noise%2520to%2520further%2520interfere%2520with%2520the%2520generation.%2520By%2520targeting%2520both%2520attention%2520and%2520noise%2520prediction%2520mechanisms%252C%2520DANP%2520exhibits%2520impressive%2520immunity%2520against%2520malicious%2520edits%252C%2520and%2520extensive%2520experiments%2520confirm%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Attention%20Guided%20Defense%20Against%20Malicious%20Edits&entry.906535625=Jie%20Zhang%20and%20Shuai%20Dong%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=Recent%20progress%20in%20text-to-image%20diffusion%20models%20has%20transformed%20image%20editing%20via%20text%20prompts%2C%20yet%20this%20also%20introduces%20significant%20ethical%20challenges%20from%20potential%20misuse%20in%20creating%20deceptive%20or%20harmful%20content.%20While%20current%20defenses%20seek%20to%20mitigate%20this%20risk%20by%20embedding%20imperceptible%20perturbations%2C%20their%20effectiveness%20is%20limited%20against%20malicious%20tampering.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Dual%20Attention-Guided%20Noise%20Perturbation%20%28DANP%29%20immunization%20method%20that%20adds%20imperceptible%20perturbations%20to%20disrupt%20the%20model%27s%20semantic%20understanding%20and%20generation%20process.%20DANP%20functions%20over%20multiple%20timesteps%20to%20manipulate%20both%20cross-attention%20maps%20and%20the%20noise%20prediction%20process%2C%20using%20a%20dynamic%20threshold%20to%20generate%20masks%20that%20identify%20text-relevant%20and%20irrelevant%20regions.%20It%20then%20reduces%20attention%20in%20relevant%20areas%20while%20increasing%20it%20in%20irrelevant%20ones%2C%20thereby%20misguides%20the%20edit%20towards%20incorrect%20regions%20and%20preserves%20the%20intended%20targets.%20Additionally%2C%20our%20method%20maximizes%20the%20discrepancy%20between%20the%20injected%20noise%20and%20the%20model%27s%20predicted%20noise%20to%20further%20interfere%20with%20the%20generation.%20By%20targeting%20both%20attention%20and%20noise%20prediction%20mechanisms%2C%20DANP%20exhibits%20impressive%20immunity%20against%20malicious%20edits%2C%20and%20extensive%20experiments%20confirm%20that%20our%20method%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.14333v1&entry.124074799=Read"},
{"title": "Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization", "author": "Henrik Hose and Paul Brunzema and Alexander von Rohr and Alexander Gr\u00e4fe and Angela P. Schoellig and Sebastian Trimpe", "abstract": "Approximate model-predictive control (AMPC) aims to imitate an MPC's behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC's optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.", "link": "http://arxiv.org/abs/2512.14350v1", "date": "2025-12-16", "relevancy": 2.0951, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5602}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5338}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20of%20Neural%20Network%20Approximate%20MPC%20without%20Retraining%20via%20Bayesian%20Optimization&body=Title%3A%20Fine-Tuning%20of%20Neural%20Network%20Approximate%20MPC%20without%20Retraining%20via%20Bayesian%20Optimization%0AAuthor%3A%20Henrik%20Hose%20and%20Paul%20Brunzema%20and%20Alexander%20von%20Rohr%20and%20Alexander%20Gr%C3%A4fe%20and%20Angela%20P.%20Schoellig%20and%20Sebastian%20Trimpe%0AAbstract%3A%20Approximate%20model-predictive%20control%20%28AMPC%29%20aims%20to%20imitate%20an%20MPC%27s%20behavior%20with%20a%20neural%20network%2C%20removing%20the%20need%20to%20solve%20an%20expensive%20optimization%20problem%20at%20runtime.%20However%2C%20during%20deployment%2C%20the%20parameters%20of%20the%20underlying%20MPC%20must%20usually%20be%20fine-tuned.%20This%20often%20renders%20AMPC%20impractical%20as%20it%20requires%20repeatedly%20generating%20a%20new%20dataset%20and%20retraining%20the%20neural%20network.%20Recent%20work%20addresses%20this%20problem%20by%20adapting%20AMPC%20without%20retraining%20using%20approximated%20sensitivities%20of%20the%20MPC%27s%20optimization%20problem.%20Currently%2C%20this%20adaption%20must%20be%20done%20by%20hand%2C%20which%20is%20labor-intensive%20and%20can%20be%20unintuitive%20for%20high-dimensional%20systems.%20To%20solve%20this%20issue%2C%20we%20propose%20using%20Bayesian%20optimization%20to%20tune%20the%20parameters%20of%20AMPC%20policies%20based%20on%20experimental%20data.%20By%20combining%20model-based%20control%20with%20direct%20and%20local%20learning%2C%20our%20approach%20achieves%20superior%20performance%20to%20nominal%20AMPC%20on%20hardware%2C%20with%20minimal%20experimentation.%20This%20allows%20automatic%20and%20data-efficient%20adaptation%20of%20AMPC%20to%20new%20system%20instances%20and%20fine-tuning%20to%20cost%20functions%20that%20are%20difficult%20to%20directly%20implement%20in%20MPC.%20We%20demonstrate%20the%20proposed%20method%20in%20hardware%20experiments%20for%20the%20swing-up%20maneuver%20on%20an%20inverted%20cartpole%20and%20yaw%20control%20of%20an%20under-actuated%20balancing%20unicycle%20robot%2C%20a%20challenging%20control%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuning%2520of%2520Neural%2520Network%2520Approximate%2520MPC%2520without%2520Retraining%2520via%2520Bayesian%2520Optimization%26entry.906535625%3DHenrik%2520Hose%2520and%2520Paul%2520Brunzema%2520and%2520Alexander%2520von%2520Rohr%2520and%2520Alexander%2520Gr%25C3%25A4fe%2520and%2520Angela%2520P.%2520Schoellig%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3DApproximate%2520model-predictive%2520control%2520%2528AMPC%2529%2520aims%2520to%2520imitate%2520an%2520MPC%2527s%2520behavior%2520with%2520a%2520neural%2520network%252C%2520removing%2520the%2520need%2520to%2520solve%2520an%2520expensive%2520optimization%2520problem%2520at%2520runtime.%2520However%252C%2520during%2520deployment%252C%2520the%2520parameters%2520of%2520the%2520underlying%2520MPC%2520must%2520usually%2520be%2520fine-tuned.%2520This%2520often%2520renders%2520AMPC%2520impractical%2520as%2520it%2520requires%2520repeatedly%2520generating%2520a%2520new%2520dataset%2520and%2520retraining%2520the%2520neural%2520network.%2520Recent%2520work%2520addresses%2520this%2520problem%2520by%2520adapting%2520AMPC%2520without%2520retraining%2520using%2520approximated%2520sensitivities%2520of%2520the%2520MPC%2527s%2520optimization%2520problem.%2520Currently%252C%2520this%2520adaption%2520must%2520be%2520done%2520by%2520hand%252C%2520which%2520is%2520labor-intensive%2520and%2520can%2520be%2520unintuitive%2520for%2520high-dimensional%2520systems.%2520To%2520solve%2520this%2520issue%252C%2520we%2520propose%2520using%2520Bayesian%2520optimization%2520to%2520tune%2520the%2520parameters%2520of%2520AMPC%2520policies%2520based%2520on%2520experimental%2520data.%2520By%2520combining%2520model-based%2520control%2520with%2520direct%2520and%2520local%2520learning%252C%2520our%2520approach%2520achieves%2520superior%2520performance%2520to%2520nominal%2520AMPC%2520on%2520hardware%252C%2520with%2520minimal%2520experimentation.%2520This%2520allows%2520automatic%2520and%2520data-efficient%2520adaptation%2520of%2520AMPC%2520to%2520new%2520system%2520instances%2520and%2520fine-tuning%2520to%2520cost%2520functions%2520that%2520are%2520difficult%2520to%2520directly%2520implement%2520in%2520MPC.%2520We%2520demonstrate%2520the%2520proposed%2520method%2520in%2520hardware%2520experiments%2520for%2520the%2520swing-up%2520maneuver%2520on%2520an%2520inverted%2520cartpole%2520and%2520yaw%2520control%2520of%2520an%2520under-actuated%2520balancing%2520unicycle%2520robot%252C%2520a%2520challenging%2520control%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20of%20Neural%20Network%20Approximate%20MPC%20without%20Retraining%20via%20Bayesian%20Optimization&entry.906535625=Henrik%20Hose%20and%20Paul%20Brunzema%20and%20Alexander%20von%20Rohr%20and%20Alexander%20Gr%C3%A4fe%20and%20Angela%20P.%20Schoellig%20and%20Sebastian%20Trimpe&entry.1292438233=Approximate%20model-predictive%20control%20%28AMPC%29%20aims%20to%20imitate%20an%20MPC%27s%20behavior%20with%20a%20neural%20network%2C%20removing%20the%20need%20to%20solve%20an%20expensive%20optimization%20problem%20at%20runtime.%20However%2C%20during%20deployment%2C%20the%20parameters%20of%20the%20underlying%20MPC%20must%20usually%20be%20fine-tuned.%20This%20often%20renders%20AMPC%20impractical%20as%20it%20requires%20repeatedly%20generating%20a%20new%20dataset%20and%20retraining%20the%20neural%20network.%20Recent%20work%20addresses%20this%20problem%20by%20adapting%20AMPC%20without%20retraining%20using%20approximated%20sensitivities%20of%20the%20MPC%27s%20optimization%20problem.%20Currently%2C%20this%20adaption%20must%20be%20done%20by%20hand%2C%20which%20is%20labor-intensive%20and%20can%20be%20unintuitive%20for%20high-dimensional%20systems.%20To%20solve%20this%20issue%2C%20we%20propose%20using%20Bayesian%20optimization%20to%20tune%20the%20parameters%20of%20AMPC%20policies%20based%20on%20experimental%20data.%20By%20combining%20model-based%20control%20with%20direct%20and%20local%20learning%2C%20our%20approach%20achieves%20superior%20performance%20to%20nominal%20AMPC%20on%20hardware%2C%20with%20minimal%20experimentation.%20This%20allows%20automatic%20and%20data-efficient%20adaptation%20of%20AMPC%20to%20new%20system%20instances%20and%20fine-tuning%20to%20cost%20functions%20that%20are%20difficult%20to%20directly%20implement%20in%20MPC.%20We%20demonstrate%20the%20proposed%20method%20in%20hardware%20experiments%20for%20the%20swing-up%20maneuver%20on%20an%20inverted%20cartpole%20and%20yaw%20control%20of%20an%20under-actuated%20balancing%20unicycle%20robot%2C%20a%20challenging%20control%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2512.14350v1&entry.124074799=Read"},
{"title": "Physically consistent model learning for reaction-diffusion systems", "author": "Erion Morina and Martin Holler", "abstract": "This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.", "link": "http://arxiv.org/abs/2512.14240v1", "date": "2025-12-16", "relevancy": 1.9233, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5034}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.478}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20consistent%20model%20learning%20for%20reaction-diffusion%20systems&body=Title%3A%20Physically%20consistent%20model%20learning%20for%20reaction-diffusion%20systems%0AAuthor%3A%20Erion%20Morina%20and%20Martin%20Holler%0AAbstract%3A%20This%20paper%20addresses%20the%20problem%20of%20learning%20reaction-diffusion%20%28RD%29%20systems%20from%20data%20while%20ensuring%20physical%20consistency%20and%20well-posedness%20of%20the%20learned%20models.%20Building%20on%20a%20regularization-based%20framework%20for%20structured%20model%20learning%2C%20we%20focus%20on%20learning%20parameterized%20reaction%20terms%20and%20investigate%20how%20to%20incorporate%20key%20physical%20properties%2C%20such%20as%20mass%20conservation%20and%20quasipositivity%2C%20directly%20into%20the%20learning%20process.%20Our%20main%20contributions%20are%20twofold%3A%20First%2C%20we%20propose%20techniques%20to%20systematically%20modify%20a%20given%20class%20of%20parameterized%20reaction%20terms%20such%20that%20the%20resulting%20terms%20inherently%20satisfy%20mass%20conservation%20and%20quasipositivity%2C%20ensuring%20that%20the%20learned%20RD%20systems%20preserve%20non-negativity%20and%20adhere%20to%20physical%20principles.%20These%20modifications%20also%20guarantee%20well-posedness%20of%20the%20resulting%20PDEs%20under%20additional%20regularity%20and%20growth%20conditions.%20Second%2C%20we%20extend%20existing%20theoretical%20results%20on%20regularization-based%20model%20learning%20to%20RD%20systems%20using%20these%20physically%20consistent%20reaction%20terms.%20Specifically%2C%20we%20prove%20that%20solutions%20to%20the%20learning%20problem%20converge%20to%20a%20unique%2C%20regularization-minimizing%20solution%20of%20a%20limit%20system%20even%20when%20conservation%20laws%20and%20quasipositivity%20are%20enforced.%20In%20addition%2C%20we%20provide%20approximation%20results%20for%20quasipositive%20functions%2C%20essential%20for%20constructing%20physically%20consistent%20parameterizations.%20These%20results%20advance%20the%20development%20of%20interpretable%20and%20reliable%20data-driven%20models%20for%20RD%20systems%20that%20align%20with%20fundamental%20physical%20laws.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520consistent%2520model%2520learning%2520for%2520reaction-diffusion%2520systems%26entry.906535625%3DErion%2520Morina%2520and%2520Martin%2520Holler%26entry.1292438233%3DThis%2520paper%2520addresses%2520the%2520problem%2520of%2520learning%2520reaction-diffusion%2520%2528RD%2529%2520systems%2520from%2520data%2520while%2520ensuring%2520physical%2520consistency%2520and%2520well-posedness%2520of%2520the%2520learned%2520models.%2520Building%2520on%2520a%2520regularization-based%2520framework%2520for%2520structured%2520model%2520learning%252C%2520we%2520focus%2520on%2520learning%2520parameterized%2520reaction%2520terms%2520and%2520investigate%2520how%2520to%2520incorporate%2520key%2520physical%2520properties%252C%2520such%2520as%2520mass%2520conservation%2520and%2520quasipositivity%252C%2520directly%2520into%2520the%2520learning%2520process.%2520Our%2520main%2520contributions%2520are%2520twofold%253A%2520First%252C%2520we%2520propose%2520techniques%2520to%2520systematically%2520modify%2520a%2520given%2520class%2520of%2520parameterized%2520reaction%2520terms%2520such%2520that%2520the%2520resulting%2520terms%2520inherently%2520satisfy%2520mass%2520conservation%2520and%2520quasipositivity%252C%2520ensuring%2520that%2520the%2520learned%2520RD%2520systems%2520preserve%2520non-negativity%2520and%2520adhere%2520to%2520physical%2520principles.%2520These%2520modifications%2520also%2520guarantee%2520well-posedness%2520of%2520the%2520resulting%2520PDEs%2520under%2520additional%2520regularity%2520and%2520growth%2520conditions.%2520Second%252C%2520we%2520extend%2520existing%2520theoretical%2520results%2520on%2520regularization-based%2520model%2520learning%2520to%2520RD%2520systems%2520using%2520these%2520physically%2520consistent%2520reaction%2520terms.%2520Specifically%252C%2520we%2520prove%2520that%2520solutions%2520to%2520the%2520learning%2520problem%2520converge%2520to%2520a%2520unique%252C%2520regularization-minimizing%2520solution%2520of%2520a%2520limit%2520system%2520even%2520when%2520conservation%2520laws%2520and%2520quasipositivity%2520are%2520enforced.%2520In%2520addition%252C%2520we%2520provide%2520approximation%2520results%2520for%2520quasipositive%2520functions%252C%2520essential%2520for%2520constructing%2520physically%2520consistent%2520parameterizations.%2520These%2520results%2520advance%2520the%2520development%2520of%2520interpretable%2520and%2520reliable%2520data-driven%2520models%2520for%2520RD%2520systems%2520that%2520align%2520with%2520fundamental%2520physical%2520laws.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20consistent%20model%20learning%20for%20reaction-diffusion%20systems&entry.906535625=Erion%20Morina%20and%20Martin%20Holler&entry.1292438233=This%20paper%20addresses%20the%20problem%20of%20learning%20reaction-diffusion%20%28RD%29%20systems%20from%20data%20while%20ensuring%20physical%20consistency%20and%20well-posedness%20of%20the%20learned%20models.%20Building%20on%20a%20regularization-based%20framework%20for%20structured%20model%20learning%2C%20we%20focus%20on%20learning%20parameterized%20reaction%20terms%20and%20investigate%20how%20to%20incorporate%20key%20physical%20properties%2C%20such%20as%20mass%20conservation%20and%20quasipositivity%2C%20directly%20into%20the%20learning%20process.%20Our%20main%20contributions%20are%20twofold%3A%20First%2C%20we%20propose%20techniques%20to%20systematically%20modify%20a%20given%20class%20of%20parameterized%20reaction%20terms%20such%20that%20the%20resulting%20terms%20inherently%20satisfy%20mass%20conservation%20and%20quasipositivity%2C%20ensuring%20that%20the%20learned%20RD%20systems%20preserve%20non-negativity%20and%20adhere%20to%20physical%20principles.%20These%20modifications%20also%20guarantee%20well-posedness%20of%20the%20resulting%20PDEs%20under%20additional%20regularity%20and%20growth%20conditions.%20Second%2C%20we%20extend%20existing%20theoretical%20results%20on%20regularization-based%20model%20learning%20to%20RD%20systems%20using%20these%20physically%20consistent%20reaction%20terms.%20Specifically%2C%20we%20prove%20that%20solutions%20to%20the%20learning%20problem%20converge%20to%20a%20unique%2C%20regularization-minimizing%20solution%20of%20a%20limit%20system%20even%20when%20conservation%20laws%20and%20quasipositivity%20are%20enforced.%20In%20addition%2C%20we%20provide%20approximation%20results%20for%20quasipositive%20functions%2C%20essential%20for%20constructing%20physically%20consistent%20parameterizations.%20These%20results%20advance%20the%20development%20of%20interpretable%20and%20reliable%20data-driven%20models%20for%20RD%20systems%20that%20align%20with%20fundamental%20physical%20laws.&entry.1838667208=http%3A//arxiv.org/abs/2512.14240v1&entry.124074799=Read"},
{"title": "GraphBench: Next-generation graph learning benchmarking", "author": "Timo Stoll and Chendi Qian and Ben Finkelshtein and Ali Parviz and Darius Weber and Fabrizio Frasca and Hadar Shavit and Antoine Siraudin and Arman Mielke and Marie Anastacio and Erik M\u00fcller and Maya Bechler-Speicher and Michael Bronstein and Mikhail Galkin and Holger Hoos and Mathias Niepert and Bryan Perozzi and Jan T\u00f6nshoff and Christopher Morris", "abstract": "Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.", "link": "http://arxiv.org/abs/2512.04475v3", "date": "2025-12-16", "relevancy": 1.9853, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5081}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4888}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphBench%3A%20Next-generation%20graph%20learning%20benchmarking&body=Title%3A%20GraphBench%3A%20Next-generation%20graph%20learning%20benchmarking%0AAuthor%3A%20Timo%20Stoll%20and%20Chendi%20Qian%20and%20Ben%20Finkelshtein%20and%20Ali%20Parviz%20and%20Darius%20Weber%20and%20Fabrizio%20Frasca%20and%20Hadar%20Shavit%20and%20Antoine%20Siraudin%20and%20Arman%20Mielke%20and%20Marie%20Anastacio%20and%20Erik%20M%C3%BCller%20and%20Maya%20Bechler-Speicher%20and%20Michael%20Bronstein%20and%20Mikhail%20Galkin%20and%20Holger%20Hoos%20and%20Mathias%20Niepert%20and%20Bryan%20Perozzi%20and%20Jan%20T%C3%B6nshoff%20and%20Christopher%20Morris%0AAbstract%3A%20Machine%20learning%20on%20graphs%20has%20recently%20achieved%20impressive%20progress%20in%20various%20domains%2C%20including%20molecular%20property%20prediction%20and%20chip%20design.%20However%2C%20benchmarking%20practices%20remain%20fragmented%2C%20often%20relying%20on%20narrow%2C%20task-specific%20datasets%20and%20inconsistent%20evaluation%20protocols%2C%20which%20hampers%20reproducibility%20and%20broader%20progress.%20To%20address%20this%2C%20we%20introduce%20GraphBench%2C%20a%20comprehensive%20benchmarking%20suite%20that%20spans%20diverse%20domains%20and%20prediction%20tasks%2C%20including%20node-level%2C%20edge-level%2C%20graph-level%2C%20and%20generative%20settings.%20GraphBench%20provides%20standardized%20evaluation%20protocols%20--%20with%20consistent%20dataset%20splits%20and%20performance%20metrics%20that%20account%20for%20out-of-distribution%20generalization%20--%20as%20well%20as%20a%20unified%20hyperparameter%20tuning%20framework.%20Additionally%2C%20we%20benchmark%20GraphBench%20using%20message-passing%20neural%20networks%20and%20graph%20transformer%20models%2C%20providing%20principled%20baselines%20and%20establishing%20a%20reference%20performance.%20See%20www.graphbench.io%20for%20further%20details.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04475v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphBench%253A%2520Next-generation%2520graph%2520learning%2520benchmarking%26entry.906535625%3DTimo%2520Stoll%2520and%2520Chendi%2520Qian%2520and%2520Ben%2520Finkelshtein%2520and%2520Ali%2520Parviz%2520and%2520Darius%2520Weber%2520and%2520Fabrizio%2520Frasca%2520and%2520Hadar%2520Shavit%2520and%2520Antoine%2520Siraudin%2520and%2520Arman%2520Mielke%2520and%2520Marie%2520Anastacio%2520and%2520Erik%2520M%25C3%25BCller%2520and%2520Maya%2520Bechler-Speicher%2520and%2520Michael%2520Bronstein%2520and%2520Mikhail%2520Galkin%2520and%2520Holger%2520Hoos%2520and%2520Mathias%2520Niepert%2520and%2520Bryan%2520Perozzi%2520and%2520Jan%2520T%25C3%25B6nshoff%2520and%2520Christopher%2520Morris%26entry.1292438233%3DMachine%2520learning%2520on%2520graphs%2520has%2520recently%2520achieved%2520impressive%2520progress%2520in%2520various%2520domains%252C%2520including%2520molecular%2520property%2520prediction%2520and%2520chip%2520design.%2520However%252C%2520benchmarking%2520practices%2520remain%2520fragmented%252C%2520often%2520relying%2520on%2520narrow%252C%2520task-specific%2520datasets%2520and%2520inconsistent%2520evaluation%2520protocols%252C%2520which%2520hampers%2520reproducibility%2520and%2520broader%2520progress.%2520To%2520address%2520this%252C%2520we%2520introduce%2520GraphBench%252C%2520a%2520comprehensive%2520benchmarking%2520suite%2520that%2520spans%2520diverse%2520domains%2520and%2520prediction%2520tasks%252C%2520including%2520node-level%252C%2520edge-level%252C%2520graph-level%252C%2520and%2520generative%2520settings.%2520GraphBench%2520provides%2520standardized%2520evaluation%2520protocols%2520--%2520with%2520consistent%2520dataset%2520splits%2520and%2520performance%2520metrics%2520that%2520account%2520for%2520out-of-distribution%2520generalization%2520--%2520as%2520well%2520as%2520a%2520unified%2520hyperparameter%2520tuning%2520framework.%2520Additionally%252C%2520we%2520benchmark%2520GraphBench%2520using%2520message-passing%2520neural%2520networks%2520and%2520graph%2520transformer%2520models%252C%2520providing%2520principled%2520baselines%2520and%2520establishing%2520a%2520reference%2520performance.%2520See%2520www.graphbench.io%2520for%2520further%2520details.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04475v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphBench%3A%20Next-generation%20graph%20learning%20benchmarking&entry.906535625=Timo%20Stoll%20and%20Chendi%20Qian%20and%20Ben%20Finkelshtein%20and%20Ali%20Parviz%20and%20Darius%20Weber%20and%20Fabrizio%20Frasca%20and%20Hadar%20Shavit%20and%20Antoine%20Siraudin%20and%20Arman%20Mielke%20and%20Marie%20Anastacio%20and%20Erik%20M%C3%BCller%20and%20Maya%20Bechler-Speicher%20and%20Michael%20Bronstein%20and%20Mikhail%20Galkin%20and%20Holger%20Hoos%20and%20Mathias%20Niepert%20and%20Bryan%20Perozzi%20and%20Jan%20T%C3%B6nshoff%20and%20Christopher%20Morris&entry.1292438233=Machine%20learning%20on%20graphs%20has%20recently%20achieved%20impressive%20progress%20in%20various%20domains%2C%20including%20molecular%20property%20prediction%20and%20chip%20design.%20However%2C%20benchmarking%20practices%20remain%20fragmented%2C%20often%20relying%20on%20narrow%2C%20task-specific%20datasets%20and%20inconsistent%20evaluation%20protocols%2C%20which%20hampers%20reproducibility%20and%20broader%20progress.%20To%20address%20this%2C%20we%20introduce%20GraphBench%2C%20a%20comprehensive%20benchmarking%20suite%20that%20spans%20diverse%20domains%20and%20prediction%20tasks%2C%20including%20node-level%2C%20edge-level%2C%20graph-level%2C%20and%20generative%20settings.%20GraphBench%20provides%20standardized%20evaluation%20protocols%20--%20with%20consistent%20dataset%20splits%20and%20performance%20metrics%20that%20account%20for%20out-of-distribution%20generalization%20--%20as%20well%20as%20a%20unified%20hyperparameter%20tuning%20framework.%20Additionally%2C%20we%20benchmark%20GraphBench%20using%20message-passing%20neural%20networks%20and%20graph%20transformer%20models%2C%20providing%20principled%20baselines%20and%20establishing%20a%20reference%20performance.%20See%20www.graphbench.io%20for%20further%20details.&entry.1838667208=http%3A//arxiv.org/abs/2512.04475v3&entry.124074799=Read"},
{"title": "TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios", "author": "Mengyu Li and Xingcheng Zhou and Guang Chen and Alois Knoll and Hu Cao", "abstract": "In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.", "link": "http://arxiv.org/abs/2512.14595v1", "date": "2025-12-16", "relevancy": 1.997, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5053}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TUMTraf%20EMOT%3A%20Event-Based%20Multi-Object%20Tracking%20Dataset%20and%20Baseline%20for%20Traffic%20Scenarios&body=Title%3A%20TUMTraf%20EMOT%3A%20Event-Based%20Multi-Object%20Tracking%20Dataset%20and%20Baseline%20for%20Traffic%20Scenarios%0AAuthor%3A%20Mengyu%20Li%20and%20Xingcheng%20Zhou%20and%20Guang%20Chen%20and%20Alois%20Knoll%20and%20Hu%20Cao%0AAbstract%3A%20In%20Intelligent%20Transportation%20Systems%20%28ITS%29%2C%20multi-object%20tracking%20is%20primarily%20based%20on%20frame-based%20cameras.%20However%2C%20these%20cameras%20tend%20to%20perform%20poorly%20under%20dim%20lighting%20and%20high-speed%20motion%20conditions.%20Event%20cameras%2C%20characterized%20by%20low%20latency%2C%20high%20dynamic%20range%20and%20high%20temporal%20resolution%2C%20have%20considerable%20potential%20to%20mitigate%20these%20issues.%20Compared%20to%20frame-based%20vision%2C%20there%20are%20far%20fewer%20studies%20on%20event-based%20vision.%20To%20address%20this%20research%20gap%2C%20we%20introduce%20an%20initial%20pilot%20dataset%20tailored%20for%20event-based%20ITS%2C%20covering%20vehicle%20and%20pedestrian%20detection%20and%20tracking.%20We%20establish%20a%20tracking-by-detection%20benchmark%20with%20a%20specialized%20feature%20extractor%20based%20on%20this%20dataset%2C%20achieving%20excellent%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.14595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTUMTraf%2520EMOT%253A%2520Event-Based%2520Multi-Object%2520Tracking%2520Dataset%2520and%2520Baseline%2520for%2520Traffic%2520Scenarios%26entry.906535625%3DMengyu%2520Li%2520and%2520Xingcheng%2520Zhou%2520and%2520Guang%2520Chen%2520and%2520Alois%2520Knoll%2520and%2520Hu%2520Cao%26entry.1292438233%3DIn%2520Intelligent%2520Transportation%2520Systems%2520%2528ITS%2529%252C%2520multi-object%2520tracking%2520is%2520primarily%2520based%2520on%2520frame-based%2520cameras.%2520However%252C%2520these%2520cameras%2520tend%2520to%2520perform%2520poorly%2520under%2520dim%2520lighting%2520and%2520high-speed%2520motion%2520conditions.%2520Event%2520cameras%252C%2520characterized%2520by%2520low%2520latency%252C%2520high%2520dynamic%2520range%2520and%2520high%2520temporal%2520resolution%252C%2520have%2520considerable%2520potential%2520to%2520mitigate%2520these%2520issues.%2520Compared%2520to%2520frame-based%2520vision%252C%2520there%2520are%2520far%2520fewer%2520studies%2520on%2520event-based%2520vision.%2520To%2520address%2520this%2520research%2520gap%252C%2520we%2520introduce%2520an%2520initial%2520pilot%2520dataset%2520tailored%2520for%2520event-based%2520ITS%252C%2520covering%2520vehicle%2520and%2520pedestrian%2520detection%2520and%2520tracking.%2520We%2520establish%2520a%2520tracking-by-detection%2520benchmark%2520with%2520a%2520specialized%2520feature%2520extractor%2520based%2520on%2520this%2520dataset%252C%2520achieving%2520excellent%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.14595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TUMTraf%20EMOT%3A%20Event-Based%20Multi-Object%20Tracking%20Dataset%20and%20Baseline%20for%20Traffic%20Scenarios&entry.906535625=Mengyu%20Li%20and%20Xingcheng%20Zhou%20and%20Guang%20Chen%20and%20Alois%20Knoll%20and%20Hu%20Cao&entry.1292438233=In%20Intelligent%20Transportation%20Systems%20%28ITS%29%2C%20multi-object%20tracking%20is%20primarily%20based%20on%20frame-based%20cameras.%20However%2C%20these%20cameras%20tend%20to%20perform%20poorly%20under%20dim%20lighting%20and%20high-speed%20motion%20conditions.%20Event%20cameras%2C%20characterized%20by%20low%20latency%2C%20high%20dynamic%20range%20and%20high%20temporal%20resolution%2C%20have%20considerable%20potential%20to%20mitigate%20these%20issues.%20Compared%20to%20frame-based%20vision%2C%20there%20are%20far%20fewer%20studies%20on%20event-based%20vision.%20To%20address%20this%20research%20gap%2C%20we%20introduce%20an%20initial%20pilot%20dataset%20tailored%20for%20event-based%20ITS%2C%20covering%20vehicle%20and%20pedestrian%20detection%20and%20tracking.%20We%20establish%20a%20tracking-by-detection%20benchmark%20with%20a%20specialized%20feature%20extractor%20based%20on%20this%20dataset%2C%20achieving%20excellent%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.14595v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


