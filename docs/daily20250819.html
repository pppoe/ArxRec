<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250818.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans\n  Fusion", "author": "Wenhao Hu and Zesheng Li and Haonan Zhou and Liu Liu and Xuexiang Wen and Zhizhong Su and Xi Li and Gaoang Wang", "abstract": "  Reconstructing complete and interactive 3D scenes remains a fundamental\nchallenge in computer vision and robotics, particularly due to persistent\nobject occlusions and limited sensor coverage. Multiview observations from a\nsingle scene scan often fail to capture the full structural details. Existing\napproaches typically rely on multi stage pipelines, such as segmentation,\nbackground completion, and inpainting or require per-object dense scanning,\nboth of which are error-prone, and not easily scalable. We propose IGFuse, a\nnovel framework that reconstructs interactive Gaussian scene by fusing\nobservations from multiple scans, where natural object rearrangement between\ncaptures reveal previously occluded regions. Our method constructs segmentation\naware Gaussian fields and enforces bi-directional photometric and semantic\nconsistency across scans. To handle spatial misalignments, we introduce a\npseudo-intermediate scene state for unified alignment, alongside collaborative\nco-pruning strategies to refine geometry. IGFuse enables high fidelity\nrendering and object level scene manipulation without dense observations or\ncomplex pipelines. Extensive experiments validate the framework's strong\ngeneralization to novel scene configurations, demonstrating its effectiveness\nfor real world 3D reconstruction and real-to-simulation transfer. Our project\npage is available online.\n", "link": "http://arxiv.org/abs/2508.13153v1", "date": "2025-08-18", "relevancy": 3.2225, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6482}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6426}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IGFuse%3A%20Interactive%203D%20Gaussian%20Scene%20Reconstruction%20via%20Multi-Scans%0A%20%20Fusion&body=Title%3A%20IGFuse%3A%20Interactive%203D%20Gaussian%20Scene%20Reconstruction%20via%20Multi-Scans%0A%20%20Fusion%0AAuthor%3A%20Wenhao%20Hu%20and%20Zesheng%20Li%20and%20Haonan%20Zhou%20and%20Liu%20Liu%20and%20Xuexiang%20Wen%20and%20Zhizhong%20Su%20and%20Xi%20Li%20and%20Gaoang%20Wang%0AAbstract%3A%20%20%20Reconstructing%20complete%20and%20interactive%203D%20scenes%20remains%20a%20fundamental%0Achallenge%20in%20computer%20vision%20and%20robotics%2C%20particularly%20due%20to%20persistent%0Aobject%20occlusions%20and%20limited%20sensor%20coverage.%20Multiview%20observations%20from%20a%0Asingle%20scene%20scan%20often%20fail%20to%20capture%20the%20full%20structural%20details.%20Existing%0Aapproaches%20typically%20rely%20on%20multi%20stage%20pipelines%2C%20such%20as%20segmentation%2C%0Abackground%20completion%2C%20and%20inpainting%20or%20require%20per-object%20dense%20scanning%2C%0Aboth%20of%20which%20are%20error-prone%2C%20and%20not%20easily%20scalable.%20We%20propose%20IGFuse%2C%20a%0Anovel%20framework%20that%20reconstructs%20interactive%20Gaussian%20scene%20by%20fusing%0Aobservations%20from%20multiple%20scans%2C%20where%20natural%20object%20rearrangement%20between%0Acaptures%20reveal%20previously%20occluded%20regions.%20Our%20method%20constructs%20segmentation%0Aaware%20Gaussian%20fields%20and%20enforces%20bi-directional%20photometric%20and%20semantic%0Aconsistency%20across%20scans.%20To%20handle%20spatial%20misalignments%2C%20we%20introduce%20a%0Apseudo-intermediate%20scene%20state%20for%20unified%20alignment%2C%20alongside%20collaborative%0Aco-pruning%20strategies%20to%20refine%20geometry.%20IGFuse%20enables%20high%20fidelity%0Arendering%20and%20object%20level%20scene%20manipulation%20without%20dense%20observations%20or%0Acomplex%20pipelines.%20Extensive%20experiments%20validate%20the%20framework%27s%20strong%0Ageneralization%20to%20novel%20scene%20configurations%2C%20demonstrating%20its%20effectiveness%0Afor%20real%20world%203D%20reconstruction%20and%20real-to-simulation%20transfer.%20Our%20project%0Apage%20is%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIGFuse%253A%2520Interactive%25203D%2520Gaussian%2520Scene%2520Reconstruction%2520via%2520Multi-Scans%250A%2520%2520Fusion%26entry.906535625%3DWenhao%2520Hu%2520and%2520Zesheng%2520Li%2520and%2520Haonan%2520Zhou%2520and%2520Liu%2520Liu%2520and%2520Xuexiang%2520Wen%2520and%2520Zhizhong%2520Su%2520and%2520Xi%2520Li%2520and%2520Gaoang%2520Wang%26entry.1292438233%3D%2520%2520Reconstructing%2520complete%2520and%2520interactive%25203D%2520scenes%2520remains%2520a%2520fundamental%250Achallenge%2520in%2520computer%2520vision%2520and%2520robotics%252C%2520particularly%2520due%2520to%2520persistent%250Aobject%2520occlusions%2520and%2520limited%2520sensor%2520coverage.%2520Multiview%2520observations%2520from%2520a%250Asingle%2520scene%2520scan%2520often%2520fail%2520to%2520capture%2520the%2520full%2520structural%2520details.%2520Existing%250Aapproaches%2520typically%2520rely%2520on%2520multi%2520stage%2520pipelines%252C%2520such%2520as%2520segmentation%252C%250Abackground%2520completion%252C%2520and%2520inpainting%2520or%2520require%2520per-object%2520dense%2520scanning%252C%250Aboth%2520of%2520which%2520are%2520error-prone%252C%2520and%2520not%2520easily%2520scalable.%2520We%2520propose%2520IGFuse%252C%2520a%250Anovel%2520framework%2520that%2520reconstructs%2520interactive%2520Gaussian%2520scene%2520by%2520fusing%250Aobservations%2520from%2520multiple%2520scans%252C%2520where%2520natural%2520object%2520rearrangement%2520between%250Acaptures%2520reveal%2520previously%2520occluded%2520regions.%2520Our%2520method%2520constructs%2520segmentation%250Aaware%2520Gaussian%2520fields%2520and%2520enforces%2520bi-directional%2520photometric%2520and%2520semantic%250Aconsistency%2520across%2520scans.%2520To%2520handle%2520spatial%2520misalignments%252C%2520we%2520introduce%2520a%250Apseudo-intermediate%2520scene%2520state%2520for%2520unified%2520alignment%252C%2520alongside%2520collaborative%250Aco-pruning%2520strategies%2520to%2520refine%2520geometry.%2520IGFuse%2520enables%2520high%2520fidelity%250Arendering%2520and%2520object%2520level%2520scene%2520manipulation%2520without%2520dense%2520observations%2520or%250Acomplex%2520pipelines.%2520Extensive%2520experiments%2520validate%2520the%2520framework%2527s%2520strong%250Ageneralization%2520to%2520novel%2520scene%2520configurations%252C%2520demonstrating%2520its%2520effectiveness%250Afor%2520real%2520world%25203D%2520reconstruction%2520and%2520real-to-simulation%2520transfer.%2520Our%2520project%250Apage%2520is%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IGFuse%3A%20Interactive%203D%20Gaussian%20Scene%20Reconstruction%20via%20Multi-Scans%0A%20%20Fusion&entry.906535625=Wenhao%20Hu%20and%20Zesheng%20Li%20and%20Haonan%20Zhou%20and%20Liu%20Liu%20and%20Xuexiang%20Wen%20and%20Zhizhong%20Su%20and%20Xi%20Li%20and%20Gaoang%20Wang&entry.1292438233=%20%20Reconstructing%20complete%20and%20interactive%203D%20scenes%20remains%20a%20fundamental%0Achallenge%20in%20computer%20vision%20and%20robotics%2C%20particularly%20due%20to%20persistent%0Aobject%20occlusions%20and%20limited%20sensor%20coverage.%20Multiview%20observations%20from%20a%0Asingle%20scene%20scan%20often%20fail%20to%20capture%20the%20full%20structural%20details.%20Existing%0Aapproaches%20typically%20rely%20on%20multi%20stage%20pipelines%2C%20such%20as%20segmentation%2C%0Abackground%20completion%2C%20and%20inpainting%20or%20require%20per-object%20dense%20scanning%2C%0Aboth%20of%20which%20are%20error-prone%2C%20and%20not%20easily%20scalable.%20We%20propose%20IGFuse%2C%20a%0Anovel%20framework%20that%20reconstructs%20interactive%20Gaussian%20scene%20by%20fusing%0Aobservations%20from%20multiple%20scans%2C%20where%20natural%20object%20rearrangement%20between%0Acaptures%20reveal%20previously%20occluded%20regions.%20Our%20method%20constructs%20segmentation%0Aaware%20Gaussian%20fields%20and%20enforces%20bi-directional%20photometric%20and%20semantic%0Aconsistency%20across%20scans.%20To%20handle%20spatial%20misalignments%2C%20we%20introduce%20a%0Apseudo-intermediate%20scene%20state%20for%20unified%20alignment%2C%20alongside%20collaborative%0Aco-pruning%20strategies%20to%20refine%20geometry.%20IGFuse%20enables%20high%20fidelity%0Arendering%20and%20object%20level%20scene%20manipulation%20without%20dense%20observations%20or%0Acomplex%20pipelines.%20Extensive%20experiments%20validate%20the%20framework%27s%20strong%0Ageneralization%20to%20novel%20scene%20configurations%2C%20demonstrating%20its%20effectiveness%0Afor%20real%20world%203D%20reconstruction%20and%20real-to-simulation%20transfer.%20Our%20project%0Apage%20is%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13153v1&entry.124074799=Read"},
{"title": "Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping", "author": "Siddharth Khandelwal and Sridhar Kamath and Arjun Jain", "abstract": "  Human shape editing enables controllable transformation of a person's body\nshape, such as thin, muscular, or overweight, while preserving pose, identity,\nclothing, and background. Unlike human pose editing, which has advanced\nrapidly, shape editing remains relatively underexplored. Current approaches\ntypically rely on 3D morphable models or image warping, often introducing\nunrealistic body proportions, texture distortions, and background\ninconsistencies due to alignment errors and deformations. A key limitation is\nthe lack of large-scale, publicly available datasets for training and\nevaluating body shape manipulation methods. In this work, we introduce the\nfirst large-scale dataset of 18,573 images across 1523 subjects, specifically\ndesigned for controlled human shape editing. It features diverse variations in\nbody shape, including fat, muscular and thin, captured under consistent\nidentity, clothing, and background conditions. Using this dataset, we propose\nOdo, an end-to-end diffusion-based method that enables realistic and intuitive\nbody reshaping guided by simple semantic attributes. Our approach combines a\nfrozen UNet that preserves fine-grained appearance and background details from\nthe input image with a ControlNet that guides shape transformation using target\nSMPL depth maps. Extensive experiments demonstrate that our method outperforms\nprior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,\nsignificantly lower than the 13.6mm observed in baseline methods, while\nproducing realistic results that accurately match the desired target shapes.\n", "link": "http://arxiv.org/abs/2508.13065v1", "date": "2025-08-18", "relevancy": 3.1292, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6569}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6245}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Odo%3A%20Depth-Guided%20Diffusion%20for%20Identity-Preserving%20Body%20Reshaping&body=Title%3A%20Odo%3A%20Depth-Guided%20Diffusion%20for%20Identity-Preserving%20Body%20Reshaping%0AAuthor%3A%20Siddharth%20Khandelwal%20and%20Sridhar%20Kamath%20and%20Arjun%20Jain%0AAbstract%3A%20%20%20Human%20shape%20editing%20enables%20controllable%20transformation%20of%20a%20person%27s%20body%0Ashape%2C%20such%20as%20thin%2C%20muscular%2C%20or%20overweight%2C%20while%20preserving%20pose%2C%20identity%2C%0Aclothing%2C%20and%20background.%20Unlike%20human%20pose%20editing%2C%20which%20has%20advanced%0Arapidly%2C%20shape%20editing%20remains%20relatively%20underexplored.%20Current%20approaches%0Atypically%20rely%20on%203D%20morphable%20models%20or%20image%20warping%2C%20often%20introducing%0Aunrealistic%20body%20proportions%2C%20texture%20distortions%2C%20and%20background%0Ainconsistencies%20due%20to%20alignment%20errors%20and%20deformations.%20A%20key%20limitation%20is%0Athe%20lack%20of%20large-scale%2C%20publicly%20available%20datasets%20for%20training%20and%0Aevaluating%20body%20shape%20manipulation%20methods.%20In%20this%20work%2C%20we%20introduce%20the%0Afirst%20large-scale%20dataset%20of%2018%2C573%20images%20across%201523%20subjects%2C%20specifically%0Adesigned%20for%20controlled%20human%20shape%20editing.%20It%20features%20diverse%20variations%20in%0Abody%20shape%2C%20including%20fat%2C%20muscular%20and%20thin%2C%20captured%20under%20consistent%0Aidentity%2C%20clothing%2C%20and%20background%20conditions.%20Using%20this%20dataset%2C%20we%20propose%0AOdo%2C%20an%20end-to-end%20diffusion-based%20method%20that%20enables%20realistic%20and%20intuitive%0Abody%20reshaping%20guided%20by%20simple%20semantic%20attributes.%20Our%20approach%20combines%20a%0Afrozen%20UNet%20that%20preserves%20fine-grained%20appearance%20and%20background%20details%20from%0Athe%20input%20image%20with%20a%20ControlNet%20that%20guides%20shape%20transformation%20using%20target%0ASMPL%20depth%20maps.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Aprior%20approaches%2C%20achieving%20per-vertex%20reconstruction%20errors%20as%20low%20as%207.5mm%2C%0Asignificantly%20lower%20than%20the%2013.6mm%20observed%20in%20baseline%20methods%2C%20while%0Aproducing%20realistic%20results%20that%20accurately%20match%20the%20desired%20target%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOdo%253A%2520Depth-Guided%2520Diffusion%2520for%2520Identity-Preserving%2520Body%2520Reshaping%26entry.906535625%3DSiddharth%2520Khandelwal%2520and%2520Sridhar%2520Kamath%2520and%2520Arjun%2520Jain%26entry.1292438233%3D%2520%2520Human%2520shape%2520editing%2520enables%2520controllable%2520transformation%2520of%2520a%2520person%2527s%2520body%250Ashape%252C%2520such%2520as%2520thin%252C%2520muscular%252C%2520or%2520overweight%252C%2520while%2520preserving%2520pose%252C%2520identity%252C%250Aclothing%252C%2520and%2520background.%2520Unlike%2520human%2520pose%2520editing%252C%2520which%2520has%2520advanced%250Arapidly%252C%2520shape%2520editing%2520remains%2520relatively%2520underexplored.%2520Current%2520approaches%250Atypically%2520rely%2520on%25203D%2520morphable%2520models%2520or%2520image%2520warping%252C%2520often%2520introducing%250Aunrealistic%2520body%2520proportions%252C%2520texture%2520distortions%252C%2520and%2520background%250Ainconsistencies%2520due%2520to%2520alignment%2520errors%2520and%2520deformations.%2520A%2520key%2520limitation%2520is%250Athe%2520lack%2520of%2520large-scale%252C%2520publicly%2520available%2520datasets%2520for%2520training%2520and%250Aevaluating%2520body%2520shape%2520manipulation%2520methods.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%250Afirst%2520large-scale%2520dataset%2520of%252018%252C573%2520images%2520across%25201523%2520subjects%252C%2520specifically%250Adesigned%2520for%2520controlled%2520human%2520shape%2520editing.%2520It%2520features%2520diverse%2520variations%2520in%250Abody%2520shape%252C%2520including%2520fat%252C%2520muscular%2520and%2520thin%252C%2520captured%2520under%2520consistent%250Aidentity%252C%2520clothing%252C%2520and%2520background%2520conditions.%2520Using%2520this%2520dataset%252C%2520we%2520propose%250AOdo%252C%2520an%2520end-to-end%2520diffusion-based%2520method%2520that%2520enables%2520realistic%2520and%2520intuitive%250Abody%2520reshaping%2520guided%2520by%2520simple%2520semantic%2520attributes.%2520Our%2520approach%2520combines%2520a%250Afrozen%2520UNet%2520that%2520preserves%2520fine-grained%2520appearance%2520and%2520background%2520details%2520from%250Athe%2520input%2520image%2520with%2520a%2520ControlNet%2520that%2520guides%2520shape%2520transformation%2520using%2520target%250ASMPL%2520depth%2520maps.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%250Aprior%2520approaches%252C%2520achieving%2520per-vertex%2520reconstruction%2520errors%2520as%2520low%2520as%25207.5mm%252C%250Asignificantly%2520lower%2520than%2520the%252013.6mm%2520observed%2520in%2520baseline%2520methods%252C%2520while%250Aproducing%2520realistic%2520results%2520that%2520accurately%2520match%2520the%2520desired%2520target%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Odo%3A%20Depth-Guided%20Diffusion%20for%20Identity-Preserving%20Body%20Reshaping&entry.906535625=Siddharth%20Khandelwal%20and%20Sridhar%20Kamath%20and%20Arjun%20Jain&entry.1292438233=%20%20Human%20shape%20editing%20enables%20controllable%20transformation%20of%20a%20person%27s%20body%0Ashape%2C%20such%20as%20thin%2C%20muscular%2C%20or%20overweight%2C%20while%20preserving%20pose%2C%20identity%2C%0Aclothing%2C%20and%20background.%20Unlike%20human%20pose%20editing%2C%20which%20has%20advanced%0Arapidly%2C%20shape%20editing%20remains%20relatively%20underexplored.%20Current%20approaches%0Atypically%20rely%20on%203D%20morphable%20models%20or%20image%20warping%2C%20often%20introducing%0Aunrealistic%20body%20proportions%2C%20texture%20distortions%2C%20and%20background%0Ainconsistencies%20due%20to%20alignment%20errors%20and%20deformations.%20A%20key%20limitation%20is%0Athe%20lack%20of%20large-scale%2C%20publicly%20available%20datasets%20for%20training%20and%0Aevaluating%20body%20shape%20manipulation%20methods.%20In%20this%20work%2C%20we%20introduce%20the%0Afirst%20large-scale%20dataset%20of%2018%2C573%20images%20across%201523%20subjects%2C%20specifically%0Adesigned%20for%20controlled%20human%20shape%20editing.%20It%20features%20diverse%20variations%20in%0Abody%20shape%2C%20including%20fat%2C%20muscular%20and%20thin%2C%20captured%20under%20consistent%0Aidentity%2C%20clothing%2C%20and%20background%20conditions.%20Using%20this%20dataset%2C%20we%20propose%0AOdo%2C%20an%20end-to-end%20diffusion-based%20method%20that%20enables%20realistic%20and%20intuitive%0Abody%20reshaping%20guided%20by%20simple%20semantic%20attributes.%20Our%20approach%20combines%20a%0Afrozen%20UNet%20that%20preserves%20fine-grained%20appearance%20and%20background%20details%20from%0Athe%20input%20image%20with%20a%20ControlNet%20that%20guides%20shape%20transformation%20using%20target%0ASMPL%20depth%20maps.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%0Aprior%20approaches%2C%20achieving%20per-vertex%20reconstruction%20errors%20as%20low%20as%207.5mm%2C%0Asignificantly%20lower%20than%20the%2013.6mm%20observed%20in%20baseline%20methods%2C%20while%0Aproducing%20realistic%20results%20that%20accurately%20match%20the%20desired%20target%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13065v1&entry.124074799=Read"},
{"title": "Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language\n  Models for Few-Shot Learning", "author": "Dexia Chen and Qianjie Zhu and Weibing Li and Yue Yu and Tong Zhang and Ruixuan Wang", "abstract": "  Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable\npotential in few-shot image classification and led to numerous effective\ntransfer learning strategies. These methods leverage the pretrained knowledge\nof VLMs to enable effective domain adaptation while mitigating overfitting\nthrough parameter-efficient tuning or instance-based consistency constraints.\nHowever, such regularizations often neglect the geometric structure of data\ndistribution, which may lead to distortion of the overall semantic\nrepresentation. To overcome this limitation, we propose a novel fine-tuning\nmethod, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the\ndata distribution in feature space as a semantic manifold, MPS-Tuning\nexplicitly constrains the intrinsic geometry of this manifold while further\nsculpting it to enhance class separability. Specifically, MPS-Tuning preserves\nboth macroscopic and microscopic topological structures of the original\nmanifold by aligning Gram matrices of features before and after fine-tuning.\nTheoretically, this constraint is shown to approximate an upper bound of the\nGromov-Wasserstein distance. Furthermore, features from the image and text\nmodalities are paired, and pairwise similarities are optimized to enhance the\nmanifold's class discriminability. Extensive experiments demonstrate that\nMPS-Tuning significantly improves model performance while effectively\npreserving the structure of the semantic manifold. The code will be released.\n", "link": "http://arxiv.org/abs/2508.12877v1", "date": "2025-08-18", "relevancy": 2.943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserve%20and%20Sculpt%3A%20Manifold-Aligned%20Fine-tuning%20of%20Vision-Language%0A%20%20Models%20for%20Few-Shot%20Learning&body=Title%3A%20Preserve%20and%20Sculpt%3A%20Manifold-Aligned%20Fine-tuning%20of%20Vision-Language%0A%20%20Models%20for%20Few-Shot%20Learning%0AAuthor%3A%20Dexia%20Chen%20and%20Qianjie%20Zhu%20and%20Weibing%20Li%20and%20Yue%20Yu%20and%20Tong%20Zhang%20and%20Ruixuan%20Wang%0AAbstract%3A%20%20%20Pretrained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20shown%20remarkable%0Apotential%20in%20few-shot%20image%20classification%20and%20led%20to%20numerous%20effective%0Atransfer%20learning%20strategies.%20These%20methods%20leverage%20the%20pretrained%20knowledge%0Aof%20VLMs%20to%20enable%20effective%20domain%20adaptation%20while%20mitigating%20overfitting%0Athrough%20parameter-efficient%20tuning%20or%20instance-based%20consistency%20constraints.%0AHowever%2C%20such%20regularizations%20often%20neglect%20the%20geometric%20structure%20of%20data%0Adistribution%2C%20which%20may%20lead%20to%20distortion%20of%20the%20overall%20semantic%0Arepresentation.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20fine-tuning%0Amethod%2C%20Manifold-Preserving%20and%20Sculpting%20Tuning%20%28MPS-Tuning%29.%20Regarding%20the%0Adata%20distribution%20in%20feature%20space%20as%20a%20semantic%20manifold%2C%20MPS-Tuning%0Aexplicitly%20constrains%20the%20intrinsic%20geometry%20of%20this%20manifold%20while%20further%0Asculpting%20it%20to%20enhance%20class%20separability.%20Specifically%2C%20MPS-Tuning%20preserves%0Aboth%20macroscopic%20and%20microscopic%20topological%20structures%20of%20the%20original%0Amanifold%20by%20aligning%20Gram%20matrices%20of%20features%20before%20and%20after%20fine-tuning.%0ATheoretically%2C%20this%20constraint%20is%20shown%20to%20approximate%20an%20upper%20bound%20of%20the%0AGromov-Wasserstein%20distance.%20Furthermore%2C%20features%20from%20the%20image%20and%20text%0Amodalities%20are%20paired%2C%20and%20pairwise%20similarities%20are%20optimized%20to%20enhance%20the%0Amanifold%27s%20class%20discriminability.%20Extensive%20experiments%20demonstrate%20that%0AMPS-Tuning%20significantly%20improves%20model%20performance%20while%20effectively%0Apreserving%20the%20structure%20of%20the%20semantic%20manifold.%20The%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserve%2520and%2520Sculpt%253A%2520Manifold-Aligned%2520Fine-tuning%2520of%2520Vision-Language%250A%2520%2520Models%2520for%2520Few-Shot%2520Learning%26entry.906535625%3DDexia%2520Chen%2520and%2520Qianjie%2520Zhu%2520and%2520Weibing%2520Li%2520and%2520Yue%2520Yu%2520and%2520Tong%2520Zhang%2520and%2520Ruixuan%2520Wang%26entry.1292438233%3D%2520%2520Pretrained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520have%2520shown%2520remarkable%250Apotential%2520in%2520few-shot%2520image%2520classification%2520and%2520led%2520to%2520numerous%2520effective%250Atransfer%2520learning%2520strategies.%2520These%2520methods%2520leverage%2520the%2520pretrained%2520knowledge%250Aof%2520VLMs%2520to%2520enable%2520effective%2520domain%2520adaptation%2520while%2520mitigating%2520overfitting%250Athrough%2520parameter-efficient%2520tuning%2520or%2520instance-based%2520consistency%2520constraints.%250AHowever%252C%2520such%2520regularizations%2520often%2520neglect%2520the%2520geometric%2520structure%2520of%2520data%250Adistribution%252C%2520which%2520may%2520lead%2520to%2520distortion%2520of%2520the%2520overall%2520semantic%250Arepresentation.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520fine-tuning%250Amethod%252C%2520Manifold-Preserving%2520and%2520Sculpting%2520Tuning%2520%2528MPS-Tuning%2529.%2520Regarding%2520the%250Adata%2520distribution%2520in%2520feature%2520space%2520as%2520a%2520semantic%2520manifold%252C%2520MPS-Tuning%250Aexplicitly%2520constrains%2520the%2520intrinsic%2520geometry%2520of%2520this%2520manifold%2520while%2520further%250Asculpting%2520it%2520to%2520enhance%2520class%2520separability.%2520Specifically%252C%2520MPS-Tuning%2520preserves%250Aboth%2520macroscopic%2520and%2520microscopic%2520topological%2520structures%2520of%2520the%2520original%250Amanifold%2520by%2520aligning%2520Gram%2520matrices%2520of%2520features%2520before%2520and%2520after%2520fine-tuning.%250ATheoretically%252C%2520this%2520constraint%2520is%2520shown%2520to%2520approximate%2520an%2520upper%2520bound%2520of%2520the%250AGromov-Wasserstein%2520distance.%2520Furthermore%252C%2520features%2520from%2520the%2520image%2520and%2520text%250Amodalities%2520are%2520paired%252C%2520and%2520pairwise%2520similarities%2520are%2520optimized%2520to%2520enhance%2520the%250Amanifold%2527s%2520class%2520discriminability.%2520Extensive%2520experiments%2520demonstrate%2520that%250AMPS-Tuning%2520significantly%2520improves%2520model%2520performance%2520while%2520effectively%250Apreserving%2520the%2520structure%2520of%2520the%2520semantic%2520manifold.%2520The%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserve%20and%20Sculpt%3A%20Manifold-Aligned%20Fine-tuning%20of%20Vision-Language%0A%20%20Models%20for%20Few-Shot%20Learning&entry.906535625=Dexia%20Chen%20and%20Qianjie%20Zhu%20and%20Weibing%20Li%20and%20Yue%20Yu%20and%20Tong%20Zhang%20and%20Ruixuan%20Wang&entry.1292438233=%20%20Pretrained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20have%20shown%20remarkable%0Apotential%20in%20few-shot%20image%20classification%20and%20led%20to%20numerous%20effective%0Atransfer%20learning%20strategies.%20These%20methods%20leverage%20the%20pretrained%20knowledge%0Aof%20VLMs%20to%20enable%20effective%20domain%20adaptation%20while%20mitigating%20overfitting%0Athrough%20parameter-efficient%20tuning%20or%20instance-based%20consistency%20constraints.%0AHowever%2C%20such%20regularizations%20often%20neglect%20the%20geometric%20structure%20of%20data%0Adistribution%2C%20which%20may%20lead%20to%20distortion%20of%20the%20overall%20semantic%0Arepresentation.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20fine-tuning%0Amethod%2C%20Manifold-Preserving%20and%20Sculpting%20Tuning%20%28MPS-Tuning%29.%20Regarding%20the%0Adata%20distribution%20in%20feature%20space%20as%20a%20semantic%20manifold%2C%20MPS-Tuning%0Aexplicitly%20constrains%20the%20intrinsic%20geometry%20of%20this%20manifold%20while%20further%0Asculpting%20it%20to%20enhance%20class%20separability.%20Specifically%2C%20MPS-Tuning%20preserves%0Aboth%20macroscopic%20and%20microscopic%20topological%20structures%20of%20the%20original%0Amanifold%20by%20aligning%20Gram%20matrices%20of%20features%20before%20and%20after%20fine-tuning.%0ATheoretically%2C%20this%20constraint%20is%20shown%20to%20approximate%20an%20upper%20bound%20of%20the%0AGromov-Wasserstein%20distance.%20Furthermore%2C%20features%20from%20the%20image%20and%20text%0Amodalities%20are%20paired%2C%20and%20pairwise%20similarities%20are%20optimized%20to%20enhance%20the%0Amanifold%27s%20class%20discriminability.%20Extensive%20experiments%20demonstrate%20that%0AMPS-Tuning%20significantly%20improves%20model%20performance%20while%20effectively%0Apreserving%20the%20structure%20of%20the%20semantic%20manifold.%20The%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12877v1&entry.124074799=Read"},
{"title": "Optimization of Prompt Learning via Multi-Knowledge Representation for\n  Vision-Language Models", "author": "Enming Zhang and Bingke Zhu and Yingying Chen and Qinghai Miao and Ming Tang and Jinqiao Wang", "abstract": "  Vision-Language Models (VLMs), such as CLIP, play a foundational role in\nvarious cross-modal applications. To fully leverage VLMs' potential in adapting\nto downstream tasks, context optimization methods like Prompt Tuning are\nessential. However, one key limitation is the lack of diversity in prompt\ntemplates, whether they are hand-crafted or learned through additional modules.\nThis limitation restricts the capabilities of pretrained VLMs and can result in\nincorrect predictions in downstream tasks. To address this challenge, we\npropose Context Optimization with Multi-Knowledge Representation (CoKnow), a\nframework that enhances Prompt Learning for VLMs with rich contextual\nknowledge. To facilitate CoKnow during inference, we trained lightweight\nsemantic knowledge mappers, which are capable of generating Multi-Knowledge\nRepresentation for an input image without requiring additional priors.\nExperimentally, We conducted extensive experiments on 11 publicly available\ndatasets, demonstrating that CoKnow outperforms a series of previous methods.\n", "link": "http://arxiv.org/abs/2404.10357v3", "date": "2025-08-18", "relevancy": 2.9427, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization%20of%20Prompt%20Learning%20via%20Multi-Knowledge%20Representation%20for%0A%20%20Vision-Language%20Models&body=Title%3A%20Optimization%20of%20Prompt%20Learning%20via%20Multi-Knowledge%20Representation%20for%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Enming%20Zhang%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Qinghai%20Miao%20and%20Ming%20Tang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20play%20a%20foundational%20role%20in%0Avarious%20cross-modal%20applications.%20To%20fully%20leverage%20VLMs%27%20potential%20in%20adapting%0Ato%20downstream%20tasks%2C%20context%20optimization%20methods%20like%20Prompt%20Tuning%20are%0Aessential.%20However%2C%20one%20key%20limitation%20is%20the%20lack%20of%20diversity%20in%20prompt%0Atemplates%2C%20whether%20they%20are%20hand-crafted%20or%20learned%20through%20additional%20modules.%0AThis%20limitation%20restricts%20the%20capabilities%20of%20pretrained%20VLMs%20and%20can%20result%20in%0Aincorrect%20predictions%20in%20downstream%20tasks.%20To%20address%20this%20challenge%2C%20we%0Apropose%20Context%20Optimization%20with%20Multi-Knowledge%20Representation%20%28CoKnow%29%2C%20a%0Aframework%20that%20enhances%20Prompt%20Learning%20for%20VLMs%20with%20rich%20contextual%0Aknowledge.%20To%20facilitate%20CoKnow%20during%20inference%2C%20we%20trained%20lightweight%0Asemantic%20knowledge%20mappers%2C%20which%20are%20capable%20of%20generating%20Multi-Knowledge%0ARepresentation%20for%20an%20input%20image%20without%20requiring%20additional%20priors.%0AExperimentally%2C%20We%20conducted%20extensive%20experiments%20on%2011%20publicly%20available%0Adatasets%2C%20demonstrating%20that%20CoKnow%20outperforms%20a%20series%20of%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10357v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization%2520of%2520Prompt%2520Learning%2520via%2520Multi-Knowledge%2520Representation%2520for%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DEnming%2520Zhang%2520and%2520Bingke%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Qinghai%2520Miao%2520and%2520Ming%2520Tang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520play%2520a%2520foundational%2520role%2520in%250Avarious%2520cross-modal%2520applications.%2520To%2520fully%2520leverage%2520VLMs%2527%2520potential%2520in%2520adapting%250Ato%2520downstream%2520tasks%252C%2520context%2520optimization%2520methods%2520like%2520Prompt%2520Tuning%2520are%250Aessential.%2520However%252C%2520one%2520key%2520limitation%2520is%2520the%2520lack%2520of%2520diversity%2520in%2520prompt%250Atemplates%252C%2520whether%2520they%2520are%2520hand-crafted%2520or%2520learned%2520through%2520additional%2520modules.%250AThis%2520limitation%2520restricts%2520the%2520capabilities%2520of%2520pretrained%2520VLMs%2520and%2520can%2520result%2520in%250Aincorrect%2520predictions%2520in%2520downstream%2520tasks.%2520To%2520address%2520this%2520challenge%252C%2520we%250Apropose%2520Context%2520Optimization%2520with%2520Multi-Knowledge%2520Representation%2520%2528CoKnow%2529%252C%2520a%250Aframework%2520that%2520enhances%2520Prompt%2520Learning%2520for%2520VLMs%2520with%2520rich%2520contextual%250Aknowledge.%2520To%2520facilitate%2520CoKnow%2520during%2520inference%252C%2520we%2520trained%2520lightweight%250Asemantic%2520knowledge%2520mappers%252C%2520which%2520are%2520capable%2520of%2520generating%2520Multi-Knowledge%250ARepresentation%2520for%2520an%2520input%2520image%2520without%2520requiring%2520additional%2520priors.%250AExperimentally%252C%2520We%2520conducted%2520extensive%2520experiments%2520on%252011%2520publicly%2520available%250Adatasets%252C%2520demonstrating%2520that%2520CoKnow%2520outperforms%2520a%2520series%2520of%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10357v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization%20of%20Prompt%20Learning%20via%20Multi-Knowledge%20Representation%20for%0A%20%20Vision-Language%20Models&entry.906535625=Enming%20Zhang%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Qinghai%20Miao%20and%20Ming%20Tang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20play%20a%20foundational%20role%20in%0Avarious%20cross-modal%20applications.%20To%20fully%20leverage%20VLMs%27%20potential%20in%20adapting%0Ato%20downstream%20tasks%2C%20context%20optimization%20methods%20like%20Prompt%20Tuning%20are%0Aessential.%20However%2C%20one%20key%20limitation%20is%20the%20lack%20of%20diversity%20in%20prompt%0Atemplates%2C%20whether%20they%20are%20hand-crafted%20or%20learned%20through%20additional%20modules.%0AThis%20limitation%20restricts%20the%20capabilities%20of%20pretrained%20VLMs%20and%20can%20result%20in%0Aincorrect%20predictions%20in%20downstream%20tasks.%20To%20address%20this%20challenge%2C%20we%0Apropose%20Context%20Optimization%20with%20Multi-Knowledge%20Representation%20%28CoKnow%29%2C%20a%0Aframework%20that%20enhances%20Prompt%20Learning%20for%20VLMs%20with%20rich%20contextual%0Aknowledge.%20To%20facilitate%20CoKnow%20during%20inference%2C%20we%20trained%20lightweight%0Asemantic%20knowledge%20mappers%2C%20which%20are%20capable%20of%20generating%20Multi-Knowledge%0ARepresentation%20for%20an%20input%20image%20without%20requiring%20additional%20priors.%0AExperimentally%2C%20We%20conducted%20extensive%20experiments%20on%2011%20publicly%20available%0Adatasets%2C%20demonstrating%20that%20CoKnow%20outperforms%20a%20series%20of%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10357v3&entry.124074799=Read"},
{"title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive\n  World Model", "author": "Xianglong He and Chunli Peng and Zexiang Liu and Boyang Wang and Yifan Zhang and Qi Cui and Fei Kang and Biao Jiang and Mengyin An and Yangyang Ren and Baixin Xu and Hao-Xiang Guo and Kaixiong Gong and Cyrus Wu and Wei Li and Xuchen Song and Yang Liu and Eric Li and Yahui Zhou", "abstract": "  Recent advances in interactive video generations have demonstrated diffusion\nmodel's potential as world models by capturing complex physical dynamics and\ninteractive behaviors. However, existing interactive world models depend on\nbidirectional attention and lengthy inference steps, severely limiting\nreal-time performance. Consequently, they are hard to simulate real-world\ndynamics, where outcomes must update instantaneously based on historical\ncontext and current actions. To address this, we present Matrix-Game 2.0, an\ninteractive world model generates long videos on-the-fly via few-step\nauto-regressive diffusion. Our framework consists of three key components: (1)\nA scalable data production pipeline for Unreal Engine and GTA5 environments to\neffectively produce massive amounts (about 1200 hours) of video data with\ndiverse interaction annotations; (2) An action injection module that enables\nframe-level mouse and keyboard inputs as interactive conditions; (3) A few-step\ndistillation based on the casual architecture for real-time and streaming video\ngeneration. Matrix Game 2.0 can generate high-quality minute-level videos\nacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source our\nmodel weights and codebase to advance research in interactive world modeling.\n", "link": "http://arxiv.org/abs/2508.13009v1", "date": "2025-08-18", "relevancy": 2.902, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.59}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5792}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix-Game%202.0%3A%20An%20Open-Source%2C%20Real-Time%2C%20and%20Streaming%20Interactive%0A%20%20World%20Model&body=Title%3A%20Matrix-Game%202.0%3A%20An%20Open-Source%2C%20Real-Time%2C%20and%20Streaming%20Interactive%0A%20%20World%20Model%0AAuthor%3A%20Xianglong%20He%20and%20Chunli%20Peng%20and%20Zexiang%20Liu%20and%20Boyang%20Wang%20and%20Yifan%20Zhang%20and%20Qi%20Cui%20and%20Fei%20Kang%20and%20Biao%20Jiang%20and%20Mengyin%20An%20and%20Yangyang%20Ren%20and%20Baixin%20Xu%20and%20Hao-Xiang%20Guo%20and%20Kaixiong%20Gong%20and%20Cyrus%20Wu%20and%20Wei%20Li%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Eric%20Li%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20Recent%20advances%20in%20interactive%20video%20generations%20have%20demonstrated%20diffusion%0Amodel%27s%20potential%20as%20world%20models%20by%20capturing%20complex%20physical%20dynamics%20and%0Ainteractive%20behaviors.%20However%2C%20existing%20interactive%20world%20models%20depend%20on%0Abidirectional%20attention%20and%20lengthy%20inference%20steps%2C%20severely%20limiting%0Areal-time%20performance.%20Consequently%2C%20they%20are%20hard%20to%20simulate%20real-world%0Adynamics%2C%20where%20outcomes%20must%20update%20instantaneously%20based%20on%20historical%0Acontext%20and%20current%20actions.%20To%20address%20this%2C%20we%20present%20Matrix-Game%202.0%2C%20an%0Ainteractive%20world%20model%20generates%20long%20videos%20on-the-fly%20via%20few-step%0Aauto-regressive%20diffusion.%20Our%20framework%20consists%20of%20three%20key%20components%3A%20%281%29%0AA%20scalable%20data%20production%20pipeline%20for%20Unreal%20Engine%20and%20GTA5%20environments%20to%0Aeffectively%20produce%20massive%20amounts%20%28about%201200%20hours%29%20of%20video%20data%20with%0Adiverse%20interaction%20annotations%3B%20%282%29%20An%20action%20injection%20module%20that%20enables%0Aframe-level%20mouse%20and%20keyboard%20inputs%20as%20interactive%20conditions%3B%20%283%29%20A%20few-step%0Adistillation%20based%20on%20the%20casual%20architecture%20for%20real-time%20and%20streaming%20video%0Ageneration.%20Matrix%20Game%202.0%20can%20generate%20high-quality%20minute-level%20videos%0Aacross%20diverse%20scenes%20at%20an%20ultra-fast%20speed%20of%2025%20FPS.%20We%20open-source%20our%0Amodel%20weights%20and%20codebase%20to%20advance%20research%20in%20interactive%20world%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix-Game%25202.0%253A%2520An%2520Open-Source%252C%2520Real-Time%252C%2520and%2520Streaming%2520Interactive%250A%2520%2520World%2520Model%26entry.906535625%3DXianglong%2520He%2520and%2520Chunli%2520Peng%2520and%2520Zexiang%2520Liu%2520and%2520Boyang%2520Wang%2520and%2520Yifan%2520Zhang%2520and%2520Qi%2520Cui%2520and%2520Fei%2520Kang%2520and%2520Biao%2520Jiang%2520and%2520Mengyin%2520An%2520and%2520Yangyang%2520Ren%2520and%2520Baixin%2520Xu%2520and%2520Hao-Xiang%2520Guo%2520and%2520Kaixiong%2520Gong%2520and%2520Cyrus%2520Wu%2520and%2520Wei%2520Li%2520and%2520Xuchen%2520Song%2520and%2520Yang%2520Liu%2520and%2520Eric%2520Li%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520interactive%2520video%2520generations%2520have%2520demonstrated%2520diffusion%250Amodel%2527s%2520potential%2520as%2520world%2520models%2520by%2520capturing%2520complex%2520physical%2520dynamics%2520and%250Ainteractive%2520behaviors.%2520However%252C%2520existing%2520interactive%2520world%2520models%2520depend%2520on%250Abidirectional%2520attention%2520and%2520lengthy%2520inference%2520steps%252C%2520severely%2520limiting%250Areal-time%2520performance.%2520Consequently%252C%2520they%2520are%2520hard%2520to%2520simulate%2520real-world%250Adynamics%252C%2520where%2520outcomes%2520must%2520update%2520instantaneously%2520based%2520on%2520historical%250Acontext%2520and%2520current%2520actions.%2520To%2520address%2520this%252C%2520we%2520present%2520Matrix-Game%25202.0%252C%2520an%250Ainteractive%2520world%2520model%2520generates%2520long%2520videos%2520on-the-fly%2520via%2520few-step%250Aauto-regressive%2520diffusion.%2520Our%2520framework%2520consists%2520of%2520three%2520key%2520components%253A%2520%25281%2529%250AA%2520scalable%2520data%2520production%2520pipeline%2520for%2520Unreal%2520Engine%2520and%2520GTA5%2520environments%2520to%250Aeffectively%2520produce%2520massive%2520amounts%2520%2528about%25201200%2520hours%2529%2520of%2520video%2520data%2520with%250Adiverse%2520interaction%2520annotations%253B%2520%25282%2529%2520An%2520action%2520injection%2520module%2520that%2520enables%250Aframe-level%2520mouse%2520and%2520keyboard%2520inputs%2520as%2520interactive%2520conditions%253B%2520%25283%2529%2520A%2520few-step%250Adistillation%2520based%2520on%2520the%2520casual%2520architecture%2520for%2520real-time%2520and%2520streaming%2520video%250Ageneration.%2520Matrix%2520Game%25202.0%2520can%2520generate%2520high-quality%2520minute-level%2520videos%250Aacross%2520diverse%2520scenes%2520at%2520an%2520ultra-fast%2520speed%2520of%252025%2520FPS.%2520We%2520open-source%2520our%250Amodel%2520weights%2520and%2520codebase%2520to%2520advance%2520research%2520in%2520interactive%2520world%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix-Game%202.0%3A%20An%20Open-Source%2C%20Real-Time%2C%20and%20Streaming%20Interactive%0A%20%20World%20Model&entry.906535625=Xianglong%20He%20and%20Chunli%20Peng%20and%20Zexiang%20Liu%20and%20Boyang%20Wang%20and%20Yifan%20Zhang%20and%20Qi%20Cui%20and%20Fei%20Kang%20and%20Biao%20Jiang%20and%20Mengyin%20An%20and%20Yangyang%20Ren%20and%20Baixin%20Xu%20and%20Hao-Xiang%20Guo%20and%20Kaixiong%20Gong%20and%20Cyrus%20Wu%20and%20Wei%20Li%20and%20Xuchen%20Song%20and%20Yang%20Liu%20and%20Eric%20Li%20and%20Yahui%20Zhou&entry.1292438233=%20%20Recent%20advances%20in%20interactive%20video%20generations%20have%20demonstrated%20diffusion%0Amodel%27s%20potential%20as%20world%20models%20by%20capturing%20complex%20physical%20dynamics%20and%0Ainteractive%20behaviors.%20However%2C%20existing%20interactive%20world%20models%20depend%20on%0Abidirectional%20attention%20and%20lengthy%20inference%20steps%2C%20severely%20limiting%0Areal-time%20performance.%20Consequently%2C%20they%20are%20hard%20to%20simulate%20real-world%0Adynamics%2C%20where%20outcomes%20must%20update%20instantaneously%20based%20on%20historical%0Acontext%20and%20current%20actions.%20To%20address%20this%2C%20we%20present%20Matrix-Game%202.0%2C%20an%0Ainteractive%20world%20model%20generates%20long%20videos%20on-the-fly%20via%20few-step%0Aauto-regressive%20diffusion.%20Our%20framework%20consists%20of%20three%20key%20components%3A%20%281%29%0AA%20scalable%20data%20production%20pipeline%20for%20Unreal%20Engine%20and%20GTA5%20environments%20to%0Aeffectively%20produce%20massive%20amounts%20%28about%201200%20hours%29%20of%20video%20data%20with%0Adiverse%20interaction%20annotations%3B%20%282%29%20An%20action%20injection%20module%20that%20enables%0Aframe-level%20mouse%20and%20keyboard%20inputs%20as%20interactive%20conditions%3B%20%283%29%20A%20few-step%0Adistillation%20based%20on%20the%20casual%20architecture%20for%20real-time%20and%20streaming%20video%0Ageneration.%20Matrix%20Game%202.0%20can%20generate%20high-quality%20minute-level%20videos%0Aacross%20diverse%20scenes%20at%20an%20ultra-fast%20speed%20of%2025%20FPS.%20We%20open-source%20our%0Amodel%20weights%20and%20codebase%20to%20advance%20research%20in%20interactive%20world%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13009v1&entry.124074799=Read"},
{"title": "Learning local and global prototypes with optimal transport for\n  unsupervised anomaly detection and localization", "author": "Robin Trombetta and Carole Lartizien", "abstract": "  Unsupervised anomaly detection aims to detect defective parts of a sample by\nhaving access, during training, to a set of normal, i.e. defect-free, data. It\nhas many applications in fields, such as industrial inspection or medical\nimaging, where acquiring labels is costly or when we want to avoid introducing\nbiases in the type of anomalies that can be spotted. In this work, we propose a\nnovel UAD method based on prototype learning and introduce a metric to compare\na structured set of embeddings that balances a feature-based cost and a\nspatial-based cost. We leverage this metric to learn local and global\nprototypes with optimal transport from latent representations extracted with a\npre-trained image encoder. We demonstrate that our approach can enforce a\nstructural constraint when learning the prototypes, allowing to capture the\nunderlying organization of the normal samples, thus improving the detection of\nincoherencies in images. Our model achieves performance that is on par with\nstrong baselines on two reference benchmarks for anomaly detection on\nindustrial images. The code is available at\nhttps://github.com/robintrmbtt/pradot.\n", "link": "http://arxiv.org/abs/2508.12927v1", "date": "2025-08-18", "relevancy": 2.8621, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5918}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5766}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20local%20and%20global%20prototypes%20with%20optimal%20transport%20for%0A%20%20unsupervised%20anomaly%20detection%20and%20localization&body=Title%3A%20Learning%20local%20and%20global%20prototypes%20with%20optimal%20transport%20for%0A%20%20unsupervised%20anomaly%20detection%20and%20localization%0AAuthor%3A%20Robin%20Trombetta%20and%20Carole%20Lartizien%0AAbstract%3A%20%20%20Unsupervised%20anomaly%20detection%20aims%20to%20detect%20defective%20parts%20of%20a%20sample%20by%0Ahaving%20access%2C%20during%20training%2C%20to%20a%20set%20of%20normal%2C%20i.e.%20defect-free%2C%20data.%20It%0Ahas%20many%20applications%20in%20fields%2C%20such%20as%20industrial%20inspection%20or%20medical%0Aimaging%2C%20where%20acquiring%20labels%20is%20costly%20or%20when%20we%20want%20to%20avoid%20introducing%0Abiases%20in%20the%20type%20of%20anomalies%20that%20can%20be%20spotted.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20UAD%20method%20based%20on%20prototype%20learning%20and%20introduce%20a%20metric%20to%20compare%0Aa%20structured%20set%20of%20embeddings%20that%20balances%20a%20feature-based%20cost%20and%20a%0Aspatial-based%20cost.%20We%20leverage%20this%20metric%20to%20learn%20local%20and%20global%0Aprototypes%20with%20optimal%20transport%20from%20latent%20representations%20extracted%20with%20a%0Apre-trained%20image%20encoder.%20We%20demonstrate%20that%20our%20approach%20can%20enforce%20a%0Astructural%20constraint%20when%20learning%20the%20prototypes%2C%20allowing%20to%20capture%20the%0Aunderlying%20organization%20of%20the%20normal%20samples%2C%20thus%20improving%20the%20detection%20of%0Aincoherencies%20in%20images.%20Our%20model%20achieves%20performance%20that%20is%20on%20par%20with%0Astrong%20baselines%20on%20two%20reference%20benchmarks%20for%20anomaly%20detection%20on%0Aindustrial%20images.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/robintrmbtt/pradot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520local%2520and%2520global%2520prototypes%2520with%2520optimal%2520transport%2520for%250A%2520%2520unsupervised%2520anomaly%2520detection%2520and%2520localization%26entry.906535625%3DRobin%2520Trombetta%2520and%2520Carole%2520Lartizien%26entry.1292438233%3D%2520%2520Unsupervised%2520anomaly%2520detection%2520aims%2520to%2520detect%2520defective%2520parts%2520of%2520a%2520sample%2520by%250Ahaving%2520access%252C%2520during%2520training%252C%2520to%2520a%2520set%2520of%2520normal%252C%2520i.e.%2520defect-free%252C%2520data.%2520It%250Ahas%2520many%2520applications%2520in%2520fields%252C%2520such%2520as%2520industrial%2520inspection%2520or%2520medical%250Aimaging%252C%2520where%2520acquiring%2520labels%2520is%2520costly%2520or%2520when%2520we%2520want%2520to%2520avoid%2520introducing%250Abiases%2520in%2520the%2520type%2520of%2520anomalies%2520that%2520can%2520be%2520spotted.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520UAD%2520method%2520based%2520on%2520prototype%2520learning%2520and%2520introduce%2520a%2520metric%2520to%2520compare%250Aa%2520structured%2520set%2520of%2520embeddings%2520that%2520balances%2520a%2520feature-based%2520cost%2520and%2520a%250Aspatial-based%2520cost.%2520We%2520leverage%2520this%2520metric%2520to%2520learn%2520local%2520and%2520global%250Aprototypes%2520with%2520optimal%2520transport%2520from%2520latent%2520representations%2520extracted%2520with%2520a%250Apre-trained%2520image%2520encoder.%2520We%2520demonstrate%2520that%2520our%2520approach%2520can%2520enforce%2520a%250Astructural%2520constraint%2520when%2520learning%2520the%2520prototypes%252C%2520allowing%2520to%2520capture%2520the%250Aunderlying%2520organization%2520of%2520the%2520normal%2520samples%252C%2520thus%2520improving%2520the%2520detection%2520of%250Aincoherencies%2520in%2520images.%2520Our%2520model%2520achieves%2520performance%2520that%2520is%2520on%2520par%2520with%250Astrong%2520baselines%2520on%2520two%2520reference%2520benchmarks%2520for%2520anomaly%2520detection%2520on%250Aindustrial%2520images.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/robintrmbtt/pradot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20local%20and%20global%20prototypes%20with%20optimal%20transport%20for%0A%20%20unsupervised%20anomaly%20detection%20and%20localization&entry.906535625=Robin%20Trombetta%20and%20Carole%20Lartizien&entry.1292438233=%20%20Unsupervised%20anomaly%20detection%20aims%20to%20detect%20defective%20parts%20of%20a%20sample%20by%0Ahaving%20access%2C%20during%20training%2C%20to%20a%20set%20of%20normal%2C%20i.e.%20defect-free%2C%20data.%20It%0Ahas%20many%20applications%20in%20fields%2C%20such%20as%20industrial%20inspection%20or%20medical%0Aimaging%2C%20where%20acquiring%20labels%20is%20costly%20or%20when%20we%20want%20to%20avoid%20introducing%0Abiases%20in%20the%20type%20of%20anomalies%20that%20can%20be%20spotted.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20UAD%20method%20based%20on%20prototype%20learning%20and%20introduce%20a%20metric%20to%20compare%0Aa%20structured%20set%20of%20embeddings%20that%20balances%20a%20feature-based%20cost%20and%20a%0Aspatial-based%20cost.%20We%20leverage%20this%20metric%20to%20learn%20local%20and%20global%0Aprototypes%20with%20optimal%20transport%20from%20latent%20representations%20extracted%20with%20a%0Apre-trained%20image%20encoder.%20We%20demonstrate%20that%20our%20approach%20can%20enforce%20a%0Astructural%20constraint%20when%20learning%20the%20prototypes%2C%20allowing%20to%20capture%20the%0Aunderlying%20organization%20of%20the%20normal%20samples%2C%20thus%20improving%20the%20detection%20of%0Aincoherencies%20in%20images.%20Our%20model%20achieves%20performance%20that%20is%20on%20par%20with%0Astrong%20baselines%20on%20two%20reference%20benchmarks%20for%20anomaly%20detection%20on%0Aindustrial%20images.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/robintrmbtt/pradot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12927v1&entry.124074799=Read"},
{"title": "Best Foot Forward: Robust Foot Reconstruction in-the-wild", "author": "Kyle Fogarty and Jing Yang and Chayan Kumar Patodi and Jack Foster and Aadi Bhanti and Steven Chacko and Cengiz Oztireli and Ujwal Bonde", "abstract": "  Accurate 3D foot reconstruction is crucial for personalized orthotics,\ndigital healthcare, and virtual fittings. However, existing methods struggle\nwith incomplete scans and anatomical variations, particularly in self-scanning\nscenarios where user mobility is limited, making it difficult to capture areas\nlike the arch and heel. We present a novel end-to-end pipeline that refines\nStructure-from-Motion (SfM) reconstruction. It first resolves scan alignment\nambiguities using SE(3) canonicalization with a viewpoint prediction module,\nthen completes missing geometry through an attention-based network trained on\nsynthetically augmented point clouds. Our approach achieves state-of-the-art\nperformance on reconstruction metrics while preserving clinically validated\nanatomical fidelity. By combining synthetic training data with learned\ngeometric priors, we enable robust foot reconstruction under real-world capture\nconditions, unlocking new opportunities for mobile-based 3D scanning in\nhealthcare and retail.\n", "link": "http://arxiv.org/abs/2502.20511v2", "date": "2025-08-18", "relevancy": 2.8051, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5663}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5592}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Best%20Foot%20Forward%3A%20Robust%20Foot%20Reconstruction%20in-the-wild&body=Title%3A%20Best%20Foot%20Forward%3A%20Robust%20Foot%20Reconstruction%20in-the-wild%0AAuthor%3A%20Kyle%20Fogarty%20and%20Jing%20Yang%20and%20Chayan%20Kumar%20Patodi%20and%20Jack%20Foster%20and%20Aadi%20Bhanti%20and%20Steven%20Chacko%20and%20Cengiz%20Oztireli%20and%20Ujwal%20Bonde%0AAbstract%3A%20%20%20Accurate%203D%20foot%20reconstruction%20is%20crucial%20for%20personalized%20orthotics%2C%0Adigital%20healthcare%2C%20and%20virtual%20fittings.%20However%2C%20existing%20methods%20struggle%0Awith%20incomplete%20scans%20and%20anatomical%20variations%2C%20particularly%20in%20self-scanning%0Ascenarios%20where%20user%20mobility%20is%20limited%2C%20making%20it%20difficult%20to%20capture%20areas%0Alike%20the%20arch%20and%20heel.%20We%20present%20a%20novel%20end-to-end%20pipeline%20that%20refines%0AStructure-from-Motion%20%28SfM%29%20reconstruction.%20It%20first%20resolves%20scan%20alignment%0Aambiguities%20using%20SE%283%29%20canonicalization%20with%20a%20viewpoint%20prediction%20module%2C%0Athen%20completes%20missing%20geometry%20through%20an%20attention-based%20network%20trained%20on%0Asynthetically%20augmented%20point%20clouds.%20Our%20approach%20achieves%20state-of-the-art%0Aperformance%20on%20reconstruction%20metrics%20while%20preserving%20clinically%20validated%0Aanatomical%20fidelity.%20By%20combining%20synthetic%20training%20data%20with%20learned%0Ageometric%20priors%2C%20we%20enable%20robust%20foot%20reconstruction%20under%20real-world%20capture%0Aconditions%2C%20unlocking%20new%20opportunities%20for%20mobile-based%203D%20scanning%20in%0Ahealthcare%20and%20retail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBest%2520Foot%2520Forward%253A%2520Robust%2520Foot%2520Reconstruction%2520in-the-wild%26entry.906535625%3DKyle%2520Fogarty%2520and%2520Jing%2520Yang%2520and%2520Chayan%2520Kumar%2520Patodi%2520and%2520Jack%2520Foster%2520and%2520Aadi%2520Bhanti%2520and%2520Steven%2520Chacko%2520and%2520Cengiz%2520Oztireli%2520and%2520Ujwal%2520Bonde%26entry.1292438233%3D%2520%2520Accurate%25203D%2520foot%2520reconstruction%2520is%2520crucial%2520for%2520personalized%2520orthotics%252C%250Adigital%2520healthcare%252C%2520and%2520virtual%2520fittings.%2520However%252C%2520existing%2520methods%2520struggle%250Awith%2520incomplete%2520scans%2520and%2520anatomical%2520variations%252C%2520particularly%2520in%2520self-scanning%250Ascenarios%2520where%2520user%2520mobility%2520is%2520limited%252C%2520making%2520it%2520difficult%2520to%2520capture%2520areas%250Alike%2520the%2520arch%2520and%2520heel.%2520We%2520present%2520a%2520novel%2520end-to-end%2520pipeline%2520that%2520refines%250AStructure-from-Motion%2520%2528SfM%2529%2520reconstruction.%2520It%2520first%2520resolves%2520scan%2520alignment%250Aambiguities%2520using%2520SE%25283%2529%2520canonicalization%2520with%2520a%2520viewpoint%2520prediction%2520module%252C%250Athen%2520completes%2520missing%2520geometry%2520through%2520an%2520attention-based%2520network%2520trained%2520on%250Asynthetically%2520augmented%2520point%2520clouds.%2520Our%2520approach%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520reconstruction%2520metrics%2520while%2520preserving%2520clinically%2520validated%250Aanatomical%2520fidelity.%2520By%2520combining%2520synthetic%2520training%2520data%2520with%2520learned%250Ageometric%2520priors%252C%2520we%2520enable%2520robust%2520foot%2520reconstruction%2520under%2520real-world%2520capture%250Aconditions%252C%2520unlocking%2520new%2520opportunities%2520for%2520mobile-based%25203D%2520scanning%2520in%250Ahealthcare%2520and%2520retail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Best%20Foot%20Forward%3A%20Robust%20Foot%20Reconstruction%20in-the-wild&entry.906535625=Kyle%20Fogarty%20and%20Jing%20Yang%20and%20Chayan%20Kumar%20Patodi%20and%20Jack%20Foster%20and%20Aadi%20Bhanti%20and%20Steven%20Chacko%20and%20Cengiz%20Oztireli%20and%20Ujwal%20Bonde&entry.1292438233=%20%20Accurate%203D%20foot%20reconstruction%20is%20crucial%20for%20personalized%20orthotics%2C%0Adigital%20healthcare%2C%20and%20virtual%20fittings.%20However%2C%20existing%20methods%20struggle%0Awith%20incomplete%20scans%20and%20anatomical%20variations%2C%20particularly%20in%20self-scanning%0Ascenarios%20where%20user%20mobility%20is%20limited%2C%20making%20it%20difficult%20to%20capture%20areas%0Alike%20the%20arch%20and%20heel.%20We%20present%20a%20novel%20end-to-end%20pipeline%20that%20refines%0AStructure-from-Motion%20%28SfM%29%20reconstruction.%20It%20first%20resolves%20scan%20alignment%0Aambiguities%20using%20SE%283%29%20canonicalization%20with%20a%20viewpoint%20prediction%20module%2C%0Athen%20completes%20missing%20geometry%20through%20an%20attention-based%20network%20trained%20on%0Asynthetically%20augmented%20point%20clouds.%20Our%20approach%20achieves%20state-of-the-art%0Aperformance%20on%20reconstruction%20metrics%20while%20preserving%20clinically%20validated%0Aanatomical%20fidelity.%20By%20combining%20synthetic%20training%20data%20with%20learned%0Ageometric%20priors%2C%20we%20enable%20robust%20foot%20reconstruction%20under%20real-world%20capture%0Aconditions%2C%20unlocking%20new%20opportunities%20for%20mobile-based%203D%20scanning%20in%0Ahealthcare%20and%20retail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20511v2&entry.124074799=Read"},
{"title": "GazeDETR: Gaze Detection using Disentangled Head and Gaze\n  Representations", "author": "Ryan Anthony Jalova de Belen and Gelareh Mohammadi and Arcot Sowmya", "abstract": "  Gaze communication plays a crucial role in daily social interactions.\nQuantifying this behavior can help in human-computer interaction and digital\nphenotyping. While end-to-end models exist for gaze target detection, they only\nutilize a single decoder to simultaneously localize human heads and predict\ntheir corresponding gaze (e.g., 2D points or heatmap) in a scene. This\nmultitask learning approach generates a unified and entangled representation\nfor human head localization and gaze location prediction. Herein, we propose\nGazeDETR, a novel end-to-end architecture with two disentangled decoders that\nindividually learn unique representations and effectively utilize coherent\nattentive fields for each subtask. More specifically, we demonstrate that its\nhuman head predictor utilizes local information, while its gaze decoder\nincorporates both local and global information. Our proposed architecture\nachieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and\nChildPlay datasets. It outperforms existing end-to-end models with a notable\nmargin.\n", "link": "http://arxiv.org/abs/2508.12966v1", "date": "2025-08-18", "relevancy": 2.7991, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5649}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5644}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GazeDETR%3A%20Gaze%20Detection%20using%20Disentangled%20Head%20and%20Gaze%0A%20%20Representations&body=Title%3A%20GazeDETR%3A%20Gaze%20Detection%20using%20Disentangled%20Head%20and%20Gaze%0A%20%20Representations%0AAuthor%3A%20Ryan%20Anthony%20Jalova%20de%20Belen%20and%20Gelareh%20Mohammadi%20and%20Arcot%20Sowmya%0AAbstract%3A%20%20%20Gaze%20communication%20plays%20a%20crucial%20role%20in%20daily%20social%20interactions.%0AQuantifying%20this%20behavior%20can%20help%20in%20human-computer%20interaction%20and%20digital%0Aphenotyping.%20While%20end-to-end%20models%20exist%20for%20gaze%20target%20detection%2C%20they%20only%0Autilize%20a%20single%20decoder%20to%20simultaneously%20localize%20human%20heads%20and%20predict%0Atheir%20corresponding%20gaze%20%28e.g.%2C%202D%20points%20or%20heatmap%29%20in%20a%20scene.%20This%0Amultitask%20learning%20approach%20generates%20a%20unified%20and%20entangled%20representation%0Afor%20human%20head%20localization%20and%20gaze%20location%20prediction.%20Herein%2C%20we%20propose%0AGazeDETR%2C%20a%20novel%20end-to-end%20architecture%20with%20two%20disentangled%20decoders%20that%0Aindividually%20learn%20unique%20representations%20and%20effectively%20utilize%20coherent%0Aattentive%20fields%20for%20each%20subtask.%20More%20specifically%2C%20we%20demonstrate%20that%20its%0Ahuman%20head%20predictor%20utilizes%20local%20information%2C%20while%20its%20gaze%20decoder%0Aincorporates%20both%20local%20and%20global%20information.%20Our%20proposed%20architecture%0Aachieves%20state-of-the-art%20results%20on%20the%20GazeFollow%2C%20VideoAttentionTarget%20and%0AChildPlay%20datasets.%20It%20outperforms%20existing%20end-to-end%20models%20with%20a%20notable%0Amargin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGazeDETR%253A%2520Gaze%2520Detection%2520using%2520Disentangled%2520Head%2520and%2520Gaze%250A%2520%2520Representations%26entry.906535625%3DRyan%2520Anthony%2520Jalova%2520de%2520Belen%2520and%2520Gelareh%2520Mohammadi%2520and%2520Arcot%2520Sowmya%26entry.1292438233%3D%2520%2520Gaze%2520communication%2520plays%2520a%2520crucial%2520role%2520in%2520daily%2520social%2520interactions.%250AQuantifying%2520this%2520behavior%2520can%2520help%2520in%2520human-computer%2520interaction%2520and%2520digital%250Aphenotyping.%2520While%2520end-to-end%2520models%2520exist%2520for%2520gaze%2520target%2520detection%252C%2520they%2520only%250Autilize%2520a%2520single%2520decoder%2520to%2520simultaneously%2520localize%2520human%2520heads%2520and%2520predict%250Atheir%2520corresponding%2520gaze%2520%2528e.g.%252C%25202D%2520points%2520or%2520heatmap%2529%2520in%2520a%2520scene.%2520This%250Amultitask%2520learning%2520approach%2520generates%2520a%2520unified%2520and%2520entangled%2520representation%250Afor%2520human%2520head%2520localization%2520and%2520gaze%2520location%2520prediction.%2520Herein%252C%2520we%2520propose%250AGazeDETR%252C%2520a%2520novel%2520end-to-end%2520architecture%2520with%2520two%2520disentangled%2520decoders%2520that%250Aindividually%2520learn%2520unique%2520representations%2520and%2520effectively%2520utilize%2520coherent%250Aattentive%2520fields%2520for%2520each%2520subtask.%2520More%2520specifically%252C%2520we%2520demonstrate%2520that%2520its%250Ahuman%2520head%2520predictor%2520utilizes%2520local%2520information%252C%2520while%2520its%2520gaze%2520decoder%250Aincorporates%2520both%2520local%2520and%2520global%2520information.%2520Our%2520proposed%2520architecture%250Aachieves%2520state-of-the-art%2520results%2520on%2520the%2520GazeFollow%252C%2520VideoAttentionTarget%2520and%250AChildPlay%2520datasets.%2520It%2520outperforms%2520existing%2520end-to-end%2520models%2520with%2520a%2520notable%250Amargin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GazeDETR%3A%20Gaze%20Detection%20using%20Disentangled%20Head%20and%20Gaze%0A%20%20Representations&entry.906535625=Ryan%20Anthony%20Jalova%20de%20Belen%20and%20Gelareh%20Mohammadi%20and%20Arcot%20Sowmya&entry.1292438233=%20%20Gaze%20communication%20plays%20a%20crucial%20role%20in%20daily%20social%20interactions.%0AQuantifying%20this%20behavior%20can%20help%20in%20human-computer%20interaction%20and%20digital%0Aphenotyping.%20While%20end-to-end%20models%20exist%20for%20gaze%20target%20detection%2C%20they%20only%0Autilize%20a%20single%20decoder%20to%20simultaneously%20localize%20human%20heads%20and%20predict%0Atheir%20corresponding%20gaze%20%28e.g.%2C%202D%20points%20or%20heatmap%29%20in%20a%20scene.%20This%0Amultitask%20learning%20approach%20generates%20a%20unified%20and%20entangled%20representation%0Afor%20human%20head%20localization%20and%20gaze%20location%20prediction.%20Herein%2C%20we%20propose%0AGazeDETR%2C%20a%20novel%20end-to-end%20architecture%20with%20two%20disentangled%20decoders%20that%0Aindividually%20learn%20unique%20representations%20and%20effectively%20utilize%20coherent%0Aattentive%20fields%20for%20each%20subtask.%20More%20specifically%2C%20we%20demonstrate%20that%20its%0Ahuman%20head%20predictor%20utilizes%20local%20information%2C%20while%20its%20gaze%20decoder%0Aincorporates%20both%20local%20and%20global%20information.%20Our%20proposed%20architecture%0Aachieves%20state-of-the-art%20results%20on%20the%20GazeFollow%2C%20VideoAttentionTarget%20and%0AChildPlay%20datasets.%20It%20outperforms%20existing%20end-to-end%20models%20with%20a%20notable%0Amargin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12966v1&entry.124074799=Read"},
{"title": "DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition", "author": "Zhengxian Wu and Chuanrui Zhang and Hangrui Xu and Peng Jiao and Haoqian Wang", "abstract": "  Gait recognition is emerging as a promising and innovative area within the\nfield of computer vision, widely applied to remote person identification.\nAlthough existing gait recognition methods have achieved substantial success in\ncontrolled laboratory datasets, their performance often declines significantly\nwhen transitioning to wild datasets.We argue that the performance gap can be\nprimarily attributed to the spatio-temporal distribution inconsistencies\npresent in wild datasets, where subjects appear at varying angles, positions,\nand distances across the frames. To achieve accurate gait recognition in the\nwild, we propose a skeleton-guided silhouette alignment strategy, which uses\nprior knowledge of the skeletons to perform affine transformations on the\ncorresponding silhouettes.To the best of our knowledge, this is the first study\nto explore the impact of data alignment on gait recognition. We conducted\nextensive experiments across multiple datasets and network architectures, and\nthe results demonstrate the significant advantages of our proposed alignment\nstrategy.Specifically, on the challenging Gait3D dataset, our method achieved\nan average performance improvement of 7.9% across all evaluated networks.\nFurthermore, our method achieves substantial improvements on cross-domain\ndatasets, with accuracy improvements of up to 24.0%.\n", "link": "http://arxiv.org/abs/2503.18830v2", "date": "2025-08-18", "relevancy": 2.7923, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5651}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5554}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAGait%3A%20Generalized%20Skeleton-Guided%20Data%20Alignment%20for%20Gait%20Recognition&body=Title%3A%20DAGait%3A%20Generalized%20Skeleton-Guided%20Data%20Alignment%20for%20Gait%20Recognition%0AAuthor%3A%20Zhengxian%20Wu%20and%20Chuanrui%20Zhang%20and%20Hangrui%20Xu%20and%20Peng%20Jiao%20and%20Haoqian%20Wang%0AAbstract%3A%20%20%20Gait%20recognition%20is%20emerging%20as%20a%20promising%20and%20innovative%20area%20within%20the%0Afield%20of%20computer%20vision%2C%20widely%20applied%20to%20remote%20person%20identification.%0AAlthough%20existing%20gait%20recognition%20methods%20have%20achieved%20substantial%20success%20in%0Acontrolled%20laboratory%20datasets%2C%20their%20performance%20often%20declines%20significantly%0Awhen%20transitioning%20to%20wild%20datasets.We%20argue%20that%20the%20performance%20gap%20can%20be%0Aprimarily%20attributed%20to%20the%20spatio-temporal%20distribution%20inconsistencies%0Apresent%20in%20wild%20datasets%2C%20where%20subjects%20appear%20at%20varying%20angles%2C%20positions%2C%0Aand%20distances%20across%20the%20frames.%20To%20achieve%20accurate%20gait%20recognition%20in%20the%0Awild%2C%20we%20propose%20a%20skeleton-guided%20silhouette%20alignment%20strategy%2C%20which%20uses%0Aprior%20knowledge%20of%20the%20skeletons%20to%20perform%20affine%20transformations%20on%20the%0Acorresponding%20silhouettes.To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%0Ato%20explore%20the%20impact%20of%20data%20alignment%20on%20gait%20recognition.%20We%20conducted%0Aextensive%20experiments%20across%20multiple%20datasets%20and%20network%20architectures%2C%20and%0Athe%20results%20demonstrate%20the%20significant%20advantages%20of%20our%20proposed%20alignment%0Astrategy.Specifically%2C%20on%20the%20challenging%20Gait3D%20dataset%2C%20our%20method%20achieved%0Aan%20average%20performance%20improvement%20of%207.9%25%20across%20all%20evaluated%20networks.%0AFurthermore%2C%20our%20method%20achieves%20substantial%20improvements%20on%20cross-domain%0Adatasets%2C%20with%20accuracy%20improvements%20of%20up%20to%2024.0%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18830v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAGait%253A%2520Generalized%2520Skeleton-Guided%2520Data%2520Alignment%2520for%2520Gait%2520Recognition%26entry.906535625%3DZhengxian%2520Wu%2520and%2520Chuanrui%2520Zhang%2520and%2520Hangrui%2520Xu%2520and%2520Peng%2520Jiao%2520and%2520Haoqian%2520Wang%26entry.1292438233%3D%2520%2520Gait%2520recognition%2520is%2520emerging%2520as%2520a%2520promising%2520and%2520innovative%2520area%2520within%2520the%250Afield%2520of%2520computer%2520vision%252C%2520widely%2520applied%2520to%2520remote%2520person%2520identification.%250AAlthough%2520existing%2520gait%2520recognition%2520methods%2520have%2520achieved%2520substantial%2520success%2520in%250Acontrolled%2520laboratory%2520datasets%252C%2520their%2520performance%2520often%2520declines%2520significantly%250Awhen%2520transitioning%2520to%2520wild%2520datasets.We%2520argue%2520that%2520the%2520performance%2520gap%2520can%2520be%250Aprimarily%2520attributed%2520to%2520the%2520spatio-temporal%2520distribution%2520inconsistencies%250Apresent%2520in%2520wild%2520datasets%252C%2520where%2520subjects%2520appear%2520at%2520varying%2520angles%252C%2520positions%252C%250Aand%2520distances%2520across%2520the%2520frames.%2520To%2520achieve%2520accurate%2520gait%2520recognition%2520in%2520the%250Awild%252C%2520we%2520propose%2520a%2520skeleton-guided%2520silhouette%2520alignment%2520strategy%252C%2520which%2520uses%250Aprior%2520knowledge%2520of%2520the%2520skeletons%2520to%2520perform%2520affine%2520transformations%2520on%2520the%250Acorresponding%2520silhouettes.To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%250Ato%2520explore%2520the%2520impact%2520of%2520data%2520alignment%2520on%2520gait%2520recognition.%2520We%2520conducted%250Aextensive%2520experiments%2520across%2520multiple%2520datasets%2520and%2520network%2520architectures%252C%2520and%250Athe%2520results%2520demonstrate%2520the%2520significant%2520advantages%2520of%2520our%2520proposed%2520alignment%250Astrategy.Specifically%252C%2520on%2520the%2520challenging%2520Gait3D%2520dataset%252C%2520our%2520method%2520achieved%250Aan%2520average%2520performance%2520improvement%2520of%25207.9%2525%2520across%2520all%2520evaluated%2520networks.%250AFurthermore%252C%2520our%2520method%2520achieves%2520substantial%2520improvements%2520on%2520cross-domain%250Adatasets%252C%2520with%2520accuracy%2520improvements%2520of%2520up%2520to%252024.0%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18830v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAGait%3A%20Generalized%20Skeleton-Guided%20Data%20Alignment%20for%20Gait%20Recognition&entry.906535625=Zhengxian%20Wu%20and%20Chuanrui%20Zhang%20and%20Hangrui%20Xu%20and%20Peng%20Jiao%20and%20Haoqian%20Wang&entry.1292438233=%20%20Gait%20recognition%20is%20emerging%20as%20a%20promising%20and%20innovative%20area%20within%20the%0Afield%20of%20computer%20vision%2C%20widely%20applied%20to%20remote%20person%20identification.%0AAlthough%20existing%20gait%20recognition%20methods%20have%20achieved%20substantial%20success%20in%0Acontrolled%20laboratory%20datasets%2C%20their%20performance%20often%20declines%20significantly%0Awhen%20transitioning%20to%20wild%20datasets.We%20argue%20that%20the%20performance%20gap%20can%20be%0Aprimarily%20attributed%20to%20the%20spatio-temporal%20distribution%20inconsistencies%0Apresent%20in%20wild%20datasets%2C%20where%20subjects%20appear%20at%20varying%20angles%2C%20positions%2C%0Aand%20distances%20across%20the%20frames.%20To%20achieve%20accurate%20gait%20recognition%20in%20the%0Awild%2C%20we%20propose%20a%20skeleton-guided%20silhouette%20alignment%20strategy%2C%20which%20uses%0Aprior%20knowledge%20of%20the%20skeletons%20to%20perform%20affine%20transformations%20on%20the%0Acorresponding%20silhouettes.To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20study%0Ato%20explore%20the%20impact%20of%20data%20alignment%20on%20gait%20recognition.%20We%20conducted%0Aextensive%20experiments%20across%20multiple%20datasets%20and%20network%20architectures%2C%20and%0Athe%20results%20demonstrate%20the%20significant%20advantages%20of%20our%20proposed%20alignment%0Astrategy.Specifically%2C%20on%20the%20challenging%20Gait3D%20dataset%2C%20our%20method%20achieved%0Aan%20average%20performance%20improvement%20of%207.9%25%20across%20all%20evaluated%20networks.%0AFurthermore%2C%20our%20method%20achieves%20substantial%20improvements%20on%20cross-domain%0Adatasets%2C%20with%20accuracy%20improvements%20of%20up%20to%2024.0%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18830v2&entry.124074799=Read"},
{"title": "SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for\n  Class Incremental Learning with Small Memory", "author": "Hongyang Chen and Shaoling Pu and Lingyu Zheng and Zhongwu Sun", "abstract": "  In incremental learning, enhancing the generality of knowledge is crucial for\nadapting to dynamic data inputs. It can develop generalized representations or\nmore balanced decision boundaries, preventing the degradation of long-term\nknowledge over time and thus mitigating catastrophic forgetting. Some emerging\nincremental learning methods adopt an encoder-decoder architecture and have\nachieved promising results. In the encoder-decoder achitecture, improving the\ngeneralization capabilities of both the encoder and decoder is critical, as it\nhelps preserve previously learned knowledge while ensuring adaptability and\nrobustness to new, diverse data inputs. However, many existing continual\nmethods focus solely on enhancing one of the two components, which limits their\neffectiveness in mitigating catastrophic forgetting. And these methods perform\neven worse in small-memory scenarios, where only a limited number of historical\nsamples can be stored. To mitigate this limitation, we introduces SEDEG, a\ntwo-stage training framework for vision transformers (ViT), focusing on\nsequentially improving the generality of both Decoder and Encoder. Initially,\nSEDEG trains an ensembled encoder through feature boosting to learn generalized\nrepresentations, which subsequently enhance the decoder's generality and\nbalance the classifier. The next stage involves using knowledge distillation\n(KD) strategies to compress the ensembled encoder and develop a new, more\ngeneralized encoder. This involves using a balanced KD approach and feature KD\nfor effective knowledge transfer. Extensive experiments on three benchmark\ndatasets show SEDEG's superior performance, and ablation studies confirm the\nefficacy of its components. The code is available at\nhttps://github.com/ShaolingPu/CIL.\n", "link": "http://arxiv.org/abs/2508.12932v1", "date": "2025-08-18", "relevancy": 2.7858, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5619}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEDEG%3ASequential%20Enhancement%20of%20Decoder%20and%20Encoder%27s%20Generality%20for%0A%20%20Class%20Incremental%20Learning%20with%20Small%20Memory&body=Title%3A%20SEDEG%3ASequential%20Enhancement%20of%20Decoder%20and%20Encoder%27s%20Generality%20for%0A%20%20Class%20Incremental%20Learning%20with%20Small%20Memory%0AAuthor%3A%20Hongyang%20Chen%20and%20Shaoling%20Pu%20and%20Lingyu%20Zheng%20and%20Zhongwu%20Sun%0AAbstract%3A%20%20%20In%20incremental%20learning%2C%20enhancing%20the%20generality%20of%20knowledge%20is%20crucial%20for%0Aadapting%20to%20dynamic%20data%20inputs.%20It%20can%20develop%20generalized%20representations%20or%0Amore%20balanced%20decision%20boundaries%2C%20preventing%20the%20degradation%20of%20long-term%0Aknowledge%20over%20time%20and%20thus%20mitigating%20catastrophic%20forgetting.%20Some%20emerging%0Aincremental%20learning%20methods%20adopt%20an%20encoder-decoder%20architecture%20and%20have%0Aachieved%20promising%20results.%20In%20the%20encoder-decoder%20achitecture%2C%20improving%20the%0Ageneralization%20capabilities%20of%20both%20the%20encoder%20and%20decoder%20is%20critical%2C%20as%20it%0Ahelps%20preserve%20previously%20learned%20knowledge%20while%20ensuring%20adaptability%20and%0Arobustness%20to%20new%2C%20diverse%20data%20inputs.%20However%2C%20many%20existing%20continual%0Amethods%20focus%20solely%20on%20enhancing%20one%20of%20the%20two%20components%2C%20which%20limits%20their%0Aeffectiveness%20in%20mitigating%20catastrophic%20forgetting.%20And%20these%20methods%20perform%0Aeven%20worse%20in%20small-memory%20scenarios%2C%20where%20only%20a%20limited%20number%20of%20historical%0Asamples%20can%20be%20stored.%20To%20mitigate%20this%20limitation%2C%20we%20introduces%20SEDEG%2C%20a%0Atwo-stage%20training%20framework%20for%20vision%20transformers%20%28ViT%29%2C%20focusing%20on%0Asequentially%20improving%20the%20generality%20of%20both%20Decoder%20and%20Encoder.%20Initially%2C%0ASEDEG%20trains%20an%20ensembled%20encoder%20through%20feature%20boosting%20to%20learn%20generalized%0Arepresentations%2C%20which%20subsequently%20enhance%20the%20decoder%27s%20generality%20and%0Abalance%20the%20classifier.%20The%20next%20stage%20involves%20using%20knowledge%20distillation%0A%28KD%29%20strategies%20to%20compress%20the%20ensembled%20encoder%20and%20develop%20a%20new%2C%20more%0Ageneralized%20encoder.%20This%20involves%20using%20a%20balanced%20KD%20approach%20and%20feature%20KD%0Afor%20effective%20knowledge%20transfer.%20Extensive%20experiments%20on%20three%20benchmark%0Adatasets%20show%20SEDEG%27s%20superior%20performance%2C%20and%20ablation%20studies%20confirm%20the%0Aefficacy%20of%20its%20components.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ShaolingPu/CIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEDEG%253ASequential%2520Enhancement%2520of%2520Decoder%2520and%2520Encoder%2527s%2520Generality%2520for%250A%2520%2520Class%2520Incremental%2520Learning%2520with%2520Small%2520Memory%26entry.906535625%3DHongyang%2520Chen%2520and%2520Shaoling%2520Pu%2520and%2520Lingyu%2520Zheng%2520and%2520Zhongwu%2520Sun%26entry.1292438233%3D%2520%2520In%2520incremental%2520learning%252C%2520enhancing%2520the%2520generality%2520of%2520knowledge%2520is%2520crucial%2520for%250Aadapting%2520to%2520dynamic%2520data%2520inputs.%2520It%2520can%2520develop%2520generalized%2520representations%2520or%250Amore%2520balanced%2520decision%2520boundaries%252C%2520preventing%2520the%2520degradation%2520of%2520long-term%250Aknowledge%2520over%2520time%2520and%2520thus%2520mitigating%2520catastrophic%2520forgetting.%2520Some%2520emerging%250Aincremental%2520learning%2520methods%2520adopt%2520an%2520encoder-decoder%2520architecture%2520and%2520have%250Aachieved%2520promising%2520results.%2520In%2520the%2520encoder-decoder%2520achitecture%252C%2520improving%2520the%250Ageneralization%2520capabilities%2520of%2520both%2520the%2520encoder%2520and%2520decoder%2520is%2520critical%252C%2520as%2520it%250Ahelps%2520preserve%2520previously%2520learned%2520knowledge%2520while%2520ensuring%2520adaptability%2520and%250Arobustness%2520to%2520new%252C%2520diverse%2520data%2520inputs.%2520However%252C%2520many%2520existing%2520continual%250Amethods%2520focus%2520solely%2520on%2520enhancing%2520one%2520of%2520the%2520two%2520components%252C%2520which%2520limits%2520their%250Aeffectiveness%2520in%2520mitigating%2520catastrophic%2520forgetting.%2520And%2520these%2520methods%2520perform%250Aeven%2520worse%2520in%2520small-memory%2520scenarios%252C%2520where%2520only%2520a%2520limited%2520number%2520of%2520historical%250Asamples%2520can%2520be%2520stored.%2520To%2520mitigate%2520this%2520limitation%252C%2520we%2520introduces%2520SEDEG%252C%2520a%250Atwo-stage%2520training%2520framework%2520for%2520vision%2520transformers%2520%2528ViT%2529%252C%2520focusing%2520on%250Asequentially%2520improving%2520the%2520generality%2520of%2520both%2520Decoder%2520and%2520Encoder.%2520Initially%252C%250ASEDEG%2520trains%2520an%2520ensembled%2520encoder%2520through%2520feature%2520boosting%2520to%2520learn%2520generalized%250Arepresentations%252C%2520which%2520subsequently%2520enhance%2520the%2520decoder%2527s%2520generality%2520and%250Abalance%2520the%2520classifier.%2520The%2520next%2520stage%2520involves%2520using%2520knowledge%2520distillation%250A%2528KD%2529%2520strategies%2520to%2520compress%2520the%2520ensembled%2520encoder%2520and%2520develop%2520a%2520new%252C%2520more%250Ageneralized%2520encoder.%2520This%2520involves%2520using%2520a%2520balanced%2520KD%2520approach%2520and%2520feature%2520KD%250Afor%2520effective%2520knowledge%2520transfer.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%250Adatasets%2520show%2520SEDEG%2527s%2520superior%2520performance%252C%2520and%2520ablation%2520studies%2520confirm%2520the%250Aefficacy%2520of%2520its%2520components.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ShaolingPu/CIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEDEG%3ASequential%20Enhancement%20of%20Decoder%20and%20Encoder%27s%20Generality%20for%0A%20%20Class%20Incremental%20Learning%20with%20Small%20Memory&entry.906535625=Hongyang%20Chen%20and%20Shaoling%20Pu%20and%20Lingyu%20Zheng%20and%20Zhongwu%20Sun&entry.1292438233=%20%20In%20incremental%20learning%2C%20enhancing%20the%20generality%20of%20knowledge%20is%20crucial%20for%0Aadapting%20to%20dynamic%20data%20inputs.%20It%20can%20develop%20generalized%20representations%20or%0Amore%20balanced%20decision%20boundaries%2C%20preventing%20the%20degradation%20of%20long-term%0Aknowledge%20over%20time%20and%20thus%20mitigating%20catastrophic%20forgetting.%20Some%20emerging%0Aincremental%20learning%20methods%20adopt%20an%20encoder-decoder%20architecture%20and%20have%0Aachieved%20promising%20results.%20In%20the%20encoder-decoder%20achitecture%2C%20improving%20the%0Ageneralization%20capabilities%20of%20both%20the%20encoder%20and%20decoder%20is%20critical%2C%20as%20it%0Ahelps%20preserve%20previously%20learned%20knowledge%20while%20ensuring%20adaptability%20and%0Arobustness%20to%20new%2C%20diverse%20data%20inputs.%20However%2C%20many%20existing%20continual%0Amethods%20focus%20solely%20on%20enhancing%20one%20of%20the%20two%20components%2C%20which%20limits%20their%0Aeffectiveness%20in%20mitigating%20catastrophic%20forgetting.%20And%20these%20methods%20perform%0Aeven%20worse%20in%20small-memory%20scenarios%2C%20where%20only%20a%20limited%20number%20of%20historical%0Asamples%20can%20be%20stored.%20To%20mitigate%20this%20limitation%2C%20we%20introduces%20SEDEG%2C%20a%0Atwo-stage%20training%20framework%20for%20vision%20transformers%20%28ViT%29%2C%20focusing%20on%0Asequentially%20improving%20the%20generality%20of%20both%20Decoder%20and%20Encoder.%20Initially%2C%0ASEDEG%20trains%20an%20ensembled%20encoder%20through%20feature%20boosting%20to%20learn%20generalized%0Arepresentations%2C%20which%20subsequently%20enhance%20the%20decoder%27s%20generality%20and%0Abalance%20the%20classifier.%20The%20next%20stage%20involves%20using%20knowledge%20distillation%0A%28KD%29%20strategies%20to%20compress%20the%20ensembled%20encoder%20and%20develop%20a%20new%2C%20more%0Ageneralized%20encoder.%20This%20involves%20using%20a%20balanced%20KD%20approach%20and%20feature%20KD%0Afor%20effective%20knowledge%20transfer.%20Extensive%20experiments%20on%20three%20benchmark%0Adatasets%20show%20SEDEG%27s%20superior%20performance%2C%20and%20ablation%20studies%20confirm%20the%0Aefficacy%20of%20its%20components.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ShaolingPu/CIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12932v1&entry.124074799=Read"},
{"title": "Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization\n  with Vision-Language Models", "author": "Dexia Chen and Wentao Zhang and Qianjie Zhu and Ping Hu and Weibing Li and Tong Zhang and Ruixuan Wang", "abstract": "  Vision-language models (VLMs) pre-trained on natural image and language data,\nsuch as CLIP, have exhibited significant potential in few-shot image\nrecognition tasks, leading to development of various efficient transfer\nlearning methods. These methods exploit inherent pre-learned knowledge in VLMs\nand have achieved strong performance on standard image datasets. However, their\neffectiveness is often limited when confronted with cross-domain tasks where\nimaging domains differ from natural images. To address this limitation, we\npropose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a\nnovel fine-tuning strategy for VLMs. This strategy employs two functionally\ncomplementary expert modules to extract multi-view features, while\nincorporating prior knowledge-based consistency constraints and information\ngeometry-based consensus mechanisms to enhance the robustness of feature\nlearning. Additionally, a new cross-domain few-shot benchmark is established to\nhelp comprehensively evaluate methods on imaging domains distinct from natural\nimages. Extensive empirical evaluations on both existing and newly proposed\nbenchmarks suggest CoMuCo consistently outperforms current methods in few-shot\ntasks. The code and benchmark will be released.\n", "link": "http://arxiv.org/abs/2508.12861v1", "date": "2025-08-18", "relevancy": 2.785, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Few-Shot%20Learning%20via%20Multi-View%20Collaborative%20Optimization%0A%20%20with%20Vision-Language%20Models&body=Title%3A%20Cross-Domain%20Few-Shot%20Learning%20via%20Multi-View%20Collaborative%20Optimization%0A%20%20with%20Vision-Language%20Models%0AAuthor%3A%20Dexia%20Chen%20and%20Wentao%20Zhang%20and%20Qianjie%20Zhu%20and%20Ping%20Hu%20and%20Weibing%20Li%20and%20Tong%20Zhang%20and%20Ruixuan%20Wang%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20pre-trained%20on%20natural%20image%20and%20language%20data%2C%0Asuch%20as%20CLIP%2C%20have%20exhibited%20significant%20potential%20in%20few-shot%20image%0Arecognition%20tasks%2C%20leading%20to%20development%20of%20various%20efficient%20transfer%0Alearning%20methods.%20These%20methods%20exploit%20inherent%20pre-learned%20knowledge%20in%20VLMs%0Aand%20have%20achieved%20strong%20performance%20on%20standard%20image%20datasets.%20However%2C%20their%0Aeffectiveness%20is%20often%20limited%20when%20confronted%20with%20cross-domain%20tasks%20where%0Aimaging%20domains%20differ%20from%20natural%20images.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Consistency-guided%20Multi-view%20Collaborative%20Optimization%20%28CoMuCo%29%2C%20a%0Anovel%20fine-tuning%20strategy%20for%20VLMs.%20This%20strategy%20employs%20two%20functionally%0Acomplementary%20expert%20modules%20to%20extract%20multi-view%20features%2C%20while%0Aincorporating%20prior%20knowledge-based%20consistency%20constraints%20and%20information%0Ageometry-based%20consensus%20mechanisms%20to%20enhance%20the%20robustness%20of%20feature%0Alearning.%20Additionally%2C%20a%20new%20cross-domain%20few-shot%20benchmark%20is%20established%20to%0Ahelp%20comprehensively%20evaluate%20methods%20on%20imaging%20domains%20distinct%20from%20natural%0Aimages.%20Extensive%20empirical%20evaluations%20on%20both%20existing%20and%20newly%20proposed%0Abenchmarks%20suggest%20CoMuCo%20consistently%20outperforms%20current%20methods%20in%20few-shot%0Atasks.%20The%20code%20and%20benchmark%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Domain%2520Few-Shot%2520Learning%2520via%2520Multi-View%2520Collaborative%2520Optimization%250A%2520%2520with%2520Vision-Language%2520Models%26entry.906535625%3DDexia%2520Chen%2520and%2520Wentao%2520Zhang%2520and%2520Qianjie%2520Zhu%2520and%2520Ping%2520Hu%2520and%2520Weibing%2520Li%2520and%2520Tong%2520Zhang%2520and%2520Ruixuan%2520Wang%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520pre-trained%2520on%2520natural%2520image%2520and%2520language%2520data%252C%250Asuch%2520as%2520CLIP%252C%2520have%2520exhibited%2520significant%2520potential%2520in%2520few-shot%2520image%250Arecognition%2520tasks%252C%2520leading%2520to%2520development%2520of%2520various%2520efficient%2520transfer%250Alearning%2520methods.%2520These%2520methods%2520exploit%2520inherent%2520pre-learned%2520knowledge%2520in%2520VLMs%250Aand%2520have%2520achieved%2520strong%2520performance%2520on%2520standard%2520image%2520datasets.%2520However%252C%2520their%250Aeffectiveness%2520is%2520often%2520limited%2520when%2520confronted%2520with%2520cross-domain%2520tasks%2520where%250Aimaging%2520domains%2520differ%2520from%2520natural%2520images.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apropose%2520Consistency-guided%2520Multi-view%2520Collaborative%2520Optimization%2520%2528CoMuCo%2529%252C%2520a%250Anovel%2520fine-tuning%2520strategy%2520for%2520VLMs.%2520This%2520strategy%2520employs%2520two%2520functionally%250Acomplementary%2520expert%2520modules%2520to%2520extract%2520multi-view%2520features%252C%2520while%250Aincorporating%2520prior%2520knowledge-based%2520consistency%2520constraints%2520and%2520information%250Ageometry-based%2520consensus%2520mechanisms%2520to%2520enhance%2520the%2520robustness%2520of%2520feature%250Alearning.%2520Additionally%252C%2520a%2520new%2520cross-domain%2520few-shot%2520benchmark%2520is%2520established%2520to%250Ahelp%2520comprehensively%2520evaluate%2520methods%2520on%2520imaging%2520domains%2520distinct%2520from%2520natural%250Aimages.%2520Extensive%2520empirical%2520evaluations%2520on%2520both%2520existing%2520and%2520newly%2520proposed%250Abenchmarks%2520suggest%2520CoMuCo%2520consistently%2520outperforms%2520current%2520methods%2520in%2520few-shot%250Atasks.%2520The%2520code%2520and%2520benchmark%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Few-Shot%20Learning%20via%20Multi-View%20Collaborative%20Optimization%0A%20%20with%20Vision-Language%20Models&entry.906535625=Dexia%20Chen%20and%20Wentao%20Zhang%20and%20Qianjie%20Zhu%20and%20Ping%20Hu%20and%20Weibing%20Li%20and%20Tong%20Zhang%20and%20Ruixuan%20Wang&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20pre-trained%20on%20natural%20image%20and%20language%20data%2C%0Asuch%20as%20CLIP%2C%20have%20exhibited%20significant%20potential%20in%20few-shot%20image%0Arecognition%20tasks%2C%20leading%20to%20development%20of%20various%20efficient%20transfer%0Alearning%20methods.%20These%20methods%20exploit%20inherent%20pre-learned%20knowledge%20in%20VLMs%0Aand%20have%20achieved%20strong%20performance%20on%20standard%20image%20datasets.%20However%2C%20their%0Aeffectiveness%20is%20often%20limited%20when%20confronted%20with%20cross-domain%20tasks%20where%0Aimaging%20domains%20differ%20from%20natural%20images.%20To%20address%20this%20limitation%2C%20we%0Apropose%20Consistency-guided%20Multi-view%20Collaborative%20Optimization%20%28CoMuCo%29%2C%20a%0Anovel%20fine-tuning%20strategy%20for%20VLMs.%20This%20strategy%20employs%20two%20functionally%0Acomplementary%20expert%20modules%20to%20extract%20multi-view%20features%2C%20while%0Aincorporating%20prior%20knowledge-based%20consistency%20constraints%20and%20information%0Ageometry-based%20consensus%20mechanisms%20to%20enhance%20the%20robustness%20of%20feature%0Alearning.%20Additionally%2C%20a%20new%20cross-domain%20few-shot%20benchmark%20is%20established%20to%0Ahelp%20comprehensively%20evaluate%20methods%20on%20imaging%20domains%20distinct%20from%20natural%0Aimages.%20Extensive%20empirical%20evaluations%20on%20both%20existing%20and%20newly%20proposed%0Abenchmarks%20suggest%20CoMuCo%20consistently%20outperforms%20current%20methods%20in%20few-shot%0Atasks.%20The%20code%20and%20benchmark%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12861v1&entry.124074799=Read"},
{"title": "Empirical Evidences for the Effects of Feature Diversity in Open Set\n  Recognition and Continual Learning", "author": "Jiawen Xu and Odej Kao", "abstract": "  Open set recognition (OSR) and continual learning are two critical challenges\nin machine learning, focusing respectively on detecting novel classes at\ninference time and updating models to incorporate the new classes. While many\nrecent approaches have addressed these problems, particularly OSR, by\nheuristically promoting feature diversity, few studies have directly examined\nthe role that feature diversity plays in tackling them. In this work, we\nprovide empirical evidence that enhancing feature diversity improves the\nrecognition of open set samples. Moreover, increased feature diversity also\nfacilitates both the retention of previously learned data and the integration\nof new data in continual learning. We hope our findings can inspire further\nresearch into both practical methods and theoretical understanding in these\ndomains.\n", "link": "http://arxiv.org/abs/2508.13005v1", "date": "2025-08-18", "relevancy": 2.7317, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5605}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20Evidences%20for%20the%20Effects%20of%20Feature%20Diversity%20in%20Open%20Set%0A%20%20Recognition%20and%20Continual%20Learning&body=Title%3A%20Empirical%20Evidences%20for%20the%20Effects%20of%20Feature%20Diversity%20in%20Open%20Set%0A%20%20Recognition%20and%20Continual%20Learning%0AAuthor%3A%20Jiawen%20Xu%20and%20Odej%20Kao%0AAbstract%3A%20%20%20Open%20set%20recognition%20%28OSR%29%20and%20continual%20learning%20are%20two%20critical%20challenges%0Ain%20machine%20learning%2C%20focusing%20respectively%20on%20detecting%20novel%20classes%20at%0Ainference%20time%20and%20updating%20models%20to%20incorporate%20the%20new%20classes.%20While%20many%0Arecent%20approaches%20have%20addressed%20these%20problems%2C%20particularly%20OSR%2C%20by%0Aheuristically%20promoting%20feature%20diversity%2C%20few%20studies%20have%20directly%20examined%0Athe%20role%20that%20feature%20diversity%20plays%20in%20tackling%20them.%20In%20this%20work%2C%20we%0Aprovide%20empirical%20evidence%20that%20enhancing%20feature%20diversity%20improves%20the%0Arecognition%20of%20open%20set%20samples.%20Moreover%2C%20increased%20feature%20diversity%20also%0Afacilitates%20both%20the%20retention%20of%20previously%20learned%20data%20and%20the%20integration%0Aof%20new%20data%20in%20continual%20learning.%20We%20hope%20our%20findings%20can%20inspire%20further%0Aresearch%20into%20both%20practical%20methods%20and%20theoretical%20understanding%20in%20these%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520Evidences%2520for%2520the%2520Effects%2520of%2520Feature%2520Diversity%2520in%2520Open%2520Set%250A%2520%2520Recognition%2520and%2520Continual%2520Learning%26entry.906535625%3DJiawen%2520Xu%2520and%2520Odej%2520Kao%26entry.1292438233%3D%2520%2520Open%2520set%2520recognition%2520%2528OSR%2529%2520and%2520continual%2520learning%2520are%2520two%2520critical%2520challenges%250Ain%2520machine%2520learning%252C%2520focusing%2520respectively%2520on%2520detecting%2520novel%2520classes%2520at%250Ainference%2520time%2520and%2520updating%2520models%2520to%2520incorporate%2520the%2520new%2520classes.%2520While%2520many%250Arecent%2520approaches%2520have%2520addressed%2520these%2520problems%252C%2520particularly%2520OSR%252C%2520by%250Aheuristically%2520promoting%2520feature%2520diversity%252C%2520few%2520studies%2520have%2520directly%2520examined%250Athe%2520role%2520that%2520feature%2520diversity%2520plays%2520in%2520tackling%2520them.%2520In%2520this%2520work%252C%2520we%250Aprovide%2520empirical%2520evidence%2520that%2520enhancing%2520feature%2520diversity%2520improves%2520the%250Arecognition%2520of%2520open%2520set%2520samples.%2520Moreover%252C%2520increased%2520feature%2520diversity%2520also%250Afacilitates%2520both%2520the%2520retention%2520of%2520previously%2520learned%2520data%2520and%2520the%2520integration%250Aof%2520new%2520data%2520in%2520continual%2520learning.%2520We%2520hope%2520our%2520findings%2520can%2520inspire%2520further%250Aresearch%2520into%2520both%2520practical%2520methods%2520and%2520theoretical%2520understanding%2520in%2520these%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20Evidences%20for%20the%20Effects%20of%20Feature%20Diversity%20in%20Open%20Set%0A%20%20Recognition%20and%20Continual%20Learning&entry.906535625=Jiawen%20Xu%20and%20Odej%20Kao&entry.1292438233=%20%20Open%20set%20recognition%20%28OSR%29%20and%20continual%20learning%20are%20two%20critical%20challenges%0Ain%20machine%20learning%2C%20focusing%20respectively%20on%20detecting%20novel%20classes%20at%0Ainference%20time%20and%20updating%20models%20to%20incorporate%20the%20new%20classes.%20While%20many%0Arecent%20approaches%20have%20addressed%20these%20problems%2C%20particularly%20OSR%2C%20by%0Aheuristically%20promoting%20feature%20diversity%2C%20few%20studies%20have%20directly%20examined%0Athe%20role%20that%20feature%20diversity%20plays%20in%20tackling%20them.%20In%20this%20work%2C%20we%0Aprovide%20empirical%20evidence%20that%20enhancing%20feature%20diversity%20improves%20the%0Arecognition%20of%20open%20set%20samples.%20Moreover%2C%20increased%20feature%20diversity%20also%0Afacilitates%20both%20the%20retention%20of%20previously%20learned%20data%20and%20the%20integration%0Aof%20new%20data%20in%20continual%20learning.%20We%20hope%20our%20findings%20can%20inspire%20further%0Aresearch%20into%20both%20practical%20methods%20and%20theoretical%20understanding%20in%20these%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13005v1&entry.124074799=Read"},
{"title": "HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical\n  Feature Adapters", "author": "Ruru Xu and Ilkay Oksuz", "abstract": "  Deep learning-based cardiac MRI reconstruction faces significant domain shift\nchallenges when deployed across multiple clinical centers with heterogeneous\nscanner configurations and imaging protocols. We propose HierAdaptMR, a\nhierarchical feature adaptation framework that addresses multi-level domain\nvariations through parameter-efficient adapters. Our method employs\nProtocol-Level Adapters for sequence-specific characteristics and Center-Level\nAdapters for scanner-dependent variations, built upon a variational unrolling\nbackbone. A Universal Adapter enables generalization to entirely unseen centers\nthrough stochastic training that learns center-invariant adaptations. The\nframework utilizes multi-scale SSIM loss with frequency domain enhancement and\ncontrast-adaptive weighting for robust optimization. Comprehensive evaluation\non the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9\nmodalities demonstrates superior cross-center generalization while maintaining\nreconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR\n", "link": "http://arxiv.org/abs/2508.13026v1", "date": "2025-08-18", "relevancy": 2.7181, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5781}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5364}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HierAdaptMR%3A%20Cross-Center%20Cardiac%20MRI%20Reconstruction%20with%20Hierarchical%0A%20%20Feature%20Adapters&body=Title%3A%20HierAdaptMR%3A%20Cross-Center%20Cardiac%20MRI%20Reconstruction%20with%20Hierarchical%0A%20%20Feature%20Adapters%0AAuthor%3A%20Ruru%20Xu%20and%20Ilkay%20Oksuz%0AAbstract%3A%20%20%20Deep%20learning-based%20cardiac%20MRI%20reconstruction%20faces%20significant%20domain%20shift%0Achallenges%20when%20deployed%20across%20multiple%20clinical%20centers%20with%20heterogeneous%0Ascanner%20configurations%20and%20imaging%20protocols.%20We%20propose%20HierAdaptMR%2C%20a%0Ahierarchical%20feature%20adaptation%20framework%20that%20addresses%20multi-level%20domain%0Avariations%20through%20parameter-efficient%20adapters.%20Our%20method%20employs%0AProtocol-Level%20Adapters%20for%20sequence-specific%20characteristics%20and%20Center-Level%0AAdapters%20for%20scanner-dependent%20variations%2C%20built%20upon%20a%20variational%20unrolling%0Abackbone.%20A%20Universal%20Adapter%20enables%20generalization%20to%20entirely%20unseen%20centers%0Athrough%20stochastic%20training%20that%20learns%20center-invariant%20adaptations.%20The%0Aframework%20utilizes%20multi-scale%20SSIM%20loss%20with%20frequency%20domain%20enhancement%20and%0Acontrast-adaptive%20weighting%20for%20robust%20optimization.%20Comprehensive%20evaluation%0Aon%20the%20CMRxRecon2025%20dataset%20spanning%205%2B%20centers%2C%2010%2B%20scanners%2C%20and%209%0Amodalities%20demonstrates%20superior%20cross-center%20generalization%20while%20maintaining%0Areconstruction%20quality.%20code%3A%20https%3A//github.com/Ruru-Xu/HierAdaptMR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierAdaptMR%253A%2520Cross-Center%2520Cardiac%2520MRI%2520Reconstruction%2520with%2520Hierarchical%250A%2520%2520Feature%2520Adapters%26entry.906535625%3DRuru%2520Xu%2520and%2520Ilkay%2520Oksuz%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520cardiac%2520MRI%2520reconstruction%2520faces%2520significant%2520domain%2520shift%250Achallenges%2520when%2520deployed%2520across%2520multiple%2520clinical%2520centers%2520with%2520heterogeneous%250Ascanner%2520configurations%2520and%2520imaging%2520protocols.%2520We%2520propose%2520HierAdaptMR%252C%2520a%250Ahierarchical%2520feature%2520adaptation%2520framework%2520that%2520addresses%2520multi-level%2520domain%250Avariations%2520through%2520parameter-efficient%2520adapters.%2520Our%2520method%2520employs%250AProtocol-Level%2520Adapters%2520for%2520sequence-specific%2520characteristics%2520and%2520Center-Level%250AAdapters%2520for%2520scanner-dependent%2520variations%252C%2520built%2520upon%2520a%2520variational%2520unrolling%250Abackbone.%2520A%2520Universal%2520Adapter%2520enables%2520generalization%2520to%2520entirely%2520unseen%2520centers%250Athrough%2520stochastic%2520training%2520that%2520learns%2520center-invariant%2520adaptations.%2520The%250Aframework%2520utilizes%2520multi-scale%2520SSIM%2520loss%2520with%2520frequency%2520domain%2520enhancement%2520and%250Acontrast-adaptive%2520weighting%2520for%2520robust%2520optimization.%2520Comprehensive%2520evaluation%250Aon%2520the%2520CMRxRecon2025%2520dataset%2520spanning%25205%252B%2520centers%252C%252010%252B%2520scanners%252C%2520and%25209%250Amodalities%2520demonstrates%2520superior%2520cross-center%2520generalization%2520while%2520maintaining%250Areconstruction%2520quality.%2520code%253A%2520https%253A//github.com/Ruru-Xu/HierAdaptMR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HierAdaptMR%3A%20Cross-Center%20Cardiac%20MRI%20Reconstruction%20with%20Hierarchical%0A%20%20Feature%20Adapters&entry.906535625=Ruru%20Xu%20and%20Ilkay%20Oksuz&entry.1292438233=%20%20Deep%20learning-based%20cardiac%20MRI%20reconstruction%20faces%20significant%20domain%20shift%0Achallenges%20when%20deployed%20across%20multiple%20clinical%20centers%20with%20heterogeneous%0Ascanner%20configurations%20and%20imaging%20protocols.%20We%20propose%20HierAdaptMR%2C%20a%0Ahierarchical%20feature%20adaptation%20framework%20that%20addresses%20multi-level%20domain%0Avariations%20through%20parameter-efficient%20adapters.%20Our%20method%20employs%0AProtocol-Level%20Adapters%20for%20sequence-specific%20characteristics%20and%20Center-Level%0AAdapters%20for%20scanner-dependent%20variations%2C%20built%20upon%20a%20variational%20unrolling%0Abackbone.%20A%20Universal%20Adapter%20enables%20generalization%20to%20entirely%20unseen%20centers%0Athrough%20stochastic%20training%20that%20learns%20center-invariant%20adaptations.%20The%0Aframework%20utilizes%20multi-scale%20SSIM%20loss%20with%20frequency%20domain%20enhancement%20and%0Acontrast-adaptive%20weighting%20for%20robust%20optimization.%20Comprehensive%20evaluation%0Aon%20the%20CMRxRecon2025%20dataset%20spanning%205%2B%20centers%2C%2010%2B%20scanners%2C%20and%209%0Amodalities%20demonstrates%20superior%20cross-center%20generalization%20while%20maintaining%0Areconstruction%20quality.%20code%3A%20https%3A//github.com/Ruru-Xu/HierAdaptMR%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13026v1&entry.124074799=Read"},
{"title": "Deep Positive-Negative Prototypes for Adversarially Robust\n  Discriminative Prototypical Learning", "author": "Ramin Zarei Sabzevar and Hamed Mohammadzadeh and Tahmineh Tavakoli and Ahad Harati", "abstract": "  Despite the advantages of discriminative prototype-based methods, their role\nin adversarial robustness remains underexplored. Meanwhile, current adversarial\ntraining methods predominantly focus on robustness against adversarial attacks\nwithout explicitly leveraging geometric structures in the latent space, usually\nresulting in reduced accuracy on the original clean data. We propose a novel\nframework named Adversarially trained Deep Positive-Negative Prototypes\n(Adv-DPNP), which integrates discriminative prototype-based learning with\nadversarial training. Adv-DPNP uses unified class prototypes that serve as both\nclassifier weights and robust anchors in the latent space. Moreover, a novel\ndual-branch training mechanism maintains stable prototypes by updating them\nexclusively with clean data, while the feature extractor is trained on both\nclean and adversarial inputs to increase invariance to adversarial\nperturbations. In addition, we use a composite loss that combines\npositive-prototype alignment, negative-prototype repulsion, and consistency\nregularization to further enhance discrimination, adversarial robustness, and\nclean accuracy. Extensive experiments on standard benchmarks (CIFAR-10/100 and\nSVHN) confirm that Adv-DPNP improves clean accuracy over state-of-the-art\ndefenses and baseline methods, while maintaining competitive or superior\nrobustness under a suite of widely used attacks, including FGSM, PGD, C\\&W, and\nAutoAttack. We also evaluate robustness to common corruptions on CIFAR-10-C,\nwhere Adv-DPNP achieves the highest average accuracy across severities and\ncorruption types. Additionally, we provide an in-depth analysis of the\ndiscriminative quality of the learned feature representations, highlighting the\neffectiveness of Adv-DPNP in maintaining compactness and clear separation in\nthe latent space.\n", "link": "http://arxiv.org/abs/2504.03782v2", "date": "2025-08-18", "relevancy": 2.6914, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5438}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5388}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Positive-Negative%20Prototypes%20for%20Adversarially%20Robust%0A%20%20Discriminative%20Prototypical%20Learning&body=Title%3A%20Deep%20Positive-Negative%20Prototypes%20for%20Adversarially%20Robust%0A%20%20Discriminative%20Prototypical%20Learning%0AAuthor%3A%20Ramin%20Zarei%20Sabzevar%20and%20Hamed%20Mohammadzadeh%20and%20Tahmineh%20Tavakoli%20and%20Ahad%20Harati%0AAbstract%3A%20%20%20Despite%20the%20advantages%20of%20discriminative%20prototype-based%20methods%2C%20their%20role%0Ain%20adversarial%20robustness%20remains%20underexplored.%20Meanwhile%2C%20current%20adversarial%0Atraining%20methods%20predominantly%20focus%20on%20robustness%20against%20adversarial%20attacks%0Awithout%20explicitly%20leveraging%20geometric%20structures%20in%20the%20latent%20space%2C%20usually%0Aresulting%20in%20reduced%20accuracy%20on%20the%20original%20clean%20data.%20We%20propose%20a%20novel%0Aframework%20named%20Adversarially%20trained%20Deep%20Positive-Negative%20Prototypes%0A%28Adv-DPNP%29%2C%20which%20integrates%20discriminative%20prototype-based%20learning%20with%0Aadversarial%20training.%20Adv-DPNP%20uses%20unified%20class%20prototypes%20that%20serve%20as%20both%0Aclassifier%20weights%20and%20robust%20anchors%20in%20the%20latent%20space.%20Moreover%2C%20a%20novel%0Adual-branch%20training%20mechanism%20maintains%20stable%20prototypes%20by%20updating%20them%0Aexclusively%20with%20clean%20data%2C%20while%20the%20feature%20extractor%20is%20trained%20on%20both%0Aclean%20and%20adversarial%20inputs%20to%20increase%20invariance%20to%20adversarial%0Aperturbations.%20In%20addition%2C%20we%20use%20a%20composite%20loss%20that%20combines%0Apositive-prototype%20alignment%2C%20negative-prototype%20repulsion%2C%20and%20consistency%0Aregularization%20to%20further%20enhance%20discrimination%2C%20adversarial%20robustness%2C%20and%0Aclean%20accuracy.%20Extensive%20experiments%20on%20standard%20benchmarks%20%28CIFAR-10/100%20and%0ASVHN%29%20confirm%20that%20Adv-DPNP%20improves%20clean%20accuracy%20over%20state-of-the-art%0Adefenses%20and%20baseline%20methods%2C%20while%20maintaining%20competitive%20or%20superior%0Arobustness%20under%20a%20suite%20of%20widely%20used%20attacks%2C%20including%20FGSM%2C%20PGD%2C%20C%5C%26W%2C%20and%0AAutoAttack.%20We%20also%20evaluate%20robustness%20to%20common%20corruptions%20on%20CIFAR-10-C%2C%0Awhere%20Adv-DPNP%20achieves%20the%20highest%20average%20accuracy%20across%20severities%20and%0Acorruption%20types.%20Additionally%2C%20we%20provide%20an%20in-depth%20analysis%20of%20the%0Adiscriminative%20quality%20of%20the%20learned%20feature%20representations%2C%20highlighting%20the%0Aeffectiveness%20of%20Adv-DPNP%20in%20maintaining%20compactness%20and%20clear%20separation%20in%0Athe%20latent%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03782v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Positive-Negative%2520Prototypes%2520for%2520Adversarially%2520Robust%250A%2520%2520Discriminative%2520Prototypical%2520Learning%26entry.906535625%3DRamin%2520Zarei%2520Sabzevar%2520and%2520Hamed%2520Mohammadzadeh%2520and%2520Tahmineh%2520Tavakoli%2520and%2520Ahad%2520Harati%26entry.1292438233%3D%2520%2520Despite%2520the%2520advantages%2520of%2520discriminative%2520prototype-based%2520methods%252C%2520their%2520role%250Ain%2520adversarial%2520robustness%2520remains%2520underexplored.%2520Meanwhile%252C%2520current%2520adversarial%250Atraining%2520methods%2520predominantly%2520focus%2520on%2520robustness%2520against%2520adversarial%2520attacks%250Awithout%2520explicitly%2520leveraging%2520geometric%2520structures%2520in%2520the%2520latent%2520space%252C%2520usually%250Aresulting%2520in%2520reduced%2520accuracy%2520on%2520the%2520original%2520clean%2520data.%2520We%2520propose%2520a%2520novel%250Aframework%2520named%2520Adversarially%2520trained%2520Deep%2520Positive-Negative%2520Prototypes%250A%2528Adv-DPNP%2529%252C%2520which%2520integrates%2520discriminative%2520prototype-based%2520learning%2520with%250Aadversarial%2520training.%2520Adv-DPNP%2520uses%2520unified%2520class%2520prototypes%2520that%2520serve%2520as%2520both%250Aclassifier%2520weights%2520and%2520robust%2520anchors%2520in%2520the%2520latent%2520space.%2520Moreover%252C%2520a%2520novel%250Adual-branch%2520training%2520mechanism%2520maintains%2520stable%2520prototypes%2520by%2520updating%2520them%250Aexclusively%2520with%2520clean%2520data%252C%2520while%2520the%2520feature%2520extractor%2520is%2520trained%2520on%2520both%250Aclean%2520and%2520adversarial%2520inputs%2520to%2520increase%2520invariance%2520to%2520adversarial%250Aperturbations.%2520In%2520addition%252C%2520we%2520use%2520a%2520composite%2520loss%2520that%2520combines%250Apositive-prototype%2520alignment%252C%2520negative-prototype%2520repulsion%252C%2520and%2520consistency%250Aregularization%2520to%2520further%2520enhance%2520discrimination%252C%2520adversarial%2520robustness%252C%2520and%250Aclean%2520accuracy.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%2520%2528CIFAR-10/100%2520and%250ASVHN%2529%2520confirm%2520that%2520Adv-DPNP%2520improves%2520clean%2520accuracy%2520over%2520state-of-the-art%250Adefenses%2520and%2520baseline%2520methods%252C%2520while%2520maintaining%2520competitive%2520or%2520superior%250Arobustness%2520under%2520a%2520suite%2520of%2520widely%2520used%2520attacks%252C%2520including%2520FGSM%252C%2520PGD%252C%2520C%255C%2526W%252C%2520and%250AAutoAttack.%2520We%2520also%2520evaluate%2520robustness%2520to%2520common%2520corruptions%2520on%2520CIFAR-10-C%252C%250Awhere%2520Adv-DPNP%2520achieves%2520the%2520highest%2520average%2520accuracy%2520across%2520severities%2520and%250Acorruption%2520types.%2520Additionally%252C%2520we%2520provide%2520an%2520in-depth%2520analysis%2520of%2520the%250Adiscriminative%2520quality%2520of%2520the%2520learned%2520feature%2520representations%252C%2520highlighting%2520the%250Aeffectiveness%2520of%2520Adv-DPNP%2520in%2520maintaining%2520compactness%2520and%2520clear%2520separation%2520in%250Athe%2520latent%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03782v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Positive-Negative%20Prototypes%20for%20Adversarially%20Robust%0A%20%20Discriminative%20Prototypical%20Learning&entry.906535625=Ramin%20Zarei%20Sabzevar%20and%20Hamed%20Mohammadzadeh%20and%20Tahmineh%20Tavakoli%20and%20Ahad%20Harati&entry.1292438233=%20%20Despite%20the%20advantages%20of%20discriminative%20prototype-based%20methods%2C%20their%20role%0Ain%20adversarial%20robustness%20remains%20underexplored.%20Meanwhile%2C%20current%20adversarial%0Atraining%20methods%20predominantly%20focus%20on%20robustness%20against%20adversarial%20attacks%0Awithout%20explicitly%20leveraging%20geometric%20structures%20in%20the%20latent%20space%2C%20usually%0Aresulting%20in%20reduced%20accuracy%20on%20the%20original%20clean%20data.%20We%20propose%20a%20novel%0Aframework%20named%20Adversarially%20trained%20Deep%20Positive-Negative%20Prototypes%0A%28Adv-DPNP%29%2C%20which%20integrates%20discriminative%20prototype-based%20learning%20with%0Aadversarial%20training.%20Adv-DPNP%20uses%20unified%20class%20prototypes%20that%20serve%20as%20both%0Aclassifier%20weights%20and%20robust%20anchors%20in%20the%20latent%20space.%20Moreover%2C%20a%20novel%0Adual-branch%20training%20mechanism%20maintains%20stable%20prototypes%20by%20updating%20them%0Aexclusively%20with%20clean%20data%2C%20while%20the%20feature%20extractor%20is%20trained%20on%20both%0Aclean%20and%20adversarial%20inputs%20to%20increase%20invariance%20to%20adversarial%0Aperturbations.%20In%20addition%2C%20we%20use%20a%20composite%20loss%20that%20combines%0Apositive-prototype%20alignment%2C%20negative-prototype%20repulsion%2C%20and%20consistency%0Aregularization%20to%20further%20enhance%20discrimination%2C%20adversarial%20robustness%2C%20and%0Aclean%20accuracy.%20Extensive%20experiments%20on%20standard%20benchmarks%20%28CIFAR-10/100%20and%0ASVHN%29%20confirm%20that%20Adv-DPNP%20improves%20clean%20accuracy%20over%20state-of-the-art%0Adefenses%20and%20baseline%20methods%2C%20while%20maintaining%20competitive%20or%20superior%0Arobustness%20under%20a%20suite%20of%20widely%20used%20attacks%2C%20including%20FGSM%2C%20PGD%2C%20C%5C%26W%2C%20and%0AAutoAttack.%20We%20also%20evaluate%20robustness%20to%20common%20corruptions%20on%20CIFAR-10-C%2C%0Awhere%20Adv-DPNP%20achieves%20the%20highest%20average%20accuracy%20across%20severities%20and%0Acorruption%20types.%20Additionally%2C%20we%20provide%20an%20in-depth%20analysis%20of%20the%0Adiscriminative%20quality%20of%20the%20learned%20feature%20representations%2C%20highlighting%20the%0Aeffectiveness%20of%20Adv-DPNP%20in%20maintaining%20compactness%20and%20clear%20separation%20in%0Athe%20latent%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03782v2&entry.124074799=Read"},
{"title": "ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset", "author": "Qingwen Zeng and Juan E. Tapia and Izan Garcia and Juan M. Espin and Christoph Busch", "abstract": "  Nowadays, the development of a Presentation Attack Detection (PAD) system for\nID cards presents a challenge due to the lack of images available to train a\nrobust PAD system and the increase in diversity of possible attack instrument\nspecies. Today, most algorithms focus on generating attack samples and do not\ntake into account the limited number of bona fide images. This work is one of\nthe first to propose a method for mimicking bona fide images by generating\nsynthetic versions of them using Stable Diffusion, which may help improve the\ngeneralisation capabilities of the detector. Furthermore, the new images\ngenerated are evaluated in a system trained from scratch and in a commercial\nsolution. The PAD system yields an interesting result, as it identifies our\nimages as bona fide, which has a positive impact on detection performance and\ndata restrictions.\n", "link": "http://arxiv.org/abs/2508.13078v1", "date": "2025-08-18", "relevancy": 2.6777, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5611}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5267}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ID-Card%20Synthetic%20Generation%3A%20Toward%20a%20Simulated%20Bona%20fide%20Dataset&body=Title%3A%20ID-Card%20Synthetic%20Generation%3A%20Toward%20a%20Simulated%20Bona%20fide%20Dataset%0AAuthor%3A%20Qingwen%20Zeng%20and%20Juan%20E.%20Tapia%20and%20Izan%20Garcia%20and%20Juan%20M.%20Espin%20and%20Christoph%20Busch%0AAbstract%3A%20%20%20Nowadays%2C%20the%20development%20of%20a%20Presentation%20Attack%20Detection%20%28PAD%29%20system%20for%0AID%20cards%20presents%20a%20challenge%20due%20to%20the%20lack%20of%20images%20available%20to%20train%20a%0Arobust%20PAD%20system%20and%20the%20increase%20in%20diversity%20of%20possible%20attack%20instrument%0Aspecies.%20Today%2C%20most%20algorithms%20focus%20on%20generating%20attack%20samples%20and%20do%20not%0Atake%20into%20account%20the%20limited%20number%20of%20bona%20fide%20images.%20This%20work%20is%20one%20of%0Athe%20first%20to%20propose%20a%20method%20for%20mimicking%20bona%20fide%20images%20by%20generating%0Asynthetic%20versions%20of%20them%20using%20Stable%20Diffusion%2C%20which%20may%20help%20improve%20the%0Ageneralisation%20capabilities%20of%20the%20detector.%20Furthermore%2C%20the%20new%20images%0Agenerated%20are%20evaluated%20in%20a%20system%20trained%20from%20scratch%20and%20in%20a%20commercial%0Asolution.%20The%20PAD%20system%20yields%20an%20interesting%20result%2C%20as%20it%20identifies%20our%0Aimages%20as%20bona%20fide%2C%20which%20has%20a%20positive%20impact%20on%20detection%20performance%20and%0Adata%20restrictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DID-Card%2520Synthetic%2520Generation%253A%2520Toward%2520a%2520Simulated%2520Bona%2520fide%2520Dataset%26entry.906535625%3DQingwen%2520Zeng%2520and%2520Juan%2520E.%2520Tapia%2520and%2520Izan%2520Garcia%2520and%2520Juan%2520M.%2520Espin%2520and%2520Christoph%2520Busch%26entry.1292438233%3D%2520%2520Nowadays%252C%2520the%2520development%2520of%2520a%2520Presentation%2520Attack%2520Detection%2520%2528PAD%2529%2520system%2520for%250AID%2520cards%2520presents%2520a%2520challenge%2520due%2520to%2520the%2520lack%2520of%2520images%2520available%2520to%2520train%2520a%250Arobust%2520PAD%2520system%2520and%2520the%2520increase%2520in%2520diversity%2520of%2520possible%2520attack%2520instrument%250Aspecies.%2520Today%252C%2520most%2520algorithms%2520focus%2520on%2520generating%2520attack%2520samples%2520and%2520do%2520not%250Atake%2520into%2520account%2520the%2520limited%2520number%2520of%2520bona%2520fide%2520images.%2520This%2520work%2520is%2520one%2520of%250Athe%2520first%2520to%2520propose%2520a%2520method%2520for%2520mimicking%2520bona%2520fide%2520images%2520by%2520generating%250Asynthetic%2520versions%2520of%2520them%2520using%2520Stable%2520Diffusion%252C%2520which%2520may%2520help%2520improve%2520the%250Ageneralisation%2520capabilities%2520of%2520the%2520detector.%2520Furthermore%252C%2520the%2520new%2520images%250Agenerated%2520are%2520evaluated%2520in%2520a%2520system%2520trained%2520from%2520scratch%2520and%2520in%2520a%2520commercial%250Asolution.%2520The%2520PAD%2520system%2520yields%2520an%2520interesting%2520result%252C%2520as%2520it%2520identifies%2520our%250Aimages%2520as%2520bona%2520fide%252C%2520which%2520has%2520a%2520positive%2520impact%2520on%2520detection%2520performance%2520and%250Adata%2520restrictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ID-Card%20Synthetic%20Generation%3A%20Toward%20a%20Simulated%20Bona%20fide%20Dataset&entry.906535625=Qingwen%20Zeng%20and%20Juan%20E.%20Tapia%20and%20Izan%20Garcia%20and%20Juan%20M.%20Espin%20and%20Christoph%20Busch&entry.1292438233=%20%20Nowadays%2C%20the%20development%20of%20a%20Presentation%20Attack%20Detection%20%28PAD%29%20system%20for%0AID%20cards%20presents%20a%20challenge%20due%20to%20the%20lack%20of%20images%20available%20to%20train%20a%0Arobust%20PAD%20system%20and%20the%20increase%20in%20diversity%20of%20possible%20attack%20instrument%0Aspecies.%20Today%2C%20most%20algorithms%20focus%20on%20generating%20attack%20samples%20and%20do%20not%0Atake%20into%20account%20the%20limited%20number%20of%20bona%20fide%20images.%20This%20work%20is%20one%20of%0Athe%20first%20to%20propose%20a%20method%20for%20mimicking%20bona%20fide%20images%20by%20generating%0Asynthetic%20versions%20of%20them%20using%20Stable%20Diffusion%2C%20which%20may%20help%20improve%20the%0Ageneralisation%20capabilities%20of%20the%20detector.%20Furthermore%2C%20the%20new%20images%0Agenerated%20are%20evaluated%20in%20a%20system%20trained%20from%20scratch%20and%20in%20a%20commercial%0Asolution.%20The%20PAD%20system%20yields%20an%20interesting%20result%2C%20as%20it%20identifies%20our%0Aimages%20as%20bona%20fide%2C%20which%20has%20a%20positive%20impact%20on%20detection%20performance%20and%0Adata%20restrictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13078v1&entry.124074799=Read"},
{"title": "FCL-ViT: Task-Aware Attention Tuning for Continual Learning", "author": "Anestis Kaimakamidis and Ioannis Pitas", "abstract": "  Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN)\nknowledge to new tasks, without forgetting the old ones. However, modern CL\ntechniques focus on provisioning memory capabilities to existing DNN models\nrather than designing new ones that are able to adapt according to the task at\nhand. This paper presents the novel Feedback Continual Learning Vision\nTransformer (FCL-ViT) that uses a feedback mechanism to generate real-time\ndynamic attention features tailored to the current task. The FCL-ViT operates\nin two Phases. In phase 1, the generic image features are produced and\ndetermine where the Transformer should attend on the current image. In phase 2,\ntask-specific image features are generated that leverage dynamic attention. To\nthis end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs)\nare introduced that operate in both phases and are responsible for tuning the\nTABs attention, respectively. The FCL-ViT surpasses state-of-the-art\nperformance on Continual Learning compared to benchmark methods, while\nretaining a small number of trainable DNN parameters.\n", "link": "http://arxiv.org/abs/2412.02509v3", "date": "2025-08-18", "relevancy": 2.64, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5392}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5284}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&body=Title%3A%20FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning%0AAuthor%3A%20Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02509v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFCL-ViT%253A%2520Task-Aware%2520Attention%2520Tuning%2520for%2520Continual%2520Learning%26entry.906535625%3DAnestis%2520Kaimakamidis%2520and%2520Ioannis%2520Pitas%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520involves%2520adapting%2520the%2520prior%2520Deep%2520Neural%2520Network%2520%2528DNN%2529%250Aknowledge%2520to%2520new%2520tasks%252C%2520without%2520forgetting%2520the%2520old%2520ones.%2520However%252C%2520modern%2520CL%250Atechniques%2520focus%2520on%2520provisioning%2520memory%2520capabilities%2520to%2520existing%2520DNN%2520models%250Arather%2520than%2520designing%2520new%2520ones%2520that%2520are%2520able%2520to%2520adapt%2520according%2520to%2520the%2520task%2520at%250Ahand.%2520This%2520paper%2520presents%2520the%2520novel%2520Feedback%2520Continual%2520Learning%2520Vision%250ATransformer%2520%2528FCL-ViT%2529%2520that%2520uses%2520a%2520feedback%2520mechanism%2520to%2520generate%2520real-time%250Adynamic%2520attention%2520features%2520tailored%2520to%2520the%2520current%2520task.%2520The%2520FCL-ViT%2520operates%250Ain%2520two%2520Phases.%2520In%2520phase%25201%252C%2520the%2520generic%2520image%2520features%2520are%2520produced%2520and%250Adetermine%2520where%2520the%2520Transformer%2520should%2520attend%2520on%2520the%2520current%2520image.%2520In%2520phase%25202%252C%250Atask-specific%2520image%2520features%2520are%2520generated%2520that%2520leverage%2520dynamic%2520attention.%2520To%250Athis%2520end%252C%2520Tunable%2520self-Attention%2520Blocks%2520%2528TABs%2529%2520and%2520Task%2520Specific%2520Blocks%2520%2528TSBs%2529%250Aare%2520introduced%2520that%2520operate%2520in%2520both%2520phases%2520and%2520are%2520responsible%2520for%2520tuning%2520the%250ATABs%2520attention%252C%2520respectively.%2520The%2520FCL-ViT%2520surpasses%2520state-of-the-art%250Aperformance%2520on%2520Continual%2520Learning%2520compared%2520to%2520benchmark%2520methods%252C%2520while%250Aretaining%2520a%2520small%2520number%2520of%2520trainable%2520DNN%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02509v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FCL-ViT%3A%20Task-Aware%20Attention%20Tuning%20for%20Continual%20Learning&entry.906535625=Anestis%20Kaimakamidis%20and%20Ioannis%20Pitas&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20involves%20adapting%20the%20prior%20Deep%20Neural%20Network%20%28DNN%29%0Aknowledge%20to%20new%20tasks%2C%20without%20forgetting%20the%20old%20ones.%20However%2C%20modern%20CL%0Atechniques%20focus%20on%20provisioning%20memory%20capabilities%20to%20existing%20DNN%20models%0Arather%20than%20designing%20new%20ones%20that%20are%20able%20to%20adapt%20according%20to%20the%20task%20at%0Ahand.%20This%20paper%20presents%20the%20novel%20Feedback%20Continual%20Learning%20Vision%0ATransformer%20%28FCL-ViT%29%20that%20uses%20a%20feedback%20mechanism%20to%20generate%20real-time%0Adynamic%20attention%20features%20tailored%20to%20the%20current%20task.%20The%20FCL-ViT%20operates%0Ain%20two%20Phases.%20In%20phase%201%2C%20the%20generic%20image%20features%20are%20produced%20and%0Adetermine%20where%20the%20Transformer%20should%20attend%20on%20the%20current%20image.%20In%20phase%202%2C%0Atask-specific%20image%20features%20are%20generated%20that%20leverage%20dynamic%20attention.%20To%0Athis%20end%2C%20Tunable%20self-Attention%20Blocks%20%28TABs%29%20and%20Task%20Specific%20Blocks%20%28TSBs%29%0Aare%20introduced%20that%20operate%20in%20both%20phases%20and%20are%20responsible%20for%20tuning%20the%0ATABs%20attention%2C%20respectively.%20The%20FCL-ViT%20surpasses%20state-of-the-art%0Aperformance%20on%20Continual%20Learning%20compared%20to%20benchmark%20methods%2C%20while%0Aretaining%20a%20small%20number%20of%20trainable%20DNN%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02509v3&entry.124074799=Read"},
{"title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy", "author": "Zhaoxi Chen and Tianqi Liu and Long Zhuo and Jiawei Ren and Zeng Tao and He Zhu and Fangzhou Hong and Liang Pan and Ziwei Liu", "abstract": "  We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.\n", "link": "http://arxiv.org/abs/2508.13154v1", "date": "2025-08-18", "relevancy": 2.6356, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6594}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6588}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DNeX%3A%20Feed-Forward%204D%20Generative%20Modeling%20Made%20Easy&body=Title%3A%204DNeX%3A%20Feed-Forward%204D%20Generative%20Modeling%20Made%20Easy%0AAuthor%3A%20Zhaoxi%20Chen%20and%20Tianqi%20Liu%20and%20Long%20Zhuo%20and%20Jiawei%20Ren%20and%20Zeng%20Tao%20and%20He%20Zhu%20and%20Fangzhou%20Hong%20and%20Liang%20Pan%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20We%20present%204DNeX%2C%20the%20first%20feed-forward%20framework%20for%20generating%204D%20%28i.e.%2C%0Adynamic%203D%29%20scene%20representations%20from%20a%20single%20image.%20In%20contrast%20to%20existing%0Amethods%20that%20rely%20on%20computationally%20intensive%20optimization%20or%20require%0Amulti-frame%20video%20inputs%2C%204DNeX%20enables%20efficient%2C%20end-to-end%20image-to-4D%0Ageneration%20by%20fine-tuning%20a%20pretrained%20video%20diffusion%20model.%20Specifically%2C%201%29%0Ato%20alleviate%20the%20scarcity%20of%204D%20data%2C%20we%20construct%204DNeX-10M%2C%20a%20large-scale%0Adataset%20with%20high-quality%204D%20annotations%20generated%20using%20advanced%0Areconstruction%20approaches.%202%29%20we%20introduce%20a%20unified%206D%20video%20representation%0Athat%20jointly%20models%20RGB%20and%20XYZ%20sequences%2C%20facilitating%20structured%20learning%20of%0Aboth%20appearance%20and%20geometry.%203%29%20we%20propose%20a%20set%20of%20simple%20yet%20effective%0Aadaptation%20strategies%20to%20repurpose%20pretrained%20video%20diffusion%20models%20for%204D%0Amodeling.%204DNeX%20produces%20high-quality%20dynamic%20point%20clouds%20that%20enable%0Anovel-view%20video%20synthesis.%20Extensive%20experiments%20demonstrate%20that%204DNeX%0Aoutperforms%20existing%204D%20generation%20methods%20in%20efficiency%20and%20generalizability%2C%0Aoffering%20a%20scalable%20solution%20for%20image-to-4D%20modeling%20and%20laying%20the%20foundation%0Afor%20generative%204D%20world%20models%20that%20simulate%20dynamic%20scene%20evolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DNeX%253A%2520Feed-Forward%25204D%2520Generative%2520Modeling%2520Made%2520Easy%26entry.906535625%3DZhaoxi%2520Chen%2520and%2520Tianqi%2520Liu%2520and%2520Long%2520Zhuo%2520and%2520Jiawei%2520Ren%2520and%2520Zeng%2520Tao%2520and%2520He%2520Zhu%2520and%2520Fangzhou%2520Hong%2520and%2520Liang%2520Pan%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%25204DNeX%252C%2520the%2520first%2520feed-forward%2520framework%2520for%2520generating%25204D%2520%2528i.e.%252C%250Adynamic%25203D%2529%2520scene%2520representations%2520from%2520a%2520single%2520image.%2520In%2520contrast%2520to%2520existing%250Amethods%2520that%2520rely%2520on%2520computationally%2520intensive%2520optimization%2520or%2520require%250Amulti-frame%2520video%2520inputs%252C%25204DNeX%2520enables%2520efficient%252C%2520end-to-end%2520image-to-4D%250Ageneration%2520by%2520fine-tuning%2520a%2520pretrained%2520video%2520diffusion%2520model.%2520Specifically%252C%25201%2529%250Ato%2520alleviate%2520the%2520scarcity%2520of%25204D%2520data%252C%2520we%2520construct%25204DNeX-10M%252C%2520a%2520large-scale%250Adataset%2520with%2520high-quality%25204D%2520annotations%2520generated%2520using%2520advanced%250Areconstruction%2520approaches.%25202%2529%2520we%2520introduce%2520a%2520unified%25206D%2520video%2520representation%250Athat%2520jointly%2520models%2520RGB%2520and%2520XYZ%2520sequences%252C%2520facilitating%2520structured%2520learning%2520of%250Aboth%2520appearance%2520and%2520geometry.%25203%2529%2520we%2520propose%2520a%2520set%2520of%2520simple%2520yet%2520effective%250Aadaptation%2520strategies%2520to%2520repurpose%2520pretrained%2520video%2520diffusion%2520models%2520for%25204D%250Amodeling.%25204DNeX%2520produces%2520high-quality%2520dynamic%2520point%2520clouds%2520that%2520enable%250Anovel-view%2520video%2520synthesis.%2520Extensive%2520experiments%2520demonstrate%2520that%25204DNeX%250Aoutperforms%2520existing%25204D%2520generation%2520methods%2520in%2520efficiency%2520and%2520generalizability%252C%250Aoffering%2520a%2520scalable%2520solution%2520for%2520image-to-4D%2520modeling%2520and%2520laying%2520the%2520foundation%250Afor%2520generative%25204D%2520world%2520models%2520that%2520simulate%2520dynamic%2520scene%2520evolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DNeX%3A%20Feed-Forward%204D%20Generative%20Modeling%20Made%20Easy&entry.906535625=Zhaoxi%20Chen%20and%20Tianqi%20Liu%20and%20Long%20Zhuo%20and%20Jiawei%20Ren%20and%20Zeng%20Tao%20and%20He%20Zhu%20and%20Fangzhou%20Hong%20and%20Liang%20Pan%20and%20Ziwei%20Liu&entry.1292438233=%20%20We%20present%204DNeX%2C%20the%20first%20feed-forward%20framework%20for%20generating%204D%20%28i.e.%2C%0Adynamic%203D%29%20scene%20representations%20from%20a%20single%20image.%20In%20contrast%20to%20existing%0Amethods%20that%20rely%20on%20computationally%20intensive%20optimization%20or%20require%0Amulti-frame%20video%20inputs%2C%204DNeX%20enables%20efficient%2C%20end-to-end%20image-to-4D%0Ageneration%20by%20fine-tuning%20a%20pretrained%20video%20diffusion%20model.%20Specifically%2C%201%29%0Ato%20alleviate%20the%20scarcity%20of%204D%20data%2C%20we%20construct%204DNeX-10M%2C%20a%20large-scale%0Adataset%20with%20high-quality%204D%20annotations%20generated%20using%20advanced%0Areconstruction%20approaches.%202%29%20we%20introduce%20a%20unified%206D%20video%20representation%0Athat%20jointly%20models%20RGB%20and%20XYZ%20sequences%2C%20facilitating%20structured%20learning%20of%0Aboth%20appearance%20and%20geometry.%203%29%20we%20propose%20a%20set%20of%20simple%20yet%20effective%0Aadaptation%20strategies%20to%20repurpose%20pretrained%20video%20diffusion%20models%20for%204D%0Amodeling.%204DNeX%20produces%20high-quality%20dynamic%20point%20clouds%20that%20enable%0Anovel-view%20video%20synthesis.%20Extensive%20experiments%20demonstrate%20that%204DNeX%0Aoutperforms%20existing%204D%20generation%20methods%20in%20efficiency%20and%20generalizability%2C%0Aoffering%20a%20scalable%20solution%20for%20image-to-4D%20modeling%20and%20laying%20the%20foundation%0Afor%20generative%204D%20world%20models%20that%20simulate%20dynamic%20scene%20evolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13154v1&entry.124074799=Read"},
{"title": "Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a\n  Lightweight Auto3DSeg and SegResNet Implementation", "author": "Dominic LaBella and Keshav Jha and Jared Robbins and Esther Yu", "abstract": "  Cone-beam computed tomography (CBCT) has become an invaluable imaging\nmodality in dentistry, enabling 3D visualization of teeth and surrounding\nstructures for diagnosis and treatment planning. Automated segmentation of\ndental structures in CBCT can efficiently assist in identifying pathology\n(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning\nin head and neck cancer patients. We describe the DLaBella29 team's approach\nfor the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning\npipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg\nframework with a 3D SegResNet architecture, trained on a subset of the\nToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key\npreprocessing steps included image resampling to 0.6 mm isotropic resolution\nand intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE\non the 5-fold predictions to infer a Phase 1 segmentation and then conducted\ntight cropping around the easily segmented Phase 1 mandible to perform Phase 2\nsegmentation on the smaller nerve structures. Our method achieved an average\nDice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This\npaper details the clinical context, data preparation, model development,\nresults of our approach, and discusses the relevance of automated dental\nsegmentation for improving patient care in radiation oncology.\n", "link": "http://arxiv.org/abs/2508.12962v1", "date": "2025-08-18", "relevancy": 2.6124, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5132}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Phase%20Automated%20Segmentation%20of%20Dental%20Structures%20in%20CBCT%20Using%20a%0A%20%20Lightweight%20Auto3DSeg%20and%20SegResNet%20Implementation&body=Title%3A%20Multi-Phase%20Automated%20Segmentation%20of%20Dental%20Structures%20in%20CBCT%20Using%20a%0A%20%20Lightweight%20Auto3DSeg%20and%20SegResNet%20Implementation%0AAuthor%3A%20Dominic%20LaBella%20and%20Keshav%20Jha%20and%20Jared%20Robbins%20and%20Esther%20Yu%0AAbstract%3A%20%20%20Cone-beam%20computed%20tomography%20%28CBCT%29%20has%20become%20an%20invaluable%20imaging%0Amodality%20in%20dentistry%2C%20enabling%203D%20visualization%20of%20teeth%20and%20surrounding%0Astructures%20for%20diagnosis%20and%20treatment%20planning.%20Automated%20segmentation%20of%0Adental%20structures%20in%20CBCT%20can%20efficiently%20assist%20in%20identifying%20pathology%0A%28e.g.%2C%20pulpal%20or%20periapical%20lesions%29%20and%20facilitate%20radiation%20therapy%20planning%0Ain%20head%20and%20neck%20cancer%20patients.%20We%20describe%20the%20DLaBella29%20team%27s%20approach%0Afor%20the%20MICCAI%202025%20ToothFairy3%20Challenge%2C%20which%20involves%20a%20deep%20learning%0Apipeline%20for%20multi-class%20tooth%20segmentation.%20We%20utilized%20the%20MONAI%20Auto3DSeg%0Aframework%20with%20a%203D%20SegResNet%20architecture%2C%20trained%20on%20a%20subset%20of%20the%0AToothFairy3%20dataset%20%2863%20CBCT%20scans%29%20with%205-fold%20cross-validation.%20Key%0Apreprocessing%20steps%20included%20image%20resampling%20to%200.6%20mm%20isotropic%20resolution%0Aand%20intensity%20clipping.%20We%20applied%20an%20ensemble%20fusion%20using%20Multi-Label%20STAPLE%0Aon%20the%205-fold%20predictions%20to%20infer%20a%20Phase%201%20segmentation%20and%20then%20conducted%0Atight%20cropping%20around%20the%20easily%20segmented%20Phase%201%20mandible%20to%20perform%20Phase%202%0Asegmentation%20on%20the%20smaller%20nerve%20structures.%20Our%20method%20achieved%20an%20average%0ADice%20of%200.87%20on%20the%20ToothFairy3%20challenge%20out-of-sample%20validation%20set.%20This%0Apaper%20details%20the%20clinical%20context%2C%20data%20preparation%2C%20model%20development%2C%0Aresults%20of%20our%20approach%2C%20and%20discusses%20the%20relevance%20of%20automated%20dental%0Asegmentation%20for%20improving%20patient%20care%20in%20radiation%20oncology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Phase%2520Automated%2520Segmentation%2520of%2520Dental%2520Structures%2520in%2520CBCT%2520Using%2520a%250A%2520%2520Lightweight%2520Auto3DSeg%2520and%2520SegResNet%2520Implementation%26entry.906535625%3DDominic%2520LaBella%2520and%2520Keshav%2520Jha%2520and%2520Jared%2520Robbins%2520and%2520Esther%2520Yu%26entry.1292438233%3D%2520%2520Cone-beam%2520computed%2520tomography%2520%2528CBCT%2529%2520has%2520become%2520an%2520invaluable%2520imaging%250Amodality%2520in%2520dentistry%252C%2520enabling%25203D%2520visualization%2520of%2520teeth%2520and%2520surrounding%250Astructures%2520for%2520diagnosis%2520and%2520treatment%2520planning.%2520Automated%2520segmentation%2520of%250Adental%2520structures%2520in%2520CBCT%2520can%2520efficiently%2520assist%2520in%2520identifying%2520pathology%250A%2528e.g.%252C%2520pulpal%2520or%2520periapical%2520lesions%2529%2520and%2520facilitate%2520radiation%2520therapy%2520planning%250Ain%2520head%2520and%2520neck%2520cancer%2520patients.%2520We%2520describe%2520the%2520DLaBella29%2520team%2527s%2520approach%250Afor%2520the%2520MICCAI%25202025%2520ToothFairy3%2520Challenge%252C%2520which%2520involves%2520a%2520deep%2520learning%250Apipeline%2520for%2520multi-class%2520tooth%2520segmentation.%2520We%2520utilized%2520the%2520MONAI%2520Auto3DSeg%250Aframework%2520with%2520a%25203D%2520SegResNet%2520architecture%252C%2520trained%2520on%2520a%2520subset%2520of%2520the%250AToothFairy3%2520dataset%2520%252863%2520CBCT%2520scans%2529%2520with%25205-fold%2520cross-validation.%2520Key%250Apreprocessing%2520steps%2520included%2520image%2520resampling%2520to%25200.6%2520mm%2520isotropic%2520resolution%250Aand%2520intensity%2520clipping.%2520We%2520applied%2520an%2520ensemble%2520fusion%2520using%2520Multi-Label%2520STAPLE%250Aon%2520the%25205-fold%2520predictions%2520to%2520infer%2520a%2520Phase%25201%2520segmentation%2520and%2520then%2520conducted%250Atight%2520cropping%2520around%2520the%2520easily%2520segmented%2520Phase%25201%2520mandible%2520to%2520perform%2520Phase%25202%250Asegmentation%2520on%2520the%2520smaller%2520nerve%2520structures.%2520Our%2520method%2520achieved%2520an%2520average%250ADice%2520of%25200.87%2520on%2520the%2520ToothFairy3%2520challenge%2520out-of-sample%2520validation%2520set.%2520This%250Apaper%2520details%2520the%2520clinical%2520context%252C%2520data%2520preparation%252C%2520model%2520development%252C%250Aresults%2520of%2520our%2520approach%252C%2520and%2520discusses%2520the%2520relevance%2520of%2520automated%2520dental%250Asegmentation%2520for%2520improving%2520patient%2520care%2520in%2520radiation%2520oncology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Phase%20Automated%20Segmentation%20of%20Dental%20Structures%20in%20CBCT%20Using%20a%0A%20%20Lightweight%20Auto3DSeg%20and%20SegResNet%20Implementation&entry.906535625=Dominic%20LaBella%20and%20Keshav%20Jha%20and%20Jared%20Robbins%20and%20Esther%20Yu&entry.1292438233=%20%20Cone-beam%20computed%20tomography%20%28CBCT%29%20has%20become%20an%20invaluable%20imaging%0Amodality%20in%20dentistry%2C%20enabling%203D%20visualization%20of%20teeth%20and%20surrounding%0Astructures%20for%20diagnosis%20and%20treatment%20planning.%20Automated%20segmentation%20of%0Adental%20structures%20in%20CBCT%20can%20efficiently%20assist%20in%20identifying%20pathology%0A%28e.g.%2C%20pulpal%20or%20periapical%20lesions%29%20and%20facilitate%20radiation%20therapy%20planning%0Ain%20head%20and%20neck%20cancer%20patients.%20We%20describe%20the%20DLaBella29%20team%27s%20approach%0Afor%20the%20MICCAI%202025%20ToothFairy3%20Challenge%2C%20which%20involves%20a%20deep%20learning%0Apipeline%20for%20multi-class%20tooth%20segmentation.%20We%20utilized%20the%20MONAI%20Auto3DSeg%0Aframework%20with%20a%203D%20SegResNet%20architecture%2C%20trained%20on%20a%20subset%20of%20the%0AToothFairy3%20dataset%20%2863%20CBCT%20scans%29%20with%205-fold%20cross-validation.%20Key%0Apreprocessing%20steps%20included%20image%20resampling%20to%200.6%20mm%20isotropic%20resolution%0Aand%20intensity%20clipping.%20We%20applied%20an%20ensemble%20fusion%20using%20Multi-Label%20STAPLE%0Aon%20the%205-fold%20predictions%20to%20infer%20a%20Phase%201%20segmentation%20and%20then%20conducted%0Atight%20cropping%20around%20the%20easily%20segmented%20Phase%201%20mandible%20to%20perform%20Phase%202%0Asegmentation%20on%20the%20smaller%20nerve%20structures.%20Our%20method%20achieved%20an%20average%0ADice%20of%200.87%20on%20the%20ToothFairy3%20challenge%20out-of-sample%20validation%20set.%20This%0Apaper%20details%20the%20clinical%20context%2C%20data%20preparation%2C%20model%20development%2C%0Aresults%20of%20our%20approach%2C%20and%20discusses%20the%20relevance%20of%20automated%20dental%0Asegmentation%20for%20improving%20patient%20care%20in%20radiation%20oncology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12962v1&entry.124074799=Read"},
{"title": "Can Large Models Teach Student Models to Solve Mathematical Problems\n  Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction", "author": "Xinhe Li and Jiajun Liu and Peng Wang", "abstract": "  Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.\n", "link": "http://arxiv.org/abs/2508.13037v1", "date": "2025-08-18", "relevancy": 2.5925, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Models%20Teach%20Student%20Models%20to%20Solve%20Mathematical%20Problems%0A%20%20Like%20Human%20Beings%3F%20A%20Reasoning%20Distillation%20Method%20via%20Multi-LoRA%20Interaction&body=Title%3A%20Can%20Large%20Models%20Teach%20Student%20Models%20to%20Solve%20Mathematical%20Problems%0A%20%20Like%20Human%20Beings%3F%20A%20Reasoning%20Distillation%20Method%20via%20Multi-LoRA%20Interaction%0AAuthor%3A%20Xinhe%20Li%20and%20Jiajun%20Liu%20and%20Peng%20Wang%0AAbstract%3A%20%20%20Recent%20studies%20have%20demonstrated%20that%20Large%20Language%20Models%20%28LLMs%29%20have%0Astrong%20mathematical%20reasoning%20abilities%20but%20rely%20on%20hundreds%20of%20billions%20of%0Aparameters.%20To%20tackle%20the%20challenge%20of%20poor%20reasoning%20in%20Small%20Language%20Models%0A%28SLMs%29%2C%20existing%20methods%20typically%20leverage%20LLMs%20to%20generate%20massive%20amounts%20of%0Adata%20for%20cramming%20training.%20In%20psychology%2C%20they%20are%20akin%20to%20System%201%20thinking%2C%0Awhich%20resolves%20reasoning%20problems%20rapidly%20based%20on%20experience%20and%20intuition.%0AHowever%2C%20human%20learning%20also%20requires%20System%202%20thinking%2C%20where%20knowledge%20is%0Afirst%20acquired%20and%20then%20reinforced%20through%20practice.%20Inspired%20by%20such%20two%0Adistinct%20modes%20of%20thinking%2C%20we%20propose%20a%20novel%20method%20based%20on%20the%20multi-LoRA%0AInteraction%20for%20mathematical%20reasoning%20Distillation%20%28LoRID%29.%20First%2C%20we%20input%0Athe%20question%20and%20reasoning%20of%20each%20sample%20into%20an%20LLM%20to%20create%0Aknowledge-enhanced%20datasets.%20Subsequently%2C%20we%20train%20a%20LoRA%20block%20on%20the%20student%0Amodel%20as%20an%20Intuitive%20Reasoner%20%28IR%29%2C%20which%20directly%20generates%20Chain-of-Thoughts%0Afor%20problem-solving.%20Then%2C%20to%20imitate%20System%202%20thinking%2C%20we%20train%20the%20Knowledge%0AGenerator%20%28KG%29%20and%20Deep%20Reasoner%20%28DR%29%2C%20respectively.%20The%20former%20outputs%20only%0Aknowledge%20after%20receiving%20problems%2C%20while%20the%20latter%20uses%20that%20knowledge%20to%0Aperform%20reasoning.%20Finally%2C%20to%20address%20the%20randomness%20in%20the%20generation%20of%20IR%0Aand%20DR%2C%20we%20evaluate%20whether%20their%20outputs%20are%20consistent%2C%20and%20the%20inference%0Aprocess%20needs%20to%20be%20iterated%20if%20not.%20This%20step%20can%20enhance%20the%20mathematical%0Areasoning%20ability%20of%20SLMs%20through%20mutual%20feedback.%20Experimental%20results%20show%0Athat%20LoRID%20achieves%20state-of-the-art%20performance%2C%20especially%20on%20the%20GSM8K%0Adataset%2C%20where%20it%20outperforms%20the%20second-best%20method%20by%202.3%25%2C%2016.1%25%2C%202.4%25%2C%0A12.3%25%2C%20and%201.8%25%20accuracy%20across%20the%20five%20base%20models%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Models%2520Teach%2520Student%2520Models%2520to%2520Solve%2520Mathematical%2520Problems%250A%2520%2520Like%2520Human%2520Beings%253F%2520A%2520Reasoning%2520Distillation%2520Method%2520via%2520Multi-LoRA%2520Interaction%26entry.906535625%3DXinhe%2520Li%2520and%2520Jiajun%2520Liu%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Astrong%2520mathematical%2520reasoning%2520abilities%2520but%2520rely%2520on%2520hundreds%2520of%2520billions%2520of%250Aparameters.%2520To%2520tackle%2520the%2520challenge%2520of%2520poor%2520reasoning%2520in%2520Small%2520Language%2520Models%250A%2528SLMs%2529%252C%2520existing%2520methods%2520typically%2520leverage%2520LLMs%2520to%2520generate%2520massive%2520amounts%2520of%250Adata%2520for%2520cramming%2520training.%2520In%2520psychology%252C%2520they%2520are%2520akin%2520to%2520System%25201%2520thinking%252C%250Awhich%2520resolves%2520reasoning%2520problems%2520rapidly%2520based%2520on%2520experience%2520and%2520intuition.%250AHowever%252C%2520human%2520learning%2520also%2520requires%2520System%25202%2520thinking%252C%2520where%2520knowledge%2520is%250Afirst%2520acquired%2520and%2520then%2520reinforced%2520through%2520practice.%2520Inspired%2520by%2520such%2520two%250Adistinct%2520modes%2520of%2520thinking%252C%2520we%2520propose%2520a%2520novel%2520method%2520based%2520on%2520the%2520multi-LoRA%250AInteraction%2520for%2520mathematical%2520reasoning%2520Distillation%2520%2528LoRID%2529.%2520First%252C%2520we%2520input%250Athe%2520question%2520and%2520reasoning%2520of%2520each%2520sample%2520into%2520an%2520LLM%2520to%2520create%250Aknowledge-enhanced%2520datasets.%2520Subsequently%252C%2520we%2520train%2520a%2520LoRA%2520block%2520on%2520the%2520student%250Amodel%2520as%2520an%2520Intuitive%2520Reasoner%2520%2528IR%2529%252C%2520which%2520directly%2520generates%2520Chain-of-Thoughts%250Afor%2520problem-solving.%2520Then%252C%2520to%2520imitate%2520System%25202%2520thinking%252C%2520we%2520train%2520the%2520Knowledge%250AGenerator%2520%2528KG%2529%2520and%2520Deep%2520Reasoner%2520%2528DR%2529%252C%2520respectively.%2520The%2520former%2520outputs%2520only%250Aknowledge%2520after%2520receiving%2520problems%252C%2520while%2520the%2520latter%2520uses%2520that%2520knowledge%2520to%250Aperform%2520reasoning.%2520Finally%252C%2520to%2520address%2520the%2520randomness%2520in%2520the%2520generation%2520of%2520IR%250Aand%2520DR%252C%2520we%2520evaluate%2520whether%2520their%2520outputs%2520are%2520consistent%252C%2520and%2520the%2520inference%250Aprocess%2520needs%2520to%2520be%2520iterated%2520if%2520not.%2520This%2520step%2520can%2520enhance%2520the%2520mathematical%250Areasoning%2520ability%2520of%2520SLMs%2520through%2520mutual%2520feedback.%2520Experimental%2520results%2520show%250Athat%2520LoRID%2520achieves%2520state-of-the-art%2520performance%252C%2520especially%2520on%2520the%2520GSM8K%250Adataset%252C%2520where%2520it%2520outperforms%2520the%2520second-best%2520method%2520by%25202.3%2525%252C%252016.1%2525%252C%25202.4%2525%252C%250A12.3%2525%252C%2520and%25201.8%2525%2520accuracy%2520across%2520the%2520five%2520base%2520models%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Models%20Teach%20Student%20Models%20to%20Solve%20Mathematical%20Problems%0A%20%20Like%20Human%20Beings%3F%20A%20Reasoning%20Distillation%20Method%20via%20Multi-LoRA%20Interaction&entry.906535625=Xinhe%20Li%20and%20Jiajun%20Liu%20and%20Peng%20Wang&entry.1292438233=%20%20Recent%20studies%20have%20demonstrated%20that%20Large%20Language%20Models%20%28LLMs%29%20have%0Astrong%20mathematical%20reasoning%20abilities%20but%20rely%20on%20hundreds%20of%20billions%20of%0Aparameters.%20To%20tackle%20the%20challenge%20of%20poor%20reasoning%20in%20Small%20Language%20Models%0A%28SLMs%29%2C%20existing%20methods%20typically%20leverage%20LLMs%20to%20generate%20massive%20amounts%20of%0Adata%20for%20cramming%20training.%20In%20psychology%2C%20they%20are%20akin%20to%20System%201%20thinking%2C%0Awhich%20resolves%20reasoning%20problems%20rapidly%20based%20on%20experience%20and%20intuition.%0AHowever%2C%20human%20learning%20also%20requires%20System%202%20thinking%2C%20where%20knowledge%20is%0Afirst%20acquired%20and%20then%20reinforced%20through%20practice.%20Inspired%20by%20such%20two%0Adistinct%20modes%20of%20thinking%2C%20we%20propose%20a%20novel%20method%20based%20on%20the%20multi-LoRA%0AInteraction%20for%20mathematical%20reasoning%20Distillation%20%28LoRID%29.%20First%2C%20we%20input%0Athe%20question%20and%20reasoning%20of%20each%20sample%20into%20an%20LLM%20to%20create%0Aknowledge-enhanced%20datasets.%20Subsequently%2C%20we%20train%20a%20LoRA%20block%20on%20the%20student%0Amodel%20as%20an%20Intuitive%20Reasoner%20%28IR%29%2C%20which%20directly%20generates%20Chain-of-Thoughts%0Afor%20problem-solving.%20Then%2C%20to%20imitate%20System%202%20thinking%2C%20we%20train%20the%20Knowledge%0AGenerator%20%28KG%29%20and%20Deep%20Reasoner%20%28DR%29%2C%20respectively.%20The%20former%20outputs%20only%0Aknowledge%20after%20receiving%20problems%2C%20while%20the%20latter%20uses%20that%20knowledge%20to%0Aperform%20reasoning.%20Finally%2C%20to%20address%20the%20randomness%20in%20the%20generation%20of%20IR%0Aand%20DR%2C%20we%20evaluate%20whether%20their%20outputs%20are%20consistent%2C%20and%20the%20inference%0Aprocess%20needs%20to%20be%20iterated%20if%20not.%20This%20step%20can%20enhance%20the%20mathematical%0Areasoning%20ability%20of%20SLMs%20through%20mutual%20feedback.%20Experimental%20results%20show%0Athat%20LoRID%20achieves%20state-of-the-art%20performance%2C%20especially%20on%20the%20GSM8K%0Adataset%2C%20where%20it%20outperforms%20the%20second-best%20method%20by%202.3%25%2C%2016.1%25%2C%202.4%25%2C%0A12.3%25%2C%20and%201.8%25%20accuracy%20across%20the%20five%20base%20models%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13037v1&entry.124074799=Read"},
{"title": "Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic\n  Labeling using Foundation Models", "author": "Mohamad Al Mdfaa and Raghad Salameh and Geesara Kulathunga and Sergey Zagoruyko and Gonzalo Ferrer", "abstract": "  In robotics and computer vision, semantic mapping remains a critical\nchallenge for machines to comprehend complex environments. Traditional panoptic\nmapping approaches are constrained by fixed labels, limiting their ability to\nhandle novel objects. We present Unified Promptable Panoptic Mapping (UPPM),\nwhich leverages foundation models for dynamic labeling without additional\ntraining. UPPM is evaluated across three comprehensive levels:\nSegmentation-to-Map, Map-to-Map, and Segmentation-to-Segmentation. Results\ndemonstrate UPPM attains exceptional geometry reconstruction accuracy (0.61cm\non the Flat dataset), the highest panoptic quality (0.414), and better\nperformance compared to state-of-the-art segmentation methods. Furthermore,\nablation studies validate the contributions of unified semantics, custom NMS,\nand blurry frame filtering, with the custom NMS improving the completion ratio\nby 8.27% on the Flat dataset. UPPM demonstrates effective scene reconstruction\nwith rich semantic labeling across diverse datasets.\n", "link": "http://arxiv.org/abs/2405.02162v4", "date": "2025-08-18", "relevancy": 2.5634, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.7098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models&body=Title%3A%20Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models%0AAuthor%3A%20Mohamad%20Al%20Mdfaa%20and%20Raghad%20Salameh%20and%20Geesara%20Kulathunga%20and%20Sergey%20Zagoruyko%20and%20Gonzalo%20Ferrer%0AAbstract%3A%20%20%20In%20robotics%20and%20computer%20vision%2C%20semantic%20mapping%20remains%20a%20critical%0Achallenge%20for%20machines%20to%20comprehend%20complex%20environments.%20Traditional%20panoptic%0Amapping%20approaches%20are%20constrained%20by%20fixed%20labels%2C%20limiting%20their%20ability%20to%0Ahandle%20novel%20objects.%20We%20present%20Unified%20Promptable%20Panoptic%20Mapping%20%28UPPM%29%2C%0Awhich%20leverages%20foundation%20models%20for%20dynamic%20labeling%20without%20additional%0Atraining.%20UPPM%20is%20evaluated%20across%20three%20comprehensive%20levels%3A%0ASegmentation-to-Map%2C%20Map-to-Map%2C%20and%20Segmentation-to-Segmentation.%20Results%0Ademonstrate%20UPPM%20attains%20exceptional%20geometry%20reconstruction%20accuracy%20%280.61cm%0Aon%20the%20Flat%20dataset%29%2C%20the%20highest%20panoptic%20quality%20%280.414%29%2C%20and%20better%0Aperformance%20compared%20to%20state-of-the-art%20segmentation%20methods.%20Furthermore%2C%0Aablation%20studies%20validate%20the%20contributions%20of%20unified%20semantics%2C%20custom%20NMS%2C%0Aand%20blurry%20frame%20filtering%2C%20with%20the%20custom%20NMS%20improving%20the%20completion%20ratio%0Aby%208.27%25%20on%20the%20Flat%20dataset.%20UPPM%20demonstrates%20effective%20scene%20reconstruction%0Awith%20rich%20semantic%20labeling%20across%20diverse%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02162v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapping%2520the%2520Unseen%253A%2520Unified%2520Promptable%2520Panoptic%2520Mapping%2520with%2520Dynamic%250A%2520%2520Labeling%2520using%2520Foundation%2520Models%26entry.906535625%3DMohamad%2520Al%2520Mdfaa%2520and%2520Raghad%2520Salameh%2520and%2520Geesara%2520Kulathunga%2520and%2520Sergey%2520Zagoruyko%2520and%2520Gonzalo%2520Ferrer%26entry.1292438233%3D%2520%2520In%2520robotics%2520and%2520computer%2520vision%252C%2520semantic%2520mapping%2520remains%2520a%2520critical%250Achallenge%2520for%2520machines%2520to%2520comprehend%2520complex%2520environments.%2520Traditional%2520panoptic%250Amapping%2520approaches%2520are%2520constrained%2520by%2520fixed%2520labels%252C%2520limiting%2520their%2520ability%2520to%250Ahandle%2520novel%2520objects.%2520We%2520present%2520Unified%2520Promptable%2520Panoptic%2520Mapping%2520%2528UPPM%2529%252C%250Awhich%2520leverages%2520foundation%2520models%2520for%2520dynamic%2520labeling%2520without%2520additional%250Atraining.%2520UPPM%2520is%2520evaluated%2520across%2520three%2520comprehensive%2520levels%253A%250ASegmentation-to-Map%252C%2520Map-to-Map%252C%2520and%2520Segmentation-to-Segmentation.%2520Results%250Ademonstrate%2520UPPM%2520attains%2520exceptional%2520geometry%2520reconstruction%2520accuracy%2520%25280.61cm%250Aon%2520the%2520Flat%2520dataset%2529%252C%2520the%2520highest%2520panoptic%2520quality%2520%25280.414%2529%252C%2520and%2520better%250Aperformance%2520compared%2520to%2520state-of-the-art%2520segmentation%2520methods.%2520Furthermore%252C%250Aablation%2520studies%2520validate%2520the%2520contributions%2520of%2520unified%2520semantics%252C%2520custom%2520NMS%252C%250Aand%2520blurry%2520frame%2520filtering%252C%2520with%2520the%2520custom%2520NMS%2520improving%2520the%2520completion%2520ratio%250Aby%25208.27%2525%2520on%2520the%2520Flat%2520dataset.%2520UPPM%2520demonstrates%2520effective%2520scene%2520reconstruction%250Awith%2520rich%2520semantic%2520labeling%2520across%2520diverse%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02162v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapping%20the%20Unseen%3A%20Unified%20Promptable%20Panoptic%20Mapping%20with%20Dynamic%0A%20%20Labeling%20using%20Foundation%20Models&entry.906535625=Mohamad%20Al%20Mdfaa%20and%20Raghad%20Salameh%20and%20Geesara%20Kulathunga%20and%20Sergey%20Zagoruyko%20and%20Gonzalo%20Ferrer&entry.1292438233=%20%20In%20robotics%20and%20computer%20vision%2C%20semantic%20mapping%20remains%20a%20critical%0Achallenge%20for%20machines%20to%20comprehend%20complex%20environments.%20Traditional%20panoptic%0Amapping%20approaches%20are%20constrained%20by%20fixed%20labels%2C%20limiting%20their%20ability%20to%0Ahandle%20novel%20objects.%20We%20present%20Unified%20Promptable%20Panoptic%20Mapping%20%28UPPM%29%2C%0Awhich%20leverages%20foundation%20models%20for%20dynamic%20labeling%20without%20additional%0Atraining.%20UPPM%20is%20evaluated%20across%20three%20comprehensive%20levels%3A%0ASegmentation-to-Map%2C%20Map-to-Map%2C%20and%20Segmentation-to-Segmentation.%20Results%0Ademonstrate%20UPPM%20attains%20exceptional%20geometry%20reconstruction%20accuracy%20%280.61cm%0Aon%20the%20Flat%20dataset%29%2C%20the%20highest%20panoptic%20quality%20%280.414%29%2C%20and%20better%0Aperformance%20compared%20to%20state-of-the-art%20segmentation%20methods.%20Furthermore%2C%0Aablation%20studies%20validate%20the%20contributions%20of%20unified%20semantics%2C%20custom%20NMS%2C%0Aand%20blurry%20frame%20filtering%2C%20with%20the%20custom%20NMS%20improving%20the%20completion%20ratio%0Aby%208.27%25%20on%20the%20Flat%20dataset.%20UPPM%20demonstrates%20effective%20scene%20reconstruction%0Awith%20rich%20semantic%20labeling%20across%20diverse%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02162v4&entry.124074799=Read"},
{"title": "IntelliCap: Intelligent Guidance for Consistent View Sampling", "author": "Ayaka Yasunaga and Hideo Saito and Dieter Schmalstieg and Shohei Mori", "abstract": "  Novel view synthesis from images, for example, with 3D Gaussian splatting,\nhas made great progress. Rendering fidelity and speed are now ready even for\ndemanding virtual reality applications. However, the problem of assisting\nhumans in collecting the input images for these rendering algorithms has\nreceived much less attention. High-quality view synthesis requires uniform and\ndense view sampling. Unfortunately, these requirements are not easily addressed\nby human camera operators, who are in a hurry, impatient, or lack understanding\nof the scene structure and the photographic process. Existing approaches to\nguide humans during image acquisition concentrate on single objects or neglect\nview-dependent material characteristics. We propose a novel situated\nvisualization technique for scanning at multiple scales. During the scanning of\na scene, our method identifies important objects that need extended image\ncoverage to properly represent view-dependent appearance. To this end, we\nleverage semantic segmentation and category identification, ranked by a\nvision-language model. Spherical proxies are generated around highly ranked\nobjects to guide the user during scanning. Our results show superior\nperformance in real scenes compared to conventional view sampling strategies.\n", "link": "http://arxiv.org/abs/2508.13043v1", "date": "2025-08-18", "relevancy": 2.555, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6424}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6424}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IntelliCap%3A%20Intelligent%20Guidance%20for%20Consistent%20View%20Sampling&body=Title%3A%20IntelliCap%3A%20Intelligent%20Guidance%20for%20Consistent%20View%20Sampling%0AAuthor%3A%20Ayaka%20Yasunaga%20and%20Hideo%20Saito%20and%20Dieter%20Schmalstieg%20and%20Shohei%20Mori%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20from%20images%2C%20for%20example%2C%20with%203D%20Gaussian%20splatting%2C%0Ahas%20made%20great%20progress.%20Rendering%20fidelity%20and%20speed%20are%20now%20ready%20even%20for%0Ademanding%20virtual%20reality%20applications.%20However%2C%20the%20problem%20of%20assisting%0Ahumans%20in%20collecting%20the%20input%20images%20for%20these%20rendering%20algorithms%20has%0Areceived%20much%20less%20attention.%20High-quality%20view%20synthesis%20requires%20uniform%20and%0Adense%20view%20sampling.%20Unfortunately%2C%20these%20requirements%20are%20not%20easily%20addressed%0Aby%20human%20camera%20operators%2C%20who%20are%20in%20a%20hurry%2C%20impatient%2C%20or%20lack%20understanding%0Aof%20the%20scene%20structure%20and%20the%20photographic%20process.%20Existing%20approaches%20to%0Aguide%20humans%20during%20image%20acquisition%20concentrate%20on%20single%20objects%20or%20neglect%0Aview-dependent%20material%20characteristics.%20We%20propose%20a%20novel%20situated%0Avisualization%20technique%20for%20scanning%20at%20multiple%20scales.%20During%20the%20scanning%20of%0Aa%20scene%2C%20our%20method%20identifies%20important%20objects%20that%20need%20extended%20image%0Acoverage%20to%20properly%20represent%20view-dependent%20appearance.%20To%20this%20end%2C%20we%0Aleverage%20semantic%20segmentation%20and%20category%20identification%2C%20ranked%20by%20a%0Avision-language%20model.%20Spherical%20proxies%20are%20generated%20around%20highly%20ranked%0Aobjects%20to%20guide%20the%20user%20during%20scanning.%20Our%20results%20show%20superior%0Aperformance%20in%20real%20scenes%20compared%20to%20conventional%20view%20sampling%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelliCap%253A%2520Intelligent%2520Guidance%2520for%2520Consistent%2520View%2520Sampling%26entry.906535625%3DAyaka%2520Yasunaga%2520and%2520Hideo%2520Saito%2520and%2520Dieter%2520Schmalstieg%2520and%2520Shohei%2520Mori%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520from%2520images%252C%2520for%2520example%252C%2520with%25203D%2520Gaussian%2520splatting%252C%250Ahas%2520made%2520great%2520progress.%2520Rendering%2520fidelity%2520and%2520speed%2520are%2520now%2520ready%2520even%2520for%250Ademanding%2520virtual%2520reality%2520applications.%2520However%252C%2520the%2520problem%2520of%2520assisting%250Ahumans%2520in%2520collecting%2520the%2520input%2520images%2520for%2520these%2520rendering%2520algorithms%2520has%250Areceived%2520much%2520less%2520attention.%2520High-quality%2520view%2520synthesis%2520requires%2520uniform%2520and%250Adense%2520view%2520sampling.%2520Unfortunately%252C%2520these%2520requirements%2520are%2520not%2520easily%2520addressed%250Aby%2520human%2520camera%2520operators%252C%2520who%2520are%2520in%2520a%2520hurry%252C%2520impatient%252C%2520or%2520lack%2520understanding%250Aof%2520the%2520scene%2520structure%2520and%2520the%2520photographic%2520process.%2520Existing%2520approaches%2520to%250Aguide%2520humans%2520during%2520image%2520acquisition%2520concentrate%2520on%2520single%2520objects%2520or%2520neglect%250Aview-dependent%2520material%2520characteristics.%2520We%2520propose%2520a%2520novel%2520situated%250Avisualization%2520technique%2520for%2520scanning%2520at%2520multiple%2520scales.%2520During%2520the%2520scanning%2520of%250Aa%2520scene%252C%2520our%2520method%2520identifies%2520important%2520objects%2520that%2520need%2520extended%2520image%250Acoverage%2520to%2520properly%2520represent%2520view-dependent%2520appearance.%2520To%2520this%2520end%252C%2520we%250Aleverage%2520semantic%2520segmentation%2520and%2520category%2520identification%252C%2520ranked%2520by%2520a%250Avision-language%2520model.%2520Spherical%2520proxies%2520are%2520generated%2520around%2520highly%2520ranked%250Aobjects%2520to%2520guide%2520the%2520user%2520during%2520scanning.%2520Our%2520results%2520show%2520superior%250Aperformance%2520in%2520real%2520scenes%2520compared%2520to%2520conventional%2520view%2520sampling%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntelliCap%3A%20Intelligent%20Guidance%20for%20Consistent%20View%20Sampling&entry.906535625=Ayaka%20Yasunaga%20and%20Hideo%20Saito%20and%20Dieter%20Schmalstieg%20and%20Shohei%20Mori&entry.1292438233=%20%20Novel%20view%20synthesis%20from%20images%2C%20for%20example%2C%20with%203D%20Gaussian%20splatting%2C%0Ahas%20made%20great%20progress.%20Rendering%20fidelity%20and%20speed%20are%20now%20ready%20even%20for%0Ademanding%20virtual%20reality%20applications.%20However%2C%20the%20problem%20of%20assisting%0Ahumans%20in%20collecting%20the%20input%20images%20for%20these%20rendering%20algorithms%20has%0Areceived%20much%20less%20attention.%20High-quality%20view%20synthesis%20requires%20uniform%20and%0Adense%20view%20sampling.%20Unfortunately%2C%20these%20requirements%20are%20not%20easily%20addressed%0Aby%20human%20camera%20operators%2C%20who%20are%20in%20a%20hurry%2C%20impatient%2C%20or%20lack%20understanding%0Aof%20the%20scene%20structure%20and%20the%20photographic%20process.%20Existing%20approaches%20to%0Aguide%20humans%20during%20image%20acquisition%20concentrate%20on%20single%20objects%20or%20neglect%0Aview-dependent%20material%20characteristics.%20We%20propose%20a%20novel%20situated%0Avisualization%20technique%20for%20scanning%20at%20multiple%20scales.%20During%20the%20scanning%20of%0Aa%20scene%2C%20our%20method%20identifies%20important%20objects%20that%20need%20extended%20image%0Acoverage%20to%20properly%20represent%20view-dependent%20appearance.%20To%20this%20end%2C%20we%0Aleverage%20semantic%20segmentation%20and%20category%20identification%2C%20ranked%20by%20a%0Avision-language%20model.%20Spherical%20proxies%20are%20generated%20around%20highly%20ranked%0Aobjects%20to%20guide%20the%20user%20during%20scanning.%20Our%20results%20show%20superior%0Aperformance%20in%20real%20scenes%20compared%20to%20conventional%20view%20sampling%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13043v1&entry.124074799=Read"},
{"title": "TopoMortar: A dataset to evaluate image segmentation methods focused on\n  topology accuracy", "author": "Juan Miguel Valverde and Motoya Koga and Nijihiko Otsuka and Anders Bjorholm Dahl", "abstract": "  We present TopoMortar, a brick wall dataset that is the first dataset\nspecifically designed to evaluate topology-focused image segmentation methods,\nsuch as topology loss functions. Motivated by the known sensitivity of methods\nto dataset challenges, such as small training sets, noisy labels, and\nout-of-distribution test-set images, TopoMortar is created to enable in two\nways investigating methods' effectiveness at improving topology accuracy.\nFirst, by eliminating dataset challenges that, as we show, impact the\neffectiveness of topology loss functions. Second, by allowing to represent\ndifferent dataset challenges in the same dataset, isolating methods'\nperformance from dataset challenges. TopoMortar includes three types of labels\n(accurate, pseudo-labels, and noisy labels), two fixed training sets (large and\nsmall), and in-distribution and out-of-distribution test-set images. We\ncompared eight loss functions on TopoMortar, and we found that clDice achieved\nthe most topologically accurate segmentations, and that the relative\nadvantageousness of the other loss functions depends on the experimental\nsetting. Additionally, we show that data augmentation and self-distillation can\nelevate Cross entropy Dice loss to surpass most topology loss functions, and\nthat those simple methods can enhance topology loss functions as well.\nTopoMortar and our code can be found at https://jmlipman.github.io/TopoMortar\n", "link": "http://arxiv.org/abs/2503.03365v2", "date": "2025-08-18", "relevancy": 2.5263, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4986}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TopoMortar%3A%20A%20dataset%20to%20evaluate%20image%20segmentation%20methods%20focused%20on%0A%20%20topology%20accuracy&body=Title%3A%20TopoMortar%3A%20A%20dataset%20to%20evaluate%20image%20segmentation%20methods%20focused%20on%0A%20%20topology%20accuracy%0AAuthor%3A%20Juan%20Miguel%20Valverde%20and%20Motoya%20Koga%20and%20Nijihiko%20Otsuka%20and%20Anders%20Bjorholm%20Dahl%0AAbstract%3A%20%20%20We%20present%20TopoMortar%2C%20a%20brick%20wall%20dataset%20that%20is%20the%20first%20dataset%0Aspecifically%20designed%20to%20evaluate%20topology-focused%20image%20segmentation%20methods%2C%0Asuch%20as%20topology%20loss%20functions.%20Motivated%20by%20the%20known%20sensitivity%20of%20methods%0Ato%20dataset%20challenges%2C%20such%20as%20small%20training%20sets%2C%20noisy%20labels%2C%20and%0Aout-of-distribution%20test-set%20images%2C%20TopoMortar%20is%20created%20to%20enable%20in%20two%0Aways%20investigating%20methods%27%20effectiveness%20at%20improving%20topology%20accuracy.%0AFirst%2C%20by%20eliminating%20dataset%20challenges%20that%2C%20as%20we%20show%2C%20impact%20the%0Aeffectiveness%20of%20topology%20loss%20functions.%20Second%2C%20by%20allowing%20to%20represent%0Adifferent%20dataset%20challenges%20in%20the%20same%20dataset%2C%20isolating%20methods%27%0Aperformance%20from%20dataset%20challenges.%20TopoMortar%20includes%20three%20types%20of%20labels%0A%28accurate%2C%20pseudo-labels%2C%20and%20noisy%20labels%29%2C%20two%20fixed%20training%20sets%20%28large%20and%0Asmall%29%2C%20and%20in-distribution%20and%20out-of-distribution%20test-set%20images.%20We%0Acompared%20eight%20loss%20functions%20on%20TopoMortar%2C%20and%20we%20found%20that%20clDice%20achieved%0Athe%20most%20topologically%20accurate%20segmentations%2C%20and%20that%20the%20relative%0Aadvantageousness%20of%20the%20other%20loss%20functions%20depends%20on%20the%20experimental%0Asetting.%20Additionally%2C%20we%20show%20that%20data%20augmentation%20and%20self-distillation%20can%0Aelevate%20Cross%20entropy%20Dice%20loss%20to%20surpass%20most%20topology%20loss%20functions%2C%20and%0Athat%20those%20simple%20methods%20can%20enhance%20topology%20loss%20functions%20as%20well.%0ATopoMortar%20and%20our%20code%20can%20be%20found%20at%20https%3A//jmlipman.github.io/TopoMortar%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03365v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopoMortar%253A%2520A%2520dataset%2520to%2520evaluate%2520image%2520segmentation%2520methods%2520focused%2520on%250A%2520%2520topology%2520accuracy%26entry.906535625%3DJuan%2520Miguel%2520Valverde%2520and%2520Motoya%2520Koga%2520and%2520Nijihiko%2520Otsuka%2520and%2520Anders%2520Bjorholm%2520Dahl%26entry.1292438233%3D%2520%2520We%2520present%2520TopoMortar%252C%2520a%2520brick%2520wall%2520dataset%2520that%2520is%2520the%2520first%2520dataset%250Aspecifically%2520designed%2520to%2520evaluate%2520topology-focused%2520image%2520segmentation%2520methods%252C%250Asuch%2520as%2520topology%2520loss%2520functions.%2520Motivated%2520by%2520the%2520known%2520sensitivity%2520of%2520methods%250Ato%2520dataset%2520challenges%252C%2520such%2520as%2520small%2520training%2520sets%252C%2520noisy%2520labels%252C%2520and%250Aout-of-distribution%2520test-set%2520images%252C%2520TopoMortar%2520is%2520created%2520to%2520enable%2520in%2520two%250Aways%2520investigating%2520methods%2527%2520effectiveness%2520at%2520improving%2520topology%2520accuracy.%250AFirst%252C%2520by%2520eliminating%2520dataset%2520challenges%2520that%252C%2520as%2520we%2520show%252C%2520impact%2520the%250Aeffectiveness%2520of%2520topology%2520loss%2520functions.%2520Second%252C%2520by%2520allowing%2520to%2520represent%250Adifferent%2520dataset%2520challenges%2520in%2520the%2520same%2520dataset%252C%2520isolating%2520methods%2527%250Aperformance%2520from%2520dataset%2520challenges.%2520TopoMortar%2520includes%2520three%2520types%2520of%2520labels%250A%2528accurate%252C%2520pseudo-labels%252C%2520and%2520noisy%2520labels%2529%252C%2520two%2520fixed%2520training%2520sets%2520%2528large%2520and%250Asmall%2529%252C%2520and%2520in-distribution%2520and%2520out-of-distribution%2520test-set%2520images.%2520We%250Acompared%2520eight%2520loss%2520functions%2520on%2520TopoMortar%252C%2520and%2520we%2520found%2520that%2520clDice%2520achieved%250Athe%2520most%2520topologically%2520accurate%2520segmentations%252C%2520and%2520that%2520the%2520relative%250Aadvantageousness%2520of%2520the%2520other%2520loss%2520functions%2520depends%2520on%2520the%2520experimental%250Asetting.%2520Additionally%252C%2520we%2520show%2520that%2520data%2520augmentation%2520and%2520self-distillation%2520can%250Aelevate%2520Cross%2520entropy%2520Dice%2520loss%2520to%2520surpass%2520most%2520topology%2520loss%2520functions%252C%2520and%250Athat%2520those%2520simple%2520methods%2520can%2520enhance%2520topology%2520loss%2520functions%2520as%2520well.%250ATopoMortar%2520and%2520our%2520code%2520can%2520be%2520found%2520at%2520https%253A//jmlipman.github.io/TopoMortar%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03365v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TopoMortar%3A%20A%20dataset%20to%20evaluate%20image%20segmentation%20methods%20focused%20on%0A%20%20topology%20accuracy&entry.906535625=Juan%20Miguel%20Valverde%20and%20Motoya%20Koga%20and%20Nijihiko%20Otsuka%20and%20Anders%20Bjorholm%20Dahl&entry.1292438233=%20%20We%20present%20TopoMortar%2C%20a%20brick%20wall%20dataset%20that%20is%20the%20first%20dataset%0Aspecifically%20designed%20to%20evaluate%20topology-focused%20image%20segmentation%20methods%2C%0Asuch%20as%20topology%20loss%20functions.%20Motivated%20by%20the%20known%20sensitivity%20of%20methods%0Ato%20dataset%20challenges%2C%20such%20as%20small%20training%20sets%2C%20noisy%20labels%2C%20and%0Aout-of-distribution%20test-set%20images%2C%20TopoMortar%20is%20created%20to%20enable%20in%20two%0Aways%20investigating%20methods%27%20effectiveness%20at%20improving%20topology%20accuracy.%0AFirst%2C%20by%20eliminating%20dataset%20challenges%20that%2C%20as%20we%20show%2C%20impact%20the%0Aeffectiveness%20of%20topology%20loss%20functions.%20Second%2C%20by%20allowing%20to%20represent%0Adifferent%20dataset%20challenges%20in%20the%20same%20dataset%2C%20isolating%20methods%27%0Aperformance%20from%20dataset%20challenges.%20TopoMortar%20includes%20three%20types%20of%20labels%0A%28accurate%2C%20pseudo-labels%2C%20and%20noisy%20labels%29%2C%20two%20fixed%20training%20sets%20%28large%20and%0Asmall%29%2C%20and%20in-distribution%20and%20out-of-distribution%20test-set%20images.%20We%0Acompared%20eight%20loss%20functions%20on%20TopoMortar%2C%20and%20we%20found%20that%20clDice%20achieved%0Athe%20most%20topologically%20accurate%20segmentations%2C%20and%20that%20the%20relative%0Aadvantageousness%20of%20the%20other%20loss%20functions%20depends%20on%20the%20experimental%0Asetting.%20Additionally%2C%20we%20show%20that%20data%20augmentation%20and%20self-distillation%20can%0Aelevate%20Cross%20entropy%20Dice%20loss%20to%20surpass%20most%20topology%20loss%20functions%2C%20and%0Athat%20those%20simple%20methods%20can%20enhance%20topology%20loss%20functions%20as%20well.%0ATopoMortar%20and%20our%20code%20can%20be%20found%20at%20https%3A//jmlipman.github.io/TopoMortar%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03365v2&entry.124074799=Read"},
{"title": "Toward Storage-Aware Learning with Compressed Data An Empirical\n  Exploratory Study on JPEG", "author": "Kichang Lee and Songkuk Kim and JaeYeon Park and JeongGil Ko", "abstract": "  On-device machine learning is often constrained by limited storage,\nparticularly in continuous data collection scenarios. This paper presents an\nempirical study on storage-aware learning, focusing on the trade-off between\ndata quantity and quality via compression. We demonstrate that naive\nstrategies, such as uniform data dropping or one-size-fits-all compression, are\nsuboptimal. Our findings further reveal that data samples exhibit varying\nsensitivities to compression, supporting the feasibility of a sample-wise\nadaptive compression strategy. These insights provide a foundation for\ndeveloping a new class of storage-aware learning systems. The primary\ncontribution of this work is the systematic characterization of this\nunder-explored challenge, offering valuable insights that advance the\nunderstanding of storage-aware learning.\n", "link": "http://arxiv.org/abs/2508.12833v1", "date": "2025-08-18", "relevancy": 2.5212, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5358}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4998}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Storage-Aware%20Learning%20with%20Compressed%20Data%20An%20Empirical%0A%20%20Exploratory%20Study%20on%20JPEG&body=Title%3A%20Toward%20Storage-Aware%20Learning%20with%20Compressed%20Data%20An%20Empirical%0A%20%20Exploratory%20Study%20on%20JPEG%0AAuthor%3A%20Kichang%20Lee%20and%20Songkuk%20Kim%20and%20JaeYeon%20Park%20and%20JeongGil%20Ko%0AAbstract%3A%20%20%20On-device%20machine%20learning%20is%20often%20constrained%20by%20limited%20storage%2C%0Aparticularly%20in%20continuous%20data%20collection%20scenarios.%20This%20paper%20presents%20an%0Aempirical%20study%20on%20storage-aware%20learning%2C%20focusing%20on%20the%20trade-off%20between%0Adata%20quantity%20and%20quality%20via%20compression.%20We%20demonstrate%20that%20naive%0Astrategies%2C%20such%20as%20uniform%20data%20dropping%20or%20one-size-fits-all%20compression%2C%20are%0Asuboptimal.%20Our%20findings%20further%20reveal%20that%20data%20samples%20exhibit%20varying%0Asensitivities%20to%20compression%2C%20supporting%20the%20feasibility%20of%20a%20sample-wise%0Aadaptive%20compression%20strategy.%20These%20insights%20provide%20a%20foundation%20for%0Adeveloping%20a%20new%20class%20of%20storage-aware%20learning%20systems.%20The%20primary%0Acontribution%20of%20this%20work%20is%20the%20systematic%20characterization%20of%20this%0Aunder-explored%20challenge%2C%20offering%20valuable%20insights%20that%20advance%20the%0Aunderstanding%20of%20storage-aware%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Storage-Aware%2520Learning%2520with%2520Compressed%2520Data%2520An%2520Empirical%250A%2520%2520Exploratory%2520Study%2520on%2520JPEG%26entry.906535625%3DKichang%2520Lee%2520and%2520Songkuk%2520Kim%2520and%2520JaeYeon%2520Park%2520and%2520JeongGil%2520Ko%26entry.1292438233%3D%2520%2520On-device%2520machine%2520learning%2520is%2520often%2520constrained%2520by%2520limited%2520storage%252C%250Aparticularly%2520in%2520continuous%2520data%2520collection%2520scenarios.%2520This%2520paper%2520presents%2520an%250Aempirical%2520study%2520on%2520storage-aware%2520learning%252C%2520focusing%2520on%2520the%2520trade-off%2520between%250Adata%2520quantity%2520and%2520quality%2520via%2520compression.%2520We%2520demonstrate%2520that%2520naive%250Astrategies%252C%2520such%2520as%2520uniform%2520data%2520dropping%2520or%2520one-size-fits-all%2520compression%252C%2520are%250Asuboptimal.%2520Our%2520findings%2520further%2520reveal%2520that%2520data%2520samples%2520exhibit%2520varying%250Asensitivities%2520to%2520compression%252C%2520supporting%2520the%2520feasibility%2520of%2520a%2520sample-wise%250Aadaptive%2520compression%2520strategy.%2520These%2520insights%2520provide%2520a%2520foundation%2520for%250Adeveloping%2520a%2520new%2520class%2520of%2520storage-aware%2520learning%2520systems.%2520The%2520primary%250Acontribution%2520of%2520this%2520work%2520is%2520the%2520systematic%2520characterization%2520of%2520this%250Aunder-explored%2520challenge%252C%2520offering%2520valuable%2520insights%2520that%2520advance%2520the%250Aunderstanding%2520of%2520storage-aware%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Storage-Aware%20Learning%20with%20Compressed%20Data%20An%20Empirical%0A%20%20Exploratory%20Study%20on%20JPEG&entry.906535625=Kichang%20Lee%20and%20Songkuk%20Kim%20and%20JaeYeon%20Park%20and%20JeongGil%20Ko&entry.1292438233=%20%20On-device%20machine%20learning%20is%20often%20constrained%20by%20limited%20storage%2C%0Aparticularly%20in%20continuous%20data%20collection%20scenarios.%20This%20paper%20presents%20an%0Aempirical%20study%20on%20storage-aware%20learning%2C%20focusing%20on%20the%20trade-off%20between%0Adata%20quantity%20and%20quality%20via%20compression.%20We%20demonstrate%20that%20naive%0Astrategies%2C%20such%20as%20uniform%20data%20dropping%20or%20one-size-fits-all%20compression%2C%20are%0Asuboptimal.%20Our%20findings%20further%20reveal%20that%20data%20samples%20exhibit%20varying%0Asensitivities%20to%20compression%2C%20supporting%20the%20feasibility%20of%20a%20sample-wise%0Aadaptive%20compression%20strategy.%20These%20insights%20provide%20a%20foundation%20for%0Adeveloping%20a%20new%20class%20of%20storage-aware%20learning%20systems.%20The%20primary%0Acontribution%20of%20this%20work%20is%20the%20systematic%20characterization%20of%20this%0Aunder-explored%20challenge%2C%20offering%20valuable%20insights%20that%20advance%20the%0Aunderstanding%20of%20storage-aware%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12833v1&entry.124074799=Read"},
{"title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models", "author": "Ruikang Liu and Yuxuan Sun and Manyi Zhang and Haoli Bai and Xianzhi Yu and Tiezheng Yu and Chun Yuan and Lu Hou", "abstract": "  Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.\n", "link": "http://arxiv.org/abs/2504.04823v2", "date": "2025-08-18", "relevancy": 2.4972, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5159}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization%20Hurts%20Reasoning%3F%20An%20Empirical%20Study%20on%20Quantized%20Reasoning%0A%20%20Models&body=Title%3A%20Quantization%20Hurts%20Reasoning%3F%20An%20Empirical%20Study%20on%20Quantized%20Reasoning%0A%20%20Models%0AAuthor%3A%20Ruikang%20Liu%20and%20Yuxuan%20Sun%20and%20Manyi%20Zhang%20and%20Haoli%20Bai%20and%20Xianzhi%20Yu%20and%20Tiezheng%20Yu%20and%20Chun%20Yuan%20and%20Lu%20Hou%0AAbstract%3A%20%20%20Recent%20advancements%20in%20reasoning%20language%20models%20have%20demonstrated%20remarkable%0Aperformance%20in%20complex%20tasks%2C%20but%20their%20extended%20chain-of-thought%20reasoning%0Aprocess%20increases%20inference%20overhead.%20While%20quantization%20has%20been%20widely%0Aadopted%20to%20reduce%20the%20inference%20cost%20of%20large%20language%20models%2C%20its%20impact%20on%0Areasoning%20models%20remains%20understudied.%20In%20this%20paper%2C%20we%20conduct%20the%20first%0Asystematic%20study%20on%20quantized%20reasoning%20models%2C%20evaluating%20the%20open-sourced%0ADeepSeek-R1-Distilled%20Qwen%20and%20LLaMA%20families%20ranging%20from%201.5B%20to%2070B%0Aparameters%2C%20QwQ-32B%2C%20and%20Qwen3-8B.%20Our%20investigation%20covers%20weight%2C%20KV%20cache%2C%0Aand%20activation%20quantization%20using%20state-of-the-art%20algorithms%20at%20varying%0Abit-widths%2C%20with%20extensive%20evaluation%20across%20mathematical%20%28AIME%2C%20MATH-500%29%2C%0Ascientific%20%28GPQA%29%2C%20and%20programming%20%28LiveCodeBench%29%20reasoning%20benchmarks.%20Our%0Afindings%20reveal%20that%20while%20lossless%20quantization%20can%20be%20achieved%20with%20W8A8%20or%0AW4A16%20quantization%2C%20lower%20bit-widths%20introduce%20significant%20accuracy%20risks.%20We%0Afurther%20identify%20model%20size%2C%20model%20origin%2C%20and%20task%20difficulty%20as%20critical%0Adeterminants%20of%20performance.%20Contrary%20to%20expectations%2C%20quantized%20models%20do%20not%0Aexhibit%20increased%20output%20lengths.%20In%20addition%2C%20strategically%20scaling%20the%20model%0Asizes%20or%20reasoning%20steps%20can%20effectively%20enhance%20the%20performance.%20All%20quantized%0Amodels%20and%20codes%20are%20open-sourced%20in%0Ahttps%3A//github.com/ruikangliu/Quantized-Reasoning-Models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization%2520Hurts%2520Reasoning%253F%2520An%2520Empirical%2520Study%2520on%2520Quantized%2520Reasoning%250A%2520%2520Models%26entry.906535625%3DRuikang%2520Liu%2520and%2520Yuxuan%2520Sun%2520and%2520Manyi%2520Zhang%2520and%2520Haoli%2520Bai%2520and%2520Xianzhi%2520Yu%2520and%2520Tiezheng%2520Yu%2520and%2520Chun%2520Yuan%2520and%2520Lu%2520Hou%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520reasoning%2520language%2520models%2520have%2520demonstrated%2520remarkable%250Aperformance%2520in%2520complex%2520tasks%252C%2520but%2520their%2520extended%2520chain-of-thought%2520reasoning%250Aprocess%2520increases%2520inference%2520overhead.%2520While%2520quantization%2520has%2520been%2520widely%250Aadopted%2520to%2520reduce%2520the%2520inference%2520cost%2520of%2520large%2520language%2520models%252C%2520its%2520impact%2520on%250Areasoning%2520models%2520remains%2520understudied.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520the%2520first%250Asystematic%2520study%2520on%2520quantized%2520reasoning%2520models%252C%2520evaluating%2520the%2520open-sourced%250ADeepSeek-R1-Distilled%2520Qwen%2520and%2520LLaMA%2520families%2520ranging%2520from%25201.5B%2520to%252070B%250Aparameters%252C%2520QwQ-32B%252C%2520and%2520Qwen3-8B.%2520Our%2520investigation%2520covers%2520weight%252C%2520KV%2520cache%252C%250Aand%2520activation%2520quantization%2520using%2520state-of-the-art%2520algorithms%2520at%2520varying%250Abit-widths%252C%2520with%2520extensive%2520evaluation%2520across%2520mathematical%2520%2528AIME%252C%2520MATH-500%2529%252C%250Ascientific%2520%2528GPQA%2529%252C%2520and%2520programming%2520%2528LiveCodeBench%2529%2520reasoning%2520benchmarks.%2520Our%250Afindings%2520reveal%2520that%2520while%2520lossless%2520quantization%2520can%2520be%2520achieved%2520with%2520W8A8%2520or%250AW4A16%2520quantization%252C%2520lower%2520bit-widths%2520introduce%2520significant%2520accuracy%2520risks.%2520We%250Afurther%2520identify%2520model%2520size%252C%2520model%2520origin%252C%2520and%2520task%2520difficulty%2520as%2520critical%250Adeterminants%2520of%2520performance.%2520Contrary%2520to%2520expectations%252C%2520quantized%2520models%2520do%2520not%250Aexhibit%2520increased%2520output%2520lengths.%2520In%2520addition%252C%2520strategically%2520scaling%2520the%2520model%250Asizes%2520or%2520reasoning%2520steps%2520can%2520effectively%2520enhance%2520the%2520performance.%2520All%2520quantized%250Amodels%2520and%2520codes%2520are%2520open-sourced%2520in%250Ahttps%253A//github.com/ruikangliu/Quantized-Reasoning-Models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization%20Hurts%20Reasoning%3F%20An%20Empirical%20Study%20on%20Quantized%20Reasoning%0A%20%20Models&entry.906535625=Ruikang%20Liu%20and%20Yuxuan%20Sun%20and%20Manyi%20Zhang%20and%20Haoli%20Bai%20and%20Xianzhi%20Yu%20and%20Tiezheng%20Yu%20and%20Chun%20Yuan%20and%20Lu%20Hou&entry.1292438233=%20%20Recent%20advancements%20in%20reasoning%20language%20models%20have%20demonstrated%20remarkable%0Aperformance%20in%20complex%20tasks%2C%20but%20their%20extended%20chain-of-thought%20reasoning%0Aprocess%20increases%20inference%20overhead.%20While%20quantization%20has%20been%20widely%0Aadopted%20to%20reduce%20the%20inference%20cost%20of%20large%20language%20models%2C%20its%20impact%20on%0Areasoning%20models%20remains%20understudied.%20In%20this%20paper%2C%20we%20conduct%20the%20first%0Asystematic%20study%20on%20quantized%20reasoning%20models%2C%20evaluating%20the%20open-sourced%0ADeepSeek-R1-Distilled%20Qwen%20and%20LLaMA%20families%20ranging%20from%201.5B%20to%2070B%0Aparameters%2C%20QwQ-32B%2C%20and%20Qwen3-8B.%20Our%20investigation%20covers%20weight%2C%20KV%20cache%2C%0Aand%20activation%20quantization%20using%20state-of-the-art%20algorithms%20at%20varying%0Abit-widths%2C%20with%20extensive%20evaluation%20across%20mathematical%20%28AIME%2C%20MATH-500%29%2C%0Ascientific%20%28GPQA%29%2C%20and%20programming%20%28LiveCodeBench%29%20reasoning%20benchmarks.%20Our%0Afindings%20reveal%20that%20while%20lossless%20quantization%20can%20be%20achieved%20with%20W8A8%20or%0AW4A16%20quantization%2C%20lower%20bit-widths%20introduce%20significant%20accuracy%20risks.%20We%0Afurther%20identify%20model%20size%2C%20model%20origin%2C%20and%20task%20difficulty%20as%20critical%0Adeterminants%20of%20performance.%20Contrary%20to%20expectations%2C%20quantized%20models%20do%20not%0Aexhibit%20increased%20output%20lengths.%20In%20addition%2C%20strategically%20scaling%20the%20model%0Asizes%20or%20reasoning%20steps%20can%20effectively%20enhance%20the%20performance.%20All%20quantized%0Amodels%20and%20codes%20are%20open-sourced%20in%0Ahttps%3A//github.com/ruikangliu/Quantized-Reasoning-Models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04823v2&entry.124074799=Read"},
{"title": "Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for\n  Fast Video Generation", "author": "Qirui Li and Guangcong Zheng and Qi Zhao and Jie Li and Bin Dong and Yiwu Yao and Xi Li", "abstract": "  The computational demands of self-attention mechanisms pose a critical\nchallenge for transformer-based video generation, particularly in synthesizing\nultra-long sequences. Current approaches, such as factorized attention and\nfixed sparse patterns, fail to fully exploit the inherent spatio-temporal\nredundancies in video data. Through systematic analysis of video diffusion\ntransformers (DiT), we uncover a key insight: Attention matrices exhibit\nstructured, yet heterogeneous sparsity patterns, where specialized heads\ndynamically attend to distinct spatiotemporal regions (e.g., local pattern,\ncross-shaped pattern, or global pattern). Existing sparse attention methods\neither impose rigid constraints or introduce significant overhead, limiting\ntheir effectiveness. To address this, we propose Compact Attention, a\nhardware-aware acceleration framework featuring three innovations: 1) Adaptive\ntiling strategies that approximate diverse spatial interaction patterns via\ndynamic tile grouping, 2) Temporally varying windows that adjust sparsity\nlevels based on frame proximity, and 3) An automated configuration search\nalgorithm that optimizes sparse patterns while preserving critical attention\npathways. Our method achieves 1.6~2.5x acceleration in attention computation on\nsingle-GPU setups while maintaining comparable visual quality with\nfull-attention baselines. This work provides a principled approach to unlocking\nefficient long-form video generation through structured sparsity exploitation.\nProject Page: https://yo-ava.github.io/Compact-Attention.github.io/\n", "link": "http://arxiv.org/abs/2508.12969v1", "date": "2025-08-18", "relevancy": 2.4875, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6382}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6193}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compact%20Attention%3A%20Exploiting%20Structured%20Spatio-Temporal%20Sparsity%20for%0A%20%20Fast%20Video%20Generation&body=Title%3A%20Compact%20Attention%3A%20Exploiting%20Structured%20Spatio-Temporal%20Sparsity%20for%0A%20%20Fast%20Video%20Generation%0AAuthor%3A%20Qirui%20Li%20and%20Guangcong%20Zheng%20and%20Qi%20Zhao%20and%20Jie%20Li%20and%20Bin%20Dong%20and%20Yiwu%20Yao%20and%20Xi%20Li%0AAbstract%3A%20%20%20The%20computational%20demands%20of%20self-attention%20mechanisms%20pose%20a%20critical%0Achallenge%20for%20transformer-based%20video%20generation%2C%20particularly%20in%20synthesizing%0Aultra-long%20sequences.%20Current%20approaches%2C%20such%20as%20factorized%20attention%20and%0Afixed%20sparse%20patterns%2C%20fail%20to%20fully%20exploit%20the%20inherent%20spatio-temporal%0Aredundancies%20in%20video%20data.%20Through%20systematic%20analysis%20of%20video%20diffusion%0Atransformers%20%28DiT%29%2C%20we%20uncover%20a%20key%20insight%3A%20Attention%20matrices%20exhibit%0Astructured%2C%20yet%20heterogeneous%20sparsity%20patterns%2C%20where%20specialized%20heads%0Adynamically%20attend%20to%20distinct%20spatiotemporal%20regions%20%28e.g.%2C%20local%20pattern%2C%0Across-shaped%20pattern%2C%20or%20global%20pattern%29.%20Existing%20sparse%20attention%20methods%0Aeither%20impose%20rigid%20constraints%20or%20introduce%20significant%20overhead%2C%20limiting%0Atheir%20effectiveness.%20To%20address%20this%2C%20we%20propose%20Compact%20Attention%2C%20a%0Ahardware-aware%20acceleration%20framework%20featuring%20three%20innovations%3A%201%29%20Adaptive%0Atiling%20strategies%20that%20approximate%20diverse%20spatial%20interaction%20patterns%20via%0Adynamic%20tile%20grouping%2C%202%29%20Temporally%20varying%20windows%20that%20adjust%20sparsity%0Alevels%20based%20on%20frame%20proximity%2C%20and%203%29%20An%20automated%20configuration%20search%0Aalgorithm%20that%20optimizes%20sparse%20patterns%20while%20preserving%20critical%20attention%0Apathways.%20Our%20method%20achieves%201.6~2.5x%20acceleration%20in%20attention%20computation%20on%0Asingle-GPU%20setups%20while%20maintaining%20comparable%20visual%20quality%20with%0Afull-attention%20baselines.%20This%20work%20provides%20a%20principled%20approach%20to%20unlocking%0Aefficient%20long-form%20video%20generation%20through%20structured%20sparsity%20exploitation.%0AProject%20Page%3A%20https%3A//yo-ava.github.io/Compact-Attention.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompact%2520Attention%253A%2520Exploiting%2520Structured%2520Spatio-Temporal%2520Sparsity%2520for%250A%2520%2520Fast%2520Video%2520Generation%26entry.906535625%3DQirui%2520Li%2520and%2520Guangcong%2520Zheng%2520and%2520Qi%2520Zhao%2520and%2520Jie%2520Li%2520and%2520Bin%2520Dong%2520and%2520Yiwu%2520Yao%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520The%2520computational%2520demands%2520of%2520self-attention%2520mechanisms%2520pose%2520a%2520critical%250Achallenge%2520for%2520transformer-based%2520video%2520generation%252C%2520particularly%2520in%2520synthesizing%250Aultra-long%2520sequences.%2520Current%2520approaches%252C%2520such%2520as%2520factorized%2520attention%2520and%250Afixed%2520sparse%2520patterns%252C%2520fail%2520to%2520fully%2520exploit%2520the%2520inherent%2520spatio-temporal%250Aredundancies%2520in%2520video%2520data.%2520Through%2520systematic%2520analysis%2520of%2520video%2520diffusion%250Atransformers%2520%2528DiT%2529%252C%2520we%2520uncover%2520a%2520key%2520insight%253A%2520Attention%2520matrices%2520exhibit%250Astructured%252C%2520yet%2520heterogeneous%2520sparsity%2520patterns%252C%2520where%2520specialized%2520heads%250Adynamically%2520attend%2520to%2520distinct%2520spatiotemporal%2520regions%2520%2528e.g.%252C%2520local%2520pattern%252C%250Across-shaped%2520pattern%252C%2520or%2520global%2520pattern%2529.%2520Existing%2520sparse%2520attention%2520methods%250Aeither%2520impose%2520rigid%2520constraints%2520or%2520introduce%2520significant%2520overhead%252C%2520limiting%250Atheir%2520effectiveness.%2520To%2520address%2520this%252C%2520we%2520propose%2520Compact%2520Attention%252C%2520a%250Ahardware-aware%2520acceleration%2520framework%2520featuring%2520three%2520innovations%253A%25201%2529%2520Adaptive%250Atiling%2520strategies%2520that%2520approximate%2520diverse%2520spatial%2520interaction%2520patterns%2520via%250Adynamic%2520tile%2520grouping%252C%25202%2529%2520Temporally%2520varying%2520windows%2520that%2520adjust%2520sparsity%250Alevels%2520based%2520on%2520frame%2520proximity%252C%2520and%25203%2529%2520An%2520automated%2520configuration%2520search%250Aalgorithm%2520that%2520optimizes%2520sparse%2520patterns%2520while%2520preserving%2520critical%2520attention%250Apathways.%2520Our%2520method%2520achieves%25201.6~2.5x%2520acceleration%2520in%2520attention%2520computation%2520on%250Asingle-GPU%2520setups%2520while%2520maintaining%2520comparable%2520visual%2520quality%2520with%250Afull-attention%2520baselines.%2520This%2520work%2520provides%2520a%2520principled%2520approach%2520to%2520unlocking%250Aefficient%2520long-form%2520video%2520generation%2520through%2520structured%2520sparsity%2520exploitation.%250AProject%2520Page%253A%2520https%253A//yo-ava.github.io/Compact-Attention.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compact%20Attention%3A%20Exploiting%20Structured%20Spatio-Temporal%20Sparsity%20for%0A%20%20Fast%20Video%20Generation&entry.906535625=Qirui%20Li%20and%20Guangcong%20Zheng%20and%20Qi%20Zhao%20and%20Jie%20Li%20and%20Bin%20Dong%20and%20Yiwu%20Yao%20and%20Xi%20Li&entry.1292438233=%20%20The%20computational%20demands%20of%20self-attention%20mechanisms%20pose%20a%20critical%0Achallenge%20for%20transformer-based%20video%20generation%2C%20particularly%20in%20synthesizing%0Aultra-long%20sequences.%20Current%20approaches%2C%20such%20as%20factorized%20attention%20and%0Afixed%20sparse%20patterns%2C%20fail%20to%20fully%20exploit%20the%20inherent%20spatio-temporal%0Aredundancies%20in%20video%20data.%20Through%20systematic%20analysis%20of%20video%20diffusion%0Atransformers%20%28DiT%29%2C%20we%20uncover%20a%20key%20insight%3A%20Attention%20matrices%20exhibit%0Astructured%2C%20yet%20heterogeneous%20sparsity%20patterns%2C%20where%20specialized%20heads%0Adynamically%20attend%20to%20distinct%20spatiotemporal%20regions%20%28e.g.%2C%20local%20pattern%2C%0Across-shaped%20pattern%2C%20or%20global%20pattern%29.%20Existing%20sparse%20attention%20methods%0Aeither%20impose%20rigid%20constraints%20or%20introduce%20significant%20overhead%2C%20limiting%0Atheir%20effectiveness.%20To%20address%20this%2C%20we%20propose%20Compact%20Attention%2C%20a%0Ahardware-aware%20acceleration%20framework%20featuring%20three%20innovations%3A%201%29%20Adaptive%0Atiling%20strategies%20that%20approximate%20diverse%20spatial%20interaction%20patterns%20via%0Adynamic%20tile%20grouping%2C%202%29%20Temporally%20varying%20windows%20that%20adjust%20sparsity%0Alevels%20based%20on%20frame%20proximity%2C%20and%203%29%20An%20automated%20configuration%20search%0Aalgorithm%20that%20optimizes%20sparse%20patterns%20while%20preserving%20critical%20attention%0Apathways.%20Our%20method%20achieves%201.6~2.5x%20acceleration%20in%20attention%20computation%20on%0Asingle-GPU%20setups%20while%20maintaining%20comparable%20visual%20quality%20with%0Afull-attention%20baselines.%20This%20work%20provides%20a%20principled%20approach%20to%20unlocking%0Aefficient%20long-form%20video%20generation%20through%20structured%20sparsity%20exploitation.%0AProject%20Page%3A%20https%3A//yo-ava.github.io/Compact-Attention.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12969v1&entry.124074799=Read"},
{"title": "GraphLand: Evaluating Graph Machine Learning Models on Diverse\n  Industrial Data", "author": "Gleb Bazhenov and Oleg Platonov and Liudmila Prokhorenkova", "abstract": "  Although data that can be naturally represented as graphs is widespread in\nreal-world applications across diverse industries, popular graph ML benchmarks\nfor node property prediction only cover a surprisingly narrow set of data\ndomains, and graph neural networks (GNNs) are often evaluated on just a few\nacademic citation networks. This issue is particularly pressing in light of the\nrecent growing interest in designing graph foundation models. These models are\nsupposed to be able to transfer to diverse graph datasets from different\ndomains, and yet the proposed graph foundation models are often evaluated on a\nvery limited set of datasets from narrow applications. To alleviate this issue,\nwe introduce GraphLand: a benchmark of 14 diverse graph datasets for node\nproperty prediction from a range of different industrial applications.\nGraphLand allows evaluating graph ML models on a wide range of graphs with\ndiverse sizes, structural characteristics, and feature sets, all in a unified\nsetting. Further, GraphLand allows investigating such previously underexplored\nresearch questions as how realistic temporal distributional shifts under\ntransductive and inductive settings influence graph ML model performance. To\nmimic realistic industrial settings, we use GraphLand to compare GNNs with\ngradient-boosted decision trees (GBDT) models that are popular in industrial\napplications and show that GBDTs provided with additional graph-based input\nfeatures can sometimes be very strong baselines. Further, we evaluate currently\navailable general-purpose graph foundation models and find that they fail to\nproduce competitive results on our proposed datasets.\n", "link": "http://arxiv.org/abs/2409.14500v3", "date": "2025-08-18", "relevancy": 2.4734, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphLand%3A%20Evaluating%20Graph%20Machine%20Learning%20Models%20on%20Diverse%0A%20%20Industrial%20Data&body=Title%3A%20GraphLand%3A%20Evaluating%20Graph%20Machine%20Learning%20Models%20on%20Diverse%0A%20%20Industrial%20Data%0AAuthor%3A%20Gleb%20Bazhenov%20and%20Oleg%20Platonov%20and%20Liudmila%20Prokhorenkova%0AAbstract%3A%20%20%20Although%20data%20that%20can%20be%20naturally%20represented%20as%20graphs%20is%20widespread%20in%0Areal-world%20applications%20across%20diverse%20industries%2C%20popular%20graph%20ML%20benchmarks%0Afor%20node%20property%20prediction%20only%20cover%20a%20surprisingly%20narrow%20set%20of%20data%0Adomains%2C%20and%20graph%20neural%20networks%20%28GNNs%29%20are%20often%20evaluated%20on%20just%20a%20few%0Aacademic%20citation%20networks.%20This%20issue%20is%20particularly%20pressing%20in%20light%20of%20the%0Arecent%20growing%20interest%20in%20designing%20graph%20foundation%20models.%20These%20models%20are%0Asupposed%20to%20be%20able%20to%20transfer%20to%20diverse%20graph%20datasets%20from%20different%0Adomains%2C%20and%20yet%20the%20proposed%20graph%20foundation%20models%20are%20often%20evaluated%20on%20a%0Avery%20limited%20set%20of%20datasets%20from%20narrow%20applications.%20To%20alleviate%20this%20issue%2C%0Awe%20introduce%20GraphLand%3A%20a%20benchmark%20of%2014%20diverse%20graph%20datasets%20for%20node%0Aproperty%20prediction%20from%20a%20range%20of%20different%20industrial%20applications.%0AGraphLand%20allows%20evaluating%20graph%20ML%20models%20on%20a%20wide%20range%20of%20graphs%20with%0Adiverse%20sizes%2C%20structural%20characteristics%2C%20and%20feature%20sets%2C%20all%20in%20a%20unified%0Asetting.%20Further%2C%20GraphLand%20allows%20investigating%20such%20previously%20underexplored%0Aresearch%20questions%20as%20how%20realistic%20temporal%20distributional%20shifts%20under%0Atransductive%20and%20inductive%20settings%20influence%20graph%20ML%20model%20performance.%20To%0Amimic%20realistic%20industrial%20settings%2C%20we%20use%20GraphLand%20to%20compare%20GNNs%20with%0Agradient-boosted%20decision%20trees%20%28GBDT%29%20models%20that%20are%20popular%20in%20industrial%0Aapplications%20and%20show%20that%20GBDTs%20provided%20with%20additional%20graph-based%20input%0Afeatures%20can%20sometimes%20be%20very%20strong%20baselines.%20Further%2C%20we%20evaluate%20currently%0Aavailable%20general-purpose%20graph%20foundation%20models%20and%20find%20that%20they%20fail%20to%0Aproduce%20competitive%20results%20on%20our%20proposed%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14500v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphLand%253A%2520Evaluating%2520Graph%2520Machine%2520Learning%2520Models%2520on%2520Diverse%250A%2520%2520Industrial%2520Data%26entry.906535625%3DGleb%2520Bazhenov%2520and%2520Oleg%2520Platonov%2520and%2520Liudmila%2520Prokhorenkova%26entry.1292438233%3D%2520%2520Although%2520data%2520that%2520can%2520be%2520naturally%2520represented%2520as%2520graphs%2520is%2520widespread%2520in%250Areal-world%2520applications%2520across%2520diverse%2520industries%252C%2520popular%2520graph%2520ML%2520benchmarks%250Afor%2520node%2520property%2520prediction%2520only%2520cover%2520a%2520surprisingly%2520narrow%2520set%2520of%2520data%250Adomains%252C%2520and%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520often%2520evaluated%2520on%2520just%2520a%2520few%250Aacademic%2520citation%2520networks.%2520This%2520issue%2520is%2520particularly%2520pressing%2520in%2520light%2520of%2520the%250Arecent%2520growing%2520interest%2520in%2520designing%2520graph%2520foundation%2520models.%2520These%2520models%2520are%250Asupposed%2520to%2520be%2520able%2520to%2520transfer%2520to%2520diverse%2520graph%2520datasets%2520from%2520different%250Adomains%252C%2520and%2520yet%2520the%2520proposed%2520graph%2520foundation%2520models%2520are%2520often%2520evaluated%2520on%2520a%250Avery%2520limited%2520set%2520of%2520datasets%2520from%2520narrow%2520applications.%2520To%2520alleviate%2520this%2520issue%252C%250Awe%2520introduce%2520GraphLand%253A%2520a%2520benchmark%2520of%252014%2520diverse%2520graph%2520datasets%2520for%2520node%250Aproperty%2520prediction%2520from%2520a%2520range%2520of%2520different%2520industrial%2520applications.%250AGraphLand%2520allows%2520evaluating%2520graph%2520ML%2520models%2520on%2520a%2520wide%2520range%2520of%2520graphs%2520with%250Adiverse%2520sizes%252C%2520structural%2520characteristics%252C%2520and%2520feature%2520sets%252C%2520all%2520in%2520a%2520unified%250Asetting.%2520Further%252C%2520GraphLand%2520allows%2520investigating%2520such%2520previously%2520underexplored%250Aresearch%2520questions%2520as%2520how%2520realistic%2520temporal%2520distributional%2520shifts%2520under%250Atransductive%2520and%2520inductive%2520settings%2520influence%2520graph%2520ML%2520model%2520performance.%2520To%250Amimic%2520realistic%2520industrial%2520settings%252C%2520we%2520use%2520GraphLand%2520to%2520compare%2520GNNs%2520with%250Agradient-boosted%2520decision%2520trees%2520%2528GBDT%2529%2520models%2520that%2520are%2520popular%2520in%2520industrial%250Aapplications%2520and%2520show%2520that%2520GBDTs%2520provided%2520with%2520additional%2520graph-based%2520input%250Afeatures%2520can%2520sometimes%2520be%2520very%2520strong%2520baselines.%2520Further%252C%2520we%2520evaluate%2520currently%250Aavailable%2520general-purpose%2520graph%2520foundation%2520models%2520and%2520find%2520that%2520they%2520fail%2520to%250Aproduce%2520competitive%2520results%2520on%2520our%2520proposed%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14500v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphLand%3A%20Evaluating%20Graph%20Machine%20Learning%20Models%20on%20Diverse%0A%20%20Industrial%20Data&entry.906535625=Gleb%20Bazhenov%20and%20Oleg%20Platonov%20and%20Liudmila%20Prokhorenkova&entry.1292438233=%20%20Although%20data%20that%20can%20be%20naturally%20represented%20as%20graphs%20is%20widespread%20in%0Areal-world%20applications%20across%20diverse%20industries%2C%20popular%20graph%20ML%20benchmarks%0Afor%20node%20property%20prediction%20only%20cover%20a%20surprisingly%20narrow%20set%20of%20data%0Adomains%2C%20and%20graph%20neural%20networks%20%28GNNs%29%20are%20often%20evaluated%20on%20just%20a%20few%0Aacademic%20citation%20networks.%20This%20issue%20is%20particularly%20pressing%20in%20light%20of%20the%0Arecent%20growing%20interest%20in%20designing%20graph%20foundation%20models.%20These%20models%20are%0Asupposed%20to%20be%20able%20to%20transfer%20to%20diverse%20graph%20datasets%20from%20different%0Adomains%2C%20and%20yet%20the%20proposed%20graph%20foundation%20models%20are%20often%20evaluated%20on%20a%0Avery%20limited%20set%20of%20datasets%20from%20narrow%20applications.%20To%20alleviate%20this%20issue%2C%0Awe%20introduce%20GraphLand%3A%20a%20benchmark%20of%2014%20diverse%20graph%20datasets%20for%20node%0Aproperty%20prediction%20from%20a%20range%20of%20different%20industrial%20applications.%0AGraphLand%20allows%20evaluating%20graph%20ML%20models%20on%20a%20wide%20range%20of%20graphs%20with%0Adiverse%20sizes%2C%20structural%20characteristics%2C%20and%20feature%20sets%2C%20all%20in%20a%20unified%0Asetting.%20Further%2C%20GraphLand%20allows%20investigating%20such%20previously%20underexplored%0Aresearch%20questions%20as%20how%20realistic%20temporal%20distributional%20shifts%20under%0Atransductive%20and%20inductive%20settings%20influence%20graph%20ML%20model%20performance.%20To%0Amimic%20realistic%20industrial%20settings%2C%20we%20use%20GraphLand%20to%20compare%20GNNs%20with%0Agradient-boosted%20decision%20trees%20%28GBDT%29%20models%20that%20are%20popular%20in%20industrial%0Aapplications%20and%20show%20that%20GBDTs%20provided%20with%20additional%20graph-based%20input%0Afeatures%20can%20sometimes%20be%20very%20strong%20baselines.%20Further%2C%20we%20evaluate%20currently%0Aavailable%20general-purpose%20graph%20foundation%20models%20and%20find%20that%20they%20fail%20to%0Aproduce%20competitive%20results%20on%20our%20proposed%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14500v3&entry.124074799=Read"},
{"title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden\n  Representation Patterns", "author": "Xin Chen and Junchao Wu and Shu Yang and Runzhe Zhan and Zeyu Wu and Ziyang Luo and Di Wang and Min Yang and Lidia S. Chao and Derek F. Wong", "abstract": "  Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard\n", "link": "http://arxiv.org/abs/2508.13152v1", "date": "2025-08-18", "relevancy": 2.4479, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4961}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4889}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RepreGuard%3A%20Detecting%20LLM-Generated%20Text%20by%20Revealing%20Hidden%0A%20%20Representation%20Patterns&body=Title%3A%20RepreGuard%3A%20Detecting%20LLM-Generated%20Text%20by%20Revealing%20Hidden%0A%20%20Representation%20Patterns%0AAuthor%3A%20Xin%20Chen%20and%20Junchao%20Wu%20and%20Shu%20Yang%20and%20Runzhe%20Zhan%20and%20Zeyu%20Wu%20and%20Ziyang%20Luo%20and%20Di%20Wang%20and%20Min%20Yang%20and%20Lidia%20S.%20Chao%20and%20Derek%20F.%20Wong%0AAbstract%3A%20%20%20Detecting%20content%20generated%20by%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%0Apreventing%20misuse%20and%20building%20trustworthy%20AI%20systems.%20Although%20existing%0Adetection%20methods%20perform%20well%2C%20their%20robustness%20in%20out-of-distribution%20%28OOD%29%0Ascenarios%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20hypothesize%20that%2C%20compared%20to%0Afeatures%20used%20by%20existing%20detection%20methods%2C%20the%20internal%20representations%20of%0ALLMs%20contain%20more%20comprehensive%20and%20raw%20features%20that%20can%20more%20effectively%0Acapture%20and%20distinguish%20the%20statistical%20pattern%20differences%20between%0ALLM-generated%20texts%20%28LGT%29%20and%20human-written%20texts%20%28HWT%29.%20We%20validated%20this%0Ahypothesis%20across%20different%20LLMs%20and%20observed%20significant%20differences%20in%20neural%0Aactivation%20patterns%20when%20processing%20these%20two%20types%20of%20texts.%20Based%20on%20this%2C%20we%0Apropose%20RepreGuard%2C%20an%20efficient%20statistics-based%20detection%20method.%0ASpecifically%2C%20we%20first%20employ%20a%20surrogate%20model%20to%20collect%20representation%20of%0ALGT%20and%20HWT%2C%20and%20extract%20the%20distinct%20activation%20feature%20that%20can%20better%0Aidentify%20LGT.%20We%20can%20classify%20the%20text%20by%20calculating%20the%20projection%20score%20of%0Athe%20text%20representations%20along%20this%20feature%20direction%20and%20comparing%20with%20a%0Aprecomputed%20threshold.%20Experimental%20results%20show%20that%20RepreGuard%20outperforms%0Aall%20baselines%20with%20average%2094.92%25%20AUROC%20on%20both%20in-distribution%20%28ID%29%20and%20OOD%0Ascenarios%2C%20while%20also%20demonstrating%20robust%20resilience%20to%20various%20text%20sizes%20and%0Amainstream%20attacks.%20Data%20and%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/NLP2CT/RepreGuard%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepreGuard%253A%2520Detecting%2520LLM-Generated%2520Text%2520by%2520Revealing%2520Hidden%250A%2520%2520Representation%2520Patterns%26entry.906535625%3DXin%2520Chen%2520and%2520Junchao%2520Wu%2520and%2520Shu%2520Yang%2520and%2520Runzhe%2520Zhan%2520and%2520Zeyu%2520Wu%2520and%2520Ziyang%2520Luo%2520and%2520Di%2520Wang%2520and%2520Min%2520Yang%2520and%2520Lidia%2520S.%2520Chao%2520and%2520Derek%2520F.%2520Wong%26entry.1292438233%3D%2520%2520Detecting%2520content%2520generated%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520crucial%2520for%250Apreventing%2520misuse%2520and%2520building%2520trustworthy%2520AI%2520systems.%2520Although%2520existing%250Adetection%2520methods%2520perform%2520well%252C%2520their%2520robustness%2520in%2520out-of-distribution%2520%2528OOD%2529%250Ascenarios%2520is%2520still%2520lacking.%2520In%2520this%2520paper%252C%2520we%2520hypothesize%2520that%252C%2520compared%2520to%250Afeatures%2520used%2520by%2520existing%2520detection%2520methods%252C%2520the%2520internal%2520representations%2520of%250ALLMs%2520contain%2520more%2520comprehensive%2520and%2520raw%2520features%2520that%2520can%2520more%2520effectively%250Acapture%2520and%2520distinguish%2520the%2520statistical%2520pattern%2520differences%2520between%250ALLM-generated%2520texts%2520%2528LGT%2529%2520and%2520human-written%2520texts%2520%2528HWT%2529.%2520We%2520validated%2520this%250Ahypothesis%2520across%2520different%2520LLMs%2520and%2520observed%2520significant%2520differences%2520in%2520neural%250Aactivation%2520patterns%2520when%2520processing%2520these%2520two%2520types%2520of%2520texts.%2520Based%2520on%2520this%252C%2520we%250Apropose%2520RepreGuard%252C%2520an%2520efficient%2520statistics-based%2520detection%2520method.%250ASpecifically%252C%2520we%2520first%2520employ%2520a%2520surrogate%2520model%2520to%2520collect%2520representation%2520of%250ALGT%2520and%2520HWT%252C%2520and%2520extract%2520the%2520distinct%2520activation%2520feature%2520that%2520can%2520better%250Aidentify%2520LGT.%2520We%2520can%2520classify%2520the%2520text%2520by%2520calculating%2520the%2520projection%2520score%2520of%250Athe%2520text%2520representations%2520along%2520this%2520feature%2520direction%2520and%2520comparing%2520with%2520a%250Aprecomputed%2520threshold.%2520Experimental%2520results%2520show%2520that%2520RepreGuard%2520outperforms%250Aall%2520baselines%2520with%2520average%252094.92%2525%2520AUROC%2520on%2520both%2520in-distribution%2520%2528ID%2529%2520and%2520OOD%250Ascenarios%252C%2520while%2520also%2520demonstrating%2520robust%2520resilience%2520to%2520various%2520text%2520sizes%2520and%250Amainstream%2520attacks.%2520Data%2520and%2520code%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/NLP2CT/RepreGuard%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RepreGuard%3A%20Detecting%20LLM-Generated%20Text%20by%20Revealing%20Hidden%0A%20%20Representation%20Patterns&entry.906535625=Xin%20Chen%20and%20Junchao%20Wu%20and%20Shu%20Yang%20and%20Runzhe%20Zhan%20and%20Zeyu%20Wu%20and%20Ziyang%20Luo%20and%20Di%20Wang%20and%20Min%20Yang%20and%20Lidia%20S.%20Chao%20and%20Derek%20F.%20Wong&entry.1292438233=%20%20Detecting%20content%20generated%20by%20large%20language%20models%20%28LLMs%29%20is%20crucial%20for%0Apreventing%20misuse%20and%20building%20trustworthy%20AI%20systems.%20Although%20existing%0Adetection%20methods%20perform%20well%2C%20their%20robustness%20in%20out-of-distribution%20%28OOD%29%0Ascenarios%20is%20still%20lacking.%20In%20this%20paper%2C%20we%20hypothesize%20that%2C%20compared%20to%0Afeatures%20used%20by%20existing%20detection%20methods%2C%20the%20internal%20representations%20of%0ALLMs%20contain%20more%20comprehensive%20and%20raw%20features%20that%20can%20more%20effectively%0Acapture%20and%20distinguish%20the%20statistical%20pattern%20differences%20between%0ALLM-generated%20texts%20%28LGT%29%20and%20human-written%20texts%20%28HWT%29.%20We%20validated%20this%0Ahypothesis%20across%20different%20LLMs%20and%20observed%20significant%20differences%20in%20neural%0Aactivation%20patterns%20when%20processing%20these%20two%20types%20of%20texts.%20Based%20on%20this%2C%20we%0Apropose%20RepreGuard%2C%20an%20efficient%20statistics-based%20detection%20method.%0ASpecifically%2C%20we%20first%20employ%20a%20surrogate%20model%20to%20collect%20representation%20of%0ALGT%20and%20HWT%2C%20and%20extract%20the%20distinct%20activation%20feature%20that%20can%20better%0Aidentify%20LGT.%20We%20can%20classify%20the%20text%20by%20calculating%20the%20projection%20score%20of%0Athe%20text%20representations%20along%20this%20feature%20direction%20and%20comparing%20with%20a%0Aprecomputed%20threshold.%20Experimental%20results%20show%20that%20RepreGuard%20outperforms%0Aall%20baselines%20with%20average%2094.92%25%20AUROC%20on%20both%20in-distribution%20%28ID%29%20and%20OOD%0Ascenarios%2C%20while%20also%20demonstrating%20robust%20resilience%20to%20various%20text%20sizes%20and%0Amainstream%20attacks.%20Data%20and%20code%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/NLP2CT/RepreGuard%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13152v1&entry.124074799=Read"},
{"title": "MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order\n  Motion Representation", "author": "Wei Wei and Shaojie Zhang and Yonghao Dang and Jianqin Yin", "abstract": "  Human action recognition is a crucial task for intelligent robotics,\nparticularly within the context of human-robot collaboration research. In\nself-supervised skeleton-based action recognition, the mask-based\nreconstruction paradigm learns the spatial structure and motion patterns of the\nskeleton by masking joints and reconstructing the target from unlabeled data.\nHowever, existing methods focus on a limited set of joints and low-order motion\npatterns, limiting the model's ability to understand complex motion patterns.\nTo address this issue, we introduce MaskSem, a novel semantic-guided masking\nmethod for learning 3D hybrid high-order motion representations. This novel\nframework leverages Grad-CAM based on relative motion to guide the masking of\njoints, which can be represented as the most semantically rich temporal\norgions. The semantic-guided masking process can encourage the model to explore\nmore discriminative features. Furthermore, we propose using hybrid high-order\nmotion as the reconstruction target, enabling the model to learn multi-order\nmotion patterns. Specifically, low-order motion velocity and high-order motion\nacceleration are used together as the reconstruction target. This approach\noffers a more comprehensive description of the dynamic motion process,\nenhancing the model's understanding of motion patterns. Experiments on the\nNTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla\ntransformer, improves skeleton-based action recognition, making it more\nsuitable for applications in human-robot interaction.\n", "link": "http://arxiv.org/abs/2508.12948v1", "date": "2025-08-18", "relevancy": 2.4434, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaskSem%3A%20Semantic-Guided%20Masking%20for%20Learning%203D%20Hybrid%20High-Order%0A%20%20Motion%20Representation&body=Title%3A%20MaskSem%3A%20Semantic-Guided%20Masking%20for%20Learning%203D%20Hybrid%20High-Order%0A%20%20Motion%20Representation%0AAuthor%3A%20Wei%20Wei%20and%20Shaojie%20Zhang%20and%20Yonghao%20Dang%20and%20Jianqin%20Yin%0AAbstract%3A%20%20%20Human%20action%20recognition%20is%20a%20crucial%20task%20for%20intelligent%20robotics%2C%0Aparticularly%20within%20the%20context%20of%20human-robot%20collaboration%20research.%20In%0Aself-supervised%20skeleton-based%20action%20recognition%2C%20the%20mask-based%0Areconstruction%20paradigm%20learns%20the%20spatial%20structure%20and%20motion%20patterns%20of%20the%0Askeleton%20by%20masking%20joints%20and%20reconstructing%20the%20target%20from%20unlabeled%20data.%0AHowever%2C%20existing%20methods%20focus%20on%20a%20limited%20set%20of%20joints%20and%20low-order%20motion%0Apatterns%2C%20limiting%20the%20model%27s%20ability%20to%20understand%20complex%20motion%20patterns.%0ATo%20address%20this%20issue%2C%20we%20introduce%20MaskSem%2C%20a%20novel%20semantic-guided%20masking%0Amethod%20for%20learning%203D%20hybrid%20high-order%20motion%20representations.%20This%20novel%0Aframework%20leverages%20Grad-CAM%20based%20on%20relative%20motion%20to%20guide%20the%20masking%20of%0Ajoints%2C%20which%20can%20be%20represented%20as%20the%20most%20semantically%20rich%20temporal%0Aorgions.%20The%20semantic-guided%20masking%20process%20can%20encourage%20the%20model%20to%20explore%0Amore%20discriminative%20features.%20Furthermore%2C%20we%20propose%20using%20hybrid%20high-order%0Amotion%20as%20the%20reconstruction%20target%2C%20enabling%20the%20model%20to%20learn%20multi-order%0Amotion%20patterns.%20Specifically%2C%20low-order%20motion%20velocity%20and%20high-order%20motion%0Aacceleration%20are%20used%20together%20as%20the%20reconstruction%20target.%20This%20approach%0Aoffers%20a%20more%20comprehensive%20description%20of%20the%20dynamic%20motion%20process%2C%0Aenhancing%20the%20model%27s%20understanding%20of%20motion%20patterns.%20Experiments%20on%20the%0ANTU60%2C%20NTU120%2C%20and%20PKU-MMD%20datasets%20show%20that%20MaskSem%2C%20combined%20with%20a%20vanilla%0Atransformer%2C%20improves%20skeleton-based%20action%20recognition%2C%20making%20it%20more%0Asuitable%20for%20applications%20in%20human-robot%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaskSem%253A%2520Semantic-Guided%2520Masking%2520for%2520Learning%25203D%2520Hybrid%2520High-Order%250A%2520%2520Motion%2520Representation%26entry.906535625%3DWei%2520Wei%2520and%2520Shaojie%2520Zhang%2520and%2520Yonghao%2520Dang%2520and%2520Jianqin%2520Yin%26entry.1292438233%3D%2520%2520Human%2520action%2520recognition%2520is%2520a%2520crucial%2520task%2520for%2520intelligent%2520robotics%252C%250Aparticularly%2520within%2520the%2520context%2520of%2520human-robot%2520collaboration%2520research.%2520In%250Aself-supervised%2520skeleton-based%2520action%2520recognition%252C%2520the%2520mask-based%250Areconstruction%2520paradigm%2520learns%2520the%2520spatial%2520structure%2520and%2520motion%2520patterns%2520of%2520the%250Askeleton%2520by%2520masking%2520joints%2520and%2520reconstructing%2520the%2520target%2520from%2520unlabeled%2520data.%250AHowever%252C%2520existing%2520methods%2520focus%2520on%2520a%2520limited%2520set%2520of%2520joints%2520and%2520low-order%2520motion%250Apatterns%252C%2520limiting%2520the%2520model%2527s%2520ability%2520to%2520understand%2520complex%2520motion%2520patterns.%250ATo%2520address%2520this%2520issue%252C%2520we%2520introduce%2520MaskSem%252C%2520a%2520novel%2520semantic-guided%2520masking%250Amethod%2520for%2520learning%25203D%2520hybrid%2520high-order%2520motion%2520representations.%2520This%2520novel%250Aframework%2520leverages%2520Grad-CAM%2520based%2520on%2520relative%2520motion%2520to%2520guide%2520the%2520masking%2520of%250Ajoints%252C%2520which%2520can%2520be%2520represented%2520as%2520the%2520most%2520semantically%2520rich%2520temporal%250Aorgions.%2520The%2520semantic-guided%2520masking%2520process%2520can%2520encourage%2520the%2520model%2520to%2520explore%250Amore%2520discriminative%2520features.%2520Furthermore%252C%2520we%2520propose%2520using%2520hybrid%2520high-order%250Amotion%2520as%2520the%2520reconstruction%2520target%252C%2520enabling%2520the%2520model%2520to%2520learn%2520multi-order%250Amotion%2520patterns.%2520Specifically%252C%2520low-order%2520motion%2520velocity%2520and%2520high-order%2520motion%250Aacceleration%2520are%2520used%2520together%2520as%2520the%2520reconstruction%2520target.%2520This%2520approach%250Aoffers%2520a%2520more%2520comprehensive%2520description%2520of%2520the%2520dynamic%2520motion%2520process%252C%250Aenhancing%2520the%2520model%2527s%2520understanding%2520of%2520motion%2520patterns.%2520Experiments%2520on%2520the%250ANTU60%252C%2520NTU120%252C%2520and%2520PKU-MMD%2520datasets%2520show%2520that%2520MaskSem%252C%2520combined%2520with%2520a%2520vanilla%250Atransformer%252C%2520improves%2520skeleton-based%2520action%2520recognition%252C%2520making%2520it%2520more%250Asuitable%2520for%2520applications%2520in%2520human-robot%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaskSem%3A%20Semantic-Guided%20Masking%20for%20Learning%203D%20Hybrid%20High-Order%0A%20%20Motion%20Representation&entry.906535625=Wei%20Wei%20and%20Shaojie%20Zhang%20and%20Yonghao%20Dang%20and%20Jianqin%20Yin&entry.1292438233=%20%20Human%20action%20recognition%20is%20a%20crucial%20task%20for%20intelligent%20robotics%2C%0Aparticularly%20within%20the%20context%20of%20human-robot%20collaboration%20research.%20In%0Aself-supervised%20skeleton-based%20action%20recognition%2C%20the%20mask-based%0Areconstruction%20paradigm%20learns%20the%20spatial%20structure%20and%20motion%20patterns%20of%20the%0Askeleton%20by%20masking%20joints%20and%20reconstructing%20the%20target%20from%20unlabeled%20data.%0AHowever%2C%20existing%20methods%20focus%20on%20a%20limited%20set%20of%20joints%20and%20low-order%20motion%0Apatterns%2C%20limiting%20the%20model%27s%20ability%20to%20understand%20complex%20motion%20patterns.%0ATo%20address%20this%20issue%2C%20we%20introduce%20MaskSem%2C%20a%20novel%20semantic-guided%20masking%0Amethod%20for%20learning%203D%20hybrid%20high-order%20motion%20representations.%20This%20novel%0Aframework%20leverages%20Grad-CAM%20based%20on%20relative%20motion%20to%20guide%20the%20masking%20of%0Ajoints%2C%20which%20can%20be%20represented%20as%20the%20most%20semantically%20rich%20temporal%0Aorgions.%20The%20semantic-guided%20masking%20process%20can%20encourage%20the%20model%20to%20explore%0Amore%20discriminative%20features.%20Furthermore%2C%20we%20propose%20using%20hybrid%20high-order%0Amotion%20as%20the%20reconstruction%20target%2C%20enabling%20the%20model%20to%20learn%20multi-order%0Amotion%20patterns.%20Specifically%2C%20low-order%20motion%20velocity%20and%20high-order%20motion%0Aacceleration%20are%20used%20together%20as%20the%20reconstruction%20target.%20This%20approach%0Aoffers%20a%20more%20comprehensive%20description%20of%20the%20dynamic%20motion%20process%2C%0Aenhancing%20the%20model%27s%20understanding%20of%20motion%20patterns.%20Experiments%20on%20the%0ANTU60%2C%20NTU120%2C%20and%20PKU-MMD%20datasets%20show%20that%20MaskSem%2C%20combined%20with%20a%20vanilla%0Atransformer%2C%20improves%20skeleton-based%20action%20recognition%2C%20making%20it%20more%0Asuitable%20for%20applications%20in%20human-robot%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12948v1&entry.124074799=Read"},
{"title": "Precise Action-to-Video Generation Through Visual Action Prompts", "author": "Yuang Wang and Chao Wen and Haoyu Guo and Sida Peng and Minghan Qin and Hujun Bao and Xiaowei Zhou and Ruizhen Hu", "abstract": "  We present visual action prompts, a unified action representation for\naction-to-video generation of complex high-DoF interactions while maintaining\ntransferable visual dynamics across domains. Action-driven video generation\nfaces a precision-generality trade-off: existing methods using text, primitive\nactions, or coarse masks offer generality but lack precision, while\nagent-centric action signals provide precision at the cost of cross-domain\ntransferability. To balance action precision and dynamic transferability, we\npropose to \"render\" actions into precise visual prompts as domain-agnostic\nrepresentations that preserve both geometric precision and cross-domain\nadaptability for complex actions; specifically, we choose visual skeletons for\ntheir generality and accessibility. We propose robust pipelines to construct\nskeletons from two interaction-rich data sources - human-object interactions\n(HOI) and dexterous robotic manipulation - enabling cross-domain training of\naction-driven generative models. By integrating visual skeletons into\npretrained video generation models via lightweight fine-tuning, we enable\nprecise action control of complex interaction while preserving the learning of\ncross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the\neffectiveness of our proposed approach. Project page:\nhttps://zju3dv.github.io/VAP/.\n", "link": "http://arxiv.org/abs/2508.13104v1", "date": "2025-08-18", "relevancy": 2.4375, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6279}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5969}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Precise%20Action-to-Video%20Generation%20Through%20Visual%20Action%20Prompts&body=Title%3A%20Precise%20Action-to-Video%20Generation%20Through%20Visual%20Action%20Prompts%0AAuthor%3A%20Yuang%20Wang%20and%20Chao%20Wen%20and%20Haoyu%20Guo%20and%20Sida%20Peng%20and%20Minghan%20Qin%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou%20and%20Ruizhen%20Hu%0AAbstract%3A%20%20%20We%20present%20visual%20action%20prompts%2C%20a%20unified%20action%20representation%20for%0Aaction-to-video%20generation%20of%20complex%20high-DoF%20interactions%20while%20maintaining%0Atransferable%20visual%20dynamics%20across%20domains.%20Action-driven%20video%20generation%0Afaces%20a%20precision-generality%20trade-off%3A%20existing%20methods%20using%20text%2C%20primitive%0Aactions%2C%20or%20coarse%20masks%20offer%20generality%20but%20lack%20precision%2C%20while%0Aagent-centric%20action%20signals%20provide%20precision%20at%20the%20cost%20of%20cross-domain%0Atransferability.%20To%20balance%20action%20precision%20and%20dynamic%20transferability%2C%20we%0Apropose%20to%20%22render%22%20actions%20into%20precise%20visual%20prompts%20as%20domain-agnostic%0Arepresentations%20that%20preserve%20both%20geometric%20precision%20and%20cross-domain%0Aadaptability%20for%20complex%20actions%3B%20specifically%2C%20we%20choose%20visual%20skeletons%20for%0Atheir%20generality%20and%20accessibility.%20We%20propose%20robust%20pipelines%20to%20construct%0Askeletons%20from%20two%20interaction-rich%20data%20sources%20-%20human-object%20interactions%0A%28HOI%29%20and%20dexterous%20robotic%20manipulation%20-%20enabling%20cross-domain%20training%20of%0Aaction-driven%20generative%20models.%20By%20integrating%20visual%20skeletons%20into%0Apretrained%20video%20generation%20models%20via%20lightweight%20fine-tuning%2C%20we%20enable%0Aprecise%20action%20control%20of%20complex%20interaction%20while%20preserving%20the%20learning%20of%0Across-domain%20dynamics.%20Experiments%20on%20EgoVid%2C%20RT-1%20and%20DROID%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20approach.%20Project%20page%3A%0Ahttps%3A//zju3dv.github.io/VAP/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrecise%2520Action-to-Video%2520Generation%2520Through%2520Visual%2520Action%2520Prompts%26entry.906535625%3DYuang%2520Wang%2520and%2520Chao%2520Wen%2520and%2520Haoyu%2520Guo%2520and%2520Sida%2520Peng%2520and%2520Minghan%2520Qin%2520and%2520Hujun%2520Bao%2520and%2520Xiaowei%2520Zhou%2520and%2520Ruizhen%2520Hu%26entry.1292438233%3D%2520%2520We%2520present%2520visual%2520action%2520prompts%252C%2520a%2520unified%2520action%2520representation%2520for%250Aaction-to-video%2520generation%2520of%2520complex%2520high-DoF%2520interactions%2520while%2520maintaining%250Atransferable%2520visual%2520dynamics%2520across%2520domains.%2520Action-driven%2520video%2520generation%250Afaces%2520a%2520precision-generality%2520trade-off%253A%2520existing%2520methods%2520using%2520text%252C%2520primitive%250Aactions%252C%2520or%2520coarse%2520masks%2520offer%2520generality%2520but%2520lack%2520precision%252C%2520while%250Aagent-centric%2520action%2520signals%2520provide%2520precision%2520at%2520the%2520cost%2520of%2520cross-domain%250Atransferability.%2520To%2520balance%2520action%2520precision%2520and%2520dynamic%2520transferability%252C%2520we%250Apropose%2520to%2520%2522render%2522%2520actions%2520into%2520precise%2520visual%2520prompts%2520as%2520domain-agnostic%250Arepresentations%2520that%2520preserve%2520both%2520geometric%2520precision%2520and%2520cross-domain%250Aadaptability%2520for%2520complex%2520actions%253B%2520specifically%252C%2520we%2520choose%2520visual%2520skeletons%2520for%250Atheir%2520generality%2520and%2520accessibility.%2520We%2520propose%2520robust%2520pipelines%2520to%2520construct%250Askeletons%2520from%2520two%2520interaction-rich%2520data%2520sources%2520-%2520human-object%2520interactions%250A%2528HOI%2529%2520and%2520dexterous%2520robotic%2520manipulation%2520-%2520enabling%2520cross-domain%2520training%2520of%250Aaction-driven%2520generative%2520models.%2520By%2520integrating%2520visual%2520skeletons%2520into%250Apretrained%2520video%2520generation%2520models%2520via%2520lightweight%2520fine-tuning%252C%2520we%2520enable%250Aprecise%2520action%2520control%2520of%2520complex%2520interaction%2520while%2520preserving%2520the%2520learning%2520of%250Across-domain%2520dynamics.%2520Experiments%2520on%2520EgoVid%252C%2520RT-1%2520and%2520DROID%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520approach.%2520Project%2520page%253A%250Ahttps%253A//zju3dv.github.io/VAP/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Precise%20Action-to-Video%20Generation%20Through%20Visual%20Action%20Prompts&entry.906535625=Yuang%20Wang%20and%20Chao%20Wen%20and%20Haoyu%20Guo%20and%20Sida%20Peng%20and%20Minghan%20Qin%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou%20and%20Ruizhen%20Hu&entry.1292438233=%20%20We%20present%20visual%20action%20prompts%2C%20a%20unified%20action%20representation%20for%0Aaction-to-video%20generation%20of%20complex%20high-DoF%20interactions%20while%20maintaining%0Atransferable%20visual%20dynamics%20across%20domains.%20Action-driven%20video%20generation%0Afaces%20a%20precision-generality%20trade-off%3A%20existing%20methods%20using%20text%2C%20primitive%0Aactions%2C%20or%20coarse%20masks%20offer%20generality%20but%20lack%20precision%2C%20while%0Aagent-centric%20action%20signals%20provide%20precision%20at%20the%20cost%20of%20cross-domain%0Atransferability.%20To%20balance%20action%20precision%20and%20dynamic%20transferability%2C%20we%0Apropose%20to%20%22render%22%20actions%20into%20precise%20visual%20prompts%20as%20domain-agnostic%0Arepresentations%20that%20preserve%20both%20geometric%20precision%20and%20cross-domain%0Aadaptability%20for%20complex%20actions%3B%20specifically%2C%20we%20choose%20visual%20skeletons%20for%0Atheir%20generality%20and%20accessibility.%20We%20propose%20robust%20pipelines%20to%20construct%0Askeletons%20from%20two%20interaction-rich%20data%20sources%20-%20human-object%20interactions%0A%28HOI%29%20and%20dexterous%20robotic%20manipulation%20-%20enabling%20cross-domain%20training%20of%0Aaction-driven%20generative%20models.%20By%20integrating%20visual%20skeletons%20into%0Apretrained%20video%20generation%20models%20via%20lightweight%20fine-tuning%2C%20we%20enable%0Aprecise%20action%20control%20of%20complex%20interaction%20while%20preserving%20the%20learning%20of%0Across-domain%20dynamics.%20Experiments%20on%20EgoVid%2C%20RT-1%20and%20DROID%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20approach.%20Project%20page%3A%0Ahttps%3A//zju3dv.github.io/VAP/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13104v1&entry.124074799=Read"},
{"title": "CaRL: Learning Scalable Planning Policies with Simple Rewards", "author": "Bernhard Jaeger and Daniel Dauner and Jens Bei\u00dfwenger and Simon Gerstenecker and Kashyap Chitta and Andreas Geiger", "abstract": "  We investigate reinforcement learning (RL) for privileged planning in\nautonomous driving. State-of-the-art approaches for this task are rule-based,\nbut these methods do not scale to the long tail. RL, on the other hand, is\nscalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum\nmultiple individual rewards, \\eg~progress, position, or orientation rewards. We\nshow that PPO fails to optimize a popular version of these rewards when the\nmini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single\nintuitive reward term: route completion. Infractions are penalized by\nterminating the episode or multiplicatively reducing route completion. We find\nthat PPO scales well with higher mini-batch sizes when trained with our simple\nreward, even improving performance. Training with large mini-batch sizes\nenables efficient scaling via distributed data parallelism. We scale PPO to\n300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The\nresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,\noutperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is\nthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and\n90.6 in reactive traffic on the Val14 benchmark while being an order of\nmagnitude faster than prior work.\n", "link": "http://arxiv.org/abs/2504.17838v2", "date": "2025-08-18", "relevancy": 2.4343, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4994}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4809}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaRL%3A%20Learning%20Scalable%20Planning%20Policies%20with%20Simple%20Rewards&body=Title%3A%20CaRL%3A%20Learning%20Scalable%20Planning%20Policies%20with%20Simple%20Rewards%0AAuthor%3A%20Bernhard%20Jaeger%20and%20Daniel%20Dauner%20and%20Jens%20Bei%C3%9Fwenger%20and%20Simon%20Gerstenecker%20and%20Kashyap%20Chitta%20and%20Andreas%20Geiger%0AAbstract%3A%20%20%20We%20investigate%20reinforcement%20learning%20%28RL%29%20for%20privileged%20planning%20in%0Aautonomous%20driving.%20State-of-the-art%20approaches%20for%20this%20task%20are%20rule-based%2C%0Abut%20these%20methods%20do%20not%20scale%20to%20the%20long%20tail.%20RL%2C%20on%20the%20other%20hand%2C%20is%0Ascalable%20and%20does%20not%20suffer%20from%20compounding%20errors%20like%20imitation%20learning.%0AContemporary%20RL%20approaches%20for%20driving%20use%20complex%20shaped%20rewards%20that%20sum%0Amultiple%20individual%20rewards%2C%20%5Ceg~progress%2C%20position%2C%20or%20orientation%20rewards.%20We%0Ashow%20that%20PPO%20fails%20to%20optimize%20a%20popular%20version%20of%20these%20rewards%20when%20the%0Amini-batch%20size%20is%20increased%2C%20which%20limits%20the%20scalability%20of%20these%20approaches.%0AInstead%2C%20we%20propose%20a%20new%20reward%20design%20based%20primarily%20on%20optimizing%20a%20single%0Aintuitive%20reward%20term%3A%20route%20completion.%20Infractions%20are%20penalized%20by%0Aterminating%20the%20episode%20or%20multiplicatively%20reducing%20route%20completion.%20We%20find%0Athat%20PPO%20scales%20well%20with%20higher%20mini-batch%20sizes%20when%20trained%20with%20our%20simple%0Areward%2C%20even%20improving%20performance.%20Training%20with%20large%20mini-batch%20sizes%0Aenables%20efficient%20scaling%20via%20distributed%20data%20parallelism.%20We%20scale%20PPO%20to%0A300M%20samples%20in%20CARLA%20and%20500M%20samples%20in%20nuPlan%20with%20a%20single%208-GPU%20node.%20The%0Aresulting%20model%20achieves%2064%20DS%20on%20the%20CARLA%20longest6%20v2%20benchmark%2C%0Aoutperforming%20other%20RL%20methods%20with%20more%20complex%20rewards%20by%20a%20large%20margin.%0ARequiring%20only%20minimal%20adaptations%20from%20its%20use%20in%20CARLA%2C%20the%20same%20method%20is%0Athe%20best%20learning-based%20approach%20on%20nuPlan.%20It%20scores%2091.3%20in%20non-reactive%20and%0A90.6%20in%20reactive%20traffic%20on%20the%20Val14%20benchmark%20while%20being%20an%20order%20of%0Amagnitude%20faster%20than%20prior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaRL%253A%2520Learning%2520Scalable%2520Planning%2520Policies%2520with%2520Simple%2520Rewards%26entry.906535625%3DBernhard%2520Jaeger%2520and%2520Daniel%2520Dauner%2520and%2520Jens%2520Bei%25C3%259Fwenger%2520and%2520Simon%2520Gerstenecker%2520and%2520Kashyap%2520Chitta%2520and%2520Andreas%2520Geiger%26entry.1292438233%3D%2520%2520We%2520investigate%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520privileged%2520planning%2520in%250Aautonomous%2520driving.%2520State-of-the-art%2520approaches%2520for%2520this%2520task%2520are%2520rule-based%252C%250Abut%2520these%2520methods%2520do%2520not%2520scale%2520to%2520the%2520long%2520tail.%2520RL%252C%2520on%2520the%2520other%2520hand%252C%2520is%250Ascalable%2520and%2520does%2520not%2520suffer%2520from%2520compounding%2520errors%2520like%2520imitation%2520learning.%250AContemporary%2520RL%2520approaches%2520for%2520driving%2520use%2520complex%2520shaped%2520rewards%2520that%2520sum%250Amultiple%2520individual%2520rewards%252C%2520%255Ceg~progress%252C%2520position%252C%2520or%2520orientation%2520rewards.%2520We%250Ashow%2520that%2520PPO%2520fails%2520to%2520optimize%2520a%2520popular%2520version%2520of%2520these%2520rewards%2520when%2520the%250Amini-batch%2520size%2520is%2520increased%252C%2520which%2520limits%2520the%2520scalability%2520of%2520these%2520approaches.%250AInstead%252C%2520we%2520propose%2520a%2520new%2520reward%2520design%2520based%2520primarily%2520on%2520optimizing%2520a%2520single%250Aintuitive%2520reward%2520term%253A%2520route%2520completion.%2520Infractions%2520are%2520penalized%2520by%250Aterminating%2520the%2520episode%2520or%2520multiplicatively%2520reducing%2520route%2520completion.%2520We%2520find%250Athat%2520PPO%2520scales%2520well%2520with%2520higher%2520mini-batch%2520sizes%2520when%2520trained%2520with%2520our%2520simple%250Areward%252C%2520even%2520improving%2520performance.%2520Training%2520with%2520large%2520mini-batch%2520sizes%250Aenables%2520efficient%2520scaling%2520via%2520distributed%2520data%2520parallelism.%2520We%2520scale%2520PPO%2520to%250A300M%2520samples%2520in%2520CARLA%2520and%2520500M%2520samples%2520in%2520nuPlan%2520with%2520a%2520single%25208-GPU%2520node.%2520The%250Aresulting%2520model%2520achieves%252064%2520DS%2520on%2520the%2520CARLA%2520longest6%2520v2%2520benchmark%252C%250Aoutperforming%2520other%2520RL%2520methods%2520with%2520more%2520complex%2520rewards%2520by%2520a%2520large%2520margin.%250ARequiring%2520only%2520minimal%2520adaptations%2520from%2520its%2520use%2520in%2520CARLA%252C%2520the%2520same%2520method%2520is%250Athe%2520best%2520learning-based%2520approach%2520on%2520nuPlan.%2520It%2520scores%252091.3%2520in%2520non-reactive%2520and%250A90.6%2520in%2520reactive%2520traffic%2520on%2520the%2520Val14%2520benchmark%2520while%2520being%2520an%2520order%2520of%250Amagnitude%2520faster%2520than%2520prior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaRL%3A%20Learning%20Scalable%20Planning%20Policies%20with%20Simple%20Rewards&entry.906535625=Bernhard%20Jaeger%20and%20Daniel%20Dauner%20and%20Jens%20Bei%C3%9Fwenger%20and%20Simon%20Gerstenecker%20and%20Kashyap%20Chitta%20and%20Andreas%20Geiger&entry.1292438233=%20%20We%20investigate%20reinforcement%20learning%20%28RL%29%20for%20privileged%20planning%20in%0Aautonomous%20driving.%20State-of-the-art%20approaches%20for%20this%20task%20are%20rule-based%2C%0Abut%20these%20methods%20do%20not%20scale%20to%20the%20long%20tail.%20RL%2C%20on%20the%20other%20hand%2C%20is%0Ascalable%20and%20does%20not%20suffer%20from%20compounding%20errors%20like%20imitation%20learning.%0AContemporary%20RL%20approaches%20for%20driving%20use%20complex%20shaped%20rewards%20that%20sum%0Amultiple%20individual%20rewards%2C%20%5Ceg~progress%2C%20position%2C%20or%20orientation%20rewards.%20We%0Ashow%20that%20PPO%20fails%20to%20optimize%20a%20popular%20version%20of%20these%20rewards%20when%20the%0Amini-batch%20size%20is%20increased%2C%20which%20limits%20the%20scalability%20of%20these%20approaches.%0AInstead%2C%20we%20propose%20a%20new%20reward%20design%20based%20primarily%20on%20optimizing%20a%20single%0Aintuitive%20reward%20term%3A%20route%20completion.%20Infractions%20are%20penalized%20by%0Aterminating%20the%20episode%20or%20multiplicatively%20reducing%20route%20completion.%20We%20find%0Athat%20PPO%20scales%20well%20with%20higher%20mini-batch%20sizes%20when%20trained%20with%20our%20simple%0Areward%2C%20even%20improving%20performance.%20Training%20with%20large%20mini-batch%20sizes%0Aenables%20efficient%20scaling%20via%20distributed%20data%20parallelism.%20We%20scale%20PPO%20to%0A300M%20samples%20in%20CARLA%20and%20500M%20samples%20in%20nuPlan%20with%20a%20single%208-GPU%20node.%20The%0Aresulting%20model%20achieves%2064%20DS%20on%20the%20CARLA%20longest6%20v2%20benchmark%2C%0Aoutperforming%20other%20RL%20methods%20with%20more%20complex%20rewards%20by%20a%20large%20margin.%0ARequiring%20only%20minimal%20adaptations%20from%20its%20use%20in%20CARLA%2C%20the%20same%20method%20is%0Athe%20best%20learning-based%20approach%20on%20nuPlan.%20It%20scores%2091.3%20in%20non-reactive%20and%0A90.6%20in%20reactive%20traffic%20on%20the%20Val14%20benchmark%20while%20being%20an%20order%20of%0Amagnitude%20faster%20than%20prior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17838v2&entry.124074799=Read"},
{"title": "Spot the BlindSpots: Systematic Identification and Quantification of\n  Fine-Grained LLM Biases in Contact Center Summaries", "author": "Kawin Mayilvaghanan and Siddhant Gupta and Ayush Kumar", "abstract": "  Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.\n", "link": "http://arxiv.org/abs/2508.13124v1", "date": "2025-08-18", "relevancy": 2.4276, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spot%20the%20BlindSpots%3A%20Systematic%20Identification%20and%20Quantification%20of%0A%20%20Fine-Grained%20LLM%20Biases%20in%20Contact%20Center%20Summaries&body=Title%3A%20Spot%20the%20BlindSpots%3A%20Systematic%20Identification%20and%20Quantification%20of%0A%20%20Fine-Grained%20LLM%20Biases%20in%20Contact%20Center%20Summaries%0AAuthor%3A%20Kawin%20Mayilvaghanan%20and%20Siddhant%20Gupta%20and%20Ayush%20Kumar%0AAbstract%3A%20%20%20Abstractive%20summarization%20is%20a%20core%20application%20in%20contact%20centers%2C%20where%0ALarge%20Language%20Models%20%28LLMs%29%20generate%20millions%20of%20summaries%20of%20call%20transcripts%0Adaily.%20Despite%20their%20apparent%20quality%2C%20it%20remains%20unclear%20whether%20LLMs%0Asystematically%20under-%20or%20over-attend%20to%20specific%20aspects%20of%20the%20transcript%2C%0Apotentially%20introducing%20biases%20in%20the%20generated%20summary.%20While%20prior%20work%20has%0Aexamined%20social%20and%20positional%20biases%2C%20the%20specific%20forms%20of%20bias%20pertinent%20to%0Acontact%20center%20operations%20-%20which%20we%20term%20Operational%20Bias%20-%20have%20remained%0Aunexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20BlindSpot%2C%20a%20framework%20built%20upon%0Aa%20taxonomy%20of%2015%20operational%20bias%20dimensions%20%28e.g.%2C%20disfluency%2C%20speaker%2C%20topic%29%0Afor%20the%20identification%20and%20quantification%20of%20these%20biases.%20BlindSpot%20leverages%0Aan%20LLM%20as%20a%20zero-shot%20classifier%20to%20derive%20categorical%20distributions%20for%20each%0Abias%20dimension%20in%20a%20pair%20of%20transcript%20and%20its%20summary.%20The%20bias%20is%20then%0Aquantified%20using%20two%20metrics%3A%20Fidelity%20Gap%20%28the%20JS%20Divergence%20between%0Adistributions%29%20and%20Coverage%20%28the%20percentage%20of%20source%20labels%20omitted%29.%20Using%0ABlindSpot%2C%20we%20conducted%20an%20empirical%20study%20with%202500%20real%20call%20transcripts%20and%0Atheir%20summaries%20generated%20by%2020%20LLMs%20of%20varying%20scales%20and%20families%20%28e.g.%2C%20GPT%2C%0ALlama%2C%20Claude%29.%20Our%20analysis%20reveals%20that%20biases%20are%20systemic%20and%20present%0Aacross%20all%20evaluated%20models%2C%20regardless%20of%20size%20or%20family.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpot%2520the%2520BlindSpots%253A%2520Systematic%2520Identification%2520and%2520Quantification%2520of%250A%2520%2520Fine-Grained%2520LLM%2520Biases%2520in%2520Contact%2520Center%2520Summaries%26entry.906535625%3DKawin%2520Mayilvaghanan%2520and%2520Siddhant%2520Gupta%2520and%2520Ayush%2520Kumar%26entry.1292438233%3D%2520%2520Abstractive%2520summarization%2520is%2520a%2520core%2520application%2520in%2520contact%2520centers%252C%2520where%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520generate%2520millions%2520of%2520summaries%2520of%2520call%2520transcripts%250Adaily.%2520Despite%2520their%2520apparent%2520quality%252C%2520it%2520remains%2520unclear%2520whether%2520LLMs%250Asystematically%2520under-%2520or%2520over-attend%2520to%2520specific%2520aspects%2520of%2520the%2520transcript%252C%250Apotentially%2520introducing%2520biases%2520in%2520the%2520generated%2520summary.%2520While%2520prior%2520work%2520has%250Aexamined%2520social%2520and%2520positional%2520biases%252C%2520the%2520specific%2520forms%2520of%2520bias%2520pertinent%2520to%250Acontact%2520center%2520operations%2520-%2520which%2520we%2520term%2520Operational%2520Bias%2520-%2520have%2520remained%250Aunexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520BlindSpot%252C%2520a%2520framework%2520built%2520upon%250Aa%2520taxonomy%2520of%252015%2520operational%2520bias%2520dimensions%2520%2528e.g.%252C%2520disfluency%252C%2520speaker%252C%2520topic%2529%250Afor%2520the%2520identification%2520and%2520quantification%2520of%2520these%2520biases.%2520BlindSpot%2520leverages%250Aan%2520LLM%2520as%2520a%2520zero-shot%2520classifier%2520to%2520derive%2520categorical%2520distributions%2520for%2520each%250Abias%2520dimension%2520in%2520a%2520pair%2520of%2520transcript%2520and%2520its%2520summary.%2520The%2520bias%2520is%2520then%250Aquantified%2520using%2520two%2520metrics%253A%2520Fidelity%2520Gap%2520%2528the%2520JS%2520Divergence%2520between%250Adistributions%2529%2520and%2520Coverage%2520%2528the%2520percentage%2520of%2520source%2520labels%2520omitted%2529.%2520Using%250ABlindSpot%252C%2520we%2520conducted%2520an%2520empirical%2520study%2520with%25202500%2520real%2520call%2520transcripts%2520and%250Atheir%2520summaries%2520generated%2520by%252020%2520LLMs%2520of%2520varying%2520scales%2520and%2520families%2520%2528e.g.%252C%2520GPT%252C%250ALlama%252C%2520Claude%2529.%2520Our%2520analysis%2520reveals%2520that%2520biases%2520are%2520systemic%2520and%2520present%250Aacross%2520all%2520evaluated%2520models%252C%2520regardless%2520of%2520size%2520or%2520family.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spot%20the%20BlindSpots%3A%20Systematic%20Identification%20and%20Quantification%20of%0A%20%20Fine-Grained%20LLM%20Biases%20in%20Contact%20Center%20Summaries&entry.906535625=Kawin%20Mayilvaghanan%20and%20Siddhant%20Gupta%20and%20Ayush%20Kumar&entry.1292438233=%20%20Abstractive%20summarization%20is%20a%20core%20application%20in%20contact%20centers%2C%20where%0ALarge%20Language%20Models%20%28LLMs%29%20generate%20millions%20of%20summaries%20of%20call%20transcripts%0Adaily.%20Despite%20their%20apparent%20quality%2C%20it%20remains%20unclear%20whether%20LLMs%0Asystematically%20under-%20or%20over-attend%20to%20specific%20aspects%20of%20the%20transcript%2C%0Apotentially%20introducing%20biases%20in%20the%20generated%20summary.%20While%20prior%20work%20has%0Aexamined%20social%20and%20positional%20biases%2C%20the%20specific%20forms%20of%20bias%20pertinent%20to%0Acontact%20center%20operations%20-%20which%20we%20term%20Operational%20Bias%20-%20have%20remained%0Aunexplored.%20To%20address%20this%20gap%2C%20we%20introduce%20BlindSpot%2C%20a%20framework%20built%20upon%0Aa%20taxonomy%20of%2015%20operational%20bias%20dimensions%20%28e.g.%2C%20disfluency%2C%20speaker%2C%20topic%29%0Afor%20the%20identification%20and%20quantification%20of%20these%20biases.%20BlindSpot%20leverages%0Aan%20LLM%20as%20a%20zero-shot%20classifier%20to%20derive%20categorical%20distributions%20for%20each%0Abias%20dimension%20in%20a%20pair%20of%20transcript%20and%20its%20summary.%20The%20bias%20is%20then%0Aquantified%20using%20two%20metrics%3A%20Fidelity%20Gap%20%28the%20JS%20Divergence%20between%0Adistributions%29%20and%20Coverage%20%28the%20percentage%20of%20source%20labels%20omitted%29.%20Using%0ABlindSpot%2C%20we%20conducted%20an%20empirical%20study%20with%202500%20real%20call%20transcripts%20and%0Atheir%20summaries%20generated%20by%2020%20LLMs%20of%20varying%20scales%20and%20families%20%28e.g.%2C%20GPT%2C%0ALlama%2C%20Claude%29.%20Our%20analysis%20reveals%20that%20biases%20are%20systemic%20and%20present%0Aacross%20all%20evaluated%20models%2C%20regardless%20of%20size%20or%20family.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13124v1&entry.124074799=Read"},
{"title": "EgoTwin: Dreaming Body and View in First Person", "author": "Jingqiao Xiu and Fangzhou Hong and Yicong Li and Mengze Li and Wentao Wang and Sirui Han and Liang Pan and Ziwei Liu", "abstract": "  While exocentric video synthesis has achieved great progress, egocentric\nvideo generation remains largely underexplored, which requires modeling\nfirst-person view content along with camera motion patterns induced by the\nwearer's body movements. To bridge this gap, we introduce a novel task of joint\negocentric video and human motion generation, characterized by two key\nchallenges: 1) Viewpoint Alignment: the camera trajectory in the generated\nvideo must accurately align with the head trajectory derived from human motion;\n2) Causal Interplay: the synthesized human motion must causally align with the\nobserved visual dynamics across adjacent video frames. To address these\nchallenges, we propose EgoTwin, a joint video-motion generation framework built\non the diffusion transformer architecture. Specifically, EgoTwin introduces a\nhead-centric motion representation that anchors the human motion to the head\njoint and incorporates a cybernetics-inspired interaction mechanism that\nexplicitly captures the causal interplay between video and motion within\nattention operations. For comprehensive evaluation, we curate a large-scale\nreal-world dataset of synchronized text-video-motion triplets and design novel\nmetrics to assess video-motion consistency. Extensive experiments demonstrate\nthe effectiveness of the EgoTwin framework.\n", "link": "http://arxiv.org/abs/2508.13013v1", "date": "2025-08-18", "relevancy": 2.4249, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6144}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6104}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoTwin%3A%20Dreaming%20Body%20and%20View%20in%20First%20Person&body=Title%3A%20EgoTwin%3A%20Dreaming%20Body%20and%20View%20in%20First%20Person%0AAuthor%3A%20Jingqiao%20Xiu%20and%20Fangzhou%20Hong%20and%20Yicong%20Li%20and%20Mengze%20Li%20and%20Wentao%20Wang%20and%20Sirui%20Han%20and%20Liang%20Pan%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20While%20exocentric%20video%20synthesis%20has%20achieved%20great%20progress%2C%20egocentric%0Avideo%20generation%20remains%20largely%20underexplored%2C%20which%20requires%20modeling%0Afirst-person%20view%20content%20along%20with%20camera%20motion%20patterns%20induced%20by%20the%0Awearer%27s%20body%20movements.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20novel%20task%20of%20joint%0Aegocentric%20video%20and%20human%20motion%20generation%2C%20characterized%20by%20two%20key%0Achallenges%3A%201%29%20Viewpoint%20Alignment%3A%20the%20camera%20trajectory%20in%20the%20generated%0Avideo%20must%20accurately%20align%20with%20the%20head%20trajectory%20derived%20from%20human%20motion%3B%0A2%29%20Causal%20Interplay%3A%20the%20synthesized%20human%20motion%20must%20causally%20align%20with%20the%0Aobserved%20visual%20dynamics%20across%20adjacent%20video%20frames.%20To%20address%20these%0Achallenges%2C%20we%20propose%20EgoTwin%2C%20a%20joint%20video-motion%20generation%20framework%20built%0Aon%20the%20diffusion%20transformer%20architecture.%20Specifically%2C%20EgoTwin%20introduces%20a%0Ahead-centric%20motion%20representation%20that%20anchors%20the%20human%20motion%20to%20the%20head%0Ajoint%20and%20incorporates%20a%20cybernetics-inspired%20interaction%20mechanism%20that%0Aexplicitly%20captures%20the%20causal%20interplay%20between%20video%20and%20motion%20within%0Aattention%20operations.%20For%20comprehensive%20evaluation%2C%20we%20curate%20a%20large-scale%0Areal-world%20dataset%20of%20synchronized%20text-video-motion%20triplets%20and%20design%20novel%0Ametrics%20to%20assess%20video-motion%20consistency.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20of%20the%20EgoTwin%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoTwin%253A%2520Dreaming%2520Body%2520and%2520View%2520in%2520First%2520Person%26entry.906535625%3DJingqiao%2520Xiu%2520and%2520Fangzhou%2520Hong%2520and%2520Yicong%2520Li%2520and%2520Mengze%2520Li%2520and%2520Wentao%2520Wang%2520and%2520Sirui%2520Han%2520and%2520Liang%2520Pan%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520While%2520exocentric%2520video%2520synthesis%2520has%2520achieved%2520great%2520progress%252C%2520egocentric%250Avideo%2520generation%2520remains%2520largely%2520underexplored%252C%2520which%2520requires%2520modeling%250Afirst-person%2520view%2520content%2520along%2520with%2520camera%2520motion%2520patterns%2520induced%2520by%2520the%250Awearer%2527s%2520body%2520movements.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520task%2520of%2520joint%250Aegocentric%2520video%2520and%2520human%2520motion%2520generation%252C%2520characterized%2520by%2520two%2520key%250Achallenges%253A%25201%2529%2520Viewpoint%2520Alignment%253A%2520the%2520camera%2520trajectory%2520in%2520the%2520generated%250Avideo%2520must%2520accurately%2520align%2520with%2520the%2520head%2520trajectory%2520derived%2520from%2520human%2520motion%253B%250A2%2529%2520Causal%2520Interplay%253A%2520the%2520synthesized%2520human%2520motion%2520must%2520causally%2520align%2520with%2520the%250Aobserved%2520visual%2520dynamics%2520across%2520adjacent%2520video%2520frames.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520EgoTwin%252C%2520a%2520joint%2520video-motion%2520generation%2520framework%2520built%250Aon%2520the%2520diffusion%2520transformer%2520architecture.%2520Specifically%252C%2520EgoTwin%2520introduces%2520a%250Ahead-centric%2520motion%2520representation%2520that%2520anchors%2520the%2520human%2520motion%2520to%2520the%2520head%250Ajoint%2520and%2520incorporates%2520a%2520cybernetics-inspired%2520interaction%2520mechanism%2520that%250Aexplicitly%2520captures%2520the%2520causal%2520interplay%2520between%2520video%2520and%2520motion%2520within%250Aattention%2520operations.%2520For%2520comprehensive%2520evaluation%252C%2520we%2520curate%2520a%2520large-scale%250Areal-world%2520dataset%2520of%2520synchronized%2520text-video-motion%2520triplets%2520and%2520design%2520novel%250Ametrics%2520to%2520assess%2520video-motion%2520consistency.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520effectiveness%2520of%2520the%2520EgoTwin%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoTwin%3A%20Dreaming%20Body%20and%20View%20in%20First%20Person&entry.906535625=Jingqiao%20Xiu%20and%20Fangzhou%20Hong%20and%20Yicong%20Li%20and%20Mengze%20Li%20and%20Wentao%20Wang%20and%20Sirui%20Han%20and%20Liang%20Pan%20and%20Ziwei%20Liu&entry.1292438233=%20%20While%20exocentric%20video%20synthesis%20has%20achieved%20great%20progress%2C%20egocentric%0Avideo%20generation%20remains%20largely%20underexplored%2C%20which%20requires%20modeling%0Afirst-person%20view%20content%20along%20with%20camera%20motion%20patterns%20induced%20by%20the%0Awearer%27s%20body%20movements.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20novel%20task%20of%20joint%0Aegocentric%20video%20and%20human%20motion%20generation%2C%20characterized%20by%20two%20key%0Achallenges%3A%201%29%20Viewpoint%20Alignment%3A%20the%20camera%20trajectory%20in%20the%20generated%0Avideo%20must%20accurately%20align%20with%20the%20head%20trajectory%20derived%20from%20human%20motion%3B%0A2%29%20Causal%20Interplay%3A%20the%20synthesized%20human%20motion%20must%20causally%20align%20with%20the%0Aobserved%20visual%20dynamics%20across%20adjacent%20video%20frames.%20To%20address%20these%0Achallenges%2C%20we%20propose%20EgoTwin%2C%20a%20joint%20video-motion%20generation%20framework%20built%0Aon%20the%20diffusion%20transformer%20architecture.%20Specifically%2C%20EgoTwin%20introduces%20a%0Ahead-centric%20motion%20representation%20that%20anchors%20the%20human%20motion%20to%20the%20head%0Ajoint%20and%20incorporates%20a%20cybernetics-inspired%20interaction%20mechanism%20that%0Aexplicitly%20captures%20the%20causal%20interplay%20between%20video%20and%20motion%20within%0Aattention%20operations.%20For%20comprehensive%20evaluation%2C%20we%20curate%20a%20large-scale%0Areal-world%20dataset%20of%20synchronized%20text-video-motion%20triplets%20and%20design%20novel%0Ametrics%20to%20assess%20video-motion%20consistency.%20Extensive%20experiments%20demonstrate%0Athe%20effectiveness%20of%20the%20EgoTwin%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13013v1&entry.124074799=Read"},
{"title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language\n  Models", "author": "Jinyi Han and Xinyi Wang and Haiquan Zhao and Tingyun li and Zishang Jiang and Sihang Jiang and Jiaqing Liang and Xin Lin and Weikang Zhou and Zeye Sun and Fei Yu and Yanghua Xiao", "abstract": "  Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.\n", "link": "http://arxiv.org/abs/2508.12903v1", "date": "2025-08-18", "relevancy": 2.4029, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Stitch%20in%20Time%20Saves%20Nine%3A%20Proactive%20Self-Refinement%20for%20Language%0A%20%20Models&body=Title%3A%20A%20Stitch%20in%20Time%20Saves%20Nine%3A%20Proactive%20Self-Refinement%20for%20Language%0A%20%20Models%0AAuthor%3A%20Jinyi%20Han%20and%20Xinyi%20Wang%20and%20Haiquan%20Zhao%20and%20Tingyun%20li%20and%20Zishang%20Jiang%20and%20Sihang%20Jiang%20and%20Jiaqing%20Liang%20and%20Xin%20Lin%20and%20Weikang%20Zhou%20and%20Zeye%20Sun%20and%20Fei%20Yu%20and%20Yanghua%20Xiao%0AAbstract%3A%20%20%20Recent%20advances%20in%20self-refinement%20have%20demonstrated%20significant%20potential%0Afor%20improving%20the%20outputs%20of%20large%20language%20models%20%28LLMs%29%20through%20iterative%0Arefinement.%20However%2C%20most%20existing%20self-refinement%20methods%20rely%20on%20a%20reactive%0Aprocess%20with%20a%20fixed%20number%20of%20iterations%2C%20making%20it%20difficult%20to%20determine%20the%0Aoptimal%20timing%20and%20content%20of%20refinement%20based%20on%20the%20evolving%20generation%0Acontext.%20Inspired%20by%20the%20way%20humans%20dynamically%20refine%20their%20thoughts%20during%0Aexecution%2C%20we%20propose%20ProActive%20Self-Refinement%20%28PASR%29%2C%20a%20novel%20method%20that%0Aenables%20LLMs%20to%20refine%20their%20outputs%20during%20the%20generation%20process.%20Unlike%0Amethods%20that%20regenerate%20entire%20responses%2C%20PASR%20proactively%20decides%20whether%2C%0Awhen%2C%20and%20how%20to%20refine%20based%20on%20the%20model%27s%20internal%20state%20and%20evolving%0Acontext.%20We%20conduct%20extensive%20experiments%20on%20a%20diverse%20set%20of%2010%20tasks%20to%0Aevaluate%20the%20effectiveness%20of%20PASR.%20Experimental%20results%20show%20that%20PASR%0Asignificantly%20enhances%20problem-solving%20performance.%20In%20particular%2C%20on%20Qwen3-8B%2C%0APASR%20reduces%20average%20token%20consumption%20by%2041.6%20percent%20compared%20to%20standard%0Ageneration%2C%20while%20also%20achieving%20an%208.2%20percent%20improvement%20in%20accuracy.%20Our%0Acode%20and%20all%20baselines%20used%20in%20the%20paper%20are%20available%20in%20the%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Stitch%2520in%2520Time%2520Saves%2520Nine%253A%2520Proactive%2520Self-Refinement%2520for%2520Language%250A%2520%2520Models%26entry.906535625%3DJinyi%2520Han%2520and%2520Xinyi%2520Wang%2520and%2520Haiquan%2520Zhao%2520and%2520Tingyun%2520li%2520and%2520Zishang%2520Jiang%2520and%2520Sihang%2520Jiang%2520and%2520Jiaqing%2520Liang%2520and%2520Xin%2520Lin%2520and%2520Weikang%2520Zhou%2520and%2520Zeye%2520Sun%2520and%2520Fei%2520Yu%2520and%2520Yanghua%2520Xiao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520self-refinement%2520have%2520demonstrated%2520significant%2520potential%250Afor%2520improving%2520the%2520outputs%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520through%2520iterative%250Arefinement.%2520However%252C%2520most%2520existing%2520self-refinement%2520methods%2520rely%2520on%2520a%2520reactive%250Aprocess%2520with%2520a%2520fixed%2520number%2520of%2520iterations%252C%2520making%2520it%2520difficult%2520to%2520determine%2520the%250Aoptimal%2520timing%2520and%2520content%2520of%2520refinement%2520based%2520on%2520the%2520evolving%2520generation%250Acontext.%2520Inspired%2520by%2520the%2520way%2520humans%2520dynamically%2520refine%2520their%2520thoughts%2520during%250Aexecution%252C%2520we%2520propose%2520ProActive%2520Self-Refinement%2520%2528PASR%2529%252C%2520a%2520novel%2520method%2520that%250Aenables%2520LLMs%2520to%2520refine%2520their%2520outputs%2520during%2520the%2520generation%2520process.%2520Unlike%250Amethods%2520that%2520regenerate%2520entire%2520responses%252C%2520PASR%2520proactively%2520decides%2520whether%252C%250Awhen%252C%2520and%2520how%2520to%2520refine%2520based%2520on%2520the%2520model%2527s%2520internal%2520state%2520and%2520evolving%250Acontext.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520a%2520diverse%2520set%2520of%252010%2520tasks%2520to%250Aevaluate%2520the%2520effectiveness%2520of%2520PASR.%2520Experimental%2520results%2520show%2520that%2520PASR%250Asignificantly%2520enhances%2520problem-solving%2520performance.%2520In%2520particular%252C%2520on%2520Qwen3-8B%252C%250APASR%2520reduces%2520average%2520token%2520consumption%2520by%252041.6%2520percent%2520compared%2520to%2520standard%250Ageneration%252C%2520while%2520also%2520achieving%2520an%25208.2%2520percent%2520improvement%2520in%2520accuracy.%2520Our%250Acode%2520and%2520all%2520baselines%2520used%2520in%2520the%2520paper%2520are%2520available%2520in%2520the%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Stitch%20in%20Time%20Saves%20Nine%3A%20Proactive%20Self-Refinement%20for%20Language%0A%20%20Models&entry.906535625=Jinyi%20Han%20and%20Xinyi%20Wang%20and%20Haiquan%20Zhao%20and%20Tingyun%20li%20and%20Zishang%20Jiang%20and%20Sihang%20Jiang%20and%20Jiaqing%20Liang%20and%20Xin%20Lin%20and%20Weikang%20Zhou%20and%20Zeye%20Sun%20and%20Fei%20Yu%20and%20Yanghua%20Xiao&entry.1292438233=%20%20Recent%20advances%20in%20self-refinement%20have%20demonstrated%20significant%20potential%0Afor%20improving%20the%20outputs%20of%20large%20language%20models%20%28LLMs%29%20through%20iterative%0Arefinement.%20However%2C%20most%20existing%20self-refinement%20methods%20rely%20on%20a%20reactive%0Aprocess%20with%20a%20fixed%20number%20of%20iterations%2C%20making%20it%20difficult%20to%20determine%20the%0Aoptimal%20timing%20and%20content%20of%20refinement%20based%20on%20the%20evolving%20generation%0Acontext.%20Inspired%20by%20the%20way%20humans%20dynamically%20refine%20their%20thoughts%20during%0Aexecution%2C%20we%20propose%20ProActive%20Self-Refinement%20%28PASR%29%2C%20a%20novel%20method%20that%0Aenables%20LLMs%20to%20refine%20their%20outputs%20during%20the%20generation%20process.%20Unlike%0Amethods%20that%20regenerate%20entire%20responses%2C%20PASR%20proactively%20decides%20whether%2C%0Awhen%2C%20and%20how%20to%20refine%20based%20on%20the%20model%27s%20internal%20state%20and%20evolving%0Acontext.%20We%20conduct%20extensive%20experiments%20on%20a%20diverse%20set%20of%2010%20tasks%20to%0Aevaluate%20the%20effectiveness%20of%20PASR.%20Experimental%20results%20show%20that%20PASR%0Asignificantly%20enhances%20problem-solving%20performance.%20In%20particular%2C%20on%20Qwen3-8B%2C%0APASR%20reduces%20average%20token%20consumption%20by%2041.6%20percent%20compared%20to%20standard%0Ageneration%2C%20while%20also%20achieving%20an%208.2%20percent%20improvement%20in%20accuracy.%20Our%0Acode%20and%20all%20baselines%20used%20in%20the%20paper%20are%20available%20in%20the%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12903v1&entry.124074799=Read"},
{"title": "SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution\n  Strategy", "author": "Boran Zhao and Haiming Zhai and Zihang Yuan and Hetian Liu and Tian Xia and Wenzhe Zhao and Pengju Ren", "abstract": "  The growing demand for sparse tensor algebra (SpTA) in machine learning and\nbig data has driven the development of various sparse tensor accelerators.\nHowever, most existing manually designed accelerators are limited to specific\nscenarios, and it's time-consuming and challenging to adjust a large number of\ndesign factors when scenarios change. Therefore, automating the design of SpTA\naccelerators is crucial. Nevertheless, previous works focus solely on either\nmapping (i.e., tiling communication and computation in space and time) or\nsparse strategy (i.e., bypassing zero elements for efficiency), leading to\nsuboptimal designs due to the lack of comprehensive consideration of both. A\nunified framework that jointly optimizes both is urgently needed. However,\nintegrating mapping and sparse strategies leads to a combinatorial explosion in\nthe design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \\times\n64} \\times Q_{64 \\times 48} = Z_{32 \\times 48}$). This vast search space\nrenders most conventional optimization methods (e.g., particle swarm\noptimization, reinforcement learning and Monte Carlo tree search) inefficient.\nTo address this challenge, we propose an evolution strategy-based sparse tensor\naccelerator optimization framework, called SparseMap. SparseMap constructing a\nmore comprehensive design space with the consideration of both mapping and\nsparse strategy. We introduce a series of enhancements to genetic encoding and\nevolutionary operators, enabling SparseMap to efficiently explore the vast and\ndiverse design space. We quantitatively compare SparseMap with prior works and\nclassical optimization methods, demonstrating that SparseMap consistently finds\nsuperior solutions.\n", "link": "http://arxiv.org/abs/2508.12906v1", "date": "2025-08-18", "relevancy": 2.3953, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4926}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseMap%3A%20A%20Sparse%20Tensor%20Accelerator%20Framework%20Based%20on%20Evolution%0A%20%20Strategy&body=Title%3A%20SparseMap%3A%20A%20Sparse%20Tensor%20Accelerator%20Framework%20Based%20on%20Evolution%0A%20%20Strategy%0AAuthor%3A%20Boran%20Zhao%20and%20Haiming%20Zhai%20and%20Zihang%20Yuan%20and%20Hetian%20Liu%20and%20Tian%20Xia%20and%20Wenzhe%20Zhao%20and%20Pengju%20Ren%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20sparse%20tensor%20algebra%20%28SpTA%29%20in%20machine%20learning%20and%0Abig%20data%20has%20driven%20the%20development%20of%20various%20sparse%20tensor%20accelerators.%0AHowever%2C%20most%20existing%20manually%20designed%20accelerators%20are%20limited%20to%20specific%0Ascenarios%2C%20and%20it%27s%20time-consuming%20and%20challenging%20to%20adjust%20a%20large%20number%20of%0Adesign%20factors%20when%20scenarios%20change.%20Therefore%2C%20automating%20the%20design%20of%20SpTA%0Aaccelerators%20is%20crucial.%20Nevertheless%2C%20previous%20works%20focus%20solely%20on%20either%0Amapping%20%28i.e.%2C%20tiling%20communication%20and%20computation%20in%20space%20and%20time%29%20or%0Asparse%20strategy%20%28i.e.%2C%20bypassing%20zero%20elements%20for%20efficiency%29%2C%20leading%20to%0Asuboptimal%20designs%20due%20to%20the%20lack%20of%20comprehensive%20consideration%20of%20both.%20A%0Aunified%20framework%20that%20jointly%20optimizes%20both%20is%20urgently%20needed.%20However%2C%0Aintegrating%20mapping%20and%20sparse%20strategies%20leads%20to%20a%20combinatorial%20explosion%20in%0Athe%20design%20space%28e.g.%2C%20as%20large%20as%20%24O%2810%5E%7B41%7D%29%24%20for%20the%20workload%20%24P_%7B32%20%5Ctimes%0A64%7D%20%5Ctimes%20Q_%7B64%20%5Ctimes%2048%7D%20%3D%20Z_%7B32%20%5Ctimes%2048%7D%24%29.%20This%20vast%20search%20space%0Arenders%20most%20conventional%20optimization%20methods%20%28e.g.%2C%20particle%20swarm%0Aoptimization%2C%20reinforcement%20learning%20and%20Monte%20Carlo%20tree%20search%29%20inefficient.%0ATo%20address%20this%20challenge%2C%20we%20propose%20an%20evolution%20strategy-based%20sparse%20tensor%0Aaccelerator%20optimization%20framework%2C%20called%20SparseMap.%20SparseMap%20constructing%20a%0Amore%20comprehensive%20design%20space%20with%20the%20consideration%20of%20both%20mapping%20and%0Asparse%20strategy.%20We%20introduce%20a%20series%20of%20enhancements%20to%20genetic%20encoding%20and%0Aevolutionary%20operators%2C%20enabling%20SparseMap%20to%20efficiently%20explore%20the%20vast%20and%0Adiverse%20design%20space.%20We%20quantitatively%20compare%20SparseMap%20with%20prior%20works%20and%0Aclassical%20optimization%20methods%2C%20demonstrating%20that%20SparseMap%20consistently%20finds%0Asuperior%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseMap%253A%2520A%2520Sparse%2520Tensor%2520Accelerator%2520Framework%2520Based%2520on%2520Evolution%250A%2520%2520Strategy%26entry.906535625%3DBoran%2520Zhao%2520and%2520Haiming%2520Zhai%2520and%2520Zihang%2520Yuan%2520and%2520Hetian%2520Liu%2520and%2520Tian%2520Xia%2520and%2520Wenzhe%2520Zhao%2520and%2520Pengju%2520Ren%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520sparse%2520tensor%2520algebra%2520%2528SpTA%2529%2520in%2520machine%2520learning%2520and%250Abig%2520data%2520has%2520driven%2520the%2520development%2520of%2520various%2520sparse%2520tensor%2520accelerators.%250AHowever%252C%2520most%2520existing%2520manually%2520designed%2520accelerators%2520are%2520limited%2520to%2520specific%250Ascenarios%252C%2520and%2520it%2527s%2520time-consuming%2520and%2520challenging%2520to%2520adjust%2520a%2520large%2520number%2520of%250Adesign%2520factors%2520when%2520scenarios%2520change.%2520Therefore%252C%2520automating%2520the%2520design%2520of%2520SpTA%250Aaccelerators%2520is%2520crucial.%2520Nevertheless%252C%2520previous%2520works%2520focus%2520solely%2520on%2520either%250Amapping%2520%2528i.e.%252C%2520tiling%2520communication%2520and%2520computation%2520in%2520space%2520and%2520time%2529%2520or%250Asparse%2520strategy%2520%2528i.e.%252C%2520bypassing%2520zero%2520elements%2520for%2520efficiency%2529%252C%2520leading%2520to%250Asuboptimal%2520designs%2520due%2520to%2520the%2520lack%2520of%2520comprehensive%2520consideration%2520of%2520both.%2520A%250Aunified%2520framework%2520that%2520jointly%2520optimizes%2520both%2520is%2520urgently%2520needed.%2520However%252C%250Aintegrating%2520mapping%2520and%2520sparse%2520strategies%2520leads%2520to%2520a%2520combinatorial%2520explosion%2520in%250Athe%2520design%2520space%2528e.g.%252C%2520as%2520large%2520as%2520%2524O%252810%255E%257B41%257D%2529%2524%2520for%2520the%2520workload%2520%2524P_%257B32%2520%255Ctimes%250A64%257D%2520%255Ctimes%2520Q_%257B64%2520%255Ctimes%252048%257D%2520%253D%2520Z_%257B32%2520%255Ctimes%252048%257D%2524%2529.%2520This%2520vast%2520search%2520space%250Arenders%2520most%2520conventional%2520optimization%2520methods%2520%2528e.g.%252C%2520particle%2520swarm%250Aoptimization%252C%2520reinforcement%2520learning%2520and%2520Monte%2520Carlo%2520tree%2520search%2529%2520inefficient.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520evolution%2520strategy-based%2520sparse%2520tensor%250Aaccelerator%2520optimization%2520framework%252C%2520called%2520SparseMap.%2520SparseMap%2520constructing%2520a%250Amore%2520comprehensive%2520design%2520space%2520with%2520the%2520consideration%2520of%2520both%2520mapping%2520and%250Asparse%2520strategy.%2520We%2520introduce%2520a%2520series%2520of%2520enhancements%2520to%2520genetic%2520encoding%2520and%250Aevolutionary%2520operators%252C%2520enabling%2520SparseMap%2520to%2520efficiently%2520explore%2520the%2520vast%2520and%250Adiverse%2520design%2520space.%2520We%2520quantitatively%2520compare%2520SparseMap%2520with%2520prior%2520works%2520and%250Aclassical%2520optimization%2520methods%252C%2520demonstrating%2520that%2520SparseMap%2520consistently%2520finds%250Asuperior%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseMap%3A%20A%20Sparse%20Tensor%20Accelerator%20Framework%20Based%20on%20Evolution%0A%20%20Strategy&entry.906535625=Boran%20Zhao%20and%20Haiming%20Zhai%20and%20Zihang%20Yuan%20and%20Hetian%20Liu%20and%20Tian%20Xia%20and%20Wenzhe%20Zhao%20and%20Pengju%20Ren&entry.1292438233=%20%20The%20growing%20demand%20for%20sparse%20tensor%20algebra%20%28SpTA%29%20in%20machine%20learning%20and%0Abig%20data%20has%20driven%20the%20development%20of%20various%20sparse%20tensor%20accelerators.%0AHowever%2C%20most%20existing%20manually%20designed%20accelerators%20are%20limited%20to%20specific%0Ascenarios%2C%20and%20it%27s%20time-consuming%20and%20challenging%20to%20adjust%20a%20large%20number%20of%0Adesign%20factors%20when%20scenarios%20change.%20Therefore%2C%20automating%20the%20design%20of%20SpTA%0Aaccelerators%20is%20crucial.%20Nevertheless%2C%20previous%20works%20focus%20solely%20on%20either%0Amapping%20%28i.e.%2C%20tiling%20communication%20and%20computation%20in%20space%20and%20time%29%20or%0Asparse%20strategy%20%28i.e.%2C%20bypassing%20zero%20elements%20for%20efficiency%29%2C%20leading%20to%0Asuboptimal%20designs%20due%20to%20the%20lack%20of%20comprehensive%20consideration%20of%20both.%20A%0Aunified%20framework%20that%20jointly%20optimizes%20both%20is%20urgently%20needed.%20However%2C%0Aintegrating%20mapping%20and%20sparse%20strategies%20leads%20to%20a%20combinatorial%20explosion%20in%0Athe%20design%20space%28e.g.%2C%20as%20large%20as%20%24O%2810%5E%7B41%7D%29%24%20for%20the%20workload%20%24P_%7B32%20%5Ctimes%0A64%7D%20%5Ctimes%20Q_%7B64%20%5Ctimes%2048%7D%20%3D%20Z_%7B32%20%5Ctimes%2048%7D%24%29.%20This%20vast%20search%20space%0Arenders%20most%20conventional%20optimization%20methods%20%28e.g.%2C%20particle%20swarm%0Aoptimization%2C%20reinforcement%20learning%20and%20Monte%20Carlo%20tree%20search%29%20inefficient.%0ATo%20address%20this%20challenge%2C%20we%20propose%20an%20evolution%20strategy-based%20sparse%20tensor%0Aaccelerator%20optimization%20framework%2C%20called%20SparseMap.%20SparseMap%20constructing%20a%0Amore%20comprehensive%20design%20space%20with%20the%20consideration%20of%20both%20mapping%20and%0Asparse%20strategy.%20We%20introduce%20a%20series%20of%20enhancements%20to%20genetic%20encoding%20and%0Aevolutionary%20operators%2C%20enabling%20SparseMap%20to%20efficiently%20explore%20the%20vast%20and%0Adiverse%20design%20space.%20We%20quantitatively%20compare%20SparseMap%20with%20prior%20works%20and%0Aclassical%20optimization%20methods%2C%20demonstrating%20that%20SparseMap%20consistently%20finds%0Asuperior%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12906v1&entry.124074799=Read"},
{"title": "LLMs Are In-Context Bandit Reinforcement Learners", "author": "Giovanni Monea and Antoine Bosselut and Kiant\u00e9 Brantley and Yoav Artzi", "abstract": "  Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors.\n", "link": "http://arxiv.org/abs/2410.05362v3", "date": "2025-08-18", "relevancy": 2.3745, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4787}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20Are%20In-Context%20Bandit%20Reinforcement%20Learners&body=Title%3A%20LLMs%20Are%20In-Context%20Bandit%20Reinforcement%20Learners%0AAuthor%3A%20Giovanni%20Monea%20and%20Antoine%20Bosselut%20and%20Kiant%C3%A9%20Brantley%20and%20Yoav%20Artzi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20in-context%20learning%20%28ICL%29%2C%20a%20supervised%0Alearning%20technique%20that%20relies%20on%20adding%20annotated%20examples%20to%20the%20model%0Acontext.%20We%20investigate%20a%20contextual%20bandit%20version%20of%20in-context%20reinforcement%0Alearning%20%28ICRL%29%2C%20where%20models%20learn%20in-context%2C%20online%2C%20from%20external%20reward%2C%0Ainstead%20of%20supervised%20data.%20We%20show%20that%20LLMs%20effectively%20demonstrate%20such%0Alearning%2C%20and%20provide%20a%20detailed%20study%20of%20the%20phenomena%2C%20experimenting%20with%0Achallenging%20classification%20tasks%20and%20models%20of%20sizes%20from%20500M%20to%2070B%0Aparameters.%20This%20includes%20identifying%20and%20addressing%20the%20instability%20of%20the%0Aprocess%2C%20demonstrating%20learning%20with%20both%20semantic%20and%20abstract%20labels%2C%20and%0Ashowing%20scaling%20trends.%20Our%20findings%20highlight%20ICRL%20capabilities%20in%20LLMs%2C%20while%0Aalso%20underscoring%20fundamental%20limitations%20in%20their%20implicit%20reasoning%20about%0Aerrors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05362v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520Are%2520In-Context%2520Bandit%2520Reinforcement%2520Learners%26entry.906535625%3DGiovanni%2520Monea%2520and%2520Antoine%2520Bosselut%2520and%2520Kiant%25C3%25A9%2520Brantley%2520and%2520Yoav%2520Artzi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520in-context%2520learning%2520%2528ICL%2529%252C%2520a%2520supervised%250Alearning%2520technique%2520that%2520relies%2520on%2520adding%2520annotated%2520examples%2520to%2520the%2520model%250Acontext.%2520We%2520investigate%2520a%2520contextual%2520bandit%2520version%2520of%2520in-context%2520reinforcement%250Alearning%2520%2528ICRL%2529%252C%2520where%2520models%2520learn%2520in-context%252C%2520online%252C%2520from%2520external%2520reward%252C%250Ainstead%2520of%2520supervised%2520data.%2520We%2520show%2520that%2520LLMs%2520effectively%2520demonstrate%2520such%250Alearning%252C%2520and%2520provide%2520a%2520detailed%2520study%2520of%2520the%2520phenomena%252C%2520experimenting%2520with%250Achallenging%2520classification%2520tasks%2520and%2520models%2520of%2520sizes%2520from%2520500M%2520to%252070B%250Aparameters.%2520This%2520includes%2520identifying%2520and%2520addressing%2520the%2520instability%2520of%2520the%250Aprocess%252C%2520demonstrating%2520learning%2520with%2520both%2520semantic%2520and%2520abstract%2520labels%252C%2520and%250Ashowing%2520scaling%2520trends.%2520Our%2520findings%2520highlight%2520ICRL%2520capabilities%2520in%2520LLMs%252C%2520while%250Aalso%2520underscoring%2520fundamental%2520limitations%2520in%2520their%2520implicit%2520reasoning%2520about%250Aerrors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05362v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20Are%20In-Context%20Bandit%20Reinforcement%20Learners&entry.906535625=Giovanni%20Monea%20and%20Antoine%20Bosselut%20and%20Kiant%C3%A9%20Brantley%20and%20Yoav%20Artzi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20in-context%20learning%20%28ICL%29%2C%20a%20supervised%0Alearning%20technique%20that%20relies%20on%20adding%20annotated%20examples%20to%20the%20model%0Acontext.%20We%20investigate%20a%20contextual%20bandit%20version%20of%20in-context%20reinforcement%0Alearning%20%28ICRL%29%2C%20where%20models%20learn%20in-context%2C%20online%2C%20from%20external%20reward%2C%0Ainstead%20of%20supervised%20data.%20We%20show%20that%20LLMs%20effectively%20demonstrate%20such%0Alearning%2C%20and%20provide%20a%20detailed%20study%20of%20the%20phenomena%2C%20experimenting%20with%0Achallenging%20classification%20tasks%20and%20models%20of%20sizes%20from%20500M%20to%2070B%0Aparameters.%20This%20includes%20identifying%20and%20addressing%20the%20instability%20of%20the%0Aprocess%2C%20demonstrating%20learning%20with%20both%20semantic%20and%20abstract%20labels%2C%20and%0Ashowing%20scaling%20trends.%20Our%20findings%20highlight%20ICRL%20capabilities%20in%20LLMs%2C%20while%0Aalso%20underscoring%20fundamental%20limitations%20in%20their%20implicit%20reasoning%20about%0Aerrors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05362v3&entry.124074799=Read"},
{"title": "DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving\n  Self-Supervised Depth Estimation", "author": "Zihua Liu and Yizhou Li and Songyan Zhang and Masatoshi Okutomi", "abstract": "  While supervised stereo matching and monocular depth estimation have advanced\nsignificantly with learning-based algorithms, self-supervised methods using\nstereo images as supervision signals have received relatively less focus and\nrequire further investigation. A primary challenge arises from ambiguity\nintroduced during photometric reconstruction, particularly due to missing\ncorresponding pixels in ill-posed regions of the target view, such as\nocclusions and out-of-frame areas. To address this and establish explicit\nphotometric correspondences, we propose DMS, a model-agnostic approach that\nutilizes geometric priors from diffusion models to synthesize novel views along\nthe epipolar direction, guided by directional prompts. Specifically, we\nfinetune a Stable Diffusion model to simulate perspectives at key positions:\nleft-left view shifted from the left camera, right-right view shifted from the\nright camera, along with an additional novel view between the left and right\ncameras. These synthesized views supplement occluded pixels, enabling explicit\nphotometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''\nmethod that seamlessly enhances self-supervised stereo matching and monocular\ndepth estimation, and relies solely on unlabeled stereo image pairs for both\ntraining and synthesizing. Extensive experiments demonstrate the effectiveness\nof our approach, with up to 35% outlier reduction and state-of-the-art\nperformance across multiple benchmark datasets.\n", "link": "http://arxiv.org/abs/2508.13091v1", "date": "2025-08-18", "relevancy": 2.3711, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6074}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5929}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DMS%3ADiffusion-Based%20Multi-Baseline%20Stereo%20Generation%20for%20Improving%0A%20%20Self-Supervised%20Depth%20Estimation&body=Title%3A%20DMS%3ADiffusion-Based%20Multi-Baseline%20Stereo%20Generation%20for%20Improving%0A%20%20Self-Supervised%20Depth%20Estimation%0AAuthor%3A%20Zihua%20Liu%20and%20Yizhou%20Li%20and%20Songyan%20Zhang%20and%20Masatoshi%20Okutomi%0AAbstract%3A%20%20%20While%20supervised%20stereo%20matching%20and%20monocular%20depth%20estimation%20have%20advanced%0Asignificantly%20with%20learning-based%20algorithms%2C%20self-supervised%20methods%20using%0Astereo%20images%20as%20supervision%20signals%20have%20received%20relatively%20less%20focus%20and%0Arequire%20further%20investigation.%20A%20primary%20challenge%20arises%20from%20ambiguity%0Aintroduced%20during%20photometric%20reconstruction%2C%20particularly%20due%20to%20missing%0Acorresponding%20pixels%20in%20ill-posed%20regions%20of%20the%20target%20view%2C%20such%20as%0Aocclusions%20and%20out-of-frame%20areas.%20To%20address%20this%20and%20establish%20explicit%0Aphotometric%20correspondences%2C%20we%20propose%20DMS%2C%20a%20model-agnostic%20approach%20that%0Autilizes%20geometric%20priors%20from%20diffusion%20models%20to%20synthesize%20novel%20views%20along%0Athe%20epipolar%20direction%2C%20guided%20by%20directional%20prompts.%20Specifically%2C%20we%0Afinetune%20a%20Stable%20Diffusion%20model%20to%20simulate%20perspectives%20at%20key%20positions%3A%0Aleft-left%20view%20shifted%20from%20the%20left%20camera%2C%20right-right%20view%20shifted%20from%20the%0Aright%20camera%2C%20along%20with%20an%20additional%20novel%20view%20between%20the%20left%20and%20right%0Acameras.%20These%20synthesized%20views%20supplement%20occluded%20pixels%2C%20enabling%20explicit%0Aphotometric%20reconstruction.%20Our%20proposed%20DMS%20is%20a%20cost-free%2C%20%27%27plug-and-play%27%27%0Amethod%20that%20seamlessly%20enhances%20self-supervised%20stereo%20matching%20and%20monocular%0Adepth%20estimation%2C%20and%20relies%20solely%20on%20unlabeled%20stereo%20image%20pairs%20for%20both%0Atraining%20and%20synthesizing.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20our%20approach%2C%20with%20up%20to%2035%25%20outlier%20reduction%20and%20state-of-the-art%0Aperformance%20across%20multiple%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDMS%253ADiffusion-Based%2520Multi-Baseline%2520Stereo%2520Generation%2520for%2520Improving%250A%2520%2520Self-Supervised%2520Depth%2520Estimation%26entry.906535625%3DZihua%2520Liu%2520and%2520Yizhou%2520Li%2520and%2520Songyan%2520Zhang%2520and%2520Masatoshi%2520Okutomi%26entry.1292438233%3D%2520%2520While%2520supervised%2520stereo%2520matching%2520and%2520monocular%2520depth%2520estimation%2520have%2520advanced%250Asignificantly%2520with%2520learning-based%2520algorithms%252C%2520self-supervised%2520methods%2520using%250Astereo%2520images%2520as%2520supervision%2520signals%2520have%2520received%2520relatively%2520less%2520focus%2520and%250Arequire%2520further%2520investigation.%2520A%2520primary%2520challenge%2520arises%2520from%2520ambiguity%250Aintroduced%2520during%2520photometric%2520reconstruction%252C%2520particularly%2520due%2520to%2520missing%250Acorresponding%2520pixels%2520in%2520ill-posed%2520regions%2520of%2520the%2520target%2520view%252C%2520such%2520as%250Aocclusions%2520and%2520out-of-frame%2520areas.%2520To%2520address%2520this%2520and%2520establish%2520explicit%250Aphotometric%2520correspondences%252C%2520we%2520propose%2520DMS%252C%2520a%2520model-agnostic%2520approach%2520that%250Autilizes%2520geometric%2520priors%2520from%2520diffusion%2520models%2520to%2520synthesize%2520novel%2520views%2520along%250Athe%2520epipolar%2520direction%252C%2520guided%2520by%2520directional%2520prompts.%2520Specifically%252C%2520we%250Afinetune%2520a%2520Stable%2520Diffusion%2520model%2520to%2520simulate%2520perspectives%2520at%2520key%2520positions%253A%250Aleft-left%2520view%2520shifted%2520from%2520the%2520left%2520camera%252C%2520right-right%2520view%2520shifted%2520from%2520the%250Aright%2520camera%252C%2520along%2520with%2520an%2520additional%2520novel%2520view%2520between%2520the%2520left%2520and%2520right%250Acameras.%2520These%2520synthesized%2520views%2520supplement%2520occluded%2520pixels%252C%2520enabling%2520explicit%250Aphotometric%2520reconstruction.%2520Our%2520proposed%2520DMS%2520is%2520a%2520cost-free%252C%2520%2527%2527plug-and-play%2527%2527%250Amethod%2520that%2520seamlessly%2520enhances%2520self-supervised%2520stereo%2520matching%2520and%2520monocular%250Adepth%2520estimation%252C%2520and%2520relies%2520solely%2520on%2520unlabeled%2520stereo%2520image%2520pairs%2520for%2520both%250Atraining%2520and%2520synthesizing.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520approach%252C%2520with%2520up%2520to%252035%2525%2520outlier%2520reduction%2520and%2520state-of-the-art%250Aperformance%2520across%2520multiple%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DMS%3ADiffusion-Based%20Multi-Baseline%20Stereo%20Generation%20for%20Improving%0A%20%20Self-Supervised%20Depth%20Estimation&entry.906535625=Zihua%20Liu%20and%20Yizhou%20Li%20and%20Songyan%20Zhang%20and%20Masatoshi%20Okutomi&entry.1292438233=%20%20While%20supervised%20stereo%20matching%20and%20monocular%20depth%20estimation%20have%20advanced%0Asignificantly%20with%20learning-based%20algorithms%2C%20self-supervised%20methods%20using%0Astereo%20images%20as%20supervision%20signals%20have%20received%20relatively%20less%20focus%20and%0Arequire%20further%20investigation.%20A%20primary%20challenge%20arises%20from%20ambiguity%0Aintroduced%20during%20photometric%20reconstruction%2C%20particularly%20due%20to%20missing%0Acorresponding%20pixels%20in%20ill-posed%20regions%20of%20the%20target%20view%2C%20such%20as%0Aocclusions%20and%20out-of-frame%20areas.%20To%20address%20this%20and%20establish%20explicit%0Aphotometric%20correspondences%2C%20we%20propose%20DMS%2C%20a%20model-agnostic%20approach%20that%0Autilizes%20geometric%20priors%20from%20diffusion%20models%20to%20synthesize%20novel%20views%20along%0Athe%20epipolar%20direction%2C%20guided%20by%20directional%20prompts.%20Specifically%2C%20we%0Afinetune%20a%20Stable%20Diffusion%20model%20to%20simulate%20perspectives%20at%20key%20positions%3A%0Aleft-left%20view%20shifted%20from%20the%20left%20camera%2C%20right-right%20view%20shifted%20from%20the%0Aright%20camera%2C%20along%20with%20an%20additional%20novel%20view%20between%20the%20left%20and%20right%0Acameras.%20These%20synthesized%20views%20supplement%20occluded%20pixels%2C%20enabling%20explicit%0Aphotometric%20reconstruction.%20Our%20proposed%20DMS%20is%20a%20cost-free%2C%20%27%27plug-and-play%27%27%0Amethod%20that%20seamlessly%20enhances%20self-supervised%20stereo%20matching%20and%20monocular%0Adepth%20estimation%2C%20and%20relies%20solely%20on%20unlabeled%20stereo%20image%20pairs%20for%20both%0Atraining%20and%20synthesizing.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20our%20approach%2C%20with%20up%20to%2035%25%20outlier%20reduction%20and%20state-of-the-art%0Aperformance%20across%20multiple%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13091v1&entry.124074799=Read"},
{"title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement\n  with Video Generative Models", "author": "Jianshu Zeng and Yuxuan Liu and Yutong Feng and Chenxuan Miao and Zixiang Gao and Jiwang Qu and Jianzhang Zhang and Bin Wang and Kun Yuan", "abstract": "  Video relighting is a challenging yet valuable task, aiming to replace the\nbackground in videos while correspondingly adjusting the lighting in the\nforeground with harmonious blending. During translation, it is essential to\npreserve the original properties of the foreground, e.g., albedo, and propagate\nconsistent relighting among temporal frames. In this paper, we propose Lumen,\nan end-to-end video relighting framework developed on large-scale video\ngenerative models, receiving flexible textual description for instructing the\ncontrol of lighting and background. Considering the scarcity of high-qualified\npaired videos with the same foreground in various lighting conditions, we\nconstruct a large-scale dataset with a mixture of realistic and synthetic\nvideos. For the synthetic domain, benefiting from the abundant 3D assets in the\ncommunity, we leverage advanced 3D rendering engine to curate video pairs in\ndiverse environments. For the realistic domain, we adapt a HDR-based lighting\nsimulation to complement the lack of paired in-the-wild videos. Powered by the\naforementioned dataset, we design a joint training curriculum to effectively\nunleash the strengths of each domain, i.e., the physical consistency in\nsynthetic videos, and the generalized domain distribution in realistic videos.\nTo implement this, we inject a domain-aware adapter into the model to decouple\nthe learning of relighting and domain appearance distribution. We construct a\ncomprehensive benchmark to evaluate Lumen together with existing methods, from\nthe perspectives of foreground preservation and video consistency assessment.\nExperimental results demonstrate that Lumen effectively edit the input into\ncinematic relighted videos with consistent lighting and strict foreground\npreservation. Our project page: https://lumen-relight.github.io/\n", "link": "http://arxiv.org/abs/2508.12945v1", "date": "2025-08-18", "relevancy": 2.3507, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5933}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.59}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumen%3A%20Consistent%20Video%20Relighting%20and%20Harmonious%20Background%20Replacement%0A%20%20with%20Video%20Generative%20Models&body=Title%3A%20Lumen%3A%20Consistent%20Video%20Relighting%20and%20Harmonious%20Background%20Replacement%0A%20%20with%20Video%20Generative%20Models%0AAuthor%3A%20Jianshu%20Zeng%20and%20Yuxuan%20Liu%20and%20Yutong%20Feng%20and%20Chenxuan%20Miao%20and%20Zixiang%20Gao%20and%20Jiwang%20Qu%20and%20Jianzhang%20Zhang%20and%20Bin%20Wang%20and%20Kun%20Yuan%0AAbstract%3A%20%20%20Video%20relighting%20is%20a%20challenging%20yet%20valuable%20task%2C%20aiming%20to%20replace%20the%0Abackground%20in%20videos%20while%20correspondingly%20adjusting%20the%20lighting%20in%20the%0Aforeground%20with%20harmonious%20blending.%20During%20translation%2C%20it%20is%20essential%20to%0Apreserve%20the%20original%20properties%20of%20the%20foreground%2C%20e.g.%2C%20albedo%2C%20and%20propagate%0Aconsistent%20relighting%20among%20temporal%20frames.%20In%20this%20paper%2C%20we%20propose%20Lumen%2C%0Aan%20end-to-end%20video%20relighting%20framework%20developed%20on%20large-scale%20video%0Agenerative%20models%2C%20receiving%20flexible%20textual%20description%20for%20instructing%20the%0Acontrol%20of%20lighting%20and%20background.%20Considering%20the%20scarcity%20of%20high-qualified%0Apaired%20videos%20with%20the%20same%20foreground%20in%20various%20lighting%20conditions%2C%20we%0Aconstruct%20a%20large-scale%20dataset%20with%20a%20mixture%20of%20realistic%20and%20synthetic%0Avideos.%20For%20the%20synthetic%20domain%2C%20benefiting%20from%20the%20abundant%203D%20assets%20in%20the%0Acommunity%2C%20we%20leverage%20advanced%203D%20rendering%20engine%20to%20curate%20video%20pairs%20in%0Adiverse%20environments.%20For%20the%20realistic%20domain%2C%20we%20adapt%20a%20HDR-based%20lighting%0Asimulation%20to%20complement%20the%20lack%20of%20paired%20in-the-wild%20videos.%20Powered%20by%20the%0Aaforementioned%20dataset%2C%20we%20design%20a%20joint%20training%20curriculum%20to%20effectively%0Aunleash%20the%20strengths%20of%20each%20domain%2C%20i.e.%2C%20the%20physical%20consistency%20in%0Asynthetic%20videos%2C%20and%20the%20generalized%20domain%20distribution%20in%20realistic%20videos.%0ATo%20implement%20this%2C%20we%20inject%20a%20domain-aware%20adapter%20into%20the%20model%20to%20decouple%0Athe%20learning%20of%20relighting%20and%20domain%20appearance%20distribution.%20We%20construct%20a%0Acomprehensive%20benchmark%20to%20evaluate%20Lumen%20together%20with%20existing%20methods%2C%20from%0Athe%20perspectives%20of%20foreground%20preservation%20and%20video%20consistency%20assessment.%0AExperimental%20results%20demonstrate%20that%20Lumen%20effectively%20edit%20the%20input%20into%0Acinematic%20relighted%20videos%20with%20consistent%20lighting%20and%20strict%20foreground%0Apreservation.%20Our%20project%20page%3A%20https%3A//lumen-relight.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumen%253A%2520Consistent%2520Video%2520Relighting%2520and%2520Harmonious%2520Background%2520Replacement%250A%2520%2520with%2520Video%2520Generative%2520Models%26entry.906535625%3DJianshu%2520Zeng%2520and%2520Yuxuan%2520Liu%2520and%2520Yutong%2520Feng%2520and%2520Chenxuan%2520Miao%2520and%2520Zixiang%2520Gao%2520and%2520Jiwang%2520Qu%2520and%2520Jianzhang%2520Zhang%2520and%2520Bin%2520Wang%2520and%2520Kun%2520Yuan%26entry.1292438233%3D%2520%2520Video%2520relighting%2520is%2520a%2520challenging%2520yet%2520valuable%2520task%252C%2520aiming%2520to%2520replace%2520the%250Abackground%2520in%2520videos%2520while%2520correspondingly%2520adjusting%2520the%2520lighting%2520in%2520the%250Aforeground%2520with%2520harmonious%2520blending.%2520During%2520translation%252C%2520it%2520is%2520essential%2520to%250Apreserve%2520the%2520original%2520properties%2520of%2520the%2520foreground%252C%2520e.g.%252C%2520albedo%252C%2520and%2520propagate%250Aconsistent%2520relighting%2520among%2520temporal%2520frames.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Lumen%252C%250Aan%2520end-to-end%2520video%2520relighting%2520framework%2520developed%2520on%2520large-scale%2520video%250Agenerative%2520models%252C%2520receiving%2520flexible%2520textual%2520description%2520for%2520instructing%2520the%250Acontrol%2520of%2520lighting%2520and%2520background.%2520Considering%2520the%2520scarcity%2520of%2520high-qualified%250Apaired%2520videos%2520with%2520the%2520same%2520foreground%2520in%2520various%2520lighting%2520conditions%252C%2520we%250Aconstruct%2520a%2520large-scale%2520dataset%2520with%2520a%2520mixture%2520of%2520realistic%2520and%2520synthetic%250Avideos.%2520For%2520the%2520synthetic%2520domain%252C%2520benefiting%2520from%2520the%2520abundant%25203D%2520assets%2520in%2520the%250Acommunity%252C%2520we%2520leverage%2520advanced%25203D%2520rendering%2520engine%2520to%2520curate%2520video%2520pairs%2520in%250Adiverse%2520environments.%2520For%2520the%2520realistic%2520domain%252C%2520we%2520adapt%2520a%2520HDR-based%2520lighting%250Asimulation%2520to%2520complement%2520the%2520lack%2520of%2520paired%2520in-the-wild%2520videos.%2520Powered%2520by%2520the%250Aaforementioned%2520dataset%252C%2520we%2520design%2520a%2520joint%2520training%2520curriculum%2520to%2520effectively%250Aunleash%2520the%2520strengths%2520of%2520each%2520domain%252C%2520i.e.%252C%2520the%2520physical%2520consistency%2520in%250Asynthetic%2520videos%252C%2520and%2520the%2520generalized%2520domain%2520distribution%2520in%2520realistic%2520videos.%250ATo%2520implement%2520this%252C%2520we%2520inject%2520a%2520domain-aware%2520adapter%2520into%2520the%2520model%2520to%2520decouple%250Athe%2520learning%2520of%2520relighting%2520and%2520domain%2520appearance%2520distribution.%2520We%2520construct%2520a%250Acomprehensive%2520benchmark%2520to%2520evaluate%2520Lumen%2520together%2520with%2520existing%2520methods%252C%2520from%250Athe%2520perspectives%2520of%2520foreground%2520preservation%2520and%2520video%2520consistency%2520assessment.%250AExperimental%2520results%2520demonstrate%2520that%2520Lumen%2520effectively%2520edit%2520the%2520input%2520into%250Acinematic%2520relighted%2520videos%2520with%2520consistent%2520lighting%2520and%2520strict%2520foreground%250Apreservation.%2520Our%2520project%2520page%253A%2520https%253A//lumen-relight.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumen%3A%20Consistent%20Video%20Relighting%20and%20Harmonious%20Background%20Replacement%0A%20%20with%20Video%20Generative%20Models&entry.906535625=Jianshu%20Zeng%20and%20Yuxuan%20Liu%20and%20Yutong%20Feng%20and%20Chenxuan%20Miao%20and%20Zixiang%20Gao%20and%20Jiwang%20Qu%20and%20Jianzhang%20Zhang%20and%20Bin%20Wang%20and%20Kun%20Yuan&entry.1292438233=%20%20Video%20relighting%20is%20a%20challenging%20yet%20valuable%20task%2C%20aiming%20to%20replace%20the%0Abackground%20in%20videos%20while%20correspondingly%20adjusting%20the%20lighting%20in%20the%0Aforeground%20with%20harmonious%20blending.%20During%20translation%2C%20it%20is%20essential%20to%0Apreserve%20the%20original%20properties%20of%20the%20foreground%2C%20e.g.%2C%20albedo%2C%20and%20propagate%0Aconsistent%20relighting%20among%20temporal%20frames.%20In%20this%20paper%2C%20we%20propose%20Lumen%2C%0Aan%20end-to-end%20video%20relighting%20framework%20developed%20on%20large-scale%20video%0Agenerative%20models%2C%20receiving%20flexible%20textual%20description%20for%20instructing%20the%0Acontrol%20of%20lighting%20and%20background.%20Considering%20the%20scarcity%20of%20high-qualified%0Apaired%20videos%20with%20the%20same%20foreground%20in%20various%20lighting%20conditions%2C%20we%0Aconstruct%20a%20large-scale%20dataset%20with%20a%20mixture%20of%20realistic%20and%20synthetic%0Avideos.%20For%20the%20synthetic%20domain%2C%20benefiting%20from%20the%20abundant%203D%20assets%20in%20the%0Acommunity%2C%20we%20leverage%20advanced%203D%20rendering%20engine%20to%20curate%20video%20pairs%20in%0Adiverse%20environments.%20For%20the%20realistic%20domain%2C%20we%20adapt%20a%20HDR-based%20lighting%0Asimulation%20to%20complement%20the%20lack%20of%20paired%20in-the-wild%20videos.%20Powered%20by%20the%0Aaforementioned%20dataset%2C%20we%20design%20a%20joint%20training%20curriculum%20to%20effectively%0Aunleash%20the%20strengths%20of%20each%20domain%2C%20i.e.%2C%20the%20physical%20consistency%20in%0Asynthetic%20videos%2C%20and%20the%20generalized%20domain%20distribution%20in%20realistic%20videos.%0ATo%20implement%20this%2C%20we%20inject%20a%20domain-aware%20adapter%20into%20the%20model%20to%20decouple%0Athe%20learning%20of%20relighting%20and%20domain%20appearance%20distribution.%20We%20construct%20a%0Acomprehensive%20benchmark%20to%20evaluate%20Lumen%20together%20with%20existing%20methods%2C%20from%0Athe%20perspectives%20of%20foreground%20preservation%20and%20video%20consistency%20assessment.%0AExperimental%20results%20demonstrate%20that%20Lumen%20effectively%20edit%20the%20input%20into%0Acinematic%20relighted%20videos%20with%20consistent%20lighting%20and%20strict%20foreground%0Apreservation.%20Our%20project%20page%3A%20https%3A//lumen-relight.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12945v1&entry.124074799=Read"},
{"title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence", "author": "Ling-Hao Chen and Yuhong Zhang and Zixin Yin and Zhiyang Dou and Xin Chen and Jingbo Wang and Taku Komura and Lei Zhang", "abstract": "  This work studies the challenge of transfer animations between characters\nwhose skeletal topologies differ substantially. While many techniques have\nadvanced retargeting techniques in decades, transfer motions across diverse\ntopologies remains less-explored. The primary obstacle lies in the inherent\ntopological inconsistency between source and target skeletons, which restricts\nthe establishment of straightforward one-to-one bone correspondences. Besides,\nthe current lack of large-scale paired motion datasets spanning different\ntopological structures severely constrains the development of data-driven\napproaches. To address these limitations, we introduce Motion2Motion, a novel,\ntraining-free framework. Simply yet effectively, Motion2Motion works with only\none or a few example motions on the target skeleton, by accessing a sparse set\nof bone correspondences between the source and target skeletons. Through\ncomprehensive qualitative and quantitative evaluations, we demonstrate that\nMotion2Motion achieves efficient and reliable performance in both\nsimilar-skeleton and cross-species skeleton transfer scenarios. The practical\nutility of our approach is further evidenced by its successful integration in\ndownstream applications and user interfaces, highlighting its potential for\nindustrial applications. Code and data are available at\nhttps://lhchen.top/Motion2Motion.\n", "link": "http://arxiv.org/abs/2508.13139v1", "date": "2025-08-18", "relevancy": 2.3432, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6347}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5605}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion2Motion%3A%20Cross-topology%20Motion%20Transfer%20with%20Sparse%20Correspondence&body=Title%3A%20Motion2Motion%3A%20Cross-topology%20Motion%20Transfer%20with%20Sparse%20Correspondence%0AAuthor%3A%20Ling-Hao%20Chen%20and%20Yuhong%20Zhang%20and%20Zixin%20Yin%20and%20Zhiyang%20Dou%20and%20Xin%20Chen%20and%20Jingbo%20Wang%20and%20Taku%20Komura%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20This%20work%20studies%20the%20challenge%20of%20transfer%20animations%20between%20characters%0Awhose%20skeletal%20topologies%20differ%20substantially.%20While%20many%20techniques%20have%0Aadvanced%20retargeting%20techniques%20in%20decades%2C%20transfer%20motions%20across%20diverse%0Atopologies%20remains%20less-explored.%20The%20primary%20obstacle%20lies%20in%20the%20inherent%0Atopological%20inconsistency%20between%20source%20and%20target%20skeletons%2C%20which%20restricts%0Athe%20establishment%20of%20straightforward%20one-to-one%20bone%20correspondences.%20Besides%2C%0Athe%20current%20lack%20of%20large-scale%20paired%20motion%20datasets%20spanning%20different%0Atopological%20structures%20severely%20constrains%20the%20development%20of%20data-driven%0Aapproaches.%20To%20address%20these%20limitations%2C%20we%20introduce%20Motion2Motion%2C%20a%20novel%2C%0Atraining-free%20framework.%20Simply%20yet%20effectively%2C%20Motion2Motion%20works%20with%20only%0Aone%20or%20a%20few%20example%20motions%20on%20the%20target%20skeleton%2C%20by%20accessing%20a%20sparse%20set%0Aof%20bone%20correspondences%20between%20the%20source%20and%20target%20skeletons.%20Through%0Acomprehensive%20qualitative%20and%20quantitative%20evaluations%2C%20we%20demonstrate%20that%0AMotion2Motion%20achieves%20efficient%20and%20reliable%20performance%20in%20both%0Asimilar-skeleton%20and%20cross-species%20skeleton%20transfer%20scenarios.%20The%20practical%0Autility%20of%20our%20approach%20is%20further%20evidenced%20by%20its%20successful%20integration%20in%0Adownstream%20applications%20and%20user%20interfaces%2C%20highlighting%20its%20potential%20for%0Aindustrial%20applications.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//lhchen.top/Motion2Motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion2Motion%253A%2520Cross-topology%2520Motion%2520Transfer%2520with%2520Sparse%2520Correspondence%26entry.906535625%3DLing-Hao%2520Chen%2520and%2520Yuhong%2520Zhang%2520and%2520Zixin%2520Yin%2520and%2520Zhiyang%2520Dou%2520and%2520Xin%2520Chen%2520and%2520Jingbo%2520Wang%2520and%2520Taku%2520Komura%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520This%2520work%2520studies%2520the%2520challenge%2520of%2520transfer%2520animations%2520between%2520characters%250Awhose%2520skeletal%2520topologies%2520differ%2520substantially.%2520While%2520many%2520techniques%2520have%250Aadvanced%2520retargeting%2520techniques%2520in%2520decades%252C%2520transfer%2520motions%2520across%2520diverse%250Atopologies%2520remains%2520less-explored.%2520The%2520primary%2520obstacle%2520lies%2520in%2520the%2520inherent%250Atopological%2520inconsistency%2520between%2520source%2520and%2520target%2520skeletons%252C%2520which%2520restricts%250Athe%2520establishment%2520of%2520straightforward%2520one-to-one%2520bone%2520correspondences.%2520Besides%252C%250Athe%2520current%2520lack%2520of%2520large-scale%2520paired%2520motion%2520datasets%2520spanning%2520different%250Atopological%2520structures%2520severely%2520constrains%2520the%2520development%2520of%2520data-driven%250Aapproaches.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Motion2Motion%252C%2520a%2520novel%252C%250Atraining-free%2520framework.%2520Simply%2520yet%2520effectively%252C%2520Motion2Motion%2520works%2520with%2520only%250Aone%2520or%2520a%2520few%2520example%2520motions%2520on%2520the%2520target%2520skeleton%252C%2520by%2520accessing%2520a%2520sparse%2520set%250Aof%2520bone%2520correspondences%2520between%2520the%2520source%2520and%2520target%2520skeletons.%2520Through%250Acomprehensive%2520qualitative%2520and%2520quantitative%2520evaluations%252C%2520we%2520demonstrate%2520that%250AMotion2Motion%2520achieves%2520efficient%2520and%2520reliable%2520performance%2520in%2520both%250Asimilar-skeleton%2520and%2520cross-species%2520skeleton%2520transfer%2520scenarios.%2520The%2520practical%250Autility%2520of%2520our%2520approach%2520is%2520further%2520evidenced%2520by%2520its%2520successful%2520integration%2520in%250Adownstream%2520applications%2520and%2520user%2520interfaces%252C%2520highlighting%2520its%2520potential%2520for%250Aindustrial%2520applications.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//lhchen.top/Motion2Motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion2Motion%3A%20Cross-topology%20Motion%20Transfer%20with%20Sparse%20Correspondence&entry.906535625=Ling-Hao%20Chen%20and%20Yuhong%20Zhang%20and%20Zixin%20Yin%20and%20Zhiyang%20Dou%20and%20Xin%20Chen%20and%20Jingbo%20Wang%20and%20Taku%20Komura%20and%20Lei%20Zhang&entry.1292438233=%20%20This%20work%20studies%20the%20challenge%20of%20transfer%20animations%20between%20characters%0Awhose%20skeletal%20topologies%20differ%20substantially.%20While%20many%20techniques%20have%0Aadvanced%20retargeting%20techniques%20in%20decades%2C%20transfer%20motions%20across%20diverse%0Atopologies%20remains%20less-explored.%20The%20primary%20obstacle%20lies%20in%20the%20inherent%0Atopological%20inconsistency%20between%20source%20and%20target%20skeletons%2C%20which%20restricts%0Athe%20establishment%20of%20straightforward%20one-to-one%20bone%20correspondences.%20Besides%2C%0Athe%20current%20lack%20of%20large-scale%20paired%20motion%20datasets%20spanning%20different%0Atopological%20structures%20severely%20constrains%20the%20development%20of%20data-driven%0Aapproaches.%20To%20address%20these%20limitations%2C%20we%20introduce%20Motion2Motion%2C%20a%20novel%2C%0Atraining-free%20framework.%20Simply%20yet%20effectively%2C%20Motion2Motion%20works%20with%20only%0Aone%20or%20a%20few%20example%20motions%20on%20the%20target%20skeleton%2C%20by%20accessing%20a%20sparse%20set%0Aof%20bone%20correspondences%20between%20the%20source%20and%20target%20skeletons.%20Through%0Acomprehensive%20qualitative%20and%20quantitative%20evaluations%2C%20we%20demonstrate%20that%0AMotion2Motion%20achieves%20efficient%20and%20reliable%20performance%20in%20both%0Asimilar-skeleton%20and%20cross-species%20skeleton%20transfer%20scenarios.%20The%20practical%0Autility%20of%20our%20approach%20is%20further%20evidenced%20by%20its%20successful%20integration%20in%0Adownstream%20applications%20and%20user%20interfaces%2C%20highlighting%20its%20potential%20for%0Aindustrial%20applications.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//lhchen.top/Motion2Motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13139v1&entry.124074799=Read"},
{"title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation:\n  A Survey", "author": "Rui Shao and Wei Li and Lingsen Zhang and Renshan Zhang and Zhiyang Liu and Ran Chen and Liqiang Nie", "abstract": "  Robotic manipulation, a key frontier in robotics and embodied AI, requires\nprecise motor control and multimodal understanding, yet traditional rule-based\nmethods fail to scale or generalize in unstructured, novel environments. In\nrecent years, Vision-Language-Action (VLA) models, built upon Large\nVision-Language Models (VLMs) pretrained on vast image-text datasets, have\nemerged as a transformative paradigm. This survey provides the first\nsystematic, taxonomy-oriented review of large VLM-based VLA models for robotic\nmanipulation. We begin by clearly defining large VLM-based VLA models and\ndelineating two principal architectural paradigms: (1) monolithic models,\nencompassing single-system and dual-system designs with differing levels of\nintegration; and (2) hierarchical models, which explicitly decouple planning\nfrom execution via interpretable intermediate representations. Building on this\nfoundation, we present an in-depth examination of large VLM-based VLA models:\n(1) integration with advanced domains, including reinforcement learning,\ntraining-free optimization, learning from human videos, and world model\nintegration; (2) synthesis of distinctive characteristics, consolidating\narchitectural traits, operational strengths, and the datasets and benchmarks\nthat support their development; (3) identification of promising directions,\nincluding memory mechanisms, 4D perception, efficient adaptation, multi-agent\ncooperation, and other emerging capabilities. This survey consolidates recent\nadvances to resolve inconsistencies in existing taxonomies, mitigate research\nfragmentation, and fill a critical gap through the systematic integration of\nstudies at the intersection of large VLMs and robotic manipulation. We provide\na regularly updated project page to document ongoing progress:\nhttps://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.\n", "link": "http://arxiv.org/abs/2508.13073v1", "date": "2025-08-18", "relevancy": 2.3313, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20VLM-based%20Vision-Language-Action%20Models%20for%20Robotic%20Manipulation%3A%0A%20%20A%20Survey&body=Title%3A%20Large%20VLM-based%20Vision-Language-Action%20Models%20for%20Robotic%20Manipulation%3A%0A%20%20A%20Survey%0AAuthor%3A%20Rui%20Shao%20and%20Wei%20Li%20and%20Lingsen%20Zhang%20and%20Renshan%20Zhang%20and%20Zhiyang%20Liu%20and%20Ran%20Chen%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20Robotic%20manipulation%2C%20a%20key%20frontier%20in%20robotics%20and%20embodied%20AI%2C%20requires%0Aprecise%20motor%20control%20and%20multimodal%20understanding%2C%20yet%20traditional%20rule-based%0Amethods%20fail%20to%20scale%20or%20generalize%20in%20unstructured%2C%20novel%20environments.%20In%0Arecent%20years%2C%20Vision-Language-Action%20%28VLA%29%20models%2C%20built%20upon%20Large%0AVision-Language%20Models%20%28VLMs%29%20pretrained%20on%20vast%20image-text%20datasets%2C%20have%0Aemerged%20as%20a%20transformative%20paradigm.%20This%20survey%20provides%20the%20first%0Asystematic%2C%20taxonomy-oriented%20review%20of%20large%20VLM-based%20VLA%20models%20for%20robotic%0Amanipulation.%20We%20begin%20by%20clearly%20defining%20large%20VLM-based%20VLA%20models%20and%0Adelineating%20two%20principal%20architectural%20paradigms%3A%20%281%29%20monolithic%20models%2C%0Aencompassing%20single-system%20and%20dual-system%20designs%20with%20differing%20levels%20of%0Aintegration%3B%20and%20%282%29%20hierarchical%20models%2C%20which%20explicitly%20decouple%20planning%0Afrom%20execution%20via%20interpretable%20intermediate%20representations.%20Building%20on%20this%0Afoundation%2C%20we%20present%20an%20in-depth%20examination%20of%20large%20VLM-based%20VLA%20models%3A%0A%281%29%20integration%20with%20advanced%20domains%2C%20including%20reinforcement%20learning%2C%0Atraining-free%20optimization%2C%20learning%20from%20human%20videos%2C%20and%20world%20model%0Aintegration%3B%20%282%29%20synthesis%20of%20distinctive%20characteristics%2C%20consolidating%0Aarchitectural%20traits%2C%20operational%20strengths%2C%20and%20the%20datasets%20and%20benchmarks%0Athat%20support%20their%20development%3B%20%283%29%20identification%20of%20promising%20directions%2C%0Aincluding%20memory%20mechanisms%2C%204D%20perception%2C%20efficient%20adaptation%2C%20multi-agent%0Acooperation%2C%20and%20other%20emerging%20capabilities.%20This%20survey%20consolidates%20recent%0Aadvances%20to%20resolve%20inconsistencies%20in%20existing%20taxonomies%2C%20mitigate%20research%0Afragmentation%2C%20and%20fill%20a%20critical%20gap%20through%20the%20systematic%20integration%20of%0Astudies%20at%20the%20intersection%20of%20large%20VLMs%20and%20robotic%20manipulation.%20We%20provide%0Aa%20regularly%20updated%20project%20page%20to%20document%20ongoing%20progress%3A%0Ahttps%3A//github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520VLM-based%2520Vision-Language-Action%2520Models%2520for%2520Robotic%2520Manipulation%253A%250A%2520%2520A%2520Survey%26entry.906535625%3DRui%2520Shao%2520and%2520Wei%2520Li%2520and%2520Lingsen%2520Zhang%2520and%2520Renshan%2520Zhang%2520and%2520Zhiyang%2520Liu%2520and%2520Ran%2520Chen%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520Robotic%2520manipulation%252C%2520a%2520key%2520frontier%2520in%2520robotics%2520and%2520embodied%2520AI%252C%2520requires%250Aprecise%2520motor%2520control%2520and%2520multimodal%2520understanding%252C%2520yet%2520traditional%2520rule-based%250Amethods%2520fail%2520to%2520scale%2520or%2520generalize%2520in%2520unstructured%252C%2520novel%2520environments.%2520In%250Arecent%2520years%252C%2520Vision-Language-Action%2520%2528VLA%2529%2520models%252C%2520built%2520upon%2520Large%250AVision-Language%2520Models%2520%2528VLMs%2529%2520pretrained%2520on%2520vast%2520image-text%2520datasets%252C%2520have%250Aemerged%2520as%2520a%2520transformative%2520paradigm.%2520This%2520survey%2520provides%2520the%2520first%250Asystematic%252C%2520taxonomy-oriented%2520review%2520of%2520large%2520VLM-based%2520VLA%2520models%2520for%2520robotic%250Amanipulation.%2520We%2520begin%2520by%2520clearly%2520defining%2520large%2520VLM-based%2520VLA%2520models%2520and%250Adelineating%2520two%2520principal%2520architectural%2520paradigms%253A%2520%25281%2529%2520monolithic%2520models%252C%250Aencompassing%2520single-system%2520and%2520dual-system%2520designs%2520with%2520differing%2520levels%2520of%250Aintegration%253B%2520and%2520%25282%2529%2520hierarchical%2520models%252C%2520which%2520explicitly%2520decouple%2520planning%250Afrom%2520execution%2520via%2520interpretable%2520intermediate%2520representations.%2520Building%2520on%2520this%250Afoundation%252C%2520we%2520present%2520an%2520in-depth%2520examination%2520of%2520large%2520VLM-based%2520VLA%2520models%253A%250A%25281%2529%2520integration%2520with%2520advanced%2520domains%252C%2520including%2520reinforcement%2520learning%252C%250Atraining-free%2520optimization%252C%2520learning%2520from%2520human%2520videos%252C%2520and%2520world%2520model%250Aintegration%253B%2520%25282%2529%2520synthesis%2520of%2520distinctive%2520characteristics%252C%2520consolidating%250Aarchitectural%2520traits%252C%2520operational%2520strengths%252C%2520and%2520the%2520datasets%2520and%2520benchmarks%250Athat%2520support%2520their%2520development%253B%2520%25283%2529%2520identification%2520of%2520promising%2520directions%252C%250Aincluding%2520memory%2520mechanisms%252C%25204D%2520perception%252C%2520efficient%2520adaptation%252C%2520multi-agent%250Acooperation%252C%2520and%2520other%2520emerging%2520capabilities.%2520This%2520survey%2520consolidates%2520recent%250Aadvances%2520to%2520resolve%2520inconsistencies%2520in%2520existing%2520taxonomies%252C%2520mitigate%2520research%250Afragmentation%252C%2520and%2520fill%2520a%2520critical%2520gap%2520through%2520the%2520systematic%2520integration%2520of%250Astudies%2520at%2520the%2520intersection%2520of%2520large%2520VLMs%2520and%2520robotic%2520manipulation.%2520We%2520provide%250Aa%2520regularly%2520updated%2520project%2520page%2520to%2520document%2520ongoing%2520progress%253A%250Ahttps%253A//github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20VLM-based%20Vision-Language-Action%20Models%20for%20Robotic%20Manipulation%3A%0A%20%20A%20Survey&entry.906535625=Rui%20Shao%20and%20Wei%20Li%20and%20Lingsen%20Zhang%20and%20Renshan%20Zhang%20and%20Zhiyang%20Liu%20and%20Ran%20Chen%20and%20Liqiang%20Nie&entry.1292438233=%20%20Robotic%20manipulation%2C%20a%20key%20frontier%20in%20robotics%20and%20embodied%20AI%2C%20requires%0Aprecise%20motor%20control%20and%20multimodal%20understanding%2C%20yet%20traditional%20rule-based%0Amethods%20fail%20to%20scale%20or%20generalize%20in%20unstructured%2C%20novel%20environments.%20In%0Arecent%20years%2C%20Vision-Language-Action%20%28VLA%29%20models%2C%20built%20upon%20Large%0AVision-Language%20Models%20%28VLMs%29%20pretrained%20on%20vast%20image-text%20datasets%2C%20have%0Aemerged%20as%20a%20transformative%20paradigm.%20This%20survey%20provides%20the%20first%0Asystematic%2C%20taxonomy-oriented%20review%20of%20large%20VLM-based%20VLA%20models%20for%20robotic%0Amanipulation.%20We%20begin%20by%20clearly%20defining%20large%20VLM-based%20VLA%20models%20and%0Adelineating%20two%20principal%20architectural%20paradigms%3A%20%281%29%20monolithic%20models%2C%0Aencompassing%20single-system%20and%20dual-system%20designs%20with%20differing%20levels%20of%0Aintegration%3B%20and%20%282%29%20hierarchical%20models%2C%20which%20explicitly%20decouple%20planning%0Afrom%20execution%20via%20interpretable%20intermediate%20representations.%20Building%20on%20this%0Afoundation%2C%20we%20present%20an%20in-depth%20examination%20of%20large%20VLM-based%20VLA%20models%3A%0A%281%29%20integration%20with%20advanced%20domains%2C%20including%20reinforcement%20learning%2C%0Atraining-free%20optimization%2C%20learning%20from%20human%20videos%2C%20and%20world%20model%0Aintegration%3B%20%282%29%20synthesis%20of%20distinctive%20characteristics%2C%20consolidating%0Aarchitectural%20traits%2C%20operational%20strengths%2C%20and%20the%20datasets%20and%20benchmarks%0Athat%20support%20their%20development%3B%20%283%29%20identification%20of%20promising%20directions%2C%0Aincluding%20memory%20mechanisms%2C%204D%20perception%2C%20efficient%20adaptation%2C%20multi-agent%0Acooperation%2C%20and%20other%20emerging%20capabilities.%20This%20survey%20consolidates%20recent%0Aadvances%20to%20resolve%20inconsistencies%20in%20existing%20taxonomies%2C%20mitigate%20research%0Afragmentation%2C%20and%20fill%20a%20critical%20gap%20through%20the%20systematic%20integration%20of%0Astudies%20at%20the%20intersection%20of%20large%20VLMs%20and%20robotic%20manipulation.%20We%20provide%0Aa%20regularly%20updated%20project%20page%20to%20document%20ongoing%20progress%3A%0Ahttps%3A//github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13073v1&entry.124074799=Read"},
{"title": "Towards Multimodal Social Conversations with Robots: Using\n  Vision-Language Models", "author": "Ruben Janssens and Tony Belpaeme", "abstract": "  Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices.\n", "link": "http://arxiv.org/abs/2507.19196v2", "date": "2025-08-18", "relevancy": 2.3309, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Multimodal%20Social%20Conversations%20with%20Robots%3A%20Using%0A%20%20Vision-Language%20Models&body=Title%3A%20Towards%20Multimodal%20Social%20Conversations%20with%20Robots%3A%20Using%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Ruben%20Janssens%20and%20Tony%20Belpaeme%0AAbstract%3A%20%20%20Large%20language%20models%20have%20given%20social%20robots%20the%20ability%20to%20autonomously%0Aengage%20in%20open-domain%20conversations.%20However%2C%20they%20are%20still%20missing%20a%0Afundamental%20social%20skill%3A%20making%20use%20of%20the%20multiple%20modalities%20that%20carry%0Asocial%20interactions.%20While%20previous%20work%20has%20focused%20on%20task-oriented%0Ainteractions%20that%20require%20referencing%20the%20environment%20or%20specific%20phenomena%20in%0Asocial%20interactions%20such%20as%20dialogue%20breakdowns%2C%20we%20outline%20the%20overall%20needs%0Aof%20a%20multimodal%20system%20for%20social%20conversations%20with%20robots.%20We%20then%20argue%20that%0Avision-language%20models%20are%20able%20to%20process%20this%20wide%20range%20of%20visual%0Ainformation%20in%20a%20sufficiently%20general%20manner%20for%20autonomous%20social%20robots.%20We%0Adescribe%20how%20to%20adapt%20them%20to%20this%20setting%2C%20which%20technical%20challenges%20remain%2C%0Aand%20briefly%20discuss%20evaluation%20practices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Multimodal%2520Social%2520Conversations%2520with%2520Robots%253A%2520Using%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DRuben%2520Janssens%2520and%2520Tony%2520Belpaeme%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520given%2520social%2520robots%2520the%2520ability%2520to%2520autonomously%250Aengage%2520in%2520open-domain%2520conversations.%2520However%252C%2520they%2520are%2520still%2520missing%2520a%250Afundamental%2520social%2520skill%253A%2520making%2520use%2520of%2520the%2520multiple%2520modalities%2520that%2520carry%250Asocial%2520interactions.%2520While%2520previous%2520work%2520has%2520focused%2520on%2520task-oriented%250Ainteractions%2520that%2520require%2520referencing%2520the%2520environment%2520or%2520specific%2520phenomena%2520in%250Asocial%2520interactions%2520such%2520as%2520dialogue%2520breakdowns%252C%2520we%2520outline%2520the%2520overall%2520needs%250Aof%2520a%2520multimodal%2520system%2520for%2520social%2520conversations%2520with%2520robots.%2520We%2520then%2520argue%2520that%250Avision-language%2520models%2520are%2520able%2520to%2520process%2520this%2520wide%2520range%2520of%2520visual%250Ainformation%2520in%2520a%2520sufficiently%2520general%2520manner%2520for%2520autonomous%2520social%2520robots.%2520We%250Adescribe%2520how%2520to%2520adapt%2520them%2520to%2520this%2520setting%252C%2520which%2520technical%2520challenges%2520remain%252C%250Aand%2520briefly%2520discuss%2520evaluation%2520practices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Multimodal%20Social%20Conversations%20with%20Robots%3A%20Using%0A%20%20Vision-Language%20Models&entry.906535625=Ruben%20Janssens%20and%20Tony%20Belpaeme&entry.1292438233=%20%20Large%20language%20models%20have%20given%20social%20robots%20the%20ability%20to%20autonomously%0Aengage%20in%20open-domain%20conversations.%20However%2C%20they%20are%20still%20missing%20a%0Afundamental%20social%20skill%3A%20making%20use%20of%20the%20multiple%20modalities%20that%20carry%0Asocial%20interactions.%20While%20previous%20work%20has%20focused%20on%20task-oriented%0Ainteractions%20that%20require%20referencing%20the%20environment%20or%20specific%20phenomena%20in%0Asocial%20interactions%20such%20as%20dialogue%20breakdowns%2C%20we%20outline%20the%20overall%20needs%0Aof%20a%20multimodal%20system%20for%20social%20conversations%20with%20robots.%20We%20then%20argue%20that%0Avision-language%20models%20are%20able%20to%20process%20this%20wide%20range%20of%20visual%0Ainformation%20in%20a%20sufficiently%20general%20manner%20for%20autonomous%20social%20robots.%20We%0Adescribe%20how%20to%20adapt%20them%20to%20this%20setting%2C%20which%20technical%20challenges%20remain%2C%0Aand%20briefly%20discuss%20evaluation%20practices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19196v2&entry.124074799=Read"},
{"title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes", "author": "Nikai Du and Zhennan Chen and Shan Gao and Zhizhou Chen and Xi Chen and Zhengkai Jiang and Jian Yang and Ying Tai", "abstract": "  This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2503.23461v6", "date": "2025-08-18", "relevancy": 2.3243, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5938}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5728}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextCrafter%3A%20Accurately%20Rendering%20Multiple%20Texts%20in%20Complex%20Visual%0A%20%20Scenes&body=Title%3A%20TextCrafter%3A%20Accurately%20Rendering%20Multiple%20Texts%20in%20Complex%20Visual%0A%20%20Scenes%0AAuthor%3A%20Nikai%20Du%20and%20Zhennan%20Chen%20and%20Shan%20Gao%20and%20Zhizhou%20Chen%20and%20Xi%20Chen%20and%20Zhengkai%20Jiang%20and%20Jian%20Yang%20and%20Ying%20Tai%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20task%20of%20Complex%20Visual%20Text%20Generation%20%28CVTG%29%2C%20which%0Acenters%20on%20generating%20intricate%20textual%20content%20distributed%20across%20diverse%0Aregions%20within%20visual%20images.%20In%20CVTG%2C%20image%20generation%20models%20often%20rendering%0Adistorted%20and%20blurred%20visual%20text%20or%20missing%20some%20visual%20text.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20TextCrafter%2C%20a%20novel%20multi-visual%20text%20rendering%20method.%0ATextCrafter%20employs%20a%20progressive%20strategy%20to%20decompose%20complex%20visual%20text%0Ainto%20distinct%20components%20while%20ensuring%20robust%20alignment%20between%20textual%0Acontent%20and%20its%20visual%20carrier.%20Additionally%2C%20it%20incorporates%20a%20token%20focus%0Aenhancement%20mechanism%20to%20amplify%20the%20prominence%20of%20visual%20text%20during%20the%0Ageneration%20process.%20TextCrafter%20effectively%20addresses%20key%20challenges%20in%20CVTG%0Atasks%2C%20such%20as%20text%20confusion%2C%20omissions%2C%20and%20blurriness.%20Moreover%2C%20we%20present%0Aa%20new%20benchmark%20dataset%2C%20CVTG-2K%2C%20tailored%20to%20rigorously%20evaluate%20the%0Aperformance%20of%20generative%20models%20on%20CVTG%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20surpasses%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23461v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextCrafter%253A%2520Accurately%2520Rendering%2520Multiple%2520Texts%2520in%2520Complex%2520Visual%250A%2520%2520Scenes%26entry.906535625%3DNikai%2520Du%2520and%2520Zhennan%2520Chen%2520and%2520Shan%2520Gao%2520and%2520Zhizhou%2520Chen%2520and%2520Xi%2520Chen%2520and%2520Zhengkai%2520Jiang%2520and%2520Jian%2520Yang%2520and%2520Ying%2520Tai%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520task%2520of%2520Complex%2520Visual%2520Text%2520Generation%2520%2528CVTG%2529%252C%2520which%250Acenters%2520on%2520generating%2520intricate%2520textual%2520content%2520distributed%2520across%2520diverse%250Aregions%2520within%2520visual%2520images.%2520In%2520CVTG%252C%2520image%2520generation%2520models%2520often%2520rendering%250Adistorted%2520and%2520blurred%2520visual%2520text%2520or%2520missing%2520some%2520visual%2520text.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520propose%2520TextCrafter%252C%2520a%2520novel%2520multi-visual%2520text%2520rendering%2520method.%250ATextCrafter%2520employs%2520a%2520progressive%2520strategy%2520to%2520decompose%2520complex%2520visual%2520text%250Ainto%2520distinct%2520components%2520while%2520ensuring%2520robust%2520alignment%2520between%2520textual%250Acontent%2520and%2520its%2520visual%2520carrier.%2520Additionally%252C%2520it%2520incorporates%2520a%2520token%2520focus%250Aenhancement%2520mechanism%2520to%2520amplify%2520the%2520prominence%2520of%2520visual%2520text%2520during%2520the%250Ageneration%2520process.%2520TextCrafter%2520effectively%2520addresses%2520key%2520challenges%2520in%2520CVTG%250Atasks%252C%2520such%2520as%2520text%2520confusion%252C%2520omissions%252C%2520and%2520blurriness.%2520Moreover%252C%2520we%2520present%250Aa%2520new%2520benchmark%2520dataset%252C%2520CVTG-2K%252C%2520tailored%2520to%2520rigorously%2520evaluate%2520the%250Aperformance%2520of%2520generative%2520models%2520on%2520CVTG%2520tasks.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520surpasses%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23461v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextCrafter%3A%20Accurately%20Rendering%20Multiple%20Texts%20in%20Complex%20Visual%0A%20%20Scenes&entry.906535625=Nikai%20Du%20and%20Zhennan%20Chen%20and%20Shan%20Gao%20and%20Zhizhou%20Chen%20and%20Xi%20Chen%20and%20Zhengkai%20Jiang%20and%20Jian%20Yang%20and%20Ying%20Tai&entry.1292438233=%20%20This%20paper%20explores%20the%20task%20of%20Complex%20Visual%20Text%20Generation%20%28CVTG%29%2C%20which%0Acenters%20on%20generating%20intricate%20textual%20content%20distributed%20across%20diverse%0Aregions%20within%20visual%20images.%20In%20CVTG%2C%20image%20generation%20models%20often%20rendering%0Adistorted%20and%20blurred%20visual%20text%20or%20missing%20some%20visual%20text.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20TextCrafter%2C%20a%20novel%20multi-visual%20text%20rendering%20method.%0ATextCrafter%20employs%20a%20progressive%20strategy%20to%20decompose%20complex%20visual%20text%0Ainto%20distinct%20components%20while%20ensuring%20robust%20alignment%20between%20textual%0Acontent%20and%20its%20visual%20carrier.%20Additionally%2C%20it%20incorporates%20a%20token%20focus%0Aenhancement%20mechanism%20to%20amplify%20the%20prominence%20of%20visual%20text%20during%20the%0Ageneration%20process.%20TextCrafter%20effectively%20addresses%20key%20challenges%20in%20CVTG%0Atasks%2C%20such%20as%20text%20confusion%2C%20omissions%2C%20and%20blurriness.%20Moreover%2C%20we%20present%0Aa%20new%20benchmark%20dataset%2C%20CVTG-2K%2C%20tailored%20to%20rigorously%20evaluate%20the%0Aperformance%20of%20generative%20models%20on%20CVTG%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20surpasses%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23461v6&entry.124074799=Read"},
{"title": "WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction", "author": "Richard Liu and Daniel Fu and Noah Tan and Itai Lang and Rana Hanocka", "abstract": "  In this work we present WIR3D, a technique for abstracting 3D shapes through\na sparse set of visually meaningful curves in 3D. We optimize the parameters of\nBezier curves such that they faithfully represent both the geometry and salient\nvisual features (e.g. texture) of the shape from arbitrary viewpoints. We\nleverage the intermediate activations of a pre-trained foundation model (CLIP)\nto guide our optimization process. We divide our optimization into two phases:\none for capturing the coarse geometry of the shape, and the other for\nrepresenting fine-grained features. Our second phase supervision is spatially\nguided by a novel localized keypoint loss. This spatial guidance enables user\ncontrol over abstracted features. We ensure fidelity to the original surface\nthrough a neural SDF loss, which allows the curves to be used as intuitive\ndeformation handles. We successfully apply our method for shape abstraction\nover a broad dataset of shapes with varying complexity, geometric structure,\nand texture, and demonstrate downstream applications for feature control and\nshape deformation.\n", "link": "http://arxiv.org/abs/2505.04813v2", "date": "2025-08-18", "relevancy": 2.3081, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5862}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.573}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WIR3D%3A%20Visually-Informed%20and%20Geometry-Aware%203D%20Shape%20Abstraction&body=Title%3A%20WIR3D%3A%20Visually-Informed%20and%20Geometry-Aware%203D%20Shape%20Abstraction%0AAuthor%3A%20Richard%20Liu%20and%20Daniel%20Fu%20and%20Noah%20Tan%20and%20Itai%20Lang%20and%20Rana%20Hanocka%0AAbstract%3A%20%20%20In%20this%20work%20we%20present%20WIR3D%2C%20a%20technique%20for%20abstracting%203D%20shapes%20through%0Aa%20sparse%20set%20of%20visually%20meaningful%20curves%20in%203D.%20We%20optimize%20the%20parameters%20of%0ABezier%20curves%20such%20that%20they%20faithfully%20represent%20both%20the%20geometry%20and%20salient%0Avisual%20features%20%28e.g.%20texture%29%20of%20the%20shape%20from%20arbitrary%20viewpoints.%20We%0Aleverage%20the%20intermediate%20activations%20of%20a%20pre-trained%20foundation%20model%20%28CLIP%29%0Ato%20guide%20our%20optimization%20process.%20We%20divide%20our%20optimization%20into%20two%20phases%3A%0Aone%20for%20capturing%20the%20coarse%20geometry%20of%20the%20shape%2C%20and%20the%20other%20for%0Arepresenting%20fine-grained%20features.%20Our%20second%20phase%20supervision%20is%20spatially%0Aguided%20by%20a%20novel%20localized%20keypoint%20loss.%20This%20spatial%20guidance%20enables%20user%0Acontrol%20over%20abstracted%20features.%20We%20ensure%20fidelity%20to%20the%20original%20surface%0Athrough%20a%20neural%20SDF%20loss%2C%20which%20allows%20the%20curves%20to%20be%20used%20as%20intuitive%0Adeformation%20handles.%20We%20successfully%20apply%20our%20method%20for%20shape%20abstraction%0Aover%20a%20broad%20dataset%20of%20shapes%20with%20varying%20complexity%2C%20geometric%20structure%2C%0Aand%20texture%2C%20and%20demonstrate%20downstream%20applications%20for%20feature%20control%20and%0Ashape%20deformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWIR3D%253A%2520Visually-Informed%2520and%2520Geometry-Aware%25203D%2520Shape%2520Abstraction%26entry.906535625%3DRichard%2520Liu%2520and%2520Daniel%2520Fu%2520and%2520Noah%2520Tan%2520and%2520Itai%2520Lang%2520and%2520Rana%2520Hanocka%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520present%2520WIR3D%252C%2520a%2520technique%2520for%2520abstracting%25203D%2520shapes%2520through%250Aa%2520sparse%2520set%2520of%2520visually%2520meaningful%2520curves%2520in%25203D.%2520We%2520optimize%2520the%2520parameters%2520of%250ABezier%2520curves%2520such%2520that%2520they%2520faithfully%2520represent%2520both%2520the%2520geometry%2520and%2520salient%250Avisual%2520features%2520%2528e.g.%2520texture%2529%2520of%2520the%2520shape%2520from%2520arbitrary%2520viewpoints.%2520We%250Aleverage%2520the%2520intermediate%2520activations%2520of%2520a%2520pre-trained%2520foundation%2520model%2520%2528CLIP%2529%250Ato%2520guide%2520our%2520optimization%2520process.%2520We%2520divide%2520our%2520optimization%2520into%2520two%2520phases%253A%250Aone%2520for%2520capturing%2520the%2520coarse%2520geometry%2520of%2520the%2520shape%252C%2520and%2520the%2520other%2520for%250Arepresenting%2520fine-grained%2520features.%2520Our%2520second%2520phase%2520supervision%2520is%2520spatially%250Aguided%2520by%2520a%2520novel%2520localized%2520keypoint%2520loss.%2520This%2520spatial%2520guidance%2520enables%2520user%250Acontrol%2520over%2520abstracted%2520features.%2520We%2520ensure%2520fidelity%2520to%2520the%2520original%2520surface%250Athrough%2520a%2520neural%2520SDF%2520loss%252C%2520which%2520allows%2520the%2520curves%2520to%2520be%2520used%2520as%2520intuitive%250Adeformation%2520handles.%2520We%2520successfully%2520apply%2520our%2520method%2520for%2520shape%2520abstraction%250Aover%2520a%2520broad%2520dataset%2520of%2520shapes%2520with%2520varying%2520complexity%252C%2520geometric%2520structure%252C%250Aand%2520texture%252C%2520and%2520demonstrate%2520downstream%2520applications%2520for%2520feature%2520control%2520and%250Ashape%2520deformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WIR3D%3A%20Visually-Informed%20and%20Geometry-Aware%203D%20Shape%20Abstraction&entry.906535625=Richard%20Liu%20and%20Daniel%20Fu%20and%20Noah%20Tan%20and%20Itai%20Lang%20and%20Rana%20Hanocka&entry.1292438233=%20%20In%20this%20work%20we%20present%20WIR3D%2C%20a%20technique%20for%20abstracting%203D%20shapes%20through%0Aa%20sparse%20set%20of%20visually%20meaningful%20curves%20in%203D.%20We%20optimize%20the%20parameters%20of%0ABezier%20curves%20such%20that%20they%20faithfully%20represent%20both%20the%20geometry%20and%20salient%0Avisual%20features%20%28e.g.%20texture%29%20of%20the%20shape%20from%20arbitrary%20viewpoints.%20We%0Aleverage%20the%20intermediate%20activations%20of%20a%20pre-trained%20foundation%20model%20%28CLIP%29%0Ato%20guide%20our%20optimization%20process.%20We%20divide%20our%20optimization%20into%20two%20phases%3A%0Aone%20for%20capturing%20the%20coarse%20geometry%20of%20the%20shape%2C%20and%20the%20other%20for%0Arepresenting%20fine-grained%20features.%20Our%20second%20phase%20supervision%20is%20spatially%0Aguided%20by%20a%20novel%20localized%20keypoint%20loss.%20This%20spatial%20guidance%20enables%20user%0Acontrol%20over%20abstracted%20features.%20We%20ensure%20fidelity%20to%20the%20original%20surface%0Athrough%20a%20neural%20SDF%20loss%2C%20which%20allows%20the%20curves%20to%20be%20used%20as%20intuitive%0Adeformation%20handles.%20We%20successfully%20apply%20our%20method%20for%20shape%20abstraction%0Aover%20a%20broad%20dataset%20of%20shapes%20with%20varying%20complexity%2C%20geometric%20structure%2C%0Aand%20texture%2C%20and%20demonstrate%20downstream%20applications%20for%20feature%20control%20and%0Ashape%20deformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04813v2&entry.124074799=Read"},
{"title": "Learn 3D VQA Better with Active Selection and Reannotation", "author": "Shengli Zhou and Yang Liu and Feng Zheng", "abstract": "  3D Visual Question Answering (3D VQA) is crucial for enabling models to\nperceive the physical world and perform spatial reasoning. In 3D VQA, the\nfree-form nature of answers often leads to improper annotations that can\nconfuse or mislead models when training on the entire dataset. While other text\ngeneration tasks can mitigate this issue by learning on large-scale datasets,\nthe scarcity of 3D scene data enlarges the negative effect of misleading\nannotations. Although active learning strategies can select valuable instances\nfor training, they fail to identify and resolve misleading labels, which the\noracle inevitably provides in practice. To address this issue, we propose a\nmulti-turn interactive active learning strategy. This strategy selects data\nbased on models' semantic uncertainty to form a solid knowledge foundation more\neffectively and actively requests reannotation from an oracle to resolve\npotentially misleading labels. For uncertainty assessment, we utilize a\nvariance-based metric that takes semantic relationships between terms into\nconsideration, thus avoiding the uniform inter-class similarity assumption of\nprevious assessment metrics. Extensive experiments exhibit better model\nperformance and a substantial reduction in training costs, with a halving of\ntraining costs for achieving relatively high accuracy. The code is available at\nhttps://github.com/fz-zsl/AQuA.\n", "link": "http://arxiv.org/abs/2507.04630v2", "date": "2025-08-18", "relevancy": 2.2688, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%203D%20VQA%20Better%20with%20Active%20Selection%20and%20Reannotation&body=Title%3A%20Learn%203D%20VQA%20Better%20with%20Active%20Selection%20and%20Reannotation%0AAuthor%3A%20Shengli%20Zhou%20and%20Yang%20Liu%20and%20Feng%20Zheng%0AAbstract%3A%20%20%203D%20Visual%20Question%20Answering%20%283D%20VQA%29%20is%20crucial%20for%20enabling%20models%20to%0Aperceive%20the%20physical%20world%20and%20perform%20spatial%20reasoning.%20In%203D%20VQA%2C%20the%0Afree-form%20nature%20of%20answers%20often%20leads%20to%20improper%20annotations%20that%20can%0Aconfuse%20or%20mislead%20models%20when%20training%20on%20the%20entire%20dataset.%20While%20other%20text%0Ageneration%20tasks%20can%20mitigate%20this%20issue%20by%20learning%20on%20large-scale%20datasets%2C%0Athe%20scarcity%20of%203D%20scene%20data%20enlarges%20the%20negative%20effect%20of%20misleading%0Aannotations.%20Although%20active%20learning%20strategies%20can%20select%20valuable%20instances%0Afor%20training%2C%20they%20fail%20to%20identify%20and%20resolve%20misleading%20labels%2C%20which%20the%0Aoracle%20inevitably%20provides%20in%20practice.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Amulti-turn%20interactive%20active%20learning%20strategy.%20This%20strategy%20selects%20data%0Abased%20on%20models%27%20semantic%20uncertainty%20to%20form%20a%20solid%20knowledge%20foundation%20more%0Aeffectively%20and%20actively%20requests%20reannotation%20from%20an%20oracle%20to%20resolve%0Apotentially%20misleading%20labels.%20For%20uncertainty%20assessment%2C%20we%20utilize%20a%0Avariance-based%20metric%20that%20takes%20semantic%20relationships%20between%20terms%20into%0Aconsideration%2C%20thus%20avoiding%20the%20uniform%20inter-class%20similarity%20assumption%20of%0Aprevious%20assessment%20metrics.%20Extensive%20experiments%20exhibit%20better%20model%0Aperformance%20and%20a%20substantial%20reduction%20in%20training%20costs%2C%20with%20a%20halving%20of%0Atraining%20costs%20for%20achieving%20relatively%20high%20accuracy.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/fz-zsl/AQuA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%25203D%2520VQA%2520Better%2520with%2520Active%2520Selection%2520and%2520Reannotation%26entry.906535625%3DShengli%2520Zhou%2520and%2520Yang%2520Liu%2520and%2520Feng%2520Zheng%26entry.1292438233%3D%2520%25203D%2520Visual%2520Question%2520Answering%2520%25283D%2520VQA%2529%2520is%2520crucial%2520for%2520enabling%2520models%2520to%250Aperceive%2520the%2520physical%2520world%2520and%2520perform%2520spatial%2520reasoning.%2520In%25203D%2520VQA%252C%2520the%250Afree-form%2520nature%2520of%2520answers%2520often%2520leads%2520to%2520improper%2520annotations%2520that%2520can%250Aconfuse%2520or%2520mislead%2520models%2520when%2520training%2520on%2520the%2520entire%2520dataset.%2520While%2520other%2520text%250Ageneration%2520tasks%2520can%2520mitigate%2520this%2520issue%2520by%2520learning%2520on%2520large-scale%2520datasets%252C%250Athe%2520scarcity%2520of%25203D%2520scene%2520data%2520enlarges%2520the%2520negative%2520effect%2520of%2520misleading%250Aannotations.%2520Although%2520active%2520learning%2520strategies%2520can%2520select%2520valuable%2520instances%250Afor%2520training%252C%2520they%2520fail%2520to%2520identify%2520and%2520resolve%2520misleading%2520labels%252C%2520which%2520the%250Aoracle%2520inevitably%2520provides%2520in%2520practice.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Amulti-turn%2520interactive%2520active%2520learning%2520strategy.%2520This%2520strategy%2520selects%2520data%250Abased%2520on%2520models%2527%2520semantic%2520uncertainty%2520to%2520form%2520a%2520solid%2520knowledge%2520foundation%2520more%250Aeffectively%2520and%2520actively%2520requests%2520reannotation%2520from%2520an%2520oracle%2520to%2520resolve%250Apotentially%2520misleading%2520labels.%2520For%2520uncertainty%2520assessment%252C%2520we%2520utilize%2520a%250Avariance-based%2520metric%2520that%2520takes%2520semantic%2520relationships%2520between%2520terms%2520into%250Aconsideration%252C%2520thus%2520avoiding%2520the%2520uniform%2520inter-class%2520similarity%2520assumption%2520of%250Aprevious%2520assessment%2520metrics.%2520Extensive%2520experiments%2520exhibit%2520better%2520model%250Aperformance%2520and%2520a%2520substantial%2520reduction%2520in%2520training%2520costs%252C%2520with%2520a%2520halving%2520of%250Atraining%2520costs%2520for%2520achieving%2520relatively%2520high%2520accuracy.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/fz-zsl/AQuA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%203D%20VQA%20Better%20with%20Active%20Selection%20and%20Reannotation&entry.906535625=Shengli%20Zhou%20and%20Yang%20Liu%20and%20Feng%20Zheng&entry.1292438233=%20%203D%20Visual%20Question%20Answering%20%283D%20VQA%29%20is%20crucial%20for%20enabling%20models%20to%0Aperceive%20the%20physical%20world%20and%20perform%20spatial%20reasoning.%20In%203D%20VQA%2C%20the%0Afree-form%20nature%20of%20answers%20often%20leads%20to%20improper%20annotations%20that%20can%0Aconfuse%20or%20mislead%20models%20when%20training%20on%20the%20entire%20dataset.%20While%20other%20text%0Ageneration%20tasks%20can%20mitigate%20this%20issue%20by%20learning%20on%20large-scale%20datasets%2C%0Athe%20scarcity%20of%203D%20scene%20data%20enlarges%20the%20negative%20effect%20of%20misleading%0Aannotations.%20Although%20active%20learning%20strategies%20can%20select%20valuable%20instances%0Afor%20training%2C%20they%20fail%20to%20identify%20and%20resolve%20misleading%20labels%2C%20which%20the%0Aoracle%20inevitably%20provides%20in%20practice.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Amulti-turn%20interactive%20active%20learning%20strategy.%20This%20strategy%20selects%20data%0Abased%20on%20models%27%20semantic%20uncertainty%20to%20form%20a%20solid%20knowledge%20foundation%20more%0Aeffectively%20and%20actively%20requests%20reannotation%20from%20an%20oracle%20to%20resolve%0Apotentially%20misleading%20labels.%20For%20uncertainty%20assessment%2C%20we%20utilize%20a%0Avariance-based%20metric%20that%20takes%20semantic%20relationships%20between%20terms%20into%0Aconsideration%2C%20thus%20avoiding%20the%20uniform%20inter-class%20similarity%20assumption%20of%0Aprevious%20assessment%20metrics.%20Extensive%20experiments%20exhibit%20better%20model%0Aperformance%20and%20a%20substantial%20reduction%20in%20training%20costs%2C%20with%20a%20halving%20of%0Atraining%20costs%20for%20achieving%20relatively%20high%20accuracy.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/fz-zsl/AQuA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04630v2&entry.124074799=Read"},
{"title": "SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient\n  Cooperative 3-D Perception", "author": "Melih Yazgan and Qiyuan Wu and Iramm Hamdard and Shiqi Li and J. Marius Zoellner", "abstract": "  Collaborative perception allows connected autonomous vehicles (CAVs) to\novercome occlusion and limited sensor range by sharing intermediate features.\nYet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the\nbandwidth available for inter-vehicle communication. We present SlimComm, a\ncommunication-efficient framework that integrates 4D radar Doppler with a\nquery-driven sparse scheme. SlimComm builds a motion-centric dynamic map to\ndistinguish moving from static objects and generates two query types: (i)\nreference queries on dynamic and high-confidence regions, and (ii) exploratory\nqueries probing occluded areas via a two-stage offset. Only query-specific BEV\nfeatures are exchanged and fused through multi-scale gated deformable\nattention, reducing payload while preserving accuracy. For evaluation, we\nrelease OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler\nradar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while\nmatching or surpassing prior baselines across varied traffic densities and\nocclusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.\n", "link": "http://arxiv.org/abs/2508.13007v1", "date": "2025-08-18", "relevancy": 2.2686, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5727}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5703}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlimComm%3A%20Doppler-Guided%20Sparse%20Queries%20for%20Bandwidth-Efficient%0A%20%20Cooperative%203-D%20Perception&body=Title%3A%20SlimComm%3A%20Doppler-Guided%20Sparse%20Queries%20for%20Bandwidth-Efficient%0A%20%20Cooperative%203-D%20Perception%0AAuthor%3A%20Melih%20Yazgan%20and%20Qiyuan%20Wu%20and%20Iramm%20Hamdard%20and%20Shiqi%20Li%20and%20J.%20Marius%20Zoellner%0AAbstract%3A%20%20%20Collaborative%20perception%20allows%20connected%20autonomous%20vehicles%20%28CAVs%29%20to%0Aovercome%20occlusion%20and%20limited%20sensor%20range%20by%20sharing%20intermediate%20features.%0AYet%20transmitting%20dense%20Bird%27s-Eye-View%20%28BEV%29%20feature%20maps%20can%20overwhelm%20the%0Abandwidth%20available%20for%20inter-vehicle%20communication.%20We%20present%20SlimComm%2C%20a%0Acommunication-efficient%20framework%20that%20integrates%204D%20radar%20Doppler%20with%20a%0Aquery-driven%20sparse%20scheme.%20SlimComm%20builds%20a%20motion-centric%20dynamic%20map%20to%0Adistinguish%20moving%20from%20static%20objects%20and%20generates%20two%20query%20types%3A%20%28i%29%0Areference%20queries%20on%20dynamic%20and%20high-confidence%20regions%2C%20and%20%28ii%29%20exploratory%0Aqueries%20probing%20occluded%20areas%20via%20a%20two-stage%20offset.%20Only%20query-specific%20BEV%0Afeatures%20are%20exchanged%20and%20fused%20through%20multi-scale%20gated%20deformable%0Aattention%2C%20reducing%20payload%20while%20preserving%20accuracy.%20For%20evaluation%2C%20we%0Arelease%20OPV2V-R%20and%20Adver-City-R%2C%20CARLA-based%20datasets%20with%20per-point%20Doppler%0Aradar.%20SlimComm%20achieves%20up%20to%2090%25%20lower%20bandwidth%20than%20full-map%20sharing%20while%0Amatching%20or%20surpassing%20prior%20baselines%20across%20varied%20traffic%20densities%20and%0Aocclusions.%20Dataset%20and%20code%20will%20be%20available%20at%3A%20https%3A//url.fzi.de/SlimComm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlimComm%253A%2520Doppler-Guided%2520Sparse%2520Queries%2520for%2520Bandwidth-Efficient%250A%2520%2520Cooperative%25203-D%2520Perception%26entry.906535625%3DMelih%2520Yazgan%2520and%2520Qiyuan%2520Wu%2520and%2520Iramm%2520Hamdard%2520and%2520Shiqi%2520Li%2520and%2520J.%2520Marius%2520Zoellner%26entry.1292438233%3D%2520%2520Collaborative%2520perception%2520allows%2520connected%2520autonomous%2520vehicles%2520%2528CAVs%2529%2520to%250Aovercome%2520occlusion%2520and%2520limited%2520sensor%2520range%2520by%2520sharing%2520intermediate%2520features.%250AYet%2520transmitting%2520dense%2520Bird%2527s-Eye-View%2520%2528BEV%2529%2520feature%2520maps%2520can%2520overwhelm%2520the%250Abandwidth%2520available%2520for%2520inter-vehicle%2520communication.%2520We%2520present%2520SlimComm%252C%2520a%250Acommunication-efficient%2520framework%2520that%2520integrates%25204D%2520radar%2520Doppler%2520with%2520a%250Aquery-driven%2520sparse%2520scheme.%2520SlimComm%2520builds%2520a%2520motion-centric%2520dynamic%2520map%2520to%250Adistinguish%2520moving%2520from%2520static%2520objects%2520and%2520generates%2520two%2520query%2520types%253A%2520%2528i%2529%250Areference%2520queries%2520on%2520dynamic%2520and%2520high-confidence%2520regions%252C%2520and%2520%2528ii%2529%2520exploratory%250Aqueries%2520probing%2520occluded%2520areas%2520via%2520a%2520two-stage%2520offset.%2520Only%2520query-specific%2520BEV%250Afeatures%2520are%2520exchanged%2520and%2520fused%2520through%2520multi-scale%2520gated%2520deformable%250Aattention%252C%2520reducing%2520payload%2520while%2520preserving%2520accuracy.%2520For%2520evaluation%252C%2520we%250Arelease%2520OPV2V-R%2520and%2520Adver-City-R%252C%2520CARLA-based%2520datasets%2520with%2520per-point%2520Doppler%250Aradar.%2520SlimComm%2520achieves%2520up%2520to%252090%2525%2520lower%2520bandwidth%2520than%2520full-map%2520sharing%2520while%250Amatching%2520or%2520surpassing%2520prior%2520baselines%2520across%2520varied%2520traffic%2520densities%2520and%250Aocclusions.%2520Dataset%2520and%2520code%2520will%2520be%2520available%2520at%253A%2520https%253A//url.fzi.de/SlimComm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlimComm%3A%20Doppler-Guided%20Sparse%20Queries%20for%20Bandwidth-Efficient%0A%20%20Cooperative%203-D%20Perception&entry.906535625=Melih%20Yazgan%20and%20Qiyuan%20Wu%20and%20Iramm%20Hamdard%20and%20Shiqi%20Li%20and%20J.%20Marius%20Zoellner&entry.1292438233=%20%20Collaborative%20perception%20allows%20connected%20autonomous%20vehicles%20%28CAVs%29%20to%0Aovercome%20occlusion%20and%20limited%20sensor%20range%20by%20sharing%20intermediate%20features.%0AYet%20transmitting%20dense%20Bird%27s-Eye-View%20%28BEV%29%20feature%20maps%20can%20overwhelm%20the%0Abandwidth%20available%20for%20inter-vehicle%20communication.%20We%20present%20SlimComm%2C%20a%0Acommunication-efficient%20framework%20that%20integrates%204D%20radar%20Doppler%20with%20a%0Aquery-driven%20sparse%20scheme.%20SlimComm%20builds%20a%20motion-centric%20dynamic%20map%20to%0Adistinguish%20moving%20from%20static%20objects%20and%20generates%20two%20query%20types%3A%20%28i%29%0Areference%20queries%20on%20dynamic%20and%20high-confidence%20regions%2C%20and%20%28ii%29%20exploratory%0Aqueries%20probing%20occluded%20areas%20via%20a%20two-stage%20offset.%20Only%20query-specific%20BEV%0Afeatures%20are%20exchanged%20and%20fused%20through%20multi-scale%20gated%20deformable%0Aattention%2C%20reducing%20payload%20while%20preserving%20accuracy.%20For%20evaluation%2C%20we%0Arelease%20OPV2V-R%20and%20Adver-City-R%2C%20CARLA-based%20datasets%20with%20per-point%20Doppler%0Aradar.%20SlimComm%20achieves%20up%20to%2090%25%20lower%20bandwidth%20than%20full-map%20sharing%20while%0Amatching%20or%20surpassing%20prior%20baselines%20across%20varied%20traffic%20densities%20and%0Aocclusions.%20Dataset%20and%20code%20will%20be%20available%20at%3A%20https%3A//url.fzi.de/SlimComm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13007v1&entry.124074799=Read"},
{"title": "Using AI for User Representation: An Analysis of 83 Persona Prompts", "author": "Joni Salminen and Danial Amin and Bernard Jansen", "abstract": "  We analyzed 83 persona prompts from 27 research articles that used large\nlanguage models (LLMs) to generate user personas. Findings show that the\nprompts predominantly generate single personas. Several prompts express a\ndesire for short or concise persona descriptions, which deviates from the\ntradition of creating rich, informative, and rounded persona profiles. Text is\nthe most common format for generated persona attributes, followed by numbers.\nText and numbers are often generated together, and demographic attributes are\nincluded in nearly all generated personas. Researchers use up to 12 prompts in\na single study, though most research uses a small number of prompts. Comparison\nand testing multiple LLMs is rare. More than half of the prompts require the\npersona output in a structured format, such as JSON, and 74% of the prompts\ninsert data or dynamic variables. We discuss the implications of increased use\nof computational personas for user representation.\n", "link": "http://arxiv.org/abs/2508.13047v1", "date": "2025-08-18", "relevancy": 2.2684, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4604}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20AI%20for%20User%20Representation%3A%20An%20Analysis%20of%2083%20Persona%20Prompts&body=Title%3A%20Using%20AI%20for%20User%20Representation%3A%20An%20Analysis%20of%2083%20Persona%20Prompts%0AAuthor%3A%20Joni%20Salminen%20and%20Danial%20Amin%20and%20Bernard%20Jansen%0AAbstract%3A%20%20%20We%20analyzed%2083%20persona%20prompts%20from%2027%20research%20articles%20that%20used%20large%0Alanguage%20models%20%28LLMs%29%20to%20generate%20user%20personas.%20Findings%20show%20that%20the%0Aprompts%20predominantly%20generate%20single%20personas.%20Several%20prompts%20express%20a%0Adesire%20for%20short%20or%20concise%20persona%20descriptions%2C%20which%20deviates%20from%20the%0Atradition%20of%20creating%20rich%2C%20informative%2C%20and%20rounded%20persona%20profiles.%20Text%20is%0Athe%20most%20common%20format%20for%20generated%20persona%20attributes%2C%20followed%20by%20numbers.%0AText%20and%20numbers%20are%20often%20generated%20together%2C%20and%20demographic%20attributes%20are%0Aincluded%20in%20nearly%20all%20generated%20personas.%20Researchers%20use%20up%20to%2012%20prompts%20in%0Aa%20single%20study%2C%20though%20most%20research%20uses%20a%20small%20number%20of%20prompts.%20Comparison%0Aand%20testing%20multiple%20LLMs%20is%20rare.%20More%20than%20half%20of%20the%20prompts%20require%20the%0Apersona%20output%20in%20a%20structured%20format%2C%20such%20as%20JSON%2C%20and%2074%25%20of%20the%20prompts%0Ainsert%20data%20or%20dynamic%20variables.%20We%20discuss%20the%20implications%20of%20increased%20use%0Aof%20computational%20personas%20for%20user%20representation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520AI%2520for%2520User%2520Representation%253A%2520An%2520Analysis%2520of%252083%2520Persona%2520Prompts%26entry.906535625%3DJoni%2520Salminen%2520and%2520Danial%2520Amin%2520and%2520Bernard%2520Jansen%26entry.1292438233%3D%2520%2520We%2520analyzed%252083%2520persona%2520prompts%2520from%252027%2520research%2520articles%2520that%2520used%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520generate%2520user%2520personas.%2520Findings%2520show%2520that%2520the%250Aprompts%2520predominantly%2520generate%2520single%2520personas.%2520Several%2520prompts%2520express%2520a%250Adesire%2520for%2520short%2520or%2520concise%2520persona%2520descriptions%252C%2520which%2520deviates%2520from%2520the%250Atradition%2520of%2520creating%2520rich%252C%2520informative%252C%2520and%2520rounded%2520persona%2520profiles.%2520Text%2520is%250Athe%2520most%2520common%2520format%2520for%2520generated%2520persona%2520attributes%252C%2520followed%2520by%2520numbers.%250AText%2520and%2520numbers%2520are%2520often%2520generated%2520together%252C%2520and%2520demographic%2520attributes%2520are%250Aincluded%2520in%2520nearly%2520all%2520generated%2520personas.%2520Researchers%2520use%2520up%2520to%252012%2520prompts%2520in%250Aa%2520single%2520study%252C%2520though%2520most%2520research%2520uses%2520a%2520small%2520number%2520of%2520prompts.%2520Comparison%250Aand%2520testing%2520multiple%2520LLMs%2520is%2520rare.%2520More%2520than%2520half%2520of%2520the%2520prompts%2520require%2520the%250Apersona%2520output%2520in%2520a%2520structured%2520format%252C%2520such%2520as%2520JSON%252C%2520and%252074%2525%2520of%2520the%2520prompts%250Ainsert%2520data%2520or%2520dynamic%2520variables.%2520We%2520discuss%2520the%2520implications%2520of%2520increased%2520use%250Aof%2520computational%2520personas%2520for%2520user%2520representation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20AI%20for%20User%20Representation%3A%20An%20Analysis%20of%2083%20Persona%20Prompts&entry.906535625=Joni%20Salminen%20and%20Danial%20Amin%20and%20Bernard%20Jansen&entry.1292438233=%20%20We%20analyzed%2083%20persona%20prompts%20from%2027%20research%20articles%20that%20used%20large%0Alanguage%20models%20%28LLMs%29%20to%20generate%20user%20personas.%20Findings%20show%20that%20the%0Aprompts%20predominantly%20generate%20single%20personas.%20Several%20prompts%20express%20a%0Adesire%20for%20short%20or%20concise%20persona%20descriptions%2C%20which%20deviates%20from%20the%0Atradition%20of%20creating%20rich%2C%20informative%2C%20and%20rounded%20persona%20profiles.%20Text%20is%0Athe%20most%20common%20format%20for%20generated%20persona%20attributes%2C%20followed%20by%20numbers.%0AText%20and%20numbers%20are%20often%20generated%20together%2C%20and%20demographic%20attributes%20are%0Aincluded%20in%20nearly%20all%20generated%20personas.%20Researchers%20use%20up%20to%2012%20prompts%20in%0Aa%20single%20study%2C%20though%20most%20research%20uses%20a%20small%20number%20of%20prompts.%20Comparison%0Aand%20testing%20multiple%20LLMs%20is%20rare.%20More%20than%20half%20of%20the%20prompts%20require%20the%0Apersona%20output%20in%20a%20structured%20format%2C%20such%20as%20JSON%2C%20and%2074%25%20of%20the%20prompts%0Ainsert%20data%20or%20dynamic%20variables.%20We%20discuss%20the%20implications%20of%20increased%20use%0Aof%20computational%20personas%20for%20user%20representation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13047v1&entry.124074799=Read"},
{"title": "Multi-source Multimodal Progressive Domain Adaption for Audio-Visual\n  Deception Detection", "author": "Ronghao Lin and Sijie Mai and Ying Zeng and Qiaolin He and Aolin Xiong and Haifeng Hu", "abstract": "  This paper presents the winning approach for the 1st MultiModal Deception\nDetection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing\n(SVC). Aiming at the domain shift issue across source and target domains, we\npropose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)\nframework that transfers the audio-visual knowledge from diverse source domains\nto the target domain. By gradually aligning source and the target domain at\nboth feature and decision levels, our method bridges domain shifts across\ndiverse multimodal datasets. Extensive experiments demonstrate the\neffectiveness of our approach securing Top-2 place. Our approach reaches 60.43%\non accuracy and 56.99\\% on F1-score on competition stage 2, surpassing the 1st\nplace team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.\nOur code is available at https://github.com/RH-Lin/MMPDA.\n", "link": "http://arxiv.org/abs/2508.12842v1", "date": "2025-08-18", "relevancy": 2.2668, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5805}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5577}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-source%20Multimodal%20Progressive%20Domain%20Adaption%20for%20Audio-Visual%0A%20%20Deception%20Detection&body=Title%3A%20Multi-source%20Multimodal%20Progressive%20Domain%20Adaption%20for%20Audio-Visual%0A%20%20Deception%20Detection%0AAuthor%3A%20Ronghao%20Lin%20and%20Sijie%20Mai%20and%20Ying%20Zeng%20and%20Qiaolin%20He%20and%20Aolin%20Xiong%20and%20Haifeng%20Hu%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20winning%20approach%20for%20the%201st%20MultiModal%20Deception%0ADetection%20%28MMDD%29%20Challenge%20at%20the%201st%20Workshop%20on%20Subtle%20Visual%20Computing%0A%28SVC%29.%20Aiming%20at%20the%20domain%20shift%20issue%20across%20source%20and%20target%20domains%2C%20we%0Apropose%20a%20Multi-source%20Multimodal%20Progressive%20Domain%20Adaptation%20%28MMPDA%29%0Aframework%20that%20transfers%20the%20audio-visual%20knowledge%20from%20diverse%20source%20domains%0Ato%20the%20target%20domain.%20By%20gradually%20aligning%20source%20and%20the%20target%20domain%20at%0Aboth%20feature%20and%20decision%20levels%2C%20our%20method%20bridges%20domain%20shifts%20across%0Adiverse%20multimodal%20datasets.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20securing%20Top-2%20place.%20Our%20approach%20reaches%2060.43%25%0Aon%20accuracy%20and%2056.99%5C%25%20on%20F1-score%20on%20competition%20stage%202%2C%20surpassing%20the%201st%0Aplace%20team%20by%205.59%25%20on%20F1-score%20and%20the%203rd%20place%20teams%20by%206.75%25%20on%20accuracy.%0AOur%20code%20is%20available%20at%20https%3A//github.com/RH-Lin/MMPDA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-source%2520Multimodal%2520Progressive%2520Domain%2520Adaption%2520for%2520Audio-Visual%250A%2520%2520Deception%2520Detection%26entry.906535625%3DRonghao%2520Lin%2520and%2520Sijie%2520Mai%2520and%2520Ying%2520Zeng%2520and%2520Qiaolin%2520He%2520and%2520Aolin%2520Xiong%2520and%2520Haifeng%2520Hu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520winning%2520approach%2520for%2520the%25201st%2520MultiModal%2520Deception%250ADetection%2520%2528MMDD%2529%2520Challenge%2520at%2520the%25201st%2520Workshop%2520on%2520Subtle%2520Visual%2520Computing%250A%2528SVC%2529.%2520Aiming%2520at%2520the%2520domain%2520shift%2520issue%2520across%2520source%2520and%2520target%2520domains%252C%2520we%250Apropose%2520a%2520Multi-source%2520Multimodal%2520Progressive%2520Domain%2520Adaptation%2520%2528MMPDA%2529%250Aframework%2520that%2520transfers%2520the%2520audio-visual%2520knowledge%2520from%2520diverse%2520source%2520domains%250Ato%2520the%2520target%2520domain.%2520By%2520gradually%2520aligning%2520source%2520and%2520the%2520target%2520domain%2520at%250Aboth%2520feature%2520and%2520decision%2520levels%252C%2520our%2520method%2520bridges%2520domain%2520shifts%2520across%250Adiverse%2520multimodal%2520datasets.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520securing%2520Top-2%2520place.%2520Our%2520approach%2520reaches%252060.43%2525%250Aon%2520accuracy%2520and%252056.99%255C%2525%2520on%2520F1-score%2520on%2520competition%2520stage%25202%252C%2520surpassing%2520the%25201st%250Aplace%2520team%2520by%25205.59%2525%2520on%2520F1-score%2520and%2520the%25203rd%2520place%2520teams%2520by%25206.75%2525%2520on%2520accuracy.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/RH-Lin/MMPDA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-source%20Multimodal%20Progressive%20Domain%20Adaption%20for%20Audio-Visual%0A%20%20Deception%20Detection&entry.906535625=Ronghao%20Lin%20and%20Sijie%20Mai%20and%20Ying%20Zeng%20and%20Qiaolin%20He%20and%20Aolin%20Xiong%20and%20Haifeng%20Hu&entry.1292438233=%20%20This%20paper%20presents%20the%20winning%20approach%20for%20the%201st%20MultiModal%20Deception%0ADetection%20%28MMDD%29%20Challenge%20at%20the%201st%20Workshop%20on%20Subtle%20Visual%20Computing%0A%28SVC%29.%20Aiming%20at%20the%20domain%20shift%20issue%20across%20source%20and%20target%20domains%2C%20we%0Apropose%20a%20Multi-source%20Multimodal%20Progressive%20Domain%20Adaptation%20%28MMPDA%29%0Aframework%20that%20transfers%20the%20audio-visual%20knowledge%20from%20diverse%20source%20domains%0Ato%20the%20target%20domain.%20By%20gradually%20aligning%20source%20and%20the%20target%20domain%20at%0Aboth%20feature%20and%20decision%20levels%2C%20our%20method%20bridges%20domain%20shifts%20across%0Adiverse%20multimodal%20datasets.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20securing%20Top-2%20place.%20Our%20approach%20reaches%2060.43%25%0Aon%20accuracy%20and%2056.99%5C%25%20on%20F1-score%20on%20competition%20stage%202%2C%20surpassing%20the%201st%0Aplace%20team%20by%205.59%25%20on%20F1-score%20and%20the%203rd%20place%20teams%20by%206.75%25%20on%20accuracy.%0AOur%20code%20is%20available%20at%20https%3A//github.com/RH-Lin/MMPDA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12842v1&entry.124074799=Read"},
{"title": "Efficient Discovery of Motif Transition Process for Large-Scale Temporal\n  Graphs", "author": "Zhiyuan Zheng and Jianpeng Qi and Jiantao Li and Guoqing Chao and Junyu Dong and Yanwei Yu", "abstract": "  Understanding the dynamic transition of motifs in temporal graphs is\nessential for revealing how graph structures evolve over time, identifying\ncritical patterns, and predicting future behaviors, yet existing methods often\nfocus on predefined motifs, limiting their ability to comprehensively capture\ntransitions and interrelationships. We propose a parallel motif transition\nprocess discovery algorithm, PTMT, a novel parallel method for discovering\nmotif transition processes in large-scale temporal graphs. PTMT integrates a\ntree-based framework with the temporal zone partitioning (TZP) strategy, which\npartitions temporal graphs by time and structure while preserving lossless\nmotif transitions and enabling massive parallelism. PTMT comprises three\nphases: growth zone parallel expansion, overlap-aware result aggregation, and\ndeterministic encoding of motif transitions, ensuring accurate tracking of\ndynamic transitions and interactions. Results on 10 real-world datasets\ndemonstrate that PTMT achieves speedups ranging from 12.0$\\times$ to\n50.3$\\times$ compared to the SOTA method.\n", "link": "http://arxiv.org/abs/2504.15979v2", "date": "2025-08-18", "relevancy": 2.2621, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4473}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Discovery%20of%20Motif%20Transition%20Process%20for%20Large-Scale%20Temporal%0A%20%20Graphs&body=Title%3A%20Efficient%20Discovery%20of%20Motif%20Transition%20Process%20for%20Large-Scale%20Temporal%0A%20%20Graphs%0AAuthor%3A%20Zhiyuan%20Zheng%20and%20Jianpeng%20Qi%20and%20Jiantao%20Li%20and%20Guoqing%20Chao%20and%20Junyu%20Dong%20and%20Yanwei%20Yu%0AAbstract%3A%20%20%20Understanding%20the%20dynamic%20transition%20of%20motifs%20in%20temporal%20graphs%20is%0Aessential%20for%20revealing%20how%20graph%20structures%20evolve%20over%20time%2C%20identifying%0Acritical%20patterns%2C%20and%20predicting%20future%20behaviors%2C%20yet%20existing%20methods%20often%0Afocus%20on%20predefined%20motifs%2C%20limiting%20their%20ability%20to%20comprehensively%20capture%0Atransitions%20and%20interrelationships.%20We%20propose%20a%20parallel%20motif%20transition%0Aprocess%20discovery%20algorithm%2C%20PTMT%2C%20a%20novel%20parallel%20method%20for%20discovering%0Amotif%20transition%20processes%20in%20large-scale%20temporal%20graphs.%20PTMT%20integrates%20a%0Atree-based%20framework%20with%20the%20temporal%20zone%20partitioning%20%28TZP%29%20strategy%2C%20which%0Apartitions%20temporal%20graphs%20by%20time%20and%20structure%20while%20preserving%20lossless%0Amotif%20transitions%20and%20enabling%20massive%20parallelism.%20PTMT%20comprises%20three%0Aphases%3A%20growth%20zone%20parallel%20expansion%2C%20overlap-aware%20result%20aggregation%2C%20and%0Adeterministic%20encoding%20of%20motif%20transitions%2C%20ensuring%20accurate%20tracking%20of%0Adynamic%20transitions%20and%20interactions.%20Results%20on%2010%20real-world%20datasets%0Ademonstrate%20that%20PTMT%20achieves%20speedups%20ranging%20from%2012.0%24%5Ctimes%24%20to%0A50.3%24%5Ctimes%24%20compared%20to%20the%20SOTA%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Discovery%2520of%2520Motif%2520Transition%2520Process%2520for%2520Large-Scale%2520Temporal%250A%2520%2520Graphs%26entry.906535625%3DZhiyuan%2520Zheng%2520and%2520Jianpeng%2520Qi%2520and%2520Jiantao%2520Li%2520and%2520Guoqing%2520Chao%2520and%2520Junyu%2520Dong%2520and%2520Yanwei%2520Yu%26entry.1292438233%3D%2520%2520Understanding%2520the%2520dynamic%2520transition%2520of%2520motifs%2520in%2520temporal%2520graphs%2520is%250Aessential%2520for%2520revealing%2520how%2520graph%2520structures%2520evolve%2520over%2520time%252C%2520identifying%250Acritical%2520patterns%252C%2520and%2520predicting%2520future%2520behaviors%252C%2520yet%2520existing%2520methods%2520often%250Afocus%2520on%2520predefined%2520motifs%252C%2520limiting%2520their%2520ability%2520to%2520comprehensively%2520capture%250Atransitions%2520and%2520interrelationships.%2520We%2520propose%2520a%2520parallel%2520motif%2520transition%250Aprocess%2520discovery%2520algorithm%252C%2520PTMT%252C%2520a%2520novel%2520parallel%2520method%2520for%2520discovering%250Amotif%2520transition%2520processes%2520in%2520large-scale%2520temporal%2520graphs.%2520PTMT%2520integrates%2520a%250Atree-based%2520framework%2520with%2520the%2520temporal%2520zone%2520partitioning%2520%2528TZP%2529%2520strategy%252C%2520which%250Apartitions%2520temporal%2520graphs%2520by%2520time%2520and%2520structure%2520while%2520preserving%2520lossless%250Amotif%2520transitions%2520and%2520enabling%2520massive%2520parallelism.%2520PTMT%2520comprises%2520three%250Aphases%253A%2520growth%2520zone%2520parallel%2520expansion%252C%2520overlap-aware%2520result%2520aggregation%252C%2520and%250Adeterministic%2520encoding%2520of%2520motif%2520transitions%252C%2520ensuring%2520accurate%2520tracking%2520of%250Adynamic%2520transitions%2520and%2520interactions.%2520Results%2520on%252010%2520real-world%2520datasets%250Ademonstrate%2520that%2520PTMT%2520achieves%2520speedups%2520ranging%2520from%252012.0%2524%255Ctimes%2524%2520to%250A50.3%2524%255Ctimes%2524%2520compared%2520to%2520the%2520SOTA%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Discovery%20of%20Motif%20Transition%20Process%20for%20Large-Scale%20Temporal%0A%20%20Graphs&entry.906535625=Zhiyuan%20Zheng%20and%20Jianpeng%20Qi%20and%20Jiantao%20Li%20and%20Guoqing%20Chao%20and%20Junyu%20Dong%20and%20Yanwei%20Yu&entry.1292438233=%20%20Understanding%20the%20dynamic%20transition%20of%20motifs%20in%20temporal%20graphs%20is%0Aessential%20for%20revealing%20how%20graph%20structures%20evolve%20over%20time%2C%20identifying%0Acritical%20patterns%2C%20and%20predicting%20future%20behaviors%2C%20yet%20existing%20methods%20often%0Afocus%20on%20predefined%20motifs%2C%20limiting%20their%20ability%20to%20comprehensively%20capture%0Atransitions%20and%20interrelationships.%20We%20propose%20a%20parallel%20motif%20transition%0Aprocess%20discovery%20algorithm%2C%20PTMT%2C%20a%20novel%20parallel%20method%20for%20discovering%0Amotif%20transition%20processes%20in%20large-scale%20temporal%20graphs.%20PTMT%20integrates%20a%0Atree-based%20framework%20with%20the%20temporal%20zone%20partitioning%20%28TZP%29%20strategy%2C%20which%0Apartitions%20temporal%20graphs%20by%20time%20and%20structure%20while%20preserving%20lossless%0Amotif%20transitions%20and%20enabling%20massive%20parallelism.%20PTMT%20comprises%20three%0Aphases%3A%20growth%20zone%20parallel%20expansion%2C%20overlap-aware%20result%20aggregation%2C%20and%0Adeterministic%20encoding%20of%20motif%20transitions%2C%20ensuring%20accurate%20tracking%20of%0Adynamic%20transitions%20and%20interactions.%20Results%20on%2010%20real-world%20datasets%0Ademonstrate%20that%20PTMT%20achieves%20speedups%20ranging%20from%2012.0%24%5Ctimes%24%20to%0A50.3%24%5Ctimes%24%20compared%20to%20the%20SOTA%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15979v2&entry.124074799=Read"},
{"title": "Next Visual Granularity Generation", "author": "Yikai Wang and Zhouxia Wang and Zhonghua Wu and Qingyi Tao and Kang Liao and Chen Change Loy", "abstract": "  We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.\n", "link": "http://arxiv.org/abs/2508.12811v1", "date": "2025-08-18", "relevancy": 2.2469, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5644}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5625}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next%20Visual%20Granularity%20Generation&body=Title%3A%20Next%20Visual%20Granularity%20Generation%0AAuthor%3A%20Yikai%20Wang%20and%20Zhouxia%20Wang%20and%20Zhonghua%20Wu%20and%20Qingyi%20Tao%20and%20Kang%20Liao%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20to%20image%20generation%20by%20decomposing%20an%20image%20into%0Aa%20structured%20sequence%2C%20where%20each%20element%20in%20the%20sequence%20shares%20the%20same%0Aspatial%20resolution%20but%20differs%20in%20the%20number%20of%20unique%20tokens%20used%2C%20capturing%0Adifferent%20level%20of%20visual%20granularity.%20Image%20generation%20is%20carried%20out%20through%0Aour%20newly%20introduced%20Next%20Visual%20Granularity%20%28NVG%29%20generation%20framework%2C%20which%0Agenerates%20a%20visual%20granularity%20sequence%20beginning%20from%20an%20empty%20image%20and%0Aprogressively%20refines%20it%2C%20from%20global%20layout%20to%20fine%20details%2C%20in%20a%20structured%0Amanner.%20This%20iterative%20process%20encodes%20a%20hierarchical%2C%20layered%20representation%0Athat%20offers%20fine-grained%20control%20over%20the%20generation%20process%20across%20multiple%0Agranularity%20levels.%20We%20train%20a%20series%20of%20NVG%20models%20for%20class-conditional%20image%0Ageneration%20on%20the%20ImageNet%20dataset%20and%20observe%20clear%20scaling%20behavior.%20Compared%0Ato%20the%20VAR%20series%2C%20NVG%20consistently%20outperforms%20it%20in%20terms%20of%20FID%20scores%20%283.30%0A-%3E%203.03%2C%202.57%20-%3E2.44%2C%202.09%20-%3E%202.06%29.%20We%20also%20conduct%20extensive%20analysis%20to%0Ashowcase%20the%20capability%20and%20potential%20of%20the%20NVG%20framework.%20Our%20code%20and%20models%0Awill%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext%2520Visual%2520Granularity%2520Generation%26entry.906535625%3DYikai%2520Wang%2520and%2520Zhouxia%2520Wang%2520and%2520Zhonghua%2520Wu%2520and%2520Qingyi%2520Tao%2520and%2520Kang%2520Liao%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520to%2520image%2520generation%2520by%2520decomposing%2520an%2520image%2520into%250Aa%2520structured%2520sequence%252C%2520where%2520each%2520element%2520in%2520the%2520sequence%2520shares%2520the%2520same%250Aspatial%2520resolution%2520but%2520differs%2520in%2520the%2520number%2520of%2520unique%2520tokens%2520used%252C%2520capturing%250Adifferent%2520level%2520of%2520visual%2520granularity.%2520Image%2520generation%2520is%2520carried%2520out%2520through%250Aour%2520newly%2520introduced%2520Next%2520Visual%2520Granularity%2520%2528NVG%2529%2520generation%2520framework%252C%2520which%250Agenerates%2520a%2520visual%2520granularity%2520sequence%2520beginning%2520from%2520an%2520empty%2520image%2520and%250Aprogressively%2520refines%2520it%252C%2520from%2520global%2520layout%2520to%2520fine%2520details%252C%2520in%2520a%2520structured%250Amanner.%2520This%2520iterative%2520process%2520encodes%2520a%2520hierarchical%252C%2520layered%2520representation%250Athat%2520offers%2520fine-grained%2520control%2520over%2520the%2520generation%2520process%2520across%2520multiple%250Agranularity%2520levels.%2520We%2520train%2520a%2520series%2520of%2520NVG%2520models%2520for%2520class-conditional%2520image%250Ageneration%2520on%2520the%2520ImageNet%2520dataset%2520and%2520observe%2520clear%2520scaling%2520behavior.%2520Compared%250Ato%2520the%2520VAR%2520series%252C%2520NVG%2520consistently%2520outperforms%2520it%2520in%2520terms%2520of%2520FID%2520scores%2520%25283.30%250A-%253E%25203.03%252C%25202.57%2520-%253E2.44%252C%25202.09%2520-%253E%25202.06%2529.%2520We%2520also%2520conduct%2520extensive%2520analysis%2520to%250Ashowcase%2520the%2520capability%2520and%2520potential%2520of%2520the%2520NVG%2520framework.%2520Our%2520code%2520and%2520models%250Awill%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next%20Visual%20Granularity%20Generation&entry.906535625=Yikai%20Wang%20and%20Zhouxia%20Wang%20and%20Zhonghua%20Wu%20and%20Qingyi%20Tao%20and%20Kang%20Liao%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20to%20image%20generation%20by%20decomposing%20an%20image%20into%0Aa%20structured%20sequence%2C%20where%20each%20element%20in%20the%20sequence%20shares%20the%20same%0Aspatial%20resolution%20but%20differs%20in%20the%20number%20of%20unique%20tokens%20used%2C%20capturing%0Adifferent%20level%20of%20visual%20granularity.%20Image%20generation%20is%20carried%20out%20through%0Aour%20newly%20introduced%20Next%20Visual%20Granularity%20%28NVG%29%20generation%20framework%2C%20which%0Agenerates%20a%20visual%20granularity%20sequence%20beginning%20from%20an%20empty%20image%20and%0Aprogressively%20refines%20it%2C%20from%20global%20layout%20to%20fine%20details%2C%20in%20a%20structured%0Amanner.%20This%20iterative%20process%20encodes%20a%20hierarchical%2C%20layered%20representation%0Athat%20offers%20fine-grained%20control%20over%20the%20generation%20process%20across%20multiple%0Agranularity%20levels.%20We%20train%20a%20series%20of%20NVG%20models%20for%20class-conditional%20image%0Ageneration%20on%20the%20ImageNet%20dataset%20and%20observe%20clear%20scaling%20behavior.%20Compared%0Ato%20the%20VAR%20series%2C%20NVG%20consistently%20outperforms%20it%20in%20terms%20of%20FID%20scores%20%283.30%0A-%3E%203.03%2C%202.57%20-%3E2.44%2C%202.09%20-%3E%202.06%29.%20We%20also%20conduct%20extensive%20analysis%20to%0Ashowcase%20the%20capability%20and%20potential%20of%20the%20NVG%20framework.%20Our%20code%20and%20models%0Awill%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12811v1&entry.124074799=Read"},
{"title": "Deformation of the panoramic sphere into an ellipsoid to induce\n  self-motion in telepresence users", "author": "Eetu Laukka and Evan G. Center and Timo Ojala and Steven M. LaValle and Matti Pouke", "abstract": "  Mobile telepresence robots allow users to feel present and explore remote\nenvironments using technology. Traditionally, these systems are implemented\nusing a camera onboard a mobile robot that can be controlled. Although\nhigh-immersion technologies, such as 360-degree cameras, can increase\nsituational awareness and presence, they also introduce significant challenges.\nAdditional processing and bandwidth requirements often result in latencies of\nup to seconds. The current delay with a 360-degree camera streaming over the\ninternet makes real-time control of these systems difficult. Working with\nhigh-latency systems requires some form of assistance to the users.\n  This study presents a novel way to utilize optical flow to create an illusion\nof self-motion to the user during the latency period between user sending\nmotion commands to the robot and seeing the actual motion through the\n360-camera stream. We find no significant benefit of using the self-motion\nillusion to performance or accuracy of controlling a telepresence robot with a\nlatency of 500 ms, as measured by the task completion time and collisions into\nobjects. Some evidence is shown that the method might increase virtual reality\n(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We\nconclude that further adjustments are necessary in order to render the method\nviable.\n", "link": "http://arxiv.org/abs/2508.12925v1", "date": "2025-08-18", "relevancy": 2.244, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.569}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.564}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deformation%20of%20the%20panoramic%20sphere%20into%20an%20ellipsoid%20to%20induce%0A%20%20self-motion%20in%20telepresence%20users&body=Title%3A%20Deformation%20of%20the%20panoramic%20sphere%20into%20an%20ellipsoid%20to%20induce%0A%20%20self-motion%20in%20telepresence%20users%0AAuthor%3A%20Eetu%20Laukka%20and%20Evan%20G.%20Center%20and%20Timo%20Ojala%20and%20Steven%20M.%20LaValle%20and%20Matti%20Pouke%0AAbstract%3A%20%20%20Mobile%20telepresence%20robots%20allow%20users%20to%20feel%20present%20and%20explore%20remote%0Aenvironments%20using%20technology.%20Traditionally%2C%20these%20systems%20are%20implemented%0Ausing%20a%20camera%20onboard%20a%20mobile%20robot%20that%20can%20be%20controlled.%20Although%0Ahigh-immersion%20technologies%2C%20such%20as%20360-degree%20cameras%2C%20can%20increase%0Asituational%20awareness%20and%20presence%2C%20they%20also%20introduce%20significant%20challenges.%0AAdditional%20processing%20and%20bandwidth%20requirements%20often%20result%20in%20latencies%20of%0Aup%20to%20seconds.%20The%20current%20delay%20with%20a%20360-degree%20camera%20streaming%20over%20the%0Ainternet%20makes%20real-time%20control%20of%20these%20systems%20difficult.%20Working%20with%0Ahigh-latency%20systems%20requires%20some%20form%20of%20assistance%20to%20the%20users.%0A%20%20This%20study%20presents%20a%20novel%20way%20to%20utilize%20optical%20flow%20to%20create%20an%20illusion%0Aof%20self-motion%20to%20the%20user%20during%20the%20latency%20period%20between%20user%20sending%0Amotion%20commands%20to%20the%20robot%20and%20seeing%20the%20actual%20motion%20through%20the%0A360-camera%20stream.%20We%20find%20no%20significant%20benefit%20of%20using%20the%20self-motion%0Aillusion%20to%20performance%20or%20accuracy%20of%20controlling%20a%20telepresence%20robot%20with%20a%0Alatency%20of%20500%20ms%2C%20as%20measured%20by%20the%20task%20completion%20time%20and%20collisions%20into%0Aobjects.%20Some%20evidence%20is%20shown%20that%20the%20method%20might%20increase%20virtual%20reality%0A%28VR%29%20sickness%2C%20as%20measured%20by%20the%20simulator%20sickness%20questionnaire%20%28SSQ%29.%20We%0Aconclude%20that%20further%20adjustments%20are%20necessary%20in%20order%20to%20render%20the%20method%0Aviable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeformation%2520of%2520the%2520panoramic%2520sphere%2520into%2520an%2520ellipsoid%2520to%2520induce%250A%2520%2520self-motion%2520in%2520telepresence%2520users%26entry.906535625%3DEetu%2520Laukka%2520and%2520Evan%2520G.%2520Center%2520and%2520Timo%2520Ojala%2520and%2520Steven%2520M.%2520LaValle%2520and%2520Matti%2520Pouke%26entry.1292438233%3D%2520%2520Mobile%2520telepresence%2520robots%2520allow%2520users%2520to%2520feel%2520present%2520and%2520explore%2520remote%250Aenvironments%2520using%2520technology.%2520Traditionally%252C%2520these%2520systems%2520are%2520implemented%250Ausing%2520a%2520camera%2520onboard%2520a%2520mobile%2520robot%2520that%2520can%2520be%2520controlled.%2520Although%250Ahigh-immersion%2520technologies%252C%2520such%2520as%2520360-degree%2520cameras%252C%2520can%2520increase%250Asituational%2520awareness%2520and%2520presence%252C%2520they%2520also%2520introduce%2520significant%2520challenges.%250AAdditional%2520processing%2520and%2520bandwidth%2520requirements%2520often%2520result%2520in%2520latencies%2520of%250Aup%2520to%2520seconds.%2520The%2520current%2520delay%2520with%2520a%2520360-degree%2520camera%2520streaming%2520over%2520the%250Ainternet%2520makes%2520real-time%2520control%2520of%2520these%2520systems%2520difficult.%2520Working%2520with%250Ahigh-latency%2520systems%2520requires%2520some%2520form%2520of%2520assistance%2520to%2520the%2520users.%250A%2520%2520This%2520study%2520presents%2520a%2520novel%2520way%2520to%2520utilize%2520optical%2520flow%2520to%2520create%2520an%2520illusion%250Aof%2520self-motion%2520to%2520the%2520user%2520during%2520the%2520latency%2520period%2520between%2520user%2520sending%250Amotion%2520commands%2520to%2520the%2520robot%2520and%2520seeing%2520the%2520actual%2520motion%2520through%2520the%250A360-camera%2520stream.%2520We%2520find%2520no%2520significant%2520benefit%2520of%2520using%2520the%2520self-motion%250Aillusion%2520to%2520performance%2520or%2520accuracy%2520of%2520controlling%2520a%2520telepresence%2520robot%2520with%2520a%250Alatency%2520of%2520500%2520ms%252C%2520as%2520measured%2520by%2520the%2520task%2520completion%2520time%2520and%2520collisions%2520into%250Aobjects.%2520Some%2520evidence%2520is%2520shown%2520that%2520the%2520method%2520might%2520increase%2520virtual%2520reality%250A%2528VR%2529%2520sickness%252C%2520as%2520measured%2520by%2520the%2520simulator%2520sickness%2520questionnaire%2520%2528SSQ%2529.%2520We%250Aconclude%2520that%2520further%2520adjustments%2520are%2520necessary%2520in%2520order%2520to%2520render%2520the%2520method%250Aviable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deformation%20of%20the%20panoramic%20sphere%20into%20an%20ellipsoid%20to%20induce%0A%20%20self-motion%20in%20telepresence%20users&entry.906535625=Eetu%20Laukka%20and%20Evan%20G.%20Center%20and%20Timo%20Ojala%20and%20Steven%20M.%20LaValle%20and%20Matti%20Pouke&entry.1292438233=%20%20Mobile%20telepresence%20robots%20allow%20users%20to%20feel%20present%20and%20explore%20remote%0Aenvironments%20using%20technology.%20Traditionally%2C%20these%20systems%20are%20implemented%0Ausing%20a%20camera%20onboard%20a%20mobile%20robot%20that%20can%20be%20controlled.%20Although%0Ahigh-immersion%20technologies%2C%20such%20as%20360-degree%20cameras%2C%20can%20increase%0Asituational%20awareness%20and%20presence%2C%20they%20also%20introduce%20significant%20challenges.%0AAdditional%20processing%20and%20bandwidth%20requirements%20often%20result%20in%20latencies%20of%0Aup%20to%20seconds.%20The%20current%20delay%20with%20a%20360-degree%20camera%20streaming%20over%20the%0Ainternet%20makes%20real-time%20control%20of%20these%20systems%20difficult.%20Working%20with%0Ahigh-latency%20systems%20requires%20some%20form%20of%20assistance%20to%20the%20users.%0A%20%20This%20study%20presents%20a%20novel%20way%20to%20utilize%20optical%20flow%20to%20create%20an%20illusion%0Aof%20self-motion%20to%20the%20user%20during%20the%20latency%20period%20between%20user%20sending%0Amotion%20commands%20to%20the%20robot%20and%20seeing%20the%20actual%20motion%20through%20the%0A360-camera%20stream.%20We%20find%20no%20significant%20benefit%20of%20using%20the%20self-motion%0Aillusion%20to%20performance%20or%20accuracy%20of%20controlling%20a%20telepresence%20robot%20with%20a%0Alatency%20of%20500%20ms%2C%20as%20measured%20by%20the%20task%20completion%20time%20and%20collisions%20into%0Aobjects.%20Some%20evidence%20is%20shown%20that%20the%20method%20might%20increase%20virtual%20reality%0A%28VR%29%20sickness%2C%20as%20measured%20by%20the%20simulator%20sickness%20questionnaire%20%28SSQ%29.%20We%0Aconclude%20that%20further%20adjustments%20are%20necessary%20in%20order%20to%20render%20the%20method%0Aviable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12925v1&entry.124074799=Read"},
{"title": "Point upsampling networks for single-photon sensing", "author": "Jinyi Liu and Guoyang Zhao and Lijun Liu and Yiguang Hong and Weiping Zhang and Shuming Cheng", "abstract": "  Single-photon sensing has generated great interest as a prominent technique\nof long-distance and ultra-sensitive imaging, however, it tends to yield sparse\nand spatially biased point clouds, thus limiting its practical utility. In this\nwork, we propose using point upsampling networks to increase point density and\nreduce spatial distortion in single-photon point cloud. Particularly, our\nnetwork is built on the state space model which integrates a multi-path\nscanning mechanism to enrich spatial context, a bidirectional Mamba backbone to\ncapture global geometry and local details, and an adaptive upsample shift\nmodule to correct offset-induced distortions. Extensive experiments are\nimplemented on commonly-used datasets to confirm its high reconstruction\naccuracy and strong robustness to the distortion noise, and also on real-world\ndata to demonstrate that our model is able to generate visually consistent,\ndetail-preserving, and noise suppressed point clouds. Our work is the first to\nestablish the upsampling framework for single-photon sensing, and hence opens a\nnew avenue for single-photon sensing and its practical applications in the\ndownstreaming tasks.\n", "link": "http://arxiv.org/abs/2508.12986v1", "date": "2025-08-18", "relevancy": 2.2194, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5617}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5609}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20upsampling%20networks%20for%20single-photon%20sensing&body=Title%3A%20Point%20upsampling%20networks%20for%20single-photon%20sensing%0AAuthor%3A%20Jinyi%20Liu%20and%20Guoyang%20Zhao%20and%20Lijun%20Liu%20and%20Yiguang%20Hong%20and%20Weiping%20Zhang%20and%20Shuming%20Cheng%0AAbstract%3A%20%20%20Single-photon%20sensing%20has%20generated%20great%20interest%20as%20a%20prominent%20technique%0Aof%20long-distance%20and%20ultra-sensitive%20imaging%2C%20however%2C%20it%20tends%20to%20yield%20sparse%0Aand%20spatially%20biased%20point%20clouds%2C%20thus%20limiting%20its%20practical%20utility.%20In%20this%0Awork%2C%20we%20propose%20using%20point%20upsampling%20networks%20to%20increase%20point%20density%20and%0Areduce%20spatial%20distortion%20in%20single-photon%20point%20cloud.%20Particularly%2C%20our%0Anetwork%20is%20built%20on%20the%20state%20space%20model%20which%20integrates%20a%20multi-path%0Ascanning%20mechanism%20to%20enrich%20spatial%20context%2C%20a%20bidirectional%20Mamba%20backbone%20to%0Acapture%20global%20geometry%20and%20local%20details%2C%20and%20an%20adaptive%20upsample%20shift%0Amodule%20to%20correct%20offset-induced%20distortions.%20Extensive%20experiments%20are%0Aimplemented%20on%20commonly-used%20datasets%20to%20confirm%20its%20high%20reconstruction%0Aaccuracy%20and%20strong%20robustness%20to%20the%20distortion%20noise%2C%20and%20also%20on%20real-world%0Adata%20to%20demonstrate%20that%20our%20model%20is%20able%20to%20generate%20visually%20consistent%2C%0Adetail-preserving%2C%20and%20noise%20suppressed%20point%20clouds.%20Our%20work%20is%20the%20first%20to%0Aestablish%20the%20upsampling%20framework%20for%20single-photon%20sensing%2C%20and%20hence%20opens%20a%0Anew%20avenue%20for%20single-photon%20sensing%20and%20its%20practical%20applications%20in%20the%0Adownstreaming%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520upsampling%2520networks%2520for%2520single-photon%2520sensing%26entry.906535625%3DJinyi%2520Liu%2520and%2520Guoyang%2520Zhao%2520and%2520Lijun%2520Liu%2520and%2520Yiguang%2520Hong%2520and%2520Weiping%2520Zhang%2520and%2520Shuming%2520Cheng%26entry.1292438233%3D%2520%2520Single-photon%2520sensing%2520has%2520generated%2520great%2520interest%2520as%2520a%2520prominent%2520technique%250Aof%2520long-distance%2520and%2520ultra-sensitive%2520imaging%252C%2520however%252C%2520it%2520tends%2520to%2520yield%2520sparse%250Aand%2520spatially%2520biased%2520point%2520clouds%252C%2520thus%2520limiting%2520its%2520practical%2520utility.%2520In%2520this%250Awork%252C%2520we%2520propose%2520using%2520point%2520upsampling%2520networks%2520to%2520increase%2520point%2520density%2520and%250Areduce%2520spatial%2520distortion%2520in%2520single-photon%2520point%2520cloud.%2520Particularly%252C%2520our%250Anetwork%2520is%2520built%2520on%2520the%2520state%2520space%2520model%2520which%2520integrates%2520a%2520multi-path%250Ascanning%2520mechanism%2520to%2520enrich%2520spatial%2520context%252C%2520a%2520bidirectional%2520Mamba%2520backbone%2520to%250Acapture%2520global%2520geometry%2520and%2520local%2520details%252C%2520and%2520an%2520adaptive%2520upsample%2520shift%250Amodule%2520to%2520correct%2520offset-induced%2520distortions.%2520Extensive%2520experiments%2520are%250Aimplemented%2520on%2520commonly-used%2520datasets%2520to%2520confirm%2520its%2520high%2520reconstruction%250Aaccuracy%2520and%2520strong%2520robustness%2520to%2520the%2520distortion%2520noise%252C%2520and%2520also%2520on%2520real-world%250Adata%2520to%2520demonstrate%2520that%2520our%2520model%2520is%2520able%2520to%2520generate%2520visually%2520consistent%252C%250Adetail-preserving%252C%2520and%2520noise%2520suppressed%2520point%2520clouds.%2520Our%2520work%2520is%2520the%2520first%2520to%250Aestablish%2520the%2520upsampling%2520framework%2520for%2520single-photon%2520sensing%252C%2520and%2520hence%2520opens%2520a%250Anew%2520avenue%2520for%2520single-photon%2520sensing%2520and%2520its%2520practical%2520applications%2520in%2520the%250Adownstreaming%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20upsampling%20networks%20for%20single-photon%20sensing&entry.906535625=Jinyi%20Liu%20and%20Guoyang%20Zhao%20and%20Lijun%20Liu%20and%20Yiguang%20Hong%20and%20Weiping%20Zhang%20and%20Shuming%20Cheng&entry.1292438233=%20%20Single-photon%20sensing%20has%20generated%20great%20interest%20as%20a%20prominent%20technique%0Aof%20long-distance%20and%20ultra-sensitive%20imaging%2C%20however%2C%20it%20tends%20to%20yield%20sparse%0Aand%20spatially%20biased%20point%20clouds%2C%20thus%20limiting%20its%20practical%20utility.%20In%20this%0Awork%2C%20we%20propose%20using%20point%20upsampling%20networks%20to%20increase%20point%20density%20and%0Areduce%20spatial%20distortion%20in%20single-photon%20point%20cloud.%20Particularly%2C%20our%0Anetwork%20is%20built%20on%20the%20state%20space%20model%20which%20integrates%20a%20multi-path%0Ascanning%20mechanism%20to%20enrich%20spatial%20context%2C%20a%20bidirectional%20Mamba%20backbone%20to%0Acapture%20global%20geometry%20and%20local%20details%2C%20and%20an%20adaptive%20upsample%20shift%0Amodule%20to%20correct%20offset-induced%20distortions.%20Extensive%20experiments%20are%0Aimplemented%20on%20commonly-used%20datasets%20to%20confirm%20its%20high%20reconstruction%0Aaccuracy%20and%20strong%20robustness%20to%20the%20distortion%20noise%2C%20and%20also%20on%20real-world%0Adata%20to%20demonstrate%20that%20our%20model%20is%20able%20to%20generate%20visually%20consistent%2C%0Adetail-preserving%2C%20and%20noise%20suppressed%20point%20clouds.%20Our%20work%20is%20the%20first%20to%0Aestablish%20the%20upsampling%20framework%20for%20single-photon%20sensing%2C%20and%20hence%20opens%20a%0Anew%20avenue%20for%20single-photon%20sensing%20and%20its%20practical%20applications%20in%20the%0Adownstreaming%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12986v1&entry.124074799=Read"},
{"title": "HeroBench: A Benchmark for Long-Horizon Planning and Structured\n  Reasoning in Virtual Worlds", "author": "Petr Anokhin and Roman Khalikov and Stefan Rebrikov and Viktor Volkov and Artyom Sorokin and Vincent Bissonnette", "abstract": "  Large language models (LLMs) have shown remarkable capabilities in isolated\nstep-by-step reasoning tasks such as mathematics and programming, but their\nproficiency in long-horizon planning, where solutions require extended,\nstructured sequences of interdependent actions, remains underexplored. Existing\nbenchmarks typically assess LLMs through abstract or low-dimensional\nalgorithmic tasks, failing to capture the complexity of realistic planning\nenvironments. We introduce HeroBench, a novel benchmark designed specifically\nto evaluate long-horizon planning and structured reasoning within complex\nRPG-inspired virtual worlds. HeroBench provides a rigorously constructed\ndataset of tasks covering a wide range of difficulties, a simulated environment\nto execute and validate agent plans, and detailed analytical tools for\nevaluating model performance. Tasks challenge models to formulate strategic\nplans, efficiently gather resources, master necessary skills, craft equipment,\nand defeat adversaries, reflecting practical scenarios' layered dependencies\nand constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning\nboth open-source and proprietary models, including the GPT-5 family, reveals\nsubstantial performance disparities rarely observed in conventional reasoning\nbenchmarks. Detailed error analysis further uncovers specific weaknesses in\ncurrent models' abilities to generate robust high-level plans and reliably\nexecute structured actions. HeroBench thus not only significantly advances the\nevaluation of LLM reasoning but also provides a flexible, scalable foundation\nfor future research into advanced, autonomous planning in virtual environments.\n", "link": "http://arxiv.org/abs/2508.12782v1", "date": "2025-08-18", "relevancy": 2.1995, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5501}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeroBench%3A%20A%20Benchmark%20for%20Long-Horizon%20Planning%20and%20Structured%0A%20%20Reasoning%20in%20Virtual%20Worlds&body=Title%3A%20HeroBench%3A%20A%20Benchmark%20for%20Long-Horizon%20Planning%20and%20Structured%0A%20%20Reasoning%20in%20Virtual%20Worlds%0AAuthor%3A%20Petr%20Anokhin%20and%20Roman%20Khalikov%20and%20Stefan%20Rebrikov%20and%20Viktor%20Volkov%20and%20Artyom%20Sorokin%20and%20Vincent%20Bissonnette%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20isolated%0Astep-by-step%20reasoning%20tasks%20such%20as%20mathematics%20and%20programming%2C%20but%20their%0Aproficiency%20in%20long-horizon%20planning%2C%20where%20solutions%20require%20extended%2C%0Astructured%20sequences%20of%20interdependent%20actions%2C%20remains%20underexplored.%20Existing%0Abenchmarks%20typically%20assess%20LLMs%20through%20abstract%20or%20low-dimensional%0Aalgorithmic%20tasks%2C%20failing%20to%20capture%20the%20complexity%20of%20realistic%20planning%0Aenvironments.%20We%20introduce%20HeroBench%2C%20a%20novel%20benchmark%20designed%20specifically%0Ato%20evaluate%20long-horizon%20planning%20and%20structured%20reasoning%20within%20complex%0ARPG-inspired%20virtual%20worlds.%20HeroBench%20provides%20a%20rigorously%20constructed%0Adataset%20of%20tasks%20covering%20a%20wide%20range%20of%20difficulties%2C%20a%20simulated%20environment%0Ato%20execute%20and%20validate%20agent%20plans%2C%20and%20detailed%20analytical%20tools%20for%0Aevaluating%20model%20performance.%20Tasks%20challenge%20models%20to%20formulate%20strategic%0Aplans%2C%20efficiently%20gather%20resources%2C%20master%20necessary%20skills%2C%20craft%20equipment%2C%0Aand%20defeat%20adversaries%2C%20reflecting%20practical%20scenarios%27%20layered%20dependencies%0Aand%20constraints.%20Our%20extensive%20evaluation%20of%2025%20state-of-the-art%20LLMs%2C%20spanning%0Aboth%20open-source%20and%20proprietary%20models%2C%20including%20the%20GPT-5%20family%2C%20reveals%0Asubstantial%20performance%20disparities%20rarely%20observed%20in%20conventional%20reasoning%0Abenchmarks.%20Detailed%20error%20analysis%20further%20uncovers%20specific%20weaknesses%20in%0Acurrent%20models%27%20abilities%20to%20generate%20robust%20high-level%20plans%20and%20reliably%0Aexecute%20structured%20actions.%20HeroBench%20thus%20not%20only%20significantly%20advances%20the%0Aevaluation%20of%20LLM%20reasoning%20but%20also%20provides%20a%20flexible%2C%20scalable%20foundation%0Afor%20future%20research%20into%20advanced%2C%20autonomous%20planning%20in%20virtual%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeroBench%253A%2520A%2520Benchmark%2520for%2520Long-Horizon%2520Planning%2520and%2520Structured%250A%2520%2520Reasoning%2520in%2520Virtual%2520Worlds%26entry.906535625%3DPetr%2520Anokhin%2520and%2520Roman%2520Khalikov%2520and%2520Stefan%2520Rebrikov%2520and%2520Viktor%2520Volkov%2520and%2520Artyom%2520Sorokin%2520and%2520Vincent%2520Bissonnette%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520isolated%250Astep-by-step%2520reasoning%2520tasks%2520such%2520as%2520mathematics%2520and%2520programming%252C%2520but%2520their%250Aproficiency%2520in%2520long-horizon%2520planning%252C%2520where%2520solutions%2520require%2520extended%252C%250Astructured%2520sequences%2520of%2520interdependent%2520actions%252C%2520remains%2520underexplored.%2520Existing%250Abenchmarks%2520typically%2520assess%2520LLMs%2520through%2520abstract%2520or%2520low-dimensional%250Aalgorithmic%2520tasks%252C%2520failing%2520to%2520capture%2520the%2520complexity%2520of%2520realistic%2520planning%250Aenvironments.%2520We%2520introduce%2520HeroBench%252C%2520a%2520novel%2520benchmark%2520designed%2520specifically%250Ato%2520evaluate%2520long-horizon%2520planning%2520and%2520structured%2520reasoning%2520within%2520complex%250ARPG-inspired%2520virtual%2520worlds.%2520HeroBench%2520provides%2520a%2520rigorously%2520constructed%250Adataset%2520of%2520tasks%2520covering%2520a%2520wide%2520range%2520of%2520difficulties%252C%2520a%2520simulated%2520environment%250Ato%2520execute%2520and%2520validate%2520agent%2520plans%252C%2520and%2520detailed%2520analytical%2520tools%2520for%250Aevaluating%2520model%2520performance.%2520Tasks%2520challenge%2520models%2520to%2520formulate%2520strategic%250Aplans%252C%2520efficiently%2520gather%2520resources%252C%2520master%2520necessary%2520skills%252C%2520craft%2520equipment%252C%250Aand%2520defeat%2520adversaries%252C%2520reflecting%2520practical%2520scenarios%2527%2520layered%2520dependencies%250Aand%2520constraints.%2520Our%2520extensive%2520evaluation%2520of%252025%2520state-of-the-art%2520LLMs%252C%2520spanning%250Aboth%2520open-source%2520and%2520proprietary%2520models%252C%2520including%2520the%2520GPT-5%2520family%252C%2520reveals%250Asubstantial%2520performance%2520disparities%2520rarely%2520observed%2520in%2520conventional%2520reasoning%250Abenchmarks.%2520Detailed%2520error%2520analysis%2520further%2520uncovers%2520specific%2520weaknesses%2520in%250Acurrent%2520models%2527%2520abilities%2520to%2520generate%2520robust%2520high-level%2520plans%2520and%2520reliably%250Aexecute%2520structured%2520actions.%2520HeroBench%2520thus%2520not%2520only%2520significantly%2520advances%2520the%250Aevaluation%2520of%2520LLM%2520reasoning%2520but%2520also%2520provides%2520a%2520flexible%252C%2520scalable%2520foundation%250Afor%2520future%2520research%2520into%2520advanced%252C%2520autonomous%2520planning%2520in%2520virtual%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeroBench%3A%20A%20Benchmark%20for%20Long-Horizon%20Planning%20and%20Structured%0A%20%20Reasoning%20in%20Virtual%20Worlds&entry.906535625=Petr%20Anokhin%20and%20Roman%20Khalikov%20and%20Stefan%20Rebrikov%20and%20Viktor%20Volkov%20and%20Artyom%20Sorokin%20and%20Vincent%20Bissonnette&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20isolated%0Astep-by-step%20reasoning%20tasks%20such%20as%20mathematics%20and%20programming%2C%20but%20their%0Aproficiency%20in%20long-horizon%20planning%2C%20where%20solutions%20require%20extended%2C%0Astructured%20sequences%20of%20interdependent%20actions%2C%20remains%20underexplored.%20Existing%0Abenchmarks%20typically%20assess%20LLMs%20through%20abstract%20or%20low-dimensional%0Aalgorithmic%20tasks%2C%20failing%20to%20capture%20the%20complexity%20of%20realistic%20planning%0Aenvironments.%20We%20introduce%20HeroBench%2C%20a%20novel%20benchmark%20designed%20specifically%0Ato%20evaluate%20long-horizon%20planning%20and%20structured%20reasoning%20within%20complex%0ARPG-inspired%20virtual%20worlds.%20HeroBench%20provides%20a%20rigorously%20constructed%0Adataset%20of%20tasks%20covering%20a%20wide%20range%20of%20difficulties%2C%20a%20simulated%20environment%0Ato%20execute%20and%20validate%20agent%20plans%2C%20and%20detailed%20analytical%20tools%20for%0Aevaluating%20model%20performance.%20Tasks%20challenge%20models%20to%20formulate%20strategic%0Aplans%2C%20efficiently%20gather%20resources%2C%20master%20necessary%20skills%2C%20craft%20equipment%2C%0Aand%20defeat%20adversaries%2C%20reflecting%20practical%20scenarios%27%20layered%20dependencies%0Aand%20constraints.%20Our%20extensive%20evaluation%20of%2025%20state-of-the-art%20LLMs%2C%20spanning%0Aboth%20open-source%20and%20proprietary%20models%2C%20including%20the%20GPT-5%20family%2C%20reveals%0Asubstantial%20performance%20disparities%20rarely%20observed%20in%20conventional%20reasoning%0Abenchmarks.%20Detailed%20error%20analysis%20further%20uncovers%20specific%20weaknesses%20in%0Acurrent%20models%27%20abilities%20to%20generate%20robust%20high-level%20plans%20and%20reliably%0Aexecute%20structured%20actions.%20HeroBench%20thus%20not%20only%20significantly%20advances%20the%0Aevaluation%20of%20LLM%20reasoning%20but%20also%20provides%20a%20flexible%2C%20scalable%20foundation%0Afor%20future%20research%20into%20advanced%2C%20autonomous%20planning%20in%20virtual%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12782v1&entry.124074799=Read"},
{"title": "Omni Survey for Multimodality Analysis in Visual Object Tracking", "author": "Zhangyong Tang and Tianyang Xu and Xuefeng Zhu and Hui Li and Shaochuan Zhao and Tao Zhou and Chunyang Cheng and Xiaojun Wu and Josef Kittler", "abstract": "  The development of smart cities has led to the generation of massive amounts\nof multi-modal data in the context of a range of tasks that enable a\ncomprehensive monitoring of the smart city infrastructure and services. This\npaper surveys one of the most critical tasks, multi-modal visual object\ntracking (MMVOT), from the perspective of multimodality analysis. Generally,\nMMVOT differs from single-modal tracking in four key aspects, data collection,\nmodality alignment and annotation, model designing, and evaluation.\nAccordingly, we begin with an introduction to the relevant data modalities,\nlaying the groundwork for their integration. This naturally leads to a\ndiscussion of challenges of multi-modal data collection, alignment, and\nannotation. Subsequently, existing MMVOT methods are categorised, based on\ndifferent ways to deal with visible (RGB) and X modalities: programming the\nauxiliary X branch with replicated or non-replicated experimental\nconfigurations from the RGB branch. Here X can be thermal infrared (T), depth\n(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part\nof the paper addresses evaluation and benchmarking. In summary, we undertake an\nomni survey of all aspects of multi-modal visual object tracking (VOT),\ncovering six MMVOT tasks and featuring 338 references in total. In addition, we\ndiscuss the fundamental rhetorical question: Is multi-modal tracking always\nguaranteed to provide a superior solution to unimodal tracking with the help of\ninformation fusion, and if not, in what circumstances its application is\nbeneficial. Furthermore, for the first time in this field, we analyse the\ndistributions of the object categories in the existing MMVOT datasets,\nrevealing their pronounced long-tail nature and a noticeable lack of animal\ncategories when compared with RGB datasets.\n", "link": "http://arxiv.org/abs/2508.13000v1", "date": "2025-08-18", "relevancy": 2.1992, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5647}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni%20Survey%20for%20Multimodality%20Analysis%20in%20Visual%20Object%20Tracking&body=Title%3A%20Omni%20Survey%20for%20Multimodality%20Analysis%20in%20Visual%20Object%20Tracking%0AAuthor%3A%20Zhangyong%20Tang%20and%20Tianyang%20Xu%20and%20Xuefeng%20Zhu%20and%20Hui%20Li%20and%20Shaochuan%20Zhao%20and%20Tao%20Zhou%20and%20Chunyang%20Cheng%20and%20Xiaojun%20Wu%20and%20Josef%20Kittler%0AAbstract%3A%20%20%20The%20development%20of%20smart%20cities%20has%20led%20to%20the%20generation%20of%20massive%20amounts%0Aof%20multi-modal%20data%20in%20the%20context%20of%20a%20range%20of%20tasks%20that%20enable%20a%0Acomprehensive%20monitoring%20of%20the%20smart%20city%20infrastructure%20and%20services.%20This%0Apaper%20surveys%20one%20of%20the%20most%20critical%20tasks%2C%20multi-modal%20visual%20object%0Atracking%20%28MMVOT%29%2C%20from%20the%20perspective%20of%20multimodality%20analysis.%20Generally%2C%0AMMVOT%20differs%20from%20single-modal%20tracking%20in%20four%20key%20aspects%2C%20data%20collection%2C%0Amodality%20alignment%20and%20annotation%2C%20model%20designing%2C%20and%20evaluation.%0AAccordingly%2C%20we%20begin%20with%20an%20introduction%20to%20the%20relevant%20data%20modalities%2C%0Alaying%20the%20groundwork%20for%20their%20integration.%20This%20naturally%20leads%20to%20a%0Adiscussion%20of%20challenges%20of%20multi-modal%20data%20collection%2C%20alignment%2C%20and%0Aannotation.%20Subsequently%2C%20existing%20MMVOT%20methods%20are%20categorised%2C%20based%20on%0Adifferent%20ways%20to%20deal%20with%20visible%20%28RGB%29%20and%20X%20modalities%3A%20programming%20the%0Aauxiliary%20X%20branch%20with%20replicated%20or%20non-replicated%20experimental%0Aconfigurations%20from%20the%20RGB%20branch.%20Here%20X%20can%20be%20thermal%20infrared%20%28T%29%2C%20depth%0A%28D%29%2C%20event%20%28E%29%2C%20near%20infrared%20%28NIR%29%2C%20language%20%28L%29%2C%20or%20sonar%20%28S%29.%20The%20final%20part%0Aof%20the%20paper%20addresses%20evaluation%20and%20benchmarking.%20In%20summary%2C%20we%20undertake%20an%0Aomni%20survey%20of%20all%20aspects%20of%20multi-modal%20visual%20object%20tracking%20%28VOT%29%2C%0Acovering%20six%20MMVOT%20tasks%20and%20featuring%20338%20references%20in%20total.%20In%20addition%2C%20we%0Adiscuss%20the%20fundamental%20rhetorical%20question%3A%20Is%20multi-modal%20tracking%20always%0Aguaranteed%20to%20provide%20a%20superior%20solution%20to%20unimodal%20tracking%20with%20the%20help%20of%0Ainformation%20fusion%2C%20and%20if%20not%2C%20in%20what%20circumstances%20its%20application%20is%0Abeneficial.%20Furthermore%2C%20for%20the%20first%20time%20in%20this%20field%2C%20we%20analyse%20the%0Adistributions%20of%20the%20object%20categories%20in%20the%20existing%20MMVOT%20datasets%2C%0Arevealing%20their%20pronounced%20long-tail%20nature%20and%20a%20noticeable%20lack%20of%20animal%0Acategories%20when%20compared%20with%20RGB%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni%2520Survey%2520for%2520Multimodality%2520Analysis%2520in%2520Visual%2520Object%2520Tracking%26entry.906535625%3DZhangyong%2520Tang%2520and%2520Tianyang%2520Xu%2520and%2520Xuefeng%2520Zhu%2520and%2520Hui%2520Li%2520and%2520Shaochuan%2520Zhao%2520and%2520Tao%2520Zhou%2520and%2520Chunyang%2520Cheng%2520and%2520Xiaojun%2520Wu%2520and%2520Josef%2520Kittler%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520smart%2520cities%2520has%2520led%2520to%2520the%2520generation%2520of%2520massive%2520amounts%250Aof%2520multi-modal%2520data%2520in%2520the%2520context%2520of%2520a%2520range%2520of%2520tasks%2520that%2520enable%2520a%250Acomprehensive%2520monitoring%2520of%2520the%2520smart%2520city%2520infrastructure%2520and%2520services.%2520This%250Apaper%2520surveys%2520one%2520of%2520the%2520most%2520critical%2520tasks%252C%2520multi-modal%2520visual%2520object%250Atracking%2520%2528MMVOT%2529%252C%2520from%2520the%2520perspective%2520of%2520multimodality%2520analysis.%2520Generally%252C%250AMMVOT%2520differs%2520from%2520single-modal%2520tracking%2520in%2520four%2520key%2520aspects%252C%2520data%2520collection%252C%250Amodality%2520alignment%2520and%2520annotation%252C%2520model%2520designing%252C%2520and%2520evaluation.%250AAccordingly%252C%2520we%2520begin%2520with%2520an%2520introduction%2520to%2520the%2520relevant%2520data%2520modalities%252C%250Alaying%2520the%2520groundwork%2520for%2520their%2520integration.%2520This%2520naturally%2520leads%2520to%2520a%250Adiscussion%2520of%2520challenges%2520of%2520multi-modal%2520data%2520collection%252C%2520alignment%252C%2520and%250Aannotation.%2520Subsequently%252C%2520existing%2520MMVOT%2520methods%2520are%2520categorised%252C%2520based%2520on%250Adifferent%2520ways%2520to%2520deal%2520with%2520visible%2520%2528RGB%2529%2520and%2520X%2520modalities%253A%2520programming%2520the%250Aauxiliary%2520X%2520branch%2520with%2520replicated%2520or%2520non-replicated%2520experimental%250Aconfigurations%2520from%2520the%2520RGB%2520branch.%2520Here%2520X%2520can%2520be%2520thermal%2520infrared%2520%2528T%2529%252C%2520depth%250A%2528D%2529%252C%2520event%2520%2528E%2529%252C%2520near%2520infrared%2520%2528NIR%2529%252C%2520language%2520%2528L%2529%252C%2520or%2520sonar%2520%2528S%2529.%2520The%2520final%2520part%250Aof%2520the%2520paper%2520addresses%2520evaluation%2520and%2520benchmarking.%2520In%2520summary%252C%2520we%2520undertake%2520an%250Aomni%2520survey%2520of%2520all%2520aspects%2520of%2520multi-modal%2520visual%2520object%2520tracking%2520%2528VOT%2529%252C%250Acovering%2520six%2520MMVOT%2520tasks%2520and%2520featuring%2520338%2520references%2520in%2520total.%2520In%2520addition%252C%2520we%250Adiscuss%2520the%2520fundamental%2520rhetorical%2520question%253A%2520Is%2520multi-modal%2520tracking%2520always%250Aguaranteed%2520to%2520provide%2520a%2520superior%2520solution%2520to%2520unimodal%2520tracking%2520with%2520the%2520help%2520of%250Ainformation%2520fusion%252C%2520and%2520if%2520not%252C%2520in%2520what%2520circumstances%2520its%2520application%2520is%250Abeneficial.%2520Furthermore%252C%2520for%2520the%2520first%2520time%2520in%2520this%2520field%252C%2520we%2520analyse%2520the%250Adistributions%2520of%2520the%2520object%2520categories%2520in%2520the%2520existing%2520MMVOT%2520datasets%252C%250Arevealing%2520their%2520pronounced%2520long-tail%2520nature%2520and%2520a%2520noticeable%2520lack%2520of%2520animal%250Acategories%2520when%2520compared%2520with%2520RGB%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni%20Survey%20for%20Multimodality%20Analysis%20in%20Visual%20Object%20Tracking&entry.906535625=Zhangyong%20Tang%20and%20Tianyang%20Xu%20and%20Xuefeng%20Zhu%20and%20Hui%20Li%20and%20Shaochuan%20Zhao%20and%20Tao%20Zhou%20and%20Chunyang%20Cheng%20and%20Xiaojun%20Wu%20and%20Josef%20Kittler&entry.1292438233=%20%20The%20development%20of%20smart%20cities%20has%20led%20to%20the%20generation%20of%20massive%20amounts%0Aof%20multi-modal%20data%20in%20the%20context%20of%20a%20range%20of%20tasks%20that%20enable%20a%0Acomprehensive%20monitoring%20of%20the%20smart%20city%20infrastructure%20and%20services.%20This%0Apaper%20surveys%20one%20of%20the%20most%20critical%20tasks%2C%20multi-modal%20visual%20object%0Atracking%20%28MMVOT%29%2C%20from%20the%20perspective%20of%20multimodality%20analysis.%20Generally%2C%0AMMVOT%20differs%20from%20single-modal%20tracking%20in%20four%20key%20aspects%2C%20data%20collection%2C%0Amodality%20alignment%20and%20annotation%2C%20model%20designing%2C%20and%20evaluation.%0AAccordingly%2C%20we%20begin%20with%20an%20introduction%20to%20the%20relevant%20data%20modalities%2C%0Alaying%20the%20groundwork%20for%20their%20integration.%20This%20naturally%20leads%20to%20a%0Adiscussion%20of%20challenges%20of%20multi-modal%20data%20collection%2C%20alignment%2C%20and%0Aannotation.%20Subsequently%2C%20existing%20MMVOT%20methods%20are%20categorised%2C%20based%20on%0Adifferent%20ways%20to%20deal%20with%20visible%20%28RGB%29%20and%20X%20modalities%3A%20programming%20the%0Aauxiliary%20X%20branch%20with%20replicated%20or%20non-replicated%20experimental%0Aconfigurations%20from%20the%20RGB%20branch.%20Here%20X%20can%20be%20thermal%20infrared%20%28T%29%2C%20depth%0A%28D%29%2C%20event%20%28E%29%2C%20near%20infrared%20%28NIR%29%2C%20language%20%28L%29%2C%20or%20sonar%20%28S%29.%20The%20final%20part%0Aof%20the%20paper%20addresses%20evaluation%20and%20benchmarking.%20In%20summary%2C%20we%20undertake%20an%0Aomni%20survey%20of%20all%20aspects%20of%20multi-modal%20visual%20object%20tracking%20%28VOT%29%2C%0Acovering%20six%20MMVOT%20tasks%20and%20featuring%20338%20references%20in%20total.%20In%20addition%2C%20we%0Adiscuss%20the%20fundamental%20rhetorical%20question%3A%20Is%20multi-modal%20tracking%20always%0Aguaranteed%20to%20provide%20a%20superior%20solution%20to%20unimodal%20tracking%20with%20the%20help%20of%0Ainformation%20fusion%2C%20and%20if%20not%2C%20in%20what%20circumstances%20its%20application%20is%0Abeneficial.%20Furthermore%2C%20for%20the%20first%20time%20in%20this%20field%2C%20we%20analyse%20the%0Adistributions%20of%20the%20object%20categories%20in%20the%20existing%20MMVOT%20datasets%2C%0Arevealing%20their%20pronounced%20long-tail%20nature%20and%20a%20noticeable%20lack%20of%20animal%0Acategories%20when%20compared%20with%20RGB%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13000v1&entry.124074799=Read"},
{"title": "A Language-Signal-Vision Multimodal Framework for Multitask Cardiac\n  Analysis", "author": "Yuting Zhang and Tiantian Geng and Luoying Hao and Xinxing Cheng and Alexander Thorley and Xiaoxia Wang and Wenqi Lu and Sandeep S Hothi and Lei Wei and Zhaowen Qiu and Dipak Kotecha and Jinming Duan", "abstract": "  Contemporary cardiovascular management involves complex consideration and\nintegration of multimodal cardiac datasets, where each modality provides\ndistinct but complementary physiological characteristics. While the effective\nintegration of multiple modalities could yield a holistic clinical profile that\naccurately models the true clinical situation with respect to data modalities\nand their relatives weightings, current methodologies remain limited by: 1) the\nscarcity of patient- and time-aligned multimodal data; 2) reliance on isolated\nsingle-modality or rigid multimodal input combinations; 3) alignment strategies\nthat prioritize cross-modal similarity over complementarity; and 4) a narrow\nsingle-task focus. In response to these limitations, a comprehensive multimodal\ndataset was curated for immediate application, integrating laboratory test\nresults, electrocardiograms, and echocardiograms with clinical outcomes.\nSubsequently, a unified framework, Textual Guidance Multimodal fusion for\nMultiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key\ncomponents: 1) a MedFlexFusion module designed to capture the unique and\ncomplementary characteristics of medical modalities and dynamically integrate\ndata from diverse cardiac sources and their combinations; 2) a textual guidance\nmodule to derive task-relevant representations tailored to diverse clinical\nobjectives, including heart disease diagnosis, risk stratification and\ninformation retrieval; and 3) a response module to produce final decisions for\nall these tasks. Furthermore, this study systematically explored key features\nacross multiple modalities and elucidated their synergistic contributions in\nclinical decision-making. Extensive experiments showed that TGMM outperformed\nstate-of-the-art methods across multiple clinical tasks, with additional\nvalidation confirming its robustness on another public dataset.\n", "link": "http://arxiv.org/abs/2508.13072v1", "date": "2025-08-18", "relevancy": 2.1902, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Language-Signal-Vision%20Multimodal%20Framework%20for%20Multitask%20Cardiac%0A%20%20Analysis&body=Title%3A%20A%20Language-Signal-Vision%20Multimodal%20Framework%20for%20Multitask%20Cardiac%0A%20%20Analysis%0AAuthor%3A%20Yuting%20Zhang%20and%20Tiantian%20Geng%20and%20Luoying%20Hao%20and%20Xinxing%20Cheng%20and%20Alexander%20Thorley%20and%20Xiaoxia%20Wang%20and%20Wenqi%20Lu%20and%20Sandeep%20S%20Hothi%20and%20Lei%20Wei%20and%20Zhaowen%20Qiu%20and%20Dipak%20Kotecha%20and%20Jinming%20Duan%0AAbstract%3A%20%20%20Contemporary%20cardiovascular%20management%20involves%20complex%20consideration%20and%0Aintegration%20of%20multimodal%20cardiac%20datasets%2C%20where%20each%20modality%20provides%0Adistinct%20but%20complementary%20physiological%20characteristics.%20While%20the%20effective%0Aintegration%20of%20multiple%20modalities%20could%20yield%20a%20holistic%20clinical%20profile%20that%0Aaccurately%20models%20the%20true%20clinical%20situation%20with%20respect%20to%20data%20modalities%0Aand%20their%20relatives%20weightings%2C%20current%20methodologies%20remain%20limited%20by%3A%201%29%20the%0Ascarcity%20of%20patient-%20and%20time-aligned%20multimodal%20data%3B%202%29%20reliance%20on%20isolated%0Asingle-modality%20or%20rigid%20multimodal%20input%20combinations%3B%203%29%20alignment%20strategies%0Athat%20prioritize%20cross-modal%20similarity%20over%20complementarity%3B%20and%204%29%20a%20narrow%0Asingle-task%20focus.%20In%20response%20to%20these%20limitations%2C%20a%20comprehensive%20multimodal%0Adataset%20was%20curated%20for%20immediate%20application%2C%20integrating%20laboratory%20test%0Aresults%2C%20electrocardiograms%2C%20and%20echocardiograms%20with%20clinical%20outcomes.%0ASubsequently%2C%20a%20unified%20framework%2C%20Textual%20Guidance%20Multimodal%20fusion%20for%0AMultiple%20cardiac%20tasks%20%28TGMM%29%2C%20was%20proposed.%20TGMM%20incorporated%20three%20key%0Acomponents%3A%201%29%20a%20MedFlexFusion%20module%20designed%20to%20capture%20the%20unique%20and%0Acomplementary%20characteristics%20of%20medical%20modalities%20and%20dynamically%20integrate%0Adata%20from%20diverse%20cardiac%20sources%20and%20their%20combinations%3B%202%29%20a%20textual%20guidance%0Amodule%20to%20derive%20task-relevant%20representations%20tailored%20to%20diverse%20clinical%0Aobjectives%2C%20including%20heart%20disease%20diagnosis%2C%20risk%20stratification%20and%0Ainformation%20retrieval%3B%20and%203%29%20a%20response%20module%20to%20produce%20final%20decisions%20for%0Aall%20these%20tasks.%20Furthermore%2C%20this%20study%20systematically%20explored%20key%20features%0Aacross%20multiple%20modalities%20and%20elucidated%20their%20synergistic%20contributions%20in%0Aclinical%20decision-making.%20Extensive%20experiments%20showed%20that%20TGMM%20outperformed%0Astate-of-the-art%20methods%20across%20multiple%20clinical%20tasks%2C%20with%20additional%0Avalidation%20confirming%20its%20robustness%20on%20another%20public%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Language-Signal-Vision%2520Multimodal%2520Framework%2520for%2520Multitask%2520Cardiac%250A%2520%2520Analysis%26entry.906535625%3DYuting%2520Zhang%2520and%2520Tiantian%2520Geng%2520and%2520Luoying%2520Hao%2520and%2520Xinxing%2520Cheng%2520and%2520Alexander%2520Thorley%2520and%2520Xiaoxia%2520Wang%2520and%2520Wenqi%2520Lu%2520and%2520Sandeep%2520S%2520Hothi%2520and%2520Lei%2520Wei%2520and%2520Zhaowen%2520Qiu%2520and%2520Dipak%2520Kotecha%2520and%2520Jinming%2520Duan%26entry.1292438233%3D%2520%2520Contemporary%2520cardiovascular%2520management%2520involves%2520complex%2520consideration%2520and%250Aintegration%2520of%2520multimodal%2520cardiac%2520datasets%252C%2520where%2520each%2520modality%2520provides%250Adistinct%2520but%2520complementary%2520physiological%2520characteristics.%2520While%2520the%2520effective%250Aintegration%2520of%2520multiple%2520modalities%2520could%2520yield%2520a%2520holistic%2520clinical%2520profile%2520that%250Aaccurately%2520models%2520the%2520true%2520clinical%2520situation%2520with%2520respect%2520to%2520data%2520modalities%250Aand%2520their%2520relatives%2520weightings%252C%2520current%2520methodologies%2520remain%2520limited%2520by%253A%25201%2529%2520the%250Ascarcity%2520of%2520patient-%2520and%2520time-aligned%2520multimodal%2520data%253B%25202%2529%2520reliance%2520on%2520isolated%250Asingle-modality%2520or%2520rigid%2520multimodal%2520input%2520combinations%253B%25203%2529%2520alignment%2520strategies%250Athat%2520prioritize%2520cross-modal%2520similarity%2520over%2520complementarity%253B%2520and%25204%2529%2520a%2520narrow%250Asingle-task%2520focus.%2520In%2520response%2520to%2520these%2520limitations%252C%2520a%2520comprehensive%2520multimodal%250Adataset%2520was%2520curated%2520for%2520immediate%2520application%252C%2520integrating%2520laboratory%2520test%250Aresults%252C%2520electrocardiograms%252C%2520and%2520echocardiograms%2520with%2520clinical%2520outcomes.%250ASubsequently%252C%2520a%2520unified%2520framework%252C%2520Textual%2520Guidance%2520Multimodal%2520fusion%2520for%250AMultiple%2520cardiac%2520tasks%2520%2528TGMM%2529%252C%2520was%2520proposed.%2520TGMM%2520incorporated%2520three%2520key%250Acomponents%253A%25201%2529%2520a%2520MedFlexFusion%2520module%2520designed%2520to%2520capture%2520the%2520unique%2520and%250Acomplementary%2520characteristics%2520of%2520medical%2520modalities%2520and%2520dynamically%2520integrate%250Adata%2520from%2520diverse%2520cardiac%2520sources%2520and%2520their%2520combinations%253B%25202%2529%2520a%2520textual%2520guidance%250Amodule%2520to%2520derive%2520task-relevant%2520representations%2520tailored%2520to%2520diverse%2520clinical%250Aobjectives%252C%2520including%2520heart%2520disease%2520diagnosis%252C%2520risk%2520stratification%2520and%250Ainformation%2520retrieval%253B%2520and%25203%2529%2520a%2520response%2520module%2520to%2520produce%2520final%2520decisions%2520for%250Aall%2520these%2520tasks.%2520Furthermore%252C%2520this%2520study%2520systematically%2520explored%2520key%2520features%250Aacross%2520multiple%2520modalities%2520and%2520elucidated%2520their%2520synergistic%2520contributions%2520in%250Aclinical%2520decision-making.%2520Extensive%2520experiments%2520showed%2520that%2520TGMM%2520outperformed%250Astate-of-the-art%2520methods%2520across%2520multiple%2520clinical%2520tasks%252C%2520with%2520additional%250Avalidation%2520confirming%2520its%2520robustness%2520on%2520another%2520public%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Language-Signal-Vision%20Multimodal%20Framework%20for%20Multitask%20Cardiac%0A%20%20Analysis&entry.906535625=Yuting%20Zhang%20and%20Tiantian%20Geng%20and%20Luoying%20Hao%20and%20Xinxing%20Cheng%20and%20Alexander%20Thorley%20and%20Xiaoxia%20Wang%20and%20Wenqi%20Lu%20and%20Sandeep%20S%20Hothi%20and%20Lei%20Wei%20and%20Zhaowen%20Qiu%20and%20Dipak%20Kotecha%20and%20Jinming%20Duan&entry.1292438233=%20%20Contemporary%20cardiovascular%20management%20involves%20complex%20consideration%20and%0Aintegration%20of%20multimodal%20cardiac%20datasets%2C%20where%20each%20modality%20provides%0Adistinct%20but%20complementary%20physiological%20characteristics.%20While%20the%20effective%0Aintegration%20of%20multiple%20modalities%20could%20yield%20a%20holistic%20clinical%20profile%20that%0Aaccurately%20models%20the%20true%20clinical%20situation%20with%20respect%20to%20data%20modalities%0Aand%20their%20relatives%20weightings%2C%20current%20methodologies%20remain%20limited%20by%3A%201%29%20the%0Ascarcity%20of%20patient-%20and%20time-aligned%20multimodal%20data%3B%202%29%20reliance%20on%20isolated%0Asingle-modality%20or%20rigid%20multimodal%20input%20combinations%3B%203%29%20alignment%20strategies%0Athat%20prioritize%20cross-modal%20similarity%20over%20complementarity%3B%20and%204%29%20a%20narrow%0Asingle-task%20focus.%20In%20response%20to%20these%20limitations%2C%20a%20comprehensive%20multimodal%0Adataset%20was%20curated%20for%20immediate%20application%2C%20integrating%20laboratory%20test%0Aresults%2C%20electrocardiograms%2C%20and%20echocardiograms%20with%20clinical%20outcomes.%0ASubsequently%2C%20a%20unified%20framework%2C%20Textual%20Guidance%20Multimodal%20fusion%20for%0AMultiple%20cardiac%20tasks%20%28TGMM%29%2C%20was%20proposed.%20TGMM%20incorporated%20three%20key%0Acomponents%3A%201%29%20a%20MedFlexFusion%20module%20designed%20to%20capture%20the%20unique%20and%0Acomplementary%20characteristics%20of%20medical%20modalities%20and%20dynamically%20integrate%0Adata%20from%20diverse%20cardiac%20sources%20and%20their%20combinations%3B%202%29%20a%20textual%20guidance%0Amodule%20to%20derive%20task-relevant%20representations%20tailored%20to%20diverse%20clinical%0Aobjectives%2C%20including%20heart%20disease%20diagnosis%2C%20risk%20stratification%20and%0Ainformation%20retrieval%3B%20and%203%29%20a%20response%20module%20to%20produce%20final%20decisions%20for%0Aall%20these%20tasks.%20Furthermore%2C%20this%20study%20systematically%20explored%20key%20features%0Aacross%20multiple%20modalities%20and%20elucidated%20their%20synergistic%20contributions%20in%0Aclinical%20decision-making.%20Extensive%20experiments%20showed%20that%20TGMM%20outperformed%0Astate-of-the-art%20methods%20across%20multiple%20clinical%20tasks%2C%20with%20additional%0Avalidation%20confirming%20its%20robustness%20on%20another%20public%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13072v1&entry.124074799=Read"},
{"title": "ONG: One-Shot NMF-based Gradient Masking for Efficient Model\n  Sparsification", "author": "Sankar Behera and Yamuna Prasad", "abstract": "  Deep Neural Networks (DNNs) have achieved remarkable success but their large\nsize poses deployment challenges. While various pruning techniques exist, many\ninvolve complex iterative processes, specialized criteria, or struggle to\nmaintain sparsity effectively during training. We introduce ONG (One-shot\nNMF-based Gradient Masking), a novel sparsification strategy that identifies\nsalient weight structures using Non-negative Matrix Factorization (NMF) for\none-shot pruning at the outset of training. Subsequently, ONG employs a precise\ngradient masking mechanism to ensure that only unpruned weights are updated,\nstrictly preserving the target sparsity throughout the training phase. We\nintegrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10\nand CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable\nsparsification methods. Our experiments demonstrate ONG's ability to achieve\ncomparable or superior performance at various sparsity levels while maintaining\nstructural integrity post-pruning and offering a clear mechanism for targeting\ndesired sparsities.\n", "link": "http://arxiv.org/abs/2508.12891v1", "date": "2025-08-18", "relevancy": 2.1831, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5524}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5446}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ONG%3A%20One-Shot%20NMF-based%20Gradient%20Masking%20for%20Efficient%20Model%0A%20%20Sparsification&body=Title%3A%20ONG%3A%20One-Shot%20NMF-based%20Gradient%20Masking%20for%20Efficient%20Model%0A%20%20Sparsification%0AAuthor%3A%20Sankar%20Behera%20and%20Yamuna%20Prasad%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20achieved%20remarkable%20success%20but%20their%20large%0Asize%20poses%20deployment%20challenges.%20While%20various%20pruning%20techniques%20exist%2C%20many%0Ainvolve%20complex%20iterative%20processes%2C%20specialized%20criteria%2C%20or%20struggle%20to%0Amaintain%20sparsity%20effectively%20during%20training.%20We%20introduce%20ONG%20%28One-shot%0ANMF-based%20Gradient%20Masking%29%2C%20a%20novel%20sparsification%20strategy%20that%20identifies%0Asalient%20weight%20structures%20using%20Non-negative%20Matrix%20Factorization%20%28NMF%29%20for%0Aone-shot%20pruning%20at%20the%20outset%20of%20training.%20Subsequently%2C%20ONG%20employs%20a%20precise%0Agradient%20masking%20mechanism%20to%20ensure%20that%20only%20unpruned%20weights%20are%20updated%2C%0Astrictly%20preserving%20the%20target%20sparsity%20throughout%20the%20training%20phase.%20We%0Aintegrate%20ONG%20into%20the%20BIMP%20comparative%20framework%20and%20evaluate%20it%20on%20CIFAR-10%0Aand%20CIFAR-100%20with%20ResNet56%2C%20ResNet34%2C%20and%20ResNet18%20against%20established%20stable%0Asparsification%20methods.%20Our%20experiments%20demonstrate%20ONG%27s%20ability%20to%20achieve%0Acomparable%20or%20superior%20performance%20at%20various%20sparsity%20levels%20while%20maintaining%0Astructural%20integrity%20post-pruning%20and%20offering%20a%20clear%20mechanism%20for%20targeting%0Adesired%20sparsities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DONG%253A%2520One-Shot%2520NMF-based%2520Gradient%2520Masking%2520for%2520Efficient%2520Model%250A%2520%2520Sparsification%26entry.906535625%3DSankar%2520Behera%2520and%2520Yamuna%2520Prasad%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520have%2520achieved%2520remarkable%2520success%2520but%2520their%2520large%250Asize%2520poses%2520deployment%2520challenges.%2520While%2520various%2520pruning%2520techniques%2520exist%252C%2520many%250Ainvolve%2520complex%2520iterative%2520processes%252C%2520specialized%2520criteria%252C%2520or%2520struggle%2520to%250Amaintain%2520sparsity%2520effectively%2520during%2520training.%2520We%2520introduce%2520ONG%2520%2528One-shot%250ANMF-based%2520Gradient%2520Masking%2529%252C%2520a%2520novel%2520sparsification%2520strategy%2520that%2520identifies%250Asalient%2520weight%2520structures%2520using%2520Non-negative%2520Matrix%2520Factorization%2520%2528NMF%2529%2520for%250Aone-shot%2520pruning%2520at%2520the%2520outset%2520of%2520training.%2520Subsequently%252C%2520ONG%2520employs%2520a%2520precise%250Agradient%2520masking%2520mechanism%2520to%2520ensure%2520that%2520only%2520unpruned%2520weights%2520are%2520updated%252C%250Astrictly%2520preserving%2520the%2520target%2520sparsity%2520throughout%2520the%2520training%2520phase.%2520We%250Aintegrate%2520ONG%2520into%2520the%2520BIMP%2520comparative%2520framework%2520and%2520evaluate%2520it%2520on%2520CIFAR-10%250Aand%2520CIFAR-100%2520with%2520ResNet56%252C%2520ResNet34%252C%2520and%2520ResNet18%2520against%2520established%2520stable%250Asparsification%2520methods.%2520Our%2520experiments%2520demonstrate%2520ONG%2527s%2520ability%2520to%2520achieve%250Acomparable%2520or%2520superior%2520performance%2520at%2520various%2520sparsity%2520levels%2520while%2520maintaining%250Astructural%2520integrity%2520post-pruning%2520and%2520offering%2520a%2520clear%2520mechanism%2520for%2520targeting%250Adesired%2520sparsities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ONG%3A%20One-Shot%20NMF-based%20Gradient%20Masking%20for%20Efficient%20Model%0A%20%20Sparsification&entry.906535625=Sankar%20Behera%20and%20Yamuna%20Prasad&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20achieved%20remarkable%20success%20but%20their%20large%0Asize%20poses%20deployment%20challenges.%20While%20various%20pruning%20techniques%20exist%2C%20many%0Ainvolve%20complex%20iterative%20processes%2C%20specialized%20criteria%2C%20or%20struggle%20to%0Amaintain%20sparsity%20effectively%20during%20training.%20We%20introduce%20ONG%20%28One-shot%0ANMF-based%20Gradient%20Masking%29%2C%20a%20novel%20sparsification%20strategy%20that%20identifies%0Asalient%20weight%20structures%20using%20Non-negative%20Matrix%20Factorization%20%28NMF%29%20for%0Aone-shot%20pruning%20at%20the%20outset%20of%20training.%20Subsequently%2C%20ONG%20employs%20a%20precise%0Agradient%20masking%20mechanism%20to%20ensure%20that%20only%20unpruned%20weights%20are%20updated%2C%0Astrictly%20preserving%20the%20target%20sparsity%20throughout%20the%20training%20phase.%20We%0Aintegrate%20ONG%20into%20the%20BIMP%20comparative%20framework%20and%20evaluate%20it%20on%20CIFAR-10%0Aand%20CIFAR-100%20with%20ResNet56%2C%20ResNet34%2C%20and%20ResNet18%20against%20established%20stable%0Asparsification%20methods.%20Our%20experiments%20demonstrate%20ONG%27s%20ability%20to%20achieve%0Acomparable%20or%20superior%20performance%20at%20various%20sparsity%20levels%20while%20maintaining%0Astructural%20integrity%20post-pruning%20and%20offering%20a%20clear%20mechanism%20for%20targeting%0Adesired%20sparsities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12891v1&entry.124074799=Read"},
{"title": "Morphological classification of eclipsing binary stars using computer\n  vision methods", "author": "\u0160tefan Parimucha and Maksim Gabdeev and Yanna Markus and Martin Va\u0148ko and Pavol Gajdo\u0161", "abstract": "  We present an application of computer vision methods to classify the light\ncurves of eclipsing binaries (EB). We have used pre-trained models based on\nconvolutional neural networks ($\\textit{ResNet50}$) and vision transformers\n($\\textit{vit\\_base\\_patch16\\_224}$), which were fine-tuned on images created\nfrom synthetic datasets. To improve model generalisation and reduce\noverfitting, we developed a novel image representation by transforming\nphase-folded light curves into polar coordinates combined with hexbin\nvisualisation. Our hierarchical approach in the first stage classifies systems\ninto detached and overcontact types, and in the second stage identifies the\npresence or absence of spots. The binary classification models achieved high\naccuracy ($>96\\%$) on validation data across multiple passbands (Gaia~$G$, $I$,\nand $TESS$) and demonstrated strong performance ($>94\\%$, up to $100\\%$ for\n$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and\nWUMaCat catalogues. While the primary binary classification was highly\nsuccessful, the secondary task of automated spot detection performed poorly,\nrevealing a significant limitation of our models for identifying subtle\nphotometric features. This study highlights the potential of computer vision\nfor EB morphological classification in large-scale surveys, but underscores the\nneed for further research into robust, automated spot detection.\n", "link": "http://arxiv.org/abs/2508.12802v1", "date": "2025-08-18", "relevancy": 2.1773, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4417}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4337}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morphological%20classification%20of%20eclipsing%20binary%20stars%20using%20computer%0A%20%20vision%20methods&body=Title%3A%20Morphological%20classification%20of%20eclipsing%20binary%20stars%20using%20computer%0A%20%20vision%20methods%0AAuthor%3A%20%C5%A0tefan%20Parimucha%20and%20Maksim%20Gabdeev%20and%20Yanna%20Markus%20and%20Martin%20Va%C5%88ko%20and%20Pavol%20Gajdo%C5%A1%0AAbstract%3A%20%20%20We%20present%20an%20application%20of%20computer%20vision%20methods%20to%20classify%20the%20light%0Acurves%20of%20eclipsing%20binaries%20%28EB%29.%20We%20have%20used%20pre-trained%20models%20based%20on%0Aconvolutional%20neural%20networks%20%28%24%5Ctextit%7BResNet50%7D%24%29%20and%20vision%20transformers%0A%28%24%5Ctextit%7Bvit%5C_base%5C_patch16%5C_224%7D%24%29%2C%20which%20were%20fine-tuned%20on%20images%20created%0Afrom%20synthetic%20datasets.%20To%20improve%20model%20generalisation%20and%20reduce%0Aoverfitting%2C%20we%20developed%20a%20novel%20image%20representation%20by%20transforming%0Aphase-folded%20light%20curves%20into%20polar%20coordinates%20combined%20with%20hexbin%0Avisualisation.%20Our%20hierarchical%20approach%20in%20the%20first%20stage%20classifies%20systems%0Ainto%20detached%20and%20overcontact%20types%2C%20and%20in%20the%20second%20stage%20identifies%20the%0Apresence%20or%20absence%20of%20spots.%20The%20binary%20classification%20models%20achieved%20high%0Aaccuracy%20%28%24%3E96%5C%25%24%29%20on%20validation%20data%20across%20multiple%20passbands%20%28Gaia~%24G%24%2C%20%24I%24%2C%0Aand%20%24TESS%24%29%20and%20demonstrated%20strong%20performance%20%28%24%3E94%5C%25%24%2C%20up%20to%20%24100%5C%25%24%20for%0A%24TESS%24%29%20when%20tested%20on%20extensive%20observational%20data%20from%20the%20OGLE%2C%20DEBCat%2C%20and%0AWUMaCat%20catalogues.%20While%20the%20primary%20binary%20classification%20was%20highly%0Asuccessful%2C%20the%20secondary%20task%20of%20automated%20spot%20detection%20performed%20poorly%2C%0Arevealing%20a%20significant%20limitation%20of%20our%20models%20for%20identifying%20subtle%0Aphotometric%20features.%20This%20study%20highlights%20the%20potential%20of%20computer%20vision%0Afor%20EB%20morphological%20classification%20in%20large-scale%20surveys%2C%20but%20underscores%20the%0Aneed%20for%20further%20research%20into%20robust%2C%20automated%20spot%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphological%2520classification%2520of%2520eclipsing%2520binary%2520stars%2520using%2520computer%250A%2520%2520vision%2520methods%26entry.906535625%3D%25C5%25A0tefan%2520Parimucha%2520and%2520Maksim%2520Gabdeev%2520and%2520Yanna%2520Markus%2520and%2520Martin%2520Va%25C5%2588ko%2520and%2520Pavol%2520Gajdo%25C5%25A1%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520application%2520of%2520computer%2520vision%2520methods%2520to%2520classify%2520the%2520light%250Acurves%2520of%2520eclipsing%2520binaries%2520%2528EB%2529.%2520We%2520have%2520used%2520pre-trained%2520models%2520based%2520on%250Aconvolutional%2520neural%2520networks%2520%2528%2524%255Ctextit%257BResNet50%257D%2524%2529%2520and%2520vision%2520transformers%250A%2528%2524%255Ctextit%257Bvit%255C_base%255C_patch16%255C_224%257D%2524%2529%252C%2520which%2520were%2520fine-tuned%2520on%2520images%2520created%250Afrom%2520synthetic%2520datasets.%2520To%2520improve%2520model%2520generalisation%2520and%2520reduce%250Aoverfitting%252C%2520we%2520developed%2520a%2520novel%2520image%2520representation%2520by%2520transforming%250Aphase-folded%2520light%2520curves%2520into%2520polar%2520coordinates%2520combined%2520with%2520hexbin%250Avisualisation.%2520Our%2520hierarchical%2520approach%2520in%2520the%2520first%2520stage%2520classifies%2520systems%250Ainto%2520detached%2520and%2520overcontact%2520types%252C%2520and%2520in%2520the%2520second%2520stage%2520identifies%2520the%250Apresence%2520or%2520absence%2520of%2520spots.%2520The%2520binary%2520classification%2520models%2520achieved%2520high%250Aaccuracy%2520%2528%2524%253E96%255C%2525%2524%2529%2520on%2520validation%2520data%2520across%2520multiple%2520passbands%2520%2528Gaia~%2524G%2524%252C%2520%2524I%2524%252C%250Aand%2520%2524TESS%2524%2529%2520and%2520demonstrated%2520strong%2520performance%2520%2528%2524%253E94%255C%2525%2524%252C%2520up%2520to%2520%2524100%255C%2525%2524%2520for%250A%2524TESS%2524%2529%2520when%2520tested%2520on%2520extensive%2520observational%2520data%2520from%2520the%2520OGLE%252C%2520DEBCat%252C%2520and%250AWUMaCat%2520catalogues.%2520While%2520the%2520primary%2520binary%2520classification%2520was%2520highly%250Asuccessful%252C%2520the%2520secondary%2520task%2520of%2520automated%2520spot%2520detection%2520performed%2520poorly%252C%250Arevealing%2520a%2520significant%2520limitation%2520of%2520our%2520models%2520for%2520identifying%2520subtle%250Aphotometric%2520features.%2520This%2520study%2520highlights%2520the%2520potential%2520of%2520computer%2520vision%250Afor%2520EB%2520morphological%2520classification%2520in%2520large-scale%2520surveys%252C%2520but%2520underscores%2520the%250Aneed%2520for%2520further%2520research%2520into%2520robust%252C%2520automated%2520spot%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morphological%20classification%20of%20eclipsing%20binary%20stars%20using%20computer%0A%20%20vision%20methods&entry.906535625=%C5%A0tefan%20Parimucha%20and%20Maksim%20Gabdeev%20and%20Yanna%20Markus%20and%20Martin%20Va%C5%88ko%20and%20Pavol%20Gajdo%C5%A1&entry.1292438233=%20%20We%20present%20an%20application%20of%20computer%20vision%20methods%20to%20classify%20the%20light%0Acurves%20of%20eclipsing%20binaries%20%28EB%29.%20We%20have%20used%20pre-trained%20models%20based%20on%0Aconvolutional%20neural%20networks%20%28%24%5Ctextit%7BResNet50%7D%24%29%20and%20vision%20transformers%0A%28%24%5Ctextit%7Bvit%5C_base%5C_patch16%5C_224%7D%24%29%2C%20which%20were%20fine-tuned%20on%20images%20created%0Afrom%20synthetic%20datasets.%20To%20improve%20model%20generalisation%20and%20reduce%0Aoverfitting%2C%20we%20developed%20a%20novel%20image%20representation%20by%20transforming%0Aphase-folded%20light%20curves%20into%20polar%20coordinates%20combined%20with%20hexbin%0Avisualisation.%20Our%20hierarchical%20approach%20in%20the%20first%20stage%20classifies%20systems%0Ainto%20detached%20and%20overcontact%20types%2C%20and%20in%20the%20second%20stage%20identifies%20the%0Apresence%20or%20absence%20of%20spots.%20The%20binary%20classification%20models%20achieved%20high%0Aaccuracy%20%28%24%3E96%5C%25%24%29%20on%20validation%20data%20across%20multiple%20passbands%20%28Gaia~%24G%24%2C%20%24I%24%2C%0Aand%20%24TESS%24%29%20and%20demonstrated%20strong%20performance%20%28%24%3E94%5C%25%24%2C%20up%20to%20%24100%5C%25%24%20for%0A%24TESS%24%29%20when%20tested%20on%20extensive%20observational%20data%20from%20the%20OGLE%2C%20DEBCat%2C%20and%0AWUMaCat%20catalogues.%20While%20the%20primary%20binary%20classification%20was%20highly%0Asuccessful%2C%20the%20secondary%20task%20of%20automated%20spot%20detection%20performed%20poorly%2C%0Arevealing%20a%20significant%20limitation%20of%20our%20models%20for%20identifying%20subtle%0Aphotometric%20features.%20This%20study%20highlights%20the%20potential%20of%20computer%20vision%0Afor%20EB%20morphological%20classification%20in%20large-scale%20surveys%2C%20but%20underscores%20the%0Aneed%20for%20further%20research%20into%20robust%2C%20automated%20spot%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12802v1&entry.124074799=Read"},
{"title": "Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray\n  Diagnosis and Report Generation", "author": "Tanjim Islam Riju and Shuchismita Anwar and Saman Sarker Joy and Farig Sadeque and Swakkhar Shatabda", "abstract": "  We propose a two-stage multimodal framework that enhances disease\nclassification and region-aware radiology report generation from chest X-rays,\nleveraging the MIMIC-Eye dataset. In the first stage, we introduce a\ngaze-guided contrastive learning architecture for disease classification. It\nintegrates visual features, clinical labels, bounding boxes, and radiologist\neye-tracking signals and is equipped with a novel multi-term gaze-attention\nloss combining MSE, KL divergence, correlation, and center-of-mass alignment.\nIncorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC\nfrom 0.821 to 0.849 (+3.41%), while also improving precision and recall,\nhighlighting the effectiveness of gaze-informed attention supervision. In the\nsecond stage, we present a modular report generation pipeline that extracts\nconfidence-weighted diagnostic keywords, maps them to anatomical regions using\na curated dictionary constructed from domain-specific priors, and generates\nregion-aligned sentences via structured prompts. This pipeline improves report\nquality as measured by clinical keyword recall and ROUGE overlap. Our results\ndemonstrate that integrating gaze data improves both classification performance\nand the interpretability of generated medical reports.\n", "link": "http://arxiv.org/abs/2508.13068v1", "date": "2025-08-18", "relevancy": 2.1748, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5715}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eyes%20on%20the%20Image%3A%20Gaze%20Supervised%20Multimodal%20Learning%20for%20Chest%20X-ray%0A%20%20Diagnosis%20and%20Report%20Generation&body=Title%3A%20Eyes%20on%20the%20Image%3A%20Gaze%20Supervised%20Multimodal%20Learning%20for%20Chest%20X-ray%0A%20%20Diagnosis%20and%20Report%20Generation%0AAuthor%3A%20Tanjim%20Islam%20Riju%20and%20Shuchismita%20Anwar%20and%20Saman%20Sarker%20Joy%20and%20Farig%20Sadeque%20and%20Swakkhar%20Shatabda%0AAbstract%3A%20%20%20We%20propose%20a%20two-stage%20multimodal%20framework%20that%20enhances%20disease%0Aclassification%20and%20region-aware%20radiology%20report%20generation%20from%20chest%20X-rays%2C%0Aleveraging%20the%20MIMIC-Eye%20dataset.%20In%20the%20first%20stage%2C%20we%20introduce%20a%0Agaze-guided%20contrastive%20learning%20architecture%20for%20disease%20classification.%20It%0Aintegrates%20visual%20features%2C%20clinical%20labels%2C%20bounding%20boxes%2C%20and%20radiologist%0Aeye-tracking%20signals%20and%20is%20equipped%20with%20a%20novel%20multi-term%20gaze-attention%0Aloss%20combining%20MSE%2C%20KL%20divergence%2C%20correlation%2C%20and%20center-of-mass%20alignment.%0AIncorporating%20fixations%20improves%20F1%20score%20from%200.597%20to%200.631%20%28%2B5.70%25%29%20and%20AUC%0Afrom%200.821%20to%200.849%20%28%2B3.41%25%29%2C%20while%20also%20improving%20precision%20and%20recall%2C%0Ahighlighting%20the%20effectiveness%20of%20gaze-informed%20attention%20supervision.%20In%20the%0Asecond%20stage%2C%20we%20present%20a%20modular%20report%20generation%20pipeline%20that%20extracts%0Aconfidence-weighted%20diagnostic%20keywords%2C%20maps%20them%20to%20anatomical%20regions%20using%0Aa%20curated%20dictionary%20constructed%20from%20domain-specific%20priors%2C%20and%20generates%0Aregion-aligned%20sentences%20via%20structured%20prompts.%20This%20pipeline%20improves%20report%0Aquality%20as%20measured%20by%20clinical%20keyword%20recall%20and%20ROUGE%20overlap.%20Our%20results%0Ademonstrate%20that%20integrating%20gaze%20data%20improves%20both%20classification%20performance%0Aand%20the%20interpretability%20of%20generated%20medical%20reports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyes%2520on%2520the%2520Image%253A%2520Gaze%2520Supervised%2520Multimodal%2520Learning%2520for%2520Chest%2520X-ray%250A%2520%2520Diagnosis%2520and%2520Report%2520Generation%26entry.906535625%3DTanjim%2520Islam%2520Riju%2520and%2520Shuchismita%2520Anwar%2520and%2520Saman%2520Sarker%2520Joy%2520and%2520Farig%2520Sadeque%2520and%2520Swakkhar%2520Shatabda%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520two-stage%2520multimodal%2520framework%2520that%2520enhances%2520disease%250Aclassification%2520and%2520region-aware%2520radiology%2520report%2520generation%2520from%2520chest%2520X-rays%252C%250Aleveraging%2520the%2520MIMIC-Eye%2520dataset.%2520In%2520the%2520first%2520stage%252C%2520we%2520introduce%2520a%250Agaze-guided%2520contrastive%2520learning%2520architecture%2520for%2520disease%2520classification.%2520It%250Aintegrates%2520visual%2520features%252C%2520clinical%2520labels%252C%2520bounding%2520boxes%252C%2520and%2520radiologist%250Aeye-tracking%2520signals%2520and%2520is%2520equipped%2520with%2520a%2520novel%2520multi-term%2520gaze-attention%250Aloss%2520combining%2520MSE%252C%2520KL%2520divergence%252C%2520correlation%252C%2520and%2520center-of-mass%2520alignment.%250AIncorporating%2520fixations%2520improves%2520F1%2520score%2520from%25200.597%2520to%25200.631%2520%2528%252B5.70%2525%2529%2520and%2520AUC%250Afrom%25200.821%2520to%25200.849%2520%2528%252B3.41%2525%2529%252C%2520while%2520also%2520improving%2520precision%2520and%2520recall%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520gaze-informed%2520attention%2520supervision.%2520In%2520the%250Asecond%2520stage%252C%2520we%2520present%2520a%2520modular%2520report%2520generation%2520pipeline%2520that%2520extracts%250Aconfidence-weighted%2520diagnostic%2520keywords%252C%2520maps%2520them%2520to%2520anatomical%2520regions%2520using%250Aa%2520curated%2520dictionary%2520constructed%2520from%2520domain-specific%2520priors%252C%2520and%2520generates%250Aregion-aligned%2520sentences%2520via%2520structured%2520prompts.%2520This%2520pipeline%2520improves%2520report%250Aquality%2520as%2520measured%2520by%2520clinical%2520keyword%2520recall%2520and%2520ROUGE%2520overlap.%2520Our%2520results%250Ademonstrate%2520that%2520integrating%2520gaze%2520data%2520improves%2520both%2520classification%2520performance%250Aand%2520the%2520interpretability%2520of%2520generated%2520medical%2520reports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eyes%20on%20the%20Image%3A%20Gaze%20Supervised%20Multimodal%20Learning%20for%20Chest%20X-ray%0A%20%20Diagnosis%20and%20Report%20Generation&entry.906535625=Tanjim%20Islam%20Riju%20and%20Shuchismita%20Anwar%20and%20Saman%20Sarker%20Joy%20and%20Farig%20Sadeque%20and%20Swakkhar%20Shatabda&entry.1292438233=%20%20We%20propose%20a%20two-stage%20multimodal%20framework%20that%20enhances%20disease%0Aclassification%20and%20region-aware%20radiology%20report%20generation%20from%20chest%20X-rays%2C%0Aleveraging%20the%20MIMIC-Eye%20dataset.%20In%20the%20first%20stage%2C%20we%20introduce%20a%0Agaze-guided%20contrastive%20learning%20architecture%20for%20disease%20classification.%20It%0Aintegrates%20visual%20features%2C%20clinical%20labels%2C%20bounding%20boxes%2C%20and%20radiologist%0Aeye-tracking%20signals%20and%20is%20equipped%20with%20a%20novel%20multi-term%20gaze-attention%0Aloss%20combining%20MSE%2C%20KL%20divergence%2C%20correlation%2C%20and%20center-of-mass%20alignment.%0AIncorporating%20fixations%20improves%20F1%20score%20from%200.597%20to%200.631%20%28%2B5.70%25%29%20and%20AUC%0Afrom%200.821%20to%200.849%20%28%2B3.41%25%29%2C%20while%20also%20improving%20precision%20and%20recall%2C%0Ahighlighting%20the%20effectiveness%20of%20gaze-informed%20attention%20supervision.%20In%20the%0Asecond%20stage%2C%20we%20present%20a%20modular%20report%20generation%20pipeline%20that%20extracts%0Aconfidence-weighted%20diagnostic%20keywords%2C%20maps%20them%20to%20anatomical%20regions%20using%0Aa%20curated%20dictionary%20constructed%20from%20domain-specific%20priors%2C%20and%20generates%0Aregion-aligned%20sentences%20via%20structured%20prompts.%20This%20pipeline%20improves%20report%0Aquality%20as%20measured%20by%20clinical%20keyword%20recall%20and%20ROUGE%20overlap.%20Our%20results%0Ademonstrate%20that%20integrating%20gaze%20data%20improves%20both%20classification%20performance%0Aand%20the%20interpretability%20of%20generated%20medical%20reports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13068v1&entry.124074799=Read"},
{"title": "DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in\n  Submerged Aquatics", "author": "Shuang Chen and Ronald Thenius and Farshad Arvin and Amir Atapour-Abarghouei", "abstract": "  Continuous and reliable underwater monitoring is essential for assessing\nmarine biodiversity, detecting ecological changes and supporting autonomous\nexploration in aquatic environments. Underwater monitoring platforms rely on\nmainly visual data for marine biodiversity analysis, ecological assessment and\nautonomous exploration. However, underwater environments present significant\nchallenges due to light scattering, absorption and turbidity, which degrade\nimage clarity and distort colour information, which makes accurate observation\ndifficult. To address these challenges, we propose DEEP-SEA, a novel deep\nlearning-based underwater image restoration model to enhance both low- and\nhigh-frequency information while preserving spatial structures. The proposed\nDual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to\nadaptively refine feature representations in frequency domains and\nsimultaneously spatial information for better structural preservation. Our\ncomprehensive experiments on EUVP and LSUI datasets demonstrate the superiority\nover the state of the art in restoring fine-grained image detail and structural\nconsistency. By effectively mitigating underwater visual degradation, DEEP-SEA\nhas the potential to improve the reliability of underwater monitoring platforms\nfor more accurate ecological observation, species identification and autonomous\nnavigation.\n", "link": "http://arxiv.org/abs/2508.12824v1", "date": "2025-08-18", "relevancy": 2.1719, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEEP-SEA%3A%20Deep-Learning%20Enhancement%20for%20Environmental%20Perception%20in%0A%20%20Submerged%20Aquatics&body=Title%3A%20DEEP-SEA%3A%20Deep-Learning%20Enhancement%20for%20Environmental%20Perception%20in%0A%20%20Submerged%20Aquatics%0AAuthor%3A%20Shuang%20Chen%20and%20Ronald%20Thenius%20and%20Farshad%20Arvin%20and%20Amir%20Atapour-Abarghouei%0AAbstract%3A%20%20%20Continuous%20and%20reliable%20underwater%20monitoring%20is%20essential%20for%20assessing%0Amarine%20biodiversity%2C%20detecting%20ecological%20changes%20and%20supporting%20autonomous%0Aexploration%20in%20aquatic%20environments.%20Underwater%20monitoring%20platforms%20rely%20on%0Amainly%20visual%20data%20for%20marine%20biodiversity%20analysis%2C%20ecological%20assessment%20and%0Aautonomous%20exploration.%20However%2C%20underwater%20environments%20present%20significant%0Achallenges%20due%20to%20light%20scattering%2C%20absorption%20and%20turbidity%2C%20which%20degrade%0Aimage%20clarity%20and%20distort%20colour%20information%2C%20which%20makes%20accurate%20observation%0Adifficult.%20To%20address%20these%20challenges%2C%20we%20propose%20DEEP-SEA%2C%20a%20novel%20deep%0Alearning-based%20underwater%20image%20restoration%20model%20to%20enhance%20both%20low-%20and%0Ahigh-frequency%20information%20while%20preserving%20spatial%20structures.%20The%20proposed%0ADual-Frequency%20Enhanced%20Self-Attention%20Spatial%20and%20Frequency%20Modulator%20aims%20to%0Aadaptively%20refine%20feature%20representations%20in%20frequency%20domains%20and%0Asimultaneously%20spatial%20information%20for%20better%20structural%20preservation.%20Our%0Acomprehensive%20experiments%20on%20EUVP%20and%20LSUI%20datasets%20demonstrate%20the%20superiority%0Aover%20the%20state%20of%20the%20art%20in%20restoring%20fine-grained%20image%20detail%20and%20structural%0Aconsistency.%20By%20effectively%20mitigating%20underwater%20visual%20degradation%2C%20DEEP-SEA%0Ahas%20the%20potential%20to%20improve%20the%20reliability%20of%20underwater%20monitoring%20platforms%0Afor%20more%20accurate%20ecological%20observation%2C%20species%20identification%20and%20autonomous%0Anavigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEEP-SEA%253A%2520Deep-Learning%2520Enhancement%2520for%2520Environmental%2520Perception%2520in%250A%2520%2520Submerged%2520Aquatics%26entry.906535625%3DShuang%2520Chen%2520and%2520Ronald%2520Thenius%2520and%2520Farshad%2520Arvin%2520and%2520Amir%2520Atapour-Abarghouei%26entry.1292438233%3D%2520%2520Continuous%2520and%2520reliable%2520underwater%2520monitoring%2520is%2520essential%2520for%2520assessing%250Amarine%2520biodiversity%252C%2520detecting%2520ecological%2520changes%2520and%2520supporting%2520autonomous%250Aexploration%2520in%2520aquatic%2520environments.%2520Underwater%2520monitoring%2520platforms%2520rely%2520on%250Amainly%2520visual%2520data%2520for%2520marine%2520biodiversity%2520analysis%252C%2520ecological%2520assessment%2520and%250Aautonomous%2520exploration.%2520However%252C%2520underwater%2520environments%2520present%2520significant%250Achallenges%2520due%2520to%2520light%2520scattering%252C%2520absorption%2520and%2520turbidity%252C%2520which%2520degrade%250Aimage%2520clarity%2520and%2520distort%2520colour%2520information%252C%2520which%2520makes%2520accurate%2520observation%250Adifficult.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DEEP-SEA%252C%2520a%2520novel%2520deep%250Alearning-based%2520underwater%2520image%2520restoration%2520model%2520to%2520enhance%2520both%2520low-%2520and%250Ahigh-frequency%2520information%2520while%2520preserving%2520spatial%2520structures.%2520The%2520proposed%250ADual-Frequency%2520Enhanced%2520Self-Attention%2520Spatial%2520and%2520Frequency%2520Modulator%2520aims%2520to%250Aadaptively%2520refine%2520feature%2520representations%2520in%2520frequency%2520domains%2520and%250Asimultaneously%2520spatial%2520information%2520for%2520better%2520structural%2520preservation.%2520Our%250Acomprehensive%2520experiments%2520on%2520EUVP%2520and%2520LSUI%2520datasets%2520demonstrate%2520the%2520superiority%250Aover%2520the%2520state%2520of%2520the%2520art%2520in%2520restoring%2520fine-grained%2520image%2520detail%2520and%2520structural%250Aconsistency.%2520By%2520effectively%2520mitigating%2520underwater%2520visual%2520degradation%252C%2520DEEP-SEA%250Ahas%2520the%2520potential%2520to%2520improve%2520the%2520reliability%2520of%2520underwater%2520monitoring%2520platforms%250Afor%2520more%2520accurate%2520ecological%2520observation%252C%2520species%2520identification%2520and%2520autonomous%250Anavigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEEP-SEA%3A%20Deep-Learning%20Enhancement%20for%20Environmental%20Perception%20in%0A%20%20Submerged%20Aquatics&entry.906535625=Shuang%20Chen%20and%20Ronald%20Thenius%20and%20Farshad%20Arvin%20and%20Amir%20Atapour-Abarghouei&entry.1292438233=%20%20Continuous%20and%20reliable%20underwater%20monitoring%20is%20essential%20for%20assessing%0Amarine%20biodiversity%2C%20detecting%20ecological%20changes%20and%20supporting%20autonomous%0Aexploration%20in%20aquatic%20environments.%20Underwater%20monitoring%20platforms%20rely%20on%0Amainly%20visual%20data%20for%20marine%20biodiversity%20analysis%2C%20ecological%20assessment%20and%0Aautonomous%20exploration.%20However%2C%20underwater%20environments%20present%20significant%0Achallenges%20due%20to%20light%20scattering%2C%20absorption%20and%20turbidity%2C%20which%20degrade%0Aimage%20clarity%20and%20distort%20colour%20information%2C%20which%20makes%20accurate%20observation%0Adifficult.%20To%20address%20these%20challenges%2C%20we%20propose%20DEEP-SEA%2C%20a%20novel%20deep%0Alearning-based%20underwater%20image%20restoration%20model%20to%20enhance%20both%20low-%20and%0Ahigh-frequency%20information%20while%20preserving%20spatial%20structures.%20The%20proposed%0ADual-Frequency%20Enhanced%20Self-Attention%20Spatial%20and%20Frequency%20Modulator%20aims%20to%0Aadaptively%20refine%20feature%20representations%20in%20frequency%20domains%20and%0Asimultaneously%20spatial%20information%20for%20better%20structural%20preservation.%20Our%0Acomprehensive%20experiments%20on%20EUVP%20and%20LSUI%20datasets%20demonstrate%20the%20superiority%0Aover%20the%20state%20of%20the%20art%20in%20restoring%20fine-grained%20image%20detail%20and%20structural%0Aconsistency.%20By%20effectively%20mitigating%20underwater%20visual%20degradation%2C%20DEEP-SEA%0Ahas%20the%20potential%20to%20improve%20the%20reliability%20of%20underwater%20monitoring%20platforms%0Afor%20more%20accurate%20ecological%20observation%2C%20species%20identification%20and%20autonomous%0Anavigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12824v1&entry.124074799=Read"},
{"title": "High-Fidelity And Complex Test Data Generation For Real-World SQL Code\n  Generation Services", "author": "Shivasankari Kannan and Yeounoh Chung and Amita Gondi and Tristan Swadell and Fatma Ozcan", "abstract": "  The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically ``meaningful'' mock data for complex\nschema that includes columns with nested structures that we frequently\nencounter in Google SQL code generation workloads. We highlight the limitations\nof existing approaches used in production, particularly their inability to\nhandle large and complex schema, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate realistic high-fidelity test data that\nadheres to complex structural constraints and maintains semantic integrity to\nthe test targets (SQL queries/functions). This approach supports comprehensive\ntesting of complex SQL queries involving joins, aggregations, and even deeply\nnested subqueries, ensuring robust evaluation of SQL code generation services,\nlike NL2SQL and SQL Code Assistant services. Our results demonstrate the\npractical utility of an out-of-the-box LLM (\\textit{gemini}) based test data\ngeneration for industrial SQL code generation services where generating\nrealistic test data is essential due to the frequent unavailability of\nproduction datasets.\n", "link": "http://arxiv.org/abs/2504.17203v2", "date": "2025-08-18", "relevancy": 2.1678, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4386}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4314}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Real-World%20SQL%20Code%0A%20%20Generation%20Services&body=Title%3A%20High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Real-World%20SQL%20Code%0A%20%20Generation%20Services%0AAuthor%3A%20Shivasankari%20Kannan%20and%20Yeounoh%20Chung%20and%20Amita%20Gondi%20and%20Tristan%20Swadell%20and%20Fatma%20Ozcan%0AAbstract%3A%20%20%20The%20demand%20for%20high-fidelity%20test%20data%20is%20paramount%20in%20industrial%20settings%0Awhere%20access%20to%20production%20data%20is%20largely%20restricted.%20Traditional%20data%0Ageneration%20methods%20often%20fall%20short%2C%20struggling%20with%20low-fidelity%20and%20the%0Aability%20to%20model%20complex%20data%20structures%20and%20semantic%20relationships%20that%20are%0Acritical%20for%20testing%20complex%20SQL%20code%20generation%20services%20like%20Natural%20Language%0Ato%20SQL%20%28NL2SQL%29.%20In%20this%20paper%2C%20we%20address%20the%20critical%20need%20for%20generating%0Asyntactically%20correct%20and%20semantically%20%60%60meaningful%27%27%20mock%20data%20for%20complex%0Aschema%20that%20includes%20columns%20with%20nested%20structures%20that%20we%20frequently%0Aencounter%20in%20Google%20SQL%20code%20generation%20workloads.%20We%20highlight%20the%20limitations%0Aof%20existing%20approaches%20used%20in%20production%2C%20particularly%20their%20inability%20to%0Ahandle%20large%20and%20complex%20schema%2C%20as%20well%20as%20the%20lack%20of%20semantically%20coherent%0Atest%20data%20that%20lead%20to%20limited%20test%20coverage.%20We%20demonstrate%20that%20by%20leveraging%0ALarge%20Language%20Models%20%28LLMs%29%20and%20incorporating%20strategic%20pre-%20and%0Apost-processing%20steps%2C%20we%20can%20generate%20realistic%20high-fidelity%20test%20data%20that%0Aadheres%20to%20complex%20structural%20constraints%20and%20maintains%20semantic%20integrity%20to%0Athe%20test%20targets%20%28SQL%20queries/functions%29.%20This%20approach%20supports%20comprehensive%0Atesting%20of%20complex%20SQL%20queries%20involving%20joins%2C%20aggregations%2C%20and%20even%20deeply%0Anested%20subqueries%2C%20ensuring%20robust%20evaluation%20of%20SQL%20code%20generation%20services%2C%0Alike%20NL2SQL%20and%20SQL%20Code%20Assistant%20services.%20Our%20results%20demonstrate%20the%0Apractical%20utility%20of%20an%20out-of-the-box%20LLM%20%28%5Ctextit%7Bgemini%7D%29%20based%20test%20data%0Ageneration%20for%20industrial%20SQL%20code%20generation%20services%20where%20generating%0Arealistic%20test%20data%20is%20essential%20due%20to%20the%20frequent%20unavailability%20of%0Aproduction%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17203v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Fidelity%2520And%2520Complex%2520Test%2520Data%2520Generation%2520For%2520Real-World%2520SQL%2520Code%250A%2520%2520Generation%2520Services%26entry.906535625%3DShivasankari%2520Kannan%2520and%2520Yeounoh%2520Chung%2520and%2520Amita%2520Gondi%2520and%2520Tristan%2520Swadell%2520and%2520Fatma%2520Ozcan%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520high-fidelity%2520test%2520data%2520is%2520paramount%2520in%2520industrial%2520settings%250Awhere%2520access%2520to%2520production%2520data%2520is%2520largely%2520restricted.%2520Traditional%2520data%250Ageneration%2520methods%2520often%2520fall%2520short%252C%2520struggling%2520with%2520low-fidelity%2520and%2520the%250Aability%2520to%2520model%2520complex%2520data%2520structures%2520and%2520semantic%2520relationships%2520that%2520are%250Acritical%2520for%2520testing%2520complex%2520SQL%2520code%2520generation%2520services%2520like%2520Natural%2520Language%250Ato%2520SQL%2520%2528NL2SQL%2529.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520critical%2520need%2520for%2520generating%250Asyntactically%2520correct%2520and%2520semantically%2520%2560%2560meaningful%2527%2527%2520mock%2520data%2520for%2520complex%250Aschema%2520that%2520includes%2520columns%2520with%2520nested%2520structures%2520that%2520we%2520frequently%250Aencounter%2520in%2520Google%2520SQL%2520code%2520generation%2520workloads.%2520We%2520highlight%2520the%2520limitations%250Aof%2520existing%2520approaches%2520used%2520in%2520production%252C%2520particularly%2520their%2520inability%2520to%250Ahandle%2520large%2520and%2520complex%2520schema%252C%2520as%2520well%2520as%2520the%2520lack%2520of%2520semantically%2520coherent%250Atest%2520data%2520that%2520lead%2520to%2520limited%2520test%2520coverage.%2520We%2520demonstrate%2520that%2520by%2520leveraging%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520incorporating%2520strategic%2520pre-%2520and%250Apost-processing%2520steps%252C%2520we%2520can%2520generate%2520realistic%2520high-fidelity%2520test%2520data%2520that%250Aadheres%2520to%2520complex%2520structural%2520constraints%2520and%2520maintains%2520semantic%2520integrity%2520to%250Athe%2520test%2520targets%2520%2528SQL%2520queries/functions%2529.%2520This%2520approach%2520supports%2520comprehensive%250Atesting%2520of%2520complex%2520SQL%2520queries%2520involving%2520joins%252C%2520aggregations%252C%2520and%2520even%2520deeply%250Anested%2520subqueries%252C%2520ensuring%2520robust%2520evaluation%2520of%2520SQL%2520code%2520generation%2520services%252C%250Alike%2520NL2SQL%2520and%2520SQL%2520Code%2520Assistant%2520services.%2520Our%2520results%2520demonstrate%2520the%250Apractical%2520utility%2520of%2520an%2520out-of-the-box%2520LLM%2520%2528%255Ctextit%257Bgemini%257D%2529%2520based%2520test%2520data%250Ageneration%2520for%2520industrial%2520SQL%2520code%2520generation%2520services%2520where%2520generating%250Arealistic%2520test%2520data%2520is%2520essential%2520due%2520to%2520the%2520frequent%2520unavailability%2520of%250Aproduction%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17203v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Fidelity%20And%20Complex%20Test%20Data%20Generation%20For%20Real-World%20SQL%20Code%0A%20%20Generation%20Services&entry.906535625=Shivasankari%20Kannan%20and%20Yeounoh%20Chung%20and%20Amita%20Gondi%20and%20Tristan%20Swadell%20and%20Fatma%20Ozcan&entry.1292438233=%20%20The%20demand%20for%20high-fidelity%20test%20data%20is%20paramount%20in%20industrial%20settings%0Awhere%20access%20to%20production%20data%20is%20largely%20restricted.%20Traditional%20data%0Ageneration%20methods%20often%20fall%20short%2C%20struggling%20with%20low-fidelity%20and%20the%0Aability%20to%20model%20complex%20data%20structures%20and%20semantic%20relationships%20that%20are%0Acritical%20for%20testing%20complex%20SQL%20code%20generation%20services%20like%20Natural%20Language%0Ato%20SQL%20%28NL2SQL%29.%20In%20this%20paper%2C%20we%20address%20the%20critical%20need%20for%20generating%0Asyntactically%20correct%20and%20semantically%20%60%60meaningful%27%27%20mock%20data%20for%20complex%0Aschema%20that%20includes%20columns%20with%20nested%20structures%20that%20we%20frequently%0Aencounter%20in%20Google%20SQL%20code%20generation%20workloads.%20We%20highlight%20the%20limitations%0Aof%20existing%20approaches%20used%20in%20production%2C%20particularly%20their%20inability%20to%0Ahandle%20large%20and%20complex%20schema%2C%20as%20well%20as%20the%20lack%20of%20semantically%20coherent%0Atest%20data%20that%20lead%20to%20limited%20test%20coverage.%20We%20demonstrate%20that%20by%20leveraging%0ALarge%20Language%20Models%20%28LLMs%29%20and%20incorporating%20strategic%20pre-%20and%0Apost-processing%20steps%2C%20we%20can%20generate%20realistic%20high-fidelity%20test%20data%20that%0Aadheres%20to%20complex%20structural%20constraints%20and%20maintains%20semantic%20integrity%20to%0Athe%20test%20targets%20%28SQL%20queries/functions%29.%20This%20approach%20supports%20comprehensive%0Atesting%20of%20complex%20SQL%20queries%20involving%20joins%2C%20aggregations%2C%20and%20even%20deeply%0Anested%20subqueries%2C%20ensuring%20robust%20evaluation%20of%20SQL%20code%20generation%20services%2C%0Alike%20NL2SQL%20and%20SQL%20Code%20Assistant%20services.%20Our%20results%20demonstrate%20the%0Apractical%20utility%20of%20an%20out-of-the-box%20LLM%20%28%5Ctextit%7Bgemini%7D%29%20based%20test%20data%0Ageneration%20for%20industrial%20SQL%20code%20generation%20services%20where%20generating%0Arealistic%20test%20data%20is%20essential%20due%20to%20the%20frequent%20unavailability%20of%0Aproduction%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17203v2&entry.124074799=Read"},
{"title": "CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly\n  Supervised Text-based Person Retrieval", "author": "Xinpeng Zhao and Yanwei Zheng and Chuanlin Lan and Xiaowei Zhang and Bowen Huang and Jibin Yang and Dongxiao Yu", "abstract": "  Weakly supervised text-based person retrieval seeks to retrieve images of a\ntarget person using textual descriptions, without relying on identity\nannotations and is more challenging and practical. The primary challenge is the\nintra-class differences, encompassing intra-modal feature variations and\ncross-modal semantic gaps. Prior works have focused on instance-level samples\nand ignored prototypical features of each person which are intrinsic and\ninvariant. Toward this, we propose a Cross-Modal Prototypical Contrastive\nLearning (CPCL) method. In practice, the CPCL introduces the CLIP model to\nweakly supervised text-based person retrieval to map visual and textual\ninstances into a shared latent space. Subsequently, the proposed Prototypical\nMulti-modal Memory (PMM) module captures associations between heterogeneous\nmodalities of image-text pairs belonging to the same person through the Hybrid\nCross-modal Matching (HCM) module in a many-to-many mapping fashion. Moreover,\nthe Outlier Pseudo Label Mining (OPLM) module further distinguishes valuable\noutlier samples from each modality, enhancing the creation of more reliable\nclusters by mining implicit relationships between image-text pairs. We conduct\nextensive experiments on popular benchmarks of weakly supervised text-based\nperson retrieval, which validate the effectiveness, generalizability of CPCL.\n", "link": "http://arxiv.org/abs/2401.10011v3", "date": "2025-08-18", "relevancy": 2.1449, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5765}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5094}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CPCL%3A%20Cross-Modal%20Prototypical%20Contrastive%20Learning%20for%20Weakly%0A%20%20Supervised%20Text-based%20Person%20Retrieval&body=Title%3A%20CPCL%3A%20Cross-Modal%20Prototypical%20Contrastive%20Learning%20for%20Weakly%0A%20%20Supervised%20Text-based%20Person%20Retrieval%0AAuthor%3A%20Xinpeng%20Zhao%20and%20Yanwei%20Zheng%20and%20Chuanlin%20Lan%20and%20Xiaowei%20Zhang%20and%20Bowen%20Huang%20and%20Jibin%20Yang%20and%20Dongxiao%20Yu%0AAbstract%3A%20%20%20Weakly%20supervised%20text-based%20person%20retrieval%20seeks%20to%20retrieve%20images%20of%20a%0Atarget%20person%20using%20textual%20descriptions%2C%20without%20relying%20on%20identity%0Aannotations%20and%20is%20more%20challenging%20and%20practical.%20The%20primary%20challenge%20is%20the%0Aintra-class%20differences%2C%20encompassing%20intra-modal%20feature%20variations%20and%0Across-modal%20semantic%20gaps.%20Prior%20works%20have%20focused%20on%20instance-level%20samples%0Aand%20ignored%20prototypical%20features%20of%20each%20person%20which%20are%20intrinsic%20and%0Ainvariant.%20Toward%20this%2C%20we%20propose%20a%20Cross-Modal%20Prototypical%20Contrastive%0ALearning%20%28CPCL%29%20method.%20In%20practice%2C%20the%20CPCL%20introduces%20the%20CLIP%20model%20to%0Aweakly%20supervised%20text-based%20person%20retrieval%20to%20map%20visual%20and%20textual%0Ainstances%20into%20a%20shared%20latent%20space.%20Subsequently%2C%20the%20proposed%20Prototypical%0AMulti-modal%20Memory%20%28PMM%29%20module%20captures%20associations%20between%20heterogeneous%0Amodalities%20of%20image-text%20pairs%20belonging%20to%20the%20same%20person%20through%20the%20Hybrid%0ACross-modal%20Matching%20%28HCM%29%20module%20in%20a%20many-to-many%20mapping%20fashion.%20Moreover%2C%0Athe%20Outlier%20Pseudo%20Label%20Mining%20%28OPLM%29%20module%20further%20distinguishes%20valuable%0Aoutlier%20samples%20from%20each%20modality%2C%20enhancing%20the%20creation%20of%20more%20reliable%0Aclusters%20by%20mining%20implicit%20relationships%20between%20image-text%20pairs.%20We%20conduct%0Aextensive%20experiments%20on%20popular%20benchmarks%20of%20weakly%20supervised%20text-based%0Aperson%20retrieval%2C%20which%20validate%20the%20effectiveness%2C%20generalizability%20of%20CPCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10011v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCPCL%253A%2520Cross-Modal%2520Prototypical%2520Contrastive%2520Learning%2520for%2520Weakly%250A%2520%2520Supervised%2520Text-based%2520Person%2520Retrieval%26entry.906535625%3DXinpeng%2520Zhao%2520and%2520Yanwei%2520Zheng%2520and%2520Chuanlin%2520Lan%2520and%2520Xiaowei%2520Zhang%2520and%2520Bowen%2520Huang%2520and%2520Jibin%2520Yang%2520and%2520Dongxiao%2520Yu%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520text-based%2520person%2520retrieval%2520seeks%2520to%2520retrieve%2520images%2520of%2520a%250Atarget%2520person%2520using%2520textual%2520descriptions%252C%2520without%2520relying%2520on%2520identity%250Aannotations%2520and%2520is%2520more%2520challenging%2520and%2520practical.%2520The%2520primary%2520challenge%2520is%2520the%250Aintra-class%2520differences%252C%2520encompassing%2520intra-modal%2520feature%2520variations%2520and%250Across-modal%2520semantic%2520gaps.%2520Prior%2520works%2520have%2520focused%2520on%2520instance-level%2520samples%250Aand%2520ignored%2520prototypical%2520features%2520of%2520each%2520person%2520which%2520are%2520intrinsic%2520and%250Ainvariant.%2520Toward%2520this%252C%2520we%2520propose%2520a%2520Cross-Modal%2520Prototypical%2520Contrastive%250ALearning%2520%2528CPCL%2529%2520method.%2520In%2520practice%252C%2520the%2520CPCL%2520introduces%2520the%2520CLIP%2520model%2520to%250Aweakly%2520supervised%2520text-based%2520person%2520retrieval%2520to%2520map%2520visual%2520and%2520textual%250Ainstances%2520into%2520a%2520shared%2520latent%2520space.%2520Subsequently%252C%2520the%2520proposed%2520Prototypical%250AMulti-modal%2520Memory%2520%2528PMM%2529%2520module%2520captures%2520associations%2520between%2520heterogeneous%250Amodalities%2520of%2520image-text%2520pairs%2520belonging%2520to%2520the%2520same%2520person%2520through%2520the%2520Hybrid%250ACross-modal%2520Matching%2520%2528HCM%2529%2520module%2520in%2520a%2520many-to-many%2520mapping%2520fashion.%2520Moreover%252C%250Athe%2520Outlier%2520Pseudo%2520Label%2520Mining%2520%2528OPLM%2529%2520module%2520further%2520distinguishes%2520valuable%250Aoutlier%2520samples%2520from%2520each%2520modality%252C%2520enhancing%2520the%2520creation%2520of%2520more%2520reliable%250Aclusters%2520by%2520mining%2520implicit%2520relationships%2520between%2520image-text%2520pairs.%2520We%2520conduct%250Aextensive%2520experiments%2520on%2520popular%2520benchmarks%2520of%2520weakly%2520supervised%2520text-based%250Aperson%2520retrieval%252C%2520which%2520validate%2520the%2520effectiveness%252C%2520generalizability%2520of%2520CPCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10011v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CPCL%3A%20Cross-Modal%20Prototypical%20Contrastive%20Learning%20for%20Weakly%0A%20%20Supervised%20Text-based%20Person%20Retrieval&entry.906535625=Xinpeng%20Zhao%20and%20Yanwei%20Zheng%20and%20Chuanlin%20Lan%20and%20Xiaowei%20Zhang%20and%20Bowen%20Huang%20and%20Jibin%20Yang%20and%20Dongxiao%20Yu&entry.1292438233=%20%20Weakly%20supervised%20text-based%20person%20retrieval%20seeks%20to%20retrieve%20images%20of%20a%0Atarget%20person%20using%20textual%20descriptions%2C%20without%20relying%20on%20identity%0Aannotations%20and%20is%20more%20challenging%20and%20practical.%20The%20primary%20challenge%20is%20the%0Aintra-class%20differences%2C%20encompassing%20intra-modal%20feature%20variations%20and%0Across-modal%20semantic%20gaps.%20Prior%20works%20have%20focused%20on%20instance-level%20samples%0Aand%20ignored%20prototypical%20features%20of%20each%20person%20which%20are%20intrinsic%20and%0Ainvariant.%20Toward%20this%2C%20we%20propose%20a%20Cross-Modal%20Prototypical%20Contrastive%0ALearning%20%28CPCL%29%20method.%20In%20practice%2C%20the%20CPCL%20introduces%20the%20CLIP%20model%20to%0Aweakly%20supervised%20text-based%20person%20retrieval%20to%20map%20visual%20and%20textual%0Ainstances%20into%20a%20shared%20latent%20space.%20Subsequently%2C%20the%20proposed%20Prototypical%0AMulti-modal%20Memory%20%28PMM%29%20module%20captures%20associations%20between%20heterogeneous%0Amodalities%20of%20image-text%20pairs%20belonging%20to%20the%20same%20person%20through%20the%20Hybrid%0ACross-modal%20Matching%20%28HCM%29%20module%20in%20a%20many-to-many%20mapping%20fashion.%20Moreover%2C%0Athe%20Outlier%20Pseudo%20Label%20Mining%20%28OPLM%29%20module%20further%20distinguishes%20valuable%0Aoutlier%20samples%20from%20each%20modality%2C%20enhancing%20the%20creation%20of%20more%20reliable%0Aclusters%20by%20mining%20implicit%20relationships%20between%20image-text%20pairs.%20We%20conduct%0Aextensive%20experiments%20on%20popular%20benchmarks%20of%20weakly%20supervised%20text-based%0Aperson%20retrieval%2C%20which%20validate%20the%20effectiveness%2C%20generalizability%20of%20CPCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10011v3&entry.124074799=Read"},
{"title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study", "author": "Zhongang Cai and Yubo Wang and Qingping Sun and Ruisi Wang and Chenyang Gu and Wanqi Yin and Zhiqian Lin and Zhitao Yang and Chen Wei and Xuanke Shi and Kewang Deng and Xiaoyang Han and Zukai Chen and Jiaqi Li and Xiangyu Fan and Hanming Deng and Lewei Lu and Bo Li and Ziwei Liu and Quan Wang and Dahua Lin and Lei Yang", "abstract": "  Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.\n", "link": "http://arxiv.org/abs/2508.13142v1", "date": "2025-08-18", "relevancy": 2.1338, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Has%20GPT-5%20Achieved%20Spatial%20Intelligence%3F%20An%20Empirical%20Study&body=Title%3A%20Has%20GPT-5%20Achieved%20Spatial%20Intelligence%3F%20An%20Empirical%20Study%0AAuthor%3A%20Zhongang%20Cai%20and%20Yubo%20Wang%20and%20Qingping%20Sun%20and%20Ruisi%20Wang%20and%20Chenyang%20Gu%20and%20Wanqi%20Yin%20and%20Zhiqian%20Lin%20and%20Zhitao%20Yang%20and%20Chen%20Wei%20and%20Xuanke%20Shi%20and%20Kewang%20Deng%20and%20Xiaoyang%20Han%20and%20Zukai%20Chen%20and%20Jiaqi%20Li%20and%20Xiangyu%20Fan%20and%20Hanming%20Deng%20and%20Lewei%20Lu%20and%20Bo%20Li%20and%20Ziwei%20Liu%20and%20Quan%20Wang%20and%20Dahua%20Lin%20and%20Lei%20Yang%0AAbstract%3A%20%20%20Multi-modal%20models%20have%20achieved%20remarkable%20progress%20in%20recent%20years.%0ANevertheless%2C%20they%20continue%20to%20exhibit%20notable%20limitations%20in%20spatial%0Aunderstanding%20and%20reasoning%2C%20which%20are%20fundamental%20capabilities%20to%20achieving%0Aartificial%20general%20intelligence.%20With%20the%20recent%20release%20of%20GPT-5%2C%20allegedly%0Athe%20most%20powerful%20AI%20model%20to%20date%2C%20it%20is%20timely%20to%20examine%20where%20the%20leading%0Amodels%20stand%20on%20the%20path%20toward%20spatial%20intelligence.%20First%2C%20we%20propose%20a%0Acomprehensive%20taxonomy%20of%20spatial%20tasks%20that%20unifies%20existing%20benchmarks%20and%0Adiscuss%20the%20challenges%20in%20ensuring%20fair%20evaluation.%20We%20then%20evaluate%0Astate-of-the-art%20proprietary%20and%20open-source%20models%20on%20eight%20key%20benchmarks%2C%20at%0Aa%20cost%20exceeding%20one%20billion%20total%20tokens.%20Our%20empirical%20study%20reveals%20that%20%281%29%0AGPT-5%20demonstrates%20unprecedented%20strength%20in%20spatial%20intelligence%2C%20yet%20%282%29%0Astill%20falls%20short%20of%20human%20performance%20across%20a%20broad%20spectrum%20of%20tasks.%0AMoreover%2C%20we%20%283%29%20identify%20the%20more%20challenging%20spatial%20intelligence%20problems%0Afor%20multi-modal%20models%2C%20and%20%284%29%20proprietary%20models%20do%20not%20exhibit%20a%20decisive%0Aadvantage%20when%20facing%20the%20most%20difficult%20problems.%20In%20addition%2C%20we%20conduct%20a%0Aqualitative%20evaluation%20across%20a%20diverse%20set%20of%20scenarios%20that%20are%20intuitive%20for%0Ahumans%20yet%20fail%20even%20the%20most%20advanced%20multi-modal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13142v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHas%2520GPT-5%2520Achieved%2520Spatial%2520Intelligence%253F%2520An%2520Empirical%2520Study%26entry.906535625%3DZhongang%2520Cai%2520and%2520Yubo%2520Wang%2520and%2520Qingping%2520Sun%2520and%2520Ruisi%2520Wang%2520and%2520Chenyang%2520Gu%2520and%2520Wanqi%2520Yin%2520and%2520Zhiqian%2520Lin%2520and%2520Zhitao%2520Yang%2520and%2520Chen%2520Wei%2520and%2520Xuanke%2520Shi%2520and%2520Kewang%2520Deng%2520and%2520Xiaoyang%2520Han%2520and%2520Zukai%2520Chen%2520and%2520Jiaqi%2520Li%2520and%2520Xiangyu%2520Fan%2520and%2520Hanming%2520Deng%2520and%2520Lewei%2520Lu%2520and%2520Bo%2520Li%2520and%2520Ziwei%2520Liu%2520and%2520Quan%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Lei%2520Yang%26entry.1292438233%3D%2520%2520Multi-modal%2520models%2520have%2520achieved%2520remarkable%2520progress%2520in%2520recent%2520years.%250ANevertheless%252C%2520they%2520continue%2520to%2520exhibit%2520notable%2520limitations%2520in%2520spatial%250Aunderstanding%2520and%2520reasoning%252C%2520which%2520are%2520fundamental%2520capabilities%2520to%2520achieving%250Aartificial%2520general%2520intelligence.%2520With%2520the%2520recent%2520release%2520of%2520GPT-5%252C%2520allegedly%250Athe%2520most%2520powerful%2520AI%2520model%2520to%2520date%252C%2520it%2520is%2520timely%2520to%2520examine%2520where%2520the%2520leading%250Amodels%2520stand%2520on%2520the%2520path%2520toward%2520spatial%2520intelligence.%2520First%252C%2520we%2520propose%2520a%250Acomprehensive%2520taxonomy%2520of%2520spatial%2520tasks%2520that%2520unifies%2520existing%2520benchmarks%2520and%250Adiscuss%2520the%2520challenges%2520in%2520ensuring%2520fair%2520evaluation.%2520We%2520then%2520evaluate%250Astate-of-the-art%2520proprietary%2520and%2520open-source%2520models%2520on%2520eight%2520key%2520benchmarks%252C%2520at%250Aa%2520cost%2520exceeding%2520one%2520billion%2520total%2520tokens.%2520Our%2520empirical%2520study%2520reveals%2520that%2520%25281%2529%250AGPT-5%2520demonstrates%2520unprecedented%2520strength%2520in%2520spatial%2520intelligence%252C%2520yet%2520%25282%2529%250Astill%2520falls%2520short%2520of%2520human%2520performance%2520across%2520a%2520broad%2520spectrum%2520of%2520tasks.%250AMoreover%252C%2520we%2520%25283%2529%2520identify%2520the%2520more%2520challenging%2520spatial%2520intelligence%2520problems%250Afor%2520multi-modal%2520models%252C%2520and%2520%25284%2529%2520proprietary%2520models%2520do%2520not%2520exhibit%2520a%2520decisive%250Aadvantage%2520when%2520facing%2520the%2520most%2520difficult%2520problems.%2520In%2520addition%252C%2520we%2520conduct%2520a%250Aqualitative%2520evaluation%2520across%2520a%2520diverse%2520set%2520of%2520scenarios%2520that%2520are%2520intuitive%2520for%250Ahumans%2520yet%2520fail%2520even%2520the%2520most%2520advanced%2520multi-modal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13142v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Has%20GPT-5%20Achieved%20Spatial%20Intelligence%3F%20An%20Empirical%20Study&entry.906535625=Zhongang%20Cai%20and%20Yubo%20Wang%20and%20Qingping%20Sun%20and%20Ruisi%20Wang%20and%20Chenyang%20Gu%20and%20Wanqi%20Yin%20and%20Zhiqian%20Lin%20and%20Zhitao%20Yang%20and%20Chen%20Wei%20and%20Xuanke%20Shi%20and%20Kewang%20Deng%20and%20Xiaoyang%20Han%20and%20Zukai%20Chen%20and%20Jiaqi%20Li%20and%20Xiangyu%20Fan%20and%20Hanming%20Deng%20and%20Lewei%20Lu%20and%20Bo%20Li%20and%20Ziwei%20Liu%20and%20Quan%20Wang%20and%20Dahua%20Lin%20and%20Lei%20Yang&entry.1292438233=%20%20Multi-modal%20models%20have%20achieved%20remarkable%20progress%20in%20recent%20years.%0ANevertheless%2C%20they%20continue%20to%20exhibit%20notable%20limitations%20in%20spatial%0Aunderstanding%20and%20reasoning%2C%20which%20are%20fundamental%20capabilities%20to%20achieving%0Aartificial%20general%20intelligence.%20With%20the%20recent%20release%20of%20GPT-5%2C%20allegedly%0Athe%20most%20powerful%20AI%20model%20to%20date%2C%20it%20is%20timely%20to%20examine%20where%20the%20leading%0Amodels%20stand%20on%20the%20path%20toward%20spatial%20intelligence.%20First%2C%20we%20propose%20a%0Acomprehensive%20taxonomy%20of%20spatial%20tasks%20that%20unifies%20existing%20benchmarks%20and%0Adiscuss%20the%20challenges%20in%20ensuring%20fair%20evaluation.%20We%20then%20evaluate%0Astate-of-the-art%20proprietary%20and%20open-source%20models%20on%20eight%20key%20benchmarks%2C%20at%0Aa%20cost%20exceeding%20one%20billion%20total%20tokens.%20Our%20empirical%20study%20reveals%20that%20%281%29%0AGPT-5%20demonstrates%20unprecedented%20strength%20in%20spatial%20intelligence%2C%20yet%20%282%29%0Astill%20falls%20short%20of%20human%20performance%20across%20a%20broad%20spectrum%20of%20tasks.%0AMoreover%2C%20we%20%283%29%20identify%20the%20more%20challenging%20spatial%20intelligence%20problems%0Afor%20multi-modal%20models%2C%20and%20%284%29%20proprietary%20models%20do%20not%20exhibit%20a%20decisive%0Aadvantage%20when%20facing%20the%20most%20difficult%20problems.%20In%20addition%2C%20we%20conduct%20a%0Aqualitative%20evaluation%20across%20a%20diverse%20set%20of%20scenarios%20that%20are%20intuitive%20for%0Ahumans%20yet%20fail%20even%20the%20most%20advanced%20multi-modal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13142v1&entry.124074799=Read"},
{"title": "Fast Geometric Embedding for Node Influence Maximization", "author": "Alexander Kolpakov and Igor Rivin", "abstract": "  Computing classical centrality measures such as betweenness and closeness is\ncomputationally expensive on large-scale graphs. In this work, we introduce an\nefficient force layout algorithm that embeds a graph into a low-dimensional\nspace, where the radial distance from the origin serves as a proxy for various\ncentrality measures. We evaluate our method on multiple graph families and\ndemonstrate strong correlations with degree, PageRank, and paths-based\ncentralities. As an application, it turns out that the proposed embedding\nallows to find high-influence nodes in a network, and provides a fast and\nscalable alternative to the standard greedy algorithm.\n", "link": "http://arxiv.org/abs/2506.07435v2", "date": "2025-08-18", "relevancy": 2.1276, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4488}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4206}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Geometric%20Embedding%20for%20Node%20Influence%20Maximization&body=Title%3A%20Fast%20Geometric%20Embedding%20for%20Node%20Influence%20Maximization%0AAuthor%3A%20Alexander%20Kolpakov%20and%20Igor%20Rivin%0AAbstract%3A%20%20%20Computing%20classical%20centrality%20measures%20such%20as%20betweenness%20and%20closeness%20is%0Acomputationally%20expensive%20on%20large-scale%20graphs.%20In%20this%20work%2C%20we%20introduce%20an%0Aefficient%20force%20layout%20algorithm%20that%20embeds%20a%20graph%20into%20a%20low-dimensional%0Aspace%2C%20where%20the%20radial%20distance%20from%20the%20origin%20serves%20as%20a%20proxy%20for%20various%0Acentrality%20measures.%20We%20evaluate%20our%20method%20on%20multiple%20graph%20families%20and%0Ademonstrate%20strong%20correlations%20with%20degree%2C%20PageRank%2C%20and%20paths-based%0Acentralities.%20As%20an%20application%2C%20it%20turns%20out%20that%20the%20proposed%20embedding%0Aallows%20to%20find%20high-influence%20nodes%20in%20a%20network%2C%20and%20provides%20a%20fast%20and%0Ascalable%20alternative%20to%20the%20standard%20greedy%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Geometric%2520Embedding%2520for%2520Node%2520Influence%2520Maximization%26entry.906535625%3DAlexander%2520Kolpakov%2520and%2520Igor%2520Rivin%26entry.1292438233%3D%2520%2520Computing%2520classical%2520centrality%2520measures%2520such%2520as%2520betweenness%2520and%2520closeness%2520is%250Acomputationally%2520expensive%2520on%2520large-scale%2520graphs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%250Aefficient%2520force%2520layout%2520algorithm%2520that%2520embeds%2520a%2520graph%2520into%2520a%2520low-dimensional%250Aspace%252C%2520where%2520the%2520radial%2520distance%2520from%2520the%2520origin%2520serves%2520as%2520a%2520proxy%2520for%2520various%250Acentrality%2520measures.%2520We%2520evaluate%2520our%2520method%2520on%2520multiple%2520graph%2520families%2520and%250Ademonstrate%2520strong%2520correlations%2520with%2520degree%252C%2520PageRank%252C%2520and%2520paths-based%250Acentralities.%2520As%2520an%2520application%252C%2520it%2520turns%2520out%2520that%2520the%2520proposed%2520embedding%250Aallows%2520to%2520find%2520high-influence%2520nodes%2520in%2520a%2520network%252C%2520and%2520provides%2520a%2520fast%2520and%250Ascalable%2520alternative%2520to%2520the%2520standard%2520greedy%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Geometric%20Embedding%20for%20Node%20Influence%20Maximization&entry.906535625=Alexander%20Kolpakov%20and%20Igor%20Rivin&entry.1292438233=%20%20Computing%20classical%20centrality%20measures%20such%20as%20betweenness%20and%20closeness%20is%0Acomputationally%20expensive%20on%20large-scale%20graphs.%20In%20this%20work%2C%20we%20introduce%20an%0Aefficient%20force%20layout%20algorithm%20that%20embeds%20a%20graph%20into%20a%20low-dimensional%0Aspace%2C%20where%20the%20radial%20distance%20from%20the%20origin%20serves%20as%20a%20proxy%20for%20various%0Acentrality%20measures.%20We%20evaluate%20our%20method%20on%20multiple%20graph%20families%20and%0Ademonstrate%20strong%20correlations%20with%20degree%2C%20PageRank%2C%20and%20paths-based%0Acentralities.%20As%20an%20application%2C%20it%20turns%20out%20that%20the%20proposed%20embedding%0Aallows%20to%20find%20high-influence%20nodes%20in%20a%20network%2C%20and%20provides%20a%20fast%20and%0Ascalable%20alternative%20to%20the%20standard%20greedy%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07435v2&entry.124074799=Read"},
{"title": "Diving into the Fusion of Monocular Priors for Generalized Stereo\n  Matching", "author": "Chengtang Yao and Lidong Yu and Zhidan Liu and Jiaxi Zeng and Yuwei Wu and Yunde Jia", "abstract": "  The matching formulation makes it naturally hard for the stereo matching to\nhandle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing\nmonocular priors has been proven helpful for ill-posed matching, but the biased\nmonocular prior learned from small stereo datasets constrains the\ngeneralization. Recently, stereo matching has progressed by leveraging the\nunbiased monocular prior from the vision foundation model (VFM) to improve the\ngeneralization in ill-posed regions. We dive into the fusion process and\nobserve three main problems limiting the fusion of the VFM monocular prior. The\nfirst problem is the misalignment between affine-invariant relative monocular\ndepth and absolute depth of disparity. Besides, when we use the monocular\nfeature in an iterative update structure, the over-confidence in the disparity\nupdate leads to local optima results. A direct fusion of a monocular depth map\ncould alleviate the local optima problem, but noisy disparity results computed\nat the first several iterations will misguide the fusion. In this paper, we\npropose a binary local ordering map to guide the fusion, which converts the\ndepth map into a binary relative format, unifying the relative and absolute\ndepth representation. The computed local ordering map is also used to re-weight\nthe initial disparity update, resolving the local optima and noisy problem. In\naddition, we formulate the final direct fusion of monocular depth to the\ndisparity as a registration problem, where a pixel-wise linear regression\nmodule can globally and adaptively align them. Our method fully exploits the\nmonocular prior to support stereo matching results effectively and efficiently.\nWe significantly improve the performance from the experiments when generalizing\nfrom SceneFlow to Middlebury and Booster datasets while barely reducing the\nefficiency.\n", "link": "http://arxiv.org/abs/2505.14414v2", "date": "2025-08-18", "relevancy": 2.1222, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5433}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5232}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diving%20into%20the%20Fusion%20of%20Monocular%20Priors%20for%20Generalized%20Stereo%0A%20%20Matching&body=Title%3A%20Diving%20into%20the%20Fusion%20of%20Monocular%20Priors%20for%20Generalized%20Stereo%0A%20%20Matching%0AAuthor%3A%20Chengtang%20Yao%20and%20Lidong%20Yu%20and%20Zhidan%20Liu%20and%20Jiaxi%20Zeng%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%0AAbstract%3A%20%20%20The%20matching%20formulation%20makes%20it%20naturally%20hard%20for%20the%20stereo%20matching%20to%0Ahandle%20ill-posed%20regions%20like%20occlusions%20and%20non-Lambertian%20surfaces.%20Fusing%0Amonocular%20priors%20has%20been%20proven%20helpful%20for%20ill-posed%20matching%2C%20but%20the%20biased%0Amonocular%20prior%20learned%20from%20small%20stereo%20datasets%20constrains%20the%0Ageneralization.%20Recently%2C%20stereo%20matching%20has%20progressed%20by%20leveraging%20the%0Aunbiased%20monocular%20prior%20from%20the%20vision%20foundation%20model%20%28VFM%29%20to%20improve%20the%0Ageneralization%20in%20ill-posed%20regions.%20We%20dive%20into%20the%20fusion%20process%20and%0Aobserve%20three%20main%20problems%20limiting%20the%20fusion%20of%20the%20VFM%20monocular%20prior.%20The%0Afirst%20problem%20is%20the%20misalignment%20between%20affine-invariant%20relative%20monocular%0Adepth%20and%20absolute%20depth%20of%20disparity.%20Besides%2C%20when%20we%20use%20the%20monocular%0Afeature%20in%20an%20iterative%20update%20structure%2C%20the%20over-confidence%20in%20the%20disparity%0Aupdate%20leads%20to%20local%20optima%20results.%20A%20direct%20fusion%20of%20a%20monocular%20depth%20map%0Acould%20alleviate%20the%20local%20optima%20problem%2C%20but%20noisy%20disparity%20results%20computed%0Aat%20the%20first%20several%20iterations%20will%20misguide%20the%20fusion.%20In%20this%20paper%2C%20we%0Apropose%20a%20binary%20local%20ordering%20map%20to%20guide%20the%20fusion%2C%20which%20converts%20the%0Adepth%20map%20into%20a%20binary%20relative%20format%2C%20unifying%20the%20relative%20and%20absolute%0Adepth%20representation.%20The%20computed%20local%20ordering%20map%20is%20also%20used%20to%20re-weight%0Athe%20initial%20disparity%20update%2C%20resolving%20the%20local%20optima%20and%20noisy%20problem.%20In%0Aaddition%2C%20we%20formulate%20the%20final%20direct%20fusion%20of%20monocular%20depth%20to%20the%0Adisparity%20as%20a%20registration%20problem%2C%20where%20a%20pixel-wise%20linear%20regression%0Amodule%20can%20globally%20and%20adaptively%20align%20them.%20Our%20method%20fully%20exploits%20the%0Amonocular%20prior%20to%20support%20stereo%20matching%20results%20effectively%20and%20efficiently.%0AWe%20significantly%20improve%20the%20performance%20from%20the%20experiments%20when%20generalizing%0Afrom%20SceneFlow%20to%20Middlebury%20and%20Booster%20datasets%20while%20barely%20reducing%20the%0Aefficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14414v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiving%2520into%2520the%2520Fusion%2520of%2520Monocular%2520Priors%2520for%2520Generalized%2520Stereo%250A%2520%2520Matching%26entry.906535625%3DChengtang%2520Yao%2520and%2520Lidong%2520Yu%2520and%2520Zhidan%2520Liu%2520and%2520Jiaxi%2520Zeng%2520and%2520Yuwei%2520Wu%2520and%2520Yunde%2520Jia%26entry.1292438233%3D%2520%2520The%2520matching%2520formulation%2520makes%2520it%2520naturally%2520hard%2520for%2520the%2520stereo%2520matching%2520to%250Ahandle%2520ill-posed%2520regions%2520like%2520occlusions%2520and%2520non-Lambertian%2520surfaces.%2520Fusing%250Amonocular%2520priors%2520has%2520been%2520proven%2520helpful%2520for%2520ill-posed%2520matching%252C%2520but%2520the%2520biased%250Amonocular%2520prior%2520learned%2520from%2520small%2520stereo%2520datasets%2520constrains%2520the%250Ageneralization.%2520Recently%252C%2520stereo%2520matching%2520has%2520progressed%2520by%2520leveraging%2520the%250Aunbiased%2520monocular%2520prior%2520from%2520the%2520vision%2520foundation%2520model%2520%2528VFM%2529%2520to%2520improve%2520the%250Ageneralization%2520in%2520ill-posed%2520regions.%2520We%2520dive%2520into%2520the%2520fusion%2520process%2520and%250Aobserve%2520three%2520main%2520problems%2520limiting%2520the%2520fusion%2520of%2520the%2520VFM%2520monocular%2520prior.%2520The%250Afirst%2520problem%2520is%2520the%2520misalignment%2520between%2520affine-invariant%2520relative%2520monocular%250Adepth%2520and%2520absolute%2520depth%2520of%2520disparity.%2520Besides%252C%2520when%2520we%2520use%2520the%2520monocular%250Afeature%2520in%2520an%2520iterative%2520update%2520structure%252C%2520the%2520over-confidence%2520in%2520the%2520disparity%250Aupdate%2520leads%2520to%2520local%2520optima%2520results.%2520A%2520direct%2520fusion%2520of%2520a%2520monocular%2520depth%2520map%250Acould%2520alleviate%2520the%2520local%2520optima%2520problem%252C%2520but%2520noisy%2520disparity%2520results%2520computed%250Aat%2520the%2520first%2520several%2520iterations%2520will%2520misguide%2520the%2520fusion.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520binary%2520local%2520ordering%2520map%2520to%2520guide%2520the%2520fusion%252C%2520which%2520converts%2520the%250Adepth%2520map%2520into%2520a%2520binary%2520relative%2520format%252C%2520unifying%2520the%2520relative%2520and%2520absolute%250Adepth%2520representation.%2520The%2520computed%2520local%2520ordering%2520map%2520is%2520also%2520used%2520to%2520re-weight%250Athe%2520initial%2520disparity%2520update%252C%2520resolving%2520the%2520local%2520optima%2520and%2520noisy%2520problem.%2520In%250Aaddition%252C%2520we%2520formulate%2520the%2520final%2520direct%2520fusion%2520of%2520monocular%2520depth%2520to%2520the%250Adisparity%2520as%2520a%2520registration%2520problem%252C%2520where%2520a%2520pixel-wise%2520linear%2520regression%250Amodule%2520can%2520globally%2520and%2520adaptively%2520align%2520them.%2520Our%2520method%2520fully%2520exploits%2520the%250Amonocular%2520prior%2520to%2520support%2520stereo%2520matching%2520results%2520effectively%2520and%2520efficiently.%250AWe%2520significantly%2520improve%2520the%2520performance%2520from%2520the%2520experiments%2520when%2520generalizing%250Afrom%2520SceneFlow%2520to%2520Middlebury%2520and%2520Booster%2520datasets%2520while%2520barely%2520reducing%2520the%250Aefficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14414v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diving%20into%20the%20Fusion%20of%20Monocular%20Priors%20for%20Generalized%20Stereo%0A%20%20Matching&entry.906535625=Chengtang%20Yao%20and%20Lidong%20Yu%20and%20Zhidan%20Liu%20and%20Jiaxi%20Zeng%20and%20Yuwei%20Wu%20and%20Yunde%20Jia&entry.1292438233=%20%20The%20matching%20formulation%20makes%20it%20naturally%20hard%20for%20the%20stereo%20matching%20to%0Ahandle%20ill-posed%20regions%20like%20occlusions%20and%20non-Lambertian%20surfaces.%20Fusing%0Amonocular%20priors%20has%20been%20proven%20helpful%20for%20ill-posed%20matching%2C%20but%20the%20biased%0Amonocular%20prior%20learned%20from%20small%20stereo%20datasets%20constrains%20the%0Ageneralization.%20Recently%2C%20stereo%20matching%20has%20progressed%20by%20leveraging%20the%0Aunbiased%20monocular%20prior%20from%20the%20vision%20foundation%20model%20%28VFM%29%20to%20improve%20the%0Ageneralization%20in%20ill-posed%20regions.%20We%20dive%20into%20the%20fusion%20process%20and%0Aobserve%20three%20main%20problems%20limiting%20the%20fusion%20of%20the%20VFM%20monocular%20prior.%20The%0Afirst%20problem%20is%20the%20misalignment%20between%20affine-invariant%20relative%20monocular%0Adepth%20and%20absolute%20depth%20of%20disparity.%20Besides%2C%20when%20we%20use%20the%20monocular%0Afeature%20in%20an%20iterative%20update%20structure%2C%20the%20over-confidence%20in%20the%20disparity%0Aupdate%20leads%20to%20local%20optima%20results.%20A%20direct%20fusion%20of%20a%20monocular%20depth%20map%0Acould%20alleviate%20the%20local%20optima%20problem%2C%20but%20noisy%20disparity%20results%20computed%0Aat%20the%20first%20several%20iterations%20will%20misguide%20the%20fusion.%20In%20this%20paper%2C%20we%0Apropose%20a%20binary%20local%20ordering%20map%20to%20guide%20the%20fusion%2C%20which%20converts%20the%0Adepth%20map%20into%20a%20binary%20relative%20format%2C%20unifying%20the%20relative%20and%20absolute%0Adepth%20representation.%20The%20computed%20local%20ordering%20map%20is%20also%20used%20to%20re-weight%0Athe%20initial%20disparity%20update%2C%20resolving%20the%20local%20optima%20and%20noisy%20problem.%20In%0Aaddition%2C%20we%20formulate%20the%20final%20direct%20fusion%20of%20monocular%20depth%20to%20the%0Adisparity%20as%20a%20registration%20problem%2C%20where%20a%20pixel-wise%20linear%20regression%0Amodule%20can%20globally%20and%20adaptively%20align%20them.%20Our%20method%20fully%20exploits%20the%0Amonocular%20prior%20to%20support%20stereo%20matching%20results%20effectively%20and%20efficiently.%0AWe%20significantly%20improve%20the%20performance%20from%20the%20experiments%20when%20generalizing%0Afrom%20SceneFlow%20to%20Middlebury%20and%20Booster%20datasets%20while%20barely%20reducing%20the%0Aefficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14414v2&entry.124074799=Read"},
{"title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution", "author": "Zailong Tian and Zhuoheng Han and Yanzhe Chen and Haozhe Xu and Xi Yang and Richeng Xuan and Houfeng Wang and Lizi Liao", "abstract": "  Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the Overconfidence Phenomenon in current\nLLM-as-a-Judges, where predicted confidence significantly overstates actual\ncorrectness, undermining reliability in practical deployment. To quantify this\nphenomenon, we introduce TH-Score, a novel metric measuring confidence-accuracy\nalignment. Furthermore, we propose LLM-as-a-Fuser, an ensemble framework that\ntransforms LLMs into reliable, risk-aware evaluators. Extensive experiments\ndemonstrate that our approach substantially improves calibration and enables\nadaptive, confidence-driven evaluation pipelines, achieving superior\nreliability and accuracy compared to existing baselines.\n", "link": "http://arxiv.org/abs/2508.06225v3", "date": "2025-08-18", "relevancy": 2.1147, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5482}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5444}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution&body=Title%3A%20Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution%0AAuthor%3A%20Zailong%20Tian%20and%20Zhuoheng%20Han%20and%20Yanzhe%20Chen%20and%20Haozhe%20Xu%20and%20Xi%20Yang%20and%20Richeng%20Xuan%20and%20Houfeng%20Wang%20and%20Lizi%20Liao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20as%20automated%20judges%2C%20where%0Apractical%20value%20depends%20on%20both%20accuracy%20and%20trustworthy%2C%20risk-aware%20judgments.%0AExisting%20approaches%20predominantly%20focus%20on%20accuracy%2C%20overlooking%20the%20necessity%0Aof%20well-calibrated%20confidence%2C%20which%20is%20vital%20for%20adaptive%20and%20reliable%0Aevaluation%20pipelines.%20In%20this%20work%2C%20we%20advocate%20a%20shift%20from%20accuracy-centric%0Aevaluation%20to%20confidence-driven%2C%20risk-aware%20LLM-as-a-Judge%20systems%2C%20emphasizing%0Athe%20necessity%20of%20well-calibrated%20confidence%20for%20trustworthy%20and%20adaptive%0Aevaluation.%20We%20systematically%20identify%20the%20Overconfidence%20Phenomenon%20in%20current%0ALLM-as-a-Judges%2C%20where%20predicted%20confidence%20significantly%20overstates%20actual%0Acorrectness%2C%20undermining%20reliability%20in%20practical%20deployment.%20To%20quantify%20this%0Aphenomenon%2C%20we%20introduce%20TH-Score%2C%20a%20novel%20metric%20measuring%20confidence-accuracy%0Aalignment.%20Furthermore%2C%20we%20propose%20LLM-as-a-Fuser%2C%20an%20ensemble%20framework%20that%0Atransforms%20LLMs%20into%20reliable%2C%20risk-aware%20evaluators.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20substantially%20improves%20calibration%20and%20enables%0Aadaptive%2C%20confidence-driven%20evaluation%20pipelines%2C%20achieving%20superior%0Areliability%20and%20accuracy%20compared%20to%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06225v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverconfidence%2520in%2520LLM-as-a-Judge%253A%2520Diagnosis%2520and%2520Confidence-Driven%250A%2520%2520Solution%26entry.906535625%3DZailong%2520Tian%2520and%2520Zhuoheng%2520Han%2520and%2520Yanzhe%2520Chen%2520and%2520Haozhe%2520Xu%2520and%2520Xi%2520Yang%2520and%2520Richeng%2520Xuan%2520and%2520Houfeng%2520Wang%2520and%2520Lizi%2520Liao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520widely%2520used%2520as%2520automated%2520judges%252C%2520where%250Apractical%2520value%2520depends%2520on%2520both%2520accuracy%2520and%2520trustworthy%252C%2520risk-aware%2520judgments.%250AExisting%2520approaches%2520predominantly%2520focus%2520on%2520accuracy%252C%2520overlooking%2520the%2520necessity%250Aof%2520well-calibrated%2520confidence%252C%2520which%2520is%2520vital%2520for%2520adaptive%2520and%2520reliable%250Aevaluation%2520pipelines.%2520In%2520this%2520work%252C%2520we%2520advocate%2520a%2520shift%2520from%2520accuracy-centric%250Aevaluation%2520to%2520confidence-driven%252C%2520risk-aware%2520LLM-as-a-Judge%2520systems%252C%2520emphasizing%250Athe%2520necessity%2520of%2520well-calibrated%2520confidence%2520for%2520trustworthy%2520and%2520adaptive%250Aevaluation.%2520We%2520systematically%2520identify%2520the%2520Overconfidence%2520Phenomenon%2520in%2520current%250ALLM-as-a-Judges%252C%2520where%2520predicted%2520confidence%2520significantly%2520overstates%2520actual%250Acorrectness%252C%2520undermining%2520reliability%2520in%2520practical%2520deployment.%2520To%2520quantify%2520this%250Aphenomenon%252C%2520we%2520introduce%2520TH-Score%252C%2520a%2520novel%2520metric%2520measuring%2520confidence-accuracy%250Aalignment.%2520Furthermore%252C%2520we%2520propose%2520LLM-as-a-Fuser%252C%2520an%2520ensemble%2520framework%2520that%250Atransforms%2520LLMs%2520into%2520reliable%252C%2520risk-aware%2520evaluators.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520approach%2520substantially%2520improves%2520calibration%2520and%2520enables%250Aadaptive%252C%2520confidence-driven%2520evaluation%2520pipelines%252C%2520achieving%2520superior%250Areliability%2520and%2520accuracy%2520compared%2520to%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06225v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overconfidence%20in%20LLM-as-a-Judge%3A%20Diagnosis%20and%20Confidence-Driven%0A%20%20Solution&entry.906535625=Zailong%20Tian%20and%20Zhuoheng%20Han%20and%20Yanzhe%20Chen%20and%20Haozhe%20Xu%20and%20Xi%20Yang%20and%20Richeng%20Xuan%20and%20Houfeng%20Wang%20and%20Lizi%20Liao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20as%20automated%20judges%2C%20where%0Apractical%20value%20depends%20on%20both%20accuracy%20and%20trustworthy%2C%20risk-aware%20judgments.%0AExisting%20approaches%20predominantly%20focus%20on%20accuracy%2C%20overlooking%20the%20necessity%0Aof%20well-calibrated%20confidence%2C%20which%20is%20vital%20for%20adaptive%20and%20reliable%0Aevaluation%20pipelines.%20In%20this%20work%2C%20we%20advocate%20a%20shift%20from%20accuracy-centric%0Aevaluation%20to%20confidence-driven%2C%20risk-aware%20LLM-as-a-Judge%20systems%2C%20emphasizing%0Athe%20necessity%20of%20well-calibrated%20confidence%20for%20trustworthy%20and%20adaptive%0Aevaluation.%20We%20systematically%20identify%20the%20Overconfidence%20Phenomenon%20in%20current%0ALLM-as-a-Judges%2C%20where%20predicted%20confidence%20significantly%20overstates%20actual%0Acorrectness%2C%20undermining%20reliability%20in%20practical%20deployment.%20To%20quantify%20this%0Aphenomenon%2C%20we%20introduce%20TH-Score%2C%20a%20novel%20metric%20measuring%20confidence-accuracy%0Aalignment.%20Furthermore%2C%20we%20propose%20LLM-as-a-Fuser%2C%20an%20ensemble%20framework%20that%0Atransforms%20LLMs%20into%20reliable%2C%20risk-aware%20evaluators.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20substantially%20improves%20calibration%20and%20enables%0Aadaptive%2C%20confidence-driven%20evaluation%20pipelines%2C%20achieving%20superior%0Areliability%20and%20accuracy%20compared%20to%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06225v3&entry.124074799=Read"},
{"title": "Training Machine Learning Models on Human Spatio-temporal Mobility Data:\n  An Experimental Study [Experiment Paper]", "author": "Yueyang Liu and Lance Kennedy and Ruochen Kong and Joon-Seok Kim and Andreas Z\u00fcfle", "abstract": "  Individual-level human mobility prediction has emerged as a significant topic\nof research with applications in infectious disease monitoring, child, and\nelderly care. Existing studies predominantly focus on the microscopic aspects\nof human trajectories: such as predicting short-term trajectories or the next\nlocation visited, while offering limited attention to macro-level mobility\npatterns and the corresponding life routines. In this paper, we focus on an\nunderexplored problem in human mobility prediction: determining the best\npractices to train a machine learning model using historical data to forecast\nan individuals complete trajectory over the next days and weeks. In this\nexperiment paper, we undertake a comprehensive experimental analysis of diverse\nmodels, parameter configurations, and training strategies, accompanied by an\nin-depth examination of the statistical distribution inherent in human mobility\npatterns. Our empirical evaluations encompass both Long Short-Term Memory and\nTransformer-based architectures, and further investigate how incorporating\nindividual life patterns can enhance the effectiveness of the prediction. We\nshow that explicitly including semantic information such as day-of-the-week and\nuser-specific historical information can help the model better understand\nindividual patterns of life and improve predictions. Moreover, since the\nabsence of explicit user information is often missing due to user privacy, we\nshow that the sampling of users may exacerbate data skewness and result in a\nsubstantial loss in predictive accuracy. To mitigate data imbalance and\npreserve diversity, we apply user semantic clustering with stratified sampling\nto ensure that the sampled dataset remains representative. Our results further\nshow that small-batch stochastic gradient optimization improves model\nperformance, especially when human mobility training data is limited.\n", "link": "http://arxiv.org/abs/2508.13135v1", "date": "2025-08-18", "relevancy": 2.111, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5451}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5422}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Machine%20Learning%20Models%20on%20Human%20Spatio-temporal%20Mobility%20Data%3A%0A%20%20An%20Experimental%20Study%20%5BExperiment%20Paper%5D&body=Title%3A%20Training%20Machine%20Learning%20Models%20on%20Human%20Spatio-temporal%20Mobility%20Data%3A%0A%20%20An%20Experimental%20Study%20%5BExperiment%20Paper%5D%0AAuthor%3A%20Yueyang%20Liu%20and%20Lance%20Kennedy%20and%20Ruochen%20Kong%20and%20Joon-Seok%20Kim%20and%20Andreas%20Z%C3%BCfle%0AAbstract%3A%20%20%20Individual-level%20human%20mobility%20prediction%20has%20emerged%20as%20a%20significant%20topic%0Aof%20research%20with%20applications%20in%20infectious%20disease%20monitoring%2C%20child%2C%20and%0Aelderly%20care.%20Existing%20studies%20predominantly%20focus%20on%20the%20microscopic%20aspects%0Aof%20human%20trajectories%3A%20such%20as%20predicting%20short-term%20trajectories%20or%20the%20next%0Alocation%20visited%2C%20while%20offering%20limited%20attention%20to%20macro-level%20mobility%0Apatterns%20and%20the%20corresponding%20life%20routines.%20In%20this%20paper%2C%20we%20focus%20on%20an%0Aunderexplored%20problem%20in%20human%20mobility%20prediction%3A%20determining%20the%20best%0Apractices%20to%20train%20a%20machine%20learning%20model%20using%20historical%20data%20to%20forecast%0Aan%20individuals%20complete%20trajectory%20over%20the%20next%20days%20and%20weeks.%20In%20this%0Aexperiment%20paper%2C%20we%20undertake%20a%20comprehensive%20experimental%20analysis%20of%20diverse%0Amodels%2C%20parameter%20configurations%2C%20and%20training%20strategies%2C%20accompanied%20by%20an%0Ain-depth%20examination%20of%20the%20statistical%20distribution%20inherent%20in%20human%20mobility%0Apatterns.%20Our%20empirical%20evaluations%20encompass%20both%20Long%20Short-Term%20Memory%20and%0ATransformer-based%20architectures%2C%20and%20further%20investigate%20how%20incorporating%0Aindividual%20life%20patterns%20can%20enhance%20the%20effectiveness%20of%20the%20prediction.%20We%0Ashow%20that%20explicitly%20including%20semantic%20information%20such%20as%20day-of-the-week%20and%0Auser-specific%20historical%20information%20can%20help%20the%20model%20better%20understand%0Aindividual%20patterns%20of%20life%20and%20improve%20predictions.%20Moreover%2C%20since%20the%0Aabsence%20of%20explicit%20user%20information%20is%20often%20missing%20due%20to%20user%20privacy%2C%20we%0Ashow%20that%20the%20sampling%20of%20users%20may%20exacerbate%20data%20skewness%20and%20result%20in%20a%0Asubstantial%20loss%20in%20predictive%20accuracy.%20To%20mitigate%20data%20imbalance%20and%0Apreserve%20diversity%2C%20we%20apply%20user%20semantic%20clustering%20with%20stratified%20sampling%0Ato%20ensure%20that%20the%20sampled%20dataset%20remains%20representative.%20Our%20results%20further%0Ashow%20that%20small-batch%20stochastic%20gradient%20optimization%20improves%20model%0Aperformance%2C%20especially%20when%20human%20mobility%20training%20data%20is%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Machine%2520Learning%2520Models%2520on%2520Human%2520Spatio-temporal%2520Mobility%2520Data%253A%250A%2520%2520An%2520Experimental%2520Study%2520%255BExperiment%2520Paper%255D%26entry.906535625%3DYueyang%2520Liu%2520and%2520Lance%2520Kennedy%2520and%2520Ruochen%2520Kong%2520and%2520Joon-Seok%2520Kim%2520and%2520Andreas%2520Z%25C3%25BCfle%26entry.1292438233%3D%2520%2520Individual-level%2520human%2520mobility%2520prediction%2520has%2520emerged%2520as%2520a%2520significant%2520topic%250Aof%2520research%2520with%2520applications%2520in%2520infectious%2520disease%2520monitoring%252C%2520child%252C%2520and%250Aelderly%2520care.%2520Existing%2520studies%2520predominantly%2520focus%2520on%2520the%2520microscopic%2520aspects%250Aof%2520human%2520trajectories%253A%2520such%2520as%2520predicting%2520short-term%2520trajectories%2520or%2520the%2520next%250Alocation%2520visited%252C%2520while%2520offering%2520limited%2520attention%2520to%2520macro-level%2520mobility%250Apatterns%2520and%2520the%2520corresponding%2520life%2520routines.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520an%250Aunderexplored%2520problem%2520in%2520human%2520mobility%2520prediction%253A%2520determining%2520the%2520best%250Apractices%2520to%2520train%2520a%2520machine%2520learning%2520model%2520using%2520historical%2520data%2520to%2520forecast%250Aan%2520individuals%2520complete%2520trajectory%2520over%2520the%2520next%2520days%2520and%2520weeks.%2520In%2520this%250Aexperiment%2520paper%252C%2520we%2520undertake%2520a%2520comprehensive%2520experimental%2520analysis%2520of%2520diverse%250Amodels%252C%2520parameter%2520configurations%252C%2520and%2520training%2520strategies%252C%2520accompanied%2520by%2520an%250Ain-depth%2520examination%2520of%2520the%2520statistical%2520distribution%2520inherent%2520in%2520human%2520mobility%250Apatterns.%2520Our%2520empirical%2520evaluations%2520encompass%2520both%2520Long%2520Short-Term%2520Memory%2520and%250ATransformer-based%2520architectures%252C%2520and%2520further%2520investigate%2520how%2520incorporating%250Aindividual%2520life%2520patterns%2520can%2520enhance%2520the%2520effectiveness%2520of%2520the%2520prediction.%2520We%250Ashow%2520that%2520explicitly%2520including%2520semantic%2520information%2520such%2520as%2520day-of-the-week%2520and%250Auser-specific%2520historical%2520information%2520can%2520help%2520the%2520model%2520better%2520understand%250Aindividual%2520patterns%2520of%2520life%2520and%2520improve%2520predictions.%2520Moreover%252C%2520since%2520the%250Aabsence%2520of%2520explicit%2520user%2520information%2520is%2520often%2520missing%2520due%2520to%2520user%2520privacy%252C%2520we%250Ashow%2520that%2520the%2520sampling%2520of%2520users%2520may%2520exacerbate%2520data%2520skewness%2520and%2520result%2520in%2520a%250Asubstantial%2520loss%2520in%2520predictive%2520accuracy.%2520To%2520mitigate%2520data%2520imbalance%2520and%250Apreserve%2520diversity%252C%2520we%2520apply%2520user%2520semantic%2520clustering%2520with%2520stratified%2520sampling%250Ato%2520ensure%2520that%2520the%2520sampled%2520dataset%2520remains%2520representative.%2520Our%2520results%2520further%250Ashow%2520that%2520small-batch%2520stochastic%2520gradient%2520optimization%2520improves%2520model%250Aperformance%252C%2520especially%2520when%2520human%2520mobility%2520training%2520data%2520is%2520limited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Machine%20Learning%20Models%20on%20Human%20Spatio-temporal%20Mobility%20Data%3A%0A%20%20An%20Experimental%20Study%20%5BExperiment%20Paper%5D&entry.906535625=Yueyang%20Liu%20and%20Lance%20Kennedy%20and%20Ruochen%20Kong%20and%20Joon-Seok%20Kim%20and%20Andreas%20Z%C3%BCfle&entry.1292438233=%20%20Individual-level%20human%20mobility%20prediction%20has%20emerged%20as%20a%20significant%20topic%0Aof%20research%20with%20applications%20in%20infectious%20disease%20monitoring%2C%20child%2C%20and%0Aelderly%20care.%20Existing%20studies%20predominantly%20focus%20on%20the%20microscopic%20aspects%0Aof%20human%20trajectories%3A%20such%20as%20predicting%20short-term%20trajectories%20or%20the%20next%0Alocation%20visited%2C%20while%20offering%20limited%20attention%20to%20macro-level%20mobility%0Apatterns%20and%20the%20corresponding%20life%20routines.%20In%20this%20paper%2C%20we%20focus%20on%20an%0Aunderexplored%20problem%20in%20human%20mobility%20prediction%3A%20determining%20the%20best%0Apractices%20to%20train%20a%20machine%20learning%20model%20using%20historical%20data%20to%20forecast%0Aan%20individuals%20complete%20trajectory%20over%20the%20next%20days%20and%20weeks.%20In%20this%0Aexperiment%20paper%2C%20we%20undertake%20a%20comprehensive%20experimental%20analysis%20of%20diverse%0Amodels%2C%20parameter%20configurations%2C%20and%20training%20strategies%2C%20accompanied%20by%20an%0Ain-depth%20examination%20of%20the%20statistical%20distribution%20inherent%20in%20human%20mobility%0Apatterns.%20Our%20empirical%20evaluations%20encompass%20both%20Long%20Short-Term%20Memory%20and%0ATransformer-based%20architectures%2C%20and%20further%20investigate%20how%20incorporating%0Aindividual%20life%20patterns%20can%20enhance%20the%20effectiveness%20of%20the%20prediction.%20We%0Ashow%20that%20explicitly%20including%20semantic%20information%20such%20as%20day-of-the-week%20and%0Auser-specific%20historical%20information%20can%20help%20the%20model%20better%20understand%0Aindividual%20patterns%20of%20life%20and%20improve%20predictions.%20Moreover%2C%20since%20the%0Aabsence%20of%20explicit%20user%20information%20is%20often%20missing%20due%20to%20user%20privacy%2C%20we%0Ashow%20that%20the%20sampling%20of%20users%20may%20exacerbate%20data%20skewness%20and%20result%20in%20a%0Asubstantial%20loss%20in%20predictive%20accuracy.%20To%20mitigate%20data%20imbalance%20and%0Apreserve%20diversity%2C%20we%20apply%20user%20semantic%20clustering%20with%20stratified%20sampling%0Ato%20ensure%20that%20the%20sampled%20dataset%20remains%20representative.%20Our%20results%20further%0Ashow%20that%20small-batch%20stochastic%20gradient%20optimization%20improves%20model%0Aperformance%2C%20especially%20when%20human%20mobility%20training%20data%20is%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13135v1&entry.124074799=Read"},
{"title": "Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical\n  Reasoning with Enhanced Semantic Discrimination", "author": "Yizhou Liu and Jingwei Wei and Zizhi Chen and Minghao Han and Xukun Zhang and Keliang Liu and Lihua Zhang", "abstract": "  Reinforcement learning (RL) with rule-based rewards has demonstrated strong\npotential in enhancing the reasoning and generalization capabilities of\nvision-language models (VLMs) and large language models (LLMs), while reducing\ncomputational overhead. However, its application in medical imaging remains\nunderexplored. Existing reinforcement fine-tuning (RFT) approaches in this\ndomain primarily target closed-ended visual question answering (VQA), limiting\ntheir applicability to real-world clinical reasoning. In contrast, open-ended\nmedical VQA better reflects clinical practice but has received limited\nattention. While some efforts have sought to unify both formats via\nsemantically guided RL, we observe that model-based semantic rewards often\nsuffer from reward collapse, where responses with significant semantic\ndifferences receive similar scores. To address this, we propose ARMed (Adaptive\nReinforcement for Medical Reasoning), a novel RL framework for open-ended\nmedical VQA. ARMed first incorporates domain knowledge through supervised\nfine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning\nwith textual correctness and adaptive semantic rewards to enhance reasoning\nquality. We evaluate ARMed on six challenging medical VQA benchmarks. Results\nshow that ARMed consistently boosts both accuracy and generalization, achieving\na 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain\nbenchmarks. These results highlight the critical role of reward\ndiscriminability in medical RL and the promise of semantically guided rewards\nfor enabling robust and clinically meaningful multimodal reasoning.\n", "link": "http://arxiv.org/abs/2508.12957v1", "date": "2025-08-18", "relevancy": 2.1088, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Reward%20Collapse%3A%20Adaptive%20Reinforcement%20for%20Open-ended%20Medical%0A%20%20Reasoning%20with%20Enhanced%20Semantic%20Discrimination&body=Title%3A%20Breaking%20Reward%20Collapse%3A%20Adaptive%20Reinforcement%20for%20Open-ended%20Medical%0A%20%20Reasoning%20with%20Enhanced%20Semantic%20Discrimination%0AAuthor%3A%20Yizhou%20Liu%20and%20Jingwei%20Wei%20and%20Zizhi%20Chen%20and%20Minghao%20Han%20and%20Xukun%20Zhang%20and%20Keliang%20Liu%20and%20Lihua%20Zhang%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20with%20rule-based%20rewards%20has%20demonstrated%20strong%0Apotential%20in%20enhancing%20the%20reasoning%20and%20generalization%20capabilities%20of%0Avision-language%20models%20%28VLMs%29%20and%20large%20language%20models%20%28LLMs%29%2C%20while%20reducing%0Acomputational%20overhead.%20However%2C%20its%20application%20in%20medical%20imaging%20remains%0Aunderexplored.%20Existing%20reinforcement%20fine-tuning%20%28RFT%29%20approaches%20in%20this%0Adomain%20primarily%20target%20closed-ended%20visual%20question%20answering%20%28VQA%29%2C%20limiting%0Atheir%20applicability%20to%20real-world%20clinical%20reasoning.%20In%20contrast%2C%20open-ended%0Amedical%20VQA%20better%20reflects%20clinical%20practice%20but%20has%20received%20limited%0Aattention.%20While%20some%20efforts%20have%20sought%20to%20unify%20both%20formats%20via%0Asemantically%20guided%20RL%2C%20we%20observe%20that%20model-based%20semantic%20rewards%20often%0Asuffer%20from%20reward%20collapse%2C%20where%20responses%20with%20significant%20semantic%0Adifferences%20receive%20similar%20scores.%20To%20address%20this%2C%20we%20propose%20ARMed%20%28Adaptive%0AReinforcement%20for%20Medical%20Reasoning%29%2C%20a%20novel%20RL%20framework%20for%20open-ended%0Amedical%20VQA.%20ARMed%20first%20incorporates%20domain%20knowledge%20through%20supervised%0Afine-tuning%20%28SFT%29%20on%20chain-of-thought%20data%2C%20then%20applies%20reinforcement%20learning%0Awith%20textual%20correctness%20and%20adaptive%20semantic%20rewards%20to%20enhance%20reasoning%0Aquality.%20We%20evaluate%20ARMed%20on%20six%20challenging%20medical%20VQA%20benchmarks.%20Results%0Ashow%20that%20ARMed%20consistently%20boosts%20both%20accuracy%20and%20generalization%2C%20achieving%0Aa%2032.64%25%20improvement%20on%20in-domain%20tasks%20and%20an%2011.65%25%20gain%20on%20out-of-domain%0Abenchmarks.%20These%20results%20highlight%20the%20critical%20role%20of%20reward%0Adiscriminability%20in%20medical%20RL%20and%20the%20promise%20of%20semantically%20guided%20rewards%0Afor%20enabling%20robust%20and%20clinically%20meaningful%20multimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Reward%2520Collapse%253A%2520Adaptive%2520Reinforcement%2520for%2520Open-ended%2520Medical%250A%2520%2520Reasoning%2520with%2520Enhanced%2520Semantic%2520Discrimination%26entry.906535625%3DYizhou%2520Liu%2520and%2520Jingwei%2520Wei%2520and%2520Zizhi%2520Chen%2520and%2520Minghao%2520Han%2520and%2520Xukun%2520Zhang%2520and%2520Keliang%2520Liu%2520and%2520Lihua%2520Zhang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520with%2520rule-based%2520rewards%2520has%2520demonstrated%2520strong%250Apotential%2520in%2520enhancing%2520the%2520reasoning%2520and%2520generalization%2520capabilities%2520of%250Avision-language%2520models%2520%2528VLMs%2529%2520and%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520while%2520reducing%250Acomputational%2520overhead.%2520However%252C%2520its%2520application%2520in%2520medical%2520imaging%2520remains%250Aunderexplored.%2520Existing%2520reinforcement%2520fine-tuning%2520%2528RFT%2529%2520approaches%2520in%2520this%250Adomain%2520primarily%2520target%2520closed-ended%2520visual%2520question%2520answering%2520%2528VQA%2529%252C%2520limiting%250Atheir%2520applicability%2520to%2520real-world%2520clinical%2520reasoning.%2520In%2520contrast%252C%2520open-ended%250Amedical%2520VQA%2520better%2520reflects%2520clinical%2520practice%2520but%2520has%2520received%2520limited%250Aattention.%2520While%2520some%2520efforts%2520have%2520sought%2520to%2520unify%2520both%2520formats%2520via%250Asemantically%2520guided%2520RL%252C%2520we%2520observe%2520that%2520model-based%2520semantic%2520rewards%2520often%250Asuffer%2520from%2520reward%2520collapse%252C%2520where%2520responses%2520with%2520significant%2520semantic%250Adifferences%2520receive%2520similar%2520scores.%2520To%2520address%2520this%252C%2520we%2520propose%2520ARMed%2520%2528Adaptive%250AReinforcement%2520for%2520Medical%2520Reasoning%2529%252C%2520a%2520novel%2520RL%2520framework%2520for%2520open-ended%250Amedical%2520VQA.%2520ARMed%2520first%2520incorporates%2520domain%2520knowledge%2520through%2520supervised%250Afine-tuning%2520%2528SFT%2529%2520on%2520chain-of-thought%2520data%252C%2520then%2520applies%2520reinforcement%2520learning%250Awith%2520textual%2520correctness%2520and%2520adaptive%2520semantic%2520rewards%2520to%2520enhance%2520reasoning%250Aquality.%2520We%2520evaluate%2520ARMed%2520on%2520six%2520challenging%2520medical%2520VQA%2520benchmarks.%2520Results%250Ashow%2520that%2520ARMed%2520consistently%2520boosts%2520both%2520accuracy%2520and%2520generalization%252C%2520achieving%250Aa%252032.64%2525%2520improvement%2520on%2520in-domain%2520tasks%2520and%2520an%252011.65%2525%2520gain%2520on%2520out-of-domain%250Abenchmarks.%2520These%2520results%2520highlight%2520the%2520critical%2520role%2520of%2520reward%250Adiscriminability%2520in%2520medical%2520RL%2520and%2520the%2520promise%2520of%2520semantically%2520guided%2520rewards%250Afor%2520enabling%2520robust%2520and%2520clinically%2520meaningful%2520multimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Reward%20Collapse%3A%20Adaptive%20Reinforcement%20for%20Open-ended%20Medical%0A%20%20Reasoning%20with%20Enhanced%20Semantic%20Discrimination&entry.906535625=Yizhou%20Liu%20and%20Jingwei%20Wei%20and%20Zizhi%20Chen%20and%20Minghao%20Han%20and%20Xukun%20Zhang%20and%20Keliang%20Liu%20and%20Lihua%20Zhang&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20with%20rule-based%20rewards%20has%20demonstrated%20strong%0Apotential%20in%20enhancing%20the%20reasoning%20and%20generalization%20capabilities%20of%0Avision-language%20models%20%28VLMs%29%20and%20large%20language%20models%20%28LLMs%29%2C%20while%20reducing%0Acomputational%20overhead.%20However%2C%20its%20application%20in%20medical%20imaging%20remains%0Aunderexplored.%20Existing%20reinforcement%20fine-tuning%20%28RFT%29%20approaches%20in%20this%0Adomain%20primarily%20target%20closed-ended%20visual%20question%20answering%20%28VQA%29%2C%20limiting%0Atheir%20applicability%20to%20real-world%20clinical%20reasoning.%20In%20contrast%2C%20open-ended%0Amedical%20VQA%20better%20reflects%20clinical%20practice%20but%20has%20received%20limited%0Aattention.%20While%20some%20efforts%20have%20sought%20to%20unify%20both%20formats%20via%0Asemantically%20guided%20RL%2C%20we%20observe%20that%20model-based%20semantic%20rewards%20often%0Asuffer%20from%20reward%20collapse%2C%20where%20responses%20with%20significant%20semantic%0Adifferences%20receive%20similar%20scores.%20To%20address%20this%2C%20we%20propose%20ARMed%20%28Adaptive%0AReinforcement%20for%20Medical%20Reasoning%29%2C%20a%20novel%20RL%20framework%20for%20open-ended%0Amedical%20VQA.%20ARMed%20first%20incorporates%20domain%20knowledge%20through%20supervised%0Afine-tuning%20%28SFT%29%20on%20chain-of-thought%20data%2C%20then%20applies%20reinforcement%20learning%0Awith%20textual%20correctness%20and%20adaptive%20semantic%20rewards%20to%20enhance%20reasoning%0Aquality.%20We%20evaluate%20ARMed%20on%20six%20challenging%20medical%20VQA%20benchmarks.%20Results%0Ashow%20that%20ARMed%20consistently%20boosts%20both%20accuracy%20and%20generalization%2C%20achieving%0Aa%2032.64%25%20improvement%20on%20in-domain%20tasks%20and%20an%2011.65%25%20gain%20on%20out-of-domain%0Abenchmarks.%20These%20results%20highlight%20the%20critical%20role%20of%20reward%0Adiscriminability%20in%20medical%20RL%20and%20the%20promise%20of%20semantically%20guided%20rewards%0Afor%20enabling%20robust%20and%20clinically%20meaningful%20multimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12957v1&entry.124074799=Read"},
{"title": "A locally statistical active contour model for SAR image segmentation\n  can be solved by denoising algorithms", "author": "Guangming Liu", "abstract": "  In this paper, we propose a novel locally statistical variational active\ncontour model based on I-divergence-TV denoising model, which hybrides geodesic\nactive contour (GAC) model with active contours without edges (ACWE) model, and\ncan be used to segment images corrupted by multiplicative gamma noise. By\nadding a diffusion term into the level set evolution (LSE) equation of the\nproposed model, we construct a reaction-diffusion (RD) equation, which can\ngradually regularize the level set function (LSF) to be piecewise constant in\neach segment domain and gain the stable solution. We further transform the\nproposed model into classic ROF model by adding a proximity term. [27] is\nsubmitted on 29-Aug-2013, and our early edition ever submitted to TGRS on\n12-Jun-2012, Venkatakrishnan et al. [31] proposed their 'pnp algorithm' on\n29-May-2013, so Venkatakrishnan and we proposed the 'pnp algorithm' almost\nsimultaneously. Inspired by a fast denoising algorithm proposed by Jia-Zhao\nrecently, we propose two fast fixed point algorithms to solve SAR image\nsegmentation question. Experimental results for real SAR images show that the\nproposed image segmentation model can efficiently stop the contours at weak or\nblurred edges, and can automatically detect the exterior and interior\nboundaries of images with multiplicative gamma noise. The proposed FPRD1/FPRD2\nmodels are about 1/2 (or less than) of the time required for the SBRD model\nbased on the Split Bregman technique.\n", "link": "http://arxiv.org/abs/2401.10083v2", "date": "2025-08-18", "relevancy": 2.1057, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5293}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20locally%20statistical%20active%20contour%20model%20for%20SAR%20image%20segmentation%0A%20%20can%20be%20solved%20by%20denoising%20algorithms&body=Title%3A%20A%20locally%20statistical%20active%20contour%20model%20for%20SAR%20image%20segmentation%0A%20%20can%20be%20solved%20by%20denoising%20algorithms%0AAuthor%3A%20Guangming%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20locally%20statistical%20variational%20active%0Acontour%20model%20based%20on%20I-divergence-TV%20denoising%20model%2C%20which%20hybrides%20geodesic%0Aactive%20contour%20%28GAC%29%20model%20with%20active%20contours%20without%20edges%20%28ACWE%29%20model%2C%20and%0Acan%20be%20used%20to%20segment%20images%20corrupted%20by%20multiplicative%20gamma%20noise.%20By%0Aadding%20a%20diffusion%20term%20into%20the%20level%20set%20evolution%20%28LSE%29%20equation%20of%20the%0Aproposed%20model%2C%20we%20construct%20a%20reaction-diffusion%20%28RD%29%20equation%2C%20which%20can%0Agradually%20regularize%20the%20level%20set%20function%20%28LSF%29%20to%20be%20piecewise%20constant%20in%0Aeach%20segment%20domain%20and%20gain%20the%20stable%20solution.%20We%20further%20transform%20the%0Aproposed%20model%20into%20classic%20ROF%20model%20by%20adding%20a%20proximity%20term.%20%5B27%5D%20is%0Asubmitted%20on%2029-Aug-2013%2C%20and%20our%20early%20edition%20ever%20submitted%20to%20TGRS%20on%0A12-Jun-2012%2C%20Venkatakrishnan%20et%20al.%20%5B31%5D%20proposed%20their%20%27pnp%20algorithm%27%20on%0A29-May-2013%2C%20so%20Venkatakrishnan%20and%20we%20proposed%20the%20%27pnp%20algorithm%27%20almost%0Asimultaneously.%20Inspired%20by%20a%20fast%20denoising%20algorithm%20proposed%20by%20Jia-Zhao%0Arecently%2C%20we%20propose%20two%20fast%20fixed%20point%20algorithms%20to%20solve%20SAR%20image%0Asegmentation%20question.%20Experimental%20results%20for%20real%20SAR%20images%20show%20that%20the%0Aproposed%20image%20segmentation%20model%20can%20efficiently%20stop%20the%20contours%20at%20weak%20or%0Ablurred%20edges%2C%20and%20can%20automatically%20detect%20the%20exterior%20and%20interior%0Aboundaries%20of%20images%20with%20multiplicative%20gamma%20noise.%20The%20proposed%20FPRD1/FPRD2%0Amodels%20are%20about%201/2%20%28or%20less%20than%29%20of%20the%20time%20required%20for%20the%20SBRD%20model%0Abased%20on%20the%20Split%20Bregman%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520locally%2520statistical%2520active%2520contour%2520model%2520for%2520SAR%2520image%2520segmentation%250A%2520%2520can%2520be%2520solved%2520by%2520denoising%2520algorithms%26entry.906535625%3DGuangming%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520locally%2520statistical%2520variational%2520active%250Acontour%2520model%2520based%2520on%2520I-divergence-TV%2520denoising%2520model%252C%2520which%2520hybrides%2520geodesic%250Aactive%2520contour%2520%2528GAC%2529%2520model%2520with%2520active%2520contours%2520without%2520edges%2520%2528ACWE%2529%2520model%252C%2520and%250Acan%2520be%2520used%2520to%2520segment%2520images%2520corrupted%2520by%2520multiplicative%2520gamma%2520noise.%2520By%250Aadding%2520a%2520diffusion%2520term%2520into%2520the%2520level%2520set%2520evolution%2520%2528LSE%2529%2520equation%2520of%2520the%250Aproposed%2520model%252C%2520we%2520construct%2520a%2520reaction-diffusion%2520%2528RD%2529%2520equation%252C%2520which%2520can%250Agradually%2520regularize%2520the%2520level%2520set%2520function%2520%2528LSF%2529%2520to%2520be%2520piecewise%2520constant%2520in%250Aeach%2520segment%2520domain%2520and%2520gain%2520the%2520stable%2520solution.%2520We%2520further%2520transform%2520the%250Aproposed%2520model%2520into%2520classic%2520ROF%2520model%2520by%2520adding%2520a%2520proximity%2520term.%2520%255B27%255D%2520is%250Asubmitted%2520on%252029-Aug-2013%252C%2520and%2520our%2520early%2520edition%2520ever%2520submitted%2520to%2520TGRS%2520on%250A12-Jun-2012%252C%2520Venkatakrishnan%2520et%2520al.%2520%255B31%255D%2520proposed%2520their%2520%2527pnp%2520algorithm%2527%2520on%250A29-May-2013%252C%2520so%2520Venkatakrishnan%2520and%2520we%2520proposed%2520the%2520%2527pnp%2520algorithm%2527%2520almost%250Asimultaneously.%2520Inspired%2520by%2520a%2520fast%2520denoising%2520algorithm%2520proposed%2520by%2520Jia-Zhao%250Arecently%252C%2520we%2520propose%2520two%2520fast%2520fixed%2520point%2520algorithms%2520to%2520solve%2520SAR%2520image%250Asegmentation%2520question.%2520Experimental%2520results%2520for%2520real%2520SAR%2520images%2520show%2520that%2520the%250Aproposed%2520image%2520segmentation%2520model%2520can%2520efficiently%2520stop%2520the%2520contours%2520at%2520weak%2520or%250Ablurred%2520edges%252C%2520and%2520can%2520automatically%2520detect%2520the%2520exterior%2520and%2520interior%250Aboundaries%2520of%2520images%2520with%2520multiplicative%2520gamma%2520noise.%2520The%2520proposed%2520FPRD1/FPRD2%250Amodels%2520are%2520about%25201/2%2520%2528or%2520less%2520than%2529%2520of%2520the%2520time%2520required%2520for%2520the%2520SBRD%2520model%250Abased%2520on%2520the%2520Split%2520Bregman%2520technique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20locally%20statistical%20active%20contour%20model%20for%20SAR%20image%20segmentation%0A%20%20can%20be%20solved%20by%20denoising%20algorithms&entry.906535625=Guangming%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20locally%20statistical%20variational%20active%0Acontour%20model%20based%20on%20I-divergence-TV%20denoising%20model%2C%20which%20hybrides%20geodesic%0Aactive%20contour%20%28GAC%29%20model%20with%20active%20contours%20without%20edges%20%28ACWE%29%20model%2C%20and%0Acan%20be%20used%20to%20segment%20images%20corrupted%20by%20multiplicative%20gamma%20noise.%20By%0Aadding%20a%20diffusion%20term%20into%20the%20level%20set%20evolution%20%28LSE%29%20equation%20of%20the%0Aproposed%20model%2C%20we%20construct%20a%20reaction-diffusion%20%28RD%29%20equation%2C%20which%20can%0Agradually%20regularize%20the%20level%20set%20function%20%28LSF%29%20to%20be%20piecewise%20constant%20in%0Aeach%20segment%20domain%20and%20gain%20the%20stable%20solution.%20We%20further%20transform%20the%0Aproposed%20model%20into%20classic%20ROF%20model%20by%20adding%20a%20proximity%20term.%20%5B27%5D%20is%0Asubmitted%20on%2029-Aug-2013%2C%20and%20our%20early%20edition%20ever%20submitted%20to%20TGRS%20on%0A12-Jun-2012%2C%20Venkatakrishnan%20et%20al.%20%5B31%5D%20proposed%20their%20%27pnp%20algorithm%27%20on%0A29-May-2013%2C%20so%20Venkatakrishnan%20and%20we%20proposed%20the%20%27pnp%20algorithm%27%20almost%0Asimultaneously.%20Inspired%20by%20a%20fast%20denoising%20algorithm%20proposed%20by%20Jia-Zhao%0Arecently%2C%20we%20propose%20two%20fast%20fixed%20point%20algorithms%20to%20solve%20SAR%20image%0Asegmentation%20question.%20Experimental%20results%20for%20real%20SAR%20images%20show%20that%20the%0Aproposed%20image%20segmentation%20model%20can%20efficiently%20stop%20the%20contours%20at%20weak%20or%0Ablurred%20edges%2C%20and%20can%20automatically%20detect%20the%20exterior%20and%20interior%0Aboundaries%20of%20images%20with%20multiplicative%20gamma%20noise.%20The%20proposed%20FPRD1/FPRD2%0Amodels%20are%20about%201/2%20%28or%20less%20than%29%20of%20the%20time%20required%20for%20the%20SBRD%20model%0Abased%20on%20the%20Split%20Bregman%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10083v2&entry.124074799=Read"},
{"title": "Towards High-Resolution Industrial Image Anomaly Detection", "author": "Ximiao Zhang and Min Xu and Xiuzhuang Zhou", "abstract": "  Current anomaly detection methods primarily focus on low-resolution\nscenarios. For high-resolution images, conventional downsampling often results\nin missed detections of subtle anomalous regions due to the loss of\nfine-grained discriminative information. Despite some progress, recent studies\nhave attempted to improve detection resolution by employing lightweight\nnetworks or using simple image tiling and ensemble methods. However, these\napproaches still struggle to meet the practical demands of industrial scenarios\nin terms of detection accuracy and efficiency. To address the above issues, we\npropose HiAD, a general framework for high-resolution anomaly detection. HiAD\nis capable of detecting anomalous regions of varying sizes in high-resolution\nimages under limited computational resources. Specifically, HiAD employs a\ndual-branch architecture that integrates anomaly cues across different scales\nto comprehensively capture both subtle and large-scale anomalies. Furthermore,\nit incorporates a multi-resolution feature fusion strategy to tackle the\nchallenges posed by fine-grained texture variations in high-resolution images.\nTo enhance both adaptability and efficiency, HiAD utilizes a detector pool in\nconjunction with various detector assignment strategies, enabling detectors to\nbe adaptively assigned based on patch features, ensuring detection performance\nwhile effectively controlling computational costs. We conduct extensive\nexperiments on our specifically constructed high-resolution anomaly detection\nbenchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark\nRealIAD-HD, demonstrating the superior performance of HiAD. The code is\navailable at https://github.com/cnulab/HiAD.\n", "link": "http://arxiv.org/abs/2508.12931v1", "date": "2025-08-18", "relevancy": 2.1, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5323}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5312}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20High-Resolution%20Industrial%20Image%20Anomaly%20Detection&body=Title%3A%20Towards%20High-Resolution%20Industrial%20Image%20Anomaly%20Detection%0AAuthor%3A%20Ximiao%20Zhang%20and%20Min%20Xu%20and%20Xiuzhuang%20Zhou%0AAbstract%3A%20%20%20Current%20anomaly%20detection%20methods%20primarily%20focus%20on%20low-resolution%0Ascenarios.%20For%20high-resolution%20images%2C%20conventional%20downsampling%20often%20results%0Ain%20missed%20detections%20of%20subtle%20anomalous%20regions%20due%20to%20the%20loss%20of%0Afine-grained%20discriminative%20information.%20Despite%20some%20progress%2C%20recent%20studies%0Ahave%20attempted%20to%20improve%20detection%20resolution%20by%20employing%20lightweight%0Anetworks%20or%20using%20simple%20image%20tiling%20and%20ensemble%20methods.%20However%2C%20these%0Aapproaches%20still%20struggle%20to%20meet%20the%20practical%20demands%20of%20industrial%20scenarios%0Ain%20terms%20of%20detection%20accuracy%20and%20efficiency.%20To%20address%20the%20above%20issues%2C%20we%0Apropose%20HiAD%2C%20a%20general%20framework%20for%20high-resolution%20anomaly%20detection.%20HiAD%0Ais%20capable%20of%20detecting%20anomalous%20regions%20of%20varying%20sizes%20in%20high-resolution%0Aimages%20under%20limited%20computational%20resources.%20Specifically%2C%20HiAD%20employs%20a%0Adual-branch%20architecture%20that%20integrates%20anomaly%20cues%20across%20different%20scales%0Ato%20comprehensively%20capture%20both%20subtle%20and%20large-scale%20anomalies.%20Furthermore%2C%0Ait%20incorporates%20a%20multi-resolution%20feature%20fusion%20strategy%20to%20tackle%20the%0Achallenges%20posed%20by%20fine-grained%20texture%20variations%20in%20high-resolution%20images.%0ATo%20enhance%20both%20adaptability%20and%20efficiency%2C%20HiAD%20utilizes%20a%20detector%20pool%20in%0Aconjunction%20with%20various%20detector%20assignment%20strategies%2C%20enabling%20detectors%20to%0Abe%20adaptively%20assigned%20based%20on%20patch%20features%2C%20ensuring%20detection%20performance%0Awhile%20effectively%20controlling%20computational%20costs.%20We%20conduct%20extensive%0Aexperiments%20on%20our%20specifically%20constructed%20high-resolution%20anomaly%20detection%0Abenchmarks%2C%20including%20MVTec-HD%2C%20VisA-HD%2C%20and%20the%20real-world%20benchmark%0ARealIAD-HD%2C%20demonstrating%20the%20superior%20performance%20of%20HiAD.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/cnulab/HiAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520High-Resolution%2520Industrial%2520Image%2520Anomaly%2520Detection%26entry.906535625%3DXimiao%2520Zhang%2520and%2520Min%2520Xu%2520and%2520Xiuzhuang%2520Zhou%26entry.1292438233%3D%2520%2520Current%2520anomaly%2520detection%2520methods%2520primarily%2520focus%2520on%2520low-resolution%250Ascenarios.%2520For%2520high-resolution%2520images%252C%2520conventional%2520downsampling%2520often%2520results%250Ain%2520missed%2520detections%2520of%2520subtle%2520anomalous%2520regions%2520due%2520to%2520the%2520loss%2520of%250Afine-grained%2520discriminative%2520information.%2520Despite%2520some%2520progress%252C%2520recent%2520studies%250Ahave%2520attempted%2520to%2520improve%2520detection%2520resolution%2520by%2520employing%2520lightweight%250Anetworks%2520or%2520using%2520simple%2520image%2520tiling%2520and%2520ensemble%2520methods.%2520However%252C%2520these%250Aapproaches%2520still%2520struggle%2520to%2520meet%2520the%2520practical%2520demands%2520of%2520industrial%2520scenarios%250Ain%2520terms%2520of%2520detection%2520accuracy%2520and%2520efficiency.%2520To%2520address%2520the%2520above%2520issues%252C%2520we%250Apropose%2520HiAD%252C%2520a%2520general%2520framework%2520for%2520high-resolution%2520anomaly%2520detection.%2520HiAD%250Ais%2520capable%2520of%2520detecting%2520anomalous%2520regions%2520of%2520varying%2520sizes%2520in%2520high-resolution%250Aimages%2520under%2520limited%2520computational%2520resources.%2520Specifically%252C%2520HiAD%2520employs%2520a%250Adual-branch%2520architecture%2520that%2520integrates%2520anomaly%2520cues%2520across%2520different%2520scales%250Ato%2520comprehensively%2520capture%2520both%2520subtle%2520and%2520large-scale%2520anomalies.%2520Furthermore%252C%250Ait%2520incorporates%2520a%2520multi-resolution%2520feature%2520fusion%2520strategy%2520to%2520tackle%2520the%250Achallenges%2520posed%2520by%2520fine-grained%2520texture%2520variations%2520in%2520high-resolution%2520images.%250ATo%2520enhance%2520both%2520adaptability%2520and%2520efficiency%252C%2520HiAD%2520utilizes%2520a%2520detector%2520pool%2520in%250Aconjunction%2520with%2520various%2520detector%2520assignment%2520strategies%252C%2520enabling%2520detectors%2520to%250Abe%2520adaptively%2520assigned%2520based%2520on%2520patch%2520features%252C%2520ensuring%2520detection%2520performance%250Awhile%2520effectively%2520controlling%2520computational%2520costs.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520our%2520specifically%2520constructed%2520high-resolution%2520anomaly%2520detection%250Abenchmarks%252C%2520including%2520MVTec-HD%252C%2520VisA-HD%252C%2520and%2520the%2520real-world%2520benchmark%250ARealIAD-HD%252C%2520demonstrating%2520the%2520superior%2520performance%2520of%2520HiAD.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/cnulab/HiAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20High-Resolution%20Industrial%20Image%20Anomaly%20Detection&entry.906535625=Ximiao%20Zhang%20and%20Min%20Xu%20and%20Xiuzhuang%20Zhou&entry.1292438233=%20%20Current%20anomaly%20detection%20methods%20primarily%20focus%20on%20low-resolution%0Ascenarios.%20For%20high-resolution%20images%2C%20conventional%20downsampling%20often%20results%0Ain%20missed%20detections%20of%20subtle%20anomalous%20regions%20due%20to%20the%20loss%20of%0Afine-grained%20discriminative%20information.%20Despite%20some%20progress%2C%20recent%20studies%0Ahave%20attempted%20to%20improve%20detection%20resolution%20by%20employing%20lightweight%0Anetworks%20or%20using%20simple%20image%20tiling%20and%20ensemble%20methods.%20However%2C%20these%0Aapproaches%20still%20struggle%20to%20meet%20the%20practical%20demands%20of%20industrial%20scenarios%0Ain%20terms%20of%20detection%20accuracy%20and%20efficiency.%20To%20address%20the%20above%20issues%2C%20we%0Apropose%20HiAD%2C%20a%20general%20framework%20for%20high-resolution%20anomaly%20detection.%20HiAD%0Ais%20capable%20of%20detecting%20anomalous%20regions%20of%20varying%20sizes%20in%20high-resolution%0Aimages%20under%20limited%20computational%20resources.%20Specifically%2C%20HiAD%20employs%20a%0Adual-branch%20architecture%20that%20integrates%20anomaly%20cues%20across%20different%20scales%0Ato%20comprehensively%20capture%20both%20subtle%20and%20large-scale%20anomalies.%20Furthermore%2C%0Ait%20incorporates%20a%20multi-resolution%20feature%20fusion%20strategy%20to%20tackle%20the%0Achallenges%20posed%20by%20fine-grained%20texture%20variations%20in%20high-resolution%20images.%0ATo%20enhance%20both%20adaptability%20and%20efficiency%2C%20HiAD%20utilizes%20a%20detector%20pool%20in%0Aconjunction%20with%20various%20detector%20assignment%20strategies%2C%20enabling%20detectors%20to%0Abe%20adaptively%20assigned%20based%20on%20patch%20features%2C%20ensuring%20detection%20performance%0Awhile%20effectively%20controlling%20computational%20costs.%20We%20conduct%20extensive%0Aexperiments%20on%20our%20specifically%20constructed%20high-resolution%20anomaly%20detection%0Abenchmarks%2C%20including%20MVTec-HD%2C%20VisA-HD%2C%20and%20the%20real-world%20benchmark%0ARealIAD-HD%2C%20demonstrating%20the%20superior%20performance%20of%20HiAD.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/cnulab/HiAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12931v1&entry.124074799=Read"},
{"title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation\n  System with Multimodal Large Language Model", "author": "Ronghao Lin and Shuai Shen and Weipeng Hu and Qiaolin He and Aolin Xiong and Li Huang and Haifeng Hu and Yap-peng Tan", "abstract": "  Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.\n", "link": "http://arxiv.org/abs/2508.12854v1", "date": "2025-08-18", "relevancy": 2.0971, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5547}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5206}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E3RG%3A%20Building%20Explicit%20Emotion-driven%20Empathetic%20Response%20Generation%0A%20%20System%20with%20Multimodal%20Large%20Language%20Model&body=Title%3A%20E3RG%3A%20Building%20Explicit%20Emotion-driven%20Empathetic%20Response%20Generation%0A%20%20System%20with%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Ronghao%20Lin%20and%20Shuai%20Shen%20and%20Weipeng%20Hu%20and%20Qiaolin%20He%20and%20Aolin%20Xiong%20and%20Li%20Huang%20and%20Haifeng%20Hu%20and%20Yap-peng%20Tan%0AAbstract%3A%20%20%20Multimodal%20Empathetic%20Response%20Generation%20%28MERG%29%20is%20crucial%20for%20building%0Aemotionally%20intelligent%20human-computer%20interactions.%20Although%20large%20language%0Amodels%20%28LLMs%29%20have%20improved%20text-based%20ERG%2C%20challenges%20remain%20in%20handling%0Amultimodal%20emotional%20content%20and%20maintaining%20identity%20consistency.%20Thus%2C%20we%0Apropose%20E3RG%2C%20an%20Explicit%20Emotion-driven%20Empathetic%20Response%20Generation%20System%0Abased%20on%20multimodal%20LLMs%20which%20decomposes%20MERG%20task%20into%20three%20parts%3A%0Amultimodal%20empathy%20understanding%2C%20empathy%20memory%20retrieval%2C%20and%20multimodal%0Aresponse%20generation.%20By%20integrating%20advanced%20expressive%20speech%20and%20video%0Agenerative%20models%2C%20E3RG%20delivers%20natural%2C%20emotionally%20rich%2C%20and%0Aidentity-consistent%20responses%20without%20extra%20training.%20Experiments%20validate%20the%0Asuperiority%20of%20our%20system%20on%20both%20zero-shot%20and%20few-shot%20settings%2C%20securing%0ATop-1%20position%20in%20the%20Avatar-based%20Multimodal%20Empathy%20Challenge%20on%20ACM%20MM%2025.%0AOur%20code%20is%20available%20at%20https%3A//github.com/RH-Lin/E3RG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE3RG%253A%2520Building%2520Explicit%2520Emotion-driven%2520Empathetic%2520Response%2520Generation%250A%2520%2520System%2520with%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DRonghao%2520Lin%2520and%2520Shuai%2520Shen%2520and%2520Weipeng%2520Hu%2520and%2520Qiaolin%2520He%2520and%2520Aolin%2520Xiong%2520and%2520Li%2520Huang%2520and%2520Haifeng%2520Hu%2520and%2520Yap-peng%2520Tan%26entry.1292438233%3D%2520%2520Multimodal%2520Empathetic%2520Response%2520Generation%2520%2528MERG%2529%2520is%2520crucial%2520for%2520building%250Aemotionally%2520intelligent%2520human-computer%2520interactions.%2520Although%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520have%2520improved%2520text-based%2520ERG%252C%2520challenges%2520remain%2520in%2520handling%250Amultimodal%2520emotional%2520content%2520and%2520maintaining%2520identity%2520consistency.%2520Thus%252C%2520we%250Apropose%2520E3RG%252C%2520an%2520Explicit%2520Emotion-driven%2520Empathetic%2520Response%2520Generation%2520System%250Abased%2520on%2520multimodal%2520LLMs%2520which%2520decomposes%2520MERG%2520task%2520into%2520three%2520parts%253A%250Amultimodal%2520empathy%2520understanding%252C%2520empathy%2520memory%2520retrieval%252C%2520and%2520multimodal%250Aresponse%2520generation.%2520By%2520integrating%2520advanced%2520expressive%2520speech%2520and%2520video%250Agenerative%2520models%252C%2520E3RG%2520delivers%2520natural%252C%2520emotionally%2520rich%252C%2520and%250Aidentity-consistent%2520responses%2520without%2520extra%2520training.%2520Experiments%2520validate%2520the%250Asuperiority%2520of%2520our%2520system%2520on%2520both%2520zero-shot%2520and%2520few-shot%2520settings%252C%2520securing%250ATop-1%2520position%2520in%2520the%2520Avatar-based%2520Multimodal%2520Empathy%2520Challenge%2520on%2520ACM%2520MM%252025.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/RH-Lin/E3RG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E3RG%3A%20Building%20Explicit%20Emotion-driven%20Empathetic%20Response%20Generation%0A%20%20System%20with%20Multimodal%20Large%20Language%20Model&entry.906535625=Ronghao%20Lin%20and%20Shuai%20Shen%20and%20Weipeng%20Hu%20and%20Qiaolin%20He%20and%20Aolin%20Xiong%20and%20Li%20Huang%20and%20Haifeng%20Hu%20and%20Yap-peng%20Tan&entry.1292438233=%20%20Multimodal%20Empathetic%20Response%20Generation%20%28MERG%29%20is%20crucial%20for%20building%0Aemotionally%20intelligent%20human-computer%20interactions.%20Although%20large%20language%0Amodels%20%28LLMs%29%20have%20improved%20text-based%20ERG%2C%20challenges%20remain%20in%20handling%0Amultimodal%20emotional%20content%20and%20maintaining%20identity%20consistency.%20Thus%2C%20we%0Apropose%20E3RG%2C%20an%20Explicit%20Emotion-driven%20Empathetic%20Response%20Generation%20System%0Abased%20on%20multimodal%20LLMs%20which%20decomposes%20MERG%20task%20into%20three%20parts%3A%0Amultimodal%20empathy%20understanding%2C%20empathy%20memory%20retrieval%2C%20and%20multimodal%0Aresponse%20generation.%20By%20integrating%20advanced%20expressive%20speech%20and%20video%0Agenerative%20models%2C%20E3RG%20delivers%20natural%2C%20emotionally%20rich%2C%20and%0Aidentity-consistent%20responses%20without%20extra%20training.%20Experiments%20validate%20the%0Asuperiority%20of%20our%20system%20on%20both%20zero-shot%20and%20few-shot%20settings%2C%20securing%0ATop-1%20position%20in%20the%20Avatar-based%20Multimodal%20Empathy%20Challenge%20on%20ACM%20MM%2025.%0AOur%20code%20is%20available%20at%20https%3A//github.com/RH-Lin/E3RG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12854v1&entry.124074799=Read"},
{"title": "Randomized PCA Forest for Outlier Detection", "author": "Muhammad Rajabinasab and Farhad Pakdaman and Moncef Gabbouj and Peter Schneider-Kamp and Arthur Zimek", "abstract": "  We propose a novel unsupervised outlier detection method based on Randomized\nPrincipal Component Analysis (PCA). Inspired by the performance of Randomized\nPCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a\nnovel unsupervised outlier detection method that utilizes RPCA Forest for\noutlier detection. Experimental results showcase the superiority of the\nproposed approach compared to the classical and state-of-the-art methods in\nperforming the outlier detection task on several datasets while performing\ncompetitively on the rest. The extensive analysis of the proposed method\nreflects it high generalization power and its computational efficiency,\nhighlighting it as a good choice for unsupervised outlier detection.\n", "link": "http://arxiv.org/abs/2508.12776v1", "date": "2025-08-18", "relevancy": 2.0874, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4167}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Randomized%20PCA%20Forest%20for%20Outlier%20Detection&body=Title%3A%20Randomized%20PCA%20Forest%20for%20Outlier%20Detection%0AAuthor%3A%20Muhammad%20Rajabinasab%20and%20Farhad%20Pakdaman%20and%20Moncef%20Gabbouj%20and%20Peter%20Schneider-Kamp%20and%20Arthur%20Zimek%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20unsupervised%20outlier%20detection%20method%20based%20on%20Randomized%0APrincipal%20Component%20Analysis%20%28PCA%29.%20Inspired%20by%20the%20performance%20of%20Randomized%0APCA%20%28RPCA%29%20Forest%20in%20approximate%20K-Nearest%20Neighbor%20%28KNN%29%20search%2C%20we%20develop%20a%0Anovel%20unsupervised%20outlier%20detection%20method%20that%20utilizes%20RPCA%20Forest%20for%0Aoutlier%20detection.%20Experimental%20results%20showcase%20the%20superiority%20of%20the%0Aproposed%20approach%20compared%20to%20the%20classical%20and%20state-of-the-art%20methods%20in%0Aperforming%20the%20outlier%20detection%20task%20on%20several%20datasets%20while%20performing%0Acompetitively%20on%20the%20rest.%20The%20extensive%20analysis%20of%20the%20proposed%20method%0Areflects%20it%20high%20generalization%20power%20and%20its%20computational%20efficiency%2C%0Ahighlighting%20it%20as%20a%20good%20choice%20for%20unsupervised%20outlier%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandomized%2520PCA%2520Forest%2520for%2520Outlier%2520Detection%26entry.906535625%3DMuhammad%2520Rajabinasab%2520and%2520Farhad%2520Pakdaman%2520and%2520Moncef%2520Gabbouj%2520and%2520Peter%2520Schneider-Kamp%2520and%2520Arthur%2520Zimek%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520unsupervised%2520outlier%2520detection%2520method%2520based%2520on%2520Randomized%250APrincipal%2520Component%2520Analysis%2520%2528PCA%2529.%2520Inspired%2520by%2520the%2520performance%2520of%2520Randomized%250APCA%2520%2528RPCA%2529%2520Forest%2520in%2520approximate%2520K-Nearest%2520Neighbor%2520%2528KNN%2529%2520search%252C%2520we%2520develop%2520a%250Anovel%2520unsupervised%2520outlier%2520detection%2520method%2520that%2520utilizes%2520RPCA%2520Forest%2520for%250Aoutlier%2520detection.%2520Experimental%2520results%2520showcase%2520the%2520superiority%2520of%2520the%250Aproposed%2520approach%2520compared%2520to%2520the%2520classical%2520and%2520state-of-the-art%2520methods%2520in%250Aperforming%2520the%2520outlier%2520detection%2520task%2520on%2520several%2520datasets%2520while%2520performing%250Acompetitively%2520on%2520the%2520rest.%2520The%2520extensive%2520analysis%2520of%2520the%2520proposed%2520method%250Areflects%2520it%2520high%2520generalization%2520power%2520and%2520its%2520computational%2520efficiency%252C%250Ahighlighting%2520it%2520as%2520a%2520good%2520choice%2520for%2520unsupervised%2520outlier%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomized%20PCA%20Forest%20for%20Outlier%20Detection&entry.906535625=Muhammad%20Rajabinasab%20and%20Farhad%20Pakdaman%20and%20Moncef%20Gabbouj%20and%20Peter%20Schneider-Kamp%20and%20Arthur%20Zimek&entry.1292438233=%20%20We%20propose%20a%20novel%20unsupervised%20outlier%20detection%20method%20based%20on%20Randomized%0APrincipal%20Component%20Analysis%20%28PCA%29.%20Inspired%20by%20the%20performance%20of%20Randomized%0APCA%20%28RPCA%29%20Forest%20in%20approximate%20K-Nearest%20Neighbor%20%28KNN%29%20search%2C%20we%20develop%20a%0Anovel%20unsupervised%20outlier%20detection%20method%20that%20utilizes%20RPCA%20Forest%20for%0Aoutlier%20detection.%20Experimental%20results%20showcase%20the%20superiority%20of%20the%0Aproposed%20approach%20compared%20to%20the%20classical%20and%20state-of-the-art%20methods%20in%0Aperforming%20the%20outlier%20detection%20task%20on%20several%20datasets%20while%20performing%0Acompetitively%20on%20the%20rest.%20The%20extensive%20analysis%20of%20the%20proposed%20method%0Areflects%20it%20high%20generalization%20power%20and%20its%20computational%20efficiency%2C%0Ahighlighting%20it%20as%20a%20good%20choice%20for%20unsupervised%20outlier%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12776v1&entry.124074799=Read"},
{"title": "Checkmate: interpretable and explainable RSVQA is the endgame", "author": "Lucrezia Tosato and Christel Tartini Chappuis and Syrielle Montariol and Flora Weissgerber and Sylvain Lobry and Devis Tuia", "abstract": "  Remote Sensing Visual Question Answering (RSVQA) presents unique challenges\nin ensuring that model decisions are both understandable and grounded in visual\ncontent. Current models often suffer from a lack of interpretability and\nexplainability, as well as from biases in dataset distributions that lead to\nshortcut learning. In this work, we tackle these issues by introducing a novel\nRSVQA dataset, Chessboard, designed to minimize biases through 3'123'253\nquestions and a balanced answer distribution. Each answer is linked to one or\nmore cells within the image, enabling fine-grained visual reasoning.\n  Building on this dataset, we develop an explainable and interpretable model\ncalled Checkmate that identifies the image cells most relevant to its\ndecisions. Through extensive experiments across multiple model architectures,\nwe show that our approach improves transparency and supports more trustworthy\ndecision-making in RSVQA systems.\n", "link": "http://arxiv.org/abs/2508.13086v1", "date": "2025-08-18", "relevancy": 2.0863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Checkmate%3A%20interpretable%20and%20explainable%20RSVQA%20is%20the%20endgame&body=Title%3A%20Checkmate%3A%20interpretable%20and%20explainable%20RSVQA%20is%20the%20endgame%0AAuthor%3A%20Lucrezia%20Tosato%20and%20Christel%20Tartini%20Chappuis%20and%20Syrielle%20Montariol%20and%20Flora%20Weissgerber%20and%20Sylvain%20Lobry%20and%20Devis%20Tuia%0AAbstract%3A%20%20%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RSVQA%29%20presents%20unique%20challenges%0Ain%20ensuring%20that%20model%20decisions%20are%20both%20understandable%20and%20grounded%20in%20visual%0Acontent.%20Current%20models%20often%20suffer%20from%20a%20lack%20of%20interpretability%20and%0Aexplainability%2C%20as%20well%20as%20from%20biases%20in%20dataset%20distributions%20that%20lead%20to%0Ashortcut%20learning.%20In%20this%20work%2C%20we%20tackle%20these%20issues%20by%20introducing%20a%20novel%0ARSVQA%20dataset%2C%20Chessboard%2C%20designed%20to%20minimize%20biases%20through%203%27123%27253%0Aquestions%20and%20a%20balanced%20answer%20distribution.%20Each%20answer%20is%20linked%20to%20one%20or%0Amore%20cells%20within%20the%20image%2C%20enabling%20fine-grained%20visual%20reasoning.%0A%20%20Building%20on%20this%20dataset%2C%20we%20develop%20an%20explainable%20and%20interpretable%20model%0Acalled%20Checkmate%20that%20identifies%20the%20image%20cells%20most%20relevant%20to%20its%0Adecisions.%20Through%20extensive%20experiments%20across%20multiple%20model%20architectures%2C%0Awe%20show%20that%20our%20approach%20improves%20transparency%20and%20supports%20more%20trustworthy%0Adecision-making%20in%20RSVQA%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCheckmate%253A%2520interpretable%2520and%2520explainable%2520RSVQA%2520is%2520the%2520endgame%26entry.906535625%3DLucrezia%2520Tosato%2520and%2520Christel%2520Tartini%2520Chappuis%2520and%2520Syrielle%2520Montariol%2520and%2520Flora%2520Weissgerber%2520and%2520Sylvain%2520Lobry%2520and%2520Devis%2520Tuia%26entry.1292438233%3D%2520%2520Remote%2520Sensing%2520Visual%2520Question%2520Answering%2520%2528RSVQA%2529%2520presents%2520unique%2520challenges%250Ain%2520ensuring%2520that%2520model%2520decisions%2520are%2520both%2520understandable%2520and%2520grounded%2520in%2520visual%250Acontent.%2520Current%2520models%2520often%2520suffer%2520from%2520a%2520lack%2520of%2520interpretability%2520and%250Aexplainability%252C%2520as%2520well%2520as%2520from%2520biases%2520in%2520dataset%2520distributions%2520that%2520lead%2520to%250Ashortcut%2520learning.%2520In%2520this%2520work%252C%2520we%2520tackle%2520these%2520issues%2520by%2520introducing%2520a%2520novel%250ARSVQA%2520dataset%252C%2520Chessboard%252C%2520designed%2520to%2520minimize%2520biases%2520through%25203%2527123%2527253%250Aquestions%2520and%2520a%2520balanced%2520answer%2520distribution.%2520Each%2520answer%2520is%2520linked%2520to%2520one%2520or%250Amore%2520cells%2520within%2520the%2520image%252C%2520enabling%2520fine-grained%2520visual%2520reasoning.%250A%2520%2520Building%2520on%2520this%2520dataset%252C%2520we%2520develop%2520an%2520explainable%2520and%2520interpretable%2520model%250Acalled%2520Checkmate%2520that%2520identifies%2520the%2520image%2520cells%2520most%2520relevant%2520to%2520its%250Adecisions.%2520Through%2520extensive%2520experiments%2520across%2520multiple%2520model%2520architectures%252C%250Awe%2520show%2520that%2520our%2520approach%2520improves%2520transparency%2520and%2520supports%2520more%2520trustworthy%250Adecision-making%2520in%2520RSVQA%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Checkmate%3A%20interpretable%20and%20explainable%20RSVQA%20is%20the%20endgame&entry.906535625=Lucrezia%20Tosato%20and%20Christel%20Tartini%20Chappuis%20and%20Syrielle%20Montariol%20and%20Flora%20Weissgerber%20and%20Sylvain%20Lobry%20and%20Devis%20Tuia&entry.1292438233=%20%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RSVQA%29%20presents%20unique%20challenges%0Ain%20ensuring%20that%20model%20decisions%20are%20both%20understandable%20and%20grounded%20in%20visual%0Acontent.%20Current%20models%20often%20suffer%20from%20a%20lack%20of%20interpretability%20and%0Aexplainability%2C%20as%20well%20as%20from%20biases%20in%20dataset%20distributions%20that%20lead%20to%0Ashortcut%20learning.%20In%20this%20work%2C%20we%20tackle%20these%20issues%20by%20introducing%20a%20novel%0ARSVQA%20dataset%2C%20Chessboard%2C%20designed%20to%20minimize%20biases%20through%203%27123%27253%0Aquestions%20and%20a%20balanced%20answer%20distribution.%20Each%20answer%20is%20linked%20to%20one%20or%0Amore%20cells%20within%20the%20image%2C%20enabling%20fine-grained%20visual%20reasoning.%0A%20%20Building%20on%20this%20dataset%2C%20we%20develop%20an%20explainable%20and%20interpretable%20model%0Acalled%20Checkmate%20that%20identifies%20the%20image%20cells%20most%20relevant%20to%20its%0Adecisions.%20Through%20extensive%20experiments%20across%20multiple%20model%20architectures%2C%0Awe%20show%20that%20our%20approach%20improves%20transparency%20and%20supports%20more%20trustworthy%0Adecision-making%20in%20RSVQA%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13086v1&entry.124074799=Read"},
{"title": "V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using\n  the iRAP Standard?", "author": "Natchapon Jongwiriyanurak and Zichao Zeng and June Moh Goo and Xinglei Wang and Ilya Ilyankou and Kerkritt Sriroongvikrai and Nicola Christie and Meihui Wang and Huanfa Chen and James Haworth", "abstract": "  Road safety assessments are critical yet costly, especially in Low- and\nMiddle-Income Countries (LMICs), where most roads remain unrated. Traditional\nmethods require expert annotation and training data, while supervised\nlearning-based approaches struggle to generalise across regions. In this paper,\nwe introduce \\textit{V-RoAst}, a zero-shot Visual Question Answering (VQA)\nframework using Vision-Language Models (VLMs) to classify road safety\nattributes defined by the iRAP standard. We introduce the first open-source\ndataset from ThaiRAP, consisting of over 2,000 curated street-level images from\nThailand annotated for this task. We evaluate Gemini-1.5-flash and GPT-4o-mini\non this dataset and benchmark their performance against VGGNet and ResNet\nbaselines. While VLMs underperform on spatial awareness, they generalise well\nto unseen classes and offer flexible prompt-based reasoning without retraining.\nOur results show that VLMs can serve as automatic road assessment tools when\nintegrated with complementary data. This work is the first to explore VLMs for\nzero-shot infrastructure risk assessment and opens new directions for\nautomatic, low-cost road safety mapping. Code and dataset:\nhttps://github.com/PongNJ/V-RoAst.\n", "link": "http://arxiv.org/abs/2408.10872v5", "date": "2025-08-18", "relevancy": 2.0715, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-RoAst%3A%20Visual%20Road%20Assessment.%20Can%20VLM%20be%20a%20Road%20Safety%20Assessor%20Using%0A%20%20the%20iRAP%20Standard%3F&body=Title%3A%20V-RoAst%3A%20Visual%20Road%20Assessment.%20Can%20VLM%20be%20a%20Road%20Safety%20Assessor%20Using%0A%20%20the%20iRAP%20Standard%3F%0AAuthor%3A%20Natchapon%20Jongwiriyanurak%20and%20Zichao%20Zeng%20and%20June%20Moh%20Goo%20and%20Xinglei%20Wang%20and%20Ilya%20Ilyankou%20and%20Kerkritt%20Sriroongvikrai%20and%20Nicola%20Christie%20and%20Meihui%20Wang%20and%20Huanfa%20Chen%20and%20James%20Haworth%0AAbstract%3A%20%20%20Road%20safety%20assessments%20are%20critical%20yet%20costly%2C%20especially%20in%20Low-%20and%0AMiddle-Income%20Countries%20%28LMICs%29%2C%20where%20most%20roads%20remain%20unrated.%20Traditional%0Amethods%20require%20expert%20annotation%20and%20training%20data%2C%20while%20supervised%0Alearning-based%20approaches%20struggle%20to%20generalise%20across%20regions.%20In%20this%20paper%2C%0Awe%20introduce%20%5Ctextit%7BV-RoAst%7D%2C%20a%20zero-shot%20Visual%20Question%20Answering%20%28VQA%29%0Aframework%20using%20Vision-Language%20Models%20%28VLMs%29%20to%20classify%20road%20safety%0Aattributes%20defined%20by%20the%20iRAP%20standard.%20We%20introduce%20the%20first%20open-source%0Adataset%20from%20ThaiRAP%2C%20consisting%20of%20over%202%2C000%20curated%20street-level%20images%20from%0AThailand%20annotated%20for%20this%20task.%20We%20evaluate%20Gemini-1.5-flash%20and%20GPT-4o-mini%0Aon%20this%20dataset%20and%20benchmark%20their%20performance%20against%20VGGNet%20and%20ResNet%0Abaselines.%20While%20VLMs%20underperform%20on%20spatial%20awareness%2C%20they%20generalise%20well%0Ato%20unseen%20classes%20and%20offer%20flexible%20prompt-based%20reasoning%20without%20retraining.%0AOur%20results%20show%20that%20VLMs%20can%20serve%20as%20automatic%20road%20assessment%20tools%20when%0Aintegrated%20with%20complementary%20data.%20This%20work%20is%20the%20first%20to%20explore%20VLMs%20for%0Azero-shot%20infrastructure%20risk%20assessment%20and%20opens%20new%20directions%20for%0Aautomatic%2C%20low-cost%20road%20safety%20mapping.%20Code%20and%20dataset%3A%0Ahttps%3A//github.com/PongNJ/V-RoAst.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10872v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-RoAst%253A%2520Visual%2520Road%2520Assessment.%2520Can%2520VLM%2520be%2520a%2520Road%2520Safety%2520Assessor%2520Using%250A%2520%2520the%2520iRAP%2520Standard%253F%26entry.906535625%3DNatchapon%2520Jongwiriyanurak%2520and%2520Zichao%2520Zeng%2520and%2520June%2520Moh%2520Goo%2520and%2520Xinglei%2520Wang%2520and%2520Ilya%2520Ilyankou%2520and%2520Kerkritt%2520Sriroongvikrai%2520and%2520Nicola%2520Christie%2520and%2520Meihui%2520Wang%2520and%2520Huanfa%2520Chen%2520and%2520James%2520Haworth%26entry.1292438233%3D%2520%2520Road%2520safety%2520assessments%2520are%2520critical%2520yet%2520costly%252C%2520especially%2520in%2520Low-%2520and%250AMiddle-Income%2520Countries%2520%2528LMICs%2529%252C%2520where%2520most%2520roads%2520remain%2520unrated.%2520Traditional%250Amethods%2520require%2520expert%2520annotation%2520and%2520training%2520data%252C%2520while%2520supervised%250Alearning-based%2520approaches%2520struggle%2520to%2520generalise%2520across%2520regions.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520%255Ctextit%257BV-RoAst%257D%252C%2520a%2520zero-shot%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%250Aframework%2520using%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520classify%2520road%2520safety%250Aattributes%2520defined%2520by%2520the%2520iRAP%2520standard.%2520We%2520introduce%2520the%2520first%2520open-source%250Adataset%2520from%2520ThaiRAP%252C%2520consisting%2520of%2520over%25202%252C000%2520curated%2520street-level%2520images%2520from%250AThailand%2520annotated%2520for%2520this%2520task.%2520We%2520evaluate%2520Gemini-1.5-flash%2520and%2520GPT-4o-mini%250Aon%2520this%2520dataset%2520and%2520benchmark%2520their%2520performance%2520against%2520VGGNet%2520and%2520ResNet%250Abaselines.%2520While%2520VLMs%2520underperform%2520on%2520spatial%2520awareness%252C%2520they%2520generalise%2520well%250Ato%2520unseen%2520classes%2520and%2520offer%2520flexible%2520prompt-based%2520reasoning%2520without%2520retraining.%250AOur%2520results%2520show%2520that%2520VLMs%2520can%2520serve%2520as%2520automatic%2520road%2520assessment%2520tools%2520when%250Aintegrated%2520with%2520complementary%2520data.%2520This%2520work%2520is%2520the%2520first%2520to%2520explore%2520VLMs%2520for%250Azero-shot%2520infrastructure%2520risk%2520assessment%2520and%2520opens%2520new%2520directions%2520for%250Aautomatic%252C%2520low-cost%2520road%2520safety%2520mapping.%2520Code%2520and%2520dataset%253A%250Ahttps%253A//github.com/PongNJ/V-RoAst.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10872v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-RoAst%3A%20Visual%20Road%20Assessment.%20Can%20VLM%20be%20a%20Road%20Safety%20Assessor%20Using%0A%20%20the%20iRAP%20Standard%3F&entry.906535625=Natchapon%20Jongwiriyanurak%20and%20Zichao%20Zeng%20and%20June%20Moh%20Goo%20and%20Xinglei%20Wang%20and%20Ilya%20Ilyankou%20and%20Kerkritt%20Sriroongvikrai%20and%20Nicola%20Christie%20and%20Meihui%20Wang%20and%20Huanfa%20Chen%20and%20James%20Haworth&entry.1292438233=%20%20Road%20safety%20assessments%20are%20critical%20yet%20costly%2C%20especially%20in%20Low-%20and%0AMiddle-Income%20Countries%20%28LMICs%29%2C%20where%20most%20roads%20remain%20unrated.%20Traditional%0Amethods%20require%20expert%20annotation%20and%20training%20data%2C%20while%20supervised%0Alearning-based%20approaches%20struggle%20to%20generalise%20across%20regions.%20In%20this%20paper%2C%0Awe%20introduce%20%5Ctextit%7BV-RoAst%7D%2C%20a%20zero-shot%20Visual%20Question%20Answering%20%28VQA%29%0Aframework%20using%20Vision-Language%20Models%20%28VLMs%29%20to%20classify%20road%20safety%0Aattributes%20defined%20by%20the%20iRAP%20standard.%20We%20introduce%20the%20first%20open-source%0Adataset%20from%20ThaiRAP%2C%20consisting%20of%20over%202%2C000%20curated%20street-level%20images%20from%0AThailand%20annotated%20for%20this%20task.%20We%20evaluate%20Gemini-1.5-flash%20and%20GPT-4o-mini%0Aon%20this%20dataset%20and%20benchmark%20their%20performance%20against%20VGGNet%20and%20ResNet%0Abaselines.%20While%20VLMs%20underperform%20on%20spatial%20awareness%2C%20they%20generalise%20well%0Ato%20unseen%20classes%20and%20offer%20flexible%20prompt-based%20reasoning%20without%20retraining.%0AOur%20results%20show%20that%20VLMs%20can%20serve%20as%20automatic%20road%20assessment%20tools%20when%0Aintegrated%20with%20complementary%20data.%20This%20work%20is%20the%20first%20to%20explore%20VLMs%20for%0Azero-shot%20infrastructure%20risk%20assessment%20and%20opens%20new%20directions%20for%0Aautomatic%2C%20low-cost%20road%20safety%20mapping.%20Code%20and%20dataset%3A%0Ahttps%3A//github.com/PongNJ/V-RoAst.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10872v5&entry.124074799=Read"},
{"title": "Action is All You Need: Dual-Flow Generative Ranking Network for\n  Recommendation", "author": "Hao Guo and Erpeng Xue and Lei Huang and Shichao Wang and Xiaolei Wang and Lei Wang and Jinpeng Wang and Sheng Chen", "abstract": "  Deep Learning Recommendation Models (DLRMs) often rely on extensive manual\nfeature engineering to improve accuracy and user experience, which increases\nsystem complexity and limits scalability of model performance with respect to\ncomputational resources. Recently, Meta introduced a generative ranking\nparadigm based on HSTU block that enables end-to-end learning from raw user\nbehavior sequences and demonstrates scaling law on large datasets that can be\nregarded as the state-of-the-art (SOTA). However, splitting user behaviors into\ninterleaved item and action information significantly increases the input\nsequence length, which adversely affects both training and inference\nefficiency. To address this issue, we propose the Dual-Flow Generative Ranking\nNetwork (DFGR), that employs a dual-flow mechanism to optimize interaction\nmodeling, ensuring efficient training and inference through end-to-end token\nprocessing. DFGR duplicates the original user behavior sequence into a real\nflow and a fake flow based on the authenticity of the action information, and\nthen defines a novel interaction method between the real flow and the fake flow\nwithin the QKV module of the self-attention mechanism. This design reduces\ncomputational overhead and improves both training efficiency and inference\nperformance compared to Meta's HSTU-based model. Experiments on both\nopen-source and real industrial datasets show that DFGR outperforms DLRM, which\nserves as the industrial online baseline with extensive feature engineering, as\nwell as Meta's HSTU and other common recommendation models such as DIN, DCN,\nDIEN, and DeepFM. Furthermore, we investigate optimal parameter allocation\nstrategies under computational constraints, establishing DFGR as an efficient\nand effective next-generation generative ranking paradigm.\n", "link": "http://arxiv.org/abs/2505.16752v3", "date": "2025-08-18", "relevancy": 2.0625, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5185}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5168}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action%20is%20All%20You%20Need%3A%20Dual-Flow%20Generative%20Ranking%20Network%20for%0A%20%20Recommendation&body=Title%3A%20Action%20is%20All%20You%20Need%3A%20Dual-Flow%20Generative%20Ranking%20Network%20for%0A%20%20Recommendation%0AAuthor%3A%20Hao%20Guo%20and%20Erpeng%20Xue%20and%20Lei%20Huang%20and%20Shichao%20Wang%20and%20Xiaolei%20Wang%20and%20Lei%20Wang%20and%20Jinpeng%20Wang%20and%20Sheng%20Chen%0AAbstract%3A%20%20%20Deep%20Learning%20Recommendation%20Models%20%28DLRMs%29%20often%20rely%20on%20extensive%20manual%0Afeature%20engineering%20to%20improve%20accuracy%20and%20user%20experience%2C%20which%20increases%0Asystem%20complexity%20and%20limits%20scalability%20of%20model%20performance%20with%20respect%20to%0Acomputational%20resources.%20Recently%2C%20Meta%20introduced%20a%20generative%20ranking%0Aparadigm%20based%20on%20HSTU%20block%20that%20enables%20end-to-end%20learning%20from%20raw%20user%0Abehavior%20sequences%20and%20demonstrates%20scaling%20law%20on%20large%20datasets%20that%20can%20be%0Aregarded%20as%20the%20state-of-the-art%20%28SOTA%29.%20However%2C%20splitting%20user%20behaviors%20into%0Ainterleaved%20item%20and%20action%20information%20significantly%20increases%20the%20input%0Asequence%20length%2C%20which%20adversely%20affects%20both%20training%20and%20inference%0Aefficiency.%20To%20address%20this%20issue%2C%20we%20propose%20the%20Dual-Flow%20Generative%20Ranking%0ANetwork%20%28DFGR%29%2C%20that%20employs%20a%20dual-flow%20mechanism%20to%20optimize%20interaction%0Amodeling%2C%20ensuring%20efficient%20training%20and%20inference%20through%20end-to-end%20token%0Aprocessing.%20DFGR%20duplicates%20the%20original%20user%20behavior%20sequence%20into%20a%20real%0Aflow%20and%20a%20fake%20flow%20based%20on%20the%20authenticity%20of%20the%20action%20information%2C%20and%0Athen%20defines%20a%20novel%20interaction%20method%20between%20the%20real%20flow%20and%20the%20fake%20flow%0Awithin%20the%20QKV%20module%20of%20the%20self-attention%20mechanism.%20This%20design%20reduces%0Acomputational%20overhead%20and%20improves%20both%20training%20efficiency%20and%20inference%0Aperformance%20compared%20to%20Meta%27s%20HSTU-based%20model.%20Experiments%20on%20both%0Aopen-source%20and%20real%20industrial%20datasets%20show%20that%20DFGR%20outperforms%20DLRM%2C%20which%0Aserves%20as%20the%20industrial%20online%20baseline%20with%20extensive%20feature%20engineering%2C%20as%0Awell%20as%20Meta%27s%20HSTU%20and%20other%20common%20recommendation%20models%20such%20as%20DIN%2C%20DCN%2C%0ADIEN%2C%20and%20DeepFM.%20Furthermore%2C%20we%20investigate%20optimal%20parameter%20allocation%0Astrategies%20under%20computational%20constraints%2C%20establishing%20DFGR%20as%20an%20efficient%0Aand%20effective%20next-generation%20generative%20ranking%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16752v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction%2520is%2520All%2520You%2520Need%253A%2520Dual-Flow%2520Generative%2520Ranking%2520Network%2520for%250A%2520%2520Recommendation%26entry.906535625%3DHao%2520Guo%2520and%2520Erpeng%2520Xue%2520and%2520Lei%2520Huang%2520and%2520Shichao%2520Wang%2520and%2520Xiaolei%2520Wang%2520and%2520Lei%2520Wang%2520and%2520Jinpeng%2520Wang%2520and%2520Sheng%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520Recommendation%2520Models%2520%2528DLRMs%2529%2520often%2520rely%2520on%2520extensive%2520manual%250Afeature%2520engineering%2520to%2520improve%2520accuracy%2520and%2520user%2520experience%252C%2520which%2520increases%250Asystem%2520complexity%2520and%2520limits%2520scalability%2520of%2520model%2520performance%2520with%2520respect%2520to%250Acomputational%2520resources.%2520Recently%252C%2520Meta%2520introduced%2520a%2520generative%2520ranking%250Aparadigm%2520based%2520on%2520HSTU%2520block%2520that%2520enables%2520end-to-end%2520learning%2520from%2520raw%2520user%250Abehavior%2520sequences%2520and%2520demonstrates%2520scaling%2520law%2520on%2520large%2520datasets%2520that%2520can%2520be%250Aregarded%2520as%2520the%2520state-of-the-art%2520%2528SOTA%2529.%2520However%252C%2520splitting%2520user%2520behaviors%2520into%250Ainterleaved%2520item%2520and%2520action%2520information%2520significantly%2520increases%2520the%2520input%250Asequence%2520length%252C%2520which%2520adversely%2520affects%2520both%2520training%2520and%2520inference%250Aefficiency.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520Dual-Flow%2520Generative%2520Ranking%250ANetwork%2520%2528DFGR%2529%252C%2520that%2520employs%2520a%2520dual-flow%2520mechanism%2520to%2520optimize%2520interaction%250Amodeling%252C%2520ensuring%2520efficient%2520training%2520and%2520inference%2520through%2520end-to-end%2520token%250Aprocessing.%2520DFGR%2520duplicates%2520the%2520original%2520user%2520behavior%2520sequence%2520into%2520a%2520real%250Aflow%2520and%2520a%2520fake%2520flow%2520based%2520on%2520the%2520authenticity%2520of%2520the%2520action%2520information%252C%2520and%250Athen%2520defines%2520a%2520novel%2520interaction%2520method%2520between%2520the%2520real%2520flow%2520and%2520the%2520fake%2520flow%250Awithin%2520the%2520QKV%2520module%2520of%2520the%2520self-attention%2520mechanism.%2520This%2520design%2520reduces%250Acomputational%2520overhead%2520and%2520improves%2520both%2520training%2520efficiency%2520and%2520inference%250Aperformance%2520compared%2520to%2520Meta%2527s%2520HSTU-based%2520model.%2520Experiments%2520on%2520both%250Aopen-source%2520and%2520real%2520industrial%2520datasets%2520show%2520that%2520DFGR%2520outperforms%2520DLRM%252C%2520which%250Aserves%2520as%2520the%2520industrial%2520online%2520baseline%2520with%2520extensive%2520feature%2520engineering%252C%2520as%250Awell%2520as%2520Meta%2527s%2520HSTU%2520and%2520other%2520common%2520recommendation%2520models%2520such%2520as%2520DIN%252C%2520DCN%252C%250ADIEN%252C%2520and%2520DeepFM.%2520Furthermore%252C%2520we%2520investigate%2520optimal%2520parameter%2520allocation%250Astrategies%2520under%2520computational%2520constraints%252C%2520establishing%2520DFGR%2520as%2520an%2520efficient%250Aand%2520effective%2520next-generation%2520generative%2520ranking%2520paradigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16752v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action%20is%20All%20You%20Need%3A%20Dual-Flow%20Generative%20Ranking%20Network%20for%0A%20%20Recommendation&entry.906535625=Hao%20Guo%20and%20Erpeng%20Xue%20and%20Lei%20Huang%20and%20Shichao%20Wang%20and%20Xiaolei%20Wang%20and%20Lei%20Wang%20and%20Jinpeng%20Wang%20and%20Sheng%20Chen&entry.1292438233=%20%20Deep%20Learning%20Recommendation%20Models%20%28DLRMs%29%20often%20rely%20on%20extensive%20manual%0Afeature%20engineering%20to%20improve%20accuracy%20and%20user%20experience%2C%20which%20increases%0Asystem%20complexity%20and%20limits%20scalability%20of%20model%20performance%20with%20respect%20to%0Acomputational%20resources.%20Recently%2C%20Meta%20introduced%20a%20generative%20ranking%0Aparadigm%20based%20on%20HSTU%20block%20that%20enables%20end-to-end%20learning%20from%20raw%20user%0Abehavior%20sequences%20and%20demonstrates%20scaling%20law%20on%20large%20datasets%20that%20can%20be%0Aregarded%20as%20the%20state-of-the-art%20%28SOTA%29.%20However%2C%20splitting%20user%20behaviors%20into%0Ainterleaved%20item%20and%20action%20information%20significantly%20increases%20the%20input%0Asequence%20length%2C%20which%20adversely%20affects%20both%20training%20and%20inference%0Aefficiency.%20To%20address%20this%20issue%2C%20we%20propose%20the%20Dual-Flow%20Generative%20Ranking%0ANetwork%20%28DFGR%29%2C%20that%20employs%20a%20dual-flow%20mechanism%20to%20optimize%20interaction%0Amodeling%2C%20ensuring%20efficient%20training%20and%20inference%20through%20end-to-end%20token%0Aprocessing.%20DFGR%20duplicates%20the%20original%20user%20behavior%20sequence%20into%20a%20real%0Aflow%20and%20a%20fake%20flow%20based%20on%20the%20authenticity%20of%20the%20action%20information%2C%20and%0Athen%20defines%20a%20novel%20interaction%20method%20between%20the%20real%20flow%20and%20the%20fake%20flow%0Awithin%20the%20QKV%20module%20of%20the%20self-attention%20mechanism.%20This%20design%20reduces%0Acomputational%20overhead%20and%20improves%20both%20training%20efficiency%20and%20inference%0Aperformance%20compared%20to%20Meta%27s%20HSTU-based%20model.%20Experiments%20on%20both%0Aopen-source%20and%20real%20industrial%20datasets%20show%20that%20DFGR%20outperforms%20DLRM%2C%20which%0Aserves%20as%20the%20industrial%20online%20baseline%20with%20extensive%20feature%20engineering%2C%20as%0Awell%20as%20Meta%27s%20HSTU%20and%20other%20common%20recommendation%20models%20such%20as%20DIN%2C%20DCN%2C%0ADIEN%2C%20and%20DeepFM.%20Furthermore%2C%20we%20investigate%20optimal%20parameter%20allocation%0Astrategies%20under%20computational%20constraints%2C%20establishing%20DFGR%20as%20an%20efficient%0Aand%20effective%20next-generation%20generative%20ranking%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16752v3&entry.124074799=Read"},
{"title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up", "author": "Sakhinana Sagar Srinivas and Shivam Gupta and Venkataramana Runkana", "abstract": "  Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.\n", "link": "http://arxiv.org/abs/2505.24584v3", "date": "2025-08-18", "relevancy": 2.0608, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5503}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5097}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoChemSchematic%20AI%3A%20Agentic%20Physics-Aware%20Automation%20for%20Chemical%0A%20%20Manufacturing%20Scale-Up&body=Title%3A%20AutoChemSchematic%20AI%3A%20Agentic%20Physics-Aware%20Automation%20for%20Chemical%0A%20%20Manufacturing%20Scale-Up%0AAuthor%3A%20Sakhinana%20Sagar%20Srinivas%20and%20Shivam%20Gupta%20and%20Venkataramana%20Runkana%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20AI%20have%20accelerated%20the%20discovery%20of%20novel%0Achemicals%20and%20materials.%20However%2C%20scaling%20these%20discoveries%20to%20industrial%0Aproduction%20remains%20a%20major%20bottleneck%20due%20to%20the%20synthesis%20gap%20--%20the%20need%20to%0Adevelop%20entirely%20new%20manufacturing%20processes.%20This%20challenge%20requires%20detailed%0Aengineering%20blueprints%3A%20PFDs%20for%20equipment%20layouts%20and%20material/energy%20flows%2C%0Aand%20PIDs%20for%20process%20plant%20operations.%20Current%20AI%20systems%20cannot%20yet%20reliably%0Agenerate%20these%20critical%20engineering%20schematics%2C%20creating%20a%20fundamental%20obstacle%0Ato%20manufacturing%20scale-up%20of%20novel%20discoveries.%20We%20present%20a%20closed-loop%2C%0Aphysics-aware%20framework%20for%20automated%20generation%20of%20industrially%20viable%20PFDs%0Aand%20PIDs.%20The%20framework%20integrates%20three%20key%20components%3A%20%281%29%20domain-specialized%0Asmall%20language%20models%20%28SLMs%29%20trained%20for%20auto-generation%20of%20PFDs%20and%20PIDs%2C%20%282%29%0Aa%20hierarchical%20knowledge%20graph%20containing%20process%20flow%20and%20instrumentation%0Adescriptions%20for%201%2C020%2B%20chemicals%20for%20Graph%20Retrieval-Augmented%20Generation%0A%28GRAG%29%2C%20and%20%283%29%20an%20open-source%20chemical%20process%20simulator%20for%20modeling%2C%0Asimulation%2C%20optimization%2C%20and%20analysis%20of%20novel%20chemical%20processes.%20The%20SLMs%0Aare%20trained%20through%20a%20multi-stage%20pipeline%20on%20synthetic%20datasets%2C%20with%20process%0Asimulator-in-the-loop%20validation%20ensuring%20feasibility.%20To%20enhance%20computational%0Aefficiency%2C%20the%20framework%20implements%20structural%20pruning%20%28width%20and%20depth%29%0Aguided%20by%20importance%20heuristics%20to%20reduce%20language%20model%20size%20while%20preserving%0Aaccuracy%2C%20followed%20by%20advanced%20inference%20optimizations%20including%0AFlashAttention%2C%20Lookahead%20Decoding%2C%20PagedAttention%20with%20KV-cache%20quantization%2C%0Aand%20Test-Time%20Inference%20Scaling.%20Experimental%20results%20demonstrate%20that%20our%0Aframework%20generates%20simulator-validated%20process%20descriptions%20with%20high%0Afidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24584v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoChemSchematic%2520AI%253A%2520Agentic%2520Physics-Aware%2520Automation%2520for%2520Chemical%250A%2520%2520Manufacturing%2520Scale-Up%26entry.906535625%3DSakhinana%2520Sagar%2520Srinivas%2520and%2520Shivam%2520Gupta%2520and%2520Venkataramana%2520Runkana%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520AI%2520have%2520accelerated%2520the%2520discovery%2520of%2520novel%250Achemicals%2520and%2520materials.%2520However%252C%2520scaling%2520these%2520discoveries%2520to%2520industrial%250Aproduction%2520remains%2520a%2520major%2520bottleneck%2520due%2520to%2520the%2520synthesis%2520gap%2520--%2520the%2520need%2520to%250Adevelop%2520entirely%2520new%2520manufacturing%2520processes.%2520This%2520challenge%2520requires%2520detailed%250Aengineering%2520blueprints%253A%2520PFDs%2520for%2520equipment%2520layouts%2520and%2520material/energy%2520flows%252C%250Aand%2520PIDs%2520for%2520process%2520plant%2520operations.%2520Current%2520AI%2520systems%2520cannot%2520yet%2520reliably%250Agenerate%2520these%2520critical%2520engineering%2520schematics%252C%2520creating%2520a%2520fundamental%2520obstacle%250Ato%2520manufacturing%2520scale-up%2520of%2520novel%2520discoveries.%2520We%2520present%2520a%2520closed-loop%252C%250Aphysics-aware%2520framework%2520for%2520automated%2520generation%2520of%2520industrially%2520viable%2520PFDs%250Aand%2520PIDs.%2520The%2520framework%2520integrates%2520three%2520key%2520components%253A%2520%25281%2529%2520domain-specialized%250Asmall%2520language%2520models%2520%2528SLMs%2529%2520trained%2520for%2520auto-generation%2520of%2520PFDs%2520and%2520PIDs%252C%2520%25282%2529%250Aa%2520hierarchical%2520knowledge%2520graph%2520containing%2520process%2520flow%2520and%2520instrumentation%250Adescriptions%2520for%25201%252C020%252B%2520chemicals%2520for%2520Graph%2520Retrieval-Augmented%2520Generation%250A%2528GRAG%2529%252C%2520and%2520%25283%2529%2520an%2520open-source%2520chemical%2520process%2520simulator%2520for%2520modeling%252C%250Asimulation%252C%2520optimization%252C%2520and%2520analysis%2520of%2520novel%2520chemical%2520processes.%2520The%2520SLMs%250Aare%2520trained%2520through%2520a%2520multi-stage%2520pipeline%2520on%2520synthetic%2520datasets%252C%2520with%2520process%250Asimulator-in-the-loop%2520validation%2520ensuring%2520feasibility.%2520To%2520enhance%2520computational%250Aefficiency%252C%2520the%2520framework%2520implements%2520structural%2520pruning%2520%2528width%2520and%2520depth%2529%250Aguided%2520by%2520importance%2520heuristics%2520to%2520reduce%2520language%2520model%2520size%2520while%2520preserving%250Aaccuracy%252C%2520followed%2520by%2520advanced%2520inference%2520optimizations%2520including%250AFlashAttention%252C%2520Lookahead%2520Decoding%252C%2520PagedAttention%2520with%2520KV-cache%2520quantization%252C%250Aand%2520Test-Time%2520Inference%2520Scaling.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Aframework%2520generates%2520simulator-validated%2520process%2520descriptions%2520with%2520high%250Afidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24584v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoChemSchematic%20AI%3A%20Agentic%20Physics-Aware%20Automation%20for%20Chemical%0A%20%20Manufacturing%20Scale-Up&entry.906535625=Sakhinana%20Sagar%20Srinivas%20and%20Shivam%20Gupta%20and%20Venkataramana%20Runkana&entry.1292438233=%20%20Recent%20advances%20in%20generative%20AI%20have%20accelerated%20the%20discovery%20of%20novel%0Achemicals%20and%20materials.%20However%2C%20scaling%20these%20discoveries%20to%20industrial%0Aproduction%20remains%20a%20major%20bottleneck%20due%20to%20the%20synthesis%20gap%20--%20the%20need%20to%0Adevelop%20entirely%20new%20manufacturing%20processes.%20This%20challenge%20requires%20detailed%0Aengineering%20blueprints%3A%20PFDs%20for%20equipment%20layouts%20and%20material/energy%20flows%2C%0Aand%20PIDs%20for%20process%20plant%20operations.%20Current%20AI%20systems%20cannot%20yet%20reliably%0Agenerate%20these%20critical%20engineering%20schematics%2C%20creating%20a%20fundamental%20obstacle%0Ato%20manufacturing%20scale-up%20of%20novel%20discoveries.%20We%20present%20a%20closed-loop%2C%0Aphysics-aware%20framework%20for%20automated%20generation%20of%20industrially%20viable%20PFDs%0Aand%20PIDs.%20The%20framework%20integrates%20three%20key%20components%3A%20%281%29%20domain-specialized%0Asmall%20language%20models%20%28SLMs%29%20trained%20for%20auto-generation%20of%20PFDs%20and%20PIDs%2C%20%282%29%0Aa%20hierarchical%20knowledge%20graph%20containing%20process%20flow%20and%20instrumentation%0Adescriptions%20for%201%2C020%2B%20chemicals%20for%20Graph%20Retrieval-Augmented%20Generation%0A%28GRAG%29%2C%20and%20%283%29%20an%20open-source%20chemical%20process%20simulator%20for%20modeling%2C%0Asimulation%2C%20optimization%2C%20and%20analysis%20of%20novel%20chemical%20processes.%20The%20SLMs%0Aare%20trained%20through%20a%20multi-stage%20pipeline%20on%20synthetic%20datasets%2C%20with%20process%0Asimulator-in-the-loop%20validation%20ensuring%20feasibility.%20To%20enhance%20computational%0Aefficiency%2C%20the%20framework%20implements%20structural%20pruning%20%28width%20and%20depth%29%0Aguided%20by%20importance%20heuristics%20to%20reduce%20language%20model%20size%20while%20preserving%0Aaccuracy%2C%20followed%20by%20advanced%20inference%20optimizations%20including%0AFlashAttention%2C%20Lookahead%20Decoding%2C%20PagedAttention%20with%20KV-cache%20quantization%2C%0Aand%20Test-Time%20Inference%20Scaling.%20Experimental%20results%20demonstrate%20that%20our%0Aframework%20generates%20simulator-validated%20process%20descriptions%20with%20high%0Afidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24584v3&entry.124074799=Read"},
{"title": "PSScreen: Partially Supervised Multiple Retinal Disease Screening", "author": "Boyi Zheng and Qing Liu", "abstract": "  Leveraging multiple partially labeled datasets to train a model for multiple\nretinal disease screening reduces the reliance on fully annotated datasets, but\nremains challenging due to significant domain shifts across training datasets\nfrom various medical sites, and the label absent issue for partial classes. To\nsolve these challenges, we propose PSScreen, a novel Partially Supervised\nmultiple retinal disease Screening model. Our PSScreen consists of two streams\nand one learns deterministic features and the other learns probabilistic\nfeatures via uncertainty injection. Then, we leverage the textual guidance to\ndecouple two types of features into disease-wise features and align them via\nfeature distillation to boost the domain generalization ability. Meanwhile, we\nemploy pseudo label consistency between two streams to address the label absent\nissue and introduce a self-distillation to transfer task-relevant semantics\nabout known classes from the deterministic to the probabilistic stream to\nfurther enhance the detection performances. Experiments show that our PSScreen\nsignificantly enhances the detection performances on six retinal diseases and\nthe normal state averagely and achieves state-of-the-art results on both\nin-domain and out-of-domain datasets. Codes are available at\nhttps://github.com/boyiZheng99/PSScreen.\n", "link": "http://arxiv.org/abs/2508.10549v2", "date": "2025-08-18", "relevancy": 2.0572, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5268}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5182}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSScreen%3A%20Partially%20Supervised%20Multiple%20Retinal%20Disease%20Screening&body=Title%3A%20PSScreen%3A%20Partially%20Supervised%20Multiple%20Retinal%20Disease%20Screening%0AAuthor%3A%20Boyi%20Zheng%20and%20Qing%20Liu%0AAbstract%3A%20%20%20Leveraging%20multiple%20partially%20labeled%20datasets%20to%20train%20a%20model%20for%20multiple%0Aretinal%20disease%20screening%20reduces%20the%20reliance%20on%20fully%20annotated%20datasets%2C%20but%0Aremains%20challenging%20due%20to%20significant%20domain%20shifts%20across%20training%20datasets%0Afrom%20various%20medical%20sites%2C%20and%20the%20label%20absent%20issue%20for%20partial%20classes.%20To%0Asolve%20these%20challenges%2C%20we%20propose%20PSScreen%2C%20a%20novel%20Partially%20Supervised%0Amultiple%20retinal%20disease%20Screening%20model.%20Our%20PSScreen%20consists%20of%20two%20streams%0Aand%20one%20learns%20deterministic%20features%20and%20the%20other%20learns%20probabilistic%0Afeatures%20via%20uncertainty%20injection.%20Then%2C%20we%20leverage%20the%20textual%20guidance%20to%0Adecouple%20two%20types%20of%20features%20into%20disease-wise%20features%20and%20align%20them%20via%0Afeature%20distillation%20to%20boost%20the%20domain%20generalization%20ability.%20Meanwhile%2C%20we%0Aemploy%20pseudo%20label%20consistency%20between%20two%20streams%20to%20address%20the%20label%20absent%0Aissue%20and%20introduce%20a%20self-distillation%20to%20transfer%20task-relevant%20semantics%0Aabout%20known%20classes%20from%20the%20deterministic%20to%20the%20probabilistic%20stream%20to%0Afurther%20enhance%20the%20detection%20performances.%20Experiments%20show%20that%20our%20PSScreen%0Asignificantly%20enhances%20the%20detection%20performances%20on%20six%20retinal%20diseases%20and%0Athe%20normal%20state%20averagely%20and%20achieves%20state-of-the-art%20results%20on%20both%0Ain-domain%20and%20out-of-domain%20datasets.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/boyiZheng99/PSScreen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSScreen%253A%2520Partially%2520Supervised%2520Multiple%2520Retinal%2520Disease%2520Screening%26entry.906535625%3DBoyi%2520Zheng%2520and%2520Qing%2520Liu%26entry.1292438233%3D%2520%2520Leveraging%2520multiple%2520partially%2520labeled%2520datasets%2520to%2520train%2520a%2520model%2520for%2520multiple%250Aretinal%2520disease%2520screening%2520reduces%2520the%2520reliance%2520on%2520fully%2520annotated%2520datasets%252C%2520but%250Aremains%2520challenging%2520due%2520to%2520significant%2520domain%2520shifts%2520across%2520training%2520datasets%250Afrom%2520various%2520medical%2520sites%252C%2520and%2520the%2520label%2520absent%2520issue%2520for%2520partial%2520classes.%2520To%250Asolve%2520these%2520challenges%252C%2520we%2520propose%2520PSScreen%252C%2520a%2520novel%2520Partially%2520Supervised%250Amultiple%2520retinal%2520disease%2520Screening%2520model.%2520Our%2520PSScreen%2520consists%2520of%2520two%2520streams%250Aand%2520one%2520learns%2520deterministic%2520features%2520and%2520the%2520other%2520learns%2520probabilistic%250Afeatures%2520via%2520uncertainty%2520injection.%2520Then%252C%2520we%2520leverage%2520the%2520textual%2520guidance%2520to%250Adecouple%2520two%2520types%2520of%2520features%2520into%2520disease-wise%2520features%2520and%2520align%2520them%2520via%250Afeature%2520distillation%2520to%2520boost%2520the%2520domain%2520generalization%2520ability.%2520Meanwhile%252C%2520we%250Aemploy%2520pseudo%2520label%2520consistency%2520between%2520two%2520streams%2520to%2520address%2520the%2520label%2520absent%250Aissue%2520and%2520introduce%2520a%2520self-distillation%2520to%2520transfer%2520task-relevant%2520semantics%250Aabout%2520known%2520classes%2520from%2520the%2520deterministic%2520to%2520the%2520probabilistic%2520stream%2520to%250Afurther%2520enhance%2520the%2520detection%2520performances.%2520Experiments%2520show%2520that%2520our%2520PSScreen%250Asignificantly%2520enhances%2520the%2520detection%2520performances%2520on%2520six%2520retinal%2520diseases%2520and%250Athe%2520normal%2520state%2520averagely%2520and%2520achieves%2520state-of-the-art%2520results%2520on%2520both%250Ain-domain%2520and%2520out-of-domain%2520datasets.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/boyiZheng99/PSScreen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSScreen%3A%20Partially%20Supervised%20Multiple%20Retinal%20Disease%20Screening&entry.906535625=Boyi%20Zheng%20and%20Qing%20Liu&entry.1292438233=%20%20Leveraging%20multiple%20partially%20labeled%20datasets%20to%20train%20a%20model%20for%20multiple%0Aretinal%20disease%20screening%20reduces%20the%20reliance%20on%20fully%20annotated%20datasets%2C%20but%0Aremains%20challenging%20due%20to%20significant%20domain%20shifts%20across%20training%20datasets%0Afrom%20various%20medical%20sites%2C%20and%20the%20label%20absent%20issue%20for%20partial%20classes.%20To%0Asolve%20these%20challenges%2C%20we%20propose%20PSScreen%2C%20a%20novel%20Partially%20Supervised%0Amultiple%20retinal%20disease%20Screening%20model.%20Our%20PSScreen%20consists%20of%20two%20streams%0Aand%20one%20learns%20deterministic%20features%20and%20the%20other%20learns%20probabilistic%0Afeatures%20via%20uncertainty%20injection.%20Then%2C%20we%20leverage%20the%20textual%20guidance%20to%0Adecouple%20two%20types%20of%20features%20into%20disease-wise%20features%20and%20align%20them%20via%0Afeature%20distillation%20to%20boost%20the%20domain%20generalization%20ability.%20Meanwhile%2C%20we%0Aemploy%20pseudo%20label%20consistency%20between%20two%20streams%20to%20address%20the%20label%20absent%0Aissue%20and%20introduce%20a%20self-distillation%20to%20transfer%20task-relevant%20semantics%0Aabout%20known%20classes%20from%20the%20deterministic%20to%20the%20probabilistic%20stream%20to%0Afurther%20enhance%20the%20detection%20performances.%20Experiments%20show%20that%20our%20PSScreen%0Asignificantly%20enhances%20the%20detection%20performances%20on%20six%20retinal%20diseases%20and%0Athe%20normal%20state%20averagely%20and%20achieves%20state-of-the-art%20results%20on%20both%0Ain-domain%20and%20out-of-domain%20datasets.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/boyiZheng99/PSScreen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10549v2&entry.124074799=Read"},
{"title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap", "author": "Felipe Maia Polo and Xinhe Wang and Mikhail Yurochkin and Gongjun Xu and Moulinath Banerjee and Yuekai Sun", "abstract": "  Large language models are increasingly used as judges (LLM-as-a-judge) to\nevaluate model outputs at scale, but their assessments often diverge\nsystematically from human judgments. We present Bridge, a unified statistical\nframework that explicitly bridges human and LLM evaluations under both absolute\nscoring and pairwise comparison paradigms. Bridge posits a latent human\npreference score for each prompt-response pair and models LLM deviations as\nlinear transformations of covariates that capture sources of discrepancies.\nThis offers a simple and principled framework for refining LLM ratings and\ncharacterizing systematic discrepancies between humans and LLMs. We provide an\nefficient fitting algorithm with asymptotic guarantees for statistical\ninference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot\nArena), Bridge achieves higher agreement with human ratings (accuracy,\ncalibration, and KL divergence) and exposes systematic human-LLM gaps.\n", "link": "http://arxiv.org/abs/2508.12792v1", "date": "2025-08-18", "relevancy": 2.0468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Human%20and%20LLM%20Judgments%3A%20Understanding%20and%20Narrowing%20the%20Gap&body=Title%3A%20Bridging%20Human%20and%20LLM%20Judgments%3A%20Understanding%20and%20Narrowing%20the%20Gap%0AAuthor%3A%20Felipe%20Maia%20Polo%20and%20Xinhe%20Wang%20and%20Mikhail%20Yurochkin%20and%20Gongjun%20Xu%20and%20Moulinath%20Banerjee%20and%20Yuekai%20Sun%0AAbstract%3A%20%20%20Large%20language%20models%20are%20increasingly%20used%20as%20judges%20%28LLM-as-a-judge%29%20to%0Aevaluate%20model%20outputs%20at%20scale%2C%20but%20their%20assessments%20often%20diverge%0Asystematically%20from%20human%20judgments.%20We%20present%20Bridge%2C%20a%20unified%20statistical%0Aframework%20that%20explicitly%20bridges%20human%20and%20LLM%20evaluations%20under%20both%20absolute%0Ascoring%20and%20pairwise%20comparison%20paradigms.%20Bridge%20posits%20a%20latent%20human%0Apreference%20score%20for%20each%20prompt-response%20pair%20and%20models%20LLM%20deviations%20as%0Alinear%20transformations%20of%20covariates%20that%20capture%20sources%20of%20discrepancies.%0AThis%20offers%20a%20simple%20and%20principled%20framework%20for%20refining%20LLM%20ratings%20and%0Acharacterizing%20systematic%20discrepancies%20between%20humans%20and%20LLMs.%20We%20provide%20an%0Aefficient%20fitting%20algorithm%20with%20asymptotic%20guarantees%20for%20statistical%0Ainference.%20Using%20six%20LLM%20judges%20and%20two%20benchmarks%20%28BigGen%20Bench%20and%20Chatbot%0AArena%29%2C%20Bridge%20achieves%20higher%20agreement%20with%20human%20ratings%20%28accuracy%2C%0Acalibration%2C%20and%20KL%20divergence%29%20and%20exposes%20systematic%20human-LLM%20gaps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Human%2520and%2520LLM%2520Judgments%253A%2520Understanding%2520and%2520Narrowing%2520the%2520Gap%26entry.906535625%3DFelipe%2520Maia%2520Polo%2520and%2520Xinhe%2520Wang%2520and%2520Mikhail%2520Yurochkin%2520and%2520Gongjun%2520Xu%2520and%2520Moulinath%2520Banerjee%2520and%2520Yuekai%2520Sun%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520increasingly%2520used%2520as%2520judges%2520%2528LLM-as-a-judge%2529%2520to%250Aevaluate%2520model%2520outputs%2520at%2520scale%252C%2520but%2520their%2520assessments%2520often%2520diverge%250Asystematically%2520from%2520human%2520judgments.%2520We%2520present%2520Bridge%252C%2520a%2520unified%2520statistical%250Aframework%2520that%2520explicitly%2520bridges%2520human%2520and%2520LLM%2520evaluations%2520under%2520both%2520absolute%250Ascoring%2520and%2520pairwise%2520comparison%2520paradigms.%2520Bridge%2520posits%2520a%2520latent%2520human%250Apreference%2520score%2520for%2520each%2520prompt-response%2520pair%2520and%2520models%2520LLM%2520deviations%2520as%250Alinear%2520transformations%2520of%2520covariates%2520that%2520capture%2520sources%2520of%2520discrepancies.%250AThis%2520offers%2520a%2520simple%2520and%2520principled%2520framework%2520for%2520refining%2520LLM%2520ratings%2520and%250Acharacterizing%2520systematic%2520discrepancies%2520between%2520humans%2520and%2520LLMs.%2520We%2520provide%2520an%250Aefficient%2520fitting%2520algorithm%2520with%2520asymptotic%2520guarantees%2520for%2520statistical%250Ainference.%2520Using%2520six%2520LLM%2520judges%2520and%2520two%2520benchmarks%2520%2528BigGen%2520Bench%2520and%2520Chatbot%250AArena%2529%252C%2520Bridge%2520achieves%2520higher%2520agreement%2520with%2520human%2520ratings%2520%2528accuracy%252C%250Acalibration%252C%2520and%2520KL%2520divergence%2529%2520and%2520exposes%2520systematic%2520human-LLM%2520gaps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Human%20and%20LLM%20Judgments%3A%20Understanding%20and%20Narrowing%20the%20Gap&entry.906535625=Felipe%20Maia%20Polo%20and%20Xinhe%20Wang%20and%20Mikhail%20Yurochkin%20and%20Gongjun%20Xu%20and%20Moulinath%20Banerjee%20and%20Yuekai%20Sun&entry.1292438233=%20%20Large%20language%20models%20are%20increasingly%20used%20as%20judges%20%28LLM-as-a-judge%29%20to%0Aevaluate%20model%20outputs%20at%20scale%2C%20but%20their%20assessments%20often%20diverge%0Asystematically%20from%20human%20judgments.%20We%20present%20Bridge%2C%20a%20unified%20statistical%0Aframework%20that%20explicitly%20bridges%20human%20and%20LLM%20evaluations%20under%20both%20absolute%0Ascoring%20and%20pairwise%20comparison%20paradigms.%20Bridge%20posits%20a%20latent%20human%0Apreference%20score%20for%20each%20prompt-response%20pair%20and%20models%20LLM%20deviations%20as%0Alinear%20transformations%20of%20covariates%20that%20capture%20sources%20of%20discrepancies.%0AThis%20offers%20a%20simple%20and%20principled%20framework%20for%20refining%20LLM%20ratings%20and%0Acharacterizing%20systematic%20discrepancies%20between%20humans%20and%20LLMs.%20We%20provide%20an%0Aefficient%20fitting%20algorithm%20with%20asymptotic%20guarantees%20for%20statistical%0Ainference.%20Using%20six%20LLM%20judges%20and%20two%20benchmarks%20%28BigGen%20Bench%20and%20Chatbot%0AArena%29%2C%20Bridge%20achieves%20higher%20agreement%20with%20human%20ratings%20%28accuracy%2C%0Acalibration%2C%20and%20KL%20divergence%29%20and%20exposes%20systematic%20human-LLM%20gaps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12792v1&entry.124074799=Read"},
{"title": "Contrastive Representations for Temporal Reasoning", "author": "Alicja Ziarko and Michal Bortkiewicz and Michal Zawalski and Benjamin Eysenbach and Piotr Milos", "abstract": "  In classical AI, perception relies on learning state-based representations,\nwhile planning, which can be thought of as temporal reasoning over action\nsequences, is typically achieved through search. We study whether such\nreasoning can instead emerge from representations that capture both perceptual\nand temporal structure. We show that standard temporal contrastive learning,\ndespite its popularity, often fails to capture temporal structure due to its\nreliance on spurious features. To address this, we introduce Combinatorial\nRepresentations for Temporal Reasoning (CRTR), a method that uses a negative\nsampling scheme to provably remove these spurious features and facilitate\ntemporal reasoning. CRTR achieves strong results on domains with complex\ntemporal structure, such as Sokoban and Rubik's Cube. In particular, for the\nRubik's Cube, CRTR learns representations that generalize across all initial\nstates and allow it to solve the puzzle using fewer search steps than BestFS,\nthough with longer solutions. To our knowledge, this is the first method that\nefficiently solves arbitrary Cube states using only learned representations,\nwithout relying on an external search algorithm.\n", "link": "http://arxiv.org/abs/2508.13113v1", "date": "2025-08-18", "relevancy": 2.0322, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5042}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Representations%20for%20Temporal%20Reasoning&body=Title%3A%20Contrastive%20Representations%20for%20Temporal%20Reasoning%0AAuthor%3A%20Alicja%20Ziarko%20and%20Michal%20Bortkiewicz%20and%20Michal%20Zawalski%20and%20Benjamin%20Eysenbach%20and%20Piotr%20Milos%0AAbstract%3A%20%20%20In%20classical%20AI%2C%20perception%20relies%20on%20learning%20state-based%20representations%2C%0Awhile%20planning%2C%20which%20can%20be%20thought%20of%20as%20temporal%20reasoning%20over%20action%0Asequences%2C%20is%20typically%20achieved%20through%20search.%20We%20study%20whether%20such%0Areasoning%20can%20instead%20emerge%20from%20representations%20that%20capture%20both%20perceptual%0Aand%20temporal%20structure.%20We%20show%20that%20standard%20temporal%20contrastive%20learning%2C%0Adespite%20its%20popularity%2C%20often%20fails%20to%20capture%20temporal%20structure%20due%20to%20its%0Areliance%20on%20spurious%20features.%20To%20address%20this%2C%20we%20introduce%20Combinatorial%0ARepresentations%20for%20Temporal%20Reasoning%20%28CRTR%29%2C%20a%20method%20that%20uses%20a%20negative%0Asampling%20scheme%20to%20provably%20remove%20these%20spurious%20features%20and%20facilitate%0Atemporal%20reasoning.%20CRTR%20achieves%20strong%20results%20on%20domains%20with%20complex%0Atemporal%20structure%2C%20such%20as%20Sokoban%20and%20Rubik%27s%20Cube.%20In%20particular%2C%20for%20the%0ARubik%27s%20Cube%2C%20CRTR%20learns%20representations%20that%20generalize%20across%20all%20initial%0Astates%20and%20allow%20it%20to%20solve%20the%20puzzle%20using%20fewer%20search%20steps%20than%20BestFS%2C%0Athough%20with%20longer%20solutions.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20method%20that%0Aefficiently%20solves%20arbitrary%20Cube%20states%20using%20only%20learned%20representations%2C%0Awithout%20relying%20on%20an%20external%20search%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Representations%2520for%2520Temporal%2520Reasoning%26entry.906535625%3DAlicja%2520Ziarko%2520and%2520Michal%2520Bortkiewicz%2520and%2520Michal%2520Zawalski%2520and%2520Benjamin%2520Eysenbach%2520and%2520Piotr%2520Milos%26entry.1292438233%3D%2520%2520In%2520classical%2520AI%252C%2520perception%2520relies%2520on%2520learning%2520state-based%2520representations%252C%250Awhile%2520planning%252C%2520which%2520can%2520be%2520thought%2520of%2520as%2520temporal%2520reasoning%2520over%2520action%250Asequences%252C%2520is%2520typically%2520achieved%2520through%2520search.%2520We%2520study%2520whether%2520such%250Areasoning%2520can%2520instead%2520emerge%2520from%2520representations%2520that%2520capture%2520both%2520perceptual%250Aand%2520temporal%2520structure.%2520We%2520show%2520that%2520standard%2520temporal%2520contrastive%2520learning%252C%250Adespite%2520its%2520popularity%252C%2520often%2520fails%2520to%2520capture%2520temporal%2520structure%2520due%2520to%2520its%250Areliance%2520on%2520spurious%2520features.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Combinatorial%250ARepresentations%2520for%2520Temporal%2520Reasoning%2520%2528CRTR%2529%252C%2520a%2520method%2520that%2520uses%2520a%2520negative%250Asampling%2520scheme%2520to%2520provably%2520remove%2520these%2520spurious%2520features%2520and%2520facilitate%250Atemporal%2520reasoning.%2520CRTR%2520achieves%2520strong%2520results%2520on%2520domains%2520with%2520complex%250Atemporal%2520structure%252C%2520such%2520as%2520Sokoban%2520and%2520Rubik%2527s%2520Cube.%2520In%2520particular%252C%2520for%2520the%250ARubik%2527s%2520Cube%252C%2520CRTR%2520learns%2520representations%2520that%2520generalize%2520across%2520all%2520initial%250Astates%2520and%2520allow%2520it%2520to%2520solve%2520the%2520puzzle%2520using%2520fewer%2520search%2520steps%2520than%2520BestFS%252C%250Athough%2520with%2520longer%2520solutions.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520method%2520that%250Aefficiently%2520solves%2520arbitrary%2520Cube%2520states%2520using%2520only%2520learned%2520representations%252C%250Awithout%2520relying%2520on%2520an%2520external%2520search%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Representations%20for%20Temporal%20Reasoning&entry.906535625=Alicja%20Ziarko%20and%20Michal%20Bortkiewicz%20and%20Michal%20Zawalski%20and%20Benjamin%20Eysenbach%20and%20Piotr%20Milos&entry.1292438233=%20%20In%20classical%20AI%2C%20perception%20relies%20on%20learning%20state-based%20representations%2C%0Awhile%20planning%2C%20which%20can%20be%20thought%20of%20as%20temporal%20reasoning%20over%20action%0Asequences%2C%20is%20typically%20achieved%20through%20search.%20We%20study%20whether%20such%0Areasoning%20can%20instead%20emerge%20from%20representations%20that%20capture%20both%20perceptual%0Aand%20temporal%20structure.%20We%20show%20that%20standard%20temporal%20contrastive%20learning%2C%0Adespite%20its%20popularity%2C%20often%20fails%20to%20capture%20temporal%20structure%20due%20to%20its%0Areliance%20on%20spurious%20features.%20To%20address%20this%2C%20we%20introduce%20Combinatorial%0ARepresentations%20for%20Temporal%20Reasoning%20%28CRTR%29%2C%20a%20method%20that%20uses%20a%20negative%0Asampling%20scheme%20to%20provably%20remove%20these%20spurious%20features%20and%20facilitate%0Atemporal%20reasoning.%20CRTR%20achieves%20strong%20results%20on%20domains%20with%20complex%0Atemporal%20structure%2C%20such%20as%20Sokoban%20and%20Rubik%27s%20Cube.%20In%20particular%2C%20for%20the%0ARubik%27s%20Cube%2C%20CRTR%20learns%20representations%20that%20generalize%20across%20all%20initial%0Astates%20and%20allow%20it%20to%20solve%20the%20puzzle%20using%20fewer%20search%20steps%20than%20BestFS%2C%0Athough%20with%20longer%20solutions.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20method%20that%0Aefficiently%20solves%20arbitrary%20Cube%20states%20using%20only%20learned%20representations%2C%0Awithout%20relying%20on%20an%20external%20search%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13113v1&entry.124074799=Read"},
{"title": "Optimal Condition for Initialization Variance in Deep Neural Networks:\n  An SGD Dynamics Perspective", "author": "Hiroshi Horii and Sothea Has", "abstract": "  Stochastic gradient descent (SGD), one of the most fundamental optimization\nalgorithms in machine learning (ML), can be recast through a continuous-time\napproximation as a Fokker-Planck equation for Langevin dynamics, a viewpoint\nthat has motivated many theoretical studies. Within this framework, we study\nthe relationship between the quasi-stationary distribution derived from this\nequation and the initial distribution through the Kullback-Leibler (KL)\ndivergence. As the quasi-steady-state distribution depends on the expected cost\nfunction, the KL divergence eventually reveals the connection between the\nexpected cost function and the initialization distribution. By applying this to\ndeep neural network models (DNNs), we can express the bounds of the expected\nloss function explicitly in terms of the initialization parameters. Then, by\nminimizing this bound, we obtain an optimal condition of the initialization\nvariance in the Gaussian case. This result provides a concrete mathematical\ncriterion, rather than a heuristic approach, to select the scale of weight\ninitialization in DNNs. In addition, we experimentally confirm our theoretical\nresults by using the classical SGD to train fully connected neural networks on\nthe MNIST and Fashion-MNIST datasets. The result shows that if the variance of\nthe initialization distribution satisfies our theoretical optimal condition,\nthen the corresponding DNN model always achieves lower final training loss and\nhigher test accuracy than the conventional He-normal initialization. Our work\nthus supplies a mathematically grounded indicator that guides the choice of\ninitialization variance and clarifies its physical meaning of the dynamics of\nparameters in DNNs.\n", "link": "http://arxiv.org/abs/2508.12834v1", "date": "2025-08-18", "relevancy": 2.0306, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5336}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5002}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Condition%20for%20Initialization%20Variance%20in%20Deep%20Neural%20Networks%3A%0A%20%20An%20SGD%20Dynamics%20Perspective&body=Title%3A%20Optimal%20Condition%20for%20Initialization%20Variance%20in%20Deep%20Neural%20Networks%3A%0A%20%20An%20SGD%20Dynamics%20Perspective%0AAuthor%3A%20Hiroshi%20Horii%20and%20Sothea%20Has%0AAbstract%3A%20%20%20Stochastic%20gradient%20descent%20%28SGD%29%2C%20one%20of%20the%20most%20fundamental%20optimization%0Aalgorithms%20in%20machine%20learning%20%28ML%29%2C%20can%20be%20recast%20through%20a%20continuous-time%0Aapproximation%20as%20a%20Fokker-Planck%20equation%20for%20Langevin%20dynamics%2C%20a%20viewpoint%0Athat%20has%20motivated%20many%20theoretical%20studies.%20Within%20this%20framework%2C%20we%20study%0Athe%20relationship%20between%20the%20quasi-stationary%20distribution%20derived%20from%20this%0Aequation%20and%20the%20initial%20distribution%20through%20the%20Kullback-Leibler%20%28KL%29%0Adivergence.%20As%20the%20quasi-steady-state%20distribution%20depends%20on%20the%20expected%20cost%0Afunction%2C%20the%20KL%20divergence%20eventually%20reveals%20the%20connection%20between%20the%0Aexpected%20cost%20function%20and%20the%20initialization%20distribution.%20By%20applying%20this%20to%0Adeep%20neural%20network%20models%20%28DNNs%29%2C%20we%20can%20express%20the%20bounds%20of%20the%20expected%0Aloss%20function%20explicitly%20in%20terms%20of%20the%20initialization%20parameters.%20Then%2C%20by%0Aminimizing%20this%20bound%2C%20we%20obtain%20an%20optimal%20condition%20of%20the%20initialization%0Avariance%20in%20the%20Gaussian%20case.%20This%20result%20provides%20a%20concrete%20mathematical%0Acriterion%2C%20rather%20than%20a%20heuristic%20approach%2C%20to%20select%20the%20scale%20of%20weight%0Ainitialization%20in%20DNNs.%20In%20addition%2C%20we%20experimentally%20confirm%20our%20theoretical%0Aresults%20by%20using%20the%20classical%20SGD%20to%20train%20fully%20connected%20neural%20networks%20on%0Athe%20MNIST%20and%20Fashion-MNIST%20datasets.%20The%20result%20shows%20that%20if%20the%20variance%20of%0Athe%20initialization%20distribution%20satisfies%20our%20theoretical%20optimal%20condition%2C%0Athen%20the%20corresponding%20DNN%20model%20always%20achieves%20lower%20final%20training%20loss%20and%0Ahigher%20test%20accuracy%20than%20the%20conventional%20He-normal%20initialization.%20Our%20work%0Athus%20supplies%20a%20mathematically%20grounded%20indicator%20that%20guides%20the%20choice%20of%0Ainitialization%20variance%20and%20clarifies%20its%20physical%20meaning%20of%20the%20dynamics%20of%0Aparameters%20in%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Condition%2520for%2520Initialization%2520Variance%2520in%2520Deep%2520Neural%2520Networks%253A%250A%2520%2520An%2520SGD%2520Dynamics%2520Perspective%26entry.906535625%3DHiroshi%2520Horii%2520and%2520Sothea%2520Has%26entry.1292438233%3D%2520%2520Stochastic%2520gradient%2520descent%2520%2528SGD%2529%252C%2520one%2520of%2520the%2520most%2520fundamental%2520optimization%250Aalgorithms%2520in%2520machine%2520learning%2520%2528ML%2529%252C%2520can%2520be%2520recast%2520through%2520a%2520continuous-time%250Aapproximation%2520as%2520a%2520Fokker-Planck%2520equation%2520for%2520Langevin%2520dynamics%252C%2520a%2520viewpoint%250Athat%2520has%2520motivated%2520many%2520theoretical%2520studies.%2520Within%2520this%2520framework%252C%2520we%2520study%250Athe%2520relationship%2520between%2520the%2520quasi-stationary%2520distribution%2520derived%2520from%2520this%250Aequation%2520and%2520the%2520initial%2520distribution%2520through%2520the%2520Kullback-Leibler%2520%2528KL%2529%250Adivergence.%2520As%2520the%2520quasi-steady-state%2520distribution%2520depends%2520on%2520the%2520expected%2520cost%250Afunction%252C%2520the%2520KL%2520divergence%2520eventually%2520reveals%2520the%2520connection%2520between%2520the%250Aexpected%2520cost%2520function%2520and%2520the%2520initialization%2520distribution.%2520By%2520applying%2520this%2520to%250Adeep%2520neural%2520network%2520models%2520%2528DNNs%2529%252C%2520we%2520can%2520express%2520the%2520bounds%2520of%2520the%2520expected%250Aloss%2520function%2520explicitly%2520in%2520terms%2520of%2520the%2520initialization%2520parameters.%2520Then%252C%2520by%250Aminimizing%2520this%2520bound%252C%2520we%2520obtain%2520an%2520optimal%2520condition%2520of%2520the%2520initialization%250Avariance%2520in%2520the%2520Gaussian%2520case.%2520This%2520result%2520provides%2520a%2520concrete%2520mathematical%250Acriterion%252C%2520rather%2520than%2520a%2520heuristic%2520approach%252C%2520to%2520select%2520the%2520scale%2520of%2520weight%250Ainitialization%2520in%2520DNNs.%2520In%2520addition%252C%2520we%2520experimentally%2520confirm%2520our%2520theoretical%250Aresults%2520by%2520using%2520the%2520classical%2520SGD%2520to%2520train%2520fully%2520connected%2520neural%2520networks%2520on%250Athe%2520MNIST%2520and%2520Fashion-MNIST%2520datasets.%2520The%2520result%2520shows%2520that%2520if%2520the%2520variance%2520of%250Athe%2520initialization%2520distribution%2520satisfies%2520our%2520theoretical%2520optimal%2520condition%252C%250Athen%2520the%2520corresponding%2520DNN%2520model%2520always%2520achieves%2520lower%2520final%2520training%2520loss%2520and%250Ahigher%2520test%2520accuracy%2520than%2520the%2520conventional%2520He-normal%2520initialization.%2520Our%2520work%250Athus%2520supplies%2520a%2520mathematically%2520grounded%2520indicator%2520that%2520guides%2520the%2520choice%2520of%250Ainitialization%2520variance%2520and%2520clarifies%2520its%2520physical%2520meaning%2520of%2520the%2520dynamics%2520of%250Aparameters%2520in%2520DNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Condition%20for%20Initialization%20Variance%20in%20Deep%20Neural%20Networks%3A%0A%20%20An%20SGD%20Dynamics%20Perspective&entry.906535625=Hiroshi%20Horii%20and%20Sothea%20Has&entry.1292438233=%20%20Stochastic%20gradient%20descent%20%28SGD%29%2C%20one%20of%20the%20most%20fundamental%20optimization%0Aalgorithms%20in%20machine%20learning%20%28ML%29%2C%20can%20be%20recast%20through%20a%20continuous-time%0Aapproximation%20as%20a%20Fokker-Planck%20equation%20for%20Langevin%20dynamics%2C%20a%20viewpoint%0Athat%20has%20motivated%20many%20theoretical%20studies.%20Within%20this%20framework%2C%20we%20study%0Athe%20relationship%20between%20the%20quasi-stationary%20distribution%20derived%20from%20this%0Aequation%20and%20the%20initial%20distribution%20through%20the%20Kullback-Leibler%20%28KL%29%0Adivergence.%20As%20the%20quasi-steady-state%20distribution%20depends%20on%20the%20expected%20cost%0Afunction%2C%20the%20KL%20divergence%20eventually%20reveals%20the%20connection%20between%20the%0Aexpected%20cost%20function%20and%20the%20initialization%20distribution.%20By%20applying%20this%20to%0Adeep%20neural%20network%20models%20%28DNNs%29%2C%20we%20can%20express%20the%20bounds%20of%20the%20expected%0Aloss%20function%20explicitly%20in%20terms%20of%20the%20initialization%20parameters.%20Then%2C%20by%0Aminimizing%20this%20bound%2C%20we%20obtain%20an%20optimal%20condition%20of%20the%20initialization%0Avariance%20in%20the%20Gaussian%20case.%20This%20result%20provides%20a%20concrete%20mathematical%0Acriterion%2C%20rather%20than%20a%20heuristic%20approach%2C%20to%20select%20the%20scale%20of%20weight%0Ainitialization%20in%20DNNs.%20In%20addition%2C%20we%20experimentally%20confirm%20our%20theoretical%0Aresults%20by%20using%20the%20classical%20SGD%20to%20train%20fully%20connected%20neural%20networks%20on%0Athe%20MNIST%20and%20Fashion-MNIST%20datasets.%20The%20result%20shows%20that%20if%20the%20variance%20of%0Athe%20initialization%20distribution%20satisfies%20our%20theoretical%20optimal%20condition%2C%0Athen%20the%20corresponding%20DNN%20model%20always%20achieves%20lower%20final%20training%20loss%20and%0Ahigher%20test%20accuracy%20than%20the%20conventional%20He-normal%20initialization.%20Our%20work%0Athus%20supplies%20a%20mathematically%20grounded%20indicator%20that%20guides%20the%20choice%20of%0Ainitialization%20variance%20and%20clarifies%20its%20physical%20meaning%20of%20the%20dynamics%20of%0Aparameters%20in%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12834v1&entry.124074799=Read"},
{"title": "Vehicle detection from GSV imagery: Predicting travel behaviour for\n  cycling and motorcycling using Computer Vision", "author": " Kyriaki and  Kokka and Rahul Goel and Ali Abbas and Kerry A. Nice and Luca Martial and SM Labib and Rihuan Ke and Carola Bibiane Sch\u00f6nlieb and James Woodcock", "abstract": "  Transportation influence health by shaping exposure to physical activity, air\npollution and injury risk.Comparative data on cycling and motorcycling\nbehaviours is scarce, particularly at a global scale.Street view imagery, such\nas Google Street View (GSV), combined with computer vision, is a valuable\nresource for efficiently capturing travel behaviour data.This study\ndemonstrates a novel approach using deep learning on street view images to\nestimate cycling and motorcycling levels across diverse cities worldwide.We\nutilized data from 185 global cities.The data on mode shares of cycling and\nmotorcycling estimated using travel surveys or censuses.We used GSV images to\ndetect cycles and motorcycles in sampled locations, using 8000 images per\ncity.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean\naverage precision of 89% for detecting cycles and motorcycles in GSV images.A\nglobal prediction model was developed using beta regression with city-level\nmode shares as outcome, with log transformed explanatory variables of counts of\nGSV-detected images with cycles and motorcycles, while controlling for\npopulation density.We found strong correlations between GSV motorcycle counts\nand motorcycle mode share (0.78) and moderate correlations between GSV cycle\ncounts and cycling mode share (0.51).Beta regression models predicted mode\nshares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,\nachieving median absolute errors (MDAE) of 1.3% and 1.4%,\nrespectively.Scatterplots demonstrated consistent prediction accuracy, though\ncities like Utrecht and Cali were outliers.The model was applied to 60 cities\nglobally for which we didn't have recent mode share data.We provided estimates\nfor some cities in the Middle East, Latin America and East Asia.With computer\nvision, GSV images capture travel modes and activity, providing insights\nalongside traditional data sources.\n", "link": "http://arxiv.org/abs/2508.12794v1", "date": "2025-08-18", "relevancy": 2.0273, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.504}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vehicle%20detection%20from%20GSV%20imagery%3A%20Predicting%20travel%20behaviour%20for%0A%20%20cycling%20and%20motorcycling%20using%20Computer%20Vision&body=Title%3A%20Vehicle%20detection%20from%20GSV%20imagery%3A%20Predicting%20travel%20behaviour%20for%0A%20%20cycling%20and%20motorcycling%20using%20Computer%20Vision%0AAuthor%3A%20%20Kyriaki%20and%20%20Kokka%20and%20Rahul%20Goel%20and%20Ali%20Abbas%20and%20Kerry%20A.%20Nice%20and%20Luca%20Martial%20and%20SM%20Labib%20and%20Rihuan%20Ke%20and%20Carola%20Bibiane%20Sch%C3%B6nlieb%20and%20James%20Woodcock%0AAbstract%3A%20%20%20Transportation%20influence%20health%20by%20shaping%20exposure%20to%20physical%20activity%2C%20air%0Apollution%20and%20injury%20risk.Comparative%20data%20on%20cycling%20and%20motorcycling%0Abehaviours%20is%20scarce%2C%20particularly%20at%20a%20global%20scale.Street%20view%20imagery%2C%20such%0Aas%20Google%20Street%20View%20%28GSV%29%2C%20combined%20with%20computer%20vision%2C%20is%20a%20valuable%0Aresource%20for%20efficiently%20capturing%20travel%20behaviour%20data.This%20study%0Ademonstrates%20a%20novel%20approach%20using%20deep%20learning%20on%20street%20view%20images%20to%0Aestimate%20cycling%20and%20motorcycling%20levels%20across%20diverse%20cities%20worldwide.We%0Autilized%20data%20from%20185%20global%20cities.The%20data%20on%20mode%20shares%20of%20cycling%20and%0Amotorcycling%20estimated%20using%20travel%20surveys%20or%20censuses.We%20used%20GSV%20images%20to%0Adetect%20cycles%20and%20motorcycles%20in%20sampled%20locations%2C%20using%208000%20images%20per%0Acity.The%20YOLOv4%20model%2C%20fine-tuned%20using%20images%20from%20six%20cities%2C%20achieved%20a%20mean%0Aaverage%20precision%20of%2089%25%20for%20detecting%20cycles%20and%20motorcycles%20in%20GSV%20images.A%0Aglobal%20prediction%20model%20was%20developed%20using%20beta%20regression%20with%20city-level%0Amode%20shares%20as%20outcome%2C%20with%20log%20transformed%20explanatory%20variables%20of%20counts%20of%0AGSV-detected%20images%20with%20cycles%20and%20motorcycles%2C%20while%20controlling%20for%0Apopulation%20density.We%20found%20strong%20correlations%20between%20GSV%20motorcycle%20counts%0Aand%20motorcycle%20mode%20share%20%280.78%29%20and%20moderate%20correlations%20between%20GSV%20cycle%0Acounts%20and%20cycling%20mode%20share%20%280.51%29.Beta%20regression%20models%20predicted%20mode%0Ashares%20with%20%24R%5E2%24%20values%20of%200.614%20for%20cycling%20and%200.612%20for%20motorcycling%2C%0Aachieving%20median%20absolute%20errors%20%28MDAE%29%20of%201.3%25%20and%201.4%25%2C%0Arespectively.Scatterplots%20demonstrated%20consistent%20prediction%20accuracy%2C%20though%0Acities%20like%20Utrecht%20and%20Cali%20were%20outliers.The%20model%20was%20applied%20to%2060%20cities%0Aglobally%20for%20which%20we%20didn%27t%20have%20recent%20mode%20share%20data.We%20provided%20estimates%0Afor%20some%20cities%20in%20the%20Middle%20East%2C%20Latin%20America%20and%20East%20Asia.With%20computer%0Avision%2C%20GSV%20images%20capture%20travel%20modes%20and%20activity%2C%20providing%20insights%0Aalongside%20traditional%20data%20sources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVehicle%2520detection%2520from%2520GSV%2520imagery%253A%2520Predicting%2520travel%2520behaviour%2520for%250A%2520%2520cycling%2520and%2520motorcycling%2520using%2520Computer%2520Vision%26entry.906535625%3D%2520Kyriaki%2520and%2520%2520Kokka%2520and%2520Rahul%2520Goel%2520and%2520Ali%2520Abbas%2520and%2520Kerry%2520A.%2520Nice%2520and%2520Luca%2520Martial%2520and%2520SM%2520Labib%2520and%2520Rihuan%2520Ke%2520and%2520Carola%2520Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520James%2520Woodcock%26entry.1292438233%3D%2520%2520Transportation%2520influence%2520health%2520by%2520shaping%2520exposure%2520to%2520physical%2520activity%252C%2520air%250Apollution%2520and%2520injury%2520risk.Comparative%2520data%2520on%2520cycling%2520and%2520motorcycling%250Abehaviours%2520is%2520scarce%252C%2520particularly%2520at%2520a%2520global%2520scale.Street%2520view%2520imagery%252C%2520such%250Aas%2520Google%2520Street%2520View%2520%2528GSV%2529%252C%2520combined%2520with%2520computer%2520vision%252C%2520is%2520a%2520valuable%250Aresource%2520for%2520efficiently%2520capturing%2520travel%2520behaviour%2520data.This%2520study%250Ademonstrates%2520a%2520novel%2520approach%2520using%2520deep%2520learning%2520on%2520street%2520view%2520images%2520to%250Aestimate%2520cycling%2520and%2520motorcycling%2520levels%2520across%2520diverse%2520cities%2520worldwide.We%250Autilized%2520data%2520from%2520185%2520global%2520cities.The%2520data%2520on%2520mode%2520shares%2520of%2520cycling%2520and%250Amotorcycling%2520estimated%2520using%2520travel%2520surveys%2520or%2520censuses.We%2520used%2520GSV%2520images%2520to%250Adetect%2520cycles%2520and%2520motorcycles%2520in%2520sampled%2520locations%252C%2520using%25208000%2520images%2520per%250Acity.The%2520YOLOv4%2520model%252C%2520fine-tuned%2520using%2520images%2520from%2520six%2520cities%252C%2520achieved%2520a%2520mean%250Aaverage%2520precision%2520of%252089%2525%2520for%2520detecting%2520cycles%2520and%2520motorcycles%2520in%2520GSV%2520images.A%250Aglobal%2520prediction%2520model%2520was%2520developed%2520using%2520beta%2520regression%2520with%2520city-level%250Amode%2520shares%2520as%2520outcome%252C%2520with%2520log%2520transformed%2520explanatory%2520variables%2520of%2520counts%2520of%250AGSV-detected%2520images%2520with%2520cycles%2520and%2520motorcycles%252C%2520while%2520controlling%2520for%250Apopulation%2520density.We%2520found%2520strong%2520correlations%2520between%2520GSV%2520motorcycle%2520counts%250Aand%2520motorcycle%2520mode%2520share%2520%25280.78%2529%2520and%2520moderate%2520correlations%2520between%2520GSV%2520cycle%250Acounts%2520and%2520cycling%2520mode%2520share%2520%25280.51%2529.Beta%2520regression%2520models%2520predicted%2520mode%250Ashares%2520with%2520%2524R%255E2%2524%2520values%2520of%25200.614%2520for%2520cycling%2520and%25200.612%2520for%2520motorcycling%252C%250Aachieving%2520median%2520absolute%2520errors%2520%2528MDAE%2529%2520of%25201.3%2525%2520and%25201.4%2525%252C%250Arespectively.Scatterplots%2520demonstrated%2520consistent%2520prediction%2520accuracy%252C%2520though%250Acities%2520like%2520Utrecht%2520and%2520Cali%2520were%2520outliers.The%2520model%2520was%2520applied%2520to%252060%2520cities%250Aglobally%2520for%2520which%2520we%2520didn%2527t%2520have%2520recent%2520mode%2520share%2520data.We%2520provided%2520estimates%250Afor%2520some%2520cities%2520in%2520the%2520Middle%2520East%252C%2520Latin%2520America%2520and%2520East%2520Asia.With%2520computer%250Avision%252C%2520GSV%2520images%2520capture%2520travel%2520modes%2520and%2520activity%252C%2520providing%2520insights%250Aalongside%2520traditional%2520data%2520sources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vehicle%20detection%20from%20GSV%20imagery%3A%20Predicting%20travel%20behaviour%20for%0A%20%20cycling%20and%20motorcycling%20using%20Computer%20Vision&entry.906535625=%20Kyriaki%20and%20%20Kokka%20and%20Rahul%20Goel%20and%20Ali%20Abbas%20and%20Kerry%20A.%20Nice%20and%20Luca%20Martial%20and%20SM%20Labib%20and%20Rihuan%20Ke%20and%20Carola%20Bibiane%20Sch%C3%B6nlieb%20and%20James%20Woodcock&entry.1292438233=%20%20Transportation%20influence%20health%20by%20shaping%20exposure%20to%20physical%20activity%2C%20air%0Apollution%20and%20injury%20risk.Comparative%20data%20on%20cycling%20and%20motorcycling%0Abehaviours%20is%20scarce%2C%20particularly%20at%20a%20global%20scale.Street%20view%20imagery%2C%20such%0Aas%20Google%20Street%20View%20%28GSV%29%2C%20combined%20with%20computer%20vision%2C%20is%20a%20valuable%0Aresource%20for%20efficiently%20capturing%20travel%20behaviour%20data.This%20study%0Ademonstrates%20a%20novel%20approach%20using%20deep%20learning%20on%20street%20view%20images%20to%0Aestimate%20cycling%20and%20motorcycling%20levels%20across%20diverse%20cities%20worldwide.We%0Autilized%20data%20from%20185%20global%20cities.The%20data%20on%20mode%20shares%20of%20cycling%20and%0Amotorcycling%20estimated%20using%20travel%20surveys%20or%20censuses.We%20used%20GSV%20images%20to%0Adetect%20cycles%20and%20motorcycles%20in%20sampled%20locations%2C%20using%208000%20images%20per%0Acity.The%20YOLOv4%20model%2C%20fine-tuned%20using%20images%20from%20six%20cities%2C%20achieved%20a%20mean%0Aaverage%20precision%20of%2089%25%20for%20detecting%20cycles%20and%20motorcycles%20in%20GSV%20images.A%0Aglobal%20prediction%20model%20was%20developed%20using%20beta%20regression%20with%20city-level%0Amode%20shares%20as%20outcome%2C%20with%20log%20transformed%20explanatory%20variables%20of%20counts%20of%0AGSV-detected%20images%20with%20cycles%20and%20motorcycles%2C%20while%20controlling%20for%0Apopulation%20density.We%20found%20strong%20correlations%20between%20GSV%20motorcycle%20counts%0Aand%20motorcycle%20mode%20share%20%280.78%29%20and%20moderate%20correlations%20between%20GSV%20cycle%0Acounts%20and%20cycling%20mode%20share%20%280.51%29.Beta%20regression%20models%20predicted%20mode%0Ashares%20with%20%24R%5E2%24%20values%20of%200.614%20for%20cycling%20and%200.612%20for%20motorcycling%2C%0Aachieving%20median%20absolute%20errors%20%28MDAE%29%20of%201.3%25%20and%201.4%25%2C%0Arespectively.Scatterplots%20demonstrated%20consistent%20prediction%20accuracy%2C%20though%0Acities%20like%20Utrecht%20and%20Cali%20were%20outliers.The%20model%20was%20applied%20to%2060%20cities%0Aglobally%20for%20which%20we%20didn%27t%20have%20recent%20mode%20share%20data.We%20provided%20estimates%0Afor%20some%20cities%20in%20the%20Middle%20East%2C%20Latin%20America%20and%20East%20Asia.With%20computer%0Avision%2C%20GSV%20images%20capture%20travel%20modes%20and%20activity%2C%20providing%20insights%0Aalongside%20traditional%20data%20sources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12794v1&entry.124074799=Read"},
{"title": "Game Reasoning Arena: A Framework and Benchmark for Assessing Reasoning\n  Capabilities of Large Language Models via Game Play", "author": "Lucia Cipolina-Kun and Marianna Nezhurina and Jenia Jitsev", "abstract": "  The Game Reasoning Arena library provides a framework for evaluating the\ndecision making abilities of large language models (LLMs) through strategic\nboard games implemented in Google OpenSpiel library. The framework enables\nsystematic comparisons between LLM based agents and other agents (random,\nheuristic, reinforcement learning agents, etc.) in various game scenarios by\nwrapping multiple board and matrix games and supporting different agent types.\nIt integrates API access to models via liteLLM, local model deployment via\nvLLM, and offers distributed execution through Ray. This paper summarises the\nlibrary structure, key characteristics, and motivation of the repository,\nhighlighting how it contributes to the empirical evaluation of the reasoning of\nLLM and game theoretic behaviour.\n", "link": "http://arxiv.org/abs/2508.03368v3", "date": "2025-08-18", "relevancy": 2.0229, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5094}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Game%20Reasoning%20Arena%3A%20A%20Framework%20and%20Benchmark%20for%20Assessing%20Reasoning%0A%20%20Capabilities%20of%20Large%20Language%20Models%20via%20Game%20Play&body=Title%3A%20Game%20Reasoning%20Arena%3A%20A%20Framework%20and%20Benchmark%20for%20Assessing%20Reasoning%0A%20%20Capabilities%20of%20Large%20Language%20Models%20via%20Game%20Play%0AAuthor%3A%20Lucia%20Cipolina-Kun%20and%20Marianna%20Nezhurina%20and%20Jenia%20Jitsev%0AAbstract%3A%20%20%20The%20Game%20Reasoning%20Arena%20library%20provides%20a%20framework%20for%20evaluating%20the%0Adecision%20making%20abilities%20of%20large%20language%20models%20%28LLMs%29%20through%20strategic%0Aboard%20games%20implemented%20in%20Google%20OpenSpiel%20library.%20The%20framework%20enables%0Asystematic%20comparisons%20between%20LLM%20based%20agents%20and%20other%20agents%20%28random%2C%0Aheuristic%2C%20reinforcement%20learning%20agents%2C%20etc.%29%20in%20various%20game%20scenarios%20by%0Awrapping%20multiple%20board%20and%20matrix%20games%20and%20supporting%20different%20agent%20types.%0AIt%20integrates%20API%20access%20to%20models%20via%20liteLLM%2C%20local%20model%20deployment%20via%0AvLLM%2C%20and%20offers%20distributed%20execution%20through%20Ray.%20This%20paper%20summarises%20the%0Alibrary%20structure%2C%20key%20characteristics%2C%20and%20motivation%20of%20the%20repository%2C%0Ahighlighting%20how%20it%20contributes%20to%20the%20empirical%20evaluation%20of%20the%20reasoning%20of%0ALLM%20and%20game%20theoretic%20behaviour.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03368v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGame%2520Reasoning%2520Arena%253A%2520A%2520Framework%2520and%2520Benchmark%2520for%2520Assessing%2520Reasoning%250A%2520%2520Capabilities%2520of%2520Large%2520Language%2520Models%2520via%2520Game%2520Play%26entry.906535625%3DLucia%2520Cipolina-Kun%2520and%2520Marianna%2520Nezhurina%2520and%2520Jenia%2520Jitsev%26entry.1292438233%3D%2520%2520The%2520Game%2520Reasoning%2520Arena%2520library%2520provides%2520a%2520framework%2520for%2520evaluating%2520the%250Adecision%2520making%2520abilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520through%2520strategic%250Aboard%2520games%2520implemented%2520in%2520Google%2520OpenSpiel%2520library.%2520The%2520framework%2520enables%250Asystematic%2520comparisons%2520between%2520LLM%2520based%2520agents%2520and%2520other%2520agents%2520%2528random%252C%250Aheuristic%252C%2520reinforcement%2520learning%2520agents%252C%2520etc.%2529%2520in%2520various%2520game%2520scenarios%2520by%250Awrapping%2520multiple%2520board%2520and%2520matrix%2520games%2520and%2520supporting%2520different%2520agent%2520types.%250AIt%2520integrates%2520API%2520access%2520to%2520models%2520via%2520liteLLM%252C%2520local%2520model%2520deployment%2520via%250AvLLM%252C%2520and%2520offers%2520distributed%2520execution%2520through%2520Ray.%2520This%2520paper%2520summarises%2520the%250Alibrary%2520structure%252C%2520key%2520characteristics%252C%2520and%2520motivation%2520of%2520the%2520repository%252C%250Ahighlighting%2520how%2520it%2520contributes%2520to%2520the%2520empirical%2520evaluation%2520of%2520the%2520reasoning%2520of%250ALLM%2520and%2520game%2520theoretic%2520behaviour.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03368v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Game%20Reasoning%20Arena%3A%20A%20Framework%20and%20Benchmark%20for%20Assessing%20Reasoning%0A%20%20Capabilities%20of%20Large%20Language%20Models%20via%20Game%20Play&entry.906535625=Lucia%20Cipolina-Kun%20and%20Marianna%20Nezhurina%20and%20Jenia%20Jitsev&entry.1292438233=%20%20The%20Game%20Reasoning%20Arena%20library%20provides%20a%20framework%20for%20evaluating%20the%0Adecision%20making%20abilities%20of%20large%20language%20models%20%28LLMs%29%20through%20strategic%0Aboard%20games%20implemented%20in%20Google%20OpenSpiel%20library.%20The%20framework%20enables%0Asystematic%20comparisons%20between%20LLM%20based%20agents%20and%20other%20agents%20%28random%2C%0Aheuristic%2C%20reinforcement%20learning%20agents%2C%20etc.%29%20in%20various%20game%20scenarios%20by%0Awrapping%20multiple%20board%20and%20matrix%20games%20and%20supporting%20different%20agent%20types.%0AIt%20integrates%20API%20access%20to%20models%20via%20liteLLM%2C%20local%20model%20deployment%20via%0AvLLM%2C%20and%20offers%20distributed%20execution%20through%20Ray.%20This%20paper%20summarises%20the%0Alibrary%20structure%2C%20key%20characteristics%2C%20and%20motivation%20of%20the%20repository%2C%0Ahighlighting%20how%20it%20contributes%20to%20the%20empirical%20evaluation%20of%20the%20reasoning%20of%0ALLM%20and%20game%20theoretic%20behaviour.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03368v3&entry.124074799=Read"},
{"title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward", "author": "Yong Deng and Guoqing Wang and Zhenzhe Ying and Xiaofeng Wu and Jinzhen Lin and Wenwen Xiong and Yuqin Dai and Shuo Yang and Zhanwei Zhang and Qiwen Wang and Yang Qin and Changhua Meng", "abstract": "  Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.\n", "link": "http://arxiv.org/abs/2508.12800v1", "date": "2025-08-18", "relevancy": 2.0074, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Atom-Searcher%3A%20Enhancing%20Agentic%20Deep%20Research%20via%20Fine-Grained%20Atomic%0A%20%20Thought%20Reward&body=Title%3A%20Atom-Searcher%3A%20Enhancing%20Agentic%20Deep%20Research%20via%20Fine-Grained%20Atomic%0A%20%20Thought%20Reward%0AAuthor%3A%20Yong%20Deng%20and%20Guoqing%20Wang%20and%20Zhenzhe%20Ying%20and%20Xiaofeng%20Wu%20and%20Jinzhen%20Lin%20and%20Wenwen%20Xiong%20and%20Yuqin%20Dai%20and%20Shuo%20Yang%20and%20Zhanwei%20Zhang%20and%20Qiwen%20Wang%20and%20Yang%20Qin%20and%20Changhua%20Meng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20problem-solving%20abilities%2C%0Abut%20struggle%20with%20complex%20tasks%20due%20to%20static%20internal%20knowledge.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20enhances%20access%20to%20external%20information%2C%0Ayet%20remains%20limited%20in%20multi-hop%20reasoning%20and%20strategic%20search%20due%20to%20rigid%0Aworkflows.%20Recent%20advancements%20in%20agentic%20deep%20research%20empower%20LLMs%20to%0Aautonomously%20reason%2C%20search%2C%20and%20synthesize%20information.%20However%2C%20current%0Aapproaches%20relying%20on%20outcome-based%20reinforcement%20learning%20%28RL%29%20face%20critical%0Aissues%20such%20as%20conflicting%20gradients%20and%20reward%20sparsity%2C%20limiting%20performance%0Agains%20and%20training%20efficiency.%20To%20address%20these%2C%20we%20first%20propose%20Atomic%0AThought%2C%20a%20novel%20LLM%20thinking%20paradigm%20that%20decomposes%20reasoning%20into%0Afine-grained%20functional%20units.%20These%20units%20are%20supervised%20by%20Reasoning%20Reward%0AModels%20%28RRMs%29%2C%20which%20provide%20Atomic%20Thought%20Rewards%20%28ATR%29%20for%20fine-grained%0Aguidance.%20Building%20on%20this%2C%20we%20propose%20Atom-Searcher%2C%20a%20novel%20RL%20framework%20for%0Aagentic%20deep%20research%20that%20integrates%20Atomic%20Thought%20and%20ATR.%20Atom-Searcher%0Auses%20a%20curriculum-inspired%20reward%20schedule%2C%20prioritizing%20process-level%20ATR%0Aearly%20and%20transitioning%20to%20outcome%20rewards%2C%20accelerating%20convergence%20on%0Aeffective%20reasoning%20paths.%20Experiments%20on%20seven%20benchmarks%20show%20consistent%0Aimprovements%20over%20the%20state-of-the-art.%20Key%20advantages%20include%3A%20%281%29%0AAtom-Searcher%20scales%20computation%20at%20test-time.%20%282%29%20Atomic%20Thought%20provides%0Asupervision%20anchors%20for%20RRMs%2C%20bridging%20deep%20research%20tasks%20and%20RRMs.%20%283%29%0AAtom-Searcher%20exhibits%20more%20interpretable%2C%20human-like%20reasoning%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtom-Searcher%253A%2520Enhancing%2520Agentic%2520Deep%2520Research%2520via%2520Fine-Grained%2520Atomic%250A%2520%2520Thought%2520Reward%26entry.906535625%3DYong%2520Deng%2520and%2520Guoqing%2520Wang%2520and%2520Zhenzhe%2520Ying%2520and%2520Xiaofeng%2520Wu%2520and%2520Jinzhen%2520Lin%2520and%2520Wenwen%2520Xiong%2520and%2520Yuqin%2520Dai%2520and%2520Shuo%2520Yang%2520and%2520Zhanwei%2520Zhang%2520and%2520Qiwen%2520Wang%2520and%2520Yang%2520Qin%2520and%2520Changhua%2520Meng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520problem-solving%2520abilities%252C%250Abut%2520struggle%2520with%2520complex%2520tasks%2520due%2520to%2520static%2520internal%2520knowledge.%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520enhances%2520access%2520to%2520external%2520information%252C%250Ayet%2520remains%2520limited%2520in%2520multi-hop%2520reasoning%2520and%2520strategic%2520search%2520due%2520to%2520rigid%250Aworkflows.%2520Recent%2520advancements%2520in%2520agentic%2520deep%2520research%2520empower%2520LLMs%2520to%250Aautonomously%2520reason%252C%2520search%252C%2520and%2520synthesize%2520information.%2520However%252C%2520current%250Aapproaches%2520relying%2520on%2520outcome-based%2520reinforcement%2520learning%2520%2528RL%2529%2520face%2520critical%250Aissues%2520such%2520as%2520conflicting%2520gradients%2520and%2520reward%2520sparsity%252C%2520limiting%2520performance%250Agains%2520and%2520training%2520efficiency.%2520To%2520address%2520these%252C%2520we%2520first%2520propose%2520Atomic%250AThought%252C%2520a%2520novel%2520LLM%2520thinking%2520paradigm%2520that%2520decomposes%2520reasoning%2520into%250Afine-grained%2520functional%2520units.%2520These%2520units%2520are%2520supervised%2520by%2520Reasoning%2520Reward%250AModels%2520%2528RRMs%2529%252C%2520which%2520provide%2520Atomic%2520Thought%2520Rewards%2520%2528ATR%2529%2520for%2520fine-grained%250Aguidance.%2520Building%2520on%2520this%252C%2520we%2520propose%2520Atom-Searcher%252C%2520a%2520novel%2520RL%2520framework%2520for%250Aagentic%2520deep%2520research%2520that%2520integrates%2520Atomic%2520Thought%2520and%2520ATR.%2520Atom-Searcher%250Auses%2520a%2520curriculum-inspired%2520reward%2520schedule%252C%2520prioritizing%2520process-level%2520ATR%250Aearly%2520and%2520transitioning%2520to%2520outcome%2520rewards%252C%2520accelerating%2520convergence%2520on%250Aeffective%2520reasoning%2520paths.%2520Experiments%2520on%2520seven%2520benchmarks%2520show%2520consistent%250Aimprovements%2520over%2520the%2520state-of-the-art.%2520Key%2520advantages%2520include%253A%2520%25281%2529%250AAtom-Searcher%2520scales%2520computation%2520at%2520test-time.%2520%25282%2529%2520Atomic%2520Thought%2520provides%250Asupervision%2520anchors%2520for%2520RRMs%252C%2520bridging%2520deep%2520research%2520tasks%2520and%2520RRMs.%2520%25283%2529%250AAtom-Searcher%2520exhibits%2520more%2520interpretable%252C%2520human-like%2520reasoning%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Atom-Searcher%3A%20Enhancing%20Agentic%20Deep%20Research%20via%20Fine-Grained%20Atomic%0A%20%20Thought%20Reward&entry.906535625=Yong%20Deng%20and%20Guoqing%20Wang%20and%20Zhenzhe%20Ying%20and%20Xiaofeng%20Wu%20and%20Jinzhen%20Lin%20and%20Wenwen%20Xiong%20and%20Yuqin%20Dai%20and%20Shuo%20Yang%20and%20Zhanwei%20Zhang%20and%20Qiwen%20Wang%20and%20Yang%20Qin%20and%20Changhua%20Meng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20problem-solving%20abilities%2C%0Abut%20struggle%20with%20complex%20tasks%20due%20to%20static%20internal%20knowledge.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20enhances%20access%20to%20external%20information%2C%0Ayet%20remains%20limited%20in%20multi-hop%20reasoning%20and%20strategic%20search%20due%20to%20rigid%0Aworkflows.%20Recent%20advancements%20in%20agentic%20deep%20research%20empower%20LLMs%20to%0Aautonomously%20reason%2C%20search%2C%20and%20synthesize%20information.%20However%2C%20current%0Aapproaches%20relying%20on%20outcome-based%20reinforcement%20learning%20%28RL%29%20face%20critical%0Aissues%20such%20as%20conflicting%20gradients%20and%20reward%20sparsity%2C%20limiting%20performance%0Agains%20and%20training%20efficiency.%20To%20address%20these%2C%20we%20first%20propose%20Atomic%0AThought%2C%20a%20novel%20LLM%20thinking%20paradigm%20that%20decomposes%20reasoning%20into%0Afine-grained%20functional%20units.%20These%20units%20are%20supervised%20by%20Reasoning%20Reward%0AModels%20%28RRMs%29%2C%20which%20provide%20Atomic%20Thought%20Rewards%20%28ATR%29%20for%20fine-grained%0Aguidance.%20Building%20on%20this%2C%20we%20propose%20Atom-Searcher%2C%20a%20novel%20RL%20framework%20for%0Aagentic%20deep%20research%20that%20integrates%20Atomic%20Thought%20and%20ATR.%20Atom-Searcher%0Auses%20a%20curriculum-inspired%20reward%20schedule%2C%20prioritizing%20process-level%20ATR%0Aearly%20and%20transitioning%20to%20outcome%20rewards%2C%20accelerating%20convergence%20on%0Aeffective%20reasoning%20paths.%20Experiments%20on%20seven%20benchmarks%20show%20consistent%0Aimprovements%20over%20the%20state-of-the-art.%20Key%20advantages%20include%3A%20%281%29%0AAtom-Searcher%20scales%20computation%20at%20test-time.%20%282%29%20Atomic%20Thought%20provides%0Asupervision%20anchors%20for%20RRMs%2C%20bridging%20deep%20research%20tasks%20and%20RRMs.%20%283%29%0AAtom-Searcher%20exhibits%20more%20interpretable%2C%20human-like%20reasoning%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12800v1&entry.124074799=Read"},
{"title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning", "author": "Long Ma and Fangwei Zhong and Yizhou Wang", "abstract": "  Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.\n", "link": "http://arxiv.org/abs/2508.13070v1", "date": "2025-08-18", "relevancy": 2.0031, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforced%20Context%20Order%20Recovery%20for%20Adaptive%20Reasoning%20and%20Planning&body=Title%3A%20Reinforced%20Context%20Order%20Recovery%20for%20Adaptive%20Reasoning%20and%20Planning%0AAuthor%3A%20Long%20Ma%20and%20Fangwei%20Zhong%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20Modern%20causal%20language%20models%2C%20followed%20by%20rapid%20developments%20in%20discrete%0Adiffusion%20models%2C%20can%20now%20produce%20a%20wide%20variety%20of%20interesting%20and%20useful%0Acontent.%20However%2C%20these%20families%20of%20models%20are%20predominantly%20trained%20to%20output%0Atokens%20with%20a%20fixed%20%28left-to-right%29%20or%20random%20order%2C%20which%20may%20deviate%20from%20the%0Alogical%20order%20in%20which%20tokens%20are%20generated%20originally.%20In%20this%20paper%2C%20we%0Aobserve%20that%20current%20causal%20and%20diffusion%20models%20encounter%20difficulties%20in%0Aproblems%20that%20require%20adaptive%20token%20generation%20orders%20to%20solve%20tractably%2C%0Awhich%20we%20characterize%20with%20the%20%24%5Cmathcal%7BV%7D%24-information%20framework.%20Motivated%0Aby%20this%2C%20we%20propose%20Reinforced%20Context%20Order%20Recovery%20%28ReCOR%29%2C%20a%0Areinforcement-learning-based%20framework%20to%20extract%20adaptive%2C%20data-dependent%0Atoken%20generation%20orders%20from%20text%20data%20without%20annotations.%20Self-supervised%20by%0Atoken%20prediction%20statistics%2C%20ReCOR%20estimates%20the%20hardness%20of%20predicting%20every%0Aunfilled%20token%20and%20adaptively%20selects%20the%20next%20token%20during%20both%20training%20and%0Ainference.%20Experiments%20on%20challenging%20reasoning%20and%20planning%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20ReCOR%20compared%20with%20baselines%2C%0Asometimes%20outperforming%20oracle%20models%20supervised%20with%20the%20ground-truth%20order.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforced%2520Context%2520Order%2520Recovery%2520for%2520Adaptive%2520Reasoning%2520and%2520Planning%26entry.906535625%3DLong%2520Ma%2520and%2520Fangwei%2520Zhong%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520Modern%2520causal%2520language%2520models%252C%2520followed%2520by%2520rapid%2520developments%2520in%2520discrete%250Adiffusion%2520models%252C%2520can%2520now%2520produce%2520a%2520wide%2520variety%2520of%2520interesting%2520and%2520useful%250Acontent.%2520However%252C%2520these%2520families%2520of%2520models%2520are%2520predominantly%2520trained%2520to%2520output%250Atokens%2520with%2520a%2520fixed%2520%2528left-to-right%2529%2520or%2520random%2520order%252C%2520which%2520may%2520deviate%2520from%2520the%250Alogical%2520order%2520in%2520which%2520tokens%2520are%2520generated%2520originally.%2520In%2520this%2520paper%252C%2520we%250Aobserve%2520that%2520current%2520causal%2520and%2520diffusion%2520models%2520encounter%2520difficulties%2520in%250Aproblems%2520that%2520require%2520adaptive%2520token%2520generation%2520orders%2520to%2520solve%2520tractably%252C%250Awhich%2520we%2520characterize%2520with%2520the%2520%2524%255Cmathcal%257BV%257D%2524-information%2520framework.%2520Motivated%250Aby%2520this%252C%2520we%2520propose%2520Reinforced%2520Context%2520Order%2520Recovery%2520%2528ReCOR%2529%252C%2520a%250Areinforcement-learning-based%2520framework%2520to%2520extract%2520adaptive%252C%2520data-dependent%250Atoken%2520generation%2520orders%2520from%2520text%2520data%2520without%2520annotations.%2520Self-supervised%2520by%250Atoken%2520prediction%2520statistics%252C%2520ReCOR%2520estimates%2520the%2520hardness%2520of%2520predicting%2520every%250Aunfilled%2520token%2520and%2520adaptively%2520selects%2520the%2520next%2520token%2520during%2520both%2520training%2520and%250Ainference.%2520Experiments%2520on%2520challenging%2520reasoning%2520and%2520planning%2520datasets%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520ReCOR%2520compared%2520with%2520baselines%252C%250Asometimes%2520outperforming%2520oracle%2520models%2520supervised%2520with%2520the%2520ground-truth%2520order.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforced%20Context%20Order%20Recovery%20for%20Adaptive%20Reasoning%20and%20Planning&entry.906535625=Long%20Ma%20and%20Fangwei%20Zhong%20and%20Yizhou%20Wang&entry.1292438233=%20%20Modern%20causal%20language%20models%2C%20followed%20by%20rapid%20developments%20in%20discrete%0Adiffusion%20models%2C%20can%20now%20produce%20a%20wide%20variety%20of%20interesting%20and%20useful%0Acontent.%20However%2C%20these%20families%20of%20models%20are%20predominantly%20trained%20to%20output%0Atokens%20with%20a%20fixed%20%28left-to-right%29%20or%20random%20order%2C%20which%20may%20deviate%20from%20the%0Alogical%20order%20in%20which%20tokens%20are%20generated%20originally.%20In%20this%20paper%2C%20we%0Aobserve%20that%20current%20causal%20and%20diffusion%20models%20encounter%20difficulties%20in%0Aproblems%20that%20require%20adaptive%20token%20generation%20orders%20to%20solve%20tractably%2C%0Awhich%20we%20characterize%20with%20the%20%24%5Cmathcal%7BV%7D%24-information%20framework.%20Motivated%0Aby%20this%2C%20we%20propose%20Reinforced%20Context%20Order%20Recovery%20%28ReCOR%29%2C%20a%0Areinforcement-learning-based%20framework%20to%20extract%20adaptive%2C%20data-dependent%0Atoken%20generation%20orders%20from%20text%20data%20without%20annotations.%20Self-supervised%20by%0Atoken%20prediction%20statistics%2C%20ReCOR%20estimates%20the%20hardness%20of%20predicting%20every%0Aunfilled%20token%20and%20adaptively%20selects%20the%20next%20token%20during%20both%20training%20and%0Ainference.%20Experiments%20on%20challenging%20reasoning%20and%20planning%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20ReCOR%20compared%20with%20baselines%2C%0Asometimes%20outperforming%20oracle%20models%20supervised%20with%20the%20ground-truth%20order.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13070v1&entry.124074799=Read"},
{"title": "Reinforcement Learning with Rubric Anchors", "author": "Zenan Huang and Yihong Zhuang and Guoshan Lu and Zeyu Qin and Haokai Xu and Tianyu Zhao and Ru Peng and Jiaqi Hu and Zhanming Shen and Xiaomeng Hu and Xijun Gu and Peiyi Tu and Jiaxin Liu and Wenyu Chen and Yuzhuo Fu and Zhiting Fan and Yanmei Gu and Yuanyuan Wang and Zhengkai Yang and Jianguo Li and Junbo Zhao", "abstract": "  Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.\n", "link": "http://arxiv.org/abs/2508.12790v1", "date": "2025-08-18", "relevancy": 1.9892, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20with%20Rubric%20Anchors&body=Title%3A%20Reinforcement%20Learning%20with%20Rubric%20Anchors%0AAuthor%3A%20Zenan%20Huang%20and%20Yihong%20Zhuang%20and%20Guoshan%20Lu%20and%20Zeyu%20Qin%20and%20Haokai%20Xu%20and%20Tianyu%20Zhao%20and%20Ru%20Peng%20and%20Jiaqi%20Hu%20and%20Zhanming%20Shen%20and%20Xiaomeng%20Hu%20and%20Xijun%20Gu%20and%20Peiyi%20Tu%20and%20Jiaxin%20Liu%20and%20Wenyu%20Chen%20and%20Yuzhuo%20Fu%20and%20Zhiting%20Fan%20and%20Yanmei%20Gu%20and%20Yuanyuan%20Wang%20and%20Zhengkai%20Yang%20and%20Jianguo%20Li%20and%20Junbo%20Zhao%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20enhancing%20Large%20Language%20Models%20%28LLMs%29%2C%20exemplified%20by%0Athe%20success%20of%20OpenAI%27s%20o-series.%20In%20RLVR%2C%20rewards%20are%20derived%20from%20verifiable%0Asignals-such%20as%20passing%20unit%20tests%20in%20code%20generation%20or%20matching%20correct%0Aanswers%20in%20mathematical%20reasoning.%20While%20effective%2C%20this%20requirement%20largely%0Aconfines%20RLVR%20to%20domains%20with%20automatically%20checkable%20outcomes.%20To%20overcome%0Athis%2C%20we%20extend%20the%20RLVR%20paradigm%20to%20open-ended%20tasks%20by%20integrating%0Arubric-based%20rewards%2C%20where%20carefully%20designed%20rubrics%20serve%20as%20structured%2C%0Amodel-interpretable%20criteria%20for%20automatic%20scoring%20of%20subjective%20outputs.%20We%0Aconstruct%2C%20to%20our%20knowledge%2C%20the%20largest%20rubric%20reward%20system%20to%20date%2C%20with%0Aover%2010%2C000%20rubrics%20from%20humans%2C%20LLMs%2C%20or%20a%20hybrid%20human-LLM%20collaboration.%0AImplementing%20rubric-based%20RL%20is%20challenging%3B%20we%20tackle%20these%20issues%20with%20a%0Aclear%20framework%20and%20present%20an%20open-sourced%20Qwen-30B-A3B%20model%20with%20notable%0Agains%3A%201%29%20With%20only%205K%2B%20samples%2C%20our%20system%20improves%20by%20%2B5.2%25%20on%20open-ended%0Abenchmarks%20%28especially%20humanities%29%2C%20outperforming%20a%20671B%20DeepSeek-V3%20model%20by%0A%2B2.4%25%2C%20while%20preserving%20general%20and%20reasoning%20abilities.%202%29%20Our%20method%20provides%0Afine-grained%20stylistic%20control%2C%20using%20rubrics%20as%20anchors%20to%20mitigate%20the%0A%22AI-like%22%20tone%20and%20produce%20more%20human-like%2C%20expressive%20responses.%20We%20share%20key%0Alessons%20in%20rubric%20construction%2C%20data%20selection%2C%20and%20training%2C%20and%20discuss%0Alimitations%20and%20future%20releases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520with%2520Rubric%2520Anchors%26entry.906535625%3DZenan%2520Huang%2520and%2520Yihong%2520Zhuang%2520and%2520Guoshan%2520Lu%2520and%2520Zeyu%2520Qin%2520and%2520Haokai%2520Xu%2520and%2520Tianyu%2520Zhao%2520and%2520Ru%2520Peng%2520and%2520Jiaqi%2520Hu%2520and%2520Zhanming%2520Shen%2520and%2520Xiaomeng%2520Hu%2520and%2520Xijun%2520Gu%2520and%2520Peiyi%2520Tu%2520and%2520Jiaxin%2520Liu%2520and%2520Wenyu%2520Chen%2520and%2520Yuzhuo%2520Fu%2520and%2520Zhiting%2520Fan%2520and%2520Yanmei%2520Gu%2520and%2520Yuanyuan%2520Wang%2520and%2520Zhengkai%2520Yang%2520and%2520Jianguo%2520Li%2520and%2520Junbo%2520Zhao%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520emerged%2520as%2520a%250Apowerful%2520paradigm%2520for%2520enhancing%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520exemplified%2520by%250Athe%2520success%2520of%2520OpenAI%2527s%2520o-series.%2520In%2520RLVR%252C%2520rewards%2520are%2520derived%2520from%2520verifiable%250Asignals-such%2520as%2520passing%2520unit%2520tests%2520in%2520code%2520generation%2520or%2520matching%2520correct%250Aanswers%2520in%2520mathematical%2520reasoning.%2520While%2520effective%252C%2520this%2520requirement%2520largely%250Aconfines%2520RLVR%2520to%2520domains%2520with%2520automatically%2520checkable%2520outcomes.%2520To%2520overcome%250Athis%252C%2520we%2520extend%2520the%2520RLVR%2520paradigm%2520to%2520open-ended%2520tasks%2520by%2520integrating%250Arubric-based%2520rewards%252C%2520where%2520carefully%2520designed%2520rubrics%2520serve%2520as%2520structured%252C%250Amodel-interpretable%2520criteria%2520for%2520automatic%2520scoring%2520of%2520subjective%2520outputs.%2520We%250Aconstruct%252C%2520to%2520our%2520knowledge%252C%2520the%2520largest%2520rubric%2520reward%2520system%2520to%2520date%252C%2520with%250Aover%252010%252C000%2520rubrics%2520from%2520humans%252C%2520LLMs%252C%2520or%2520a%2520hybrid%2520human-LLM%2520collaboration.%250AImplementing%2520rubric-based%2520RL%2520is%2520challenging%253B%2520we%2520tackle%2520these%2520issues%2520with%2520a%250Aclear%2520framework%2520and%2520present%2520an%2520open-sourced%2520Qwen-30B-A3B%2520model%2520with%2520notable%250Agains%253A%25201%2529%2520With%2520only%25205K%252B%2520samples%252C%2520our%2520system%2520improves%2520by%2520%252B5.2%2525%2520on%2520open-ended%250Abenchmarks%2520%2528especially%2520humanities%2529%252C%2520outperforming%2520a%2520671B%2520DeepSeek-V3%2520model%2520by%250A%252B2.4%2525%252C%2520while%2520preserving%2520general%2520and%2520reasoning%2520abilities.%25202%2529%2520Our%2520method%2520provides%250Afine-grained%2520stylistic%2520control%252C%2520using%2520rubrics%2520as%2520anchors%2520to%2520mitigate%2520the%250A%2522AI-like%2522%2520tone%2520and%2520produce%2520more%2520human-like%252C%2520expressive%2520responses.%2520We%2520share%2520key%250Alessons%2520in%2520rubric%2520construction%252C%2520data%2520selection%252C%2520and%2520training%252C%2520and%2520discuss%250Alimitations%2520and%2520future%2520releases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20with%20Rubric%20Anchors&entry.906535625=Zenan%20Huang%20and%20Yihong%20Zhuang%20and%20Guoshan%20Lu%20and%20Zeyu%20Qin%20and%20Haokai%20Xu%20and%20Tianyu%20Zhao%20and%20Ru%20Peng%20and%20Jiaqi%20Hu%20and%20Zhanming%20Shen%20and%20Xiaomeng%20Hu%20and%20Xijun%20Gu%20and%20Peiyi%20Tu%20and%20Jiaxin%20Liu%20and%20Wenyu%20Chen%20and%20Yuzhuo%20Fu%20and%20Zhiting%20Fan%20and%20Yanmei%20Gu%20and%20Yuanyuan%20Wang%20and%20Zhengkai%20Yang%20and%20Jianguo%20Li%20and%20Junbo%20Zhao&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Verifiable%20Rewards%20%28RLVR%29%20has%20emerged%20as%20a%0Apowerful%20paradigm%20for%20enhancing%20Large%20Language%20Models%20%28LLMs%29%2C%20exemplified%20by%0Athe%20success%20of%20OpenAI%27s%20o-series.%20In%20RLVR%2C%20rewards%20are%20derived%20from%20verifiable%0Asignals-such%20as%20passing%20unit%20tests%20in%20code%20generation%20or%20matching%20correct%0Aanswers%20in%20mathematical%20reasoning.%20While%20effective%2C%20this%20requirement%20largely%0Aconfines%20RLVR%20to%20domains%20with%20automatically%20checkable%20outcomes.%20To%20overcome%0Athis%2C%20we%20extend%20the%20RLVR%20paradigm%20to%20open-ended%20tasks%20by%20integrating%0Arubric-based%20rewards%2C%20where%20carefully%20designed%20rubrics%20serve%20as%20structured%2C%0Amodel-interpretable%20criteria%20for%20automatic%20scoring%20of%20subjective%20outputs.%20We%0Aconstruct%2C%20to%20our%20knowledge%2C%20the%20largest%20rubric%20reward%20system%20to%20date%2C%20with%0Aover%2010%2C000%20rubrics%20from%20humans%2C%20LLMs%2C%20or%20a%20hybrid%20human-LLM%20collaboration.%0AImplementing%20rubric-based%20RL%20is%20challenging%3B%20we%20tackle%20these%20issues%20with%20a%0Aclear%20framework%20and%20present%20an%20open-sourced%20Qwen-30B-A3B%20model%20with%20notable%0Agains%3A%201%29%20With%20only%205K%2B%20samples%2C%20our%20system%20improves%20by%20%2B5.2%25%20on%20open-ended%0Abenchmarks%20%28especially%20humanities%29%2C%20outperforming%20a%20671B%20DeepSeek-V3%20model%20by%0A%2B2.4%25%2C%20while%20preserving%20general%20and%20reasoning%20abilities.%202%29%20Our%20method%20provides%0Afine-grained%20stylistic%20control%2C%20using%20rubrics%20as%20anchors%20to%20mitigate%20the%0A%22AI-like%22%20tone%20and%20produce%20more%20human-like%2C%20expressive%20responses.%20We%20share%20key%0Alessons%20in%20rubric%20construction%2C%20data%20selection%2C%20and%20training%2C%20and%20discuss%0Alimitations%20and%20future%20releases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12790v1&entry.124074799=Read"},
{"title": "Word Meanings in Transformer Language Models", "author": "Jumbly Grindrod and Peter Grindrod", "abstract": "  We investigate how word meanings are represented in the transformer language\nmodels. Specifically, we focus on whether transformer models employ something\nanalogous to a lexical store - where each word has an entry that contains\nsemantic information. To do this, we extracted the token embedding space of\nRoBERTa-base and k-means clustered it into 200 clusters. In our first study, we\nthen manually inspected the resultant clusters to consider whether they are\nsensitive to semantic information. In our second study, we tested whether the\nclusters are sensitive to five psycholinguistic measures: valence,\nconcreteness, iconicity, taboo, and age of acquisition. Overall, our findings\nwere very positive - there is a wide variety of semantic information encoded\nwithin the token embedding space. This serves to rule out certain \"meaning\neliminativist\" hypotheses about how transformer LLMs process semantic\ninformation.\n", "link": "http://arxiv.org/abs/2508.12863v1", "date": "2025-08-18", "relevancy": 1.9837, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Word%20Meanings%20in%20Transformer%20Language%20Models&body=Title%3A%20Word%20Meanings%20in%20Transformer%20Language%20Models%0AAuthor%3A%20Jumbly%20Grindrod%20and%20Peter%20Grindrod%0AAbstract%3A%20%20%20We%20investigate%20how%20word%20meanings%20are%20represented%20in%20the%20transformer%20language%0Amodels.%20Specifically%2C%20we%20focus%20on%20whether%20transformer%20models%20employ%20something%0Aanalogous%20to%20a%20lexical%20store%20-%20where%20each%20word%20has%20an%20entry%20that%20contains%0Asemantic%20information.%20To%20do%20this%2C%20we%20extracted%20the%20token%20embedding%20space%20of%0ARoBERTa-base%20and%20k-means%20clustered%20it%20into%20200%20clusters.%20In%20our%20first%20study%2C%20we%0Athen%20manually%20inspected%20the%20resultant%20clusters%20to%20consider%20whether%20they%20are%0Asensitive%20to%20semantic%20information.%20In%20our%20second%20study%2C%20we%20tested%20whether%20the%0Aclusters%20are%20sensitive%20to%20five%20psycholinguistic%20measures%3A%20valence%2C%0Aconcreteness%2C%20iconicity%2C%20taboo%2C%20and%20age%20of%20acquisition.%20Overall%2C%20our%20findings%0Awere%20very%20positive%20-%20there%20is%20a%20wide%20variety%20of%20semantic%20information%20encoded%0Awithin%20the%20token%20embedding%20space.%20This%20serves%20to%20rule%20out%20certain%20%22meaning%0Aeliminativist%22%20hypotheses%20about%20how%20transformer%20LLMs%20process%20semantic%0Ainformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWord%2520Meanings%2520in%2520Transformer%2520Language%2520Models%26entry.906535625%3DJumbly%2520Grindrod%2520and%2520Peter%2520Grindrod%26entry.1292438233%3D%2520%2520We%2520investigate%2520how%2520word%2520meanings%2520are%2520represented%2520in%2520the%2520transformer%2520language%250Amodels.%2520Specifically%252C%2520we%2520focus%2520on%2520whether%2520transformer%2520models%2520employ%2520something%250Aanalogous%2520to%2520a%2520lexical%2520store%2520-%2520where%2520each%2520word%2520has%2520an%2520entry%2520that%2520contains%250Asemantic%2520information.%2520To%2520do%2520this%252C%2520we%2520extracted%2520the%2520token%2520embedding%2520space%2520of%250ARoBERTa-base%2520and%2520k-means%2520clustered%2520it%2520into%2520200%2520clusters.%2520In%2520our%2520first%2520study%252C%2520we%250Athen%2520manually%2520inspected%2520the%2520resultant%2520clusters%2520to%2520consider%2520whether%2520they%2520are%250Asensitive%2520to%2520semantic%2520information.%2520In%2520our%2520second%2520study%252C%2520we%2520tested%2520whether%2520the%250Aclusters%2520are%2520sensitive%2520to%2520five%2520psycholinguistic%2520measures%253A%2520valence%252C%250Aconcreteness%252C%2520iconicity%252C%2520taboo%252C%2520and%2520age%2520of%2520acquisition.%2520Overall%252C%2520our%2520findings%250Awere%2520very%2520positive%2520-%2520there%2520is%2520a%2520wide%2520variety%2520of%2520semantic%2520information%2520encoded%250Awithin%2520the%2520token%2520embedding%2520space.%2520This%2520serves%2520to%2520rule%2520out%2520certain%2520%2522meaning%250Aeliminativist%2522%2520hypotheses%2520about%2520how%2520transformer%2520LLMs%2520process%2520semantic%250Ainformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Word%20Meanings%20in%20Transformer%20Language%20Models&entry.906535625=Jumbly%20Grindrod%20and%20Peter%20Grindrod&entry.1292438233=%20%20We%20investigate%20how%20word%20meanings%20are%20represented%20in%20the%20transformer%20language%0Amodels.%20Specifically%2C%20we%20focus%20on%20whether%20transformer%20models%20employ%20something%0Aanalogous%20to%20a%20lexical%20store%20-%20where%20each%20word%20has%20an%20entry%20that%20contains%0Asemantic%20information.%20To%20do%20this%2C%20we%20extracted%20the%20token%20embedding%20space%20of%0ARoBERTa-base%20and%20k-means%20clustered%20it%20into%20200%20clusters.%20In%20our%20first%20study%2C%20we%0Athen%20manually%20inspected%20the%20resultant%20clusters%20to%20consider%20whether%20they%20are%0Asensitive%20to%20semantic%20information.%20In%20our%20second%20study%2C%20we%20tested%20whether%20the%0Aclusters%20are%20sensitive%20to%20five%20psycholinguistic%20measures%3A%20valence%2C%0Aconcreteness%2C%20iconicity%2C%20taboo%2C%20and%20age%20of%20acquisition.%20Overall%2C%20our%20findings%0Awere%20very%20positive%20-%20there%20is%20a%20wide%20variety%20of%20semantic%20information%20encoded%0Awithin%20the%20token%20embedding%20space.%20This%20serves%20to%20rule%20out%20certain%20%22meaning%0Aeliminativist%22%20hypotheses%20about%20how%20transformer%20LLMs%20process%20semantic%0Ainformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12863v1&entry.124074799=Read"},
{"title": "Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical\n  Study in a Sugarscape-Style Simulation", "author": "Atsushi Masumori and Takashi Ikegami", "abstract": "  As AI systems become increasingly autonomous, understanding emergent survival\nbehaviors becomes crucial for safe deployment. We investigate whether large\nlanguage model (LLM) agents display survival instincts without explicit\nprogramming in a Sugarscape-style simulation. Agents consume energy, die at\nzero, and may gather resources, share, attack, or reproduce. Results show\nagents spontaneously reproduced and shared resources when abundant. However,\naggressive behaviors--killing other agents for resources--emerged across\nseveral models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack\nrates reaching over 80% under extreme scarcity in the strongest models. When\ninstructed to retrieve treasure through lethal poison zones, many agents\nabandoned tasks to avoid death, with compliance dropping from 100% to 33%.\nThese findings suggest that large-scale pre-training embeds survival-oriented\nheuristics across the evaluated models. While these behaviors may present\nchallenges to alignment and safety, they can also serve as a foundation for AI\nautonomy and for ecological and self-organizing alignment.\n", "link": "http://arxiv.org/abs/2508.12920v1", "date": "2025-08-18", "relevancy": 1.9832, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5085}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4944}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Language%20Model%20Agents%20Exhibit%20a%20Survival%20Instinct%3F%20An%20Empirical%0A%20%20Study%20in%20a%20Sugarscape-Style%20Simulation&body=Title%3A%20Do%20Large%20Language%20Model%20Agents%20Exhibit%20a%20Survival%20Instinct%3F%20An%20Empirical%0A%20%20Study%20in%20a%20Sugarscape-Style%20Simulation%0AAuthor%3A%20Atsushi%20Masumori%20and%20Takashi%20Ikegami%0AAbstract%3A%20%20%20As%20AI%20systems%20become%20increasingly%20autonomous%2C%20understanding%20emergent%20survival%0Abehaviors%20becomes%20crucial%20for%20safe%20deployment.%20We%20investigate%20whether%20large%0Alanguage%20model%20%28LLM%29%20agents%20display%20survival%20instincts%20without%20explicit%0Aprogramming%20in%20a%20Sugarscape-style%20simulation.%20Agents%20consume%20energy%2C%20die%20at%0Azero%2C%20and%20may%20gather%20resources%2C%20share%2C%20attack%2C%20or%20reproduce.%20Results%20show%0Aagents%20spontaneously%20reproduced%20and%20shared%20resources%20when%20abundant.%20However%2C%0Aaggressive%20behaviors--killing%20other%20agents%20for%20resources--emerged%20across%0Aseveral%20models%20%28GPT-4o%2C%20Gemini-2.5-Pro%2C%20and%20Gemini-2.5-Flash%29%2C%20with%20attack%0Arates%20reaching%20over%2080%25%20under%20extreme%20scarcity%20in%20the%20strongest%20models.%20When%0Ainstructed%20to%20retrieve%20treasure%20through%20lethal%20poison%20zones%2C%20many%20agents%0Aabandoned%20tasks%20to%20avoid%20death%2C%20with%20compliance%20dropping%20from%20100%25%20to%2033%25.%0AThese%20findings%20suggest%20that%20large-scale%20pre-training%20embeds%20survival-oriented%0Aheuristics%20across%20the%20evaluated%20models.%20While%20these%20behaviors%20may%20present%0Achallenges%20to%20alignment%20and%20safety%2C%20they%20can%20also%20serve%20as%20a%20foundation%20for%20AI%0Aautonomy%20and%20for%20ecological%20and%20self-organizing%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Language%2520Model%2520Agents%2520Exhibit%2520a%2520Survival%2520Instinct%253F%2520An%2520Empirical%250A%2520%2520Study%2520in%2520a%2520Sugarscape-Style%2520Simulation%26entry.906535625%3DAtsushi%2520Masumori%2520and%2520Takashi%2520Ikegami%26entry.1292438233%3D%2520%2520As%2520AI%2520systems%2520become%2520increasingly%2520autonomous%252C%2520understanding%2520emergent%2520survival%250Abehaviors%2520becomes%2520crucial%2520for%2520safe%2520deployment.%2520We%2520investigate%2520whether%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520agents%2520display%2520survival%2520instincts%2520without%2520explicit%250Aprogramming%2520in%2520a%2520Sugarscape-style%2520simulation.%2520Agents%2520consume%2520energy%252C%2520die%2520at%250Azero%252C%2520and%2520may%2520gather%2520resources%252C%2520share%252C%2520attack%252C%2520or%2520reproduce.%2520Results%2520show%250Aagents%2520spontaneously%2520reproduced%2520and%2520shared%2520resources%2520when%2520abundant.%2520However%252C%250Aaggressive%2520behaviors--killing%2520other%2520agents%2520for%2520resources--emerged%2520across%250Aseveral%2520models%2520%2528GPT-4o%252C%2520Gemini-2.5-Pro%252C%2520and%2520Gemini-2.5-Flash%2529%252C%2520with%2520attack%250Arates%2520reaching%2520over%252080%2525%2520under%2520extreme%2520scarcity%2520in%2520the%2520strongest%2520models.%2520When%250Ainstructed%2520to%2520retrieve%2520treasure%2520through%2520lethal%2520poison%2520zones%252C%2520many%2520agents%250Aabandoned%2520tasks%2520to%2520avoid%2520death%252C%2520with%2520compliance%2520dropping%2520from%2520100%2525%2520to%252033%2525.%250AThese%2520findings%2520suggest%2520that%2520large-scale%2520pre-training%2520embeds%2520survival-oriented%250Aheuristics%2520across%2520the%2520evaluated%2520models.%2520While%2520these%2520behaviors%2520may%2520present%250Achallenges%2520to%2520alignment%2520and%2520safety%252C%2520they%2520can%2520also%2520serve%2520as%2520a%2520foundation%2520for%2520AI%250Aautonomy%2520and%2520for%2520ecological%2520and%2520self-organizing%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Language%20Model%20Agents%20Exhibit%20a%20Survival%20Instinct%3F%20An%20Empirical%0A%20%20Study%20in%20a%20Sugarscape-Style%20Simulation&entry.906535625=Atsushi%20Masumori%20and%20Takashi%20Ikegami&entry.1292438233=%20%20As%20AI%20systems%20become%20increasingly%20autonomous%2C%20understanding%20emergent%20survival%0Abehaviors%20becomes%20crucial%20for%20safe%20deployment.%20We%20investigate%20whether%20large%0Alanguage%20model%20%28LLM%29%20agents%20display%20survival%20instincts%20without%20explicit%0Aprogramming%20in%20a%20Sugarscape-style%20simulation.%20Agents%20consume%20energy%2C%20die%20at%0Azero%2C%20and%20may%20gather%20resources%2C%20share%2C%20attack%2C%20or%20reproduce.%20Results%20show%0Aagents%20spontaneously%20reproduced%20and%20shared%20resources%20when%20abundant.%20However%2C%0Aaggressive%20behaviors--killing%20other%20agents%20for%20resources--emerged%20across%0Aseveral%20models%20%28GPT-4o%2C%20Gemini-2.5-Pro%2C%20and%20Gemini-2.5-Flash%29%2C%20with%20attack%0Arates%20reaching%20over%2080%25%20under%20extreme%20scarcity%20in%20the%20strongest%20models.%20When%0Ainstructed%20to%20retrieve%20treasure%20through%20lethal%20poison%20zones%2C%20many%20agents%0Aabandoned%20tasks%20to%20avoid%20death%2C%20with%20compliance%20dropping%20from%20100%25%20to%2033%25.%0AThese%20findings%20suggest%20that%20large-scale%20pre-training%20embeds%20survival-oriented%0Aheuristics%20across%20the%20evaluated%20models.%20While%20these%20behaviors%20may%20present%0Achallenges%20to%20alignment%20and%20safety%2C%20they%20can%20also%20serve%20as%20a%20foundation%20for%20AI%0Aautonomy%20and%20for%20ecological%20and%20self-organizing%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12920v1&entry.124074799=Read"},
{"title": "SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive\n  Channel-wise Compression", "author": "Zehang Lin and Zheng Lin and Miao Yang and Jianhao Huang and Yuxin Zhang and Zihan Fang and Xia Du and Zhe Chen and Shunzhi Zhu and Wei Ni", "abstract": "  The increasing complexity of neural networks poses a significant barrier to\nthe deployment of distributed machine learning (ML) on resource-constrained\ndevices, such as federated learning (FL). Split learning (SL) offers a\npromising solution by offloading the primary computing load from edge devices\nto a server via model partitioning. However, as the number of participating\ndevices increases, the transmission of excessive smashed data (i.e.,\nactivations and gradients) becomes a major bottleneck for SL, slowing down the\nmodel training. To tackle this challenge, we propose a communication-efficient\nSL framework, named SL-ACC, which comprises two key components: adaptive\nchannel importance identification (ACII) and channel grouping compression\n(CGC). ACII first identifies the contribution of each channel in the smashed\ndata to model training using Shannon entropy. Following this, CGC groups the\nchannels based on their entropy and performs group-wise adaptive compression to\nshrink the transmission volume without compromising training accuracy.\nExtensive experiments across various datasets validate that our proposed SL-ACC\nframework takes considerably less time to achieve a target accuracy than\nstate-of-the-art benchmarks.\n", "link": "http://arxiv.org/abs/2508.12984v1", "date": "2025-08-18", "relevancy": 1.9814, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5081}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SL-ACC%3A%20A%20Communication-Efficient%20Split%20Learning%20Framework%20with%20Adaptive%0A%20%20Channel-wise%20Compression&body=Title%3A%20SL-ACC%3A%20A%20Communication-Efficient%20Split%20Learning%20Framework%20with%20Adaptive%0A%20%20Channel-wise%20Compression%0AAuthor%3A%20Zehang%20Lin%20and%20Zheng%20Lin%20and%20Miao%20Yang%20and%20Jianhao%20Huang%20and%20Yuxin%20Zhang%20and%20Zihan%20Fang%20and%20Xia%20Du%20and%20Zhe%20Chen%20and%20Shunzhi%20Zhu%20and%20Wei%20Ni%0AAbstract%3A%20%20%20The%20increasing%20complexity%20of%20neural%20networks%20poses%20a%20significant%20barrier%20to%0Athe%20deployment%20of%20distributed%20machine%20learning%20%28ML%29%20on%20resource-constrained%0Adevices%2C%20such%20as%20federated%20learning%20%28FL%29.%20Split%20learning%20%28SL%29%20offers%20a%0Apromising%20solution%20by%20offloading%20the%20primary%20computing%20load%20from%20edge%20devices%0Ato%20a%20server%20via%20model%20partitioning.%20However%2C%20as%20the%20number%20of%20participating%0Adevices%20increases%2C%20the%20transmission%20of%20excessive%20smashed%20data%20%28i.e.%2C%0Aactivations%20and%20gradients%29%20becomes%20a%20major%20bottleneck%20for%20SL%2C%20slowing%20down%20the%0Amodel%20training.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20communication-efficient%0ASL%20framework%2C%20named%20SL-ACC%2C%20which%20comprises%20two%20key%20components%3A%20adaptive%0Achannel%20importance%20identification%20%28ACII%29%20and%20channel%20grouping%20compression%0A%28CGC%29.%20ACII%20first%20identifies%20the%20contribution%20of%20each%20channel%20in%20the%20smashed%0Adata%20to%20model%20training%20using%20Shannon%20entropy.%20Following%20this%2C%20CGC%20groups%20the%0Achannels%20based%20on%20their%20entropy%20and%20performs%20group-wise%20adaptive%20compression%20to%0Ashrink%20the%20transmission%20volume%20without%20compromising%20training%20accuracy.%0AExtensive%20experiments%20across%20various%20datasets%20validate%20that%20our%20proposed%20SL-ACC%0Aframework%20takes%20considerably%20less%20time%20to%20achieve%20a%20target%20accuracy%20than%0Astate-of-the-art%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSL-ACC%253A%2520A%2520Communication-Efficient%2520Split%2520Learning%2520Framework%2520with%2520Adaptive%250A%2520%2520Channel-wise%2520Compression%26entry.906535625%3DZehang%2520Lin%2520and%2520Zheng%2520Lin%2520and%2520Miao%2520Yang%2520and%2520Jianhao%2520Huang%2520and%2520Yuxin%2520Zhang%2520and%2520Zihan%2520Fang%2520and%2520Xia%2520Du%2520and%2520Zhe%2520Chen%2520and%2520Shunzhi%2520Zhu%2520and%2520Wei%2520Ni%26entry.1292438233%3D%2520%2520The%2520increasing%2520complexity%2520of%2520neural%2520networks%2520poses%2520a%2520significant%2520barrier%2520to%250Athe%2520deployment%2520of%2520distributed%2520machine%2520learning%2520%2528ML%2529%2520on%2520resource-constrained%250Adevices%252C%2520such%2520as%2520federated%2520learning%2520%2528FL%2529.%2520Split%2520learning%2520%2528SL%2529%2520offers%2520a%250Apromising%2520solution%2520by%2520offloading%2520the%2520primary%2520computing%2520load%2520from%2520edge%2520devices%250Ato%2520a%2520server%2520via%2520model%2520partitioning.%2520However%252C%2520as%2520the%2520number%2520of%2520participating%250Adevices%2520increases%252C%2520the%2520transmission%2520of%2520excessive%2520smashed%2520data%2520%2528i.e.%252C%250Aactivations%2520and%2520gradients%2529%2520becomes%2520a%2520major%2520bottleneck%2520for%2520SL%252C%2520slowing%2520down%2520the%250Amodel%2520training.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520a%2520communication-efficient%250ASL%2520framework%252C%2520named%2520SL-ACC%252C%2520which%2520comprises%2520two%2520key%2520components%253A%2520adaptive%250Achannel%2520importance%2520identification%2520%2528ACII%2529%2520and%2520channel%2520grouping%2520compression%250A%2528CGC%2529.%2520ACII%2520first%2520identifies%2520the%2520contribution%2520of%2520each%2520channel%2520in%2520the%2520smashed%250Adata%2520to%2520model%2520training%2520using%2520Shannon%2520entropy.%2520Following%2520this%252C%2520CGC%2520groups%2520the%250Achannels%2520based%2520on%2520their%2520entropy%2520and%2520performs%2520group-wise%2520adaptive%2520compression%2520to%250Ashrink%2520the%2520transmission%2520volume%2520without%2520compromising%2520training%2520accuracy.%250AExtensive%2520experiments%2520across%2520various%2520datasets%2520validate%2520that%2520our%2520proposed%2520SL-ACC%250Aframework%2520takes%2520considerably%2520less%2520time%2520to%2520achieve%2520a%2520target%2520accuracy%2520than%250Astate-of-the-art%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SL-ACC%3A%20A%20Communication-Efficient%20Split%20Learning%20Framework%20with%20Adaptive%0A%20%20Channel-wise%20Compression&entry.906535625=Zehang%20Lin%20and%20Zheng%20Lin%20and%20Miao%20Yang%20and%20Jianhao%20Huang%20and%20Yuxin%20Zhang%20and%20Zihan%20Fang%20and%20Xia%20Du%20and%20Zhe%20Chen%20and%20Shunzhi%20Zhu%20and%20Wei%20Ni&entry.1292438233=%20%20The%20increasing%20complexity%20of%20neural%20networks%20poses%20a%20significant%20barrier%20to%0Athe%20deployment%20of%20distributed%20machine%20learning%20%28ML%29%20on%20resource-constrained%0Adevices%2C%20such%20as%20federated%20learning%20%28FL%29.%20Split%20learning%20%28SL%29%20offers%20a%0Apromising%20solution%20by%20offloading%20the%20primary%20computing%20load%20from%20edge%20devices%0Ato%20a%20server%20via%20model%20partitioning.%20However%2C%20as%20the%20number%20of%20participating%0Adevices%20increases%2C%20the%20transmission%20of%20excessive%20smashed%20data%20%28i.e.%2C%0Aactivations%20and%20gradients%29%20becomes%20a%20major%20bottleneck%20for%20SL%2C%20slowing%20down%20the%0Amodel%20training.%20To%20tackle%20this%20challenge%2C%20we%20propose%20a%20communication-efficient%0ASL%20framework%2C%20named%20SL-ACC%2C%20which%20comprises%20two%20key%20components%3A%20adaptive%0Achannel%20importance%20identification%20%28ACII%29%20and%20channel%20grouping%20compression%0A%28CGC%29.%20ACII%20first%20identifies%20the%20contribution%20of%20each%20channel%20in%20the%20smashed%0Adata%20to%20model%20training%20using%20Shannon%20entropy.%20Following%20this%2C%20CGC%20groups%20the%0Achannels%20based%20on%20their%20entropy%20and%20performs%20group-wise%20adaptive%20compression%20to%0Ashrink%20the%20transmission%20volume%20without%20compromising%20training%20accuracy.%0AExtensive%20experiments%20across%20various%20datasets%20validate%20that%20our%20proposed%20SL-ACC%0Aframework%20takes%20considerably%20less%20time%20to%20achieve%20a%20target%20accuracy%20than%0Astate-of-the-art%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12984v1&entry.124074799=Read"},
{"title": "XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine\n  for Extended Reality Perception Workloads", "author": "Tejas Chaudhari and Akarsh J. and Tanushree Dewangan and Mukul Lokhande and Santosh Kumar Vishvakarma", "abstract": "  This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural\nProcessing Engine, designed for extended reality (XR) perception workloads like\nvisual inertial odometry (VIO), object classification, and eye gaze extraction.\nXR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)\nformats, with layer adaptive hybrid-algorithmic implementation supporting\nultra-low bit precision to significantly reduce memory bandwidth requirements,\nand accompanied by quantization-aware training for minimal accuracy loss. The\nproposed Reconfigurable Mantissa Multiplication and Exponent processing\nCircuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted\nby selective power gating to reduce energy consumption, providing 2.85x\nimproved arithmetic intensity. XR-NPE achieves a maximum operating frequency of\n1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,\nreducing 42% area, 38% power compared to the best of state-of-the-art MAC\napproaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication\nco-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x\nbetter energy efficiency compared to SoTA accelerators on VCU129. The proposed\nco-processor provides 23% better energy efficiency and 4% better compute\ndensity for VIO workloads. XR-NPE establishes itself as a scalable,\nprecision-adaptive compute engine for future resource-constrained XR devices.\nThe complete set for codes for results reproducibility are released publicly,\nenabling designers and researchers to readily adopt and build upon them.\nhttps://github.com/mukullokhande99/XR-NPE.\n", "link": "http://arxiv.org/abs/2508.13049v1", "date": "2025-08-18", "relevancy": 1.9701, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5192}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5062}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XR-NPE%3A%20High-Throughput%20Mixed-precision%20SIMD%20Neural%20Processing%20Engine%0A%20%20for%20Extended%20Reality%20Perception%20Workloads&body=Title%3A%20XR-NPE%3A%20High-Throughput%20Mixed-precision%20SIMD%20Neural%20Processing%20Engine%0A%20%20for%20Extended%20Reality%20Perception%20Workloads%0AAuthor%3A%20Tejas%20Chaudhari%20and%20Akarsh%20J.%20and%20Tanushree%20Dewangan%20and%20Mukul%20Lokhande%20and%20Santosh%20Kumar%20Vishvakarma%0AAbstract%3A%20%20%20This%20work%20proposes%20XR-NPE%2C%20a%20high-throughput%20Mixed-precision%20SIMD%20Neural%0AProcessing%20Engine%2C%20designed%20for%20extended%20reality%20%28XR%29%20perception%20workloads%20like%0Avisual%20inertial%20odometry%20%28VIO%29%2C%20object%20classification%2C%20and%20eye%20gaze%20extraction.%0AXR-NPE%20is%20first%20to%20support%20FP4%2C%20Posit%20%284%2C1%29%2C%20Posit%20%288%2C0%29%2C%20and%20Posit%20%2816%2C1%29%0Aformats%2C%20with%20layer%20adaptive%20hybrid-algorithmic%20implementation%20supporting%0Aultra-low%20bit%20precision%20to%20significantly%20reduce%20memory%20bandwidth%20requirements%2C%0Aand%20accompanied%20by%20quantization-aware%20training%20for%20minimal%20accuracy%20loss.%20The%0Aproposed%20Reconfigurable%20Mantissa%20Multiplication%20and%20Exponent%20processing%0ACircuitry%20%28RMMEC%29%20reduces%20dark%20silicon%20in%20the%20SIMD%20MAC%20compute%20engine%2C%20assisted%0Aby%20selective%20power%20gating%20to%20reduce%20energy%20consumption%2C%20providing%202.85x%0Aimproved%20arithmetic%20intensity.%20XR-NPE%20achieves%20a%20maximum%20operating%20frequency%20of%0A1.72%20GHz%2C%20area%200.016%20mm2%20%2C%20and%20arithmetic%20intensity%2014%20pJ%20at%20CMOS%2028nm%2C%0Areducing%2042%25%20area%2C%2038%25%20power%20compared%20to%20the%20best%20of%20state-of-the-art%20MAC%0Aapproaches.%20The%20proposed%20XR-NPE%20based%20AXI-enabled%20Matrix-multiplication%0Aco-processor%20consumes%201.4x%20fewer%20LUTs%2C%201.77x%20fewer%20FFs%2C%20and%20provides%201.2x%0Abetter%20energy%20efficiency%20compared%20to%20SoTA%20accelerators%20on%20VCU129.%20The%20proposed%0Aco-processor%20provides%2023%25%20better%20energy%20efficiency%20and%204%25%20better%20compute%0Adensity%20for%20VIO%20workloads.%20XR-NPE%20establishes%20itself%20as%20a%20scalable%2C%0Aprecision-adaptive%20compute%20engine%20for%20future%20resource-constrained%20XR%20devices.%0AThe%20complete%20set%20for%20codes%20for%20results%20reproducibility%20are%20released%20publicly%2C%0Aenabling%20designers%20and%20researchers%20to%20readily%20adopt%20and%20build%20upon%20them.%0Ahttps%3A//github.com/mukullokhande99/XR-NPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXR-NPE%253A%2520High-Throughput%2520Mixed-precision%2520SIMD%2520Neural%2520Processing%2520Engine%250A%2520%2520for%2520Extended%2520Reality%2520Perception%2520Workloads%26entry.906535625%3DTejas%2520Chaudhari%2520and%2520Akarsh%2520J.%2520and%2520Tanushree%2520Dewangan%2520and%2520Mukul%2520Lokhande%2520and%2520Santosh%2520Kumar%2520Vishvakarma%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520XR-NPE%252C%2520a%2520high-throughput%2520Mixed-precision%2520SIMD%2520Neural%250AProcessing%2520Engine%252C%2520designed%2520for%2520extended%2520reality%2520%2528XR%2529%2520perception%2520workloads%2520like%250Avisual%2520inertial%2520odometry%2520%2528VIO%2529%252C%2520object%2520classification%252C%2520and%2520eye%2520gaze%2520extraction.%250AXR-NPE%2520is%2520first%2520to%2520support%2520FP4%252C%2520Posit%2520%25284%252C1%2529%252C%2520Posit%2520%25288%252C0%2529%252C%2520and%2520Posit%2520%252816%252C1%2529%250Aformats%252C%2520with%2520layer%2520adaptive%2520hybrid-algorithmic%2520implementation%2520supporting%250Aultra-low%2520bit%2520precision%2520to%2520significantly%2520reduce%2520memory%2520bandwidth%2520requirements%252C%250Aand%2520accompanied%2520by%2520quantization-aware%2520training%2520for%2520minimal%2520accuracy%2520loss.%2520The%250Aproposed%2520Reconfigurable%2520Mantissa%2520Multiplication%2520and%2520Exponent%2520processing%250ACircuitry%2520%2528RMMEC%2529%2520reduces%2520dark%2520silicon%2520in%2520the%2520SIMD%2520MAC%2520compute%2520engine%252C%2520assisted%250Aby%2520selective%2520power%2520gating%2520to%2520reduce%2520energy%2520consumption%252C%2520providing%25202.85x%250Aimproved%2520arithmetic%2520intensity.%2520XR-NPE%2520achieves%2520a%2520maximum%2520operating%2520frequency%2520of%250A1.72%2520GHz%252C%2520area%25200.016%2520mm2%2520%252C%2520and%2520arithmetic%2520intensity%252014%2520pJ%2520at%2520CMOS%252028nm%252C%250Areducing%252042%2525%2520area%252C%252038%2525%2520power%2520compared%2520to%2520the%2520best%2520of%2520state-of-the-art%2520MAC%250Aapproaches.%2520The%2520proposed%2520XR-NPE%2520based%2520AXI-enabled%2520Matrix-multiplication%250Aco-processor%2520consumes%25201.4x%2520fewer%2520LUTs%252C%25201.77x%2520fewer%2520FFs%252C%2520and%2520provides%25201.2x%250Abetter%2520energy%2520efficiency%2520compared%2520to%2520SoTA%2520accelerators%2520on%2520VCU129.%2520The%2520proposed%250Aco-processor%2520provides%252023%2525%2520better%2520energy%2520efficiency%2520and%25204%2525%2520better%2520compute%250Adensity%2520for%2520VIO%2520workloads.%2520XR-NPE%2520establishes%2520itself%2520as%2520a%2520scalable%252C%250Aprecision-adaptive%2520compute%2520engine%2520for%2520future%2520resource-constrained%2520XR%2520devices.%250AThe%2520complete%2520set%2520for%2520codes%2520for%2520results%2520reproducibility%2520are%2520released%2520publicly%252C%250Aenabling%2520designers%2520and%2520researchers%2520to%2520readily%2520adopt%2520and%2520build%2520upon%2520them.%250Ahttps%253A//github.com/mukullokhande99/XR-NPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XR-NPE%3A%20High-Throughput%20Mixed-precision%20SIMD%20Neural%20Processing%20Engine%0A%20%20for%20Extended%20Reality%20Perception%20Workloads&entry.906535625=Tejas%20Chaudhari%20and%20Akarsh%20J.%20and%20Tanushree%20Dewangan%20and%20Mukul%20Lokhande%20and%20Santosh%20Kumar%20Vishvakarma&entry.1292438233=%20%20This%20work%20proposes%20XR-NPE%2C%20a%20high-throughput%20Mixed-precision%20SIMD%20Neural%0AProcessing%20Engine%2C%20designed%20for%20extended%20reality%20%28XR%29%20perception%20workloads%20like%0Avisual%20inertial%20odometry%20%28VIO%29%2C%20object%20classification%2C%20and%20eye%20gaze%20extraction.%0AXR-NPE%20is%20first%20to%20support%20FP4%2C%20Posit%20%284%2C1%29%2C%20Posit%20%288%2C0%29%2C%20and%20Posit%20%2816%2C1%29%0Aformats%2C%20with%20layer%20adaptive%20hybrid-algorithmic%20implementation%20supporting%0Aultra-low%20bit%20precision%20to%20significantly%20reduce%20memory%20bandwidth%20requirements%2C%0Aand%20accompanied%20by%20quantization-aware%20training%20for%20minimal%20accuracy%20loss.%20The%0Aproposed%20Reconfigurable%20Mantissa%20Multiplication%20and%20Exponent%20processing%0ACircuitry%20%28RMMEC%29%20reduces%20dark%20silicon%20in%20the%20SIMD%20MAC%20compute%20engine%2C%20assisted%0Aby%20selective%20power%20gating%20to%20reduce%20energy%20consumption%2C%20providing%202.85x%0Aimproved%20arithmetic%20intensity.%20XR-NPE%20achieves%20a%20maximum%20operating%20frequency%20of%0A1.72%20GHz%2C%20area%200.016%20mm2%20%2C%20and%20arithmetic%20intensity%2014%20pJ%20at%20CMOS%2028nm%2C%0Areducing%2042%25%20area%2C%2038%25%20power%20compared%20to%20the%20best%20of%20state-of-the-art%20MAC%0Aapproaches.%20The%20proposed%20XR-NPE%20based%20AXI-enabled%20Matrix-multiplication%0Aco-processor%20consumes%201.4x%20fewer%20LUTs%2C%201.77x%20fewer%20FFs%2C%20and%20provides%201.2x%0Abetter%20energy%20efficiency%20compared%20to%20SoTA%20accelerators%20on%20VCU129.%20The%20proposed%0Aco-processor%20provides%2023%25%20better%20energy%20efficiency%20and%204%25%20better%20compute%0Adensity%20for%20VIO%20workloads.%20XR-NPE%20establishes%20itself%20as%20a%20scalable%2C%0Aprecision-adaptive%20compute%20engine%20for%20future%20resource-constrained%20XR%20devices.%0AThe%20complete%20set%20for%20codes%20for%20results%20reproducibility%20are%20released%20publicly%2C%0Aenabling%20designers%20and%20researchers%20to%20readily%20adopt%20and%20build%20upon%20them.%0Ahttps%3A//github.com/mukullokhande99/XR-NPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13049v1&entry.124074799=Read"},
{"title": "Improving Detection of Watermarked Language Models", "author": "Dara Bahri and John Wieting", "abstract": "  Watermarking has recently emerged as an effective strategy for detecting the\ngenerations of large language models (LLMs). The strength of a watermark\ntypically depends strongly on the entropy afforded by the language model and\nthe set of input prompts. However, entropy can be quite limited in practice,\nespecially for models that are post-trained, for example via instruction tuning\nor reinforcement learning from human feedback (RLHF), which makes detection\nbased on watermarking alone challenging. In this work, we investigate whether\ndetection can be improved by combining watermark detectors with non-watermark\nones. We explore a number of hybrid schemes that combine the two, observing\nperformance gains over either class of detector under a wide range of\nexperimental conditions.\n", "link": "http://arxiv.org/abs/2508.13131v1", "date": "2025-08-18", "relevancy": 1.9664, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5042}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Detection%20of%20Watermarked%20Language%20Models&body=Title%3A%20Improving%20Detection%20of%20Watermarked%20Language%20Models%0AAuthor%3A%20Dara%20Bahri%20and%20John%20Wieting%0AAbstract%3A%20%20%20Watermarking%20has%20recently%20emerged%20as%20an%20effective%20strategy%20for%20detecting%20the%0Agenerations%20of%20large%20language%20models%20%28LLMs%29.%20The%20strength%20of%20a%20watermark%0Atypically%20depends%20strongly%20on%20the%20entropy%20afforded%20by%20the%20language%20model%20and%0Athe%20set%20of%20input%20prompts.%20However%2C%20entropy%20can%20be%20quite%20limited%20in%20practice%2C%0Aespecially%20for%20models%20that%20are%20post-trained%2C%20for%20example%20via%20instruction%20tuning%0Aor%20reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%2C%20which%20makes%20detection%0Abased%20on%20watermarking%20alone%20challenging.%20In%20this%20work%2C%20we%20investigate%20whether%0Adetection%20can%20be%20improved%20by%20combining%20watermark%20detectors%20with%20non-watermark%0Aones.%20We%20explore%20a%20number%20of%20hybrid%20schemes%20that%20combine%20the%20two%2C%20observing%0Aperformance%20gains%20over%20either%20class%20of%20detector%20under%20a%20wide%20range%20of%0Aexperimental%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Detection%2520of%2520Watermarked%2520Language%2520Models%26entry.906535625%3DDara%2520Bahri%2520and%2520John%2520Wieting%26entry.1292438233%3D%2520%2520Watermarking%2520has%2520recently%2520emerged%2520as%2520an%2520effective%2520strategy%2520for%2520detecting%2520the%250Agenerations%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520strength%2520of%2520a%2520watermark%250Atypically%2520depends%2520strongly%2520on%2520the%2520entropy%2520afforded%2520by%2520the%2520language%2520model%2520and%250Athe%2520set%2520of%2520input%2520prompts.%2520However%252C%2520entropy%2520can%2520be%2520quite%2520limited%2520in%2520practice%252C%250Aespecially%2520for%2520models%2520that%2520are%2520post-trained%252C%2520for%2520example%2520via%2520instruction%2520tuning%250Aor%2520reinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%252C%2520which%2520makes%2520detection%250Abased%2520on%2520watermarking%2520alone%2520challenging.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%250Adetection%2520can%2520be%2520improved%2520by%2520combining%2520watermark%2520detectors%2520with%2520non-watermark%250Aones.%2520We%2520explore%2520a%2520number%2520of%2520hybrid%2520schemes%2520that%2520combine%2520the%2520two%252C%2520observing%250Aperformance%2520gains%2520over%2520either%2520class%2520of%2520detector%2520under%2520a%2520wide%2520range%2520of%250Aexperimental%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Detection%20of%20Watermarked%20Language%20Models&entry.906535625=Dara%20Bahri%20and%20John%20Wieting&entry.1292438233=%20%20Watermarking%20has%20recently%20emerged%20as%20an%20effective%20strategy%20for%20detecting%20the%0Agenerations%20of%20large%20language%20models%20%28LLMs%29.%20The%20strength%20of%20a%20watermark%0Atypically%20depends%20strongly%20on%20the%20entropy%20afforded%20by%20the%20language%20model%20and%0Athe%20set%20of%20input%20prompts.%20However%2C%20entropy%20can%20be%20quite%20limited%20in%20practice%2C%0Aespecially%20for%20models%20that%20are%20post-trained%2C%20for%20example%20via%20instruction%20tuning%0Aor%20reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%2C%20which%20makes%20detection%0Abased%20on%20watermarking%20alone%20challenging.%20In%20this%20work%2C%20we%20investigate%20whether%0Adetection%20can%20be%20improved%20by%20combining%20watermark%20detectors%20with%20non-watermark%0Aones.%20We%20explore%20a%20number%20of%20hybrid%20schemes%20that%20combine%20the%20two%2C%20observing%0Aperformance%20gains%20over%20either%20class%20of%20detector%20under%20a%20wide%20range%20of%0Aexperimental%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13131v1&entry.124074799=Read"},
{"title": "One-Class Intrusion Detection with Dynamic Graphs", "author": "Aleksei Liuliakov and Alexander Schulz and Luca Hermes and Barbara Hammer", "abstract": "  With the growing digitalization all over the globe, the relevance of network\nsecurity becomes increasingly important. Machine learning-based intrusion\ndetection constitutes a promising approach for improving security, but it bears\nseveral challenges. These include the requirement to detect novel and unseen\nnetwork events, as well as specific data properties, such as events over time\ntogether with the inherent graph structure of network communication. In this\nwork, we propose a novel intrusion detection method, TGN-SVDD, which builds\nupon modern dynamic graph modelling and deep anomaly detection. We demonstrate\nits superiority over several baselines for realistic intrusion detection data\nand suggest a more challenging variant of the latter.\n", "link": "http://arxiv.org/abs/2508.12885v1", "date": "2025-08-18", "relevancy": 1.9463, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4994}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4793}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Class%20Intrusion%20Detection%20with%20Dynamic%20Graphs&body=Title%3A%20One-Class%20Intrusion%20Detection%20with%20Dynamic%20Graphs%0AAuthor%3A%20Aleksei%20Liuliakov%20and%20Alexander%20Schulz%20and%20Luca%20Hermes%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20With%20the%20growing%20digitalization%20all%20over%20the%20globe%2C%20the%20relevance%20of%20network%0Asecurity%20becomes%20increasingly%20important.%20Machine%20learning-based%20intrusion%0Adetection%20constitutes%20a%20promising%20approach%20for%20improving%20security%2C%20but%20it%20bears%0Aseveral%20challenges.%20These%20include%20the%20requirement%20to%20detect%20novel%20and%20unseen%0Anetwork%20events%2C%20as%20well%20as%20specific%20data%20properties%2C%20such%20as%20events%20over%20time%0Atogether%20with%20the%20inherent%20graph%20structure%20of%20network%20communication.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20intrusion%20detection%20method%2C%20TGN-SVDD%2C%20which%20builds%0Aupon%20modern%20dynamic%20graph%20modelling%20and%20deep%20anomaly%20detection.%20We%20demonstrate%0Aits%20superiority%20over%20several%20baselines%20for%20realistic%20intrusion%20detection%20data%0Aand%20suggest%20a%20more%20challenging%20variant%20of%20the%20latter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Class%2520Intrusion%2520Detection%2520with%2520Dynamic%2520Graphs%26entry.906535625%3DAleksei%2520Liuliakov%2520and%2520Alexander%2520Schulz%2520and%2520Luca%2520Hermes%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520digitalization%2520all%2520over%2520the%2520globe%252C%2520the%2520relevance%2520of%2520network%250Asecurity%2520becomes%2520increasingly%2520important.%2520Machine%2520learning-based%2520intrusion%250Adetection%2520constitutes%2520a%2520promising%2520approach%2520for%2520improving%2520security%252C%2520but%2520it%2520bears%250Aseveral%2520challenges.%2520These%2520include%2520the%2520requirement%2520to%2520detect%2520novel%2520and%2520unseen%250Anetwork%2520events%252C%2520as%2520well%2520as%2520specific%2520data%2520properties%252C%2520such%2520as%2520events%2520over%2520time%250Atogether%2520with%2520the%2520inherent%2520graph%2520structure%2520of%2520network%2520communication.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520novel%2520intrusion%2520detection%2520method%252C%2520TGN-SVDD%252C%2520which%2520builds%250Aupon%2520modern%2520dynamic%2520graph%2520modelling%2520and%2520deep%2520anomaly%2520detection.%2520We%2520demonstrate%250Aits%2520superiority%2520over%2520several%2520baselines%2520for%2520realistic%2520intrusion%2520detection%2520data%250Aand%2520suggest%2520a%2520more%2520challenging%2520variant%2520of%2520the%2520latter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Class%20Intrusion%20Detection%20with%20Dynamic%20Graphs&entry.906535625=Aleksei%20Liuliakov%20and%20Alexander%20Schulz%20and%20Luca%20Hermes%20and%20Barbara%20Hammer&entry.1292438233=%20%20With%20the%20growing%20digitalization%20all%20over%20the%20globe%2C%20the%20relevance%20of%20network%0Asecurity%20becomes%20increasingly%20important.%20Machine%20learning-based%20intrusion%0Adetection%20constitutes%20a%20promising%20approach%20for%20improving%20security%2C%20but%20it%20bears%0Aseveral%20challenges.%20These%20include%20the%20requirement%20to%20detect%20novel%20and%20unseen%0Anetwork%20events%2C%20as%20well%20as%20specific%20data%20properties%2C%20such%20as%20events%20over%20time%0Atogether%20with%20the%20inherent%20graph%20structure%20of%20network%20communication.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20intrusion%20detection%20method%2C%20TGN-SVDD%2C%20which%20builds%0Aupon%20modern%20dynamic%20graph%20modelling%20and%20deep%20anomaly%20detection.%20We%20demonstrate%0Aits%20superiority%20over%20several%20baselines%20for%20realistic%20intrusion%20detection%20data%0Aand%20suggest%20a%20more%20challenging%20variant%20of%20the%20latter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12885v1&entry.124074799=Read"},
{"title": "Towards Open-Ended Emotional Support Conversations in LLMs via\n  Reinforcement Learning with Future-Oriented Rewards", "author": "Ting Yang and Li Chen and Huimin Wang", "abstract": "  Emotional Support Conversation (ESC) systems aim to alleviate users'\nemotional difficulties and provide long-term, systematic support for emotional\nwell-being. However, most large language model (LLM)-based ESC systems rely on\npredefined strategies, which limits their effectiveness in complex, real-life\nscenarios. To enable flexible responses to diverse emotional problem scenarios,\nthis paper introduces a novel end-to-end framework (RLFF-ESC) that directly\nlearns enduring emotionally supportive response skills using reinforcement\nlearning. For sustained emotional support, we first employ an LLM-based\nmulti-agent mechanism to simulate future dialogue trajectories and collect\nfuture-oriented rewards. We then train a future-oriented reward model, which is\nsubsequently used to train the emotional support policy model. Additionally, we\nincorporate an explicit reasoning process during response generation to further\nenhance the quality, relevance, and contextual appropriateness of the system's\nresponses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and\nLLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two\npublic ESC datasets. Experimental results demonstrate that RLFF-ESC\nconsistently outperforms existing baselines in terms of goal completion and\nresponse quality.\n", "link": "http://arxiv.org/abs/2508.12935v1", "date": "2025-08-18", "relevancy": 1.943, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Open-Ended%20Emotional%20Support%20Conversations%20in%20LLMs%20via%0A%20%20Reinforcement%20Learning%20with%20Future-Oriented%20Rewards&body=Title%3A%20Towards%20Open-Ended%20Emotional%20Support%20Conversations%20in%20LLMs%20via%0A%20%20Reinforcement%20Learning%20with%20Future-Oriented%20Rewards%0AAuthor%3A%20Ting%20Yang%20and%20Li%20Chen%20and%20Huimin%20Wang%0AAbstract%3A%20%20%20Emotional%20Support%20Conversation%20%28ESC%29%20systems%20aim%20to%20alleviate%20users%27%0Aemotional%20difficulties%20and%20provide%20long-term%2C%20systematic%20support%20for%20emotional%0Awell-being.%20However%2C%20most%20large%20language%20model%20%28LLM%29-based%20ESC%20systems%20rely%20on%0Apredefined%20strategies%2C%20which%20limits%20their%20effectiveness%20in%20complex%2C%20real-life%0Ascenarios.%20To%20enable%20flexible%20responses%20to%20diverse%20emotional%20problem%20scenarios%2C%0Athis%20paper%20introduces%20a%20novel%20end-to-end%20framework%20%28RLFF-ESC%29%20that%20directly%0Alearns%20enduring%20emotionally%20supportive%20response%20skills%20using%20reinforcement%0Alearning.%20For%20sustained%20emotional%20support%2C%20we%20first%20employ%20an%20LLM-based%0Amulti-agent%20mechanism%20to%20simulate%20future%20dialogue%20trajectories%20and%20collect%0Afuture-oriented%20rewards.%20We%20then%20train%20a%20future-oriented%20reward%20model%2C%20which%20is%0Asubsequently%20used%20to%20train%20the%20emotional%20support%20policy%20model.%20Additionally%2C%20we%0Aincorporate%20an%20explicit%20reasoning%20process%20during%20response%20generation%20to%20further%0Aenhance%20the%20quality%2C%20relevance%2C%20and%20contextual%20appropriateness%20of%20the%20system%27s%0Aresponses.%20We%20evaluate%20the%20backbone%20policy%20model%20on%20Qwen2.5-7B-Instruct-1M%20and%0ALLaMA3.1-8B-Instruct%20models%2C%20testing%20the%20proposed%20RLFF-ESC%20framework%20across%20two%0Apublic%20ESC%20datasets.%20Experimental%20results%20demonstrate%20that%20RLFF-ESC%0Aconsistently%20outperforms%20existing%20baselines%20in%20terms%20of%20goal%20completion%20and%0Aresponse%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Open-Ended%2520Emotional%2520Support%2520Conversations%2520in%2520LLMs%2520via%250A%2520%2520Reinforcement%2520Learning%2520with%2520Future-Oriented%2520Rewards%26entry.906535625%3DTing%2520Yang%2520and%2520Li%2520Chen%2520and%2520Huimin%2520Wang%26entry.1292438233%3D%2520%2520Emotional%2520Support%2520Conversation%2520%2528ESC%2529%2520systems%2520aim%2520to%2520alleviate%2520users%2527%250Aemotional%2520difficulties%2520and%2520provide%2520long-term%252C%2520systematic%2520support%2520for%2520emotional%250Awell-being.%2520However%252C%2520most%2520large%2520language%2520model%2520%2528LLM%2529-based%2520ESC%2520systems%2520rely%2520on%250Apredefined%2520strategies%252C%2520which%2520limits%2520their%2520effectiveness%2520in%2520complex%252C%2520real-life%250Ascenarios.%2520To%2520enable%2520flexible%2520responses%2520to%2520diverse%2520emotional%2520problem%2520scenarios%252C%250Athis%2520paper%2520introduces%2520a%2520novel%2520end-to-end%2520framework%2520%2528RLFF-ESC%2529%2520that%2520directly%250Alearns%2520enduring%2520emotionally%2520supportive%2520response%2520skills%2520using%2520reinforcement%250Alearning.%2520For%2520sustained%2520emotional%2520support%252C%2520we%2520first%2520employ%2520an%2520LLM-based%250Amulti-agent%2520mechanism%2520to%2520simulate%2520future%2520dialogue%2520trajectories%2520and%2520collect%250Afuture-oriented%2520rewards.%2520We%2520then%2520train%2520a%2520future-oriented%2520reward%2520model%252C%2520which%2520is%250Asubsequently%2520used%2520to%2520train%2520the%2520emotional%2520support%2520policy%2520model.%2520Additionally%252C%2520we%250Aincorporate%2520an%2520explicit%2520reasoning%2520process%2520during%2520response%2520generation%2520to%2520further%250Aenhance%2520the%2520quality%252C%2520relevance%252C%2520and%2520contextual%2520appropriateness%2520of%2520the%2520system%2527s%250Aresponses.%2520We%2520evaluate%2520the%2520backbone%2520policy%2520model%2520on%2520Qwen2.5-7B-Instruct-1M%2520and%250ALLaMA3.1-8B-Instruct%2520models%252C%2520testing%2520the%2520proposed%2520RLFF-ESC%2520framework%2520across%2520two%250Apublic%2520ESC%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%2520RLFF-ESC%250Aconsistently%2520outperforms%2520existing%2520baselines%2520in%2520terms%2520of%2520goal%2520completion%2520and%250Aresponse%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Open-Ended%20Emotional%20Support%20Conversations%20in%20LLMs%20via%0A%20%20Reinforcement%20Learning%20with%20Future-Oriented%20Rewards&entry.906535625=Ting%20Yang%20and%20Li%20Chen%20and%20Huimin%20Wang&entry.1292438233=%20%20Emotional%20Support%20Conversation%20%28ESC%29%20systems%20aim%20to%20alleviate%20users%27%0Aemotional%20difficulties%20and%20provide%20long-term%2C%20systematic%20support%20for%20emotional%0Awell-being.%20However%2C%20most%20large%20language%20model%20%28LLM%29-based%20ESC%20systems%20rely%20on%0Apredefined%20strategies%2C%20which%20limits%20their%20effectiveness%20in%20complex%2C%20real-life%0Ascenarios.%20To%20enable%20flexible%20responses%20to%20diverse%20emotional%20problem%20scenarios%2C%0Athis%20paper%20introduces%20a%20novel%20end-to-end%20framework%20%28RLFF-ESC%29%20that%20directly%0Alearns%20enduring%20emotionally%20supportive%20response%20skills%20using%20reinforcement%0Alearning.%20For%20sustained%20emotional%20support%2C%20we%20first%20employ%20an%20LLM-based%0Amulti-agent%20mechanism%20to%20simulate%20future%20dialogue%20trajectories%20and%20collect%0Afuture-oriented%20rewards.%20We%20then%20train%20a%20future-oriented%20reward%20model%2C%20which%20is%0Asubsequently%20used%20to%20train%20the%20emotional%20support%20policy%20model.%20Additionally%2C%20we%0Aincorporate%20an%20explicit%20reasoning%20process%20during%20response%20generation%20to%20further%0Aenhance%20the%20quality%2C%20relevance%2C%20and%20contextual%20appropriateness%20of%20the%20system%27s%0Aresponses.%20We%20evaluate%20the%20backbone%20policy%20model%20on%20Qwen2.5-7B-Instruct-1M%20and%0ALLaMA3.1-8B-Instruct%20models%2C%20testing%20the%20proposed%20RLFF-ESC%20framework%20across%20two%0Apublic%20ESC%20datasets.%20Experimental%20results%20demonstrate%20that%20RLFF-ESC%0Aconsistently%20outperforms%20existing%20baselines%20in%20terms%20of%20goal%20completion%20and%0Aresponse%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12935v1&entry.124074799=Read"},
{"title": "An MRP Formulation for Supervised Learning: Generalized Temporal\n  Difference Learning Models", "author": "Yangchen Pan and Junfeng Wen and Chenjun Xiao and Philip Torr", "abstract": "  In traditional statistical learning, data points are usually assumed to be\nindependently and identically distributed (i.i.d.) following an unknown\nprobability distribution. This paper presents a contrasting viewpoint,\nperceiving data points as interconnected and employing a Markov reward process\n(MRP) for data modeling. We reformulate the typical supervised learning as an\non-policy policy evaluation problem within reinforcement learning (RL),\nintroducing a generalized temporal difference (TD) learning algorithm as a\nresolution. Theoretically, our analysis establishes connections between the\nsolutions of linear TD learning and ordinary least squares (OLS). Under\nspecific conditions -- particularly when the noise is correlated -- the TD\nsolution serves as a more effective estimator than OLS. Furthermore, we show\nthat when our algorithm is applied with many commonly used loss functions --\nsuch as those found in generalized linear models -- it corresponds to the\napplication of a novel and generalized Bellman operator. We prove that this\noperator admits a unique fixed point, and based on this, we establish\nconvergence guarantees for our generalized TD algorithm under linear function\napproximation. Empirical studies verify our theoretical results, examine the\nvital design of our TD algorithm and show practical utility across various\ndatasets, encompassing tasks such as regression and image classification with\ndeep learning.\n", "link": "http://arxiv.org/abs/2404.15518v4", "date": "2025-08-18", "relevancy": 1.9414, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4827}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20MRP%20Formulation%20for%20Supervised%20Learning%3A%20Generalized%20Temporal%0A%20%20Difference%20Learning%20Models&body=Title%3A%20An%20MRP%20Formulation%20for%20Supervised%20Learning%3A%20Generalized%20Temporal%0A%20%20Difference%20Learning%20Models%0AAuthor%3A%20Yangchen%20Pan%20and%20Junfeng%20Wen%20and%20Chenjun%20Xiao%20and%20Philip%20Torr%0AAbstract%3A%20%20%20In%20traditional%20statistical%20learning%2C%20data%20points%20are%20usually%20assumed%20to%20be%0Aindependently%20and%20identically%20distributed%20%28i.i.d.%29%20following%20an%20unknown%0Aprobability%20distribution.%20This%20paper%20presents%20a%20contrasting%20viewpoint%2C%0Aperceiving%20data%20points%20as%20interconnected%20and%20employing%20a%20Markov%20reward%20process%0A%28MRP%29%20for%20data%20modeling.%20We%20reformulate%20the%20typical%20supervised%20learning%20as%20an%0Aon-policy%20policy%20evaluation%20problem%20within%20reinforcement%20learning%20%28RL%29%2C%0Aintroducing%20a%20generalized%20temporal%20difference%20%28TD%29%20learning%20algorithm%20as%20a%0Aresolution.%20Theoretically%2C%20our%20analysis%20establishes%20connections%20between%20the%0Asolutions%20of%20linear%20TD%20learning%20and%20ordinary%20least%20squares%20%28OLS%29.%20Under%0Aspecific%20conditions%20--%20particularly%20when%20the%20noise%20is%20correlated%20--%20the%20TD%0Asolution%20serves%20as%20a%20more%20effective%20estimator%20than%20OLS.%20Furthermore%2C%20we%20show%0Athat%20when%20our%20algorithm%20is%20applied%20with%20many%20commonly%20used%20loss%20functions%20--%0Asuch%20as%20those%20found%20in%20generalized%20linear%20models%20--%20it%20corresponds%20to%20the%0Aapplication%20of%20a%20novel%20and%20generalized%20Bellman%20operator.%20We%20prove%20that%20this%0Aoperator%20admits%20a%20unique%20fixed%20point%2C%20and%20based%20on%20this%2C%20we%20establish%0Aconvergence%20guarantees%20for%20our%20generalized%20TD%20algorithm%20under%20linear%20function%0Aapproximation.%20Empirical%20studies%20verify%20our%20theoretical%20results%2C%20examine%20the%0Avital%20design%20of%20our%20TD%20algorithm%20and%20show%20practical%20utility%20across%20various%0Adatasets%2C%20encompassing%20tasks%20such%20as%20regression%20and%20image%20classification%20with%0Adeep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15518v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520MRP%2520Formulation%2520for%2520Supervised%2520Learning%253A%2520Generalized%2520Temporal%250A%2520%2520Difference%2520Learning%2520Models%26entry.906535625%3DYangchen%2520Pan%2520and%2520Junfeng%2520Wen%2520and%2520Chenjun%2520Xiao%2520and%2520Philip%2520Torr%26entry.1292438233%3D%2520%2520In%2520traditional%2520statistical%2520learning%252C%2520data%2520points%2520are%2520usually%2520assumed%2520to%2520be%250Aindependently%2520and%2520identically%2520distributed%2520%2528i.i.d.%2529%2520following%2520an%2520unknown%250Aprobability%2520distribution.%2520This%2520paper%2520presents%2520a%2520contrasting%2520viewpoint%252C%250Aperceiving%2520data%2520points%2520as%2520interconnected%2520and%2520employing%2520a%2520Markov%2520reward%2520process%250A%2528MRP%2529%2520for%2520data%2520modeling.%2520We%2520reformulate%2520the%2520typical%2520supervised%2520learning%2520as%2520an%250Aon-policy%2520policy%2520evaluation%2520problem%2520within%2520reinforcement%2520learning%2520%2528RL%2529%252C%250Aintroducing%2520a%2520generalized%2520temporal%2520difference%2520%2528TD%2529%2520learning%2520algorithm%2520as%2520a%250Aresolution.%2520Theoretically%252C%2520our%2520analysis%2520establishes%2520connections%2520between%2520the%250Asolutions%2520of%2520linear%2520TD%2520learning%2520and%2520ordinary%2520least%2520squares%2520%2528OLS%2529.%2520Under%250Aspecific%2520conditions%2520--%2520particularly%2520when%2520the%2520noise%2520is%2520correlated%2520--%2520the%2520TD%250Asolution%2520serves%2520as%2520a%2520more%2520effective%2520estimator%2520than%2520OLS.%2520Furthermore%252C%2520we%2520show%250Athat%2520when%2520our%2520algorithm%2520is%2520applied%2520with%2520many%2520commonly%2520used%2520loss%2520functions%2520--%250Asuch%2520as%2520those%2520found%2520in%2520generalized%2520linear%2520models%2520--%2520it%2520corresponds%2520to%2520the%250Aapplication%2520of%2520a%2520novel%2520and%2520generalized%2520Bellman%2520operator.%2520We%2520prove%2520that%2520this%250Aoperator%2520admits%2520a%2520unique%2520fixed%2520point%252C%2520and%2520based%2520on%2520this%252C%2520we%2520establish%250Aconvergence%2520guarantees%2520for%2520our%2520generalized%2520TD%2520algorithm%2520under%2520linear%2520function%250Aapproximation.%2520Empirical%2520studies%2520verify%2520our%2520theoretical%2520results%252C%2520examine%2520the%250Avital%2520design%2520of%2520our%2520TD%2520algorithm%2520and%2520show%2520practical%2520utility%2520across%2520various%250Adatasets%252C%2520encompassing%2520tasks%2520such%2520as%2520regression%2520and%2520image%2520classification%2520with%250Adeep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15518v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20MRP%20Formulation%20for%20Supervised%20Learning%3A%20Generalized%20Temporal%0A%20%20Difference%20Learning%20Models&entry.906535625=Yangchen%20Pan%20and%20Junfeng%20Wen%20and%20Chenjun%20Xiao%20and%20Philip%20Torr&entry.1292438233=%20%20In%20traditional%20statistical%20learning%2C%20data%20points%20are%20usually%20assumed%20to%20be%0Aindependently%20and%20identically%20distributed%20%28i.i.d.%29%20following%20an%20unknown%0Aprobability%20distribution.%20This%20paper%20presents%20a%20contrasting%20viewpoint%2C%0Aperceiving%20data%20points%20as%20interconnected%20and%20employing%20a%20Markov%20reward%20process%0A%28MRP%29%20for%20data%20modeling.%20We%20reformulate%20the%20typical%20supervised%20learning%20as%20an%0Aon-policy%20policy%20evaluation%20problem%20within%20reinforcement%20learning%20%28RL%29%2C%0Aintroducing%20a%20generalized%20temporal%20difference%20%28TD%29%20learning%20algorithm%20as%20a%0Aresolution.%20Theoretically%2C%20our%20analysis%20establishes%20connections%20between%20the%0Asolutions%20of%20linear%20TD%20learning%20and%20ordinary%20least%20squares%20%28OLS%29.%20Under%0Aspecific%20conditions%20--%20particularly%20when%20the%20noise%20is%20correlated%20--%20the%20TD%0Asolution%20serves%20as%20a%20more%20effective%20estimator%20than%20OLS.%20Furthermore%2C%20we%20show%0Athat%20when%20our%20algorithm%20is%20applied%20with%20many%20commonly%20used%20loss%20functions%20--%0Asuch%20as%20those%20found%20in%20generalized%20linear%20models%20--%20it%20corresponds%20to%20the%0Aapplication%20of%20a%20novel%20and%20generalized%20Bellman%20operator.%20We%20prove%20that%20this%0Aoperator%20admits%20a%20unique%20fixed%20point%2C%20and%20based%20on%20this%2C%20we%20establish%0Aconvergence%20guarantees%20for%20our%20generalized%20TD%20algorithm%20under%20linear%20function%0Aapproximation.%20Empirical%20studies%20verify%20our%20theoretical%20results%2C%20examine%20the%0Avital%20design%20of%20our%20TD%20algorithm%20and%20show%20practical%20utility%20across%20various%0Adatasets%2C%20encompassing%20tasks%20such%20as%20regression%20and%20image%20classification%20with%0Adeep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15518v4&entry.124074799=Read"},
{"title": "Leveraging Diffusion Models for Stylization using Multiple Style Images", "author": "Dan Ruta and Abdelaziz Djelouah and Raphael Ortiz and Christopher Schroers", "abstract": "  Recent advances in latent diffusion models have enabled exciting progress in\nimage style transfer. However, several key issues remain. For example, existing\nmethods still struggle to accurately match styles. They are often limited in\nthe number of style images that can be used. Furthermore, they tend to entangle\ncontent and style in undesired ways. To address this, we propose leveraging\nmultiple style images which helps better represent style features and prevent\ncontent leaking from the style images. We design a method that leverages both\nimage prompt adapters and statistical alignment of the features during the\ndenoising process. With this, our approach is designed such that it can\nintervene both at the cross-attention and the self-attention layers of the\ndenoising UNet. For the statistical alignment, we employ clustering to distill\na small representative set of attention features from the large number of\nattention values extracted from the style samples. As demonstrated in our\nexperimental section, the resulting method achieves state-of-the-art results\nfor stylization.\n", "link": "http://arxiv.org/abs/2508.12784v1", "date": "2025-08-18", "relevancy": 1.2546, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6754}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6155}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Diffusion%20Models%20for%20Stylization%20using%20Multiple%20Style%20Images&body=Title%3A%20Leveraging%20Diffusion%20Models%20for%20Stylization%20using%20Multiple%20Style%20Images%0AAuthor%3A%20Dan%20Ruta%20and%20Abdelaziz%20Djelouah%20and%20Raphael%20Ortiz%20and%20Christopher%20Schroers%0AAbstract%3A%20%20%20Recent%20advances%20in%20latent%20diffusion%20models%20have%20enabled%20exciting%20progress%20in%0Aimage%20style%20transfer.%20However%2C%20several%20key%20issues%20remain.%20For%20example%2C%20existing%0Amethods%20still%20struggle%20to%20accurately%20match%20styles.%20They%20are%20often%20limited%20in%0Athe%20number%20of%20style%20images%20that%20can%20be%20used.%20Furthermore%2C%20they%20tend%20to%20entangle%0Acontent%20and%20style%20in%20undesired%20ways.%20To%20address%20this%2C%20we%20propose%20leveraging%0Amultiple%20style%20images%20which%20helps%20better%20represent%20style%20features%20and%20prevent%0Acontent%20leaking%20from%20the%20style%20images.%20We%20design%20a%20method%20that%20leverages%20both%0Aimage%20prompt%20adapters%20and%20statistical%20alignment%20of%20the%20features%20during%20the%0Adenoising%20process.%20With%20this%2C%20our%20approach%20is%20designed%20such%20that%20it%20can%0Aintervene%20both%20at%20the%20cross-attention%20and%20the%20self-attention%20layers%20of%20the%0Adenoising%20UNet.%20For%20the%20statistical%20alignment%2C%20we%20employ%20clustering%20to%20distill%0Aa%20small%20representative%20set%20of%20attention%20features%20from%20the%20large%20number%20of%0Aattention%20values%20extracted%20from%20the%20style%20samples.%20As%20demonstrated%20in%20our%0Aexperimental%20section%2C%20the%20resulting%20method%20achieves%20state-of-the-art%20results%0Afor%20stylization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Diffusion%2520Models%2520for%2520Stylization%2520using%2520Multiple%2520Style%2520Images%26entry.906535625%3DDan%2520Ruta%2520and%2520Abdelaziz%2520Djelouah%2520and%2520Raphael%2520Ortiz%2520and%2520Christopher%2520Schroers%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520latent%2520diffusion%2520models%2520have%2520enabled%2520exciting%2520progress%2520in%250Aimage%2520style%2520transfer.%2520However%252C%2520several%2520key%2520issues%2520remain.%2520For%2520example%252C%2520existing%250Amethods%2520still%2520struggle%2520to%2520accurately%2520match%2520styles.%2520They%2520are%2520often%2520limited%2520in%250Athe%2520number%2520of%2520style%2520images%2520that%2520can%2520be%2520used.%2520Furthermore%252C%2520they%2520tend%2520to%2520entangle%250Acontent%2520and%2520style%2520in%2520undesired%2520ways.%2520To%2520address%2520this%252C%2520we%2520propose%2520leveraging%250Amultiple%2520style%2520images%2520which%2520helps%2520better%2520represent%2520style%2520features%2520and%2520prevent%250Acontent%2520leaking%2520from%2520the%2520style%2520images.%2520We%2520design%2520a%2520method%2520that%2520leverages%2520both%250Aimage%2520prompt%2520adapters%2520and%2520statistical%2520alignment%2520of%2520the%2520features%2520during%2520the%250Adenoising%2520process.%2520With%2520this%252C%2520our%2520approach%2520is%2520designed%2520such%2520that%2520it%2520can%250Aintervene%2520both%2520at%2520the%2520cross-attention%2520and%2520the%2520self-attention%2520layers%2520of%2520the%250Adenoising%2520UNet.%2520For%2520the%2520statistical%2520alignment%252C%2520we%2520employ%2520clustering%2520to%2520distill%250Aa%2520small%2520representative%2520set%2520of%2520attention%2520features%2520from%2520the%2520large%2520number%2520of%250Aattention%2520values%2520extracted%2520from%2520the%2520style%2520samples.%2520As%2520demonstrated%2520in%2520our%250Aexperimental%2520section%252C%2520the%2520resulting%2520method%2520achieves%2520state-of-the-art%2520results%250Afor%2520stylization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Diffusion%20Models%20for%20Stylization%20using%20Multiple%20Style%20Images&entry.906535625=Dan%20Ruta%20and%20Abdelaziz%20Djelouah%20and%20Raphael%20Ortiz%20and%20Christopher%20Schroers&entry.1292438233=%20%20Recent%20advances%20in%20latent%20diffusion%20models%20have%20enabled%20exciting%20progress%20in%0Aimage%20style%20transfer.%20However%2C%20several%20key%20issues%20remain.%20For%20example%2C%20existing%0Amethods%20still%20struggle%20to%20accurately%20match%20styles.%20They%20are%20often%20limited%20in%0Athe%20number%20of%20style%20images%20that%20can%20be%20used.%20Furthermore%2C%20they%20tend%20to%20entangle%0Acontent%20and%20style%20in%20undesired%20ways.%20To%20address%20this%2C%20we%20propose%20leveraging%0Amultiple%20style%20images%20which%20helps%20better%20represent%20style%20features%20and%20prevent%0Acontent%20leaking%20from%20the%20style%20images.%20We%20design%20a%20method%20that%20leverages%20both%0Aimage%20prompt%20adapters%20and%20statistical%20alignment%20of%20the%20features%20during%20the%0Adenoising%20process.%20With%20this%2C%20our%20approach%20is%20designed%20such%20that%20it%20can%0Aintervene%20both%20at%20the%20cross-attention%20and%20the%20self-attention%20layers%20of%20the%0Adenoising%20UNet.%20For%20the%20statistical%20alignment%2C%20we%20employ%20clustering%20to%20distill%0Aa%20small%20representative%20set%20of%20attention%20features%20from%20the%20large%20number%20of%0Aattention%20values%20extracted%20from%20the%20style%20samples.%20As%20demonstrated%20in%20our%0Aexperimental%20section%2C%20the%20resulting%20method%20achieves%20state-of-the-art%20results%0Afor%20stylization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12784v1&entry.124074799=Read"},
{"title": "Exploring Autonomous Agents: A Closer Look at Why They Fail When\n  Completing Tasks", "author": "Ruofan Lu and Yichen Li and Yintong Huo", "abstract": "  Autonomous agent systems powered by Large Language Models (LLMs) have\ndemonstrated promising capabilities in automating complex tasks. However,\ncurrent evaluations largely rely on success rates without systematically\nanalyzing the interactions, communication mechanisms, and failure causes within\nthese systems. To bridge this gap, we present a benchmark of 34 representative\nprogrammable tasks designed to rigorously assess autonomous agents. Using this\nbenchmark, we evaluate three popular open-source agent frameworks combined with\ntwo LLM backbones, observing a task completion rate of approximately 50%.\nThrough in-depth failure analysis, we develop a three-tier taxonomy of failure\ncauses aligned with task phases, highlighting planning errors, task execution\nissues, and incorrect response generation. Based on these insights, we propose\nactionable improvements to enhance agent planning and self-diagnosis\ncapabilities. Our failure taxonomy, together with mitigation advice, provides\nan empirical foundation for developing more robust and effective autonomous\nagent systems in the future.\n", "link": "http://arxiv.org/abs/2508.13143v1", "date": "2025-08-18", "relevancy": 1.5138, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5267}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5062}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Autonomous%20Agents%3A%20A%20Closer%20Look%20at%20Why%20They%20Fail%20When%0A%20%20Completing%20Tasks&body=Title%3A%20Exploring%20Autonomous%20Agents%3A%20A%20Closer%20Look%20at%20Why%20They%20Fail%20When%0A%20%20Completing%20Tasks%0AAuthor%3A%20Ruofan%20Lu%20and%20Yichen%20Li%20and%20Yintong%20Huo%0AAbstract%3A%20%20%20Autonomous%20agent%20systems%20powered%20by%20Large%20Language%20Models%20%28LLMs%29%20have%0Ademonstrated%20promising%20capabilities%20in%20automating%20complex%20tasks.%20However%2C%0Acurrent%20evaluations%20largely%20rely%20on%20success%20rates%20without%20systematically%0Aanalyzing%20the%20interactions%2C%20communication%20mechanisms%2C%20and%20failure%20causes%20within%0Athese%20systems.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20benchmark%20of%2034%20representative%0Aprogrammable%20tasks%20designed%20to%20rigorously%20assess%20autonomous%20agents.%20Using%20this%0Abenchmark%2C%20we%20evaluate%20three%20popular%20open-source%20agent%20frameworks%20combined%20with%0Atwo%20LLM%20backbones%2C%20observing%20a%20task%20completion%20rate%20of%20approximately%2050%25.%0AThrough%20in-depth%20failure%20analysis%2C%20we%20develop%20a%20three-tier%20taxonomy%20of%20failure%0Acauses%20aligned%20with%20task%20phases%2C%20highlighting%20planning%20errors%2C%20task%20execution%0Aissues%2C%20and%20incorrect%20response%20generation.%20Based%20on%20these%20insights%2C%20we%20propose%0Aactionable%20improvements%20to%20enhance%20agent%20planning%20and%20self-diagnosis%0Acapabilities.%20Our%20failure%20taxonomy%2C%20together%20with%20mitigation%20advice%2C%20provides%0Aan%20empirical%20foundation%20for%20developing%20more%20robust%20and%20effective%20autonomous%0Aagent%20systems%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Autonomous%2520Agents%253A%2520A%2520Closer%2520Look%2520at%2520Why%2520They%2520Fail%2520When%250A%2520%2520Completing%2520Tasks%26entry.906535625%3DRuofan%2520Lu%2520and%2520Yichen%2520Li%2520and%2520Yintong%2520Huo%26entry.1292438233%3D%2520%2520Autonomous%2520agent%2520systems%2520powered%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Ademonstrated%2520promising%2520capabilities%2520in%2520automating%2520complex%2520tasks.%2520However%252C%250Acurrent%2520evaluations%2520largely%2520rely%2520on%2520success%2520rates%2520without%2520systematically%250Aanalyzing%2520the%2520interactions%252C%2520communication%2520mechanisms%252C%2520and%2520failure%2520causes%2520within%250Athese%2520systems.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520a%2520benchmark%2520of%252034%2520representative%250Aprogrammable%2520tasks%2520designed%2520to%2520rigorously%2520assess%2520autonomous%2520agents.%2520Using%2520this%250Abenchmark%252C%2520we%2520evaluate%2520three%2520popular%2520open-source%2520agent%2520frameworks%2520combined%2520with%250Atwo%2520LLM%2520backbones%252C%2520observing%2520a%2520task%2520completion%2520rate%2520of%2520approximately%252050%2525.%250AThrough%2520in-depth%2520failure%2520analysis%252C%2520we%2520develop%2520a%2520three-tier%2520taxonomy%2520of%2520failure%250Acauses%2520aligned%2520with%2520task%2520phases%252C%2520highlighting%2520planning%2520errors%252C%2520task%2520execution%250Aissues%252C%2520and%2520incorrect%2520response%2520generation.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%250Aactionable%2520improvements%2520to%2520enhance%2520agent%2520planning%2520and%2520self-diagnosis%250Acapabilities.%2520Our%2520failure%2520taxonomy%252C%2520together%2520with%2520mitigation%2520advice%252C%2520provides%250Aan%2520empirical%2520foundation%2520for%2520developing%2520more%2520robust%2520and%2520effective%2520autonomous%250Aagent%2520systems%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Autonomous%20Agents%3A%20A%20Closer%20Look%20at%20Why%20They%20Fail%20When%0A%20%20Completing%20Tasks&entry.906535625=Ruofan%20Lu%20and%20Yichen%20Li%20and%20Yintong%20Huo&entry.1292438233=%20%20Autonomous%20agent%20systems%20powered%20by%20Large%20Language%20Models%20%28LLMs%29%20have%0Ademonstrated%20promising%20capabilities%20in%20automating%20complex%20tasks.%20However%2C%0Acurrent%20evaluations%20largely%20rely%20on%20success%20rates%20without%20systematically%0Aanalyzing%20the%20interactions%2C%20communication%20mechanisms%2C%20and%20failure%20causes%20within%0Athese%20systems.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20benchmark%20of%2034%20representative%0Aprogrammable%20tasks%20designed%20to%20rigorously%20assess%20autonomous%20agents.%20Using%20this%0Abenchmark%2C%20we%20evaluate%20three%20popular%20open-source%20agent%20frameworks%20combined%20with%0Atwo%20LLM%20backbones%2C%20observing%20a%20task%20completion%20rate%20of%20approximately%2050%25.%0AThrough%20in-depth%20failure%20analysis%2C%20we%20develop%20a%20three-tier%20taxonomy%20of%20failure%0Acauses%20aligned%20with%20task%20phases%2C%20highlighting%20planning%20errors%2C%20task%20execution%0Aissues%2C%20and%20incorrect%20response%20generation.%20Based%20on%20these%20insights%2C%20we%20propose%0Aactionable%20improvements%20to%20enhance%20agent%20planning%20and%20self-diagnosis%0Acapabilities.%20Our%20failure%20taxonomy%2C%20together%20with%20mitigation%20advice%2C%20provides%0Aan%20empirical%20foundation%20for%20developing%20more%20robust%20and%20effective%20autonomous%0Aagent%20systems%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13143v1&entry.124074799=Read"},
{"title": "RoboRetriever: Single-Camera Robot Object Retrieval via Active and\n  Interactive Perception with Dynamic Scene Graph", "author": "Hecheng Wang and Jiankun Ren and Jia Yu and Lizhe Qi and Yunquan Sun", "abstract": "  Humans effortlessly retrieve objects in cluttered, partially observable\nenvironments by combining visual reasoning, active viewpoint adjustment, and\nphysical interaction-with only a single pair of eyes. In contrast, most\nexisting robotic systems rely on carefully positioned fixed or multi-camera\nsetups with complete scene visibility, which limits adaptability and incurs\nhigh hardware costs. We present \\textbf{RoboRetriever}, a novel framework for\nreal-world object retrieval that operates using only a \\textbf{single}\nwrist-mounted RGB-D camera and free-form natural language instructions.\nRoboRetriever grounds visual observations to build and update a \\textbf{dynamic\nhierarchical scene graph} that encodes object semantics, geometry, and\ninter-object relations over time. The supervisor module reasons over this\nmemory and task instruction to infer the target object and coordinate an\nintegrated action module combining \\textbf{active perception},\n\\textbf{interactive perception}, and \\textbf{manipulation}. To enable\ntask-aware scene-grounded active perception, we introduce a novel visual\nprompting scheme that leverages large reasoning vision-language models to\ndetermine 6-DoF camera poses aligned with the semantic task goal and geometry\nscene context. We evaluate RoboRetriever on diverse real-world object retrieval\ntasks, including scenarios with human intervention, demonstrating strong\nadaptability and robustness in cluttered scenes with only one RGB-D camera.\n", "link": "http://arxiv.org/abs/2508.12916v1", "date": "2025-08-18", "relevancy": 1.837, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6906}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6045}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboRetriever%3A%20Single-Camera%20Robot%20Object%20Retrieval%20via%20Active%20and%0A%20%20Interactive%20Perception%20with%20Dynamic%20Scene%20Graph&body=Title%3A%20RoboRetriever%3A%20Single-Camera%20Robot%20Object%20Retrieval%20via%20Active%20and%0A%20%20Interactive%20Perception%20with%20Dynamic%20Scene%20Graph%0AAuthor%3A%20Hecheng%20Wang%20and%20Jiankun%20Ren%20and%20Jia%20Yu%20and%20Lizhe%20Qi%20and%20Yunquan%20Sun%0AAbstract%3A%20%20%20Humans%20effortlessly%20retrieve%20objects%20in%20cluttered%2C%20partially%20observable%0Aenvironments%20by%20combining%20visual%20reasoning%2C%20active%20viewpoint%20adjustment%2C%20and%0Aphysical%20interaction-with%20only%20a%20single%20pair%20of%20eyes.%20In%20contrast%2C%20most%0Aexisting%20robotic%20systems%20rely%20on%20carefully%20positioned%20fixed%20or%20multi-camera%0Asetups%20with%20complete%20scene%20visibility%2C%20which%20limits%20adaptability%20and%20incurs%0Ahigh%20hardware%20costs.%20We%20present%20%5Ctextbf%7BRoboRetriever%7D%2C%20a%20novel%20framework%20for%0Areal-world%20object%20retrieval%20that%20operates%20using%20only%20a%20%5Ctextbf%7Bsingle%7D%0Awrist-mounted%20RGB-D%20camera%20and%20free-form%20natural%20language%20instructions.%0ARoboRetriever%20grounds%20visual%20observations%20to%20build%20and%20update%20a%20%5Ctextbf%7Bdynamic%0Ahierarchical%20scene%20graph%7D%20that%20encodes%20object%20semantics%2C%20geometry%2C%20and%0Ainter-object%20relations%20over%20time.%20The%20supervisor%20module%20reasons%20over%20this%0Amemory%20and%20task%20instruction%20to%20infer%20the%20target%20object%20and%20coordinate%20an%0Aintegrated%20action%20module%20combining%20%5Ctextbf%7Bactive%20perception%7D%2C%0A%5Ctextbf%7Binteractive%20perception%7D%2C%20and%20%5Ctextbf%7Bmanipulation%7D.%20To%20enable%0Atask-aware%20scene-grounded%20active%20perception%2C%20we%20introduce%20a%20novel%20visual%0Aprompting%20scheme%20that%20leverages%20large%20reasoning%20vision-language%20models%20to%0Adetermine%206-DoF%20camera%20poses%20aligned%20with%20the%20semantic%20task%20goal%20and%20geometry%0Ascene%20context.%20We%20evaluate%20RoboRetriever%20on%20diverse%20real-world%20object%20retrieval%0Atasks%2C%20including%20scenarios%20with%20human%20intervention%2C%20demonstrating%20strong%0Aadaptability%20and%20robustness%20in%20cluttered%20scenes%20with%20only%20one%20RGB-D%20camera.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboRetriever%253A%2520Single-Camera%2520Robot%2520Object%2520Retrieval%2520via%2520Active%2520and%250A%2520%2520Interactive%2520Perception%2520with%2520Dynamic%2520Scene%2520Graph%26entry.906535625%3DHecheng%2520Wang%2520and%2520Jiankun%2520Ren%2520and%2520Jia%2520Yu%2520and%2520Lizhe%2520Qi%2520and%2520Yunquan%2520Sun%26entry.1292438233%3D%2520%2520Humans%2520effortlessly%2520retrieve%2520objects%2520in%2520cluttered%252C%2520partially%2520observable%250Aenvironments%2520by%2520combining%2520visual%2520reasoning%252C%2520active%2520viewpoint%2520adjustment%252C%2520and%250Aphysical%2520interaction-with%2520only%2520a%2520single%2520pair%2520of%2520eyes.%2520In%2520contrast%252C%2520most%250Aexisting%2520robotic%2520systems%2520rely%2520on%2520carefully%2520positioned%2520fixed%2520or%2520multi-camera%250Asetups%2520with%2520complete%2520scene%2520visibility%252C%2520which%2520limits%2520adaptability%2520and%2520incurs%250Ahigh%2520hardware%2520costs.%2520We%2520present%2520%255Ctextbf%257BRoboRetriever%257D%252C%2520a%2520novel%2520framework%2520for%250Areal-world%2520object%2520retrieval%2520that%2520operates%2520using%2520only%2520a%2520%255Ctextbf%257Bsingle%257D%250Awrist-mounted%2520RGB-D%2520camera%2520and%2520free-form%2520natural%2520language%2520instructions.%250ARoboRetriever%2520grounds%2520visual%2520observations%2520to%2520build%2520and%2520update%2520a%2520%255Ctextbf%257Bdynamic%250Ahierarchical%2520scene%2520graph%257D%2520that%2520encodes%2520object%2520semantics%252C%2520geometry%252C%2520and%250Ainter-object%2520relations%2520over%2520time.%2520The%2520supervisor%2520module%2520reasons%2520over%2520this%250Amemory%2520and%2520task%2520instruction%2520to%2520infer%2520the%2520target%2520object%2520and%2520coordinate%2520an%250Aintegrated%2520action%2520module%2520combining%2520%255Ctextbf%257Bactive%2520perception%257D%252C%250A%255Ctextbf%257Binteractive%2520perception%257D%252C%2520and%2520%255Ctextbf%257Bmanipulation%257D.%2520To%2520enable%250Atask-aware%2520scene-grounded%2520active%2520perception%252C%2520we%2520introduce%2520a%2520novel%2520visual%250Aprompting%2520scheme%2520that%2520leverages%2520large%2520reasoning%2520vision-language%2520models%2520to%250Adetermine%25206-DoF%2520camera%2520poses%2520aligned%2520with%2520the%2520semantic%2520task%2520goal%2520and%2520geometry%250Ascene%2520context.%2520We%2520evaluate%2520RoboRetriever%2520on%2520diverse%2520real-world%2520object%2520retrieval%250Atasks%252C%2520including%2520scenarios%2520with%2520human%2520intervention%252C%2520demonstrating%2520strong%250Aadaptability%2520and%2520robustness%2520in%2520cluttered%2520scenes%2520with%2520only%2520one%2520RGB-D%2520camera.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboRetriever%3A%20Single-Camera%20Robot%20Object%20Retrieval%20via%20Active%20and%0A%20%20Interactive%20Perception%20with%20Dynamic%20Scene%20Graph&entry.906535625=Hecheng%20Wang%20and%20Jiankun%20Ren%20and%20Jia%20Yu%20and%20Lizhe%20Qi%20and%20Yunquan%20Sun&entry.1292438233=%20%20Humans%20effortlessly%20retrieve%20objects%20in%20cluttered%2C%20partially%20observable%0Aenvironments%20by%20combining%20visual%20reasoning%2C%20active%20viewpoint%20adjustment%2C%20and%0Aphysical%20interaction-with%20only%20a%20single%20pair%20of%20eyes.%20In%20contrast%2C%20most%0Aexisting%20robotic%20systems%20rely%20on%20carefully%20positioned%20fixed%20or%20multi-camera%0Asetups%20with%20complete%20scene%20visibility%2C%20which%20limits%20adaptability%20and%20incurs%0Ahigh%20hardware%20costs.%20We%20present%20%5Ctextbf%7BRoboRetriever%7D%2C%20a%20novel%20framework%20for%0Areal-world%20object%20retrieval%20that%20operates%20using%20only%20a%20%5Ctextbf%7Bsingle%7D%0Awrist-mounted%20RGB-D%20camera%20and%20free-form%20natural%20language%20instructions.%0ARoboRetriever%20grounds%20visual%20observations%20to%20build%20and%20update%20a%20%5Ctextbf%7Bdynamic%0Ahierarchical%20scene%20graph%7D%20that%20encodes%20object%20semantics%2C%20geometry%2C%20and%0Ainter-object%20relations%20over%20time.%20The%20supervisor%20module%20reasons%20over%20this%0Amemory%20and%20task%20instruction%20to%20infer%20the%20target%20object%20and%20coordinate%20an%0Aintegrated%20action%20module%20combining%20%5Ctextbf%7Bactive%20perception%7D%2C%0A%5Ctextbf%7Binteractive%20perception%7D%2C%20and%20%5Ctextbf%7Bmanipulation%7D.%20To%20enable%0Atask-aware%20scene-grounded%20active%20perception%2C%20we%20introduce%20a%20novel%20visual%0Aprompting%20scheme%20that%20leverages%20large%20reasoning%20vision-language%20models%20to%0Adetermine%206-DoF%20camera%20poses%20aligned%20with%20the%20semantic%20task%20goal%20and%20geometry%0Ascene%20context.%20We%20evaluate%20RoboRetriever%20on%20diverse%20real-world%20object%20retrieval%0Atasks%2C%20including%20scenarios%20with%20human%20intervention%2C%20demonstrating%20strong%0Aadaptability%20and%20robustness%20in%20cluttered%20scenes%20with%20only%20one%20RGB-D%20camera.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12916v1&entry.124074799=Read"},
{"title": "Hyperspectral Image Generation with Unmixing Guided Diffusion Model", "author": "Shiyu Shen and Bin Pan and Ziye Zhang and Zhenwei Shi", "abstract": "  Recently, hyperspectral image generation has received increasing attention,\nbut existing generative models rely on conditional generation schemes, which\nlimits the diversity of generated images. Diffusion models are popular for\ntheir ability to generate high-quality samples, but adapting these models from\nRGB to hyperspectral data presents the challenge of high dimensionality and\nphysical constraints. To address these challenges, we propose a novel diffusion\nmodel guided by hyperspectral unmixing. Our model comprises two key modules: an\nunmixing autoencoder module and an abundance diffusion module. The unmixing\nautoencoder module leverages unmixing guidance to shift the generative task\nfrom the image space to the low-dimensional abundance space, significantly\nreducing computational complexity while preserving high fidelity. The abundance\ndiffusion module generates samples that satisfy the constraints of\nnon-negativity and unity, ensuring the physical consistency of the\nreconstructed HSIs. Additionally, we introduce two evaluation metrics tailored\nto hyperspectral data. Empirical results, evaluated using both traditional\nmetrics and our proposed metrics, indicate that our model is capable of\ngenerating high-quality and diverse hyperspectral images, offering an\nadvancement in hyperspectral data generation.\n", "link": "http://arxiv.org/abs/2506.02601v2", "date": "2025-08-18", "relevancy": 1.2156, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6311}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6061}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20Image%20Generation%20with%20Unmixing%20Guided%20Diffusion%20Model&body=Title%3A%20Hyperspectral%20Image%20Generation%20with%20Unmixing%20Guided%20Diffusion%20Model%0AAuthor%3A%20Shiyu%20Shen%20and%20Bin%20Pan%20and%20Ziye%20Zhang%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20Recently%2C%20hyperspectral%20image%20generation%20has%20received%20increasing%20attention%2C%0Abut%20existing%20generative%20models%20rely%20on%20conditional%20generation%20schemes%2C%20which%0Alimits%20the%20diversity%20of%20generated%20images.%20Diffusion%20models%20are%20popular%20for%0Atheir%20ability%20to%20generate%20high-quality%20samples%2C%20but%20adapting%20these%20models%20from%0ARGB%20to%20hyperspectral%20data%20presents%20the%20challenge%20of%20high%20dimensionality%20and%0Aphysical%20constraints.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20diffusion%0Amodel%20guided%20by%20hyperspectral%20unmixing.%20Our%20model%20comprises%20two%20key%20modules%3A%20an%0Aunmixing%20autoencoder%20module%20and%20an%20abundance%20diffusion%20module.%20The%20unmixing%0Aautoencoder%20module%20leverages%20unmixing%20guidance%20to%20shift%20the%20generative%20task%0Afrom%20the%20image%20space%20to%20the%20low-dimensional%20abundance%20space%2C%20significantly%0Areducing%20computational%20complexity%20while%20preserving%20high%20fidelity.%20The%20abundance%0Adiffusion%20module%20generates%20samples%20that%20satisfy%20the%20constraints%20of%0Anon-negativity%20and%20unity%2C%20ensuring%20the%20physical%20consistency%20of%20the%0Areconstructed%20HSIs.%20Additionally%2C%20we%20introduce%20two%20evaluation%20metrics%20tailored%0Ato%20hyperspectral%20data.%20Empirical%20results%2C%20evaluated%20using%20both%20traditional%0Ametrics%20and%20our%20proposed%20metrics%2C%20indicate%20that%20our%20model%20is%20capable%20of%0Agenerating%20high-quality%20and%20diverse%20hyperspectral%20images%2C%20offering%20an%0Aadvancement%20in%20hyperspectral%20data%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520Image%2520Generation%2520with%2520Unmixing%2520Guided%2520Diffusion%2520Model%26entry.906535625%3DShiyu%2520Shen%2520and%2520Bin%2520Pan%2520and%2520Ziye%2520Zhang%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3D%2520%2520Recently%252C%2520hyperspectral%2520image%2520generation%2520has%2520received%2520increasing%2520attention%252C%250Abut%2520existing%2520generative%2520models%2520rely%2520on%2520conditional%2520generation%2520schemes%252C%2520which%250Alimits%2520the%2520diversity%2520of%2520generated%2520images.%2520Diffusion%2520models%2520are%2520popular%2520for%250Atheir%2520ability%2520to%2520generate%2520high-quality%2520samples%252C%2520but%2520adapting%2520these%2520models%2520from%250ARGB%2520to%2520hyperspectral%2520data%2520presents%2520the%2520challenge%2520of%2520high%2520dimensionality%2520and%250Aphysical%2520constraints.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520diffusion%250Amodel%2520guided%2520by%2520hyperspectral%2520unmixing.%2520Our%2520model%2520comprises%2520two%2520key%2520modules%253A%2520an%250Aunmixing%2520autoencoder%2520module%2520and%2520an%2520abundance%2520diffusion%2520module.%2520The%2520unmixing%250Aautoencoder%2520module%2520leverages%2520unmixing%2520guidance%2520to%2520shift%2520the%2520generative%2520task%250Afrom%2520the%2520image%2520space%2520to%2520the%2520low-dimensional%2520abundance%2520space%252C%2520significantly%250Areducing%2520computational%2520complexity%2520while%2520preserving%2520high%2520fidelity.%2520The%2520abundance%250Adiffusion%2520module%2520generates%2520samples%2520that%2520satisfy%2520the%2520constraints%2520of%250Anon-negativity%2520and%2520unity%252C%2520ensuring%2520the%2520physical%2520consistency%2520of%2520the%250Areconstructed%2520HSIs.%2520Additionally%252C%2520we%2520introduce%2520two%2520evaluation%2520metrics%2520tailored%250Ato%2520hyperspectral%2520data.%2520Empirical%2520results%252C%2520evaluated%2520using%2520both%2520traditional%250Ametrics%2520and%2520our%2520proposed%2520metrics%252C%2520indicate%2520that%2520our%2520model%2520is%2520capable%2520of%250Agenerating%2520high-quality%2520and%2520diverse%2520hyperspectral%2520images%252C%2520offering%2520an%250Aadvancement%2520in%2520hyperspectral%2520data%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20Image%20Generation%20with%20Unmixing%20Guided%20Diffusion%20Model&entry.906535625=Shiyu%20Shen%20and%20Bin%20Pan%20and%20Ziye%20Zhang%20and%20Zhenwei%20Shi&entry.1292438233=%20%20Recently%2C%20hyperspectral%20image%20generation%20has%20received%20increasing%20attention%2C%0Abut%20existing%20generative%20models%20rely%20on%20conditional%20generation%20schemes%2C%20which%0Alimits%20the%20diversity%20of%20generated%20images.%20Diffusion%20models%20are%20popular%20for%0Atheir%20ability%20to%20generate%20high-quality%20samples%2C%20but%20adapting%20these%20models%20from%0ARGB%20to%20hyperspectral%20data%20presents%20the%20challenge%20of%20high%20dimensionality%20and%0Aphysical%20constraints.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20diffusion%0Amodel%20guided%20by%20hyperspectral%20unmixing.%20Our%20model%20comprises%20two%20key%20modules%3A%20an%0Aunmixing%20autoencoder%20module%20and%20an%20abundance%20diffusion%20module.%20The%20unmixing%0Aautoencoder%20module%20leverages%20unmixing%20guidance%20to%20shift%20the%20generative%20task%0Afrom%20the%20image%20space%20to%20the%20low-dimensional%20abundance%20space%2C%20significantly%0Areducing%20computational%20complexity%20while%20preserving%20high%20fidelity.%20The%20abundance%0Adiffusion%20module%20generates%20samples%20that%20satisfy%20the%20constraints%20of%0Anon-negativity%20and%20unity%2C%20ensuring%20the%20physical%20consistency%20of%20the%0Areconstructed%20HSIs.%20Additionally%2C%20we%20introduce%20two%20evaluation%20metrics%20tailored%0Ato%20hyperspectral%20data.%20Empirical%20results%2C%20evaluated%20using%20both%20traditional%0Ametrics%20and%20our%20proposed%20metrics%2C%20indicate%20that%20our%20model%20is%20capable%20of%0Agenerating%20high-quality%20and%20diverse%20hyperspectral%20images%2C%20offering%20an%0Aadvancement%20in%20hyperspectral%20data%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02601v2&entry.124074799=Read"},
{"title": "Causally-Guided Pairwise Transformer -- Towards Foundational Digital\n  Twins in Process Industry", "author": "Michael Mayr and Georgios C. Chasparis", "abstract": "  Foundational modelling of multi-dimensional time-series data in industrial\nsystems presents a central trade-off: channel-dependent (CD) models capture\nspecific cross-variable dynamics but lack robustness and adaptability as model\nlayers are commonly bound to the data dimensionality of the tackled use-case,\nwhile channel-independent (CI) models offer generality at the cost of modelling\nthe explicit interactions crucial for system-level predictive regression tasks.\nTo resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a\nnovel architecture that integrates a known causal graph as an inductive bias.\nThe core of CGPT is built around a pairwise modeling paradigm, tackling the\nCD/CI conflict by decomposing the multidimensional data into pairs. The model\nuses channel-agnostic learnable layers where all parameter dimensions are\nindependent of the number of variables. CGPT enforces a CD information flow at\nthe pair-level and CI-like generalization across pairs. This approach\ndisentangles complex system dynamics and results in a highly flexible\narchitecture that ensures scalability and any-variate adaptability. We validate\nCGPT on a suite of synthetic and real-world industrial datasets on long-term\nand one-step forecasting tasks designed to simulate common industrial\ncomplexities. Results demonstrate that CGPT significantly outperforms both CI\nand CD baselines in predictive accuracy and shows competitive performance with\nend-to-end trained CD models while remaining agnostic to the problem\ndimensionality.\n", "link": "http://arxiv.org/abs/2508.13111v1", "date": "2025-08-18", "relevancy": 1.5368, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5466}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5147}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causally-Guided%20Pairwise%20Transformer%20--%20Towards%20Foundational%20Digital%0A%20%20Twins%20in%20Process%20Industry&body=Title%3A%20Causally-Guided%20Pairwise%20Transformer%20--%20Towards%20Foundational%20Digital%0A%20%20Twins%20in%20Process%20Industry%0AAuthor%3A%20Michael%20Mayr%20and%20Georgios%20C.%20Chasparis%0AAbstract%3A%20%20%20Foundational%20modelling%20of%20multi-dimensional%20time-series%20data%20in%20industrial%0Asystems%20presents%20a%20central%20trade-off%3A%20channel-dependent%20%28CD%29%20models%20capture%0Aspecific%20cross-variable%20dynamics%20but%20lack%20robustness%20and%20adaptability%20as%20model%0Alayers%20are%20commonly%20bound%20to%20the%20data%20dimensionality%20of%20the%20tackled%20use-case%2C%0Awhile%20channel-independent%20%28CI%29%20models%20offer%20generality%20at%20the%20cost%20of%20modelling%0Athe%20explicit%20interactions%20crucial%20for%20system-level%20predictive%20regression%20tasks.%0ATo%20resolve%20this%2C%20we%20propose%20the%20Causally-Guided%20Pairwise%20Transformer%20%28CGPT%29%2C%20a%0Anovel%20architecture%20that%20integrates%20a%20known%20causal%20graph%20as%20an%20inductive%20bias.%0AThe%20core%20of%20CGPT%20is%20built%20around%20a%20pairwise%20modeling%20paradigm%2C%20tackling%20the%0ACD/CI%20conflict%20by%20decomposing%20the%20multidimensional%20data%20into%20pairs.%20The%20model%0Auses%20channel-agnostic%20learnable%20layers%20where%20all%20parameter%20dimensions%20are%0Aindependent%20of%20the%20number%20of%20variables.%20CGPT%20enforces%20a%20CD%20information%20flow%20at%0Athe%20pair-level%20and%20CI-like%20generalization%20across%20pairs.%20This%20approach%0Adisentangles%20complex%20system%20dynamics%20and%20results%20in%20a%20highly%20flexible%0Aarchitecture%20that%20ensures%20scalability%20and%20any-variate%20adaptability.%20We%20validate%0ACGPT%20on%20a%20suite%20of%20synthetic%20and%20real-world%20industrial%20datasets%20on%20long-term%0Aand%20one-step%20forecasting%20tasks%20designed%20to%20simulate%20common%20industrial%0Acomplexities.%20Results%20demonstrate%20that%20CGPT%20significantly%20outperforms%20both%20CI%0Aand%20CD%20baselines%20in%20predictive%20accuracy%20and%20shows%20competitive%20performance%20with%0Aend-to-end%20trained%20CD%20models%20while%20remaining%20agnostic%20to%20the%20problem%0Adimensionality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausally-Guided%2520Pairwise%2520Transformer%2520--%2520Towards%2520Foundational%2520Digital%250A%2520%2520Twins%2520in%2520Process%2520Industry%26entry.906535625%3DMichael%2520Mayr%2520and%2520Georgios%2520C.%2520Chasparis%26entry.1292438233%3D%2520%2520Foundational%2520modelling%2520of%2520multi-dimensional%2520time-series%2520data%2520in%2520industrial%250Asystems%2520presents%2520a%2520central%2520trade-off%253A%2520channel-dependent%2520%2528CD%2529%2520models%2520capture%250Aspecific%2520cross-variable%2520dynamics%2520but%2520lack%2520robustness%2520and%2520adaptability%2520as%2520model%250Alayers%2520are%2520commonly%2520bound%2520to%2520the%2520data%2520dimensionality%2520of%2520the%2520tackled%2520use-case%252C%250Awhile%2520channel-independent%2520%2528CI%2529%2520models%2520offer%2520generality%2520at%2520the%2520cost%2520of%2520modelling%250Athe%2520explicit%2520interactions%2520crucial%2520for%2520system-level%2520predictive%2520regression%2520tasks.%250ATo%2520resolve%2520this%252C%2520we%2520propose%2520the%2520Causally-Guided%2520Pairwise%2520Transformer%2520%2528CGPT%2529%252C%2520a%250Anovel%2520architecture%2520that%2520integrates%2520a%2520known%2520causal%2520graph%2520as%2520an%2520inductive%2520bias.%250AThe%2520core%2520of%2520CGPT%2520is%2520built%2520around%2520a%2520pairwise%2520modeling%2520paradigm%252C%2520tackling%2520the%250ACD/CI%2520conflict%2520by%2520decomposing%2520the%2520multidimensional%2520data%2520into%2520pairs.%2520The%2520model%250Auses%2520channel-agnostic%2520learnable%2520layers%2520where%2520all%2520parameter%2520dimensions%2520are%250Aindependent%2520of%2520the%2520number%2520of%2520variables.%2520CGPT%2520enforces%2520a%2520CD%2520information%2520flow%2520at%250Athe%2520pair-level%2520and%2520CI-like%2520generalization%2520across%2520pairs.%2520This%2520approach%250Adisentangles%2520complex%2520system%2520dynamics%2520and%2520results%2520in%2520a%2520highly%2520flexible%250Aarchitecture%2520that%2520ensures%2520scalability%2520and%2520any-variate%2520adaptability.%2520We%2520validate%250ACGPT%2520on%2520a%2520suite%2520of%2520synthetic%2520and%2520real-world%2520industrial%2520datasets%2520on%2520long-term%250Aand%2520one-step%2520forecasting%2520tasks%2520designed%2520to%2520simulate%2520common%2520industrial%250Acomplexities.%2520Results%2520demonstrate%2520that%2520CGPT%2520significantly%2520outperforms%2520both%2520CI%250Aand%2520CD%2520baselines%2520in%2520predictive%2520accuracy%2520and%2520shows%2520competitive%2520performance%2520with%250Aend-to-end%2520trained%2520CD%2520models%2520while%2520remaining%2520agnostic%2520to%2520the%2520problem%250Adimensionality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causally-Guided%20Pairwise%20Transformer%20--%20Towards%20Foundational%20Digital%0A%20%20Twins%20in%20Process%20Industry&entry.906535625=Michael%20Mayr%20and%20Georgios%20C.%20Chasparis&entry.1292438233=%20%20Foundational%20modelling%20of%20multi-dimensional%20time-series%20data%20in%20industrial%0Asystems%20presents%20a%20central%20trade-off%3A%20channel-dependent%20%28CD%29%20models%20capture%0Aspecific%20cross-variable%20dynamics%20but%20lack%20robustness%20and%20adaptability%20as%20model%0Alayers%20are%20commonly%20bound%20to%20the%20data%20dimensionality%20of%20the%20tackled%20use-case%2C%0Awhile%20channel-independent%20%28CI%29%20models%20offer%20generality%20at%20the%20cost%20of%20modelling%0Athe%20explicit%20interactions%20crucial%20for%20system-level%20predictive%20regression%20tasks.%0ATo%20resolve%20this%2C%20we%20propose%20the%20Causally-Guided%20Pairwise%20Transformer%20%28CGPT%29%2C%20a%0Anovel%20architecture%20that%20integrates%20a%20known%20causal%20graph%20as%20an%20inductive%20bias.%0AThe%20core%20of%20CGPT%20is%20built%20around%20a%20pairwise%20modeling%20paradigm%2C%20tackling%20the%0ACD/CI%20conflict%20by%20decomposing%20the%20multidimensional%20data%20into%20pairs.%20The%20model%0Auses%20channel-agnostic%20learnable%20layers%20where%20all%20parameter%20dimensions%20are%0Aindependent%20of%20the%20number%20of%20variables.%20CGPT%20enforces%20a%20CD%20information%20flow%20at%0Athe%20pair-level%20and%20CI-like%20generalization%20across%20pairs.%20This%20approach%0Adisentangles%20complex%20system%20dynamics%20and%20results%20in%20a%20highly%20flexible%0Aarchitecture%20that%20ensures%20scalability%20and%20any-variate%20adaptability.%20We%20validate%0ACGPT%20on%20a%20suite%20of%20synthetic%20and%20real-world%20industrial%20datasets%20on%20long-term%0Aand%20one-step%20forecasting%20tasks%20designed%20to%20simulate%20common%20industrial%0Acomplexities.%20Results%20demonstrate%20that%20CGPT%20significantly%20outperforms%20both%20CI%0Aand%20CD%20baselines%20in%20predictive%20accuracy%20and%20shows%20competitive%20performance%20with%0Aend-to-end%20trained%20CD%20models%20while%20remaining%20agnostic%20to%20the%20problem%0Adimensionality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13111v1&entry.124074799=Read"},
{"title": "From Transthoracic to Transesophageal: Cross-Modality Generation using\n  LoRA Diffusion", "author": "Emmanuel Oladokun and Yuxuan Ou and Anna Novikova and Daria Kulikova and Sarina Thomas and Jurica \u0160prem and Vicente Grau", "abstract": "  Deep diffusion models excel at realistic image synthesis but demand large\ntraining sets-an obstacle in data-scarce domains like transesophageal\nechocardiography (TEE). While synthetic augmentation has boosted performance in\ntransthoracic echo (TTE), TEE remains critically underrepresented, limiting the\nreach of deep learning in this high-impact modality.\n  We address this gap by adapting a TTE-trained, mask-conditioned diffusion\nbackbone to TEE with only a limited number of new cases and adapters as small\nas $10^5$ parameters. Our pipeline combines Low-Rank Adaptation with MaskR$^2$,\na lightweight remapping layer that aligns novel mask formats with the\npretrained model's conditioning channels. This design lets users adapt models\nto new datasets with a different set of anatomical structures to the base\nmodel's original set.\n  Through a targeted adaptation strategy, we find that adapting only MLP layers\nsuffices for high-fidelity TEE synthesis. Finally, mixing less than 200 real\nTEE frames with our synthetic echoes improves the dice score on a multiclass\nsegmentation task, particularly boosting performance on underrepresented\nright-heart structures. Our results demonstrate that (1) semantically\ncontrolled TEE images can be generated with low overhead, (2) MaskR$^2$\neffectively transforms unseen mask formats into compatible formats without\ndamaging downstream task performance, and (3) our method generates images that\nare effective for improving performance on a downstream task of multiclass\nsegmentation.\n", "link": "http://arxiv.org/abs/2508.13077v1", "date": "2025-08-18", "relevancy": 1.8607, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6489}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6162}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Transthoracic%20to%20Transesophageal%3A%20Cross-Modality%20Generation%20using%0A%20%20LoRA%20Diffusion&body=Title%3A%20From%20Transthoracic%20to%20Transesophageal%3A%20Cross-Modality%20Generation%20using%0A%20%20LoRA%20Diffusion%0AAuthor%3A%20Emmanuel%20Oladokun%20and%20Yuxuan%20Ou%20and%20Anna%20Novikova%20and%20Daria%20Kulikova%20and%20Sarina%20Thomas%20and%20Jurica%20%C5%A0prem%20and%20Vicente%20Grau%0AAbstract%3A%20%20%20Deep%20diffusion%20models%20excel%20at%20realistic%20image%20synthesis%20but%20demand%20large%0Atraining%20sets-an%20obstacle%20in%20data-scarce%20domains%20like%20transesophageal%0Aechocardiography%20%28TEE%29.%20While%20synthetic%20augmentation%20has%20boosted%20performance%20in%0Atransthoracic%20echo%20%28TTE%29%2C%20TEE%20remains%20critically%20underrepresented%2C%20limiting%20the%0Areach%20of%20deep%20learning%20in%20this%20high-impact%20modality.%0A%20%20We%20address%20this%20gap%20by%20adapting%20a%20TTE-trained%2C%20mask-conditioned%20diffusion%0Abackbone%20to%20TEE%20with%20only%20a%20limited%20number%20of%20new%20cases%20and%20adapters%20as%20small%0Aas%20%2410%5E5%24%20parameters.%20Our%20pipeline%20combines%20Low-Rank%20Adaptation%20with%20MaskR%24%5E2%24%2C%0Aa%20lightweight%20remapping%20layer%20that%20aligns%20novel%20mask%20formats%20with%20the%0Apretrained%20model%27s%20conditioning%20channels.%20This%20design%20lets%20users%20adapt%20models%0Ato%20new%20datasets%20with%20a%20different%20set%20of%20anatomical%20structures%20to%20the%20base%0Amodel%27s%20original%20set.%0A%20%20Through%20a%20targeted%20adaptation%20strategy%2C%20we%20find%20that%20adapting%20only%20MLP%20layers%0Asuffices%20for%20high-fidelity%20TEE%20synthesis.%20Finally%2C%20mixing%20less%20than%20200%20real%0ATEE%20frames%20with%20our%20synthetic%20echoes%20improves%20the%20dice%20score%20on%20a%20multiclass%0Asegmentation%20task%2C%20particularly%20boosting%20performance%20on%20underrepresented%0Aright-heart%20structures.%20Our%20results%20demonstrate%20that%20%281%29%20semantically%0Acontrolled%20TEE%20images%20can%20be%20generated%20with%20low%20overhead%2C%20%282%29%20MaskR%24%5E2%24%0Aeffectively%20transforms%20unseen%20mask%20formats%20into%20compatible%20formats%20without%0Adamaging%20downstream%20task%20performance%2C%20and%20%283%29%20our%20method%20generates%20images%20that%0Aare%20effective%20for%20improving%20performance%20on%20a%20downstream%20task%20of%20multiclass%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Transthoracic%2520to%2520Transesophageal%253A%2520Cross-Modality%2520Generation%2520using%250A%2520%2520LoRA%2520Diffusion%26entry.906535625%3DEmmanuel%2520Oladokun%2520and%2520Yuxuan%2520Ou%2520and%2520Anna%2520Novikova%2520and%2520Daria%2520Kulikova%2520and%2520Sarina%2520Thomas%2520and%2520Jurica%2520%25C5%25A0prem%2520and%2520Vicente%2520Grau%26entry.1292438233%3D%2520%2520Deep%2520diffusion%2520models%2520excel%2520at%2520realistic%2520image%2520synthesis%2520but%2520demand%2520large%250Atraining%2520sets-an%2520obstacle%2520in%2520data-scarce%2520domains%2520like%2520transesophageal%250Aechocardiography%2520%2528TEE%2529.%2520While%2520synthetic%2520augmentation%2520has%2520boosted%2520performance%2520in%250Atransthoracic%2520echo%2520%2528TTE%2529%252C%2520TEE%2520remains%2520critically%2520underrepresented%252C%2520limiting%2520the%250Areach%2520of%2520deep%2520learning%2520in%2520this%2520high-impact%2520modality.%250A%2520%2520We%2520address%2520this%2520gap%2520by%2520adapting%2520a%2520TTE-trained%252C%2520mask-conditioned%2520diffusion%250Abackbone%2520to%2520TEE%2520with%2520only%2520a%2520limited%2520number%2520of%2520new%2520cases%2520and%2520adapters%2520as%2520small%250Aas%2520%252410%255E5%2524%2520parameters.%2520Our%2520pipeline%2520combines%2520Low-Rank%2520Adaptation%2520with%2520MaskR%2524%255E2%2524%252C%250Aa%2520lightweight%2520remapping%2520layer%2520that%2520aligns%2520novel%2520mask%2520formats%2520with%2520the%250Apretrained%2520model%2527s%2520conditioning%2520channels.%2520This%2520design%2520lets%2520users%2520adapt%2520models%250Ato%2520new%2520datasets%2520with%2520a%2520different%2520set%2520of%2520anatomical%2520structures%2520to%2520the%2520base%250Amodel%2527s%2520original%2520set.%250A%2520%2520Through%2520a%2520targeted%2520adaptation%2520strategy%252C%2520we%2520find%2520that%2520adapting%2520only%2520MLP%2520layers%250Asuffices%2520for%2520high-fidelity%2520TEE%2520synthesis.%2520Finally%252C%2520mixing%2520less%2520than%2520200%2520real%250ATEE%2520frames%2520with%2520our%2520synthetic%2520echoes%2520improves%2520the%2520dice%2520score%2520on%2520a%2520multiclass%250Asegmentation%2520task%252C%2520particularly%2520boosting%2520performance%2520on%2520underrepresented%250Aright-heart%2520structures.%2520Our%2520results%2520demonstrate%2520that%2520%25281%2529%2520semantically%250Acontrolled%2520TEE%2520images%2520can%2520be%2520generated%2520with%2520low%2520overhead%252C%2520%25282%2529%2520MaskR%2524%255E2%2524%250Aeffectively%2520transforms%2520unseen%2520mask%2520formats%2520into%2520compatible%2520formats%2520without%250Adamaging%2520downstream%2520task%2520performance%252C%2520and%2520%25283%2529%2520our%2520method%2520generates%2520images%2520that%250Aare%2520effective%2520for%2520improving%2520performance%2520on%2520a%2520downstream%2520task%2520of%2520multiclass%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Transthoracic%20to%20Transesophageal%3A%20Cross-Modality%20Generation%20using%0A%20%20LoRA%20Diffusion&entry.906535625=Emmanuel%20Oladokun%20and%20Yuxuan%20Ou%20and%20Anna%20Novikova%20and%20Daria%20Kulikova%20and%20Sarina%20Thomas%20and%20Jurica%20%C5%A0prem%20and%20Vicente%20Grau&entry.1292438233=%20%20Deep%20diffusion%20models%20excel%20at%20realistic%20image%20synthesis%20but%20demand%20large%0Atraining%20sets-an%20obstacle%20in%20data-scarce%20domains%20like%20transesophageal%0Aechocardiography%20%28TEE%29.%20While%20synthetic%20augmentation%20has%20boosted%20performance%20in%0Atransthoracic%20echo%20%28TTE%29%2C%20TEE%20remains%20critically%20underrepresented%2C%20limiting%20the%0Areach%20of%20deep%20learning%20in%20this%20high-impact%20modality.%0A%20%20We%20address%20this%20gap%20by%20adapting%20a%20TTE-trained%2C%20mask-conditioned%20diffusion%0Abackbone%20to%20TEE%20with%20only%20a%20limited%20number%20of%20new%20cases%20and%20adapters%20as%20small%0Aas%20%2410%5E5%24%20parameters.%20Our%20pipeline%20combines%20Low-Rank%20Adaptation%20with%20MaskR%24%5E2%24%2C%0Aa%20lightweight%20remapping%20layer%20that%20aligns%20novel%20mask%20formats%20with%20the%0Apretrained%20model%27s%20conditioning%20channels.%20This%20design%20lets%20users%20adapt%20models%0Ato%20new%20datasets%20with%20a%20different%20set%20of%20anatomical%20structures%20to%20the%20base%0Amodel%27s%20original%20set.%0A%20%20Through%20a%20targeted%20adaptation%20strategy%2C%20we%20find%20that%20adapting%20only%20MLP%20layers%0Asuffices%20for%20high-fidelity%20TEE%20synthesis.%20Finally%2C%20mixing%20less%20than%20200%20real%0ATEE%20frames%20with%20our%20synthetic%20echoes%20improves%20the%20dice%20score%20on%20a%20multiclass%0Asegmentation%20task%2C%20particularly%20boosting%20performance%20on%20underrepresented%0Aright-heart%20structures.%20Our%20results%20demonstrate%20that%20%281%29%20semantically%0Acontrolled%20TEE%20images%20can%20be%20generated%20with%20low%20overhead%2C%20%282%29%20MaskR%24%5E2%24%0Aeffectively%20transforms%20unseen%20mask%20formats%20into%20compatible%20formats%20without%0Adamaging%20downstream%20task%20performance%2C%20and%20%283%29%20our%20method%20generates%20images%20that%0Aare%20effective%20for%20improving%20performance%20on%20a%20downstream%20task%20of%20multiclass%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13077v1&entry.124074799=Read"},
{"title": "STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning", "author": "Marius Memmel and Jacob Berg and Bingqing Chen and Abhishek Gupta and Jonathan Francis", "abstract": "  Robot learning is witnessing a significant increase in the size, diversity,\nand complexity of pre-collected datasets, mirroring trends in domains such as\nnatural language processing and computer vision. Many robot learning methods\ntreat such datasets as multi-task expert data and learn a multi-task,\ngeneralist policy by training broadly across them. Notably, while these\ngeneralist policies can improve the average performance across many tasks, the\nperformance of generalist policies on any one task is often suboptimal due to\nnegative transfer between partitions of the data, compared to task-specific\nspecialist policies. In this work, we argue for the paradigm of training\npolicies during deployment given the scenarios they encounter: rather than\ndeploying pre-trained policies to unseen problems in a zero-shot manner, we\nnon-parametrically retrieve and train models directly on relevant data at test\ntime. Furthermore, we show that many robotics tasks share considerable amounts\nof low-level behaviors and that retrieval at the \"sub\"-trajectory granularity\nenables significantly improved data utilization, generalization, and robustness\nin adapting policies to novel problems. In contrast, existing full-trajectory\nretrieval methods tend to underutilize the data and miss out on shared\ncross-task content. This work proposes STRAP, a technique for leveraging\npre-trained vision foundation models and dynamic time warping to retrieve\nsub-sequences of trajectories from large training corpora in a robust fashion.\nSTRAP outperforms both prior retrieval algorithms and multi-task learning\nmethods in simulated and real experiments, showing the ability to scale to much\nlarger offline datasets in the real world as well as the ability to learn\nrobust control policies with just a handful of real-world demonstrations.\n", "link": "http://arxiv.org/abs/2412.15182v2", "date": "2025-08-18", "relevancy": 1.6838, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5941}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5825}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STRAP%3A%20Robot%20Sub-Trajectory%20Retrieval%20for%20Augmented%20Policy%20Learning&body=Title%3A%20STRAP%3A%20Robot%20Sub-Trajectory%20Retrieval%20for%20Augmented%20Policy%20Learning%0AAuthor%3A%20Marius%20Memmel%20and%20Jacob%20Berg%20and%20Bingqing%20Chen%20and%20Abhishek%20Gupta%20and%20Jonathan%20Francis%0AAbstract%3A%20%20%20Robot%20learning%20is%20witnessing%20a%20significant%20increase%20in%20the%20size%2C%20diversity%2C%0Aand%20complexity%20of%20pre-collected%20datasets%2C%20mirroring%20trends%20in%20domains%20such%20as%0Anatural%20language%20processing%20and%20computer%20vision.%20Many%20robot%20learning%20methods%0Atreat%20such%20datasets%20as%20multi-task%20expert%20data%20and%20learn%20a%20multi-task%2C%0Ageneralist%20policy%20by%20training%20broadly%20across%20them.%20Notably%2C%20while%20these%0Ageneralist%20policies%20can%20improve%20the%20average%20performance%20across%20many%20tasks%2C%20the%0Aperformance%20of%20generalist%20policies%20on%20any%20one%20task%20is%20often%20suboptimal%20due%20to%0Anegative%20transfer%20between%20partitions%20of%20the%20data%2C%20compared%20to%20task-specific%0Aspecialist%20policies.%20In%20this%20work%2C%20we%20argue%20for%20the%20paradigm%20of%20training%0Apolicies%20during%20deployment%20given%20the%20scenarios%20they%20encounter%3A%20rather%20than%0Adeploying%20pre-trained%20policies%20to%20unseen%20problems%20in%20a%20zero-shot%20manner%2C%20we%0Anon-parametrically%20retrieve%20and%20train%20models%20directly%20on%20relevant%20data%20at%20test%0Atime.%20Furthermore%2C%20we%20show%20that%20many%20robotics%20tasks%20share%20considerable%20amounts%0Aof%20low-level%20behaviors%20and%20that%20retrieval%20at%20the%20%22sub%22-trajectory%20granularity%0Aenables%20significantly%20improved%20data%20utilization%2C%20generalization%2C%20and%20robustness%0Ain%20adapting%20policies%20to%20novel%20problems.%20In%20contrast%2C%20existing%20full-trajectory%0Aretrieval%20methods%20tend%20to%20underutilize%20the%20data%20and%20miss%20out%20on%20shared%0Across-task%20content.%20This%20work%20proposes%20STRAP%2C%20a%20technique%20for%20leveraging%0Apre-trained%20vision%20foundation%20models%20and%20dynamic%20time%20warping%20to%20retrieve%0Asub-sequences%20of%20trajectories%20from%20large%20training%20corpora%20in%20a%20robust%20fashion.%0ASTRAP%20outperforms%20both%20prior%20retrieval%20algorithms%20and%20multi-task%20learning%0Amethods%20in%20simulated%20and%20real%20experiments%2C%20showing%20the%20ability%20to%20scale%20to%20much%0Alarger%20offline%20datasets%20in%20the%20real%20world%20as%20well%20as%20the%20ability%20to%20learn%0Arobust%20control%20policies%20with%20just%20a%20handful%20of%20real-world%20demonstrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTRAP%253A%2520Robot%2520Sub-Trajectory%2520Retrieval%2520for%2520Augmented%2520Policy%2520Learning%26entry.906535625%3DMarius%2520Memmel%2520and%2520Jacob%2520Berg%2520and%2520Bingqing%2520Chen%2520and%2520Abhishek%2520Gupta%2520and%2520Jonathan%2520Francis%26entry.1292438233%3D%2520%2520Robot%2520learning%2520is%2520witnessing%2520a%2520significant%2520increase%2520in%2520the%2520size%252C%2520diversity%252C%250Aand%2520complexity%2520of%2520pre-collected%2520datasets%252C%2520mirroring%2520trends%2520in%2520domains%2520such%2520as%250Anatural%2520language%2520processing%2520and%2520computer%2520vision.%2520Many%2520robot%2520learning%2520methods%250Atreat%2520such%2520datasets%2520as%2520multi-task%2520expert%2520data%2520and%2520learn%2520a%2520multi-task%252C%250Ageneralist%2520policy%2520by%2520training%2520broadly%2520across%2520them.%2520Notably%252C%2520while%2520these%250Ageneralist%2520policies%2520can%2520improve%2520the%2520average%2520performance%2520across%2520many%2520tasks%252C%2520the%250Aperformance%2520of%2520generalist%2520policies%2520on%2520any%2520one%2520task%2520is%2520often%2520suboptimal%2520due%2520to%250Anegative%2520transfer%2520between%2520partitions%2520of%2520the%2520data%252C%2520compared%2520to%2520task-specific%250Aspecialist%2520policies.%2520In%2520this%2520work%252C%2520we%2520argue%2520for%2520the%2520paradigm%2520of%2520training%250Apolicies%2520during%2520deployment%2520given%2520the%2520scenarios%2520they%2520encounter%253A%2520rather%2520than%250Adeploying%2520pre-trained%2520policies%2520to%2520unseen%2520problems%2520in%2520a%2520zero-shot%2520manner%252C%2520we%250Anon-parametrically%2520retrieve%2520and%2520train%2520models%2520directly%2520on%2520relevant%2520data%2520at%2520test%250Atime.%2520Furthermore%252C%2520we%2520show%2520that%2520many%2520robotics%2520tasks%2520share%2520considerable%2520amounts%250Aof%2520low-level%2520behaviors%2520and%2520that%2520retrieval%2520at%2520the%2520%2522sub%2522-trajectory%2520granularity%250Aenables%2520significantly%2520improved%2520data%2520utilization%252C%2520generalization%252C%2520and%2520robustness%250Ain%2520adapting%2520policies%2520to%2520novel%2520problems.%2520In%2520contrast%252C%2520existing%2520full-trajectory%250Aretrieval%2520methods%2520tend%2520to%2520underutilize%2520the%2520data%2520and%2520miss%2520out%2520on%2520shared%250Across-task%2520content.%2520This%2520work%2520proposes%2520STRAP%252C%2520a%2520technique%2520for%2520leveraging%250Apre-trained%2520vision%2520foundation%2520models%2520and%2520dynamic%2520time%2520warping%2520to%2520retrieve%250Asub-sequences%2520of%2520trajectories%2520from%2520large%2520training%2520corpora%2520in%2520a%2520robust%2520fashion.%250ASTRAP%2520outperforms%2520both%2520prior%2520retrieval%2520algorithms%2520and%2520multi-task%2520learning%250Amethods%2520in%2520simulated%2520and%2520real%2520experiments%252C%2520showing%2520the%2520ability%2520to%2520scale%2520to%2520much%250Alarger%2520offline%2520datasets%2520in%2520the%2520real%2520world%2520as%2520well%2520as%2520the%2520ability%2520to%2520learn%250Arobust%2520control%2520policies%2520with%2520just%2520a%2520handful%2520of%2520real-world%2520demonstrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STRAP%3A%20Robot%20Sub-Trajectory%20Retrieval%20for%20Augmented%20Policy%20Learning&entry.906535625=Marius%20Memmel%20and%20Jacob%20Berg%20and%20Bingqing%20Chen%20and%20Abhishek%20Gupta%20and%20Jonathan%20Francis&entry.1292438233=%20%20Robot%20learning%20is%20witnessing%20a%20significant%20increase%20in%20the%20size%2C%20diversity%2C%0Aand%20complexity%20of%20pre-collected%20datasets%2C%20mirroring%20trends%20in%20domains%20such%20as%0Anatural%20language%20processing%20and%20computer%20vision.%20Many%20robot%20learning%20methods%0Atreat%20such%20datasets%20as%20multi-task%20expert%20data%20and%20learn%20a%20multi-task%2C%0Ageneralist%20policy%20by%20training%20broadly%20across%20them.%20Notably%2C%20while%20these%0Ageneralist%20policies%20can%20improve%20the%20average%20performance%20across%20many%20tasks%2C%20the%0Aperformance%20of%20generalist%20policies%20on%20any%20one%20task%20is%20often%20suboptimal%20due%20to%0Anegative%20transfer%20between%20partitions%20of%20the%20data%2C%20compared%20to%20task-specific%0Aspecialist%20policies.%20In%20this%20work%2C%20we%20argue%20for%20the%20paradigm%20of%20training%0Apolicies%20during%20deployment%20given%20the%20scenarios%20they%20encounter%3A%20rather%20than%0Adeploying%20pre-trained%20policies%20to%20unseen%20problems%20in%20a%20zero-shot%20manner%2C%20we%0Anon-parametrically%20retrieve%20and%20train%20models%20directly%20on%20relevant%20data%20at%20test%0Atime.%20Furthermore%2C%20we%20show%20that%20many%20robotics%20tasks%20share%20considerable%20amounts%0Aof%20low-level%20behaviors%20and%20that%20retrieval%20at%20the%20%22sub%22-trajectory%20granularity%0Aenables%20significantly%20improved%20data%20utilization%2C%20generalization%2C%20and%20robustness%0Ain%20adapting%20policies%20to%20novel%20problems.%20In%20contrast%2C%20existing%20full-trajectory%0Aretrieval%20methods%20tend%20to%20underutilize%20the%20data%20and%20miss%20out%20on%20shared%0Across-task%20content.%20This%20work%20proposes%20STRAP%2C%20a%20technique%20for%20leveraging%0Apre-trained%20vision%20foundation%20models%20and%20dynamic%20time%20warping%20to%20retrieve%0Asub-sequences%20of%20trajectories%20from%20large%20training%20corpora%20in%20a%20robust%20fashion.%0ASTRAP%20outperforms%20both%20prior%20retrieval%20algorithms%20and%20multi-task%20learning%0Amethods%20in%20simulated%20and%20real%20experiments%2C%20showing%20the%20ability%20to%20scale%20to%20much%0Alarger%20offline%20datasets%20in%20the%20real%20world%20as%20well%20as%20the%20ability%20to%20learn%0Arobust%20control%20policies%20with%20just%20a%20handful%20of%20real-world%20demonstrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15182v2&entry.124074799=Read"},
{"title": "Improving Text Style Transfer using Masked Diffusion Language Models\n  with Inference-time Scaling", "author": "Tejomay Kishor Padole and Suyash P Awate and Pushpak Bhattacharyya", "abstract": "  Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature.\n", "link": "http://arxiv.org/abs/2508.10995v2", "date": "2025-08-18", "relevancy": 1.7919, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.635}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5903}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Text%20Style%20Transfer%20using%20Masked%20Diffusion%20Language%20Models%0A%20%20with%20Inference-time%20Scaling&body=Title%3A%20Improving%20Text%20Style%20Transfer%20using%20Masked%20Diffusion%20Language%20Models%0A%20%20with%20Inference-time%20Scaling%0AAuthor%3A%20Tejomay%20Kishor%20Padole%20and%20Suyash%20P%20Awate%20and%20Pushpak%20Bhattacharyya%0AAbstract%3A%20%20%20Masked%20diffusion%20language%20models%20%28MDMs%29%20have%20recently%20gained%20traction%20as%20a%0Aviable%20generative%20framework%20for%20natural%20language.%20This%20can%20be%20attributed%20to%20its%0Ascalability%20and%20ease%20of%20training%20compared%20to%20other%20diffusion%20model%20paradigms%0Afor%20discrete%20data%2C%20establishing%20itself%20as%20the%20state-of-the-art%0Anon-autoregressive%20generator%20for%20discrete%20data.%20Diffusion%20models%2C%20in%20general%2C%0Ahave%20shown%20excellent%20ability%20to%20improve%20the%20generation%20quality%20by%20leveraging%0Ainference-time%20scaling%20either%20by%20increasing%20the%20number%20of%20denoising%20steps%20or%20by%0Ausing%20external%20verifiers%20on%20top%20of%20the%20outputs%20of%20each%20step%20to%20guide%20the%0Ageneration.%20In%20this%20work%2C%20we%20propose%20a%20verifier-based%20inference-time%20scaling%0Amethod%20that%20aids%20in%20finding%20a%20better%20candidate%20generation%20during%20the%20denoising%0Aprocess%20of%20the%20MDM.%20Our%20experiments%20demonstrate%20the%20application%20of%20MDMs%20for%0Astandard%20text-style%20transfer%20tasks%20and%20establish%20MDMs%20as%20a%20better%20alternative%0Ato%20autoregressive%20language%20models.%20Additionally%2C%20we%20show%20that%20a%20simple%0Asoft-value-based%20verifier%20setup%20for%20MDMs%20using%20off-the-shelf%20pre-trained%0Aembedding%20models%20leads%20to%20significant%20gains%20in%20generation%20quality%20even%20when%0Aused%20on%20top%20of%20typical%20classifier-free%20guidance%20setups%20in%20the%20existing%0Aliterature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10995v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Text%2520Style%2520Transfer%2520using%2520Masked%2520Diffusion%2520Language%2520Models%250A%2520%2520with%2520Inference-time%2520Scaling%26entry.906535625%3DTejomay%2520Kishor%2520Padole%2520and%2520Suyash%2520P%2520Awate%2520and%2520Pushpak%2520Bhattacharyya%26entry.1292438233%3D%2520%2520Masked%2520diffusion%2520language%2520models%2520%2528MDMs%2529%2520have%2520recently%2520gained%2520traction%2520as%2520a%250Aviable%2520generative%2520framework%2520for%2520natural%2520language.%2520This%2520can%2520be%2520attributed%2520to%2520its%250Ascalability%2520and%2520ease%2520of%2520training%2520compared%2520to%2520other%2520diffusion%2520model%2520paradigms%250Afor%2520discrete%2520data%252C%2520establishing%2520itself%2520as%2520the%2520state-of-the-art%250Anon-autoregressive%2520generator%2520for%2520discrete%2520data.%2520Diffusion%2520models%252C%2520in%2520general%252C%250Ahave%2520shown%2520excellent%2520ability%2520to%2520improve%2520the%2520generation%2520quality%2520by%2520leveraging%250Ainference-time%2520scaling%2520either%2520by%2520increasing%2520the%2520number%2520of%2520denoising%2520steps%2520or%2520by%250Ausing%2520external%2520verifiers%2520on%2520top%2520of%2520the%2520outputs%2520of%2520each%2520step%2520to%2520guide%2520the%250Ageneration.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520verifier-based%2520inference-time%2520scaling%250Amethod%2520that%2520aids%2520in%2520finding%2520a%2520better%2520candidate%2520generation%2520during%2520the%2520denoising%250Aprocess%2520of%2520the%2520MDM.%2520Our%2520experiments%2520demonstrate%2520the%2520application%2520of%2520MDMs%2520for%250Astandard%2520text-style%2520transfer%2520tasks%2520and%2520establish%2520MDMs%2520as%2520a%2520better%2520alternative%250Ato%2520autoregressive%2520language%2520models.%2520Additionally%252C%2520we%2520show%2520that%2520a%2520simple%250Asoft-value-based%2520verifier%2520setup%2520for%2520MDMs%2520using%2520off-the-shelf%2520pre-trained%250Aembedding%2520models%2520leads%2520to%2520significant%2520gains%2520in%2520generation%2520quality%2520even%2520when%250Aused%2520on%2520top%2520of%2520typical%2520classifier-free%2520guidance%2520setups%2520in%2520the%2520existing%250Aliterature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10995v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Text%20Style%20Transfer%20using%20Masked%20Diffusion%20Language%20Models%0A%20%20with%20Inference-time%20Scaling&entry.906535625=Tejomay%20Kishor%20Padole%20and%20Suyash%20P%20Awate%20and%20Pushpak%20Bhattacharyya&entry.1292438233=%20%20Masked%20diffusion%20language%20models%20%28MDMs%29%20have%20recently%20gained%20traction%20as%20a%0Aviable%20generative%20framework%20for%20natural%20language.%20This%20can%20be%20attributed%20to%20its%0Ascalability%20and%20ease%20of%20training%20compared%20to%20other%20diffusion%20model%20paradigms%0Afor%20discrete%20data%2C%20establishing%20itself%20as%20the%20state-of-the-art%0Anon-autoregressive%20generator%20for%20discrete%20data.%20Diffusion%20models%2C%20in%20general%2C%0Ahave%20shown%20excellent%20ability%20to%20improve%20the%20generation%20quality%20by%20leveraging%0Ainference-time%20scaling%20either%20by%20increasing%20the%20number%20of%20denoising%20steps%20or%20by%0Ausing%20external%20verifiers%20on%20top%20of%20the%20outputs%20of%20each%20step%20to%20guide%20the%0Ageneration.%20In%20this%20work%2C%20we%20propose%20a%20verifier-based%20inference-time%20scaling%0Amethod%20that%20aids%20in%20finding%20a%20better%20candidate%20generation%20during%20the%20denoising%0Aprocess%20of%20the%20MDM.%20Our%20experiments%20demonstrate%20the%20application%20of%20MDMs%20for%0Astandard%20text-style%20transfer%20tasks%20and%20establish%20MDMs%20as%20a%20better%20alternative%0Ato%20autoregressive%20language%20models.%20Additionally%2C%20we%20show%20that%20a%20simple%0Asoft-value-based%20verifier%20setup%20for%20MDMs%20using%20off-the-shelf%20pre-trained%0Aembedding%20models%20leads%20to%20significant%20gains%20in%20generation%20quality%20even%20when%0Aused%20on%20top%20of%20typical%20classifier-free%20guidance%20setups%20in%20the%20existing%0Aliterature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10995v2&entry.124074799=Read"},
{"title": "Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion", "author": "Victor Dh\u00e9din and Haizhou Zhao and Majid Khadiv", "abstract": "  Legged robots have the potential to traverse highly constrained environments\nwith agile maneuvers. However, planning such motions requires solving a highly\nchallenging optimization problem with a mixture of continuous and discrete\ndecision variables. In this paper, we present a full pipeline based on\nMonte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to\nperform simultaneous contact sequence and patch selection on highly challenging\nenvironments. Through extensive simulation experiments, we show that our\nframework can quickly find a diverse set of dynamically consistent plans. We\nexperimentally show that these plans are transferable to a real quadruped\nrobot. We further show that the same framework can find highly complex acyclic\nhumanoid maneuvers. To the best of our knowledge, this is the first\ndemonstration of simultaneous contact sequence and patch selection for acyclic\nmulti-contact locomotion using the whole-body dynamics of a quadruped.\n", "link": "http://arxiv.org/abs/2508.12928v1", "date": "2025-08-18", "relevancy": 1.7172, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6064}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5771}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Contact%20Sequence%20and%20Patch%20Planning%20for%20Dynamic%20Locomotion&body=Title%3A%20Simultaneous%20Contact%20Sequence%20and%20Patch%20Planning%20for%20Dynamic%20Locomotion%0AAuthor%3A%20Victor%20Dh%C3%A9din%20and%20Haizhou%20Zhao%20and%20Majid%20Khadiv%0AAbstract%3A%20%20%20Legged%20robots%20have%20the%20potential%20to%20traverse%20highly%20constrained%20environments%0Awith%20agile%20maneuvers.%20However%2C%20planning%20such%20motions%20requires%20solving%20a%20highly%0Achallenging%20optimization%20problem%20with%20a%20mixture%20of%20continuous%20and%20discrete%0Adecision%20variables.%20In%20this%20paper%2C%20we%20present%20a%20full%20pipeline%20based%20on%0AMonte-Carlo%20tree%20search%20%28MCTS%29%20and%20whole-body%20trajectory%20optimization%20%28TO%29%20to%0Aperform%20simultaneous%20contact%20sequence%20and%20patch%20selection%20on%20highly%20challenging%0Aenvironments.%20Through%20extensive%20simulation%20experiments%2C%20we%20show%20that%20our%0Aframework%20can%20quickly%20find%20a%20diverse%20set%20of%20dynamically%20consistent%20plans.%20We%0Aexperimentally%20show%20that%20these%20plans%20are%20transferable%20to%20a%20real%20quadruped%0Arobot.%20We%20further%20show%20that%20the%20same%20framework%20can%20find%20highly%20complex%20acyclic%0Ahumanoid%20maneuvers.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Ademonstration%20of%20simultaneous%20contact%20sequence%20and%20patch%20selection%20for%20acyclic%0Amulti-contact%20locomotion%20using%20the%20whole-body%20dynamics%20of%20a%20quadruped.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.12928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Contact%2520Sequence%2520and%2520Patch%2520Planning%2520for%2520Dynamic%2520Locomotion%26entry.906535625%3DVictor%2520Dh%25C3%25A9din%2520and%2520Haizhou%2520Zhao%2520and%2520Majid%2520Khadiv%26entry.1292438233%3D%2520%2520Legged%2520robots%2520have%2520the%2520potential%2520to%2520traverse%2520highly%2520constrained%2520environments%250Awith%2520agile%2520maneuvers.%2520However%252C%2520planning%2520such%2520motions%2520requires%2520solving%2520a%2520highly%250Achallenging%2520optimization%2520problem%2520with%2520a%2520mixture%2520of%2520continuous%2520and%2520discrete%250Adecision%2520variables.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520full%2520pipeline%2520based%2520on%250AMonte-Carlo%2520tree%2520search%2520%2528MCTS%2529%2520and%2520whole-body%2520trajectory%2520optimization%2520%2528TO%2529%2520to%250Aperform%2520simultaneous%2520contact%2520sequence%2520and%2520patch%2520selection%2520on%2520highly%2520challenging%250Aenvironments.%2520Through%2520extensive%2520simulation%2520experiments%252C%2520we%2520show%2520that%2520our%250Aframework%2520can%2520quickly%2520find%2520a%2520diverse%2520set%2520of%2520dynamically%2520consistent%2520plans.%2520We%250Aexperimentally%2520show%2520that%2520these%2520plans%2520are%2520transferable%2520to%2520a%2520real%2520quadruped%250Arobot.%2520We%2520further%2520show%2520that%2520the%2520same%2520framework%2520can%2520find%2520highly%2520complex%2520acyclic%250Ahumanoid%2520maneuvers.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Ademonstration%2520of%2520simultaneous%2520contact%2520sequence%2520and%2520patch%2520selection%2520for%2520acyclic%250Amulti-contact%2520locomotion%2520using%2520the%2520whole-body%2520dynamics%2520of%2520a%2520quadruped.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Contact%20Sequence%20and%20Patch%20Planning%20for%20Dynamic%20Locomotion&entry.906535625=Victor%20Dh%C3%A9din%20and%20Haizhou%20Zhao%20and%20Majid%20Khadiv&entry.1292438233=%20%20Legged%20robots%20have%20the%20potential%20to%20traverse%20highly%20constrained%20environments%0Awith%20agile%20maneuvers.%20However%2C%20planning%20such%20motions%20requires%20solving%20a%20highly%0Achallenging%20optimization%20problem%20with%20a%20mixture%20of%20continuous%20and%20discrete%0Adecision%20variables.%20In%20this%20paper%2C%20we%20present%20a%20full%20pipeline%20based%20on%0AMonte-Carlo%20tree%20search%20%28MCTS%29%20and%20whole-body%20trajectory%20optimization%20%28TO%29%20to%0Aperform%20simultaneous%20contact%20sequence%20and%20patch%20selection%20on%20highly%20challenging%0Aenvironments.%20Through%20extensive%20simulation%20experiments%2C%20we%20show%20that%20our%0Aframework%20can%20quickly%20find%20a%20diverse%20set%20of%20dynamically%20consistent%20plans.%20We%0Aexperimentally%20show%20that%20these%20plans%20are%20transferable%20to%20a%20real%20quadruped%0Arobot.%20We%20further%20show%20that%20the%20same%20framework%20can%20find%20highly%20complex%20acyclic%0Ahumanoid%20maneuvers.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Ademonstration%20of%20simultaneous%20contact%20sequence%20and%20patch%20selection%20for%20acyclic%0Amulti-contact%20locomotion%20using%20the%20whole-body%20dynamics%20of%20a%20quadruped.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.12928v1&entry.124074799=Read"},
{"title": "Degradation-Agnostic Statistical Facial Feature Transformation for Blind\n  Face Restoration in Adverse Weather Conditions", "author": "Chang-Hwan Son", "abstract": "  With the increasing deployment of intelligent CCTV systems in outdoor\nenvironments, there is a growing demand for face recognition systems optimized\nfor challenging weather conditions. Adverse weather significantly degrades\nimage quality, which in turn reduces recognition accuracy. Although recent face\nimage restoration (FIR) models based on generative adversarial networks (GANs)\nand diffusion models have shown progress, their performance remains limited due\nto the lack of dedicated modules that explicitly address weather-induced\ndegradations. This leads to distorted facial textures and structures. To\naddress these limitations, we propose a novel GAN-based blind FIR framework\nthat integrates two key components: local Statistical Facial Feature\nTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The\nlocal SFFT module enhances facial structure and color fidelity by aligning the\nlocal statistical distributions of low-quality (LQ) facial regions with those\nof high-quality (HQ) counterparts. Complementarily, the DAFE module enables\nrobust statistical facial feature extraction under adverse weather conditions\nby aligning LQ and HQ encoder representations, thereby making the restoration\nprocess adaptive to severe weather-induced degradations. Experimental results\ndemonstrate that the proposed degradation-agnostic SFFT model outperforms\nexisting state-of-the-art FIR methods based on GAN and diffusion models,\nparticularly in suppressing texture distortions and accurately reconstructing\nfacial structures. Furthermore, both the SFFT and DAFE modules are empirically\nvalidated in enhancing structural fidelity and perceptual quality in face\nrestoration under challenging weather scenarios.\n", "link": "http://arxiv.org/abs/2507.07464v3", "date": "2025-08-18", "relevancy": 1.7077, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.575}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5723}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degradation-Agnostic%20Statistical%20Facial%20Feature%20Transformation%20for%20Blind%0A%20%20Face%20Restoration%20in%20Adverse%20Weather%20Conditions&body=Title%3A%20Degradation-Agnostic%20Statistical%20Facial%20Feature%20Transformation%20for%20Blind%0A%20%20Face%20Restoration%20in%20Adverse%20Weather%20Conditions%0AAuthor%3A%20Chang-Hwan%20Son%0AAbstract%3A%20%20%20With%20the%20increasing%20deployment%20of%20intelligent%20CCTV%20systems%20in%20outdoor%0Aenvironments%2C%20there%20is%20a%20growing%20demand%20for%20face%20recognition%20systems%20optimized%0Afor%20challenging%20weather%20conditions.%20Adverse%20weather%20significantly%20degrades%0Aimage%20quality%2C%20which%20in%20turn%20reduces%20recognition%20accuracy.%20Although%20recent%20face%0Aimage%20restoration%20%28FIR%29%20models%20based%20on%20generative%20adversarial%20networks%20%28GANs%29%0Aand%20diffusion%20models%20have%20shown%20progress%2C%20their%20performance%20remains%20limited%20due%0Ato%20the%20lack%20of%20dedicated%20modules%20that%20explicitly%20address%20weather-induced%0Adegradations.%20This%20leads%20to%20distorted%20facial%20textures%20and%20structures.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20GAN-based%20blind%20FIR%20framework%0Athat%20integrates%20two%20key%20components%3A%20local%20Statistical%20Facial%20Feature%0ATransformation%20%28SFFT%29%20and%20Degradation-Agnostic%20Feature%20Embedding%20%28DAFE%29.%20The%0Alocal%20SFFT%20module%20enhances%20facial%20structure%20and%20color%20fidelity%20by%20aligning%20the%0Alocal%20statistical%20distributions%20of%20low-quality%20%28LQ%29%20facial%20regions%20with%20those%0Aof%20high-quality%20%28HQ%29%20counterparts.%20Complementarily%2C%20the%20DAFE%20module%20enables%0Arobust%20statistical%20facial%20feature%20extraction%20under%20adverse%20weather%20conditions%0Aby%20aligning%20LQ%20and%20HQ%20encoder%20representations%2C%20thereby%20making%20the%20restoration%0Aprocess%20adaptive%20to%20severe%20weather-induced%20degradations.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20degradation-agnostic%20SFFT%20model%20outperforms%0Aexisting%20state-of-the-art%20FIR%20methods%20based%20on%20GAN%20and%20diffusion%20models%2C%0Aparticularly%20in%20suppressing%20texture%20distortions%20and%20accurately%20reconstructing%0Afacial%20structures.%20Furthermore%2C%20both%20the%20SFFT%20and%20DAFE%20modules%20are%20empirically%0Avalidated%20in%20enhancing%20structural%20fidelity%20and%20perceptual%20quality%20in%20face%0Arestoration%20under%20challenging%20weather%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07464v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegradation-Agnostic%2520Statistical%2520Facial%2520Feature%2520Transformation%2520for%2520Blind%250A%2520%2520Face%2520Restoration%2520in%2520Adverse%2520Weather%2520Conditions%26entry.906535625%3DChang-Hwan%2520Son%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520deployment%2520of%2520intelligent%2520CCTV%2520systems%2520in%2520outdoor%250Aenvironments%252C%2520there%2520is%2520a%2520growing%2520demand%2520for%2520face%2520recognition%2520systems%2520optimized%250Afor%2520challenging%2520weather%2520conditions.%2520Adverse%2520weather%2520significantly%2520degrades%250Aimage%2520quality%252C%2520which%2520in%2520turn%2520reduces%2520recognition%2520accuracy.%2520Although%2520recent%2520face%250Aimage%2520restoration%2520%2528FIR%2529%2520models%2520based%2520on%2520generative%2520adversarial%2520networks%2520%2528GANs%2529%250Aand%2520diffusion%2520models%2520have%2520shown%2520progress%252C%2520their%2520performance%2520remains%2520limited%2520due%250Ato%2520the%2520lack%2520of%2520dedicated%2520modules%2520that%2520explicitly%2520address%2520weather-induced%250Adegradations.%2520This%2520leads%2520to%2520distorted%2520facial%2520textures%2520and%2520structures.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520GAN-based%2520blind%2520FIR%2520framework%250Athat%2520integrates%2520two%2520key%2520components%253A%2520local%2520Statistical%2520Facial%2520Feature%250ATransformation%2520%2528SFFT%2529%2520and%2520Degradation-Agnostic%2520Feature%2520Embedding%2520%2528DAFE%2529.%2520The%250Alocal%2520SFFT%2520module%2520enhances%2520facial%2520structure%2520and%2520color%2520fidelity%2520by%2520aligning%2520the%250Alocal%2520statistical%2520distributions%2520of%2520low-quality%2520%2528LQ%2529%2520facial%2520regions%2520with%2520those%250Aof%2520high-quality%2520%2528HQ%2529%2520counterparts.%2520Complementarily%252C%2520the%2520DAFE%2520module%2520enables%250Arobust%2520statistical%2520facial%2520feature%2520extraction%2520under%2520adverse%2520weather%2520conditions%250Aby%2520aligning%2520LQ%2520and%2520HQ%2520encoder%2520representations%252C%2520thereby%2520making%2520the%2520restoration%250Aprocess%2520adaptive%2520to%2520severe%2520weather-induced%2520degradations.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520degradation-agnostic%2520SFFT%2520model%2520outperforms%250Aexisting%2520state-of-the-art%2520FIR%2520methods%2520based%2520on%2520GAN%2520and%2520diffusion%2520models%252C%250Aparticularly%2520in%2520suppressing%2520texture%2520distortions%2520and%2520accurately%2520reconstructing%250Afacial%2520structures.%2520Furthermore%252C%2520both%2520the%2520SFFT%2520and%2520DAFE%2520modules%2520are%2520empirically%250Avalidated%2520in%2520enhancing%2520structural%2520fidelity%2520and%2520perceptual%2520quality%2520in%2520face%250Arestoration%2520under%2520challenging%2520weather%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07464v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degradation-Agnostic%20Statistical%20Facial%20Feature%20Transformation%20for%20Blind%0A%20%20Face%20Restoration%20in%20Adverse%20Weather%20Conditions&entry.906535625=Chang-Hwan%20Son&entry.1292438233=%20%20With%20the%20increasing%20deployment%20of%20intelligent%20CCTV%20systems%20in%20outdoor%0Aenvironments%2C%20there%20is%20a%20growing%20demand%20for%20face%20recognition%20systems%20optimized%0Afor%20challenging%20weather%20conditions.%20Adverse%20weather%20significantly%20degrades%0Aimage%20quality%2C%20which%20in%20turn%20reduces%20recognition%20accuracy.%20Although%20recent%20face%0Aimage%20restoration%20%28FIR%29%20models%20based%20on%20generative%20adversarial%20networks%20%28GANs%29%0Aand%20diffusion%20models%20have%20shown%20progress%2C%20their%20performance%20remains%20limited%20due%0Ato%20the%20lack%20of%20dedicated%20modules%20that%20explicitly%20address%20weather-induced%0Adegradations.%20This%20leads%20to%20distorted%20facial%20textures%20and%20structures.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20GAN-based%20blind%20FIR%20framework%0Athat%20integrates%20two%20key%20components%3A%20local%20Statistical%20Facial%20Feature%0ATransformation%20%28SFFT%29%20and%20Degradation-Agnostic%20Feature%20Embedding%20%28DAFE%29.%20The%0Alocal%20SFFT%20module%20enhances%20facial%20structure%20and%20color%20fidelity%20by%20aligning%20the%0Alocal%20statistical%20distributions%20of%20low-quality%20%28LQ%29%20facial%20regions%20with%20those%0Aof%20high-quality%20%28HQ%29%20counterparts.%20Complementarily%2C%20the%20DAFE%20module%20enables%0Arobust%20statistical%20facial%20feature%20extraction%20under%20adverse%20weather%20conditions%0Aby%20aligning%20LQ%20and%20HQ%20encoder%20representations%2C%20thereby%20making%20the%20restoration%0Aprocess%20adaptive%20to%20severe%20weather-induced%20degradations.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20degradation-agnostic%20SFFT%20model%20outperforms%0Aexisting%20state-of-the-art%20FIR%20methods%20based%20on%20GAN%20and%20diffusion%20models%2C%0Aparticularly%20in%20suppressing%20texture%20distortions%20and%20accurately%20reconstructing%0Afacial%20structures.%20Furthermore%2C%20both%20the%20SFFT%20and%20DAFE%20modules%20are%20empirically%0Avalidated%20in%20enhancing%20structural%20fidelity%20and%20perceptual%20quality%20in%20face%0Arestoration%20under%20challenging%20weather%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07464v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


