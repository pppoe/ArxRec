<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240625.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Director3D: Real-world Camera Trajectory and 3D Scene Generation from\n  Text", "author": "Xinyang Li and Zhangyu Lai and Linning Xu and Yansong Qu and Liujuan Cao and Shengchuan Zhang and Bo Dai and Rongrong Ji", "abstract": "  Recent advancements in 3D generation have leveraged synthetic datasets with\nground truth 3D assets and predefined cameras. However, the potential of\nadopting real-world datasets, which can produce significantly more realistic 3D\nscenes, remains largely unexplored. In this work, we delve into the key\nchallenge of the complex and scene-specific camera trajectories found in\nreal-world captures. We introduce Director3D, a robust open-world text-to-3D\ngeneration framework, designed to generate both real-world 3D scenes and\nadaptive camera trajectories. To achieve this, (1) we first utilize a\nTrajectory Diffusion Transformer, acting as the Cinematographer, to model the\ndistribution of camera trajectories based on textual descriptions. (2) Next, a\nGaussian-driven Multi-view Latent Diffusion Model serves as the Decorator,\nmodeling the image sequence distribution given the camera trajectories and\ntexts. This model, fine-tuned from a 2D diffusion model, directly generates\npixel-aligned 3D Gaussians as an immediate 3D scene representation for\nconsistent denoising. (3) Lastly, the 3D Gaussians are refined by a novel SDS++\nloss as the Detailer, which incorporates the prior of the 2D diffusion model.\nExtensive experiments demonstrate that Director3D outperforms existing methods,\noffering superior performance in real-world 3D generation.\n", "link": "http://arxiv.org/abs/2406.17601v1", "date": "2024-06-25", "relevancy": 3.3659, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.696}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.696}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Director3D%3A%20Real-world%20Camera%20Trajectory%20and%203D%20Scene%20Generation%20from%0A%20%20Text&body=Title%3A%20Director3D%3A%20Real-world%20Camera%20Trajectory%20and%203D%20Scene%20Generation%20from%0A%20%20Text%0AAuthor%3A%20Xinyang%20Li%20and%20Zhangyu%20Lai%20and%20Linning%20Xu%20and%20Yansong%20Qu%20and%20Liujuan%20Cao%20and%20Shengchuan%20Zhang%20and%20Bo%20Dai%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Recent%20advancements%20in%203D%20generation%20have%20leveraged%20synthetic%20datasets%20with%0Aground%20truth%203D%20assets%20and%20predefined%20cameras.%20However%2C%20the%20potential%20of%0Aadopting%20real-world%20datasets%2C%20which%20can%20produce%20significantly%20more%20realistic%203D%0Ascenes%2C%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20delve%20into%20the%20key%0Achallenge%20of%20the%20complex%20and%20scene-specific%20camera%20trajectories%20found%20in%0Areal-world%20captures.%20We%20introduce%20Director3D%2C%20a%20robust%20open-world%20text-to-3D%0Ageneration%20framework%2C%20designed%20to%20generate%20both%20real-world%203D%20scenes%20and%0Aadaptive%20camera%20trajectories.%20To%20achieve%20this%2C%20%281%29%20we%20first%20utilize%20a%0ATrajectory%20Diffusion%20Transformer%2C%20acting%20as%20the%20Cinematographer%2C%20to%20model%20the%0Adistribution%20of%20camera%20trajectories%20based%20on%20textual%20descriptions.%20%282%29%20Next%2C%20a%0AGaussian-driven%20Multi-view%20Latent%20Diffusion%20Model%20serves%20as%20the%20Decorator%2C%0Amodeling%20the%20image%20sequence%20distribution%20given%20the%20camera%20trajectories%20and%0Atexts.%20This%20model%2C%20fine-tuned%20from%20a%202D%20diffusion%20model%2C%20directly%20generates%0Apixel-aligned%203D%20Gaussians%20as%20an%20immediate%203D%20scene%20representation%20for%0Aconsistent%20denoising.%20%283%29%20Lastly%2C%20the%203D%20Gaussians%20are%20refined%20by%20a%20novel%20SDS%2B%2B%0Aloss%20as%20the%20Detailer%2C%20which%20incorporates%20the%20prior%20of%20the%202D%20diffusion%20model.%0AExtensive%20experiments%20demonstrate%20that%20Director3D%20outperforms%20existing%20methods%2C%0Aoffering%20superior%20performance%20in%20real-world%203D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirector3D%253A%2520Real-world%2520Camera%2520Trajectory%2520and%25203D%2520Scene%2520Generation%2520from%250A%2520%2520Text%26entry.906535625%3DXinyang%2520Li%2520and%2520Zhangyu%2520Lai%2520and%2520Linning%2520Xu%2520and%2520Yansong%2520Qu%2520and%2520Liujuan%2520Cao%2520and%2520Shengchuan%2520Zhang%2520and%2520Bo%2520Dai%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%25203D%2520generation%2520have%2520leveraged%2520synthetic%2520datasets%2520with%250Aground%2520truth%25203D%2520assets%2520and%2520predefined%2520cameras.%2520However%252C%2520the%2520potential%2520of%250Aadopting%2520real-world%2520datasets%252C%2520which%2520can%2520produce%2520significantly%2520more%2520realistic%25203D%250Ascenes%252C%2520remains%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520delve%2520into%2520the%2520key%250Achallenge%2520of%2520the%2520complex%2520and%2520scene-specific%2520camera%2520trajectories%2520found%2520in%250Areal-world%2520captures.%2520We%2520introduce%2520Director3D%252C%2520a%2520robust%2520open-world%2520text-to-3D%250Ageneration%2520framework%252C%2520designed%2520to%2520generate%2520both%2520real-world%25203D%2520scenes%2520and%250Aadaptive%2520camera%2520trajectories.%2520To%2520achieve%2520this%252C%2520%25281%2529%2520we%2520first%2520utilize%2520a%250ATrajectory%2520Diffusion%2520Transformer%252C%2520acting%2520as%2520the%2520Cinematographer%252C%2520to%2520model%2520the%250Adistribution%2520of%2520camera%2520trajectories%2520based%2520on%2520textual%2520descriptions.%2520%25282%2529%2520Next%252C%2520a%250AGaussian-driven%2520Multi-view%2520Latent%2520Diffusion%2520Model%2520serves%2520as%2520the%2520Decorator%252C%250Amodeling%2520the%2520image%2520sequence%2520distribution%2520given%2520the%2520camera%2520trajectories%2520and%250Atexts.%2520This%2520model%252C%2520fine-tuned%2520from%2520a%25202D%2520diffusion%2520model%252C%2520directly%2520generates%250Apixel-aligned%25203D%2520Gaussians%2520as%2520an%2520immediate%25203D%2520scene%2520representation%2520for%250Aconsistent%2520denoising.%2520%25283%2529%2520Lastly%252C%2520the%25203D%2520Gaussians%2520are%2520refined%2520by%2520a%2520novel%2520SDS%252B%252B%250Aloss%2520as%2520the%2520Detailer%252C%2520which%2520incorporates%2520the%2520prior%2520of%2520the%25202D%2520diffusion%2520model.%250AExtensive%2520experiments%2520demonstrate%2520that%2520Director3D%2520outperforms%2520existing%2520methods%252C%250Aoffering%2520superior%2520performance%2520in%2520real-world%25203D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Director3D%3A%20Real-world%20Camera%20Trajectory%20and%203D%20Scene%20Generation%20from%0A%20%20Text&entry.906535625=Xinyang%20Li%20and%20Zhangyu%20Lai%20and%20Linning%20Xu%20and%20Yansong%20Qu%20and%20Liujuan%20Cao%20and%20Shengchuan%20Zhang%20and%20Bo%20Dai%20and%20Rongrong%20Ji&entry.1292438233=%20%20Recent%20advancements%20in%203D%20generation%20have%20leveraged%20synthetic%20datasets%20with%0Aground%20truth%203D%20assets%20and%20predefined%20cameras.%20However%2C%20the%20potential%20of%0Aadopting%20real-world%20datasets%2C%20which%20can%20produce%20significantly%20more%20realistic%203D%0Ascenes%2C%20remains%20largely%20unexplored.%20In%20this%20work%2C%20we%20delve%20into%20the%20key%0Achallenge%20of%20the%20complex%20and%20scene-specific%20camera%20trajectories%20found%20in%0Areal-world%20captures.%20We%20introduce%20Director3D%2C%20a%20robust%20open-world%20text-to-3D%0Ageneration%20framework%2C%20designed%20to%20generate%20both%20real-world%203D%20scenes%20and%0Aadaptive%20camera%20trajectories.%20To%20achieve%20this%2C%20%281%29%20we%20first%20utilize%20a%0ATrajectory%20Diffusion%20Transformer%2C%20acting%20as%20the%20Cinematographer%2C%20to%20model%20the%0Adistribution%20of%20camera%20trajectories%20based%20on%20textual%20descriptions.%20%282%29%20Next%2C%20a%0AGaussian-driven%20Multi-view%20Latent%20Diffusion%20Model%20serves%20as%20the%20Decorator%2C%0Amodeling%20the%20image%20sequence%20distribution%20given%20the%20camera%20trajectories%20and%0Atexts.%20This%20model%2C%20fine-tuned%20from%20a%202D%20diffusion%20model%2C%20directly%20generates%0Apixel-aligned%203D%20Gaussians%20as%20an%20immediate%203D%20scene%20representation%20for%0Aconsistent%20denoising.%20%283%29%20Lastly%2C%20the%203D%20Gaussians%20are%20refined%20by%20a%20novel%20SDS%2B%2B%0Aloss%20as%20the%20Detailer%2C%20which%20incorporates%20the%20prior%20of%20the%202D%20diffusion%20model.%0AExtensive%20experiments%20demonstrate%20that%20Director3D%20outperforms%20existing%20methods%2C%0Aoffering%20superior%20performance%20in%20real-world%203D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17601v1&entry.124074799=Read"},
{"title": "XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies", "author": "Xuanchi Ren and Jiahui Huang and Xiaohui Zeng and Ken Museth and Sanja Fidler and Francis Williams", "abstract": "  We present XCube (abbreviated as $\\mathcal{X}^3$), a novel generative model\nfor high-resolution sparse 3D voxel grids with arbitrary attributes. Our model\ncan generate millions of voxels with a finest effective resolution of up to\n$1024^3$ in a feed-forward fashion without time-consuming test-time\noptimization. To achieve this, we employ a hierarchical voxel latent diffusion\nmodel which generates progressively higher resolution grids in a coarse-to-fine\nmanner using a custom framework built on the highly efficient VDB data\nstructure. Apart from generating high-resolution objects, we demonstrate the\neffectiveness of XCube on large outdoor scenes at scales of 100m$\\times$100m\nwith a voxel size as small as 10cm. We observe clear qualitative and\nquantitative improvements over past approaches. In addition to unconditional\ngeneration, we show that our model can be used to solve a variety of tasks such\nas user-guided editing, scene completion from a single scan, and text-to-3D.\nThe source code and more results can be found at\nhttps://research.nvidia.com/labs/toronto-ai/xcube/.\n", "link": "http://arxiv.org/abs/2312.03806v2", "date": "2024-06-25", "relevancy": 3.0655, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6364}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6364}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XCube%3A%20Large-Scale%203D%20Generative%20Modeling%20using%20Sparse%20Voxel%20Hierarchies&body=Title%3A%20XCube%3A%20Large-Scale%203D%20Generative%20Modeling%20using%20Sparse%20Voxel%20Hierarchies%0AAuthor%3A%20Xuanchi%20Ren%20and%20Jiahui%20Huang%20and%20Xiaohui%20Zeng%20and%20Ken%20Museth%20and%20Sanja%20Fidler%20and%20Francis%20Williams%0AAbstract%3A%20%20%20We%20present%20XCube%20%28abbreviated%20as%20%24%5Cmathcal%7BX%7D%5E3%24%29%2C%20a%20novel%20generative%20model%0Afor%20high-resolution%20sparse%203D%20voxel%20grids%20with%20arbitrary%20attributes.%20Our%20model%0Acan%20generate%20millions%20of%20voxels%20with%20a%20finest%20effective%20resolution%20of%20up%20to%0A%241024%5E3%24%20in%20a%20feed-forward%20fashion%20without%20time-consuming%20test-time%0Aoptimization.%20To%20achieve%20this%2C%20we%20employ%20a%20hierarchical%20voxel%20latent%20diffusion%0Amodel%20which%20generates%20progressively%20higher%20resolution%20grids%20in%20a%20coarse-to-fine%0Amanner%20using%20a%20custom%20framework%20built%20on%20the%20highly%20efficient%20VDB%20data%0Astructure.%20Apart%20from%20generating%20high-resolution%20objects%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20XCube%20on%20large%20outdoor%20scenes%20at%20scales%20of%20100m%24%5Ctimes%24100m%0Awith%20a%20voxel%20size%20as%20small%20as%2010cm.%20We%20observe%20clear%20qualitative%20and%0Aquantitative%20improvements%20over%20past%20approaches.%20In%20addition%20to%20unconditional%0Ageneration%2C%20we%20show%20that%20our%20model%20can%20be%20used%20to%20solve%20a%20variety%20of%20tasks%20such%0Aas%20user-guided%20editing%2C%20scene%20completion%20from%20a%20single%20scan%2C%20and%20text-to-3D.%0AThe%20source%20code%20and%20more%20results%20can%20be%20found%20at%0Ahttps%3A//research.nvidia.com/labs/toronto-ai/xcube/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03806v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXCube%253A%2520Large-Scale%25203D%2520Generative%2520Modeling%2520using%2520Sparse%2520Voxel%2520Hierarchies%26entry.906535625%3DXuanchi%2520Ren%2520and%2520Jiahui%2520Huang%2520and%2520Xiaohui%2520Zeng%2520and%2520Ken%2520Museth%2520and%2520Sanja%2520Fidler%2520and%2520Francis%2520Williams%26entry.1292438233%3D%2520%2520We%2520present%2520XCube%2520%2528abbreviated%2520as%2520%2524%255Cmathcal%257BX%257D%255E3%2524%2529%252C%2520a%2520novel%2520generative%2520model%250Afor%2520high-resolution%2520sparse%25203D%2520voxel%2520grids%2520with%2520arbitrary%2520attributes.%2520Our%2520model%250Acan%2520generate%2520millions%2520of%2520voxels%2520with%2520a%2520finest%2520effective%2520resolution%2520of%2520up%2520to%250A%25241024%255E3%2524%2520in%2520a%2520feed-forward%2520fashion%2520without%2520time-consuming%2520test-time%250Aoptimization.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520a%2520hierarchical%2520voxel%2520latent%2520diffusion%250Amodel%2520which%2520generates%2520progressively%2520higher%2520resolution%2520grids%2520in%2520a%2520coarse-to-fine%250Amanner%2520using%2520a%2520custom%2520framework%2520built%2520on%2520the%2520highly%2520efficient%2520VDB%2520data%250Astructure.%2520Apart%2520from%2520generating%2520high-resolution%2520objects%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520XCube%2520on%2520large%2520outdoor%2520scenes%2520at%2520scales%2520of%2520100m%2524%255Ctimes%2524100m%250Awith%2520a%2520voxel%2520size%2520as%2520small%2520as%252010cm.%2520We%2520observe%2520clear%2520qualitative%2520and%250Aquantitative%2520improvements%2520over%2520past%2520approaches.%2520In%2520addition%2520to%2520unconditional%250Ageneration%252C%2520we%2520show%2520that%2520our%2520model%2520can%2520be%2520used%2520to%2520solve%2520a%2520variety%2520of%2520tasks%2520such%250Aas%2520user-guided%2520editing%252C%2520scene%2520completion%2520from%2520a%2520single%2520scan%252C%2520and%2520text-to-3D.%250AThe%2520source%2520code%2520and%2520more%2520results%2520can%2520be%2520found%2520at%250Ahttps%253A//research.nvidia.com/labs/toronto-ai/xcube/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03806v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XCube%3A%20Large-Scale%203D%20Generative%20Modeling%20using%20Sparse%20Voxel%20Hierarchies&entry.906535625=Xuanchi%20Ren%20and%20Jiahui%20Huang%20and%20Xiaohui%20Zeng%20and%20Ken%20Museth%20and%20Sanja%20Fidler%20and%20Francis%20Williams&entry.1292438233=%20%20We%20present%20XCube%20%28abbreviated%20as%20%24%5Cmathcal%7BX%7D%5E3%24%29%2C%20a%20novel%20generative%20model%0Afor%20high-resolution%20sparse%203D%20voxel%20grids%20with%20arbitrary%20attributes.%20Our%20model%0Acan%20generate%20millions%20of%20voxels%20with%20a%20finest%20effective%20resolution%20of%20up%20to%0A%241024%5E3%24%20in%20a%20feed-forward%20fashion%20without%20time-consuming%20test-time%0Aoptimization.%20To%20achieve%20this%2C%20we%20employ%20a%20hierarchical%20voxel%20latent%20diffusion%0Amodel%20which%20generates%20progressively%20higher%20resolution%20grids%20in%20a%20coarse-to-fine%0Amanner%20using%20a%20custom%20framework%20built%20on%20the%20highly%20efficient%20VDB%20data%0Astructure.%20Apart%20from%20generating%20high-resolution%20objects%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20XCube%20on%20large%20outdoor%20scenes%20at%20scales%20of%20100m%24%5Ctimes%24100m%0Awith%20a%20voxel%20size%20as%20small%20as%2010cm.%20We%20observe%20clear%20qualitative%20and%0Aquantitative%20improvements%20over%20past%20approaches.%20In%20addition%20to%20unconditional%0Ageneration%2C%20we%20show%20that%20our%20model%20can%20be%20used%20to%20solve%20a%20variety%20of%20tasks%20such%0Aas%20user-guided%20editing%2C%20scene%20completion%20from%20a%20single%20scan%2C%20and%20text-to-3D.%0AThe%20source%20code%20and%20more%20results%20can%20be%20found%20at%0Ahttps%3A//research.nvidia.com/labs/toronto-ai/xcube/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03806v2&entry.124074799=Read"},
{"title": "Investigating Self-Supervised Methods for Label-Efficient Learning", "author": "Srinivasa Rao Nandam and Sara Atito and Zhenhua Feng and Josef Kittler and Muhammad Awais", "abstract": "  Vision transformers combined with self-supervised learning have enabled the\ndevelopment of models which scale across large datasets for several downstream\ntasks like classification, segmentation and detection. The low-shot learning\ncapability of these models, across several low-shot downstream tasks, has been\nlargely under explored. We perform a system level study of different self\nsupervised pretext tasks, namely contrastive learning, clustering, and masked\nimage modelling for their low-shot capabilities by comparing the pretrained\nmodels. In addition we also study the effects of collapse avoidance methods,\nnamely centring, ME-MAX, sinkhorn, on these downstream tasks. Based on our\ndetailed analysis, we introduce a framework involving both mask image modelling\nand clustering as pretext tasks, which performs better across all low-shot\ndownstream tasks, including multi-class classification, multi-label\nclassification and semantic segmentation. Furthermore, when testing the model\non full scale datasets, we show performance gains in multi-class\nclassification, multi-label classification and semantic segmentation.\n", "link": "http://arxiv.org/abs/2406.17460v1", "date": "2024-06-25", "relevancy": 3.0277, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6307}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6142}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Self-Supervised%20Methods%20for%20Label-Efficient%20Learning&body=Title%3A%20Investigating%20Self-Supervised%20Methods%20for%20Label-Efficient%20Learning%0AAuthor%3A%20Srinivasa%20Rao%20Nandam%20and%20Sara%20Atito%20and%20Zhenhua%20Feng%20and%20Josef%20Kittler%20and%20Muhammad%20Awais%0AAbstract%3A%20%20%20Vision%20transformers%20combined%20with%20self-supervised%20learning%20have%20enabled%20the%0Adevelopment%20of%20models%20which%20scale%20across%20large%20datasets%20for%20several%20downstream%0Atasks%20like%20classification%2C%20segmentation%20and%20detection.%20The%20low-shot%20learning%0Acapability%20of%20these%20models%2C%20across%20several%20low-shot%20downstream%20tasks%2C%20has%20been%0Alargely%20under%20explored.%20We%20perform%20a%20system%20level%20study%20of%20different%20self%0Asupervised%20pretext%20tasks%2C%20namely%20contrastive%20learning%2C%20clustering%2C%20and%20masked%0Aimage%20modelling%20for%20their%20low-shot%20capabilities%20by%20comparing%20the%20pretrained%0Amodels.%20In%20addition%20we%20also%20study%20the%20effects%20of%20collapse%20avoidance%20methods%2C%0Anamely%20centring%2C%20ME-MAX%2C%20sinkhorn%2C%20on%20these%20downstream%20tasks.%20Based%20on%20our%0Adetailed%20analysis%2C%20we%20introduce%20a%20framework%20involving%20both%20mask%20image%20modelling%0Aand%20clustering%20as%20pretext%20tasks%2C%20which%20performs%20better%20across%20all%20low-shot%0Adownstream%20tasks%2C%20including%20multi-class%20classification%2C%20multi-label%0Aclassification%20and%20semantic%20segmentation.%20Furthermore%2C%20when%20testing%20the%20model%0Aon%20full%20scale%20datasets%2C%20we%20show%20performance%20gains%20in%20multi-class%0Aclassification%2C%20multi-label%20classification%20and%20semantic%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Self-Supervised%2520Methods%2520for%2520Label-Efficient%2520Learning%26entry.906535625%3DSrinivasa%2520Rao%2520Nandam%2520and%2520Sara%2520Atito%2520and%2520Zhenhua%2520Feng%2520and%2520Josef%2520Kittler%2520and%2520Muhammad%2520Awais%26entry.1292438233%3D%2520%2520Vision%2520transformers%2520combined%2520with%2520self-supervised%2520learning%2520have%2520enabled%2520the%250Adevelopment%2520of%2520models%2520which%2520scale%2520across%2520large%2520datasets%2520for%2520several%2520downstream%250Atasks%2520like%2520classification%252C%2520segmentation%2520and%2520detection.%2520The%2520low-shot%2520learning%250Acapability%2520of%2520these%2520models%252C%2520across%2520several%2520low-shot%2520downstream%2520tasks%252C%2520has%2520been%250Alargely%2520under%2520explored.%2520We%2520perform%2520a%2520system%2520level%2520study%2520of%2520different%2520self%250Asupervised%2520pretext%2520tasks%252C%2520namely%2520contrastive%2520learning%252C%2520clustering%252C%2520and%2520masked%250Aimage%2520modelling%2520for%2520their%2520low-shot%2520capabilities%2520by%2520comparing%2520the%2520pretrained%250Amodels.%2520In%2520addition%2520we%2520also%2520study%2520the%2520effects%2520of%2520collapse%2520avoidance%2520methods%252C%250Anamely%2520centring%252C%2520ME-MAX%252C%2520sinkhorn%252C%2520on%2520these%2520downstream%2520tasks.%2520Based%2520on%2520our%250Adetailed%2520analysis%252C%2520we%2520introduce%2520a%2520framework%2520involving%2520both%2520mask%2520image%2520modelling%250Aand%2520clustering%2520as%2520pretext%2520tasks%252C%2520which%2520performs%2520better%2520across%2520all%2520low-shot%250Adownstream%2520tasks%252C%2520including%2520multi-class%2520classification%252C%2520multi-label%250Aclassification%2520and%2520semantic%2520segmentation.%2520Furthermore%252C%2520when%2520testing%2520the%2520model%250Aon%2520full%2520scale%2520datasets%252C%2520we%2520show%2520performance%2520gains%2520in%2520multi-class%250Aclassification%252C%2520multi-label%2520classification%2520and%2520semantic%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Self-Supervised%20Methods%20for%20Label-Efficient%20Learning&entry.906535625=Srinivasa%20Rao%20Nandam%20and%20Sara%20Atito%20and%20Zhenhua%20Feng%20and%20Josef%20Kittler%20and%20Muhammad%20Awais&entry.1292438233=%20%20Vision%20transformers%20combined%20with%20self-supervised%20learning%20have%20enabled%20the%0Adevelopment%20of%20models%20which%20scale%20across%20large%20datasets%20for%20several%20downstream%0Atasks%20like%20classification%2C%20segmentation%20and%20detection.%20The%20low-shot%20learning%0Acapability%20of%20these%20models%2C%20across%20several%20low-shot%20downstream%20tasks%2C%20has%20been%0Alargely%20under%20explored.%20We%20perform%20a%20system%20level%20study%20of%20different%20self%0Asupervised%20pretext%20tasks%2C%20namely%20contrastive%20learning%2C%20clustering%2C%20and%20masked%0Aimage%20modelling%20for%20their%20low-shot%20capabilities%20by%20comparing%20the%20pretrained%0Amodels.%20In%20addition%20we%20also%20study%20the%20effects%20of%20collapse%20avoidance%20methods%2C%0Anamely%20centring%2C%20ME-MAX%2C%20sinkhorn%2C%20on%20these%20downstream%20tasks.%20Based%20on%20our%0Adetailed%20analysis%2C%20we%20introduce%20a%20framework%20involving%20both%20mask%20image%20modelling%0Aand%20clustering%20as%20pretext%20tasks%2C%20which%20performs%20better%20across%20all%20low-shot%0Adownstream%20tasks%2C%20including%20multi-class%20classification%2C%20multi-label%0Aclassification%20and%20semantic%20segmentation.%20Furthermore%2C%20when%20testing%20the%20model%0Aon%20full%20scale%20datasets%2C%20we%20show%20performance%20gains%20in%20multi-class%0Aclassification%2C%20multi-label%20classification%20and%20semantic%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17460v1&entry.124074799=Read"},
{"title": "Point-SAM: Promptable 3D Segmentation Model for Point Clouds", "author": "Yuchen Zhou and Jiayuan Gu and Tung Yen Chiang and Fanbo Xiang and Hao Su", "abstract": "  The development of 2D foundation models for image segmentation has been\nsignificantly advanced by the Segment Anything Model (SAM). However, achieving\nsimilar success in 3D models remains a challenge due to issues such as\nnon-unified data formats, lightweight models, and the scarcity of labeled data\nwith diverse masks. To this end, we propose a 3D promptable segmentation model\n(Point-SAM) focusing on point clouds. Our approach utilizes a transformer-based\nmethod, extending SAM to the 3D domain. We leverage part-level and object-level\nannotations and introduce a data engine to generate pseudo labels from SAM,\nthereby distilling 2D knowledge into our 3D model. Our model outperforms\nstate-of-the-art models on several indoor and outdoor benchmarks and\ndemonstrates a variety of applications, such as 3D annotation. Codes and demo\ncan be found at https://github.com/zyc00/Point-SAM.\n", "link": "http://arxiv.org/abs/2406.17741v1", "date": "2024-06-25", "relevancy": 2.9856, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6025}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6025}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point-SAM%3A%20Promptable%203D%20Segmentation%20Model%20for%20Point%20Clouds&body=Title%3A%20Point-SAM%3A%20Promptable%203D%20Segmentation%20Model%20for%20Point%20Clouds%0AAuthor%3A%20Yuchen%20Zhou%20and%20Jiayuan%20Gu%20and%20Tung%20Yen%20Chiang%20and%20Fanbo%20Xiang%20and%20Hao%20Su%0AAbstract%3A%20%20%20The%20development%20of%202D%20foundation%20models%20for%20image%20segmentation%20has%20been%0Asignificantly%20advanced%20by%20the%20Segment%20Anything%20Model%20%28SAM%29.%20However%2C%20achieving%0Asimilar%20success%20in%203D%20models%20remains%20a%20challenge%20due%20to%20issues%20such%20as%0Anon-unified%20data%20formats%2C%20lightweight%20models%2C%20and%20the%20scarcity%20of%20labeled%20data%0Awith%20diverse%20masks.%20To%20this%20end%2C%20we%20propose%20a%203D%20promptable%20segmentation%20model%0A%28Point-SAM%29%20focusing%20on%20point%20clouds.%20Our%20approach%20utilizes%20a%20transformer-based%0Amethod%2C%20extending%20SAM%20to%20the%203D%20domain.%20We%20leverage%20part-level%20and%20object-level%0Aannotations%20and%20introduce%20a%20data%20engine%20to%20generate%20pseudo%20labels%20from%20SAM%2C%0Athereby%20distilling%202D%20knowledge%20into%20our%203D%20model.%20Our%20model%20outperforms%0Astate-of-the-art%20models%20on%20several%20indoor%20and%20outdoor%20benchmarks%20and%0Ademonstrates%20a%20variety%20of%20applications%2C%20such%20as%203D%20annotation.%20Codes%20and%20demo%0Acan%20be%20found%20at%20https%3A//github.com/zyc00/Point-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint-SAM%253A%2520Promptable%25203D%2520Segmentation%2520Model%2520for%2520Point%2520Clouds%26entry.906535625%3DYuchen%2520Zhou%2520and%2520Jiayuan%2520Gu%2520and%2520Tung%2520Yen%2520Chiang%2520and%2520Fanbo%2520Xiang%2520and%2520Hao%2520Su%26entry.1292438233%3D%2520%2520The%2520development%2520of%25202D%2520foundation%2520models%2520for%2520image%2520segmentation%2520has%2520been%250Asignificantly%2520advanced%2520by%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529.%2520However%252C%2520achieving%250Asimilar%2520success%2520in%25203D%2520models%2520remains%2520a%2520challenge%2520due%2520to%2520issues%2520such%2520as%250Anon-unified%2520data%2520formats%252C%2520lightweight%2520models%252C%2520and%2520the%2520scarcity%2520of%2520labeled%2520data%250Awith%2520diverse%2520masks.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%25203D%2520promptable%2520segmentation%2520model%250A%2528Point-SAM%2529%2520focusing%2520on%2520point%2520clouds.%2520Our%2520approach%2520utilizes%2520a%2520transformer-based%250Amethod%252C%2520extending%2520SAM%2520to%2520the%25203D%2520domain.%2520We%2520leverage%2520part-level%2520and%2520object-level%250Aannotations%2520and%2520introduce%2520a%2520data%2520engine%2520to%2520generate%2520pseudo%2520labels%2520from%2520SAM%252C%250Athereby%2520distilling%25202D%2520knowledge%2520into%2520our%25203D%2520model.%2520Our%2520model%2520outperforms%250Astate-of-the-art%2520models%2520on%2520several%2520indoor%2520and%2520outdoor%2520benchmarks%2520and%250Ademonstrates%2520a%2520variety%2520of%2520applications%252C%2520such%2520as%25203D%2520annotation.%2520Codes%2520and%2520demo%250Acan%2520be%2520found%2520at%2520https%253A//github.com/zyc00/Point-SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-SAM%3A%20Promptable%203D%20Segmentation%20Model%20for%20Point%20Clouds&entry.906535625=Yuchen%20Zhou%20and%20Jiayuan%20Gu%20and%20Tung%20Yen%20Chiang%20and%20Fanbo%20Xiang%20and%20Hao%20Su&entry.1292438233=%20%20The%20development%20of%202D%20foundation%20models%20for%20image%20segmentation%20has%20been%0Asignificantly%20advanced%20by%20the%20Segment%20Anything%20Model%20%28SAM%29.%20However%2C%20achieving%0Asimilar%20success%20in%203D%20models%20remains%20a%20challenge%20due%20to%20issues%20such%20as%0Anon-unified%20data%20formats%2C%20lightweight%20models%2C%20and%20the%20scarcity%20of%20labeled%20data%0Awith%20diverse%20masks.%20To%20this%20end%2C%20we%20propose%20a%203D%20promptable%20segmentation%20model%0A%28Point-SAM%29%20focusing%20on%20point%20clouds.%20Our%20approach%20utilizes%20a%20transformer-based%0Amethod%2C%20extending%20SAM%20to%20the%203D%20domain.%20We%20leverage%20part-level%20and%20object-level%0Aannotations%20and%20introduce%20a%20data%20engine%20to%20generate%20pseudo%20labels%20from%20SAM%2C%0Athereby%20distilling%202D%20knowledge%20into%20our%203D%20model.%20Our%20model%20outperforms%0Astate-of-the-art%20models%20on%20several%20indoor%20and%20outdoor%20benchmarks%20and%0Ademonstrates%20a%20variety%20of%20applications%2C%20such%20as%203D%20annotation.%20Codes%20and%20demo%0Acan%20be%20found%20at%20https%3A//github.com/zyc00/Point-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17741v1&entry.124074799=Read"},
{"title": "Fast and Uncertainty-Aware SVBRDF Recovery from Multi-View Capture using\n  Frequency Domain Analysis", "author": "Ruben Wiersma and Julien Philip and Milo\u0161 Ha\u0161an and Krishna Mullia and Fujun Luan and Elmar Eisemann and Valentin Deschaintre", "abstract": "  Relightable object acquisition is a key challenge in simplifying digital\nasset creation. Complete reconstruction of an object typically requires\ncapturing hundreds to thousands of photographs under controlled illumination,\nwith specialized equipment. The recent progress in differentiable rendering\nimproved the quality and accessibility of inverse rendering optimization.\nNevertheless, under uncontrolled illumination and unstructured viewpoints,\nthere is no guarantee that the observations contain enough information to\nreconstruct the appearance properties of the captured object.\n  We thus propose to consider the acquisition process from a signal-processing\nperspective. Given an object's geometry and a lighting environment, we estimate\nthe properties of the materials on the object's surface in seconds. We do so by\nleveraging frequency domain analysis, considering the recovery of material\nproperties as a deconvolution, enabling fast error estimation. We then quantify\nthe uncertainty of the estimation, based on the available data, highlighting\nthe areas for which priors or additional samples would be required for improved\nacquisition quality. We compare our approach to previous work and\nquantitatively evaluate our results, showing similar quality as previous work\nin a fraction of the time, and providing key information about the certainty of\nthe results.\n", "link": "http://arxiv.org/abs/2406.17774v1", "date": "2024-06-25", "relevancy": 2.8981, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5808}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5808}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Uncertainty-Aware%20SVBRDF%20Recovery%20from%20Multi-View%20Capture%20using%0A%20%20Frequency%20Domain%20Analysis&body=Title%3A%20Fast%20and%20Uncertainty-Aware%20SVBRDF%20Recovery%20from%20Multi-View%20Capture%20using%0A%20%20Frequency%20Domain%20Analysis%0AAuthor%3A%20Ruben%20Wiersma%20and%20Julien%20Philip%20and%20Milo%C5%A1%20Ha%C5%A1an%20and%20Krishna%20Mullia%20and%20Fujun%20Luan%20and%20Elmar%20Eisemann%20and%20Valentin%20Deschaintre%0AAbstract%3A%20%20%20Relightable%20object%20acquisition%20is%20a%20key%20challenge%20in%20simplifying%20digital%0Aasset%20creation.%20Complete%20reconstruction%20of%20an%20object%20typically%20requires%0Acapturing%20hundreds%20to%20thousands%20of%20photographs%20under%20controlled%20illumination%2C%0Awith%20specialized%20equipment.%20The%20recent%20progress%20in%20differentiable%20rendering%0Aimproved%20the%20quality%20and%20accessibility%20of%20inverse%20rendering%20optimization.%0ANevertheless%2C%20under%20uncontrolled%20illumination%20and%20unstructured%20viewpoints%2C%0Athere%20is%20no%20guarantee%20that%20the%20observations%20contain%20enough%20information%20to%0Areconstruct%20the%20appearance%20properties%20of%20the%20captured%20object.%0A%20%20We%20thus%20propose%20to%20consider%20the%20acquisition%20process%20from%20a%20signal-processing%0Aperspective.%20Given%20an%20object%27s%20geometry%20and%20a%20lighting%20environment%2C%20we%20estimate%0Athe%20properties%20of%20the%20materials%20on%20the%20object%27s%20surface%20in%20seconds.%20We%20do%20so%20by%0Aleveraging%20frequency%20domain%20analysis%2C%20considering%20the%20recovery%20of%20material%0Aproperties%20as%20a%20deconvolution%2C%20enabling%20fast%20error%20estimation.%20We%20then%20quantify%0Athe%20uncertainty%20of%20the%20estimation%2C%20based%20on%20the%20available%20data%2C%20highlighting%0Athe%20areas%20for%20which%20priors%20or%20additional%20samples%20would%20be%20required%20for%20improved%0Aacquisition%20quality.%20We%20compare%20our%20approach%20to%20previous%20work%20and%0Aquantitatively%20evaluate%20our%20results%2C%20showing%20similar%20quality%20as%20previous%20work%0Ain%20a%20fraction%20of%20the%20time%2C%20and%20providing%20key%20information%20about%20the%20certainty%20of%0Athe%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Uncertainty-Aware%2520SVBRDF%2520Recovery%2520from%2520Multi-View%2520Capture%2520using%250A%2520%2520Frequency%2520Domain%2520Analysis%26entry.906535625%3DRuben%2520Wiersma%2520and%2520Julien%2520Philip%2520and%2520Milo%25C5%25A1%2520Ha%25C5%25A1an%2520and%2520Krishna%2520Mullia%2520and%2520Fujun%2520Luan%2520and%2520Elmar%2520Eisemann%2520and%2520Valentin%2520Deschaintre%26entry.1292438233%3D%2520%2520Relightable%2520object%2520acquisition%2520is%2520a%2520key%2520challenge%2520in%2520simplifying%2520digital%250Aasset%2520creation.%2520Complete%2520reconstruction%2520of%2520an%2520object%2520typically%2520requires%250Acapturing%2520hundreds%2520to%2520thousands%2520of%2520photographs%2520under%2520controlled%2520illumination%252C%250Awith%2520specialized%2520equipment.%2520The%2520recent%2520progress%2520in%2520differentiable%2520rendering%250Aimproved%2520the%2520quality%2520and%2520accessibility%2520of%2520inverse%2520rendering%2520optimization.%250ANevertheless%252C%2520under%2520uncontrolled%2520illumination%2520and%2520unstructured%2520viewpoints%252C%250Athere%2520is%2520no%2520guarantee%2520that%2520the%2520observations%2520contain%2520enough%2520information%2520to%250Areconstruct%2520the%2520appearance%2520properties%2520of%2520the%2520captured%2520object.%250A%2520%2520We%2520thus%2520propose%2520to%2520consider%2520the%2520acquisition%2520process%2520from%2520a%2520signal-processing%250Aperspective.%2520Given%2520an%2520object%2527s%2520geometry%2520and%2520a%2520lighting%2520environment%252C%2520we%2520estimate%250Athe%2520properties%2520of%2520the%2520materials%2520on%2520the%2520object%2527s%2520surface%2520in%2520seconds.%2520We%2520do%2520so%2520by%250Aleveraging%2520frequency%2520domain%2520analysis%252C%2520considering%2520the%2520recovery%2520of%2520material%250Aproperties%2520as%2520a%2520deconvolution%252C%2520enabling%2520fast%2520error%2520estimation.%2520We%2520then%2520quantify%250Athe%2520uncertainty%2520of%2520the%2520estimation%252C%2520based%2520on%2520the%2520available%2520data%252C%2520highlighting%250Athe%2520areas%2520for%2520which%2520priors%2520or%2520additional%2520samples%2520would%2520be%2520required%2520for%2520improved%250Aacquisition%2520quality.%2520We%2520compare%2520our%2520approach%2520to%2520previous%2520work%2520and%250Aquantitatively%2520evaluate%2520our%2520results%252C%2520showing%2520similar%2520quality%2520as%2520previous%2520work%250Ain%2520a%2520fraction%2520of%2520the%2520time%252C%2520and%2520providing%2520key%2520information%2520about%2520the%2520certainty%2520of%250Athe%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Uncertainty-Aware%20SVBRDF%20Recovery%20from%20Multi-View%20Capture%20using%0A%20%20Frequency%20Domain%20Analysis&entry.906535625=Ruben%20Wiersma%20and%20Julien%20Philip%20and%20Milo%C5%A1%20Ha%C5%A1an%20and%20Krishna%20Mullia%20and%20Fujun%20Luan%20and%20Elmar%20Eisemann%20and%20Valentin%20Deschaintre&entry.1292438233=%20%20Relightable%20object%20acquisition%20is%20a%20key%20challenge%20in%20simplifying%20digital%0Aasset%20creation.%20Complete%20reconstruction%20of%20an%20object%20typically%20requires%0Acapturing%20hundreds%20to%20thousands%20of%20photographs%20under%20controlled%20illumination%2C%0Awith%20specialized%20equipment.%20The%20recent%20progress%20in%20differentiable%20rendering%0Aimproved%20the%20quality%20and%20accessibility%20of%20inverse%20rendering%20optimization.%0ANevertheless%2C%20under%20uncontrolled%20illumination%20and%20unstructured%20viewpoints%2C%0Athere%20is%20no%20guarantee%20that%20the%20observations%20contain%20enough%20information%20to%0Areconstruct%20the%20appearance%20properties%20of%20the%20captured%20object.%0A%20%20We%20thus%20propose%20to%20consider%20the%20acquisition%20process%20from%20a%20signal-processing%0Aperspective.%20Given%20an%20object%27s%20geometry%20and%20a%20lighting%20environment%2C%20we%20estimate%0Athe%20properties%20of%20the%20materials%20on%20the%20object%27s%20surface%20in%20seconds.%20We%20do%20so%20by%0Aleveraging%20frequency%20domain%20analysis%2C%20considering%20the%20recovery%20of%20material%0Aproperties%20as%20a%20deconvolution%2C%20enabling%20fast%20error%20estimation.%20We%20then%20quantify%0Athe%20uncertainty%20of%20the%20estimation%2C%20based%20on%20the%20available%20data%2C%20highlighting%0Athe%20areas%20for%20which%20priors%20or%20additional%20samples%20would%20be%20required%20for%20improved%0Aacquisition%20quality.%20We%20compare%20our%20approach%20to%20previous%20work%20and%0Aquantitatively%20evaluate%20our%20results%2C%20showing%20similar%20quality%20as%20previous%20work%0Ain%20a%20fraction%20of%20the%20time%2C%20and%20providing%20key%20information%20about%20the%20certainty%20of%0Athe%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17774v1&entry.124074799=Read"},
{"title": "ID-Animator: Zero-Shot Identity-Preserving Human Video Generation", "author": "Xuanhua He and Quande Liu and Shengju Qian and Xin Wang and Tao Hu and Ke Cao and Keyu Yan and Jie Zhang", "abstract": "  Generating high-fidelity human video with specified identities has attracted\nsignificant attention in the content generation community. However, existing\ntechniques struggle to strike a balance between training efficiency and\nidentity preservation, either requiring tedious case-by-case fine-tuning or\nusually missing identity details in the video generation process. In this\nstudy, we present \\textbf{ID-Animator}, a zero-shot human-video generation\napproach that can perform personalized video generation given a single\nreference facial image without further training. ID-Animator inherits existing\ndiffusion-based video generation backbones with a face adapter to encode the\nID-relevant embeddings from learnable facial latent queries. To facilitate the\nextraction of identity information in video generation, we introduce an\nID-oriented dataset construction pipeline that incorporates unified human\nattributes and action captioning techniques from a constructed facial image\npool. Based on this pipeline, a random reference training strategy is further\ndevised to precisely capture the ID-relevant embeddings with an ID-preserving\nloss, thus improving the fidelity and generalization capacity of our model for\nID-specific video generation. Extensive experiments demonstrate the superiority\nof ID-Animator to generate personalized human videos over previous models.\nMoreover, our method is highly compatible with popular pre-trained T2V models\nlike animatediff and various community backbone models, showing high\nextendability in real-world applications for video generation where identity\npreservation is highly desired. Our codes and checkpoints are released at\nhttps://github.com/ID-Animator/ID-Animator.\n", "link": "http://arxiv.org/abs/2404.15275v3", "date": "2024-06-25", "relevancy": 2.8317, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7565}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6919}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ID-Animator%3A%20Zero-Shot%20Identity-Preserving%20Human%20Video%20Generation&body=Title%3A%20ID-Animator%3A%20Zero-Shot%20Identity-Preserving%20Human%20Video%20Generation%0AAuthor%3A%20Xuanhua%20He%20and%20Quande%20Liu%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Tao%20Hu%20and%20Ke%20Cao%20and%20Keyu%20Yan%20and%20Jie%20Zhang%0AAbstract%3A%20%20%20Generating%20high-fidelity%20human%20video%20with%20specified%20identities%20has%20attracted%0Asignificant%20attention%20in%20the%20content%20generation%20community.%20However%2C%20existing%0Atechniques%20struggle%20to%20strike%20a%20balance%20between%20training%20efficiency%20and%0Aidentity%20preservation%2C%20either%20requiring%20tedious%20case-by-case%20fine-tuning%20or%0Ausually%20missing%20identity%20details%20in%20the%20video%20generation%20process.%20In%20this%0Astudy%2C%20we%20present%20%5Ctextbf%7BID-Animator%7D%2C%20a%20zero-shot%20human-video%20generation%0Aapproach%20that%20can%20perform%20personalized%20video%20generation%20given%20a%20single%0Areference%20facial%20image%20without%20further%20training.%20ID-Animator%20inherits%20existing%0Adiffusion-based%20video%20generation%20backbones%20with%20a%20face%20adapter%20to%20encode%20the%0AID-relevant%20embeddings%20from%20learnable%20facial%20latent%20queries.%20To%20facilitate%20the%0Aextraction%20of%20identity%20information%20in%20video%20generation%2C%20we%20introduce%20an%0AID-oriented%20dataset%20construction%20pipeline%20that%20incorporates%20unified%20human%0Aattributes%20and%20action%20captioning%20techniques%20from%20a%20constructed%20facial%20image%0Apool.%20Based%20on%20this%20pipeline%2C%20a%20random%20reference%20training%20strategy%20is%20further%0Adevised%20to%20precisely%20capture%20the%20ID-relevant%20embeddings%20with%20an%20ID-preserving%0Aloss%2C%20thus%20improving%20the%20fidelity%20and%20generalization%20capacity%20of%20our%20model%20for%0AID-specific%20video%20generation.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20ID-Animator%20to%20generate%20personalized%20human%20videos%20over%20previous%20models.%0AMoreover%2C%20our%20method%20is%20highly%20compatible%20with%20popular%20pre-trained%20T2V%20models%0Alike%20animatediff%20and%20various%20community%20backbone%20models%2C%20showing%20high%0Aextendability%20in%20real-world%20applications%20for%20video%20generation%20where%20identity%0Apreservation%20is%20highly%20desired.%20Our%20codes%20and%20checkpoints%20are%20released%20at%0Ahttps%3A//github.com/ID-Animator/ID-Animator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15275v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DID-Animator%253A%2520Zero-Shot%2520Identity-Preserving%2520Human%2520Video%2520Generation%26entry.906535625%3DXuanhua%2520He%2520and%2520Quande%2520Liu%2520and%2520Shengju%2520Qian%2520and%2520Xin%2520Wang%2520and%2520Tao%2520Hu%2520and%2520Ke%2520Cao%2520and%2520Keyu%2520Yan%2520and%2520Jie%2520Zhang%26entry.1292438233%3D%2520%2520Generating%2520high-fidelity%2520human%2520video%2520with%2520specified%2520identities%2520has%2520attracted%250Asignificant%2520attention%2520in%2520the%2520content%2520generation%2520community.%2520However%252C%2520existing%250Atechniques%2520struggle%2520to%2520strike%2520a%2520balance%2520between%2520training%2520efficiency%2520and%250Aidentity%2520preservation%252C%2520either%2520requiring%2520tedious%2520case-by-case%2520fine-tuning%2520or%250Ausually%2520missing%2520identity%2520details%2520in%2520the%2520video%2520generation%2520process.%2520In%2520this%250Astudy%252C%2520we%2520present%2520%255Ctextbf%257BID-Animator%257D%252C%2520a%2520zero-shot%2520human-video%2520generation%250Aapproach%2520that%2520can%2520perform%2520personalized%2520video%2520generation%2520given%2520a%2520single%250Areference%2520facial%2520image%2520without%2520further%2520training.%2520ID-Animator%2520inherits%2520existing%250Adiffusion-based%2520video%2520generation%2520backbones%2520with%2520a%2520face%2520adapter%2520to%2520encode%2520the%250AID-relevant%2520embeddings%2520from%2520learnable%2520facial%2520latent%2520queries.%2520To%2520facilitate%2520the%250Aextraction%2520of%2520identity%2520information%2520in%2520video%2520generation%252C%2520we%2520introduce%2520an%250AID-oriented%2520dataset%2520construction%2520pipeline%2520that%2520incorporates%2520unified%2520human%250Aattributes%2520and%2520action%2520captioning%2520techniques%2520from%2520a%2520constructed%2520facial%2520image%250Apool.%2520Based%2520on%2520this%2520pipeline%252C%2520a%2520random%2520reference%2520training%2520strategy%2520is%2520further%250Adevised%2520to%2520precisely%2520capture%2520the%2520ID-relevant%2520embeddings%2520with%2520an%2520ID-preserving%250Aloss%252C%2520thus%2520improving%2520the%2520fidelity%2520and%2520generalization%2520capacity%2520of%2520our%2520model%2520for%250AID-specific%2520video%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%250Aof%2520ID-Animator%2520to%2520generate%2520personalized%2520human%2520videos%2520over%2520previous%2520models.%250AMoreover%252C%2520our%2520method%2520is%2520highly%2520compatible%2520with%2520popular%2520pre-trained%2520T2V%2520models%250Alike%2520animatediff%2520and%2520various%2520community%2520backbone%2520models%252C%2520showing%2520high%250Aextendability%2520in%2520real-world%2520applications%2520for%2520video%2520generation%2520where%2520identity%250Apreservation%2520is%2520highly%2520desired.%2520Our%2520codes%2520and%2520checkpoints%2520are%2520released%2520at%250Ahttps%253A//github.com/ID-Animator/ID-Animator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15275v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ID-Animator%3A%20Zero-Shot%20Identity-Preserving%20Human%20Video%20Generation&entry.906535625=Xuanhua%20He%20and%20Quande%20Liu%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Tao%20Hu%20and%20Ke%20Cao%20and%20Keyu%20Yan%20and%20Jie%20Zhang&entry.1292438233=%20%20Generating%20high-fidelity%20human%20video%20with%20specified%20identities%20has%20attracted%0Asignificant%20attention%20in%20the%20content%20generation%20community.%20However%2C%20existing%0Atechniques%20struggle%20to%20strike%20a%20balance%20between%20training%20efficiency%20and%0Aidentity%20preservation%2C%20either%20requiring%20tedious%20case-by-case%20fine-tuning%20or%0Ausually%20missing%20identity%20details%20in%20the%20video%20generation%20process.%20In%20this%0Astudy%2C%20we%20present%20%5Ctextbf%7BID-Animator%7D%2C%20a%20zero-shot%20human-video%20generation%0Aapproach%20that%20can%20perform%20personalized%20video%20generation%20given%20a%20single%0Areference%20facial%20image%20without%20further%20training.%20ID-Animator%20inherits%20existing%0Adiffusion-based%20video%20generation%20backbones%20with%20a%20face%20adapter%20to%20encode%20the%0AID-relevant%20embeddings%20from%20learnable%20facial%20latent%20queries.%20To%20facilitate%20the%0Aextraction%20of%20identity%20information%20in%20video%20generation%2C%20we%20introduce%20an%0AID-oriented%20dataset%20construction%20pipeline%20that%20incorporates%20unified%20human%0Aattributes%20and%20action%20captioning%20techniques%20from%20a%20constructed%20facial%20image%0Apool.%20Based%20on%20this%20pipeline%2C%20a%20random%20reference%20training%20strategy%20is%20further%0Adevised%20to%20precisely%20capture%20the%20ID-relevant%20embeddings%20with%20an%20ID-preserving%0Aloss%2C%20thus%20improving%20the%20fidelity%20and%20generalization%20capacity%20of%20our%20model%20for%0AID-specific%20video%20generation.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20ID-Animator%20to%20generate%20personalized%20human%20videos%20over%20previous%20models.%0AMoreover%2C%20our%20method%20is%20highly%20compatible%20with%20popular%20pre-trained%20T2V%20models%0Alike%20animatediff%20and%20various%20community%20backbone%20models%2C%20showing%20high%0Aextendability%20in%20real-world%20applications%20for%20video%20generation%20where%20identity%0Apreservation%20is%20highly%20desired.%20Our%20codes%20and%20checkpoints%20are%20released%20at%0Ahttps%3A//github.com/ID-Animator/ID-Animator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15275v3&entry.124074799=Read"},
{"title": "Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP", "author": "Sedigheh Eslami and Gerard de Melo", "abstract": "  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering two\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, in\norder to answer these questions and show that answers to both questions are\npositive. Through extensive experiments, we show that AlignCLIP achieves\nnoticeable enhancements in the cross-modal alignment of the embeddings, and\nthereby, reduces the modality gap, while maintaining the performance across\nseveral downstream evaluations, such as zero-shot image classification,\nzero-shot multi-modal retrieval and zero-shot semantic text similarity.\n", "link": "http://arxiv.org/abs/2406.17639v1", "date": "2024-06-25", "relevancy": 2.8064, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6705}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5234}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP&body=Title%3A%20Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP%0AAuthor%3A%20Sedigheh%20Eslami%20and%20Gerard%20de%20Melo%0AAbstract%3A%20%20%20Contrastive%20Language--Image%20Pre-training%20%28CLIP%29%20has%20manifested%20remarkable%0Aimprovements%20in%20zero-shot%20classification%20and%20cross-modal%20vision-language%20tasks.%0AYet%2C%20from%20a%20geometrical%20point%20of%20view%2C%20the%20CLIP%20embedding%20space%20has%20been%20found%0Ato%20have%20a%20pronounced%20modality%20gap.%20This%20gap%20renders%20the%20embedding%20space%20overly%0Asparse%20and%20disconnected%2C%20with%20different%20modalities%20being%20densely%20distributed%20in%0Adistinct%20subregions%20of%20the%20hypersphere.%20In%20this%20work%2C%20we%20aim%20at%20answering%20two%0Amain%20questions%3A%201.%20Does%20sharing%20the%20parameter%20space%20between%20the%20multi-modal%0Aencoders%20reduce%20the%20modality%20gap%3F%202.%20Can%20the%20gap%20be%20mitigated%20by%20pushing%20apart%0Athe%20uni-modal%20embeddings%20via%20intra-modality%20separation%3F%20We%20design%20AlignCLIP%2C%20in%0Aorder%20to%20answer%20these%20questions%20and%20show%20that%20answers%20to%20both%20questions%20are%0Apositive.%20Through%20extensive%20experiments%2C%20we%20show%20that%20AlignCLIP%20achieves%0Anoticeable%20enhancements%20in%20the%20cross-modal%20alignment%20of%20the%20embeddings%2C%20and%0Athereby%2C%20reduces%20the%20modality%20gap%2C%20while%20maintaining%20the%20performance%20across%0Aseveral%20downstream%20evaluations%2C%20such%20as%20zero-shot%20image%20classification%2C%0Azero-shot%20multi-modal%20retrieval%20and%20zero-shot%20semantic%20text%20similarity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigate%2520the%2520Gap%253A%2520Investigating%2520Approaches%2520for%2520Improving%2520Cross-Modal%250A%2520%2520Alignment%2520in%2520CLIP%26entry.906535625%3DSedigheh%2520Eslami%2520and%2520Gerard%2520de%2520Melo%26entry.1292438233%3D%2520%2520Contrastive%2520Language--Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520manifested%2520remarkable%250Aimprovements%2520in%2520zero-shot%2520classification%2520and%2520cross-modal%2520vision-language%2520tasks.%250AYet%252C%2520from%2520a%2520geometrical%2520point%2520of%2520view%252C%2520the%2520CLIP%2520embedding%2520space%2520has%2520been%2520found%250Ato%2520have%2520a%2520pronounced%2520modality%2520gap.%2520This%2520gap%2520renders%2520the%2520embedding%2520space%2520overly%250Asparse%2520and%2520disconnected%252C%2520with%2520different%2520modalities%2520being%2520densely%2520distributed%2520in%250Adistinct%2520subregions%2520of%2520the%2520hypersphere.%2520In%2520this%2520work%252C%2520we%2520aim%2520at%2520answering%2520two%250Amain%2520questions%253A%25201.%2520Does%2520sharing%2520the%2520parameter%2520space%2520between%2520the%2520multi-modal%250Aencoders%2520reduce%2520the%2520modality%2520gap%253F%25202.%2520Can%2520the%2520gap%2520be%2520mitigated%2520by%2520pushing%2520apart%250Athe%2520uni-modal%2520embeddings%2520via%2520intra-modality%2520separation%253F%2520We%2520design%2520AlignCLIP%252C%2520in%250Aorder%2520to%2520answer%2520these%2520questions%2520and%2520show%2520that%2520answers%2520to%2520both%2520questions%2520are%250Apositive.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520AlignCLIP%2520achieves%250Anoticeable%2520enhancements%2520in%2520the%2520cross-modal%2520alignment%2520of%2520the%2520embeddings%252C%2520and%250Athereby%252C%2520reduces%2520the%2520modality%2520gap%252C%2520while%2520maintaining%2520the%2520performance%2520across%250Aseveral%2520downstream%2520evaluations%252C%2520such%2520as%2520zero-shot%2520image%2520classification%252C%250Azero-shot%2520multi-modal%2520retrieval%2520and%2520zero-shot%2520semantic%2520text%2520similarity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigate%20the%20Gap%3A%20Investigating%20Approaches%20for%20Improving%20Cross-Modal%0A%20%20Alignment%20in%20CLIP&entry.906535625=Sedigheh%20Eslami%20and%20Gerard%20de%20Melo&entry.1292438233=%20%20Contrastive%20Language--Image%20Pre-training%20%28CLIP%29%20has%20manifested%20remarkable%0Aimprovements%20in%20zero-shot%20classification%20and%20cross-modal%20vision-language%20tasks.%0AYet%2C%20from%20a%20geometrical%20point%20of%20view%2C%20the%20CLIP%20embedding%20space%20has%20been%20found%0Ato%20have%20a%20pronounced%20modality%20gap.%20This%20gap%20renders%20the%20embedding%20space%20overly%0Asparse%20and%20disconnected%2C%20with%20different%20modalities%20being%20densely%20distributed%20in%0Adistinct%20subregions%20of%20the%20hypersphere.%20In%20this%20work%2C%20we%20aim%20at%20answering%20two%0Amain%20questions%3A%201.%20Does%20sharing%20the%20parameter%20space%20between%20the%20multi-modal%0Aencoders%20reduce%20the%20modality%20gap%3F%202.%20Can%20the%20gap%20be%20mitigated%20by%20pushing%20apart%0Athe%20uni-modal%20embeddings%20via%20intra-modality%20separation%3F%20We%20design%20AlignCLIP%2C%20in%0Aorder%20to%20answer%20these%20questions%20and%20show%20that%20answers%20to%20both%20questions%20are%0Apositive.%20Through%20extensive%20experiments%2C%20we%20show%20that%20AlignCLIP%20achieves%0Anoticeable%20enhancements%20in%20the%20cross-modal%20alignment%20of%20the%20embeddings%2C%20and%0Athereby%2C%20reduces%20the%20modality%20gap%2C%20while%20maintaining%20the%20performance%20across%0Aseveral%20downstream%20evaluations%2C%20such%20as%20zero-shot%20image%20classification%2C%0Azero-shot%20multi-modal%20retrieval%20and%20zero-shot%20semantic%20text%20similarity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17639v1&entry.124074799=Read"},
{"title": "Mamba24/8D: Enhancing Global Interaction in Point Clouds via State Space\n  Model", "author": "Zhuoyuan Li and Yubo Ai and Jiahao Lu and ChuXin Wang and Jiacheng Deng and Hanzhi Chang and Yanzhe Liang and Wenfei Yang and Shifeng Zhang and Tianzhu Zhang", "abstract": "  Transformers have demonstrated impressive results for 3D point cloud semantic\nsegmentation. However, the quadratic complexity of transformer makes\ncomputation cost high, limiting the number of points that can be processed\nsimultaneously and impeding the modeling of long-range dependencies. Drawing\ninspiration from the great potential of recent state space models (SSM) for\nlong sequence modeling, we introduce Mamba, a SSM-based architecture, to the\npoint cloud domain and propose Mamba24/8D, which has strong global modeling\ncapability under linear complexity. Specifically, to make disorderness of point\nclouds fit in with the causal nature of Mamba, we propose a multi-path\nserialization strategy applicable to point clouds. Besides, we propose the\nConvMamba block to compensate for the shortcomings of Mamba in modeling local\ngeometries and in unidirectional modeling. Mamba24/8D obtains state of the art\nresults on several 3D point cloud segmentation tasks, including ScanNet v2,\nScanNet200 and nuScenes, while its effectiveness is validated by extensive\nexperiments.\n", "link": "http://arxiv.org/abs/2406.17442v1", "date": "2024-06-25", "relevancy": 2.793, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5603}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5577}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba24/8D%3A%20Enhancing%20Global%20Interaction%20in%20Point%20Clouds%20via%20State%20Space%0A%20%20Model&body=Title%3A%20Mamba24/8D%3A%20Enhancing%20Global%20Interaction%20in%20Point%20Clouds%20via%20State%20Space%0A%20%20Model%0AAuthor%3A%20Zhuoyuan%20Li%20and%20Yubo%20Ai%20and%20Jiahao%20Lu%20and%20ChuXin%20Wang%20and%20Jiacheng%20Deng%20and%20Hanzhi%20Chang%20and%20Yanzhe%20Liang%20and%20Wenfei%20Yang%20and%20Shifeng%20Zhang%20and%20Tianzhu%20Zhang%0AAbstract%3A%20%20%20Transformers%20have%20demonstrated%20impressive%20results%20for%203D%20point%20cloud%20semantic%0Asegmentation.%20However%2C%20the%20quadratic%20complexity%20of%20transformer%20makes%0Acomputation%20cost%20high%2C%20limiting%20the%20number%20of%20points%20that%20can%20be%20processed%0Asimultaneously%20and%20impeding%20the%20modeling%20of%20long-range%20dependencies.%20Drawing%0Ainspiration%20from%20the%20great%20potential%20of%20recent%20state%20space%20models%20%28SSM%29%20for%0Along%20sequence%20modeling%2C%20we%20introduce%20Mamba%2C%20a%20SSM-based%20architecture%2C%20to%20the%0Apoint%20cloud%20domain%20and%20propose%20Mamba24/8D%2C%20which%20has%20strong%20global%20modeling%0Acapability%20under%20linear%20complexity.%20Specifically%2C%20to%20make%20disorderness%20of%20point%0Aclouds%20fit%20in%20with%20the%20causal%20nature%20of%20Mamba%2C%20we%20propose%20a%20multi-path%0Aserialization%20strategy%20applicable%20to%20point%20clouds.%20Besides%2C%20we%20propose%20the%0AConvMamba%20block%20to%20compensate%20for%20the%20shortcomings%20of%20Mamba%20in%20modeling%20local%0Ageometries%20and%20in%20unidirectional%20modeling.%20Mamba24/8D%20obtains%20state%20of%20the%20art%0Aresults%20on%20several%203D%20point%20cloud%20segmentation%20tasks%2C%20including%20ScanNet%20v2%2C%0AScanNet200%20and%20nuScenes%2C%20while%20its%20effectiveness%20is%20validated%20by%20extensive%0Aexperiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba24/8D%253A%2520Enhancing%2520Global%2520Interaction%2520in%2520Point%2520Clouds%2520via%2520State%2520Space%250A%2520%2520Model%26entry.906535625%3DZhuoyuan%2520Li%2520and%2520Yubo%2520Ai%2520and%2520Jiahao%2520Lu%2520and%2520ChuXin%2520Wang%2520and%2520Jiacheng%2520Deng%2520and%2520Hanzhi%2520Chang%2520and%2520Yanzhe%2520Liang%2520and%2520Wenfei%2520Yang%2520and%2520Shifeng%2520Zhang%2520and%2520Tianzhu%2520Zhang%26entry.1292438233%3D%2520%2520Transformers%2520have%2520demonstrated%2520impressive%2520results%2520for%25203D%2520point%2520cloud%2520semantic%250Asegmentation.%2520However%252C%2520the%2520quadratic%2520complexity%2520of%2520transformer%2520makes%250Acomputation%2520cost%2520high%252C%2520limiting%2520the%2520number%2520of%2520points%2520that%2520can%2520be%2520processed%250Asimultaneously%2520and%2520impeding%2520the%2520modeling%2520of%2520long-range%2520dependencies.%2520Drawing%250Ainspiration%2520from%2520the%2520great%2520potential%2520of%2520recent%2520state%2520space%2520models%2520%2528SSM%2529%2520for%250Along%2520sequence%2520modeling%252C%2520we%2520introduce%2520Mamba%252C%2520a%2520SSM-based%2520architecture%252C%2520to%2520the%250Apoint%2520cloud%2520domain%2520and%2520propose%2520Mamba24/8D%252C%2520which%2520has%2520strong%2520global%2520modeling%250Acapability%2520under%2520linear%2520complexity.%2520Specifically%252C%2520to%2520make%2520disorderness%2520of%2520point%250Aclouds%2520fit%2520in%2520with%2520the%2520causal%2520nature%2520of%2520Mamba%252C%2520we%2520propose%2520a%2520multi-path%250Aserialization%2520strategy%2520applicable%2520to%2520point%2520clouds.%2520Besides%252C%2520we%2520propose%2520the%250AConvMamba%2520block%2520to%2520compensate%2520for%2520the%2520shortcomings%2520of%2520Mamba%2520in%2520modeling%2520local%250Ageometries%2520and%2520in%2520unidirectional%2520modeling.%2520Mamba24/8D%2520obtains%2520state%2520of%2520the%2520art%250Aresults%2520on%2520several%25203D%2520point%2520cloud%2520segmentation%2520tasks%252C%2520including%2520ScanNet%2520v2%252C%250AScanNet200%2520and%2520nuScenes%252C%2520while%2520its%2520effectiveness%2520is%2520validated%2520by%2520extensive%250Aexperiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba24/8D%3A%20Enhancing%20Global%20Interaction%20in%20Point%20Clouds%20via%20State%20Space%0A%20%20Model&entry.906535625=Zhuoyuan%20Li%20and%20Yubo%20Ai%20and%20Jiahao%20Lu%20and%20ChuXin%20Wang%20and%20Jiacheng%20Deng%20and%20Hanzhi%20Chang%20and%20Yanzhe%20Liang%20and%20Wenfei%20Yang%20and%20Shifeng%20Zhang%20and%20Tianzhu%20Zhang&entry.1292438233=%20%20Transformers%20have%20demonstrated%20impressive%20results%20for%203D%20point%20cloud%20semantic%0Asegmentation.%20However%2C%20the%20quadratic%20complexity%20of%20transformer%20makes%0Acomputation%20cost%20high%2C%20limiting%20the%20number%20of%20points%20that%20can%20be%20processed%0Asimultaneously%20and%20impeding%20the%20modeling%20of%20long-range%20dependencies.%20Drawing%0Ainspiration%20from%20the%20great%20potential%20of%20recent%20state%20space%20models%20%28SSM%29%20for%0Along%20sequence%20modeling%2C%20we%20introduce%20Mamba%2C%20a%20SSM-based%20architecture%2C%20to%20the%0Apoint%20cloud%20domain%20and%20propose%20Mamba24/8D%2C%20which%20has%20strong%20global%20modeling%0Acapability%20under%20linear%20complexity.%20Specifically%2C%20to%20make%20disorderness%20of%20point%0Aclouds%20fit%20in%20with%20the%20causal%20nature%20of%20Mamba%2C%20we%20propose%20a%20multi-path%0Aserialization%20strategy%20applicable%20to%20point%20clouds.%20Besides%2C%20we%20propose%20the%0AConvMamba%20block%20to%20compensate%20for%20the%20shortcomings%20of%20Mamba%20in%20modeling%20local%0Ageometries%20and%20in%20unidirectional%20modeling.%20Mamba24/8D%20obtains%20state%20of%20the%20art%0Aresults%20on%20several%203D%20point%20cloud%20segmentation%20tasks%2C%20including%20ScanNet%20v2%2C%0AScanNet200%20and%20nuScenes%2C%20while%20its%20effectiveness%20is%20validated%20by%20extensive%0Aexperiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17442v1&entry.124074799=Read"},
{"title": "MaPa: Text-driven Photorealistic Material Painting for 3D Shapes", "author": "Shangzhan Zhang and Sida Peng and Tao Xu and Yuanbo Yang and Tianrun Chen and Nan Xue and Yujun Shen and Hujun Bao and Ruizhen Hu and Xiaowei Zhou", "abstract": "  This paper aims to generate materials for 3D meshes from text descriptions.\nUnlike existing methods that synthesize texture maps, we propose to generate\nsegment-wise procedural material graphs as the appearance representation, which\nsupports high-quality rendering and provides substantial flexibility in\nediting. Instead of relying on extensive paired data, i.e., 3D meshes with\nmaterial graphs and corresponding text descriptions, to train a material graph\ngenerative model, we propose to leverage the pre-trained 2D diffusion model as\na bridge to connect the text and material graphs. Specifically, our approach\ndecomposes a shape into a set of segments and designs a segment-controlled\ndiffusion model to synthesize 2D images that are aligned with mesh parts. Based\non generated images, we initialize parameters of material graphs and fine-tune\nthem through the differentiable rendering module to produce materials in\naccordance with the textual description. Extensive experiments demonstrate the\nsuperior performance of our framework in photorealism, resolution, and\neditability over existing methods. Project page: https://zju3dv.github.io/MaPa\n", "link": "http://arxiv.org/abs/2404.17569v2", "date": "2024-06-25", "relevancy": 2.7532, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5647}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5436}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaPa%3A%20Text-driven%20Photorealistic%20Material%20Painting%20for%203D%20Shapes&body=Title%3A%20MaPa%3A%20Text-driven%20Photorealistic%20Material%20Painting%20for%203D%20Shapes%0AAuthor%3A%20Shangzhan%20Zhang%20and%20Sida%20Peng%20and%20Tao%20Xu%20and%20Yuanbo%20Yang%20and%20Tianrun%20Chen%20and%20Nan%20Xue%20and%20Yujun%20Shen%20and%20Hujun%20Bao%20and%20Ruizhen%20Hu%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20generate%20materials%20for%203D%20meshes%20from%20text%20descriptions.%0AUnlike%20existing%20methods%20that%20synthesize%20texture%20maps%2C%20we%20propose%20to%20generate%0Asegment-wise%20procedural%20material%20graphs%20as%20the%20appearance%20representation%2C%20which%0Asupports%20high-quality%20rendering%20and%20provides%20substantial%20flexibility%20in%0Aediting.%20Instead%20of%20relying%20on%20extensive%20paired%20data%2C%20i.e.%2C%203D%20meshes%20with%0Amaterial%20graphs%20and%20corresponding%20text%20descriptions%2C%20to%20train%20a%20material%20graph%0Agenerative%20model%2C%20we%20propose%20to%20leverage%20the%20pre-trained%202D%20diffusion%20model%20as%0Aa%20bridge%20to%20connect%20the%20text%20and%20material%20graphs.%20Specifically%2C%20our%20approach%0Adecomposes%20a%20shape%20into%20a%20set%20of%20segments%20and%20designs%20a%20segment-controlled%0Adiffusion%20model%20to%20synthesize%202D%20images%20that%20are%20aligned%20with%20mesh%20parts.%20Based%0Aon%20generated%20images%2C%20we%20initialize%20parameters%20of%20material%20graphs%20and%20fine-tune%0Athem%20through%20the%20differentiable%20rendering%20module%20to%20produce%20materials%20in%0Aaccordance%20with%20the%20textual%20description.%20Extensive%20experiments%20demonstrate%20the%0Asuperior%20performance%20of%20our%20framework%20in%20photorealism%2C%20resolution%2C%20and%0Aeditability%20over%20existing%20methods.%20Project%20page%3A%20https%3A//zju3dv.github.io/MaPa%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaPa%253A%2520Text-driven%2520Photorealistic%2520Material%2520Painting%2520for%25203D%2520Shapes%26entry.906535625%3DShangzhan%2520Zhang%2520and%2520Sida%2520Peng%2520and%2520Tao%2520Xu%2520and%2520Yuanbo%2520Yang%2520and%2520Tianrun%2520Chen%2520and%2520Nan%2520Xue%2520and%2520Yujun%2520Shen%2520and%2520Hujun%2520Bao%2520and%2520Ruizhen%2520Hu%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520generate%2520materials%2520for%25203D%2520meshes%2520from%2520text%2520descriptions.%250AUnlike%2520existing%2520methods%2520that%2520synthesize%2520texture%2520maps%252C%2520we%2520propose%2520to%2520generate%250Asegment-wise%2520procedural%2520material%2520graphs%2520as%2520the%2520appearance%2520representation%252C%2520which%250Asupports%2520high-quality%2520rendering%2520and%2520provides%2520substantial%2520flexibility%2520in%250Aediting.%2520Instead%2520of%2520relying%2520on%2520extensive%2520paired%2520data%252C%2520i.e.%252C%25203D%2520meshes%2520with%250Amaterial%2520graphs%2520and%2520corresponding%2520text%2520descriptions%252C%2520to%2520train%2520a%2520material%2520graph%250Agenerative%2520model%252C%2520we%2520propose%2520to%2520leverage%2520the%2520pre-trained%25202D%2520diffusion%2520model%2520as%250Aa%2520bridge%2520to%2520connect%2520the%2520text%2520and%2520material%2520graphs.%2520Specifically%252C%2520our%2520approach%250Adecomposes%2520a%2520shape%2520into%2520a%2520set%2520of%2520segments%2520and%2520designs%2520a%2520segment-controlled%250Adiffusion%2520model%2520to%2520synthesize%25202D%2520images%2520that%2520are%2520aligned%2520with%2520mesh%2520parts.%2520Based%250Aon%2520generated%2520images%252C%2520we%2520initialize%2520parameters%2520of%2520material%2520graphs%2520and%2520fine-tune%250Athem%2520through%2520the%2520differentiable%2520rendering%2520module%2520to%2520produce%2520materials%2520in%250Aaccordance%2520with%2520the%2520textual%2520description.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520our%2520framework%2520in%2520photorealism%252C%2520resolution%252C%2520and%250Aeditability%2520over%2520existing%2520methods.%2520Project%2520page%253A%2520https%253A//zju3dv.github.io/MaPa%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaPa%3A%20Text-driven%20Photorealistic%20Material%20Painting%20for%203D%20Shapes&entry.906535625=Shangzhan%20Zhang%20and%20Sida%20Peng%20and%20Tao%20Xu%20and%20Yuanbo%20Yang%20and%20Tianrun%20Chen%20and%20Nan%20Xue%20and%20Yujun%20Shen%20and%20Hujun%20Bao%20and%20Ruizhen%20Hu%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20This%20paper%20aims%20to%20generate%20materials%20for%203D%20meshes%20from%20text%20descriptions.%0AUnlike%20existing%20methods%20that%20synthesize%20texture%20maps%2C%20we%20propose%20to%20generate%0Asegment-wise%20procedural%20material%20graphs%20as%20the%20appearance%20representation%2C%20which%0Asupports%20high-quality%20rendering%20and%20provides%20substantial%20flexibility%20in%0Aediting.%20Instead%20of%20relying%20on%20extensive%20paired%20data%2C%20i.e.%2C%203D%20meshes%20with%0Amaterial%20graphs%20and%20corresponding%20text%20descriptions%2C%20to%20train%20a%20material%20graph%0Agenerative%20model%2C%20we%20propose%20to%20leverage%20the%20pre-trained%202D%20diffusion%20model%20as%0Aa%20bridge%20to%20connect%20the%20text%20and%20material%20graphs.%20Specifically%2C%20our%20approach%0Adecomposes%20a%20shape%20into%20a%20set%20of%20segments%20and%20designs%20a%20segment-controlled%0Adiffusion%20model%20to%20synthesize%202D%20images%20that%20are%20aligned%20with%20mesh%20parts.%20Based%0Aon%20generated%20images%2C%20we%20initialize%20parameters%20of%20material%20graphs%20and%20fine-tune%0Athem%20through%20the%20differentiable%20rendering%20module%20to%20produce%20materials%20in%0Aaccordance%20with%20the%20textual%20description.%20Extensive%20experiments%20demonstrate%20the%0Asuperior%20performance%20of%20our%20framework%20in%20photorealism%2C%20resolution%2C%20and%0Aeditability%20over%20existing%20methods.%20Project%20page%3A%20https%3A//zju3dv.github.io/MaPa%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17569v2&entry.124074799=Read"},
{"title": "Test-Time Generative Augmentation for Medical Image Segmentation", "author": "Xiao Ma and Yuhui Tao and Yuhan Zhang and Zexuan Ji and Yizhe Zhang and Qiang Chen", "abstract": "  In this paper, we propose a novel approach to enhance medical image\nsegmentation during test time. Instead of employing hand-crafted transforms or\nfunctions on the input test image to create multiple views for test-time\naugmentation, we advocate for the utilization of an advanced domain-fine-tuned\ngenerative model (GM), e.g., stable diffusion (SD), for test-time augmentation.\nGiven that the GM has been trained to comprehend and encapsulate comprehensive\ndomain data knowledge, it is superior than segmentation models in terms of\nrepresenting the data characteristics and distribution. Hence, by integrating\nthe GM into test-time augmentation, we can effectively generate multiple views\nof a given test sample, aligning with the content and appearance\ncharacteristics of the sample and the related local data distribution. This\napproach renders the augmentation process more adaptable and resilient compared\nto conventional handcrafted transforms. Comprehensive experiments conducted\nacross three medical image segmentation tasks (nine datasets) demonstrate the\nefficacy and versatility of the proposed TTGA in enhancing segmentation\noutcomes. Moreover, TTGA significantly improves pixel-wise error estimation,\nthereby facilitating the deployment of a more reliable segmentation system.\nCode will be released at: https://github.com/maxiao0234/TTGA.\n", "link": "http://arxiv.org/abs/2406.17608v1", "date": "2024-06-25", "relevancy": 2.6901, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5398}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5396}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Generative%20Augmentation%20for%20Medical%20Image%20Segmentation&body=Title%3A%20Test-Time%20Generative%20Augmentation%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Xiao%20Ma%20and%20Yuhui%20Tao%20and%20Yuhan%20Zhang%20and%20Zexuan%20Ji%20and%20Yizhe%20Zhang%20and%20Qiang%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20enhance%20medical%20image%0Asegmentation%20during%20test%20time.%20Instead%20of%20employing%20hand-crafted%20transforms%20or%0Afunctions%20on%20the%20input%20test%20image%20to%20create%20multiple%20views%20for%20test-time%0Aaugmentation%2C%20we%20advocate%20for%20the%20utilization%20of%20an%20advanced%20domain-fine-tuned%0Agenerative%20model%20%28GM%29%2C%20e.g.%2C%20stable%20diffusion%20%28SD%29%2C%20for%20test-time%20augmentation.%0AGiven%20that%20the%20GM%20has%20been%20trained%20to%20comprehend%20and%20encapsulate%20comprehensive%0Adomain%20data%20knowledge%2C%20it%20is%20superior%20than%20segmentation%20models%20in%20terms%20of%0Arepresenting%20the%20data%20characteristics%20and%20distribution.%20Hence%2C%20by%20integrating%0Athe%20GM%20into%20test-time%20augmentation%2C%20we%20can%20effectively%20generate%20multiple%20views%0Aof%20a%20given%20test%20sample%2C%20aligning%20with%20the%20content%20and%20appearance%0Acharacteristics%20of%20the%20sample%20and%20the%20related%20local%20data%20distribution.%20This%0Aapproach%20renders%20the%20augmentation%20process%20more%20adaptable%20and%20resilient%20compared%0Ato%20conventional%20handcrafted%20transforms.%20Comprehensive%20experiments%20conducted%0Aacross%20three%20medical%20image%20segmentation%20tasks%20%28nine%20datasets%29%20demonstrate%20the%0Aefficacy%20and%20versatility%20of%20the%20proposed%20TTGA%20in%20enhancing%20segmentation%0Aoutcomes.%20Moreover%2C%20TTGA%20significantly%20improves%20pixel-wise%20error%20estimation%2C%0Athereby%20facilitating%20the%20deployment%20of%20a%20more%20reliable%20segmentation%20system.%0ACode%20will%20be%20released%20at%3A%20https%3A//github.com/maxiao0234/TTGA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Generative%2520Augmentation%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DXiao%2520Ma%2520and%2520Yuhui%2520Tao%2520and%2520Yuhan%2520Zhang%2520and%2520Zexuan%2520Ji%2520and%2520Yizhe%2520Zhang%2520and%2520Qiang%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520enhance%2520medical%2520image%250Asegmentation%2520during%2520test%2520time.%2520Instead%2520of%2520employing%2520hand-crafted%2520transforms%2520or%250Afunctions%2520on%2520the%2520input%2520test%2520image%2520to%2520create%2520multiple%2520views%2520for%2520test-time%250Aaugmentation%252C%2520we%2520advocate%2520for%2520the%2520utilization%2520of%2520an%2520advanced%2520domain-fine-tuned%250Agenerative%2520model%2520%2528GM%2529%252C%2520e.g.%252C%2520stable%2520diffusion%2520%2528SD%2529%252C%2520for%2520test-time%2520augmentation.%250AGiven%2520that%2520the%2520GM%2520has%2520been%2520trained%2520to%2520comprehend%2520and%2520encapsulate%2520comprehensive%250Adomain%2520data%2520knowledge%252C%2520it%2520is%2520superior%2520than%2520segmentation%2520models%2520in%2520terms%2520of%250Arepresenting%2520the%2520data%2520characteristics%2520and%2520distribution.%2520Hence%252C%2520by%2520integrating%250Athe%2520GM%2520into%2520test-time%2520augmentation%252C%2520we%2520can%2520effectively%2520generate%2520multiple%2520views%250Aof%2520a%2520given%2520test%2520sample%252C%2520aligning%2520with%2520the%2520content%2520and%2520appearance%250Acharacteristics%2520of%2520the%2520sample%2520and%2520the%2520related%2520local%2520data%2520distribution.%2520This%250Aapproach%2520renders%2520the%2520augmentation%2520process%2520more%2520adaptable%2520and%2520resilient%2520compared%250Ato%2520conventional%2520handcrafted%2520transforms.%2520Comprehensive%2520experiments%2520conducted%250Aacross%2520three%2520medical%2520image%2520segmentation%2520tasks%2520%2528nine%2520datasets%2529%2520demonstrate%2520the%250Aefficacy%2520and%2520versatility%2520of%2520the%2520proposed%2520TTGA%2520in%2520enhancing%2520segmentation%250Aoutcomes.%2520Moreover%252C%2520TTGA%2520significantly%2520improves%2520pixel-wise%2520error%2520estimation%252C%250Athereby%2520facilitating%2520the%2520deployment%2520of%2520a%2520more%2520reliable%2520segmentation%2520system.%250ACode%2520will%2520be%2520released%2520at%253A%2520https%253A//github.com/maxiao0234/TTGA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Generative%20Augmentation%20for%20Medical%20Image%20Segmentation&entry.906535625=Xiao%20Ma%20and%20Yuhui%20Tao%20and%20Yuhan%20Zhang%20and%20Zexuan%20Ji%20and%20Yizhe%20Zhang%20and%20Qiang%20Chen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20enhance%20medical%20image%0Asegmentation%20during%20test%20time.%20Instead%20of%20employing%20hand-crafted%20transforms%20or%0Afunctions%20on%20the%20input%20test%20image%20to%20create%20multiple%20views%20for%20test-time%0Aaugmentation%2C%20we%20advocate%20for%20the%20utilization%20of%20an%20advanced%20domain-fine-tuned%0Agenerative%20model%20%28GM%29%2C%20e.g.%2C%20stable%20diffusion%20%28SD%29%2C%20for%20test-time%20augmentation.%0AGiven%20that%20the%20GM%20has%20been%20trained%20to%20comprehend%20and%20encapsulate%20comprehensive%0Adomain%20data%20knowledge%2C%20it%20is%20superior%20than%20segmentation%20models%20in%20terms%20of%0Arepresenting%20the%20data%20characteristics%20and%20distribution.%20Hence%2C%20by%20integrating%0Athe%20GM%20into%20test-time%20augmentation%2C%20we%20can%20effectively%20generate%20multiple%20views%0Aof%20a%20given%20test%20sample%2C%20aligning%20with%20the%20content%20and%20appearance%0Acharacteristics%20of%20the%20sample%20and%20the%20related%20local%20data%20distribution.%20This%0Aapproach%20renders%20the%20augmentation%20process%20more%20adaptable%20and%20resilient%20compared%0Ato%20conventional%20handcrafted%20transforms.%20Comprehensive%20experiments%20conducted%0Aacross%20three%20medical%20image%20segmentation%20tasks%20%28nine%20datasets%29%20demonstrate%20the%0Aefficacy%20and%20versatility%20of%20the%20proposed%20TTGA%20in%20enhancing%20segmentation%0Aoutcomes.%20Moreover%2C%20TTGA%20significantly%20improves%20pixel-wise%20error%20estimation%2C%0Athereby%20facilitating%20the%20deployment%20of%20a%20more%20reliable%20segmentation%20system.%0ACode%20will%20be%20released%20at%3A%20https%3A//github.com/maxiao0234/TTGA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17608v1&entry.124074799=Read"},
{"title": "Diverse Part Synthesis for 3D Shape Creation", "author": "Yanran Guan and Oliver van Kaick", "abstract": "  Methods that use neural networks for synthesizing 3D shapes in the form of a\npart-based representation have been introduced over the last few years. These\nmethods represent shapes as a graph or hierarchy of parts and enable a variety\nof applications such as shape sampling and reconstruction. However, current\nmethods do not allow easily regenerating individual shape parts according to\nuser preferences. In this paper, we investigate techniques that allow the user\nto generate multiple, diverse suggestions for individual parts. Specifically,\nwe experiment with multimodal deep generative models that allow sampling\ndiverse suggestions for shape parts and focus on models which have not been\nconsidered in previous work on shape synthesis. To provide a comparative study\nof these techniques, we introduce a method for synthesizing 3D shapes in a\npart-based representation and evaluate all the part suggestion techniques\nwithin this synthesis method. In our method, which is inspired by previous\nwork, shapes are represented as a set of parts in the form of implicit\nfunctions which are then positioned in space to form the final shape. Synthesis\nin this representation is enabled by a neural network architecture based on an\nimplicit decoder and a spatial transformer. We compare the various multimodal\ngenerative models by evaluating their performance in generating part\nsuggestions. Our contribution is to show with qualitative and quantitative\nevaluations which of the new techniques for multimodal part generation perform\nthe best and that a synthesis method based on the top-performing techniques\nallows the user to more finely control the parts that are generated in the 3D\nshapes while maintaining high shape fidelity when reconstructing shapes.\n", "link": "http://arxiv.org/abs/2401.09384v2", "date": "2024-06-25", "relevancy": 2.6788, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5359}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5359}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diverse%20Part%20Synthesis%20for%203D%20Shape%20Creation&body=Title%3A%20Diverse%20Part%20Synthesis%20for%203D%20Shape%20Creation%0AAuthor%3A%20Yanran%20Guan%20and%20Oliver%20van%20Kaick%0AAbstract%3A%20%20%20Methods%20that%20use%20neural%20networks%20for%20synthesizing%203D%20shapes%20in%20the%20form%20of%20a%0Apart-based%20representation%20have%20been%20introduced%20over%20the%20last%20few%20years.%20These%0Amethods%20represent%20shapes%20as%20a%20graph%20or%20hierarchy%20of%20parts%20and%20enable%20a%20variety%0Aof%20applications%20such%20as%20shape%20sampling%20and%20reconstruction.%20However%2C%20current%0Amethods%20do%20not%20allow%20easily%20regenerating%20individual%20shape%20parts%20according%20to%0Auser%20preferences.%20In%20this%20paper%2C%20we%20investigate%20techniques%20that%20allow%20the%20user%0Ato%20generate%20multiple%2C%20diverse%20suggestions%20for%20individual%20parts.%20Specifically%2C%0Awe%20experiment%20with%20multimodal%20deep%20generative%20models%20that%20allow%20sampling%0Adiverse%20suggestions%20for%20shape%20parts%20and%20focus%20on%20models%20which%20have%20not%20been%0Aconsidered%20in%20previous%20work%20on%20shape%20synthesis.%20To%20provide%20a%20comparative%20study%0Aof%20these%20techniques%2C%20we%20introduce%20a%20method%20for%20synthesizing%203D%20shapes%20in%20a%0Apart-based%20representation%20and%20evaluate%20all%20the%20part%20suggestion%20techniques%0Awithin%20this%20synthesis%20method.%20In%20our%20method%2C%20which%20is%20inspired%20by%20previous%0Awork%2C%20shapes%20are%20represented%20as%20a%20set%20of%20parts%20in%20the%20form%20of%20implicit%0Afunctions%20which%20are%20then%20positioned%20in%20space%20to%20form%20the%20final%20shape.%20Synthesis%0Ain%20this%20representation%20is%20enabled%20by%20a%20neural%20network%20architecture%20based%20on%20an%0Aimplicit%20decoder%20and%20a%20spatial%20transformer.%20We%20compare%20the%20various%20multimodal%0Agenerative%20models%20by%20evaluating%20their%20performance%20in%20generating%20part%0Asuggestions.%20Our%20contribution%20is%20to%20show%20with%20qualitative%20and%20quantitative%0Aevaluations%20which%20of%20the%20new%20techniques%20for%20multimodal%20part%20generation%20perform%0Athe%20best%20and%20that%20a%20synthesis%20method%20based%20on%20the%20top-performing%20techniques%0Aallows%20the%20user%20to%20more%20finely%20control%20the%20parts%20that%20are%20generated%20in%20the%203D%0Ashapes%20while%20maintaining%20high%20shape%20fidelity%20when%20reconstructing%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09384v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverse%2520Part%2520Synthesis%2520for%25203D%2520Shape%2520Creation%26entry.906535625%3DYanran%2520Guan%2520and%2520Oliver%2520van%2520Kaick%26entry.1292438233%3D%2520%2520Methods%2520that%2520use%2520neural%2520networks%2520for%2520synthesizing%25203D%2520shapes%2520in%2520the%2520form%2520of%2520a%250Apart-based%2520representation%2520have%2520been%2520introduced%2520over%2520the%2520last%2520few%2520years.%2520These%250Amethods%2520represent%2520shapes%2520as%2520a%2520graph%2520or%2520hierarchy%2520of%2520parts%2520and%2520enable%2520a%2520variety%250Aof%2520applications%2520such%2520as%2520shape%2520sampling%2520and%2520reconstruction.%2520However%252C%2520current%250Amethods%2520do%2520not%2520allow%2520easily%2520regenerating%2520individual%2520shape%2520parts%2520according%2520to%250Auser%2520preferences.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520techniques%2520that%2520allow%2520the%2520user%250Ato%2520generate%2520multiple%252C%2520diverse%2520suggestions%2520for%2520individual%2520parts.%2520Specifically%252C%250Awe%2520experiment%2520with%2520multimodal%2520deep%2520generative%2520models%2520that%2520allow%2520sampling%250Adiverse%2520suggestions%2520for%2520shape%2520parts%2520and%2520focus%2520on%2520models%2520which%2520have%2520not%2520been%250Aconsidered%2520in%2520previous%2520work%2520on%2520shape%2520synthesis.%2520To%2520provide%2520a%2520comparative%2520study%250Aof%2520these%2520techniques%252C%2520we%2520introduce%2520a%2520method%2520for%2520synthesizing%25203D%2520shapes%2520in%2520a%250Apart-based%2520representation%2520and%2520evaluate%2520all%2520the%2520part%2520suggestion%2520techniques%250Awithin%2520this%2520synthesis%2520method.%2520In%2520our%2520method%252C%2520which%2520is%2520inspired%2520by%2520previous%250Awork%252C%2520shapes%2520are%2520represented%2520as%2520a%2520set%2520of%2520parts%2520in%2520the%2520form%2520of%2520implicit%250Afunctions%2520which%2520are%2520then%2520positioned%2520in%2520space%2520to%2520form%2520the%2520final%2520shape.%2520Synthesis%250Ain%2520this%2520representation%2520is%2520enabled%2520by%2520a%2520neural%2520network%2520architecture%2520based%2520on%2520an%250Aimplicit%2520decoder%2520and%2520a%2520spatial%2520transformer.%2520We%2520compare%2520the%2520various%2520multimodal%250Agenerative%2520models%2520by%2520evaluating%2520their%2520performance%2520in%2520generating%2520part%250Asuggestions.%2520Our%2520contribution%2520is%2520to%2520show%2520with%2520qualitative%2520and%2520quantitative%250Aevaluations%2520which%2520of%2520the%2520new%2520techniques%2520for%2520multimodal%2520part%2520generation%2520perform%250Athe%2520best%2520and%2520that%2520a%2520synthesis%2520method%2520based%2520on%2520the%2520top-performing%2520techniques%250Aallows%2520the%2520user%2520to%2520more%2520finely%2520control%2520the%2520parts%2520that%2520are%2520generated%2520in%2520the%25203D%250Ashapes%2520while%2520maintaining%2520high%2520shape%2520fidelity%2520when%2520reconstructing%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09384v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diverse%20Part%20Synthesis%20for%203D%20Shape%20Creation&entry.906535625=Yanran%20Guan%20and%20Oliver%20van%20Kaick&entry.1292438233=%20%20Methods%20that%20use%20neural%20networks%20for%20synthesizing%203D%20shapes%20in%20the%20form%20of%20a%0Apart-based%20representation%20have%20been%20introduced%20over%20the%20last%20few%20years.%20These%0Amethods%20represent%20shapes%20as%20a%20graph%20or%20hierarchy%20of%20parts%20and%20enable%20a%20variety%0Aof%20applications%20such%20as%20shape%20sampling%20and%20reconstruction.%20However%2C%20current%0Amethods%20do%20not%20allow%20easily%20regenerating%20individual%20shape%20parts%20according%20to%0Auser%20preferences.%20In%20this%20paper%2C%20we%20investigate%20techniques%20that%20allow%20the%20user%0Ato%20generate%20multiple%2C%20diverse%20suggestions%20for%20individual%20parts.%20Specifically%2C%0Awe%20experiment%20with%20multimodal%20deep%20generative%20models%20that%20allow%20sampling%0Adiverse%20suggestions%20for%20shape%20parts%20and%20focus%20on%20models%20which%20have%20not%20been%0Aconsidered%20in%20previous%20work%20on%20shape%20synthesis.%20To%20provide%20a%20comparative%20study%0Aof%20these%20techniques%2C%20we%20introduce%20a%20method%20for%20synthesizing%203D%20shapes%20in%20a%0Apart-based%20representation%20and%20evaluate%20all%20the%20part%20suggestion%20techniques%0Awithin%20this%20synthesis%20method.%20In%20our%20method%2C%20which%20is%20inspired%20by%20previous%0Awork%2C%20shapes%20are%20represented%20as%20a%20set%20of%20parts%20in%20the%20form%20of%20implicit%0Afunctions%20which%20are%20then%20positioned%20in%20space%20to%20form%20the%20final%20shape.%20Synthesis%0Ain%20this%20representation%20is%20enabled%20by%20a%20neural%20network%20architecture%20based%20on%20an%0Aimplicit%20decoder%20and%20a%20spatial%20transformer.%20We%20compare%20the%20various%20multimodal%0Agenerative%20models%20by%20evaluating%20their%20performance%20in%20generating%20part%0Asuggestions.%20Our%20contribution%20is%20to%20show%20with%20qualitative%20and%20quantitative%0Aevaluations%20which%20of%20the%20new%20techniques%20for%20multimodal%20part%20generation%20perform%0Athe%20best%20and%20that%20a%20synthesis%20method%20based%20on%20the%20top-performing%20techniques%0Aallows%20the%20user%20to%20more%20finely%20control%20the%20parts%20that%20are%20generated%20in%20the%203D%0Ashapes%20while%20maintaining%20high%20shape%20fidelity%20when%20reconstructing%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09384v2&entry.124074799=Read"},
{"title": "Local-to-Global Cross-Modal Attention-Aware Fusion for HSI-X Semantic\n  Segmentation", "author": "Xuming Zhang and Naoto Yokoya and Xingfa Gu and Qingjiu Tian and Lorenzo Bruzzone", "abstract": "  Hyperspectral image (HSI) classification has recently reached its performance\nbottleneck. Multimodal data fusion is emerging as a promising approach to\novercome this bottleneck by providing rich complementary information from the\nsupplementary modality (X-modality). However, achieving comprehensive\ncross-modal interaction and fusion that can be generalized across different\nsensing modalities is challenging due to the disparity in imaging sensors,\nresolution, and content of different modalities. In this study, we propose a\nLocal-to-Global Cross-modal Attention-aware Fusion (LoGoCAF) framework for\nHSI-X classification that jointly considers efficiency, accuracy, and\ngeneralizability. LoGoCAF adopts a pixel-to-pixel two-branch semantic\nsegmentation architecture to learn information from HSI and X modalities. The\npipeline of LoGoCAF consists of a local-to-global encoder and a lightweight\nmultilayer perceptron (MLP) decoder. In the encoder, convolutions are used to\nencode local and high-resolution fine details in shallow layers, while\ntransformers are used to integrate global and low-resolution coarse features in\ndeeper layers. The MLP decoder aggregates information from the encoder for\nfeature fusion and prediction. In particular, two cross-modality modules, the\nfeature enhancement module (FEM) and the feature interaction and fusion module\n(FIFM), are introduced in each encoder stage. The FEM is used to enhance\ncomplementary information by combining the feature from the other modality\nacross direction-aware, position-sensitive, and channel-wise dimensions. With\nthe enhanced features, the FIFM is designed to promote cross-modality\ninformation interaction and fusion for the final semantic prediction. Extensive\nexperiments demonstrate that our LoGoCAF achieves superior performance and\ngeneralizes well. The code will be made publicly available.\n", "link": "http://arxiv.org/abs/2406.17679v1", "date": "2024-06-25", "relevancy": 2.6762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5484}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5312}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local-to-Global%20Cross-Modal%20Attention-Aware%20Fusion%20for%20HSI-X%20Semantic%0A%20%20Segmentation&body=Title%3A%20Local-to-Global%20Cross-Modal%20Attention-Aware%20Fusion%20for%20HSI-X%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Xuming%20Zhang%20and%20Naoto%20Yokoya%20and%20Xingfa%20Gu%20and%20Qingjiu%20Tian%20and%20Lorenzo%20Bruzzone%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20classification%20has%20recently%20reached%20its%20performance%0Abottleneck.%20Multimodal%20data%20fusion%20is%20emerging%20as%20a%20promising%20approach%20to%0Aovercome%20this%20bottleneck%20by%20providing%20rich%20complementary%20information%20from%20the%0Asupplementary%20modality%20%28X-modality%29.%20However%2C%20achieving%20comprehensive%0Across-modal%20interaction%20and%20fusion%20that%20can%20be%20generalized%20across%20different%0Asensing%20modalities%20is%20challenging%20due%20to%20the%20disparity%20in%20imaging%20sensors%2C%0Aresolution%2C%20and%20content%20of%20different%20modalities.%20In%20this%20study%2C%20we%20propose%20a%0ALocal-to-Global%20Cross-modal%20Attention-aware%20Fusion%20%28LoGoCAF%29%20framework%20for%0AHSI-X%20classification%20that%20jointly%20considers%20efficiency%2C%20accuracy%2C%20and%0Ageneralizability.%20LoGoCAF%20adopts%20a%20pixel-to-pixel%20two-branch%20semantic%0Asegmentation%20architecture%20to%20learn%20information%20from%20HSI%20and%20X%20modalities.%20The%0Apipeline%20of%20LoGoCAF%20consists%20of%20a%20local-to-global%20encoder%20and%20a%20lightweight%0Amultilayer%20perceptron%20%28MLP%29%20decoder.%20In%20the%20encoder%2C%20convolutions%20are%20used%20to%0Aencode%20local%20and%20high-resolution%20fine%20details%20in%20shallow%20layers%2C%20while%0Atransformers%20are%20used%20to%20integrate%20global%20and%20low-resolution%20coarse%20features%20in%0Adeeper%20layers.%20The%20MLP%20decoder%20aggregates%20information%20from%20the%20encoder%20for%0Afeature%20fusion%20and%20prediction.%20In%20particular%2C%20two%20cross-modality%20modules%2C%20the%0Afeature%20enhancement%20module%20%28FEM%29%20and%20the%20feature%20interaction%20and%20fusion%20module%0A%28FIFM%29%2C%20are%20introduced%20in%20each%20encoder%20stage.%20The%20FEM%20is%20used%20to%20enhance%0Acomplementary%20information%20by%20combining%20the%20feature%20from%20the%20other%20modality%0Aacross%20direction-aware%2C%20position-sensitive%2C%20and%20channel-wise%20dimensions.%20With%0Athe%20enhanced%20features%2C%20the%20FIFM%20is%20designed%20to%20promote%20cross-modality%0Ainformation%20interaction%20and%20fusion%20for%20the%20final%20semantic%20prediction.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20LoGoCAF%20achieves%20superior%20performance%20and%0Ageneralizes%20well.%20The%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal-to-Global%2520Cross-Modal%2520Attention-Aware%2520Fusion%2520for%2520HSI-X%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DXuming%2520Zhang%2520and%2520Naoto%2520Yokoya%2520and%2520Xingfa%2520Gu%2520and%2520Qingjiu%2520Tian%2520and%2520Lorenzo%2520Bruzzone%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520classification%2520has%2520recently%2520reached%2520its%2520performance%250Abottleneck.%2520Multimodal%2520data%2520fusion%2520is%2520emerging%2520as%2520a%2520promising%2520approach%2520to%250Aovercome%2520this%2520bottleneck%2520by%2520providing%2520rich%2520complementary%2520information%2520from%2520the%250Asupplementary%2520modality%2520%2528X-modality%2529.%2520However%252C%2520achieving%2520comprehensive%250Across-modal%2520interaction%2520and%2520fusion%2520that%2520can%2520be%2520generalized%2520across%2520different%250Asensing%2520modalities%2520is%2520challenging%2520due%2520to%2520the%2520disparity%2520in%2520imaging%2520sensors%252C%250Aresolution%252C%2520and%2520content%2520of%2520different%2520modalities.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250ALocal-to-Global%2520Cross-modal%2520Attention-aware%2520Fusion%2520%2528LoGoCAF%2529%2520framework%2520for%250AHSI-X%2520classification%2520that%2520jointly%2520considers%2520efficiency%252C%2520accuracy%252C%2520and%250Ageneralizability.%2520LoGoCAF%2520adopts%2520a%2520pixel-to-pixel%2520two-branch%2520semantic%250Asegmentation%2520architecture%2520to%2520learn%2520information%2520from%2520HSI%2520and%2520X%2520modalities.%2520The%250Apipeline%2520of%2520LoGoCAF%2520consists%2520of%2520a%2520local-to-global%2520encoder%2520and%2520a%2520lightweight%250Amultilayer%2520perceptron%2520%2528MLP%2529%2520decoder.%2520In%2520the%2520encoder%252C%2520convolutions%2520are%2520used%2520to%250Aencode%2520local%2520and%2520high-resolution%2520fine%2520details%2520in%2520shallow%2520layers%252C%2520while%250Atransformers%2520are%2520used%2520to%2520integrate%2520global%2520and%2520low-resolution%2520coarse%2520features%2520in%250Adeeper%2520layers.%2520The%2520MLP%2520decoder%2520aggregates%2520information%2520from%2520the%2520encoder%2520for%250Afeature%2520fusion%2520and%2520prediction.%2520In%2520particular%252C%2520two%2520cross-modality%2520modules%252C%2520the%250Afeature%2520enhancement%2520module%2520%2528FEM%2529%2520and%2520the%2520feature%2520interaction%2520and%2520fusion%2520module%250A%2528FIFM%2529%252C%2520are%2520introduced%2520in%2520each%2520encoder%2520stage.%2520The%2520FEM%2520is%2520used%2520to%2520enhance%250Acomplementary%2520information%2520by%2520combining%2520the%2520feature%2520from%2520the%2520other%2520modality%250Aacross%2520direction-aware%252C%2520position-sensitive%252C%2520and%2520channel-wise%2520dimensions.%2520With%250Athe%2520enhanced%2520features%252C%2520the%2520FIFM%2520is%2520designed%2520to%2520promote%2520cross-modality%250Ainformation%2520interaction%2520and%2520fusion%2520for%2520the%2520final%2520semantic%2520prediction.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520LoGoCAF%2520achieves%2520superior%2520performance%2520and%250Ageneralizes%2520well.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local-to-Global%20Cross-Modal%20Attention-Aware%20Fusion%20for%20HSI-X%20Semantic%0A%20%20Segmentation&entry.906535625=Xuming%20Zhang%20and%20Naoto%20Yokoya%20and%20Xingfa%20Gu%20and%20Qingjiu%20Tian%20and%20Lorenzo%20Bruzzone&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20classification%20has%20recently%20reached%20its%20performance%0Abottleneck.%20Multimodal%20data%20fusion%20is%20emerging%20as%20a%20promising%20approach%20to%0Aovercome%20this%20bottleneck%20by%20providing%20rich%20complementary%20information%20from%20the%0Asupplementary%20modality%20%28X-modality%29.%20However%2C%20achieving%20comprehensive%0Across-modal%20interaction%20and%20fusion%20that%20can%20be%20generalized%20across%20different%0Asensing%20modalities%20is%20challenging%20due%20to%20the%20disparity%20in%20imaging%20sensors%2C%0Aresolution%2C%20and%20content%20of%20different%20modalities.%20In%20this%20study%2C%20we%20propose%20a%0ALocal-to-Global%20Cross-modal%20Attention-aware%20Fusion%20%28LoGoCAF%29%20framework%20for%0AHSI-X%20classification%20that%20jointly%20considers%20efficiency%2C%20accuracy%2C%20and%0Ageneralizability.%20LoGoCAF%20adopts%20a%20pixel-to-pixel%20two-branch%20semantic%0Asegmentation%20architecture%20to%20learn%20information%20from%20HSI%20and%20X%20modalities.%20The%0Apipeline%20of%20LoGoCAF%20consists%20of%20a%20local-to-global%20encoder%20and%20a%20lightweight%0Amultilayer%20perceptron%20%28MLP%29%20decoder.%20In%20the%20encoder%2C%20convolutions%20are%20used%20to%0Aencode%20local%20and%20high-resolution%20fine%20details%20in%20shallow%20layers%2C%20while%0Atransformers%20are%20used%20to%20integrate%20global%20and%20low-resolution%20coarse%20features%20in%0Adeeper%20layers.%20The%20MLP%20decoder%20aggregates%20information%20from%20the%20encoder%20for%0Afeature%20fusion%20and%20prediction.%20In%20particular%2C%20two%20cross-modality%20modules%2C%20the%0Afeature%20enhancement%20module%20%28FEM%29%20and%20the%20feature%20interaction%20and%20fusion%20module%0A%28FIFM%29%2C%20are%20introduced%20in%20each%20encoder%20stage.%20The%20FEM%20is%20used%20to%20enhance%0Acomplementary%20information%20by%20combining%20the%20feature%20from%20the%20other%20modality%0Aacross%20direction-aware%2C%20position-sensitive%2C%20and%20channel-wise%20dimensions.%20With%0Athe%20enhanced%20features%2C%20the%20FIFM%20is%20designed%20to%20promote%20cross-modality%0Ainformation%20interaction%20and%20fusion%20for%20the%20final%20semantic%20prediction.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20LoGoCAF%20achieves%20superior%20performance%20and%0Ageneralizes%20well.%20The%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17679v1&entry.124074799=Read"},
{"title": "Minimal Interaction Edge Tuning: A New Paradigm for Visual Adaptation", "author": "Ningyuan Tang and Minghao Fu and Jianxin Wu", "abstract": "  The rapid scaling of large vision pretrained models makes fine-tuning tasks\nmore and more difficult on edge devices with low computational resources. We\nexplore a new visual adaptation paradigm called edge tuning, which treats large\npretrained models as standalone feature extractors that run on powerful cloud\nservers. The fine-tuning carries out on edge devices with small networks which\nrequire low computational resources. Existing methods that are potentially\nsuitable for our edge tuning paradigm are discussed. But, three major drawbacks\nhinder their application in edge tuning: low adaptation capability, large\nadapter network, and high information transfer overhead. To address these\nissues, we propose Minimal Interaction Edge Tuning, or MIET, which reveals that\nthe sum of intermediate features from pretrained models not only has minimal\ninformation transfer but also has high adaptation capability. With a\nlightweight attention-based adaptor network, MIET achieves information transfer\nefficiency, parameter efficiency, computational and memory efficiency, and at\nthe same time demonstrates competitive results on various visual adaptation\nbenchmarks.\n", "link": "http://arxiv.org/abs/2406.17559v1", "date": "2024-06-25", "relevancy": 2.6697, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5478}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5419}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimal%20Interaction%20Edge%20Tuning%3A%20A%20New%20Paradigm%20for%20Visual%20Adaptation&body=Title%3A%20Minimal%20Interaction%20Edge%20Tuning%3A%20A%20New%20Paradigm%20for%20Visual%20Adaptation%0AAuthor%3A%20Ningyuan%20Tang%20and%20Minghao%20Fu%20and%20Jianxin%20Wu%0AAbstract%3A%20%20%20The%20rapid%20scaling%20of%20large%20vision%20pretrained%20models%20makes%20fine-tuning%20tasks%0Amore%20and%20more%20difficult%20on%20edge%20devices%20with%20low%20computational%20resources.%20We%0Aexplore%20a%20new%20visual%20adaptation%20paradigm%20called%20edge%20tuning%2C%20which%20treats%20large%0Apretrained%20models%20as%20standalone%20feature%20extractors%20that%20run%20on%20powerful%20cloud%0Aservers.%20The%20fine-tuning%20carries%20out%20on%20edge%20devices%20with%20small%20networks%20which%0Arequire%20low%20computational%20resources.%20Existing%20methods%20that%20are%20potentially%0Asuitable%20for%20our%20edge%20tuning%20paradigm%20are%20discussed.%20But%2C%20three%20major%20drawbacks%0Ahinder%20their%20application%20in%20edge%20tuning%3A%20low%20adaptation%20capability%2C%20large%0Aadapter%20network%2C%20and%20high%20information%20transfer%20overhead.%20To%20address%20these%0Aissues%2C%20we%20propose%20Minimal%20Interaction%20Edge%20Tuning%2C%20or%20MIET%2C%20which%20reveals%20that%0Athe%20sum%20of%20intermediate%20features%20from%20pretrained%20models%20not%20only%20has%20minimal%0Ainformation%20transfer%20but%20also%20has%20high%20adaptation%20capability.%20With%20a%0Alightweight%20attention-based%20adaptor%20network%2C%20MIET%20achieves%20information%20transfer%0Aefficiency%2C%20parameter%20efficiency%2C%20computational%20and%20memory%20efficiency%2C%20and%20at%0Athe%20same%20time%20demonstrates%20competitive%20results%20on%20various%20visual%20adaptation%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimal%2520Interaction%2520Edge%2520Tuning%253A%2520A%2520New%2520Paradigm%2520for%2520Visual%2520Adaptation%26entry.906535625%3DNingyuan%2520Tang%2520and%2520Minghao%2520Fu%2520and%2520Jianxin%2520Wu%26entry.1292438233%3D%2520%2520The%2520rapid%2520scaling%2520of%2520large%2520vision%2520pretrained%2520models%2520makes%2520fine-tuning%2520tasks%250Amore%2520and%2520more%2520difficult%2520on%2520edge%2520devices%2520with%2520low%2520computational%2520resources.%2520We%250Aexplore%2520a%2520new%2520visual%2520adaptation%2520paradigm%2520called%2520edge%2520tuning%252C%2520which%2520treats%2520large%250Apretrained%2520models%2520as%2520standalone%2520feature%2520extractors%2520that%2520run%2520on%2520powerful%2520cloud%250Aservers.%2520The%2520fine-tuning%2520carries%2520out%2520on%2520edge%2520devices%2520with%2520small%2520networks%2520which%250Arequire%2520low%2520computational%2520resources.%2520Existing%2520methods%2520that%2520are%2520potentially%250Asuitable%2520for%2520our%2520edge%2520tuning%2520paradigm%2520are%2520discussed.%2520But%252C%2520three%2520major%2520drawbacks%250Ahinder%2520their%2520application%2520in%2520edge%2520tuning%253A%2520low%2520adaptation%2520capability%252C%2520large%250Aadapter%2520network%252C%2520and%2520high%2520information%2520transfer%2520overhead.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520Minimal%2520Interaction%2520Edge%2520Tuning%252C%2520or%2520MIET%252C%2520which%2520reveals%2520that%250Athe%2520sum%2520of%2520intermediate%2520features%2520from%2520pretrained%2520models%2520not%2520only%2520has%2520minimal%250Ainformation%2520transfer%2520but%2520also%2520has%2520high%2520adaptation%2520capability.%2520With%2520a%250Alightweight%2520attention-based%2520adaptor%2520network%252C%2520MIET%2520achieves%2520information%2520transfer%250Aefficiency%252C%2520parameter%2520efficiency%252C%2520computational%2520and%2520memory%2520efficiency%252C%2520and%2520at%250Athe%2520same%2520time%2520demonstrates%2520competitive%2520results%2520on%2520various%2520visual%2520adaptation%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimal%20Interaction%20Edge%20Tuning%3A%20A%20New%20Paradigm%20for%20Visual%20Adaptation&entry.906535625=Ningyuan%20Tang%20and%20Minghao%20Fu%20and%20Jianxin%20Wu&entry.1292438233=%20%20The%20rapid%20scaling%20of%20large%20vision%20pretrained%20models%20makes%20fine-tuning%20tasks%0Amore%20and%20more%20difficult%20on%20edge%20devices%20with%20low%20computational%20resources.%20We%0Aexplore%20a%20new%20visual%20adaptation%20paradigm%20called%20edge%20tuning%2C%20which%20treats%20large%0Apretrained%20models%20as%20standalone%20feature%20extractors%20that%20run%20on%20powerful%20cloud%0Aservers.%20The%20fine-tuning%20carries%20out%20on%20edge%20devices%20with%20small%20networks%20which%0Arequire%20low%20computational%20resources.%20Existing%20methods%20that%20are%20potentially%0Asuitable%20for%20our%20edge%20tuning%20paradigm%20are%20discussed.%20But%2C%20three%20major%20drawbacks%0Ahinder%20their%20application%20in%20edge%20tuning%3A%20low%20adaptation%20capability%2C%20large%0Aadapter%20network%2C%20and%20high%20information%20transfer%20overhead.%20To%20address%20these%0Aissues%2C%20we%20propose%20Minimal%20Interaction%20Edge%20Tuning%2C%20or%20MIET%2C%20which%20reveals%20that%0Athe%20sum%20of%20intermediate%20features%20from%20pretrained%20models%20not%20only%20has%20minimal%0Ainformation%20transfer%20but%20also%20has%20high%20adaptation%20capability.%20With%20a%0Alightweight%20attention-based%20adaptor%20network%2C%20MIET%20achieves%20information%20transfer%0Aefficiency%2C%20parameter%20efficiency%2C%20computational%20and%20memory%20efficiency%2C%20and%20at%0Athe%20same%20time%20demonstrates%20competitive%20results%20on%20various%20visual%20adaptation%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17559v1&entry.124074799=Read"},
{"title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation", "author": "Jianzong Wu and Xiangtai Li and Yanhong Zeng and Jiangning Zhang and Qianyu Zhou and Yining Li and Yunhai Tong and Kai Chen", "abstract": "  In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth\n", "link": "http://arxiv.org/abs/2406.17758v1", "date": "2024-06-25", "relevancy": 2.6494, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7542}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6669}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation&body=Title%3A%20MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation%0AAuthor%3A%20Jianzong%20Wu%20and%20Xiangtai%20Li%20and%20Yanhong%20Zeng%20and%20Jiangning%20Zhang%20and%20Qianyu%20Zhou%20and%20Yining%20Li%20and%20Yunhai%20Tong%20and%20Kai%20Chen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20MotionBooth%2C%20an%20innovative%20framework%20designed%20for%0Aanimating%20customized%20subjects%20with%20precise%20control%20over%20both%20object%20and%20camera%0Amovements.%20By%20leveraging%20a%20few%20images%20of%20a%20specific%20object%2C%20we%20efficiently%0Afine-tune%20a%20text-to-video%20model%20to%20capture%20the%20object%27s%20shape%20and%20attributes%0Aaccurately.%20Our%20approach%20presents%20subject%20region%20loss%20and%20video%20preservation%0Aloss%20to%20enhance%20the%20subject%27s%20learning%20performance%2C%20along%20with%20a%20subject%20token%0Across-attention%20loss%20to%20integrate%20the%20customized%20subject%20with%20motion%20control%0Asignals.%20Additionally%2C%20we%20propose%20training-free%20techniques%20for%20managing%20subject%0Aand%20camera%20motions%20during%20inference.%20In%20particular%2C%20we%20utilize%20cross-attention%0Amap%20manipulation%20to%20govern%20subject%20motion%20and%20introduce%20a%20novel%20latent%20shift%0Amodule%20for%20camera%20movement%20control%20as%20well.%20MotionBooth%20excels%20in%20preserving%0Athe%20appearance%20of%20subjects%20while%20simultaneously%20controlling%20the%20motions%20in%0Agenerated%20videos.%20Extensive%20quantitative%20and%20qualitative%20evaluations%0Ademonstrate%20the%20superiority%20and%20effectiveness%20of%20our%20method.%20Our%20project%20page%0Ais%20at%20https%3A//jianzongwu.github.io/projects/motionbooth%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionBooth%253A%2520Motion-Aware%2520Customized%2520Text-to-Video%2520Generation%26entry.906535625%3DJianzong%2520Wu%2520and%2520Xiangtai%2520Li%2520and%2520Yanhong%2520Zeng%2520and%2520Jiangning%2520Zhang%2520and%2520Qianyu%2520Zhou%2520and%2520Yining%2520Li%2520and%2520Yunhai%2520Tong%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520MotionBooth%252C%2520an%2520innovative%2520framework%2520designed%2520for%250Aanimating%2520customized%2520subjects%2520with%2520precise%2520control%2520over%2520both%2520object%2520and%2520camera%250Amovements.%2520By%2520leveraging%2520a%2520few%2520images%2520of%2520a%2520specific%2520object%252C%2520we%2520efficiently%250Afine-tune%2520a%2520text-to-video%2520model%2520to%2520capture%2520the%2520object%2527s%2520shape%2520and%2520attributes%250Aaccurately.%2520Our%2520approach%2520presents%2520subject%2520region%2520loss%2520and%2520video%2520preservation%250Aloss%2520to%2520enhance%2520the%2520subject%2527s%2520learning%2520performance%252C%2520along%2520with%2520a%2520subject%2520token%250Across-attention%2520loss%2520to%2520integrate%2520the%2520customized%2520subject%2520with%2520motion%2520control%250Asignals.%2520Additionally%252C%2520we%2520propose%2520training-free%2520techniques%2520for%2520managing%2520subject%250Aand%2520camera%2520motions%2520during%2520inference.%2520In%2520particular%252C%2520we%2520utilize%2520cross-attention%250Amap%2520manipulation%2520to%2520govern%2520subject%2520motion%2520and%2520introduce%2520a%2520novel%2520latent%2520shift%250Amodule%2520for%2520camera%2520movement%2520control%2520as%2520well.%2520MotionBooth%2520excels%2520in%2520preserving%250Athe%2520appearance%2520of%2520subjects%2520while%2520simultaneously%2520controlling%2520the%2520motions%2520in%250Agenerated%2520videos.%2520Extensive%2520quantitative%2520and%2520qualitative%2520evaluations%250Ademonstrate%2520the%2520superiority%2520and%2520effectiveness%2520of%2520our%2520method.%2520Our%2520project%2520page%250Ais%2520at%2520https%253A//jianzongwu.github.io/projects/motionbooth%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionBooth%3A%20Motion-Aware%20Customized%20Text-to-Video%20Generation&entry.906535625=Jianzong%20Wu%20and%20Xiangtai%20Li%20and%20Yanhong%20Zeng%20and%20Jiangning%20Zhang%20and%20Qianyu%20Zhou%20and%20Yining%20Li%20and%20Yunhai%20Tong%20and%20Kai%20Chen&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20MotionBooth%2C%20an%20innovative%20framework%20designed%20for%0Aanimating%20customized%20subjects%20with%20precise%20control%20over%20both%20object%20and%20camera%0Amovements.%20By%20leveraging%20a%20few%20images%20of%20a%20specific%20object%2C%20we%20efficiently%0Afine-tune%20a%20text-to-video%20model%20to%20capture%20the%20object%27s%20shape%20and%20attributes%0Aaccurately.%20Our%20approach%20presents%20subject%20region%20loss%20and%20video%20preservation%0Aloss%20to%20enhance%20the%20subject%27s%20learning%20performance%2C%20along%20with%20a%20subject%20token%0Across-attention%20loss%20to%20integrate%20the%20customized%20subject%20with%20motion%20control%0Asignals.%20Additionally%2C%20we%20propose%20training-free%20techniques%20for%20managing%20subject%0Aand%20camera%20motions%20during%20inference.%20In%20particular%2C%20we%20utilize%20cross-attention%0Amap%20manipulation%20to%20govern%20subject%20motion%20and%20introduce%20a%20novel%20latent%20shift%0Amodule%20for%20camera%20movement%20control%20as%20well.%20MotionBooth%20excels%20in%20preserving%0Athe%20appearance%20of%20subjects%20while%20simultaneously%20controlling%20the%20motions%20in%0Agenerated%20videos.%20Extensive%20quantitative%20and%20qualitative%20evaluations%0Ademonstrate%20the%20superiority%20and%20effectiveness%20of%20our%20method.%20Our%20project%20page%0Ais%20at%20https%3A//jianzongwu.github.io/projects/motionbooth%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17758v1&entry.124074799=Read"},
{"title": "Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs", "author": "Ashwinee Panda and Berivan Isik and Xiangyu Qi and Sanmi Koyejo and Tsachy Weissman and Prateek Mittal", "abstract": "  Existing methods for adapting large language models (LLMs) to new tasks are\nnot suited to multi-task adaptation because they modify all the model weights\n-- causing destructive interference between tasks. The resulting effects, such\nas catastrophic forgetting of earlier tasks, make it challenging to obtain good\nperformance on multiple tasks at the same time. To mitigate this, we propose\nLottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies\nand optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide\nrange of challenging tasks such as instruction following, reasoning, math, and\nsummarization. LoTA obtains better performance than full fine-tuning and\nlow-rank adaptation (LoRA), and maintains good performance even after training\non other tasks -- thus, avoiding catastrophic forgetting. By extracting and\nfine-tuning over lottery tickets (or sparse task vectors), LoTA also enables\nmodel merging over highly dissimilar tasks. Our code is made publicly available\nat https://github.com/kiddyboots216/lottery-ticket-adaptation.\n", "link": "http://arxiv.org/abs/2406.16797v2", "date": "2024-06-25", "relevancy": 2.5977, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5393}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5159}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs&body=Title%3A%20Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs%0AAuthor%3A%20Ashwinee%20Panda%20and%20Berivan%20Isik%20and%20Xiangyu%20Qi%20and%20Sanmi%20Koyejo%20and%20Tsachy%20Weissman%20and%20Prateek%20Mittal%0AAbstract%3A%20%20%20Existing%20methods%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20new%20tasks%20are%0Anot%20suited%20to%20multi-task%20adaptation%20because%20they%20modify%20all%20the%20model%20weights%0A--%20causing%20destructive%20interference%20between%20tasks.%20The%20resulting%20effects%2C%20such%0Aas%20catastrophic%20forgetting%20of%20earlier%20tasks%2C%20make%20it%20challenging%20to%20obtain%20good%0Aperformance%20on%20multiple%20tasks%20at%20the%20same%20time.%20To%20mitigate%20this%2C%20we%20propose%0ALottery%20Ticket%20Adaptation%20%28LoTA%29%2C%20a%20sparse%20adaptation%20method%20that%20identifies%0Aand%20optimizes%20only%20a%20sparse%20subnetwork%20of%20the%20model.%20We%20evaluate%20LoTA%20on%20a%20wide%0Arange%20of%20challenging%20tasks%20such%20as%20instruction%20following%2C%20reasoning%2C%20math%2C%20and%0Asummarization.%20LoTA%20obtains%20better%20performance%20than%20full%20fine-tuning%20and%0Alow-rank%20adaptation%20%28LoRA%29%2C%20and%20maintains%20good%20performance%20even%20after%20training%0Aon%20other%20tasks%20--%20thus%2C%20avoiding%20catastrophic%20forgetting.%20By%20extracting%20and%0Afine-tuning%20over%20lottery%20tickets%20%28or%20sparse%20task%20vectors%29%2C%20LoTA%20also%20enables%0Amodel%20merging%20over%20highly%20dissimilar%20tasks.%20Our%20code%20is%20made%20publicly%20available%0Aat%20https%3A//github.com/kiddyboots216/lottery-ticket-adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLottery%2520Ticket%2520Adaptation%253A%2520Mitigating%2520Destructive%2520Interference%2520in%2520LLMs%26entry.906535625%3DAshwinee%2520Panda%2520and%2520Berivan%2520Isik%2520and%2520Xiangyu%2520Qi%2520and%2520Sanmi%2520Koyejo%2520and%2520Tsachy%2520Weissman%2520and%2520Prateek%2520Mittal%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520adapting%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520new%2520tasks%2520are%250Anot%2520suited%2520to%2520multi-task%2520adaptation%2520because%2520they%2520modify%2520all%2520the%2520model%2520weights%250A--%2520causing%2520destructive%2520interference%2520between%2520tasks.%2520The%2520resulting%2520effects%252C%2520such%250Aas%2520catastrophic%2520forgetting%2520of%2520earlier%2520tasks%252C%2520make%2520it%2520challenging%2520to%2520obtain%2520good%250Aperformance%2520on%2520multiple%2520tasks%2520at%2520the%2520same%2520time.%2520To%2520mitigate%2520this%252C%2520we%2520propose%250ALottery%2520Ticket%2520Adaptation%2520%2528LoTA%2529%252C%2520a%2520sparse%2520adaptation%2520method%2520that%2520identifies%250Aand%2520optimizes%2520only%2520a%2520sparse%2520subnetwork%2520of%2520the%2520model.%2520We%2520evaluate%2520LoTA%2520on%2520a%2520wide%250Arange%2520of%2520challenging%2520tasks%2520such%2520as%2520instruction%2520following%252C%2520reasoning%252C%2520math%252C%2520and%250Asummarization.%2520LoTA%2520obtains%2520better%2520performance%2520than%2520full%2520fine-tuning%2520and%250Alow-rank%2520adaptation%2520%2528LoRA%2529%252C%2520and%2520maintains%2520good%2520performance%2520even%2520after%2520training%250Aon%2520other%2520tasks%2520--%2520thus%252C%2520avoiding%2520catastrophic%2520forgetting.%2520By%2520extracting%2520and%250Afine-tuning%2520over%2520lottery%2520tickets%2520%2528or%2520sparse%2520task%2520vectors%2529%252C%2520LoTA%2520also%2520enables%250Amodel%2520merging%2520over%2520highly%2520dissimilar%2520tasks.%2520Our%2520code%2520is%2520made%2520publicly%2520available%250Aat%2520https%253A//github.com/kiddyboots216/lottery-ticket-adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs&entry.906535625=Ashwinee%20Panda%20and%20Berivan%20Isik%20and%20Xiangyu%20Qi%20and%20Sanmi%20Koyejo%20and%20Tsachy%20Weissman%20and%20Prateek%20Mittal&entry.1292438233=%20%20Existing%20methods%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20new%20tasks%20are%0Anot%20suited%20to%20multi-task%20adaptation%20because%20they%20modify%20all%20the%20model%20weights%0A--%20causing%20destructive%20interference%20between%20tasks.%20The%20resulting%20effects%2C%20such%0Aas%20catastrophic%20forgetting%20of%20earlier%20tasks%2C%20make%20it%20challenging%20to%20obtain%20good%0Aperformance%20on%20multiple%20tasks%20at%20the%20same%20time.%20To%20mitigate%20this%2C%20we%20propose%0ALottery%20Ticket%20Adaptation%20%28LoTA%29%2C%20a%20sparse%20adaptation%20method%20that%20identifies%0Aand%20optimizes%20only%20a%20sparse%20subnetwork%20of%20the%20model.%20We%20evaluate%20LoTA%20on%20a%20wide%0Arange%20of%20challenging%20tasks%20such%20as%20instruction%20following%2C%20reasoning%2C%20math%2C%20and%0Asummarization.%20LoTA%20obtains%20better%20performance%20than%20full%20fine-tuning%20and%0Alow-rank%20adaptation%20%28LoRA%29%2C%20and%20maintains%20good%20performance%20even%20after%20training%0Aon%20other%20tasks%20--%20thus%2C%20avoiding%20catastrophic%20forgetting.%20By%20extracting%20and%0Afine-tuning%20over%20lottery%20tickets%20%28or%20sparse%20task%20vectors%29%2C%20LoTA%20also%20enables%0Amodel%20merging%20over%20highly%20dissimilar%20tasks.%20Our%20code%20is%20made%20publicly%20available%0Aat%20https%3A//github.com/kiddyboots216/lottery-ticket-adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16797v2&entry.124074799=Read"},
{"title": "Text-Animator: Controllable Visual Text Video Generation", "author": "Lin Liu and Quande Liu and Shengju Qian and Yuan Zhou and Wengang Zhou and Houqiang Li and Lingxi Xie and Qi Tian", "abstract": "  Video generation is a challenging yet pivotal task in various industries,\nsuch as gaming, e-commerce, and advertising. One significant unresolved aspect\nwithin T2V is the effective visualization of text within generated videos.\nDespite the progress achieved in Text-to-Video~(T2V) generation, current\nmethods still cannot effectively visualize texts in videos directly, as they\nmainly focus on summarizing semantic scene information, understanding, and\ndepicting actions. While recent advances in image-level visual text generation\nshow promise, transitioning these techniques into the video domain faces\nproblems, notably in preserving textual fidelity and motion coherence. In this\npaper, we propose an innovative approach termed Text-Animator for visual text\nvideo generation. Text-Animator contains a text embedding injection module to\nprecisely depict the structures of visual text in generated videos. Besides, we\ndevelop a camera control module and a text refinement module to improve the\nstability of generated visual text by controlling the camera movement as well\nas the motion of visualized text. Quantitative and qualitative experimental\nresults demonstrate the superiority of our approach to the accuracy of\ngenerated visual text over state-of-the-art video generation methods. The\nproject page can be found at https://laulampaul.github.io/text-animator.html.\n", "link": "http://arxiv.org/abs/2406.17777v1", "date": "2024-06-25", "relevancy": 2.5851, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.711}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.666}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Animator%3A%20Controllable%20Visual%20Text%20Video%20Generation&body=Title%3A%20Text-Animator%3A%20Controllable%20Visual%20Text%20Video%20Generation%0AAuthor%3A%20Lin%20Liu%20and%20Quande%20Liu%20and%20Shengju%20Qian%20and%20Yuan%20Zhou%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%20and%20Lingxi%20Xie%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Video%20generation%20is%20a%20challenging%20yet%20pivotal%20task%20in%20various%20industries%2C%0Asuch%20as%20gaming%2C%20e-commerce%2C%20and%20advertising.%20One%20significant%20unresolved%20aspect%0Awithin%20T2V%20is%20the%20effective%20visualization%20of%20text%20within%20generated%20videos.%0ADespite%20the%20progress%20achieved%20in%20Text-to-Video~%28T2V%29%20generation%2C%20current%0Amethods%20still%20cannot%20effectively%20visualize%20texts%20in%20videos%20directly%2C%20as%20they%0Amainly%20focus%20on%20summarizing%20semantic%20scene%20information%2C%20understanding%2C%20and%0Adepicting%20actions.%20While%20recent%20advances%20in%20image-level%20visual%20text%20generation%0Ashow%20promise%2C%20transitioning%20these%20techniques%20into%20the%20video%20domain%20faces%0Aproblems%2C%20notably%20in%20preserving%20textual%20fidelity%20and%20motion%20coherence.%20In%20this%0Apaper%2C%20we%20propose%20an%20innovative%20approach%20termed%20Text-Animator%20for%20visual%20text%0Avideo%20generation.%20Text-Animator%20contains%20a%20text%20embedding%20injection%20module%20to%0Aprecisely%20depict%20the%20structures%20of%20visual%20text%20in%20generated%20videos.%20Besides%2C%20we%0Adevelop%20a%20camera%20control%20module%20and%20a%20text%20refinement%20module%20to%20improve%20the%0Astability%20of%20generated%20visual%20text%20by%20controlling%20the%20camera%20movement%20as%20well%0Aas%20the%20motion%20of%20visualized%20text.%20Quantitative%20and%20qualitative%20experimental%0Aresults%20demonstrate%20the%20superiority%20of%20our%20approach%20to%20the%20accuracy%20of%0Agenerated%20visual%20text%20over%20state-of-the-art%20video%20generation%20methods.%20The%0Aproject%20page%20can%20be%20found%20at%20https%3A//laulampaul.github.io/text-animator.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Animator%253A%2520Controllable%2520Visual%2520Text%2520Video%2520Generation%26entry.906535625%3DLin%2520Liu%2520and%2520Quande%2520Liu%2520and%2520Shengju%2520Qian%2520and%2520Yuan%2520Zhou%2520and%2520Wengang%2520Zhou%2520and%2520Houqiang%2520Li%2520and%2520Lingxi%2520Xie%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Video%2520generation%2520is%2520a%2520challenging%2520yet%2520pivotal%2520task%2520in%2520various%2520industries%252C%250Asuch%2520as%2520gaming%252C%2520e-commerce%252C%2520and%2520advertising.%2520One%2520significant%2520unresolved%2520aspect%250Awithin%2520T2V%2520is%2520the%2520effective%2520visualization%2520of%2520text%2520within%2520generated%2520videos.%250ADespite%2520the%2520progress%2520achieved%2520in%2520Text-to-Video~%2528T2V%2529%2520generation%252C%2520current%250Amethods%2520still%2520cannot%2520effectively%2520visualize%2520texts%2520in%2520videos%2520directly%252C%2520as%2520they%250Amainly%2520focus%2520on%2520summarizing%2520semantic%2520scene%2520information%252C%2520understanding%252C%2520and%250Adepicting%2520actions.%2520While%2520recent%2520advances%2520in%2520image-level%2520visual%2520text%2520generation%250Ashow%2520promise%252C%2520transitioning%2520these%2520techniques%2520into%2520the%2520video%2520domain%2520faces%250Aproblems%252C%2520notably%2520in%2520preserving%2520textual%2520fidelity%2520and%2520motion%2520coherence.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520an%2520innovative%2520approach%2520termed%2520Text-Animator%2520for%2520visual%2520text%250Avideo%2520generation.%2520Text-Animator%2520contains%2520a%2520text%2520embedding%2520injection%2520module%2520to%250Aprecisely%2520depict%2520the%2520structures%2520of%2520visual%2520text%2520in%2520generated%2520videos.%2520Besides%252C%2520we%250Adevelop%2520a%2520camera%2520control%2520module%2520and%2520a%2520text%2520refinement%2520module%2520to%2520improve%2520the%250Astability%2520of%2520generated%2520visual%2520text%2520by%2520controlling%2520the%2520camera%2520movement%2520as%2520well%250Aas%2520the%2520motion%2520of%2520visualized%2520text.%2520Quantitative%2520and%2520qualitative%2520experimental%250Aresults%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520to%2520the%2520accuracy%2520of%250Agenerated%2520visual%2520text%2520over%2520state-of-the-art%2520video%2520generation%2520methods.%2520The%250Aproject%2520page%2520can%2520be%2520found%2520at%2520https%253A//laulampaul.github.io/text-animator.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Animator%3A%20Controllable%20Visual%20Text%20Video%20Generation&entry.906535625=Lin%20Liu%20and%20Quande%20Liu%20and%20Shengju%20Qian%20and%20Yuan%20Zhou%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%20and%20Lingxi%20Xie%20and%20Qi%20Tian&entry.1292438233=%20%20Video%20generation%20is%20a%20challenging%20yet%20pivotal%20task%20in%20various%20industries%2C%0Asuch%20as%20gaming%2C%20e-commerce%2C%20and%20advertising.%20One%20significant%20unresolved%20aspect%0Awithin%20T2V%20is%20the%20effective%20visualization%20of%20text%20within%20generated%20videos.%0ADespite%20the%20progress%20achieved%20in%20Text-to-Video~%28T2V%29%20generation%2C%20current%0Amethods%20still%20cannot%20effectively%20visualize%20texts%20in%20videos%20directly%2C%20as%20they%0Amainly%20focus%20on%20summarizing%20semantic%20scene%20information%2C%20understanding%2C%20and%0Adepicting%20actions.%20While%20recent%20advances%20in%20image-level%20visual%20text%20generation%0Ashow%20promise%2C%20transitioning%20these%20techniques%20into%20the%20video%20domain%20faces%0Aproblems%2C%20notably%20in%20preserving%20textual%20fidelity%20and%20motion%20coherence.%20In%20this%0Apaper%2C%20we%20propose%20an%20innovative%20approach%20termed%20Text-Animator%20for%20visual%20text%0Avideo%20generation.%20Text-Animator%20contains%20a%20text%20embedding%20injection%20module%20to%0Aprecisely%20depict%20the%20structures%20of%20visual%20text%20in%20generated%20videos.%20Besides%2C%20we%0Adevelop%20a%20camera%20control%20module%20and%20a%20text%20refinement%20module%20to%20improve%20the%0Astability%20of%20generated%20visual%20text%20by%20controlling%20the%20camera%20movement%20as%20well%0Aas%20the%20motion%20of%20visualized%20text.%20Quantitative%20and%20qualitative%20experimental%0Aresults%20demonstrate%20the%20superiority%20of%20our%20approach%20to%20the%20accuracy%20of%0Agenerated%20visual%20text%20over%20state-of-the-art%20video%20generation%20methods.%20The%0Aproject%20page%20can%20be%20found%20at%20https%3A//laulampaul.github.io/text-animator.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17777v1&entry.124074799=Read"},
{"title": "Cross-Modal Spherical Aggregation for Weakly Supervised Remote Sensing\n  Shadow Removal", "author": "Kaichen Chi and Wei Jing and Junjie Li and Qiang Li and Qi Wang", "abstract": "  Remote sensing shadow removal, which aims to recover contaminated surface\ninformation, is tricky since shadows typically display overwhelmingly low\nillumination intensities. In contrast, the infrared image is robust toward\nsignificant light changes, providing visual clues complementary to the visible\nimage. Nevertheless, the existing methods ignore the collaboration between\nheterogeneous modalities, leading to undesired quality degradation. To fill\nthis gap, we propose a weakly supervised shadow removal network with a\nspherical feature space, dubbed S2-ShadowNet, to explore the best of both\nworlds for visible and infrared modalities. Specifically, we employ a modal\ntranslation (visible-to-infrared) model to learn the cross-domain mapping, thus\ngenerating realistic infrared samples. Then, Swin Transformer is utilized to\nextract strong representational visible/infrared features. Simultaneously, the\nextracted features are mapped to the smooth spherical manifold, which\nalleviates the domain shift through regularization. Well-designed similarity\nloss and orthogonality loss are embedded into the spherical space, prompting\nthe separation of private visible/infrared features and the alignment of shared\nvisible/infrared features through constraints on both representation content\nand orientation. Such a manner encourages implicit reciprocity between\nmodalities, thus providing a novel insight into shadow removal. Notably, ground\ntruth is not available in practice, thus S2-ShadowNet is trained by cropping\nshadow and shadow-free patches from the shadow image itself, avoiding\nstereotypical and strict pair data acquisition. More importantly, we contribute\na large-scale weakly supervised shadow removal benchmark, including 4000 shadow\nimages with corresponding shadow masks.\n", "link": "http://arxiv.org/abs/2406.17469v1", "date": "2024-06-25", "relevancy": 2.5847, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5281}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5124}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modal%20Spherical%20Aggregation%20for%20Weakly%20Supervised%20Remote%20Sensing%0A%20%20Shadow%20Removal&body=Title%3A%20Cross-Modal%20Spherical%20Aggregation%20for%20Weakly%20Supervised%20Remote%20Sensing%0A%20%20Shadow%20Removal%0AAuthor%3A%20Kaichen%20Chi%20and%20Wei%20Jing%20and%20Junjie%20Li%20and%20Qiang%20Li%20and%20Qi%20Wang%0AAbstract%3A%20%20%20Remote%20sensing%20shadow%20removal%2C%20which%20aims%20to%20recover%20contaminated%20surface%0Ainformation%2C%20is%20tricky%20since%20shadows%20typically%20display%20overwhelmingly%20low%0Aillumination%20intensities.%20In%20contrast%2C%20the%20infrared%20image%20is%20robust%20toward%0Asignificant%20light%20changes%2C%20providing%20visual%20clues%20complementary%20to%20the%20visible%0Aimage.%20Nevertheless%2C%20the%20existing%20methods%20ignore%20the%20collaboration%20between%0Aheterogeneous%20modalities%2C%20leading%20to%20undesired%20quality%20degradation.%20To%20fill%0Athis%20gap%2C%20we%20propose%20a%20weakly%20supervised%20shadow%20removal%20network%20with%20a%0Aspherical%20feature%20space%2C%20dubbed%20S2-ShadowNet%2C%20to%20explore%20the%20best%20of%20both%0Aworlds%20for%20visible%20and%20infrared%20modalities.%20Specifically%2C%20we%20employ%20a%20modal%0Atranslation%20%28visible-to-infrared%29%20model%20to%20learn%20the%20cross-domain%20mapping%2C%20thus%0Agenerating%20realistic%20infrared%20samples.%20Then%2C%20Swin%20Transformer%20is%20utilized%20to%0Aextract%20strong%20representational%20visible/infrared%20features.%20Simultaneously%2C%20the%0Aextracted%20features%20are%20mapped%20to%20the%20smooth%20spherical%20manifold%2C%20which%0Aalleviates%20the%20domain%20shift%20through%20regularization.%20Well-designed%20similarity%0Aloss%20and%20orthogonality%20loss%20are%20embedded%20into%20the%20spherical%20space%2C%20prompting%0Athe%20separation%20of%20private%20visible/infrared%20features%20and%20the%20alignment%20of%20shared%0Avisible/infrared%20features%20through%20constraints%20on%20both%20representation%20content%0Aand%20orientation.%20Such%20a%20manner%20encourages%20implicit%20reciprocity%20between%0Amodalities%2C%20thus%20providing%20a%20novel%20insight%20into%20shadow%20removal.%20Notably%2C%20ground%0Atruth%20is%20not%20available%20in%20practice%2C%20thus%20S2-ShadowNet%20is%20trained%20by%20cropping%0Ashadow%20and%20shadow-free%20patches%20from%20the%20shadow%20image%20itself%2C%20avoiding%0Astereotypical%20and%20strict%20pair%20data%20acquisition.%20More%20importantly%2C%20we%20contribute%0Aa%20large-scale%20weakly%20supervised%20shadow%20removal%20benchmark%2C%20including%204000%20shadow%0Aimages%20with%20corresponding%20shadow%20masks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modal%2520Spherical%2520Aggregation%2520for%2520Weakly%2520Supervised%2520Remote%2520Sensing%250A%2520%2520Shadow%2520Removal%26entry.906535625%3DKaichen%2520Chi%2520and%2520Wei%2520Jing%2520and%2520Junjie%2520Li%2520and%2520Qiang%2520Li%2520and%2520Qi%2520Wang%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520shadow%2520removal%252C%2520which%2520aims%2520to%2520recover%2520contaminated%2520surface%250Ainformation%252C%2520is%2520tricky%2520since%2520shadows%2520typically%2520display%2520overwhelmingly%2520low%250Aillumination%2520intensities.%2520In%2520contrast%252C%2520the%2520infrared%2520image%2520is%2520robust%2520toward%250Asignificant%2520light%2520changes%252C%2520providing%2520visual%2520clues%2520complementary%2520to%2520the%2520visible%250Aimage.%2520Nevertheless%252C%2520the%2520existing%2520methods%2520ignore%2520the%2520collaboration%2520between%250Aheterogeneous%2520modalities%252C%2520leading%2520to%2520undesired%2520quality%2520degradation.%2520To%2520fill%250Athis%2520gap%252C%2520we%2520propose%2520a%2520weakly%2520supervised%2520shadow%2520removal%2520network%2520with%2520a%250Aspherical%2520feature%2520space%252C%2520dubbed%2520S2-ShadowNet%252C%2520to%2520explore%2520the%2520best%2520of%2520both%250Aworlds%2520for%2520visible%2520and%2520infrared%2520modalities.%2520Specifically%252C%2520we%2520employ%2520a%2520modal%250Atranslation%2520%2528visible-to-infrared%2529%2520model%2520to%2520learn%2520the%2520cross-domain%2520mapping%252C%2520thus%250Agenerating%2520realistic%2520infrared%2520samples.%2520Then%252C%2520Swin%2520Transformer%2520is%2520utilized%2520to%250Aextract%2520strong%2520representational%2520visible/infrared%2520features.%2520Simultaneously%252C%2520the%250Aextracted%2520features%2520are%2520mapped%2520to%2520the%2520smooth%2520spherical%2520manifold%252C%2520which%250Aalleviates%2520the%2520domain%2520shift%2520through%2520regularization.%2520Well-designed%2520similarity%250Aloss%2520and%2520orthogonality%2520loss%2520are%2520embedded%2520into%2520the%2520spherical%2520space%252C%2520prompting%250Athe%2520separation%2520of%2520private%2520visible/infrared%2520features%2520and%2520the%2520alignment%2520of%2520shared%250Avisible/infrared%2520features%2520through%2520constraints%2520on%2520both%2520representation%2520content%250Aand%2520orientation.%2520Such%2520a%2520manner%2520encourages%2520implicit%2520reciprocity%2520between%250Amodalities%252C%2520thus%2520providing%2520a%2520novel%2520insight%2520into%2520shadow%2520removal.%2520Notably%252C%2520ground%250Atruth%2520is%2520not%2520available%2520in%2520practice%252C%2520thus%2520S2-ShadowNet%2520is%2520trained%2520by%2520cropping%250Ashadow%2520and%2520shadow-free%2520patches%2520from%2520the%2520shadow%2520image%2520itself%252C%2520avoiding%250Astereotypical%2520and%2520strict%2520pair%2520data%2520acquisition.%2520More%2520importantly%252C%2520we%2520contribute%250Aa%2520large-scale%2520weakly%2520supervised%2520shadow%2520removal%2520benchmark%252C%2520including%25204000%2520shadow%250Aimages%2520with%2520corresponding%2520shadow%2520masks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modal%20Spherical%20Aggregation%20for%20Weakly%20Supervised%20Remote%20Sensing%0A%20%20Shadow%20Removal&entry.906535625=Kaichen%20Chi%20and%20Wei%20Jing%20and%20Junjie%20Li%20and%20Qiang%20Li%20and%20Qi%20Wang&entry.1292438233=%20%20Remote%20sensing%20shadow%20removal%2C%20which%20aims%20to%20recover%20contaminated%20surface%0Ainformation%2C%20is%20tricky%20since%20shadows%20typically%20display%20overwhelmingly%20low%0Aillumination%20intensities.%20In%20contrast%2C%20the%20infrared%20image%20is%20robust%20toward%0Asignificant%20light%20changes%2C%20providing%20visual%20clues%20complementary%20to%20the%20visible%0Aimage.%20Nevertheless%2C%20the%20existing%20methods%20ignore%20the%20collaboration%20between%0Aheterogeneous%20modalities%2C%20leading%20to%20undesired%20quality%20degradation.%20To%20fill%0Athis%20gap%2C%20we%20propose%20a%20weakly%20supervised%20shadow%20removal%20network%20with%20a%0Aspherical%20feature%20space%2C%20dubbed%20S2-ShadowNet%2C%20to%20explore%20the%20best%20of%20both%0Aworlds%20for%20visible%20and%20infrared%20modalities.%20Specifically%2C%20we%20employ%20a%20modal%0Atranslation%20%28visible-to-infrared%29%20model%20to%20learn%20the%20cross-domain%20mapping%2C%20thus%0Agenerating%20realistic%20infrared%20samples.%20Then%2C%20Swin%20Transformer%20is%20utilized%20to%0Aextract%20strong%20representational%20visible/infrared%20features.%20Simultaneously%2C%20the%0Aextracted%20features%20are%20mapped%20to%20the%20smooth%20spherical%20manifold%2C%20which%0Aalleviates%20the%20domain%20shift%20through%20regularization.%20Well-designed%20similarity%0Aloss%20and%20orthogonality%20loss%20are%20embedded%20into%20the%20spherical%20space%2C%20prompting%0Athe%20separation%20of%20private%20visible/infrared%20features%20and%20the%20alignment%20of%20shared%0Avisible/infrared%20features%20through%20constraints%20on%20both%20representation%20content%0Aand%20orientation.%20Such%20a%20manner%20encourages%20implicit%20reciprocity%20between%0Amodalities%2C%20thus%20providing%20a%20novel%20insight%20into%20shadow%20removal.%20Notably%2C%20ground%0Atruth%20is%20not%20available%20in%20practice%2C%20thus%20S2-ShadowNet%20is%20trained%20by%20cropping%0Ashadow%20and%20shadow-free%20patches%20from%20the%20shadow%20image%20itself%2C%20avoiding%0Astereotypical%20and%20strict%20pair%20data%20acquisition.%20More%20importantly%2C%20we%20contribute%0Aa%20large-scale%20weakly%20supervised%20shadow%20removal%20benchmark%2C%20including%204000%20shadow%0Aimages%20with%20corresponding%20shadow%20masks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17469v1&entry.124074799=Read"},
{"title": "Regularization and Optimal Multiclass Learning", "author": "Julian Asilis and Siddartha Devic and Shaddin Dughmi and Vatsal Sharan and Shang-Hua Teng", "abstract": "  The quintessential learning algorithm of empirical risk minimization (ERM) is\nknown to fail in various settings for which uniform convergence does not\ncharacterize learning. It is therefore unsurprising that the practice of\nmachine learning is rife with considerably richer algorithmic techniques for\nsuccessfully controlling model capacity. Nevertheless, no such technique or\nprinciple has broken away from the pack to characterize optimal learning in\nthese more general settings.\n  The purpose of this work is to characterize the role of regularization in\nperhaps the simplest setting for which ERM fails: multiclass learning with\narbitrary label sets. Using one-inclusion graphs (OIGs), we exhibit optimal\nlearning algorithms that dovetail with tried-and-true algorithmic principles:\nOccam's Razor as embodied by structural risk minimization (SRM), the principle\nof maximum entropy, and Bayesian reasoning. Most notably, we introduce an\noptimal learner which relaxes structural risk minimization on two dimensions:\nit allows the regularization function to be \"local\" to datapoints, and uses an\nunsupervised learning stage to learn this regularizer at the outset. We justify\nthese relaxations by showing that they are necessary: removing either dimension\nfails to yield a near-optimal learner. We also extract from OIGs a\ncombinatorial sequence we term the Hall complexity, which is the first to\ncharacterize a problem's transductive error rate exactly.\n  Lastly, we introduce a generalization of OIGs and the transductive learning\nsetting to the agnostic case, where we show that optimal orientations of\nHamming graphs -- judged using nodes' outdegrees minus a system of\nnode-dependent credits -- characterize optimal learners exactly. We demonstrate\nthat an agnostic version of the Hall complexity again characterizes error rates\nexactly, and exhibit an optimal learner using maximum entropy programs.\n", "link": "http://arxiv.org/abs/2309.13692v2", "date": "2024-06-25", "relevancy": 2.5554, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5455}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5025}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularization%20and%20Optimal%20Multiclass%20Learning&body=Title%3A%20Regularization%20and%20Optimal%20Multiclass%20Learning%0AAuthor%3A%20Julian%20Asilis%20and%20Siddartha%20Devic%20and%20Shaddin%20Dughmi%20and%20Vatsal%20Sharan%20and%20Shang-Hua%20Teng%0AAbstract%3A%20%20%20The%20quintessential%20learning%20algorithm%20of%20empirical%20risk%20minimization%20%28ERM%29%20is%0Aknown%20to%20fail%20in%20various%20settings%20for%20which%20uniform%20convergence%20does%20not%0Acharacterize%20learning.%20It%20is%20therefore%20unsurprising%20that%20the%20practice%20of%0Amachine%20learning%20is%20rife%20with%20considerably%20richer%20algorithmic%20techniques%20for%0Asuccessfully%20controlling%20model%20capacity.%20Nevertheless%2C%20no%20such%20technique%20or%0Aprinciple%20has%20broken%20away%20from%20the%20pack%20to%20characterize%20optimal%20learning%20in%0Athese%20more%20general%20settings.%0A%20%20The%20purpose%20of%20this%20work%20is%20to%20characterize%20the%20role%20of%20regularization%20in%0Aperhaps%20the%20simplest%20setting%20for%20which%20ERM%20fails%3A%20multiclass%20learning%20with%0Aarbitrary%20label%20sets.%20Using%20one-inclusion%20graphs%20%28OIGs%29%2C%20we%20exhibit%20optimal%0Alearning%20algorithms%20that%20dovetail%20with%20tried-and-true%20algorithmic%20principles%3A%0AOccam%27s%20Razor%20as%20embodied%20by%20structural%20risk%20minimization%20%28SRM%29%2C%20the%20principle%0Aof%20maximum%20entropy%2C%20and%20Bayesian%20reasoning.%20Most%20notably%2C%20we%20introduce%20an%0Aoptimal%20learner%20which%20relaxes%20structural%20risk%20minimization%20on%20two%20dimensions%3A%0Ait%20allows%20the%20regularization%20function%20to%20be%20%22local%22%20to%20datapoints%2C%20and%20uses%20an%0Aunsupervised%20learning%20stage%20to%20learn%20this%20regularizer%20at%20the%20outset.%20We%20justify%0Athese%20relaxations%20by%20showing%20that%20they%20are%20necessary%3A%20removing%20either%20dimension%0Afails%20to%20yield%20a%20near-optimal%20learner.%20We%20also%20extract%20from%20OIGs%20a%0Acombinatorial%20sequence%20we%20term%20the%20Hall%20complexity%2C%20which%20is%20the%20first%20to%0Acharacterize%20a%20problem%27s%20transductive%20error%20rate%20exactly.%0A%20%20Lastly%2C%20we%20introduce%20a%20generalization%20of%20OIGs%20and%20the%20transductive%20learning%0Asetting%20to%20the%20agnostic%20case%2C%20where%20we%20show%20that%20optimal%20orientations%20of%0AHamming%20graphs%20--%20judged%20using%20nodes%27%20outdegrees%20minus%20a%20system%20of%0Anode-dependent%20credits%20--%20characterize%20optimal%20learners%20exactly.%20We%20demonstrate%0Athat%20an%20agnostic%20version%20of%20the%20Hall%20complexity%20again%20characterizes%20error%20rates%0Aexactly%2C%20and%20exhibit%20an%20optimal%20learner%20using%20maximum%20entropy%20programs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.13692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularization%2520and%2520Optimal%2520Multiclass%2520Learning%26entry.906535625%3DJulian%2520Asilis%2520and%2520Siddartha%2520Devic%2520and%2520Shaddin%2520Dughmi%2520and%2520Vatsal%2520Sharan%2520and%2520Shang-Hua%2520Teng%26entry.1292438233%3D%2520%2520The%2520quintessential%2520learning%2520algorithm%2520of%2520empirical%2520risk%2520minimization%2520%2528ERM%2529%2520is%250Aknown%2520to%2520fail%2520in%2520various%2520settings%2520for%2520which%2520uniform%2520convergence%2520does%2520not%250Acharacterize%2520learning.%2520It%2520is%2520therefore%2520unsurprising%2520that%2520the%2520practice%2520of%250Amachine%2520learning%2520is%2520rife%2520with%2520considerably%2520richer%2520algorithmic%2520techniques%2520for%250Asuccessfully%2520controlling%2520model%2520capacity.%2520Nevertheless%252C%2520no%2520such%2520technique%2520or%250Aprinciple%2520has%2520broken%2520away%2520from%2520the%2520pack%2520to%2520characterize%2520optimal%2520learning%2520in%250Athese%2520more%2520general%2520settings.%250A%2520%2520The%2520purpose%2520of%2520this%2520work%2520is%2520to%2520characterize%2520the%2520role%2520of%2520regularization%2520in%250Aperhaps%2520the%2520simplest%2520setting%2520for%2520which%2520ERM%2520fails%253A%2520multiclass%2520learning%2520with%250Aarbitrary%2520label%2520sets.%2520Using%2520one-inclusion%2520graphs%2520%2528OIGs%2529%252C%2520we%2520exhibit%2520optimal%250Alearning%2520algorithms%2520that%2520dovetail%2520with%2520tried-and-true%2520algorithmic%2520principles%253A%250AOccam%2527s%2520Razor%2520as%2520embodied%2520by%2520structural%2520risk%2520minimization%2520%2528SRM%2529%252C%2520the%2520principle%250Aof%2520maximum%2520entropy%252C%2520and%2520Bayesian%2520reasoning.%2520Most%2520notably%252C%2520we%2520introduce%2520an%250Aoptimal%2520learner%2520which%2520relaxes%2520structural%2520risk%2520minimization%2520on%2520two%2520dimensions%253A%250Ait%2520allows%2520the%2520regularization%2520function%2520to%2520be%2520%2522local%2522%2520to%2520datapoints%252C%2520and%2520uses%2520an%250Aunsupervised%2520learning%2520stage%2520to%2520learn%2520this%2520regularizer%2520at%2520the%2520outset.%2520We%2520justify%250Athese%2520relaxations%2520by%2520showing%2520that%2520they%2520are%2520necessary%253A%2520removing%2520either%2520dimension%250Afails%2520to%2520yield%2520a%2520near-optimal%2520learner.%2520We%2520also%2520extract%2520from%2520OIGs%2520a%250Acombinatorial%2520sequence%2520we%2520term%2520the%2520Hall%2520complexity%252C%2520which%2520is%2520the%2520first%2520to%250Acharacterize%2520a%2520problem%2527s%2520transductive%2520error%2520rate%2520exactly.%250A%2520%2520Lastly%252C%2520we%2520introduce%2520a%2520generalization%2520of%2520OIGs%2520and%2520the%2520transductive%2520learning%250Asetting%2520to%2520the%2520agnostic%2520case%252C%2520where%2520we%2520show%2520that%2520optimal%2520orientations%2520of%250AHamming%2520graphs%2520--%2520judged%2520using%2520nodes%2527%2520outdegrees%2520minus%2520a%2520system%2520of%250Anode-dependent%2520credits%2520--%2520characterize%2520optimal%2520learners%2520exactly.%2520We%2520demonstrate%250Athat%2520an%2520agnostic%2520version%2520of%2520the%2520Hall%2520complexity%2520again%2520characterizes%2520error%2520rates%250Aexactly%252C%2520and%2520exhibit%2520an%2520optimal%2520learner%2520using%2520maximum%2520entropy%2520programs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.13692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularization%20and%20Optimal%20Multiclass%20Learning&entry.906535625=Julian%20Asilis%20and%20Siddartha%20Devic%20and%20Shaddin%20Dughmi%20and%20Vatsal%20Sharan%20and%20Shang-Hua%20Teng&entry.1292438233=%20%20The%20quintessential%20learning%20algorithm%20of%20empirical%20risk%20minimization%20%28ERM%29%20is%0Aknown%20to%20fail%20in%20various%20settings%20for%20which%20uniform%20convergence%20does%20not%0Acharacterize%20learning.%20It%20is%20therefore%20unsurprising%20that%20the%20practice%20of%0Amachine%20learning%20is%20rife%20with%20considerably%20richer%20algorithmic%20techniques%20for%0Asuccessfully%20controlling%20model%20capacity.%20Nevertheless%2C%20no%20such%20technique%20or%0Aprinciple%20has%20broken%20away%20from%20the%20pack%20to%20characterize%20optimal%20learning%20in%0Athese%20more%20general%20settings.%0A%20%20The%20purpose%20of%20this%20work%20is%20to%20characterize%20the%20role%20of%20regularization%20in%0Aperhaps%20the%20simplest%20setting%20for%20which%20ERM%20fails%3A%20multiclass%20learning%20with%0Aarbitrary%20label%20sets.%20Using%20one-inclusion%20graphs%20%28OIGs%29%2C%20we%20exhibit%20optimal%0Alearning%20algorithms%20that%20dovetail%20with%20tried-and-true%20algorithmic%20principles%3A%0AOccam%27s%20Razor%20as%20embodied%20by%20structural%20risk%20minimization%20%28SRM%29%2C%20the%20principle%0Aof%20maximum%20entropy%2C%20and%20Bayesian%20reasoning.%20Most%20notably%2C%20we%20introduce%20an%0Aoptimal%20learner%20which%20relaxes%20structural%20risk%20minimization%20on%20two%20dimensions%3A%0Ait%20allows%20the%20regularization%20function%20to%20be%20%22local%22%20to%20datapoints%2C%20and%20uses%20an%0Aunsupervised%20learning%20stage%20to%20learn%20this%20regularizer%20at%20the%20outset.%20We%20justify%0Athese%20relaxations%20by%20showing%20that%20they%20are%20necessary%3A%20removing%20either%20dimension%0Afails%20to%20yield%20a%20near-optimal%20learner.%20We%20also%20extract%20from%20OIGs%20a%0Acombinatorial%20sequence%20we%20term%20the%20Hall%20complexity%2C%20which%20is%20the%20first%20to%0Acharacterize%20a%20problem%27s%20transductive%20error%20rate%20exactly.%0A%20%20Lastly%2C%20we%20introduce%20a%20generalization%20of%20OIGs%20and%20the%20transductive%20learning%0Asetting%20to%20the%20agnostic%20case%2C%20where%20we%20show%20that%20optimal%20orientations%20of%0AHamming%20graphs%20--%20judged%20using%20nodes%27%20outdegrees%20minus%20a%20system%20of%0Anode-dependent%20credits%20--%20characterize%20optimal%20learners%20exactly.%20We%20demonstrate%0Athat%20an%20agnostic%20version%20of%20the%20Hall%20complexity%20again%20characterizes%20error%20rates%0Aexactly%2C%20and%20exhibit%20an%20optimal%20learner%20using%20maximum%20entropy%20programs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.13692v2&entry.124074799=Read"},
{"title": "Overcoming the Paradox of Certified Training with Gaussian Smoothing", "author": "Stefan Balauca and Mark Niklas M\u00fcller and Yuhao Mao and Maximilian Baader and Marc Fischer and Martin Vechev", "abstract": "  Training neural networks with high certified accuracy against adversarial\nexamples remains an open problem despite significant efforts. While\ncertification methods can effectively leverage tight convex relaxations for\nbound computation, in training, these methods perform worse than looser\nrelaxations. Prior work hypothesized that this is caused by the discontinuity\nand perturbation sensitivity of the loss surface induced by these tighter\nrelaxations. In this work, we show theoretically that Gaussian Loss Smoothing\ncan alleviate both issues. We confirm this empirically by proposing a certified\ntraining method combining PGPE, an algorithm computing gradients of a smoothed\nloss, with different convex relaxations. When using this training method, we\nobserve that tighter bounds indeed lead to strictly better networks. While\nscaling PGPE training remains challenging due to high computational cost, we\nshow that by using a not theoretically sound, yet much cheaper smoothing\napproximation, we obtain better certified accuracies than state-of-the-art\nmethods when training on the same network architecture. Our results clearly\ndemonstrate the promise of Gaussian Loss Smoothing for training certifiably\nrobust neural networks.\n", "link": "http://arxiv.org/abs/2403.07095v2", "date": "2024-06-25", "relevancy": 2.5156, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5379}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4878}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20the%20Paradox%20of%20Certified%20Training%20with%20Gaussian%20Smoothing&body=Title%3A%20Overcoming%20the%20Paradox%20of%20Certified%20Training%20with%20Gaussian%20Smoothing%0AAuthor%3A%20Stefan%20Balauca%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Yuhao%20Mao%20and%20Maximilian%20Baader%20and%20Marc%20Fischer%20and%20Martin%20Vechev%0AAbstract%3A%20%20%20Training%20neural%20networks%20with%20high%20certified%20accuracy%20against%20adversarial%0Aexamples%20remains%20an%20open%20problem%20despite%20significant%20efforts.%20While%0Acertification%20methods%20can%20effectively%20leverage%20tight%20convex%20relaxations%20for%0Abound%20computation%2C%20in%20training%2C%20these%20methods%20perform%20worse%20than%20looser%0Arelaxations.%20Prior%20work%20hypothesized%20that%20this%20is%20caused%20by%20the%20discontinuity%0Aand%20perturbation%20sensitivity%20of%20the%20loss%20surface%20induced%20by%20these%20tighter%0Arelaxations.%20In%20this%20work%2C%20we%20show%20theoretically%20that%20Gaussian%20Loss%20Smoothing%0Acan%20alleviate%20both%20issues.%20We%20confirm%20this%20empirically%20by%20proposing%20a%20certified%0Atraining%20method%20combining%20PGPE%2C%20an%20algorithm%20computing%20gradients%20of%20a%20smoothed%0Aloss%2C%20with%20different%20convex%20relaxations.%20When%20using%20this%20training%20method%2C%20we%0Aobserve%20that%20tighter%20bounds%20indeed%20lead%20to%20strictly%20better%20networks.%20While%0Ascaling%20PGPE%20training%20remains%20challenging%20due%20to%20high%20computational%20cost%2C%20we%0Ashow%20that%20by%20using%20a%20not%20theoretically%20sound%2C%20yet%20much%20cheaper%20smoothing%0Aapproximation%2C%20we%20obtain%20better%20certified%20accuracies%20than%20state-of-the-art%0Amethods%20when%20training%20on%20the%20same%20network%20architecture.%20Our%20results%20clearly%0Ademonstrate%20the%20promise%20of%20Gaussian%20Loss%20Smoothing%20for%20training%20certifiably%0Arobust%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520the%2520Paradox%2520of%2520Certified%2520Training%2520with%2520Gaussian%2520Smoothing%26entry.906535625%3DStefan%2520Balauca%2520and%2520Mark%2520Niklas%2520M%25C3%25BCller%2520and%2520Yuhao%2520Mao%2520and%2520Maximilian%2520Baader%2520and%2520Marc%2520Fischer%2520and%2520Martin%2520Vechev%26entry.1292438233%3D%2520%2520Training%2520neural%2520networks%2520with%2520high%2520certified%2520accuracy%2520against%2520adversarial%250Aexamples%2520remains%2520an%2520open%2520problem%2520despite%2520significant%2520efforts.%2520While%250Acertification%2520methods%2520can%2520effectively%2520leverage%2520tight%2520convex%2520relaxations%2520for%250Abound%2520computation%252C%2520in%2520training%252C%2520these%2520methods%2520perform%2520worse%2520than%2520looser%250Arelaxations.%2520Prior%2520work%2520hypothesized%2520that%2520this%2520is%2520caused%2520by%2520the%2520discontinuity%250Aand%2520perturbation%2520sensitivity%2520of%2520the%2520loss%2520surface%2520induced%2520by%2520these%2520tighter%250Arelaxations.%2520In%2520this%2520work%252C%2520we%2520show%2520theoretically%2520that%2520Gaussian%2520Loss%2520Smoothing%250Acan%2520alleviate%2520both%2520issues.%2520We%2520confirm%2520this%2520empirically%2520by%2520proposing%2520a%2520certified%250Atraining%2520method%2520combining%2520PGPE%252C%2520an%2520algorithm%2520computing%2520gradients%2520of%2520a%2520smoothed%250Aloss%252C%2520with%2520different%2520convex%2520relaxations.%2520When%2520using%2520this%2520training%2520method%252C%2520we%250Aobserve%2520that%2520tighter%2520bounds%2520indeed%2520lead%2520to%2520strictly%2520better%2520networks.%2520While%250Ascaling%2520PGPE%2520training%2520remains%2520challenging%2520due%2520to%2520high%2520computational%2520cost%252C%2520we%250Ashow%2520that%2520by%2520using%2520a%2520not%2520theoretically%2520sound%252C%2520yet%2520much%2520cheaper%2520smoothing%250Aapproximation%252C%2520we%2520obtain%2520better%2520certified%2520accuracies%2520than%2520state-of-the-art%250Amethods%2520when%2520training%2520on%2520the%2520same%2520network%2520architecture.%2520Our%2520results%2520clearly%250Ademonstrate%2520the%2520promise%2520of%2520Gaussian%2520Loss%2520Smoothing%2520for%2520training%2520certifiably%250Arobust%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20the%20Paradox%20of%20Certified%20Training%20with%20Gaussian%20Smoothing&entry.906535625=Stefan%20Balauca%20and%20Mark%20Niklas%20M%C3%BCller%20and%20Yuhao%20Mao%20and%20Maximilian%20Baader%20and%20Marc%20Fischer%20and%20Martin%20Vechev&entry.1292438233=%20%20Training%20neural%20networks%20with%20high%20certified%20accuracy%20against%20adversarial%0Aexamples%20remains%20an%20open%20problem%20despite%20significant%20efforts.%20While%0Acertification%20methods%20can%20effectively%20leverage%20tight%20convex%20relaxations%20for%0Abound%20computation%2C%20in%20training%2C%20these%20methods%20perform%20worse%20than%20looser%0Arelaxations.%20Prior%20work%20hypothesized%20that%20this%20is%20caused%20by%20the%20discontinuity%0Aand%20perturbation%20sensitivity%20of%20the%20loss%20surface%20induced%20by%20these%20tighter%0Arelaxations.%20In%20this%20work%2C%20we%20show%20theoretically%20that%20Gaussian%20Loss%20Smoothing%0Acan%20alleviate%20both%20issues.%20We%20confirm%20this%20empirically%20by%20proposing%20a%20certified%0Atraining%20method%20combining%20PGPE%2C%20an%20algorithm%20computing%20gradients%20of%20a%20smoothed%0Aloss%2C%20with%20different%20convex%20relaxations.%20When%20using%20this%20training%20method%2C%20we%0Aobserve%20that%20tighter%20bounds%20indeed%20lead%20to%20strictly%20better%20networks.%20While%0Ascaling%20PGPE%20training%20remains%20challenging%20due%20to%20high%20computational%20cost%2C%20we%0Ashow%20that%20by%20using%20a%20not%20theoretically%20sound%2C%20yet%20much%20cheaper%20smoothing%0Aapproximation%2C%20we%20obtain%20better%20certified%20accuracies%20than%20state-of-the-art%0Amethods%20when%20training%20on%20the%20same%20network%20architecture.%20Our%20results%20clearly%0Ademonstrate%20the%20promise%20of%20Gaussian%20Loss%20Smoothing%20for%20training%20certifiably%0Arobust%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07095v2&entry.124074799=Read"},
{"title": "HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target\n  Binding Affinity Prediction", "author": "Xi Xiao and Wentao Wang and Jiacheng Xie and Lijing Zhu and Gaofei Chen and Zhengji Li and Tianyang Wang and Min Xu", "abstract": "  Drug target binding affinity (DTA) is a key criterion for drug screening.\nExisting experimental methods are time-consuming and rely on limited structural\nand domain information. While learning-based methods can model sequence and\nstructural information, they struggle to integrate contextual data and often\nlack comprehensive modeling of drug-target interactions. In this study, we\npropose a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic\nprompts within a hybrid Graph-Transformer framework. Our method generates\ncontext-specific prompts for each drug-target pair, enhancing the model's\nability to capture unique interactions. The introduction of prompt tuning\nfurther optimizes the prediction process by filtering out irrelevant noise and\nemphasizing task-relevant information, dynamically adjusting the input features\nof the molecular graph. The proposed hybrid Graph-Transformer architecture\ncombines structural information from Graph Convolutional Networks (GCNs) with\nsequence information captured by Transformers, facilitating the interaction\nbetween global and local information. Additionally, we adopted the multi-view\nfeature fusion method to project molecular graph views and affinity subgraph\nviews into a common feature space, effectively combining structural and\ncontextual information. Experiments on two widely used public datasets, Davis\nand KIBA, show that HGTDP-DTA outperforms state-of-the-art DTA prediction\nmethods in both prediction performance and generalization ability.\n", "link": "http://arxiv.org/abs/2406.17697v1", "date": "2024-06-25", "relevancy": 2.5153, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5166}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGTDP-DTA%3A%20Hybrid%20Graph-Transformer%20with%20Dynamic%20Prompt%20for%20Drug-Target%0A%20%20Binding%20Affinity%20Prediction&body=Title%3A%20HGTDP-DTA%3A%20Hybrid%20Graph-Transformer%20with%20Dynamic%20Prompt%20for%20Drug-Target%0A%20%20Binding%20Affinity%20Prediction%0AAuthor%3A%20Xi%20Xiao%20and%20Wentao%20Wang%20and%20Jiacheng%20Xie%20and%20Lijing%20Zhu%20and%20Gaofei%20Chen%20and%20Zhengji%20Li%20and%20Tianyang%20Wang%20and%20Min%20Xu%0AAbstract%3A%20%20%20Drug%20target%20binding%20affinity%20%28DTA%29%20is%20a%20key%20criterion%20for%20drug%20screening.%0AExisting%20experimental%20methods%20are%20time-consuming%20and%20rely%20on%20limited%20structural%0Aand%20domain%20information.%20While%20learning-based%20methods%20can%20model%20sequence%20and%0Astructural%20information%2C%20they%20struggle%20to%20integrate%20contextual%20data%20and%20often%0Alack%20comprehensive%20modeling%20of%20drug-target%20interactions.%20In%20this%20study%2C%20we%0Apropose%20a%20novel%20DTA%20prediction%20method%2C%20termed%20HGTDP-DTA%2C%20which%20utilizes%20dynamic%0Aprompts%20within%20a%20hybrid%20Graph-Transformer%20framework.%20Our%20method%20generates%0Acontext-specific%20prompts%20for%20each%20drug-target%20pair%2C%20enhancing%20the%20model%27s%0Aability%20to%20capture%20unique%20interactions.%20The%20introduction%20of%20prompt%20tuning%0Afurther%20optimizes%20the%20prediction%20process%20by%20filtering%20out%20irrelevant%20noise%20and%0Aemphasizing%20task-relevant%20information%2C%20dynamically%20adjusting%20the%20input%20features%0Aof%20the%20molecular%20graph.%20The%20proposed%20hybrid%20Graph-Transformer%20architecture%0Acombines%20structural%20information%20from%20Graph%20Convolutional%20Networks%20%28GCNs%29%20with%0Asequence%20information%20captured%20by%20Transformers%2C%20facilitating%20the%20interaction%0Abetween%20global%20and%20local%20information.%20Additionally%2C%20we%20adopted%20the%20multi-view%0Afeature%20fusion%20method%20to%20project%20molecular%20graph%20views%20and%20affinity%20subgraph%0Aviews%20into%20a%20common%20feature%20space%2C%20effectively%20combining%20structural%20and%0Acontextual%20information.%20Experiments%20on%20two%20widely%20used%20public%20datasets%2C%20Davis%0Aand%20KIBA%2C%20show%20that%20HGTDP-DTA%20outperforms%20state-of-the-art%20DTA%20prediction%0Amethods%20in%20both%20prediction%20performance%20and%20generalization%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGTDP-DTA%253A%2520Hybrid%2520Graph-Transformer%2520with%2520Dynamic%2520Prompt%2520for%2520Drug-Target%250A%2520%2520Binding%2520Affinity%2520Prediction%26entry.906535625%3DXi%2520Xiao%2520and%2520Wentao%2520Wang%2520and%2520Jiacheng%2520Xie%2520and%2520Lijing%2520Zhu%2520and%2520Gaofei%2520Chen%2520and%2520Zhengji%2520Li%2520and%2520Tianyang%2520Wang%2520and%2520Min%2520Xu%26entry.1292438233%3D%2520%2520Drug%2520target%2520binding%2520affinity%2520%2528DTA%2529%2520is%2520a%2520key%2520criterion%2520for%2520drug%2520screening.%250AExisting%2520experimental%2520methods%2520are%2520time-consuming%2520and%2520rely%2520on%2520limited%2520structural%250Aand%2520domain%2520information.%2520While%2520learning-based%2520methods%2520can%2520model%2520sequence%2520and%250Astructural%2520information%252C%2520they%2520struggle%2520to%2520integrate%2520contextual%2520data%2520and%2520often%250Alack%2520comprehensive%2520modeling%2520of%2520drug-target%2520interactions.%2520In%2520this%2520study%252C%2520we%250Apropose%2520a%2520novel%2520DTA%2520prediction%2520method%252C%2520termed%2520HGTDP-DTA%252C%2520which%2520utilizes%2520dynamic%250Aprompts%2520within%2520a%2520hybrid%2520Graph-Transformer%2520framework.%2520Our%2520method%2520generates%250Acontext-specific%2520prompts%2520for%2520each%2520drug-target%2520pair%252C%2520enhancing%2520the%2520model%2527s%250Aability%2520to%2520capture%2520unique%2520interactions.%2520The%2520introduction%2520of%2520prompt%2520tuning%250Afurther%2520optimizes%2520the%2520prediction%2520process%2520by%2520filtering%2520out%2520irrelevant%2520noise%2520and%250Aemphasizing%2520task-relevant%2520information%252C%2520dynamically%2520adjusting%2520the%2520input%2520features%250Aof%2520the%2520molecular%2520graph.%2520The%2520proposed%2520hybrid%2520Graph-Transformer%2520architecture%250Acombines%2520structural%2520information%2520from%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520with%250Asequence%2520information%2520captured%2520by%2520Transformers%252C%2520facilitating%2520the%2520interaction%250Abetween%2520global%2520and%2520local%2520information.%2520Additionally%252C%2520we%2520adopted%2520the%2520multi-view%250Afeature%2520fusion%2520method%2520to%2520project%2520molecular%2520graph%2520views%2520and%2520affinity%2520subgraph%250Aviews%2520into%2520a%2520common%2520feature%2520space%252C%2520effectively%2520combining%2520structural%2520and%250Acontextual%2520information.%2520Experiments%2520on%2520two%2520widely%2520used%2520public%2520datasets%252C%2520Davis%250Aand%2520KIBA%252C%2520show%2520that%2520HGTDP-DTA%2520outperforms%2520state-of-the-art%2520DTA%2520prediction%250Amethods%2520in%2520both%2520prediction%2520performance%2520and%2520generalization%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGTDP-DTA%3A%20Hybrid%20Graph-Transformer%20with%20Dynamic%20Prompt%20for%20Drug-Target%0A%20%20Binding%20Affinity%20Prediction&entry.906535625=Xi%20Xiao%20and%20Wentao%20Wang%20and%20Jiacheng%20Xie%20and%20Lijing%20Zhu%20and%20Gaofei%20Chen%20and%20Zhengji%20Li%20and%20Tianyang%20Wang%20and%20Min%20Xu&entry.1292438233=%20%20Drug%20target%20binding%20affinity%20%28DTA%29%20is%20a%20key%20criterion%20for%20drug%20screening.%0AExisting%20experimental%20methods%20are%20time-consuming%20and%20rely%20on%20limited%20structural%0Aand%20domain%20information.%20While%20learning-based%20methods%20can%20model%20sequence%20and%0Astructural%20information%2C%20they%20struggle%20to%20integrate%20contextual%20data%20and%20often%0Alack%20comprehensive%20modeling%20of%20drug-target%20interactions.%20In%20this%20study%2C%20we%0Apropose%20a%20novel%20DTA%20prediction%20method%2C%20termed%20HGTDP-DTA%2C%20which%20utilizes%20dynamic%0Aprompts%20within%20a%20hybrid%20Graph-Transformer%20framework.%20Our%20method%20generates%0Acontext-specific%20prompts%20for%20each%20drug-target%20pair%2C%20enhancing%20the%20model%27s%0Aability%20to%20capture%20unique%20interactions.%20The%20introduction%20of%20prompt%20tuning%0Afurther%20optimizes%20the%20prediction%20process%20by%20filtering%20out%20irrelevant%20noise%20and%0Aemphasizing%20task-relevant%20information%2C%20dynamically%20adjusting%20the%20input%20features%0Aof%20the%20molecular%20graph.%20The%20proposed%20hybrid%20Graph-Transformer%20architecture%0Acombines%20structural%20information%20from%20Graph%20Convolutional%20Networks%20%28GCNs%29%20with%0Asequence%20information%20captured%20by%20Transformers%2C%20facilitating%20the%20interaction%0Abetween%20global%20and%20local%20information.%20Additionally%2C%20we%20adopted%20the%20multi-view%0Afeature%20fusion%20method%20to%20project%20molecular%20graph%20views%20and%20affinity%20subgraph%0Aviews%20into%20a%20common%20feature%20space%2C%20effectively%20combining%20structural%20and%0Acontextual%20information.%20Experiments%20on%20two%20widely%20used%20public%20datasets%2C%20Davis%0Aand%20KIBA%2C%20show%20that%20HGTDP-DTA%20outperforms%20state-of-the-art%20DTA%20prediction%0Amethods%20in%20both%20prediction%20performance%20and%20generalization%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17697v1&entry.124074799=Read"},
{"title": "Early learning of the optimal constant solution in neural networks and\n  humans", "author": "Jirko Rubruck and Jan P. Bauer and Andrew Saxe and Christopher Summerfield", "abstract": "  Deep neural networks learn increasingly complex functions over the course of\ntraining. Here, we show both empirically and theoretically that learning of the\ntarget function is preceded by an early phase in which networks learn the\noptimal constant solution (OCS) - that is, initial model responses mirror the\ndistribution of target labels, while entirely ignoring information provided in\nthe input. Using a hierarchical category learning task, we derive exact\nsolutions for learning dynamics in deep linear networks trained with bias\nterms. Even when initialized to zero, this simple architectural feature induces\nsubstantial changes in early dynamics. We identify hallmarks of this early OCS\nphase and illustrate how these signatures are observed in deep linear networks\nand larger, more complex (and nonlinear) convolutional neural networks solving\na hierarchical learning task based on MNIST and CIFAR10. We explain these\nobservations by proving that deep linear networks necessarily learn the OCS\nduring early learning. To further probe the generality of our results, we train\nhuman learners over the course of three days on the category learning task. We\nthen identify qualitative signatures of this early OCS phase in terms of the\ndynamics of true negative (correct-rejection) rates. Surprisingly, we find the\nsame early reliance on the OCS in the behaviour of human learners. Finally, we\nshow that learning of the OCS can emerge even in the absence of bias terms and\nis equivalently driven by generic correlations in the input data. Overall, our\nwork suggests the OCS as a universal learning principle in supervised,\nerror-corrective learning, and the mechanistic reasons for its prevalence.\n", "link": "http://arxiv.org/abs/2406.17467v1", "date": "2024-06-25", "relevancy": 2.4841, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5347}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4912}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20learning%20of%20the%20optimal%20constant%20solution%20in%20neural%20networks%20and%0A%20%20humans&body=Title%3A%20Early%20learning%20of%20the%20optimal%20constant%20solution%20in%20neural%20networks%20and%0A%20%20humans%0AAuthor%3A%20Jirko%20Rubruck%20and%20Jan%20P.%20Bauer%20and%20Andrew%20Saxe%20and%20Christopher%20Summerfield%0AAbstract%3A%20%20%20Deep%20neural%20networks%20learn%20increasingly%20complex%20functions%20over%20the%20course%20of%0Atraining.%20Here%2C%20we%20show%20both%20empirically%20and%20theoretically%20that%20learning%20of%20the%0Atarget%20function%20is%20preceded%20by%20an%20early%20phase%20in%20which%20networks%20learn%20the%0Aoptimal%20constant%20solution%20%28OCS%29%20-%20that%20is%2C%20initial%20model%20responses%20mirror%20the%0Adistribution%20of%20target%20labels%2C%20while%20entirely%20ignoring%20information%20provided%20in%0Athe%20input.%20Using%20a%20hierarchical%20category%20learning%20task%2C%20we%20derive%20exact%0Asolutions%20for%20learning%20dynamics%20in%20deep%20linear%20networks%20trained%20with%20bias%0Aterms.%20Even%20when%20initialized%20to%20zero%2C%20this%20simple%20architectural%20feature%20induces%0Asubstantial%20changes%20in%20early%20dynamics.%20We%20identify%20hallmarks%20of%20this%20early%20OCS%0Aphase%20and%20illustrate%20how%20these%20signatures%20are%20observed%20in%20deep%20linear%20networks%0Aand%20larger%2C%20more%20complex%20%28and%20nonlinear%29%20convolutional%20neural%20networks%20solving%0Aa%20hierarchical%20learning%20task%20based%20on%20MNIST%20and%20CIFAR10.%20We%20explain%20these%0Aobservations%20by%20proving%20that%20deep%20linear%20networks%20necessarily%20learn%20the%20OCS%0Aduring%20early%20learning.%20To%20further%20probe%20the%20generality%20of%20our%20results%2C%20we%20train%0Ahuman%20learners%20over%20the%20course%20of%20three%20days%20on%20the%20category%20learning%20task.%20We%0Athen%20identify%20qualitative%20signatures%20of%20this%20early%20OCS%20phase%20in%20terms%20of%20the%0Adynamics%20of%20true%20negative%20%28correct-rejection%29%20rates.%20Surprisingly%2C%20we%20find%20the%0Asame%20early%20reliance%20on%20the%20OCS%20in%20the%20behaviour%20of%20human%20learners.%20Finally%2C%20we%0Ashow%20that%20learning%20of%20the%20OCS%20can%20emerge%20even%20in%20the%20absence%20of%20bias%20terms%20and%0Ais%20equivalently%20driven%20by%20generic%20correlations%20in%20the%20input%20data.%20Overall%2C%20our%0Awork%20suggests%20the%20OCS%20as%20a%20universal%20learning%20principle%20in%20supervised%2C%0Aerror-corrective%20learning%2C%20and%20the%20mechanistic%20reasons%20for%20its%20prevalence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520learning%2520of%2520the%2520optimal%2520constant%2520solution%2520in%2520neural%2520networks%2520and%250A%2520%2520humans%26entry.906535625%3DJirko%2520Rubruck%2520and%2520Jan%2520P.%2520Bauer%2520and%2520Andrew%2520Saxe%2520and%2520Christopher%2520Summerfield%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520learn%2520increasingly%2520complex%2520functions%2520over%2520the%2520course%2520of%250Atraining.%2520Here%252C%2520we%2520show%2520both%2520empirically%2520and%2520theoretically%2520that%2520learning%2520of%2520the%250Atarget%2520function%2520is%2520preceded%2520by%2520an%2520early%2520phase%2520in%2520which%2520networks%2520learn%2520the%250Aoptimal%2520constant%2520solution%2520%2528OCS%2529%2520-%2520that%2520is%252C%2520initial%2520model%2520responses%2520mirror%2520the%250Adistribution%2520of%2520target%2520labels%252C%2520while%2520entirely%2520ignoring%2520information%2520provided%2520in%250Athe%2520input.%2520Using%2520a%2520hierarchical%2520category%2520learning%2520task%252C%2520we%2520derive%2520exact%250Asolutions%2520for%2520learning%2520dynamics%2520in%2520deep%2520linear%2520networks%2520trained%2520with%2520bias%250Aterms.%2520Even%2520when%2520initialized%2520to%2520zero%252C%2520this%2520simple%2520architectural%2520feature%2520induces%250Asubstantial%2520changes%2520in%2520early%2520dynamics.%2520We%2520identify%2520hallmarks%2520of%2520this%2520early%2520OCS%250Aphase%2520and%2520illustrate%2520how%2520these%2520signatures%2520are%2520observed%2520in%2520deep%2520linear%2520networks%250Aand%2520larger%252C%2520more%2520complex%2520%2528and%2520nonlinear%2529%2520convolutional%2520neural%2520networks%2520solving%250Aa%2520hierarchical%2520learning%2520task%2520based%2520on%2520MNIST%2520and%2520CIFAR10.%2520We%2520explain%2520these%250Aobservations%2520by%2520proving%2520that%2520deep%2520linear%2520networks%2520necessarily%2520learn%2520the%2520OCS%250Aduring%2520early%2520learning.%2520To%2520further%2520probe%2520the%2520generality%2520of%2520our%2520results%252C%2520we%2520train%250Ahuman%2520learners%2520over%2520the%2520course%2520of%2520three%2520days%2520on%2520the%2520category%2520learning%2520task.%2520We%250Athen%2520identify%2520qualitative%2520signatures%2520of%2520this%2520early%2520OCS%2520phase%2520in%2520terms%2520of%2520the%250Adynamics%2520of%2520true%2520negative%2520%2528correct-rejection%2529%2520rates.%2520Surprisingly%252C%2520we%2520find%2520the%250Asame%2520early%2520reliance%2520on%2520the%2520OCS%2520in%2520the%2520behaviour%2520of%2520human%2520learners.%2520Finally%252C%2520we%250Ashow%2520that%2520learning%2520of%2520the%2520OCS%2520can%2520emerge%2520even%2520in%2520the%2520absence%2520of%2520bias%2520terms%2520and%250Ais%2520equivalently%2520driven%2520by%2520generic%2520correlations%2520in%2520the%2520input%2520data.%2520Overall%252C%2520our%250Awork%2520suggests%2520the%2520OCS%2520as%2520a%2520universal%2520learning%2520principle%2520in%2520supervised%252C%250Aerror-corrective%2520learning%252C%2520and%2520the%2520mechanistic%2520reasons%2520for%2520its%2520prevalence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20learning%20of%20the%20optimal%20constant%20solution%20in%20neural%20networks%20and%0A%20%20humans&entry.906535625=Jirko%20Rubruck%20and%20Jan%20P.%20Bauer%20and%20Andrew%20Saxe%20and%20Christopher%20Summerfield&entry.1292438233=%20%20Deep%20neural%20networks%20learn%20increasingly%20complex%20functions%20over%20the%20course%20of%0Atraining.%20Here%2C%20we%20show%20both%20empirically%20and%20theoretically%20that%20learning%20of%20the%0Atarget%20function%20is%20preceded%20by%20an%20early%20phase%20in%20which%20networks%20learn%20the%0Aoptimal%20constant%20solution%20%28OCS%29%20-%20that%20is%2C%20initial%20model%20responses%20mirror%20the%0Adistribution%20of%20target%20labels%2C%20while%20entirely%20ignoring%20information%20provided%20in%0Athe%20input.%20Using%20a%20hierarchical%20category%20learning%20task%2C%20we%20derive%20exact%0Asolutions%20for%20learning%20dynamics%20in%20deep%20linear%20networks%20trained%20with%20bias%0Aterms.%20Even%20when%20initialized%20to%20zero%2C%20this%20simple%20architectural%20feature%20induces%0Asubstantial%20changes%20in%20early%20dynamics.%20We%20identify%20hallmarks%20of%20this%20early%20OCS%0Aphase%20and%20illustrate%20how%20these%20signatures%20are%20observed%20in%20deep%20linear%20networks%0Aand%20larger%2C%20more%20complex%20%28and%20nonlinear%29%20convolutional%20neural%20networks%20solving%0Aa%20hierarchical%20learning%20task%20based%20on%20MNIST%20and%20CIFAR10.%20We%20explain%20these%0Aobservations%20by%20proving%20that%20deep%20linear%20networks%20necessarily%20learn%20the%20OCS%0Aduring%20early%20learning.%20To%20further%20probe%20the%20generality%20of%20our%20results%2C%20we%20train%0Ahuman%20learners%20over%20the%20course%20of%20three%20days%20on%20the%20category%20learning%20task.%20We%0Athen%20identify%20qualitative%20signatures%20of%20this%20early%20OCS%20phase%20in%20terms%20of%20the%0Adynamics%20of%20true%20negative%20%28correct-rejection%29%20rates.%20Surprisingly%2C%20we%20find%20the%0Asame%20early%20reliance%20on%20the%20OCS%20in%20the%20behaviour%20of%20human%20learners.%20Finally%2C%20we%0Ashow%20that%20learning%20of%20the%20OCS%20can%20emerge%20even%20in%20the%20absence%20of%20bias%20terms%20and%0Ais%20equivalently%20driven%20by%20generic%20correlations%20in%20the%20input%20data.%20Overall%2C%20our%0Awork%20suggests%20the%20OCS%20as%20a%20universal%20learning%20principle%20in%20supervised%2C%0Aerror-corrective%20learning%2C%20and%20the%20mechanistic%20reasons%20for%20its%20prevalence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17467v1&entry.124074799=Read"},
{"title": "MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for\n  Multi-View 3D Object Detection", "author": "Michelle Adeline and Junn Yong Loo and Vishnu Monn Baskaran", "abstract": "  Multi-view 3D object detection is a crucial component of autonomous driving\nsystems. Contemporary query-based methods primarily depend either on\ndataset-specific initialization of 3D anchors, introducing bias, or utilize\ndense attention mechanisms, which are computationally inefficient and\nunscalable. To overcome these issues, we present MDHA, a novel sparse\nquery-based framework, which constructs adaptive 3D output proposals using\nhybrid anchors from multi-view, multi-scale input. Fixed 2D anchors are\ncombined with depth predictions to form 2.5D anchors, which are projected to\nobtain 3D proposals. To ensure high efficiency, our proposed Anchor Encoder\nperforms sparse refinement and selects the top-k anchors and features.\nMoreover, while existing multi-view attention mechanisms rely on projecting\nreference points to multiple images, our novel Circular Deformable Attention\nmechanism only projects to a single image but allows reference points to\nseamlessly attend to adjacent images, improving efficiency without compromising\non performance. On the nuScenes val set, it achieves 46.4% mAP and 55.0% NDS\nwith a ResNet101 backbone. MDHA significantly outperforms the baseline, where\nanchor proposals are modelled as learnable embeddings.\n", "link": "http://arxiv.org/abs/2406.17654v1", "date": "2024-06-25", "relevancy": 2.4786, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6231}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDHA%3A%20Multi-Scale%20Deformable%20Transformer%20with%20Hybrid%20Anchors%20for%0A%20%20Multi-View%203D%20Object%20Detection&body=Title%3A%20MDHA%3A%20Multi-Scale%20Deformable%20Transformer%20with%20Hybrid%20Anchors%20for%0A%20%20Multi-View%203D%20Object%20Detection%0AAuthor%3A%20Michelle%20Adeline%20and%20Junn%20Yong%20Loo%20and%20Vishnu%20Monn%20Baskaran%0AAbstract%3A%20%20%20Multi-view%203D%20object%20detection%20is%20a%20crucial%20component%20of%20autonomous%20driving%0Asystems.%20Contemporary%20query-based%20methods%20primarily%20depend%20either%20on%0Adataset-specific%20initialization%20of%203D%20anchors%2C%20introducing%20bias%2C%20or%20utilize%0Adense%20attention%20mechanisms%2C%20which%20are%20computationally%20inefficient%20and%0Aunscalable.%20To%20overcome%20these%20issues%2C%20we%20present%20MDHA%2C%20a%20novel%20sparse%0Aquery-based%20framework%2C%20which%20constructs%20adaptive%203D%20output%20proposals%20using%0Ahybrid%20anchors%20from%20multi-view%2C%20multi-scale%20input.%20Fixed%202D%20anchors%20are%0Acombined%20with%20depth%20predictions%20to%20form%202.5D%20anchors%2C%20which%20are%20projected%20to%0Aobtain%203D%20proposals.%20To%20ensure%20high%20efficiency%2C%20our%20proposed%20Anchor%20Encoder%0Aperforms%20sparse%20refinement%20and%20selects%20the%20top-k%20anchors%20and%20features.%0AMoreover%2C%20while%20existing%20multi-view%20attention%20mechanisms%20rely%20on%20projecting%0Areference%20points%20to%20multiple%20images%2C%20our%20novel%20Circular%20Deformable%20Attention%0Amechanism%20only%20projects%20to%20a%20single%20image%20but%20allows%20reference%20points%20to%0Aseamlessly%20attend%20to%20adjacent%20images%2C%20improving%20efficiency%20without%20compromising%0Aon%20performance.%20On%20the%20nuScenes%20val%20set%2C%20it%20achieves%2046.4%25%20mAP%20and%2055.0%25%20NDS%0Awith%20a%20ResNet101%20backbone.%20MDHA%20significantly%20outperforms%20the%20baseline%2C%20where%0Aanchor%20proposals%20are%20modelled%20as%20learnable%20embeddings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDHA%253A%2520Multi-Scale%2520Deformable%2520Transformer%2520with%2520Hybrid%2520Anchors%2520for%250A%2520%2520Multi-View%25203D%2520Object%2520Detection%26entry.906535625%3DMichelle%2520Adeline%2520and%2520Junn%2520Yong%2520Loo%2520and%2520Vishnu%2520Monn%2520Baskaran%26entry.1292438233%3D%2520%2520Multi-view%25203D%2520object%2520detection%2520is%2520a%2520crucial%2520component%2520of%2520autonomous%2520driving%250Asystems.%2520Contemporary%2520query-based%2520methods%2520primarily%2520depend%2520either%2520on%250Adataset-specific%2520initialization%2520of%25203D%2520anchors%252C%2520introducing%2520bias%252C%2520or%2520utilize%250Adense%2520attention%2520mechanisms%252C%2520which%2520are%2520computationally%2520inefficient%2520and%250Aunscalable.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520present%2520MDHA%252C%2520a%2520novel%2520sparse%250Aquery-based%2520framework%252C%2520which%2520constructs%2520adaptive%25203D%2520output%2520proposals%2520using%250Ahybrid%2520anchors%2520from%2520multi-view%252C%2520multi-scale%2520input.%2520Fixed%25202D%2520anchors%2520are%250Acombined%2520with%2520depth%2520predictions%2520to%2520form%25202.5D%2520anchors%252C%2520which%2520are%2520projected%2520to%250Aobtain%25203D%2520proposals.%2520To%2520ensure%2520high%2520efficiency%252C%2520our%2520proposed%2520Anchor%2520Encoder%250Aperforms%2520sparse%2520refinement%2520and%2520selects%2520the%2520top-k%2520anchors%2520and%2520features.%250AMoreover%252C%2520while%2520existing%2520multi-view%2520attention%2520mechanisms%2520rely%2520on%2520projecting%250Areference%2520points%2520to%2520multiple%2520images%252C%2520our%2520novel%2520Circular%2520Deformable%2520Attention%250Amechanism%2520only%2520projects%2520to%2520a%2520single%2520image%2520but%2520allows%2520reference%2520points%2520to%250Aseamlessly%2520attend%2520to%2520adjacent%2520images%252C%2520improving%2520efficiency%2520without%2520compromising%250Aon%2520performance.%2520On%2520the%2520nuScenes%2520val%2520set%252C%2520it%2520achieves%252046.4%2525%2520mAP%2520and%252055.0%2525%2520NDS%250Awith%2520a%2520ResNet101%2520backbone.%2520MDHA%2520significantly%2520outperforms%2520the%2520baseline%252C%2520where%250Aanchor%2520proposals%2520are%2520modelled%2520as%2520learnable%2520embeddings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDHA%3A%20Multi-Scale%20Deformable%20Transformer%20with%20Hybrid%20Anchors%20for%0A%20%20Multi-View%203D%20Object%20Detection&entry.906535625=Michelle%20Adeline%20and%20Junn%20Yong%20Loo%20and%20Vishnu%20Monn%20Baskaran&entry.1292438233=%20%20Multi-view%203D%20object%20detection%20is%20a%20crucial%20component%20of%20autonomous%20driving%0Asystems.%20Contemporary%20query-based%20methods%20primarily%20depend%20either%20on%0Adataset-specific%20initialization%20of%203D%20anchors%2C%20introducing%20bias%2C%20or%20utilize%0Adense%20attention%20mechanisms%2C%20which%20are%20computationally%20inefficient%20and%0Aunscalable.%20To%20overcome%20these%20issues%2C%20we%20present%20MDHA%2C%20a%20novel%20sparse%0Aquery-based%20framework%2C%20which%20constructs%20adaptive%203D%20output%20proposals%20using%0Ahybrid%20anchors%20from%20multi-view%2C%20multi-scale%20input.%20Fixed%202D%20anchors%20are%0Acombined%20with%20depth%20predictions%20to%20form%202.5D%20anchors%2C%20which%20are%20projected%20to%0Aobtain%203D%20proposals.%20To%20ensure%20high%20efficiency%2C%20our%20proposed%20Anchor%20Encoder%0Aperforms%20sparse%20refinement%20and%20selects%20the%20top-k%20anchors%20and%20features.%0AMoreover%2C%20while%20existing%20multi-view%20attention%20mechanisms%20rely%20on%20projecting%0Areference%20points%20to%20multiple%20images%2C%20our%20novel%20Circular%20Deformable%20Attention%0Amechanism%20only%20projects%20to%20a%20single%20image%20but%20allows%20reference%20points%20to%0Aseamlessly%20attend%20to%20adjacent%20images%2C%20improving%20efficiency%20without%20compromising%0Aon%20performance.%20On%20the%20nuScenes%20val%20set%2C%20it%20achieves%2046.4%25%20mAP%20and%2055.0%25%20NDS%0Awith%20a%20ResNet101%20backbone.%20MDHA%20significantly%20outperforms%20the%20baseline%2C%20where%0Aanchor%20proposals%20are%20modelled%20as%20learnable%20embeddings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17654v1&entry.124074799=Read"},
{"title": "Towards Diverse Evaluation of Class Incremental Learning: A\n  Representation Learning Perspective", "author": "Sungmin Cha and Jihwan Kwak and Dongsub Shim and Hyunwoo Kim and Moontae Lee and Honglak Lee and Taesup Moon", "abstract": "  Class incremental learning (CIL) algorithms aim to continually learn new\nobject classes from incrementally arriving data while not forgetting past\nlearned classes. The common evaluation protocol for CIL algorithms is to\nmeasure the average test accuracy across all classes learned so far -- however,\nwe argue that solely focusing on maximizing the test accuracy may not\nnecessarily lead to developing a CIL algorithm that also continually learns and\nupdates the representations, which may be transferred to the downstream tasks.\nTo that end, we experimentally analyze neural network models trained by CIL\nalgorithms using various evaluation protocols in representation learning and\npropose new analysis methods. Our experiments show that most state-of-the-art\nalgorithms prioritize high stability and do not significantly change the\nlearned representation, and sometimes even learn a representation of lower\nquality than a naive baseline. However, we observe that these algorithms can\nstill achieve high test accuracy because they enable a model to learn a\nclassifier that closely resembles an estimated linear classifier trained for\nlinear probing. Furthermore, the base model learned in the first task, which\ninvolves single-task learning, exhibits varying levels of representation\nquality across different algorithms, and this variance impacts the final\nperformance of CIL algorithms. Therefore, we suggest that the\nrepresentation-level evaluation should be considered as an additional recipe\nfor more diverse evaluation for CIL algorithms.\n", "link": "http://arxiv.org/abs/2206.08101v3", "date": "2024-06-25", "relevancy": 2.4284, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.492}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4883}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Diverse%20Evaluation%20of%20Class%20Incremental%20Learning%3A%20A%0A%20%20Representation%20Learning%20Perspective&body=Title%3A%20Towards%20Diverse%20Evaluation%20of%20Class%20Incremental%20Learning%3A%20A%0A%20%20Representation%20Learning%20Perspective%0AAuthor%3A%20Sungmin%20Cha%20and%20Jihwan%20Kwak%20and%20Dongsub%20Shim%20and%20Hyunwoo%20Kim%20and%20Moontae%20Lee%20and%20Honglak%20Lee%20and%20Taesup%20Moon%0AAbstract%3A%20%20%20Class%20incremental%20learning%20%28CIL%29%20algorithms%20aim%20to%20continually%20learn%20new%0Aobject%20classes%20from%20incrementally%20arriving%20data%20while%20not%20forgetting%20past%0Alearned%20classes.%20The%20common%20evaluation%20protocol%20for%20CIL%20algorithms%20is%20to%0Ameasure%20the%20average%20test%20accuracy%20across%20all%20classes%20learned%20so%20far%20--%20however%2C%0Awe%20argue%20that%20solely%20focusing%20on%20maximizing%20the%20test%20accuracy%20may%20not%0Anecessarily%20lead%20to%20developing%20a%20CIL%20algorithm%20that%20also%20continually%20learns%20and%0Aupdates%20the%20representations%2C%20which%20may%20be%20transferred%20to%20the%20downstream%20tasks.%0ATo%20that%20end%2C%20we%20experimentally%20analyze%20neural%20network%20models%20trained%20by%20CIL%0Aalgorithms%20using%20various%20evaluation%20protocols%20in%20representation%20learning%20and%0Apropose%20new%20analysis%20methods.%20Our%20experiments%20show%20that%20most%20state-of-the-art%0Aalgorithms%20prioritize%20high%20stability%20and%20do%20not%20significantly%20change%20the%0Alearned%20representation%2C%20and%20sometimes%20even%20learn%20a%20representation%20of%20lower%0Aquality%20than%20a%20naive%20baseline.%20However%2C%20we%20observe%20that%20these%20algorithms%20can%0Astill%20achieve%20high%20test%20accuracy%20because%20they%20enable%20a%20model%20to%20learn%20a%0Aclassifier%20that%20closely%20resembles%20an%20estimated%20linear%20classifier%20trained%20for%0Alinear%20probing.%20Furthermore%2C%20the%20base%20model%20learned%20in%20the%20first%20task%2C%20which%0Ainvolves%20single-task%20learning%2C%20exhibits%20varying%20levels%20of%20representation%0Aquality%20across%20different%20algorithms%2C%20and%20this%20variance%20impacts%20the%20final%0Aperformance%20of%20CIL%20algorithms.%20Therefore%2C%20we%20suggest%20that%20the%0Arepresentation-level%20evaluation%20should%20be%20considered%20as%20an%20additional%20recipe%0Afor%20more%20diverse%20evaluation%20for%20CIL%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.08101v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Diverse%2520Evaluation%2520of%2520Class%2520Incremental%2520Learning%253A%2520A%250A%2520%2520Representation%2520Learning%2520Perspective%26entry.906535625%3DSungmin%2520Cha%2520and%2520Jihwan%2520Kwak%2520and%2520Dongsub%2520Shim%2520and%2520Hyunwoo%2520Kim%2520and%2520Moontae%2520Lee%2520and%2520Honglak%2520Lee%2520and%2520Taesup%2520Moon%26entry.1292438233%3D%2520%2520Class%2520incremental%2520learning%2520%2528CIL%2529%2520algorithms%2520aim%2520to%2520continually%2520learn%2520new%250Aobject%2520classes%2520from%2520incrementally%2520arriving%2520data%2520while%2520not%2520forgetting%2520past%250Alearned%2520classes.%2520The%2520common%2520evaluation%2520protocol%2520for%2520CIL%2520algorithms%2520is%2520to%250Ameasure%2520the%2520average%2520test%2520accuracy%2520across%2520all%2520classes%2520learned%2520so%2520far%2520--%2520however%252C%250Awe%2520argue%2520that%2520solely%2520focusing%2520on%2520maximizing%2520the%2520test%2520accuracy%2520may%2520not%250Anecessarily%2520lead%2520to%2520developing%2520a%2520CIL%2520algorithm%2520that%2520also%2520continually%2520learns%2520and%250Aupdates%2520the%2520representations%252C%2520which%2520may%2520be%2520transferred%2520to%2520the%2520downstream%2520tasks.%250ATo%2520that%2520end%252C%2520we%2520experimentally%2520analyze%2520neural%2520network%2520models%2520trained%2520by%2520CIL%250Aalgorithms%2520using%2520various%2520evaluation%2520protocols%2520in%2520representation%2520learning%2520and%250Apropose%2520new%2520analysis%2520methods.%2520Our%2520experiments%2520show%2520that%2520most%2520state-of-the-art%250Aalgorithms%2520prioritize%2520high%2520stability%2520and%2520do%2520not%2520significantly%2520change%2520the%250Alearned%2520representation%252C%2520and%2520sometimes%2520even%2520learn%2520a%2520representation%2520of%2520lower%250Aquality%2520than%2520a%2520naive%2520baseline.%2520However%252C%2520we%2520observe%2520that%2520these%2520algorithms%2520can%250Astill%2520achieve%2520high%2520test%2520accuracy%2520because%2520they%2520enable%2520a%2520model%2520to%2520learn%2520a%250Aclassifier%2520that%2520closely%2520resembles%2520an%2520estimated%2520linear%2520classifier%2520trained%2520for%250Alinear%2520probing.%2520Furthermore%252C%2520the%2520base%2520model%2520learned%2520in%2520the%2520first%2520task%252C%2520which%250Ainvolves%2520single-task%2520learning%252C%2520exhibits%2520varying%2520levels%2520of%2520representation%250Aquality%2520across%2520different%2520algorithms%252C%2520and%2520this%2520variance%2520impacts%2520the%2520final%250Aperformance%2520of%2520CIL%2520algorithms.%2520Therefore%252C%2520we%2520suggest%2520that%2520the%250Arepresentation-level%2520evaluation%2520should%2520be%2520considered%2520as%2520an%2520additional%2520recipe%250Afor%2520more%2520diverse%2520evaluation%2520for%2520CIL%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.08101v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Diverse%20Evaluation%20of%20Class%20Incremental%20Learning%3A%20A%0A%20%20Representation%20Learning%20Perspective&entry.906535625=Sungmin%20Cha%20and%20Jihwan%20Kwak%20and%20Dongsub%20Shim%20and%20Hyunwoo%20Kim%20and%20Moontae%20Lee%20and%20Honglak%20Lee%20and%20Taesup%20Moon&entry.1292438233=%20%20Class%20incremental%20learning%20%28CIL%29%20algorithms%20aim%20to%20continually%20learn%20new%0Aobject%20classes%20from%20incrementally%20arriving%20data%20while%20not%20forgetting%20past%0Alearned%20classes.%20The%20common%20evaluation%20protocol%20for%20CIL%20algorithms%20is%20to%0Ameasure%20the%20average%20test%20accuracy%20across%20all%20classes%20learned%20so%20far%20--%20however%2C%0Awe%20argue%20that%20solely%20focusing%20on%20maximizing%20the%20test%20accuracy%20may%20not%0Anecessarily%20lead%20to%20developing%20a%20CIL%20algorithm%20that%20also%20continually%20learns%20and%0Aupdates%20the%20representations%2C%20which%20may%20be%20transferred%20to%20the%20downstream%20tasks.%0ATo%20that%20end%2C%20we%20experimentally%20analyze%20neural%20network%20models%20trained%20by%20CIL%0Aalgorithms%20using%20various%20evaluation%20protocols%20in%20representation%20learning%20and%0Apropose%20new%20analysis%20methods.%20Our%20experiments%20show%20that%20most%20state-of-the-art%0Aalgorithms%20prioritize%20high%20stability%20and%20do%20not%20significantly%20change%20the%0Alearned%20representation%2C%20and%20sometimes%20even%20learn%20a%20representation%20of%20lower%0Aquality%20than%20a%20naive%20baseline.%20However%2C%20we%20observe%20that%20these%20algorithms%20can%0Astill%20achieve%20high%20test%20accuracy%20because%20they%20enable%20a%20model%20to%20learn%20a%0Aclassifier%20that%20closely%20resembles%20an%20estimated%20linear%20classifier%20trained%20for%0Alinear%20probing.%20Furthermore%2C%20the%20base%20model%20learned%20in%20the%20first%20task%2C%20which%0Ainvolves%20single-task%20learning%2C%20exhibits%20varying%20levels%20of%20representation%0Aquality%20across%20different%20algorithms%2C%20and%20this%20variance%20impacts%20the%20final%0Aperformance%20of%20CIL%20algorithms.%20Therefore%2C%20we%20suggest%20that%20the%0Arepresentation-level%20evaluation%20should%20be%20considered%20as%20an%20additional%20recipe%0Afor%20more%20diverse%20evaluation%20for%20CIL%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.08101v3&entry.124074799=Read"},
{"title": "Video Inpainting Localization with Contrastive Learning", "author": "Zijie Lou and Gang Cao and Man Lin", "abstract": "  Deep video inpainting is typically used as malicious manipulation to remove\nimportant objects for creating fake videos. It is significant to identify the\ninpainted regions blindly. This letter proposes a simple yet effective forensic\nscheme for Video Inpainting LOcalization with ContrAstive Learning (ViLocal).\nSpecifically, a 3D Uniformer encoder is applied to the video noise residual for\nlearning effective spatiotemporal forensic features. To enhance the\ndiscriminative power, supervised contrastive learning is adopted to capture the\nlocal inconsistency of inpainted videos through attracting/repelling the\npositive/negative pristine and forged pixel pairs. A pixel-wise inpainting\nlocalization map is yielded by a lightweight convolution decoder with a\nspecialized two-stage training strategy. To prepare enough training samples, we\nbuild a video object segmentation dataset of 2500 videos with pixel-level\nannotations per frame. Extensive experimental results validate the superiority\nof ViLocal over state-of-the-arts. Code and dataset will be available at\nhttps://github.com/multimediaFor/ViLocal.\n", "link": "http://arxiv.org/abs/2406.17628v1", "date": "2024-06-25", "relevancy": 2.4087, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6195}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.592}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Inpainting%20Localization%20with%20Contrastive%20Learning&body=Title%3A%20Video%20Inpainting%20Localization%20with%20Contrastive%20Learning%0AAuthor%3A%20Zijie%20Lou%20and%20Gang%20Cao%20and%20Man%20Lin%0AAbstract%3A%20%20%20Deep%20video%20inpainting%20is%20typically%20used%20as%20malicious%20manipulation%20to%20remove%0Aimportant%20objects%20for%20creating%20fake%20videos.%20It%20is%20significant%20to%20identify%20the%0Ainpainted%20regions%20blindly.%20This%20letter%20proposes%20a%20simple%20yet%20effective%20forensic%0Ascheme%20for%20Video%20Inpainting%20LOcalization%20with%20ContrAstive%20Learning%20%28ViLocal%29.%0ASpecifically%2C%20a%203D%20Uniformer%20encoder%20is%20applied%20to%20the%20video%20noise%20residual%20for%0Alearning%20effective%20spatiotemporal%20forensic%20features.%20To%20enhance%20the%0Adiscriminative%20power%2C%20supervised%20contrastive%20learning%20is%20adopted%20to%20capture%20the%0Alocal%20inconsistency%20of%20inpainted%20videos%20through%20attracting/repelling%20the%0Apositive/negative%20pristine%20and%20forged%20pixel%20pairs.%20A%20pixel-wise%20inpainting%0Alocalization%20map%20is%20yielded%20by%20a%20lightweight%20convolution%20decoder%20with%20a%0Aspecialized%20two-stage%20training%20strategy.%20To%20prepare%20enough%20training%20samples%2C%20we%0Abuild%20a%20video%20object%20segmentation%20dataset%20of%202500%20videos%20with%20pixel-level%0Aannotations%20per%20frame.%20Extensive%20experimental%20results%20validate%20the%20superiority%0Aof%20ViLocal%20over%20state-of-the-arts.%20Code%20and%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/multimediaFor/ViLocal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Inpainting%2520Localization%2520with%2520Contrastive%2520Learning%26entry.906535625%3DZijie%2520Lou%2520and%2520Gang%2520Cao%2520and%2520Man%2520Lin%26entry.1292438233%3D%2520%2520Deep%2520video%2520inpainting%2520is%2520typically%2520used%2520as%2520malicious%2520manipulation%2520to%2520remove%250Aimportant%2520objects%2520for%2520creating%2520fake%2520videos.%2520It%2520is%2520significant%2520to%2520identify%2520the%250Ainpainted%2520regions%2520blindly.%2520This%2520letter%2520proposes%2520a%2520simple%2520yet%2520effective%2520forensic%250Ascheme%2520for%2520Video%2520Inpainting%2520LOcalization%2520with%2520ContrAstive%2520Learning%2520%2528ViLocal%2529.%250ASpecifically%252C%2520a%25203D%2520Uniformer%2520encoder%2520is%2520applied%2520to%2520the%2520video%2520noise%2520residual%2520for%250Alearning%2520effective%2520spatiotemporal%2520forensic%2520features.%2520To%2520enhance%2520the%250Adiscriminative%2520power%252C%2520supervised%2520contrastive%2520learning%2520is%2520adopted%2520to%2520capture%2520the%250Alocal%2520inconsistency%2520of%2520inpainted%2520videos%2520through%2520attracting/repelling%2520the%250Apositive/negative%2520pristine%2520and%2520forged%2520pixel%2520pairs.%2520A%2520pixel-wise%2520inpainting%250Alocalization%2520map%2520is%2520yielded%2520by%2520a%2520lightweight%2520convolution%2520decoder%2520with%2520a%250Aspecialized%2520two-stage%2520training%2520strategy.%2520To%2520prepare%2520enough%2520training%2520samples%252C%2520we%250Abuild%2520a%2520video%2520object%2520segmentation%2520dataset%2520of%25202500%2520videos%2520with%2520pixel-level%250Aannotations%2520per%2520frame.%2520Extensive%2520experimental%2520results%2520validate%2520the%2520superiority%250Aof%2520ViLocal%2520over%2520state-of-the-arts.%2520Code%2520and%2520dataset%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/multimediaFor/ViLocal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Inpainting%20Localization%20with%20Contrastive%20Learning&entry.906535625=Zijie%20Lou%20and%20Gang%20Cao%20and%20Man%20Lin&entry.1292438233=%20%20Deep%20video%20inpainting%20is%20typically%20used%20as%20malicious%20manipulation%20to%20remove%0Aimportant%20objects%20for%20creating%20fake%20videos.%20It%20is%20significant%20to%20identify%20the%0Ainpainted%20regions%20blindly.%20This%20letter%20proposes%20a%20simple%20yet%20effective%20forensic%0Ascheme%20for%20Video%20Inpainting%20LOcalization%20with%20ContrAstive%20Learning%20%28ViLocal%29.%0ASpecifically%2C%20a%203D%20Uniformer%20encoder%20is%20applied%20to%20the%20video%20noise%20residual%20for%0Alearning%20effective%20spatiotemporal%20forensic%20features.%20To%20enhance%20the%0Adiscriminative%20power%2C%20supervised%20contrastive%20learning%20is%20adopted%20to%20capture%20the%0Alocal%20inconsistency%20of%20inpainted%20videos%20through%20attracting/repelling%20the%0Apositive/negative%20pristine%20and%20forged%20pixel%20pairs.%20A%20pixel-wise%20inpainting%0Alocalization%20map%20is%20yielded%20by%20a%20lightweight%20convolution%20decoder%20with%20a%0Aspecialized%20two-stage%20training%20strategy.%20To%20prepare%20enough%20training%20samples%2C%20we%0Abuild%20a%20video%20object%20segmentation%20dataset%20of%202500%20videos%20with%20pixel-level%0Aannotations%20per%20frame.%20Extensive%20experimental%20results%20validate%20the%20superiority%0Aof%20ViLocal%20over%20state-of-the-arts.%20Code%20and%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/multimediaFor/ViLocal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17628v1&entry.124074799=Read"},
{"title": "SincVAE: a New Approach to Improve Anomaly Detection on EEG Data Using\n  SincNet and Variational Autoencoder", "author": "Andrea Pollastro and Francesco Isgr\u00f2 and Roberto Prevete", "abstract": "  Over the past few decades, electroencephalography (EEG) monitoring has become\na pivotal tool for diagnosing neurological disorders, particularly for\ndetecting seizures. Epilepsy, one of the most prevalent neurological diseases\nworldwide, affects approximately the 1 \\% of the population. These patients\nface significant risks, underscoring the need for reliable, continuous seizure\nmonitoring in daily life. Most of the techniques discussed in the literature\nrely on supervised Machine Learning (ML) methods. However, the challenge of\naccurately labeling variations in epileptic EEG waveforms complicates the use\nof these approaches. Additionally, the rarity of ictal events introduces an\nhigh imbalancing within the data, which could lead to poor prediction\nperformance in supervised learning approaches. Instead, a semi-supervised\napproach allows to train the model only on data not containing seizures, thus\navoiding the issues related to the data imbalancing. This work proposes a\nsemi-supervised approach for detecting epileptic seizures from EEG data,\nutilizing a novel Deep Learning-based method called SincVAE. This proposal\nincorporates the learning of an ad-hoc array of bandpass filter as a first\nlayer of a Variational Autoencoder (VAE), potentially eliminating the\npreprocessing stage where informative band frequencies are identified and\nisolated. Results indicate that SincVAE improves seizure detection in EEG data\nand is capable of identifying early seizures during the preictal stage as well\nas monitoring patients throughout the postictal stage.\n", "link": "http://arxiv.org/abs/2406.17537v1", "date": "2024-06-25", "relevancy": 2.3811, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4909}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4717}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SincVAE%3A%20a%20New%20Approach%20to%20Improve%20Anomaly%20Detection%20on%20EEG%20Data%20Using%0A%20%20SincNet%20and%20Variational%20Autoencoder&body=Title%3A%20SincVAE%3A%20a%20New%20Approach%20to%20Improve%20Anomaly%20Detection%20on%20EEG%20Data%20Using%0A%20%20SincNet%20and%20Variational%20Autoencoder%0AAuthor%3A%20Andrea%20Pollastro%20and%20Francesco%20Isgr%C3%B2%20and%20Roberto%20Prevete%0AAbstract%3A%20%20%20Over%20the%20past%20few%20decades%2C%20electroencephalography%20%28EEG%29%20monitoring%20has%20become%0Aa%20pivotal%20tool%20for%20diagnosing%20neurological%20disorders%2C%20particularly%20for%0Adetecting%20seizures.%20Epilepsy%2C%20one%20of%20the%20most%20prevalent%20neurological%20diseases%0Aworldwide%2C%20affects%20approximately%20the%201%20%5C%25%20of%20the%20population.%20These%20patients%0Aface%20significant%20risks%2C%20underscoring%20the%20need%20for%20reliable%2C%20continuous%20seizure%0Amonitoring%20in%20daily%20life.%20Most%20of%20the%20techniques%20discussed%20in%20the%20literature%0Arely%20on%20supervised%20Machine%20Learning%20%28ML%29%20methods.%20However%2C%20the%20challenge%20of%0Aaccurately%20labeling%20variations%20in%20epileptic%20EEG%20waveforms%20complicates%20the%20use%0Aof%20these%20approaches.%20Additionally%2C%20the%20rarity%20of%20ictal%20events%20introduces%20an%0Ahigh%20imbalancing%20within%20the%20data%2C%20which%20could%20lead%20to%20poor%20prediction%0Aperformance%20in%20supervised%20learning%20approaches.%20Instead%2C%20a%20semi-supervised%0Aapproach%20allows%20to%20train%20the%20model%20only%20on%20data%20not%20containing%20seizures%2C%20thus%0Aavoiding%20the%20issues%20related%20to%20the%20data%20imbalancing.%20This%20work%20proposes%20a%0Asemi-supervised%20approach%20for%20detecting%20epileptic%20seizures%20from%20EEG%20data%2C%0Autilizing%20a%20novel%20Deep%20Learning-based%20method%20called%20SincVAE.%20This%20proposal%0Aincorporates%20the%20learning%20of%20an%20ad-hoc%20array%20of%20bandpass%20filter%20as%20a%20first%0Alayer%20of%20a%20Variational%20Autoencoder%20%28VAE%29%2C%20potentially%20eliminating%20the%0Apreprocessing%20stage%20where%20informative%20band%20frequencies%20are%20identified%20and%0Aisolated.%20Results%20indicate%20that%20SincVAE%20improves%20seizure%20detection%20in%20EEG%20data%0Aand%20is%20capable%20of%20identifying%20early%20seizures%20during%20the%20preictal%20stage%20as%20well%0Aas%20monitoring%20patients%20throughout%20the%20postictal%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSincVAE%253A%2520a%2520New%2520Approach%2520to%2520Improve%2520Anomaly%2520Detection%2520on%2520EEG%2520Data%2520Using%250A%2520%2520SincNet%2520and%2520Variational%2520Autoencoder%26entry.906535625%3DAndrea%2520Pollastro%2520and%2520Francesco%2520Isgr%25C3%25B2%2520and%2520Roberto%2520Prevete%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520few%2520decades%252C%2520electroencephalography%2520%2528EEG%2529%2520monitoring%2520has%2520become%250Aa%2520pivotal%2520tool%2520for%2520diagnosing%2520neurological%2520disorders%252C%2520particularly%2520for%250Adetecting%2520seizures.%2520Epilepsy%252C%2520one%2520of%2520the%2520most%2520prevalent%2520neurological%2520diseases%250Aworldwide%252C%2520affects%2520approximately%2520the%25201%2520%255C%2525%2520of%2520the%2520population.%2520These%2520patients%250Aface%2520significant%2520risks%252C%2520underscoring%2520the%2520need%2520for%2520reliable%252C%2520continuous%2520seizure%250Amonitoring%2520in%2520daily%2520life.%2520Most%2520of%2520the%2520techniques%2520discussed%2520in%2520the%2520literature%250Arely%2520on%2520supervised%2520Machine%2520Learning%2520%2528ML%2529%2520methods.%2520However%252C%2520the%2520challenge%2520of%250Aaccurately%2520labeling%2520variations%2520in%2520epileptic%2520EEG%2520waveforms%2520complicates%2520the%2520use%250Aof%2520these%2520approaches.%2520Additionally%252C%2520the%2520rarity%2520of%2520ictal%2520events%2520introduces%2520an%250Ahigh%2520imbalancing%2520within%2520the%2520data%252C%2520which%2520could%2520lead%2520to%2520poor%2520prediction%250Aperformance%2520in%2520supervised%2520learning%2520approaches.%2520Instead%252C%2520a%2520semi-supervised%250Aapproach%2520allows%2520to%2520train%2520the%2520model%2520only%2520on%2520data%2520not%2520containing%2520seizures%252C%2520thus%250Aavoiding%2520the%2520issues%2520related%2520to%2520the%2520data%2520imbalancing.%2520This%2520work%2520proposes%2520a%250Asemi-supervised%2520approach%2520for%2520detecting%2520epileptic%2520seizures%2520from%2520EEG%2520data%252C%250Autilizing%2520a%2520novel%2520Deep%2520Learning-based%2520method%2520called%2520SincVAE.%2520This%2520proposal%250Aincorporates%2520the%2520learning%2520of%2520an%2520ad-hoc%2520array%2520of%2520bandpass%2520filter%2520as%2520a%2520first%250Alayer%2520of%2520a%2520Variational%2520Autoencoder%2520%2528VAE%2529%252C%2520potentially%2520eliminating%2520the%250Apreprocessing%2520stage%2520where%2520informative%2520band%2520frequencies%2520are%2520identified%2520and%250Aisolated.%2520Results%2520indicate%2520that%2520SincVAE%2520improves%2520seizure%2520detection%2520in%2520EEG%2520data%250Aand%2520is%2520capable%2520of%2520identifying%2520early%2520seizures%2520during%2520the%2520preictal%2520stage%2520as%2520well%250Aas%2520monitoring%2520patients%2520throughout%2520the%2520postictal%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SincVAE%3A%20a%20New%20Approach%20to%20Improve%20Anomaly%20Detection%20on%20EEG%20Data%20Using%0A%20%20SincNet%20and%20Variational%20Autoencoder&entry.906535625=Andrea%20Pollastro%20and%20Francesco%20Isgr%C3%B2%20and%20Roberto%20Prevete&entry.1292438233=%20%20Over%20the%20past%20few%20decades%2C%20electroencephalography%20%28EEG%29%20monitoring%20has%20become%0Aa%20pivotal%20tool%20for%20diagnosing%20neurological%20disorders%2C%20particularly%20for%0Adetecting%20seizures.%20Epilepsy%2C%20one%20of%20the%20most%20prevalent%20neurological%20diseases%0Aworldwide%2C%20affects%20approximately%20the%201%20%5C%25%20of%20the%20population.%20These%20patients%0Aface%20significant%20risks%2C%20underscoring%20the%20need%20for%20reliable%2C%20continuous%20seizure%0Amonitoring%20in%20daily%20life.%20Most%20of%20the%20techniques%20discussed%20in%20the%20literature%0Arely%20on%20supervised%20Machine%20Learning%20%28ML%29%20methods.%20However%2C%20the%20challenge%20of%0Aaccurately%20labeling%20variations%20in%20epileptic%20EEG%20waveforms%20complicates%20the%20use%0Aof%20these%20approaches.%20Additionally%2C%20the%20rarity%20of%20ictal%20events%20introduces%20an%0Ahigh%20imbalancing%20within%20the%20data%2C%20which%20could%20lead%20to%20poor%20prediction%0Aperformance%20in%20supervised%20learning%20approaches.%20Instead%2C%20a%20semi-supervised%0Aapproach%20allows%20to%20train%20the%20model%20only%20on%20data%20not%20containing%20seizures%2C%20thus%0Aavoiding%20the%20issues%20related%20to%20the%20data%20imbalancing.%20This%20work%20proposes%20a%0Asemi-supervised%20approach%20for%20detecting%20epileptic%20seizures%20from%20EEG%20data%2C%0Autilizing%20a%20novel%20Deep%20Learning-based%20method%20called%20SincVAE.%20This%20proposal%0Aincorporates%20the%20learning%20of%20an%20ad-hoc%20array%20of%20bandpass%20filter%20as%20a%20first%0Alayer%20of%20a%20Variational%20Autoencoder%20%28VAE%29%2C%20potentially%20eliminating%20the%0Apreprocessing%20stage%20where%20informative%20band%20frequencies%20are%20identified%20and%0Aisolated.%20Results%20indicate%20that%20SincVAE%20improves%20seizure%20detection%20in%20EEG%20data%0Aand%20is%20capable%20of%20identifying%20early%20seizures%20during%20the%20preictal%20stage%20as%20well%0Aas%20monitoring%20patients%20throughout%20the%20postictal%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17537v1&entry.124074799=Read"},
{"title": "Advancing Cell Detection in Anterior Segment Optical Coherence\n  Tomography Images", "author": "Boyu Chen and Ameenat L. Solebo and Paul Taylor", "abstract": "  Anterior uveitis, a common form of eye inflammation, can lead to permanent\nvision loss if not promptly diagnosed. Monitoring this condition involves\nquantifying inflammatory cells in the anterior chamber (AC) of the eye, which\ncan be captured using Anterior Segment Optical Coherence Tomography (AS-OCT).\nHowever, manually identifying cells in AS-OCT images is time-consuming and\nsubjective. Moreover, existing automated approaches may have limitations in\nboth the effectiveness of detecting cells and the reliability of their\ndetection results. To address these challenges, we propose an automated\nframework to detect cells in the AS-OCT images. This framework consists of a\nzero-shot chamber segmentation module and a cell detection module. The first\nmodule segments the AC area in the image without requiring human-annotated\ntraining data. Subsequently, the second module identifies individual cells\nwithin the segmented AC region. Through experiments, our framework demonstrates\nsuperior performance compared to current state-of-the-art methods for both AC\nsegmentation and cell detection tasks. Notably, we find that previous cell\ndetection approaches could suffer from low recall, potentially overlooking a\nsignificant number of cells. In contrast, our framework offers an improved\nsolution, which could benefit the diagnosis and study of anterior uveitis. Our\ncode for cell detection is publicly available at:\nhttps://github.com/joeybyc/cell_detection.\n", "link": "http://arxiv.org/abs/2406.17577v1", "date": "2024-06-25", "relevancy": 2.3305, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.482}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4582}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Cell%20Detection%20in%20Anterior%20Segment%20Optical%20Coherence%0A%20%20Tomography%20Images&body=Title%3A%20Advancing%20Cell%20Detection%20in%20Anterior%20Segment%20Optical%20Coherence%0A%20%20Tomography%20Images%0AAuthor%3A%20Boyu%20Chen%20and%20Ameenat%20L.%20Solebo%20and%20Paul%20Taylor%0AAbstract%3A%20%20%20Anterior%20uveitis%2C%20a%20common%20form%20of%20eye%20inflammation%2C%20can%20lead%20to%20permanent%0Avision%20loss%20if%20not%20promptly%20diagnosed.%20Monitoring%20this%20condition%20involves%0Aquantifying%20inflammatory%20cells%20in%20the%20anterior%20chamber%20%28AC%29%20of%20the%20eye%2C%20which%0Acan%20be%20captured%20using%20Anterior%20Segment%20Optical%20Coherence%20Tomography%20%28AS-OCT%29.%0AHowever%2C%20manually%20identifying%20cells%20in%20AS-OCT%20images%20is%20time-consuming%20and%0Asubjective.%20Moreover%2C%20existing%20automated%20approaches%20may%20have%20limitations%20in%0Aboth%20the%20effectiveness%20of%20detecting%20cells%20and%20the%20reliability%20of%20their%0Adetection%20results.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20automated%0Aframework%20to%20detect%20cells%20in%20the%20AS-OCT%20images.%20This%20framework%20consists%20of%20a%0Azero-shot%20chamber%20segmentation%20module%20and%20a%20cell%20detection%20module.%20The%20first%0Amodule%20segments%20the%20AC%20area%20in%20the%20image%20without%20requiring%20human-annotated%0Atraining%20data.%20Subsequently%2C%20the%20second%20module%20identifies%20individual%20cells%0Awithin%20the%20segmented%20AC%20region.%20Through%20experiments%2C%20our%20framework%20demonstrates%0Asuperior%20performance%20compared%20to%20current%20state-of-the-art%20methods%20for%20both%20AC%0Asegmentation%20and%20cell%20detection%20tasks.%20Notably%2C%20we%20find%20that%20previous%20cell%0Adetection%20approaches%20could%20suffer%20from%20low%20recall%2C%20potentially%20overlooking%20a%0Asignificant%20number%20of%20cells.%20In%20contrast%2C%20our%20framework%20offers%20an%20improved%0Asolution%2C%20which%20could%20benefit%20the%20diagnosis%20and%20study%20of%20anterior%20uveitis.%20Our%0Acode%20for%20cell%20detection%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/joeybyc/cell_detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Cell%2520Detection%2520in%2520Anterior%2520Segment%2520Optical%2520Coherence%250A%2520%2520Tomography%2520Images%26entry.906535625%3DBoyu%2520Chen%2520and%2520Ameenat%2520L.%2520Solebo%2520and%2520Paul%2520Taylor%26entry.1292438233%3D%2520%2520Anterior%2520uveitis%252C%2520a%2520common%2520form%2520of%2520eye%2520inflammation%252C%2520can%2520lead%2520to%2520permanent%250Avision%2520loss%2520if%2520not%2520promptly%2520diagnosed.%2520Monitoring%2520this%2520condition%2520involves%250Aquantifying%2520inflammatory%2520cells%2520in%2520the%2520anterior%2520chamber%2520%2528AC%2529%2520of%2520the%2520eye%252C%2520which%250Acan%2520be%2520captured%2520using%2520Anterior%2520Segment%2520Optical%2520Coherence%2520Tomography%2520%2528AS-OCT%2529.%250AHowever%252C%2520manually%2520identifying%2520cells%2520in%2520AS-OCT%2520images%2520is%2520time-consuming%2520and%250Asubjective.%2520Moreover%252C%2520existing%2520automated%2520approaches%2520may%2520have%2520limitations%2520in%250Aboth%2520the%2520effectiveness%2520of%2520detecting%2520cells%2520and%2520the%2520reliability%2520of%2520their%250Adetection%2520results.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520automated%250Aframework%2520to%2520detect%2520cells%2520in%2520the%2520AS-OCT%2520images.%2520This%2520framework%2520consists%2520of%2520a%250Azero-shot%2520chamber%2520segmentation%2520module%2520and%2520a%2520cell%2520detection%2520module.%2520The%2520first%250Amodule%2520segments%2520the%2520AC%2520area%2520in%2520the%2520image%2520without%2520requiring%2520human-annotated%250Atraining%2520data.%2520Subsequently%252C%2520the%2520second%2520module%2520identifies%2520individual%2520cells%250Awithin%2520the%2520segmented%2520AC%2520region.%2520Through%2520experiments%252C%2520our%2520framework%2520demonstrates%250Asuperior%2520performance%2520compared%2520to%2520current%2520state-of-the-art%2520methods%2520for%2520both%2520AC%250Asegmentation%2520and%2520cell%2520detection%2520tasks.%2520Notably%252C%2520we%2520find%2520that%2520previous%2520cell%250Adetection%2520approaches%2520could%2520suffer%2520from%2520low%2520recall%252C%2520potentially%2520overlooking%2520a%250Asignificant%2520number%2520of%2520cells.%2520In%2520contrast%252C%2520our%2520framework%2520offers%2520an%2520improved%250Asolution%252C%2520which%2520could%2520benefit%2520the%2520diagnosis%2520and%2520study%2520of%2520anterior%2520uveitis.%2520Our%250Acode%2520for%2520cell%2520detection%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/joeybyc/cell_detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Cell%20Detection%20in%20Anterior%20Segment%20Optical%20Coherence%0A%20%20Tomography%20Images&entry.906535625=Boyu%20Chen%20and%20Ameenat%20L.%20Solebo%20and%20Paul%20Taylor&entry.1292438233=%20%20Anterior%20uveitis%2C%20a%20common%20form%20of%20eye%20inflammation%2C%20can%20lead%20to%20permanent%0Avision%20loss%20if%20not%20promptly%20diagnosed.%20Monitoring%20this%20condition%20involves%0Aquantifying%20inflammatory%20cells%20in%20the%20anterior%20chamber%20%28AC%29%20of%20the%20eye%2C%20which%0Acan%20be%20captured%20using%20Anterior%20Segment%20Optical%20Coherence%20Tomography%20%28AS-OCT%29.%0AHowever%2C%20manually%20identifying%20cells%20in%20AS-OCT%20images%20is%20time-consuming%20and%0Asubjective.%20Moreover%2C%20existing%20automated%20approaches%20may%20have%20limitations%20in%0Aboth%20the%20effectiveness%20of%20detecting%20cells%20and%20the%20reliability%20of%20their%0Adetection%20results.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20automated%0Aframework%20to%20detect%20cells%20in%20the%20AS-OCT%20images.%20This%20framework%20consists%20of%20a%0Azero-shot%20chamber%20segmentation%20module%20and%20a%20cell%20detection%20module.%20The%20first%0Amodule%20segments%20the%20AC%20area%20in%20the%20image%20without%20requiring%20human-annotated%0Atraining%20data.%20Subsequently%2C%20the%20second%20module%20identifies%20individual%20cells%0Awithin%20the%20segmented%20AC%20region.%20Through%20experiments%2C%20our%20framework%20demonstrates%0Asuperior%20performance%20compared%20to%20current%20state-of-the-art%20methods%20for%20both%20AC%0Asegmentation%20and%20cell%20detection%20tasks.%20Notably%2C%20we%20find%20that%20previous%20cell%0Adetection%20approaches%20could%20suffer%20from%20low%20recall%2C%20potentially%20overlooking%20a%0Asignificant%20number%20of%20cells.%20In%20contrast%2C%20our%20framework%20offers%20an%20improved%0Asolution%2C%20which%20could%20benefit%20the%20diagnosis%20and%20study%20of%20anterior%20uveitis.%20Our%0Acode%20for%20cell%20detection%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/joeybyc/cell_detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17577v1&entry.124074799=Read"},
{"title": "End-to-End Autonomous Driving without Costly Modularization and 3D\n  Manual Annotation", "author": "Mingzhe Guo and Zhipeng Zhang and Yuan He and Ke Wang and Liping Jing", "abstract": "  We propose UAD, a method for vision-based end-to-end autonomous driving\n(E2EAD), achieving the best open-loop evaluation performance in nuScenes,\nmeanwhile showing robust closed-loop driving quality in CARLA. Our motivation\nstems from the observation that current E2EAD models still mimic the modular\narchitecture in typical driving stacks, with carefully designed supervised\nperception and prediction subtasks to provide environment information for\noriented planning. Although achieving groundbreaking progress, such design has\ncertain drawbacks: 1) preceding subtasks require massive high-quality 3D\nannotations as supervision, posing a significant impediment to scaling the\ntraining data; 2) each submodule entails substantial computation overhead in\nboth training and inference. To this end, we propose UAD, an E2EAD framework\nwith an unsupervised proxy to address all these issues. Firstly, we design a\nnovel Angular Perception Pretext to eliminate the annotation requirement. The\npretext models the driving scene by predicting the angular-wise spatial\nobjectness and temporal dynamics, without manual annotation. Secondly, a\nself-supervised training strategy, which learns the consistency of the\npredicted trajectories under different augment views, is proposed to enhance\nthe planning robustness in steering scenarios. Our UAD achieves 38.7% relative\nimprovements over UniAD on the average collision rate in nuScenes and surpasses\nVAD for 41.32 points on the driving score in CARLA's Town05 Long benchmark.\nMoreover, the proposed method only consumes 44.3% training resources of UniAD\nand runs 3.4 times faster in inference. Our innovative design not only for the\nfirst time demonstrates unarguable performance advantages over supervised\ncounterparts, but also enjoys unprecedented efficiency in data, training, and\ninference. Code and models will be released at\nhttps://github.com/KargoBot_Research/UAD.\n", "link": "http://arxiv.org/abs/2406.17680v1", "date": "2024-06-25", "relevancy": 2.3279, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6002}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5812}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Autonomous%20Driving%20without%20Costly%20Modularization%20and%203D%0A%20%20Manual%20Annotation&body=Title%3A%20End-to-End%20Autonomous%20Driving%20without%20Costly%20Modularization%20and%203D%0A%20%20Manual%20Annotation%0AAuthor%3A%20Mingzhe%20Guo%20and%20Zhipeng%20Zhang%20and%20Yuan%20He%20and%20Ke%20Wang%20and%20Liping%20Jing%0AAbstract%3A%20%20%20We%20propose%20UAD%2C%20a%20method%20for%20vision-based%20end-to-end%20autonomous%20driving%0A%28E2EAD%29%2C%20achieving%20the%20best%20open-loop%20evaluation%20performance%20in%20nuScenes%2C%0Ameanwhile%20showing%20robust%20closed-loop%20driving%20quality%20in%20CARLA.%20Our%20motivation%0Astems%20from%20the%20observation%20that%20current%20E2EAD%20models%20still%20mimic%20the%20modular%0Aarchitecture%20in%20typical%20driving%20stacks%2C%20with%20carefully%20designed%20supervised%0Aperception%20and%20prediction%20subtasks%20to%20provide%20environment%20information%20for%0Aoriented%20planning.%20Although%20achieving%20groundbreaking%20progress%2C%20such%20design%20has%0Acertain%20drawbacks%3A%201%29%20preceding%20subtasks%20require%20massive%20high-quality%203D%0Aannotations%20as%20supervision%2C%20posing%20a%20significant%20impediment%20to%20scaling%20the%0Atraining%20data%3B%202%29%20each%20submodule%20entails%20substantial%20computation%20overhead%20in%0Aboth%20training%20and%20inference.%20To%20this%20end%2C%20we%20propose%20UAD%2C%20an%20E2EAD%20framework%0Awith%20an%20unsupervised%20proxy%20to%20address%20all%20these%20issues.%20Firstly%2C%20we%20design%20a%0Anovel%20Angular%20Perception%20Pretext%20to%20eliminate%20the%20annotation%20requirement.%20The%0Apretext%20models%20the%20driving%20scene%20by%20predicting%20the%20angular-wise%20spatial%0Aobjectness%20and%20temporal%20dynamics%2C%20without%20manual%20annotation.%20Secondly%2C%20a%0Aself-supervised%20training%20strategy%2C%20which%20learns%20the%20consistency%20of%20the%0Apredicted%20trajectories%20under%20different%20augment%20views%2C%20is%20proposed%20to%20enhance%0Athe%20planning%20robustness%20in%20steering%20scenarios.%20Our%20UAD%20achieves%2038.7%25%20relative%0Aimprovements%20over%20UniAD%20on%20the%20average%20collision%20rate%20in%20nuScenes%20and%20surpasses%0AVAD%20for%2041.32%20points%20on%20the%20driving%20score%20in%20CARLA%27s%20Town05%20Long%20benchmark.%0AMoreover%2C%20the%20proposed%20method%20only%20consumes%2044.3%25%20training%20resources%20of%20UniAD%0Aand%20runs%203.4%20times%20faster%20in%20inference.%20Our%20innovative%20design%20not%20only%20for%20the%0Afirst%20time%20demonstrates%20unarguable%20performance%20advantages%20over%20supervised%0Acounterparts%2C%20but%20also%20enjoys%20unprecedented%20efficiency%20in%20data%2C%20training%2C%20and%0Ainference.%20Code%20and%20models%20will%20be%20released%20at%0Ahttps%3A//github.com/KargoBot_Research/UAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Autonomous%2520Driving%2520without%2520Costly%2520Modularization%2520and%25203D%250A%2520%2520Manual%2520Annotation%26entry.906535625%3DMingzhe%2520Guo%2520and%2520Zhipeng%2520Zhang%2520and%2520Yuan%2520He%2520and%2520Ke%2520Wang%2520and%2520Liping%2520Jing%26entry.1292438233%3D%2520%2520We%2520propose%2520UAD%252C%2520a%2520method%2520for%2520vision-based%2520end-to-end%2520autonomous%2520driving%250A%2528E2EAD%2529%252C%2520achieving%2520the%2520best%2520open-loop%2520evaluation%2520performance%2520in%2520nuScenes%252C%250Ameanwhile%2520showing%2520robust%2520closed-loop%2520driving%2520quality%2520in%2520CARLA.%2520Our%2520motivation%250Astems%2520from%2520the%2520observation%2520that%2520current%2520E2EAD%2520models%2520still%2520mimic%2520the%2520modular%250Aarchitecture%2520in%2520typical%2520driving%2520stacks%252C%2520with%2520carefully%2520designed%2520supervised%250Aperception%2520and%2520prediction%2520subtasks%2520to%2520provide%2520environment%2520information%2520for%250Aoriented%2520planning.%2520Although%2520achieving%2520groundbreaking%2520progress%252C%2520such%2520design%2520has%250Acertain%2520drawbacks%253A%25201%2529%2520preceding%2520subtasks%2520require%2520massive%2520high-quality%25203D%250Aannotations%2520as%2520supervision%252C%2520posing%2520a%2520significant%2520impediment%2520to%2520scaling%2520the%250Atraining%2520data%253B%25202%2529%2520each%2520submodule%2520entails%2520substantial%2520computation%2520overhead%2520in%250Aboth%2520training%2520and%2520inference.%2520To%2520this%2520end%252C%2520we%2520propose%2520UAD%252C%2520an%2520E2EAD%2520framework%250Awith%2520an%2520unsupervised%2520proxy%2520to%2520address%2520all%2520these%2520issues.%2520Firstly%252C%2520we%2520design%2520a%250Anovel%2520Angular%2520Perception%2520Pretext%2520to%2520eliminate%2520the%2520annotation%2520requirement.%2520The%250Apretext%2520models%2520the%2520driving%2520scene%2520by%2520predicting%2520the%2520angular-wise%2520spatial%250Aobjectness%2520and%2520temporal%2520dynamics%252C%2520without%2520manual%2520annotation.%2520Secondly%252C%2520a%250Aself-supervised%2520training%2520strategy%252C%2520which%2520learns%2520the%2520consistency%2520of%2520the%250Apredicted%2520trajectories%2520under%2520different%2520augment%2520views%252C%2520is%2520proposed%2520to%2520enhance%250Athe%2520planning%2520robustness%2520in%2520steering%2520scenarios.%2520Our%2520UAD%2520achieves%252038.7%2525%2520relative%250Aimprovements%2520over%2520UniAD%2520on%2520the%2520average%2520collision%2520rate%2520in%2520nuScenes%2520and%2520surpasses%250AVAD%2520for%252041.32%2520points%2520on%2520the%2520driving%2520score%2520in%2520CARLA%2527s%2520Town05%2520Long%2520benchmark.%250AMoreover%252C%2520the%2520proposed%2520method%2520only%2520consumes%252044.3%2525%2520training%2520resources%2520of%2520UniAD%250Aand%2520runs%25203.4%2520times%2520faster%2520in%2520inference.%2520Our%2520innovative%2520design%2520not%2520only%2520for%2520the%250Afirst%2520time%2520demonstrates%2520unarguable%2520performance%2520advantages%2520over%2520supervised%250Acounterparts%252C%2520but%2520also%2520enjoys%2520unprecedented%2520efficiency%2520in%2520data%252C%2520training%252C%2520and%250Ainference.%2520Code%2520and%2520models%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/KargoBot_Research/UAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Autonomous%20Driving%20without%20Costly%20Modularization%20and%203D%0A%20%20Manual%20Annotation&entry.906535625=Mingzhe%20Guo%20and%20Zhipeng%20Zhang%20and%20Yuan%20He%20and%20Ke%20Wang%20and%20Liping%20Jing&entry.1292438233=%20%20We%20propose%20UAD%2C%20a%20method%20for%20vision-based%20end-to-end%20autonomous%20driving%0A%28E2EAD%29%2C%20achieving%20the%20best%20open-loop%20evaluation%20performance%20in%20nuScenes%2C%0Ameanwhile%20showing%20robust%20closed-loop%20driving%20quality%20in%20CARLA.%20Our%20motivation%0Astems%20from%20the%20observation%20that%20current%20E2EAD%20models%20still%20mimic%20the%20modular%0Aarchitecture%20in%20typical%20driving%20stacks%2C%20with%20carefully%20designed%20supervised%0Aperception%20and%20prediction%20subtasks%20to%20provide%20environment%20information%20for%0Aoriented%20planning.%20Although%20achieving%20groundbreaking%20progress%2C%20such%20design%20has%0Acertain%20drawbacks%3A%201%29%20preceding%20subtasks%20require%20massive%20high-quality%203D%0Aannotations%20as%20supervision%2C%20posing%20a%20significant%20impediment%20to%20scaling%20the%0Atraining%20data%3B%202%29%20each%20submodule%20entails%20substantial%20computation%20overhead%20in%0Aboth%20training%20and%20inference.%20To%20this%20end%2C%20we%20propose%20UAD%2C%20an%20E2EAD%20framework%0Awith%20an%20unsupervised%20proxy%20to%20address%20all%20these%20issues.%20Firstly%2C%20we%20design%20a%0Anovel%20Angular%20Perception%20Pretext%20to%20eliminate%20the%20annotation%20requirement.%20The%0Apretext%20models%20the%20driving%20scene%20by%20predicting%20the%20angular-wise%20spatial%0Aobjectness%20and%20temporal%20dynamics%2C%20without%20manual%20annotation.%20Secondly%2C%20a%0Aself-supervised%20training%20strategy%2C%20which%20learns%20the%20consistency%20of%20the%0Apredicted%20trajectories%20under%20different%20augment%20views%2C%20is%20proposed%20to%20enhance%0Athe%20planning%20robustness%20in%20steering%20scenarios.%20Our%20UAD%20achieves%2038.7%25%20relative%0Aimprovements%20over%20UniAD%20on%20the%20average%20collision%20rate%20in%20nuScenes%20and%20surpasses%0AVAD%20for%2041.32%20points%20on%20the%20driving%20score%20in%20CARLA%27s%20Town05%20Long%20benchmark.%0AMoreover%2C%20the%20proposed%20method%20only%20consumes%2044.3%25%20training%20resources%20of%20UniAD%0Aand%20runs%203.4%20times%20faster%20in%20inference.%20Our%20innovative%20design%20not%20only%20for%20the%0Afirst%20time%20demonstrates%20unarguable%20performance%20advantages%20over%20supervised%0Acounterparts%2C%20but%20also%20enjoys%20unprecedented%20efficiency%20in%20data%2C%20training%2C%20and%0Ainference.%20Code%20and%20models%20will%20be%20released%20at%0Ahttps%3A//github.com/KargoBot_Research/UAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17680v1&entry.124074799=Read"},
{"title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted\n  Phenomenon", "author": "USVSN Sai Prashanth and Alvin Deng and Kyle O'Brien and Jyothir S V and Mohammad Aflah Khan and Jaydeep Borkar and Christopher A. Choquette-Choo and Jacob Ray Fuehne and Stella Biderman and Tracy Ke and Katherine Lee and Naomi Saphra", "abstract": "  Memorization in language models is typically treated as a homogenous\nphenomenon, neglecting the specifics of the memorized data. We instead model\nmemorization as the effect of a set of complex factors that describe each\nsample and relate it to the model and corpus. To build intuition around these\nfactors, we break memorization down into a taxonomy: recitation of highly\nduplicated sequences, reconstruction of inherently predictable sequences, and\nrecollection of sequences that are neither. We demonstrate the usefulness of\nour taxonomy by using it to construct a predictive model for memorization. By\nanalyzing dependencies and inspecting the weights of the predictive model, we\nfind that different factors influence the likelihood of memorization\ndifferently depending on the taxonomic category.\n", "link": "http://arxiv.org/abs/2406.17746v1", "date": "2024-06-25", "relevancy": 2.2974, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4747}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4704}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recite%2C%20Reconstruct%2C%20Recollect%3A%20Memorization%20in%20LMs%20as%20a%20Multifaceted%0A%20%20Phenomenon&body=Title%3A%20Recite%2C%20Reconstruct%2C%20Recollect%3A%20Memorization%20in%20LMs%20as%20a%20Multifaceted%0A%20%20Phenomenon%0AAuthor%3A%20USVSN%20Sai%20Prashanth%20and%20Alvin%20Deng%20and%20Kyle%20O%27Brien%20and%20Jyothir%20S%20V%20and%20Mohammad%20Aflah%20Khan%20and%20Jaydeep%20Borkar%20and%20Christopher%20A.%20Choquette-Choo%20and%20Jacob%20Ray%20Fuehne%20and%20Stella%20Biderman%20and%20Tracy%20Ke%20and%20Katherine%20Lee%20and%20Naomi%20Saphra%0AAbstract%3A%20%20%20Memorization%20in%20language%20models%20is%20typically%20treated%20as%20a%20homogenous%0Aphenomenon%2C%20neglecting%20the%20specifics%20of%20the%20memorized%20data.%20We%20instead%20model%0Amemorization%20as%20the%20effect%20of%20a%20set%20of%20complex%20factors%20that%20describe%20each%0Asample%20and%20relate%20it%20to%20the%20model%20and%20corpus.%20To%20build%20intuition%20around%20these%0Afactors%2C%20we%20break%20memorization%20down%20into%20a%20taxonomy%3A%20recitation%20of%20highly%0Aduplicated%20sequences%2C%20reconstruction%20of%20inherently%20predictable%20sequences%2C%20and%0Arecollection%20of%20sequences%20that%20are%20neither.%20We%20demonstrate%20the%20usefulness%20of%0Aour%20taxonomy%20by%20using%20it%20to%20construct%20a%20predictive%20model%20for%20memorization.%20By%0Aanalyzing%20dependencies%20and%20inspecting%20the%20weights%20of%20the%20predictive%20model%2C%20we%0Afind%20that%20different%20factors%20influence%20the%20likelihood%20of%20memorization%0Adifferently%20depending%20on%20the%20taxonomic%20category.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecite%252C%2520Reconstruct%252C%2520Recollect%253A%2520Memorization%2520in%2520LMs%2520as%2520a%2520Multifaceted%250A%2520%2520Phenomenon%26entry.906535625%3DUSVSN%2520Sai%2520Prashanth%2520and%2520Alvin%2520Deng%2520and%2520Kyle%2520O%2527Brien%2520and%2520Jyothir%2520S%2520V%2520and%2520Mohammad%2520Aflah%2520Khan%2520and%2520Jaydeep%2520Borkar%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520Jacob%2520Ray%2520Fuehne%2520and%2520Stella%2520Biderman%2520and%2520Tracy%2520Ke%2520and%2520Katherine%2520Lee%2520and%2520Naomi%2520Saphra%26entry.1292438233%3D%2520%2520Memorization%2520in%2520language%2520models%2520is%2520typically%2520treated%2520as%2520a%2520homogenous%250Aphenomenon%252C%2520neglecting%2520the%2520specifics%2520of%2520the%2520memorized%2520data.%2520We%2520instead%2520model%250Amemorization%2520as%2520the%2520effect%2520of%2520a%2520set%2520of%2520complex%2520factors%2520that%2520describe%2520each%250Asample%2520and%2520relate%2520it%2520to%2520the%2520model%2520and%2520corpus.%2520To%2520build%2520intuition%2520around%2520these%250Afactors%252C%2520we%2520break%2520memorization%2520down%2520into%2520a%2520taxonomy%253A%2520recitation%2520of%2520highly%250Aduplicated%2520sequences%252C%2520reconstruction%2520of%2520inherently%2520predictable%2520sequences%252C%2520and%250Arecollection%2520of%2520sequences%2520that%2520are%2520neither.%2520We%2520demonstrate%2520the%2520usefulness%2520of%250Aour%2520taxonomy%2520by%2520using%2520it%2520to%2520construct%2520a%2520predictive%2520model%2520for%2520memorization.%2520By%250Aanalyzing%2520dependencies%2520and%2520inspecting%2520the%2520weights%2520of%2520the%2520predictive%2520model%252C%2520we%250Afind%2520that%2520different%2520factors%2520influence%2520the%2520likelihood%2520of%2520memorization%250Adifferently%2520depending%2520on%2520the%2520taxonomic%2520category.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recite%2C%20Reconstruct%2C%20Recollect%3A%20Memorization%20in%20LMs%20as%20a%20Multifaceted%0A%20%20Phenomenon&entry.906535625=USVSN%20Sai%20Prashanth%20and%20Alvin%20Deng%20and%20Kyle%20O%27Brien%20and%20Jyothir%20S%20V%20and%20Mohammad%20Aflah%20Khan%20and%20Jaydeep%20Borkar%20and%20Christopher%20A.%20Choquette-Choo%20and%20Jacob%20Ray%20Fuehne%20and%20Stella%20Biderman%20and%20Tracy%20Ke%20and%20Katherine%20Lee%20and%20Naomi%20Saphra&entry.1292438233=%20%20Memorization%20in%20language%20models%20is%20typically%20treated%20as%20a%20homogenous%0Aphenomenon%2C%20neglecting%20the%20specifics%20of%20the%20memorized%20data.%20We%20instead%20model%0Amemorization%20as%20the%20effect%20of%20a%20set%20of%20complex%20factors%20that%20describe%20each%0Asample%20and%20relate%20it%20to%20the%20model%20and%20corpus.%20To%20build%20intuition%20around%20these%0Afactors%2C%20we%20break%20memorization%20down%20into%20a%20taxonomy%3A%20recitation%20of%20highly%0Aduplicated%20sequences%2C%20reconstruction%20of%20inherently%20predictable%20sequences%2C%20and%0Arecollection%20of%20sequences%20that%20are%20neither.%20We%20demonstrate%20the%20usefulness%20of%0Aour%20taxonomy%20by%20using%20it%20to%20construct%20a%20predictive%20model%20for%20memorization.%20By%0Aanalyzing%20dependencies%20and%20inspecting%20the%20weights%20of%20the%20predictive%20model%2C%20we%0Afind%20that%20different%20factors%20influence%20the%20likelihood%20of%20memorization%0Adifferently%20depending%20on%20the%20taxonomic%20category.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17746v1&entry.124074799=Read"},
{"title": "TSynD: Targeted Synthetic Data Generation for Enhanced Medical Image\n  Classification", "author": "Joshua Niemeijer and Jan Ehrhardt and Hristina Uzunova and Heinz Handels", "abstract": "  The usage of medical image data for the training of large-scale machine\nlearning approaches is particularly challenging due to its scarce availability\nand the costly generation of data annotations, typically requiring the\nengagement of medical professionals. The rapid development of generative models\nallows towards tackling this problem by leveraging large amounts of realistic\nsynthetically generated data for the training process. However, randomly\nchoosing synthetic samples, might not be an optimal strategy.\n  In this work, we investigate the targeted generation of synthetic training\ndata, in order to improve the accuracy and robustness of image classification.\nTherefore, our approach aims to guide the generative model to synthesize data\nwith high epistemic uncertainty, since large measures of epistemic uncertainty\nindicate underrepresented data points in the training set. During the image\ngeneration we feed images reconstructed by an auto encoder into the classifier\nand compute the mutual information over the class-probability distribution as a\nmeasure for uncertainty.We alter the feature space of the autoencoder through\nan optimization process with the objective of maximizing the classifier\nuncertainty on the decoded image. By training on such data we improve the\nperformance and robustness against test time data augmentations and adversarial\nattacks on several classifications tasks.\n", "link": "http://arxiv.org/abs/2406.17473v1", "date": "2024-06-25", "relevancy": 2.2684, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5786}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.57}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSynD%3A%20Targeted%20Synthetic%20Data%20Generation%20for%20Enhanced%20Medical%20Image%0A%20%20Classification&body=Title%3A%20TSynD%3A%20Targeted%20Synthetic%20Data%20Generation%20for%20Enhanced%20Medical%20Image%0A%20%20Classification%0AAuthor%3A%20Joshua%20Niemeijer%20and%20Jan%20Ehrhardt%20and%20Hristina%20Uzunova%20and%20Heinz%20Handels%0AAbstract%3A%20%20%20The%20usage%20of%20medical%20image%20data%20for%20the%20training%20of%20large-scale%20machine%0Alearning%20approaches%20is%20particularly%20challenging%20due%20to%20its%20scarce%20availability%0Aand%20the%20costly%20generation%20of%20data%20annotations%2C%20typically%20requiring%20the%0Aengagement%20of%20medical%20professionals.%20The%20rapid%20development%20of%20generative%20models%0Aallows%20towards%20tackling%20this%20problem%20by%20leveraging%20large%20amounts%20of%20realistic%0Asynthetically%20generated%20data%20for%20the%20training%20process.%20However%2C%20randomly%0Achoosing%20synthetic%20samples%2C%20might%20not%20be%20an%20optimal%20strategy.%0A%20%20In%20this%20work%2C%20we%20investigate%20the%20targeted%20generation%20of%20synthetic%20training%0Adata%2C%20in%20order%20to%20improve%20the%20accuracy%20and%20robustness%20of%20image%20classification.%0ATherefore%2C%20our%20approach%20aims%20to%20guide%20the%20generative%20model%20to%20synthesize%20data%0Awith%20high%20epistemic%20uncertainty%2C%20since%20large%20measures%20of%20epistemic%20uncertainty%0Aindicate%20underrepresented%20data%20points%20in%20the%20training%20set.%20During%20the%20image%0Ageneration%20we%20feed%20images%20reconstructed%20by%20an%20auto%20encoder%20into%20the%20classifier%0Aand%20compute%20the%20mutual%20information%20over%20the%20class-probability%20distribution%20as%20a%0Ameasure%20for%20uncertainty.We%20alter%20the%20feature%20space%20of%20the%20autoencoder%20through%0Aan%20optimization%20process%20with%20the%20objective%20of%20maximizing%20the%20classifier%0Auncertainty%20on%20the%20decoded%20image.%20By%20training%20on%20such%20data%20we%20improve%20the%0Aperformance%20and%20robustness%20against%20test%20time%20data%20augmentations%20and%20adversarial%0Aattacks%20on%20several%20classifications%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSynD%253A%2520Targeted%2520Synthetic%2520Data%2520Generation%2520for%2520Enhanced%2520Medical%2520Image%250A%2520%2520Classification%26entry.906535625%3DJoshua%2520Niemeijer%2520and%2520Jan%2520Ehrhardt%2520and%2520Hristina%2520Uzunova%2520and%2520Heinz%2520Handels%26entry.1292438233%3D%2520%2520The%2520usage%2520of%2520medical%2520image%2520data%2520for%2520the%2520training%2520of%2520large-scale%2520machine%250Alearning%2520approaches%2520is%2520particularly%2520challenging%2520due%2520to%2520its%2520scarce%2520availability%250Aand%2520the%2520costly%2520generation%2520of%2520data%2520annotations%252C%2520typically%2520requiring%2520the%250Aengagement%2520of%2520medical%2520professionals.%2520The%2520rapid%2520development%2520of%2520generative%2520models%250Aallows%2520towards%2520tackling%2520this%2520problem%2520by%2520leveraging%2520large%2520amounts%2520of%2520realistic%250Asynthetically%2520generated%2520data%2520for%2520the%2520training%2520process.%2520However%252C%2520randomly%250Achoosing%2520synthetic%2520samples%252C%2520might%2520not%2520be%2520an%2520optimal%2520strategy.%250A%2520%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520targeted%2520generation%2520of%2520synthetic%2520training%250Adata%252C%2520in%2520order%2520to%2520improve%2520the%2520accuracy%2520and%2520robustness%2520of%2520image%2520classification.%250ATherefore%252C%2520our%2520approach%2520aims%2520to%2520guide%2520the%2520generative%2520model%2520to%2520synthesize%2520data%250Awith%2520high%2520epistemic%2520uncertainty%252C%2520since%2520large%2520measures%2520of%2520epistemic%2520uncertainty%250Aindicate%2520underrepresented%2520data%2520points%2520in%2520the%2520training%2520set.%2520During%2520the%2520image%250Ageneration%2520we%2520feed%2520images%2520reconstructed%2520by%2520an%2520auto%2520encoder%2520into%2520the%2520classifier%250Aand%2520compute%2520the%2520mutual%2520information%2520over%2520the%2520class-probability%2520distribution%2520as%2520a%250Ameasure%2520for%2520uncertainty.We%2520alter%2520the%2520feature%2520space%2520of%2520the%2520autoencoder%2520through%250Aan%2520optimization%2520process%2520with%2520the%2520objective%2520of%2520maximizing%2520the%2520classifier%250Auncertainty%2520on%2520the%2520decoded%2520image.%2520By%2520training%2520on%2520such%2520data%2520we%2520improve%2520the%250Aperformance%2520and%2520robustness%2520against%2520test%2520time%2520data%2520augmentations%2520and%2520adversarial%250Aattacks%2520on%2520several%2520classifications%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSynD%3A%20Targeted%20Synthetic%20Data%20Generation%20for%20Enhanced%20Medical%20Image%0A%20%20Classification&entry.906535625=Joshua%20Niemeijer%20and%20Jan%20Ehrhardt%20and%20Hristina%20Uzunova%20and%20Heinz%20Handels&entry.1292438233=%20%20The%20usage%20of%20medical%20image%20data%20for%20the%20training%20of%20large-scale%20machine%0Alearning%20approaches%20is%20particularly%20challenging%20due%20to%20its%20scarce%20availability%0Aand%20the%20costly%20generation%20of%20data%20annotations%2C%20typically%20requiring%20the%0Aengagement%20of%20medical%20professionals.%20The%20rapid%20development%20of%20generative%20models%0Aallows%20towards%20tackling%20this%20problem%20by%20leveraging%20large%20amounts%20of%20realistic%0Asynthetically%20generated%20data%20for%20the%20training%20process.%20However%2C%20randomly%0Achoosing%20synthetic%20samples%2C%20might%20not%20be%20an%20optimal%20strategy.%0A%20%20In%20this%20work%2C%20we%20investigate%20the%20targeted%20generation%20of%20synthetic%20training%0Adata%2C%20in%20order%20to%20improve%20the%20accuracy%20and%20robustness%20of%20image%20classification.%0ATherefore%2C%20our%20approach%20aims%20to%20guide%20the%20generative%20model%20to%20synthesize%20data%0Awith%20high%20epistemic%20uncertainty%2C%20since%20large%20measures%20of%20epistemic%20uncertainty%0Aindicate%20underrepresented%20data%20points%20in%20the%20training%20set.%20During%20the%20image%0Ageneration%20we%20feed%20images%20reconstructed%20by%20an%20auto%20encoder%20into%20the%20classifier%0Aand%20compute%20the%20mutual%20information%20over%20the%20class-probability%20distribution%20as%20a%0Ameasure%20for%20uncertainty.We%20alter%20the%20feature%20space%20of%20the%20autoencoder%20through%0Aan%20optimization%20process%20with%20the%20objective%20of%20maximizing%20the%20classifier%0Auncertainty%20on%20the%20decoded%20image.%20By%20training%20on%20such%20data%20we%20improve%20the%0Aperformance%20and%20robustness%20against%20test%20time%20data%20augmentations%20and%20adversarial%0Aattacks%20on%20several%20classifications%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17473v1&entry.124074799=Read"},
{"title": "Find Parent then Label Children: A Two-stage Taxonomy Completion Method\n  with Pre-trained Language Model", "author": "Fei Xia and Yixuan Weng and Shizhu He and Kang Liu and Jun Zhao", "abstract": "  Taxonomies, which organize domain concepts into hierarchical structures, are\ncrucial for building knowledge systems and downstream applications. As domain\nknowledge evolves, taxonomies need to be continuously updated to include new\nconcepts. Previous approaches have mainly focused on adding concepts to the\nleaf nodes of the existing hierarchical tree, which does not fully utilize the\ntaxonomy's knowledge and is unable to update the original taxonomy structure\n(usually involving non-leaf nodes). In this paper, we propose a two-stage\nmethod called ATTEMPT for taxonomy completion. Our method inserts new concepts\ninto the correct position by finding a parent node and labeling child nodes.\nSpecifically, by combining local nodes with prompts to generate natural\nsentences, we take advantage of pre-trained language models for\nhypernym/hyponymy recognition. Experimental results on two public datasets\n(including six domains) show that ATTEMPT performs best on both taxonomy\ncompletion and extension tasks, surpassing existing methods.\n", "link": "http://arxiv.org/abs/2406.17739v1", "date": "2024-06-25", "relevancy": 2.2682, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4548}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Find%20Parent%20then%20Label%20Children%3A%20A%20Two-stage%20Taxonomy%20Completion%20Method%0A%20%20with%20Pre-trained%20Language%20Model&body=Title%3A%20Find%20Parent%20then%20Label%20Children%3A%20A%20Two-stage%20Taxonomy%20Completion%20Method%0A%20%20with%20Pre-trained%20Language%20Model%0AAuthor%3A%20Fei%20Xia%20and%20Yixuan%20Weng%20and%20Shizhu%20He%20and%20Kang%20Liu%20and%20Jun%20Zhao%0AAbstract%3A%20%20%20Taxonomies%2C%20which%20organize%20domain%20concepts%20into%20hierarchical%20structures%2C%20are%0Acrucial%20for%20building%20knowledge%20systems%20and%20downstream%20applications.%20As%20domain%0Aknowledge%20evolves%2C%20taxonomies%20need%20to%20be%20continuously%20updated%20to%20include%20new%0Aconcepts.%20Previous%20approaches%20have%20mainly%20focused%20on%20adding%20concepts%20to%20the%0Aleaf%20nodes%20of%20the%20existing%20hierarchical%20tree%2C%20which%20does%20not%20fully%20utilize%20the%0Ataxonomy%27s%20knowledge%20and%20is%20unable%20to%20update%20the%20original%20taxonomy%20structure%0A%28usually%20involving%20non-leaf%20nodes%29.%20In%20this%20paper%2C%20we%20propose%20a%20two-stage%0Amethod%20called%20ATTEMPT%20for%20taxonomy%20completion.%20Our%20method%20inserts%20new%20concepts%0Ainto%20the%20correct%20position%20by%20finding%20a%20parent%20node%20and%20labeling%20child%20nodes.%0ASpecifically%2C%20by%20combining%20local%20nodes%20with%20prompts%20to%20generate%20natural%0Asentences%2C%20we%20take%20advantage%20of%20pre-trained%20language%20models%20for%0Ahypernym/hyponymy%20recognition.%20Experimental%20results%20on%20two%20public%20datasets%0A%28including%20six%20domains%29%20show%20that%20ATTEMPT%20performs%20best%20on%20both%20taxonomy%0Acompletion%20and%20extension%20tasks%2C%20surpassing%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFind%2520Parent%2520then%2520Label%2520Children%253A%2520A%2520Two-stage%2520Taxonomy%2520Completion%2520Method%250A%2520%2520with%2520Pre-trained%2520Language%2520Model%26entry.906535625%3DFei%2520Xia%2520and%2520Yixuan%2520Weng%2520and%2520Shizhu%2520He%2520and%2520Kang%2520Liu%2520and%2520Jun%2520Zhao%26entry.1292438233%3D%2520%2520Taxonomies%252C%2520which%2520organize%2520domain%2520concepts%2520into%2520hierarchical%2520structures%252C%2520are%250Acrucial%2520for%2520building%2520knowledge%2520systems%2520and%2520downstream%2520applications.%2520As%2520domain%250Aknowledge%2520evolves%252C%2520taxonomies%2520need%2520to%2520be%2520continuously%2520updated%2520to%2520include%2520new%250Aconcepts.%2520Previous%2520approaches%2520have%2520mainly%2520focused%2520on%2520adding%2520concepts%2520to%2520the%250Aleaf%2520nodes%2520of%2520the%2520existing%2520hierarchical%2520tree%252C%2520which%2520does%2520not%2520fully%2520utilize%2520the%250Ataxonomy%2527s%2520knowledge%2520and%2520is%2520unable%2520to%2520update%2520the%2520original%2520taxonomy%2520structure%250A%2528usually%2520involving%2520non-leaf%2520nodes%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520two-stage%250Amethod%2520called%2520ATTEMPT%2520for%2520taxonomy%2520completion.%2520Our%2520method%2520inserts%2520new%2520concepts%250Ainto%2520the%2520correct%2520position%2520by%2520finding%2520a%2520parent%2520node%2520and%2520labeling%2520child%2520nodes.%250ASpecifically%252C%2520by%2520combining%2520local%2520nodes%2520with%2520prompts%2520to%2520generate%2520natural%250Asentences%252C%2520we%2520take%2520advantage%2520of%2520pre-trained%2520language%2520models%2520for%250Ahypernym/hyponymy%2520recognition.%2520Experimental%2520results%2520on%2520two%2520public%2520datasets%250A%2528including%2520six%2520domains%2529%2520show%2520that%2520ATTEMPT%2520performs%2520best%2520on%2520both%2520taxonomy%250Acompletion%2520and%2520extension%2520tasks%252C%2520surpassing%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Find%20Parent%20then%20Label%20Children%3A%20A%20Two-stage%20Taxonomy%20Completion%20Method%0A%20%20with%20Pre-trained%20Language%20Model&entry.906535625=Fei%20Xia%20and%20Yixuan%20Weng%20and%20Shizhu%20He%20and%20Kang%20Liu%20and%20Jun%20Zhao&entry.1292438233=%20%20Taxonomies%2C%20which%20organize%20domain%20concepts%20into%20hierarchical%20structures%2C%20are%0Acrucial%20for%20building%20knowledge%20systems%20and%20downstream%20applications.%20As%20domain%0Aknowledge%20evolves%2C%20taxonomies%20need%20to%20be%20continuously%20updated%20to%20include%20new%0Aconcepts.%20Previous%20approaches%20have%20mainly%20focused%20on%20adding%20concepts%20to%20the%0Aleaf%20nodes%20of%20the%20existing%20hierarchical%20tree%2C%20which%20does%20not%20fully%20utilize%20the%0Ataxonomy%27s%20knowledge%20and%20is%20unable%20to%20update%20the%20original%20taxonomy%20structure%0A%28usually%20involving%20non-leaf%20nodes%29.%20In%20this%20paper%2C%20we%20propose%20a%20two-stage%0Amethod%20called%20ATTEMPT%20for%20taxonomy%20completion.%20Our%20method%20inserts%20new%20concepts%0Ainto%20the%20correct%20position%20by%20finding%20a%20parent%20node%20and%20labeling%20child%20nodes.%0ASpecifically%2C%20by%20combining%20local%20nodes%20with%20prompts%20to%20generate%20natural%0Asentences%2C%20we%20take%20advantage%20of%20pre-trained%20language%20models%20for%0Ahypernym/hyponymy%20recognition.%20Experimental%20results%20on%20two%20public%20datasets%0A%28including%20six%20domains%29%20show%20that%20ATTEMPT%20performs%20best%20on%20both%20taxonomy%0Acompletion%20and%20extension%20tasks%2C%20surpassing%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17739v1&entry.124074799=Read"},
{"title": "Light-weight End-to-End Graph Interest Network for CTR Prediction in\n  E-commerce Search", "author": "Pai Peng and Quanxiang Jia and Ziqiang Zhou and Shuang Hong and Zichong Xiao", "abstract": "  Click-through-rate (CTR) prediction has an essential impact on improving user\nexperience and revenue in e-commerce search. With the development of deep\nlearning, graph-based methods are well exploited to utilize graph structure\nextracted from user behaviors and other information to help embedding learning.\nHowever, most of the previous graph-based methods mainly focus on\nrecommendation scenarios, and therefore their graph structures highly depend on\nitem's sequential information from user behaviors, ignoring query's sequential\nsignal and query-item correlation. In this paper, we propose a new approach\nnamed Light-weight End-to-End Graph Interest Network (EGIN) to effectively mine\nusers' search interests and tackle previous challenges. (i) EGIN utilizes query\nand item's correlation and sequential information from the search system to\nbuild a heterogeneous graph for better CTR prediction in e-commerce search.\n(ii) EGIN's graph embedding learning shares the same training input and is\njointly trained with CTR prediction, making the end-to-end framework effortless\nto deploy in large-scale search systems. The proposed EGIN is composed of three\nparts: query-item heterogeneous graph, light-weight graph sampling, and\nmulti-interest network. The query-item heterogeneous graph captures correlation\nand sequential information of query and item efficiently by the proposed\nlight-weight graph sampling. The multi-interest network is well designed to\nutilize graph embedding to capture various similarity relationships between\nquery and item to enhance the final CTR prediction. We conduct extensive\nexperiments on both public and industrial datasets to demonstrate the\neffectiveness of the proposed EGIN. At the same time, the training cost of\ngraph learning is relatively low compared with the main CTR prediction task,\nensuring efficiency in practical applications.\n", "link": "http://arxiv.org/abs/2406.17745v1", "date": "2024-06-25", "relevancy": 2.2586, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4741}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4527}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light-weight%20End-to-End%20Graph%20Interest%20Network%20for%20CTR%20Prediction%20in%0A%20%20E-commerce%20Search&body=Title%3A%20Light-weight%20End-to-End%20Graph%20Interest%20Network%20for%20CTR%20Prediction%20in%0A%20%20E-commerce%20Search%0AAuthor%3A%20Pai%20Peng%20and%20Quanxiang%20Jia%20and%20Ziqiang%20Zhou%20and%20Shuang%20Hong%20and%20Zichong%20Xiao%0AAbstract%3A%20%20%20Click-through-rate%20%28CTR%29%20prediction%20has%20an%20essential%20impact%20on%20improving%20user%0Aexperience%20and%20revenue%20in%20e-commerce%20search.%20With%20the%20development%20of%20deep%0Alearning%2C%20graph-based%20methods%20are%20well%20exploited%20to%20utilize%20graph%20structure%0Aextracted%20from%20user%20behaviors%20and%20other%20information%20to%20help%20embedding%20learning.%0AHowever%2C%20most%20of%20the%20previous%20graph-based%20methods%20mainly%20focus%20on%0Arecommendation%20scenarios%2C%20and%20therefore%20their%20graph%20structures%20highly%20depend%20on%0Aitem%27s%20sequential%20information%20from%20user%20behaviors%2C%20ignoring%20query%27s%20sequential%0Asignal%20and%20query-item%20correlation.%20In%20this%20paper%2C%20we%20propose%20a%20new%20approach%0Anamed%20Light-weight%20End-to-End%20Graph%20Interest%20Network%20%28EGIN%29%20to%20effectively%20mine%0Ausers%27%20search%20interests%20and%20tackle%20previous%20challenges.%20%28i%29%20EGIN%20utilizes%20query%0Aand%20item%27s%20correlation%20and%20sequential%20information%20from%20the%20search%20system%20to%0Abuild%20a%20heterogeneous%20graph%20for%20better%20CTR%20prediction%20in%20e-commerce%20search.%0A%28ii%29%20EGIN%27s%20graph%20embedding%20learning%20shares%20the%20same%20training%20input%20and%20is%0Ajointly%20trained%20with%20CTR%20prediction%2C%20making%20the%20end-to-end%20framework%20effortless%0Ato%20deploy%20in%20large-scale%20search%20systems.%20The%20proposed%20EGIN%20is%20composed%20of%20three%0Aparts%3A%20query-item%20heterogeneous%20graph%2C%20light-weight%20graph%20sampling%2C%20and%0Amulti-interest%20network.%20The%20query-item%20heterogeneous%20graph%20captures%20correlation%0Aand%20sequential%20information%20of%20query%20and%20item%20efficiently%20by%20the%20proposed%0Alight-weight%20graph%20sampling.%20The%20multi-interest%20network%20is%20well%20designed%20to%0Autilize%20graph%20embedding%20to%20capture%20various%20similarity%20relationships%20between%0Aquery%20and%20item%20to%20enhance%20the%20final%20CTR%20prediction.%20We%20conduct%20extensive%0Aexperiments%20on%20both%20public%20and%20industrial%20datasets%20to%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20EGIN.%20At%20the%20same%20time%2C%20the%20training%20cost%20of%0Agraph%20learning%20is%20relatively%20low%20compared%20with%20the%20main%20CTR%20prediction%20task%2C%0Aensuring%20efficiency%20in%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight-weight%2520End-to-End%2520Graph%2520Interest%2520Network%2520for%2520CTR%2520Prediction%2520in%250A%2520%2520E-commerce%2520Search%26entry.906535625%3DPai%2520Peng%2520and%2520Quanxiang%2520Jia%2520and%2520Ziqiang%2520Zhou%2520and%2520Shuang%2520Hong%2520and%2520Zichong%2520Xiao%26entry.1292438233%3D%2520%2520Click-through-rate%2520%2528CTR%2529%2520prediction%2520has%2520an%2520essential%2520impact%2520on%2520improving%2520user%250Aexperience%2520and%2520revenue%2520in%2520e-commerce%2520search.%2520With%2520the%2520development%2520of%2520deep%250Alearning%252C%2520graph-based%2520methods%2520are%2520well%2520exploited%2520to%2520utilize%2520graph%2520structure%250Aextracted%2520from%2520user%2520behaviors%2520and%2520other%2520information%2520to%2520help%2520embedding%2520learning.%250AHowever%252C%2520most%2520of%2520the%2520previous%2520graph-based%2520methods%2520mainly%2520focus%2520on%250Arecommendation%2520scenarios%252C%2520and%2520therefore%2520their%2520graph%2520structures%2520highly%2520depend%2520on%250Aitem%2527s%2520sequential%2520information%2520from%2520user%2520behaviors%252C%2520ignoring%2520query%2527s%2520sequential%250Asignal%2520and%2520query-item%2520correlation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520approach%250Anamed%2520Light-weight%2520End-to-End%2520Graph%2520Interest%2520Network%2520%2528EGIN%2529%2520to%2520effectively%2520mine%250Ausers%2527%2520search%2520interests%2520and%2520tackle%2520previous%2520challenges.%2520%2528i%2529%2520EGIN%2520utilizes%2520query%250Aand%2520item%2527s%2520correlation%2520and%2520sequential%2520information%2520from%2520the%2520search%2520system%2520to%250Abuild%2520a%2520heterogeneous%2520graph%2520for%2520better%2520CTR%2520prediction%2520in%2520e-commerce%2520search.%250A%2528ii%2529%2520EGIN%2527s%2520graph%2520embedding%2520learning%2520shares%2520the%2520same%2520training%2520input%2520and%2520is%250Ajointly%2520trained%2520with%2520CTR%2520prediction%252C%2520making%2520the%2520end-to-end%2520framework%2520effortless%250Ato%2520deploy%2520in%2520large-scale%2520search%2520systems.%2520The%2520proposed%2520EGIN%2520is%2520composed%2520of%2520three%250Aparts%253A%2520query-item%2520heterogeneous%2520graph%252C%2520light-weight%2520graph%2520sampling%252C%2520and%250Amulti-interest%2520network.%2520The%2520query-item%2520heterogeneous%2520graph%2520captures%2520correlation%250Aand%2520sequential%2520information%2520of%2520query%2520and%2520item%2520efficiently%2520by%2520the%2520proposed%250Alight-weight%2520graph%2520sampling.%2520The%2520multi-interest%2520network%2520is%2520well%2520designed%2520to%250Autilize%2520graph%2520embedding%2520to%2520capture%2520various%2520similarity%2520relationships%2520between%250Aquery%2520and%2520item%2520to%2520enhance%2520the%2520final%2520CTR%2520prediction.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520both%2520public%2520and%2520industrial%2520datasets%2520to%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520EGIN.%2520At%2520the%2520same%2520time%252C%2520the%2520training%2520cost%2520of%250Agraph%2520learning%2520is%2520relatively%2520low%2520compared%2520with%2520the%2520main%2520CTR%2520prediction%2520task%252C%250Aensuring%2520efficiency%2520in%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light-weight%20End-to-End%20Graph%20Interest%20Network%20for%20CTR%20Prediction%20in%0A%20%20E-commerce%20Search&entry.906535625=Pai%20Peng%20and%20Quanxiang%20Jia%20and%20Ziqiang%20Zhou%20and%20Shuang%20Hong%20and%20Zichong%20Xiao&entry.1292438233=%20%20Click-through-rate%20%28CTR%29%20prediction%20has%20an%20essential%20impact%20on%20improving%20user%0Aexperience%20and%20revenue%20in%20e-commerce%20search.%20With%20the%20development%20of%20deep%0Alearning%2C%20graph-based%20methods%20are%20well%20exploited%20to%20utilize%20graph%20structure%0Aextracted%20from%20user%20behaviors%20and%20other%20information%20to%20help%20embedding%20learning.%0AHowever%2C%20most%20of%20the%20previous%20graph-based%20methods%20mainly%20focus%20on%0Arecommendation%20scenarios%2C%20and%20therefore%20their%20graph%20structures%20highly%20depend%20on%0Aitem%27s%20sequential%20information%20from%20user%20behaviors%2C%20ignoring%20query%27s%20sequential%0Asignal%20and%20query-item%20correlation.%20In%20this%20paper%2C%20we%20propose%20a%20new%20approach%0Anamed%20Light-weight%20End-to-End%20Graph%20Interest%20Network%20%28EGIN%29%20to%20effectively%20mine%0Ausers%27%20search%20interests%20and%20tackle%20previous%20challenges.%20%28i%29%20EGIN%20utilizes%20query%0Aand%20item%27s%20correlation%20and%20sequential%20information%20from%20the%20search%20system%20to%0Abuild%20a%20heterogeneous%20graph%20for%20better%20CTR%20prediction%20in%20e-commerce%20search.%0A%28ii%29%20EGIN%27s%20graph%20embedding%20learning%20shares%20the%20same%20training%20input%20and%20is%0Ajointly%20trained%20with%20CTR%20prediction%2C%20making%20the%20end-to-end%20framework%20effortless%0Ato%20deploy%20in%20large-scale%20search%20systems.%20The%20proposed%20EGIN%20is%20composed%20of%20three%0Aparts%3A%20query-item%20heterogeneous%20graph%2C%20light-weight%20graph%20sampling%2C%20and%0Amulti-interest%20network.%20The%20query-item%20heterogeneous%20graph%20captures%20correlation%0Aand%20sequential%20information%20of%20query%20and%20item%20efficiently%20by%20the%20proposed%0Alight-weight%20graph%20sampling.%20The%20multi-interest%20network%20is%20well%20designed%20to%0Autilize%20graph%20embedding%20to%20capture%20various%20similarity%20relationships%20between%0Aquery%20and%20item%20to%20enhance%20the%20final%20CTR%20prediction.%20We%20conduct%20extensive%0Aexperiments%20on%20both%20public%20and%20industrial%20datasets%20to%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20EGIN.%20At%20the%20same%20time%2C%20the%20training%20cost%20of%0Agraph%20learning%20is%20relatively%20low%20compared%20with%20the%20main%20CTR%20prediction%20task%2C%0Aensuring%20efficiency%20in%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17745v1&entry.124074799=Read"},
{"title": "Videogenic: Identifying Highlight Moments in Videos with Professional\n  Photographs as a Prior", "author": "David Chuan-En Lin and Fabian Caba Heilbron and Joon-Young Lee and Oliver Wang and Nikolas Martelaro", "abstract": "  This paper investigates the challenge of extracting highlight moments from\nvideos. To perform this task, we need to understand what constitutes a\nhighlight for arbitrary video domains while at the same time being able to\nscale across different domains. Our key insight is that photographs taken by\nphotographers tend to capture the most remarkable or photogenic moments of an\nactivity. Drawing on this insight, we present Videogenic, a technique capable\nof creating domain-specific highlight videos for a diverse range of domains. In\na human evaluation study (N=50), we show that a high-quality photograph\ncollection combined with CLIP-based retrieval (which uses a neural network with\nsemantic knowledge of images) can serve as an excellent prior for finding video\nhighlights. In a within-subjects expert study (N=12), we demonstrate the\nusefulness of Videogenic in helping video editors create highlight videos with\nlighter workload, shorter task completion time, and better usability.\n", "link": "http://arxiv.org/abs/2211.12493v2", "date": "2024-06-25", "relevancy": 2.2373, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5942}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5502}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Videogenic%3A%20Identifying%20Highlight%20Moments%20in%20Videos%20with%20Professional%0A%20%20Photographs%20as%20a%20Prior&body=Title%3A%20Videogenic%3A%20Identifying%20Highlight%20Moments%20in%20Videos%20with%20Professional%0A%20%20Photographs%20as%20a%20Prior%0AAuthor%3A%20David%20Chuan-En%20Lin%20and%20Fabian%20Caba%20Heilbron%20and%20Joon-Young%20Lee%20and%20Oliver%20Wang%20and%20Nikolas%20Martelaro%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20challenge%20of%20extracting%20highlight%20moments%20from%0Avideos.%20To%20perform%20this%20task%2C%20we%20need%20to%20understand%20what%20constitutes%20a%0Ahighlight%20for%20arbitrary%20video%20domains%20while%20at%20the%20same%20time%20being%20able%20to%0Ascale%20across%20different%20domains.%20Our%20key%20insight%20is%20that%20photographs%20taken%20by%0Aphotographers%20tend%20to%20capture%20the%20most%20remarkable%20or%20photogenic%20moments%20of%20an%0Aactivity.%20Drawing%20on%20this%20insight%2C%20we%20present%20Videogenic%2C%20a%20technique%20capable%0Aof%20creating%20domain-specific%20highlight%20videos%20for%20a%20diverse%20range%20of%20domains.%20In%0Aa%20human%20evaluation%20study%20%28N%3D50%29%2C%20we%20show%20that%20a%20high-quality%20photograph%0Acollection%20combined%20with%20CLIP-based%20retrieval%20%28which%20uses%20a%20neural%20network%20with%0Asemantic%20knowledge%20of%20images%29%20can%20serve%20as%20an%20excellent%20prior%20for%20finding%20video%0Ahighlights.%20In%20a%20within-subjects%20expert%20study%20%28N%3D12%29%2C%20we%20demonstrate%20the%0Ausefulness%20of%20Videogenic%20in%20helping%20video%20editors%20create%20highlight%20videos%20with%0Alighter%20workload%2C%20shorter%20task%20completion%20time%2C%20and%20better%20usability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.12493v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideogenic%253A%2520Identifying%2520Highlight%2520Moments%2520in%2520Videos%2520with%2520Professional%250A%2520%2520Photographs%2520as%2520a%2520Prior%26entry.906535625%3DDavid%2520Chuan-En%2520Lin%2520and%2520Fabian%2520Caba%2520Heilbron%2520and%2520Joon-Young%2520Lee%2520and%2520Oliver%2520Wang%2520and%2520Nikolas%2520Martelaro%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520challenge%2520of%2520extracting%2520highlight%2520moments%2520from%250Avideos.%2520To%2520perform%2520this%2520task%252C%2520we%2520need%2520to%2520understand%2520what%2520constitutes%2520a%250Ahighlight%2520for%2520arbitrary%2520video%2520domains%2520while%2520at%2520the%2520same%2520time%2520being%2520able%2520to%250Ascale%2520across%2520different%2520domains.%2520Our%2520key%2520insight%2520is%2520that%2520photographs%2520taken%2520by%250Aphotographers%2520tend%2520to%2520capture%2520the%2520most%2520remarkable%2520or%2520photogenic%2520moments%2520of%2520an%250Aactivity.%2520Drawing%2520on%2520this%2520insight%252C%2520we%2520present%2520Videogenic%252C%2520a%2520technique%2520capable%250Aof%2520creating%2520domain-specific%2520highlight%2520videos%2520for%2520a%2520diverse%2520range%2520of%2520domains.%2520In%250Aa%2520human%2520evaluation%2520study%2520%2528N%253D50%2529%252C%2520we%2520show%2520that%2520a%2520high-quality%2520photograph%250Acollection%2520combined%2520with%2520CLIP-based%2520retrieval%2520%2528which%2520uses%2520a%2520neural%2520network%2520with%250Asemantic%2520knowledge%2520of%2520images%2529%2520can%2520serve%2520as%2520an%2520excellent%2520prior%2520for%2520finding%2520video%250Ahighlights.%2520In%2520a%2520within-subjects%2520expert%2520study%2520%2528N%253D12%2529%252C%2520we%2520demonstrate%2520the%250Ausefulness%2520of%2520Videogenic%2520in%2520helping%2520video%2520editors%2520create%2520highlight%2520videos%2520with%250Alighter%2520workload%252C%2520shorter%2520task%2520completion%2520time%252C%2520and%2520better%2520usability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.12493v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Videogenic%3A%20Identifying%20Highlight%20Moments%20in%20Videos%20with%20Professional%0A%20%20Photographs%20as%20a%20Prior&entry.906535625=David%20Chuan-En%20Lin%20and%20Fabian%20Caba%20Heilbron%20and%20Joon-Young%20Lee%20and%20Oliver%20Wang%20and%20Nikolas%20Martelaro&entry.1292438233=%20%20This%20paper%20investigates%20the%20challenge%20of%20extracting%20highlight%20moments%20from%0Avideos.%20To%20perform%20this%20task%2C%20we%20need%20to%20understand%20what%20constitutes%20a%0Ahighlight%20for%20arbitrary%20video%20domains%20while%20at%20the%20same%20time%20being%20able%20to%0Ascale%20across%20different%20domains.%20Our%20key%20insight%20is%20that%20photographs%20taken%20by%0Aphotographers%20tend%20to%20capture%20the%20most%20remarkable%20or%20photogenic%20moments%20of%20an%0Aactivity.%20Drawing%20on%20this%20insight%2C%20we%20present%20Videogenic%2C%20a%20technique%20capable%0Aof%20creating%20domain-specific%20highlight%20videos%20for%20a%20diverse%20range%20of%20domains.%20In%0Aa%20human%20evaluation%20study%20%28N%3D50%29%2C%20we%20show%20that%20a%20high-quality%20photograph%0Acollection%20combined%20with%20CLIP-based%20retrieval%20%28which%20uses%20a%20neural%20network%20with%0Asemantic%20knowledge%20of%20images%29%20can%20serve%20as%20an%20excellent%20prior%20for%20finding%20video%0Ahighlights.%20In%20a%20within-subjects%20expert%20study%20%28N%3D12%29%2C%20we%20demonstrate%20the%0Ausefulness%20of%20Videogenic%20in%20helping%20video%20editors%20create%20highlight%20videos%20with%0Alighter%20workload%2C%20shorter%20task%20completion%20time%2C%20and%20better%20usability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.12493v2&entry.124074799=Read"},
{"title": "Toward Universal Medical Image Registration via Sharpness-Aware\n  Meta-Continual Learning", "author": "Bomin Wang and Xinzhe Luo and Xiahai Zhuang", "abstract": "  Current deep learning approaches in medical image registration usually face\nthe challenges of distribution shift and data collection, hindering real-world\ndeployment. In contrast, universal medical image registration aims to perform\nregistration on a wide range of clinically relevant tasks simultaneously, thus\nhaving tremendous potential for clinical applications. In this paper, we\npresent the first attempt to achieve the goal of universal 3D medical image\nregistration in sequential learning scenarios by proposing a continual learning\nmethod. Specifically, we utilize meta-learning with experience replay to\nmitigating the problem of catastrophic forgetting. To promote the\ngeneralizability of meta-continual learning, we further propose sharpness-aware\nmeta-continual learning (SAMCL). We validate the effectiveness of our method on\nfour datasets in a continual learning setup, including brain MR, abdomen CT,\nlung CT, and abdomen MR-CT image pairs. Results have shown the potential of\nSAMCL in realizing universal image registration, which performs better than or\non par with vanilla sequential or centralized multi-task training\nstrategies.The source code will be available from\nhttps://github.com/xzluo97/Continual-Reg.\n", "link": "http://arxiv.org/abs/2406.17575v1", "date": "2024-06-25", "relevancy": 2.2363, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5665}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5545}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Universal%20Medical%20Image%20Registration%20via%20Sharpness-Aware%0A%20%20Meta-Continual%20Learning&body=Title%3A%20Toward%20Universal%20Medical%20Image%20Registration%20via%20Sharpness-Aware%0A%20%20Meta-Continual%20Learning%0AAuthor%3A%20Bomin%20Wang%20and%20Xinzhe%20Luo%20and%20Xiahai%20Zhuang%0AAbstract%3A%20%20%20Current%20deep%20learning%20approaches%20in%20medical%20image%20registration%20usually%20face%0Athe%20challenges%20of%20distribution%20shift%20and%20data%20collection%2C%20hindering%20real-world%0Adeployment.%20In%20contrast%2C%20universal%20medical%20image%20registration%20aims%20to%20perform%0Aregistration%20on%20a%20wide%20range%20of%20clinically%20relevant%20tasks%20simultaneously%2C%20thus%0Ahaving%20tremendous%20potential%20for%20clinical%20applications.%20In%20this%20paper%2C%20we%0Apresent%20the%20first%20attempt%20to%20achieve%20the%20goal%20of%20universal%203D%20medical%20image%0Aregistration%20in%20sequential%20learning%20scenarios%20by%20proposing%20a%20continual%20learning%0Amethod.%20Specifically%2C%20we%20utilize%20meta-learning%20with%20experience%20replay%20to%0Amitigating%20the%20problem%20of%20catastrophic%20forgetting.%20To%20promote%20the%0Ageneralizability%20of%20meta-continual%20learning%2C%20we%20further%20propose%20sharpness-aware%0Ameta-continual%20learning%20%28SAMCL%29.%20We%20validate%20the%20effectiveness%20of%20our%20method%20on%0Afour%20datasets%20in%20a%20continual%20learning%20setup%2C%20including%20brain%20MR%2C%20abdomen%20CT%2C%0Alung%20CT%2C%20and%20abdomen%20MR-CT%20image%20pairs.%20Results%20have%20shown%20the%20potential%20of%0ASAMCL%20in%20realizing%20universal%20image%20registration%2C%20which%20performs%20better%20than%20or%0Aon%20par%20with%20vanilla%20sequential%20or%20centralized%20multi-task%20training%0Astrategies.The%20source%20code%20will%20be%20available%20from%0Ahttps%3A//github.com/xzluo97/Continual-Reg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Universal%2520Medical%2520Image%2520Registration%2520via%2520Sharpness-Aware%250A%2520%2520Meta-Continual%2520Learning%26entry.906535625%3DBomin%2520Wang%2520and%2520Xinzhe%2520Luo%2520and%2520Xiahai%2520Zhuang%26entry.1292438233%3D%2520%2520Current%2520deep%2520learning%2520approaches%2520in%2520medical%2520image%2520registration%2520usually%2520face%250Athe%2520challenges%2520of%2520distribution%2520shift%2520and%2520data%2520collection%252C%2520hindering%2520real-world%250Adeployment.%2520In%2520contrast%252C%2520universal%2520medical%2520image%2520registration%2520aims%2520to%2520perform%250Aregistration%2520on%2520a%2520wide%2520range%2520of%2520clinically%2520relevant%2520tasks%2520simultaneously%252C%2520thus%250Ahaving%2520tremendous%2520potential%2520for%2520clinical%2520applications.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520the%2520first%2520attempt%2520to%2520achieve%2520the%2520goal%2520of%2520universal%25203D%2520medical%2520image%250Aregistration%2520in%2520sequential%2520learning%2520scenarios%2520by%2520proposing%2520a%2520continual%2520learning%250Amethod.%2520Specifically%252C%2520we%2520utilize%2520meta-learning%2520with%2520experience%2520replay%2520to%250Amitigating%2520the%2520problem%2520of%2520catastrophic%2520forgetting.%2520To%2520promote%2520the%250Ageneralizability%2520of%2520meta-continual%2520learning%252C%2520we%2520further%2520propose%2520sharpness-aware%250Ameta-continual%2520learning%2520%2528SAMCL%2529.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%250Afour%2520datasets%2520in%2520a%2520continual%2520learning%2520setup%252C%2520including%2520brain%2520MR%252C%2520abdomen%2520CT%252C%250Alung%2520CT%252C%2520and%2520abdomen%2520MR-CT%2520image%2520pairs.%2520Results%2520have%2520shown%2520the%2520potential%2520of%250ASAMCL%2520in%2520realizing%2520universal%2520image%2520registration%252C%2520which%2520performs%2520better%2520than%2520or%250Aon%2520par%2520with%2520vanilla%2520sequential%2520or%2520centralized%2520multi-task%2520training%250Astrategies.The%2520source%2520code%2520will%2520be%2520available%2520from%250Ahttps%253A//github.com/xzluo97/Continual-Reg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Universal%20Medical%20Image%20Registration%20via%20Sharpness-Aware%0A%20%20Meta-Continual%20Learning&entry.906535625=Bomin%20Wang%20and%20Xinzhe%20Luo%20and%20Xiahai%20Zhuang&entry.1292438233=%20%20Current%20deep%20learning%20approaches%20in%20medical%20image%20registration%20usually%20face%0Athe%20challenges%20of%20distribution%20shift%20and%20data%20collection%2C%20hindering%20real-world%0Adeployment.%20In%20contrast%2C%20universal%20medical%20image%20registration%20aims%20to%20perform%0Aregistration%20on%20a%20wide%20range%20of%20clinically%20relevant%20tasks%20simultaneously%2C%20thus%0Ahaving%20tremendous%20potential%20for%20clinical%20applications.%20In%20this%20paper%2C%20we%0Apresent%20the%20first%20attempt%20to%20achieve%20the%20goal%20of%20universal%203D%20medical%20image%0Aregistration%20in%20sequential%20learning%20scenarios%20by%20proposing%20a%20continual%20learning%0Amethod.%20Specifically%2C%20we%20utilize%20meta-learning%20with%20experience%20replay%20to%0Amitigating%20the%20problem%20of%20catastrophic%20forgetting.%20To%20promote%20the%0Ageneralizability%20of%20meta-continual%20learning%2C%20we%20further%20propose%20sharpness-aware%0Ameta-continual%20learning%20%28SAMCL%29.%20We%20validate%20the%20effectiveness%20of%20our%20method%20on%0Afour%20datasets%20in%20a%20continual%20learning%20setup%2C%20including%20brain%20MR%2C%20abdomen%20CT%2C%0Alung%20CT%2C%20and%20abdomen%20MR-CT%20image%20pairs.%20Results%20have%20shown%20the%20potential%20of%0ASAMCL%20in%20realizing%20universal%20image%20registration%2C%20which%20performs%20better%20than%20or%0Aon%20par%20with%20vanilla%20sequential%20or%20centralized%20multi-task%20training%0Astrategies.The%20source%20code%20will%20be%20available%20from%0Ahttps%3A//github.com/xzluo97/Continual-Reg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17575v1&entry.124074799=Read"},
{"title": "Tell Me Where You Are: Multimodal LLMs Meet Place Recognition", "author": "Zonglin Lyu and Juexiao Zhang and Mingxuan Lu and Yiming Li and Chen Feng", "abstract": "  Large language models (LLMs) exhibit a variety of promising capabilities in\nrobotics, including long-horizon planning and commonsense reasoning. However,\ntheir performance in place recognition is still underexplored. In this work, we\nintroduce multimodal LLMs (MLLMs) to visual place recognition (VPR), where a\nrobot must localize itself using visual observations. Our key design is to use\nvision-based retrieval to propose several candidates and then leverage\nlanguage-based reasoning to carefully inspect each candidate for a final\ndecision. Specifically, we leverage the robust visual features produced by\noff-the-shelf vision foundation models (VFMs) to obtain several candidate\nlocations. We then prompt an MLLM to describe the differences between the\ncurrent observation and each candidate in a pairwise manner, and reason about\nthe best candidate based on these descriptions. Our results on three datasets\ndemonstrate that integrating the general-purpose visual features from VFMs with\nthe reasoning capabilities of MLLMs already provides an effective place\nrecognition solution, without any VPR-specific supervised training. We believe\nour work can inspire new possibilities for applying and designing foundation\nmodels, i.e., VFMs, LLMs, and MLLMs, to enhance the localization and navigation\nof mobile robots.\n", "link": "http://arxiv.org/abs/2406.17520v1", "date": "2024-06-25", "relevancy": 2.2329, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5909}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5663}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tell%20Me%20Where%20You%20Are%3A%20Multimodal%20LLMs%20Meet%20Place%20Recognition&body=Title%3A%20Tell%20Me%20Where%20You%20Are%3A%20Multimodal%20LLMs%20Meet%20Place%20Recognition%0AAuthor%3A%20Zonglin%20Lyu%20and%20Juexiao%20Zhang%20and%20Mingxuan%20Lu%20and%20Yiming%20Li%20and%20Chen%20Feng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20a%20variety%20of%20promising%20capabilities%20in%0Arobotics%2C%20including%20long-horizon%20planning%20and%20commonsense%20reasoning.%20However%2C%0Atheir%20performance%20in%20place%20recognition%20is%20still%20underexplored.%20In%20this%20work%2C%20we%0Aintroduce%20multimodal%20LLMs%20%28MLLMs%29%20to%20visual%20place%20recognition%20%28VPR%29%2C%20where%20a%0Arobot%20must%20localize%20itself%20using%20visual%20observations.%20Our%20key%20design%20is%20to%20use%0Avision-based%20retrieval%20to%20propose%20several%20candidates%20and%20then%20leverage%0Alanguage-based%20reasoning%20to%20carefully%20inspect%20each%20candidate%20for%20a%20final%0Adecision.%20Specifically%2C%20we%20leverage%20the%20robust%20visual%20features%20produced%20by%0Aoff-the-shelf%20vision%20foundation%20models%20%28VFMs%29%20to%20obtain%20several%20candidate%0Alocations.%20We%20then%20prompt%20an%20MLLM%20to%20describe%20the%20differences%20between%20the%0Acurrent%20observation%20and%20each%20candidate%20in%20a%20pairwise%20manner%2C%20and%20reason%20about%0Athe%20best%20candidate%20based%20on%20these%20descriptions.%20Our%20results%20on%20three%20datasets%0Ademonstrate%20that%20integrating%20the%20general-purpose%20visual%20features%20from%20VFMs%20with%0Athe%20reasoning%20capabilities%20of%20MLLMs%20already%20provides%20an%20effective%20place%0Arecognition%20solution%2C%20without%20any%20VPR-specific%20supervised%20training.%20We%20believe%0Aour%20work%20can%20inspire%20new%20possibilities%20for%20applying%20and%20designing%20foundation%0Amodels%2C%20i.e.%2C%20VFMs%2C%20LLMs%2C%20and%20MLLMs%2C%20to%20enhance%20the%20localization%20and%20navigation%0Aof%20mobile%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTell%2520Me%2520Where%2520You%2520Are%253A%2520Multimodal%2520LLMs%2520Meet%2520Place%2520Recognition%26entry.906535625%3DZonglin%2520Lyu%2520and%2520Juexiao%2520Zhang%2520and%2520Mingxuan%2520Lu%2520and%2520Yiming%2520Li%2520and%2520Chen%2520Feng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520a%2520variety%2520of%2520promising%2520capabilities%2520in%250Arobotics%252C%2520including%2520long-horizon%2520planning%2520and%2520commonsense%2520reasoning.%2520However%252C%250Atheir%2520performance%2520in%2520place%2520recognition%2520is%2520still%2520underexplored.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520to%2520visual%2520place%2520recognition%2520%2528VPR%2529%252C%2520where%2520a%250Arobot%2520must%2520localize%2520itself%2520using%2520visual%2520observations.%2520Our%2520key%2520design%2520is%2520to%2520use%250Avision-based%2520retrieval%2520to%2520propose%2520several%2520candidates%2520and%2520then%2520leverage%250Alanguage-based%2520reasoning%2520to%2520carefully%2520inspect%2520each%2520candidate%2520for%2520a%2520final%250Adecision.%2520Specifically%252C%2520we%2520leverage%2520the%2520robust%2520visual%2520features%2520produced%2520by%250Aoff-the-shelf%2520vision%2520foundation%2520models%2520%2528VFMs%2529%2520to%2520obtain%2520several%2520candidate%250Alocations.%2520We%2520then%2520prompt%2520an%2520MLLM%2520to%2520describe%2520the%2520differences%2520between%2520the%250Acurrent%2520observation%2520and%2520each%2520candidate%2520in%2520a%2520pairwise%2520manner%252C%2520and%2520reason%2520about%250Athe%2520best%2520candidate%2520based%2520on%2520these%2520descriptions.%2520Our%2520results%2520on%2520three%2520datasets%250Ademonstrate%2520that%2520integrating%2520the%2520general-purpose%2520visual%2520features%2520from%2520VFMs%2520with%250Athe%2520reasoning%2520capabilities%2520of%2520MLLMs%2520already%2520provides%2520an%2520effective%2520place%250Arecognition%2520solution%252C%2520without%2520any%2520VPR-specific%2520supervised%2520training.%2520We%2520believe%250Aour%2520work%2520can%2520inspire%2520new%2520possibilities%2520for%2520applying%2520and%2520designing%2520foundation%250Amodels%252C%2520i.e.%252C%2520VFMs%252C%2520LLMs%252C%2520and%2520MLLMs%252C%2520to%2520enhance%2520the%2520localization%2520and%2520navigation%250Aof%2520mobile%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tell%20Me%20Where%20You%20Are%3A%20Multimodal%20LLMs%20Meet%20Place%20Recognition&entry.906535625=Zonglin%20Lyu%20and%20Juexiao%20Zhang%20and%20Mingxuan%20Lu%20and%20Yiming%20Li%20and%20Chen%20Feng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20a%20variety%20of%20promising%20capabilities%20in%0Arobotics%2C%20including%20long-horizon%20planning%20and%20commonsense%20reasoning.%20However%2C%0Atheir%20performance%20in%20place%20recognition%20is%20still%20underexplored.%20In%20this%20work%2C%20we%0Aintroduce%20multimodal%20LLMs%20%28MLLMs%29%20to%20visual%20place%20recognition%20%28VPR%29%2C%20where%20a%0Arobot%20must%20localize%20itself%20using%20visual%20observations.%20Our%20key%20design%20is%20to%20use%0Avision-based%20retrieval%20to%20propose%20several%20candidates%20and%20then%20leverage%0Alanguage-based%20reasoning%20to%20carefully%20inspect%20each%20candidate%20for%20a%20final%0Adecision.%20Specifically%2C%20we%20leverage%20the%20robust%20visual%20features%20produced%20by%0Aoff-the-shelf%20vision%20foundation%20models%20%28VFMs%29%20to%20obtain%20several%20candidate%0Alocations.%20We%20then%20prompt%20an%20MLLM%20to%20describe%20the%20differences%20between%20the%0Acurrent%20observation%20and%20each%20candidate%20in%20a%20pairwise%20manner%2C%20and%20reason%20about%0Athe%20best%20candidate%20based%20on%20these%20descriptions.%20Our%20results%20on%20three%20datasets%0Ademonstrate%20that%20integrating%20the%20general-purpose%20visual%20features%20from%20VFMs%20with%0Athe%20reasoning%20capabilities%20of%20MLLMs%20already%20provides%20an%20effective%20place%0Arecognition%20solution%2C%20without%20any%20VPR-specific%20supervised%20training.%20We%20believe%0Aour%20work%20can%20inspire%20new%20possibilities%20for%20applying%20and%20designing%20foundation%0Amodels%2C%20i.e.%2C%20VFMs%2C%20LLMs%2C%20and%20MLLMs%2C%20to%20enhance%20the%20localization%20and%20navigation%0Aof%20mobile%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17520v1&entry.124074799=Read"},
{"title": "DriveVLM: The Convergence of Autonomous Driving and Large\n  Vision-Language Models", "author": "Xiaoyu Tian and Junru Gu and Bailin Li and Yicheng Liu and Yang Wang and Zhiyong Zhao and Kun Zhan and Peng Jia and Xianpeng Lang and Hang Zhao", "abstract": "  A primary hurdle of autonomous driving in urban environments is understanding\ncomplex and long-tail scenarios, such as challenging road conditions and\ndelicate human behaviors. We introduce DriveVLM, an autonomous driving system\nleveraging Vision-Language Models (VLMs) for enhanced scene understanding and\nplanning capabilities. DriveVLM integrates a unique combination of reasoning\nmodules for scene description, scene analysis, and hierarchical planning.\nFurthermore, recognizing the limitations of VLMs in spatial reasoning and heavy\ncomputational requirements, we propose DriveVLM-Dual, a hybrid system that\nsynergizes the strengths of DriveVLM with the traditional autonomous driving\npipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset\ndemonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and\nunpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a\nproduction vehicle, verifying it is effective in real-world autonomous driving\nenvironments.\n", "link": "http://arxiv.org/abs/2402.12289v5", "date": "2024-06-25", "relevancy": 2.2262, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveVLM%3A%20The%20Convergence%20of%20Autonomous%20Driving%20and%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20DriveVLM%3A%20The%20Convergence%20of%20Autonomous%20Driving%20and%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Xiaoyu%20Tian%20and%20Junru%20Gu%20and%20Bailin%20Li%20and%20Yicheng%20Liu%20and%20Yang%20Wang%20and%20Zhiyong%20Zhao%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20A%20primary%20hurdle%20of%20autonomous%20driving%20in%20urban%20environments%20is%20understanding%0Acomplex%20and%20long-tail%20scenarios%2C%20such%20as%20challenging%20road%20conditions%20and%0Adelicate%20human%20behaviors.%20We%20introduce%20DriveVLM%2C%20an%20autonomous%20driving%20system%0Aleveraging%20Vision-Language%20Models%20%28VLMs%29%20for%20enhanced%20scene%20understanding%20and%0Aplanning%20capabilities.%20DriveVLM%20integrates%20a%20unique%20combination%20of%20reasoning%0Amodules%20for%20scene%20description%2C%20scene%20analysis%2C%20and%20hierarchical%20planning.%0AFurthermore%2C%20recognizing%20the%20limitations%20of%20VLMs%20in%20spatial%20reasoning%20and%20heavy%0Acomputational%20requirements%2C%20we%20propose%20DriveVLM-Dual%2C%20a%20hybrid%20system%20that%0Asynergizes%20the%20strengths%20of%20DriveVLM%20with%20the%20traditional%20autonomous%20driving%0Apipeline.%20Experiments%20on%20both%20the%20nuScenes%20dataset%20and%20our%20SUP-AD%20dataset%0Ademonstrate%20the%20efficacy%20of%20DriveVLM%20and%20DriveVLM-Dual%20in%20handling%20complex%20and%0Aunpredictable%20driving%20conditions.%20Finally%2C%20we%20deploy%20the%20DriveVLM-Dual%20on%20a%0Aproduction%20vehicle%2C%20verifying%20it%20is%20effective%20in%20real-world%20autonomous%20driving%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12289v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveVLM%253A%2520The%2520Convergence%2520of%2520Autonomous%2520Driving%2520and%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DXiaoyu%2520Tian%2520and%2520Junru%2520Gu%2520and%2520Bailin%2520Li%2520and%2520Yicheng%2520Liu%2520and%2520Yang%2520Wang%2520and%2520Zhiyong%2520Zhao%2520and%2520Kun%2520Zhan%2520and%2520Peng%2520Jia%2520and%2520Xianpeng%2520Lang%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520A%2520primary%2520hurdle%2520of%2520autonomous%2520driving%2520in%2520urban%2520environments%2520is%2520understanding%250Acomplex%2520and%2520long-tail%2520scenarios%252C%2520such%2520as%2520challenging%2520road%2520conditions%2520and%250Adelicate%2520human%2520behaviors.%2520We%2520introduce%2520DriveVLM%252C%2520an%2520autonomous%2520driving%2520system%250Aleveraging%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520enhanced%2520scene%2520understanding%2520and%250Aplanning%2520capabilities.%2520DriveVLM%2520integrates%2520a%2520unique%2520combination%2520of%2520reasoning%250Amodules%2520for%2520scene%2520description%252C%2520scene%2520analysis%252C%2520and%2520hierarchical%2520planning.%250AFurthermore%252C%2520recognizing%2520the%2520limitations%2520of%2520VLMs%2520in%2520spatial%2520reasoning%2520and%2520heavy%250Acomputational%2520requirements%252C%2520we%2520propose%2520DriveVLM-Dual%252C%2520a%2520hybrid%2520system%2520that%250Asynergizes%2520the%2520strengths%2520of%2520DriveVLM%2520with%2520the%2520traditional%2520autonomous%2520driving%250Apipeline.%2520Experiments%2520on%2520both%2520the%2520nuScenes%2520dataset%2520and%2520our%2520SUP-AD%2520dataset%250Ademonstrate%2520the%2520efficacy%2520of%2520DriveVLM%2520and%2520DriveVLM-Dual%2520in%2520handling%2520complex%2520and%250Aunpredictable%2520driving%2520conditions.%2520Finally%252C%2520we%2520deploy%2520the%2520DriveVLM-Dual%2520on%2520a%250Aproduction%2520vehicle%252C%2520verifying%2520it%2520is%2520effective%2520in%2520real-world%2520autonomous%2520driving%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12289v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveVLM%3A%20The%20Convergence%20of%20Autonomous%20Driving%20and%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Xiaoyu%20Tian%20and%20Junru%20Gu%20and%20Bailin%20Li%20and%20Yicheng%20Liu%20and%20Yang%20Wang%20and%20Zhiyong%20Zhao%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Hang%20Zhao&entry.1292438233=%20%20A%20primary%20hurdle%20of%20autonomous%20driving%20in%20urban%20environments%20is%20understanding%0Acomplex%20and%20long-tail%20scenarios%2C%20such%20as%20challenging%20road%20conditions%20and%0Adelicate%20human%20behaviors.%20We%20introduce%20DriveVLM%2C%20an%20autonomous%20driving%20system%0Aleveraging%20Vision-Language%20Models%20%28VLMs%29%20for%20enhanced%20scene%20understanding%20and%0Aplanning%20capabilities.%20DriveVLM%20integrates%20a%20unique%20combination%20of%20reasoning%0Amodules%20for%20scene%20description%2C%20scene%20analysis%2C%20and%20hierarchical%20planning.%0AFurthermore%2C%20recognizing%20the%20limitations%20of%20VLMs%20in%20spatial%20reasoning%20and%20heavy%0Acomputational%20requirements%2C%20we%20propose%20DriveVLM-Dual%2C%20a%20hybrid%20system%20that%0Asynergizes%20the%20strengths%20of%20DriveVLM%20with%20the%20traditional%20autonomous%20driving%0Apipeline.%20Experiments%20on%20both%20the%20nuScenes%20dataset%20and%20our%20SUP-AD%20dataset%0Ademonstrate%20the%20efficacy%20of%20DriveVLM%20and%20DriveVLM-Dual%20in%20handling%20complex%20and%0Aunpredictable%20driving%20conditions.%20Finally%2C%20we%20deploy%20the%20DriveVLM-Dual%20on%20a%0Aproduction%20vehicle%2C%20verifying%20it%20is%20effective%20in%20real-world%20autonomous%20driving%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12289v5&entry.124074799=Read"},
{"title": "Enhancing Active Learning for Sentinel 2 Imagery through Contrastive\n  Learning and Uncertainty Estimation", "author": "David Pogorzelski and Peter Arlinghaus and Wenyan Zhang", "abstract": "  In this paper, we introduce a novel method designed to enhance label\nefficiency in satellite imagery analysis by integrating semi-supervised\nlearning (SSL) with active learning strategies. Our approach utilizes\ncontrastive learning together with uncertainty estimations via Monte Carlo\nDropout (MC Dropout), with a particular focus on Sentinel-2 imagery analyzed\nusing the Eurosat dataset. We explore the effectiveness of our method in\nscenarios featuring both balanced and unbalanced class distributions. Our\nresults show that the proposed method performs better than several other\npopular methods in this field, enabling significant savings in labeling effort\nwhile maintaining high classification accuracy. These findings highlight the\npotential of our approach to facilitate scalable and cost-effective satellite\nimage analysis, particularly advantageous for extensive environmental\nmonitoring and land use classification tasks.\n", "link": "http://arxiv.org/abs/2405.13285v2", "date": "2024-06-25", "relevancy": 2.2007, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5954}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5529}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Active%20Learning%20for%20Sentinel%202%20Imagery%20through%20Contrastive%0A%20%20Learning%20and%20Uncertainty%20Estimation&body=Title%3A%20Enhancing%20Active%20Learning%20for%20Sentinel%202%20Imagery%20through%20Contrastive%0A%20%20Learning%20and%20Uncertainty%20Estimation%0AAuthor%3A%20David%20Pogorzelski%20and%20Peter%20Arlinghaus%20and%20Wenyan%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20designed%20to%20enhance%20label%0Aefficiency%20in%20satellite%20imagery%20analysis%20by%20integrating%20semi-supervised%0Alearning%20%28SSL%29%20with%20active%20learning%20strategies.%20Our%20approach%20utilizes%0Acontrastive%20learning%20together%20with%20uncertainty%20estimations%20via%20Monte%20Carlo%0ADropout%20%28MC%20Dropout%29%2C%20with%20a%20particular%20focus%20on%20Sentinel-2%20imagery%20analyzed%0Ausing%20the%20Eurosat%20dataset.%20We%20explore%20the%20effectiveness%20of%20our%20method%20in%0Ascenarios%20featuring%20both%20balanced%20and%20unbalanced%20class%20distributions.%20Our%0Aresults%20show%20that%20the%20proposed%20method%20performs%20better%20than%20several%20other%0Apopular%20methods%20in%20this%20field%2C%20enabling%20significant%20savings%20in%20labeling%20effort%0Awhile%20maintaining%20high%20classification%20accuracy.%20These%20findings%20highlight%20the%0Apotential%20of%20our%20approach%20to%20facilitate%20scalable%20and%20cost-effective%20satellite%0Aimage%20analysis%2C%20particularly%20advantageous%20for%20extensive%20environmental%0Amonitoring%20and%20land%20use%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Active%2520Learning%2520for%2520Sentinel%25202%2520Imagery%2520through%2520Contrastive%250A%2520%2520Learning%2520and%2520Uncertainty%2520Estimation%26entry.906535625%3DDavid%2520Pogorzelski%2520and%2520Peter%2520Arlinghaus%2520and%2520Wenyan%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520method%2520designed%2520to%2520enhance%2520label%250Aefficiency%2520in%2520satellite%2520imagery%2520analysis%2520by%2520integrating%2520semi-supervised%250Alearning%2520%2528SSL%2529%2520with%2520active%2520learning%2520strategies.%2520Our%2520approach%2520utilizes%250Acontrastive%2520learning%2520together%2520with%2520uncertainty%2520estimations%2520via%2520Monte%2520Carlo%250ADropout%2520%2528MC%2520Dropout%2529%252C%2520with%2520a%2520particular%2520focus%2520on%2520Sentinel-2%2520imagery%2520analyzed%250Ausing%2520the%2520Eurosat%2520dataset.%2520We%2520explore%2520the%2520effectiveness%2520of%2520our%2520method%2520in%250Ascenarios%2520featuring%2520both%2520balanced%2520and%2520unbalanced%2520class%2520distributions.%2520Our%250Aresults%2520show%2520that%2520the%2520proposed%2520method%2520performs%2520better%2520than%2520several%2520other%250Apopular%2520methods%2520in%2520this%2520field%252C%2520enabling%2520significant%2520savings%2520in%2520labeling%2520effort%250Awhile%2520maintaining%2520high%2520classification%2520accuracy.%2520These%2520findings%2520highlight%2520the%250Apotential%2520of%2520our%2520approach%2520to%2520facilitate%2520scalable%2520and%2520cost-effective%2520satellite%250Aimage%2520analysis%252C%2520particularly%2520advantageous%2520for%2520extensive%2520environmental%250Amonitoring%2520and%2520land%2520use%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Active%20Learning%20for%20Sentinel%202%20Imagery%20through%20Contrastive%0A%20%20Learning%20and%20Uncertainty%20Estimation&entry.906535625=David%20Pogorzelski%20and%20Peter%20Arlinghaus%20and%20Wenyan%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20designed%20to%20enhance%20label%0Aefficiency%20in%20satellite%20imagery%20analysis%20by%20integrating%20semi-supervised%0Alearning%20%28SSL%29%20with%20active%20learning%20strategies.%20Our%20approach%20utilizes%0Acontrastive%20learning%20together%20with%20uncertainty%20estimations%20via%20Monte%20Carlo%0ADropout%20%28MC%20Dropout%29%2C%20with%20a%20particular%20focus%20on%20Sentinel-2%20imagery%20analyzed%0Ausing%20the%20Eurosat%20dataset.%20We%20explore%20the%20effectiveness%20of%20our%20method%20in%0Ascenarios%20featuring%20both%20balanced%20and%20unbalanced%20class%20distributions.%20Our%0Aresults%20show%20that%20the%20proposed%20method%20performs%20better%20than%20several%20other%0Apopular%20methods%20in%20this%20field%2C%20enabling%20significant%20savings%20in%20labeling%20effort%0Awhile%20maintaining%20high%20classification%20accuracy.%20These%20findings%20highlight%20the%0Apotential%20of%20our%20approach%20to%20facilitate%20scalable%20and%20cost-effective%20satellite%0Aimage%20analysis%2C%20particularly%20advantageous%20for%20extensive%20environmental%0Amonitoring%20and%20land%20use%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13285v2&entry.124074799=Read"},
{"title": "In value-based deep reinforcement learning, a pruned network is a good\n  network", "author": "Johan Obando-Ceron and Aaron Courville and Pablo Samuel Castro", "abstract": "  Recent work has shown that deep reinforcement learning agents have difficulty\nin effectively using their network parameters. We leverage prior insights into\nthe advantages of sparse training techniques and demonstrate that gradual\nmagnitude pruning enables value-based agents to maximize parameter\neffectiveness. This results in networks that yield dramatic performance\nimprovements over traditional networks, using only a small fraction of the full\nnetwork parameters.\n", "link": "http://arxiv.org/abs/2402.12479v3", "date": "2024-06-25", "relevancy": 2.1899, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4521}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4338}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20value-based%20deep%20reinforcement%20learning%2C%20a%20pruned%20network%20is%20a%20good%0A%20%20network&body=Title%3A%20In%20value-based%20deep%20reinforcement%20learning%2C%20a%20pruned%20network%20is%20a%20good%0A%20%20network%0AAuthor%3A%20Johan%20Obando-Ceron%20and%20Aaron%20Courville%20and%20Pablo%20Samuel%20Castro%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%20deep%20reinforcement%20learning%20agents%20have%20difficulty%0Ain%20effectively%20using%20their%20network%20parameters.%20We%20leverage%20prior%20insights%20into%0Athe%20advantages%20of%20sparse%20training%20techniques%20and%20demonstrate%20that%20gradual%0Amagnitude%20pruning%20enables%20value-based%20agents%20to%20maximize%20parameter%0Aeffectiveness.%20This%20results%20in%20networks%20that%20yield%20dramatic%20performance%0Aimprovements%20over%20traditional%20networks%2C%20using%20only%20a%20small%20fraction%20of%20the%20full%0Anetwork%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12479v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520value-based%2520deep%2520reinforcement%2520learning%252C%2520a%2520pruned%2520network%2520is%2520a%2520good%250A%2520%2520network%26entry.906535625%3DJohan%2520Obando-Ceron%2520and%2520Aaron%2520Courville%2520and%2520Pablo%2520Samuel%2520Castro%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%2520deep%2520reinforcement%2520learning%2520agents%2520have%2520difficulty%250Ain%2520effectively%2520using%2520their%2520network%2520parameters.%2520We%2520leverage%2520prior%2520insights%2520into%250Athe%2520advantages%2520of%2520sparse%2520training%2520techniques%2520and%2520demonstrate%2520that%2520gradual%250Amagnitude%2520pruning%2520enables%2520value-based%2520agents%2520to%2520maximize%2520parameter%250Aeffectiveness.%2520This%2520results%2520in%2520networks%2520that%2520yield%2520dramatic%2520performance%250Aimprovements%2520over%2520traditional%2520networks%252C%2520using%2520only%2520a%2520small%2520fraction%2520of%2520the%2520full%250Anetwork%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12479v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20value-based%20deep%20reinforcement%20learning%2C%20a%20pruned%20network%20is%20a%20good%0A%20%20network&entry.906535625=Johan%20Obando-Ceron%20and%20Aaron%20Courville%20and%20Pablo%20Samuel%20Castro&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%20deep%20reinforcement%20learning%20agents%20have%20difficulty%0Ain%20effectively%20using%20their%20network%20parameters.%20We%20leverage%20prior%20insights%20into%0Athe%20advantages%20of%20sparse%20training%20techniques%20and%20demonstrate%20that%20gradual%0Amagnitude%20pruning%20enables%20value-based%20agents%20to%20maximize%20parameter%0Aeffectiveness.%20This%20results%20in%20networks%20that%20yield%20dramatic%20performance%0Aimprovements%20over%20traditional%20networks%2C%20using%20only%20a%20small%20fraction%20of%20the%20full%0Anetwork%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12479v3&entry.124074799=Read"},
{"title": "Essentially Sharp Estimates on the Entropy Regularization Error in\n  Discrete Discounted Markov Decision Processes", "author": "Johannes M\u00fcller and Semih Cayci", "abstract": "  We study the error introduced by entropy regularization of infinite-horizon\ndiscrete discounted Markov decision processes. We show that this error\ndecreases exponentially in the inverse regularization strength both in a\nweighted KL-divergence and in value with a problem-specific exponent. We\nprovide a lower bound matching our upper bound up to a polynomial factor. Our\nproof relies on the correspondence of the solutions of entropy-regularized\nMarkov decision processes with gradient flows of the unregularized reward with\nrespect to a Riemannian metric common in natural policy gradient methods.\nFurther, this correspondence allows us to identify the limit of the gradient\nflow as the generalized maximum entropy optimal policy, thereby characterizing\nthe implicit bias of the Kakade gradient flow which corresponds to a\ntime-continuous version of the natural policy gradient method. We use this to\nshow that for entropy-regularized natural policy gradient methods the overall\nerror decays exponentially in the square root of the number of iterations\nimproving existing sublinear guarantees.\n", "link": "http://arxiv.org/abs/2406.04163v2", "date": "2024-06-25", "relevancy": 2.1872, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4388}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.437}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Essentially%20Sharp%20Estimates%20on%20the%20Entropy%20Regularization%20Error%20in%0A%20%20Discrete%20Discounted%20Markov%20Decision%20Processes&body=Title%3A%20Essentially%20Sharp%20Estimates%20on%20the%20Entropy%20Regularization%20Error%20in%0A%20%20Discrete%20Discounted%20Markov%20Decision%20Processes%0AAuthor%3A%20Johannes%20M%C3%BCller%20and%20Semih%20Cayci%0AAbstract%3A%20%20%20We%20study%20the%20error%20introduced%20by%20entropy%20regularization%20of%20infinite-horizon%0Adiscrete%20discounted%20Markov%20decision%20processes.%20We%20show%20that%20this%20error%0Adecreases%20exponentially%20in%20the%20inverse%20regularization%20strength%20both%20in%20a%0Aweighted%20KL-divergence%20and%20in%20value%20with%20a%20problem-specific%20exponent.%20We%0Aprovide%20a%20lower%20bound%20matching%20our%20upper%20bound%20up%20to%20a%20polynomial%20factor.%20Our%0Aproof%20relies%20on%20the%20correspondence%20of%20the%20solutions%20of%20entropy-regularized%0AMarkov%20decision%20processes%20with%20gradient%20flows%20of%20the%20unregularized%20reward%20with%0Arespect%20to%20a%20Riemannian%20metric%20common%20in%20natural%20policy%20gradient%20methods.%0AFurther%2C%20this%20correspondence%20allows%20us%20to%20identify%20the%20limit%20of%20the%20gradient%0Aflow%20as%20the%20generalized%20maximum%20entropy%20optimal%20policy%2C%20thereby%20characterizing%0Athe%20implicit%20bias%20of%20the%20Kakade%20gradient%20flow%20which%20corresponds%20to%20a%0Atime-continuous%20version%20of%20the%20natural%20policy%20gradient%20method.%20We%20use%20this%20to%0Ashow%20that%20for%20entropy-regularized%20natural%20policy%20gradient%20methods%20the%20overall%0Aerror%20decays%20exponentially%20in%20the%20square%20root%20of%20the%20number%20of%20iterations%0Aimproving%20existing%20sublinear%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04163v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEssentially%2520Sharp%2520Estimates%2520on%2520the%2520Entropy%2520Regularization%2520Error%2520in%250A%2520%2520Discrete%2520Discounted%2520Markov%2520Decision%2520Processes%26entry.906535625%3DJohannes%2520M%25C3%25BCller%2520and%2520Semih%2520Cayci%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520error%2520introduced%2520by%2520entropy%2520regularization%2520of%2520infinite-horizon%250Adiscrete%2520discounted%2520Markov%2520decision%2520processes.%2520We%2520show%2520that%2520this%2520error%250Adecreases%2520exponentially%2520in%2520the%2520inverse%2520regularization%2520strength%2520both%2520in%2520a%250Aweighted%2520KL-divergence%2520and%2520in%2520value%2520with%2520a%2520problem-specific%2520exponent.%2520We%250Aprovide%2520a%2520lower%2520bound%2520matching%2520our%2520upper%2520bound%2520up%2520to%2520a%2520polynomial%2520factor.%2520Our%250Aproof%2520relies%2520on%2520the%2520correspondence%2520of%2520the%2520solutions%2520of%2520entropy-regularized%250AMarkov%2520decision%2520processes%2520with%2520gradient%2520flows%2520of%2520the%2520unregularized%2520reward%2520with%250Arespect%2520to%2520a%2520Riemannian%2520metric%2520common%2520in%2520natural%2520policy%2520gradient%2520methods.%250AFurther%252C%2520this%2520correspondence%2520allows%2520us%2520to%2520identify%2520the%2520limit%2520of%2520the%2520gradient%250Aflow%2520as%2520the%2520generalized%2520maximum%2520entropy%2520optimal%2520policy%252C%2520thereby%2520characterizing%250Athe%2520implicit%2520bias%2520of%2520the%2520Kakade%2520gradient%2520flow%2520which%2520corresponds%2520to%2520a%250Atime-continuous%2520version%2520of%2520the%2520natural%2520policy%2520gradient%2520method.%2520We%2520use%2520this%2520to%250Ashow%2520that%2520for%2520entropy-regularized%2520natural%2520policy%2520gradient%2520methods%2520the%2520overall%250Aerror%2520decays%2520exponentially%2520in%2520the%2520square%2520root%2520of%2520the%2520number%2520of%2520iterations%250Aimproving%2520existing%2520sublinear%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04163v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Essentially%20Sharp%20Estimates%20on%20the%20Entropy%20Regularization%20Error%20in%0A%20%20Discrete%20Discounted%20Markov%20Decision%20Processes&entry.906535625=Johannes%20M%C3%BCller%20and%20Semih%20Cayci&entry.1292438233=%20%20We%20study%20the%20error%20introduced%20by%20entropy%20regularization%20of%20infinite-horizon%0Adiscrete%20discounted%20Markov%20decision%20processes.%20We%20show%20that%20this%20error%0Adecreases%20exponentially%20in%20the%20inverse%20regularization%20strength%20both%20in%20a%0Aweighted%20KL-divergence%20and%20in%20value%20with%20a%20problem-specific%20exponent.%20We%0Aprovide%20a%20lower%20bound%20matching%20our%20upper%20bound%20up%20to%20a%20polynomial%20factor.%20Our%0Aproof%20relies%20on%20the%20correspondence%20of%20the%20solutions%20of%20entropy-regularized%0AMarkov%20decision%20processes%20with%20gradient%20flows%20of%20the%20unregularized%20reward%20with%0Arespect%20to%20a%20Riemannian%20metric%20common%20in%20natural%20policy%20gradient%20methods.%0AFurther%2C%20this%20correspondence%20allows%20us%20to%20identify%20the%20limit%20of%20the%20gradient%0Aflow%20as%20the%20generalized%20maximum%20entropy%20optimal%20policy%2C%20thereby%20characterizing%0Athe%20implicit%20bias%20of%20the%20Kakade%20gradient%20flow%20which%20corresponds%20to%20a%0Atime-continuous%20version%20of%20the%20natural%20policy%20gradient%20method.%20We%20use%20this%20to%0Ashow%20that%20for%20entropy-regularized%20natural%20policy%20gradient%20methods%20the%20overall%0Aerror%20decays%20exponentially%20in%20the%20square%20root%20of%20the%20number%20of%20iterations%0Aimproving%20existing%20sublinear%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04163v2&entry.124074799=Read"},
{"title": "Fast gradient-free activation maximization for neurons in spiking neural\n  networks", "author": "Nikita Pospelov and Andrei Chertkov and Maxim Beketov and Ivan Oseledets and Konstantin Anokhin", "abstract": "  Elements of neural networks, both biological and artificial, can be described\nby their selectivity for specific cognitive features. Understanding these\nfeatures is important for understanding the inner workings of neural networks.\nFor a living system, such as a neuron, whose response to a stimulus is unknown\nand not differentiable, the only way to reveal these features is through a\nfeedback loop that exposes it to a large set of different stimuli. The\nproperties of these stimuli should be varied iteratively in order to maximize\nthe neuronal response. To utilize this feedback loop for a biological neural\nnetwork, it is important to run it quickly and efficiently in order to reach\nthe stimuli that maximizes certain neurons' activation with the least number of\niterations possible. Here we present a framework with an efficient design for\nsuch a loop. We successfully tested it on an artificial spiking neural network\n(SNN), which is a model that simulates the asynchronous spiking activity of\nneurons in living brains. Our optimization method for activation maximization\nis based on the low-rank Tensor Train decomposition of the discrete activation\nfunction. The optimization space is the latent parameter space of images\ngenerated by SN-GAN or VQ-VAE generative models. To our knowledge, this is the\nfirst time that effective AM has been applied to SNNs. We track changes in the\noptimal stimuli for artificial neurons during training and show that highly\nselective neurons can form already in the early epochs of training and in the\nearly layers of a convolutional spiking network. This formation of refined\noptimal stimuli is associated with an increase in classification accuracy. Some\nneurons, especially in the deeper layers, may gradually change the concepts\nthey are selective for during learning, potentially explaining their importance\nfor model performance.\n", "link": "http://arxiv.org/abs/2401.10748v2", "date": "2024-06-25", "relevancy": 2.1792, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5892}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5198}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20gradient-free%20activation%20maximization%20for%20neurons%20in%20spiking%20neural%0A%20%20networks&body=Title%3A%20Fast%20gradient-free%20activation%20maximization%20for%20neurons%20in%20spiking%20neural%0A%20%20networks%0AAuthor%3A%20Nikita%20Pospelov%20and%20Andrei%20Chertkov%20and%20Maxim%20Beketov%20and%20Ivan%20Oseledets%20and%20Konstantin%20Anokhin%0AAbstract%3A%20%20%20Elements%20of%20neural%20networks%2C%20both%20biological%20and%20artificial%2C%20can%20be%20described%0Aby%20their%20selectivity%20for%20specific%20cognitive%20features.%20Understanding%20these%0Afeatures%20is%20important%20for%20understanding%20the%20inner%20workings%20of%20neural%20networks.%0AFor%20a%20living%20system%2C%20such%20as%20a%20neuron%2C%20whose%20response%20to%20a%20stimulus%20is%20unknown%0Aand%20not%20differentiable%2C%20the%20only%20way%20to%20reveal%20these%20features%20is%20through%20a%0Afeedback%20loop%20that%20exposes%20it%20to%20a%20large%20set%20of%20different%20stimuli.%20The%0Aproperties%20of%20these%20stimuli%20should%20be%20varied%20iteratively%20in%20order%20to%20maximize%0Athe%20neuronal%20response.%20To%20utilize%20this%20feedback%20loop%20for%20a%20biological%20neural%0Anetwork%2C%20it%20is%20important%20to%20run%20it%20quickly%20and%20efficiently%20in%20order%20to%20reach%0Athe%20stimuli%20that%20maximizes%20certain%20neurons%27%20activation%20with%20the%20least%20number%20of%0Aiterations%20possible.%20Here%20we%20present%20a%20framework%20with%20an%20efficient%20design%20for%0Asuch%20a%20loop.%20We%20successfully%20tested%20it%20on%20an%20artificial%20spiking%20neural%20network%0A%28SNN%29%2C%20which%20is%20a%20model%20that%20simulates%20the%20asynchronous%20spiking%20activity%20of%0Aneurons%20in%20living%20brains.%20Our%20optimization%20method%20for%20activation%20maximization%0Ais%20based%20on%20the%20low-rank%20Tensor%20Train%20decomposition%20of%20the%20discrete%20activation%0Afunction.%20The%20optimization%20space%20is%20the%20latent%20parameter%20space%20of%20images%0Agenerated%20by%20SN-GAN%20or%20VQ-VAE%20generative%20models.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20time%20that%20effective%20AM%20has%20been%20applied%20to%20SNNs.%20We%20track%20changes%20in%20the%0Aoptimal%20stimuli%20for%20artificial%20neurons%20during%20training%20and%20show%20that%20highly%0Aselective%20neurons%20can%20form%20already%20in%20the%20early%20epochs%20of%20training%20and%20in%20the%0Aearly%20layers%20of%20a%20convolutional%20spiking%20network.%20This%20formation%20of%20refined%0Aoptimal%20stimuli%20is%20associated%20with%20an%20increase%20in%20classification%20accuracy.%20Some%0Aneurons%2C%20especially%20in%20the%20deeper%20layers%2C%20may%20gradually%20change%20the%20concepts%0Athey%20are%20selective%20for%20during%20learning%2C%20potentially%20explaining%20their%20importance%0Afor%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10748v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520gradient-free%2520activation%2520maximization%2520for%2520neurons%2520in%2520spiking%2520neural%250A%2520%2520networks%26entry.906535625%3DNikita%2520Pospelov%2520and%2520Andrei%2520Chertkov%2520and%2520Maxim%2520Beketov%2520and%2520Ivan%2520Oseledets%2520and%2520Konstantin%2520Anokhin%26entry.1292438233%3D%2520%2520Elements%2520of%2520neural%2520networks%252C%2520both%2520biological%2520and%2520artificial%252C%2520can%2520be%2520described%250Aby%2520their%2520selectivity%2520for%2520specific%2520cognitive%2520features.%2520Understanding%2520these%250Afeatures%2520is%2520important%2520for%2520understanding%2520the%2520inner%2520workings%2520of%2520neural%2520networks.%250AFor%2520a%2520living%2520system%252C%2520such%2520as%2520a%2520neuron%252C%2520whose%2520response%2520to%2520a%2520stimulus%2520is%2520unknown%250Aand%2520not%2520differentiable%252C%2520the%2520only%2520way%2520to%2520reveal%2520these%2520features%2520is%2520through%2520a%250Afeedback%2520loop%2520that%2520exposes%2520it%2520to%2520a%2520large%2520set%2520of%2520different%2520stimuli.%2520The%250Aproperties%2520of%2520these%2520stimuli%2520should%2520be%2520varied%2520iteratively%2520in%2520order%2520to%2520maximize%250Athe%2520neuronal%2520response.%2520To%2520utilize%2520this%2520feedback%2520loop%2520for%2520a%2520biological%2520neural%250Anetwork%252C%2520it%2520is%2520important%2520to%2520run%2520it%2520quickly%2520and%2520efficiently%2520in%2520order%2520to%2520reach%250Athe%2520stimuli%2520that%2520maximizes%2520certain%2520neurons%2527%2520activation%2520with%2520the%2520least%2520number%2520of%250Aiterations%2520possible.%2520Here%2520we%2520present%2520a%2520framework%2520with%2520an%2520efficient%2520design%2520for%250Asuch%2520a%2520loop.%2520We%2520successfully%2520tested%2520it%2520on%2520an%2520artificial%2520spiking%2520neural%2520network%250A%2528SNN%2529%252C%2520which%2520is%2520a%2520model%2520that%2520simulates%2520the%2520asynchronous%2520spiking%2520activity%2520of%250Aneurons%2520in%2520living%2520brains.%2520Our%2520optimization%2520method%2520for%2520activation%2520maximization%250Ais%2520based%2520on%2520the%2520low-rank%2520Tensor%2520Train%2520decomposition%2520of%2520the%2520discrete%2520activation%250Afunction.%2520The%2520optimization%2520space%2520is%2520the%2520latent%2520parameter%2520space%2520of%2520images%250Agenerated%2520by%2520SN-GAN%2520or%2520VQ-VAE%2520generative%2520models.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520time%2520that%2520effective%2520AM%2520has%2520been%2520applied%2520to%2520SNNs.%2520We%2520track%2520changes%2520in%2520the%250Aoptimal%2520stimuli%2520for%2520artificial%2520neurons%2520during%2520training%2520and%2520show%2520that%2520highly%250Aselective%2520neurons%2520can%2520form%2520already%2520in%2520the%2520early%2520epochs%2520of%2520training%2520and%2520in%2520the%250Aearly%2520layers%2520of%2520a%2520convolutional%2520spiking%2520network.%2520This%2520formation%2520of%2520refined%250Aoptimal%2520stimuli%2520is%2520associated%2520with%2520an%2520increase%2520in%2520classification%2520accuracy.%2520Some%250Aneurons%252C%2520especially%2520in%2520the%2520deeper%2520layers%252C%2520may%2520gradually%2520change%2520the%2520concepts%250Athey%2520are%2520selective%2520for%2520during%2520learning%252C%2520potentially%2520explaining%2520their%2520importance%250Afor%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10748v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20gradient-free%20activation%20maximization%20for%20neurons%20in%20spiking%20neural%0A%20%20networks&entry.906535625=Nikita%20Pospelov%20and%20Andrei%20Chertkov%20and%20Maxim%20Beketov%20and%20Ivan%20Oseledets%20and%20Konstantin%20Anokhin&entry.1292438233=%20%20Elements%20of%20neural%20networks%2C%20both%20biological%20and%20artificial%2C%20can%20be%20described%0Aby%20their%20selectivity%20for%20specific%20cognitive%20features.%20Understanding%20these%0Afeatures%20is%20important%20for%20understanding%20the%20inner%20workings%20of%20neural%20networks.%0AFor%20a%20living%20system%2C%20such%20as%20a%20neuron%2C%20whose%20response%20to%20a%20stimulus%20is%20unknown%0Aand%20not%20differentiable%2C%20the%20only%20way%20to%20reveal%20these%20features%20is%20through%20a%0Afeedback%20loop%20that%20exposes%20it%20to%20a%20large%20set%20of%20different%20stimuli.%20The%0Aproperties%20of%20these%20stimuli%20should%20be%20varied%20iteratively%20in%20order%20to%20maximize%0Athe%20neuronal%20response.%20To%20utilize%20this%20feedback%20loop%20for%20a%20biological%20neural%0Anetwork%2C%20it%20is%20important%20to%20run%20it%20quickly%20and%20efficiently%20in%20order%20to%20reach%0Athe%20stimuli%20that%20maximizes%20certain%20neurons%27%20activation%20with%20the%20least%20number%20of%0Aiterations%20possible.%20Here%20we%20present%20a%20framework%20with%20an%20efficient%20design%20for%0Asuch%20a%20loop.%20We%20successfully%20tested%20it%20on%20an%20artificial%20spiking%20neural%20network%0A%28SNN%29%2C%20which%20is%20a%20model%20that%20simulates%20the%20asynchronous%20spiking%20activity%20of%0Aneurons%20in%20living%20brains.%20Our%20optimization%20method%20for%20activation%20maximization%0Ais%20based%20on%20the%20low-rank%20Tensor%20Train%20decomposition%20of%20the%20discrete%20activation%0Afunction.%20The%20optimization%20space%20is%20the%20latent%20parameter%20space%20of%20images%0Agenerated%20by%20SN-GAN%20or%20VQ-VAE%20generative%20models.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20time%20that%20effective%20AM%20has%20been%20applied%20to%20SNNs.%20We%20track%20changes%20in%20the%0Aoptimal%20stimuli%20for%20artificial%20neurons%20during%20training%20and%20show%20that%20highly%0Aselective%20neurons%20can%20form%20already%20in%20the%20early%20epochs%20of%20training%20and%20in%20the%0Aearly%20layers%20of%20a%20convolutional%20spiking%20network.%20This%20formation%20of%20refined%0Aoptimal%20stimuli%20is%20associated%20with%20an%20increase%20in%20classification%20accuracy.%20Some%0Aneurons%2C%20especially%20in%20the%20deeper%20layers%2C%20may%20gradually%20change%20the%20concepts%0Athey%20are%20selective%20for%20during%20learning%2C%20potentially%20explaining%20their%20importance%0Afor%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10748v2&entry.124074799=Read"},
{"title": "SKD-TSTSAN: Three-Stream Temporal-Shift Attention Network Based on\n  Self-Knowledge Distillation for Micro-Expression Recognition", "author": "Guanghao Zhu and Lin Liu and Yuhao Hu and Haixin Sun and Fang Liu and Xiaohui Du and Ruqian Hao and Juanxiu Liu and Yong Liu and Hao Deng and Jing Zhang", "abstract": "  Micro-expressions (MEs) are subtle facial movements that occur spontaneously\nwhen people try to conceal the real emotions. Micro-expression recognition\n(MER) is crucial in many fields, including criminal analysis and psychotherapy.\nHowever, MER is challenging since MEs have low intensity and ME datasets are\nsmall in size. To this end, a three-stream temporal-shift attention network\nbased on self-knowledge distillation (SKD-TSTSAN) is proposed in this paper.\nFirstly, to address the low intensity of ME muscle movements, we utilize\nlearning-based motion magnification modules to enhance the intensity of ME\nmuscle movements. Secondly, we employ efficient channel attention (ECA) modules\nin the local-spatial stream to make the network focus on facial regions that\nare highly relevant to MEs. In addition, temporal shift modules (TSMs) are used\nin the dynamic-temporal stream, which enables temporal modeling with no\nadditional parameters by mixing ME motion information from two different\ntemporal domains. Furthermore, we introduce self-knowledge distillation (SKD)\ninto the MER task by introducing auxiliary classifiers and using the deepest\nsection of the network for supervision, encouraging all blocks to fully explore\nthe features of the training set. Finally, extensive experiments are conducted\non four ME datasets: CASME II, SAMM, MMEW, and CAS(ME)3. The experimental\nresults demonstrate that our SKD-TSTSAN outperforms other existing methods and\nachieves new state-of-the-art performance. Our code will be available at\nhttps://github.com/GuanghaoZhu663/SKD-TSTSAN.\n", "link": "http://arxiv.org/abs/2406.17538v1", "date": "2024-06-25", "relevancy": 2.1774, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5473}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5426}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKD-TSTSAN%3A%20Three-Stream%20Temporal-Shift%20Attention%20Network%20Based%20on%0A%20%20Self-Knowledge%20Distillation%20for%20Micro-Expression%20Recognition&body=Title%3A%20SKD-TSTSAN%3A%20Three-Stream%20Temporal-Shift%20Attention%20Network%20Based%20on%0A%20%20Self-Knowledge%20Distillation%20for%20Micro-Expression%20Recognition%0AAuthor%3A%20Guanghao%20Zhu%20and%20Lin%20Liu%20and%20Yuhao%20Hu%20and%20Haixin%20Sun%20and%20Fang%20Liu%20and%20Xiaohui%20Du%20and%20Ruqian%20Hao%20and%20Juanxiu%20Liu%20and%20Yong%20Liu%20and%20Hao%20Deng%20and%20Jing%20Zhang%0AAbstract%3A%20%20%20Micro-expressions%20%28MEs%29%20are%20subtle%20facial%20movements%20that%20occur%20spontaneously%0Awhen%20people%20try%20to%20conceal%20the%20real%20emotions.%20Micro-expression%20recognition%0A%28MER%29%20is%20crucial%20in%20many%20fields%2C%20including%20criminal%20analysis%20and%20psychotherapy.%0AHowever%2C%20MER%20is%20challenging%20since%20MEs%20have%20low%20intensity%20and%20ME%20datasets%20are%0Asmall%20in%20size.%20To%20this%20end%2C%20a%20three-stream%20temporal-shift%20attention%20network%0Abased%20on%20self-knowledge%20distillation%20%28SKD-TSTSAN%29%20is%20proposed%20in%20this%20paper.%0AFirstly%2C%20to%20address%20the%20low%20intensity%20of%20ME%20muscle%20movements%2C%20we%20utilize%0Alearning-based%20motion%20magnification%20modules%20to%20enhance%20the%20intensity%20of%20ME%0Amuscle%20movements.%20Secondly%2C%20we%20employ%20efficient%20channel%20attention%20%28ECA%29%20modules%0Ain%20the%20local-spatial%20stream%20to%20make%20the%20network%20focus%20on%20facial%20regions%20that%0Aare%20highly%20relevant%20to%20MEs.%20In%20addition%2C%20temporal%20shift%20modules%20%28TSMs%29%20are%20used%0Ain%20the%20dynamic-temporal%20stream%2C%20which%20enables%20temporal%20modeling%20with%20no%0Aadditional%20parameters%20by%20mixing%20ME%20motion%20information%20from%20two%20different%0Atemporal%20domains.%20Furthermore%2C%20we%20introduce%20self-knowledge%20distillation%20%28SKD%29%0Ainto%20the%20MER%20task%20by%20introducing%20auxiliary%20classifiers%20and%20using%20the%20deepest%0Asection%20of%20the%20network%20for%20supervision%2C%20encouraging%20all%20blocks%20to%20fully%20explore%0Athe%20features%20of%20the%20training%20set.%20Finally%2C%20extensive%20experiments%20are%20conducted%0Aon%20four%20ME%20datasets%3A%20CASME%20II%2C%20SAMM%2C%20MMEW%2C%20and%20CAS%28ME%293.%20The%20experimental%0Aresults%20demonstrate%20that%20our%20SKD-TSTSAN%20outperforms%20other%20existing%20methods%20and%0Aachieves%20new%20state-of-the-art%20performance.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/GuanghaoZhu663/SKD-TSTSAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKD-TSTSAN%253A%2520Three-Stream%2520Temporal-Shift%2520Attention%2520Network%2520Based%2520on%250A%2520%2520Self-Knowledge%2520Distillation%2520for%2520Micro-Expression%2520Recognition%26entry.906535625%3DGuanghao%2520Zhu%2520and%2520Lin%2520Liu%2520and%2520Yuhao%2520Hu%2520and%2520Haixin%2520Sun%2520and%2520Fang%2520Liu%2520and%2520Xiaohui%2520Du%2520and%2520Ruqian%2520Hao%2520and%2520Juanxiu%2520Liu%2520and%2520Yong%2520Liu%2520and%2520Hao%2520Deng%2520and%2520Jing%2520Zhang%26entry.1292438233%3D%2520%2520Micro-expressions%2520%2528MEs%2529%2520are%2520subtle%2520facial%2520movements%2520that%2520occur%2520spontaneously%250Awhen%2520people%2520try%2520to%2520conceal%2520the%2520real%2520emotions.%2520Micro-expression%2520recognition%250A%2528MER%2529%2520is%2520crucial%2520in%2520many%2520fields%252C%2520including%2520criminal%2520analysis%2520and%2520psychotherapy.%250AHowever%252C%2520MER%2520is%2520challenging%2520since%2520MEs%2520have%2520low%2520intensity%2520and%2520ME%2520datasets%2520are%250Asmall%2520in%2520size.%2520To%2520this%2520end%252C%2520a%2520three-stream%2520temporal-shift%2520attention%2520network%250Abased%2520on%2520self-knowledge%2520distillation%2520%2528SKD-TSTSAN%2529%2520is%2520proposed%2520in%2520this%2520paper.%250AFirstly%252C%2520to%2520address%2520the%2520low%2520intensity%2520of%2520ME%2520muscle%2520movements%252C%2520we%2520utilize%250Alearning-based%2520motion%2520magnification%2520modules%2520to%2520enhance%2520the%2520intensity%2520of%2520ME%250Amuscle%2520movements.%2520Secondly%252C%2520we%2520employ%2520efficient%2520channel%2520attention%2520%2528ECA%2529%2520modules%250Ain%2520the%2520local-spatial%2520stream%2520to%2520make%2520the%2520network%2520focus%2520on%2520facial%2520regions%2520that%250Aare%2520highly%2520relevant%2520to%2520MEs.%2520In%2520addition%252C%2520temporal%2520shift%2520modules%2520%2528TSMs%2529%2520are%2520used%250Ain%2520the%2520dynamic-temporal%2520stream%252C%2520which%2520enables%2520temporal%2520modeling%2520with%2520no%250Aadditional%2520parameters%2520by%2520mixing%2520ME%2520motion%2520information%2520from%2520two%2520different%250Atemporal%2520domains.%2520Furthermore%252C%2520we%2520introduce%2520self-knowledge%2520distillation%2520%2528SKD%2529%250Ainto%2520the%2520MER%2520task%2520by%2520introducing%2520auxiliary%2520classifiers%2520and%2520using%2520the%2520deepest%250Asection%2520of%2520the%2520network%2520for%2520supervision%252C%2520encouraging%2520all%2520blocks%2520to%2520fully%2520explore%250Athe%2520features%2520of%2520the%2520training%2520set.%2520Finally%252C%2520extensive%2520experiments%2520are%2520conducted%250Aon%2520four%2520ME%2520datasets%253A%2520CASME%2520II%252C%2520SAMM%252C%2520MMEW%252C%2520and%2520CAS%2528ME%25293.%2520The%2520experimental%250Aresults%2520demonstrate%2520that%2520our%2520SKD-TSTSAN%2520outperforms%2520other%2520existing%2520methods%2520and%250Aachieves%2520new%2520state-of-the-art%2520performance.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/GuanghaoZhu663/SKD-TSTSAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKD-TSTSAN%3A%20Three-Stream%20Temporal-Shift%20Attention%20Network%20Based%20on%0A%20%20Self-Knowledge%20Distillation%20for%20Micro-Expression%20Recognition&entry.906535625=Guanghao%20Zhu%20and%20Lin%20Liu%20and%20Yuhao%20Hu%20and%20Haixin%20Sun%20and%20Fang%20Liu%20and%20Xiaohui%20Du%20and%20Ruqian%20Hao%20and%20Juanxiu%20Liu%20and%20Yong%20Liu%20and%20Hao%20Deng%20and%20Jing%20Zhang&entry.1292438233=%20%20Micro-expressions%20%28MEs%29%20are%20subtle%20facial%20movements%20that%20occur%20spontaneously%0Awhen%20people%20try%20to%20conceal%20the%20real%20emotions.%20Micro-expression%20recognition%0A%28MER%29%20is%20crucial%20in%20many%20fields%2C%20including%20criminal%20analysis%20and%20psychotherapy.%0AHowever%2C%20MER%20is%20challenging%20since%20MEs%20have%20low%20intensity%20and%20ME%20datasets%20are%0Asmall%20in%20size.%20To%20this%20end%2C%20a%20three-stream%20temporal-shift%20attention%20network%0Abased%20on%20self-knowledge%20distillation%20%28SKD-TSTSAN%29%20is%20proposed%20in%20this%20paper.%0AFirstly%2C%20to%20address%20the%20low%20intensity%20of%20ME%20muscle%20movements%2C%20we%20utilize%0Alearning-based%20motion%20magnification%20modules%20to%20enhance%20the%20intensity%20of%20ME%0Amuscle%20movements.%20Secondly%2C%20we%20employ%20efficient%20channel%20attention%20%28ECA%29%20modules%0Ain%20the%20local-spatial%20stream%20to%20make%20the%20network%20focus%20on%20facial%20regions%20that%0Aare%20highly%20relevant%20to%20MEs.%20In%20addition%2C%20temporal%20shift%20modules%20%28TSMs%29%20are%20used%0Ain%20the%20dynamic-temporal%20stream%2C%20which%20enables%20temporal%20modeling%20with%20no%0Aadditional%20parameters%20by%20mixing%20ME%20motion%20information%20from%20two%20different%0Atemporal%20domains.%20Furthermore%2C%20we%20introduce%20self-knowledge%20distillation%20%28SKD%29%0Ainto%20the%20MER%20task%20by%20introducing%20auxiliary%20classifiers%20and%20using%20the%20deepest%0Asection%20of%20the%20network%20for%20supervision%2C%20encouraging%20all%20blocks%20to%20fully%20explore%0Athe%20features%20of%20the%20training%20set.%20Finally%2C%20extensive%20experiments%20are%20conducted%0Aon%20four%20ME%20datasets%3A%20CASME%20II%2C%20SAMM%2C%20MMEW%2C%20and%20CAS%28ME%293.%20The%20experimental%0Aresults%20demonstrate%20that%20our%20SKD-TSTSAN%20outperforms%20other%20existing%20methods%20and%0Aachieves%20new%20state-of-the-art%20performance.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/GuanghaoZhu663/SKD-TSTSAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17538v1&entry.124074799=Read"},
{"title": "Pseudo Labelling for Enhanced Masked Autoencoders", "author": "Srinivasa Rao Nandam and Sara Atito and Zhenhua Feng and Josef Kittler and Muhammad Awais", "abstract": "  Masked Image Modeling (MIM)-based models, such as SdAE, CAE, GreenMIM, and\nMixAE, have explored different strategies to enhance the performance of Masked\nAutoencoders (MAE) by modifying prediction, loss functions, or incorporating\nadditional architectural components. In this paper, we propose an enhanced\napproach that boosts MAE performance by integrating pseudo labelling for both\nclass and data tokens, alongside replacing the traditional pixel-level\nreconstruction with token-level reconstruction. This strategy uses cluster\nassignments as pseudo labels to promote instance-level discrimination within\nthe network, while token reconstruction requires generation of discrete tokens\nencapturing local context. The targets for pseudo labelling and reconstruction\nneeds to be generated by a teacher network. To disentangle the generation of\ntarget pseudo labels and the reconstruction of the token features, we decouple\nthe teacher into two distinct models, where one serves as a labelling teacher\nand the other as a reconstruction teacher. This separation proves empirically\nsuperior to a single teacher, while having negligible impact on throughput and\nmemory consumption. Incorporating pseudo-labelling as an auxiliary task has\ndemonstrated notable improvements in ImageNet-1K and other downstream tasks,\nincluding classification, semantic segmentation, and detection.\n", "link": "http://arxiv.org/abs/2406.17450v1", "date": "2024-06-25", "relevancy": 2.1743, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5626}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo%20Labelling%20for%20Enhanced%20Masked%20Autoencoders&body=Title%3A%20Pseudo%20Labelling%20for%20Enhanced%20Masked%20Autoencoders%0AAuthor%3A%20Srinivasa%20Rao%20Nandam%20and%20Sara%20Atito%20and%20Zhenhua%20Feng%20and%20Josef%20Kittler%20and%20Muhammad%20Awais%0AAbstract%3A%20%20%20Masked%20Image%20Modeling%20%28MIM%29-based%20models%2C%20such%20as%20SdAE%2C%20CAE%2C%20GreenMIM%2C%20and%0AMixAE%2C%20have%20explored%20different%20strategies%20to%20enhance%20the%20performance%20of%20Masked%0AAutoencoders%20%28MAE%29%20by%20modifying%20prediction%2C%20loss%20functions%2C%20or%20incorporating%0Aadditional%20architectural%20components.%20In%20this%20paper%2C%20we%20propose%20an%20enhanced%0Aapproach%20that%20boosts%20MAE%20performance%20by%20integrating%20pseudo%20labelling%20for%20both%0Aclass%20and%20data%20tokens%2C%20alongside%20replacing%20the%20traditional%20pixel-level%0Areconstruction%20with%20token-level%20reconstruction.%20This%20strategy%20uses%20cluster%0Aassignments%20as%20pseudo%20labels%20to%20promote%20instance-level%20discrimination%20within%0Athe%20network%2C%20while%20token%20reconstruction%20requires%20generation%20of%20discrete%20tokens%0Aencapturing%20local%20context.%20The%20targets%20for%20pseudo%20labelling%20and%20reconstruction%0Aneeds%20to%20be%20generated%20by%20a%20teacher%20network.%20To%20disentangle%20the%20generation%20of%0Atarget%20pseudo%20labels%20and%20the%20reconstruction%20of%20the%20token%20features%2C%20we%20decouple%0Athe%20teacher%20into%20two%20distinct%20models%2C%20where%20one%20serves%20as%20a%20labelling%20teacher%0Aand%20the%20other%20as%20a%20reconstruction%20teacher.%20This%20separation%20proves%20empirically%0Asuperior%20to%20a%20single%20teacher%2C%20while%20having%20negligible%20impact%20on%20throughput%20and%0Amemory%20consumption.%20Incorporating%20pseudo-labelling%20as%20an%20auxiliary%20task%20has%0Ademonstrated%20notable%20improvements%20in%20ImageNet-1K%20and%20other%20downstream%20tasks%2C%0Aincluding%20classification%2C%20semantic%20segmentation%2C%20and%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo%2520Labelling%2520for%2520Enhanced%2520Masked%2520Autoencoders%26entry.906535625%3DSrinivasa%2520Rao%2520Nandam%2520and%2520Sara%2520Atito%2520and%2520Zhenhua%2520Feng%2520and%2520Josef%2520Kittler%2520and%2520Muhammad%2520Awais%26entry.1292438233%3D%2520%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529-based%2520models%252C%2520such%2520as%2520SdAE%252C%2520CAE%252C%2520GreenMIM%252C%2520and%250AMixAE%252C%2520have%2520explored%2520different%2520strategies%2520to%2520enhance%2520the%2520performance%2520of%2520Masked%250AAutoencoders%2520%2528MAE%2529%2520by%2520modifying%2520prediction%252C%2520loss%2520functions%252C%2520or%2520incorporating%250Aadditional%2520architectural%2520components.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520enhanced%250Aapproach%2520that%2520boosts%2520MAE%2520performance%2520by%2520integrating%2520pseudo%2520labelling%2520for%2520both%250Aclass%2520and%2520data%2520tokens%252C%2520alongside%2520replacing%2520the%2520traditional%2520pixel-level%250Areconstruction%2520with%2520token-level%2520reconstruction.%2520This%2520strategy%2520uses%2520cluster%250Aassignments%2520as%2520pseudo%2520labels%2520to%2520promote%2520instance-level%2520discrimination%2520within%250Athe%2520network%252C%2520while%2520token%2520reconstruction%2520requires%2520generation%2520of%2520discrete%2520tokens%250Aencapturing%2520local%2520context.%2520The%2520targets%2520for%2520pseudo%2520labelling%2520and%2520reconstruction%250Aneeds%2520to%2520be%2520generated%2520by%2520a%2520teacher%2520network.%2520To%2520disentangle%2520the%2520generation%2520of%250Atarget%2520pseudo%2520labels%2520and%2520the%2520reconstruction%2520of%2520the%2520token%2520features%252C%2520we%2520decouple%250Athe%2520teacher%2520into%2520two%2520distinct%2520models%252C%2520where%2520one%2520serves%2520as%2520a%2520labelling%2520teacher%250Aand%2520the%2520other%2520as%2520a%2520reconstruction%2520teacher.%2520This%2520separation%2520proves%2520empirically%250Asuperior%2520to%2520a%2520single%2520teacher%252C%2520while%2520having%2520negligible%2520impact%2520on%2520throughput%2520and%250Amemory%2520consumption.%2520Incorporating%2520pseudo-labelling%2520as%2520an%2520auxiliary%2520task%2520has%250Ademonstrated%2520notable%2520improvements%2520in%2520ImageNet-1K%2520and%2520other%2520downstream%2520tasks%252C%250Aincluding%2520classification%252C%2520semantic%2520segmentation%252C%2520and%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo%20Labelling%20for%20Enhanced%20Masked%20Autoencoders&entry.906535625=Srinivasa%20Rao%20Nandam%20and%20Sara%20Atito%20and%20Zhenhua%20Feng%20and%20Josef%20Kittler%20and%20Muhammad%20Awais&entry.1292438233=%20%20Masked%20Image%20Modeling%20%28MIM%29-based%20models%2C%20such%20as%20SdAE%2C%20CAE%2C%20GreenMIM%2C%20and%0AMixAE%2C%20have%20explored%20different%20strategies%20to%20enhance%20the%20performance%20of%20Masked%0AAutoencoders%20%28MAE%29%20by%20modifying%20prediction%2C%20loss%20functions%2C%20or%20incorporating%0Aadditional%20architectural%20components.%20In%20this%20paper%2C%20we%20propose%20an%20enhanced%0Aapproach%20that%20boosts%20MAE%20performance%20by%20integrating%20pseudo%20labelling%20for%20both%0Aclass%20and%20data%20tokens%2C%20alongside%20replacing%20the%20traditional%20pixel-level%0Areconstruction%20with%20token-level%20reconstruction.%20This%20strategy%20uses%20cluster%0Aassignments%20as%20pseudo%20labels%20to%20promote%20instance-level%20discrimination%20within%0Athe%20network%2C%20while%20token%20reconstruction%20requires%20generation%20of%20discrete%20tokens%0Aencapturing%20local%20context.%20The%20targets%20for%20pseudo%20labelling%20and%20reconstruction%0Aneeds%20to%20be%20generated%20by%20a%20teacher%20network.%20To%20disentangle%20the%20generation%20of%0Atarget%20pseudo%20labels%20and%20the%20reconstruction%20of%20the%20token%20features%2C%20we%20decouple%0Athe%20teacher%20into%20two%20distinct%20models%2C%20where%20one%20serves%20as%20a%20labelling%20teacher%0Aand%20the%20other%20as%20a%20reconstruction%20teacher.%20This%20separation%20proves%20empirically%0Asuperior%20to%20a%20single%20teacher%2C%20while%20having%20negligible%20impact%20on%20throughput%20and%0Amemory%20consumption.%20Incorporating%20pseudo-labelling%20as%20an%20auxiliary%20task%20has%0Ademonstrated%20notable%20improvements%20in%20ImageNet-1K%20and%20other%20downstream%20tasks%2C%0Aincluding%20classification%2C%20semantic%20segmentation%2C%20and%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17450v1&entry.124074799=Read"},
{"title": "TRIP: Trainable Region-of-Interest Prediction for Hardware-Efficient\n  Neuromorphic Processing on Event-based Vision", "author": "Cina Arjmand and Yingfu Xu and Kevin Shidqi and Alexandra F. Dobrita and Kanishkan Vadivel and Paul Detterer and Manolis Sifalakis and Amirreza Yousefzadeh and Guangzhi Tang", "abstract": "  Neuromorphic processors are well-suited for efficiently handling sparse\nevents from event-based cameras. However, they face significant challenges in\nthe growth of computing demand and hardware costs as the input resolution\nincreases. This paper proposes the Trainable Region-of-Interest Prediction\n(TRIP), the first hardware-efficient hard attention framework for event-based\nvision processing on a neuromorphic processor. Our TRIP framework actively\nproduces low-resolution Region-of-Interest (ROIs) for efficient and accurate\nclassification. The framework exploits sparse events' inherent low information\ndensity to reduce the overhead of ROI prediction. We introduced extensive\nhardware-aware optimizations for TRIP and implemented the hardware-optimized\nalgorithm on the SENECA neuromorphic processor. We utilized multiple\nevent-based classification datasets for evaluation. Our approach achieves\nstate-of-the-art accuracies in all datasets and produces reasonable ROIs with\nvarying locations and sizes. On the DvsGesture dataset, our solution requires\n46x less computation than the state-of-the-art while achieving higher accuracy.\nFurthermore, TRIP enables more than 2x latency and energy improvements on the\nSENECA neuromorphic processor compared to the conventional solution.\n", "link": "http://arxiv.org/abs/2406.17483v1", "date": "2024-06-25", "relevancy": 2.1723, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5677}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5459}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRIP%3A%20Trainable%20Region-of-Interest%20Prediction%20for%20Hardware-Efficient%0A%20%20Neuromorphic%20Processing%20on%20Event-based%20Vision&body=Title%3A%20TRIP%3A%20Trainable%20Region-of-Interest%20Prediction%20for%20Hardware-Efficient%0A%20%20Neuromorphic%20Processing%20on%20Event-based%20Vision%0AAuthor%3A%20Cina%20Arjmand%20and%20Yingfu%20Xu%20and%20Kevin%20Shidqi%20and%20Alexandra%20F.%20Dobrita%20and%20Kanishkan%20Vadivel%20and%20Paul%20Detterer%20and%20Manolis%20Sifalakis%20and%20Amirreza%20Yousefzadeh%20and%20Guangzhi%20Tang%0AAbstract%3A%20%20%20Neuromorphic%20processors%20are%20well-suited%20for%20efficiently%20handling%20sparse%0Aevents%20from%20event-based%20cameras.%20However%2C%20they%20face%20significant%20challenges%20in%0Athe%20growth%20of%20computing%20demand%20and%20hardware%20costs%20as%20the%20input%20resolution%0Aincreases.%20This%20paper%20proposes%20the%20Trainable%20Region-of-Interest%20Prediction%0A%28TRIP%29%2C%20the%20first%20hardware-efficient%20hard%20attention%20framework%20for%20event-based%0Avision%20processing%20on%20a%20neuromorphic%20processor.%20Our%20TRIP%20framework%20actively%0Aproduces%20low-resolution%20Region-of-Interest%20%28ROIs%29%20for%20efficient%20and%20accurate%0Aclassification.%20The%20framework%20exploits%20sparse%20events%27%20inherent%20low%20information%0Adensity%20to%20reduce%20the%20overhead%20of%20ROI%20prediction.%20We%20introduced%20extensive%0Ahardware-aware%20optimizations%20for%20TRIP%20and%20implemented%20the%20hardware-optimized%0Aalgorithm%20on%20the%20SENECA%20neuromorphic%20processor.%20We%20utilized%20multiple%0Aevent-based%20classification%20datasets%20for%20evaluation.%20Our%20approach%20achieves%0Astate-of-the-art%20accuracies%20in%20all%20datasets%20and%20produces%20reasonable%20ROIs%20with%0Avarying%20locations%20and%20sizes.%20On%20the%20DvsGesture%20dataset%2C%20our%20solution%20requires%0A46x%20less%20computation%20than%20the%20state-of-the-art%20while%20achieving%20higher%20accuracy.%0AFurthermore%2C%20TRIP%20enables%20more%20than%202x%20latency%20and%20energy%20improvements%20on%20the%0ASENECA%20neuromorphic%20processor%20compared%20to%20the%20conventional%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRIP%253A%2520Trainable%2520Region-of-Interest%2520Prediction%2520for%2520Hardware-Efficient%250A%2520%2520Neuromorphic%2520Processing%2520on%2520Event-based%2520Vision%26entry.906535625%3DCina%2520Arjmand%2520and%2520Yingfu%2520Xu%2520and%2520Kevin%2520Shidqi%2520and%2520Alexandra%2520F.%2520Dobrita%2520and%2520Kanishkan%2520Vadivel%2520and%2520Paul%2520Detterer%2520and%2520Manolis%2520Sifalakis%2520and%2520Amirreza%2520Yousefzadeh%2520and%2520Guangzhi%2520Tang%26entry.1292438233%3D%2520%2520Neuromorphic%2520processors%2520are%2520well-suited%2520for%2520efficiently%2520handling%2520sparse%250Aevents%2520from%2520event-based%2520cameras.%2520However%252C%2520they%2520face%2520significant%2520challenges%2520in%250Athe%2520growth%2520of%2520computing%2520demand%2520and%2520hardware%2520costs%2520as%2520the%2520input%2520resolution%250Aincreases.%2520This%2520paper%2520proposes%2520the%2520Trainable%2520Region-of-Interest%2520Prediction%250A%2528TRIP%2529%252C%2520the%2520first%2520hardware-efficient%2520hard%2520attention%2520framework%2520for%2520event-based%250Avision%2520processing%2520on%2520a%2520neuromorphic%2520processor.%2520Our%2520TRIP%2520framework%2520actively%250Aproduces%2520low-resolution%2520Region-of-Interest%2520%2528ROIs%2529%2520for%2520efficient%2520and%2520accurate%250Aclassification.%2520The%2520framework%2520exploits%2520sparse%2520events%2527%2520inherent%2520low%2520information%250Adensity%2520to%2520reduce%2520the%2520overhead%2520of%2520ROI%2520prediction.%2520We%2520introduced%2520extensive%250Ahardware-aware%2520optimizations%2520for%2520TRIP%2520and%2520implemented%2520the%2520hardware-optimized%250Aalgorithm%2520on%2520the%2520SENECA%2520neuromorphic%2520processor.%2520We%2520utilized%2520multiple%250Aevent-based%2520classification%2520datasets%2520for%2520evaluation.%2520Our%2520approach%2520achieves%250Astate-of-the-art%2520accuracies%2520in%2520all%2520datasets%2520and%2520produces%2520reasonable%2520ROIs%2520with%250Avarying%2520locations%2520and%2520sizes.%2520On%2520the%2520DvsGesture%2520dataset%252C%2520our%2520solution%2520requires%250A46x%2520less%2520computation%2520than%2520the%2520state-of-the-art%2520while%2520achieving%2520higher%2520accuracy.%250AFurthermore%252C%2520TRIP%2520enables%2520more%2520than%25202x%2520latency%2520and%2520energy%2520improvements%2520on%2520the%250ASENECA%2520neuromorphic%2520processor%2520compared%2520to%2520the%2520conventional%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRIP%3A%20Trainable%20Region-of-Interest%20Prediction%20for%20Hardware-Efficient%0A%20%20Neuromorphic%20Processing%20on%20Event-based%20Vision&entry.906535625=Cina%20Arjmand%20and%20Yingfu%20Xu%20and%20Kevin%20Shidqi%20and%20Alexandra%20F.%20Dobrita%20and%20Kanishkan%20Vadivel%20and%20Paul%20Detterer%20and%20Manolis%20Sifalakis%20and%20Amirreza%20Yousefzadeh%20and%20Guangzhi%20Tang&entry.1292438233=%20%20Neuromorphic%20processors%20are%20well-suited%20for%20efficiently%20handling%20sparse%0Aevents%20from%20event-based%20cameras.%20However%2C%20they%20face%20significant%20challenges%20in%0Athe%20growth%20of%20computing%20demand%20and%20hardware%20costs%20as%20the%20input%20resolution%0Aincreases.%20This%20paper%20proposes%20the%20Trainable%20Region-of-Interest%20Prediction%0A%28TRIP%29%2C%20the%20first%20hardware-efficient%20hard%20attention%20framework%20for%20event-based%0Avision%20processing%20on%20a%20neuromorphic%20processor.%20Our%20TRIP%20framework%20actively%0Aproduces%20low-resolution%20Region-of-Interest%20%28ROIs%29%20for%20efficient%20and%20accurate%0Aclassification.%20The%20framework%20exploits%20sparse%20events%27%20inherent%20low%20information%0Adensity%20to%20reduce%20the%20overhead%20of%20ROI%20prediction.%20We%20introduced%20extensive%0Ahardware-aware%20optimizations%20for%20TRIP%20and%20implemented%20the%20hardware-optimized%0Aalgorithm%20on%20the%20SENECA%20neuromorphic%20processor.%20We%20utilized%20multiple%0Aevent-based%20classification%20datasets%20for%20evaluation.%20Our%20approach%20achieves%0Astate-of-the-art%20accuracies%20in%20all%20datasets%20and%20produces%20reasonable%20ROIs%20with%0Avarying%20locations%20and%20sizes.%20On%20the%20DvsGesture%20dataset%2C%20our%20solution%20requires%0A46x%20less%20computation%20than%20the%20state-of-the-art%20while%20achieving%20higher%20accuracy.%0AFurthermore%2C%20TRIP%20enables%20more%20than%202x%20latency%20and%20energy%20improvements%20on%20the%0ASENECA%20neuromorphic%20processor%20compared%20to%20the%20conventional%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17483v1&entry.124074799=Read"},
{"title": "Preserving Node Distinctness in Graph Autoencoders via Similarity\n  Distillation", "author": "Ge Chen and Yulan Hu and Sheng Ouyang and Yong Liu and Cuicui Luo", "abstract": "  Graph autoencoders (GAEs), as a kind of generative self-supervised learning\napproach, have shown great potential in recent years. GAEs typically rely on\ndistance-based criteria, such as mean-square-error (MSE), to reconstruct the\ninput graph. However, relying solely on a single reconstruction criterion may\nlead to a loss of distinctiveness in the reconstructed graph, causing nodes to\ncollapse into similar representations and resulting in sub-optimal performance.\nTo address this issue, we have developed a simple yet effective strategy to\npreserve the necessary distinctness in the reconstructed graph. Inspired by the\nknowledge distillation technique, we found that the dual encoder-decoder\narchitecture of GAEs can be viewed as a teacher-student relationship.\nTherefore, we propose transferring the knowledge of distinctness from the raw\ngraph to the reconstructed graph, achieved through a simple KL constraint.\nSpecifically, we compute pairwise node similarity scores in the raw graph and\nreconstructed graph. During the training process, the KL constraint is\noptimized alongside the reconstruction criterion. We conducted extensive\nexperiments across three types of graph tasks, demonstrating the effectiveness\nand generality of our strategy. This indicates that the proposed approach can\nbe employed as a plug-and-play method to avoid vague reconstructions and\nenhance overall performance.\n", "link": "http://arxiv.org/abs/2406.17517v1", "date": "2024-06-25", "relevancy": 2.1694, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5584}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5475}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserving%20Node%20Distinctness%20in%20Graph%20Autoencoders%20via%20Similarity%0A%20%20Distillation&body=Title%3A%20Preserving%20Node%20Distinctness%20in%20Graph%20Autoencoders%20via%20Similarity%0A%20%20Distillation%0AAuthor%3A%20Ge%20Chen%20and%20Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Yong%20Liu%20and%20Cuicui%20Luo%0AAbstract%3A%20%20%20Graph%20autoencoders%20%28GAEs%29%2C%20as%20a%20kind%20of%20generative%20self-supervised%20learning%0Aapproach%2C%20have%20shown%20great%20potential%20in%20recent%20years.%20GAEs%20typically%20rely%20on%0Adistance-based%20criteria%2C%20such%20as%20mean-square-error%20%28MSE%29%2C%20to%20reconstruct%20the%0Ainput%20graph.%20However%2C%20relying%20solely%20on%20a%20single%20reconstruction%20criterion%20may%0Alead%20to%20a%20loss%20of%20distinctiveness%20in%20the%20reconstructed%20graph%2C%20causing%20nodes%20to%0Acollapse%20into%20similar%20representations%20and%20resulting%20in%20sub-optimal%20performance.%0ATo%20address%20this%20issue%2C%20we%20have%20developed%20a%20simple%20yet%20effective%20strategy%20to%0Apreserve%20the%20necessary%20distinctness%20in%20the%20reconstructed%20graph.%20Inspired%20by%20the%0Aknowledge%20distillation%20technique%2C%20we%20found%20that%20the%20dual%20encoder-decoder%0Aarchitecture%20of%20GAEs%20can%20be%20viewed%20as%20a%20teacher-student%20relationship.%0ATherefore%2C%20we%20propose%20transferring%20the%20knowledge%20of%20distinctness%20from%20the%20raw%0Agraph%20to%20the%20reconstructed%20graph%2C%20achieved%20through%20a%20simple%20KL%20constraint.%0ASpecifically%2C%20we%20compute%20pairwise%20node%20similarity%20scores%20in%20the%20raw%20graph%20and%0Areconstructed%20graph.%20During%20the%20training%20process%2C%20the%20KL%20constraint%20is%0Aoptimized%20alongside%20the%20reconstruction%20criterion.%20We%20conducted%20extensive%0Aexperiments%20across%20three%20types%20of%20graph%20tasks%2C%20demonstrating%20the%20effectiveness%0Aand%20generality%20of%20our%20strategy.%20This%20indicates%20that%20the%20proposed%20approach%20can%0Abe%20employed%20as%20a%20plug-and-play%20method%20to%20avoid%20vague%20reconstructions%20and%0Aenhance%20overall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserving%2520Node%2520Distinctness%2520in%2520Graph%2520Autoencoders%2520via%2520Similarity%250A%2520%2520Distillation%26entry.906535625%3DGe%2520Chen%2520and%2520Yulan%2520Hu%2520and%2520Sheng%2520Ouyang%2520and%2520Yong%2520Liu%2520and%2520Cuicui%2520Luo%26entry.1292438233%3D%2520%2520Graph%2520autoencoders%2520%2528GAEs%2529%252C%2520as%2520a%2520kind%2520of%2520generative%2520self-supervised%2520learning%250Aapproach%252C%2520have%2520shown%2520great%2520potential%2520in%2520recent%2520years.%2520GAEs%2520typically%2520rely%2520on%250Adistance-based%2520criteria%252C%2520such%2520as%2520mean-square-error%2520%2528MSE%2529%252C%2520to%2520reconstruct%2520the%250Ainput%2520graph.%2520However%252C%2520relying%2520solely%2520on%2520a%2520single%2520reconstruction%2520criterion%2520may%250Alead%2520to%2520a%2520loss%2520of%2520distinctiveness%2520in%2520the%2520reconstructed%2520graph%252C%2520causing%2520nodes%2520to%250Acollapse%2520into%2520similar%2520representations%2520and%2520resulting%2520in%2520sub-optimal%2520performance.%250ATo%2520address%2520this%2520issue%252C%2520we%2520have%2520developed%2520a%2520simple%2520yet%2520effective%2520strategy%2520to%250Apreserve%2520the%2520necessary%2520distinctness%2520in%2520the%2520reconstructed%2520graph.%2520Inspired%2520by%2520the%250Aknowledge%2520distillation%2520technique%252C%2520we%2520found%2520that%2520the%2520dual%2520encoder-decoder%250Aarchitecture%2520of%2520GAEs%2520can%2520be%2520viewed%2520as%2520a%2520teacher-student%2520relationship.%250ATherefore%252C%2520we%2520propose%2520transferring%2520the%2520knowledge%2520of%2520distinctness%2520from%2520the%2520raw%250Agraph%2520to%2520the%2520reconstructed%2520graph%252C%2520achieved%2520through%2520a%2520simple%2520KL%2520constraint.%250ASpecifically%252C%2520we%2520compute%2520pairwise%2520node%2520similarity%2520scores%2520in%2520the%2520raw%2520graph%2520and%250Areconstructed%2520graph.%2520During%2520the%2520training%2520process%252C%2520the%2520KL%2520constraint%2520is%250Aoptimized%2520alongside%2520the%2520reconstruction%2520criterion.%2520We%2520conducted%2520extensive%250Aexperiments%2520across%2520three%2520types%2520of%2520graph%2520tasks%252C%2520demonstrating%2520the%2520effectiveness%250Aand%2520generality%2520of%2520our%2520strategy.%2520This%2520indicates%2520that%2520the%2520proposed%2520approach%2520can%250Abe%2520employed%2520as%2520a%2520plug-and-play%2520method%2520to%2520avoid%2520vague%2520reconstructions%2520and%250Aenhance%2520overall%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserving%20Node%20Distinctness%20in%20Graph%20Autoencoders%20via%20Similarity%0A%20%20Distillation&entry.906535625=Ge%20Chen%20and%20Yulan%20Hu%20and%20Sheng%20Ouyang%20and%20Yong%20Liu%20and%20Cuicui%20Luo&entry.1292438233=%20%20Graph%20autoencoders%20%28GAEs%29%2C%20as%20a%20kind%20of%20generative%20self-supervised%20learning%0Aapproach%2C%20have%20shown%20great%20potential%20in%20recent%20years.%20GAEs%20typically%20rely%20on%0Adistance-based%20criteria%2C%20such%20as%20mean-square-error%20%28MSE%29%2C%20to%20reconstruct%20the%0Ainput%20graph.%20However%2C%20relying%20solely%20on%20a%20single%20reconstruction%20criterion%20may%0Alead%20to%20a%20loss%20of%20distinctiveness%20in%20the%20reconstructed%20graph%2C%20causing%20nodes%20to%0Acollapse%20into%20similar%20representations%20and%20resulting%20in%20sub-optimal%20performance.%0ATo%20address%20this%20issue%2C%20we%20have%20developed%20a%20simple%20yet%20effective%20strategy%20to%0Apreserve%20the%20necessary%20distinctness%20in%20the%20reconstructed%20graph.%20Inspired%20by%20the%0Aknowledge%20distillation%20technique%2C%20we%20found%20that%20the%20dual%20encoder-decoder%0Aarchitecture%20of%20GAEs%20can%20be%20viewed%20as%20a%20teacher-student%20relationship.%0ATherefore%2C%20we%20propose%20transferring%20the%20knowledge%20of%20distinctness%20from%20the%20raw%0Agraph%20to%20the%20reconstructed%20graph%2C%20achieved%20through%20a%20simple%20KL%20constraint.%0ASpecifically%2C%20we%20compute%20pairwise%20node%20similarity%20scores%20in%20the%20raw%20graph%20and%0Areconstructed%20graph.%20During%20the%20training%20process%2C%20the%20KL%20constraint%20is%0Aoptimized%20alongside%20the%20reconstruction%20criterion.%20We%20conducted%20extensive%0Aexperiments%20across%20three%20types%20of%20graph%20tasks%2C%20demonstrating%20the%20effectiveness%0Aand%20generality%20of%20our%20strategy.%20This%20indicates%20that%20the%20proposed%20approach%20can%0Abe%20employed%20as%20a%20plug-and-play%20method%20to%20avoid%20vague%20reconstructions%20and%0Aenhance%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17517v1&entry.124074799=Read"},
{"title": "Detection of Synthetic Face Images: Accuracy, Robustness, Generalization", "author": "Nela Petrzelkova and Jan Cech", "abstract": "  An experimental study on detecting synthetic face images is presented. We\ncollected a dataset, called FF5, of five fake face image generators, including\nrecent diffusion models. We find that a simple model trained on a specific\nimage generator can achieve near-perfect accuracy in separating synthetic and\nreal images. The model handles common image distortions (reduced resolution,\ncompression) by using data augmentation. Moreover, partial manipulations, where\nsynthetic images are blended into real ones by inpainting, are identified and\nthe area of the manipulation is localized by a simple model of YOLO\narchitecture. However, the model turned out to be vulnerable to adversarial\nattacks and does not generalize to unseen generators. Failure to generalize to\ndetect images produced by a newer generator also occurs for recent\nstate-of-the-art methods, which we tested on Realistic Vision, a fine-tuned\nversion of StabilityAI's Stable Diffusion image generator.\n", "link": "http://arxiv.org/abs/2406.17547v1", "date": "2024-06-25", "relevancy": 2.1551, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5472}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.538}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20of%20Synthetic%20Face%20Images%3A%20Accuracy%2C%20Robustness%2C%20Generalization&body=Title%3A%20Detection%20of%20Synthetic%20Face%20Images%3A%20Accuracy%2C%20Robustness%2C%20Generalization%0AAuthor%3A%20Nela%20Petrzelkova%20and%20Jan%20Cech%0AAbstract%3A%20%20%20An%20experimental%20study%20on%20detecting%20synthetic%20face%20images%20is%20presented.%20We%0Acollected%20a%20dataset%2C%20called%20FF5%2C%20of%20five%20fake%20face%20image%20generators%2C%20including%0Arecent%20diffusion%20models.%20We%20find%20that%20a%20simple%20model%20trained%20on%20a%20specific%0Aimage%20generator%20can%20achieve%20near-perfect%20accuracy%20in%20separating%20synthetic%20and%0Areal%20images.%20The%20model%20handles%20common%20image%20distortions%20%28reduced%20resolution%2C%0Acompression%29%20by%20using%20data%20augmentation.%20Moreover%2C%20partial%20manipulations%2C%20where%0Asynthetic%20images%20are%20blended%20into%20real%20ones%20by%20inpainting%2C%20are%20identified%20and%0Athe%20area%20of%20the%20manipulation%20is%20localized%20by%20a%20simple%20model%20of%20YOLO%0Aarchitecture.%20However%2C%20the%20model%20turned%20out%20to%20be%20vulnerable%20to%20adversarial%0Aattacks%20and%20does%20not%20generalize%20to%20unseen%20generators.%20Failure%20to%20generalize%20to%0Adetect%20images%20produced%20by%20a%20newer%20generator%20also%20occurs%20for%20recent%0Astate-of-the-art%20methods%2C%20which%20we%20tested%20on%20Realistic%20Vision%2C%20a%20fine-tuned%0Aversion%20of%20StabilityAI%27s%20Stable%20Diffusion%20image%20generator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17547v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520of%2520Synthetic%2520Face%2520Images%253A%2520Accuracy%252C%2520Robustness%252C%2520Generalization%26entry.906535625%3DNela%2520Petrzelkova%2520and%2520Jan%2520Cech%26entry.1292438233%3D%2520%2520An%2520experimental%2520study%2520on%2520detecting%2520synthetic%2520face%2520images%2520is%2520presented.%2520We%250Acollected%2520a%2520dataset%252C%2520called%2520FF5%252C%2520of%2520five%2520fake%2520face%2520image%2520generators%252C%2520including%250Arecent%2520diffusion%2520models.%2520We%2520find%2520that%2520a%2520simple%2520model%2520trained%2520on%2520a%2520specific%250Aimage%2520generator%2520can%2520achieve%2520near-perfect%2520accuracy%2520in%2520separating%2520synthetic%2520and%250Areal%2520images.%2520The%2520model%2520handles%2520common%2520image%2520distortions%2520%2528reduced%2520resolution%252C%250Acompression%2529%2520by%2520using%2520data%2520augmentation.%2520Moreover%252C%2520partial%2520manipulations%252C%2520where%250Asynthetic%2520images%2520are%2520blended%2520into%2520real%2520ones%2520by%2520inpainting%252C%2520are%2520identified%2520and%250Athe%2520area%2520of%2520the%2520manipulation%2520is%2520localized%2520by%2520a%2520simple%2520model%2520of%2520YOLO%250Aarchitecture.%2520However%252C%2520the%2520model%2520turned%2520out%2520to%2520be%2520vulnerable%2520to%2520adversarial%250Aattacks%2520and%2520does%2520not%2520generalize%2520to%2520unseen%2520generators.%2520Failure%2520to%2520generalize%2520to%250Adetect%2520images%2520produced%2520by%2520a%2520newer%2520generator%2520also%2520occurs%2520for%2520recent%250Astate-of-the-art%2520methods%252C%2520which%2520we%2520tested%2520on%2520Realistic%2520Vision%252C%2520a%2520fine-tuned%250Aversion%2520of%2520StabilityAI%2527s%2520Stable%2520Diffusion%2520image%2520generator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17547v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20of%20Synthetic%20Face%20Images%3A%20Accuracy%2C%20Robustness%2C%20Generalization&entry.906535625=Nela%20Petrzelkova%20and%20Jan%20Cech&entry.1292438233=%20%20An%20experimental%20study%20on%20detecting%20synthetic%20face%20images%20is%20presented.%20We%0Acollected%20a%20dataset%2C%20called%20FF5%2C%20of%20five%20fake%20face%20image%20generators%2C%20including%0Arecent%20diffusion%20models.%20We%20find%20that%20a%20simple%20model%20trained%20on%20a%20specific%0Aimage%20generator%20can%20achieve%20near-perfect%20accuracy%20in%20separating%20synthetic%20and%0Areal%20images.%20The%20model%20handles%20common%20image%20distortions%20%28reduced%20resolution%2C%0Acompression%29%20by%20using%20data%20augmentation.%20Moreover%2C%20partial%20manipulations%2C%20where%0Asynthetic%20images%20are%20blended%20into%20real%20ones%20by%20inpainting%2C%20are%20identified%20and%0Athe%20area%20of%20the%20manipulation%20is%20localized%20by%20a%20simple%20model%20of%20YOLO%0Aarchitecture.%20However%2C%20the%20model%20turned%20out%20to%20be%20vulnerable%20to%20adversarial%0Aattacks%20and%20does%20not%20generalize%20to%20unseen%20generators.%20Failure%20to%20generalize%20to%0Adetect%20images%20produced%20by%20a%20newer%20generator%20also%20occurs%20for%20recent%0Astate-of-the-art%20methods%2C%20which%20we%20tested%20on%20Realistic%20Vision%2C%20a%20fine-tuned%0Aversion%20of%20StabilityAI%27s%20Stable%20Diffusion%20image%20generator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17547v1&entry.124074799=Read"},
{"title": "Deep Frequency-Aware Functional Maps for Robust Shape Matching", "author": "Feifan Luo and Qinsong Li and Ling Hu and Haibo Wang and Xinru Liu and Shengjun Liu and Hongyang Chen", "abstract": "  Deep functional map frameworks are widely employed for 3D shape matching.\nHowever, most existing deep functional map methods cannot adaptively capture\nimportant frequency information for functional map estimation in specific\nmatching scenarios, i.e., lacking \\textit{frequency awareness}, resulting in\npoor performance when dealing with large deformable shape matching. To this\nend, we propose a novel unsupervised learning-based framework called Deep\nFrequency-Aware Functional Maps, which can gracefully cope with various shape\nmatching scenarios. We first introduce a general constraint called Spectral\nFilter Operator Preservation to compute desirable functional maps, where the\nspectral filter operator encodes informative frequency information and can\npromote frequency awareness for deep functional map frameworks by learning a\nset of filter functions. Then, we directly utilize the proposed constraint as a\nloss function to supervise functional maps, pointwise maps, and filter\nfunctions simultaneously, where the filter functions are derived from the\northonormal Jacobi basis, and the coefficients of the basis are learnable\nparameters. Finally, we develop an effective refinement strategy to improve the\nfinal pointwise map, which incorporates our constraint and learned filter\nfunctions, leading to more robust and accurate correspondences during the\ninference process. Extensive experimental results on various datasets\ndemonstrate that our approach outperforms the existing state-of-the-art\nmethods, especially in challenging settings like datasets with non-isometric\ndeformation and inconsistent topology.\n", "link": "http://arxiv.org/abs/2402.03904v2", "date": "2024-06-25", "relevancy": 2.1543, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Frequency-Aware%20Functional%20Maps%20for%20Robust%20Shape%20Matching&body=Title%3A%20Deep%20Frequency-Aware%20Functional%20Maps%20for%20Robust%20Shape%20Matching%0AAuthor%3A%20Feifan%20Luo%20and%20Qinsong%20Li%20and%20Ling%20Hu%20and%20Haibo%20Wang%20and%20Xinru%20Liu%20and%20Shengjun%20Liu%20and%20Hongyang%20Chen%0AAbstract%3A%20%20%20Deep%20functional%20map%20frameworks%20are%20widely%20employed%20for%203D%20shape%20matching.%0AHowever%2C%20most%20existing%20deep%20functional%20map%20methods%20cannot%20adaptively%20capture%0Aimportant%20frequency%20information%20for%20functional%20map%20estimation%20in%20specific%0Amatching%20scenarios%2C%20i.e.%2C%20lacking%20%5Ctextit%7Bfrequency%20awareness%7D%2C%20resulting%20in%0Apoor%20performance%20when%20dealing%20with%20large%20deformable%20shape%20matching.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20unsupervised%20learning-based%20framework%20called%20Deep%0AFrequency-Aware%20Functional%20Maps%2C%20which%20can%20gracefully%20cope%20with%20various%20shape%0Amatching%20scenarios.%20We%20first%20introduce%20a%20general%20constraint%20called%20Spectral%0AFilter%20Operator%20Preservation%20to%20compute%20desirable%20functional%20maps%2C%20where%20the%0Aspectral%20filter%20operator%20encodes%20informative%20frequency%20information%20and%20can%0Apromote%20frequency%20awareness%20for%20deep%20functional%20map%20frameworks%20by%20learning%20a%0Aset%20of%20filter%20functions.%20Then%2C%20we%20directly%20utilize%20the%20proposed%20constraint%20as%20a%0Aloss%20function%20to%20supervise%20functional%20maps%2C%20pointwise%20maps%2C%20and%20filter%0Afunctions%20simultaneously%2C%20where%20the%20filter%20functions%20are%20derived%20from%20the%0Aorthonormal%20Jacobi%20basis%2C%20and%20the%20coefficients%20of%20the%20basis%20are%20learnable%0Aparameters.%20Finally%2C%20we%20develop%20an%20effective%20refinement%20strategy%20to%20improve%20the%0Afinal%20pointwise%20map%2C%20which%20incorporates%20our%20constraint%20and%20learned%20filter%0Afunctions%2C%20leading%20to%20more%20robust%20and%20accurate%20correspondences%20during%20the%0Ainference%20process.%20Extensive%20experimental%20results%20on%20various%20datasets%0Ademonstrate%20that%20our%20approach%20outperforms%20the%20existing%20state-of-the-art%0Amethods%2C%20especially%20in%20challenging%20settings%20like%20datasets%20with%20non-isometric%0Adeformation%20and%20inconsistent%20topology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Frequency-Aware%2520Functional%2520Maps%2520for%2520Robust%2520Shape%2520Matching%26entry.906535625%3DFeifan%2520Luo%2520and%2520Qinsong%2520Li%2520and%2520Ling%2520Hu%2520and%2520Haibo%2520Wang%2520and%2520Xinru%2520Liu%2520and%2520Shengjun%2520Liu%2520and%2520Hongyang%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520functional%2520map%2520frameworks%2520are%2520widely%2520employed%2520for%25203D%2520shape%2520matching.%250AHowever%252C%2520most%2520existing%2520deep%2520functional%2520map%2520methods%2520cannot%2520adaptively%2520capture%250Aimportant%2520frequency%2520information%2520for%2520functional%2520map%2520estimation%2520in%2520specific%250Amatching%2520scenarios%252C%2520i.e.%252C%2520lacking%2520%255Ctextit%257Bfrequency%2520awareness%257D%252C%2520resulting%2520in%250Apoor%2520performance%2520when%2520dealing%2520with%2520large%2520deformable%2520shape%2520matching.%2520To%2520this%250Aend%252C%2520we%2520propose%2520a%2520novel%2520unsupervised%2520learning-based%2520framework%2520called%2520Deep%250AFrequency-Aware%2520Functional%2520Maps%252C%2520which%2520can%2520gracefully%2520cope%2520with%2520various%2520shape%250Amatching%2520scenarios.%2520We%2520first%2520introduce%2520a%2520general%2520constraint%2520called%2520Spectral%250AFilter%2520Operator%2520Preservation%2520to%2520compute%2520desirable%2520functional%2520maps%252C%2520where%2520the%250Aspectral%2520filter%2520operator%2520encodes%2520informative%2520frequency%2520information%2520and%2520can%250Apromote%2520frequency%2520awareness%2520for%2520deep%2520functional%2520map%2520frameworks%2520by%2520learning%2520a%250Aset%2520of%2520filter%2520functions.%2520Then%252C%2520we%2520directly%2520utilize%2520the%2520proposed%2520constraint%2520as%2520a%250Aloss%2520function%2520to%2520supervise%2520functional%2520maps%252C%2520pointwise%2520maps%252C%2520and%2520filter%250Afunctions%2520simultaneously%252C%2520where%2520the%2520filter%2520functions%2520are%2520derived%2520from%2520the%250Aorthonormal%2520Jacobi%2520basis%252C%2520and%2520the%2520coefficients%2520of%2520the%2520basis%2520are%2520learnable%250Aparameters.%2520Finally%252C%2520we%2520develop%2520an%2520effective%2520refinement%2520strategy%2520to%2520improve%2520the%250Afinal%2520pointwise%2520map%252C%2520which%2520incorporates%2520our%2520constraint%2520and%2520learned%2520filter%250Afunctions%252C%2520leading%2520to%2520more%2520robust%2520and%2520accurate%2520correspondences%2520during%2520the%250Ainference%2520process.%2520Extensive%2520experimental%2520results%2520on%2520various%2520datasets%250Ademonstrate%2520that%2520our%2520approach%2520outperforms%2520the%2520existing%2520state-of-the-art%250Amethods%252C%2520especially%2520in%2520challenging%2520settings%2520like%2520datasets%2520with%2520non-isometric%250Adeformation%2520and%2520inconsistent%2520topology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Frequency-Aware%20Functional%20Maps%20for%20Robust%20Shape%20Matching&entry.906535625=Feifan%20Luo%20and%20Qinsong%20Li%20and%20Ling%20Hu%20and%20Haibo%20Wang%20and%20Xinru%20Liu%20and%20Shengjun%20Liu%20and%20Hongyang%20Chen&entry.1292438233=%20%20Deep%20functional%20map%20frameworks%20are%20widely%20employed%20for%203D%20shape%20matching.%0AHowever%2C%20most%20existing%20deep%20functional%20map%20methods%20cannot%20adaptively%20capture%0Aimportant%20frequency%20information%20for%20functional%20map%20estimation%20in%20specific%0Amatching%20scenarios%2C%20i.e.%2C%20lacking%20%5Ctextit%7Bfrequency%20awareness%7D%2C%20resulting%20in%0Apoor%20performance%20when%20dealing%20with%20large%20deformable%20shape%20matching.%20To%20this%0Aend%2C%20we%20propose%20a%20novel%20unsupervised%20learning-based%20framework%20called%20Deep%0AFrequency-Aware%20Functional%20Maps%2C%20which%20can%20gracefully%20cope%20with%20various%20shape%0Amatching%20scenarios.%20We%20first%20introduce%20a%20general%20constraint%20called%20Spectral%0AFilter%20Operator%20Preservation%20to%20compute%20desirable%20functional%20maps%2C%20where%20the%0Aspectral%20filter%20operator%20encodes%20informative%20frequency%20information%20and%20can%0Apromote%20frequency%20awareness%20for%20deep%20functional%20map%20frameworks%20by%20learning%20a%0Aset%20of%20filter%20functions.%20Then%2C%20we%20directly%20utilize%20the%20proposed%20constraint%20as%20a%0Aloss%20function%20to%20supervise%20functional%20maps%2C%20pointwise%20maps%2C%20and%20filter%0Afunctions%20simultaneously%2C%20where%20the%20filter%20functions%20are%20derived%20from%20the%0Aorthonormal%20Jacobi%20basis%2C%20and%20the%20coefficients%20of%20the%20basis%20are%20learnable%0Aparameters.%20Finally%2C%20we%20develop%20an%20effective%20refinement%20strategy%20to%20improve%20the%0Afinal%20pointwise%20map%2C%20which%20incorporates%20our%20constraint%20and%20learned%20filter%0Afunctions%2C%20leading%20to%20more%20robust%20and%20accurate%20correspondences%20during%20the%0Ainference%20process.%20Extensive%20experimental%20results%20on%20various%20datasets%0Ademonstrate%20that%20our%20approach%20outperforms%20the%20existing%20state-of-the-art%0Amethods%2C%20especially%20in%20challenging%20settings%20like%20datasets%20with%20non-isometric%0Adeformation%20and%20inconsistent%20topology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03904v2&entry.124074799=Read"},
{"title": "MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning", "author": "Xiangyu Zhao and Xiangtai Li and Haodong Duan and Haian Huang and Yining Li and Kai Chen and Hua Yang", "abstract": "  Multi-modal large language models (MLLMs) have made significant strides in\nvarious visual understanding tasks. However, the majority of these models are\nconstrained to process low-resolution images, which limits their effectiveness\nin perception tasks that necessitate detailed visual information. In our study,\nwe present MG-LLaVA, an innovative MLLM that enhances the model's visual\nprocessing capabilities by incorporating a multi-granularity vision flow, which\nincludes low-resolution, high-resolution, and object-centric features. We\npropose the integration of an additional high-resolution visual encoder to\ncapture fine-grained details, which are then fused with base visual features\nthrough a Conv-Gate fusion network. To further refine the model's object\nrecognition abilities, we incorporate object-level features derived from\nbounding boxes identified by offline detectors. Being trained solely on\npublicly available multimodal data through instruction tuning, MG-LLaVA\ndemonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide\nvariety of language encoders, ranging from 3.8B to 34B, to evaluate the model's\nperformance comprehensively. Extensive evaluations across multiple benchmarks\ndemonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code will be available at\nhttps://github.com/PhoenixZ810/MG-LLaVA.\n", "link": "http://arxiv.org/abs/2406.17770v1", "date": "2024-06-25", "relevancy": 2.1519, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5624}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5222}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MG-LLaVA%3A%20Towards%20Multi-Granularity%20Visual%20Instruction%20Tuning&body=Title%3A%20MG-LLaVA%3A%20Towards%20Multi-Granularity%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Xiangyu%20Zhao%20and%20Xiangtai%20Li%20and%20Haodong%20Duan%20and%20Haian%20Huang%20and%20Yining%20Li%20and%20Kai%20Chen%20and%20Hua%20Yang%0AAbstract%3A%20%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%20strides%20in%0Avarious%20visual%20understanding%20tasks.%20However%2C%20the%20majority%20of%20these%20models%20are%0Aconstrained%20to%20process%20low-resolution%20images%2C%20which%20limits%20their%20effectiveness%0Ain%20perception%20tasks%20that%20necessitate%20detailed%20visual%20information.%20In%20our%20study%2C%0Awe%20present%20MG-LLaVA%2C%20an%20innovative%20MLLM%20that%20enhances%20the%20model%27s%20visual%0Aprocessing%20capabilities%20by%20incorporating%20a%20multi-granularity%20vision%20flow%2C%20which%0Aincludes%20low-resolution%2C%20high-resolution%2C%20and%20object-centric%20features.%20We%0Apropose%20the%20integration%20of%20an%20additional%20high-resolution%20visual%20encoder%20to%0Acapture%20fine-grained%20details%2C%20which%20are%20then%20fused%20with%20base%20visual%20features%0Athrough%20a%20Conv-Gate%20fusion%20network.%20To%20further%20refine%20the%20model%27s%20object%0Arecognition%20abilities%2C%20we%20incorporate%20object-level%20features%20derived%20from%0Abounding%20boxes%20identified%20by%20offline%20detectors.%20Being%20trained%20solely%20on%0Apublicly%20available%20multimodal%20data%20through%20instruction%20tuning%2C%20MG-LLaVA%0Ademonstrates%20exceptional%20perception%20skills.%20We%20instantiate%20MG-LLaVA%20with%20a%20wide%0Avariety%20of%20language%20encoders%2C%20ranging%20from%203.8B%20to%2034B%2C%20to%20evaluate%20the%20model%27s%0Aperformance%20comprehensively.%20Extensive%20evaluations%20across%20multiple%20benchmarks%0Ademonstrate%20that%20MG-LLaVA%20outperforms%20existing%20MLLMs%20of%20comparable%20parameter%0Asizes%2C%20showcasing%20its%20remarkable%20efficacy.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/PhoenixZ810/MG-LLaVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMG-LLaVA%253A%2520Towards%2520Multi-Granularity%2520Visual%2520Instruction%2520Tuning%26entry.906535625%3DXiangyu%2520Zhao%2520and%2520Xiangtai%2520Li%2520and%2520Haodong%2520Duan%2520and%2520Haian%2520Huang%2520and%2520Yining%2520Li%2520and%2520Kai%2520Chen%2520and%2520Hua%2520Yang%26entry.1292438233%3D%2520%2520Multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520made%2520significant%2520strides%2520in%250Avarious%2520visual%2520understanding%2520tasks.%2520However%252C%2520the%2520majority%2520of%2520these%2520models%2520are%250Aconstrained%2520to%2520process%2520low-resolution%2520images%252C%2520which%2520limits%2520their%2520effectiveness%250Ain%2520perception%2520tasks%2520that%2520necessitate%2520detailed%2520visual%2520information.%2520In%2520our%2520study%252C%250Awe%2520present%2520MG-LLaVA%252C%2520an%2520innovative%2520MLLM%2520that%2520enhances%2520the%2520model%2527s%2520visual%250Aprocessing%2520capabilities%2520by%2520incorporating%2520a%2520multi-granularity%2520vision%2520flow%252C%2520which%250Aincludes%2520low-resolution%252C%2520high-resolution%252C%2520and%2520object-centric%2520features.%2520We%250Apropose%2520the%2520integration%2520of%2520an%2520additional%2520high-resolution%2520visual%2520encoder%2520to%250Acapture%2520fine-grained%2520details%252C%2520which%2520are%2520then%2520fused%2520with%2520base%2520visual%2520features%250Athrough%2520a%2520Conv-Gate%2520fusion%2520network.%2520To%2520further%2520refine%2520the%2520model%2527s%2520object%250Arecognition%2520abilities%252C%2520we%2520incorporate%2520object-level%2520features%2520derived%2520from%250Abounding%2520boxes%2520identified%2520by%2520offline%2520detectors.%2520Being%2520trained%2520solely%2520on%250Apublicly%2520available%2520multimodal%2520data%2520through%2520instruction%2520tuning%252C%2520MG-LLaVA%250Ademonstrates%2520exceptional%2520perception%2520skills.%2520We%2520instantiate%2520MG-LLaVA%2520with%2520a%2520wide%250Avariety%2520of%2520language%2520encoders%252C%2520ranging%2520from%25203.8B%2520to%252034B%252C%2520to%2520evaluate%2520the%2520model%2527s%250Aperformance%2520comprehensively.%2520Extensive%2520evaluations%2520across%2520multiple%2520benchmarks%250Ademonstrate%2520that%2520MG-LLaVA%2520outperforms%2520existing%2520MLLMs%2520of%2520comparable%2520parameter%250Asizes%252C%2520showcasing%2520its%2520remarkable%2520efficacy.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/PhoenixZ810/MG-LLaVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MG-LLaVA%3A%20Towards%20Multi-Granularity%20Visual%20Instruction%20Tuning&entry.906535625=Xiangyu%20Zhao%20and%20Xiangtai%20Li%20and%20Haodong%20Duan%20and%20Haian%20Huang%20and%20Yining%20Li%20and%20Kai%20Chen%20and%20Hua%20Yang&entry.1292438233=%20%20Multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20made%20significant%20strides%20in%0Avarious%20visual%20understanding%20tasks.%20However%2C%20the%20majority%20of%20these%20models%20are%0Aconstrained%20to%20process%20low-resolution%20images%2C%20which%20limits%20their%20effectiveness%0Ain%20perception%20tasks%20that%20necessitate%20detailed%20visual%20information.%20In%20our%20study%2C%0Awe%20present%20MG-LLaVA%2C%20an%20innovative%20MLLM%20that%20enhances%20the%20model%27s%20visual%0Aprocessing%20capabilities%20by%20incorporating%20a%20multi-granularity%20vision%20flow%2C%20which%0Aincludes%20low-resolution%2C%20high-resolution%2C%20and%20object-centric%20features.%20We%0Apropose%20the%20integration%20of%20an%20additional%20high-resolution%20visual%20encoder%20to%0Acapture%20fine-grained%20details%2C%20which%20are%20then%20fused%20with%20base%20visual%20features%0Athrough%20a%20Conv-Gate%20fusion%20network.%20To%20further%20refine%20the%20model%27s%20object%0Arecognition%20abilities%2C%20we%20incorporate%20object-level%20features%20derived%20from%0Abounding%20boxes%20identified%20by%20offline%20detectors.%20Being%20trained%20solely%20on%0Apublicly%20available%20multimodal%20data%20through%20instruction%20tuning%2C%20MG-LLaVA%0Ademonstrates%20exceptional%20perception%20skills.%20We%20instantiate%20MG-LLaVA%20with%20a%20wide%0Avariety%20of%20language%20encoders%2C%20ranging%20from%203.8B%20to%2034B%2C%20to%20evaluate%20the%20model%27s%0Aperformance%20comprehensively.%20Extensive%20evaluations%20across%20multiple%20benchmarks%0Ademonstrate%20that%20MG-LLaVA%20outperforms%20existing%20MLLMs%20of%20comparable%20parameter%0Asizes%2C%20showcasing%20its%20remarkable%20efficacy.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/PhoenixZ810/MG-LLaVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17770v1&entry.124074799=Read"},
{"title": "Medical Image Segmentation Using Directional Window Attention", "author": "Daniya Najiha Abdul Kareem and Mustansar Fiaz and Noa Novershtern and Hisham Cholakkal", "abstract": "  Accurate segmentation of medical images is crucial for diagnostic purposes,\nincluding cell segmentation, tumor identification, and organ localization.\nTraditional convolutional neural network (CNN)-based approaches struggled to\nachieve precise segmentation results due to their limited receptive fields,\nparticularly in cases involving multi-organ segmentation with varying shapes\nand sizes. The transformer-based approaches address this limitation by\nleveraging the global receptive field, but they often face challenges in\ncapturing local information required for pixel-precise segmentation. In this\nwork, we introduce DwinFormer, a hierarchical encoder-decoder architecture for\nmedical image segmentation comprising a directional window (Dwin) attention and\nglobal self-attention (GSA) for feature encoding. The focus of our design is\nthe introduction of Dwin block within DwinFormer that effectively captures\nlocal and global information along the horizontal, vertical, and depthwise\ndirections of the input feature map by separately performing attention in each\nof these directional volumes. To this end, our Dwin block introduces a nested\nDwin attention (NDA) that progressively increases the receptive field in\nhorizontal, vertical, and depthwise directions and a convolutional Dwin\nattention (CDA) that captures local contextual information for the attention\ncomputation. While the proposed Dwin block captures local and global\ndependencies at the first two high-resolution stages of DwinFormer, the GSA\nblock encodes global dependencies at the last two lower-resolution stages.\nExperiments over the challenging 3D Synapse Multi-organ dataset and Cell HMS\ndataset demonstrate the benefits of our DwinFormer over the state-of-the-art\napproaches. Our source code will be publicly available at\n\\url{https://github.com/Daniyanaj/DWINFORMER}.\n", "link": "http://arxiv.org/abs/2406.17471v1", "date": "2024-06-25", "relevancy": 2.1511, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Medical%20Image%20Segmentation%20Using%20Directional%20Window%20Attention&body=Title%3A%20Medical%20Image%20Segmentation%20Using%20Directional%20Window%20Attention%0AAuthor%3A%20Daniya%20Najiha%20Abdul%20Kareem%20and%20Mustansar%20Fiaz%20and%20Noa%20Novershtern%20and%20Hisham%20Cholakkal%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20medical%20images%20is%20crucial%20for%20diagnostic%20purposes%2C%0Aincluding%20cell%20segmentation%2C%20tumor%20identification%2C%20and%20organ%20localization.%0ATraditional%20convolutional%20neural%20network%20%28CNN%29-based%20approaches%20struggled%20to%0Aachieve%20precise%20segmentation%20results%20due%20to%20their%20limited%20receptive%20fields%2C%0Aparticularly%20in%20cases%20involving%20multi-organ%20segmentation%20with%20varying%20shapes%0Aand%20sizes.%20The%20transformer-based%20approaches%20address%20this%20limitation%20by%0Aleveraging%20the%20global%20receptive%20field%2C%20but%20they%20often%20face%20challenges%20in%0Acapturing%20local%20information%20required%20for%20pixel-precise%20segmentation.%20In%20this%0Awork%2C%20we%20introduce%20DwinFormer%2C%20a%20hierarchical%20encoder-decoder%20architecture%20for%0Amedical%20image%20segmentation%20comprising%20a%20directional%20window%20%28Dwin%29%20attention%20and%0Aglobal%20self-attention%20%28GSA%29%20for%20feature%20encoding.%20The%20focus%20of%20our%20design%20is%0Athe%20introduction%20of%20Dwin%20block%20within%20DwinFormer%20that%20effectively%20captures%0Alocal%20and%20global%20information%20along%20the%20horizontal%2C%20vertical%2C%20and%20depthwise%0Adirections%20of%20the%20input%20feature%20map%20by%20separately%20performing%20attention%20in%20each%0Aof%20these%20directional%20volumes.%20To%20this%20end%2C%20our%20Dwin%20block%20introduces%20a%20nested%0ADwin%20attention%20%28NDA%29%20that%20progressively%20increases%20the%20receptive%20field%20in%0Ahorizontal%2C%20vertical%2C%20and%20depthwise%20directions%20and%20a%20convolutional%20Dwin%0Aattention%20%28CDA%29%20that%20captures%20local%20contextual%20information%20for%20the%20attention%0Acomputation.%20While%20the%20proposed%20Dwin%20block%20captures%20local%20and%20global%0Adependencies%20at%20the%20first%20two%20high-resolution%20stages%20of%20DwinFormer%2C%20the%20GSA%0Ablock%20encodes%20global%20dependencies%20at%20the%20last%20two%20lower-resolution%20stages.%0AExperiments%20over%20the%20challenging%203D%20Synapse%20Multi-organ%20dataset%20and%20Cell%20HMS%0Adataset%20demonstrate%20the%20benefits%20of%20our%20DwinFormer%20over%20the%20state-of-the-art%0Aapproaches.%20Our%20source%20code%20will%20be%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Daniyanaj/DWINFORMER%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedical%2520Image%2520Segmentation%2520Using%2520Directional%2520Window%2520Attention%26entry.906535625%3DDaniya%2520Najiha%2520Abdul%2520Kareem%2520and%2520Mustansar%2520Fiaz%2520and%2520Noa%2520Novershtern%2520and%2520Hisham%2520Cholakkal%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520medical%2520images%2520is%2520crucial%2520for%2520diagnostic%2520purposes%252C%250Aincluding%2520cell%2520segmentation%252C%2520tumor%2520identification%252C%2520and%2520organ%2520localization.%250ATraditional%2520convolutional%2520neural%2520network%2520%2528CNN%2529-based%2520approaches%2520struggled%2520to%250Aachieve%2520precise%2520segmentation%2520results%2520due%2520to%2520their%2520limited%2520receptive%2520fields%252C%250Aparticularly%2520in%2520cases%2520involving%2520multi-organ%2520segmentation%2520with%2520varying%2520shapes%250Aand%2520sizes.%2520The%2520transformer-based%2520approaches%2520address%2520this%2520limitation%2520by%250Aleveraging%2520the%2520global%2520receptive%2520field%252C%2520but%2520they%2520often%2520face%2520challenges%2520in%250Acapturing%2520local%2520information%2520required%2520for%2520pixel-precise%2520segmentation.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520DwinFormer%252C%2520a%2520hierarchical%2520encoder-decoder%2520architecture%2520for%250Amedical%2520image%2520segmentation%2520comprising%2520a%2520directional%2520window%2520%2528Dwin%2529%2520attention%2520and%250Aglobal%2520self-attention%2520%2528GSA%2529%2520for%2520feature%2520encoding.%2520The%2520focus%2520of%2520our%2520design%2520is%250Athe%2520introduction%2520of%2520Dwin%2520block%2520within%2520DwinFormer%2520that%2520effectively%2520captures%250Alocal%2520and%2520global%2520information%2520along%2520the%2520horizontal%252C%2520vertical%252C%2520and%2520depthwise%250Adirections%2520of%2520the%2520input%2520feature%2520map%2520by%2520separately%2520performing%2520attention%2520in%2520each%250Aof%2520these%2520directional%2520volumes.%2520To%2520this%2520end%252C%2520our%2520Dwin%2520block%2520introduces%2520a%2520nested%250ADwin%2520attention%2520%2528NDA%2529%2520that%2520progressively%2520increases%2520the%2520receptive%2520field%2520in%250Ahorizontal%252C%2520vertical%252C%2520and%2520depthwise%2520directions%2520and%2520a%2520convolutional%2520Dwin%250Aattention%2520%2528CDA%2529%2520that%2520captures%2520local%2520contextual%2520information%2520for%2520the%2520attention%250Acomputation.%2520While%2520the%2520proposed%2520Dwin%2520block%2520captures%2520local%2520and%2520global%250Adependencies%2520at%2520the%2520first%2520two%2520high-resolution%2520stages%2520of%2520DwinFormer%252C%2520the%2520GSA%250Ablock%2520encodes%2520global%2520dependencies%2520at%2520the%2520last%2520two%2520lower-resolution%2520stages.%250AExperiments%2520over%2520the%2520challenging%25203D%2520Synapse%2520Multi-organ%2520dataset%2520and%2520Cell%2520HMS%250Adataset%2520demonstrate%2520the%2520benefits%2520of%2520our%2520DwinFormer%2520over%2520the%2520state-of-the-art%250Aapproaches.%2520Our%2520source%2520code%2520will%2520be%2520publicly%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Daniyanaj/DWINFORMER%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medical%20Image%20Segmentation%20Using%20Directional%20Window%20Attention&entry.906535625=Daniya%20Najiha%20Abdul%20Kareem%20and%20Mustansar%20Fiaz%20and%20Noa%20Novershtern%20and%20Hisham%20Cholakkal&entry.1292438233=%20%20Accurate%20segmentation%20of%20medical%20images%20is%20crucial%20for%20diagnostic%20purposes%2C%0Aincluding%20cell%20segmentation%2C%20tumor%20identification%2C%20and%20organ%20localization.%0ATraditional%20convolutional%20neural%20network%20%28CNN%29-based%20approaches%20struggled%20to%0Aachieve%20precise%20segmentation%20results%20due%20to%20their%20limited%20receptive%20fields%2C%0Aparticularly%20in%20cases%20involving%20multi-organ%20segmentation%20with%20varying%20shapes%0Aand%20sizes.%20The%20transformer-based%20approaches%20address%20this%20limitation%20by%0Aleveraging%20the%20global%20receptive%20field%2C%20but%20they%20often%20face%20challenges%20in%0Acapturing%20local%20information%20required%20for%20pixel-precise%20segmentation.%20In%20this%0Awork%2C%20we%20introduce%20DwinFormer%2C%20a%20hierarchical%20encoder-decoder%20architecture%20for%0Amedical%20image%20segmentation%20comprising%20a%20directional%20window%20%28Dwin%29%20attention%20and%0Aglobal%20self-attention%20%28GSA%29%20for%20feature%20encoding.%20The%20focus%20of%20our%20design%20is%0Athe%20introduction%20of%20Dwin%20block%20within%20DwinFormer%20that%20effectively%20captures%0Alocal%20and%20global%20information%20along%20the%20horizontal%2C%20vertical%2C%20and%20depthwise%0Adirections%20of%20the%20input%20feature%20map%20by%20separately%20performing%20attention%20in%20each%0Aof%20these%20directional%20volumes.%20To%20this%20end%2C%20our%20Dwin%20block%20introduces%20a%20nested%0ADwin%20attention%20%28NDA%29%20that%20progressively%20increases%20the%20receptive%20field%20in%0Ahorizontal%2C%20vertical%2C%20and%20depthwise%20directions%20and%20a%20convolutional%20Dwin%0Aattention%20%28CDA%29%20that%20captures%20local%20contextual%20information%20for%20the%20attention%0Acomputation.%20While%20the%20proposed%20Dwin%20block%20captures%20local%20and%20global%0Adependencies%20at%20the%20first%20two%20high-resolution%20stages%20of%20DwinFormer%2C%20the%20GSA%0Ablock%20encodes%20global%20dependencies%20at%20the%20last%20two%20lower-resolution%20stages.%0AExperiments%20over%20the%20challenging%203D%20Synapse%20Multi-organ%20dataset%20and%20Cell%20HMS%0Adataset%20demonstrate%20the%20benefits%20of%20our%20DwinFormer%20over%20the%20state-of-the-art%0Aapproaches.%20Our%20source%20code%20will%20be%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Daniyanaj/DWINFORMER%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17471v1&entry.124074799=Read"},
{"title": "Multi-Modal Conformal Prediction Regions with Simple Structures by\n  Optimizing Convex Shape Templates", "author": "Renukanandan Tumu and Matthew Cleaveland and Rahul Mangharam and George J. Pappas and Lars Lindemann", "abstract": "  Conformal prediction is a statistical tool for producing prediction regions\nfor machine learning models that are valid with high probability. A key\ncomponent of conformal prediction algorithms is a \\emph{non-conformity score\nfunction} that quantifies how different a model's prediction is from the\nunknown ground truth value. Essentially, these functions determine the shape\nand the size of the conformal prediction regions. While prior work has gone\ninto creating score functions that produce multi-model prediction regions, such\nregions are generally too complex for use in downstream planning and control\nproblems. We propose a method that optimizes parameterized \\emph{shape template\nfunctions} over calibration data, which results in non-conformity score\nfunctions that produce prediction regions with minimum volume. Our approach\nresults in prediction regions that are \\emph{multi-modal}, so they can properly\ncapture residuals of distributions that have multiple modes, and\n\\emph{practical}, so each region is convex and can be easily incorporated into\ndownstream tasks, such as a motion planner using conformal prediction regions.\nOur method applies to general supervised learning tasks, while we illustrate\nits use in time-series prediction. We provide a toolbox and present\nillustrative case studies of F16 fighter jets and autonomous vehicles, showing\nan up to $68\\%$ reduction in prediction region area compared to a circular\nbaseline region.\n", "link": "http://arxiv.org/abs/2312.07434v2", "date": "2024-06-25", "relevancy": 2.1441, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5888}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Conformal%20Prediction%20Regions%20with%20Simple%20Structures%20by%0A%20%20Optimizing%20Convex%20Shape%20Templates&body=Title%3A%20Multi-Modal%20Conformal%20Prediction%20Regions%20with%20Simple%20Structures%20by%0A%20%20Optimizing%20Convex%20Shape%20Templates%0AAuthor%3A%20Renukanandan%20Tumu%20and%20Matthew%20Cleaveland%20and%20Rahul%20Mangharam%20and%20George%20J.%20Pappas%20and%20Lars%20Lindemann%0AAbstract%3A%20%20%20Conformal%20prediction%20is%20a%20statistical%20tool%20for%20producing%20prediction%20regions%0Afor%20machine%20learning%20models%20that%20are%20valid%20with%20high%20probability.%20A%20key%0Acomponent%20of%20conformal%20prediction%20algorithms%20is%20a%20%5Cemph%7Bnon-conformity%20score%0Afunction%7D%20that%20quantifies%20how%20different%20a%20model%27s%20prediction%20is%20from%20the%0Aunknown%20ground%20truth%20value.%20Essentially%2C%20these%20functions%20determine%20the%20shape%0Aand%20the%20size%20of%20the%20conformal%20prediction%20regions.%20While%20prior%20work%20has%20gone%0Ainto%20creating%20score%20functions%20that%20produce%20multi-model%20prediction%20regions%2C%20such%0Aregions%20are%20generally%20too%20complex%20for%20use%20in%20downstream%20planning%20and%20control%0Aproblems.%20We%20propose%20a%20method%20that%20optimizes%20parameterized%20%5Cemph%7Bshape%20template%0Afunctions%7D%20over%20calibration%20data%2C%20which%20results%20in%20non-conformity%20score%0Afunctions%20that%20produce%20prediction%20regions%20with%20minimum%20volume.%20Our%20approach%0Aresults%20in%20prediction%20regions%20that%20are%20%5Cemph%7Bmulti-modal%7D%2C%20so%20they%20can%20properly%0Acapture%20residuals%20of%20distributions%20that%20have%20multiple%20modes%2C%20and%0A%5Cemph%7Bpractical%7D%2C%20so%20each%20region%20is%20convex%20and%20can%20be%20easily%20incorporated%20into%0Adownstream%20tasks%2C%20such%20as%20a%20motion%20planner%20using%20conformal%20prediction%20regions.%0AOur%20method%20applies%20to%20general%20supervised%20learning%20tasks%2C%20while%20we%20illustrate%0Aits%20use%20in%20time-series%20prediction.%20We%20provide%20a%20toolbox%20and%20present%0Aillustrative%20case%20studies%20of%20F16%20fighter%20jets%20and%20autonomous%20vehicles%2C%20showing%0Aan%20up%20to%20%2468%5C%25%24%20reduction%20in%20prediction%20region%20area%20compared%20to%20a%20circular%0Abaseline%20region.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Conformal%2520Prediction%2520Regions%2520with%2520Simple%2520Structures%2520by%250A%2520%2520Optimizing%2520Convex%2520Shape%2520Templates%26entry.906535625%3DRenukanandan%2520Tumu%2520and%2520Matthew%2520Cleaveland%2520and%2520Rahul%2520Mangharam%2520and%2520George%2520J.%2520Pappas%2520and%2520Lars%2520Lindemann%26entry.1292438233%3D%2520%2520Conformal%2520prediction%2520is%2520a%2520statistical%2520tool%2520for%2520producing%2520prediction%2520regions%250Afor%2520machine%2520learning%2520models%2520that%2520are%2520valid%2520with%2520high%2520probability.%2520A%2520key%250Acomponent%2520of%2520conformal%2520prediction%2520algorithms%2520is%2520a%2520%255Cemph%257Bnon-conformity%2520score%250Afunction%257D%2520that%2520quantifies%2520how%2520different%2520a%2520model%2527s%2520prediction%2520is%2520from%2520the%250Aunknown%2520ground%2520truth%2520value.%2520Essentially%252C%2520these%2520functions%2520determine%2520the%2520shape%250Aand%2520the%2520size%2520of%2520the%2520conformal%2520prediction%2520regions.%2520While%2520prior%2520work%2520has%2520gone%250Ainto%2520creating%2520score%2520functions%2520that%2520produce%2520multi-model%2520prediction%2520regions%252C%2520such%250Aregions%2520are%2520generally%2520too%2520complex%2520for%2520use%2520in%2520downstream%2520planning%2520and%2520control%250Aproblems.%2520We%2520propose%2520a%2520method%2520that%2520optimizes%2520parameterized%2520%255Cemph%257Bshape%2520template%250Afunctions%257D%2520over%2520calibration%2520data%252C%2520which%2520results%2520in%2520non-conformity%2520score%250Afunctions%2520that%2520produce%2520prediction%2520regions%2520with%2520minimum%2520volume.%2520Our%2520approach%250Aresults%2520in%2520prediction%2520regions%2520that%2520are%2520%255Cemph%257Bmulti-modal%257D%252C%2520so%2520they%2520can%2520properly%250Acapture%2520residuals%2520of%2520distributions%2520that%2520have%2520multiple%2520modes%252C%2520and%250A%255Cemph%257Bpractical%257D%252C%2520so%2520each%2520region%2520is%2520convex%2520and%2520can%2520be%2520easily%2520incorporated%2520into%250Adownstream%2520tasks%252C%2520such%2520as%2520a%2520motion%2520planner%2520using%2520conformal%2520prediction%2520regions.%250AOur%2520method%2520applies%2520to%2520general%2520supervised%2520learning%2520tasks%252C%2520while%2520we%2520illustrate%250Aits%2520use%2520in%2520time-series%2520prediction.%2520We%2520provide%2520a%2520toolbox%2520and%2520present%250Aillustrative%2520case%2520studies%2520of%2520F16%2520fighter%2520jets%2520and%2520autonomous%2520vehicles%252C%2520showing%250Aan%2520up%2520to%2520%252468%255C%2525%2524%2520reduction%2520in%2520prediction%2520region%2520area%2520compared%2520to%2520a%2520circular%250Abaseline%2520region.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Conformal%20Prediction%20Regions%20with%20Simple%20Structures%20by%0A%20%20Optimizing%20Convex%20Shape%20Templates&entry.906535625=Renukanandan%20Tumu%20and%20Matthew%20Cleaveland%20and%20Rahul%20Mangharam%20and%20George%20J.%20Pappas%20and%20Lars%20Lindemann&entry.1292438233=%20%20Conformal%20prediction%20is%20a%20statistical%20tool%20for%20producing%20prediction%20regions%0Afor%20machine%20learning%20models%20that%20are%20valid%20with%20high%20probability.%20A%20key%0Acomponent%20of%20conformal%20prediction%20algorithms%20is%20a%20%5Cemph%7Bnon-conformity%20score%0Afunction%7D%20that%20quantifies%20how%20different%20a%20model%27s%20prediction%20is%20from%20the%0Aunknown%20ground%20truth%20value.%20Essentially%2C%20these%20functions%20determine%20the%20shape%0Aand%20the%20size%20of%20the%20conformal%20prediction%20regions.%20While%20prior%20work%20has%20gone%0Ainto%20creating%20score%20functions%20that%20produce%20multi-model%20prediction%20regions%2C%20such%0Aregions%20are%20generally%20too%20complex%20for%20use%20in%20downstream%20planning%20and%20control%0Aproblems.%20We%20propose%20a%20method%20that%20optimizes%20parameterized%20%5Cemph%7Bshape%20template%0Afunctions%7D%20over%20calibration%20data%2C%20which%20results%20in%20non-conformity%20score%0Afunctions%20that%20produce%20prediction%20regions%20with%20minimum%20volume.%20Our%20approach%0Aresults%20in%20prediction%20regions%20that%20are%20%5Cemph%7Bmulti-modal%7D%2C%20so%20they%20can%20properly%0Acapture%20residuals%20of%20distributions%20that%20have%20multiple%20modes%2C%20and%0A%5Cemph%7Bpractical%7D%2C%20so%20each%20region%20is%20convex%20and%20can%20be%20easily%20incorporated%20into%0Adownstream%20tasks%2C%20such%20as%20a%20motion%20planner%20using%20conformal%20prediction%20regions.%0AOur%20method%20applies%20to%20general%20supervised%20learning%20tasks%2C%20while%20we%20illustrate%0Aits%20use%20in%20time-series%20prediction.%20We%20provide%20a%20toolbox%20and%20present%0Aillustrative%20case%20studies%20of%20F16%20fighter%20jets%20and%20autonomous%20vehicles%2C%20showing%0Aan%20up%20to%20%2468%5C%25%24%20reduction%20in%20prediction%20region%20area%20compared%20to%20a%20circular%0Abaseline%20region.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07434v2&entry.124074799=Read"},
{"title": "High-Performance Hybrid Algorithm for Minimum Sum-of-Squares Clustering\n  of Infinitely Tall Data", "author": "Ravil Mussabayev and Rustam Mussabayev", "abstract": "  This paper introduces a novel formulation of the clustering problem, namely\nthe Minimum Sum-of-Squares Clustering of Infinitely Tall Data (MSSC-ITD), and\npresents HPClust, an innovative set of hybrid parallel approaches for its\neffective solution. By utilizing modern high-performance computing techniques,\nHPClust enhances key clustering metrics: effectiveness, computational\nefficiency, and scalability. In contrast to vanilla data parallelism, which\nonly accelerates processing time through the MapReduce framework, our approach\nunlocks superior performance by leveraging the multi-strategy\ncompetitive-cooperative parallelism and intricate properties of the objective\nfunction landscape. Unlike other available algorithms that struggle to scale,\nour algorithm is inherently parallel in nature, improving solution quality\nthrough increased scalability and parallelism, and outperforming even advanced\nalgorithms designed for small and medium-sized datasets. Our evaluation of\nHPClust, featuring four parallel strategies, demonstrates its superiority over\ntraditional and cutting-edge methods by offering better performance in the key\nmetrics. These results also show that parallel processing not only enhances the\nclustering efficiency, but the accuracy as well. Additionally, we explore the\nbalance between computational efficiency and clustering quality, providing\ninsights into optimal parallel strategies based on dataset specifics and\nresource availability. This research advances our understanding of parallelism\nin clustering algorithms, demonstrating that a judicious hybridization of\nadvanced parallel approaches yields optimal results for MSSC-ITD. Experiments\non synthetic data further confirm HPClust's exceptional scalability and\nrobustness to noise.\n", "link": "http://arxiv.org/abs/2311.04517v5", "date": "2024-06-25", "relevancy": 2.1396, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4296}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4283}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data&body=Title%3A%20High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data%0AAuthor%3A%20Ravil%20Mussabayev%20and%20Rustam%20Mussabayev%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20formulation%20of%20the%20clustering%20problem%2C%20namely%0Athe%20Minimum%20Sum-of-Squares%20Clustering%20of%20Infinitely%20Tall%20Data%20%28MSSC-ITD%29%2C%20and%0Apresents%20HPClust%2C%20an%20innovative%20set%20of%20hybrid%20parallel%20approaches%20for%20its%0Aeffective%20solution.%20By%20utilizing%20modern%20high-performance%20computing%20techniques%2C%0AHPClust%20enhances%20key%20clustering%20metrics%3A%20effectiveness%2C%20computational%0Aefficiency%2C%20and%20scalability.%20In%20contrast%20to%20vanilla%20data%20parallelism%2C%20which%0Aonly%20accelerates%20processing%20time%20through%20the%20MapReduce%20framework%2C%20our%20approach%0Aunlocks%20superior%20performance%20by%20leveraging%20the%20multi-strategy%0Acompetitive-cooperative%20parallelism%20and%20intricate%20properties%20of%20the%20objective%0Afunction%20landscape.%20Unlike%20other%20available%20algorithms%20that%20struggle%20to%20scale%2C%0Aour%20algorithm%20is%20inherently%20parallel%20in%20nature%2C%20improving%20solution%20quality%0Athrough%20increased%20scalability%20and%20parallelism%2C%20and%20outperforming%20even%20advanced%0Aalgorithms%20designed%20for%20small%20and%20medium-sized%20datasets.%20Our%20evaluation%20of%0AHPClust%2C%20featuring%20four%20parallel%20strategies%2C%20demonstrates%20its%20superiority%20over%0Atraditional%20and%20cutting-edge%20methods%20by%20offering%20better%20performance%20in%20the%20key%0Ametrics.%20These%20results%20also%20show%20that%20parallel%20processing%20not%20only%20enhances%20the%0Aclustering%20efficiency%2C%20but%20the%20accuracy%20as%20well.%20Additionally%2C%20we%20explore%20the%0Abalance%20between%20computational%20efficiency%20and%20clustering%20quality%2C%20providing%0Ainsights%20into%20optimal%20parallel%20strategies%20based%20on%20dataset%20specifics%20and%0Aresource%20availability.%20This%20research%20advances%20our%20understanding%20of%20parallelism%0Ain%20clustering%20algorithms%2C%20demonstrating%20that%20a%20judicious%20hybridization%20of%0Aadvanced%20parallel%20approaches%20yields%20optimal%20results%20for%20MSSC-ITD.%20Experiments%0Aon%20synthetic%20data%20further%20confirm%20HPClust%27s%20exceptional%20scalability%20and%0Arobustness%20to%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04517v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Performance%2520Hybrid%2520Algorithm%2520for%2520Minimum%2520Sum-of-Squares%2520Clustering%250A%2520%2520of%2520Infinitely%2520Tall%2520Data%26entry.906535625%3DRavil%2520Mussabayev%2520and%2520Rustam%2520Mussabayev%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520formulation%2520of%2520the%2520clustering%2520problem%252C%2520namely%250Athe%2520Minimum%2520Sum-of-Squares%2520Clustering%2520of%2520Infinitely%2520Tall%2520Data%2520%2528MSSC-ITD%2529%252C%2520and%250Apresents%2520HPClust%252C%2520an%2520innovative%2520set%2520of%2520hybrid%2520parallel%2520approaches%2520for%2520its%250Aeffective%2520solution.%2520By%2520utilizing%2520modern%2520high-performance%2520computing%2520techniques%252C%250AHPClust%2520enhances%2520key%2520clustering%2520metrics%253A%2520effectiveness%252C%2520computational%250Aefficiency%252C%2520and%2520scalability.%2520In%2520contrast%2520to%2520vanilla%2520data%2520parallelism%252C%2520which%250Aonly%2520accelerates%2520processing%2520time%2520through%2520the%2520MapReduce%2520framework%252C%2520our%2520approach%250Aunlocks%2520superior%2520performance%2520by%2520leveraging%2520the%2520multi-strategy%250Acompetitive-cooperative%2520parallelism%2520and%2520intricate%2520properties%2520of%2520the%2520objective%250Afunction%2520landscape.%2520Unlike%2520other%2520available%2520algorithms%2520that%2520struggle%2520to%2520scale%252C%250Aour%2520algorithm%2520is%2520inherently%2520parallel%2520in%2520nature%252C%2520improving%2520solution%2520quality%250Athrough%2520increased%2520scalability%2520and%2520parallelism%252C%2520and%2520outperforming%2520even%2520advanced%250Aalgorithms%2520designed%2520for%2520small%2520and%2520medium-sized%2520datasets.%2520Our%2520evaluation%2520of%250AHPClust%252C%2520featuring%2520four%2520parallel%2520strategies%252C%2520demonstrates%2520its%2520superiority%2520over%250Atraditional%2520and%2520cutting-edge%2520methods%2520by%2520offering%2520better%2520performance%2520in%2520the%2520key%250Ametrics.%2520These%2520results%2520also%2520show%2520that%2520parallel%2520processing%2520not%2520only%2520enhances%2520the%250Aclustering%2520efficiency%252C%2520but%2520the%2520accuracy%2520as%2520well.%2520Additionally%252C%2520we%2520explore%2520the%250Abalance%2520between%2520computational%2520efficiency%2520and%2520clustering%2520quality%252C%2520providing%250Ainsights%2520into%2520optimal%2520parallel%2520strategies%2520based%2520on%2520dataset%2520specifics%2520and%250Aresource%2520availability.%2520This%2520research%2520advances%2520our%2520understanding%2520of%2520parallelism%250Ain%2520clustering%2520algorithms%252C%2520demonstrating%2520that%2520a%2520judicious%2520hybridization%2520of%250Aadvanced%2520parallel%2520approaches%2520yields%2520optimal%2520results%2520for%2520MSSC-ITD.%2520Experiments%250Aon%2520synthetic%2520data%2520further%2520confirm%2520HPClust%2527s%2520exceptional%2520scalability%2520and%250Arobustness%2520to%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04517v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Performance%20Hybrid%20Algorithm%20for%20Minimum%20Sum-of-Squares%20Clustering%0A%20%20of%20Infinitely%20Tall%20Data&entry.906535625=Ravil%20Mussabayev%20and%20Rustam%20Mussabayev&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20formulation%20of%20the%20clustering%20problem%2C%20namely%0Athe%20Minimum%20Sum-of-Squares%20Clustering%20of%20Infinitely%20Tall%20Data%20%28MSSC-ITD%29%2C%20and%0Apresents%20HPClust%2C%20an%20innovative%20set%20of%20hybrid%20parallel%20approaches%20for%20its%0Aeffective%20solution.%20By%20utilizing%20modern%20high-performance%20computing%20techniques%2C%0AHPClust%20enhances%20key%20clustering%20metrics%3A%20effectiveness%2C%20computational%0Aefficiency%2C%20and%20scalability.%20In%20contrast%20to%20vanilla%20data%20parallelism%2C%20which%0Aonly%20accelerates%20processing%20time%20through%20the%20MapReduce%20framework%2C%20our%20approach%0Aunlocks%20superior%20performance%20by%20leveraging%20the%20multi-strategy%0Acompetitive-cooperative%20parallelism%20and%20intricate%20properties%20of%20the%20objective%0Afunction%20landscape.%20Unlike%20other%20available%20algorithms%20that%20struggle%20to%20scale%2C%0Aour%20algorithm%20is%20inherently%20parallel%20in%20nature%2C%20improving%20solution%20quality%0Athrough%20increased%20scalability%20and%20parallelism%2C%20and%20outperforming%20even%20advanced%0Aalgorithms%20designed%20for%20small%20and%20medium-sized%20datasets.%20Our%20evaluation%20of%0AHPClust%2C%20featuring%20four%20parallel%20strategies%2C%20demonstrates%20its%20superiority%20over%0Atraditional%20and%20cutting-edge%20methods%20by%20offering%20better%20performance%20in%20the%20key%0Ametrics.%20These%20results%20also%20show%20that%20parallel%20processing%20not%20only%20enhances%20the%0Aclustering%20efficiency%2C%20but%20the%20accuracy%20as%20well.%20Additionally%2C%20we%20explore%20the%0Abalance%20between%20computational%20efficiency%20and%20clustering%20quality%2C%20providing%0Ainsights%20into%20optimal%20parallel%20strategies%20based%20on%20dataset%20specifics%20and%0Aresource%20availability.%20This%20research%20advances%20our%20understanding%20of%20parallelism%0Ain%20clustering%20algorithms%2C%20demonstrating%20that%20a%20judicious%20hybridization%20of%0Aadvanced%20parallel%20approaches%20yields%20optimal%20results%20for%20MSSC-ITD.%20Experiments%0Aon%20synthetic%20data%20further%20confirm%20HPClust%27s%20exceptional%20scalability%20and%0Arobustness%20to%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04517v5&entry.124074799=Read"},
{"title": "Continuous Urban Change Detection from Satellite Image Time Series with\n  Temporal Feature Refinement and Multi-Task Integration", "author": "Sebastian Hafner and Heng Fang and Hossein Azizpour and Yifang Ban", "abstract": "  Urbanization advances at unprecedented rates, resulting in negative effects\non the environment and human well-being. Remote sensing has the potential to\nmitigate these effects by supporting sustainable development strategies with\naccurate information on urban growth. Deep learning-based methods have achieved\npromising urban change detection results from optical satellite image pairs\nusing convolutional neural networks (ConvNets), transformers, and a multi-task\nlearning setup. However, transformers have not been leveraged for urban change\ndetection with multi-temporal data, i.e., >2 images, and multi-task learning\nmethods lack integration approaches that combine change and segmentation\noutputs. To fill this research gap, we propose a continuous urban change\ndetection method that identifies changes in each consecutive image pair of a\nsatellite image time series. Specifically, we propose a temporal feature\nrefinement (TFR) module that utilizes self-attention to improve ConvNet-based\nmulti-temporal building representations. Furthermore, we propose a multi-task\nintegration (MTI) module that utilizes Markov networks to find an optimal\nbuilding map time series based on segmentation and dense change outputs. The\nproposed method effectively identifies urban changes based on high-resolution\nsatellite image time series acquired by the PlanetScope constellation (F1 score\n0.551) and Gaofen-2 (F1 score 0.440). Moreover, our experiments on two\nchallenging datasets demonstrate the effectiveness of the proposed method\ncompared to bi-temporal and multi-temporal urban change detection and\nsegmentation methods.\n", "link": "http://arxiv.org/abs/2406.17458v1", "date": "2024-06-25", "relevancy": 2.1325, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5433}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Urban%20Change%20Detection%20from%20Satellite%20Image%20Time%20Series%20with%0A%20%20Temporal%20Feature%20Refinement%20and%20Multi-Task%20Integration&body=Title%3A%20Continuous%20Urban%20Change%20Detection%20from%20Satellite%20Image%20Time%20Series%20with%0A%20%20Temporal%20Feature%20Refinement%20and%20Multi-Task%20Integration%0AAuthor%3A%20Sebastian%20Hafner%20and%20Heng%20Fang%20and%20Hossein%20Azizpour%20and%20Yifang%20Ban%0AAbstract%3A%20%20%20Urbanization%20advances%20at%20unprecedented%20rates%2C%20resulting%20in%20negative%20effects%0Aon%20the%20environment%20and%20human%20well-being.%20Remote%20sensing%20has%20the%20potential%20to%0Amitigate%20these%20effects%20by%20supporting%20sustainable%20development%20strategies%20with%0Aaccurate%20information%20on%20urban%20growth.%20Deep%20learning-based%20methods%20have%20achieved%0Apromising%20urban%20change%20detection%20results%20from%20optical%20satellite%20image%20pairs%0Ausing%20convolutional%20neural%20networks%20%28ConvNets%29%2C%20transformers%2C%20and%20a%20multi-task%0Alearning%20setup.%20However%2C%20transformers%20have%20not%20been%20leveraged%20for%20urban%20change%0Adetection%20with%20multi-temporal%20data%2C%20i.e.%2C%20%3E2%20images%2C%20and%20multi-task%20learning%0Amethods%20lack%20integration%20approaches%20that%20combine%20change%20and%20segmentation%0Aoutputs.%20To%20fill%20this%20research%20gap%2C%20we%20propose%20a%20continuous%20urban%20change%0Adetection%20method%20that%20identifies%20changes%20in%20each%20consecutive%20image%20pair%20of%20a%0Asatellite%20image%20time%20series.%20Specifically%2C%20we%20propose%20a%20temporal%20feature%0Arefinement%20%28TFR%29%20module%20that%20utilizes%20self-attention%20to%20improve%20ConvNet-based%0Amulti-temporal%20building%20representations.%20Furthermore%2C%20we%20propose%20a%20multi-task%0Aintegration%20%28MTI%29%20module%20that%20utilizes%20Markov%20networks%20to%20find%20an%20optimal%0Abuilding%20map%20time%20series%20based%20on%20segmentation%20and%20dense%20change%20outputs.%20The%0Aproposed%20method%20effectively%20identifies%20urban%20changes%20based%20on%20high-resolution%0Asatellite%20image%20time%20series%20acquired%20by%20the%20PlanetScope%20constellation%20%28F1%20score%0A0.551%29%20and%20Gaofen-2%20%28F1%20score%200.440%29.%20Moreover%2C%20our%20experiments%20on%20two%0Achallenging%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%0Acompared%20to%20bi-temporal%20and%20multi-temporal%20urban%20change%20detection%20and%0Asegmentation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Urban%2520Change%2520Detection%2520from%2520Satellite%2520Image%2520Time%2520Series%2520with%250A%2520%2520Temporal%2520Feature%2520Refinement%2520and%2520Multi-Task%2520Integration%26entry.906535625%3DSebastian%2520Hafner%2520and%2520Heng%2520Fang%2520and%2520Hossein%2520Azizpour%2520and%2520Yifang%2520Ban%26entry.1292438233%3D%2520%2520Urbanization%2520advances%2520at%2520unprecedented%2520rates%252C%2520resulting%2520in%2520negative%2520effects%250Aon%2520the%2520environment%2520and%2520human%2520well-being.%2520Remote%2520sensing%2520has%2520the%2520potential%2520to%250Amitigate%2520these%2520effects%2520by%2520supporting%2520sustainable%2520development%2520strategies%2520with%250Aaccurate%2520information%2520on%2520urban%2520growth.%2520Deep%2520learning-based%2520methods%2520have%2520achieved%250Apromising%2520urban%2520change%2520detection%2520results%2520from%2520optical%2520satellite%2520image%2520pairs%250Ausing%2520convolutional%2520neural%2520networks%2520%2528ConvNets%2529%252C%2520transformers%252C%2520and%2520a%2520multi-task%250Alearning%2520setup.%2520However%252C%2520transformers%2520have%2520not%2520been%2520leveraged%2520for%2520urban%2520change%250Adetection%2520with%2520multi-temporal%2520data%252C%2520i.e.%252C%2520%253E2%2520images%252C%2520and%2520multi-task%2520learning%250Amethods%2520lack%2520integration%2520approaches%2520that%2520combine%2520change%2520and%2520segmentation%250Aoutputs.%2520To%2520fill%2520this%2520research%2520gap%252C%2520we%2520propose%2520a%2520continuous%2520urban%2520change%250Adetection%2520method%2520that%2520identifies%2520changes%2520in%2520each%2520consecutive%2520image%2520pair%2520of%2520a%250Asatellite%2520image%2520time%2520series.%2520Specifically%252C%2520we%2520propose%2520a%2520temporal%2520feature%250Arefinement%2520%2528TFR%2529%2520module%2520that%2520utilizes%2520self-attention%2520to%2520improve%2520ConvNet-based%250Amulti-temporal%2520building%2520representations.%2520Furthermore%252C%2520we%2520propose%2520a%2520multi-task%250Aintegration%2520%2528MTI%2529%2520module%2520that%2520utilizes%2520Markov%2520networks%2520to%2520find%2520an%2520optimal%250Abuilding%2520map%2520time%2520series%2520based%2520on%2520segmentation%2520and%2520dense%2520change%2520outputs.%2520The%250Aproposed%2520method%2520effectively%2520identifies%2520urban%2520changes%2520based%2520on%2520high-resolution%250Asatellite%2520image%2520time%2520series%2520acquired%2520by%2520the%2520PlanetScope%2520constellation%2520%2528F1%2520score%250A0.551%2529%2520and%2520Gaofen-2%2520%2528F1%2520score%25200.440%2529.%2520Moreover%252C%2520our%2520experiments%2520on%2520two%250Achallenging%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%250Acompared%2520to%2520bi-temporal%2520and%2520multi-temporal%2520urban%2520change%2520detection%2520and%250Asegmentation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Urban%20Change%20Detection%20from%20Satellite%20Image%20Time%20Series%20with%0A%20%20Temporal%20Feature%20Refinement%20and%20Multi-Task%20Integration&entry.906535625=Sebastian%20Hafner%20and%20Heng%20Fang%20and%20Hossein%20Azizpour%20and%20Yifang%20Ban&entry.1292438233=%20%20Urbanization%20advances%20at%20unprecedented%20rates%2C%20resulting%20in%20negative%20effects%0Aon%20the%20environment%20and%20human%20well-being.%20Remote%20sensing%20has%20the%20potential%20to%0Amitigate%20these%20effects%20by%20supporting%20sustainable%20development%20strategies%20with%0Aaccurate%20information%20on%20urban%20growth.%20Deep%20learning-based%20methods%20have%20achieved%0Apromising%20urban%20change%20detection%20results%20from%20optical%20satellite%20image%20pairs%0Ausing%20convolutional%20neural%20networks%20%28ConvNets%29%2C%20transformers%2C%20and%20a%20multi-task%0Alearning%20setup.%20However%2C%20transformers%20have%20not%20been%20leveraged%20for%20urban%20change%0Adetection%20with%20multi-temporal%20data%2C%20i.e.%2C%20%3E2%20images%2C%20and%20multi-task%20learning%0Amethods%20lack%20integration%20approaches%20that%20combine%20change%20and%20segmentation%0Aoutputs.%20To%20fill%20this%20research%20gap%2C%20we%20propose%20a%20continuous%20urban%20change%0Adetection%20method%20that%20identifies%20changes%20in%20each%20consecutive%20image%20pair%20of%20a%0Asatellite%20image%20time%20series.%20Specifically%2C%20we%20propose%20a%20temporal%20feature%0Arefinement%20%28TFR%29%20module%20that%20utilizes%20self-attention%20to%20improve%20ConvNet-based%0Amulti-temporal%20building%20representations.%20Furthermore%2C%20we%20propose%20a%20multi-task%0Aintegration%20%28MTI%29%20module%20that%20utilizes%20Markov%20networks%20to%20find%20an%20optimal%0Abuilding%20map%20time%20series%20based%20on%20segmentation%20and%20dense%20change%20outputs.%20The%0Aproposed%20method%20effectively%20identifies%20urban%20changes%20based%20on%20high-resolution%0Asatellite%20image%20time%20series%20acquired%20by%20the%20PlanetScope%20constellation%20%28F1%20score%0A0.551%29%20and%20Gaofen-2%20%28F1%20score%200.440%29.%20Moreover%2C%20our%20experiments%20on%20two%0Achallenging%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%0Acompared%20to%20bi-temporal%20and%20multi-temporal%20urban%20change%20detection%20and%0Asegmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17458v1&entry.124074799=Read"},
{"title": "BayTTA: Uncertainty-aware medical image classification with optimized\n  test-time augmentation using Bayesian model averaging", "author": "Zeinab Sherkatghanad and Moloud Abdar and Mohammadreza Bakhtyari and Vladimir Makarenkov", "abstract": "  Test-time augmentation (TTA) is a well-known technique employed during the\ntesting phase of computer vision tasks. It involves aggregating multiple\naugmented versions of input data. Combining predictions using a simple average\nformulation is a common and straightforward approach after performing TTA. This\npaper introduces a novel framework for optimizing TTA, called BayTTA\n(Bayesian-based TTA), which is based on Bayesian Model Averaging (BMA). First,\nwe generate a model list associated with different variations of the input data\ncreated through TTA. Then, we use BMA to combine model predictions weighted by\ntheir respective posterior probabilities. Such an approach allows one to take\ninto account model uncertainty, and thus to enhance the predictive performance\nof the related machine learning or deep learning model. We evaluate the\nperformance of BayTTA on various public data, including three medical image\ndatasets comprising skin cancer, breast cancer, and chest X-ray images and two\nwell-known gene editing datasets, CRISPOR and GUIDE-seq. Our experimental\nresults indicate that BayTTA can be effectively integrated into\nstate-of-the-art deep learning models used in medical image analysis as well as\ninto some popular pre-trained CNN models such as VGG-16, MobileNetV2,\nDenseNet201, ResNet152V2, and InceptionRes-NetV2, leading to the enhancement in\ntheir accuracy and robustness performance.\n", "link": "http://arxiv.org/abs/2406.17640v1", "date": "2024-06-25", "relevancy": 2.1265, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5561}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayTTA%3A%20Uncertainty-aware%20medical%20image%20classification%20with%20optimized%0A%20%20test-time%20augmentation%20using%20Bayesian%20model%20averaging&body=Title%3A%20BayTTA%3A%20Uncertainty-aware%20medical%20image%20classification%20with%20optimized%0A%20%20test-time%20augmentation%20using%20Bayesian%20model%20averaging%0AAuthor%3A%20Zeinab%20Sherkatghanad%20and%20Moloud%20Abdar%20and%20Mohammadreza%20Bakhtyari%20and%20Vladimir%20Makarenkov%0AAbstract%3A%20%20%20Test-time%20augmentation%20%28TTA%29%20is%20a%20well-known%20technique%20employed%20during%20the%0Atesting%20phase%20of%20computer%20vision%20tasks.%20It%20involves%20aggregating%20multiple%0Aaugmented%20versions%20of%20input%20data.%20Combining%20predictions%20using%20a%20simple%20average%0Aformulation%20is%20a%20common%20and%20straightforward%20approach%20after%20performing%20TTA.%20This%0Apaper%20introduces%20a%20novel%20framework%20for%20optimizing%20TTA%2C%20called%20BayTTA%0A%28Bayesian-based%20TTA%29%2C%20which%20is%20based%20on%20Bayesian%20Model%20Averaging%20%28BMA%29.%20First%2C%0Awe%20generate%20a%20model%20list%20associated%20with%20different%20variations%20of%20the%20input%20data%0Acreated%20through%20TTA.%20Then%2C%20we%20use%20BMA%20to%20combine%20model%20predictions%20weighted%20by%0Atheir%20respective%20posterior%20probabilities.%20Such%20an%20approach%20allows%20one%20to%20take%0Ainto%20account%20model%20uncertainty%2C%20and%20thus%20to%20enhance%20the%20predictive%20performance%0Aof%20the%20related%20machine%20learning%20or%20deep%20learning%20model.%20We%20evaluate%20the%0Aperformance%20of%20BayTTA%20on%20various%20public%20data%2C%20including%20three%20medical%20image%0Adatasets%20comprising%20skin%20cancer%2C%20breast%20cancer%2C%20and%20chest%20X-ray%20images%20and%20two%0Awell-known%20gene%20editing%20datasets%2C%20CRISPOR%20and%20GUIDE-seq.%20Our%20experimental%0Aresults%20indicate%20that%20BayTTA%20can%20be%20effectively%20integrated%20into%0Astate-of-the-art%20deep%20learning%20models%20used%20in%20medical%20image%20analysis%20as%20well%20as%0Ainto%20some%20popular%20pre-trained%20CNN%20models%20such%20as%20VGG-16%2C%20MobileNetV2%2C%0ADenseNet201%2C%20ResNet152V2%2C%20and%20InceptionRes-NetV2%2C%20leading%20to%20the%20enhancement%20in%0Atheir%20accuracy%20and%20robustness%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayTTA%253A%2520Uncertainty-aware%2520medical%2520image%2520classification%2520with%2520optimized%250A%2520%2520test-time%2520augmentation%2520using%2520Bayesian%2520model%2520averaging%26entry.906535625%3DZeinab%2520Sherkatghanad%2520and%2520Moloud%2520Abdar%2520and%2520Mohammadreza%2520Bakhtyari%2520and%2520Vladimir%2520Makarenkov%26entry.1292438233%3D%2520%2520Test-time%2520augmentation%2520%2528TTA%2529%2520is%2520a%2520well-known%2520technique%2520employed%2520during%2520the%250Atesting%2520phase%2520of%2520computer%2520vision%2520tasks.%2520It%2520involves%2520aggregating%2520multiple%250Aaugmented%2520versions%2520of%2520input%2520data.%2520Combining%2520predictions%2520using%2520a%2520simple%2520average%250Aformulation%2520is%2520a%2520common%2520and%2520straightforward%2520approach%2520after%2520performing%2520TTA.%2520This%250Apaper%2520introduces%2520a%2520novel%2520framework%2520for%2520optimizing%2520TTA%252C%2520called%2520BayTTA%250A%2528Bayesian-based%2520TTA%2529%252C%2520which%2520is%2520based%2520on%2520Bayesian%2520Model%2520Averaging%2520%2528BMA%2529.%2520First%252C%250Awe%2520generate%2520a%2520model%2520list%2520associated%2520with%2520different%2520variations%2520of%2520the%2520input%2520data%250Acreated%2520through%2520TTA.%2520Then%252C%2520we%2520use%2520BMA%2520to%2520combine%2520model%2520predictions%2520weighted%2520by%250Atheir%2520respective%2520posterior%2520probabilities.%2520Such%2520an%2520approach%2520allows%2520one%2520to%2520take%250Ainto%2520account%2520model%2520uncertainty%252C%2520and%2520thus%2520to%2520enhance%2520the%2520predictive%2520performance%250Aof%2520the%2520related%2520machine%2520learning%2520or%2520deep%2520learning%2520model.%2520We%2520evaluate%2520the%250Aperformance%2520of%2520BayTTA%2520on%2520various%2520public%2520data%252C%2520including%2520three%2520medical%2520image%250Adatasets%2520comprising%2520skin%2520cancer%252C%2520breast%2520cancer%252C%2520and%2520chest%2520X-ray%2520images%2520and%2520two%250Awell-known%2520gene%2520editing%2520datasets%252C%2520CRISPOR%2520and%2520GUIDE-seq.%2520Our%2520experimental%250Aresults%2520indicate%2520that%2520BayTTA%2520can%2520be%2520effectively%2520integrated%2520into%250Astate-of-the-art%2520deep%2520learning%2520models%2520used%2520in%2520medical%2520image%2520analysis%2520as%2520well%2520as%250Ainto%2520some%2520popular%2520pre-trained%2520CNN%2520models%2520such%2520as%2520VGG-16%252C%2520MobileNetV2%252C%250ADenseNet201%252C%2520ResNet152V2%252C%2520and%2520InceptionRes-NetV2%252C%2520leading%2520to%2520the%2520enhancement%2520in%250Atheir%2520accuracy%2520and%2520robustness%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayTTA%3A%20Uncertainty-aware%20medical%20image%20classification%20with%20optimized%0A%20%20test-time%20augmentation%20using%20Bayesian%20model%20averaging&entry.906535625=Zeinab%20Sherkatghanad%20and%20Moloud%20Abdar%20and%20Mohammadreza%20Bakhtyari%20and%20Vladimir%20Makarenkov&entry.1292438233=%20%20Test-time%20augmentation%20%28TTA%29%20is%20a%20well-known%20technique%20employed%20during%20the%0Atesting%20phase%20of%20computer%20vision%20tasks.%20It%20involves%20aggregating%20multiple%0Aaugmented%20versions%20of%20input%20data.%20Combining%20predictions%20using%20a%20simple%20average%0Aformulation%20is%20a%20common%20and%20straightforward%20approach%20after%20performing%20TTA.%20This%0Apaper%20introduces%20a%20novel%20framework%20for%20optimizing%20TTA%2C%20called%20BayTTA%0A%28Bayesian-based%20TTA%29%2C%20which%20is%20based%20on%20Bayesian%20Model%20Averaging%20%28BMA%29.%20First%2C%0Awe%20generate%20a%20model%20list%20associated%20with%20different%20variations%20of%20the%20input%20data%0Acreated%20through%20TTA.%20Then%2C%20we%20use%20BMA%20to%20combine%20model%20predictions%20weighted%20by%0Atheir%20respective%20posterior%20probabilities.%20Such%20an%20approach%20allows%20one%20to%20take%0Ainto%20account%20model%20uncertainty%2C%20and%20thus%20to%20enhance%20the%20predictive%20performance%0Aof%20the%20related%20machine%20learning%20or%20deep%20learning%20model.%20We%20evaluate%20the%0Aperformance%20of%20BayTTA%20on%20various%20public%20data%2C%20including%20three%20medical%20image%0Adatasets%20comprising%20skin%20cancer%2C%20breast%20cancer%2C%20and%20chest%20X-ray%20images%20and%20two%0Awell-known%20gene%20editing%20datasets%2C%20CRISPOR%20and%20GUIDE-seq.%20Our%20experimental%0Aresults%20indicate%20that%20BayTTA%20can%20be%20effectively%20integrated%20into%0Astate-of-the-art%20deep%20learning%20models%20used%20in%20medical%20image%20analysis%20as%20well%20as%0Ainto%20some%20popular%20pre-trained%20CNN%20models%20such%20as%20VGG-16%2C%20MobileNetV2%2C%0ADenseNet201%2C%20ResNet152V2%2C%20and%20InceptionRes-NetV2%2C%20leading%20to%20the%20enhancement%20in%0Atheir%20accuracy%20and%20robustness%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17640v1&entry.124074799=Read"},
{"title": "Benchmarking SLAM Algorithms in the Cloud: The SLAM Hive System", "author": "Xinzhe Liu and Yuanyuan Yang and Bowen Xu and S\u00f6ren Schwertfeger", "abstract": "  Evaluating the performance of Simultaneous Localization and Mapping (SLAM)\nalgorithms is essential for scientists and users of robotic systems alike. But\nthere are a multitude different permutations of possible options of hardware\nsetups and algorithm configurations, as well as different datasets and\nalgorithms, such that it is infeasible to thoroughly compare SLAM systems\nagainst the full state of the art. To solve that we present the SLAM Hive\nBenchmarking Suite, which is able to analyze SLAM algorithms in thousands of\nmapping runs, through its utilization of container technology and deployment in\nthe cloud. This paper presents the architecture and open source implementation\nof SLAM Hive and compares it to existing efforts on SLAM evaluation. We perform\nmapping runs of many of the most popular visual and LiDAR based SLAM algorithms\nagainst commonly used datasets and show how SLAM Hive and then be used to\nconveniently analyze the results against various aspects. Through this we\nenvision that SLAM Hive can become an essential tool for proper comparisons and\nevaluations of SLAM algorithms and thus drive the scientific development in the\nresearch on SLAM. The open source software as well as a demo to show the live\nanalysis of 100s of mapping runs can be found on our SLAM Hive website.\n", "link": "http://arxiv.org/abs/2406.17586v1", "date": "2024-06-25", "relevancy": 2.1206, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5507}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20SLAM%20Algorithms%20in%20the%20Cloud%3A%20The%20SLAM%20Hive%20System&body=Title%3A%20Benchmarking%20SLAM%20Algorithms%20in%20the%20Cloud%3A%20The%20SLAM%20Hive%20System%0AAuthor%3A%20Xinzhe%20Liu%20and%20Yuanyuan%20Yang%20and%20Bowen%20Xu%20and%20S%C3%B6ren%20Schwertfeger%0AAbstract%3A%20%20%20Evaluating%20the%20performance%20of%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%0Aalgorithms%20is%20essential%20for%20scientists%20and%20users%20of%20robotic%20systems%20alike.%20But%0Athere%20are%20a%20multitude%20different%20permutations%20of%20possible%20options%20of%20hardware%0Asetups%20and%20algorithm%20configurations%2C%20as%20well%20as%20different%20datasets%20and%0Aalgorithms%2C%20such%20that%20it%20is%20infeasible%20to%20thoroughly%20compare%20SLAM%20systems%0Aagainst%20the%20full%20state%20of%20the%20art.%20To%20solve%20that%20we%20present%20the%20SLAM%20Hive%0ABenchmarking%20Suite%2C%20which%20is%20able%20to%20analyze%20SLAM%20algorithms%20in%20thousands%20of%0Amapping%20runs%2C%20through%20its%20utilization%20of%20container%20technology%20and%20deployment%20in%0Athe%20cloud.%20This%20paper%20presents%20the%20architecture%20and%20open%20source%20implementation%0Aof%20SLAM%20Hive%20and%20compares%20it%20to%20existing%20efforts%20on%20SLAM%20evaluation.%20We%20perform%0Amapping%20runs%20of%20many%20of%20the%20most%20popular%20visual%20and%20LiDAR%20based%20SLAM%20algorithms%0Aagainst%20commonly%20used%20datasets%20and%20show%20how%20SLAM%20Hive%20and%20then%20be%20used%20to%0Aconveniently%20analyze%20the%20results%20against%20various%20aspects.%20Through%20this%20we%0Aenvision%20that%20SLAM%20Hive%20can%20become%20an%20essential%20tool%20for%20proper%20comparisons%20and%0Aevaluations%20of%20SLAM%20algorithms%20and%20thus%20drive%20the%20scientific%20development%20in%20the%0Aresearch%20on%20SLAM.%20The%20open%20source%20software%20as%20well%20as%20a%20demo%20to%20show%20the%20live%0Aanalysis%20of%20100s%20of%20mapping%20runs%20can%20be%20found%20on%20our%20SLAM%20Hive%20website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520SLAM%2520Algorithms%2520in%2520the%2520Cloud%253A%2520The%2520SLAM%2520Hive%2520System%26entry.906535625%3DXinzhe%2520Liu%2520and%2520Yuanyuan%2520Yang%2520and%2520Bowen%2520Xu%2520and%2520S%25C3%25B6ren%2520Schwertfeger%26entry.1292438233%3D%2520%2520Evaluating%2520the%2520performance%2520of%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%250Aalgorithms%2520is%2520essential%2520for%2520scientists%2520and%2520users%2520of%2520robotic%2520systems%2520alike.%2520But%250Athere%2520are%2520a%2520multitude%2520different%2520permutations%2520of%2520possible%2520options%2520of%2520hardware%250Asetups%2520and%2520algorithm%2520configurations%252C%2520as%2520well%2520as%2520different%2520datasets%2520and%250Aalgorithms%252C%2520such%2520that%2520it%2520is%2520infeasible%2520to%2520thoroughly%2520compare%2520SLAM%2520systems%250Aagainst%2520the%2520full%2520state%2520of%2520the%2520art.%2520To%2520solve%2520that%2520we%2520present%2520the%2520SLAM%2520Hive%250ABenchmarking%2520Suite%252C%2520which%2520is%2520able%2520to%2520analyze%2520SLAM%2520algorithms%2520in%2520thousands%2520of%250Amapping%2520runs%252C%2520through%2520its%2520utilization%2520of%2520container%2520technology%2520and%2520deployment%2520in%250Athe%2520cloud.%2520This%2520paper%2520presents%2520the%2520architecture%2520and%2520open%2520source%2520implementation%250Aof%2520SLAM%2520Hive%2520and%2520compares%2520it%2520to%2520existing%2520efforts%2520on%2520SLAM%2520evaluation.%2520We%2520perform%250Amapping%2520runs%2520of%2520many%2520of%2520the%2520most%2520popular%2520visual%2520and%2520LiDAR%2520based%2520SLAM%2520algorithms%250Aagainst%2520commonly%2520used%2520datasets%2520and%2520show%2520how%2520SLAM%2520Hive%2520and%2520then%2520be%2520used%2520to%250Aconveniently%2520analyze%2520the%2520results%2520against%2520various%2520aspects.%2520Through%2520this%2520we%250Aenvision%2520that%2520SLAM%2520Hive%2520can%2520become%2520an%2520essential%2520tool%2520for%2520proper%2520comparisons%2520and%250Aevaluations%2520of%2520SLAM%2520algorithms%2520and%2520thus%2520drive%2520the%2520scientific%2520development%2520in%2520the%250Aresearch%2520on%2520SLAM.%2520The%2520open%2520source%2520software%2520as%2520well%2520as%2520a%2520demo%2520to%2520show%2520the%2520live%250Aanalysis%2520of%2520100s%2520of%2520mapping%2520runs%2520can%2520be%2520found%2520on%2520our%2520SLAM%2520Hive%2520website.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20SLAM%20Algorithms%20in%20the%20Cloud%3A%20The%20SLAM%20Hive%20System&entry.906535625=Xinzhe%20Liu%20and%20Yuanyuan%20Yang%20and%20Bowen%20Xu%20and%20S%C3%B6ren%20Schwertfeger&entry.1292438233=%20%20Evaluating%20the%20performance%20of%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%0Aalgorithms%20is%20essential%20for%20scientists%20and%20users%20of%20robotic%20systems%20alike.%20But%0Athere%20are%20a%20multitude%20different%20permutations%20of%20possible%20options%20of%20hardware%0Asetups%20and%20algorithm%20configurations%2C%20as%20well%20as%20different%20datasets%20and%0Aalgorithms%2C%20such%20that%20it%20is%20infeasible%20to%20thoroughly%20compare%20SLAM%20systems%0Aagainst%20the%20full%20state%20of%20the%20art.%20To%20solve%20that%20we%20present%20the%20SLAM%20Hive%0ABenchmarking%20Suite%2C%20which%20is%20able%20to%20analyze%20SLAM%20algorithms%20in%20thousands%20of%0Amapping%20runs%2C%20through%20its%20utilization%20of%20container%20technology%20and%20deployment%20in%0Athe%20cloud.%20This%20paper%20presents%20the%20architecture%20and%20open%20source%20implementation%0Aof%20SLAM%20Hive%20and%20compares%20it%20to%20existing%20efforts%20on%20SLAM%20evaluation.%20We%20perform%0Amapping%20runs%20of%20many%20of%20the%20most%20popular%20visual%20and%20LiDAR%20based%20SLAM%20algorithms%0Aagainst%20commonly%20used%20datasets%20and%20show%20how%20SLAM%20Hive%20and%20then%20be%20used%20to%0Aconveniently%20analyze%20the%20results%20against%20various%20aspects.%20Through%20this%20we%0Aenvision%20that%20SLAM%20Hive%20can%20become%20an%20essential%20tool%20for%20proper%20comparisons%20and%0Aevaluations%20of%20SLAM%20algorithms%20and%20thus%20drive%20the%20scientific%20development%20in%20the%0Aresearch%20on%20SLAM.%20The%20open%20source%20software%20as%20well%20as%20a%20demo%20to%20show%20the%20live%0Aanalysis%20of%20100s%20of%20mapping%20runs%20can%20be%20found%20on%20our%20SLAM%20Hive%20website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17586v1&entry.124074799=Read"},
{"title": "Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse\n  Gradients", "author": "Aashiq Muhamed and Oscar Li and David Woodruff and Mona Diab and Virginia Smith", "abstract": "  Large language model (LLM) training and finetuning are often bottlenecked by\nlimited GPU memory. While existing projection-based optimization methods\naddress this by projecting gradients into a lower-dimensional subspace to\nreduce optimizer state memory, they typically rely on dense projection\nmatrices, which can introduce computational and memory overheads. In this work,\nwe propose Grass (GRAdient Stuctured Sparsification), a novel approach that\nleverages sparse projections to transform gradients into structured sparse\nupdates. This design not only significantly reduces memory usage for optimizer\nstates but also minimizes gradient memory footprint, computation, and\ncommunication costs, leading to substantial throughput improvements. Extensive\nexperiments on pretraining and finetuning tasks demonstrate that Grass achieves\ncompetitive performance to full-rank training and existing projection-based\nmethods. Notably, Grass enables half-precision pretraining of a 13B parameter\nLLaMA model on a single 40GB A100 GPU--a feat infeasible for previous\nmethods--and yields up to a $2\\times$ throughput improvement on an 8-GPU\nsystem. Code can be found at https://github.com/aashiqmuhamed/GRASS .\n", "link": "http://arxiv.org/abs/2406.17660v1", "date": "2024-06-25", "relevancy": 2.1196, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5356}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5326}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grass%3A%20Compute%20Efficient%20Low-Memory%20LLM%20Training%20with%20Structured%20Sparse%0A%20%20Gradients&body=Title%3A%20Grass%3A%20Compute%20Efficient%20Low-Memory%20LLM%20Training%20with%20Structured%20Sparse%0A%20%20Gradients%0AAuthor%3A%20Aashiq%20Muhamed%20and%20Oscar%20Li%20and%20David%20Woodruff%20and%20Mona%20Diab%20and%20Virginia%20Smith%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20training%20and%20finetuning%20are%20often%20bottlenecked%20by%0Alimited%20GPU%20memory.%20While%20existing%20projection-based%20optimization%20methods%0Aaddress%20this%20by%20projecting%20gradients%20into%20a%20lower-dimensional%20subspace%20to%0Areduce%20optimizer%20state%20memory%2C%20they%20typically%20rely%20on%20dense%20projection%0Amatrices%2C%20which%20can%20introduce%20computational%20and%20memory%20overheads.%20In%20this%20work%2C%0Awe%20propose%20Grass%20%28GRAdient%20Stuctured%20Sparsification%29%2C%20a%20novel%20approach%20that%0Aleverages%20sparse%20projections%20to%20transform%20gradients%20into%20structured%20sparse%0Aupdates.%20This%20design%20not%20only%20significantly%20reduces%20memory%20usage%20for%20optimizer%0Astates%20but%20also%20minimizes%20gradient%20memory%20footprint%2C%20computation%2C%20and%0Acommunication%20costs%2C%20leading%20to%20substantial%20throughput%20improvements.%20Extensive%0Aexperiments%20on%20pretraining%20and%20finetuning%20tasks%20demonstrate%20that%20Grass%20achieves%0Acompetitive%20performance%20to%20full-rank%20training%20and%20existing%20projection-based%0Amethods.%20Notably%2C%20Grass%20enables%20half-precision%20pretraining%20of%20a%2013B%20parameter%0ALLaMA%20model%20on%20a%20single%2040GB%20A100%20GPU--a%20feat%20infeasible%20for%20previous%0Amethods--and%20yields%20up%20to%20a%20%242%5Ctimes%24%20throughput%20improvement%20on%20an%208-GPU%0Asystem.%20Code%20can%20be%20found%20at%20https%3A//github.com/aashiqmuhamed/GRASS%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrass%253A%2520Compute%2520Efficient%2520Low-Memory%2520LLM%2520Training%2520with%2520Structured%2520Sparse%250A%2520%2520Gradients%26entry.906535625%3DAashiq%2520Muhamed%2520and%2520Oscar%2520Li%2520and%2520David%2520Woodruff%2520and%2520Mona%2520Diab%2520and%2520Virginia%2520Smith%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520training%2520and%2520finetuning%2520are%2520often%2520bottlenecked%2520by%250Alimited%2520GPU%2520memory.%2520While%2520existing%2520projection-based%2520optimization%2520methods%250Aaddress%2520this%2520by%2520projecting%2520gradients%2520into%2520a%2520lower-dimensional%2520subspace%2520to%250Areduce%2520optimizer%2520state%2520memory%252C%2520they%2520typically%2520rely%2520on%2520dense%2520projection%250Amatrices%252C%2520which%2520can%2520introduce%2520computational%2520and%2520memory%2520overheads.%2520In%2520this%2520work%252C%250Awe%2520propose%2520Grass%2520%2528GRAdient%2520Stuctured%2520Sparsification%2529%252C%2520a%2520novel%2520approach%2520that%250Aleverages%2520sparse%2520projections%2520to%2520transform%2520gradients%2520into%2520structured%2520sparse%250Aupdates.%2520This%2520design%2520not%2520only%2520significantly%2520reduces%2520memory%2520usage%2520for%2520optimizer%250Astates%2520but%2520also%2520minimizes%2520gradient%2520memory%2520footprint%252C%2520computation%252C%2520and%250Acommunication%2520costs%252C%2520leading%2520to%2520substantial%2520throughput%2520improvements.%2520Extensive%250Aexperiments%2520on%2520pretraining%2520and%2520finetuning%2520tasks%2520demonstrate%2520that%2520Grass%2520achieves%250Acompetitive%2520performance%2520to%2520full-rank%2520training%2520and%2520existing%2520projection-based%250Amethods.%2520Notably%252C%2520Grass%2520enables%2520half-precision%2520pretraining%2520of%2520a%252013B%2520parameter%250ALLaMA%2520model%2520on%2520a%2520single%252040GB%2520A100%2520GPU--a%2520feat%2520infeasible%2520for%2520previous%250Amethods--and%2520yields%2520up%2520to%2520a%2520%25242%255Ctimes%2524%2520throughput%2520improvement%2520on%2520an%25208-GPU%250Asystem.%2520Code%2520can%2520be%2520found%2520at%2520https%253A//github.com/aashiqmuhamed/GRASS%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grass%3A%20Compute%20Efficient%20Low-Memory%20LLM%20Training%20with%20Structured%20Sparse%0A%20%20Gradients&entry.906535625=Aashiq%20Muhamed%20and%20Oscar%20Li%20and%20David%20Woodruff%20and%20Mona%20Diab%20and%20Virginia%20Smith&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20training%20and%20finetuning%20are%20often%20bottlenecked%20by%0Alimited%20GPU%20memory.%20While%20existing%20projection-based%20optimization%20methods%0Aaddress%20this%20by%20projecting%20gradients%20into%20a%20lower-dimensional%20subspace%20to%0Areduce%20optimizer%20state%20memory%2C%20they%20typically%20rely%20on%20dense%20projection%0Amatrices%2C%20which%20can%20introduce%20computational%20and%20memory%20overheads.%20In%20this%20work%2C%0Awe%20propose%20Grass%20%28GRAdient%20Stuctured%20Sparsification%29%2C%20a%20novel%20approach%20that%0Aleverages%20sparse%20projections%20to%20transform%20gradients%20into%20structured%20sparse%0Aupdates.%20This%20design%20not%20only%20significantly%20reduces%20memory%20usage%20for%20optimizer%0Astates%20but%20also%20minimizes%20gradient%20memory%20footprint%2C%20computation%2C%20and%0Acommunication%20costs%2C%20leading%20to%20substantial%20throughput%20improvements.%20Extensive%0Aexperiments%20on%20pretraining%20and%20finetuning%20tasks%20demonstrate%20that%20Grass%20achieves%0Acompetitive%20performance%20to%20full-rank%20training%20and%20existing%20projection-based%0Amethods.%20Notably%2C%20Grass%20enables%20half-precision%20pretraining%20of%20a%2013B%20parameter%0ALLaMA%20model%20on%20a%20single%2040GB%20A100%20GPU--a%20feat%20infeasible%20for%20previous%0Amethods--and%20yields%20up%20to%20a%20%242%5Ctimes%24%20throughput%20improvement%20on%20an%208-GPU%0Asystem.%20Code%20can%20be%20found%20at%20https%3A//github.com/aashiqmuhamed/GRASS%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17660v1&entry.124074799=Read"},
{"title": "When does Self-Prediction help? Understanding Auxiliary Tasks in\n  Reinforcement Learning", "author": "Claas Voelcker and Tyler Kastner and Igor Gilitschenski and Amir-massoud Farahmand", "abstract": "  We investigate the impact of auxiliary learning tasks such as observation\nreconstruction and latent self-prediction on the representation learning\nproblem in reinforcement learning. We also study how they interact with\ndistractions and observation functions in the MDP. We provide a theoretical\nanalysis of the learning dynamics of observation reconstruction, latent\nself-prediction, and TD learning in the presence of distractions and\nobservation functions under linear model assumptions. With this formalization,\nwe are able to explain why latent-self prediction is a helpful \\emph{auxiliary\ntask}, while observation reconstruction can provide more useful features when\nused in isolation. Our empirical analysis shows that the insights obtained from\nour learning dynamics framework predicts the behavior of these loss functions\nbeyond the linear model assumption in non-linear neural networks. This\nreinforces the usefulness of the linear model framework not only for\ntheoretical analysis, but also practical benefit for applied problems.\n", "link": "http://arxiv.org/abs/2406.17718v1", "date": "2024-06-25", "relevancy": 2.1152, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5515}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5424}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20does%20Self-Prediction%20help%3F%20Understanding%20Auxiliary%20Tasks%20in%0A%20%20Reinforcement%20Learning&body=Title%3A%20When%20does%20Self-Prediction%20help%3F%20Understanding%20Auxiliary%20Tasks%20in%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Claas%20Voelcker%20and%20Tyler%20Kastner%20and%20Igor%20Gilitschenski%20and%20Amir-massoud%20Farahmand%0AAbstract%3A%20%20%20We%20investigate%20the%20impact%20of%20auxiliary%20learning%20tasks%20such%20as%20observation%0Areconstruction%20and%20latent%20self-prediction%20on%20the%20representation%20learning%0Aproblem%20in%20reinforcement%20learning.%20We%20also%20study%20how%20they%20interact%20with%0Adistractions%20and%20observation%20functions%20in%20the%20MDP.%20We%20provide%20a%20theoretical%0Aanalysis%20of%20the%20learning%20dynamics%20of%20observation%20reconstruction%2C%20latent%0Aself-prediction%2C%20and%20TD%20learning%20in%20the%20presence%20of%20distractions%20and%0Aobservation%20functions%20under%20linear%20model%20assumptions.%20With%20this%20formalization%2C%0Awe%20are%20able%20to%20explain%20why%20latent-self%20prediction%20is%20a%20helpful%20%5Cemph%7Bauxiliary%0Atask%7D%2C%20while%20observation%20reconstruction%20can%20provide%20more%20useful%20features%20when%0Aused%20in%20isolation.%20Our%20empirical%20analysis%20shows%20that%20the%20insights%20obtained%20from%0Aour%20learning%20dynamics%20framework%20predicts%20the%20behavior%20of%20these%20loss%20functions%0Abeyond%20the%20linear%20model%20assumption%20in%20non-linear%20neural%20networks.%20This%0Areinforces%20the%20usefulness%20of%20the%20linear%20model%20framework%20not%20only%20for%0Atheoretical%20analysis%2C%20but%20also%20practical%20benefit%20for%20applied%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520does%2520Self-Prediction%2520help%253F%2520Understanding%2520Auxiliary%2520Tasks%2520in%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DClaas%2520Voelcker%2520and%2520Tyler%2520Kastner%2520and%2520Igor%2520Gilitschenski%2520and%2520Amir-massoud%2520Farahmand%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520impact%2520of%2520auxiliary%2520learning%2520tasks%2520such%2520as%2520observation%250Areconstruction%2520and%2520latent%2520self-prediction%2520on%2520the%2520representation%2520learning%250Aproblem%2520in%2520reinforcement%2520learning.%2520We%2520also%2520study%2520how%2520they%2520interact%2520with%250Adistractions%2520and%2520observation%2520functions%2520in%2520the%2520MDP.%2520We%2520provide%2520a%2520theoretical%250Aanalysis%2520of%2520the%2520learning%2520dynamics%2520of%2520observation%2520reconstruction%252C%2520latent%250Aself-prediction%252C%2520and%2520TD%2520learning%2520in%2520the%2520presence%2520of%2520distractions%2520and%250Aobservation%2520functions%2520under%2520linear%2520model%2520assumptions.%2520With%2520this%2520formalization%252C%250Awe%2520are%2520able%2520to%2520explain%2520why%2520latent-self%2520prediction%2520is%2520a%2520helpful%2520%255Cemph%257Bauxiliary%250Atask%257D%252C%2520while%2520observation%2520reconstruction%2520can%2520provide%2520more%2520useful%2520features%2520when%250Aused%2520in%2520isolation.%2520Our%2520empirical%2520analysis%2520shows%2520that%2520the%2520insights%2520obtained%2520from%250Aour%2520learning%2520dynamics%2520framework%2520predicts%2520the%2520behavior%2520of%2520these%2520loss%2520functions%250Abeyond%2520the%2520linear%2520model%2520assumption%2520in%2520non-linear%2520neural%2520networks.%2520This%250Areinforces%2520the%2520usefulness%2520of%2520the%2520linear%2520model%2520framework%2520not%2520only%2520for%250Atheoretical%2520analysis%252C%2520but%2520also%2520practical%2520benefit%2520for%2520applied%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20does%20Self-Prediction%20help%3F%20Understanding%20Auxiliary%20Tasks%20in%0A%20%20Reinforcement%20Learning&entry.906535625=Claas%20Voelcker%20and%20Tyler%20Kastner%20and%20Igor%20Gilitschenski%20and%20Amir-massoud%20Farahmand&entry.1292438233=%20%20We%20investigate%20the%20impact%20of%20auxiliary%20learning%20tasks%20such%20as%20observation%0Areconstruction%20and%20latent%20self-prediction%20on%20the%20representation%20learning%0Aproblem%20in%20reinforcement%20learning.%20We%20also%20study%20how%20they%20interact%20with%0Adistractions%20and%20observation%20functions%20in%20the%20MDP.%20We%20provide%20a%20theoretical%0Aanalysis%20of%20the%20learning%20dynamics%20of%20observation%20reconstruction%2C%20latent%0Aself-prediction%2C%20and%20TD%20learning%20in%20the%20presence%20of%20distractions%20and%0Aobservation%20functions%20under%20linear%20model%20assumptions.%20With%20this%20formalization%2C%0Awe%20are%20able%20to%20explain%20why%20latent-self%20prediction%20is%20a%20helpful%20%5Cemph%7Bauxiliary%0Atask%7D%2C%20while%20observation%20reconstruction%20can%20provide%20more%20useful%20features%20when%0Aused%20in%20isolation.%20Our%20empirical%20analysis%20shows%20that%20the%20insights%20obtained%20from%0Aour%20learning%20dynamics%20framework%20predicts%20the%20behavior%20of%20these%20loss%20functions%0Abeyond%20the%20linear%20model%20assumption%20in%20non-linear%20neural%20networks.%20This%0Areinforces%20the%20usefulness%20of%20the%20linear%20model%20framework%20not%20only%20for%0Atheoretical%20analysis%2C%20but%20also%20practical%20benefit%20for%20applied%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17718v1&entry.124074799=Read"},
{"title": "Fine-grained Prompt Tuning: A Parameter and Memory Efficient Transfer\n  Learning Method for High-resolution Medical Image Classification", "author": "Yijin Huang and Pujin Cheng and Roger Tam and Xiaoying Tang", "abstract": "  Parameter-efficient transfer learning (PETL) is proposed as a cost-effective\nway to transfer pre-trained models to downstream tasks, avoiding the high cost\nof updating entire large-scale pre-trained models (LPMs). In this work, we\npresent Fine-grained Prompt Tuning (FPT), a novel PETL method for medical image\nclassification. FPT significantly reduces memory consumption compared to other\nPETL methods, especially in high-resolution input contexts. To achieve this, we\nfirst freeze the weights of the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM takes high-resolution images as input to extract\nfine-grained features, while the side network is fed low-resolution images to\nreduce memory usage. To allow the side network to access pre-trained knowledge,\nwe introduce fine-grained prompts that summarize information from the LPM\nthrough a fusion module. Important tokens selection and preloading techniques\nare employed to further reduce training cost and memory requirements. We\nevaluate FPT on four medical datasets with varying sizes, modalities, and\ncomplexities. Experimental results demonstrate that FPT achieves comparable\nperformance to fine-tuning the entire LPM while using only 1.8% of the\nlearnable parameters and 13% of the memory costs of an encoder ViT-B model with\na 512 x 512 input resolution.\n", "link": "http://arxiv.org/abs/2403.07576v3", "date": "2024-06-25", "relevancy": 2.0894, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-grained%20Prompt%20Tuning%3A%20A%20Parameter%20and%20Memory%20Efficient%20Transfer%0A%20%20Learning%20Method%20for%20High-resolution%20Medical%20Image%20Classification&body=Title%3A%20Fine-grained%20Prompt%20Tuning%3A%20A%20Parameter%20and%20Memory%20Efficient%20Transfer%0A%20%20Learning%20Method%20for%20High-resolution%20Medical%20Image%20Classification%0AAuthor%3A%20Yijin%20Huang%20and%20Pujin%20Cheng%20and%20Roger%20Tam%20and%20Xiaoying%20Tang%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20is%20proposed%20as%20a%20cost-effective%0Away%20to%20transfer%20pre-trained%20models%20to%20downstream%20tasks%2C%20avoiding%20the%20high%20cost%0Aof%20updating%20entire%20large-scale%20pre-trained%20models%20%28LPMs%29.%20In%20this%20work%2C%20we%0Apresent%20Fine-grained%20Prompt%20Tuning%20%28FPT%29%2C%20a%20novel%20PETL%20method%20for%20medical%20image%0Aclassification.%20FPT%20significantly%20reduces%20memory%20consumption%20compared%20to%20other%0APETL%20methods%2C%20especially%20in%20high-resolution%20input%20contexts.%20To%20achieve%20this%2C%20we%0Afirst%20freeze%20the%20weights%20of%20the%20LPM%20and%20construct%20a%20learnable%20lightweight%20side%0Anetwork.%20The%20frozen%20LPM%20takes%20high-resolution%20images%20as%20input%20to%20extract%0Afine-grained%20features%2C%20while%20the%20side%20network%20is%20fed%20low-resolution%20images%20to%0Areduce%20memory%20usage.%20To%20allow%20the%20side%20network%20to%20access%20pre-trained%20knowledge%2C%0Awe%20introduce%20fine-grained%20prompts%20that%20summarize%20information%20from%20the%20LPM%0Athrough%20a%20fusion%20module.%20Important%20tokens%20selection%20and%20preloading%20techniques%0Aare%20employed%20to%20further%20reduce%20training%20cost%20and%20memory%20requirements.%20We%0Aevaluate%20FPT%20on%20four%20medical%20datasets%20with%20varying%20sizes%2C%20modalities%2C%20and%0Acomplexities.%20Experimental%20results%20demonstrate%20that%20FPT%20achieves%20comparable%0Aperformance%20to%20fine-tuning%20the%20entire%20LPM%20while%20using%20only%201.8%25%20of%20the%0Alearnable%20parameters%20and%2013%25%20of%20the%20memory%20costs%20of%20an%20encoder%20ViT-B%20model%20with%0Aa%20512%20x%20512%20input%20resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07576v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-grained%2520Prompt%2520Tuning%253A%2520A%2520Parameter%2520and%2520Memory%2520Efficient%2520Transfer%250A%2520%2520Learning%2520Method%2520for%2520High-resolution%2520Medical%2520Image%2520Classification%26entry.906535625%3DYijin%2520Huang%2520and%2520Pujin%2520Cheng%2520and%2520Roger%2520Tam%2520and%2520Xiaoying%2520Tang%26entry.1292438233%3D%2520%2520Parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520is%2520proposed%2520as%2520a%2520cost-effective%250Away%2520to%2520transfer%2520pre-trained%2520models%2520to%2520downstream%2520tasks%252C%2520avoiding%2520the%2520high%2520cost%250Aof%2520updating%2520entire%2520large-scale%2520pre-trained%2520models%2520%2528LPMs%2529.%2520In%2520this%2520work%252C%2520we%250Apresent%2520Fine-grained%2520Prompt%2520Tuning%2520%2528FPT%2529%252C%2520a%2520novel%2520PETL%2520method%2520for%2520medical%2520image%250Aclassification.%2520FPT%2520significantly%2520reduces%2520memory%2520consumption%2520compared%2520to%2520other%250APETL%2520methods%252C%2520especially%2520in%2520high-resolution%2520input%2520contexts.%2520To%2520achieve%2520this%252C%2520we%250Afirst%2520freeze%2520the%2520weights%2520of%2520the%2520LPM%2520and%2520construct%2520a%2520learnable%2520lightweight%2520side%250Anetwork.%2520The%2520frozen%2520LPM%2520takes%2520high-resolution%2520images%2520as%2520input%2520to%2520extract%250Afine-grained%2520features%252C%2520while%2520the%2520side%2520network%2520is%2520fed%2520low-resolution%2520images%2520to%250Areduce%2520memory%2520usage.%2520To%2520allow%2520the%2520side%2520network%2520to%2520access%2520pre-trained%2520knowledge%252C%250Awe%2520introduce%2520fine-grained%2520prompts%2520that%2520summarize%2520information%2520from%2520the%2520LPM%250Athrough%2520a%2520fusion%2520module.%2520Important%2520tokens%2520selection%2520and%2520preloading%2520techniques%250Aare%2520employed%2520to%2520further%2520reduce%2520training%2520cost%2520and%2520memory%2520requirements.%2520We%250Aevaluate%2520FPT%2520on%2520four%2520medical%2520datasets%2520with%2520varying%2520sizes%252C%2520modalities%252C%2520and%250Acomplexities.%2520Experimental%2520results%2520demonstrate%2520that%2520FPT%2520achieves%2520comparable%250Aperformance%2520to%2520fine-tuning%2520the%2520entire%2520LPM%2520while%2520using%2520only%25201.8%2525%2520of%2520the%250Alearnable%2520parameters%2520and%252013%2525%2520of%2520the%2520memory%2520costs%2520of%2520an%2520encoder%2520ViT-B%2520model%2520with%250Aa%2520512%2520x%2520512%2520input%2520resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07576v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-grained%20Prompt%20Tuning%3A%20A%20Parameter%20and%20Memory%20Efficient%20Transfer%0A%20%20Learning%20Method%20for%20High-resolution%20Medical%20Image%20Classification&entry.906535625=Yijin%20Huang%20and%20Pujin%20Cheng%20and%20Roger%20Tam%20and%20Xiaoying%20Tang&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20is%20proposed%20as%20a%20cost-effective%0Away%20to%20transfer%20pre-trained%20models%20to%20downstream%20tasks%2C%20avoiding%20the%20high%20cost%0Aof%20updating%20entire%20large-scale%20pre-trained%20models%20%28LPMs%29.%20In%20this%20work%2C%20we%0Apresent%20Fine-grained%20Prompt%20Tuning%20%28FPT%29%2C%20a%20novel%20PETL%20method%20for%20medical%20image%0Aclassification.%20FPT%20significantly%20reduces%20memory%20consumption%20compared%20to%20other%0APETL%20methods%2C%20especially%20in%20high-resolution%20input%20contexts.%20To%20achieve%20this%2C%20we%0Afirst%20freeze%20the%20weights%20of%20the%20LPM%20and%20construct%20a%20learnable%20lightweight%20side%0Anetwork.%20The%20frozen%20LPM%20takes%20high-resolution%20images%20as%20input%20to%20extract%0Afine-grained%20features%2C%20while%20the%20side%20network%20is%20fed%20low-resolution%20images%20to%0Areduce%20memory%20usage.%20To%20allow%20the%20side%20network%20to%20access%20pre-trained%20knowledge%2C%0Awe%20introduce%20fine-grained%20prompts%20that%20summarize%20information%20from%20the%20LPM%0Athrough%20a%20fusion%20module.%20Important%20tokens%20selection%20and%20preloading%20techniques%0Aare%20employed%20to%20further%20reduce%20training%20cost%20and%20memory%20requirements.%20We%0Aevaluate%20FPT%20on%20four%20medical%20datasets%20with%20varying%20sizes%2C%20modalities%2C%20and%0Acomplexities.%20Experimental%20results%20demonstrate%20that%20FPT%20achieves%20comparable%0Aperformance%20to%20fine-tuning%20the%20entire%20LPM%20while%20using%20only%201.8%25%20of%20the%0Alearnable%20parameters%20and%2013%25%20of%20the%20memory%20costs%20of%20an%20encoder%20ViT-B%20model%20with%0Aa%20512%20x%20512%20input%20resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07576v3&entry.124074799=Read"},
{"title": "Dynamic Scheduling for Vehicle-to-Vehicle Communications Enhanced\n  Federated Learning", "author": "Jintao Yan and Tan Chen and Yuxuan Sun and Zhaojun Nan and Sheng Zhou and Zhisheng Niu", "abstract": "  Leveraging the computing and sensing capabilities of vehicles, vehicular\nfederated learning (VFL) has been applied to edge training for connected\nvehicles. The dynamic and interconnected nature of vehicular networks presents\nunique opportunities to harness direct vehicle-to-vehicle (V2V) communications,\nenhancing VFL training efficiency. In this paper, we formulate a stochastic\noptimization problem to optimize the VFL training performance, considering the\nenergy constraints and mobility of vehicles, and propose a V2V-enhanced dynamic\nscheduling (VEDS) algorithm to solve it. The model aggregation requirements of\nVFL and the limited transmission time due to mobility result in a stepwise\nobjective function, which presents challenges in solving the problem. We thus\npropose a derivative-based drift-plus-penalty method to convert the long-term\nstochastic optimization problem to an online mixed integer nonlinear\nprogramming (MINLP) problem, and provide a theoretical analysis to bound the\nperformance gap between the online solution and the offline optimal solution.\nFurther analysis of the scheduling priority reduces the original problem into a\nset of convex optimization problems, which are efficiently solved using the\ninterior-point method. Experimental results demonstrate that compared with the\nstate-of-the-art benchmarks, the proposed algorithm enhances the image\nclassification accuracy on the CIFAR-10 dataset by 3.18% and reduces the\naverage displacement errors on the Argoverse trajectory prediction dataset by\n10.21%.\n", "link": "http://arxiv.org/abs/2406.17470v1", "date": "2024-06-25", "relevancy": 2.0892, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5204}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Scheduling%20for%20Vehicle-to-Vehicle%20Communications%20Enhanced%0A%20%20Federated%20Learning&body=Title%3A%20Dynamic%20Scheduling%20for%20Vehicle-to-Vehicle%20Communications%20Enhanced%0A%20%20Federated%20Learning%0AAuthor%3A%20Jintao%20Yan%20and%20Tan%20Chen%20and%20Yuxuan%20Sun%20and%20Zhaojun%20Nan%20and%20Sheng%20Zhou%20and%20Zhisheng%20Niu%0AAbstract%3A%20%20%20Leveraging%20the%20computing%20and%20sensing%20capabilities%20of%20vehicles%2C%20vehicular%0Afederated%20learning%20%28VFL%29%20has%20been%20applied%20to%20edge%20training%20for%20connected%0Avehicles.%20The%20dynamic%20and%20interconnected%20nature%20of%20vehicular%20networks%20presents%0Aunique%20opportunities%20to%20harness%20direct%20vehicle-to-vehicle%20%28V2V%29%20communications%2C%0Aenhancing%20VFL%20training%20efficiency.%20In%20this%20paper%2C%20we%20formulate%20a%20stochastic%0Aoptimization%20problem%20to%20optimize%20the%20VFL%20training%20performance%2C%20considering%20the%0Aenergy%20constraints%20and%20mobility%20of%20vehicles%2C%20and%20propose%20a%20V2V-enhanced%20dynamic%0Ascheduling%20%28VEDS%29%20algorithm%20to%20solve%20it.%20The%20model%20aggregation%20requirements%20of%0AVFL%20and%20the%20limited%20transmission%20time%20due%20to%20mobility%20result%20in%20a%20stepwise%0Aobjective%20function%2C%20which%20presents%20challenges%20in%20solving%20the%20problem.%20We%20thus%0Apropose%20a%20derivative-based%20drift-plus-penalty%20method%20to%20convert%20the%20long-term%0Astochastic%20optimization%20problem%20to%20an%20online%20mixed%20integer%20nonlinear%0Aprogramming%20%28MINLP%29%20problem%2C%20and%20provide%20a%20theoretical%20analysis%20to%20bound%20the%0Aperformance%20gap%20between%20the%20online%20solution%20and%20the%20offline%20optimal%20solution.%0AFurther%20analysis%20of%20the%20scheduling%20priority%20reduces%20the%20original%20problem%20into%20a%0Aset%20of%20convex%20optimization%20problems%2C%20which%20are%20efficiently%20solved%20using%20the%0Ainterior-point%20method.%20Experimental%20results%20demonstrate%20that%20compared%20with%20the%0Astate-of-the-art%20benchmarks%2C%20the%20proposed%20algorithm%20enhances%20the%20image%0Aclassification%20accuracy%20on%20the%20CIFAR-10%20dataset%20by%203.18%25%20and%20reduces%20the%0Aaverage%20displacement%20errors%20on%20the%20Argoverse%20trajectory%20prediction%20dataset%20by%0A10.21%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Scheduling%2520for%2520Vehicle-to-Vehicle%2520Communications%2520Enhanced%250A%2520%2520Federated%2520Learning%26entry.906535625%3DJintao%2520Yan%2520and%2520Tan%2520Chen%2520and%2520Yuxuan%2520Sun%2520and%2520Zhaojun%2520Nan%2520and%2520Sheng%2520Zhou%2520and%2520Zhisheng%2520Niu%26entry.1292438233%3D%2520%2520Leveraging%2520the%2520computing%2520and%2520sensing%2520capabilities%2520of%2520vehicles%252C%2520vehicular%250Afederated%2520learning%2520%2528VFL%2529%2520has%2520been%2520applied%2520to%2520edge%2520training%2520for%2520connected%250Avehicles.%2520The%2520dynamic%2520and%2520interconnected%2520nature%2520of%2520vehicular%2520networks%2520presents%250Aunique%2520opportunities%2520to%2520harness%2520direct%2520vehicle-to-vehicle%2520%2528V2V%2529%2520communications%252C%250Aenhancing%2520VFL%2520training%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520formulate%2520a%2520stochastic%250Aoptimization%2520problem%2520to%2520optimize%2520the%2520VFL%2520training%2520performance%252C%2520considering%2520the%250Aenergy%2520constraints%2520and%2520mobility%2520of%2520vehicles%252C%2520and%2520propose%2520a%2520V2V-enhanced%2520dynamic%250Ascheduling%2520%2528VEDS%2529%2520algorithm%2520to%2520solve%2520it.%2520The%2520model%2520aggregation%2520requirements%2520of%250AVFL%2520and%2520the%2520limited%2520transmission%2520time%2520due%2520to%2520mobility%2520result%2520in%2520a%2520stepwise%250Aobjective%2520function%252C%2520which%2520presents%2520challenges%2520in%2520solving%2520the%2520problem.%2520We%2520thus%250Apropose%2520a%2520derivative-based%2520drift-plus-penalty%2520method%2520to%2520convert%2520the%2520long-term%250Astochastic%2520optimization%2520problem%2520to%2520an%2520online%2520mixed%2520integer%2520nonlinear%250Aprogramming%2520%2528MINLP%2529%2520problem%252C%2520and%2520provide%2520a%2520theoretical%2520analysis%2520to%2520bound%2520the%250Aperformance%2520gap%2520between%2520the%2520online%2520solution%2520and%2520the%2520offline%2520optimal%2520solution.%250AFurther%2520analysis%2520of%2520the%2520scheduling%2520priority%2520reduces%2520the%2520original%2520problem%2520into%2520a%250Aset%2520of%2520convex%2520optimization%2520problems%252C%2520which%2520are%2520efficiently%2520solved%2520using%2520the%250Ainterior-point%2520method.%2520Experimental%2520results%2520demonstrate%2520that%2520compared%2520with%2520the%250Astate-of-the-art%2520benchmarks%252C%2520the%2520proposed%2520algorithm%2520enhances%2520the%2520image%250Aclassification%2520accuracy%2520on%2520the%2520CIFAR-10%2520dataset%2520by%25203.18%2525%2520and%2520reduces%2520the%250Aaverage%2520displacement%2520errors%2520on%2520the%2520Argoverse%2520trajectory%2520prediction%2520dataset%2520by%250A10.21%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Scheduling%20for%20Vehicle-to-Vehicle%20Communications%20Enhanced%0A%20%20Federated%20Learning&entry.906535625=Jintao%20Yan%20and%20Tan%20Chen%20and%20Yuxuan%20Sun%20and%20Zhaojun%20Nan%20and%20Sheng%20Zhou%20and%20Zhisheng%20Niu&entry.1292438233=%20%20Leveraging%20the%20computing%20and%20sensing%20capabilities%20of%20vehicles%2C%20vehicular%0Afederated%20learning%20%28VFL%29%20has%20been%20applied%20to%20edge%20training%20for%20connected%0Avehicles.%20The%20dynamic%20and%20interconnected%20nature%20of%20vehicular%20networks%20presents%0Aunique%20opportunities%20to%20harness%20direct%20vehicle-to-vehicle%20%28V2V%29%20communications%2C%0Aenhancing%20VFL%20training%20efficiency.%20In%20this%20paper%2C%20we%20formulate%20a%20stochastic%0Aoptimization%20problem%20to%20optimize%20the%20VFL%20training%20performance%2C%20considering%20the%0Aenergy%20constraints%20and%20mobility%20of%20vehicles%2C%20and%20propose%20a%20V2V-enhanced%20dynamic%0Ascheduling%20%28VEDS%29%20algorithm%20to%20solve%20it.%20The%20model%20aggregation%20requirements%20of%0AVFL%20and%20the%20limited%20transmission%20time%20due%20to%20mobility%20result%20in%20a%20stepwise%0Aobjective%20function%2C%20which%20presents%20challenges%20in%20solving%20the%20problem.%20We%20thus%0Apropose%20a%20derivative-based%20drift-plus-penalty%20method%20to%20convert%20the%20long-term%0Astochastic%20optimization%20problem%20to%20an%20online%20mixed%20integer%20nonlinear%0Aprogramming%20%28MINLP%29%20problem%2C%20and%20provide%20a%20theoretical%20analysis%20to%20bound%20the%0Aperformance%20gap%20between%20the%20online%20solution%20and%20the%20offline%20optimal%20solution.%0AFurther%20analysis%20of%20the%20scheduling%20priority%20reduces%20the%20original%20problem%20into%20a%0Aset%20of%20convex%20optimization%20problems%2C%20which%20are%20efficiently%20solved%20using%20the%0Ainterior-point%20method.%20Experimental%20results%20demonstrate%20that%20compared%20with%20the%0Astate-of-the-art%20benchmarks%2C%20the%20proposed%20algorithm%20enhances%20the%20image%0Aclassification%20accuracy%20on%20the%20CIFAR-10%20dataset%20by%203.18%25%20and%20reduces%20the%0Aaverage%20displacement%20errors%20on%20the%20Argoverse%20trajectory%20prediction%20dataset%20by%0A10.21%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17470v1&entry.124074799=Read"},
{"title": "Benchmarking Deep Learning Models on NVIDIA Jetson Nano for Real-Time\n  Systems: An Empirical Investigation", "author": "Tushar Prasanna Swaminathan and Christopher Silver and Thangarajah Akilan", "abstract": "  The proliferation of complex deep learning (DL) models has revolutionized\nvarious applications, including computer vision-based solutions, prompting\ntheir integration into real-time systems. However, the resource-intensive\nnature of these models poses challenges for deployment on low-computational\npower and low-memory devices, like embedded and edge devices. This work\nempirically investigates the optimization of such complex DL models to analyze\ntheir functionality on an embedded device, particularly on the NVIDIA Jetson\nNano. It evaluates the effectiveness of the optimized models in terms of their\ninference speed for image classification and video action detection. The\nexperimental results reveal that, on average, optimized models exhibit a 16.11%\nspeed improvement over their non-optimized counterparts. This not only\nemphasizes the critical need to consider hardware constraints and environmental\nsustainability in model development and deployment but also underscores the\npivotal role of model optimization in enabling the widespread deployment of\nAI-assisted technologies on resource-constrained computational systems. It also\nserves as proof that prioritizing hardware-specific model optimization leads to\nefficient and scalable solutions that substantially decrease energy consumption\nand carbon footprint.\n", "link": "http://arxiv.org/abs/2406.17749v1", "date": "2024-06-25", "relevancy": 2.086, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.575}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5163}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Deep%20Learning%20Models%20on%20NVIDIA%20Jetson%20Nano%20for%20Real-Time%0A%20%20Systems%3A%20An%20Empirical%20Investigation&body=Title%3A%20Benchmarking%20Deep%20Learning%20Models%20on%20NVIDIA%20Jetson%20Nano%20for%20Real-Time%0A%20%20Systems%3A%20An%20Empirical%20Investigation%0AAuthor%3A%20Tushar%20Prasanna%20Swaminathan%20and%20Christopher%20Silver%20and%20Thangarajah%20Akilan%0AAbstract%3A%20%20%20The%20proliferation%20of%20complex%20deep%20learning%20%28DL%29%20models%20has%20revolutionized%0Avarious%20applications%2C%20including%20computer%20vision-based%20solutions%2C%20prompting%0Atheir%20integration%20into%20real-time%20systems.%20However%2C%20the%20resource-intensive%0Anature%20of%20these%20models%20poses%20challenges%20for%20deployment%20on%20low-computational%0Apower%20and%20low-memory%20devices%2C%20like%20embedded%20and%20edge%20devices.%20This%20work%0Aempirically%20investigates%20the%20optimization%20of%20such%20complex%20DL%20models%20to%20analyze%0Atheir%20functionality%20on%20an%20embedded%20device%2C%20particularly%20on%20the%20NVIDIA%20Jetson%0ANano.%20It%20evaluates%20the%20effectiveness%20of%20the%20optimized%20models%20in%20terms%20of%20their%0Ainference%20speed%20for%20image%20classification%20and%20video%20action%20detection.%20The%0Aexperimental%20results%20reveal%20that%2C%20on%20average%2C%20optimized%20models%20exhibit%20a%2016.11%25%0Aspeed%20improvement%20over%20their%20non-optimized%20counterparts.%20This%20not%20only%0Aemphasizes%20the%20critical%20need%20to%20consider%20hardware%20constraints%20and%20environmental%0Asustainability%20in%20model%20development%20and%20deployment%20but%20also%20underscores%20the%0Apivotal%20role%20of%20model%20optimization%20in%20enabling%20the%20widespread%20deployment%20of%0AAI-assisted%20technologies%20on%20resource-constrained%20computational%20systems.%20It%20also%0Aserves%20as%20proof%20that%20prioritizing%20hardware-specific%20model%20optimization%20leads%20to%0Aefficient%20and%20scalable%20solutions%20that%20substantially%20decrease%20energy%20consumption%0Aand%20carbon%20footprint.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Deep%2520Learning%2520Models%2520on%2520NVIDIA%2520Jetson%2520Nano%2520for%2520Real-Time%250A%2520%2520Systems%253A%2520An%2520Empirical%2520Investigation%26entry.906535625%3DTushar%2520Prasanna%2520Swaminathan%2520and%2520Christopher%2520Silver%2520and%2520Thangarajah%2520Akilan%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520complex%2520deep%2520learning%2520%2528DL%2529%2520models%2520has%2520revolutionized%250Avarious%2520applications%252C%2520including%2520computer%2520vision-based%2520solutions%252C%2520prompting%250Atheir%2520integration%2520into%2520real-time%2520systems.%2520However%252C%2520the%2520resource-intensive%250Anature%2520of%2520these%2520models%2520poses%2520challenges%2520for%2520deployment%2520on%2520low-computational%250Apower%2520and%2520low-memory%2520devices%252C%2520like%2520embedded%2520and%2520edge%2520devices.%2520This%2520work%250Aempirically%2520investigates%2520the%2520optimization%2520of%2520such%2520complex%2520DL%2520models%2520to%2520analyze%250Atheir%2520functionality%2520on%2520an%2520embedded%2520device%252C%2520particularly%2520on%2520the%2520NVIDIA%2520Jetson%250ANano.%2520It%2520evaluates%2520the%2520effectiveness%2520of%2520the%2520optimized%2520models%2520in%2520terms%2520of%2520their%250Ainference%2520speed%2520for%2520image%2520classification%2520and%2520video%2520action%2520detection.%2520The%250Aexperimental%2520results%2520reveal%2520that%252C%2520on%2520average%252C%2520optimized%2520models%2520exhibit%2520a%252016.11%2525%250Aspeed%2520improvement%2520over%2520their%2520non-optimized%2520counterparts.%2520This%2520not%2520only%250Aemphasizes%2520the%2520critical%2520need%2520to%2520consider%2520hardware%2520constraints%2520and%2520environmental%250Asustainability%2520in%2520model%2520development%2520and%2520deployment%2520but%2520also%2520underscores%2520the%250Apivotal%2520role%2520of%2520model%2520optimization%2520in%2520enabling%2520the%2520widespread%2520deployment%2520of%250AAI-assisted%2520technologies%2520on%2520resource-constrained%2520computational%2520systems.%2520It%2520also%250Aserves%2520as%2520proof%2520that%2520prioritizing%2520hardware-specific%2520model%2520optimization%2520leads%2520to%250Aefficient%2520and%2520scalable%2520solutions%2520that%2520substantially%2520decrease%2520energy%2520consumption%250Aand%2520carbon%2520footprint.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Deep%20Learning%20Models%20on%20NVIDIA%20Jetson%20Nano%20for%20Real-Time%0A%20%20Systems%3A%20An%20Empirical%20Investigation&entry.906535625=Tushar%20Prasanna%20Swaminathan%20and%20Christopher%20Silver%20and%20Thangarajah%20Akilan&entry.1292438233=%20%20The%20proliferation%20of%20complex%20deep%20learning%20%28DL%29%20models%20has%20revolutionized%0Avarious%20applications%2C%20including%20computer%20vision-based%20solutions%2C%20prompting%0Atheir%20integration%20into%20real-time%20systems.%20However%2C%20the%20resource-intensive%0Anature%20of%20these%20models%20poses%20challenges%20for%20deployment%20on%20low-computational%0Apower%20and%20low-memory%20devices%2C%20like%20embedded%20and%20edge%20devices.%20This%20work%0Aempirically%20investigates%20the%20optimization%20of%20such%20complex%20DL%20models%20to%20analyze%0Atheir%20functionality%20on%20an%20embedded%20device%2C%20particularly%20on%20the%20NVIDIA%20Jetson%0ANano.%20It%20evaluates%20the%20effectiveness%20of%20the%20optimized%20models%20in%20terms%20of%20their%0Ainference%20speed%20for%20image%20classification%20and%20video%20action%20detection.%20The%0Aexperimental%20results%20reveal%20that%2C%20on%20average%2C%20optimized%20models%20exhibit%20a%2016.11%25%0Aspeed%20improvement%20over%20their%20non-optimized%20counterparts.%20This%20not%20only%0Aemphasizes%20the%20critical%20need%20to%20consider%20hardware%20constraints%20and%20environmental%0Asustainability%20in%20model%20development%20and%20deployment%20but%20also%20underscores%20the%0Apivotal%20role%20of%20model%20optimization%20in%20enabling%20the%20widespread%20deployment%20of%0AAI-assisted%20technologies%20on%20resource-constrained%20computational%20systems.%20It%20also%0Aserves%20as%20proof%20that%20prioritizing%20hardware-specific%20model%20optimization%20leads%20to%0Aefficient%20and%20scalable%20solutions%20that%20substantially%20decrease%20energy%20consumption%0Aand%20carbon%20footprint.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17749v1&entry.124074799=Read"},
{"title": "CT-Bound: Robust Boundary Detection From Noisy Images Via Hybrid\n  Convolution and Transformer Neural Networks", "author": "Wei Xu and Junjie Luo and Qi Guo", "abstract": "  We present CT-Bound, a robust and fast boundary detection method for very\nnoisy images using a hybrid Convolution and Transformer neural network. The\nproposed architecture decomposes boundary estimation into two tasks: local\ndetection and global regularization. During the local detection, the model uses\na convolutional architecture to predict the boundary structure of each image\npatch in the form of a pre-defined local boundary representation, the\nfield-of-junctions (FoJ). Then, it uses a feed-forward transformer architecture\nto globally refine the boundary structures of each patch to generate an edge\nmap and a smoothed color map simultaneously. Our quantitative analysis shows\nthat CT-Bound outperforms the previous best algorithms in edge detection on\nvery noisy images. It also increases the edge detection accuracy of FoJ-based\nmethods while having a 3-time speed improvement. Finally, we demonstrate that\nCT-Bound can produce boundary and color maps on real captured images without\nextra fine-tuning and real-time boundary map and color map videos at ten frames\nper second.\n", "link": "http://arxiv.org/abs/2403.16494v2", "date": "2024-06-25", "relevancy": 2.0627, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5297}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5248}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT-Bound%3A%20Robust%20Boundary%20Detection%20From%20Noisy%20Images%20Via%20Hybrid%0A%20%20Convolution%20and%20Transformer%20Neural%20Networks&body=Title%3A%20CT-Bound%3A%20Robust%20Boundary%20Detection%20From%20Noisy%20Images%20Via%20Hybrid%0A%20%20Convolution%20and%20Transformer%20Neural%20Networks%0AAuthor%3A%20Wei%20Xu%20and%20Junjie%20Luo%20and%20Qi%20Guo%0AAbstract%3A%20%20%20We%20present%20CT-Bound%2C%20a%20robust%20and%20fast%20boundary%20detection%20method%20for%20very%0Anoisy%20images%20using%20a%20hybrid%20Convolution%20and%20Transformer%20neural%20network.%20The%0Aproposed%20architecture%20decomposes%20boundary%20estimation%20into%20two%20tasks%3A%20local%0Adetection%20and%20global%20regularization.%20During%20the%20local%20detection%2C%20the%20model%20uses%0Aa%20convolutional%20architecture%20to%20predict%20the%20boundary%20structure%20of%20each%20image%0Apatch%20in%20the%20form%20of%20a%20pre-defined%20local%20boundary%20representation%2C%20the%0Afield-of-junctions%20%28FoJ%29.%20Then%2C%20it%20uses%20a%20feed-forward%20transformer%20architecture%0Ato%20globally%20refine%20the%20boundary%20structures%20of%20each%20patch%20to%20generate%20an%20edge%0Amap%20and%20a%20smoothed%20color%20map%20simultaneously.%20Our%20quantitative%20analysis%20shows%0Athat%20CT-Bound%20outperforms%20the%20previous%20best%20algorithms%20in%20edge%20detection%20on%0Avery%20noisy%20images.%20It%20also%20increases%20the%20edge%20detection%20accuracy%20of%20FoJ-based%0Amethods%20while%20having%20a%203-time%20speed%20improvement.%20Finally%2C%20we%20demonstrate%20that%0ACT-Bound%20can%20produce%20boundary%20and%20color%20maps%20on%20real%20captured%20images%20without%0Aextra%20fine-tuning%20and%20real-time%20boundary%20map%20and%20color%20map%20videos%20at%20ten%20frames%0Aper%20second.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT-Bound%253A%2520Robust%2520Boundary%2520Detection%2520From%2520Noisy%2520Images%2520Via%2520Hybrid%250A%2520%2520Convolution%2520and%2520Transformer%2520Neural%2520Networks%26entry.906535625%3DWei%2520Xu%2520and%2520Junjie%2520Luo%2520and%2520Qi%2520Guo%26entry.1292438233%3D%2520%2520We%2520present%2520CT-Bound%252C%2520a%2520robust%2520and%2520fast%2520boundary%2520detection%2520method%2520for%2520very%250Anoisy%2520images%2520using%2520a%2520hybrid%2520Convolution%2520and%2520Transformer%2520neural%2520network.%2520The%250Aproposed%2520architecture%2520decomposes%2520boundary%2520estimation%2520into%2520two%2520tasks%253A%2520local%250Adetection%2520and%2520global%2520regularization.%2520During%2520the%2520local%2520detection%252C%2520the%2520model%2520uses%250Aa%2520convolutional%2520architecture%2520to%2520predict%2520the%2520boundary%2520structure%2520of%2520each%2520image%250Apatch%2520in%2520the%2520form%2520of%2520a%2520pre-defined%2520local%2520boundary%2520representation%252C%2520the%250Afield-of-junctions%2520%2528FoJ%2529.%2520Then%252C%2520it%2520uses%2520a%2520feed-forward%2520transformer%2520architecture%250Ato%2520globally%2520refine%2520the%2520boundary%2520structures%2520of%2520each%2520patch%2520to%2520generate%2520an%2520edge%250Amap%2520and%2520a%2520smoothed%2520color%2520map%2520simultaneously.%2520Our%2520quantitative%2520analysis%2520shows%250Athat%2520CT-Bound%2520outperforms%2520the%2520previous%2520best%2520algorithms%2520in%2520edge%2520detection%2520on%250Avery%2520noisy%2520images.%2520It%2520also%2520increases%2520the%2520edge%2520detection%2520accuracy%2520of%2520FoJ-based%250Amethods%2520while%2520having%2520a%25203-time%2520speed%2520improvement.%2520Finally%252C%2520we%2520demonstrate%2520that%250ACT-Bound%2520can%2520produce%2520boundary%2520and%2520color%2520maps%2520on%2520real%2520captured%2520images%2520without%250Aextra%2520fine-tuning%2520and%2520real-time%2520boundary%2520map%2520and%2520color%2520map%2520videos%2520at%2520ten%2520frames%250Aper%2520second.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-Bound%3A%20Robust%20Boundary%20Detection%20From%20Noisy%20Images%20Via%20Hybrid%0A%20%20Convolution%20and%20Transformer%20Neural%20Networks&entry.906535625=Wei%20Xu%20and%20Junjie%20Luo%20and%20Qi%20Guo&entry.1292438233=%20%20We%20present%20CT-Bound%2C%20a%20robust%20and%20fast%20boundary%20detection%20method%20for%20very%0Anoisy%20images%20using%20a%20hybrid%20Convolution%20and%20Transformer%20neural%20network.%20The%0Aproposed%20architecture%20decomposes%20boundary%20estimation%20into%20two%20tasks%3A%20local%0Adetection%20and%20global%20regularization.%20During%20the%20local%20detection%2C%20the%20model%20uses%0Aa%20convolutional%20architecture%20to%20predict%20the%20boundary%20structure%20of%20each%20image%0Apatch%20in%20the%20form%20of%20a%20pre-defined%20local%20boundary%20representation%2C%20the%0Afield-of-junctions%20%28FoJ%29.%20Then%2C%20it%20uses%20a%20feed-forward%20transformer%20architecture%0Ato%20globally%20refine%20the%20boundary%20structures%20of%20each%20patch%20to%20generate%20an%20edge%0Amap%20and%20a%20smoothed%20color%20map%20simultaneously.%20Our%20quantitative%20analysis%20shows%0Athat%20CT-Bound%20outperforms%20the%20previous%20best%20algorithms%20in%20edge%20detection%20on%0Avery%20noisy%20images.%20It%20also%20increases%20the%20edge%20detection%20accuracy%20of%20FoJ-based%0Amethods%20while%20having%20a%203-time%20speed%20improvement.%20Finally%2C%20we%20demonstrate%20that%0ACT-Bound%20can%20produce%20boundary%20and%20color%20maps%20on%20real%20captured%20images%20without%0Aextra%20fine-tuning%20and%20real-time%20boundary%20map%20and%20color%20map%20videos%20at%20ten%20frames%0Aper%20second.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16494v2&entry.124074799=Read"},
{"title": "Towards Unbiased Calibration using Meta-Regularization", "author": "Cheng Wang and Jacek Golebiowski", "abstract": "  Model miscalibration has been frequently identified in modern deep neural\nnetworks. Recent work aims to improve model calibration directly through a\ndifferentiable calibration proxy. However, the calibration produced is often\nbiased due to the binning mechanism. In this work, we propose to learn\nbetter-calibrated models via meta-regularization, which has two components: (1)\ngamma network (gamma-net), a meta learner that outputs sample-wise gamma values\n(continuous variable) for Focal loss for regularizing the backbone network; (2)\nsmooth expected calibration error (SECE), a Gaussian-kernel based, unbiased,\nand differentiable surrogate to ECE that enables the smooth optimization of\ngamma-Net. We evaluate the effectiveness of the proposed approach in\nregularizing neural networks towards improved and unbiased calibration on three\ncomputer vision datasets. We empirically demonstrate that: (a) learning\nsample-wise gamma as continuous variables can effectively improve calibration;\n(b) SECE smoothly optimizes gamma-net towards unbiased and robust calibration\nwith respect to the binning schemes; and (c) the combination of gamma-net and\nSECE achieves the best calibration performance across various calibration\nmetrics while retaining very competitive predictive performance as compared to\nmultiple recently proposed methods.\n", "link": "http://arxiv.org/abs/2303.15057v3", "date": "2024-06-25", "relevancy": 2.0589, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.518}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5168}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unbiased%20Calibration%20using%20Meta-Regularization&body=Title%3A%20Towards%20Unbiased%20Calibration%20using%20Meta-Regularization%0AAuthor%3A%20Cheng%20Wang%20and%20Jacek%20Golebiowski%0AAbstract%3A%20%20%20Model%20miscalibration%20has%20been%20frequently%20identified%20in%20modern%20deep%20neural%0Anetworks.%20Recent%20work%20aims%20to%20improve%20model%20calibration%20directly%20through%20a%0Adifferentiable%20calibration%20proxy.%20However%2C%20the%20calibration%20produced%20is%20often%0Abiased%20due%20to%20the%20binning%20mechanism.%20In%20this%20work%2C%20we%20propose%20to%20learn%0Abetter-calibrated%20models%20via%20meta-regularization%2C%20which%20has%20two%20components%3A%20%281%29%0Agamma%20network%20%28gamma-net%29%2C%20a%20meta%20learner%20that%20outputs%20sample-wise%20gamma%20values%0A%28continuous%20variable%29%20for%20Focal%20loss%20for%20regularizing%20the%20backbone%20network%3B%20%282%29%0Asmooth%20expected%20calibration%20error%20%28SECE%29%2C%20a%20Gaussian-kernel%20based%2C%20unbiased%2C%0Aand%20differentiable%20surrogate%20to%20ECE%20that%20enables%20the%20smooth%20optimization%20of%0Agamma-Net.%20We%20evaluate%20the%20effectiveness%20of%20the%20proposed%20approach%20in%0Aregularizing%20neural%20networks%20towards%20improved%20and%20unbiased%20calibration%20on%20three%0Acomputer%20vision%20datasets.%20We%20empirically%20demonstrate%20that%3A%20%28a%29%20learning%0Asample-wise%20gamma%20as%20continuous%20variables%20can%20effectively%20improve%20calibration%3B%0A%28b%29%20SECE%20smoothly%20optimizes%20gamma-net%20towards%20unbiased%20and%20robust%20calibration%0Awith%20respect%20to%20the%20binning%20schemes%3B%20and%20%28c%29%20the%20combination%20of%20gamma-net%20and%0ASECE%20achieves%20the%20best%20calibration%20performance%20across%20various%20calibration%0Ametrics%20while%20retaining%20very%20competitive%20predictive%20performance%20as%20compared%20to%0Amultiple%20recently%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15057v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unbiased%2520Calibration%2520using%2520Meta-Regularization%26entry.906535625%3DCheng%2520Wang%2520and%2520Jacek%2520Golebiowski%26entry.1292438233%3D%2520%2520Model%2520miscalibration%2520has%2520been%2520frequently%2520identified%2520in%2520modern%2520deep%2520neural%250Anetworks.%2520Recent%2520work%2520aims%2520to%2520improve%2520model%2520calibration%2520directly%2520through%2520a%250Adifferentiable%2520calibration%2520proxy.%2520However%252C%2520the%2520calibration%2520produced%2520is%2520often%250Abiased%2520due%2520to%2520the%2520binning%2520mechanism.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520learn%250Abetter-calibrated%2520models%2520via%2520meta-regularization%252C%2520which%2520has%2520two%2520components%253A%2520%25281%2529%250Agamma%2520network%2520%2528gamma-net%2529%252C%2520a%2520meta%2520learner%2520that%2520outputs%2520sample-wise%2520gamma%2520values%250A%2528continuous%2520variable%2529%2520for%2520Focal%2520loss%2520for%2520regularizing%2520the%2520backbone%2520network%253B%2520%25282%2529%250Asmooth%2520expected%2520calibration%2520error%2520%2528SECE%2529%252C%2520a%2520Gaussian-kernel%2520based%252C%2520unbiased%252C%250Aand%2520differentiable%2520surrogate%2520to%2520ECE%2520that%2520enables%2520the%2520smooth%2520optimization%2520of%250Agamma-Net.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520in%250Aregularizing%2520neural%2520networks%2520towards%2520improved%2520and%2520unbiased%2520calibration%2520on%2520three%250Acomputer%2520vision%2520datasets.%2520We%2520empirically%2520demonstrate%2520that%253A%2520%2528a%2529%2520learning%250Asample-wise%2520gamma%2520as%2520continuous%2520variables%2520can%2520effectively%2520improve%2520calibration%253B%250A%2528b%2529%2520SECE%2520smoothly%2520optimizes%2520gamma-net%2520towards%2520unbiased%2520and%2520robust%2520calibration%250Awith%2520respect%2520to%2520the%2520binning%2520schemes%253B%2520and%2520%2528c%2529%2520the%2520combination%2520of%2520gamma-net%2520and%250ASECE%2520achieves%2520the%2520best%2520calibration%2520performance%2520across%2520various%2520calibration%250Ametrics%2520while%2520retaining%2520very%2520competitive%2520predictive%2520performance%2520as%2520compared%2520to%250Amultiple%2520recently%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.15057v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unbiased%20Calibration%20using%20Meta-Regularization&entry.906535625=Cheng%20Wang%20and%20Jacek%20Golebiowski&entry.1292438233=%20%20Model%20miscalibration%20has%20been%20frequently%20identified%20in%20modern%20deep%20neural%0Anetworks.%20Recent%20work%20aims%20to%20improve%20model%20calibration%20directly%20through%20a%0Adifferentiable%20calibration%20proxy.%20However%2C%20the%20calibration%20produced%20is%20often%0Abiased%20due%20to%20the%20binning%20mechanism.%20In%20this%20work%2C%20we%20propose%20to%20learn%0Abetter-calibrated%20models%20via%20meta-regularization%2C%20which%20has%20two%20components%3A%20%281%29%0Agamma%20network%20%28gamma-net%29%2C%20a%20meta%20learner%20that%20outputs%20sample-wise%20gamma%20values%0A%28continuous%20variable%29%20for%20Focal%20loss%20for%20regularizing%20the%20backbone%20network%3B%20%282%29%0Asmooth%20expected%20calibration%20error%20%28SECE%29%2C%20a%20Gaussian-kernel%20based%2C%20unbiased%2C%0Aand%20differentiable%20surrogate%20to%20ECE%20that%20enables%20the%20smooth%20optimization%20of%0Agamma-Net.%20We%20evaluate%20the%20effectiveness%20of%20the%20proposed%20approach%20in%0Aregularizing%20neural%20networks%20towards%20improved%20and%20unbiased%20calibration%20on%20three%0Acomputer%20vision%20datasets.%20We%20empirically%20demonstrate%20that%3A%20%28a%29%20learning%0Asample-wise%20gamma%20as%20continuous%20variables%20can%20effectively%20improve%20calibration%3B%0A%28b%29%20SECE%20smoothly%20optimizes%20gamma-net%20towards%20unbiased%20and%20robust%20calibration%0Awith%20respect%20to%20the%20binning%20schemes%3B%20and%20%28c%29%20the%20combination%20of%20gamma-net%20and%0ASECE%20achieves%20the%20best%20calibration%20performance%20across%20various%20calibration%0Ametrics%20while%20retaining%20very%20competitive%20predictive%20performance%20as%20compared%20to%0Amultiple%20recently%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15057v3&entry.124074799=Read"},
{"title": "Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG", "author": "William Merrill and Noah A. Smith and Yanai Elazar", "abstract": "  How novel are texts generated by language models (LMs) relative to their\ntraining corpora? In this work, we investigate the extent to which modern LMs\ngenerate $n$-grams from their training data, evaluating both (i) the\nprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the\nproportion of $n$-grams generated by an LM that did not appear in the training\ndata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search\nover a corpus in constant time, we develop Rusty-DAWG, a novel search tool\ninspired by indexing of genomic data. We compare the novelty of LM-generated\ntext to human-written text and explore factors that affect generation novelty,\nfocusing on the Pythia models. We find that, for $n > 4$, LM-generated text is\nless novel than human-written text, though it is more novel for smaller $n$.\nLarger LMs and more constrained decoding strategies both decrease novelty.\nFinally, we show that LMs complete $n$-grams with lower loss if they are more\nfrequent in the training data. Overall, our results reveal factors influencing\nthe novelty of LM-generated text, and we release Rusty-DAWG to facilitate\nfurther pretraining data research.\n", "link": "http://arxiv.org/abs/2406.13069v2", "date": "2024-06-25", "relevancy": 2.0562, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4186}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4117}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20%24n%24-Gram%20Novelty%20of%20Language%20Models%20Using%20Rusty-DAWG&body=Title%3A%20Evaluating%20%24n%24-Gram%20Novelty%20of%20Language%20Models%20Using%20Rusty-DAWG%0AAuthor%3A%20William%20Merrill%20and%20Noah%20A.%20Smith%20and%20Yanai%20Elazar%0AAbstract%3A%20%20%20How%20novel%20are%20texts%20generated%20by%20language%20models%20%28LMs%29%20relative%20to%20their%0Atraining%20corpora%3F%20In%20this%20work%2C%20we%20investigate%20the%20extent%20to%20which%20modern%20LMs%0Agenerate%20%24n%24-grams%20from%20their%20training%20data%2C%20evaluating%20both%20%28i%29%20the%0Aprobability%20LMs%20assign%20to%20complete%20training%20%24n%24-grams%20and%20%28ii%29%20%24n%24-novelty%2C%20the%0Aproportion%20of%20%24n%24-grams%20generated%20by%20an%20LM%20that%20did%20not%20appear%20in%20the%20training%0Adata%20%28for%20arbitrarily%20large%20%24n%24%29.%20To%20enable%20arbitrary-length%20%24n%24-gram%20search%0Aover%20a%20corpus%20in%20constant%20time%2C%20we%20develop%20Rusty-DAWG%2C%20a%20novel%20search%20tool%0Ainspired%20by%20indexing%20of%20genomic%20data.%20We%20compare%20the%20novelty%20of%20LM-generated%0Atext%20to%20human-written%20text%20and%20explore%20factors%20that%20affect%20generation%20novelty%2C%0Afocusing%20on%20the%20Pythia%20models.%20We%20find%20that%2C%20for%20%24n%20%3E%204%24%2C%20LM-generated%20text%20is%0Aless%20novel%20than%20human-written%20text%2C%20though%20it%20is%20more%20novel%20for%20smaller%20%24n%24.%0ALarger%20LMs%20and%20more%20constrained%20decoding%20strategies%20both%20decrease%20novelty.%0AFinally%2C%20we%20show%20that%20LMs%20complete%20%24n%24-grams%20with%20lower%20loss%20if%20they%20are%20more%0Afrequent%20in%20the%20training%20data.%20Overall%2C%20our%20results%20reveal%20factors%20influencing%0Athe%20novelty%20of%20LM-generated%20text%2C%20and%20we%20release%20Rusty-DAWG%20to%20facilitate%0Afurther%20pretraining%20data%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520%2524n%2524-Gram%2520Novelty%2520of%2520Language%2520Models%2520Using%2520Rusty-DAWG%26entry.906535625%3DWilliam%2520Merrill%2520and%2520Noah%2520A.%2520Smith%2520and%2520Yanai%2520Elazar%26entry.1292438233%3D%2520%2520How%2520novel%2520are%2520texts%2520generated%2520by%2520language%2520models%2520%2528LMs%2529%2520relative%2520to%2520their%250Atraining%2520corpora%253F%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520modern%2520LMs%250Agenerate%2520%2524n%2524-grams%2520from%2520their%2520training%2520data%252C%2520evaluating%2520both%2520%2528i%2529%2520the%250Aprobability%2520LMs%2520assign%2520to%2520complete%2520training%2520%2524n%2524-grams%2520and%2520%2528ii%2529%2520%2524n%2524-novelty%252C%2520the%250Aproportion%2520of%2520%2524n%2524-grams%2520generated%2520by%2520an%2520LM%2520that%2520did%2520not%2520appear%2520in%2520the%2520training%250Adata%2520%2528for%2520arbitrarily%2520large%2520%2524n%2524%2529.%2520To%2520enable%2520arbitrary-length%2520%2524n%2524-gram%2520search%250Aover%2520a%2520corpus%2520in%2520constant%2520time%252C%2520we%2520develop%2520Rusty-DAWG%252C%2520a%2520novel%2520search%2520tool%250Ainspired%2520by%2520indexing%2520of%2520genomic%2520data.%2520We%2520compare%2520the%2520novelty%2520of%2520LM-generated%250Atext%2520to%2520human-written%2520text%2520and%2520explore%2520factors%2520that%2520affect%2520generation%2520novelty%252C%250Afocusing%2520on%2520the%2520Pythia%2520models.%2520We%2520find%2520that%252C%2520for%2520%2524n%2520%253E%25204%2524%252C%2520LM-generated%2520text%2520is%250Aless%2520novel%2520than%2520human-written%2520text%252C%2520though%2520it%2520is%2520more%2520novel%2520for%2520smaller%2520%2524n%2524.%250ALarger%2520LMs%2520and%2520more%2520constrained%2520decoding%2520strategies%2520both%2520decrease%2520novelty.%250AFinally%252C%2520we%2520show%2520that%2520LMs%2520complete%2520%2524n%2524-grams%2520with%2520lower%2520loss%2520if%2520they%2520are%2520more%250Afrequent%2520in%2520the%2520training%2520data.%2520Overall%252C%2520our%2520results%2520reveal%2520factors%2520influencing%250Athe%2520novelty%2520of%2520LM-generated%2520text%252C%2520and%2520we%2520release%2520Rusty-DAWG%2520to%2520facilitate%250Afurther%2520pretraining%2520data%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20%24n%24-Gram%20Novelty%20of%20Language%20Models%20Using%20Rusty-DAWG&entry.906535625=William%20Merrill%20and%20Noah%20A.%20Smith%20and%20Yanai%20Elazar&entry.1292438233=%20%20How%20novel%20are%20texts%20generated%20by%20language%20models%20%28LMs%29%20relative%20to%20their%0Atraining%20corpora%3F%20In%20this%20work%2C%20we%20investigate%20the%20extent%20to%20which%20modern%20LMs%0Agenerate%20%24n%24-grams%20from%20their%20training%20data%2C%20evaluating%20both%20%28i%29%20the%0Aprobability%20LMs%20assign%20to%20complete%20training%20%24n%24-grams%20and%20%28ii%29%20%24n%24-novelty%2C%20the%0Aproportion%20of%20%24n%24-grams%20generated%20by%20an%20LM%20that%20did%20not%20appear%20in%20the%20training%0Adata%20%28for%20arbitrarily%20large%20%24n%24%29.%20To%20enable%20arbitrary-length%20%24n%24-gram%20search%0Aover%20a%20corpus%20in%20constant%20time%2C%20we%20develop%20Rusty-DAWG%2C%20a%20novel%20search%20tool%0Ainspired%20by%20indexing%20of%20genomic%20data.%20We%20compare%20the%20novelty%20of%20LM-generated%0Atext%20to%20human-written%20text%20and%20explore%20factors%20that%20affect%20generation%20novelty%2C%0Afocusing%20on%20the%20Pythia%20models.%20We%20find%20that%2C%20for%20%24n%20%3E%204%24%2C%20LM-generated%20text%20is%0Aless%20novel%20than%20human-written%20text%2C%20though%20it%20is%20more%20novel%20for%20smaller%20%24n%24.%0ALarger%20LMs%20and%20more%20constrained%20decoding%20strategies%20both%20decrease%20novelty.%0AFinally%2C%20we%20show%20that%20LMs%20complete%20%24n%24-grams%20with%20lower%20loss%20if%20they%20are%20more%0Afrequent%20in%20the%20training%20data.%20Overall%2C%20our%20results%20reveal%20factors%20influencing%0Athe%20novelty%20of%20LM-generated%20text%2C%20and%20we%20release%20Rusty-DAWG%20to%20facilitate%0Afurther%20pretraining%20data%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13069v2&entry.124074799=Read"},
{"title": "MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions", "author": "Francesco Di Salvo and Sebastian Doerrich and Christian Ledig", "abstract": "  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api}{github.com/francescodisalvo05/medmnistc-api.\n", "link": "http://arxiv.org/abs/2406.17536v1", "date": "2024-06-25", "relevancy": 2.052, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.538}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5162}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions&body=Title%3A%20MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions%0AAuthor%3A%20Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Christian%20Ledig%0AAbstract%3A%20%20%20The%20integration%20of%20neural-network-based%20systems%20into%20clinical%20practice%20is%0Alimited%20by%20challenges%20related%20to%20domain%20generalization%20and%20robustness.%20The%0Acomputer%20vision%20community%20established%20benchmarks%20such%20as%20ImageNet-C%20as%20a%0Afundamental%20prerequisite%20to%20measure%20progress%20towards%20those%20challenges.%20Similar%0Adatasets%20are%20largely%20absent%20in%20the%20medical%20imaging%20community%20which%20lacks%20a%0Acomprehensive%20benchmark%20that%20spans%20across%20imaging%20modalities%20and%20applications.%0ATo%20address%20this%20gap%2C%20we%20create%20and%20open-source%20MedMNIST-C%2C%20a%20benchmark%20dataset%0Abased%20on%20the%20MedMNIST%2B%20collection%20covering%2012%20datasets%20and%209%20imaging%0Amodalities.%20We%20simulate%20task%20and%20modality-specific%20image%20corruptions%20of%20varying%0Aseverity%20to%20comprehensively%20evaluate%20the%20robustness%20of%20established%20algorithms%0Aagainst%20real-world%20artifacts%20and%20distribution%20shifts.%20We%20further%20provide%0Aquantitative%20evidence%20that%20our%20simple-to-use%20artificial%20corruptions%20allow%20for%0Ahighly%20performant%2C%20lightweight%20data%20augmentation%20to%20enhance%20model%20robustness.%0AUnlike%20traditional%2C%20generic%20augmentation%20strategies%2C%20our%20approach%20leverages%0Adomain%20knowledge%2C%20exhibiting%20significantly%20higher%20robustness%20when%20compared%20to%0Awidely%20adopted%20methods.%20By%20introducing%20MedMNIST-C%20and%20open-sourcing%20the%0Acorresponding%20library%20allowing%20for%20targeted%20data%20augmentations%2C%20we%20contribute%0Ato%20the%20development%20of%20increasingly%20robust%20methods%20tailored%20to%20the%20challenges%20of%0Amedical%20imaging.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/medmnistc-api%7D%7Bgithub.com/francescodisalvo05/medmnistc-api.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedMNIST-C%253A%2520Comprehensive%2520benchmark%2520and%2520improved%2520classifier%2520robustness%250A%2520%2520by%2520simulating%2520realistic%2520image%2520corruptions%26entry.906535625%3DFrancesco%2520Di%2520Salvo%2520and%2520Sebastian%2520Doerrich%2520and%2520Christian%2520Ledig%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520neural-network-based%2520systems%2520into%2520clinical%2520practice%2520is%250Alimited%2520by%2520challenges%2520related%2520to%2520domain%2520generalization%2520and%2520robustness.%2520The%250Acomputer%2520vision%2520community%2520established%2520benchmarks%2520such%2520as%2520ImageNet-C%2520as%2520a%250Afundamental%2520prerequisite%2520to%2520measure%2520progress%2520towards%2520those%2520challenges.%2520Similar%250Adatasets%2520are%2520largely%2520absent%2520in%2520the%2520medical%2520imaging%2520community%2520which%2520lacks%2520a%250Acomprehensive%2520benchmark%2520that%2520spans%2520across%2520imaging%2520modalities%2520and%2520applications.%250ATo%2520address%2520this%2520gap%252C%2520we%2520create%2520and%2520open-source%2520MedMNIST-C%252C%2520a%2520benchmark%2520dataset%250Abased%2520on%2520the%2520MedMNIST%252B%2520collection%2520covering%252012%2520datasets%2520and%25209%2520imaging%250Amodalities.%2520We%2520simulate%2520task%2520and%2520modality-specific%2520image%2520corruptions%2520of%2520varying%250Aseverity%2520to%2520comprehensively%2520evaluate%2520the%2520robustness%2520of%2520established%2520algorithms%250Aagainst%2520real-world%2520artifacts%2520and%2520distribution%2520shifts.%2520We%2520further%2520provide%250Aquantitative%2520evidence%2520that%2520our%2520simple-to-use%2520artificial%2520corruptions%2520allow%2520for%250Ahighly%2520performant%252C%2520lightweight%2520data%2520augmentation%2520to%2520enhance%2520model%2520robustness.%250AUnlike%2520traditional%252C%2520generic%2520augmentation%2520strategies%252C%2520our%2520approach%2520leverages%250Adomain%2520knowledge%252C%2520exhibiting%2520significantly%2520higher%2520robustness%2520when%2520compared%2520to%250Awidely%2520adopted%2520methods.%2520By%2520introducing%2520MedMNIST-C%2520and%2520open-sourcing%2520the%250Acorresponding%2520library%2520allowing%2520for%2520targeted%2520data%2520augmentations%252C%2520we%2520contribute%250Ato%2520the%2520development%2520of%2520increasingly%2520robust%2520methods%2520tailored%2520to%2520the%2520challenges%2520of%250Amedical%2520imaging.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/francescodisalvo05/medmnistc-api%257D%257Bgithub.com/francescodisalvo05/medmnistc-api.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedMNIST-C%3A%20Comprehensive%20benchmark%20and%20improved%20classifier%20robustness%0A%20%20by%20simulating%20realistic%20image%20corruptions&entry.906535625=Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Christian%20Ledig&entry.1292438233=%20%20The%20integration%20of%20neural-network-based%20systems%20into%20clinical%20practice%20is%0Alimited%20by%20challenges%20related%20to%20domain%20generalization%20and%20robustness.%20The%0Acomputer%20vision%20community%20established%20benchmarks%20such%20as%20ImageNet-C%20as%20a%0Afundamental%20prerequisite%20to%20measure%20progress%20towards%20those%20challenges.%20Similar%0Adatasets%20are%20largely%20absent%20in%20the%20medical%20imaging%20community%20which%20lacks%20a%0Acomprehensive%20benchmark%20that%20spans%20across%20imaging%20modalities%20and%20applications.%0ATo%20address%20this%20gap%2C%20we%20create%20and%20open-source%20MedMNIST-C%2C%20a%20benchmark%20dataset%0Abased%20on%20the%20MedMNIST%2B%20collection%20covering%2012%20datasets%20and%209%20imaging%0Amodalities.%20We%20simulate%20task%20and%20modality-specific%20image%20corruptions%20of%20varying%0Aseverity%20to%20comprehensively%20evaluate%20the%20robustness%20of%20established%20algorithms%0Aagainst%20real-world%20artifacts%20and%20distribution%20shifts.%20We%20further%20provide%0Aquantitative%20evidence%20that%20our%20simple-to-use%20artificial%20corruptions%20allow%20for%0Ahighly%20performant%2C%20lightweight%20data%20augmentation%20to%20enhance%20model%20robustness.%0AUnlike%20traditional%2C%20generic%20augmentation%20strategies%2C%20our%20approach%20leverages%0Adomain%20knowledge%2C%20exhibiting%20significantly%20higher%20robustness%20when%20compared%20to%0Awidely%20adopted%20methods.%20By%20introducing%20MedMNIST-C%20and%20open-sourcing%20the%0Acorresponding%20library%20allowing%20for%20targeted%20data%20augmentations%2C%20we%20contribute%0Ato%20the%20development%20of%20increasingly%20robust%20methods%20tailored%20to%20the%20challenges%20of%0Amedical%20imaging.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/medmnistc-api%7D%7Bgithub.com/francescodisalvo05/medmnistc-api.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17536v1&entry.124074799=Read"},
{"title": "LaTable: Towards Large Tabular Models", "author": "Boris van Breugel and Jonathan Crabb\u00e9 and Rob Davis and Mihaela van der Schaar", "abstract": "  Tabular data is one of the most ubiquitous modalities, yet the literature on\ntabular generative foundation models is lagging far behind its text and vision\ncounterparts. Creating such a model is hard, due to the heterogeneous feature\nspaces of different tabular datasets, tabular metadata (e.g. dataset\ndescription and feature headers), and tables lacking prior knowledge (e.g.\nfeature order). In this work we propose LaTable: a novel tabular diffusion\nmodel that addresses these challenges and can be trained across different\ndatasets. Through extensive experiments we find that LaTable outperforms\nbaselines on in-distribution generation, and that finetuning LaTable can\ngenerate out-of-distribution datasets better with fewer samples. On the other\nhand, we explore the poor zero-shot performance of LaTable, and what it may\nteach us about building generative tabular foundation models with better zero-\nand few-shot generation capabilities.\n", "link": "http://arxiv.org/abs/2406.17673v1", "date": "2024-06-25", "relevancy": 2.0458, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5218}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5165}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaTable%3A%20Towards%20Large%20Tabular%20Models&body=Title%3A%20LaTable%3A%20Towards%20Large%20Tabular%20Models%0AAuthor%3A%20Boris%20van%20Breugel%20and%20Jonathan%20Crabb%C3%A9%20and%20Rob%20Davis%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20%20%20Tabular%20data%20is%20one%20of%20the%20most%20ubiquitous%20modalities%2C%20yet%20the%20literature%20on%0Atabular%20generative%20foundation%20models%20is%20lagging%20far%20behind%20its%20text%20and%20vision%0Acounterparts.%20Creating%20such%20a%20model%20is%20hard%2C%20due%20to%20the%20heterogeneous%20feature%0Aspaces%20of%20different%20tabular%20datasets%2C%20tabular%20metadata%20%28e.g.%20dataset%0Adescription%20and%20feature%20headers%29%2C%20and%20tables%20lacking%20prior%20knowledge%20%28e.g.%0Afeature%20order%29.%20In%20this%20work%20we%20propose%20LaTable%3A%20a%20novel%20tabular%20diffusion%0Amodel%20that%20addresses%20these%20challenges%20and%20can%20be%20trained%20across%20different%0Adatasets.%20Through%20extensive%20experiments%20we%20find%20that%20LaTable%20outperforms%0Abaselines%20on%20in-distribution%20generation%2C%20and%20that%20finetuning%20LaTable%20can%0Agenerate%20out-of-distribution%20datasets%20better%20with%20fewer%20samples.%20On%20the%20other%0Ahand%2C%20we%20explore%20the%20poor%20zero-shot%20performance%20of%20LaTable%2C%20and%20what%20it%20may%0Ateach%20us%20about%20building%20generative%20tabular%20foundation%20models%20with%20better%20zero-%0Aand%20few-shot%20generation%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaTable%253A%2520Towards%2520Large%2520Tabular%2520Models%26entry.906535625%3DBoris%2520van%2520Breugel%2520and%2520Jonathan%2520Crabb%25C3%25A9%2520and%2520Rob%2520Davis%2520and%2520Mihaela%2520van%2520der%2520Schaar%26entry.1292438233%3D%2520%2520Tabular%2520data%2520is%2520one%2520of%2520the%2520most%2520ubiquitous%2520modalities%252C%2520yet%2520the%2520literature%2520on%250Atabular%2520generative%2520foundation%2520models%2520is%2520lagging%2520far%2520behind%2520its%2520text%2520and%2520vision%250Acounterparts.%2520Creating%2520such%2520a%2520model%2520is%2520hard%252C%2520due%2520to%2520the%2520heterogeneous%2520feature%250Aspaces%2520of%2520different%2520tabular%2520datasets%252C%2520tabular%2520metadata%2520%2528e.g.%2520dataset%250Adescription%2520and%2520feature%2520headers%2529%252C%2520and%2520tables%2520lacking%2520prior%2520knowledge%2520%2528e.g.%250Afeature%2520order%2529.%2520In%2520this%2520work%2520we%2520propose%2520LaTable%253A%2520a%2520novel%2520tabular%2520diffusion%250Amodel%2520that%2520addresses%2520these%2520challenges%2520and%2520can%2520be%2520trained%2520across%2520different%250Adatasets.%2520Through%2520extensive%2520experiments%2520we%2520find%2520that%2520LaTable%2520outperforms%250Abaselines%2520on%2520in-distribution%2520generation%252C%2520and%2520that%2520finetuning%2520LaTable%2520can%250Agenerate%2520out-of-distribution%2520datasets%2520better%2520with%2520fewer%2520samples.%2520On%2520the%2520other%250Ahand%252C%2520we%2520explore%2520the%2520poor%2520zero-shot%2520performance%2520of%2520LaTable%252C%2520and%2520what%2520it%2520may%250Ateach%2520us%2520about%2520building%2520generative%2520tabular%2520foundation%2520models%2520with%2520better%2520zero-%250Aand%2520few-shot%2520generation%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaTable%3A%20Towards%20Large%20Tabular%20Models&entry.906535625=Boris%20van%20Breugel%20and%20Jonathan%20Crabb%C3%A9%20and%20Rob%20Davis%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=%20%20Tabular%20data%20is%20one%20of%20the%20most%20ubiquitous%20modalities%2C%20yet%20the%20literature%20on%0Atabular%20generative%20foundation%20models%20is%20lagging%20far%20behind%20its%20text%20and%20vision%0Acounterparts.%20Creating%20such%20a%20model%20is%20hard%2C%20due%20to%20the%20heterogeneous%20feature%0Aspaces%20of%20different%20tabular%20datasets%2C%20tabular%20metadata%20%28e.g.%20dataset%0Adescription%20and%20feature%20headers%29%2C%20and%20tables%20lacking%20prior%20knowledge%20%28e.g.%0Afeature%20order%29.%20In%20this%20work%20we%20propose%20LaTable%3A%20a%20novel%20tabular%20diffusion%0Amodel%20that%20addresses%20these%20challenges%20and%20can%20be%20trained%20across%20different%0Adatasets.%20Through%20extensive%20experiments%20we%20find%20that%20LaTable%20outperforms%0Abaselines%20on%20in-distribution%20generation%2C%20and%20that%20finetuning%20LaTable%20can%0Agenerate%20out-of-distribution%20datasets%20better%20with%20fewer%20samples.%20On%20the%20other%0Ahand%2C%20we%20explore%20the%20poor%20zero-shot%20performance%20of%20LaTable%2C%20and%20what%20it%20may%0Ateach%20us%20about%20building%20generative%20tabular%20foundation%20models%20with%20better%20zero-%0Aand%20few-shot%20generation%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17673v1&entry.124074799=Read"},
{"title": "Point Tree Transformer for Point Cloud Registration", "author": "Meiling Wang and Guangyan Chen and Yi Yang and Li Yuan and Yufeng Yue", "abstract": "  Point cloud registration is a fundamental task in the fields of computer\nvision and robotics. Recent developments in transformer-based methods have\ndemonstrated enhanced performance in this domain. However, the standard\nattention mechanism utilized in these methods often integrates many\nlow-relevance points, thereby struggling to prioritize its attention weights on\nsparse yet meaningful points. This inefficiency leads to limited local\nstructure modeling capabilities and quadratic computational complexity. To\novercome these limitations, we propose the Point Tree Transformer (PTT), a\nnovel transformer-based approach for point cloud registration that efficiently\nextracts comprehensive local and global features while maintaining linear\ncomputational complexity. The PTT constructs hierarchical feature trees from\npoint clouds in a coarse-to-dense manner, and introduces a novel Point Tree\nAttention (PTA) mechanism, which follows the tree structure to facilitate the\nprogressive convergence of attended regions towards salient points.\nSpecifically, each tree layer selectively identifies a subset of key points\nwith the highest attention scores. Subsequent layers focus attention on areas\nof significant relevance, derived from the child points of the selected point\nset. The feature extraction process additionally incorporates coarse point\nfeatures that capture high-level semantic information, thus facilitating local\nstructure modeling and the progressive integration of multiscale information.\nConsequently, PTA empowers the model to concentrate on crucial local structures\nand derive detailed local information while maintaining linear computational\ncomplexity. Extensive experiments conducted on the 3DMatch, ModelNet40, and\nKITTI datasets demonstrate that our method achieves superior performance over\nthe state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.17530v1", "date": "2024-06-25", "relevancy": 2.0449, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5234}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Tree%20Transformer%20for%20Point%20Cloud%20Registration&body=Title%3A%20Point%20Tree%20Transformer%20for%20Point%20Cloud%20Registration%0AAuthor%3A%20Meiling%20Wang%20and%20Guangyan%20Chen%20and%20Yi%20Yang%20and%20Li%20Yuan%20and%20Yufeng%20Yue%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20a%20fundamental%20task%20in%20the%20fields%20of%20computer%0Avision%20and%20robotics.%20Recent%20developments%20in%20transformer-based%20methods%20have%0Ademonstrated%20enhanced%20performance%20in%20this%20domain.%20However%2C%20the%20standard%0Aattention%20mechanism%20utilized%20in%20these%20methods%20often%20integrates%20many%0Alow-relevance%20points%2C%20thereby%20struggling%20to%20prioritize%20its%20attention%20weights%20on%0Asparse%20yet%20meaningful%20points.%20This%20inefficiency%20leads%20to%20limited%20local%0Astructure%20modeling%20capabilities%20and%20quadratic%20computational%20complexity.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20the%20Point%20Tree%20Transformer%20%28PTT%29%2C%20a%0Anovel%20transformer-based%20approach%20for%20point%20cloud%20registration%20that%20efficiently%0Aextracts%20comprehensive%20local%20and%20global%20features%20while%20maintaining%20linear%0Acomputational%20complexity.%20The%20PTT%20constructs%20hierarchical%20feature%20trees%20from%0Apoint%20clouds%20in%20a%20coarse-to-dense%20manner%2C%20and%20introduces%20a%20novel%20Point%20Tree%0AAttention%20%28PTA%29%20mechanism%2C%20which%20follows%20the%20tree%20structure%20to%20facilitate%20the%0Aprogressive%20convergence%20of%20attended%20regions%20towards%20salient%20points.%0ASpecifically%2C%20each%20tree%20layer%20selectively%20identifies%20a%20subset%20of%20key%20points%0Awith%20the%20highest%20attention%20scores.%20Subsequent%20layers%20focus%20attention%20on%20areas%0Aof%20significant%20relevance%2C%20derived%20from%20the%20child%20points%20of%20the%20selected%20point%0Aset.%20The%20feature%20extraction%20process%20additionally%20incorporates%20coarse%20point%0Afeatures%20that%20capture%20high-level%20semantic%20information%2C%20thus%20facilitating%20local%0Astructure%20modeling%20and%20the%20progressive%20integration%20of%20multiscale%20information.%0AConsequently%2C%20PTA%20empowers%20the%20model%20to%20concentrate%20on%20crucial%20local%20structures%0Aand%20derive%20detailed%20local%20information%20while%20maintaining%20linear%20computational%0Acomplexity.%20Extensive%20experiments%20conducted%20on%20the%203DMatch%2C%20ModelNet40%2C%20and%0AKITTI%20datasets%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%20over%0Athe%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Tree%2520Transformer%2520for%2520Point%2520Cloud%2520Registration%26entry.906535625%3DMeiling%2520Wang%2520and%2520Guangyan%2520Chen%2520and%2520Yi%2520Yang%2520and%2520Li%2520Yuan%2520and%2520Yufeng%2520Yue%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520is%2520a%2520fundamental%2520task%2520in%2520the%2520fields%2520of%2520computer%250Avision%2520and%2520robotics.%2520Recent%2520developments%2520in%2520transformer-based%2520methods%2520have%250Ademonstrated%2520enhanced%2520performance%2520in%2520this%2520domain.%2520However%252C%2520the%2520standard%250Aattention%2520mechanism%2520utilized%2520in%2520these%2520methods%2520often%2520integrates%2520many%250Alow-relevance%2520points%252C%2520thereby%2520struggling%2520to%2520prioritize%2520its%2520attention%2520weights%2520on%250Asparse%2520yet%2520meaningful%2520points.%2520This%2520inefficiency%2520leads%2520to%2520limited%2520local%250Astructure%2520modeling%2520capabilities%2520and%2520quadratic%2520computational%2520complexity.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Point%2520Tree%2520Transformer%2520%2528PTT%2529%252C%2520a%250Anovel%2520transformer-based%2520approach%2520for%2520point%2520cloud%2520registration%2520that%2520efficiently%250Aextracts%2520comprehensive%2520local%2520and%2520global%2520features%2520while%2520maintaining%2520linear%250Acomputational%2520complexity.%2520The%2520PTT%2520constructs%2520hierarchical%2520feature%2520trees%2520from%250Apoint%2520clouds%2520in%2520a%2520coarse-to-dense%2520manner%252C%2520and%2520introduces%2520a%2520novel%2520Point%2520Tree%250AAttention%2520%2528PTA%2529%2520mechanism%252C%2520which%2520follows%2520the%2520tree%2520structure%2520to%2520facilitate%2520the%250Aprogressive%2520convergence%2520of%2520attended%2520regions%2520towards%2520salient%2520points.%250ASpecifically%252C%2520each%2520tree%2520layer%2520selectively%2520identifies%2520a%2520subset%2520of%2520key%2520points%250Awith%2520the%2520highest%2520attention%2520scores.%2520Subsequent%2520layers%2520focus%2520attention%2520on%2520areas%250Aof%2520significant%2520relevance%252C%2520derived%2520from%2520the%2520child%2520points%2520of%2520the%2520selected%2520point%250Aset.%2520The%2520feature%2520extraction%2520process%2520additionally%2520incorporates%2520coarse%2520point%250Afeatures%2520that%2520capture%2520high-level%2520semantic%2520information%252C%2520thus%2520facilitating%2520local%250Astructure%2520modeling%2520and%2520the%2520progressive%2520integration%2520of%2520multiscale%2520information.%250AConsequently%252C%2520PTA%2520empowers%2520the%2520model%2520to%2520concentrate%2520on%2520crucial%2520local%2520structures%250Aand%2520derive%2520detailed%2520local%2520information%2520while%2520maintaining%2520linear%2520computational%250Acomplexity.%2520Extensive%2520experiments%2520conducted%2520on%2520the%25203DMatch%252C%2520ModelNet40%252C%2520and%250AKITTI%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520performance%2520over%250Athe%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Tree%20Transformer%20for%20Point%20Cloud%20Registration&entry.906535625=Meiling%20Wang%20and%20Guangyan%20Chen%20and%20Yi%20Yang%20and%20Li%20Yuan%20and%20Yufeng%20Yue&entry.1292438233=%20%20Point%20cloud%20registration%20is%20a%20fundamental%20task%20in%20the%20fields%20of%20computer%0Avision%20and%20robotics.%20Recent%20developments%20in%20transformer-based%20methods%20have%0Ademonstrated%20enhanced%20performance%20in%20this%20domain.%20However%2C%20the%20standard%0Aattention%20mechanism%20utilized%20in%20these%20methods%20often%20integrates%20many%0Alow-relevance%20points%2C%20thereby%20struggling%20to%20prioritize%20its%20attention%20weights%20on%0Asparse%20yet%20meaningful%20points.%20This%20inefficiency%20leads%20to%20limited%20local%0Astructure%20modeling%20capabilities%20and%20quadratic%20computational%20complexity.%20To%0Aovercome%20these%20limitations%2C%20we%20propose%20the%20Point%20Tree%20Transformer%20%28PTT%29%2C%20a%0Anovel%20transformer-based%20approach%20for%20point%20cloud%20registration%20that%20efficiently%0Aextracts%20comprehensive%20local%20and%20global%20features%20while%20maintaining%20linear%0Acomputational%20complexity.%20The%20PTT%20constructs%20hierarchical%20feature%20trees%20from%0Apoint%20clouds%20in%20a%20coarse-to-dense%20manner%2C%20and%20introduces%20a%20novel%20Point%20Tree%0AAttention%20%28PTA%29%20mechanism%2C%20which%20follows%20the%20tree%20structure%20to%20facilitate%20the%0Aprogressive%20convergence%20of%20attended%20regions%20towards%20salient%20points.%0ASpecifically%2C%20each%20tree%20layer%20selectively%20identifies%20a%20subset%20of%20key%20points%0Awith%20the%20highest%20attention%20scores.%20Subsequent%20layers%20focus%20attention%20on%20areas%0Aof%20significant%20relevance%2C%20derived%20from%20the%20child%20points%20of%20the%20selected%20point%0Aset.%20The%20feature%20extraction%20process%20additionally%20incorporates%20coarse%20point%0Afeatures%20that%20capture%20high-level%20semantic%20information%2C%20thus%20facilitating%20local%0Astructure%20modeling%20and%20the%20progressive%20integration%20of%20multiscale%20information.%0AConsequently%2C%20PTA%20empowers%20the%20model%20to%20concentrate%20on%20crucial%20local%20structures%0Aand%20derive%20detailed%20local%20information%20while%20maintaining%20linear%20computational%0Acomplexity.%20Extensive%20experiments%20conducted%20on%20the%203DMatch%2C%20ModelNet40%2C%20and%0AKITTI%20datasets%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%20over%0Athe%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17530v1&entry.124074799=Read"},
{"title": "Automatic Parameter Tuning of Self-Driving Vehicles", "author": "Hung-Ju Wu and Vladislav Nenchev and Christian Rathgeber", "abstract": "  Modern automated driving solutions utilize trajectory planning and control\ncomponents with numerous parameters that need to be tuned for different driving\nsituations and vehicle types to achieve optimal performance. This paper\nproposes a method to automatically tune such parameters to resemble expert\ndemonstrations. We utilize a cost function which captures deviations of the\nclosed-loop operation of the controller from the recorded desired driving\nbehavior. Parameter tuning is then accomplished by using local optimization\ntechniques. Three optimization alternatives are compared in a case study, where\na trajectory planner is tuned for lane following in a real-world driving\nscenario. The results suggest that the proposed approach improves manually\ntuned initial parameters significantly even with respect to noisy demonstration\ndata.\n", "link": "http://arxiv.org/abs/2406.17757v1", "date": "2024-06-25", "relevancy": 2.0439, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5167}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5073}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Parameter%20Tuning%20of%20Self-Driving%20Vehicles&body=Title%3A%20Automatic%20Parameter%20Tuning%20of%20Self-Driving%20Vehicles%0AAuthor%3A%20Hung-Ju%20Wu%20and%20Vladislav%20Nenchev%20and%20Christian%20Rathgeber%0AAbstract%3A%20%20%20Modern%20automated%20driving%20solutions%20utilize%20trajectory%20planning%20and%20control%0Acomponents%20with%20numerous%20parameters%20that%20need%20to%20be%20tuned%20for%20different%20driving%0Asituations%20and%20vehicle%20types%20to%20achieve%20optimal%20performance.%20This%20paper%0Aproposes%20a%20method%20to%20automatically%20tune%20such%20parameters%20to%20resemble%20expert%0Ademonstrations.%20We%20utilize%20a%20cost%20function%20which%20captures%20deviations%20of%20the%0Aclosed-loop%20operation%20of%20the%20controller%20from%20the%20recorded%20desired%20driving%0Abehavior.%20Parameter%20tuning%20is%20then%20accomplished%20by%20using%20local%20optimization%0Atechniques.%20Three%20optimization%20alternatives%20are%20compared%20in%20a%20case%20study%2C%20where%0Aa%20trajectory%20planner%20is%20tuned%20for%20lane%20following%20in%20a%20real-world%20driving%0Ascenario.%20The%20results%20suggest%20that%20the%20proposed%20approach%20improves%20manually%0Atuned%20initial%20parameters%20significantly%20even%20with%20respect%20to%20noisy%20demonstration%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Parameter%2520Tuning%2520of%2520Self-Driving%2520Vehicles%26entry.906535625%3DHung-Ju%2520Wu%2520and%2520Vladislav%2520Nenchev%2520and%2520Christian%2520Rathgeber%26entry.1292438233%3D%2520%2520Modern%2520automated%2520driving%2520solutions%2520utilize%2520trajectory%2520planning%2520and%2520control%250Acomponents%2520with%2520numerous%2520parameters%2520that%2520need%2520to%2520be%2520tuned%2520for%2520different%2520driving%250Asituations%2520and%2520vehicle%2520types%2520to%2520achieve%2520optimal%2520performance.%2520This%2520paper%250Aproposes%2520a%2520method%2520to%2520automatically%2520tune%2520such%2520parameters%2520to%2520resemble%2520expert%250Ademonstrations.%2520We%2520utilize%2520a%2520cost%2520function%2520which%2520captures%2520deviations%2520of%2520the%250Aclosed-loop%2520operation%2520of%2520the%2520controller%2520from%2520the%2520recorded%2520desired%2520driving%250Abehavior.%2520Parameter%2520tuning%2520is%2520then%2520accomplished%2520by%2520using%2520local%2520optimization%250Atechniques.%2520Three%2520optimization%2520alternatives%2520are%2520compared%2520in%2520a%2520case%2520study%252C%2520where%250Aa%2520trajectory%2520planner%2520is%2520tuned%2520for%2520lane%2520following%2520in%2520a%2520real-world%2520driving%250Ascenario.%2520The%2520results%2520suggest%2520that%2520the%2520proposed%2520approach%2520improves%2520manually%250Atuned%2520initial%2520parameters%2520significantly%2520even%2520with%2520respect%2520to%2520noisy%2520demonstration%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Parameter%20Tuning%20of%20Self-Driving%20Vehicles&entry.906535625=Hung-Ju%20Wu%20and%20Vladislav%20Nenchev%20and%20Christian%20Rathgeber&entry.1292438233=%20%20Modern%20automated%20driving%20solutions%20utilize%20trajectory%20planning%20and%20control%0Acomponents%20with%20numerous%20parameters%20that%20need%20to%20be%20tuned%20for%20different%20driving%0Asituations%20and%20vehicle%20types%20to%20achieve%20optimal%20performance.%20This%20paper%0Aproposes%20a%20method%20to%20automatically%20tune%20such%20parameters%20to%20resemble%20expert%0Ademonstrations.%20We%20utilize%20a%20cost%20function%20which%20captures%20deviations%20of%20the%0Aclosed-loop%20operation%20of%20the%20controller%20from%20the%20recorded%20desired%20driving%0Abehavior.%20Parameter%20tuning%20is%20then%20accomplished%20by%20using%20local%20optimization%0Atechniques.%20Three%20optimization%20alternatives%20are%20compared%20in%20a%20case%20study%2C%20where%0Aa%20trajectory%20planner%20is%20tuned%20for%20lane%20following%20in%20a%20real-world%20driving%0Ascenario.%20The%20results%20suggest%20that%20the%20proposed%20approach%20improves%20manually%0Atuned%20initial%20parameters%20significantly%20even%20with%20respect%20to%20noisy%20demonstration%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17757v1&entry.124074799=Read"},
{"title": "Efficient 3D Molecular Generation with Flow Matching and Scale Optimal\n  Transport", "author": "Ross Irwin and Alessandro Tibo and Jon Paul Janet and Simon Olsson", "abstract": "  Generative models for 3D drug design have gained prominence recently for\ntheir potential to design ligands directly within protein pockets. Current\napproaches, however, often suffer from very slow sampling times or generate\nmolecules with poor chemical validity. Addressing these limitations, we propose\nSemla, a scalable E(3)-equivariant message passing architecture. We further\nintroduce a molecular generation model, SemlaFlow, which is trained using flow\nmatching along with scale optimal transport, a novel extension of equivariant\noptimal transport. Our model produces state-of-the-art results on benchmark\ndatasets with just 100 sampling steps. Crucially, SemlaFlow samples high\nquality molecules with as few as 20 steps, corresponding to a two\norder-of-magnitude speed-up compared to state-of-the-art, without sacrificing\nperformance. Furthermore, we highlight limitations of current evaluation\nmethods for 3D generation and propose new benchmark metrics for unconditional\nmolecular generators. Finally, using these new metrics, we compare our model's\nability to generate high quality samples against current approaches and further\ndemonstrate SemlaFlow's strong performance.\n", "link": "http://arxiv.org/abs/2406.07266v2", "date": "2024-06-25", "relevancy": 2.0394, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5638}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4991}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%203D%20Molecular%20Generation%20with%20Flow%20Matching%20and%20Scale%20Optimal%0A%20%20Transport&body=Title%3A%20Efficient%203D%20Molecular%20Generation%20with%20Flow%20Matching%20and%20Scale%20Optimal%0A%20%20Transport%0AAuthor%3A%20Ross%20Irwin%20and%20Alessandro%20Tibo%20and%20Jon%20Paul%20Janet%20and%20Simon%20Olsson%0AAbstract%3A%20%20%20Generative%20models%20for%203D%20drug%20design%20have%20gained%20prominence%20recently%20for%0Atheir%20potential%20to%20design%20ligands%20directly%20within%20protein%20pockets.%20Current%0Aapproaches%2C%20however%2C%20often%20suffer%20from%20very%20slow%20sampling%20times%20or%20generate%0Amolecules%20with%20poor%20chemical%20validity.%20Addressing%20these%20limitations%2C%20we%20propose%0ASemla%2C%20a%20scalable%20E%283%29-equivariant%20message%20passing%20architecture.%20We%20further%0Aintroduce%20a%20molecular%20generation%20model%2C%20SemlaFlow%2C%20which%20is%20trained%20using%20flow%0Amatching%20along%20with%20scale%20optimal%20transport%2C%20a%20novel%20extension%20of%20equivariant%0Aoptimal%20transport.%20Our%20model%20produces%20state-of-the-art%20results%20on%20benchmark%0Adatasets%20with%20just%20100%20sampling%20steps.%20Crucially%2C%20SemlaFlow%20samples%20high%0Aquality%20molecules%20with%20as%20few%20as%2020%20steps%2C%20corresponding%20to%20a%20two%0Aorder-of-magnitude%20speed-up%20compared%20to%20state-of-the-art%2C%20without%20sacrificing%0Aperformance.%20Furthermore%2C%20we%20highlight%20limitations%20of%20current%20evaluation%0Amethods%20for%203D%20generation%20and%20propose%20new%20benchmark%20metrics%20for%20unconditional%0Amolecular%20generators.%20Finally%2C%20using%20these%20new%20metrics%2C%20we%20compare%20our%20model%27s%0Aability%20to%20generate%20high%20quality%20samples%20against%20current%20approaches%20and%20further%0Ademonstrate%20SemlaFlow%27s%20strong%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07266v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%25203D%2520Molecular%2520Generation%2520with%2520Flow%2520Matching%2520and%2520Scale%2520Optimal%250A%2520%2520Transport%26entry.906535625%3DRoss%2520Irwin%2520and%2520Alessandro%2520Tibo%2520and%2520Jon%2520Paul%2520Janet%2520and%2520Simon%2520Olsson%26entry.1292438233%3D%2520%2520Generative%2520models%2520for%25203D%2520drug%2520design%2520have%2520gained%2520prominence%2520recently%2520for%250Atheir%2520potential%2520to%2520design%2520ligands%2520directly%2520within%2520protein%2520pockets.%2520Current%250Aapproaches%252C%2520however%252C%2520often%2520suffer%2520from%2520very%2520slow%2520sampling%2520times%2520or%2520generate%250Amolecules%2520with%2520poor%2520chemical%2520validity.%2520Addressing%2520these%2520limitations%252C%2520we%2520propose%250ASemla%252C%2520a%2520scalable%2520E%25283%2529-equivariant%2520message%2520passing%2520architecture.%2520We%2520further%250Aintroduce%2520a%2520molecular%2520generation%2520model%252C%2520SemlaFlow%252C%2520which%2520is%2520trained%2520using%2520flow%250Amatching%2520along%2520with%2520scale%2520optimal%2520transport%252C%2520a%2520novel%2520extension%2520of%2520equivariant%250Aoptimal%2520transport.%2520Our%2520model%2520produces%2520state-of-the-art%2520results%2520on%2520benchmark%250Adatasets%2520with%2520just%2520100%2520sampling%2520steps.%2520Crucially%252C%2520SemlaFlow%2520samples%2520high%250Aquality%2520molecules%2520with%2520as%2520few%2520as%252020%2520steps%252C%2520corresponding%2520to%2520a%2520two%250Aorder-of-magnitude%2520speed-up%2520compared%2520to%2520state-of-the-art%252C%2520without%2520sacrificing%250Aperformance.%2520Furthermore%252C%2520we%2520highlight%2520limitations%2520of%2520current%2520evaluation%250Amethods%2520for%25203D%2520generation%2520and%2520propose%2520new%2520benchmark%2520metrics%2520for%2520unconditional%250Amolecular%2520generators.%2520Finally%252C%2520using%2520these%2520new%2520metrics%252C%2520we%2520compare%2520our%2520model%2527s%250Aability%2520to%2520generate%2520high%2520quality%2520samples%2520against%2520current%2520approaches%2520and%2520further%250Ademonstrate%2520SemlaFlow%2527s%2520strong%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07266v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%203D%20Molecular%20Generation%20with%20Flow%20Matching%20and%20Scale%20Optimal%0A%20%20Transport&entry.906535625=Ross%20Irwin%20and%20Alessandro%20Tibo%20and%20Jon%20Paul%20Janet%20and%20Simon%20Olsson&entry.1292438233=%20%20Generative%20models%20for%203D%20drug%20design%20have%20gained%20prominence%20recently%20for%0Atheir%20potential%20to%20design%20ligands%20directly%20within%20protein%20pockets.%20Current%0Aapproaches%2C%20however%2C%20often%20suffer%20from%20very%20slow%20sampling%20times%20or%20generate%0Amolecules%20with%20poor%20chemical%20validity.%20Addressing%20these%20limitations%2C%20we%20propose%0ASemla%2C%20a%20scalable%20E%283%29-equivariant%20message%20passing%20architecture.%20We%20further%0Aintroduce%20a%20molecular%20generation%20model%2C%20SemlaFlow%2C%20which%20is%20trained%20using%20flow%0Amatching%20along%20with%20scale%20optimal%20transport%2C%20a%20novel%20extension%20of%20equivariant%0Aoptimal%20transport.%20Our%20model%20produces%20state-of-the-art%20results%20on%20benchmark%0Adatasets%20with%20just%20100%20sampling%20steps.%20Crucially%2C%20SemlaFlow%20samples%20high%0Aquality%20molecules%20with%20as%20few%20as%2020%20steps%2C%20corresponding%20to%20a%20two%0Aorder-of-magnitude%20speed-up%20compared%20to%20state-of-the-art%2C%20without%20sacrificing%0Aperformance.%20Furthermore%2C%20we%20highlight%20limitations%20of%20current%20evaluation%0Amethods%20for%203D%20generation%20and%20propose%20new%20benchmark%20metrics%20for%20unconditional%0Amolecular%20generators.%20Finally%2C%20using%20these%20new%20metrics%2C%20we%20compare%20our%20model%27s%0Aability%20to%20generate%20high%20quality%20samples%20against%20current%20approaches%20and%20further%0Ademonstrate%20SemlaFlow%27s%20strong%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07266v2&entry.124074799=Read"},
{"title": "Interpreting Attention Layer Outputs with Sparse Autoencoders", "author": "Connor Kissane and Robert Krzyzanowski and Joseph Isaac Bloom and Arthur Conmy and Neel Nanda", "abstract": "  Decomposing model activations into interpretable components is a key open\nproblem in mechanistic interpretability. Sparse autoencoders (SAEs) are a\npopular method for decomposing the internal activations of trained transformers\ninto sparse, interpretable features, and have been applied to MLP layers and\nthe residual stream. In this work we train SAEs on attention layer outputs and\nshow that also here SAEs find a sparse, interpretable decomposition. We\ndemonstrate this on transformers from several model families and up to 2B\nparameters.\n  We perform a qualitative study of the features computed by attention layers,\nand find multiple families: long-range context, short-range context and\ninduction features. We qualitatively study the role of every head in GPT-2\nSmall, and estimate that at least 90% of the heads are polysemantic, i.e. have\nmultiple unrelated roles.\n  Further, we show that Sparse Autoencoders are a useful tool that enable\nresearchers to explain model behavior in greater detail than prior work. For\nexample, we explore the mystery of why models have so many seemingly redundant\ninduction heads, use SAEs to motivate the hypothesis that some are long-prefix\nwhereas others are short-prefix, and confirm this with more rigorous analysis.\nWe use our SAEs to analyze the computation performed by the Indirect Object\nIdentification circuit (Wang et al.), validating that the SAEs find causally\nmeaningful intermediate variables, and deepening our understanding of the\nsemantics of the circuit. We open-source the trained SAEs and a tool for\nexploring arbitrary prompts through the lens of Attention Output SAEs.\n", "link": "http://arxiv.org/abs/2406.17759v1", "date": "2024-06-25", "relevancy": 2.0369, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5311}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5054}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Attention%20Layer%20Outputs%20with%20Sparse%20Autoencoders&body=Title%3A%20Interpreting%20Attention%20Layer%20Outputs%20with%20Sparse%20Autoencoders%0AAuthor%3A%20Connor%20Kissane%20and%20Robert%20Krzyzanowski%20and%20Joseph%20Isaac%20Bloom%20and%20Arthur%20Conmy%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Decomposing%20model%20activations%20into%20interpretable%20components%20is%20a%20key%20open%0Aproblem%20in%20mechanistic%20interpretability.%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%0Apopular%20method%20for%20decomposing%20the%20internal%20activations%20of%20trained%20transformers%0Ainto%20sparse%2C%20interpretable%20features%2C%20and%20have%20been%20applied%20to%20MLP%20layers%20and%0Athe%20residual%20stream.%20In%20this%20work%20we%20train%20SAEs%20on%20attention%20layer%20outputs%20and%0Ashow%20that%20also%20here%20SAEs%20find%20a%20sparse%2C%20interpretable%20decomposition.%20We%0Ademonstrate%20this%20on%20transformers%20from%20several%20model%20families%20and%20up%20to%202B%0Aparameters.%0A%20%20We%20perform%20a%20qualitative%20study%20of%20the%20features%20computed%20by%20attention%20layers%2C%0Aand%20find%20multiple%20families%3A%20long-range%20context%2C%20short-range%20context%20and%0Ainduction%20features.%20We%20qualitatively%20study%20the%20role%20of%20every%20head%20in%20GPT-2%0ASmall%2C%20and%20estimate%20that%20at%20least%2090%25%20of%20the%20heads%20are%20polysemantic%2C%20i.e.%20have%0Amultiple%20unrelated%20roles.%0A%20%20Further%2C%20we%20show%20that%20Sparse%20Autoencoders%20are%20a%20useful%20tool%20that%20enable%0Aresearchers%20to%20explain%20model%20behavior%20in%20greater%20detail%20than%20prior%20work.%20For%0Aexample%2C%20we%20explore%20the%20mystery%20of%20why%20models%20have%20so%20many%20seemingly%20redundant%0Ainduction%20heads%2C%20use%20SAEs%20to%20motivate%20the%20hypothesis%20that%20some%20are%20long-prefix%0Awhereas%20others%20are%20short-prefix%2C%20and%20confirm%20this%20with%20more%20rigorous%20analysis.%0AWe%20use%20our%20SAEs%20to%20analyze%20the%20computation%20performed%20by%20the%20Indirect%20Object%0AIdentification%20circuit%20%28Wang%20et%20al.%29%2C%20validating%20that%20the%20SAEs%20find%20causally%0Ameaningful%20intermediate%20variables%2C%20and%20deepening%20our%20understanding%20of%20the%0Asemantics%20of%20the%20circuit.%20We%20open-source%20the%20trained%20SAEs%20and%20a%20tool%20for%0Aexploring%20arbitrary%20prompts%20through%20the%20lens%20of%20Attention%20Output%20SAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Attention%2520Layer%2520Outputs%2520with%2520Sparse%2520Autoencoders%26entry.906535625%3DConnor%2520Kissane%2520and%2520Robert%2520Krzyzanowski%2520and%2520Joseph%2520Isaac%2520Bloom%2520and%2520Arthur%2520Conmy%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Decomposing%2520model%2520activations%2520into%2520interpretable%2520components%2520is%2520a%2520key%2520open%250Aproblem%2520in%2520mechanistic%2520interpretability.%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%250Apopular%2520method%2520for%2520decomposing%2520the%2520internal%2520activations%2520of%2520trained%2520transformers%250Ainto%2520sparse%252C%2520interpretable%2520features%252C%2520and%2520have%2520been%2520applied%2520to%2520MLP%2520layers%2520and%250Athe%2520residual%2520stream.%2520In%2520this%2520work%2520we%2520train%2520SAEs%2520on%2520attention%2520layer%2520outputs%2520and%250Ashow%2520that%2520also%2520here%2520SAEs%2520find%2520a%2520sparse%252C%2520interpretable%2520decomposition.%2520We%250Ademonstrate%2520this%2520on%2520transformers%2520from%2520several%2520model%2520families%2520and%2520up%2520to%25202B%250Aparameters.%250A%2520%2520We%2520perform%2520a%2520qualitative%2520study%2520of%2520the%2520features%2520computed%2520by%2520attention%2520layers%252C%250Aand%2520find%2520multiple%2520families%253A%2520long-range%2520context%252C%2520short-range%2520context%2520and%250Ainduction%2520features.%2520We%2520qualitatively%2520study%2520the%2520role%2520of%2520every%2520head%2520in%2520GPT-2%250ASmall%252C%2520and%2520estimate%2520that%2520at%2520least%252090%2525%2520of%2520the%2520heads%2520are%2520polysemantic%252C%2520i.e.%2520have%250Amultiple%2520unrelated%2520roles.%250A%2520%2520Further%252C%2520we%2520show%2520that%2520Sparse%2520Autoencoders%2520are%2520a%2520useful%2520tool%2520that%2520enable%250Aresearchers%2520to%2520explain%2520model%2520behavior%2520in%2520greater%2520detail%2520than%2520prior%2520work.%2520For%250Aexample%252C%2520we%2520explore%2520the%2520mystery%2520of%2520why%2520models%2520have%2520so%2520many%2520seemingly%2520redundant%250Ainduction%2520heads%252C%2520use%2520SAEs%2520to%2520motivate%2520the%2520hypothesis%2520that%2520some%2520are%2520long-prefix%250Awhereas%2520others%2520are%2520short-prefix%252C%2520and%2520confirm%2520this%2520with%2520more%2520rigorous%2520analysis.%250AWe%2520use%2520our%2520SAEs%2520to%2520analyze%2520the%2520computation%2520performed%2520by%2520the%2520Indirect%2520Object%250AIdentification%2520circuit%2520%2528Wang%2520et%2520al.%2529%252C%2520validating%2520that%2520the%2520SAEs%2520find%2520causally%250Ameaningful%2520intermediate%2520variables%252C%2520and%2520deepening%2520our%2520understanding%2520of%2520the%250Asemantics%2520of%2520the%2520circuit.%2520We%2520open-source%2520the%2520trained%2520SAEs%2520and%2520a%2520tool%2520for%250Aexploring%2520arbitrary%2520prompts%2520through%2520the%2520lens%2520of%2520Attention%2520Output%2520SAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Attention%20Layer%20Outputs%20with%20Sparse%20Autoencoders&entry.906535625=Connor%20Kissane%20and%20Robert%20Krzyzanowski%20and%20Joseph%20Isaac%20Bloom%20and%20Arthur%20Conmy%20and%20Neel%20Nanda&entry.1292438233=%20%20Decomposing%20model%20activations%20into%20interpretable%20components%20is%20a%20key%20open%0Aproblem%20in%20mechanistic%20interpretability.%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%0Apopular%20method%20for%20decomposing%20the%20internal%20activations%20of%20trained%20transformers%0Ainto%20sparse%2C%20interpretable%20features%2C%20and%20have%20been%20applied%20to%20MLP%20layers%20and%0Athe%20residual%20stream.%20In%20this%20work%20we%20train%20SAEs%20on%20attention%20layer%20outputs%20and%0Ashow%20that%20also%20here%20SAEs%20find%20a%20sparse%2C%20interpretable%20decomposition.%20We%0Ademonstrate%20this%20on%20transformers%20from%20several%20model%20families%20and%20up%20to%202B%0Aparameters.%0A%20%20We%20perform%20a%20qualitative%20study%20of%20the%20features%20computed%20by%20attention%20layers%2C%0Aand%20find%20multiple%20families%3A%20long-range%20context%2C%20short-range%20context%20and%0Ainduction%20features.%20We%20qualitatively%20study%20the%20role%20of%20every%20head%20in%20GPT-2%0ASmall%2C%20and%20estimate%20that%20at%20least%2090%25%20of%20the%20heads%20are%20polysemantic%2C%20i.e.%20have%0Amultiple%20unrelated%20roles.%0A%20%20Further%2C%20we%20show%20that%20Sparse%20Autoencoders%20are%20a%20useful%20tool%20that%20enable%0Aresearchers%20to%20explain%20model%20behavior%20in%20greater%20detail%20than%20prior%20work.%20For%0Aexample%2C%20we%20explore%20the%20mystery%20of%20why%20models%20have%20so%20many%20seemingly%20redundant%0Ainduction%20heads%2C%20use%20SAEs%20to%20motivate%20the%20hypothesis%20that%20some%20are%20long-prefix%0Awhereas%20others%20are%20short-prefix%2C%20and%20confirm%20this%20with%20more%20rigorous%20analysis.%0AWe%20use%20our%20SAEs%20to%20analyze%20the%20computation%20performed%20by%20the%20Indirect%20Object%0AIdentification%20circuit%20%28Wang%20et%20al.%29%2C%20validating%20that%20the%20SAEs%20find%20causally%0Ameaningful%20intermediate%20variables%2C%20and%20deepening%20our%20understanding%20of%20the%0Asemantics%20of%20the%20circuit.%20We%20open-source%20the%20trained%20SAEs%20and%20a%20tool%20for%0Aexploring%20arbitrary%20prompts%20through%20the%20lens%20of%20Attention%20Output%20SAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17759v1&entry.124074799=Read"},
{"title": "SurgeMOD: Translating image-space tissue motions into vision-based\n  surgical forces", "author": "Mikel De Iturrate Reyzabal and Dionysios Malas and Shuai Wang and Sebastien Ourselin and Hongbin Liu", "abstract": "  We present a new approach for vision-based force estimation in Minimally\nInvasive Robotic Surgery based on frequency domain basis of motion of organs\nderived directly from video. Using internal movements generated by natural\nprocesses like breathing or the cardiac cycle, we infer the image-space basis\nof the motion on the frequency domain. As we are working with this\nrepresentation, we discretize the problem to a limited amount of\nlow-frequencies to build an image-space mechanical model of the environment. We\nuse this pre-built model to define our force estimation problem as a dynamic\nconstraint problem. We demonstrate that this method can estimate point contact\nforces reliably for silicone phantom and ex-vivo experiments, matching real\nreadings from a force sensor. In addition, we perform qualitative experiments\nin which we synthesize coherent force textures from surgical videos over a\ncertain region of interest selected by the user. Our method demonstrates good\nresults for both quantitative and qualitative analysis, providing a good\nstarting point for a purely vision-based method for surgical force estimation.\n", "link": "http://arxiv.org/abs/2406.17707v1", "date": "2024-06-25", "relevancy": 2.0352, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5478}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5038}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgeMOD%3A%20Translating%20image-space%20tissue%20motions%20into%20vision-based%0A%20%20surgical%20forces&body=Title%3A%20SurgeMOD%3A%20Translating%20image-space%20tissue%20motions%20into%20vision-based%0A%20%20surgical%20forces%0AAuthor%3A%20Mikel%20De%20Iturrate%20Reyzabal%20and%20Dionysios%20Malas%20and%20Shuai%20Wang%20and%20Sebastien%20Ourselin%20and%20Hongbin%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20for%20vision-based%20force%20estimation%20in%20Minimally%0AInvasive%20Robotic%20Surgery%20based%20on%20frequency%20domain%20basis%20of%20motion%20of%20organs%0Aderived%20directly%20from%20video.%20Using%20internal%20movements%20generated%20by%20natural%0Aprocesses%20like%20breathing%20or%20the%20cardiac%20cycle%2C%20we%20infer%20the%20image-space%20basis%0Aof%20the%20motion%20on%20the%20frequency%20domain.%20As%20we%20are%20working%20with%20this%0Arepresentation%2C%20we%20discretize%20the%20problem%20to%20a%20limited%20amount%20of%0Alow-frequencies%20to%20build%20an%20image-space%20mechanical%20model%20of%20the%20environment.%20We%0Ause%20this%20pre-built%20model%20to%20define%20our%20force%20estimation%20problem%20as%20a%20dynamic%0Aconstraint%20problem.%20We%20demonstrate%20that%20this%20method%20can%20estimate%20point%20contact%0Aforces%20reliably%20for%20silicone%20phantom%20and%20ex-vivo%20experiments%2C%20matching%20real%0Areadings%20from%20a%20force%20sensor.%20In%20addition%2C%20we%20perform%20qualitative%20experiments%0Ain%20which%20we%20synthesize%20coherent%20force%20textures%20from%20surgical%20videos%20over%20a%0Acertain%20region%20of%20interest%20selected%20by%20the%20user.%20Our%20method%20demonstrates%20good%0Aresults%20for%20both%20quantitative%20and%20qualitative%20analysis%2C%20providing%20a%20good%0Astarting%20point%20for%20a%20purely%20vision-based%20method%20for%20surgical%20force%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgeMOD%253A%2520Translating%2520image-space%2520tissue%2520motions%2520into%2520vision-based%250A%2520%2520surgical%2520forces%26entry.906535625%3DMikel%2520De%2520Iturrate%2520Reyzabal%2520and%2520Dionysios%2520Malas%2520and%2520Shuai%2520Wang%2520and%2520Sebastien%2520Ourselin%2520and%2520Hongbin%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520approach%2520for%2520vision-based%2520force%2520estimation%2520in%2520Minimally%250AInvasive%2520Robotic%2520Surgery%2520based%2520on%2520frequency%2520domain%2520basis%2520of%2520motion%2520of%2520organs%250Aderived%2520directly%2520from%2520video.%2520Using%2520internal%2520movements%2520generated%2520by%2520natural%250Aprocesses%2520like%2520breathing%2520or%2520the%2520cardiac%2520cycle%252C%2520we%2520infer%2520the%2520image-space%2520basis%250Aof%2520the%2520motion%2520on%2520the%2520frequency%2520domain.%2520As%2520we%2520are%2520working%2520with%2520this%250Arepresentation%252C%2520we%2520discretize%2520the%2520problem%2520to%2520a%2520limited%2520amount%2520of%250Alow-frequencies%2520to%2520build%2520an%2520image-space%2520mechanical%2520model%2520of%2520the%2520environment.%2520We%250Ause%2520this%2520pre-built%2520model%2520to%2520define%2520our%2520force%2520estimation%2520problem%2520as%2520a%2520dynamic%250Aconstraint%2520problem.%2520We%2520demonstrate%2520that%2520this%2520method%2520can%2520estimate%2520point%2520contact%250Aforces%2520reliably%2520for%2520silicone%2520phantom%2520and%2520ex-vivo%2520experiments%252C%2520matching%2520real%250Areadings%2520from%2520a%2520force%2520sensor.%2520In%2520addition%252C%2520we%2520perform%2520qualitative%2520experiments%250Ain%2520which%2520we%2520synthesize%2520coherent%2520force%2520textures%2520from%2520surgical%2520videos%2520over%2520a%250Acertain%2520region%2520of%2520interest%2520selected%2520by%2520the%2520user.%2520Our%2520method%2520demonstrates%2520good%250Aresults%2520for%2520both%2520quantitative%2520and%2520qualitative%2520analysis%252C%2520providing%2520a%2520good%250Astarting%2520point%2520for%2520a%2520purely%2520vision-based%2520method%2520for%2520surgical%2520force%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgeMOD%3A%20Translating%20image-space%20tissue%20motions%20into%20vision-based%0A%20%20surgical%20forces&entry.906535625=Mikel%20De%20Iturrate%20Reyzabal%20and%20Dionysios%20Malas%20and%20Shuai%20Wang%20and%20Sebastien%20Ourselin%20and%20Hongbin%20Liu&entry.1292438233=%20%20We%20present%20a%20new%20approach%20for%20vision-based%20force%20estimation%20in%20Minimally%0AInvasive%20Robotic%20Surgery%20based%20on%20frequency%20domain%20basis%20of%20motion%20of%20organs%0Aderived%20directly%20from%20video.%20Using%20internal%20movements%20generated%20by%20natural%0Aprocesses%20like%20breathing%20or%20the%20cardiac%20cycle%2C%20we%20infer%20the%20image-space%20basis%0Aof%20the%20motion%20on%20the%20frequency%20domain.%20As%20we%20are%20working%20with%20this%0Arepresentation%2C%20we%20discretize%20the%20problem%20to%20a%20limited%20amount%20of%0Alow-frequencies%20to%20build%20an%20image-space%20mechanical%20model%20of%20the%20environment.%20We%0Ause%20this%20pre-built%20model%20to%20define%20our%20force%20estimation%20problem%20as%20a%20dynamic%0Aconstraint%20problem.%20We%20demonstrate%20that%20this%20method%20can%20estimate%20point%20contact%0Aforces%20reliably%20for%20silicone%20phantom%20and%20ex-vivo%20experiments%2C%20matching%20real%0Areadings%20from%20a%20force%20sensor.%20In%20addition%2C%20we%20perform%20qualitative%20experiments%0Ain%20which%20we%20synthesize%20coherent%20force%20textures%20from%20surgical%20videos%20over%20a%0Acertain%20region%20of%20interest%20selected%20by%20the%20user.%20Our%20method%20demonstrates%20good%0Aresults%20for%20both%20quantitative%20and%20qualitative%20analysis%2C%20providing%20a%20good%0Astarting%20point%20for%20a%20purely%20vision-based%20method%20for%20surgical%20force%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17707v1&entry.124074799=Read"},
{"title": "Knowledge Distillation in Automated Annotation: Supervised Text\n  Classification with LLM-Generated Training Labels", "author": "Nicholas Pangakis and Samuel Wolken", "abstract": "  Computational social science (CSS) practitioners often rely on human-labeled\ndata to fine-tune supervised text classifiers. We assess the potential for\nresearchers to augment or replace human-generated training data with surrogate\ntraining labels from generative large language models (LLMs). We introduce a\nrecommended workflow and test this LLM application by replicating 14\nclassification tasks and measuring performance. We employ a novel corpus of\nEnglish-language text classification data sets from recent CSS articles in\nhigh-impact journals. Because these data sets are stored in password-protected\narchives, our analyses are less prone to issues of contamination. For each\ntask, we compare supervised classifiers fine-tuned using GPT-4 labels against\nclassifiers fine-tuned with human annotations and against labels from GPT-4 and\nMistral-7B with few-shot in-context learning. Our findings indicate that\nsupervised classification models fine-tuned on LLM-generated labels perform\ncomparably to models fine-tuned with labels from human annotators. Fine-tuning\nmodels using LLM-generated labels can be a fast, efficient and cost-effective\nmethod of building supervised text classifiers.\n", "link": "http://arxiv.org/abs/2406.17633v1", "date": "2024-06-25", "relevancy": 2.0351, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Distillation%20in%20Automated%20Annotation%3A%20Supervised%20Text%0A%20%20Classification%20with%20LLM-Generated%20Training%20Labels&body=Title%3A%20Knowledge%20Distillation%20in%20Automated%20Annotation%3A%20Supervised%20Text%0A%20%20Classification%20with%20LLM-Generated%20Training%20Labels%0AAuthor%3A%20Nicholas%20Pangakis%20and%20Samuel%20Wolken%0AAbstract%3A%20%20%20Computational%20social%20science%20%28CSS%29%20practitioners%20often%20rely%20on%20human-labeled%0Adata%20to%20fine-tune%20supervised%20text%20classifiers.%20We%20assess%20the%20potential%20for%0Aresearchers%20to%20augment%20or%20replace%20human-generated%20training%20data%20with%20surrogate%0Atraining%20labels%20from%20generative%20large%20language%20models%20%28LLMs%29.%20We%20introduce%20a%0Arecommended%20workflow%20and%20test%20this%20LLM%20application%20by%20replicating%2014%0Aclassification%20tasks%20and%20measuring%20performance.%20We%20employ%20a%20novel%20corpus%20of%0AEnglish-language%20text%20classification%20data%20sets%20from%20recent%20CSS%20articles%20in%0Ahigh-impact%20journals.%20Because%20these%20data%20sets%20are%20stored%20in%20password-protected%0Aarchives%2C%20our%20analyses%20are%20less%20prone%20to%20issues%20of%20contamination.%20For%20each%0Atask%2C%20we%20compare%20supervised%20classifiers%20fine-tuned%20using%20GPT-4%20labels%20against%0Aclassifiers%20fine-tuned%20with%20human%20annotations%20and%20against%20labels%20from%20GPT-4%20and%0AMistral-7B%20with%20few-shot%20in-context%20learning.%20Our%20findings%20indicate%20that%0Asupervised%20classification%20models%20fine-tuned%20on%20LLM-generated%20labels%20perform%0Acomparably%20to%20models%20fine-tuned%20with%20labels%20from%20human%20annotators.%20Fine-tuning%0Amodels%20using%20LLM-generated%20labels%20can%20be%20a%20fast%2C%20efficient%20and%20cost-effective%0Amethod%20of%20building%20supervised%20text%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Distillation%2520in%2520Automated%2520Annotation%253A%2520Supervised%2520Text%250A%2520%2520Classification%2520with%2520LLM-Generated%2520Training%2520Labels%26entry.906535625%3DNicholas%2520Pangakis%2520and%2520Samuel%2520Wolken%26entry.1292438233%3D%2520%2520Computational%2520social%2520science%2520%2528CSS%2529%2520practitioners%2520often%2520rely%2520on%2520human-labeled%250Adata%2520to%2520fine-tune%2520supervised%2520text%2520classifiers.%2520We%2520assess%2520the%2520potential%2520for%250Aresearchers%2520to%2520augment%2520or%2520replace%2520human-generated%2520training%2520data%2520with%2520surrogate%250Atraining%2520labels%2520from%2520generative%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%2520introduce%2520a%250Arecommended%2520workflow%2520and%2520test%2520this%2520LLM%2520application%2520by%2520replicating%252014%250Aclassification%2520tasks%2520and%2520measuring%2520performance.%2520We%2520employ%2520a%2520novel%2520corpus%2520of%250AEnglish-language%2520text%2520classification%2520data%2520sets%2520from%2520recent%2520CSS%2520articles%2520in%250Ahigh-impact%2520journals.%2520Because%2520these%2520data%2520sets%2520are%2520stored%2520in%2520password-protected%250Aarchives%252C%2520our%2520analyses%2520are%2520less%2520prone%2520to%2520issues%2520of%2520contamination.%2520For%2520each%250Atask%252C%2520we%2520compare%2520supervised%2520classifiers%2520fine-tuned%2520using%2520GPT-4%2520labels%2520against%250Aclassifiers%2520fine-tuned%2520with%2520human%2520annotations%2520and%2520against%2520labels%2520from%2520GPT-4%2520and%250AMistral-7B%2520with%2520few-shot%2520in-context%2520learning.%2520Our%2520findings%2520indicate%2520that%250Asupervised%2520classification%2520models%2520fine-tuned%2520on%2520LLM-generated%2520labels%2520perform%250Acomparably%2520to%2520models%2520fine-tuned%2520with%2520labels%2520from%2520human%2520annotators.%2520Fine-tuning%250Amodels%2520using%2520LLM-generated%2520labels%2520can%2520be%2520a%2520fast%252C%2520efficient%2520and%2520cost-effective%250Amethod%2520of%2520building%2520supervised%2520text%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Distillation%20in%20Automated%20Annotation%3A%20Supervised%20Text%0A%20%20Classification%20with%20LLM-Generated%20Training%20Labels&entry.906535625=Nicholas%20Pangakis%20and%20Samuel%20Wolken&entry.1292438233=%20%20Computational%20social%20science%20%28CSS%29%20practitioners%20often%20rely%20on%20human-labeled%0Adata%20to%20fine-tune%20supervised%20text%20classifiers.%20We%20assess%20the%20potential%20for%0Aresearchers%20to%20augment%20or%20replace%20human-generated%20training%20data%20with%20surrogate%0Atraining%20labels%20from%20generative%20large%20language%20models%20%28LLMs%29.%20We%20introduce%20a%0Arecommended%20workflow%20and%20test%20this%20LLM%20application%20by%20replicating%2014%0Aclassification%20tasks%20and%20measuring%20performance.%20We%20employ%20a%20novel%20corpus%20of%0AEnglish-language%20text%20classification%20data%20sets%20from%20recent%20CSS%20articles%20in%0Ahigh-impact%20journals.%20Because%20these%20data%20sets%20are%20stored%20in%20password-protected%0Aarchives%2C%20our%20analyses%20are%20less%20prone%20to%20issues%20of%20contamination.%20For%20each%0Atask%2C%20we%20compare%20supervised%20classifiers%20fine-tuned%20using%20GPT-4%20labels%20against%0Aclassifiers%20fine-tuned%20with%20human%20annotations%20and%20against%20labels%20from%20GPT-4%20and%0AMistral-7B%20with%20few-shot%20in-context%20learning.%20Our%20findings%20indicate%20that%0Asupervised%20classification%20models%20fine-tuned%20on%20LLM-generated%20labels%20perform%0Acomparably%20to%20models%20fine-tuned%20with%20labels%20from%20human%20annotators.%20Fine-tuning%0Amodels%20using%20LLM-generated%20labels%20can%20be%20a%20fast%2C%20efficient%20and%20cost-effective%0Amethod%20of%20building%20supervised%20text%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17633v1&entry.124074799=Read"},
{"title": "Structured Unrestricted-Rank Matrices for Parameter Efficient\n  Fine-tuning", "author": "Arijit Sehanobish and Avinava Dubey and Krzysztof Choromanski and Somnath Basu Roy Chowdhury and Deepali Jain and Vikas Sindhwani and Snigdha Chaturvedi", "abstract": "  Recent efforts to scale Transformer models have demonstrated rapid progress\nacross a wide range of tasks (Wei et al., 2022). However, fine-tuning these\nmodels for downstream tasks is expensive due to their large parameter counts.\nParameter-efficient fine-tuning (PEFT) approaches have emerged as a viable\nalternative by allowing us to fine-tune models by updating only a small number\nof parameters. In this work, we propose a general framework for parameter\nefficient fine-tuning (PEFT), based on structured unrestricted-rank matrices\n(SURM) which can serve as a drop-in replacement for popular approaches such as\nAdapters and LoRA. Unlike other methods like LoRA, SURMs provides more\nflexibility in finding the right balance between compactness and\nexpressiveness. This is achieved by using low displacement rank matrices\n(LDRMs), which hasn't been used in this context before. SURMs remain\ncompetitive with baselines, often providing significant quality improvements\nwhile using a smaller parameter budget. SURMs achieve 5-7% accuracy gains on\nvarious image classification tasks while replacing low-rank matrices in LoRA.\nIt also results in up to 12x reduction of the number of parameters in adapters\n(with virtually no loss in quality) on the GLUE benchmark.\n", "link": "http://arxiv.org/abs/2406.17740v1", "date": "2024-06-25", "relevancy": 2.0346, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structured%20Unrestricted-Rank%20Matrices%20for%20Parameter%20Efficient%0A%20%20Fine-tuning&body=Title%3A%20Structured%20Unrestricted-Rank%20Matrices%20for%20Parameter%20Efficient%0A%20%20Fine-tuning%0AAuthor%3A%20Arijit%20Sehanobish%20and%20Avinava%20Dubey%20and%20Krzysztof%20Choromanski%20and%20Somnath%20Basu%20Roy%20Chowdhury%20and%20Deepali%20Jain%20and%20Vikas%20Sindhwani%20and%20Snigdha%20Chaturvedi%0AAbstract%3A%20%20%20Recent%20efforts%20to%20scale%20Transformer%20models%20have%20demonstrated%20rapid%20progress%0Aacross%20a%20wide%20range%20of%20tasks%20%28Wei%20et%20al.%2C%202022%29.%20However%2C%20fine-tuning%20these%0Amodels%20for%20downstream%20tasks%20is%20expensive%20due%20to%20their%20large%20parameter%20counts.%0AParameter-efficient%20fine-tuning%20%28PEFT%29%20approaches%20have%20emerged%20as%20a%20viable%0Aalternative%20by%20allowing%20us%20to%20fine-tune%20models%20by%20updating%20only%20a%20small%20number%0Aof%20parameters.%20In%20this%20work%2C%20we%20propose%20a%20general%20framework%20for%20parameter%0Aefficient%20fine-tuning%20%28PEFT%29%2C%20based%20on%20structured%20unrestricted-rank%20matrices%0A%28SURM%29%20which%20can%20serve%20as%20a%20drop-in%20replacement%20for%20popular%20approaches%20such%20as%0AAdapters%20and%20LoRA.%20Unlike%20other%20methods%20like%20LoRA%2C%20SURMs%20provides%20more%0Aflexibility%20in%20finding%20the%20right%20balance%20between%20compactness%20and%0Aexpressiveness.%20This%20is%20achieved%20by%20using%20low%20displacement%20rank%20matrices%0A%28LDRMs%29%2C%20which%20hasn%27t%20been%20used%20in%20this%20context%20before.%20SURMs%20remain%0Acompetitive%20with%20baselines%2C%20often%20providing%20significant%20quality%20improvements%0Awhile%20using%20a%20smaller%20parameter%20budget.%20SURMs%20achieve%205-7%25%20accuracy%20gains%20on%0Avarious%20image%20classification%20tasks%20while%20replacing%20low-rank%20matrices%20in%20LoRA.%0AIt%20also%20results%20in%20up%20to%2012x%20reduction%20of%20the%20number%20of%20parameters%20in%20adapters%0A%28with%20virtually%20no%20loss%20in%20quality%29%20on%20the%20GLUE%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructured%2520Unrestricted-Rank%2520Matrices%2520for%2520Parameter%2520Efficient%250A%2520%2520Fine-tuning%26entry.906535625%3DArijit%2520Sehanobish%2520and%2520Avinava%2520Dubey%2520and%2520Krzysztof%2520Choromanski%2520and%2520Somnath%2520Basu%2520Roy%2520Chowdhury%2520and%2520Deepali%2520Jain%2520and%2520Vikas%2520Sindhwani%2520and%2520Snigdha%2520Chaturvedi%26entry.1292438233%3D%2520%2520Recent%2520efforts%2520to%2520scale%2520Transformer%2520models%2520have%2520demonstrated%2520rapid%2520progress%250Aacross%2520a%2520wide%2520range%2520of%2520tasks%2520%2528Wei%2520et%2520al.%252C%25202022%2529.%2520However%252C%2520fine-tuning%2520these%250Amodels%2520for%2520downstream%2520tasks%2520is%2520expensive%2520due%2520to%2520their%2520large%2520parameter%2520counts.%250AParameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520approaches%2520have%2520emerged%2520as%2520a%2520viable%250Aalternative%2520by%2520allowing%2520us%2520to%2520fine-tune%2520models%2520by%2520updating%2520only%2520a%2520small%2520number%250Aof%2520parameters.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520general%2520framework%2520for%2520parameter%250Aefficient%2520fine-tuning%2520%2528PEFT%2529%252C%2520based%2520on%2520structured%2520unrestricted-rank%2520matrices%250A%2528SURM%2529%2520which%2520can%2520serve%2520as%2520a%2520drop-in%2520replacement%2520for%2520popular%2520approaches%2520such%2520as%250AAdapters%2520and%2520LoRA.%2520Unlike%2520other%2520methods%2520like%2520LoRA%252C%2520SURMs%2520provides%2520more%250Aflexibility%2520in%2520finding%2520the%2520right%2520balance%2520between%2520compactness%2520and%250Aexpressiveness.%2520This%2520is%2520achieved%2520by%2520using%2520low%2520displacement%2520rank%2520matrices%250A%2528LDRMs%2529%252C%2520which%2520hasn%2527t%2520been%2520used%2520in%2520this%2520context%2520before.%2520SURMs%2520remain%250Acompetitive%2520with%2520baselines%252C%2520often%2520providing%2520significant%2520quality%2520improvements%250Awhile%2520using%2520a%2520smaller%2520parameter%2520budget.%2520SURMs%2520achieve%25205-7%2525%2520accuracy%2520gains%2520on%250Avarious%2520image%2520classification%2520tasks%2520while%2520replacing%2520low-rank%2520matrices%2520in%2520LoRA.%250AIt%2520also%2520results%2520in%2520up%2520to%252012x%2520reduction%2520of%2520the%2520number%2520of%2520parameters%2520in%2520adapters%250A%2528with%2520virtually%2520no%2520loss%2520in%2520quality%2529%2520on%2520the%2520GLUE%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structured%20Unrestricted-Rank%20Matrices%20for%20Parameter%20Efficient%0A%20%20Fine-tuning&entry.906535625=Arijit%20Sehanobish%20and%20Avinava%20Dubey%20and%20Krzysztof%20Choromanski%20and%20Somnath%20Basu%20Roy%20Chowdhury%20and%20Deepali%20Jain%20and%20Vikas%20Sindhwani%20and%20Snigdha%20Chaturvedi&entry.1292438233=%20%20Recent%20efforts%20to%20scale%20Transformer%20models%20have%20demonstrated%20rapid%20progress%0Aacross%20a%20wide%20range%20of%20tasks%20%28Wei%20et%20al.%2C%202022%29.%20However%2C%20fine-tuning%20these%0Amodels%20for%20downstream%20tasks%20is%20expensive%20due%20to%20their%20large%20parameter%20counts.%0AParameter-efficient%20fine-tuning%20%28PEFT%29%20approaches%20have%20emerged%20as%20a%20viable%0Aalternative%20by%20allowing%20us%20to%20fine-tune%20models%20by%20updating%20only%20a%20small%20number%0Aof%20parameters.%20In%20this%20work%2C%20we%20propose%20a%20general%20framework%20for%20parameter%0Aefficient%20fine-tuning%20%28PEFT%29%2C%20based%20on%20structured%20unrestricted-rank%20matrices%0A%28SURM%29%20which%20can%20serve%20as%20a%20drop-in%20replacement%20for%20popular%20approaches%20such%20as%0AAdapters%20and%20LoRA.%20Unlike%20other%20methods%20like%20LoRA%2C%20SURMs%20provides%20more%0Aflexibility%20in%20finding%20the%20right%20balance%20between%20compactness%20and%0Aexpressiveness.%20This%20is%20achieved%20by%20using%20low%20displacement%20rank%20matrices%0A%28LDRMs%29%2C%20which%20hasn%27t%20been%20used%20in%20this%20context%20before.%20SURMs%20remain%0Acompetitive%20with%20baselines%2C%20often%20providing%20significant%20quality%20improvements%0Awhile%20using%20a%20smaller%20parameter%20budget.%20SURMs%20achieve%205-7%25%20accuracy%20gains%20on%0Avarious%20image%20classification%20tasks%20while%20replacing%20low-rank%20matrices%20in%20LoRA.%0AIt%20also%20results%20in%20up%20to%2012x%20reduction%20of%20the%20number%20of%20parameters%20in%20adapters%0A%28with%20virtually%20no%20loss%20in%20quality%29%20on%20the%20GLUE%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17740v1&entry.124074799=Read"},
{"title": "CuDA2: An approach for Incorporating Traitor Agents into Cooperative\n  Multi-Agent Systems", "author": "Zhen Chen and Yong Liao and Youpeng Zhao and Zipeng Dai and Jian Zhao", "abstract": "  Cooperative Multi-Agent Reinforcement Learning (CMARL) strategies are well\nknown to be vulnerable to adversarial perturbations. Previous works on\nadversarial attacks have primarily focused on white-box attacks that directly\nperturb the states or actions of victim agents, often in scenarios with a\nlimited number of attacks. However, gaining complete access to victim agents in\nreal-world environments is exceedingly difficult. To create more realistic\nadversarial attacks, we introduce a novel method that involves injecting\ntraitor agents into the CMARL system. We model this problem as a Traitor Markov\nDecision Process (TMDP), where traitors cannot directly attack the victim\nagents but can influence their formation or positioning through collisions. In\nTMDP, traitors are trained using the same MARL algorithm as the victim agents,\nwith their reward function set as the negative of the victim agents' reward.\nDespite this, the training efficiency for traitors remains low because it is\nchallenging for them to directly associate their actions with the victim\nagents' rewards. To address this issue, we propose the Curiosity-Driven\nAdversarial Attack (CuDA2) framework. CuDA2 enhances the efficiency and\naggressiveness of attacks on the specified victim agents' policies while\nmaintaining the optimal policy invariance of the traitors. Specifically, we\nemploy a pre-trained Random Network Distillation (RND) module, where the extra\nreward generated by the RND module encourages traitors to explore states\nunencountered by the victim agents. Extensive experiments on various scenarios\nfrom SMAC demonstrate that our CuDA2 framework offers comparable or superior\nadversarial attack capabilities compared to other baselines.\n", "link": "http://arxiv.org/abs/2406.17425v1", "date": "2024-06-25", "relevancy": 2.0132, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.519}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4986}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CuDA2%3A%20An%20approach%20for%20Incorporating%20Traitor%20Agents%20into%20Cooperative%0A%20%20Multi-Agent%20Systems&body=Title%3A%20CuDA2%3A%20An%20approach%20for%20Incorporating%20Traitor%20Agents%20into%20Cooperative%0A%20%20Multi-Agent%20Systems%0AAuthor%3A%20Zhen%20Chen%20and%20Yong%20Liao%20and%20Youpeng%20Zhao%20and%20Zipeng%20Dai%20and%20Jian%20Zhao%0AAbstract%3A%20%20%20Cooperative%20Multi-Agent%20Reinforcement%20Learning%20%28CMARL%29%20strategies%20are%20well%0Aknown%20to%20be%20vulnerable%20to%20adversarial%20perturbations.%20Previous%20works%20on%0Aadversarial%20attacks%20have%20primarily%20focused%20on%20white-box%20attacks%20that%20directly%0Aperturb%20the%20states%20or%20actions%20of%20victim%20agents%2C%20often%20in%20scenarios%20with%20a%0Alimited%20number%20of%20attacks.%20However%2C%20gaining%20complete%20access%20to%20victim%20agents%20in%0Areal-world%20environments%20is%20exceedingly%20difficult.%20To%20create%20more%20realistic%0Aadversarial%20attacks%2C%20we%20introduce%20a%20novel%20method%20that%20involves%20injecting%0Atraitor%20agents%20into%20the%20CMARL%20system.%20We%20model%20this%20problem%20as%20a%20Traitor%20Markov%0ADecision%20Process%20%28TMDP%29%2C%20where%20traitors%20cannot%20directly%20attack%20the%20victim%0Aagents%20but%20can%20influence%20their%20formation%20or%20positioning%20through%20collisions.%20In%0ATMDP%2C%20traitors%20are%20trained%20using%20the%20same%20MARL%20algorithm%20as%20the%20victim%20agents%2C%0Awith%20their%20reward%20function%20set%20as%20the%20negative%20of%20the%20victim%20agents%27%20reward.%0ADespite%20this%2C%20the%20training%20efficiency%20for%20traitors%20remains%20low%20because%20it%20is%0Achallenging%20for%20them%20to%20directly%20associate%20their%20actions%20with%20the%20victim%0Aagents%27%20rewards.%20To%20address%20this%20issue%2C%20we%20propose%20the%20Curiosity-Driven%0AAdversarial%20Attack%20%28CuDA2%29%20framework.%20CuDA2%20enhances%20the%20efficiency%20and%0Aaggressiveness%20of%20attacks%20on%20the%20specified%20victim%20agents%27%20policies%20while%0Amaintaining%20the%20optimal%20policy%20invariance%20of%20the%20traitors.%20Specifically%2C%20we%0Aemploy%20a%20pre-trained%20Random%20Network%20Distillation%20%28RND%29%20module%2C%20where%20the%20extra%0Areward%20generated%20by%20the%20RND%20module%20encourages%20traitors%20to%20explore%20states%0Aunencountered%20by%20the%20victim%20agents.%20Extensive%20experiments%20on%20various%20scenarios%0Afrom%20SMAC%20demonstrate%20that%20our%20CuDA2%20framework%20offers%20comparable%20or%20superior%0Aadversarial%20attack%20capabilities%20compared%20to%20other%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCuDA2%253A%2520An%2520approach%2520for%2520Incorporating%2520Traitor%2520Agents%2520into%2520Cooperative%250A%2520%2520Multi-Agent%2520Systems%26entry.906535625%3DZhen%2520Chen%2520and%2520Yong%2520Liao%2520and%2520Youpeng%2520Zhao%2520and%2520Zipeng%2520Dai%2520and%2520Jian%2520Zhao%26entry.1292438233%3D%2520%2520Cooperative%2520Multi-Agent%2520Reinforcement%2520Learning%2520%2528CMARL%2529%2520strategies%2520are%2520well%250Aknown%2520to%2520be%2520vulnerable%2520to%2520adversarial%2520perturbations.%2520Previous%2520works%2520on%250Aadversarial%2520attacks%2520have%2520primarily%2520focused%2520on%2520white-box%2520attacks%2520that%2520directly%250Aperturb%2520the%2520states%2520or%2520actions%2520of%2520victim%2520agents%252C%2520often%2520in%2520scenarios%2520with%2520a%250Alimited%2520number%2520of%2520attacks.%2520However%252C%2520gaining%2520complete%2520access%2520to%2520victim%2520agents%2520in%250Areal-world%2520environments%2520is%2520exceedingly%2520difficult.%2520To%2520create%2520more%2520realistic%250Aadversarial%2520attacks%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%2520involves%2520injecting%250Atraitor%2520agents%2520into%2520the%2520CMARL%2520system.%2520We%2520model%2520this%2520problem%2520as%2520a%2520Traitor%2520Markov%250ADecision%2520Process%2520%2528TMDP%2529%252C%2520where%2520traitors%2520cannot%2520directly%2520attack%2520the%2520victim%250Aagents%2520but%2520can%2520influence%2520their%2520formation%2520or%2520positioning%2520through%2520collisions.%2520In%250ATMDP%252C%2520traitors%2520are%2520trained%2520using%2520the%2520same%2520MARL%2520algorithm%2520as%2520the%2520victim%2520agents%252C%250Awith%2520their%2520reward%2520function%2520set%2520as%2520the%2520negative%2520of%2520the%2520victim%2520agents%2527%2520reward.%250ADespite%2520this%252C%2520the%2520training%2520efficiency%2520for%2520traitors%2520remains%2520low%2520because%2520it%2520is%250Achallenging%2520for%2520them%2520to%2520directly%2520associate%2520their%2520actions%2520with%2520the%2520victim%250Aagents%2527%2520rewards.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520Curiosity-Driven%250AAdversarial%2520Attack%2520%2528CuDA2%2529%2520framework.%2520CuDA2%2520enhances%2520the%2520efficiency%2520and%250Aaggressiveness%2520of%2520attacks%2520on%2520the%2520specified%2520victim%2520agents%2527%2520policies%2520while%250Amaintaining%2520the%2520optimal%2520policy%2520invariance%2520of%2520the%2520traitors.%2520Specifically%252C%2520we%250Aemploy%2520a%2520pre-trained%2520Random%2520Network%2520Distillation%2520%2528RND%2529%2520module%252C%2520where%2520the%2520extra%250Areward%2520generated%2520by%2520the%2520RND%2520module%2520encourages%2520traitors%2520to%2520explore%2520states%250Aunencountered%2520by%2520the%2520victim%2520agents.%2520Extensive%2520experiments%2520on%2520various%2520scenarios%250Afrom%2520SMAC%2520demonstrate%2520that%2520our%2520CuDA2%2520framework%2520offers%2520comparable%2520or%2520superior%250Aadversarial%2520attack%2520capabilities%2520compared%2520to%2520other%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CuDA2%3A%20An%20approach%20for%20Incorporating%20Traitor%20Agents%20into%20Cooperative%0A%20%20Multi-Agent%20Systems&entry.906535625=Zhen%20Chen%20and%20Yong%20Liao%20and%20Youpeng%20Zhao%20and%20Zipeng%20Dai%20and%20Jian%20Zhao&entry.1292438233=%20%20Cooperative%20Multi-Agent%20Reinforcement%20Learning%20%28CMARL%29%20strategies%20are%20well%0Aknown%20to%20be%20vulnerable%20to%20adversarial%20perturbations.%20Previous%20works%20on%0Aadversarial%20attacks%20have%20primarily%20focused%20on%20white-box%20attacks%20that%20directly%0Aperturb%20the%20states%20or%20actions%20of%20victim%20agents%2C%20often%20in%20scenarios%20with%20a%0Alimited%20number%20of%20attacks.%20However%2C%20gaining%20complete%20access%20to%20victim%20agents%20in%0Areal-world%20environments%20is%20exceedingly%20difficult.%20To%20create%20more%20realistic%0Aadversarial%20attacks%2C%20we%20introduce%20a%20novel%20method%20that%20involves%20injecting%0Atraitor%20agents%20into%20the%20CMARL%20system.%20We%20model%20this%20problem%20as%20a%20Traitor%20Markov%0ADecision%20Process%20%28TMDP%29%2C%20where%20traitors%20cannot%20directly%20attack%20the%20victim%0Aagents%20but%20can%20influence%20their%20formation%20or%20positioning%20through%20collisions.%20In%0ATMDP%2C%20traitors%20are%20trained%20using%20the%20same%20MARL%20algorithm%20as%20the%20victim%20agents%2C%0Awith%20their%20reward%20function%20set%20as%20the%20negative%20of%20the%20victim%20agents%27%20reward.%0ADespite%20this%2C%20the%20training%20efficiency%20for%20traitors%20remains%20low%20because%20it%20is%0Achallenging%20for%20them%20to%20directly%20associate%20their%20actions%20with%20the%20victim%0Aagents%27%20rewards.%20To%20address%20this%20issue%2C%20we%20propose%20the%20Curiosity-Driven%0AAdversarial%20Attack%20%28CuDA2%29%20framework.%20CuDA2%20enhances%20the%20efficiency%20and%0Aaggressiveness%20of%20attacks%20on%20the%20specified%20victim%20agents%27%20policies%20while%0Amaintaining%20the%20optimal%20policy%20invariance%20of%20the%20traitors.%20Specifically%2C%20we%0Aemploy%20a%20pre-trained%20Random%20Network%20Distillation%20%28RND%29%20module%2C%20where%20the%20extra%0Areward%20generated%20by%20the%20RND%20module%20encourages%20traitors%20to%20explore%20states%0Aunencountered%20by%20the%20victim%20agents.%20Extensive%20experiments%20on%20various%20scenarios%0Afrom%20SMAC%20demonstrate%20that%20our%20CuDA2%20framework%20offers%20comparable%20or%20superior%0Aadversarial%20attack%20capabilities%20compared%20to%20other%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17425v1&entry.124074799=Read"},
{"title": "Deep Pulse-Signal Magnification for remote Heart Rate Estimation in\n  Compressed Videos", "author": "Joaquim Comas and Adria Ruiz and Federico Sukno", "abstract": "  Recent advancements in data-driven approaches for remote photoplethysmography\n(rPPG) have significantly improved the accuracy of remote heart rate\nestimation. However, the performance of such approaches worsens considerably\nunder video compression, which is nevertheless necessary to store and transmit\nvideo data efficiently. In this paper, we present a novel approach to address\nthe impact of video compression on rPPG estimation, which leverages a\npulse-signal magnification transformation to adapt compressed videos to an\nuncompressed data domain in which the rPPG signal is magnified. We validate the\neffectiveness of our model by exhaustive evaluations on two publicly available\ndatasets, UCLA-rPPG and UBFC-rPPG, employing both intra- and cross-database\nperformance at several compression rates. Additionally, we assess the\nrobustness of our approach on two additional highly compressed and widely-used\ndatasets, MAHNOB-HCI and COHFACE, which reveal outstanding heart rate\nestimation results.\n", "link": "http://arxiv.org/abs/2405.02652v2", "date": "2024-06-25", "relevancy": 2.0099, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5134}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4961}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Pulse-Signal%20Magnification%20for%20remote%20Heart%20Rate%20Estimation%20in%0A%20%20Compressed%20Videos&body=Title%3A%20Deep%20Pulse-Signal%20Magnification%20for%20remote%20Heart%20Rate%20Estimation%20in%0A%20%20Compressed%20Videos%0AAuthor%3A%20Joaquim%20Comas%20and%20Adria%20Ruiz%20and%20Federico%20Sukno%0AAbstract%3A%20%20%20Recent%20advancements%20in%20data-driven%20approaches%20for%20remote%20photoplethysmography%0A%28rPPG%29%20have%20significantly%20improved%20the%20accuracy%20of%20remote%20heart%20rate%0Aestimation.%20However%2C%20the%20performance%20of%20such%20approaches%20worsens%20considerably%0Aunder%20video%20compression%2C%20which%20is%20nevertheless%20necessary%20to%20store%20and%20transmit%0Avideo%20data%20efficiently.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20to%20address%0Athe%20impact%20of%20video%20compression%20on%20rPPG%20estimation%2C%20which%20leverages%20a%0Apulse-signal%20magnification%20transformation%20to%20adapt%20compressed%20videos%20to%20an%0Auncompressed%20data%20domain%20in%20which%20the%20rPPG%20signal%20is%20magnified.%20We%20validate%20the%0Aeffectiveness%20of%20our%20model%20by%20exhaustive%20evaluations%20on%20two%20publicly%20available%0Adatasets%2C%20UCLA-rPPG%20and%20UBFC-rPPG%2C%20employing%20both%20intra-%20and%20cross-database%0Aperformance%20at%20several%20compression%20rates.%20Additionally%2C%20we%20assess%20the%0Arobustness%20of%20our%20approach%20on%20two%20additional%20highly%20compressed%20and%20widely-used%0Adatasets%2C%20MAHNOB-HCI%20and%20COHFACE%2C%20which%20reveal%20outstanding%20heart%20rate%0Aestimation%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02652v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Pulse-Signal%2520Magnification%2520for%2520remote%2520Heart%2520Rate%2520Estimation%2520in%250A%2520%2520Compressed%2520Videos%26entry.906535625%3DJoaquim%2520Comas%2520and%2520Adria%2520Ruiz%2520and%2520Federico%2520Sukno%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520data-driven%2520approaches%2520for%2520remote%2520photoplethysmography%250A%2528rPPG%2529%2520have%2520significantly%2520improved%2520the%2520accuracy%2520of%2520remote%2520heart%2520rate%250Aestimation.%2520However%252C%2520the%2520performance%2520of%2520such%2520approaches%2520worsens%2520considerably%250Aunder%2520video%2520compression%252C%2520which%2520is%2520nevertheless%2520necessary%2520to%2520store%2520and%2520transmit%250Avideo%2520data%2520efficiently.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520to%2520address%250Athe%2520impact%2520of%2520video%2520compression%2520on%2520rPPG%2520estimation%252C%2520which%2520leverages%2520a%250Apulse-signal%2520magnification%2520transformation%2520to%2520adapt%2520compressed%2520videos%2520to%2520an%250Auncompressed%2520data%2520domain%2520in%2520which%2520the%2520rPPG%2520signal%2520is%2520magnified.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520model%2520by%2520exhaustive%2520evaluations%2520on%2520two%2520publicly%2520available%250Adatasets%252C%2520UCLA-rPPG%2520and%2520UBFC-rPPG%252C%2520employing%2520both%2520intra-%2520and%2520cross-database%250Aperformance%2520at%2520several%2520compression%2520rates.%2520Additionally%252C%2520we%2520assess%2520the%250Arobustness%2520of%2520our%2520approach%2520on%2520two%2520additional%2520highly%2520compressed%2520and%2520widely-used%250Adatasets%252C%2520MAHNOB-HCI%2520and%2520COHFACE%252C%2520which%2520reveal%2520outstanding%2520heart%2520rate%250Aestimation%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02652v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Pulse-Signal%20Magnification%20for%20remote%20Heart%20Rate%20Estimation%20in%0A%20%20Compressed%20Videos&entry.906535625=Joaquim%20Comas%20and%20Adria%20Ruiz%20and%20Federico%20Sukno&entry.1292438233=%20%20Recent%20advancements%20in%20data-driven%20approaches%20for%20remote%20photoplethysmography%0A%28rPPG%29%20have%20significantly%20improved%20the%20accuracy%20of%20remote%20heart%20rate%0Aestimation.%20However%2C%20the%20performance%20of%20such%20approaches%20worsens%20considerably%0Aunder%20video%20compression%2C%20which%20is%20nevertheless%20necessary%20to%20store%20and%20transmit%0Avideo%20data%20efficiently.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20to%20address%0Athe%20impact%20of%20video%20compression%20on%20rPPG%20estimation%2C%20which%20leverages%20a%0Apulse-signal%20magnification%20transformation%20to%20adapt%20compressed%20videos%20to%20an%0Auncompressed%20data%20domain%20in%20which%20the%20rPPG%20signal%20is%20magnified.%20We%20validate%20the%0Aeffectiveness%20of%20our%20model%20by%20exhaustive%20evaluations%20on%20two%20publicly%20available%0Adatasets%2C%20UCLA-rPPG%20and%20UBFC-rPPG%2C%20employing%20both%20intra-%20and%20cross-database%0Aperformance%20at%20several%20compression%20rates.%20Additionally%2C%20we%20assess%20the%0Arobustness%20of%20our%20approach%20on%20two%20additional%20highly%20compressed%20and%20widely-used%0Adatasets%2C%20MAHNOB-HCI%20and%20COHFACE%2C%20which%20reveal%20outstanding%20heart%20rate%0Aestimation%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02652v2&entry.124074799=Read"},
{"title": "Brain Tumor Classification using Vision Transformer with Selective\n  Cross-Attention Mechanism and Feature Calibration", "author": "Mohammad Ali Labbaf Khaniki and Alireza Golkarieh and Mohammad Manthouri", "abstract": "  Brain tumor classification is a challenging task in medical image analysis.\nIn this paper, we propose a novel approach to brain tumor classification using\na vision transformer with a novel cross-attention mechanism. Our approach\nleverages the strengths of transformers in modeling long-range dependencies and\nmulti-scale feature fusion. We introduce two new mechanisms to improve the\nperformance of the cross-attention fusion module: Feature Calibration Mechanism\n(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from\ndifferent branches to make them more compatible, while SCA selectively attends\nto the most informative features. Our experiments demonstrate that the proposed\napproach outperforms other state-of-the-art methods in brain tumor\nclassification, achieving improved accuracy and efficiency. The proposed FCM\nand SCA mechanisms can be easily integrated into other vision transformer\narchitectures, making them a promising direction for future research in medical\nimage analysis. Experimental results confirm that our approach surpasses\nexisting methods, achieving state-of-the-art performance in brain tumor\nclassification tasks.\n", "link": "http://arxiv.org/abs/2406.17670v1", "date": "2024-06-25", "relevancy": 2.0068, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5205}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Tumor%20Classification%20using%20Vision%20Transformer%20with%20Selective%0A%20%20Cross-Attention%20Mechanism%20and%20Feature%20Calibration&body=Title%3A%20Brain%20Tumor%20Classification%20using%20Vision%20Transformer%20with%20Selective%0A%20%20Cross-Attention%20Mechanism%20and%20Feature%20Calibration%0AAuthor%3A%20Mohammad%20Ali%20Labbaf%20Khaniki%20and%20Alireza%20Golkarieh%20and%20Mohammad%20Manthouri%0AAbstract%3A%20%20%20Brain%20tumor%20classification%20is%20a%20challenging%20task%20in%20medical%20image%20analysis.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20brain%20tumor%20classification%20using%0Aa%20vision%20transformer%20with%20a%20novel%20cross-attention%20mechanism.%20Our%20approach%0Aleverages%20the%20strengths%20of%20transformers%20in%20modeling%20long-range%20dependencies%20and%0Amulti-scale%20feature%20fusion.%20We%20introduce%20two%20new%20mechanisms%20to%20improve%20the%0Aperformance%20of%20the%20cross-attention%20fusion%20module%3A%20Feature%20Calibration%20Mechanism%0A%28FCM%29%20and%20Selective%20Cross-Attention%20%28SCA%29.%20FCM%20calibrates%20the%20features%20from%0Adifferent%20branches%20to%20make%20them%20more%20compatible%2C%20while%20SCA%20selectively%20attends%0Ato%20the%20most%20informative%20features.%20Our%20experiments%20demonstrate%20that%20the%20proposed%0Aapproach%20outperforms%20other%20state-of-the-art%20methods%20in%20brain%20tumor%0Aclassification%2C%20achieving%20improved%20accuracy%20and%20efficiency.%20The%20proposed%20FCM%0Aand%20SCA%20mechanisms%20can%20be%20easily%20integrated%20into%20other%20vision%20transformer%0Aarchitectures%2C%20making%20them%20a%20promising%20direction%20for%20future%20research%20in%20medical%0Aimage%20analysis.%20Experimental%20results%20confirm%20that%20our%20approach%20surpasses%0Aexisting%20methods%2C%20achieving%20state-of-the-art%20performance%20in%20brain%20tumor%0Aclassification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Tumor%2520Classification%2520using%2520Vision%2520Transformer%2520with%2520Selective%250A%2520%2520Cross-Attention%2520Mechanism%2520and%2520Feature%2520Calibration%26entry.906535625%3DMohammad%2520Ali%2520Labbaf%2520Khaniki%2520and%2520Alireza%2520Golkarieh%2520and%2520Mohammad%2520Manthouri%26entry.1292438233%3D%2520%2520Brain%2520tumor%2520classification%2520is%2520a%2520challenging%2520task%2520in%2520medical%2520image%2520analysis.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520brain%2520tumor%2520classification%2520using%250Aa%2520vision%2520transformer%2520with%2520a%2520novel%2520cross-attention%2520mechanism.%2520Our%2520approach%250Aleverages%2520the%2520strengths%2520of%2520transformers%2520in%2520modeling%2520long-range%2520dependencies%2520and%250Amulti-scale%2520feature%2520fusion.%2520We%2520introduce%2520two%2520new%2520mechanisms%2520to%2520improve%2520the%250Aperformance%2520of%2520the%2520cross-attention%2520fusion%2520module%253A%2520Feature%2520Calibration%2520Mechanism%250A%2528FCM%2529%2520and%2520Selective%2520Cross-Attention%2520%2528SCA%2529.%2520FCM%2520calibrates%2520the%2520features%2520from%250Adifferent%2520branches%2520to%2520make%2520them%2520more%2520compatible%252C%2520while%2520SCA%2520selectively%2520attends%250Ato%2520the%2520most%2520informative%2520features.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520proposed%250Aapproach%2520outperforms%2520other%2520state-of-the-art%2520methods%2520in%2520brain%2520tumor%250Aclassification%252C%2520achieving%2520improved%2520accuracy%2520and%2520efficiency.%2520The%2520proposed%2520FCM%250Aand%2520SCA%2520mechanisms%2520can%2520be%2520easily%2520integrated%2520into%2520other%2520vision%2520transformer%250Aarchitectures%252C%2520making%2520them%2520a%2520promising%2520direction%2520for%2520future%2520research%2520in%2520medical%250Aimage%2520analysis.%2520Experimental%2520results%2520confirm%2520that%2520our%2520approach%2520surpasses%250Aexisting%2520methods%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520brain%2520tumor%250Aclassification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Tumor%20Classification%20using%20Vision%20Transformer%20with%20Selective%0A%20%20Cross-Attention%20Mechanism%20and%20Feature%20Calibration&entry.906535625=Mohammad%20Ali%20Labbaf%20Khaniki%20and%20Alireza%20Golkarieh%20and%20Mohammad%20Manthouri&entry.1292438233=%20%20Brain%20tumor%20classification%20is%20a%20challenging%20task%20in%20medical%20image%20analysis.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20brain%20tumor%20classification%20using%0Aa%20vision%20transformer%20with%20a%20novel%20cross-attention%20mechanism.%20Our%20approach%0Aleverages%20the%20strengths%20of%20transformers%20in%20modeling%20long-range%20dependencies%20and%0Amulti-scale%20feature%20fusion.%20We%20introduce%20two%20new%20mechanisms%20to%20improve%20the%0Aperformance%20of%20the%20cross-attention%20fusion%20module%3A%20Feature%20Calibration%20Mechanism%0A%28FCM%29%20and%20Selective%20Cross-Attention%20%28SCA%29.%20FCM%20calibrates%20the%20features%20from%0Adifferent%20branches%20to%20make%20them%20more%20compatible%2C%20while%20SCA%20selectively%20attends%0Ato%20the%20most%20informative%20features.%20Our%20experiments%20demonstrate%20that%20the%20proposed%0Aapproach%20outperforms%20other%20state-of-the-art%20methods%20in%20brain%20tumor%0Aclassification%2C%20achieving%20improved%20accuracy%20and%20efficiency.%20The%20proposed%20FCM%0Aand%20SCA%20mechanisms%20can%20be%20easily%20integrated%20into%20other%20vision%20transformer%0Aarchitectures%2C%20making%20them%20a%20promising%20direction%20for%20future%20research%20in%20medical%0Aimage%20analysis.%20Experimental%20results%20confirm%20that%20our%20approach%20surpasses%0Aexisting%20methods%2C%20achieving%20state-of-the-art%20performance%20in%20brain%20tumor%0Aclassification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17670v1&entry.124074799=Read"},
{"title": "The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report", "author": "Bin Ren and Yawei Li and Nancy Mehta and Radu Timofte and Hongyuan Yu and Cheng Wan and Yuxin Hong and Bingnan Han and Zhuoyuan Wu and Yajun Zou and Yuqing Liu and Jizhe Li and Keji He and Chao Fan and Heng Zhang and Xiaolin Zhang and Xuanwu Yin and Kunlong Zuo and Bohao Liao and Peizhe Xia and Long Peng and Zhibo Du and Xin Di and Wangkai Li and Yang Wang and Wei Zhai and Renjing Pei and Jiaming Guo and Songcen Xu and Yang Cao and Zhengjun Zha and Yan Wang and Yi Liu and Qing Wang and Gang Zhang and Liou Zhang and Shijie Zhao and Long Sun and Jinshan Pan and Jiangxin Dong and Jinhui Tang and Xin Liu and Min Yan and Qian Wang and Menghan Zhou and Yiqiang Yan and Yixuan Liu and Wensong Chan and Dehua Tang and Dong Zhou and Li Wang and Lu Tian and Barsoum Emad and Bohan Jia and Junbo Qiao and Yunshuai Zhou and Yun Zhang and Wei Li and Shaohui Lin and Shenglong Zhou and Binbin Chen and Jincheng Liao and Suiyi Zhao and Zhao Zhang and Bo Wang and Yan Luo and Yanyan Wei and Feng Li and Mingshen Wang and Yawei Li and Jinhan Guan and Dehua Hu and Jiawei Yu and Qisheng Xu and Tao Sun and Long Lan and Kele Xu and Xin Lin and Jingtong Yue and Lehan Yang and Shiyi Du and Lu Qi and Chao Ren and Zeyu Han and Yuhan Wang and Chaolin Chen and Haobo Li and Mingjun Zheng and Zhongbao Yang and Lianhong Song and Xingzhuo Yan and Minghan Fu and Jingyi Zhang and Baiang Li and Qi Zhu and Xiaogang Xu and Dan Guo and Chunle Guo and Jiadi Chen and Huanhuan Long and Chunjiang Duanmu and Xiaoyan Lei and Jie Liu and Weilin Jia and Weifeng Cao and Wenlong Zhang and Yanyu Mao and Ruilong Guo and Nihao Zhang and Qian Wang and Manoj Pandey and Maksym Chernozhukov and Giang Le and Shuli Cheng and Hongyuan Wang and Ziyan Wei and Qingting Tang and Liejun Wang and Yongming Li and Yanhui Guo and Hao Xu and Akram Khatami-Rizi and Ahmad Mahmoudi-Aznaveh and Chih-Chung Hsu and Chia-Ming Lee and Yi-Shiuan Chou and Amogh Joshi and Nikhil Akalwadi and Sampada Malagi and Palani Yashaswini and Chaitra Desai and Ramesh Ashok Tabib and Ujwala Patil and Uma Mudenagudi", "abstract": "  This paper provides a comprehensive review of the NTIRE 2024 challenge,\nfocusing on efficient single-image super-resolution (ESR) solutions and their\noutcomes. The task of this challenge is to super-resolve an input image with a\nmagnification factor of x4 based on pairs of low and corresponding\nhigh-resolution images. The primary objective is to develop networks that\noptimize various aspects such as runtime, parameters, and FLOPs, while still\nmaintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on\nthe DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In\naddition, this challenge has 4 tracks including the main track (overall\nperformance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3\n(parameters). In the main track, all three metrics (ie runtime, FLOPs, and\nparameter count) were considered. The ranking of the main track is calculated\nbased on a weighted sum-up of the scores of all other sub-tracks. In sub-track\n1, the practical runtime performance of the submissions was evaluated, and the\ncorresponding score was used to determine the ranking. In sub-track 2, the\nnumber of FLOPs was considered. The score calculated based on the corresponding\nFLOPs was used to determine the ranking. In sub-track 3, the number of\nparameters was considered. The score calculated based on the corresponding\nparameters was used to determine the ranking. RLFN is set as the baseline for\nefficiency measurement. The challenge had 262 registered participants, and 34\nteams made valid submissions. They gauge the state-of-the-art in efficient\nsingle-image super-resolution. To facilitate the reproducibility of the\nchallenge and enable other researchers to build upon these findings, the code\nand the pre-trained model of validated solutions are made publicly available at\nhttps://github.com/Amazingren/NTIRE2024_ESR/.\n", "link": "http://arxiv.org/abs/2404.10343v2", "date": "2024-06-25", "relevancy": 2.0037, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.521}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5081}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Ninth%20NTIRE%202024%20Efficient%20Super-Resolution%20Challenge%20Report&body=Title%3A%20The%20Ninth%20NTIRE%202024%20Efficient%20Super-Resolution%20Challenge%20Report%0AAuthor%3A%20Bin%20Ren%20and%20Yawei%20Li%20and%20Nancy%20Mehta%20and%20Radu%20Timofte%20and%20Hongyuan%20Yu%20and%20Cheng%20Wan%20and%20Yuxin%20Hong%20and%20Bingnan%20Han%20and%20Zhuoyuan%20Wu%20and%20Yajun%20Zou%20and%20Yuqing%20Liu%20and%20Jizhe%20Li%20and%20Keji%20He%20and%20Chao%20Fan%20and%20Heng%20Zhang%20and%20Xiaolin%20Zhang%20and%20Xuanwu%20Yin%20and%20Kunlong%20Zuo%20and%20Bohao%20Liao%20and%20Peizhe%20Xia%20and%20Long%20Peng%20and%20Zhibo%20Du%20and%20Xin%20Di%20and%20Wangkai%20Li%20and%20Yang%20Wang%20and%20Wei%20Zhai%20and%20Renjing%20Pei%20and%20Jiaming%20Guo%20and%20Songcen%20Xu%20and%20Yang%20Cao%20and%20Zhengjun%20Zha%20and%20Yan%20Wang%20and%20Yi%20Liu%20and%20Qing%20Wang%20and%20Gang%20Zhang%20and%20Liou%20Zhang%20and%20Shijie%20Zhao%20and%20Long%20Sun%20and%20Jinshan%20Pan%20and%20Jiangxin%20Dong%20and%20Jinhui%20Tang%20and%20Xin%20Liu%20and%20Min%20Yan%20and%20Qian%20Wang%20and%20Menghan%20Zhou%20and%20Yiqiang%20Yan%20and%20Yixuan%20Liu%20and%20Wensong%20Chan%20and%20Dehua%20Tang%20and%20Dong%20Zhou%20and%20Li%20Wang%20and%20Lu%20Tian%20and%20Barsoum%20Emad%20and%20Bohan%20Jia%20and%20Junbo%20Qiao%20and%20Yunshuai%20Zhou%20and%20Yun%20Zhang%20and%20Wei%20Li%20and%20Shaohui%20Lin%20and%20Shenglong%20Zhou%20and%20Binbin%20Chen%20and%20Jincheng%20Liao%20and%20Suiyi%20Zhao%20and%20Zhao%20Zhang%20and%20Bo%20Wang%20and%20Yan%20Luo%20and%20Yanyan%20Wei%20and%20Feng%20Li%20and%20Mingshen%20Wang%20and%20Yawei%20Li%20and%20Jinhan%20Guan%20and%20Dehua%20Hu%20and%20Jiawei%20Yu%20and%20Qisheng%20Xu%20and%20Tao%20Sun%20and%20Long%20Lan%20and%20Kele%20Xu%20and%20Xin%20Lin%20and%20Jingtong%20Yue%20and%20Lehan%20Yang%20and%20Shiyi%20Du%20and%20Lu%20Qi%20and%20Chao%20Ren%20and%20Zeyu%20Han%20and%20Yuhan%20Wang%20and%20Chaolin%20Chen%20and%20Haobo%20Li%20and%20Mingjun%20Zheng%20and%20Zhongbao%20Yang%20and%20Lianhong%20Song%20and%20Xingzhuo%20Yan%20and%20Minghan%20Fu%20and%20Jingyi%20Zhang%20and%20Baiang%20Li%20and%20Qi%20Zhu%20and%20Xiaogang%20Xu%20and%20Dan%20Guo%20and%20Chunle%20Guo%20and%20Jiadi%20Chen%20and%20Huanhuan%20Long%20and%20Chunjiang%20Duanmu%20and%20Xiaoyan%20Lei%20and%20Jie%20Liu%20and%20Weilin%20Jia%20and%20Weifeng%20Cao%20and%20Wenlong%20Zhang%20and%20Yanyu%20Mao%20and%20Ruilong%20Guo%20and%20Nihao%20Zhang%20and%20Qian%20Wang%20and%20Manoj%20Pandey%20and%20Maksym%20Chernozhukov%20and%20Giang%20Le%20and%20Shuli%20Cheng%20and%20Hongyuan%20Wang%20and%20Ziyan%20Wei%20and%20Qingting%20Tang%20and%20Liejun%20Wang%20and%20Yongming%20Li%20and%20Yanhui%20Guo%20and%20Hao%20Xu%20and%20Akram%20Khatami-Rizi%20and%20Ahmad%20Mahmoudi-Aznaveh%20and%20Chih-Chung%20Hsu%20and%20Chia-Ming%20Lee%20and%20Yi-Shiuan%20Chou%20and%20Amogh%20Joshi%20and%20Nikhil%20Akalwadi%20and%20Sampada%20Malagi%20and%20Palani%20Yashaswini%20and%20Chaitra%20Desai%20and%20Ramesh%20Ashok%20Tabib%20and%20Ujwala%20Patil%20and%20Uma%20Mudenagudi%0AAbstract%3A%20%20%20This%20paper%20provides%20a%20comprehensive%20review%20of%20the%20NTIRE%202024%20challenge%2C%0Afocusing%20on%20efficient%20single-image%20super-resolution%20%28ESR%29%20solutions%20and%20their%0Aoutcomes.%20The%20task%20of%20this%20challenge%20is%20to%20super-resolve%20an%20input%20image%20with%20a%0Amagnification%20factor%20of%20x4%20based%20on%20pairs%20of%20low%20and%20corresponding%0Ahigh-resolution%20images.%20The%20primary%20objective%20is%20to%20develop%20networks%20that%0Aoptimize%20various%20aspects%20such%20as%20runtime%2C%20parameters%2C%20and%20FLOPs%2C%20while%20still%0Amaintaining%20a%20peak%20signal-to-noise%20ratio%20%28PSNR%29%20of%20approximately%2026.90%20dB%20on%0Athe%20DIV2K_LSDIR_valid%20dataset%20and%2026.99%20dB%20on%20the%20DIV2K_LSDIR_test%20dataset.%20In%0Aaddition%2C%20this%20challenge%20has%204%20tracks%20including%20the%20main%20track%20%28overall%0Aperformance%29%2C%20sub-track%201%20%28runtime%29%2C%20sub-track%202%20%28FLOPs%29%2C%20and%20sub-track%203%0A%28parameters%29.%20In%20the%20main%20track%2C%20all%20three%20metrics%20%28ie%20runtime%2C%20FLOPs%2C%20and%0Aparameter%20count%29%20were%20considered.%20The%20ranking%20of%20the%20main%20track%20is%20calculated%0Abased%20on%20a%20weighted%20sum-up%20of%20the%20scores%20of%20all%20other%20sub-tracks.%20In%20sub-track%0A1%2C%20the%20practical%20runtime%20performance%20of%20the%20submissions%20was%20evaluated%2C%20and%20the%0Acorresponding%20score%20was%20used%20to%20determine%20the%20ranking.%20In%20sub-track%202%2C%20the%0Anumber%20of%20FLOPs%20was%20considered.%20The%20score%20calculated%20based%20on%20the%20corresponding%0AFLOPs%20was%20used%20to%20determine%20the%20ranking.%20In%20sub-track%203%2C%20the%20number%20of%0Aparameters%20was%20considered.%20The%20score%20calculated%20based%20on%20the%20corresponding%0Aparameters%20was%20used%20to%20determine%20the%20ranking.%20RLFN%20is%20set%20as%20the%20baseline%20for%0Aefficiency%20measurement.%20The%20challenge%20had%20262%20registered%20participants%2C%20and%2034%0Ateams%20made%20valid%20submissions.%20They%20gauge%20the%20state-of-the-art%20in%20efficient%0Asingle-image%20super-resolution.%20To%20facilitate%20the%20reproducibility%20of%20the%0Achallenge%20and%20enable%20other%20researchers%20to%20build%20upon%20these%20findings%2C%20the%20code%0Aand%20the%20pre-trained%20model%20of%20validated%20solutions%20are%20made%20publicly%20available%20at%0Ahttps%3A//github.com/Amazingren/NTIRE2024_ESR/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Ninth%2520NTIRE%25202024%2520Efficient%2520Super-Resolution%2520Challenge%2520Report%26entry.906535625%3DBin%2520Ren%2520and%2520Yawei%2520Li%2520and%2520Nancy%2520Mehta%2520and%2520Radu%2520Timofte%2520and%2520Hongyuan%2520Yu%2520and%2520Cheng%2520Wan%2520and%2520Yuxin%2520Hong%2520and%2520Bingnan%2520Han%2520and%2520Zhuoyuan%2520Wu%2520and%2520Yajun%2520Zou%2520and%2520Yuqing%2520Liu%2520and%2520Jizhe%2520Li%2520and%2520Keji%2520He%2520and%2520Chao%2520Fan%2520and%2520Heng%2520Zhang%2520and%2520Xiaolin%2520Zhang%2520and%2520Xuanwu%2520Yin%2520and%2520Kunlong%2520Zuo%2520and%2520Bohao%2520Liao%2520and%2520Peizhe%2520Xia%2520and%2520Long%2520Peng%2520and%2520Zhibo%2520Du%2520and%2520Xin%2520Di%2520and%2520Wangkai%2520Li%2520and%2520Yang%2520Wang%2520and%2520Wei%2520Zhai%2520and%2520Renjing%2520Pei%2520and%2520Jiaming%2520Guo%2520and%2520Songcen%2520Xu%2520and%2520Yang%2520Cao%2520and%2520Zhengjun%2520Zha%2520and%2520Yan%2520Wang%2520and%2520Yi%2520Liu%2520and%2520Qing%2520Wang%2520and%2520Gang%2520Zhang%2520and%2520Liou%2520Zhang%2520and%2520Shijie%2520Zhao%2520and%2520Long%2520Sun%2520and%2520Jinshan%2520Pan%2520and%2520Jiangxin%2520Dong%2520and%2520Jinhui%2520Tang%2520and%2520Xin%2520Liu%2520and%2520Min%2520Yan%2520and%2520Qian%2520Wang%2520and%2520Menghan%2520Zhou%2520and%2520Yiqiang%2520Yan%2520and%2520Yixuan%2520Liu%2520and%2520Wensong%2520Chan%2520and%2520Dehua%2520Tang%2520and%2520Dong%2520Zhou%2520and%2520Li%2520Wang%2520and%2520Lu%2520Tian%2520and%2520Barsoum%2520Emad%2520and%2520Bohan%2520Jia%2520and%2520Junbo%2520Qiao%2520and%2520Yunshuai%2520Zhou%2520and%2520Yun%2520Zhang%2520and%2520Wei%2520Li%2520and%2520Shaohui%2520Lin%2520and%2520Shenglong%2520Zhou%2520and%2520Binbin%2520Chen%2520and%2520Jincheng%2520Liao%2520and%2520Suiyi%2520Zhao%2520and%2520Zhao%2520Zhang%2520and%2520Bo%2520Wang%2520and%2520Yan%2520Luo%2520and%2520Yanyan%2520Wei%2520and%2520Feng%2520Li%2520and%2520Mingshen%2520Wang%2520and%2520Yawei%2520Li%2520and%2520Jinhan%2520Guan%2520and%2520Dehua%2520Hu%2520and%2520Jiawei%2520Yu%2520and%2520Qisheng%2520Xu%2520and%2520Tao%2520Sun%2520and%2520Long%2520Lan%2520and%2520Kele%2520Xu%2520and%2520Xin%2520Lin%2520and%2520Jingtong%2520Yue%2520and%2520Lehan%2520Yang%2520and%2520Shiyi%2520Du%2520and%2520Lu%2520Qi%2520and%2520Chao%2520Ren%2520and%2520Zeyu%2520Han%2520and%2520Yuhan%2520Wang%2520and%2520Chaolin%2520Chen%2520and%2520Haobo%2520Li%2520and%2520Mingjun%2520Zheng%2520and%2520Zhongbao%2520Yang%2520and%2520Lianhong%2520Song%2520and%2520Xingzhuo%2520Yan%2520and%2520Minghan%2520Fu%2520and%2520Jingyi%2520Zhang%2520and%2520Baiang%2520Li%2520and%2520Qi%2520Zhu%2520and%2520Xiaogang%2520Xu%2520and%2520Dan%2520Guo%2520and%2520Chunle%2520Guo%2520and%2520Jiadi%2520Chen%2520and%2520Huanhuan%2520Long%2520and%2520Chunjiang%2520Duanmu%2520and%2520Xiaoyan%2520Lei%2520and%2520Jie%2520Liu%2520and%2520Weilin%2520Jia%2520and%2520Weifeng%2520Cao%2520and%2520Wenlong%2520Zhang%2520and%2520Yanyu%2520Mao%2520and%2520Ruilong%2520Guo%2520and%2520Nihao%2520Zhang%2520and%2520Qian%2520Wang%2520and%2520Manoj%2520Pandey%2520and%2520Maksym%2520Chernozhukov%2520and%2520Giang%2520Le%2520and%2520Shuli%2520Cheng%2520and%2520Hongyuan%2520Wang%2520and%2520Ziyan%2520Wei%2520and%2520Qingting%2520Tang%2520and%2520Liejun%2520Wang%2520and%2520Yongming%2520Li%2520and%2520Yanhui%2520Guo%2520and%2520Hao%2520Xu%2520and%2520Akram%2520Khatami-Rizi%2520and%2520Ahmad%2520Mahmoudi-Aznaveh%2520and%2520Chih-Chung%2520Hsu%2520and%2520Chia-Ming%2520Lee%2520and%2520Yi-Shiuan%2520Chou%2520and%2520Amogh%2520Joshi%2520and%2520Nikhil%2520Akalwadi%2520and%2520Sampada%2520Malagi%2520and%2520Palani%2520Yashaswini%2520and%2520Chaitra%2520Desai%2520and%2520Ramesh%2520Ashok%2520Tabib%2520and%2520Ujwala%2520Patil%2520and%2520Uma%2520Mudenagudi%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520a%2520comprehensive%2520review%2520of%2520the%2520NTIRE%25202024%2520challenge%252C%250Afocusing%2520on%2520efficient%2520single-image%2520super-resolution%2520%2528ESR%2529%2520solutions%2520and%2520their%250Aoutcomes.%2520The%2520task%2520of%2520this%2520challenge%2520is%2520to%2520super-resolve%2520an%2520input%2520image%2520with%2520a%250Amagnification%2520factor%2520of%2520x4%2520based%2520on%2520pairs%2520of%2520low%2520and%2520corresponding%250Ahigh-resolution%2520images.%2520The%2520primary%2520objective%2520is%2520to%2520develop%2520networks%2520that%250Aoptimize%2520various%2520aspects%2520such%2520as%2520runtime%252C%2520parameters%252C%2520and%2520FLOPs%252C%2520while%2520still%250Amaintaining%2520a%2520peak%2520signal-to-noise%2520ratio%2520%2528PSNR%2529%2520of%2520approximately%252026.90%2520dB%2520on%250Athe%2520DIV2K_LSDIR_valid%2520dataset%2520and%252026.99%2520dB%2520on%2520the%2520DIV2K_LSDIR_test%2520dataset.%2520In%250Aaddition%252C%2520this%2520challenge%2520has%25204%2520tracks%2520including%2520the%2520main%2520track%2520%2528overall%250Aperformance%2529%252C%2520sub-track%25201%2520%2528runtime%2529%252C%2520sub-track%25202%2520%2528FLOPs%2529%252C%2520and%2520sub-track%25203%250A%2528parameters%2529.%2520In%2520the%2520main%2520track%252C%2520all%2520three%2520metrics%2520%2528ie%2520runtime%252C%2520FLOPs%252C%2520and%250Aparameter%2520count%2529%2520were%2520considered.%2520The%2520ranking%2520of%2520the%2520main%2520track%2520is%2520calculated%250Abased%2520on%2520a%2520weighted%2520sum-up%2520of%2520the%2520scores%2520of%2520all%2520other%2520sub-tracks.%2520In%2520sub-track%250A1%252C%2520the%2520practical%2520runtime%2520performance%2520of%2520the%2520submissions%2520was%2520evaluated%252C%2520and%2520the%250Acorresponding%2520score%2520was%2520used%2520to%2520determine%2520the%2520ranking.%2520In%2520sub-track%25202%252C%2520the%250Anumber%2520of%2520FLOPs%2520was%2520considered.%2520The%2520score%2520calculated%2520based%2520on%2520the%2520corresponding%250AFLOPs%2520was%2520used%2520to%2520determine%2520the%2520ranking.%2520In%2520sub-track%25203%252C%2520the%2520number%2520of%250Aparameters%2520was%2520considered.%2520The%2520score%2520calculated%2520based%2520on%2520the%2520corresponding%250Aparameters%2520was%2520used%2520to%2520determine%2520the%2520ranking.%2520RLFN%2520is%2520set%2520as%2520the%2520baseline%2520for%250Aefficiency%2520measurement.%2520The%2520challenge%2520had%2520262%2520registered%2520participants%252C%2520and%252034%250Ateams%2520made%2520valid%2520submissions.%2520They%2520gauge%2520the%2520state-of-the-art%2520in%2520efficient%250Asingle-image%2520super-resolution.%2520To%2520facilitate%2520the%2520reproducibility%2520of%2520the%250Achallenge%2520and%2520enable%2520other%2520researchers%2520to%2520build%2520upon%2520these%2520findings%252C%2520the%2520code%250Aand%2520the%2520pre-trained%2520model%2520of%2520validated%2520solutions%2520are%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Amazingren/NTIRE2024_ESR/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Ninth%20NTIRE%202024%20Efficient%20Super-Resolution%20Challenge%20Report&entry.906535625=Bin%20Ren%20and%20Yawei%20Li%20and%20Nancy%20Mehta%20and%20Radu%20Timofte%20and%20Hongyuan%20Yu%20and%20Cheng%20Wan%20and%20Yuxin%20Hong%20and%20Bingnan%20Han%20and%20Zhuoyuan%20Wu%20and%20Yajun%20Zou%20and%20Yuqing%20Liu%20and%20Jizhe%20Li%20and%20Keji%20He%20and%20Chao%20Fan%20and%20Heng%20Zhang%20and%20Xiaolin%20Zhang%20and%20Xuanwu%20Yin%20and%20Kunlong%20Zuo%20and%20Bohao%20Liao%20and%20Peizhe%20Xia%20and%20Long%20Peng%20and%20Zhibo%20Du%20and%20Xin%20Di%20and%20Wangkai%20Li%20and%20Yang%20Wang%20and%20Wei%20Zhai%20and%20Renjing%20Pei%20and%20Jiaming%20Guo%20and%20Songcen%20Xu%20and%20Yang%20Cao%20and%20Zhengjun%20Zha%20and%20Yan%20Wang%20and%20Yi%20Liu%20and%20Qing%20Wang%20and%20Gang%20Zhang%20and%20Liou%20Zhang%20and%20Shijie%20Zhao%20and%20Long%20Sun%20and%20Jinshan%20Pan%20and%20Jiangxin%20Dong%20and%20Jinhui%20Tang%20and%20Xin%20Liu%20and%20Min%20Yan%20and%20Qian%20Wang%20and%20Menghan%20Zhou%20and%20Yiqiang%20Yan%20and%20Yixuan%20Liu%20and%20Wensong%20Chan%20and%20Dehua%20Tang%20and%20Dong%20Zhou%20and%20Li%20Wang%20and%20Lu%20Tian%20and%20Barsoum%20Emad%20and%20Bohan%20Jia%20and%20Junbo%20Qiao%20and%20Yunshuai%20Zhou%20and%20Yun%20Zhang%20and%20Wei%20Li%20and%20Shaohui%20Lin%20and%20Shenglong%20Zhou%20and%20Binbin%20Chen%20and%20Jincheng%20Liao%20and%20Suiyi%20Zhao%20and%20Zhao%20Zhang%20and%20Bo%20Wang%20and%20Yan%20Luo%20and%20Yanyan%20Wei%20and%20Feng%20Li%20and%20Mingshen%20Wang%20and%20Yawei%20Li%20and%20Jinhan%20Guan%20and%20Dehua%20Hu%20and%20Jiawei%20Yu%20and%20Qisheng%20Xu%20and%20Tao%20Sun%20and%20Long%20Lan%20and%20Kele%20Xu%20and%20Xin%20Lin%20and%20Jingtong%20Yue%20and%20Lehan%20Yang%20and%20Shiyi%20Du%20and%20Lu%20Qi%20and%20Chao%20Ren%20and%20Zeyu%20Han%20and%20Yuhan%20Wang%20and%20Chaolin%20Chen%20and%20Haobo%20Li%20and%20Mingjun%20Zheng%20and%20Zhongbao%20Yang%20and%20Lianhong%20Song%20and%20Xingzhuo%20Yan%20and%20Minghan%20Fu%20and%20Jingyi%20Zhang%20and%20Baiang%20Li%20and%20Qi%20Zhu%20and%20Xiaogang%20Xu%20and%20Dan%20Guo%20and%20Chunle%20Guo%20and%20Jiadi%20Chen%20and%20Huanhuan%20Long%20and%20Chunjiang%20Duanmu%20and%20Xiaoyan%20Lei%20and%20Jie%20Liu%20and%20Weilin%20Jia%20and%20Weifeng%20Cao%20and%20Wenlong%20Zhang%20and%20Yanyu%20Mao%20and%20Ruilong%20Guo%20and%20Nihao%20Zhang%20and%20Qian%20Wang%20and%20Manoj%20Pandey%20and%20Maksym%20Chernozhukov%20and%20Giang%20Le%20and%20Shuli%20Cheng%20and%20Hongyuan%20Wang%20and%20Ziyan%20Wei%20and%20Qingting%20Tang%20and%20Liejun%20Wang%20and%20Yongming%20Li%20and%20Yanhui%20Guo%20and%20Hao%20Xu%20and%20Akram%20Khatami-Rizi%20and%20Ahmad%20Mahmoudi-Aznaveh%20and%20Chih-Chung%20Hsu%20and%20Chia-Ming%20Lee%20and%20Yi-Shiuan%20Chou%20and%20Amogh%20Joshi%20and%20Nikhil%20Akalwadi%20and%20Sampada%20Malagi%20and%20Palani%20Yashaswini%20and%20Chaitra%20Desai%20and%20Ramesh%20Ashok%20Tabib%20and%20Ujwala%20Patil%20and%20Uma%20Mudenagudi&entry.1292438233=%20%20This%20paper%20provides%20a%20comprehensive%20review%20of%20the%20NTIRE%202024%20challenge%2C%0Afocusing%20on%20efficient%20single-image%20super-resolution%20%28ESR%29%20solutions%20and%20their%0Aoutcomes.%20The%20task%20of%20this%20challenge%20is%20to%20super-resolve%20an%20input%20image%20with%20a%0Amagnification%20factor%20of%20x4%20based%20on%20pairs%20of%20low%20and%20corresponding%0Ahigh-resolution%20images.%20The%20primary%20objective%20is%20to%20develop%20networks%20that%0Aoptimize%20various%20aspects%20such%20as%20runtime%2C%20parameters%2C%20and%20FLOPs%2C%20while%20still%0Amaintaining%20a%20peak%20signal-to-noise%20ratio%20%28PSNR%29%20of%20approximately%2026.90%20dB%20on%0Athe%20DIV2K_LSDIR_valid%20dataset%20and%2026.99%20dB%20on%20the%20DIV2K_LSDIR_test%20dataset.%20In%0Aaddition%2C%20this%20challenge%20has%204%20tracks%20including%20the%20main%20track%20%28overall%0Aperformance%29%2C%20sub-track%201%20%28runtime%29%2C%20sub-track%202%20%28FLOPs%29%2C%20and%20sub-track%203%0A%28parameters%29.%20In%20the%20main%20track%2C%20all%20three%20metrics%20%28ie%20runtime%2C%20FLOPs%2C%20and%0Aparameter%20count%29%20were%20considered.%20The%20ranking%20of%20the%20main%20track%20is%20calculated%0Abased%20on%20a%20weighted%20sum-up%20of%20the%20scores%20of%20all%20other%20sub-tracks.%20In%20sub-track%0A1%2C%20the%20practical%20runtime%20performance%20of%20the%20submissions%20was%20evaluated%2C%20and%20the%0Acorresponding%20score%20was%20used%20to%20determine%20the%20ranking.%20In%20sub-track%202%2C%20the%0Anumber%20of%20FLOPs%20was%20considered.%20The%20score%20calculated%20based%20on%20the%20corresponding%0AFLOPs%20was%20used%20to%20determine%20the%20ranking.%20In%20sub-track%203%2C%20the%20number%20of%0Aparameters%20was%20considered.%20The%20score%20calculated%20based%20on%20the%20corresponding%0Aparameters%20was%20used%20to%20determine%20the%20ranking.%20RLFN%20is%20set%20as%20the%20baseline%20for%0Aefficiency%20measurement.%20The%20challenge%20had%20262%20registered%20participants%2C%20and%2034%0Ateams%20made%20valid%20submissions.%20They%20gauge%20the%20state-of-the-art%20in%20efficient%0Asingle-image%20super-resolution.%20To%20facilitate%20the%20reproducibility%20of%20the%0Achallenge%20and%20enable%20other%20researchers%20to%20build%20upon%20these%20findings%2C%20the%20code%0Aand%20the%20pre-trained%20model%20of%20validated%20solutions%20are%20made%20publicly%20available%20at%0Ahttps%3A//github.com/Amazingren/NTIRE2024_ESR/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10343v2&entry.124074799=Read"},
{"title": "Mask-Guided Attention U-Net for Enhanced Neonatal Brain Extraction and\n  Image Preprocessing", "author": "Bahram Jafrasteh and Simon Pedro Lubian-Lopez and Emiliano Trimarco and Macarena Roman Ruiz and Carmen Rodriguez Barrios and Yolanda Marin Almagro and Isabel Benavente-Fernandez", "abstract": "  In this study, we introduce MGA-Net, a novel mask-guided attention neural\nnetwork, which extends the U-net model for precision neonatal brain imaging.\nMGA-Net is designed to extract the brain from other structures and reconstruct\nhigh-quality brain images. The network employs a common encoder and two\ndecoders: one for brain mask extraction and the other for brain region\nreconstruction. A key feature of MGA-Net is its high-level mask-guided\nattention module, which leverages features from the brain mask decoder to\nenhance image reconstruction. To enable the same encoder and decoder to process\nboth MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional\nencoding. This encoding assigns distinct positional values to MRI and US\nimages, allowing the model to effectively learn from both modalities.\nConsequently, features learned from a single modality can aid in learning a\nmodality with less available data, such as US. We extensively validated the\nproposed MGA-Net on diverse datasets from varied clinical settings and neonatal\nage groups. The metrics used for assessment included the DICE similarity\ncoefficient, recall, and accuracy for image segmentation; structural similarity\nfor image reconstruction; and root mean squared error for total brain volume\nestimation from 3D ultrasound images. Our results demonstrate that MGA-Net\nsignificantly outperforms traditional methods, offering superior performance in\nbrain extraction and segmentation while achieving high precision in image\nreconstruction and volumetric analysis. Thus, MGA-Net represents a robust and\neffective preprocessing tool for MRI and 3D ultrasound images, marking a\nsignificant advance in neuroimaging that enhances both research and clinical\ndiagnostics in the neonatal period and beyond.\n", "link": "http://arxiv.org/abs/2406.17709v1", "date": "2024-06-25", "relevancy": 2.0024, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5038}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4997}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask-Guided%20Attention%20U-Net%20for%20Enhanced%20Neonatal%20Brain%20Extraction%20and%0A%20%20Image%20Preprocessing&body=Title%3A%20Mask-Guided%20Attention%20U-Net%20for%20Enhanced%20Neonatal%20Brain%20Extraction%20and%0A%20%20Image%20Preprocessing%0AAuthor%3A%20Bahram%20Jafrasteh%20and%20Simon%20Pedro%20Lubian-Lopez%20and%20Emiliano%20Trimarco%20and%20Macarena%20Roman%20Ruiz%20and%20Carmen%20Rodriguez%20Barrios%20and%20Yolanda%20Marin%20Almagro%20and%20Isabel%20Benavente-Fernandez%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20MGA-Net%2C%20a%20novel%20mask-guided%20attention%20neural%0Anetwork%2C%20which%20extends%20the%20U-net%20model%20for%20precision%20neonatal%20brain%20imaging.%0AMGA-Net%20is%20designed%20to%20extract%20the%20brain%20from%20other%20structures%20and%20reconstruct%0Ahigh-quality%20brain%20images.%20The%20network%20employs%20a%20common%20encoder%20and%20two%0Adecoders%3A%20one%20for%20brain%20mask%20extraction%20and%20the%20other%20for%20brain%20region%0Areconstruction.%20A%20key%20feature%20of%20MGA-Net%20is%20its%20high-level%20mask-guided%0Aattention%20module%2C%20which%20leverages%20features%20from%20the%20brain%20mask%20decoder%20to%0Aenhance%20image%20reconstruction.%20To%20enable%20the%20same%20encoder%20and%20decoder%20to%20process%0Aboth%20MRI%20and%20ultrasound%20%28US%29%20images%2C%20MGA-Net%20integrates%20sinusoidal%20positional%0Aencoding.%20This%20encoding%20assigns%20distinct%20positional%20values%20to%20MRI%20and%20US%0Aimages%2C%20allowing%20the%20model%20to%20effectively%20learn%20from%20both%20modalities.%0AConsequently%2C%20features%20learned%20from%20a%20single%20modality%20can%20aid%20in%20learning%20a%0Amodality%20with%20less%20available%20data%2C%20such%20as%20US.%20We%20extensively%20validated%20the%0Aproposed%20MGA-Net%20on%20diverse%20datasets%20from%20varied%20clinical%20settings%20and%20neonatal%0Aage%20groups.%20The%20metrics%20used%20for%20assessment%20included%20the%20DICE%20similarity%0Acoefficient%2C%20recall%2C%20and%20accuracy%20for%20image%20segmentation%3B%20structural%20similarity%0Afor%20image%20reconstruction%3B%20and%20root%20mean%20squared%20error%20for%20total%20brain%20volume%0Aestimation%20from%203D%20ultrasound%20images.%20Our%20results%20demonstrate%20that%20MGA-Net%0Asignificantly%20outperforms%20traditional%20methods%2C%20offering%20superior%20performance%20in%0Abrain%20extraction%20and%20segmentation%20while%20achieving%20high%20precision%20in%20image%0Areconstruction%20and%20volumetric%20analysis.%20Thus%2C%20MGA-Net%20represents%20a%20robust%20and%0Aeffective%20preprocessing%20tool%20for%20MRI%20and%203D%20ultrasound%20images%2C%20marking%20a%0Asignificant%20advance%20in%20neuroimaging%20that%20enhances%20both%20research%20and%20clinical%0Adiagnostics%20in%20the%20neonatal%20period%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask-Guided%2520Attention%2520U-Net%2520for%2520Enhanced%2520Neonatal%2520Brain%2520Extraction%2520and%250A%2520%2520Image%2520Preprocessing%26entry.906535625%3DBahram%2520Jafrasteh%2520and%2520Simon%2520Pedro%2520Lubian-Lopez%2520and%2520Emiliano%2520Trimarco%2520and%2520Macarena%2520Roman%2520Ruiz%2520and%2520Carmen%2520Rodriguez%2520Barrios%2520and%2520Yolanda%2520Marin%2520Almagro%2520and%2520Isabel%2520Benavente-Fernandez%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520MGA-Net%252C%2520a%2520novel%2520mask-guided%2520attention%2520neural%250Anetwork%252C%2520which%2520extends%2520the%2520U-net%2520model%2520for%2520precision%2520neonatal%2520brain%2520imaging.%250AMGA-Net%2520is%2520designed%2520to%2520extract%2520the%2520brain%2520from%2520other%2520structures%2520and%2520reconstruct%250Ahigh-quality%2520brain%2520images.%2520The%2520network%2520employs%2520a%2520common%2520encoder%2520and%2520two%250Adecoders%253A%2520one%2520for%2520brain%2520mask%2520extraction%2520and%2520the%2520other%2520for%2520brain%2520region%250Areconstruction.%2520A%2520key%2520feature%2520of%2520MGA-Net%2520is%2520its%2520high-level%2520mask-guided%250Aattention%2520module%252C%2520which%2520leverages%2520features%2520from%2520the%2520brain%2520mask%2520decoder%2520to%250Aenhance%2520image%2520reconstruction.%2520To%2520enable%2520the%2520same%2520encoder%2520and%2520decoder%2520to%2520process%250Aboth%2520MRI%2520and%2520ultrasound%2520%2528US%2529%2520images%252C%2520MGA-Net%2520integrates%2520sinusoidal%2520positional%250Aencoding.%2520This%2520encoding%2520assigns%2520distinct%2520positional%2520values%2520to%2520MRI%2520and%2520US%250Aimages%252C%2520allowing%2520the%2520model%2520to%2520effectively%2520learn%2520from%2520both%2520modalities.%250AConsequently%252C%2520features%2520learned%2520from%2520a%2520single%2520modality%2520can%2520aid%2520in%2520learning%2520a%250Amodality%2520with%2520less%2520available%2520data%252C%2520such%2520as%2520US.%2520We%2520extensively%2520validated%2520the%250Aproposed%2520MGA-Net%2520on%2520diverse%2520datasets%2520from%2520varied%2520clinical%2520settings%2520and%2520neonatal%250Aage%2520groups.%2520The%2520metrics%2520used%2520for%2520assessment%2520included%2520the%2520DICE%2520similarity%250Acoefficient%252C%2520recall%252C%2520and%2520accuracy%2520for%2520image%2520segmentation%253B%2520structural%2520similarity%250Afor%2520image%2520reconstruction%253B%2520and%2520root%2520mean%2520squared%2520error%2520for%2520total%2520brain%2520volume%250Aestimation%2520from%25203D%2520ultrasound%2520images.%2520Our%2520results%2520demonstrate%2520that%2520MGA-Net%250Asignificantly%2520outperforms%2520traditional%2520methods%252C%2520offering%2520superior%2520performance%2520in%250Abrain%2520extraction%2520and%2520segmentation%2520while%2520achieving%2520high%2520precision%2520in%2520image%250Areconstruction%2520and%2520volumetric%2520analysis.%2520Thus%252C%2520MGA-Net%2520represents%2520a%2520robust%2520and%250Aeffective%2520preprocessing%2520tool%2520for%2520MRI%2520and%25203D%2520ultrasound%2520images%252C%2520marking%2520a%250Asignificant%2520advance%2520in%2520neuroimaging%2520that%2520enhances%2520both%2520research%2520and%2520clinical%250Adiagnostics%2520in%2520the%2520neonatal%2520period%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask-Guided%20Attention%20U-Net%20for%20Enhanced%20Neonatal%20Brain%20Extraction%20and%0A%20%20Image%20Preprocessing&entry.906535625=Bahram%20Jafrasteh%20and%20Simon%20Pedro%20Lubian-Lopez%20and%20Emiliano%20Trimarco%20and%20Macarena%20Roman%20Ruiz%20and%20Carmen%20Rodriguez%20Barrios%20and%20Yolanda%20Marin%20Almagro%20and%20Isabel%20Benavente-Fernandez&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20MGA-Net%2C%20a%20novel%20mask-guided%20attention%20neural%0Anetwork%2C%20which%20extends%20the%20U-net%20model%20for%20precision%20neonatal%20brain%20imaging.%0AMGA-Net%20is%20designed%20to%20extract%20the%20brain%20from%20other%20structures%20and%20reconstruct%0Ahigh-quality%20brain%20images.%20The%20network%20employs%20a%20common%20encoder%20and%20two%0Adecoders%3A%20one%20for%20brain%20mask%20extraction%20and%20the%20other%20for%20brain%20region%0Areconstruction.%20A%20key%20feature%20of%20MGA-Net%20is%20its%20high-level%20mask-guided%0Aattention%20module%2C%20which%20leverages%20features%20from%20the%20brain%20mask%20decoder%20to%0Aenhance%20image%20reconstruction.%20To%20enable%20the%20same%20encoder%20and%20decoder%20to%20process%0Aboth%20MRI%20and%20ultrasound%20%28US%29%20images%2C%20MGA-Net%20integrates%20sinusoidal%20positional%0Aencoding.%20This%20encoding%20assigns%20distinct%20positional%20values%20to%20MRI%20and%20US%0Aimages%2C%20allowing%20the%20model%20to%20effectively%20learn%20from%20both%20modalities.%0AConsequently%2C%20features%20learned%20from%20a%20single%20modality%20can%20aid%20in%20learning%20a%0Amodality%20with%20less%20available%20data%2C%20such%20as%20US.%20We%20extensively%20validated%20the%0Aproposed%20MGA-Net%20on%20diverse%20datasets%20from%20varied%20clinical%20settings%20and%20neonatal%0Aage%20groups.%20The%20metrics%20used%20for%20assessment%20included%20the%20DICE%20similarity%0Acoefficient%2C%20recall%2C%20and%20accuracy%20for%20image%20segmentation%3B%20structural%20similarity%0Afor%20image%20reconstruction%3B%20and%20root%20mean%20squared%20error%20for%20total%20brain%20volume%0Aestimation%20from%203D%20ultrasound%20images.%20Our%20results%20demonstrate%20that%20MGA-Net%0Asignificantly%20outperforms%20traditional%20methods%2C%20offering%20superior%20performance%20in%0Abrain%20extraction%20and%20segmentation%20while%20achieving%20high%20precision%20in%20image%0Areconstruction%20and%20volumetric%20analysis.%20Thus%2C%20MGA-Net%20represents%20a%20robust%20and%0Aeffective%20preprocessing%20tool%20for%20MRI%20and%203D%20ultrasound%20images%2C%20marking%20a%0Asignificant%20advance%20in%20neuroimaging%20that%20enhances%20both%20research%20and%20clinical%0Adiagnostics%20in%20the%20neonatal%20period%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17709v1&entry.124074799=Read"},
{"title": "Using joint angles based on the international biomechanical standards\n  for human action recognition and related tasks", "author": "Kevin Schlegel and Lei Jiang and Hao Ni", "abstract": "  Keypoint data has received a considerable amount of attention in machine\nlearning for tasks like action detection and recognition. However, human\nexperts in movement such as doctors, physiotherapists, sports scientists and\ncoaches use a notion of joint angles standardised by the International Society\nof Biomechanics to precisely and efficiently communicate static body poses and\nmovements. In this paper, we introduce the basic biomechanical notions and show\nhow they can be used to convert common keypoint data into joint angles that\nuniquely describe the given pose and have various desirable mathematical\nproperties, such as independence of both the camera viewpoint and the person\nperforming the action. We experimentally demonstrate that the joint angle\nrepresentation of keypoint data is suitable for machine learning applications\nand can in some cases bring an immediate performance gain. The use of joint\nangles as a human meaningful representation of kinematic data is in particular\npromising for applications where interpretability and dialog with human experts\nis important, such as many sports and medical applications. To facilitate\nfurther research in this direction, we will release a python package to convert\nkeypoint data into joint angles as outlined in this paper.\n", "link": "http://arxiv.org/abs/2406.17443v1", "date": "2024-06-25", "relevancy": 2.0023, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5589}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4943}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20joint%20angles%20based%20on%20the%20international%20biomechanical%20standards%0A%20%20for%20human%20action%20recognition%20and%20related%20tasks&body=Title%3A%20Using%20joint%20angles%20based%20on%20the%20international%20biomechanical%20standards%0A%20%20for%20human%20action%20recognition%20and%20related%20tasks%0AAuthor%3A%20Kevin%20Schlegel%20and%20Lei%20Jiang%20and%20Hao%20Ni%0AAbstract%3A%20%20%20Keypoint%20data%20has%20received%20a%20considerable%20amount%20of%20attention%20in%20machine%0Alearning%20for%20tasks%20like%20action%20detection%20and%20recognition.%20However%2C%20human%0Aexperts%20in%20movement%20such%20as%20doctors%2C%20physiotherapists%2C%20sports%20scientists%20and%0Acoaches%20use%20a%20notion%20of%20joint%20angles%20standardised%20by%20the%20International%20Society%0Aof%20Biomechanics%20to%20precisely%20and%20efficiently%20communicate%20static%20body%20poses%20and%0Amovements.%20In%20this%20paper%2C%20we%20introduce%20the%20basic%20biomechanical%20notions%20and%20show%0Ahow%20they%20can%20be%20used%20to%20convert%20common%20keypoint%20data%20into%20joint%20angles%20that%0Auniquely%20describe%20the%20given%20pose%20and%20have%20various%20desirable%20mathematical%0Aproperties%2C%20such%20as%20independence%20of%20both%20the%20camera%20viewpoint%20and%20the%20person%0Aperforming%20the%20action.%20We%20experimentally%20demonstrate%20that%20the%20joint%20angle%0Arepresentation%20of%20keypoint%20data%20is%20suitable%20for%20machine%20learning%20applications%0Aand%20can%20in%20some%20cases%20bring%20an%20immediate%20performance%20gain.%20The%20use%20of%20joint%0Aangles%20as%20a%20human%20meaningful%20representation%20of%20kinematic%20data%20is%20in%20particular%0Apromising%20for%20applications%20where%20interpretability%20and%20dialog%20with%20human%20experts%0Ais%20important%2C%20such%20as%20many%20sports%20and%20medical%20applications.%20To%20facilitate%0Afurther%20research%20in%20this%20direction%2C%20we%20will%20release%20a%20python%20package%20to%20convert%0Akeypoint%20data%20into%20joint%20angles%20as%20outlined%20in%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520joint%2520angles%2520based%2520on%2520the%2520international%2520biomechanical%2520standards%250A%2520%2520for%2520human%2520action%2520recognition%2520and%2520related%2520tasks%26entry.906535625%3DKevin%2520Schlegel%2520and%2520Lei%2520Jiang%2520and%2520Hao%2520Ni%26entry.1292438233%3D%2520%2520Keypoint%2520data%2520has%2520received%2520a%2520considerable%2520amount%2520of%2520attention%2520in%2520machine%250Alearning%2520for%2520tasks%2520like%2520action%2520detection%2520and%2520recognition.%2520However%252C%2520human%250Aexperts%2520in%2520movement%2520such%2520as%2520doctors%252C%2520physiotherapists%252C%2520sports%2520scientists%2520and%250Acoaches%2520use%2520a%2520notion%2520of%2520joint%2520angles%2520standardised%2520by%2520the%2520International%2520Society%250Aof%2520Biomechanics%2520to%2520precisely%2520and%2520efficiently%2520communicate%2520static%2520body%2520poses%2520and%250Amovements.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520basic%2520biomechanical%2520notions%2520and%2520show%250Ahow%2520they%2520can%2520be%2520used%2520to%2520convert%2520common%2520keypoint%2520data%2520into%2520joint%2520angles%2520that%250Auniquely%2520describe%2520the%2520given%2520pose%2520and%2520have%2520various%2520desirable%2520mathematical%250Aproperties%252C%2520such%2520as%2520independence%2520of%2520both%2520the%2520camera%2520viewpoint%2520and%2520the%2520person%250Aperforming%2520the%2520action.%2520We%2520experimentally%2520demonstrate%2520that%2520the%2520joint%2520angle%250Arepresentation%2520of%2520keypoint%2520data%2520is%2520suitable%2520for%2520machine%2520learning%2520applications%250Aand%2520can%2520in%2520some%2520cases%2520bring%2520an%2520immediate%2520performance%2520gain.%2520The%2520use%2520of%2520joint%250Aangles%2520as%2520a%2520human%2520meaningful%2520representation%2520of%2520kinematic%2520data%2520is%2520in%2520particular%250Apromising%2520for%2520applications%2520where%2520interpretability%2520and%2520dialog%2520with%2520human%2520experts%250Ais%2520important%252C%2520such%2520as%2520many%2520sports%2520and%2520medical%2520applications.%2520To%2520facilitate%250Afurther%2520research%2520in%2520this%2520direction%252C%2520we%2520will%2520release%2520a%2520python%2520package%2520to%2520convert%250Akeypoint%2520data%2520into%2520joint%2520angles%2520as%2520outlined%2520in%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20joint%20angles%20based%20on%20the%20international%20biomechanical%20standards%0A%20%20for%20human%20action%20recognition%20and%20related%20tasks&entry.906535625=Kevin%20Schlegel%20and%20Lei%20Jiang%20and%20Hao%20Ni&entry.1292438233=%20%20Keypoint%20data%20has%20received%20a%20considerable%20amount%20of%20attention%20in%20machine%0Alearning%20for%20tasks%20like%20action%20detection%20and%20recognition.%20However%2C%20human%0Aexperts%20in%20movement%20such%20as%20doctors%2C%20physiotherapists%2C%20sports%20scientists%20and%0Acoaches%20use%20a%20notion%20of%20joint%20angles%20standardised%20by%20the%20International%20Society%0Aof%20Biomechanics%20to%20precisely%20and%20efficiently%20communicate%20static%20body%20poses%20and%0Amovements.%20In%20this%20paper%2C%20we%20introduce%20the%20basic%20biomechanical%20notions%20and%20show%0Ahow%20they%20can%20be%20used%20to%20convert%20common%20keypoint%20data%20into%20joint%20angles%20that%0Auniquely%20describe%20the%20given%20pose%20and%20have%20various%20desirable%20mathematical%0Aproperties%2C%20such%20as%20independence%20of%20both%20the%20camera%20viewpoint%20and%20the%20person%0Aperforming%20the%20action.%20We%20experimentally%20demonstrate%20that%20the%20joint%20angle%0Arepresentation%20of%20keypoint%20data%20is%20suitable%20for%20machine%20learning%20applications%0Aand%20can%20in%20some%20cases%20bring%20an%20immediate%20performance%20gain.%20The%20use%20of%20joint%0Aangles%20as%20a%20human%20meaningful%20representation%20of%20kinematic%20data%20is%20in%20particular%0Apromising%20for%20applications%20where%20interpretability%20and%20dialog%20with%20human%20experts%0Ais%20important%2C%20such%20as%20many%20sports%20and%20medical%20applications.%20To%20facilitate%0Afurther%20research%20in%20this%20direction%2C%20we%20will%20release%20a%20python%20package%20to%20convert%0Akeypoint%20data%20into%20joint%20angles%20as%20outlined%20in%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17443v1&entry.124074799=Read"},
{"title": "Learning with Noisy Labels through Learnable Weighting and Centroid\n  Similarity", "author": "Farooq Ahmad Wani and Maria Sofia Bucarelli and Fabrizio Silvestri", "abstract": "  We introduce a novel method for training machine learning models in the\npresence of noisy labels, which are prevalent in domains such as medical\ndiagnosis and autonomous driving and have the potential to degrade a model's\ngeneralization performance. Inspired by established literature that highlights\nhow deep learning models are prone to overfitting to noisy samples in the later\nepochs of training, we propose a strategic approach. This strategy leverages\nthe distance to class centroids in the latent space and incorporates a\ndiscounting mechanism, aiming to diminish the influence of samples that lie\ndistant from all class centroids. By doing so, we effectively counteract the\nadverse effects of noisy labels. The foundational premise of our approach is\nthe assumption that samples situated further from their respective class\ncentroid in the initial stages of training are more likely to be associated\nwith noise. Our methodology is grounded in robust theoretical principles and\nhas been validated empirically through extensive experiments on several\nbenchmark datasets. Our results show that our method consistently outperforms\nthe existing state-of-the-art techniques, achieving significant improvements in\nclassification accuracy in the presence of noisy labels. The code for our\nproposed loss function and supplementary materials is available at\nhttps://github.com/wanifarooq/NCOD\n", "link": "http://arxiv.org/abs/2303.09470v2", "date": "2024-06-25", "relevancy": 1.989, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5068}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4949}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Noisy%20Labels%20through%20Learnable%20Weighting%20and%20Centroid%0A%20%20Similarity&body=Title%3A%20Learning%20with%20Noisy%20Labels%20through%20Learnable%20Weighting%20and%20Centroid%0A%20%20Similarity%0AAuthor%3A%20Farooq%20Ahmad%20Wani%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20method%20for%20training%20machine%20learning%20models%20in%20the%0Apresence%20of%20noisy%20labels%2C%20which%20are%20prevalent%20in%20domains%20such%20as%20medical%0Adiagnosis%20and%20autonomous%20driving%20and%20have%20the%20potential%20to%20degrade%20a%20model%27s%0Ageneralization%20performance.%20Inspired%20by%20established%20literature%20that%20highlights%0Ahow%20deep%20learning%20models%20are%20prone%20to%20overfitting%20to%20noisy%20samples%20in%20the%20later%0Aepochs%20of%20training%2C%20we%20propose%20a%20strategic%20approach.%20This%20strategy%20leverages%0Athe%20distance%20to%20class%20centroids%20in%20the%20latent%20space%20and%20incorporates%20a%0Adiscounting%20mechanism%2C%20aiming%20to%20diminish%20the%20influence%20of%20samples%20that%20lie%0Adistant%20from%20all%20class%20centroids.%20By%20doing%20so%2C%20we%20effectively%20counteract%20the%0Aadverse%20effects%20of%20noisy%20labels.%20The%20foundational%20premise%20of%20our%20approach%20is%0Athe%20assumption%20that%20samples%20situated%20further%20from%20their%20respective%20class%0Acentroid%20in%20the%20initial%20stages%20of%20training%20are%20more%20likely%20to%20be%20associated%0Awith%20noise.%20Our%20methodology%20is%20grounded%20in%20robust%20theoretical%20principles%20and%0Ahas%20been%20validated%20empirically%20through%20extensive%20experiments%20on%20several%0Abenchmark%20datasets.%20Our%20results%20show%20that%20our%20method%20consistently%20outperforms%0Athe%20existing%20state-of-the-art%20techniques%2C%20achieving%20significant%20improvements%20in%0Aclassification%20accuracy%20in%20the%20presence%20of%20noisy%20labels.%20The%20code%20for%20our%0Aproposed%20loss%20function%20and%20supplementary%20materials%20is%20available%20at%0Ahttps%3A//github.com/wanifarooq/NCOD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.09470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520with%2520Noisy%2520Labels%2520through%2520Learnable%2520Weighting%2520and%2520Centroid%250A%2520%2520Similarity%26entry.906535625%3DFarooq%2520Ahmad%2520Wani%2520and%2520Maria%2520Sofia%2520Bucarelli%2520and%2520Fabrizio%2520Silvestri%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520method%2520for%2520training%2520machine%2520learning%2520models%2520in%2520the%250Apresence%2520of%2520noisy%2520labels%252C%2520which%2520are%2520prevalent%2520in%2520domains%2520such%2520as%2520medical%250Adiagnosis%2520and%2520autonomous%2520driving%2520and%2520have%2520the%2520potential%2520to%2520degrade%2520a%2520model%2527s%250Ageneralization%2520performance.%2520Inspired%2520by%2520established%2520literature%2520that%2520highlights%250Ahow%2520deep%2520learning%2520models%2520are%2520prone%2520to%2520overfitting%2520to%2520noisy%2520samples%2520in%2520the%2520later%250Aepochs%2520of%2520training%252C%2520we%2520propose%2520a%2520strategic%2520approach.%2520This%2520strategy%2520leverages%250Athe%2520distance%2520to%2520class%2520centroids%2520in%2520the%2520latent%2520space%2520and%2520incorporates%2520a%250Adiscounting%2520mechanism%252C%2520aiming%2520to%2520diminish%2520the%2520influence%2520of%2520samples%2520that%2520lie%250Adistant%2520from%2520all%2520class%2520centroids.%2520By%2520doing%2520so%252C%2520we%2520effectively%2520counteract%2520the%250Aadverse%2520effects%2520of%2520noisy%2520labels.%2520The%2520foundational%2520premise%2520of%2520our%2520approach%2520is%250Athe%2520assumption%2520that%2520samples%2520situated%2520further%2520from%2520their%2520respective%2520class%250Acentroid%2520in%2520the%2520initial%2520stages%2520of%2520training%2520are%2520more%2520likely%2520to%2520be%2520associated%250Awith%2520noise.%2520Our%2520methodology%2520is%2520grounded%2520in%2520robust%2520theoretical%2520principles%2520and%250Ahas%2520been%2520validated%2520empirically%2520through%2520extensive%2520experiments%2520on%2520several%250Abenchmark%2520datasets.%2520Our%2520results%2520show%2520that%2520our%2520method%2520consistently%2520outperforms%250Athe%2520existing%2520state-of-the-art%2520techniques%252C%2520achieving%2520significant%2520improvements%2520in%250Aclassification%2520accuracy%2520in%2520the%2520presence%2520of%2520noisy%2520labels.%2520The%2520code%2520for%2520our%250Aproposed%2520loss%2520function%2520and%2520supplementary%2520materials%2520is%2520available%2520at%250Ahttps%253A//github.com/wanifarooq/NCOD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.09470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Noisy%20Labels%20through%20Learnable%20Weighting%20and%20Centroid%0A%20%20Similarity&entry.906535625=Farooq%20Ahmad%20Wani%20and%20Maria%20Sofia%20Bucarelli%20and%20Fabrizio%20Silvestri&entry.1292438233=%20%20We%20introduce%20a%20novel%20method%20for%20training%20machine%20learning%20models%20in%20the%0Apresence%20of%20noisy%20labels%2C%20which%20are%20prevalent%20in%20domains%20such%20as%20medical%0Adiagnosis%20and%20autonomous%20driving%20and%20have%20the%20potential%20to%20degrade%20a%20model%27s%0Ageneralization%20performance.%20Inspired%20by%20established%20literature%20that%20highlights%0Ahow%20deep%20learning%20models%20are%20prone%20to%20overfitting%20to%20noisy%20samples%20in%20the%20later%0Aepochs%20of%20training%2C%20we%20propose%20a%20strategic%20approach.%20This%20strategy%20leverages%0Athe%20distance%20to%20class%20centroids%20in%20the%20latent%20space%20and%20incorporates%20a%0Adiscounting%20mechanism%2C%20aiming%20to%20diminish%20the%20influence%20of%20samples%20that%20lie%0Adistant%20from%20all%20class%20centroids.%20By%20doing%20so%2C%20we%20effectively%20counteract%20the%0Aadverse%20effects%20of%20noisy%20labels.%20The%20foundational%20premise%20of%20our%20approach%20is%0Athe%20assumption%20that%20samples%20situated%20further%20from%20their%20respective%20class%0Acentroid%20in%20the%20initial%20stages%20of%20training%20are%20more%20likely%20to%20be%20associated%0Awith%20noise.%20Our%20methodology%20is%20grounded%20in%20robust%20theoretical%20principles%20and%0Ahas%20been%20validated%20empirically%20through%20extensive%20experiments%20on%20several%0Abenchmark%20datasets.%20Our%20results%20show%20that%20our%20method%20consistently%20outperforms%0Athe%20existing%20state-of-the-art%20techniques%2C%20achieving%20significant%20improvements%20in%0Aclassification%20accuracy%20in%20the%20presence%20of%20noisy%20labels.%20The%20code%20for%20our%0Aproposed%20loss%20function%20and%20supplementary%20materials%20is%20available%20at%0Ahttps%3A//github.com/wanifarooq/NCOD%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.09470v2&entry.124074799=Read"},
{"title": "PIPE: Process Informed Parameter Estimation, a learning based approach\n  to task generalized system identification", "author": "Constantin Schempp and Christian Friedrich", "abstract": "  We address the problem of robot guided assembly tasks, by using a\nlearning-based approach to identify contact model parameters for known and\nnovel parts. First, a Variational Autoencoder (VAE) is used to extract\ngeometric features of assembly parts. Then, we combine the extracted features\nwith physical knowledge to derive the parameters of a contact model using our\nnewly proposed neural network structure. The measured force from real\nexperiments is used to supervise the predicted forces, thus avoiding the need\nfor ground truth model parameters. Although trained only on a small set of\nassembly parts, good contact model estimation for unknown objects were\nachieved. Our main contribution is the network structure that allows us to\nestimate contact models of assembly tasks depending on the geometry of the part\nto be joined. Where current system identification processes have to record new\ndata for a new assembly process, our method only requires the 3D model of the\nassembly part. We evaluate our method by estimating contact models for\nrobot-guided assembly tasks of pin connectors as well as electronic plugs and\ncompare the results with real experiments.\n", "link": "http://arxiv.org/abs/2405.06991v2", "date": "2024-06-25", "relevancy": 1.9754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5558}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4884}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIPE%3A%20Process%20Informed%20Parameter%20Estimation%2C%20a%20learning%20based%20approach%0A%20%20to%20task%20generalized%20system%20identification&body=Title%3A%20PIPE%3A%20Process%20Informed%20Parameter%20Estimation%2C%20a%20learning%20based%20approach%0A%20%20to%20task%20generalized%20system%20identification%0AAuthor%3A%20Constantin%20Schempp%20and%20Christian%20Friedrich%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20robot%20guided%20assembly%20tasks%2C%20by%20using%20a%0Alearning-based%20approach%20to%20identify%20contact%20model%20parameters%20for%20known%20and%0Anovel%20parts.%20First%2C%20a%20Variational%20Autoencoder%20%28VAE%29%20is%20used%20to%20extract%0Ageometric%20features%20of%20assembly%20parts.%20Then%2C%20we%20combine%20the%20extracted%20features%0Awith%20physical%20knowledge%20to%20derive%20the%20parameters%20of%20a%20contact%20model%20using%20our%0Anewly%20proposed%20neural%20network%20structure.%20The%20measured%20force%20from%20real%0Aexperiments%20is%20used%20to%20supervise%20the%20predicted%20forces%2C%20thus%20avoiding%20the%20need%0Afor%20ground%20truth%20model%20parameters.%20Although%20trained%20only%20on%20a%20small%20set%20of%0Aassembly%20parts%2C%20good%20contact%20model%20estimation%20for%20unknown%20objects%20were%0Aachieved.%20Our%20main%20contribution%20is%20the%20network%20structure%20that%20allows%20us%20to%0Aestimate%20contact%20models%20of%20assembly%20tasks%20depending%20on%20the%20geometry%20of%20the%20part%0Ato%20be%20joined.%20Where%20current%20system%20identification%20processes%20have%20to%20record%20new%0Adata%20for%20a%20new%20assembly%20process%2C%20our%20method%20only%20requires%20the%203D%20model%20of%20the%0Aassembly%20part.%20We%20evaluate%20our%20method%20by%20estimating%20contact%20models%20for%0Arobot-guided%20assembly%20tasks%20of%20pin%20connectors%20as%20well%20as%20electronic%20plugs%20and%0Acompare%20the%20results%20with%20real%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIPE%253A%2520Process%2520Informed%2520Parameter%2520Estimation%252C%2520a%2520learning%2520based%2520approach%250A%2520%2520to%2520task%2520generalized%2520system%2520identification%26entry.906535625%3DConstantin%2520Schempp%2520and%2520Christian%2520Friedrich%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520robot%2520guided%2520assembly%2520tasks%252C%2520by%2520using%2520a%250Alearning-based%2520approach%2520to%2520identify%2520contact%2520model%2520parameters%2520for%2520known%2520and%250Anovel%2520parts.%2520First%252C%2520a%2520Variational%2520Autoencoder%2520%2528VAE%2529%2520is%2520used%2520to%2520extract%250Ageometric%2520features%2520of%2520assembly%2520parts.%2520Then%252C%2520we%2520combine%2520the%2520extracted%2520features%250Awith%2520physical%2520knowledge%2520to%2520derive%2520the%2520parameters%2520of%2520a%2520contact%2520model%2520using%2520our%250Anewly%2520proposed%2520neural%2520network%2520structure.%2520The%2520measured%2520force%2520from%2520real%250Aexperiments%2520is%2520used%2520to%2520supervise%2520the%2520predicted%2520forces%252C%2520thus%2520avoiding%2520the%2520need%250Afor%2520ground%2520truth%2520model%2520parameters.%2520Although%2520trained%2520only%2520on%2520a%2520small%2520set%2520of%250Aassembly%2520parts%252C%2520good%2520contact%2520model%2520estimation%2520for%2520unknown%2520objects%2520were%250Aachieved.%2520Our%2520main%2520contribution%2520is%2520the%2520network%2520structure%2520that%2520allows%2520us%2520to%250Aestimate%2520contact%2520models%2520of%2520assembly%2520tasks%2520depending%2520on%2520the%2520geometry%2520of%2520the%2520part%250Ato%2520be%2520joined.%2520Where%2520current%2520system%2520identification%2520processes%2520have%2520to%2520record%2520new%250Adata%2520for%2520a%2520new%2520assembly%2520process%252C%2520our%2520method%2520only%2520requires%2520the%25203D%2520model%2520of%2520the%250Aassembly%2520part.%2520We%2520evaluate%2520our%2520method%2520by%2520estimating%2520contact%2520models%2520for%250Arobot-guided%2520assembly%2520tasks%2520of%2520pin%2520connectors%2520as%2520well%2520as%2520electronic%2520plugs%2520and%250Acompare%2520the%2520results%2520with%2520real%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIPE%3A%20Process%20Informed%20Parameter%20Estimation%2C%20a%20learning%20based%20approach%0A%20%20to%20task%20generalized%20system%20identification&entry.906535625=Constantin%20Schempp%20and%20Christian%20Friedrich&entry.1292438233=%20%20We%20address%20the%20problem%20of%20robot%20guided%20assembly%20tasks%2C%20by%20using%20a%0Alearning-based%20approach%20to%20identify%20contact%20model%20parameters%20for%20known%20and%0Anovel%20parts.%20First%2C%20a%20Variational%20Autoencoder%20%28VAE%29%20is%20used%20to%20extract%0Ageometric%20features%20of%20assembly%20parts.%20Then%2C%20we%20combine%20the%20extracted%20features%0Awith%20physical%20knowledge%20to%20derive%20the%20parameters%20of%20a%20contact%20model%20using%20our%0Anewly%20proposed%20neural%20network%20structure.%20The%20measured%20force%20from%20real%0Aexperiments%20is%20used%20to%20supervise%20the%20predicted%20forces%2C%20thus%20avoiding%20the%20need%0Afor%20ground%20truth%20model%20parameters.%20Although%20trained%20only%20on%20a%20small%20set%20of%0Aassembly%20parts%2C%20good%20contact%20model%20estimation%20for%20unknown%20objects%20were%0Aachieved.%20Our%20main%20contribution%20is%20the%20network%20structure%20that%20allows%20us%20to%0Aestimate%20contact%20models%20of%20assembly%20tasks%20depending%20on%20the%20geometry%20of%20the%20part%0Ato%20be%20joined.%20Where%20current%20system%20identification%20processes%20have%20to%20record%20new%0Adata%20for%20a%20new%20assembly%20process%2C%20our%20method%20only%20requires%20the%203D%20model%20of%20the%0Aassembly%20part.%20We%20evaluate%20our%20method%20by%20estimating%20contact%20models%20for%0Arobot-guided%20assembly%20tasks%20of%20pin%20connectors%20as%20well%20as%20electronic%20plugs%20and%0Acompare%20the%20results%20with%20real%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06991v2&entry.124074799=Read"},
{"title": "Learning Dynamic Bayesian Networks from Data: Foundations, First\n  Principles and Numerical Comparisons", "author": "Vyacheslav Kungurtsev and Petr Rysavy and Fadwa Idlahcen and Pavel Rytir and Ales Wodecki", "abstract": "  In this paper, we present a guide to the foundations of learning Dynamic\nBayesian Networks (DBNs) from data in the form of multiple samples of\ntrajectories for some length of time. We present the formalism for a generic as\nwell as a set of common types of DBNs for particular variable distributions. We\npresent the analytical form of the models, with a comprehensive discussion on\nthe interdependence between structure and weights in a DBN model and their\nimplications for learning. Next, we give a broad overview of learning methods\nand describe and categorize them based on the most important statistical\nfeatures, and how they treat the interplay between learning structure and\nweights. We give the analytical form of the likelihood and Bayesian score\nfunctions, emphasizing the distinction from the static case. We discuss\nfunctions used in optimization to enforce structural requirements. We briefly\ndiscuss more complex extensions and representations. Finally we present a set\nof comparisons in different settings for various distinct but representative\nalgorithms across the variants.\n", "link": "http://arxiv.org/abs/2406.17585v1", "date": "2024-06-25", "relevancy": 1.9699, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5293}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamic%20Bayesian%20Networks%20from%20Data%3A%20Foundations%2C%20First%0A%20%20Principles%20and%20Numerical%20Comparisons&body=Title%3A%20Learning%20Dynamic%20Bayesian%20Networks%20from%20Data%3A%20Foundations%2C%20First%0A%20%20Principles%20and%20Numerical%20Comparisons%0AAuthor%3A%20Vyacheslav%20Kungurtsev%20and%20Petr%20Rysavy%20and%20Fadwa%20Idlahcen%20and%20Pavel%20Rytir%20and%20Ales%20Wodecki%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20guide%20to%20the%20foundations%20of%20learning%20Dynamic%0ABayesian%20Networks%20%28DBNs%29%20from%20data%20in%20the%20form%20of%20multiple%20samples%20of%0Atrajectories%20for%20some%20length%20of%20time.%20We%20present%20the%20formalism%20for%20a%20generic%20as%0Awell%20as%20a%20set%20of%20common%20types%20of%20DBNs%20for%20particular%20variable%20distributions.%20We%0Apresent%20the%20analytical%20form%20of%20the%20models%2C%20with%20a%20comprehensive%20discussion%20on%0Athe%20interdependence%20between%20structure%20and%20weights%20in%20a%20DBN%20model%20and%20their%0Aimplications%20for%20learning.%20Next%2C%20we%20give%20a%20broad%20overview%20of%20learning%20methods%0Aand%20describe%20and%20categorize%20them%20based%20on%20the%20most%20important%20statistical%0Afeatures%2C%20and%20how%20they%20treat%20the%20interplay%20between%20learning%20structure%20and%0Aweights.%20We%20give%20the%20analytical%20form%20of%20the%20likelihood%20and%20Bayesian%20score%0Afunctions%2C%20emphasizing%20the%20distinction%20from%20the%20static%20case.%20We%20discuss%0Afunctions%20used%20in%20optimization%20to%20enforce%20structural%20requirements.%20We%20briefly%0Adiscuss%20more%20complex%20extensions%20and%20representations.%20Finally%20we%20present%20a%20set%0Aof%20comparisons%20in%20different%20settings%20for%20various%20distinct%20but%20representative%0Aalgorithms%20across%20the%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamic%2520Bayesian%2520Networks%2520from%2520Data%253A%2520Foundations%252C%2520First%250A%2520%2520Principles%2520and%2520Numerical%2520Comparisons%26entry.906535625%3DVyacheslav%2520Kungurtsev%2520and%2520Petr%2520Rysavy%2520and%2520Fadwa%2520Idlahcen%2520and%2520Pavel%2520Rytir%2520and%2520Ales%2520Wodecki%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520guide%2520to%2520the%2520foundations%2520of%2520learning%2520Dynamic%250ABayesian%2520Networks%2520%2528DBNs%2529%2520from%2520data%2520in%2520the%2520form%2520of%2520multiple%2520samples%2520of%250Atrajectories%2520for%2520some%2520length%2520of%2520time.%2520We%2520present%2520the%2520formalism%2520for%2520a%2520generic%2520as%250Awell%2520as%2520a%2520set%2520of%2520common%2520types%2520of%2520DBNs%2520for%2520particular%2520variable%2520distributions.%2520We%250Apresent%2520the%2520analytical%2520form%2520of%2520the%2520models%252C%2520with%2520a%2520comprehensive%2520discussion%2520on%250Athe%2520interdependence%2520between%2520structure%2520and%2520weights%2520in%2520a%2520DBN%2520model%2520and%2520their%250Aimplications%2520for%2520learning.%2520Next%252C%2520we%2520give%2520a%2520broad%2520overview%2520of%2520learning%2520methods%250Aand%2520describe%2520and%2520categorize%2520them%2520based%2520on%2520the%2520most%2520important%2520statistical%250Afeatures%252C%2520and%2520how%2520they%2520treat%2520the%2520interplay%2520between%2520learning%2520structure%2520and%250Aweights.%2520We%2520give%2520the%2520analytical%2520form%2520of%2520the%2520likelihood%2520and%2520Bayesian%2520score%250Afunctions%252C%2520emphasizing%2520the%2520distinction%2520from%2520the%2520static%2520case.%2520We%2520discuss%250Afunctions%2520used%2520in%2520optimization%2520to%2520enforce%2520structural%2520requirements.%2520We%2520briefly%250Adiscuss%2520more%2520complex%2520extensions%2520and%2520representations.%2520Finally%2520we%2520present%2520a%2520set%250Aof%2520comparisons%2520in%2520different%2520settings%2520for%2520various%2520distinct%2520but%2520representative%250Aalgorithms%2520across%2520the%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamic%20Bayesian%20Networks%20from%20Data%3A%20Foundations%2C%20First%0A%20%20Principles%20and%20Numerical%20Comparisons&entry.906535625=Vyacheslav%20Kungurtsev%20and%20Petr%20Rysavy%20and%20Fadwa%20Idlahcen%20and%20Pavel%20Rytir%20and%20Ales%20Wodecki&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20guide%20to%20the%20foundations%20of%20learning%20Dynamic%0ABayesian%20Networks%20%28DBNs%29%20from%20data%20in%20the%20form%20of%20multiple%20samples%20of%0Atrajectories%20for%20some%20length%20of%20time.%20We%20present%20the%20formalism%20for%20a%20generic%20as%0Awell%20as%20a%20set%20of%20common%20types%20of%20DBNs%20for%20particular%20variable%20distributions.%20We%0Apresent%20the%20analytical%20form%20of%20the%20models%2C%20with%20a%20comprehensive%20discussion%20on%0Athe%20interdependence%20between%20structure%20and%20weights%20in%20a%20DBN%20model%20and%20their%0Aimplications%20for%20learning.%20Next%2C%20we%20give%20a%20broad%20overview%20of%20learning%20methods%0Aand%20describe%20and%20categorize%20them%20based%20on%20the%20most%20important%20statistical%0Afeatures%2C%20and%20how%20they%20treat%20the%20interplay%20between%20learning%20structure%20and%0Aweights.%20We%20give%20the%20analytical%20form%20of%20the%20likelihood%20and%20Bayesian%20score%0Afunctions%2C%20emphasizing%20the%20distinction%20from%20the%20static%20case.%20We%20discuss%0Afunctions%20used%20in%20optimization%20to%20enforce%20structural%20requirements.%20We%20briefly%0Adiscuss%20more%20complex%20extensions%20and%20representations.%20Finally%20we%20present%20a%20set%0Aof%20comparisons%20in%20different%20settings%20for%20various%20distinct%20but%20representative%0Aalgorithms%20across%20the%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17585v1&entry.124074799=Read"},
{"title": "Aligning Programming Language and Natural Language: Exploring Design\n  Choices in Multi-Modal Transformer-Based Embedding for Bug Localization", "author": "Partha Chakraborty and Venkatraman Arumugam and Meiyappan Nagappan", "abstract": "  Bug localization refers to the identification of source code files which is\nin a programming language and also responsible for the unexpected behavior of\nsoftware using the bug report, which is a natural language. As bug localization\nis labor-intensive, bug localization models are employed to assist software\ndevelopers. Due to the domain difference between source code files and bug\nreports, modern bug-localization systems, based on deep learning models, rely\nheavily on embedding techniques that project bug reports and source code files\ninto a shared vector space. The creation of an embedding involves several\ndesign choices, but the impact of these choices on the quality of embedding and\nthe performance of bug localization models remains unexplained in current\nresearch.\n  To address this gap, our study evaluated 14 distinct embedding models to gain\ninsights into the effects of various design choices. Subsequently, we developed\nbug localization models utilizing these embedding models to assess the\ninfluence of these choices on the performance of the localization models. Our\nfindings indicate that the pre-training strategies significantly affect the\nquality of the embedding. Moreover, we discovered that the familiarity of the\nembedding models with the data has a notable impact on the bug localization\nmodel's performance. Notably, when the training and testing data are collected\nfrom different projects, the performance of the bug localization models\nexhibits substantial fluctuations.\n", "link": "http://arxiv.org/abs/2406.17615v1", "date": "2024-06-25", "relevancy": 1.9491, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4832}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Programming%20Language%20and%20Natural%20Language%3A%20Exploring%20Design%0A%20%20Choices%20in%20Multi-Modal%20Transformer-Based%20Embedding%20for%20Bug%20Localization&body=Title%3A%20Aligning%20Programming%20Language%20and%20Natural%20Language%3A%20Exploring%20Design%0A%20%20Choices%20in%20Multi-Modal%20Transformer-Based%20Embedding%20for%20Bug%20Localization%0AAuthor%3A%20Partha%20Chakraborty%20and%20Venkatraman%20Arumugam%20and%20Meiyappan%20Nagappan%0AAbstract%3A%20%20%20Bug%20localization%20refers%20to%20the%20identification%20of%20source%20code%20files%20which%20is%0Ain%20a%20programming%20language%20and%20also%20responsible%20for%20the%20unexpected%20behavior%20of%0Asoftware%20using%20the%20bug%20report%2C%20which%20is%20a%20natural%20language.%20As%20bug%20localization%0Ais%20labor-intensive%2C%20bug%20localization%20models%20are%20employed%20to%20assist%20software%0Adevelopers.%20Due%20to%20the%20domain%20difference%20between%20source%20code%20files%20and%20bug%0Areports%2C%20modern%20bug-localization%20systems%2C%20based%20on%20deep%20learning%20models%2C%20rely%0Aheavily%20on%20embedding%20techniques%20that%20project%20bug%20reports%20and%20source%20code%20files%0Ainto%20a%20shared%20vector%20space.%20The%20creation%20of%20an%20embedding%20involves%20several%0Adesign%20choices%2C%20but%20the%20impact%20of%20these%20choices%20on%20the%20quality%20of%20embedding%20and%0Athe%20performance%20of%20bug%20localization%20models%20remains%20unexplained%20in%20current%0Aresearch.%0A%20%20To%20address%20this%20gap%2C%20our%20study%20evaluated%2014%20distinct%20embedding%20models%20to%20gain%0Ainsights%20into%20the%20effects%20of%20various%20design%20choices.%20Subsequently%2C%20we%20developed%0Abug%20localization%20models%20utilizing%20these%20embedding%20models%20to%20assess%20the%0Ainfluence%20of%20these%20choices%20on%20the%20performance%20of%20the%20localization%20models.%20Our%0Afindings%20indicate%20that%20the%20pre-training%20strategies%20significantly%20affect%20the%0Aquality%20of%20the%20embedding.%20Moreover%2C%20we%20discovered%20that%20the%20familiarity%20of%20the%0Aembedding%20models%20with%20the%20data%20has%20a%20notable%20impact%20on%20the%20bug%20localization%0Amodel%27s%20performance.%20Notably%2C%20when%20the%20training%20and%20testing%20data%20are%20collected%0Afrom%20different%20projects%2C%20the%20performance%20of%20the%20bug%20localization%20models%0Aexhibits%20substantial%20fluctuations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Programming%2520Language%2520and%2520Natural%2520Language%253A%2520Exploring%2520Design%250A%2520%2520Choices%2520in%2520Multi-Modal%2520Transformer-Based%2520Embedding%2520for%2520Bug%2520Localization%26entry.906535625%3DPartha%2520Chakraborty%2520and%2520Venkatraman%2520Arumugam%2520and%2520Meiyappan%2520Nagappan%26entry.1292438233%3D%2520%2520Bug%2520localization%2520refers%2520to%2520the%2520identification%2520of%2520source%2520code%2520files%2520which%2520is%250Ain%2520a%2520programming%2520language%2520and%2520also%2520responsible%2520for%2520the%2520unexpected%2520behavior%2520of%250Asoftware%2520using%2520the%2520bug%2520report%252C%2520which%2520is%2520a%2520natural%2520language.%2520As%2520bug%2520localization%250Ais%2520labor-intensive%252C%2520bug%2520localization%2520models%2520are%2520employed%2520to%2520assist%2520software%250Adevelopers.%2520Due%2520to%2520the%2520domain%2520difference%2520between%2520source%2520code%2520files%2520and%2520bug%250Areports%252C%2520modern%2520bug-localization%2520systems%252C%2520based%2520on%2520deep%2520learning%2520models%252C%2520rely%250Aheavily%2520on%2520embedding%2520techniques%2520that%2520project%2520bug%2520reports%2520and%2520source%2520code%2520files%250Ainto%2520a%2520shared%2520vector%2520space.%2520The%2520creation%2520of%2520an%2520embedding%2520involves%2520several%250Adesign%2520choices%252C%2520but%2520the%2520impact%2520of%2520these%2520choices%2520on%2520the%2520quality%2520of%2520embedding%2520and%250Athe%2520performance%2520of%2520bug%2520localization%2520models%2520remains%2520unexplained%2520in%2520current%250Aresearch.%250A%2520%2520To%2520address%2520this%2520gap%252C%2520our%2520study%2520evaluated%252014%2520distinct%2520embedding%2520models%2520to%2520gain%250Ainsights%2520into%2520the%2520effects%2520of%2520various%2520design%2520choices.%2520Subsequently%252C%2520we%2520developed%250Abug%2520localization%2520models%2520utilizing%2520these%2520embedding%2520models%2520to%2520assess%2520the%250Ainfluence%2520of%2520these%2520choices%2520on%2520the%2520performance%2520of%2520the%2520localization%2520models.%2520Our%250Afindings%2520indicate%2520that%2520the%2520pre-training%2520strategies%2520significantly%2520affect%2520the%250Aquality%2520of%2520the%2520embedding.%2520Moreover%252C%2520we%2520discovered%2520that%2520the%2520familiarity%2520of%2520the%250Aembedding%2520models%2520with%2520the%2520data%2520has%2520a%2520notable%2520impact%2520on%2520the%2520bug%2520localization%250Amodel%2527s%2520performance.%2520Notably%252C%2520when%2520the%2520training%2520and%2520testing%2520data%2520are%2520collected%250Afrom%2520different%2520projects%252C%2520the%2520performance%2520of%2520the%2520bug%2520localization%2520models%250Aexhibits%2520substantial%2520fluctuations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Programming%20Language%20and%20Natural%20Language%3A%20Exploring%20Design%0A%20%20Choices%20in%20Multi-Modal%20Transformer-Based%20Embedding%20for%20Bug%20Localization&entry.906535625=Partha%20Chakraborty%20and%20Venkatraman%20Arumugam%20and%20Meiyappan%20Nagappan&entry.1292438233=%20%20Bug%20localization%20refers%20to%20the%20identification%20of%20source%20code%20files%20which%20is%0Ain%20a%20programming%20language%20and%20also%20responsible%20for%20the%20unexpected%20behavior%20of%0Asoftware%20using%20the%20bug%20report%2C%20which%20is%20a%20natural%20language.%20As%20bug%20localization%0Ais%20labor-intensive%2C%20bug%20localization%20models%20are%20employed%20to%20assist%20software%0Adevelopers.%20Due%20to%20the%20domain%20difference%20between%20source%20code%20files%20and%20bug%0Areports%2C%20modern%20bug-localization%20systems%2C%20based%20on%20deep%20learning%20models%2C%20rely%0Aheavily%20on%20embedding%20techniques%20that%20project%20bug%20reports%20and%20source%20code%20files%0Ainto%20a%20shared%20vector%20space.%20The%20creation%20of%20an%20embedding%20involves%20several%0Adesign%20choices%2C%20but%20the%20impact%20of%20these%20choices%20on%20the%20quality%20of%20embedding%20and%0Athe%20performance%20of%20bug%20localization%20models%20remains%20unexplained%20in%20current%0Aresearch.%0A%20%20To%20address%20this%20gap%2C%20our%20study%20evaluated%2014%20distinct%20embedding%20models%20to%20gain%0Ainsights%20into%20the%20effects%20of%20various%20design%20choices.%20Subsequently%2C%20we%20developed%0Abug%20localization%20models%20utilizing%20these%20embedding%20models%20to%20assess%20the%0Ainfluence%20of%20these%20choices%20on%20the%20performance%20of%20the%20localization%20models.%20Our%0Afindings%20indicate%20that%20the%20pre-training%20strategies%20significantly%20affect%20the%0Aquality%20of%20the%20embedding.%20Moreover%2C%20we%20discovered%20that%20the%20familiarity%20of%20the%0Aembedding%20models%20with%20the%20data%20has%20a%20notable%20impact%20on%20the%20bug%20localization%0Amodel%27s%20performance.%20Notably%2C%20when%20the%20training%20and%20testing%20data%20are%20collected%0Afrom%20different%20projects%2C%20the%20performance%20of%20the%20bug%20localization%20models%0Aexhibits%20substantial%20fluctuations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17615v1&entry.124074799=Read"},
{"title": "UHD-IQA Benchmark Database: Pushing the Boundaries of Blind Photo\n  Quality Assessment", "author": "Vlad Hosu and Lorenzo Agnolucci and Oliver Wiedemann and Daisuke Iso", "abstract": "  We introduce a novel Image Quality Assessment (IQA) dataset comprising 6073\nUHD-1 (4K) images, annotated at a fixed width of 3840 pixels. Contrary to\nexisting No-Reference (NR) IQA datasets, ours focuses on highly aesthetic\nphotos of high technical quality, filling a gap in the literature. The images,\ncarefully curated to exclude synthetic content, are sufficiently diverse to\ntrain general NR-IQA models. The dataset is annotated with perceptual quality\nratings obtained through a crowdsourcing study. Ten expert raters, comprising\nphotographers and graphics artists, assessed each image at least twice in\nmultiple sessions spanning several days, resulting in highly reliable labels.\nAnnotators were rigorously selected based on several metrics, including\nself-consistency, to ensure their reliability. The dataset includes rich\nmetadata with user and machine-generated tags from over 5,000 categories and\npopularity indicators such as favorites, likes, downloads, and views. With its\nunique characteristics, such as its focus on high-quality images, reliable\ncrowdsourced annotations, and high annotation resolution, our dataset opens up\nnew opportunities for advancing perceptual image quality assessment research\nand developing practical NR-IQA models that apply to modern photos. Our dataset\nis available at https://database.mmsp-kn.de/uhd-iqa-benchmark-database.html\n", "link": "http://arxiv.org/abs/2406.17472v1", "date": "2024-06-25", "relevancy": 1.9438, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5122}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4832}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UHD-IQA%20Benchmark%20Database%3A%20Pushing%20the%20Boundaries%20of%20Blind%20Photo%0A%20%20Quality%20Assessment&body=Title%3A%20UHD-IQA%20Benchmark%20Database%3A%20Pushing%20the%20Boundaries%20of%20Blind%20Photo%0A%20%20Quality%20Assessment%0AAuthor%3A%20Vlad%20Hosu%20and%20Lorenzo%20Agnolucci%20and%20Oliver%20Wiedemann%20and%20Daisuke%20Iso%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20Image%20Quality%20Assessment%20%28IQA%29%20dataset%20comprising%206073%0AUHD-1%20%284K%29%20images%2C%20annotated%20at%20a%20fixed%20width%20of%203840%20pixels.%20Contrary%20to%0Aexisting%20No-Reference%20%28NR%29%20IQA%20datasets%2C%20ours%20focuses%20on%20highly%20aesthetic%0Aphotos%20of%20high%20technical%20quality%2C%20filling%20a%20gap%20in%20the%20literature.%20The%20images%2C%0Acarefully%20curated%20to%20exclude%20synthetic%20content%2C%20are%20sufficiently%20diverse%20to%0Atrain%20general%20NR-IQA%20models.%20The%20dataset%20is%20annotated%20with%20perceptual%20quality%0Aratings%20obtained%20through%20a%20crowdsourcing%20study.%20Ten%20expert%20raters%2C%20comprising%0Aphotographers%20and%20graphics%20artists%2C%20assessed%20each%20image%20at%20least%20twice%20in%0Amultiple%20sessions%20spanning%20several%20days%2C%20resulting%20in%20highly%20reliable%20labels.%0AAnnotators%20were%20rigorously%20selected%20based%20on%20several%20metrics%2C%20including%0Aself-consistency%2C%20to%20ensure%20their%20reliability.%20The%20dataset%20includes%20rich%0Ametadata%20with%20user%20and%20machine-generated%20tags%20from%20over%205%2C000%20categories%20and%0Apopularity%20indicators%20such%20as%20favorites%2C%20likes%2C%20downloads%2C%20and%20views.%20With%20its%0Aunique%20characteristics%2C%20such%20as%20its%20focus%20on%20high-quality%20images%2C%20reliable%0Acrowdsourced%20annotations%2C%20and%20high%20annotation%20resolution%2C%20our%20dataset%20opens%20up%0Anew%20opportunities%20for%20advancing%20perceptual%20image%20quality%20assessment%20research%0Aand%20developing%20practical%20NR-IQA%20models%20that%20apply%20to%20modern%20photos.%20Our%20dataset%0Ais%20available%20at%20https%3A//database.mmsp-kn.de/uhd-iqa-benchmark-database.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUHD-IQA%2520Benchmark%2520Database%253A%2520Pushing%2520the%2520Boundaries%2520of%2520Blind%2520Photo%250A%2520%2520Quality%2520Assessment%26entry.906535625%3DVlad%2520Hosu%2520and%2520Lorenzo%2520Agnolucci%2520and%2520Oliver%2520Wiedemann%2520and%2520Daisuke%2520Iso%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520dataset%2520comprising%25206073%250AUHD-1%2520%25284K%2529%2520images%252C%2520annotated%2520at%2520a%2520fixed%2520width%2520of%25203840%2520pixels.%2520Contrary%2520to%250Aexisting%2520No-Reference%2520%2528NR%2529%2520IQA%2520datasets%252C%2520ours%2520focuses%2520on%2520highly%2520aesthetic%250Aphotos%2520of%2520high%2520technical%2520quality%252C%2520filling%2520a%2520gap%2520in%2520the%2520literature.%2520The%2520images%252C%250Acarefully%2520curated%2520to%2520exclude%2520synthetic%2520content%252C%2520are%2520sufficiently%2520diverse%2520to%250Atrain%2520general%2520NR-IQA%2520models.%2520The%2520dataset%2520is%2520annotated%2520with%2520perceptual%2520quality%250Aratings%2520obtained%2520through%2520a%2520crowdsourcing%2520study.%2520Ten%2520expert%2520raters%252C%2520comprising%250Aphotographers%2520and%2520graphics%2520artists%252C%2520assessed%2520each%2520image%2520at%2520least%2520twice%2520in%250Amultiple%2520sessions%2520spanning%2520several%2520days%252C%2520resulting%2520in%2520highly%2520reliable%2520labels.%250AAnnotators%2520were%2520rigorously%2520selected%2520based%2520on%2520several%2520metrics%252C%2520including%250Aself-consistency%252C%2520to%2520ensure%2520their%2520reliability.%2520The%2520dataset%2520includes%2520rich%250Ametadata%2520with%2520user%2520and%2520machine-generated%2520tags%2520from%2520over%25205%252C000%2520categories%2520and%250Apopularity%2520indicators%2520such%2520as%2520favorites%252C%2520likes%252C%2520downloads%252C%2520and%2520views.%2520With%2520its%250Aunique%2520characteristics%252C%2520such%2520as%2520its%2520focus%2520on%2520high-quality%2520images%252C%2520reliable%250Acrowdsourced%2520annotations%252C%2520and%2520high%2520annotation%2520resolution%252C%2520our%2520dataset%2520opens%2520up%250Anew%2520opportunities%2520for%2520advancing%2520perceptual%2520image%2520quality%2520assessment%2520research%250Aand%2520developing%2520practical%2520NR-IQA%2520models%2520that%2520apply%2520to%2520modern%2520photos.%2520Our%2520dataset%250Ais%2520available%2520at%2520https%253A//database.mmsp-kn.de/uhd-iqa-benchmark-database.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UHD-IQA%20Benchmark%20Database%3A%20Pushing%20the%20Boundaries%20of%20Blind%20Photo%0A%20%20Quality%20Assessment&entry.906535625=Vlad%20Hosu%20and%20Lorenzo%20Agnolucci%20and%20Oliver%20Wiedemann%20and%20Daisuke%20Iso&entry.1292438233=%20%20We%20introduce%20a%20novel%20Image%20Quality%20Assessment%20%28IQA%29%20dataset%20comprising%206073%0AUHD-1%20%284K%29%20images%2C%20annotated%20at%20a%20fixed%20width%20of%203840%20pixels.%20Contrary%20to%0Aexisting%20No-Reference%20%28NR%29%20IQA%20datasets%2C%20ours%20focuses%20on%20highly%20aesthetic%0Aphotos%20of%20high%20technical%20quality%2C%20filling%20a%20gap%20in%20the%20literature.%20The%20images%2C%0Acarefully%20curated%20to%20exclude%20synthetic%20content%2C%20are%20sufficiently%20diverse%20to%0Atrain%20general%20NR-IQA%20models.%20The%20dataset%20is%20annotated%20with%20perceptual%20quality%0Aratings%20obtained%20through%20a%20crowdsourcing%20study.%20Ten%20expert%20raters%2C%20comprising%0Aphotographers%20and%20graphics%20artists%2C%20assessed%20each%20image%20at%20least%20twice%20in%0Amultiple%20sessions%20spanning%20several%20days%2C%20resulting%20in%20highly%20reliable%20labels.%0AAnnotators%20were%20rigorously%20selected%20based%20on%20several%20metrics%2C%20including%0Aself-consistency%2C%20to%20ensure%20their%20reliability.%20The%20dataset%20includes%20rich%0Ametadata%20with%20user%20and%20machine-generated%20tags%20from%20over%205%2C000%20categories%20and%0Apopularity%20indicators%20such%20as%20favorites%2C%20likes%2C%20downloads%2C%20and%20views.%20With%20its%0Aunique%20characteristics%2C%20such%20as%20its%20focus%20on%20high-quality%20images%2C%20reliable%0Acrowdsourced%20annotations%2C%20and%20high%20annotation%20resolution%2C%20our%20dataset%20opens%20up%0Anew%20opportunities%20for%20advancing%20perceptual%20image%20quality%20assessment%20research%0Aand%20developing%20practical%20NR-IQA%20models%20that%20apply%20to%20modern%20photos.%20Our%20dataset%0Ais%20available%20at%20https%3A//database.mmsp-kn.de/uhd-iqa-benchmark-database.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17472v1&entry.124074799=Read"},
{"title": "Probabilistic Approach for Detection of High-Frequency Periodic Signals\n  using an Event Camera", "author": "David El-Chai Ben-Ezra and Ron Arad and Ayelet Padowicz and Israel Tugendhaft", "abstract": "  Being inspired by the biological eye, event camera is a novel asynchronous\ntechnology that pose a paradigm shift in acquisition of visual information.\nThis paradigm enables event cameras to capture pixel-size fast motions much\nmore naturally compared to classical cameras.\n  In this paper we present a new asynchronous event-driven algorithm for\ndetection of high-frequency pixel-size periodic signals using an event camera.\nDevelopment of such new algorithms, to efficiently process the asynchronous\ninformation of event cameras, is essential and being a main challenge in the\nresearch community, in order to utilize its special properties and potential.\n  It turns out that this algorithm, that was developed in order to satisfy the\nnew paradigm, is related to an untreated theoretical problem in probability:\nlet $0\\leq\\tau_{1}\\leq\\tau_{2}\\leq\\cdots\\leq\\tau_{m}\\leq1$, originated from an\nunknown distribution. Let also $\\epsilon,\\delta\\in\\mathbb{R}$, and\n$d\\in\\mathbb{N}$. What can be said about the probability $\\Phi(m,d)$ of having\nmore than $d$ adjacent $\\tau_{i}$-s pairs that the distance between them is\n$\\delta$, up to an error $\\epsilon$ ? This problem, that reminds the area of\norder statistic, shows how the new visualization paradigm is also an\nopportunity to develop new areas and problems in mathematics.\n", "link": "http://arxiv.org/abs/2205.04691v4", "date": "2024-06-25", "relevancy": 1.9416, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5014}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4875}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Approach%20for%20Detection%20of%20High-Frequency%20Periodic%20Signals%0A%20%20using%20an%20Event%20Camera&body=Title%3A%20Probabilistic%20Approach%20for%20Detection%20of%20High-Frequency%20Periodic%20Signals%0A%20%20using%20an%20Event%20Camera%0AAuthor%3A%20David%20El-Chai%20Ben-Ezra%20and%20Ron%20Arad%20and%20Ayelet%20Padowicz%20and%20Israel%20Tugendhaft%0AAbstract%3A%20%20%20Being%20inspired%20by%20the%20biological%20eye%2C%20event%20camera%20is%20a%20novel%20asynchronous%0Atechnology%20that%20pose%20a%20paradigm%20shift%20in%20acquisition%20of%20visual%20information.%0AThis%20paradigm%20enables%20event%20cameras%20to%20capture%20pixel-size%20fast%20motions%20much%0Amore%20naturally%20compared%20to%20classical%20cameras.%0A%20%20In%20this%20paper%20we%20present%20a%20new%20asynchronous%20event-driven%20algorithm%20for%0Adetection%20of%20high-frequency%20pixel-size%20periodic%20signals%20using%20an%20event%20camera.%0ADevelopment%20of%20such%20new%20algorithms%2C%20to%20efficiently%20process%20the%20asynchronous%0Ainformation%20of%20event%20cameras%2C%20is%20essential%20and%20being%20a%20main%20challenge%20in%20the%0Aresearch%20community%2C%20in%20order%20to%20utilize%20its%20special%20properties%20and%20potential.%0A%20%20It%20turns%20out%20that%20this%20algorithm%2C%20that%20was%20developed%20in%20order%20to%20satisfy%20the%0Anew%20paradigm%2C%20is%20related%20to%20an%20untreated%20theoretical%20problem%20in%20probability%3A%0Alet%20%240%5Cleq%5Ctau_%7B1%7D%5Cleq%5Ctau_%7B2%7D%5Cleq%5Ccdots%5Cleq%5Ctau_%7Bm%7D%5Cleq1%24%2C%20originated%20from%20an%0Aunknown%20distribution.%20Let%20also%20%24%5Cepsilon%2C%5Cdelta%5Cin%5Cmathbb%7BR%7D%24%2C%20and%0A%24d%5Cin%5Cmathbb%7BN%7D%24.%20What%20can%20be%20said%20about%20the%20probability%20%24%5CPhi%28m%2Cd%29%24%20of%20having%0Amore%20than%20%24d%24%20adjacent%20%24%5Ctau_%7Bi%7D%24-s%20pairs%20that%20the%20distance%20between%20them%20is%0A%24%5Cdelta%24%2C%20up%20to%20an%20error%20%24%5Cepsilon%24%20%3F%20This%20problem%2C%20that%20reminds%20the%20area%20of%0Aorder%20statistic%2C%20shows%20how%20the%20new%20visualization%20paradigm%20is%20also%20an%0Aopportunity%20to%20develop%20new%20areas%20and%20problems%20in%20mathematics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.04691v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Approach%2520for%2520Detection%2520of%2520High-Frequency%2520Periodic%2520Signals%250A%2520%2520using%2520an%2520Event%2520Camera%26entry.906535625%3DDavid%2520El-Chai%2520Ben-Ezra%2520and%2520Ron%2520Arad%2520and%2520Ayelet%2520Padowicz%2520and%2520Israel%2520Tugendhaft%26entry.1292438233%3D%2520%2520Being%2520inspired%2520by%2520the%2520biological%2520eye%252C%2520event%2520camera%2520is%2520a%2520novel%2520asynchronous%250Atechnology%2520that%2520pose%2520a%2520paradigm%2520shift%2520in%2520acquisition%2520of%2520visual%2520information.%250AThis%2520paradigm%2520enables%2520event%2520cameras%2520to%2520capture%2520pixel-size%2520fast%2520motions%2520much%250Amore%2520naturally%2520compared%2520to%2520classical%2520cameras.%250A%2520%2520In%2520this%2520paper%2520we%2520present%2520a%2520new%2520asynchronous%2520event-driven%2520algorithm%2520for%250Adetection%2520of%2520high-frequency%2520pixel-size%2520periodic%2520signals%2520using%2520an%2520event%2520camera.%250ADevelopment%2520of%2520such%2520new%2520algorithms%252C%2520to%2520efficiently%2520process%2520the%2520asynchronous%250Ainformation%2520of%2520event%2520cameras%252C%2520is%2520essential%2520and%2520being%2520a%2520main%2520challenge%2520in%2520the%250Aresearch%2520community%252C%2520in%2520order%2520to%2520utilize%2520its%2520special%2520properties%2520and%2520potential.%250A%2520%2520It%2520turns%2520out%2520that%2520this%2520algorithm%252C%2520that%2520was%2520developed%2520in%2520order%2520to%2520satisfy%2520the%250Anew%2520paradigm%252C%2520is%2520related%2520to%2520an%2520untreated%2520theoretical%2520problem%2520in%2520probability%253A%250Alet%2520%25240%255Cleq%255Ctau_%257B1%257D%255Cleq%255Ctau_%257B2%257D%255Cleq%255Ccdots%255Cleq%255Ctau_%257Bm%257D%255Cleq1%2524%252C%2520originated%2520from%2520an%250Aunknown%2520distribution.%2520Let%2520also%2520%2524%255Cepsilon%252C%255Cdelta%255Cin%255Cmathbb%257BR%257D%2524%252C%2520and%250A%2524d%255Cin%255Cmathbb%257BN%257D%2524.%2520What%2520can%2520be%2520said%2520about%2520the%2520probability%2520%2524%255CPhi%2528m%252Cd%2529%2524%2520of%2520having%250Amore%2520than%2520%2524d%2524%2520adjacent%2520%2524%255Ctau_%257Bi%257D%2524-s%2520pairs%2520that%2520the%2520distance%2520between%2520them%2520is%250A%2524%255Cdelta%2524%252C%2520up%2520to%2520an%2520error%2520%2524%255Cepsilon%2524%2520%253F%2520This%2520problem%252C%2520that%2520reminds%2520the%2520area%2520of%250Aorder%2520statistic%252C%2520shows%2520how%2520the%2520new%2520visualization%2520paradigm%2520is%2520also%2520an%250Aopportunity%2520to%2520develop%2520new%2520areas%2520and%2520problems%2520in%2520mathematics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.04691v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Approach%20for%20Detection%20of%20High-Frequency%20Periodic%20Signals%0A%20%20using%20an%20Event%20Camera&entry.906535625=David%20El-Chai%20Ben-Ezra%20and%20Ron%20Arad%20and%20Ayelet%20Padowicz%20and%20Israel%20Tugendhaft&entry.1292438233=%20%20Being%20inspired%20by%20the%20biological%20eye%2C%20event%20camera%20is%20a%20novel%20asynchronous%0Atechnology%20that%20pose%20a%20paradigm%20shift%20in%20acquisition%20of%20visual%20information.%0AThis%20paradigm%20enables%20event%20cameras%20to%20capture%20pixel-size%20fast%20motions%20much%0Amore%20naturally%20compared%20to%20classical%20cameras.%0A%20%20In%20this%20paper%20we%20present%20a%20new%20asynchronous%20event-driven%20algorithm%20for%0Adetection%20of%20high-frequency%20pixel-size%20periodic%20signals%20using%20an%20event%20camera.%0ADevelopment%20of%20such%20new%20algorithms%2C%20to%20efficiently%20process%20the%20asynchronous%0Ainformation%20of%20event%20cameras%2C%20is%20essential%20and%20being%20a%20main%20challenge%20in%20the%0Aresearch%20community%2C%20in%20order%20to%20utilize%20its%20special%20properties%20and%20potential.%0A%20%20It%20turns%20out%20that%20this%20algorithm%2C%20that%20was%20developed%20in%20order%20to%20satisfy%20the%0Anew%20paradigm%2C%20is%20related%20to%20an%20untreated%20theoretical%20problem%20in%20probability%3A%0Alet%20%240%5Cleq%5Ctau_%7B1%7D%5Cleq%5Ctau_%7B2%7D%5Cleq%5Ccdots%5Cleq%5Ctau_%7Bm%7D%5Cleq1%24%2C%20originated%20from%20an%0Aunknown%20distribution.%20Let%20also%20%24%5Cepsilon%2C%5Cdelta%5Cin%5Cmathbb%7BR%7D%24%2C%20and%0A%24d%5Cin%5Cmathbb%7BN%7D%24.%20What%20can%20be%20said%20about%20the%20probability%20%24%5CPhi%28m%2Cd%29%24%20of%20having%0Amore%20than%20%24d%24%20adjacent%20%24%5Ctau_%7Bi%7D%24-s%20pairs%20that%20the%20distance%20between%20them%20is%0A%24%5Cdelta%24%2C%20up%20to%20an%20error%20%24%5Cepsilon%24%20%3F%20This%20problem%2C%20that%20reminds%20the%20area%20of%0Aorder%20statistic%2C%20shows%20how%20the%20new%20visualization%20paradigm%20is%20also%20an%0Aopportunity%20to%20develop%20new%20areas%20and%20problems%20in%20mathematics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.04691v4&entry.124074799=Read"},
{"title": "Analysis of learning a flow-based generative model from limited sample\n  complexity", "author": "Hugo Cui and Florent Krzakala and Eric Vanden-Eijnden and Lenka Zdeborov\u00e1", "abstract": "  We study the problem of training a flow-based generative model, parametrized\nby a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture.\nWe provide a sharp end-to-end analysis of the problem. First, we provide a\ntight closed-form characterization of the learnt velocity field, when\nparametrized by a shallow denoising auto-encoder trained on a finite number $n$\nof samples from the target distribution. Building on this analysis, we provide\na sharp description of the corresponding generative flow, which pushes the base\nGaussian density forward to an approximation of the target density. In\nparticular, we provide closed-form formulae for the distance between the mean\nof the generated mixture and the mean of the target mixture, which we show\ndecays as $\\Theta_n(\\frac{1}{n})$. Finally, this rate is shown to be in fact\nBayes-optimal.\n", "link": "http://arxiv.org/abs/2310.03575v2", "date": "2024-06-25", "relevancy": 1.9359, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5487}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4869}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20learning%20a%20flow-based%20generative%20model%20from%20limited%20sample%0A%20%20complexity&body=Title%3A%20Analysis%20of%20learning%20a%20flow-based%20generative%20model%20from%20limited%20sample%0A%20%20complexity%0AAuthor%3A%20Hugo%20Cui%20and%20Florent%20Krzakala%20and%20Eric%20Vanden-Eijnden%20and%20Lenka%20Zdeborov%C3%A1%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20training%20a%20flow-based%20generative%20model%2C%20parametrized%0Aby%20a%20two-layer%20autoencoder%2C%20to%20sample%20from%20a%20high-dimensional%20Gaussian%20mixture.%0AWe%20provide%20a%20sharp%20end-to-end%20analysis%20of%20the%20problem.%20First%2C%20we%20provide%20a%0Atight%20closed-form%20characterization%20of%20the%20learnt%20velocity%20field%2C%20when%0Aparametrized%20by%20a%20shallow%20denoising%20auto-encoder%20trained%20on%20a%20finite%20number%20%24n%24%0Aof%20samples%20from%20the%20target%20distribution.%20Building%20on%20this%20analysis%2C%20we%20provide%0Aa%20sharp%20description%20of%20the%20corresponding%20generative%20flow%2C%20which%20pushes%20the%20base%0AGaussian%20density%20forward%20to%20an%20approximation%20of%20the%20target%20density.%20In%0Aparticular%2C%20we%20provide%20closed-form%20formulae%20for%20the%20distance%20between%20the%20mean%0Aof%20the%20generated%20mixture%20and%20the%20mean%20of%20the%20target%20mixture%2C%20which%20we%20show%0Adecays%20as%20%24%5CTheta_n%28%5Cfrac%7B1%7D%7Bn%7D%29%24.%20Finally%2C%20this%20rate%20is%20shown%20to%20be%20in%20fact%0ABayes-optimal.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03575v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520learning%2520a%2520flow-based%2520generative%2520model%2520from%2520limited%2520sample%250A%2520%2520complexity%26entry.906535625%3DHugo%2520Cui%2520and%2520Florent%2520Krzakala%2520and%2520Eric%2520Vanden-Eijnden%2520and%2520Lenka%2520Zdeborov%25C3%25A1%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520training%2520a%2520flow-based%2520generative%2520model%252C%2520parametrized%250Aby%2520a%2520two-layer%2520autoencoder%252C%2520to%2520sample%2520from%2520a%2520high-dimensional%2520Gaussian%2520mixture.%250AWe%2520provide%2520a%2520sharp%2520end-to-end%2520analysis%2520of%2520the%2520problem.%2520First%252C%2520we%2520provide%2520a%250Atight%2520closed-form%2520characterization%2520of%2520the%2520learnt%2520velocity%2520field%252C%2520when%250Aparametrized%2520by%2520a%2520shallow%2520denoising%2520auto-encoder%2520trained%2520on%2520a%2520finite%2520number%2520%2524n%2524%250Aof%2520samples%2520from%2520the%2520target%2520distribution.%2520Building%2520on%2520this%2520analysis%252C%2520we%2520provide%250Aa%2520sharp%2520description%2520of%2520the%2520corresponding%2520generative%2520flow%252C%2520which%2520pushes%2520the%2520base%250AGaussian%2520density%2520forward%2520to%2520an%2520approximation%2520of%2520the%2520target%2520density.%2520In%250Aparticular%252C%2520we%2520provide%2520closed-form%2520formulae%2520for%2520the%2520distance%2520between%2520the%2520mean%250Aof%2520the%2520generated%2520mixture%2520and%2520the%2520mean%2520of%2520the%2520target%2520mixture%252C%2520which%2520we%2520show%250Adecays%2520as%2520%2524%255CTheta_n%2528%255Cfrac%257B1%257D%257Bn%257D%2529%2524.%2520Finally%252C%2520this%2520rate%2520is%2520shown%2520to%2520be%2520in%2520fact%250ABayes-optimal.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03575v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20learning%20a%20flow-based%20generative%20model%20from%20limited%20sample%0A%20%20complexity&entry.906535625=Hugo%20Cui%20and%20Florent%20Krzakala%20and%20Eric%20Vanden-Eijnden%20and%20Lenka%20Zdeborov%C3%A1&entry.1292438233=%20%20We%20study%20the%20problem%20of%20training%20a%20flow-based%20generative%20model%2C%20parametrized%0Aby%20a%20two-layer%20autoencoder%2C%20to%20sample%20from%20a%20high-dimensional%20Gaussian%20mixture.%0AWe%20provide%20a%20sharp%20end-to-end%20analysis%20of%20the%20problem.%20First%2C%20we%20provide%20a%0Atight%20closed-form%20characterization%20of%20the%20learnt%20velocity%20field%2C%20when%0Aparametrized%20by%20a%20shallow%20denoising%20auto-encoder%20trained%20on%20a%20finite%20number%20%24n%24%0Aof%20samples%20from%20the%20target%20distribution.%20Building%20on%20this%20analysis%2C%20we%20provide%0Aa%20sharp%20description%20of%20the%20corresponding%20generative%20flow%2C%20which%20pushes%20the%20base%0AGaussian%20density%20forward%20to%20an%20approximation%20of%20the%20target%20density.%20In%0Aparticular%2C%20we%20provide%20closed-form%20formulae%20for%20the%20distance%20between%20the%20mean%0Aof%20the%20generated%20mixture%20and%20the%20mean%20of%20the%20target%20mixture%2C%20which%20we%20show%0Adecays%20as%20%24%5CTheta_n%28%5Cfrac%7B1%7D%7Bn%7D%29%24.%20Finally%2C%20this%20rate%20is%20shown%20to%20be%20in%20fact%0ABayes-optimal.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03575v2&entry.124074799=Read"},
{"title": "FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model", "author": "Feijie Wu and Zitao Li and Yaliang Li and Bolin Ding and Jing Gao", "abstract": "  Large language models (LLMs) show amazing performance on many domain-specific\ntasks after fine-tuning with some appropriate data. However, many\ndomain-specific data are privately distributed across multiple owners. Thus,\nthis dilemma raises the interest in how to perform LLM fine-tuning in federated\nlearning (FL). However, confronted with limited computation and communication\ncapacities, FL clients struggle to fine-tune an LLM effectively. To this end,\nwe introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL.\nSpecifically, our method involves the server generating a compressed LLM and\naligning its performance with the full model. Subsequently, the clients\nfine-tune a lightweight yet important part of the compressed model, referred to\nas an adapter. Notice that as the server has no access to the private data\nowned by the clients, the data used for alignment by the server has a different\ndistribution from the one used for fine-tuning by clients. We formulate the\nproblem into a bi-level optimization problem to minimize the negative effect of\ndata discrepancy and derive the updating rules for the server and clients. We\nconduct extensive experiments on LLaMA-2, empirically showing that the adapter\nhas exceptional performance when reintegrated into the global LLM. The results\nalso indicate that the proposed FedBiOT significantly reduces resource\nconsumption compared to existing benchmarks, all while achieving comparable\nperformance levels.\n", "link": "http://arxiv.org/abs/2406.17706v1", "date": "2024-06-25", "relevancy": 1.9324, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4912}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedBiOT%3A%20LLM%20Local%20Fine-tuning%20in%20Federated%20Learning%20without%20Full%20Model&body=Title%3A%20FedBiOT%3A%20LLM%20Local%20Fine-tuning%20in%20Federated%20Learning%20without%20Full%20Model%0AAuthor%3A%20Feijie%20Wu%20and%20Zitao%20Li%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jing%20Gao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20show%20amazing%20performance%20on%20many%20domain-specific%0Atasks%20after%20fine-tuning%20with%20some%20appropriate%20data.%20However%2C%20many%0Adomain-specific%20data%20are%20privately%20distributed%20across%20multiple%20owners.%20Thus%2C%0Athis%20dilemma%20raises%20the%20interest%20in%20how%20to%20perform%20LLM%20fine-tuning%20in%20federated%0Alearning%20%28FL%29.%20However%2C%20confronted%20with%20limited%20computation%20and%20communication%0Acapacities%2C%20FL%20clients%20struggle%20to%20fine-tune%20an%20LLM%20effectively.%20To%20this%20end%2C%0Awe%20introduce%20FedBiOT%2C%20a%20resource-efficient%20LLM%20fine-tuning%20approach%20to%20FL.%0ASpecifically%2C%20our%20method%20involves%20the%20server%20generating%20a%20compressed%20LLM%20and%0Aaligning%20its%20performance%20with%20the%20full%20model.%20Subsequently%2C%20the%20clients%0Afine-tune%20a%20lightweight%20yet%20important%20part%20of%20the%20compressed%20model%2C%20referred%20to%0Aas%20an%20adapter.%20Notice%20that%20as%20the%20server%20has%20no%20access%20to%20the%20private%20data%0Aowned%20by%20the%20clients%2C%20the%20data%20used%20for%20alignment%20by%20the%20server%20has%20a%20different%0Adistribution%20from%20the%20one%20used%20for%20fine-tuning%20by%20clients.%20We%20formulate%20the%0Aproblem%20into%20a%20bi-level%20optimization%20problem%20to%20minimize%20the%20negative%20effect%20of%0Adata%20discrepancy%20and%20derive%20the%20updating%20rules%20for%20the%20server%20and%20clients.%20We%0Aconduct%20extensive%20experiments%20on%20LLaMA-2%2C%20empirically%20showing%20that%20the%20adapter%0Ahas%20exceptional%20performance%20when%20reintegrated%20into%20the%20global%20LLM.%20The%20results%0Aalso%20indicate%20that%20the%20proposed%20FedBiOT%20significantly%20reduces%20resource%0Aconsumption%20compared%20to%20existing%20benchmarks%2C%20all%20while%20achieving%20comparable%0Aperformance%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedBiOT%253A%2520LLM%2520Local%2520Fine-tuning%2520in%2520Federated%2520Learning%2520without%2520Full%2520Model%26entry.906535625%3DFeijie%2520Wu%2520and%2520Zitao%2520Li%2520and%2520Yaliang%2520Li%2520and%2520Bolin%2520Ding%2520and%2520Jing%2520Gao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520show%2520amazing%2520performance%2520on%2520many%2520domain-specific%250Atasks%2520after%2520fine-tuning%2520with%2520some%2520appropriate%2520data.%2520However%252C%2520many%250Adomain-specific%2520data%2520are%2520privately%2520distributed%2520across%2520multiple%2520owners.%2520Thus%252C%250Athis%2520dilemma%2520raises%2520the%2520interest%2520in%2520how%2520to%2520perform%2520LLM%2520fine-tuning%2520in%2520federated%250Alearning%2520%2528FL%2529.%2520However%252C%2520confronted%2520with%2520limited%2520computation%2520and%2520communication%250Acapacities%252C%2520FL%2520clients%2520struggle%2520to%2520fine-tune%2520an%2520LLM%2520effectively.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520FedBiOT%252C%2520a%2520resource-efficient%2520LLM%2520fine-tuning%2520approach%2520to%2520FL.%250ASpecifically%252C%2520our%2520method%2520involves%2520the%2520server%2520generating%2520a%2520compressed%2520LLM%2520and%250Aaligning%2520its%2520performance%2520with%2520the%2520full%2520model.%2520Subsequently%252C%2520the%2520clients%250Afine-tune%2520a%2520lightweight%2520yet%2520important%2520part%2520of%2520the%2520compressed%2520model%252C%2520referred%2520to%250Aas%2520an%2520adapter.%2520Notice%2520that%2520as%2520the%2520server%2520has%2520no%2520access%2520to%2520the%2520private%2520data%250Aowned%2520by%2520the%2520clients%252C%2520the%2520data%2520used%2520for%2520alignment%2520by%2520the%2520server%2520has%2520a%2520different%250Adistribution%2520from%2520the%2520one%2520used%2520for%2520fine-tuning%2520by%2520clients.%2520We%2520formulate%2520the%250Aproblem%2520into%2520a%2520bi-level%2520optimization%2520problem%2520to%2520minimize%2520the%2520negative%2520effect%2520of%250Adata%2520discrepancy%2520and%2520derive%2520the%2520updating%2520rules%2520for%2520the%2520server%2520and%2520clients.%2520We%250Aconduct%2520extensive%2520experiments%2520on%2520LLaMA-2%252C%2520empirically%2520showing%2520that%2520the%2520adapter%250Ahas%2520exceptional%2520performance%2520when%2520reintegrated%2520into%2520the%2520global%2520LLM.%2520The%2520results%250Aalso%2520indicate%2520that%2520the%2520proposed%2520FedBiOT%2520significantly%2520reduces%2520resource%250Aconsumption%2520compared%2520to%2520existing%2520benchmarks%252C%2520all%2520while%2520achieving%2520comparable%250Aperformance%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedBiOT%3A%20LLM%20Local%20Fine-tuning%20in%20Federated%20Learning%20without%20Full%20Model&entry.906535625=Feijie%20Wu%20and%20Zitao%20Li%20and%20Yaliang%20Li%20and%20Bolin%20Ding%20and%20Jing%20Gao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20show%20amazing%20performance%20on%20many%20domain-specific%0Atasks%20after%20fine-tuning%20with%20some%20appropriate%20data.%20However%2C%20many%0Adomain-specific%20data%20are%20privately%20distributed%20across%20multiple%20owners.%20Thus%2C%0Athis%20dilemma%20raises%20the%20interest%20in%20how%20to%20perform%20LLM%20fine-tuning%20in%20federated%0Alearning%20%28FL%29.%20However%2C%20confronted%20with%20limited%20computation%20and%20communication%0Acapacities%2C%20FL%20clients%20struggle%20to%20fine-tune%20an%20LLM%20effectively.%20To%20this%20end%2C%0Awe%20introduce%20FedBiOT%2C%20a%20resource-efficient%20LLM%20fine-tuning%20approach%20to%20FL.%0ASpecifically%2C%20our%20method%20involves%20the%20server%20generating%20a%20compressed%20LLM%20and%0Aaligning%20its%20performance%20with%20the%20full%20model.%20Subsequently%2C%20the%20clients%0Afine-tune%20a%20lightweight%20yet%20important%20part%20of%20the%20compressed%20model%2C%20referred%20to%0Aas%20an%20adapter.%20Notice%20that%20as%20the%20server%20has%20no%20access%20to%20the%20private%20data%0Aowned%20by%20the%20clients%2C%20the%20data%20used%20for%20alignment%20by%20the%20server%20has%20a%20different%0Adistribution%20from%20the%20one%20used%20for%20fine-tuning%20by%20clients.%20We%20formulate%20the%0Aproblem%20into%20a%20bi-level%20optimization%20problem%20to%20minimize%20the%20negative%20effect%20of%0Adata%20discrepancy%20and%20derive%20the%20updating%20rules%20for%20the%20server%20and%20clients.%20We%0Aconduct%20extensive%20experiments%20on%20LLaMA-2%2C%20empirically%20showing%20that%20the%20adapter%0Ahas%20exceptional%20performance%20when%20reintegrated%20into%20the%20global%20LLM.%20The%20results%0Aalso%20indicate%20that%20the%20proposed%20FedBiOT%20significantly%20reduces%20resource%0Aconsumption%20compared%20to%20existing%20benchmarks%2C%20all%20while%20achieving%20comparable%0Aperformance%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17706v1&entry.124074799=Read"},
{"title": "MSRS: Training Multimodal Speech Recognition Models from Scratch with\n  Sparse Mask Optimization", "author": "Adriana Fernandez-Lopez and Honglie Chen and Pingchuan Ma and Lu Yin and Qiao Xiao and Stavros Petridis and Shiwei Liu and Maja Pantic", "abstract": "  Pre-trained models have been a foundational approach in speech recognition,\nalbeit with associated additional costs. In this study, we propose a\nregularization technique that facilitates the training of visual and\naudio-visual speech recognition models (VSR and AVSR) from scratch. This\napproach, abbreviated as \\textbf{MSRS} (Multimodal Speech Recognition from\nScratch), introduces a sparse regularization that rapidly learns sparse\nstructures within the dense model at the very beginning of training, which\nreceives healthier gradient flow than the dense equivalent. Once the sparse\nmask stabilizes, our method allows transitioning to a dense model or keeping a\nsparse model by updating non-zero values. MSRS achieves competitive results in\nVSR and AVSR with 21.1% and 0.9% WER on the LRS3 benchmark, while reducing\ntraining time by at least 2x. We explore other sparse approaches and show that\nonly MSRS enables training from scratch by implicitly masking the weights\naffected by vanishing gradients.\n", "link": "http://arxiv.org/abs/2406.17614v1", "date": "2024-06-25", "relevancy": 1.9286, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4837}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSRS%3A%20Training%20Multimodal%20Speech%20Recognition%20Models%20from%20Scratch%20with%0A%20%20Sparse%20Mask%20Optimization&body=Title%3A%20MSRS%3A%20Training%20Multimodal%20Speech%20Recognition%20Models%20from%20Scratch%20with%0A%20%20Sparse%20Mask%20Optimization%0AAuthor%3A%20Adriana%20Fernandez-Lopez%20and%20Honglie%20Chen%20and%20Pingchuan%20Ma%20and%20Lu%20Yin%20and%20Qiao%20Xiao%20and%20Stavros%20Petridis%20and%20Shiwei%20Liu%20and%20Maja%20Pantic%0AAbstract%3A%20%20%20Pre-trained%20models%20have%20been%20a%20foundational%20approach%20in%20speech%20recognition%2C%0Aalbeit%20with%20associated%20additional%20costs.%20In%20this%20study%2C%20we%20propose%20a%0Aregularization%20technique%20that%20facilitates%20the%20training%20of%20visual%20and%0Aaudio-visual%20speech%20recognition%20models%20%28VSR%20and%20AVSR%29%20from%20scratch.%20This%0Aapproach%2C%20abbreviated%20as%20%5Ctextbf%7BMSRS%7D%20%28Multimodal%20Speech%20Recognition%20from%0AScratch%29%2C%20introduces%20a%20sparse%20regularization%20that%20rapidly%20learns%20sparse%0Astructures%20within%20the%20dense%20model%20at%20the%20very%20beginning%20of%20training%2C%20which%0Areceives%20healthier%20gradient%20flow%20than%20the%20dense%20equivalent.%20Once%20the%20sparse%0Amask%20stabilizes%2C%20our%20method%20allows%20transitioning%20to%20a%20dense%20model%20or%20keeping%20a%0Asparse%20model%20by%20updating%20non-zero%20values.%20MSRS%20achieves%20competitive%20results%20in%0AVSR%20and%20AVSR%20with%2021.1%25%20and%200.9%25%20WER%20on%20the%20LRS3%20benchmark%2C%20while%20reducing%0Atraining%20time%20by%20at%20least%202x.%20We%20explore%20other%20sparse%20approaches%20and%20show%20that%0Aonly%20MSRS%20enables%20training%20from%20scratch%20by%20implicitly%20masking%20the%20weights%0Aaffected%20by%20vanishing%20gradients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSRS%253A%2520Training%2520Multimodal%2520Speech%2520Recognition%2520Models%2520from%2520Scratch%2520with%250A%2520%2520Sparse%2520Mask%2520Optimization%26entry.906535625%3DAdriana%2520Fernandez-Lopez%2520and%2520Honglie%2520Chen%2520and%2520Pingchuan%2520Ma%2520and%2520Lu%2520Yin%2520and%2520Qiao%2520Xiao%2520and%2520Stavros%2520Petridis%2520and%2520Shiwei%2520Liu%2520and%2520Maja%2520Pantic%26entry.1292438233%3D%2520%2520Pre-trained%2520models%2520have%2520been%2520a%2520foundational%2520approach%2520in%2520speech%2520recognition%252C%250Aalbeit%2520with%2520associated%2520additional%2520costs.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%250Aregularization%2520technique%2520that%2520facilitates%2520the%2520training%2520of%2520visual%2520and%250Aaudio-visual%2520speech%2520recognition%2520models%2520%2528VSR%2520and%2520AVSR%2529%2520from%2520scratch.%2520This%250Aapproach%252C%2520abbreviated%2520as%2520%255Ctextbf%257BMSRS%257D%2520%2528Multimodal%2520Speech%2520Recognition%2520from%250AScratch%2529%252C%2520introduces%2520a%2520sparse%2520regularization%2520that%2520rapidly%2520learns%2520sparse%250Astructures%2520within%2520the%2520dense%2520model%2520at%2520the%2520very%2520beginning%2520of%2520training%252C%2520which%250Areceives%2520healthier%2520gradient%2520flow%2520than%2520the%2520dense%2520equivalent.%2520Once%2520the%2520sparse%250Amask%2520stabilizes%252C%2520our%2520method%2520allows%2520transitioning%2520to%2520a%2520dense%2520model%2520or%2520keeping%2520a%250Asparse%2520model%2520by%2520updating%2520non-zero%2520values.%2520MSRS%2520achieves%2520competitive%2520results%2520in%250AVSR%2520and%2520AVSR%2520with%252021.1%2525%2520and%25200.9%2525%2520WER%2520on%2520the%2520LRS3%2520benchmark%252C%2520while%2520reducing%250Atraining%2520time%2520by%2520at%2520least%25202x.%2520We%2520explore%2520other%2520sparse%2520approaches%2520and%2520show%2520that%250Aonly%2520MSRS%2520enables%2520training%2520from%2520scratch%2520by%2520implicitly%2520masking%2520the%2520weights%250Aaffected%2520by%2520vanishing%2520gradients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSRS%3A%20Training%20Multimodal%20Speech%20Recognition%20Models%20from%20Scratch%20with%0A%20%20Sparse%20Mask%20Optimization&entry.906535625=Adriana%20Fernandez-Lopez%20and%20Honglie%20Chen%20and%20Pingchuan%20Ma%20and%20Lu%20Yin%20and%20Qiao%20Xiao%20and%20Stavros%20Petridis%20and%20Shiwei%20Liu%20and%20Maja%20Pantic&entry.1292438233=%20%20Pre-trained%20models%20have%20been%20a%20foundational%20approach%20in%20speech%20recognition%2C%0Aalbeit%20with%20associated%20additional%20costs.%20In%20this%20study%2C%20we%20propose%20a%0Aregularization%20technique%20that%20facilitates%20the%20training%20of%20visual%20and%0Aaudio-visual%20speech%20recognition%20models%20%28VSR%20and%20AVSR%29%20from%20scratch.%20This%0Aapproach%2C%20abbreviated%20as%20%5Ctextbf%7BMSRS%7D%20%28Multimodal%20Speech%20Recognition%20from%0AScratch%29%2C%20introduces%20a%20sparse%20regularization%20that%20rapidly%20learns%20sparse%0Astructures%20within%20the%20dense%20model%20at%20the%20very%20beginning%20of%20training%2C%20which%0Areceives%20healthier%20gradient%20flow%20than%20the%20dense%20equivalent.%20Once%20the%20sparse%0Amask%20stabilizes%2C%20our%20method%20allows%20transitioning%20to%20a%20dense%20model%20or%20keeping%20a%0Asparse%20model%20by%20updating%20non-zero%20values.%20MSRS%20achieves%20competitive%20results%20in%0AVSR%20and%20AVSR%20with%2021.1%25%20and%200.9%25%20WER%20on%20the%20LRS3%20benchmark%2C%20while%20reducing%0Atraining%20time%20by%20at%20least%202x.%20We%20explore%20other%20sparse%20approaches%20and%20show%20that%0Aonly%20MSRS%20enables%20training%20from%20scratch%20by%20implicitly%20masking%20the%20weights%0Aaffected%20by%20vanishing%20gradients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17614v1&entry.124074799=Read"},
{"title": "PiPar: Pipeline Parallelism for Collaborative Machine Learning", "author": "Zihan Zhang and Philip Rodgers and Peter Kilpatrick and Ivor Spence and Blesson Varghese", "abstract": "  Collaborative machine learning (CML) techniques, such as federated learning,\nhave been proposed to train deep learning models across multiple mobile devices\nand a server. CML techniques are privacy-preserving as a local model that is\ntrained on each device instead of the raw data from the device is shared with\nthe server. However, CML training is inefficient due to low resource\nutilization. We identify idling resources on the server and devices due to\nsequential computation and communication as the principal cause of low resource\nutilization. A novel framework PiPar that leverages pipeline parallelism for\nCML techniques is developed to substantially improve resource utilization. A\nnew training pipeline is designed to parallelize the computations on different\nhardware resources and communication on different bandwidth resources, thereby\naccelerating the training process in CML. A low overhead automated parameter\nselection method is proposed to optimize the pipeline, maximizing the\nutilization of available resources. The experimental results confirm the\nvalidity of the underlying approach of PiPar and highlight that when compared\nto federated learning: (i) the idle time of the server can be reduced by up to\n64.1x, and (ii) the overall training time can be accelerated by up to 34.6x\nunder varying network conditions for a collection of six small and large\npopular deep neural networks and four datasets without sacrificing accuracy. It\nis also experimentally demonstrated that PiPar achieves performance benefits\nwhen incorporating differential privacy methods and operating in environments\nwith heterogeneous devices and changing bandwidths.\n", "link": "http://arxiv.org/abs/2302.12803v2", "date": "2024-06-25", "relevancy": 1.9276, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4957}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4736}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PiPar%3A%20Pipeline%20Parallelism%20for%20Collaborative%20Machine%20Learning&body=Title%3A%20PiPar%3A%20Pipeline%20Parallelism%20for%20Collaborative%20Machine%20Learning%0AAuthor%3A%20Zihan%20Zhang%20and%20Philip%20Rodgers%20and%20Peter%20Kilpatrick%20and%20Ivor%20Spence%20and%20Blesson%20Varghese%0AAbstract%3A%20%20%20Collaborative%20machine%20learning%20%28CML%29%20techniques%2C%20such%20as%20federated%20learning%2C%0Ahave%20been%20proposed%20to%20train%20deep%20learning%20models%20across%20multiple%20mobile%20devices%0Aand%20a%20server.%20CML%20techniques%20are%20privacy-preserving%20as%20a%20local%20model%20that%20is%0Atrained%20on%20each%20device%20instead%20of%20the%20raw%20data%20from%20the%20device%20is%20shared%20with%0Athe%20server.%20However%2C%20CML%20training%20is%20inefficient%20due%20to%20low%20resource%0Autilization.%20We%20identify%20idling%20resources%20on%20the%20server%20and%20devices%20due%20to%0Asequential%20computation%20and%20communication%20as%20the%20principal%20cause%20of%20low%20resource%0Autilization.%20A%20novel%20framework%20PiPar%20that%20leverages%20pipeline%20parallelism%20for%0ACML%20techniques%20is%20developed%20to%20substantially%20improve%20resource%20utilization.%20A%0Anew%20training%20pipeline%20is%20designed%20to%20parallelize%20the%20computations%20on%20different%0Ahardware%20resources%20and%20communication%20on%20different%20bandwidth%20resources%2C%20thereby%0Aaccelerating%20the%20training%20process%20in%20CML.%20A%20low%20overhead%20automated%20parameter%0Aselection%20method%20is%20proposed%20to%20optimize%20the%20pipeline%2C%20maximizing%20the%0Autilization%20of%20available%20resources.%20The%20experimental%20results%20confirm%20the%0Avalidity%20of%20the%20underlying%20approach%20of%20PiPar%20and%20highlight%20that%20when%20compared%0Ato%20federated%20learning%3A%20%28i%29%20the%20idle%20time%20of%20the%20server%20can%20be%20reduced%20by%20up%20to%0A64.1x%2C%20and%20%28ii%29%20the%20overall%20training%20time%20can%20be%20accelerated%20by%20up%20to%2034.6x%0Aunder%20varying%20network%20conditions%20for%20a%20collection%20of%20six%20small%20and%20large%0Apopular%20deep%20neural%20networks%20and%20four%20datasets%20without%20sacrificing%20accuracy.%20It%0Ais%20also%20experimentally%20demonstrated%20that%20PiPar%20achieves%20performance%20benefits%0Awhen%20incorporating%20differential%20privacy%20methods%20and%20operating%20in%20environments%0Awith%20heterogeneous%20devices%20and%20changing%20bandwidths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.12803v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiPar%253A%2520Pipeline%2520Parallelism%2520for%2520Collaborative%2520Machine%2520Learning%26entry.906535625%3DZihan%2520Zhang%2520and%2520Philip%2520Rodgers%2520and%2520Peter%2520Kilpatrick%2520and%2520Ivor%2520Spence%2520and%2520Blesson%2520Varghese%26entry.1292438233%3D%2520%2520Collaborative%2520machine%2520learning%2520%2528CML%2529%2520techniques%252C%2520such%2520as%2520federated%2520learning%252C%250Ahave%2520been%2520proposed%2520to%2520train%2520deep%2520learning%2520models%2520across%2520multiple%2520mobile%2520devices%250Aand%2520a%2520server.%2520CML%2520techniques%2520are%2520privacy-preserving%2520as%2520a%2520local%2520model%2520that%2520is%250Atrained%2520on%2520each%2520device%2520instead%2520of%2520the%2520raw%2520data%2520from%2520the%2520device%2520is%2520shared%2520with%250Athe%2520server.%2520However%252C%2520CML%2520training%2520is%2520inefficient%2520due%2520to%2520low%2520resource%250Autilization.%2520We%2520identify%2520idling%2520resources%2520on%2520the%2520server%2520and%2520devices%2520due%2520to%250Asequential%2520computation%2520and%2520communication%2520as%2520the%2520principal%2520cause%2520of%2520low%2520resource%250Autilization.%2520A%2520novel%2520framework%2520PiPar%2520that%2520leverages%2520pipeline%2520parallelism%2520for%250ACML%2520techniques%2520is%2520developed%2520to%2520substantially%2520improve%2520resource%2520utilization.%2520A%250Anew%2520training%2520pipeline%2520is%2520designed%2520to%2520parallelize%2520the%2520computations%2520on%2520different%250Ahardware%2520resources%2520and%2520communication%2520on%2520different%2520bandwidth%2520resources%252C%2520thereby%250Aaccelerating%2520the%2520training%2520process%2520in%2520CML.%2520A%2520low%2520overhead%2520automated%2520parameter%250Aselection%2520method%2520is%2520proposed%2520to%2520optimize%2520the%2520pipeline%252C%2520maximizing%2520the%250Autilization%2520of%2520available%2520resources.%2520The%2520experimental%2520results%2520confirm%2520the%250Avalidity%2520of%2520the%2520underlying%2520approach%2520of%2520PiPar%2520and%2520highlight%2520that%2520when%2520compared%250Ato%2520federated%2520learning%253A%2520%2528i%2529%2520the%2520idle%2520time%2520of%2520the%2520server%2520can%2520be%2520reduced%2520by%2520up%2520to%250A64.1x%252C%2520and%2520%2528ii%2529%2520the%2520overall%2520training%2520time%2520can%2520be%2520accelerated%2520by%2520up%2520to%252034.6x%250Aunder%2520varying%2520network%2520conditions%2520for%2520a%2520collection%2520of%2520six%2520small%2520and%2520large%250Apopular%2520deep%2520neural%2520networks%2520and%2520four%2520datasets%2520without%2520sacrificing%2520accuracy.%2520It%250Ais%2520also%2520experimentally%2520demonstrated%2520that%2520PiPar%2520achieves%2520performance%2520benefits%250Awhen%2520incorporating%2520differential%2520privacy%2520methods%2520and%2520operating%2520in%2520environments%250Awith%2520heterogeneous%2520devices%2520and%2520changing%2520bandwidths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.12803v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PiPar%3A%20Pipeline%20Parallelism%20for%20Collaborative%20Machine%20Learning&entry.906535625=Zihan%20Zhang%20and%20Philip%20Rodgers%20and%20Peter%20Kilpatrick%20and%20Ivor%20Spence%20and%20Blesson%20Varghese&entry.1292438233=%20%20Collaborative%20machine%20learning%20%28CML%29%20techniques%2C%20such%20as%20federated%20learning%2C%0Ahave%20been%20proposed%20to%20train%20deep%20learning%20models%20across%20multiple%20mobile%20devices%0Aand%20a%20server.%20CML%20techniques%20are%20privacy-preserving%20as%20a%20local%20model%20that%20is%0Atrained%20on%20each%20device%20instead%20of%20the%20raw%20data%20from%20the%20device%20is%20shared%20with%0Athe%20server.%20However%2C%20CML%20training%20is%20inefficient%20due%20to%20low%20resource%0Autilization.%20We%20identify%20idling%20resources%20on%20the%20server%20and%20devices%20due%20to%0Asequential%20computation%20and%20communication%20as%20the%20principal%20cause%20of%20low%20resource%0Autilization.%20A%20novel%20framework%20PiPar%20that%20leverages%20pipeline%20parallelism%20for%0ACML%20techniques%20is%20developed%20to%20substantially%20improve%20resource%20utilization.%20A%0Anew%20training%20pipeline%20is%20designed%20to%20parallelize%20the%20computations%20on%20different%0Ahardware%20resources%20and%20communication%20on%20different%20bandwidth%20resources%2C%20thereby%0Aaccelerating%20the%20training%20process%20in%20CML.%20A%20low%20overhead%20automated%20parameter%0Aselection%20method%20is%20proposed%20to%20optimize%20the%20pipeline%2C%20maximizing%20the%0Autilization%20of%20available%20resources.%20The%20experimental%20results%20confirm%20the%0Avalidity%20of%20the%20underlying%20approach%20of%20PiPar%20and%20highlight%20that%20when%20compared%0Ato%20federated%20learning%3A%20%28i%29%20the%20idle%20time%20of%20the%20server%20can%20be%20reduced%20by%20up%20to%0A64.1x%2C%20and%20%28ii%29%20the%20overall%20training%20time%20can%20be%20accelerated%20by%20up%20to%2034.6x%0Aunder%20varying%20network%20conditions%20for%20a%20collection%20of%20six%20small%20and%20large%0Apopular%20deep%20neural%20networks%20and%20four%20datasets%20without%20sacrificing%20accuracy.%20It%0Ais%20also%20experimentally%20demonstrated%20that%20PiPar%20achieves%20performance%20benefits%0Awhen%20incorporating%20differential%20privacy%20methods%20and%20operating%20in%20environments%0Awith%20heterogeneous%20devices%20and%20changing%20bandwidths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.12803v2&entry.124074799=Read"},
{"title": "Adam-mini: Use Fewer Learning Rates To Gain More", "author": "Yushun Zhang and Congliang Chen and Ziniu Li and Tian Ding and Chenwei Wu and Yinyu Ye and Zhi-Quan Luo and Ruoyu Sun", "abstract": "  We propose Adam-mini, an optimizer that achieves on-par or better performance\nthan AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory by\ncutting down the number of learning rates in Adam: Instead of assigning an\nindividual learning rate for each parameter using $1/\\sqrt{v}$, Adam-mini uses\nthe average of $v$ within a pre-defined parameter block as the learning rate\nfor that block. Such a design is inspired by two empirical findings. First, the\nHessian of Transformers exhibits a near-block diagonal structure with different\nsizes of dense sub-blocks. Second, for each of these dense sub-blocks, there\nexists a single high-quality learning rate that can outperform Adam, provided\nthat sufficient resources are available to search it out. Adam-mini provides\none cost-effective way to find these good learning rates and manage to cut down\n$\\geq$ 90% $v$ in Adam. Empirically, we verify that Adam-mini performs on par\nor better than AdamW on various language models sized from 125M to 7B for\npre-training, supervised fine-tuning, and RLHF. The reduced memory footprint of\nAdam-mini also alleviates communication overheads among GPUs and CPUs, thereby\nincreasing throughput. For instance, Adam-mini achieves 49.6% higher throughput\nthan AdamW when pre-training Llama2-7B on 2x A800-80GB GPUs, which saves 33%\nwall-clock time for pre-training.\n", "link": "http://arxiv.org/abs/2406.16793v2", "date": "2024-06-25", "relevancy": 1.9114, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5012}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More&body=Title%3A%20Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More%0AAuthor%3A%20Yushun%20Zhang%20and%20Congliang%20Chen%20and%20Ziniu%20Li%20and%20Tian%20Ding%20and%20Chenwei%20Wu%20and%20Yinyu%20Ye%20and%20Zhi-Quan%20Luo%20and%20Ruoyu%20Sun%0AAbstract%3A%20%20%20We%20propose%20Adam-mini%2C%20an%20optimizer%20that%20achieves%20on-par%20or%20better%20performance%0Athan%20AdamW%20with%2045%25%20to%2050%25%20less%20memory%20footprint.%20Adam-mini%20reduces%20memory%20by%0Acutting%20down%20the%20number%20of%20learning%20rates%20in%20Adam%3A%20Instead%20of%20assigning%20an%0Aindividual%20learning%20rate%20for%20each%20parameter%20using%20%241/%5Csqrt%7Bv%7D%24%2C%20Adam-mini%20uses%0Athe%20average%20of%20%24v%24%20within%20a%20pre-defined%20parameter%20block%20as%20the%20learning%20rate%0Afor%20that%20block.%20Such%20a%20design%20is%20inspired%20by%20two%20empirical%20findings.%20First%2C%20the%0AHessian%20of%20Transformers%20exhibits%20a%20near-block%20diagonal%20structure%20with%20different%0Asizes%20of%20dense%20sub-blocks.%20Second%2C%20for%20each%20of%20these%20dense%20sub-blocks%2C%20there%0Aexists%20a%20single%20high-quality%20learning%20rate%20that%20can%20outperform%20Adam%2C%20provided%0Athat%20sufficient%20resources%20are%20available%20to%20search%20it%20out.%20Adam-mini%20provides%0Aone%20cost-effective%20way%20to%20find%20these%20good%20learning%20rates%20and%20manage%20to%20cut%20down%0A%24%5Cgeq%24%2090%25%20%24v%24%20in%20Adam.%20Empirically%2C%20we%20verify%20that%20Adam-mini%20performs%20on%20par%0Aor%20better%20than%20AdamW%20on%20various%20language%20models%20sized%20from%20125M%20to%207B%20for%0Apre-training%2C%20supervised%20fine-tuning%2C%20and%20RLHF.%20The%20reduced%20memory%20footprint%20of%0AAdam-mini%20also%20alleviates%20communication%20overheads%20among%20GPUs%20and%20CPUs%2C%20thereby%0Aincreasing%20throughput.%20For%20instance%2C%20Adam-mini%20achieves%2049.6%25%20higher%20throughput%0Athan%20AdamW%20when%20pre-training%20Llama2-7B%20on%202x%20A800-80GB%20GPUs%2C%20which%20saves%2033%25%0Awall-clock%20time%20for%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16793v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdam-mini%253A%2520Use%2520Fewer%2520Learning%2520Rates%2520To%2520Gain%2520More%26entry.906535625%3DYushun%2520Zhang%2520and%2520Congliang%2520Chen%2520and%2520Ziniu%2520Li%2520and%2520Tian%2520Ding%2520and%2520Chenwei%2520Wu%2520and%2520Yinyu%2520Ye%2520and%2520Zhi-Quan%2520Luo%2520and%2520Ruoyu%2520Sun%26entry.1292438233%3D%2520%2520We%2520propose%2520Adam-mini%252C%2520an%2520optimizer%2520that%2520achieves%2520on-par%2520or%2520better%2520performance%250Athan%2520AdamW%2520with%252045%2525%2520to%252050%2525%2520less%2520memory%2520footprint.%2520Adam-mini%2520reduces%2520memory%2520by%250Acutting%2520down%2520the%2520number%2520of%2520learning%2520rates%2520in%2520Adam%253A%2520Instead%2520of%2520assigning%2520an%250Aindividual%2520learning%2520rate%2520for%2520each%2520parameter%2520using%2520%25241/%255Csqrt%257Bv%257D%2524%252C%2520Adam-mini%2520uses%250Athe%2520average%2520of%2520%2524v%2524%2520within%2520a%2520pre-defined%2520parameter%2520block%2520as%2520the%2520learning%2520rate%250Afor%2520that%2520block.%2520Such%2520a%2520design%2520is%2520inspired%2520by%2520two%2520empirical%2520findings.%2520First%252C%2520the%250AHessian%2520of%2520Transformers%2520exhibits%2520a%2520near-block%2520diagonal%2520structure%2520with%2520different%250Asizes%2520of%2520dense%2520sub-blocks.%2520Second%252C%2520for%2520each%2520of%2520these%2520dense%2520sub-blocks%252C%2520there%250Aexists%2520a%2520single%2520high-quality%2520learning%2520rate%2520that%2520can%2520outperform%2520Adam%252C%2520provided%250Athat%2520sufficient%2520resources%2520are%2520available%2520to%2520search%2520it%2520out.%2520Adam-mini%2520provides%250Aone%2520cost-effective%2520way%2520to%2520find%2520these%2520good%2520learning%2520rates%2520and%2520manage%2520to%2520cut%2520down%250A%2524%255Cgeq%2524%252090%2525%2520%2524v%2524%2520in%2520Adam.%2520Empirically%252C%2520we%2520verify%2520that%2520Adam-mini%2520performs%2520on%2520par%250Aor%2520better%2520than%2520AdamW%2520on%2520various%2520language%2520models%2520sized%2520from%2520125M%2520to%25207B%2520for%250Apre-training%252C%2520supervised%2520fine-tuning%252C%2520and%2520RLHF.%2520The%2520reduced%2520memory%2520footprint%2520of%250AAdam-mini%2520also%2520alleviates%2520communication%2520overheads%2520among%2520GPUs%2520and%2520CPUs%252C%2520thereby%250Aincreasing%2520throughput.%2520For%2520instance%252C%2520Adam-mini%2520achieves%252049.6%2525%2520higher%2520throughput%250Athan%2520AdamW%2520when%2520pre-training%2520Llama2-7B%2520on%25202x%2520A800-80GB%2520GPUs%252C%2520which%2520saves%252033%2525%250Awall-clock%2520time%2520for%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16793v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More&entry.906535625=Yushun%20Zhang%20and%20Congliang%20Chen%20and%20Ziniu%20Li%20and%20Tian%20Ding%20and%20Chenwei%20Wu%20and%20Yinyu%20Ye%20and%20Zhi-Quan%20Luo%20and%20Ruoyu%20Sun&entry.1292438233=%20%20We%20propose%20Adam-mini%2C%20an%20optimizer%20that%20achieves%20on-par%20or%20better%20performance%0Athan%20AdamW%20with%2045%25%20to%2050%25%20less%20memory%20footprint.%20Adam-mini%20reduces%20memory%20by%0Acutting%20down%20the%20number%20of%20learning%20rates%20in%20Adam%3A%20Instead%20of%20assigning%20an%0Aindividual%20learning%20rate%20for%20each%20parameter%20using%20%241/%5Csqrt%7Bv%7D%24%2C%20Adam-mini%20uses%0Athe%20average%20of%20%24v%24%20within%20a%20pre-defined%20parameter%20block%20as%20the%20learning%20rate%0Afor%20that%20block.%20Such%20a%20design%20is%20inspired%20by%20two%20empirical%20findings.%20First%2C%20the%0AHessian%20of%20Transformers%20exhibits%20a%20near-block%20diagonal%20structure%20with%20different%0Asizes%20of%20dense%20sub-blocks.%20Second%2C%20for%20each%20of%20these%20dense%20sub-blocks%2C%20there%0Aexists%20a%20single%20high-quality%20learning%20rate%20that%20can%20outperform%20Adam%2C%20provided%0Athat%20sufficient%20resources%20are%20available%20to%20search%20it%20out.%20Adam-mini%20provides%0Aone%20cost-effective%20way%20to%20find%20these%20good%20learning%20rates%20and%20manage%20to%20cut%20down%0A%24%5Cgeq%24%2090%25%20%24v%24%20in%20Adam.%20Empirically%2C%20we%20verify%20that%20Adam-mini%20performs%20on%20par%0Aor%20better%20than%20AdamW%20on%20various%20language%20models%20sized%20from%20125M%20to%207B%20for%0Apre-training%2C%20supervised%20fine-tuning%2C%20and%20RLHF.%20The%20reduced%20memory%20footprint%20of%0AAdam-mini%20also%20alleviates%20communication%20overheads%20among%20GPUs%20and%20CPUs%2C%20thereby%0Aincreasing%20throughput.%20For%20instance%2C%20Adam-mini%20achieves%2049.6%25%20higher%20throughput%0Athan%20AdamW%20when%20pre-training%20Llama2-7B%20on%202x%20A800-80GB%20GPUs%2C%20which%20saves%2033%25%0Awall-clock%20time%20for%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16793v2&entry.124074799=Read"},
{"title": "Optimizing Energy-Efficient Braking Trajectories with Anticipatory Road\n  Data for Automated Vehicles", "author": "Andres Alvarez Prado and Vladislav Nenchev and Christian Rathgeber", "abstract": "  Trajectory planning in automated driving typically focuses on satisfying\nsafety and comfort requirements within the vehicle's onboard sensor range. This\npaper introduces a method that leverages anticipatory road data, such as speed\nlimits, road slopes, and traffic lights, beyond the local perception range to\noptimize energy-efficient braking trajectories. For that, coasting, which\nreduces energy consumption, and active braking are combined to transition from\nthe current vehicle velocity to a lower target velocity at a given distance\nahead. Finding the switching instants between the coasting phases and the\ncontinuous control for the braking phase is addressed as an optimal trade-off\nbetween maximizing coasting periods and minimizing braking effort. The\nresulting switched optimal control problem is solved by deriving necessary\noptimality conditions. To facilitate the incorporation of additional\nfeasibility constraints for multi-phase trajectories, a sub-optimal alternative\nsolution based on parametric optimization is proposed. Both methods are\ncompared in simulation.\n", "link": "http://arxiv.org/abs/2406.17604v1", "date": "2024-06-25", "relevancy": 1.9101, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4931}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.487}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Energy-Efficient%20Braking%20Trajectories%20with%20Anticipatory%20Road%0A%20%20Data%20for%20Automated%20Vehicles&body=Title%3A%20Optimizing%20Energy-Efficient%20Braking%20Trajectories%20with%20Anticipatory%20Road%0A%20%20Data%20for%20Automated%20Vehicles%0AAuthor%3A%20Andres%20Alvarez%20Prado%20and%20Vladislav%20Nenchev%20and%20Christian%20Rathgeber%0AAbstract%3A%20%20%20Trajectory%20planning%20in%20automated%20driving%20typically%20focuses%20on%20satisfying%0Asafety%20and%20comfort%20requirements%20within%20the%20vehicle%27s%20onboard%20sensor%20range.%20This%0Apaper%20introduces%20a%20method%20that%20leverages%20anticipatory%20road%20data%2C%20such%20as%20speed%0Alimits%2C%20road%20slopes%2C%20and%20traffic%20lights%2C%20beyond%20the%20local%20perception%20range%20to%0Aoptimize%20energy-efficient%20braking%20trajectories.%20For%20that%2C%20coasting%2C%20which%0Areduces%20energy%20consumption%2C%20and%20active%20braking%20are%20combined%20to%20transition%20from%0Athe%20current%20vehicle%20velocity%20to%20a%20lower%20target%20velocity%20at%20a%20given%20distance%0Aahead.%20Finding%20the%20switching%20instants%20between%20the%20coasting%20phases%20and%20the%0Acontinuous%20control%20for%20the%20braking%20phase%20is%20addressed%20as%20an%20optimal%20trade-off%0Abetween%20maximizing%20coasting%20periods%20and%20minimizing%20braking%20effort.%20The%0Aresulting%20switched%20optimal%20control%20problem%20is%20solved%20by%20deriving%20necessary%0Aoptimality%20conditions.%20To%20facilitate%20the%20incorporation%20of%20additional%0Afeasibility%20constraints%20for%20multi-phase%20trajectories%2C%20a%20sub-optimal%20alternative%0Asolution%20based%20on%20parametric%20optimization%20is%20proposed.%20Both%20methods%20are%0Acompared%20in%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Energy-Efficient%2520Braking%2520Trajectories%2520with%2520Anticipatory%2520Road%250A%2520%2520Data%2520for%2520Automated%2520Vehicles%26entry.906535625%3DAndres%2520Alvarez%2520Prado%2520and%2520Vladislav%2520Nenchev%2520and%2520Christian%2520Rathgeber%26entry.1292438233%3D%2520%2520Trajectory%2520planning%2520in%2520automated%2520driving%2520typically%2520focuses%2520on%2520satisfying%250Asafety%2520and%2520comfort%2520requirements%2520within%2520the%2520vehicle%2527s%2520onboard%2520sensor%2520range.%2520This%250Apaper%2520introduces%2520a%2520method%2520that%2520leverages%2520anticipatory%2520road%2520data%252C%2520such%2520as%2520speed%250Alimits%252C%2520road%2520slopes%252C%2520and%2520traffic%2520lights%252C%2520beyond%2520the%2520local%2520perception%2520range%2520to%250Aoptimize%2520energy-efficient%2520braking%2520trajectories.%2520For%2520that%252C%2520coasting%252C%2520which%250Areduces%2520energy%2520consumption%252C%2520and%2520active%2520braking%2520are%2520combined%2520to%2520transition%2520from%250Athe%2520current%2520vehicle%2520velocity%2520to%2520a%2520lower%2520target%2520velocity%2520at%2520a%2520given%2520distance%250Aahead.%2520Finding%2520the%2520switching%2520instants%2520between%2520the%2520coasting%2520phases%2520and%2520the%250Acontinuous%2520control%2520for%2520the%2520braking%2520phase%2520is%2520addressed%2520as%2520an%2520optimal%2520trade-off%250Abetween%2520maximizing%2520coasting%2520periods%2520and%2520minimizing%2520braking%2520effort.%2520The%250Aresulting%2520switched%2520optimal%2520control%2520problem%2520is%2520solved%2520by%2520deriving%2520necessary%250Aoptimality%2520conditions.%2520To%2520facilitate%2520the%2520incorporation%2520of%2520additional%250Afeasibility%2520constraints%2520for%2520multi-phase%2520trajectories%252C%2520a%2520sub-optimal%2520alternative%250Asolution%2520based%2520on%2520parametric%2520optimization%2520is%2520proposed.%2520Both%2520methods%2520are%250Acompared%2520in%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Energy-Efficient%20Braking%20Trajectories%20with%20Anticipatory%20Road%0A%20%20Data%20for%20Automated%20Vehicles&entry.906535625=Andres%20Alvarez%20Prado%20and%20Vladislav%20Nenchev%20and%20Christian%20Rathgeber&entry.1292438233=%20%20Trajectory%20planning%20in%20automated%20driving%20typically%20focuses%20on%20satisfying%0Asafety%20and%20comfort%20requirements%20within%20the%20vehicle%27s%20onboard%20sensor%20range.%20This%0Apaper%20introduces%20a%20method%20that%20leverages%20anticipatory%20road%20data%2C%20such%20as%20speed%0Alimits%2C%20road%20slopes%2C%20and%20traffic%20lights%2C%20beyond%20the%20local%20perception%20range%20to%0Aoptimize%20energy-efficient%20braking%20trajectories.%20For%20that%2C%20coasting%2C%20which%0Areduces%20energy%20consumption%2C%20and%20active%20braking%20are%20combined%20to%20transition%20from%0Athe%20current%20vehicle%20velocity%20to%20a%20lower%20target%20velocity%20at%20a%20given%20distance%0Aahead.%20Finding%20the%20switching%20instants%20between%20the%20coasting%20phases%20and%20the%0Acontinuous%20control%20for%20the%20braking%20phase%20is%20addressed%20as%20an%20optimal%20trade-off%0Abetween%20maximizing%20coasting%20periods%20and%20minimizing%20braking%20effort.%20The%0Aresulting%20switched%20optimal%20control%20problem%20is%20solved%20by%20deriving%20necessary%0Aoptimality%20conditions.%20To%20facilitate%20the%20incorporation%20of%20additional%0Afeasibility%20constraints%20for%20multi-phase%20trajectories%2C%20a%20sub-optimal%20alternative%0Asolution%20based%20on%20parametric%20optimization%20is%20proposed.%20Both%20methods%20are%0Acompared%20in%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17604v1&entry.124074799=Read"},
{"title": "FSBI: Deepfakes Detection with Frequency Enhanced Self-Blended Images", "author": "Ahmed Abul Hasanaath and Hamzah Luqman and Raed Katib and Saeed Anwar", "abstract": "  Advances in deepfake research have led to the creation of almost perfect\nmanipulations undetectable by human eyes and some deepfakes detection tools.\nRecently, several techniques have been proposed to differentiate deepfakes from\nrealistic images and videos. This paper introduces a Frequency Enhanced\nSelf-Blended Images (FSBI) approach for deepfakes detection. This proposed\napproach utilizes Discrete Wavelet Transforms (DWT) to extract discriminative\nfeatures from the self-blended images (SBI) to be used for training a\nconvolutional network architecture model. The SBIs blend the image with itself\nby introducing several forgery artifacts in a copy of the image before blending\nit. This prevents the classifier from overfitting specific artifacts by\nlearning more generic representations. These blended images are then fed into\nthe frequency features extractor to detect artifacts that can not be detected\neasily in the time domain. The proposed approach has been evaluated on FF++ and\nCeleb-DF datasets and the obtained results outperformed the state-of-the-art\ntechniques with the cross-dataset evaluation protocol.\n", "link": "http://arxiv.org/abs/2406.08625v2", "date": "2024-06-25", "relevancy": 1.9065, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4832}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.472}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FSBI%3A%20Deepfakes%20Detection%20with%20Frequency%20Enhanced%20Self-Blended%20Images&body=Title%3A%20FSBI%3A%20Deepfakes%20Detection%20with%20Frequency%20Enhanced%20Self-Blended%20Images%0AAuthor%3A%20Ahmed%20Abul%20Hasanaath%20and%20Hamzah%20Luqman%20and%20Raed%20Katib%20and%20Saeed%20Anwar%0AAbstract%3A%20%20%20Advances%20in%20deepfake%20research%20have%20led%20to%20the%20creation%20of%20almost%20perfect%0Amanipulations%20undetectable%20by%20human%20eyes%20and%20some%20deepfakes%20detection%20tools.%0ARecently%2C%20several%20techniques%20have%20been%20proposed%20to%20differentiate%20deepfakes%20from%0Arealistic%20images%20and%20videos.%20This%20paper%20introduces%20a%20Frequency%20Enhanced%0ASelf-Blended%20Images%20%28FSBI%29%20approach%20for%20deepfakes%20detection.%20This%20proposed%0Aapproach%20utilizes%20Discrete%20Wavelet%20Transforms%20%28DWT%29%20to%20extract%20discriminative%0Afeatures%20from%20the%20self-blended%20images%20%28SBI%29%20to%20be%20used%20for%20training%20a%0Aconvolutional%20network%20architecture%20model.%20The%20SBIs%20blend%20the%20image%20with%20itself%0Aby%20introducing%20several%20forgery%20artifacts%20in%20a%20copy%20of%20the%20image%20before%20blending%0Ait.%20This%20prevents%20the%20classifier%20from%20overfitting%20specific%20artifacts%20by%0Alearning%20more%20generic%20representations.%20These%20blended%20images%20are%20then%20fed%20into%0Athe%20frequency%20features%20extractor%20to%20detect%20artifacts%20that%20can%20not%20be%20detected%0Aeasily%20in%20the%20time%20domain.%20The%20proposed%20approach%20has%20been%20evaluated%20on%20FF%2B%2B%20and%0ACeleb-DF%20datasets%20and%20the%20obtained%20results%20outperformed%20the%20state-of-the-art%0Atechniques%20with%20the%20cross-dataset%20evaluation%20protocol.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08625v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFSBI%253A%2520Deepfakes%2520Detection%2520with%2520Frequency%2520Enhanced%2520Self-Blended%2520Images%26entry.906535625%3DAhmed%2520Abul%2520Hasanaath%2520and%2520Hamzah%2520Luqman%2520and%2520Raed%2520Katib%2520and%2520Saeed%2520Anwar%26entry.1292438233%3D%2520%2520Advances%2520in%2520deepfake%2520research%2520have%2520led%2520to%2520the%2520creation%2520of%2520almost%2520perfect%250Amanipulations%2520undetectable%2520by%2520human%2520eyes%2520and%2520some%2520deepfakes%2520detection%2520tools.%250ARecently%252C%2520several%2520techniques%2520have%2520been%2520proposed%2520to%2520differentiate%2520deepfakes%2520from%250Arealistic%2520images%2520and%2520videos.%2520This%2520paper%2520introduces%2520a%2520Frequency%2520Enhanced%250ASelf-Blended%2520Images%2520%2528FSBI%2529%2520approach%2520for%2520deepfakes%2520detection.%2520This%2520proposed%250Aapproach%2520utilizes%2520Discrete%2520Wavelet%2520Transforms%2520%2528DWT%2529%2520to%2520extract%2520discriminative%250Afeatures%2520from%2520the%2520self-blended%2520images%2520%2528SBI%2529%2520to%2520be%2520used%2520for%2520training%2520a%250Aconvolutional%2520network%2520architecture%2520model.%2520The%2520SBIs%2520blend%2520the%2520image%2520with%2520itself%250Aby%2520introducing%2520several%2520forgery%2520artifacts%2520in%2520a%2520copy%2520of%2520the%2520image%2520before%2520blending%250Ait.%2520This%2520prevents%2520the%2520classifier%2520from%2520overfitting%2520specific%2520artifacts%2520by%250Alearning%2520more%2520generic%2520representations.%2520These%2520blended%2520images%2520are%2520then%2520fed%2520into%250Athe%2520frequency%2520features%2520extractor%2520to%2520detect%2520artifacts%2520that%2520can%2520not%2520be%2520detected%250Aeasily%2520in%2520the%2520time%2520domain.%2520The%2520proposed%2520approach%2520has%2520been%2520evaluated%2520on%2520FF%252B%252B%2520and%250ACeleb-DF%2520datasets%2520and%2520the%2520obtained%2520results%2520outperformed%2520the%2520state-of-the-art%250Atechniques%2520with%2520the%2520cross-dataset%2520evaluation%2520protocol.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08625v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FSBI%3A%20Deepfakes%20Detection%20with%20Frequency%20Enhanced%20Self-Blended%20Images&entry.906535625=Ahmed%20Abul%20Hasanaath%20and%20Hamzah%20Luqman%20and%20Raed%20Katib%20and%20Saeed%20Anwar&entry.1292438233=%20%20Advances%20in%20deepfake%20research%20have%20led%20to%20the%20creation%20of%20almost%20perfect%0Amanipulations%20undetectable%20by%20human%20eyes%20and%20some%20deepfakes%20detection%20tools.%0ARecently%2C%20several%20techniques%20have%20been%20proposed%20to%20differentiate%20deepfakes%20from%0Arealistic%20images%20and%20videos.%20This%20paper%20introduces%20a%20Frequency%20Enhanced%0ASelf-Blended%20Images%20%28FSBI%29%20approach%20for%20deepfakes%20detection.%20This%20proposed%0Aapproach%20utilizes%20Discrete%20Wavelet%20Transforms%20%28DWT%29%20to%20extract%20discriminative%0Afeatures%20from%20the%20self-blended%20images%20%28SBI%29%20to%20be%20used%20for%20training%20a%0Aconvolutional%20network%20architecture%20model.%20The%20SBIs%20blend%20the%20image%20with%20itself%0Aby%20introducing%20several%20forgery%20artifacts%20in%20a%20copy%20of%20the%20image%20before%20blending%0Ait.%20This%20prevents%20the%20classifier%20from%20overfitting%20specific%20artifacts%20by%0Alearning%20more%20generic%20representations.%20These%20blended%20images%20are%20then%20fed%20into%0Athe%20frequency%20features%20extractor%20to%20detect%20artifacts%20that%20can%20not%20be%20detected%0Aeasily%20in%20the%20time%20domain.%20The%20proposed%20approach%20has%20been%20evaluated%20on%20FF%2B%2B%20and%0ACeleb-DF%20datasets%20and%20the%20obtained%20results%20outperformed%20the%20state-of-the-art%0Atechniques%20with%20the%20cross-dataset%20evaluation%20protocol.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08625v2&entry.124074799=Read"},
{"title": "Locally Differentially Private Distributed Online Learning with\n  Guaranteed Optimality", "author": "Ziqin Chen and Yongqiang Wang", "abstract": "  Distributed online learning is gaining increased traction due to its unique\nability to process large-scale datasets and streaming data. To address the\ngrowing public awareness and concern on privacy protection, plenty of\nalgorithms have been proposed to enable differential privacy in distributed\nonline optimization and learning. However, these algorithms often face the\ndilemma of trading learning accuracy for privacy. By exploiting the unique\ncharacteristics of online learning, this paper proposes an approach that\ntackles the dilemma and ensures both differential privacy and learning accuracy\nin distributed online learning. More specifically, while ensuring a diminishing\nexpected instantaneous regret, the approach can simultaneously ensure a finite\ncumulative privacy budget, even in the infinite time horizon. To cater for the\nfully distributed setting, we adopt the local differential-privacy framework,\nwhich avoids the reliance on a trusted data curator, and, hence, provides\nstronger protection than the classic \"centralized\" (global) differential\nprivacy. To the best of our knowledge, this is the first algorithm that\nsuccessfully ensures both rigorous local differential privacy and learning\naccuracy. The effectiveness of the proposed algorithm is evaluated using\nmachine learning tasks, including logistic regression on the the \"mushrooms\"\ndatasets and CNN-based image classification on the \"MNIST\" and \"CIFAR-10\"\ndatasets.\n", "link": "http://arxiv.org/abs/2306.14094v2", "date": "2024-06-25", "relevancy": 1.9064, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4842}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4721}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Differentially%20Private%20Distributed%20Online%20Learning%20with%0A%20%20Guaranteed%20Optimality&body=Title%3A%20Locally%20Differentially%20Private%20Distributed%20Online%20Learning%20with%0A%20%20Guaranteed%20Optimality%0AAuthor%3A%20Ziqin%20Chen%20and%20Yongqiang%20Wang%0AAbstract%3A%20%20%20Distributed%20online%20learning%20is%20gaining%20increased%20traction%20due%20to%20its%20unique%0Aability%20to%20process%20large-scale%20datasets%20and%20streaming%20data.%20To%20address%20the%0Agrowing%20public%20awareness%20and%20concern%20on%20privacy%20protection%2C%20plenty%20of%0Aalgorithms%20have%20been%20proposed%20to%20enable%20differential%20privacy%20in%20distributed%0Aonline%20optimization%20and%20learning.%20However%2C%20these%20algorithms%20often%20face%20the%0Adilemma%20of%20trading%20learning%20accuracy%20for%20privacy.%20By%20exploiting%20the%20unique%0Acharacteristics%20of%20online%20learning%2C%20this%20paper%20proposes%20an%20approach%20that%0Atackles%20the%20dilemma%20and%20ensures%20both%20differential%20privacy%20and%20learning%20accuracy%0Ain%20distributed%20online%20learning.%20More%20specifically%2C%20while%20ensuring%20a%20diminishing%0Aexpected%20instantaneous%20regret%2C%20the%20approach%20can%20simultaneously%20ensure%20a%20finite%0Acumulative%20privacy%20budget%2C%20even%20in%20the%20infinite%20time%20horizon.%20To%20cater%20for%20the%0Afully%20distributed%20setting%2C%20we%20adopt%20the%20local%20differential-privacy%20framework%2C%0Awhich%20avoids%20the%20reliance%20on%20a%20trusted%20data%20curator%2C%20and%2C%20hence%2C%20provides%0Astronger%20protection%20than%20the%20classic%20%22centralized%22%20%28global%29%20differential%0Aprivacy.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20algorithm%20that%0Asuccessfully%20ensures%20both%20rigorous%20local%20differential%20privacy%20and%20learning%0Aaccuracy.%20The%20effectiveness%20of%20the%20proposed%20algorithm%20is%20evaluated%20using%0Amachine%20learning%20tasks%2C%20including%20logistic%20regression%20on%20the%20the%20%22mushrooms%22%0Adatasets%20and%20CNN-based%20image%20classification%20on%20the%20%22MNIST%22%20and%20%22CIFAR-10%22%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.14094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Differentially%2520Private%2520Distributed%2520Online%2520Learning%2520with%250A%2520%2520Guaranteed%2520Optimality%26entry.906535625%3DZiqin%2520Chen%2520and%2520Yongqiang%2520Wang%26entry.1292438233%3D%2520%2520Distributed%2520online%2520learning%2520is%2520gaining%2520increased%2520traction%2520due%2520to%2520its%2520unique%250Aability%2520to%2520process%2520large-scale%2520datasets%2520and%2520streaming%2520data.%2520To%2520address%2520the%250Agrowing%2520public%2520awareness%2520and%2520concern%2520on%2520privacy%2520protection%252C%2520plenty%2520of%250Aalgorithms%2520have%2520been%2520proposed%2520to%2520enable%2520differential%2520privacy%2520in%2520distributed%250Aonline%2520optimization%2520and%2520learning.%2520However%252C%2520these%2520algorithms%2520often%2520face%2520the%250Adilemma%2520of%2520trading%2520learning%2520accuracy%2520for%2520privacy.%2520By%2520exploiting%2520the%2520unique%250Acharacteristics%2520of%2520online%2520learning%252C%2520this%2520paper%2520proposes%2520an%2520approach%2520that%250Atackles%2520the%2520dilemma%2520and%2520ensures%2520both%2520differential%2520privacy%2520and%2520learning%2520accuracy%250Ain%2520distributed%2520online%2520learning.%2520More%2520specifically%252C%2520while%2520ensuring%2520a%2520diminishing%250Aexpected%2520instantaneous%2520regret%252C%2520the%2520approach%2520can%2520simultaneously%2520ensure%2520a%2520finite%250Acumulative%2520privacy%2520budget%252C%2520even%2520in%2520the%2520infinite%2520time%2520horizon.%2520To%2520cater%2520for%2520the%250Afully%2520distributed%2520setting%252C%2520we%2520adopt%2520the%2520local%2520differential-privacy%2520framework%252C%250Awhich%2520avoids%2520the%2520reliance%2520on%2520a%2520trusted%2520data%2520curator%252C%2520and%252C%2520hence%252C%2520provides%250Astronger%2520protection%2520than%2520the%2520classic%2520%2522centralized%2522%2520%2528global%2529%2520differential%250Aprivacy.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520algorithm%2520that%250Asuccessfully%2520ensures%2520both%2520rigorous%2520local%2520differential%2520privacy%2520and%2520learning%250Aaccuracy.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520algorithm%2520is%2520evaluated%2520using%250Amachine%2520learning%2520tasks%252C%2520including%2520logistic%2520regression%2520on%2520the%2520the%2520%2522mushrooms%2522%250Adatasets%2520and%2520CNN-based%2520image%2520classification%2520on%2520the%2520%2522MNIST%2522%2520and%2520%2522CIFAR-10%2522%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.14094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Differentially%20Private%20Distributed%20Online%20Learning%20with%0A%20%20Guaranteed%20Optimality&entry.906535625=Ziqin%20Chen%20and%20Yongqiang%20Wang&entry.1292438233=%20%20Distributed%20online%20learning%20is%20gaining%20increased%20traction%20due%20to%20its%20unique%0Aability%20to%20process%20large-scale%20datasets%20and%20streaming%20data.%20To%20address%20the%0Agrowing%20public%20awareness%20and%20concern%20on%20privacy%20protection%2C%20plenty%20of%0Aalgorithms%20have%20been%20proposed%20to%20enable%20differential%20privacy%20in%20distributed%0Aonline%20optimization%20and%20learning.%20However%2C%20these%20algorithms%20often%20face%20the%0Adilemma%20of%20trading%20learning%20accuracy%20for%20privacy.%20By%20exploiting%20the%20unique%0Acharacteristics%20of%20online%20learning%2C%20this%20paper%20proposes%20an%20approach%20that%0Atackles%20the%20dilemma%20and%20ensures%20both%20differential%20privacy%20and%20learning%20accuracy%0Ain%20distributed%20online%20learning.%20More%20specifically%2C%20while%20ensuring%20a%20diminishing%0Aexpected%20instantaneous%20regret%2C%20the%20approach%20can%20simultaneously%20ensure%20a%20finite%0Acumulative%20privacy%20budget%2C%20even%20in%20the%20infinite%20time%20horizon.%20To%20cater%20for%20the%0Afully%20distributed%20setting%2C%20we%20adopt%20the%20local%20differential-privacy%20framework%2C%0Awhich%20avoids%20the%20reliance%20on%20a%20trusted%20data%20curator%2C%20and%2C%20hence%2C%20provides%0Astronger%20protection%20than%20the%20classic%20%22centralized%22%20%28global%29%20differential%0Aprivacy.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20algorithm%20that%0Asuccessfully%20ensures%20both%20rigorous%20local%20differential%20privacy%20and%20learning%0Aaccuracy.%20The%20effectiveness%20of%20the%20proposed%20algorithm%20is%20evaluated%20using%0Amachine%20learning%20tasks%2C%20including%20logistic%20regression%20on%20the%20the%20%22mushrooms%22%0Adatasets%20and%20CNN-based%20image%20classification%20on%20the%20%22MNIST%22%20and%20%22CIFAR-10%22%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14094v2&entry.124074799=Read"},
{"title": "MgNO: Efficient Parameterization of Linear Operators via Multigrid", "author": "Juncai He and Xinliang Liu and Jinchao Xu", "abstract": "  In this work, we propose a concise neural operator architecture for operator\nlearning. Drawing an analogy with a conventional fully connected neural\nnetwork, we define the neural operator as follows: the output of the $i$-th\nneuron in a nonlinear operator layer is defined by $\\mathcal O_i(u) =\n\\sigma\\left( \\sum_j \\mathcal W_{ij} u + \\mathcal B_{ij}\\right)$. Here,\n$\\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input\nneuron to $i$-th output neuron, and the bias $\\mathcal B_{ij}$ takes the form\nof a function rather than a scalar. Given its new universal approximation\nproperty, the efficient parameterization of the bounded linear operators\nbetween two neurons (Banach spaces) plays a critical role. As a result, we\nintroduce MgNO, utilizing multigrid structures to parameterize these linear\noperators between neurons. This approach offers both mathematical rigor and\npractical expressivity. Additionally, MgNO obviates the need for conventional\nlifting and projecting operators typically required in previous neural\noperators. Moreover, it seamlessly accommodates diverse boundary conditions.\nOur empirical observations reveal that MgNO exhibits superior ease of training\ncompared to other CNN-based models, while also displaying a reduced\nsusceptibility to overfitting when contrasted with spectral-type neural\noperators. We demonstrate the efficiency and accuracy of our method with\nconsistently state-of-the-art performance on different types of partial\ndifferential equations (PDEs).\n", "link": "http://arxiv.org/abs/2310.19809v2", "date": "2024-06-25", "relevancy": 1.906, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4935}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4655}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MgNO%3A%20Efficient%20Parameterization%20of%20Linear%20Operators%20via%20Multigrid&body=Title%3A%20MgNO%3A%20Efficient%20Parameterization%20of%20Linear%20Operators%20via%20Multigrid%0AAuthor%3A%20Juncai%20He%20and%20Xinliang%20Liu%20and%20Jinchao%20Xu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20concise%20neural%20operator%20architecture%20for%20operator%0Alearning.%20Drawing%20an%20analogy%20with%20a%20conventional%20fully%20connected%20neural%0Anetwork%2C%20we%20define%20the%20neural%20operator%20as%20follows%3A%20the%20output%20of%20the%20%24i%24-th%0Aneuron%20in%20a%20nonlinear%20operator%20layer%20is%20defined%20by%20%24%5Cmathcal%20O_i%28u%29%20%3D%0A%5Csigma%5Cleft%28%20%5Csum_j%20%5Cmathcal%20W_%7Bij%7D%20u%20%2B%20%5Cmathcal%20B_%7Bij%7D%5Cright%29%24.%20Here%2C%0A%24%5Cmathcal%20W_%7Bij%7D%24%20denotes%20the%20bounded%20linear%20operator%20connecting%20%24j%24-th%20input%0Aneuron%20to%20%24i%24-th%20output%20neuron%2C%20and%20the%20bias%20%24%5Cmathcal%20B_%7Bij%7D%24%20takes%20the%20form%0Aof%20a%20function%20rather%20than%20a%20scalar.%20Given%20its%20new%20universal%20approximation%0Aproperty%2C%20the%20efficient%20parameterization%20of%20the%20bounded%20linear%20operators%0Abetween%20two%20neurons%20%28Banach%20spaces%29%20plays%20a%20critical%20role.%20As%20a%20result%2C%20we%0Aintroduce%20MgNO%2C%20utilizing%20multigrid%20structures%20to%20parameterize%20these%20linear%0Aoperators%20between%20neurons.%20This%20approach%20offers%20both%20mathematical%20rigor%20and%0Apractical%20expressivity.%20Additionally%2C%20MgNO%20obviates%20the%20need%20for%20conventional%0Alifting%20and%20projecting%20operators%20typically%20required%20in%20previous%20neural%0Aoperators.%20Moreover%2C%20it%20seamlessly%20accommodates%20diverse%20boundary%20conditions.%0AOur%20empirical%20observations%20reveal%20that%20MgNO%20exhibits%20superior%20ease%20of%20training%0Acompared%20to%20other%20CNN-based%20models%2C%20while%20also%20displaying%20a%20reduced%0Asusceptibility%20to%20overfitting%20when%20contrasted%20with%20spectral-type%20neural%0Aoperators.%20We%20demonstrate%20the%20efficiency%20and%20accuracy%20of%20our%20method%20with%0Aconsistently%20state-of-the-art%20performance%20on%20different%20types%20of%20partial%0Adifferential%20equations%20%28PDEs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMgNO%253A%2520Efficient%2520Parameterization%2520of%2520Linear%2520Operators%2520via%2520Multigrid%26entry.906535625%3DJuncai%2520He%2520and%2520Xinliang%2520Liu%2520and%2520Jinchao%2520Xu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520concise%2520neural%2520operator%2520architecture%2520for%2520operator%250Alearning.%2520Drawing%2520an%2520analogy%2520with%2520a%2520conventional%2520fully%2520connected%2520neural%250Anetwork%252C%2520we%2520define%2520the%2520neural%2520operator%2520as%2520follows%253A%2520the%2520output%2520of%2520the%2520%2524i%2524-th%250Aneuron%2520in%2520a%2520nonlinear%2520operator%2520layer%2520is%2520defined%2520by%2520%2524%255Cmathcal%2520O_i%2528u%2529%2520%253D%250A%255Csigma%255Cleft%2528%2520%255Csum_j%2520%255Cmathcal%2520W_%257Bij%257D%2520u%2520%252B%2520%255Cmathcal%2520B_%257Bij%257D%255Cright%2529%2524.%2520Here%252C%250A%2524%255Cmathcal%2520W_%257Bij%257D%2524%2520denotes%2520the%2520bounded%2520linear%2520operator%2520connecting%2520%2524j%2524-th%2520input%250Aneuron%2520to%2520%2524i%2524-th%2520output%2520neuron%252C%2520and%2520the%2520bias%2520%2524%255Cmathcal%2520B_%257Bij%257D%2524%2520takes%2520the%2520form%250Aof%2520a%2520function%2520rather%2520than%2520a%2520scalar.%2520Given%2520its%2520new%2520universal%2520approximation%250Aproperty%252C%2520the%2520efficient%2520parameterization%2520of%2520the%2520bounded%2520linear%2520operators%250Abetween%2520two%2520neurons%2520%2528Banach%2520spaces%2529%2520plays%2520a%2520critical%2520role.%2520As%2520a%2520result%252C%2520we%250Aintroduce%2520MgNO%252C%2520utilizing%2520multigrid%2520structures%2520to%2520parameterize%2520these%2520linear%250Aoperators%2520between%2520neurons.%2520This%2520approach%2520offers%2520both%2520mathematical%2520rigor%2520and%250Apractical%2520expressivity.%2520Additionally%252C%2520MgNO%2520obviates%2520the%2520need%2520for%2520conventional%250Alifting%2520and%2520projecting%2520operators%2520typically%2520required%2520in%2520previous%2520neural%250Aoperators.%2520Moreover%252C%2520it%2520seamlessly%2520accommodates%2520diverse%2520boundary%2520conditions.%250AOur%2520empirical%2520observations%2520reveal%2520that%2520MgNO%2520exhibits%2520superior%2520ease%2520of%2520training%250Acompared%2520to%2520other%2520CNN-based%2520models%252C%2520while%2520also%2520displaying%2520a%2520reduced%250Asusceptibility%2520to%2520overfitting%2520when%2520contrasted%2520with%2520spectral-type%2520neural%250Aoperators.%2520We%2520demonstrate%2520the%2520efficiency%2520and%2520accuracy%2520of%2520our%2520method%2520with%250Aconsistently%2520state-of-the-art%2520performance%2520on%2520different%2520types%2520of%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MgNO%3A%20Efficient%20Parameterization%20of%20Linear%20Operators%20via%20Multigrid&entry.906535625=Juncai%20He%20and%20Xinliang%20Liu%20and%20Jinchao%20Xu&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20concise%20neural%20operator%20architecture%20for%20operator%0Alearning.%20Drawing%20an%20analogy%20with%20a%20conventional%20fully%20connected%20neural%0Anetwork%2C%20we%20define%20the%20neural%20operator%20as%20follows%3A%20the%20output%20of%20the%20%24i%24-th%0Aneuron%20in%20a%20nonlinear%20operator%20layer%20is%20defined%20by%20%24%5Cmathcal%20O_i%28u%29%20%3D%0A%5Csigma%5Cleft%28%20%5Csum_j%20%5Cmathcal%20W_%7Bij%7D%20u%20%2B%20%5Cmathcal%20B_%7Bij%7D%5Cright%29%24.%20Here%2C%0A%24%5Cmathcal%20W_%7Bij%7D%24%20denotes%20the%20bounded%20linear%20operator%20connecting%20%24j%24-th%20input%0Aneuron%20to%20%24i%24-th%20output%20neuron%2C%20and%20the%20bias%20%24%5Cmathcal%20B_%7Bij%7D%24%20takes%20the%20form%0Aof%20a%20function%20rather%20than%20a%20scalar.%20Given%20its%20new%20universal%20approximation%0Aproperty%2C%20the%20efficient%20parameterization%20of%20the%20bounded%20linear%20operators%0Abetween%20two%20neurons%20%28Banach%20spaces%29%20plays%20a%20critical%20role.%20As%20a%20result%2C%20we%0Aintroduce%20MgNO%2C%20utilizing%20multigrid%20structures%20to%20parameterize%20these%20linear%0Aoperators%20between%20neurons.%20This%20approach%20offers%20both%20mathematical%20rigor%20and%0Apractical%20expressivity.%20Additionally%2C%20MgNO%20obviates%20the%20need%20for%20conventional%0Alifting%20and%20projecting%20operators%20typically%20required%20in%20previous%20neural%0Aoperators.%20Moreover%2C%20it%20seamlessly%20accommodates%20diverse%20boundary%20conditions.%0AOur%20empirical%20observations%20reveal%20that%20MgNO%20exhibits%20superior%20ease%20of%20training%0Acompared%20to%20other%20CNN-based%20models%2C%20while%20also%20displaying%20a%20reduced%0Asusceptibility%20to%20overfitting%20when%20contrasted%20with%20spectral-type%20neural%0Aoperators.%20We%20demonstrate%20the%20efficiency%20and%20accuracy%20of%20our%20method%20with%0Aconsistently%20state-of-the-art%20performance%20on%20different%20types%20of%20partial%0Adifferential%20equations%20%28PDEs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19809v2&entry.124074799=Read"},
{"title": "High-Dimension Human Value Representation in Large Language Models", "author": "Samuel Cahyawijaya and Delong Chen and Yejin Bang and Leila Khalatbari and Bryan Wilie and Ziwei Ji and Etsuko Ishii and Pascale Fung", "abstract": "  The widespread application of Large Language Models (LLMs) across various\ntasks and fields has necessitated the alignment of these models with human\nvalues and preferences. Given various approaches of human value alignment,\nranging from Reinforcement Learning with Human Feedback (RLHF), to\nconstitutional learning, etc. there is an urgent need to understand the scope\nand nature of human values injected into these models before their release.\nThere is also a need for model alignment without a costly large scale human\nannotation effort. We propose UniVaR, a high-dimensional representation of\nhuman value distributions in LLMs, orthogonal to model architecture and\ntraining data. Trained from the value-relevant output of eight multilingual\nLLMs and tested on the output from four multilingual LLMs, namely LlaMA2,\nChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the\ndistribution of human values embedded in different LLMs with different langauge\nsources. Through UniVaR, we explore how different LLMs prioritize various\nvalues in different languages and cultures, shedding light on the complex\ninterplay between human values and language modeling.\n", "link": "http://arxiv.org/abs/2404.07900v2", "date": "2024-06-25", "relevancy": 1.8634, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4923}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Dimension%20Human%20Value%20Representation%20in%20Large%20Language%20Models&body=Title%3A%20High-Dimension%20Human%20Value%20Representation%20in%20Large%20Language%20Models%0AAuthor%3A%20Samuel%20Cahyawijaya%20and%20Delong%20Chen%20and%20Yejin%20Bang%20and%20Leila%20Khalatbari%20and%20Bryan%20Wilie%20and%20Ziwei%20Ji%20and%20Etsuko%20Ishii%20and%20Pascale%20Fung%0AAbstract%3A%20%20%20The%20widespread%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%0Atasks%20and%20fields%20has%20necessitated%20the%20alignment%20of%20these%20models%20with%20human%0Avalues%20and%20preferences.%20Given%20various%20approaches%20of%20human%20value%20alignment%2C%0Aranging%20from%20Reinforcement%20Learning%20with%20Human%20Feedback%20%28RLHF%29%2C%20to%0Aconstitutional%20learning%2C%20etc.%20there%20is%20an%20urgent%20need%20to%20understand%20the%20scope%0Aand%20nature%20of%20human%20values%20injected%20into%20these%20models%20before%20their%20release.%0AThere%20is%20also%20a%20need%20for%20model%20alignment%20without%20a%20costly%20large%20scale%20human%0Aannotation%20effort.%20We%20propose%20UniVaR%2C%20a%20high-dimensional%20representation%20of%0Ahuman%20value%20distributions%20in%20LLMs%2C%20orthogonal%20to%20model%20architecture%20and%0Atraining%20data.%20Trained%20from%20the%20value-relevant%20output%20of%20eight%20multilingual%0ALLMs%20and%20tested%20on%20the%20output%20from%20four%20multilingual%20LLMs%2C%20namely%20LlaMA2%2C%0AChatGPT%2C%20JAIS%20and%20Yi%2C%20we%20show%20that%20UniVaR%20is%20a%20powerful%20tool%20to%20compare%20the%0Adistribution%20of%20human%20values%20embedded%20in%20different%20LLMs%20with%20different%20langauge%0Asources.%20Through%20UniVaR%2C%20we%20explore%20how%20different%20LLMs%20prioritize%20various%0Avalues%20in%20different%20languages%20and%20cultures%2C%20shedding%20light%20on%20the%20complex%0Ainterplay%20between%20human%20values%20and%20language%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07900v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Dimension%2520Human%2520Value%2520Representation%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DSamuel%2520Cahyawijaya%2520and%2520Delong%2520Chen%2520and%2520Yejin%2520Bang%2520and%2520Leila%2520Khalatbari%2520and%2520Bryan%2520Wilie%2520and%2520Ziwei%2520Ji%2520and%2520Etsuko%2520Ishii%2520and%2520Pascale%2520Fung%26entry.1292438233%3D%2520%2520The%2520widespread%2520application%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520across%2520various%250Atasks%2520and%2520fields%2520has%2520necessitated%2520the%2520alignment%2520of%2520these%2520models%2520with%2520human%250Avalues%2520and%2520preferences.%2520Given%2520various%2520approaches%2520of%2520human%2520value%2520alignment%252C%250Aranging%2520from%2520Reinforcement%2520Learning%2520with%2520Human%2520Feedback%2520%2528RLHF%2529%252C%2520to%250Aconstitutional%2520learning%252C%2520etc.%2520there%2520is%2520an%2520urgent%2520need%2520to%2520understand%2520the%2520scope%250Aand%2520nature%2520of%2520human%2520values%2520injected%2520into%2520these%2520models%2520before%2520their%2520release.%250AThere%2520is%2520also%2520a%2520need%2520for%2520model%2520alignment%2520without%2520a%2520costly%2520large%2520scale%2520human%250Aannotation%2520effort.%2520We%2520propose%2520UniVaR%252C%2520a%2520high-dimensional%2520representation%2520of%250Ahuman%2520value%2520distributions%2520in%2520LLMs%252C%2520orthogonal%2520to%2520model%2520architecture%2520and%250Atraining%2520data.%2520Trained%2520from%2520the%2520value-relevant%2520output%2520of%2520eight%2520multilingual%250ALLMs%2520and%2520tested%2520on%2520the%2520output%2520from%2520four%2520multilingual%2520LLMs%252C%2520namely%2520LlaMA2%252C%250AChatGPT%252C%2520JAIS%2520and%2520Yi%252C%2520we%2520show%2520that%2520UniVaR%2520is%2520a%2520powerful%2520tool%2520to%2520compare%2520the%250Adistribution%2520of%2520human%2520values%2520embedded%2520in%2520different%2520LLMs%2520with%2520different%2520langauge%250Asources.%2520Through%2520UniVaR%252C%2520we%2520explore%2520how%2520different%2520LLMs%2520prioritize%2520various%250Avalues%2520in%2520different%2520languages%2520and%2520cultures%252C%2520shedding%2520light%2520on%2520the%2520complex%250Ainterplay%2520between%2520human%2520values%2520and%2520language%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07900v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Dimension%20Human%20Value%20Representation%20in%20Large%20Language%20Models&entry.906535625=Samuel%20Cahyawijaya%20and%20Delong%20Chen%20and%20Yejin%20Bang%20and%20Leila%20Khalatbari%20and%20Bryan%20Wilie%20and%20Ziwei%20Ji%20and%20Etsuko%20Ishii%20and%20Pascale%20Fung&entry.1292438233=%20%20The%20widespread%20application%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%0Atasks%20and%20fields%20has%20necessitated%20the%20alignment%20of%20these%20models%20with%20human%0Avalues%20and%20preferences.%20Given%20various%20approaches%20of%20human%20value%20alignment%2C%0Aranging%20from%20Reinforcement%20Learning%20with%20Human%20Feedback%20%28RLHF%29%2C%20to%0Aconstitutional%20learning%2C%20etc.%20there%20is%20an%20urgent%20need%20to%20understand%20the%20scope%0Aand%20nature%20of%20human%20values%20injected%20into%20these%20models%20before%20their%20release.%0AThere%20is%20also%20a%20need%20for%20model%20alignment%20without%20a%20costly%20large%20scale%20human%0Aannotation%20effort.%20We%20propose%20UniVaR%2C%20a%20high-dimensional%20representation%20of%0Ahuman%20value%20distributions%20in%20LLMs%2C%20orthogonal%20to%20model%20architecture%20and%0Atraining%20data.%20Trained%20from%20the%20value-relevant%20output%20of%20eight%20multilingual%0ALLMs%20and%20tested%20on%20the%20output%20from%20four%20multilingual%20LLMs%2C%20namely%20LlaMA2%2C%0AChatGPT%2C%20JAIS%20and%20Yi%2C%20we%20show%20that%20UniVaR%20is%20a%20powerful%20tool%20to%20compare%20the%0Adistribution%20of%20human%20values%20embedded%20in%20different%20LLMs%20with%20different%20langauge%0Asources.%20Through%20UniVaR%2C%20we%20explore%20how%20different%20LLMs%20prioritize%20various%0Avalues%20in%20different%20languages%20and%20cultures%2C%20shedding%20light%20on%20the%20complex%0Ainterplay%20between%20human%20values%20and%20language%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07900v2&entry.124074799=Read"},
{"title": "KANQAS: Kolmogorov Arnold Network for Quantum Architecture Search", "author": "Akash Kundu and Aritra Sarkar and Abhishek Sadhu", "abstract": "  Quantum architecture search~(QAS) is a promising direction for optimization\nand automated design of quantum circuits towards quantum advantage. Recent\ntechniques in QAS focus on machine learning-based approaches from reinforcement\nlearning, like deep Q-network. While multi-layer perceptron-based deep\nQ-networks have been applied for QAS, their interpretability remains\nchallenging due to the high number of parameters. In this work, we evaluate the\npracticality of KANs in quantum architecture search problems, analyzing their\nefficiency in terms of the probability of success, frequency of optimal\nsolutions and their dependencies on various degrees of freedom of the network.\nIn a noiseless scenario, the probability of success and the number of optimal\nquantum circuit configurations to generate the multi-qubit maximally entangled\nstates are significantly higher than MLPs. Moreover in noisy scenarios, KAN can\nachieve a better fidelity in approximating maximally entangled state than MLPs,\nwhere the performance of the MLP significantly depends on the choice of\nactivation function. Further investigation reveals that KAN requires a very\nsmall number of learnable parameters compared to MLPs, however, the average\ntime of executing each episode for KAN is much higher.\n", "link": "http://arxiv.org/abs/2406.17630v1", "date": "2024-06-25", "relevancy": 1.28, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.447}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4432}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KANQAS%3A%20Kolmogorov%20Arnold%20Network%20for%20Quantum%20Architecture%20Search&body=Title%3A%20KANQAS%3A%20Kolmogorov%20Arnold%20Network%20for%20Quantum%20Architecture%20Search%0AAuthor%3A%20Akash%20Kundu%20and%20Aritra%20Sarkar%20and%20Abhishek%20Sadhu%0AAbstract%3A%20%20%20Quantum%20architecture%20search~%28QAS%29%20is%20a%20promising%20direction%20for%20optimization%0Aand%20automated%20design%20of%20quantum%20circuits%20towards%20quantum%20advantage.%20Recent%0Atechniques%20in%20QAS%20focus%20on%20machine%20learning-based%20approaches%20from%20reinforcement%0Alearning%2C%20like%20deep%20Q-network.%20While%20multi-layer%20perceptron-based%20deep%0AQ-networks%20have%20been%20applied%20for%20QAS%2C%20their%20interpretability%20remains%0Achallenging%20due%20to%20the%20high%20number%20of%20parameters.%20In%20this%20work%2C%20we%20evaluate%20the%0Apracticality%20of%20KANs%20in%20quantum%20architecture%20search%20problems%2C%20analyzing%20their%0Aefficiency%20in%20terms%20of%20the%20probability%20of%20success%2C%20frequency%20of%20optimal%0Asolutions%20and%20their%20dependencies%20on%20various%20degrees%20of%20freedom%20of%20the%20network.%0AIn%20a%20noiseless%20scenario%2C%20the%20probability%20of%20success%20and%20the%20number%20of%20optimal%0Aquantum%20circuit%20configurations%20to%20generate%20the%20multi-qubit%20maximally%20entangled%0Astates%20are%20significantly%20higher%20than%20MLPs.%20Moreover%20in%20noisy%20scenarios%2C%20KAN%20can%0Aachieve%20a%20better%20fidelity%20in%20approximating%20maximally%20entangled%20state%20than%20MLPs%2C%0Awhere%20the%20performance%20of%20the%20MLP%20significantly%20depends%20on%20the%20choice%20of%0Aactivation%20function.%20Further%20investigation%20reveals%20that%20KAN%20requires%20a%20very%0Asmall%20number%20of%20learnable%20parameters%20compared%20to%20MLPs%2C%20however%2C%20the%20average%0Atime%20of%20executing%20each%20episode%20for%20KAN%20is%20much%20higher.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKANQAS%253A%2520Kolmogorov%2520Arnold%2520Network%2520for%2520Quantum%2520Architecture%2520Search%26entry.906535625%3DAkash%2520Kundu%2520and%2520Aritra%2520Sarkar%2520and%2520Abhishek%2520Sadhu%26entry.1292438233%3D%2520%2520Quantum%2520architecture%2520search~%2528QAS%2529%2520is%2520a%2520promising%2520direction%2520for%2520optimization%250Aand%2520automated%2520design%2520of%2520quantum%2520circuits%2520towards%2520quantum%2520advantage.%2520Recent%250Atechniques%2520in%2520QAS%2520focus%2520on%2520machine%2520learning-based%2520approaches%2520from%2520reinforcement%250Alearning%252C%2520like%2520deep%2520Q-network.%2520While%2520multi-layer%2520perceptron-based%2520deep%250AQ-networks%2520have%2520been%2520applied%2520for%2520QAS%252C%2520their%2520interpretability%2520remains%250Achallenging%2520due%2520to%2520the%2520high%2520number%2520of%2520parameters.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520the%250Apracticality%2520of%2520KANs%2520in%2520quantum%2520architecture%2520search%2520problems%252C%2520analyzing%2520their%250Aefficiency%2520in%2520terms%2520of%2520the%2520probability%2520of%2520success%252C%2520frequency%2520of%2520optimal%250Asolutions%2520and%2520their%2520dependencies%2520on%2520various%2520degrees%2520of%2520freedom%2520of%2520the%2520network.%250AIn%2520a%2520noiseless%2520scenario%252C%2520the%2520probability%2520of%2520success%2520and%2520the%2520number%2520of%2520optimal%250Aquantum%2520circuit%2520configurations%2520to%2520generate%2520the%2520multi-qubit%2520maximally%2520entangled%250Astates%2520are%2520significantly%2520higher%2520than%2520MLPs.%2520Moreover%2520in%2520noisy%2520scenarios%252C%2520KAN%2520can%250Aachieve%2520a%2520better%2520fidelity%2520in%2520approximating%2520maximally%2520entangled%2520state%2520than%2520MLPs%252C%250Awhere%2520the%2520performance%2520of%2520the%2520MLP%2520significantly%2520depends%2520on%2520the%2520choice%2520of%250Aactivation%2520function.%2520Further%2520investigation%2520reveals%2520that%2520KAN%2520requires%2520a%2520very%250Asmall%2520number%2520of%2520learnable%2520parameters%2520compared%2520to%2520MLPs%252C%2520however%252C%2520the%2520average%250Atime%2520of%2520executing%2520each%2520episode%2520for%2520KAN%2520is%2520much%2520higher.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KANQAS%3A%20Kolmogorov%20Arnold%20Network%20for%20Quantum%20Architecture%20Search&entry.906535625=Akash%20Kundu%20and%20Aritra%20Sarkar%20and%20Abhishek%20Sadhu&entry.1292438233=%20%20Quantum%20architecture%20search~%28QAS%29%20is%20a%20promising%20direction%20for%20optimization%0Aand%20automated%20design%20of%20quantum%20circuits%20towards%20quantum%20advantage.%20Recent%0Atechniques%20in%20QAS%20focus%20on%20machine%20learning-based%20approaches%20from%20reinforcement%0Alearning%2C%20like%20deep%20Q-network.%20While%20multi-layer%20perceptron-based%20deep%0AQ-networks%20have%20been%20applied%20for%20QAS%2C%20their%20interpretability%20remains%0Achallenging%20due%20to%20the%20high%20number%20of%20parameters.%20In%20this%20work%2C%20we%20evaluate%20the%0Apracticality%20of%20KANs%20in%20quantum%20architecture%20search%20problems%2C%20analyzing%20their%0Aefficiency%20in%20terms%20of%20the%20probability%20of%20success%2C%20frequency%20of%20optimal%0Asolutions%20and%20their%20dependencies%20on%20various%20degrees%20of%20freedom%20of%20the%20network.%0AIn%20a%20noiseless%20scenario%2C%20the%20probability%20of%20success%20and%20the%20number%20of%20optimal%0Aquantum%20circuit%20configurations%20to%20generate%20the%20multi-qubit%20maximally%20entangled%0Astates%20are%20significantly%20higher%20than%20MLPs.%20Moreover%20in%20noisy%20scenarios%2C%20KAN%20can%0Aachieve%20a%20better%20fidelity%20in%20approximating%20maximally%20entangled%20state%20than%20MLPs%2C%0Awhere%20the%20performance%20of%20the%20MLP%20significantly%20depends%20on%20the%20choice%20of%0Aactivation%20function.%20Further%20investigation%20reveals%20that%20KAN%20requires%20a%20very%0Asmall%20number%20of%20learnable%20parameters%20compared%20to%20MLPs%2C%20however%2C%20the%20average%0Atime%20of%20executing%20each%20episode%20for%20KAN%20is%20much%20higher.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17630v1&entry.124074799=Read"},
{"title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues", "author": "Ge Bai and Jie Liu and Xingyuan Bu and Yancheng He and Jiaheng Liu and Zhanhui Zhou and Zhuoran Lin and Wenbo Su and Tiezheng Ge and Bo Zheng and Wanli Ouyang", "abstract": "  The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}.\n", "link": "http://arxiv.org/abs/2402.14762v2", "date": "2024-06-25", "relevancy": 1.4198, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4536}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MT-Bench-101%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Large%20Language%0A%20%20Models%20in%20Multi-Turn%20Dialogues&body=Title%3A%20MT-Bench-101%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Large%20Language%0A%20%20Models%20in%20Multi-Turn%20Dialogues%0AAuthor%3A%20Ge%20Bai%20and%20Jie%20Liu%20and%20Xingyuan%20Bu%20and%20Yancheng%20He%20and%20Jiaheng%20Liu%20and%20Zhanhui%20Zhou%20and%20Zhuoran%20Lin%20and%20Wenbo%20Su%20and%20Tiezheng%20Ge%20and%20Bo%20Zheng%20and%20Wanli%20Ouyang%0AAbstract%3A%20%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20drastically%20enhanced%20dialogue%0Asystems.%20However%2C%20comprehensively%20evaluating%20the%20dialogue%20abilities%20of%20LLMs%0Aremains%20a%20challenge.%20Previous%20benchmarks%20have%20primarily%20focused%20on%20single-turn%0Adialogues%20or%20provided%20coarse-grained%20and%20incomplete%20assessments%20of%20multi-turn%0Adialogues%2C%20overlooking%20the%20complexity%20and%20fine-grained%20nuances%20of%20real-life%0Adialogues.%20To%20address%20this%20issue%2C%20we%20introduce%20MT-Bench-101%2C%20specifically%0Adesigned%20to%20evaluate%20the%20fine-grained%20abilities%20of%20LLMs%20in%20multi-turn%0Adialogues.%20By%20conducting%20a%20detailed%20analysis%20of%20real%20multi-turn%20dialogue%20data%2C%0Awe%20construct%20a%20three-tier%20hierarchical%20ability%20taxonomy%20comprising%204208%20turns%0Aacross%201388%20multi-turn%20dialogues%20in%2013%20distinct%20tasks.%20We%20then%20evaluate%2021%0Apopular%20LLMs%20based%20on%20MT-Bench-101%2C%20conducting%20comprehensive%20analyses%20from%20both%0Aability%20and%20task%20perspectives%20and%20observing%20differing%20trends%20in%20LLMs%0Aperformance%20across%20dialogue%20turns%20within%20various%20tasks.%20Further%20analysis%0Aindicates%20that%20neither%20utilizing%20common%20alignment%20techniques%20nor%20chat-specific%0Adesigns%20has%20led%20to%20obvious%20enhancements%20in%20the%20multi-turn%20abilities%20of%20LLMs.%0AExtensive%20case%20studies%20suggest%20that%20our%20designed%20tasks%20accurately%20assess%20the%0Acorresponding%20multi-turn%20abilities.%20The%20data%20and%20code%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/mtbench101/mt-bench-101%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMT-Bench-101%253A%2520A%2520Fine-Grained%2520Benchmark%2520for%2520Evaluating%2520Large%2520Language%250A%2520%2520Models%2520in%2520Multi-Turn%2520Dialogues%26entry.906535625%3DGe%2520Bai%2520and%2520Jie%2520Liu%2520and%2520Xingyuan%2520Bu%2520and%2520Yancheng%2520He%2520and%2520Jiaheng%2520Liu%2520and%2520Zhanhui%2520Zhou%2520and%2520Zhuoran%2520Lin%2520and%2520Wenbo%2520Su%2520and%2520Tiezheng%2520Ge%2520and%2520Bo%2520Zheng%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520drastically%2520enhanced%2520dialogue%250Asystems.%2520However%252C%2520comprehensively%2520evaluating%2520the%2520dialogue%2520abilities%2520of%2520LLMs%250Aremains%2520a%2520challenge.%2520Previous%2520benchmarks%2520have%2520primarily%2520focused%2520on%2520single-turn%250Adialogues%2520or%2520provided%2520coarse-grained%2520and%2520incomplete%2520assessments%2520of%2520multi-turn%250Adialogues%252C%2520overlooking%2520the%2520complexity%2520and%2520fine-grained%2520nuances%2520of%2520real-life%250Adialogues.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520MT-Bench-101%252C%2520specifically%250Adesigned%2520to%2520evaluate%2520the%2520fine-grained%2520abilities%2520of%2520LLMs%2520in%2520multi-turn%250Adialogues.%2520By%2520conducting%2520a%2520detailed%2520analysis%2520of%2520real%2520multi-turn%2520dialogue%2520data%252C%250Awe%2520construct%2520a%2520three-tier%2520hierarchical%2520ability%2520taxonomy%2520comprising%25204208%2520turns%250Aacross%25201388%2520multi-turn%2520dialogues%2520in%252013%2520distinct%2520tasks.%2520We%2520then%2520evaluate%252021%250Apopular%2520LLMs%2520based%2520on%2520MT-Bench-101%252C%2520conducting%2520comprehensive%2520analyses%2520from%2520both%250Aability%2520and%2520task%2520perspectives%2520and%2520observing%2520differing%2520trends%2520in%2520LLMs%250Aperformance%2520across%2520dialogue%2520turns%2520within%2520various%2520tasks.%2520Further%2520analysis%250Aindicates%2520that%2520neither%2520utilizing%2520common%2520alignment%2520techniques%2520nor%2520chat-specific%250Adesigns%2520has%2520led%2520to%2520obvious%2520enhancements%2520in%2520the%2520multi-turn%2520abilities%2520of%2520LLMs.%250AExtensive%2520case%2520studies%2520suggest%2520that%2520our%2520designed%2520tasks%2520accurately%2520assess%2520the%250Acorresponding%2520multi-turn%2520abilities.%2520The%2520data%2520and%2520code%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/mtbench101/mt-bench-101%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT-Bench-101%3A%20A%20Fine-Grained%20Benchmark%20for%20Evaluating%20Large%20Language%0A%20%20Models%20in%20Multi-Turn%20Dialogues&entry.906535625=Ge%20Bai%20and%20Jie%20Liu%20and%20Xingyuan%20Bu%20and%20Yancheng%20He%20and%20Jiaheng%20Liu%20and%20Zhanhui%20Zhou%20and%20Zhuoran%20Lin%20and%20Wenbo%20Su%20and%20Tiezheng%20Ge%20and%20Bo%20Zheng%20and%20Wanli%20Ouyang&entry.1292438233=%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20drastically%20enhanced%20dialogue%0Asystems.%20However%2C%20comprehensively%20evaluating%20the%20dialogue%20abilities%20of%20LLMs%0Aremains%20a%20challenge.%20Previous%20benchmarks%20have%20primarily%20focused%20on%20single-turn%0Adialogues%20or%20provided%20coarse-grained%20and%20incomplete%20assessments%20of%20multi-turn%0Adialogues%2C%20overlooking%20the%20complexity%20and%20fine-grained%20nuances%20of%20real-life%0Adialogues.%20To%20address%20this%20issue%2C%20we%20introduce%20MT-Bench-101%2C%20specifically%0Adesigned%20to%20evaluate%20the%20fine-grained%20abilities%20of%20LLMs%20in%20multi-turn%0Adialogues.%20By%20conducting%20a%20detailed%20analysis%20of%20real%20multi-turn%20dialogue%20data%2C%0Awe%20construct%20a%20three-tier%20hierarchical%20ability%20taxonomy%20comprising%204208%20turns%0Aacross%201388%20multi-turn%20dialogues%20in%2013%20distinct%20tasks.%20We%20then%20evaluate%2021%0Apopular%20LLMs%20based%20on%20MT-Bench-101%2C%20conducting%20comprehensive%20analyses%20from%20both%0Aability%20and%20task%20perspectives%20and%20observing%20differing%20trends%20in%20LLMs%0Aperformance%20across%20dialogue%20turns%20within%20various%20tasks.%20Further%20analysis%0Aindicates%20that%20neither%20utilizing%20common%20alignment%20techniques%20nor%20chat-specific%0Adesigns%20has%20led%20to%20obvious%20enhancements%20in%20the%20multi-turn%20abilities%20of%20LLMs.%0AExtensive%20case%20studies%20suggest%20that%20our%20designed%20tasks%20accurately%20assess%20the%0Acorresponding%20multi-turn%20abilities.%20The%20data%20and%20code%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/mtbench101/mt-bench-101%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14762v2&entry.124074799=Read"},
{"title": "Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo\n  is All you Need", "author": "Shangda Yang and Vitaly Zankin and Maximilian Balandat and Stefan Scherer and Kevin Carlberg and Neil Walton and Kody J. H. Law", "abstract": "  We leverage multilevel Monte Carlo (MLMC) to improve the performance of\nmulti-step look-ahead Bayesian optimization (BO) methods that involve nested\nexpectations and maximizations. Often these expectations must be computed by\nMonte Carlo (MC). The complexity rate of naive MC degrades for nested\noperations, whereas MLMC is capable of achieving the canonical MC convergence\nrate for this type of problem, independently of dimension and without any\nsmoothness assumptions. Our theoretical study focuses on the approximation\nimprovements for twoand three-step look-ahead acquisition functions, but, as we\ndiscuss, the approach is generalizable in various ways, including beyond the\ncontext of BO. Our findings are verified numerically and the benefits of MLMC\nfor BO are illustrated on several benchmark examples. Code is available at\nhttps://github.com/Shangda-Yang/MLMCBO .\n", "link": "http://arxiv.org/abs/2402.02111v2", "date": "2024-06-25", "relevancy": 1.8902, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4601}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Look-ahead%20in%20Bayesian%20Optimization%3A%20Multilevel%20Monte%20Carlo%0A%20%20is%20All%20you%20Need&body=Title%3A%20Accelerating%20Look-ahead%20in%20Bayesian%20Optimization%3A%20Multilevel%20Monte%20Carlo%0A%20%20is%20All%20you%20Need%0AAuthor%3A%20Shangda%20Yang%20and%20Vitaly%20Zankin%20and%20Maximilian%20Balandat%20and%20Stefan%20Scherer%20and%20Kevin%20Carlberg%20and%20Neil%20Walton%20and%20Kody%20J.%20H.%20Law%0AAbstract%3A%20%20%20We%20leverage%20multilevel%20Monte%20Carlo%20%28MLMC%29%20to%20improve%20the%20performance%20of%0Amulti-step%20look-ahead%20Bayesian%20optimization%20%28BO%29%20methods%20that%20involve%20nested%0Aexpectations%20and%20maximizations.%20Often%20these%20expectations%20must%20be%20computed%20by%0AMonte%20Carlo%20%28MC%29.%20The%20complexity%20rate%20of%20naive%20MC%20degrades%20for%20nested%0Aoperations%2C%20whereas%20MLMC%20is%20capable%20of%20achieving%20the%20canonical%20MC%20convergence%0Arate%20for%20this%20type%20of%20problem%2C%20independently%20of%20dimension%20and%20without%20any%0Asmoothness%20assumptions.%20Our%20theoretical%20study%20focuses%20on%20the%20approximation%0Aimprovements%20for%20twoand%20three-step%20look-ahead%20acquisition%20functions%2C%20but%2C%20as%20we%0Adiscuss%2C%20the%20approach%20is%20generalizable%20in%20various%20ways%2C%20including%20beyond%20the%0Acontext%20of%20BO.%20Our%20findings%20are%20verified%20numerically%20and%20the%20benefits%20of%20MLMC%0Afor%20BO%20are%20illustrated%20on%20several%20benchmark%20examples.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Shangda-Yang/MLMCBO%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02111v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Look-ahead%2520in%2520Bayesian%2520Optimization%253A%2520Multilevel%2520Monte%2520Carlo%250A%2520%2520is%2520All%2520you%2520Need%26entry.906535625%3DShangda%2520Yang%2520and%2520Vitaly%2520Zankin%2520and%2520Maximilian%2520Balandat%2520and%2520Stefan%2520Scherer%2520and%2520Kevin%2520Carlberg%2520and%2520Neil%2520Walton%2520and%2520Kody%2520J.%2520H.%2520Law%26entry.1292438233%3D%2520%2520We%2520leverage%2520multilevel%2520Monte%2520Carlo%2520%2528MLMC%2529%2520to%2520improve%2520the%2520performance%2520of%250Amulti-step%2520look-ahead%2520Bayesian%2520optimization%2520%2528BO%2529%2520methods%2520that%2520involve%2520nested%250Aexpectations%2520and%2520maximizations.%2520Often%2520these%2520expectations%2520must%2520be%2520computed%2520by%250AMonte%2520Carlo%2520%2528MC%2529.%2520The%2520complexity%2520rate%2520of%2520naive%2520MC%2520degrades%2520for%2520nested%250Aoperations%252C%2520whereas%2520MLMC%2520is%2520capable%2520of%2520achieving%2520the%2520canonical%2520MC%2520convergence%250Arate%2520for%2520this%2520type%2520of%2520problem%252C%2520independently%2520of%2520dimension%2520and%2520without%2520any%250Asmoothness%2520assumptions.%2520Our%2520theoretical%2520study%2520focuses%2520on%2520the%2520approximation%250Aimprovements%2520for%2520twoand%2520three-step%2520look-ahead%2520acquisition%2520functions%252C%2520but%252C%2520as%2520we%250Adiscuss%252C%2520the%2520approach%2520is%2520generalizable%2520in%2520various%2520ways%252C%2520including%2520beyond%2520the%250Acontext%2520of%2520BO.%2520Our%2520findings%2520are%2520verified%2520numerically%2520and%2520the%2520benefits%2520of%2520MLMC%250Afor%2520BO%2520are%2520illustrated%2520on%2520several%2520benchmark%2520examples.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Shangda-Yang/MLMCBO%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02111v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Look-ahead%20in%20Bayesian%20Optimization%3A%20Multilevel%20Monte%20Carlo%0A%20%20is%20All%20you%20Need&entry.906535625=Shangda%20Yang%20and%20Vitaly%20Zankin%20and%20Maximilian%20Balandat%20and%20Stefan%20Scherer%20and%20Kevin%20Carlberg%20and%20Neil%20Walton%20and%20Kody%20J.%20H.%20Law&entry.1292438233=%20%20We%20leverage%20multilevel%20Monte%20Carlo%20%28MLMC%29%20to%20improve%20the%20performance%20of%0Amulti-step%20look-ahead%20Bayesian%20optimization%20%28BO%29%20methods%20that%20involve%20nested%0Aexpectations%20and%20maximizations.%20Often%20these%20expectations%20must%20be%20computed%20by%0AMonte%20Carlo%20%28MC%29.%20The%20complexity%20rate%20of%20naive%20MC%20degrades%20for%20nested%0Aoperations%2C%20whereas%20MLMC%20is%20capable%20of%20achieving%20the%20canonical%20MC%20convergence%0Arate%20for%20this%20type%20of%20problem%2C%20independently%20of%20dimension%20and%20without%20any%0Asmoothness%20assumptions.%20Our%20theoretical%20study%20focuses%20on%20the%20approximation%0Aimprovements%20for%20twoand%20three-step%20look-ahead%20acquisition%20functions%2C%20but%2C%20as%20we%0Adiscuss%2C%20the%20approach%20is%20generalizable%20in%20various%20ways%2C%20including%20beyond%20the%0Acontext%20of%20BO.%20Our%20findings%20are%20verified%20numerically%20and%20the%20benefits%20of%20MLMC%0Afor%20BO%20are%20illustrated%20on%20several%20benchmark%20examples.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Shangda-Yang/MLMCBO%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02111v2&entry.124074799=Read"},
{"title": "Data curation via joint example selection further accelerates multimodal\n  learning", "author": "Talfan Evans and Nikhil Parthasarathy and Hamza Merzic and Olivier J. Henaff", "abstract": "  Data curation is an essential component of large-scale pretraining. In this\nwork, we demonstrate that jointly selecting batches of data is more effective\nfor learning than selecting examples independently. Multimodal contrastive\nobjectives expose the dependencies between data and thus naturally yield\ncriteria for measuring the joint learnability of a batch. We derive a simple\nand tractable algorithm for selecting such batches, which significantly\naccelerate training beyond individually-prioritized data points. As performance\nimproves by selecting from larger super-batches, we also leverage recent\nadvances in model approximation to reduce the associated computational\noverhead. As a result, our approach--multimodal contrastive learning with joint\nexample selection (JEST)--surpasses state-of-the-art models with up to\n13$\\times$ fewer iterations and 10$\\times$ less computation. Essential to the\nperformance of JEST is the ability to steer the data selection process towards\nthe distribution of smaller, well-curated datasets via pretrained reference\nmodels, exposing the level of data curation as a new dimension for neural\nscaling laws.\n", "link": "http://arxiv.org/abs/2406.17711v1", "date": "2024-06-25", "relevancy": 1.532, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5137}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20curation%20via%20joint%20example%20selection%20further%20accelerates%20multimodal%0A%20%20learning&body=Title%3A%20Data%20curation%20via%20joint%20example%20selection%20further%20accelerates%20multimodal%0A%20%20learning%0AAuthor%3A%20Talfan%20Evans%20and%20Nikhil%20Parthasarathy%20and%20Hamza%20Merzic%20and%20Olivier%20J.%20Henaff%0AAbstract%3A%20%20%20Data%20curation%20is%20an%20essential%20component%20of%20large-scale%20pretraining.%20In%20this%0Awork%2C%20we%20demonstrate%20that%20jointly%20selecting%20batches%20of%20data%20is%20more%20effective%0Afor%20learning%20than%20selecting%20examples%20independently.%20Multimodal%20contrastive%0Aobjectives%20expose%20the%20dependencies%20between%20data%20and%20thus%20naturally%20yield%0Acriteria%20for%20measuring%20the%20joint%20learnability%20of%20a%20batch.%20We%20derive%20a%20simple%0Aand%20tractable%20algorithm%20for%20selecting%20such%20batches%2C%20which%20significantly%0Aaccelerate%20training%20beyond%20individually-prioritized%20data%20points.%20As%20performance%0Aimproves%20by%20selecting%20from%20larger%20super-batches%2C%20we%20also%20leverage%20recent%0Aadvances%20in%20model%20approximation%20to%20reduce%20the%20associated%20computational%0Aoverhead.%20As%20a%20result%2C%20our%20approach--multimodal%20contrastive%20learning%20with%20joint%0Aexample%20selection%20%28JEST%29--surpasses%20state-of-the-art%20models%20with%20up%20to%0A13%24%5Ctimes%24%20fewer%20iterations%20and%2010%24%5Ctimes%24%20less%20computation.%20Essential%20to%20the%0Aperformance%20of%20JEST%20is%20the%20ability%20to%20steer%20the%20data%20selection%20process%20towards%0Athe%20distribution%20of%20smaller%2C%20well-curated%20datasets%20via%20pretrained%20reference%0Amodels%2C%20exposing%20the%20level%20of%20data%20curation%20as%20a%20new%20dimension%20for%20neural%0Ascaling%20laws.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520curation%2520via%2520joint%2520example%2520selection%2520further%2520accelerates%2520multimodal%250A%2520%2520learning%26entry.906535625%3DTalfan%2520Evans%2520and%2520Nikhil%2520Parthasarathy%2520and%2520Hamza%2520Merzic%2520and%2520Olivier%2520J.%2520Henaff%26entry.1292438233%3D%2520%2520Data%2520curation%2520is%2520an%2520essential%2520component%2520of%2520large-scale%2520pretraining.%2520In%2520this%250Awork%252C%2520we%2520demonstrate%2520that%2520jointly%2520selecting%2520batches%2520of%2520data%2520is%2520more%2520effective%250Afor%2520learning%2520than%2520selecting%2520examples%2520independently.%2520Multimodal%2520contrastive%250Aobjectives%2520expose%2520the%2520dependencies%2520between%2520data%2520and%2520thus%2520naturally%2520yield%250Acriteria%2520for%2520measuring%2520the%2520joint%2520learnability%2520of%2520a%2520batch.%2520We%2520derive%2520a%2520simple%250Aand%2520tractable%2520algorithm%2520for%2520selecting%2520such%2520batches%252C%2520which%2520significantly%250Aaccelerate%2520training%2520beyond%2520individually-prioritized%2520data%2520points.%2520As%2520performance%250Aimproves%2520by%2520selecting%2520from%2520larger%2520super-batches%252C%2520we%2520also%2520leverage%2520recent%250Aadvances%2520in%2520model%2520approximation%2520to%2520reduce%2520the%2520associated%2520computational%250Aoverhead.%2520As%2520a%2520result%252C%2520our%2520approach--multimodal%2520contrastive%2520learning%2520with%2520joint%250Aexample%2520selection%2520%2528JEST%2529--surpasses%2520state-of-the-art%2520models%2520with%2520up%2520to%250A13%2524%255Ctimes%2524%2520fewer%2520iterations%2520and%252010%2524%255Ctimes%2524%2520less%2520computation.%2520Essential%2520to%2520the%250Aperformance%2520of%2520JEST%2520is%2520the%2520ability%2520to%2520steer%2520the%2520data%2520selection%2520process%2520towards%250Athe%2520distribution%2520of%2520smaller%252C%2520well-curated%2520datasets%2520via%2520pretrained%2520reference%250Amodels%252C%2520exposing%2520the%2520level%2520of%2520data%2520curation%2520as%2520a%2520new%2520dimension%2520for%2520neural%250Ascaling%2520laws.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20curation%20via%20joint%20example%20selection%20further%20accelerates%20multimodal%0A%20%20learning&entry.906535625=Talfan%20Evans%20and%20Nikhil%20Parthasarathy%20and%20Hamza%20Merzic%20and%20Olivier%20J.%20Henaff&entry.1292438233=%20%20Data%20curation%20is%20an%20essential%20component%20of%20large-scale%20pretraining.%20In%20this%0Awork%2C%20we%20demonstrate%20that%20jointly%20selecting%20batches%20of%20data%20is%20more%20effective%0Afor%20learning%20than%20selecting%20examples%20independently.%20Multimodal%20contrastive%0Aobjectives%20expose%20the%20dependencies%20between%20data%20and%20thus%20naturally%20yield%0Acriteria%20for%20measuring%20the%20joint%20learnability%20of%20a%20batch.%20We%20derive%20a%20simple%0Aand%20tractable%20algorithm%20for%20selecting%20such%20batches%2C%20which%20significantly%0Aaccelerate%20training%20beyond%20individually-prioritized%20data%20points.%20As%20performance%0Aimproves%20by%20selecting%20from%20larger%20super-batches%2C%20we%20also%20leverage%20recent%0Aadvances%20in%20model%20approximation%20to%20reduce%20the%20associated%20computational%0Aoverhead.%20As%20a%20result%2C%20our%20approach--multimodal%20contrastive%20learning%20with%20joint%0Aexample%20selection%20%28JEST%29--surpasses%20state-of-the-art%20models%20with%20up%20to%0A13%24%5Ctimes%24%20fewer%20iterations%20and%2010%24%5Ctimes%24%20less%20computation.%20Essential%20to%20the%0Aperformance%20of%20JEST%20is%20the%20ability%20to%20steer%20the%20data%20selection%20process%20towards%0Athe%20distribution%20of%20smaller%2C%20well-curated%20datasets%20via%20pretrained%20reference%0Amodels%2C%20exposing%20the%20level%20of%20data%20curation%20as%20a%20new%20dimension%20for%20neural%0Ascaling%20laws.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17711v1&entry.124074799=Read"},
{"title": "Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking\n  Scores", "author": "\u00c1lvaro Ciudad and Adri\u00e1n Morales-Pastor and Laura Malo and Isaac Filella-Merc\u00e8 and Victor Guallar and Alexis Molina", "abstract": "  In this study, we present ScoreFormer, a novel graph transformer model\ndesigned to accurately predict molecular docking scores, thereby optimizing\nhigh-throughput virtual screening (HTVS) in drug discovery. The architecture\nintegrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk\nPositional Encodings (LRWPE), enhancing the model's ability to understand\ncomplex molecular structures and their relationship with their respective\ndocking scores. This approach significantly surpasses traditional HTVS methods\nand recent Graph Neural Network (GNN) models in both recovery and efficiency\ndue to a wider coverage of the chemical space and enhanced performance. Our\nresults demonstrate that ScoreFormer achieves competitive performance in\ndocking score prediction and offers a substantial 1.65-fold reduction in\ninference time compared to existing models. We evaluated ScoreFormer across\nmultiple datasets under various conditions, confirming its robustness and\nreliability in identifying potential drug candidates rapidly.\n", "link": "http://arxiv.org/abs/2406.09346v2", "date": "2024-06-25", "relevancy": 1.3682, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4678}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4653}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scoreformer%3A%20A%20Surrogate%20Model%20For%20Large-Scale%20Prediction%20of%20Docking%0A%20%20Scores&body=Title%3A%20Scoreformer%3A%20A%20Surrogate%20Model%20For%20Large-Scale%20Prediction%20of%20Docking%0A%20%20Scores%0AAuthor%3A%20%C3%81lvaro%20Ciudad%20and%20Adri%C3%A1n%20Morales-Pastor%20and%20Laura%20Malo%20and%20Isaac%20Filella-Merc%C3%A8%20and%20Victor%20Guallar%20and%20Alexis%20Molina%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20present%20ScoreFormer%2C%20a%20novel%20graph%20transformer%20model%0Adesigned%20to%20accurately%20predict%20molecular%20docking%20scores%2C%20thereby%20optimizing%0Ahigh-throughput%20virtual%20screening%20%28HTVS%29%20in%20drug%20discovery.%20The%20architecture%0Aintegrates%20Principal%20Neighborhood%20Aggregation%20%28PNA%29%20and%20Learnable%20Random%20Walk%0APositional%20Encodings%20%28LRWPE%29%2C%20enhancing%20the%20model%27s%20ability%20to%20understand%0Acomplex%20molecular%20structures%20and%20their%20relationship%20with%20their%20respective%0Adocking%20scores.%20This%20approach%20significantly%20surpasses%20traditional%20HTVS%20methods%0Aand%20recent%20Graph%20Neural%20Network%20%28GNN%29%20models%20in%20both%20recovery%20and%20efficiency%0Adue%20to%20a%20wider%20coverage%20of%20the%20chemical%20space%20and%20enhanced%20performance.%20Our%0Aresults%20demonstrate%20that%20ScoreFormer%20achieves%20competitive%20performance%20in%0Adocking%20score%20prediction%20and%20offers%20a%20substantial%201.65-fold%20reduction%20in%0Ainference%20time%20compared%20to%20existing%20models.%20We%20evaluated%20ScoreFormer%20across%0Amultiple%20datasets%20under%20various%20conditions%2C%20confirming%20its%20robustness%20and%0Areliability%20in%20identifying%20potential%20drug%20candidates%20rapidly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScoreformer%253A%2520A%2520Surrogate%2520Model%2520For%2520Large-Scale%2520Prediction%2520of%2520Docking%250A%2520%2520Scores%26entry.906535625%3D%25C3%2581lvaro%2520Ciudad%2520and%2520Adri%25C3%25A1n%2520Morales-Pastor%2520and%2520Laura%2520Malo%2520and%2520Isaac%2520Filella-Merc%25C3%25A8%2520and%2520Victor%2520Guallar%2520and%2520Alexis%2520Molina%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520present%2520ScoreFormer%252C%2520a%2520novel%2520graph%2520transformer%2520model%250Adesigned%2520to%2520accurately%2520predict%2520molecular%2520docking%2520scores%252C%2520thereby%2520optimizing%250Ahigh-throughput%2520virtual%2520screening%2520%2528HTVS%2529%2520in%2520drug%2520discovery.%2520The%2520architecture%250Aintegrates%2520Principal%2520Neighborhood%2520Aggregation%2520%2528PNA%2529%2520and%2520Learnable%2520Random%2520Walk%250APositional%2520Encodings%2520%2528LRWPE%2529%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520understand%250Acomplex%2520molecular%2520structures%2520and%2520their%2520relationship%2520with%2520their%2520respective%250Adocking%2520scores.%2520This%2520approach%2520significantly%2520surpasses%2520traditional%2520HTVS%2520methods%250Aand%2520recent%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520models%2520in%2520both%2520recovery%2520and%2520efficiency%250Adue%2520to%2520a%2520wider%2520coverage%2520of%2520the%2520chemical%2520space%2520and%2520enhanced%2520performance.%2520Our%250Aresults%2520demonstrate%2520that%2520ScoreFormer%2520achieves%2520competitive%2520performance%2520in%250Adocking%2520score%2520prediction%2520and%2520offers%2520a%2520substantial%25201.65-fold%2520reduction%2520in%250Ainference%2520time%2520compared%2520to%2520existing%2520models.%2520We%2520evaluated%2520ScoreFormer%2520across%250Amultiple%2520datasets%2520under%2520various%2520conditions%252C%2520confirming%2520its%2520robustness%2520and%250Areliability%2520in%2520identifying%2520potential%2520drug%2520candidates%2520rapidly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scoreformer%3A%20A%20Surrogate%20Model%20For%20Large-Scale%20Prediction%20of%20Docking%0A%20%20Scores&entry.906535625=%C3%81lvaro%20Ciudad%20and%20Adri%C3%A1n%20Morales-Pastor%20and%20Laura%20Malo%20and%20Isaac%20Filella-Merc%C3%A8%20and%20Victor%20Guallar%20and%20Alexis%20Molina&entry.1292438233=%20%20In%20this%20study%2C%20we%20present%20ScoreFormer%2C%20a%20novel%20graph%20transformer%20model%0Adesigned%20to%20accurately%20predict%20molecular%20docking%20scores%2C%20thereby%20optimizing%0Ahigh-throughput%20virtual%20screening%20%28HTVS%29%20in%20drug%20discovery.%20The%20architecture%0Aintegrates%20Principal%20Neighborhood%20Aggregation%20%28PNA%29%20and%20Learnable%20Random%20Walk%0APositional%20Encodings%20%28LRWPE%29%2C%20enhancing%20the%20model%27s%20ability%20to%20understand%0Acomplex%20molecular%20structures%20and%20their%20relationship%20with%20their%20respective%0Adocking%20scores.%20This%20approach%20significantly%20surpasses%20traditional%20HTVS%20methods%0Aand%20recent%20Graph%20Neural%20Network%20%28GNN%29%20models%20in%20both%20recovery%20and%20efficiency%0Adue%20to%20a%20wider%20coverage%20of%20the%20chemical%20space%20and%20enhanced%20performance.%20Our%0Aresults%20demonstrate%20that%20ScoreFormer%20achieves%20competitive%20performance%20in%0Adocking%20score%20prediction%20and%20offers%20a%20substantial%201.65-fold%20reduction%20in%0Ainference%20time%20compared%20to%20existing%20models.%20We%20evaluated%20ScoreFormer%20across%0Amultiple%20datasets%20under%20various%20conditions%2C%20confirming%20its%20robustness%20and%0Areliability%20in%20identifying%20potential%20drug%20candidates%20rapidly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09346v2&entry.124074799=Read"},
{"title": "ELIZA Reinterpreted: The world's first chatbot was not intended as a\n  chatbot at all", "author": "Jeff Shrager", "abstract": "  ELIZA, often considered the world's first chatbot, was written by Joseph\nWeizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot,\nbut rather to build a platform for research into human-machine conversation and\nthe important cognitive processes of interpretation and misinterpretation. His\npurpose was obscured by ELIZA's fame, resulting in large part from the\nfortuitous timing of it's creation, and it's escape into the wild. In this\npaper I provide a rich historical context for ELIZA's creation, demonstrating\nthat ELIZA arose from the intersection of some of the central threads in the\ntechnical history of AI. I also briefly discuss how ELIZA escaped into the\nworld, and how its accidental escape, along with several coincidental turns of\nthe programming language screws, led both to the misapprehension that ELIZA was\nintended as a chatbot, and to the loss of the original ELIZA to history for\nover 50 years.\n", "link": "http://arxiv.org/abs/2406.17650v1", "date": "2024-06-25", "relevancy": 1.5095, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4215}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3867}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELIZA%20Reinterpreted%3A%20The%20world%27s%20first%20chatbot%20was%20not%20intended%20as%20a%0A%20%20chatbot%20at%20all&body=Title%3A%20ELIZA%20Reinterpreted%3A%20The%20world%27s%20first%20chatbot%20was%20not%20intended%20as%20a%0A%20%20chatbot%20at%20all%0AAuthor%3A%20Jeff%20Shrager%0AAbstract%3A%20%20%20ELIZA%2C%20often%20considered%20the%20world%27s%20first%20chatbot%2C%20was%20written%20by%20Joseph%0AWeizenbaum%20in%20the%20early%201960s.%20Weizenbaum%20did%20not%20intend%20to%20invent%20the%20chatbot%2C%0Abut%20rather%20to%20build%20a%20platform%20for%20research%20into%20human-machine%20conversation%20and%0Athe%20important%20cognitive%20processes%20of%20interpretation%20and%20misinterpretation.%20His%0Apurpose%20was%20obscured%20by%20ELIZA%27s%20fame%2C%20resulting%20in%20large%20part%20from%20the%0Afortuitous%20timing%20of%20it%27s%20creation%2C%20and%20it%27s%20escape%20into%20the%20wild.%20In%20this%0Apaper%20I%20provide%20a%20rich%20historical%20context%20for%20ELIZA%27s%20creation%2C%20demonstrating%0Athat%20ELIZA%20arose%20from%20the%20intersection%20of%20some%20of%20the%20central%20threads%20in%20the%0Atechnical%20history%20of%20AI.%20I%20also%20briefly%20discuss%20how%20ELIZA%20escaped%20into%20the%0Aworld%2C%20and%20how%20its%20accidental%20escape%2C%20along%20with%20several%20coincidental%20turns%20of%0Athe%20programming%20language%20screws%2C%20led%20both%20to%20the%20misapprehension%20that%20ELIZA%20was%0Aintended%20as%20a%20chatbot%2C%20and%20to%20the%20loss%20of%20the%20original%20ELIZA%20to%20history%20for%0Aover%2050%20years.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELIZA%2520Reinterpreted%253A%2520The%2520world%2527s%2520first%2520chatbot%2520was%2520not%2520intended%2520as%2520a%250A%2520%2520chatbot%2520at%2520all%26entry.906535625%3DJeff%2520Shrager%26entry.1292438233%3D%2520%2520ELIZA%252C%2520often%2520considered%2520the%2520world%2527s%2520first%2520chatbot%252C%2520was%2520written%2520by%2520Joseph%250AWeizenbaum%2520in%2520the%2520early%25201960s.%2520Weizenbaum%2520did%2520not%2520intend%2520to%2520invent%2520the%2520chatbot%252C%250Abut%2520rather%2520to%2520build%2520a%2520platform%2520for%2520research%2520into%2520human-machine%2520conversation%2520and%250Athe%2520important%2520cognitive%2520processes%2520of%2520interpretation%2520and%2520misinterpretation.%2520His%250Apurpose%2520was%2520obscured%2520by%2520ELIZA%2527s%2520fame%252C%2520resulting%2520in%2520large%2520part%2520from%2520the%250Afortuitous%2520timing%2520of%2520it%2527s%2520creation%252C%2520and%2520it%2527s%2520escape%2520into%2520the%2520wild.%2520In%2520this%250Apaper%2520I%2520provide%2520a%2520rich%2520historical%2520context%2520for%2520ELIZA%2527s%2520creation%252C%2520demonstrating%250Athat%2520ELIZA%2520arose%2520from%2520the%2520intersection%2520of%2520some%2520of%2520the%2520central%2520threads%2520in%2520the%250Atechnical%2520history%2520of%2520AI.%2520I%2520also%2520briefly%2520discuss%2520how%2520ELIZA%2520escaped%2520into%2520the%250Aworld%252C%2520and%2520how%2520its%2520accidental%2520escape%252C%2520along%2520with%2520several%2520coincidental%2520turns%2520of%250Athe%2520programming%2520language%2520screws%252C%2520led%2520both%2520to%2520the%2520misapprehension%2520that%2520ELIZA%2520was%250Aintended%2520as%2520a%2520chatbot%252C%2520and%2520to%2520the%2520loss%2520of%2520the%2520original%2520ELIZA%2520to%2520history%2520for%250Aover%252050%2520years.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELIZA%20Reinterpreted%3A%20The%20world%27s%20first%20chatbot%20was%20not%20intended%20as%20a%0A%20%20chatbot%20at%20all&entry.906535625=Jeff%20Shrager&entry.1292438233=%20%20ELIZA%2C%20often%20considered%20the%20world%27s%20first%20chatbot%2C%20was%20written%20by%20Joseph%0AWeizenbaum%20in%20the%20early%201960s.%20Weizenbaum%20did%20not%20intend%20to%20invent%20the%20chatbot%2C%0Abut%20rather%20to%20build%20a%20platform%20for%20research%20into%20human-machine%20conversation%20and%0Athe%20important%20cognitive%20processes%20of%20interpretation%20and%20misinterpretation.%20His%0Apurpose%20was%20obscured%20by%20ELIZA%27s%20fame%2C%20resulting%20in%20large%20part%20from%20the%0Afortuitous%20timing%20of%20it%27s%20creation%2C%20and%20it%27s%20escape%20into%20the%20wild.%20In%20this%0Apaper%20I%20provide%20a%20rich%20historical%20context%20for%20ELIZA%27s%20creation%2C%20demonstrating%0Athat%20ELIZA%20arose%20from%20the%20intersection%20of%20some%20of%20the%20central%20threads%20in%20the%0Atechnical%20history%20of%20AI.%20I%20also%20briefly%20discuss%20how%20ELIZA%20escaped%20into%20the%0Aworld%2C%20and%20how%20its%20accidental%20escape%2C%20along%20with%20several%20coincidental%20turns%20of%0Athe%20programming%20language%20screws%2C%20led%20both%20to%20the%20misapprehension%20that%20ELIZA%20was%0Aintended%20as%20a%20chatbot%2C%20and%20to%20the%20loss%20of%20the%20original%20ELIZA%20to%20history%20for%0Aover%2050%20years.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17650v1&entry.124074799=Read"},
{"title": "Low Fidelity Visuo-Tactile Pretraining Improves Vision-Only Manipulation\n  Performance", "author": "Selam Gano and Abraham George and Amir Barati Farimani", "abstract": "  Tactile perception is a critical component of solving real-world manipulation\ntasks, but tactile sensors for manipulation have barriers to use such as\nfragility and cost. In this work, we engage a robust, low-cost tactile sensor,\nBeadSight, as an alternative to precise pre-calibrated sensors for a\npretraining approach to manipulation. We show that tactile pretraining, even\nwith a low-fidelity sensor as BeadSight, can improve an imitation learning\nagent's performance on complex manipulation tasks. We demonstrate this method\nagainst a baseline USB cable plugging task, previously achieved with a much\nhigher precision GelSight sensor as the tactile input to pretraining. Our best\nBeadSight pretrained visuo-tactile agent completed the task with 70\\% accuracy\ncompared to 85\\% for the best GelSight pretrained visuo-tactile agent, with\nvision-only inference for both.\n", "link": "http://arxiv.org/abs/2406.15639v2", "date": "2024-06-25", "relevancy": 1.5499, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5074}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low%20Fidelity%20Visuo-Tactile%20Pretraining%20Improves%20Vision-Only%20Manipulation%0A%20%20Performance&body=Title%3A%20Low%20Fidelity%20Visuo-Tactile%20Pretraining%20Improves%20Vision-Only%20Manipulation%0A%20%20Performance%0AAuthor%3A%20Selam%20Gano%20and%20Abraham%20George%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20Tactile%20perception%20is%20a%20critical%20component%20of%20solving%20real-world%20manipulation%0Atasks%2C%20but%20tactile%20sensors%20for%20manipulation%20have%20barriers%20to%20use%20such%20as%0Afragility%20and%20cost.%20In%20this%20work%2C%20we%20engage%20a%20robust%2C%20low-cost%20tactile%20sensor%2C%0ABeadSight%2C%20as%20an%20alternative%20to%20precise%20pre-calibrated%20sensors%20for%20a%0Apretraining%20approach%20to%20manipulation.%20We%20show%20that%20tactile%20pretraining%2C%20even%0Awith%20a%20low-fidelity%20sensor%20as%20BeadSight%2C%20can%20improve%20an%20imitation%20learning%0Aagent%27s%20performance%20on%20complex%20manipulation%20tasks.%20We%20demonstrate%20this%20method%0Aagainst%20a%20baseline%20USB%20cable%20plugging%20task%2C%20previously%20achieved%20with%20a%20much%0Ahigher%20precision%20GelSight%20sensor%20as%20the%20tactile%20input%20to%20pretraining.%20Our%20best%0ABeadSight%20pretrained%20visuo-tactile%20agent%20completed%20the%20task%20with%2070%5C%25%20accuracy%0Acompared%20to%2085%5C%25%20for%20the%20best%20GelSight%20pretrained%20visuo-tactile%20agent%2C%20with%0Avision-only%20inference%20for%20both.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow%2520Fidelity%2520Visuo-Tactile%2520Pretraining%2520Improves%2520Vision-Only%2520Manipulation%250A%2520%2520Performance%26entry.906535625%3DSelam%2520Gano%2520and%2520Abraham%2520George%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3D%2520%2520Tactile%2520perception%2520is%2520a%2520critical%2520component%2520of%2520solving%2520real-world%2520manipulation%250Atasks%252C%2520but%2520tactile%2520sensors%2520for%2520manipulation%2520have%2520barriers%2520to%2520use%2520such%2520as%250Afragility%2520and%2520cost.%2520In%2520this%2520work%252C%2520we%2520engage%2520a%2520robust%252C%2520low-cost%2520tactile%2520sensor%252C%250ABeadSight%252C%2520as%2520an%2520alternative%2520to%2520precise%2520pre-calibrated%2520sensors%2520for%2520a%250Apretraining%2520approach%2520to%2520manipulation.%2520We%2520show%2520that%2520tactile%2520pretraining%252C%2520even%250Awith%2520a%2520low-fidelity%2520sensor%2520as%2520BeadSight%252C%2520can%2520improve%2520an%2520imitation%2520learning%250Aagent%2527s%2520performance%2520on%2520complex%2520manipulation%2520tasks.%2520We%2520demonstrate%2520this%2520method%250Aagainst%2520a%2520baseline%2520USB%2520cable%2520plugging%2520task%252C%2520previously%2520achieved%2520with%2520a%2520much%250Ahigher%2520precision%2520GelSight%2520sensor%2520as%2520the%2520tactile%2520input%2520to%2520pretraining.%2520Our%2520best%250ABeadSight%2520pretrained%2520visuo-tactile%2520agent%2520completed%2520the%2520task%2520with%252070%255C%2525%2520accuracy%250Acompared%2520to%252085%255C%2525%2520for%2520the%2520best%2520GelSight%2520pretrained%2520visuo-tactile%2520agent%252C%2520with%250Avision-only%2520inference%2520for%2520both.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Fidelity%20Visuo-Tactile%20Pretraining%20Improves%20Vision-Only%20Manipulation%0A%20%20Performance&entry.906535625=Selam%20Gano%20and%20Abraham%20George%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20Tactile%20perception%20is%20a%20critical%20component%20of%20solving%20real-world%20manipulation%0Atasks%2C%20but%20tactile%20sensors%20for%20manipulation%20have%20barriers%20to%20use%20such%20as%0Afragility%20and%20cost.%20In%20this%20work%2C%20we%20engage%20a%20robust%2C%20low-cost%20tactile%20sensor%2C%0ABeadSight%2C%20as%20an%20alternative%20to%20precise%20pre-calibrated%20sensors%20for%20a%0Apretraining%20approach%20to%20manipulation.%20We%20show%20that%20tactile%20pretraining%2C%20even%0Awith%20a%20low-fidelity%20sensor%20as%20BeadSight%2C%20can%20improve%20an%20imitation%20learning%0Aagent%27s%20performance%20on%20complex%20manipulation%20tasks.%20We%20demonstrate%20this%20method%0Aagainst%20a%20baseline%20USB%20cable%20plugging%20task%2C%20previously%20achieved%20with%20a%20much%0Ahigher%20precision%20GelSight%20sensor%20as%20the%20tactile%20input%20to%20pretraining.%20Our%20best%0ABeadSight%20pretrained%20visuo-tactile%20agent%20completed%20the%20task%20with%2070%5C%25%20accuracy%0Acompared%20to%2085%5C%25%20for%20the%20best%20GelSight%20pretrained%20visuo-tactile%20agent%2C%20with%0Avision-only%20inference%20for%20both.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15639v2&entry.124074799=Read"},
{"title": "A Data-Centric Approach To Generate Faithful and High Quality Patient\n  Summaries with Large Language Models", "author": "Stefan Hegselmann and Shannon Zejiang Shen and Florian Gierse and Monica Agrawal and David Sontag and Xiaoyi Jiang", "abstract": "  Patients often face difficulties in understanding their hospitalizations,\nwhile healthcare workers have limited resources to provide explanations. In\nthis work, we investigate the potential of large language models to generate\npatient summaries based on doctors' notes and study the effect of training data\non the faithfulness and quality of the generated summaries. To this end, we\nrelease (i) a rigorous labeling protocol for errors in medical texts and (ii) a\npublicly available dataset of annotated hallucinations in 100 doctor-written\nand 100 generated summaries. We show that fine-tuning on hallucination-free\ndata effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama\n2, while preserving relevant information. We observe a similar effect on GPT-4\n(0.70 to 0.40), when the few-shot examples are hallucination-free. We also\nconduct a qualitative evaluation using hallucination-free and improved training\ndata. We find that common quantitative metrics do not correlate well with\nfaithfulness and quality. Finally, we test GPT-4 for automatic hallucination\ndetection, which clearly outperforms common baselines.\n", "link": "http://arxiv.org/abs/2402.15422v2", "date": "2024-06-25", "relevancy": 1.4284, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4687}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Data-Centric%20Approach%20To%20Generate%20Faithful%20and%20High%20Quality%20Patient%0A%20%20Summaries%20with%20Large%20Language%20Models&body=Title%3A%20A%20Data-Centric%20Approach%20To%20Generate%20Faithful%20and%20High%20Quality%20Patient%0A%20%20Summaries%20with%20Large%20Language%20Models%0AAuthor%3A%20Stefan%20Hegselmann%20and%20Shannon%20Zejiang%20Shen%20and%20Florian%20Gierse%20and%20Monica%20Agrawal%20and%20David%20Sontag%20and%20Xiaoyi%20Jiang%0AAbstract%3A%20%20%20Patients%20often%20face%20difficulties%20in%20understanding%20their%20hospitalizations%2C%0Awhile%20healthcare%20workers%20have%20limited%20resources%20to%20provide%20explanations.%20In%0Athis%20work%2C%20we%20investigate%20the%20potential%20of%20large%20language%20models%20to%20generate%0Apatient%20summaries%20based%20on%20doctors%27%20notes%20and%20study%20the%20effect%20of%20training%20data%0Aon%20the%20faithfulness%20and%20quality%20of%20the%20generated%20summaries.%20To%20this%20end%2C%20we%0Arelease%20%28i%29%20a%20rigorous%20labeling%20protocol%20for%20errors%20in%20medical%20texts%20and%20%28ii%29%20a%0Apublicly%20available%20dataset%20of%20annotated%20hallucinations%20in%20100%20doctor-written%0Aand%20100%20generated%20summaries.%20We%20show%20that%20fine-tuning%20on%20hallucination-free%0Adata%20effectively%20reduces%20hallucinations%20from%202.60%20to%201.55%20per%20summary%20for%20Llama%0A2%2C%20while%20preserving%20relevant%20information.%20We%20observe%20a%20similar%20effect%20on%20GPT-4%0A%280.70%20to%200.40%29%2C%20when%20the%20few-shot%20examples%20are%20hallucination-free.%20We%20also%0Aconduct%20a%20qualitative%20evaluation%20using%20hallucination-free%20and%20improved%20training%0Adata.%20We%20find%20that%20common%20quantitative%20metrics%20do%20not%20correlate%20well%20with%0Afaithfulness%20and%20quality.%20Finally%2C%20we%20test%20GPT-4%20for%20automatic%20hallucination%0Adetection%2C%20which%20clearly%20outperforms%20common%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Data-Centric%2520Approach%2520To%2520Generate%2520Faithful%2520and%2520High%2520Quality%2520Patient%250A%2520%2520Summaries%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DStefan%2520Hegselmann%2520and%2520Shannon%2520Zejiang%2520Shen%2520and%2520Florian%2520Gierse%2520and%2520Monica%2520Agrawal%2520and%2520David%2520Sontag%2520and%2520Xiaoyi%2520Jiang%26entry.1292438233%3D%2520%2520Patients%2520often%2520face%2520difficulties%2520in%2520understanding%2520their%2520hospitalizations%252C%250Awhile%2520healthcare%2520workers%2520have%2520limited%2520resources%2520to%2520provide%2520explanations.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520the%2520potential%2520of%2520large%2520language%2520models%2520to%2520generate%250Apatient%2520summaries%2520based%2520on%2520doctors%2527%2520notes%2520and%2520study%2520the%2520effect%2520of%2520training%2520data%250Aon%2520the%2520faithfulness%2520and%2520quality%2520of%2520the%2520generated%2520summaries.%2520To%2520this%2520end%252C%2520we%250Arelease%2520%2528i%2529%2520a%2520rigorous%2520labeling%2520protocol%2520for%2520errors%2520in%2520medical%2520texts%2520and%2520%2528ii%2529%2520a%250Apublicly%2520available%2520dataset%2520of%2520annotated%2520hallucinations%2520in%2520100%2520doctor-written%250Aand%2520100%2520generated%2520summaries.%2520We%2520show%2520that%2520fine-tuning%2520on%2520hallucination-free%250Adata%2520effectively%2520reduces%2520hallucinations%2520from%25202.60%2520to%25201.55%2520per%2520summary%2520for%2520Llama%250A2%252C%2520while%2520preserving%2520relevant%2520information.%2520We%2520observe%2520a%2520similar%2520effect%2520on%2520GPT-4%250A%25280.70%2520to%25200.40%2529%252C%2520when%2520the%2520few-shot%2520examples%2520are%2520hallucination-free.%2520We%2520also%250Aconduct%2520a%2520qualitative%2520evaluation%2520using%2520hallucination-free%2520and%2520improved%2520training%250Adata.%2520We%2520find%2520that%2520common%2520quantitative%2520metrics%2520do%2520not%2520correlate%2520well%2520with%250Afaithfulness%2520and%2520quality.%2520Finally%252C%2520we%2520test%2520GPT-4%2520for%2520automatic%2520hallucination%250Adetection%252C%2520which%2520clearly%2520outperforms%2520common%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Data-Centric%20Approach%20To%20Generate%20Faithful%20and%20High%20Quality%20Patient%0A%20%20Summaries%20with%20Large%20Language%20Models&entry.906535625=Stefan%20Hegselmann%20and%20Shannon%20Zejiang%20Shen%20and%20Florian%20Gierse%20and%20Monica%20Agrawal%20and%20David%20Sontag%20and%20Xiaoyi%20Jiang&entry.1292438233=%20%20Patients%20often%20face%20difficulties%20in%20understanding%20their%20hospitalizations%2C%0Awhile%20healthcare%20workers%20have%20limited%20resources%20to%20provide%20explanations.%20In%0Athis%20work%2C%20we%20investigate%20the%20potential%20of%20large%20language%20models%20to%20generate%0Apatient%20summaries%20based%20on%20doctors%27%20notes%20and%20study%20the%20effect%20of%20training%20data%0Aon%20the%20faithfulness%20and%20quality%20of%20the%20generated%20summaries.%20To%20this%20end%2C%20we%0Arelease%20%28i%29%20a%20rigorous%20labeling%20protocol%20for%20errors%20in%20medical%20texts%20and%20%28ii%29%20a%0Apublicly%20available%20dataset%20of%20annotated%20hallucinations%20in%20100%20doctor-written%0Aand%20100%20generated%20summaries.%20We%20show%20that%20fine-tuning%20on%20hallucination-free%0Adata%20effectively%20reduces%20hallucinations%20from%202.60%20to%201.55%20per%20summary%20for%20Llama%0A2%2C%20while%20preserving%20relevant%20information.%20We%20observe%20a%20similar%20effect%20on%20GPT-4%0A%280.70%20to%200.40%29%2C%20when%20the%20few-shot%20examples%20are%20hallucination-free.%20We%20also%0Aconduct%20a%20qualitative%20evaluation%20using%20hallucination-free%20and%20improved%20training%0Adata.%20We%20find%20that%20common%20quantitative%20metrics%20do%20not%20correlate%20well%20with%0Afaithfulness%20and%20quality.%20Finally%2C%20we%20test%20GPT-4%20for%20automatic%20hallucination%0Adetection%2C%20which%20clearly%20outperforms%20common%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15422v2&entry.124074799=Read"},
{"title": "Solving Hard Mizar Problems with Instantiation and Strategy Invention", "author": "Jan Jakub\u016fv and Mikol\u00e1\u0161 Janota and Josef Urban", "abstract": "  In this work, we prove over 3000 previously ATP-unproved Mizar/MPTP problems\nby using several ATP and AI methods, raising the number of ATP-solved Mizar\nproblems from 75\\% to above 80\\%. First, we start to experiment with the cvc5\nSMT solver which uses several instantiation-based heuristics that differ from\nthe superposition-based systems, that were previously applied to Mizar,and add\nmany new solutions. Then we use automated strategy invention to develop cvc5\nstrategies that largely improve cvc5's performance on the hard problems. In\nparticular, the best invented strategy solves over 14\\% more problems than the\nbest previously available cvc5 strategy. We also show that different\nclausification methods have a high impact on such instantiation-based methods,\nagain producing many new solutions. In total, the methods solve 3021 (21.3\\%)\nof the 14163 previously unsolved hard Mizar problems. This is a new milestone\nover the Mizar large-theory benchmark and a large strengthening of the hammer\nmethods for Mizar.\n", "link": "http://arxiv.org/abs/2406.17762v1", "date": "2024-06-25", "relevancy": 1.5502, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3891}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Hard%20Mizar%20Problems%20with%20Instantiation%20and%20Strategy%20Invention&body=Title%3A%20Solving%20Hard%20Mizar%20Problems%20with%20Instantiation%20and%20Strategy%20Invention%0AAuthor%3A%20Jan%20Jakub%C5%AFv%20and%20Mikol%C3%A1%C5%A1%20Janota%20and%20Josef%20Urban%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20prove%20over%203000%20previously%20ATP-unproved%20Mizar/MPTP%20problems%0Aby%20using%20several%20ATP%20and%20AI%20methods%2C%20raising%20the%20number%20of%20ATP-solved%20Mizar%0Aproblems%20from%2075%5C%25%20to%20above%2080%5C%25.%20First%2C%20we%20start%20to%20experiment%20with%20the%20cvc5%0ASMT%20solver%20which%20uses%20several%20instantiation-based%20heuristics%20that%20differ%20from%0Athe%20superposition-based%20systems%2C%20that%20were%20previously%20applied%20to%20Mizar%2Cand%20add%0Amany%20new%20solutions.%20Then%20we%20use%20automated%20strategy%20invention%20to%20develop%20cvc5%0Astrategies%20that%20largely%20improve%20cvc5%27s%20performance%20on%20the%20hard%20problems.%20In%0Aparticular%2C%20the%20best%20invented%20strategy%20solves%20over%2014%5C%25%20more%20problems%20than%20the%0Abest%20previously%20available%20cvc5%20strategy.%20We%20also%20show%20that%20different%0Aclausification%20methods%20have%20a%20high%20impact%20on%20such%20instantiation-based%20methods%2C%0Aagain%20producing%20many%20new%20solutions.%20In%20total%2C%20the%20methods%20solve%203021%20%2821.3%5C%25%29%0Aof%20the%2014163%20previously%20unsolved%20hard%20Mizar%20problems.%20This%20is%20a%20new%20milestone%0Aover%20the%20Mizar%20large-theory%20benchmark%20and%20a%20large%20strengthening%20of%20the%20hammer%0Amethods%20for%20Mizar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Hard%2520Mizar%2520Problems%2520with%2520Instantiation%2520and%2520Strategy%2520Invention%26entry.906535625%3DJan%2520Jakub%25C5%25AFv%2520and%2520Mikol%25C3%25A1%25C5%25A1%2520Janota%2520and%2520Josef%2520Urban%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520prove%2520over%25203000%2520previously%2520ATP-unproved%2520Mizar/MPTP%2520problems%250Aby%2520using%2520several%2520ATP%2520and%2520AI%2520methods%252C%2520raising%2520the%2520number%2520of%2520ATP-solved%2520Mizar%250Aproblems%2520from%252075%255C%2525%2520to%2520above%252080%255C%2525.%2520First%252C%2520we%2520start%2520to%2520experiment%2520with%2520the%2520cvc5%250ASMT%2520solver%2520which%2520uses%2520several%2520instantiation-based%2520heuristics%2520that%2520differ%2520from%250Athe%2520superposition-based%2520systems%252C%2520that%2520were%2520previously%2520applied%2520to%2520Mizar%252Cand%2520add%250Amany%2520new%2520solutions.%2520Then%2520we%2520use%2520automated%2520strategy%2520invention%2520to%2520develop%2520cvc5%250Astrategies%2520that%2520largely%2520improve%2520cvc5%2527s%2520performance%2520on%2520the%2520hard%2520problems.%2520In%250Aparticular%252C%2520the%2520best%2520invented%2520strategy%2520solves%2520over%252014%255C%2525%2520more%2520problems%2520than%2520the%250Abest%2520previously%2520available%2520cvc5%2520strategy.%2520We%2520also%2520show%2520that%2520different%250Aclausification%2520methods%2520have%2520a%2520high%2520impact%2520on%2520such%2520instantiation-based%2520methods%252C%250Aagain%2520producing%2520many%2520new%2520solutions.%2520In%2520total%252C%2520the%2520methods%2520solve%25203021%2520%252821.3%255C%2525%2529%250Aof%2520the%252014163%2520previously%2520unsolved%2520hard%2520Mizar%2520problems.%2520This%2520is%2520a%2520new%2520milestone%250Aover%2520the%2520Mizar%2520large-theory%2520benchmark%2520and%2520a%2520large%2520strengthening%2520of%2520the%2520hammer%250Amethods%2520for%2520Mizar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Hard%20Mizar%20Problems%20with%20Instantiation%20and%20Strategy%20Invention&entry.906535625=Jan%20Jakub%C5%AFv%20and%20Mikol%C3%A1%C5%A1%20Janota%20and%20Josef%20Urban&entry.1292438233=%20%20In%20this%20work%2C%20we%20prove%20over%203000%20previously%20ATP-unproved%20Mizar/MPTP%20problems%0Aby%20using%20several%20ATP%20and%20AI%20methods%2C%20raising%20the%20number%20of%20ATP-solved%20Mizar%0Aproblems%20from%2075%5C%25%20to%20above%2080%5C%25.%20First%2C%20we%20start%20to%20experiment%20with%20the%20cvc5%0ASMT%20solver%20which%20uses%20several%20instantiation-based%20heuristics%20that%20differ%20from%0Athe%20superposition-based%20systems%2C%20that%20were%20previously%20applied%20to%20Mizar%2Cand%20add%0Amany%20new%20solutions.%20Then%20we%20use%20automated%20strategy%20invention%20to%20develop%20cvc5%0Astrategies%20that%20largely%20improve%20cvc5%27s%20performance%20on%20the%20hard%20problems.%20In%0Aparticular%2C%20the%20best%20invented%20strategy%20solves%20over%2014%5C%25%20more%20problems%20than%20the%0Abest%20previously%20available%20cvc5%20strategy.%20We%20also%20show%20that%20different%0Aclausification%20methods%20have%20a%20high%20impact%20on%20such%20instantiation-based%20methods%2C%0Aagain%20producing%20many%20new%20solutions.%20In%20total%2C%20the%20methods%20solve%203021%20%2821.3%5C%25%29%0Aof%20the%2014163%20previously%20unsolved%20hard%20Mizar%20problems.%20This%20is%20a%20new%20milestone%0Aover%20the%20Mizar%20large-theory%20benchmark%20and%20a%20large%20strengthening%20of%20the%20hammer%0Amethods%20for%20Mizar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17762v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


