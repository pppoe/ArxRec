<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240815.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian\n  Splatting", "author": "Huapeng Li and Wenxuan Song and Tianao Xu and Alexandre Elsig and Jonas Kulhanek", "abstract": "  The underwater 3D scene reconstruction is a challenging, yet interesting\nproblem with applications ranging from naval robots to VR experiences. The\nproblem was successfully tackled by fully volumetric NeRF-based methods which\ncan model both the geometry and the medium (water). Unfortunately, these\nmethods are slow to train and do not offer real-time rendering. More recently,\n3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs.\nHowever, because it is an explicit method that renders only the geometry, it\ncannot render the medium and is therefore unsuited for underwater\nreconstruction. Therefore, we propose a novel approach that fuses volumetric\nrendering with 3DGS to handle underwater data effectively. Our method employs\n3DGS for explicit geometry representation and a separate volumetric field\n(queried once per pixel) for capturing the scattering medium. This dual\nrepresentation further allows the restoration of the scenes by removing the\nscattering medium. Our method outperforms state-of-the-art NeRF-based methods\nin rendering quality on the underwater SeaThru-NeRF dataset. Furthermore, it\ndoes so while offering real-time rendering performance, addressing the\nefficiency limitations of existing methods. Web:\nhttps://water-splatting.github.io\n", "link": "http://arxiv.org/abs/2408.08206v1", "date": "2024-08-15", "relevancy": 3.1661, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7086}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6238}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WaterSplatting%3A%20Fast%20Underwater%203D%20Scene%20Reconstruction%20Using%20Gaussian%0A%20%20Splatting&body=Title%3A%20WaterSplatting%3A%20Fast%20Underwater%203D%20Scene%20Reconstruction%20Using%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Huapeng%20Li%20and%20Wenxuan%20Song%20and%20Tianao%20Xu%20and%20Alexandre%20Elsig%20and%20Jonas%20Kulhanek%0AAbstract%3A%20%20%20The%20underwater%203D%20scene%20reconstruction%20is%20a%20challenging%2C%20yet%20interesting%0Aproblem%20with%20applications%20ranging%20from%20naval%20robots%20to%20VR%20experiences.%20The%0Aproblem%20was%20successfully%20tackled%20by%20fully%20volumetric%20NeRF-based%20methods%20which%0Acan%20model%20both%20the%20geometry%20and%20the%20medium%20%28water%29.%20Unfortunately%2C%20these%0Amethods%20are%20slow%20to%20train%20and%20do%20not%20offer%20real-time%20rendering.%20More%20recently%2C%0A3D%20Gaussian%20Splatting%20%283DGS%29%20method%20offered%20a%20fast%20alternative%20to%20NeRFs.%0AHowever%2C%20because%20it%20is%20an%20explicit%20method%20that%20renders%20only%20the%20geometry%2C%20it%0Acannot%20render%20the%20medium%20and%20is%20therefore%20unsuited%20for%20underwater%0Areconstruction.%20Therefore%2C%20we%20propose%20a%20novel%20approach%20that%20fuses%20volumetric%0Arendering%20with%203DGS%20to%20handle%20underwater%20data%20effectively.%20Our%20method%20employs%0A3DGS%20for%20explicit%20geometry%20representation%20and%20a%20separate%20volumetric%20field%0A%28queried%20once%20per%20pixel%29%20for%20capturing%20the%20scattering%20medium.%20This%20dual%0Arepresentation%20further%20allows%20the%20restoration%20of%20the%20scenes%20by%20removing%20the%0Ascattering%20medium.%20Our%20method%20outperforms%20state-of-the-art%20NeRF-based%20methods%0Ain%20rendering%20quality%20on%20the%20underwater%20SeaThru-NeRF%20dataset.%20Furthermore%2C%20it%0Adoes%20so%20while%20offering%20real-time%20rendering%20performance%2C%20addressing%20the%0Aefficiency%20limitations%20of%20existing%20methods.%20Web%3A%0Ahttps%3A//water-splatting.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaterSplatting%253A%2520Fast%2520Underwater%25203D%2520Scene%2520Reconstruction%2520Using%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DHuapeng%2520Li%2520and%2520Wenxuan%2520Song%2520and%2520Tianao%2520Xu%2520and%2520Alexandre%2520Elsig%2520and%2520Jonas%2520Kulhanek%26entry.1292438233%3D%2520%2520The%2520underwater%25203D%2520scene%2520reconstruction%2520is%2520a%2520challenging%252C%2520yet%2520interesting%250Aproblem%2520with%2520applications%2520ranging%2520from%2520naval%2520robots%2520to%2520VR%2520experiences.%2520The%250Aproblem%2520was%2520successfully%2520tackled%2520by%2520fully%2520volumetric%2520NeRF-based%2520methods%2520which%250Acan%2520model%2520both%2520the%2520geometry%2520and%2520the%2520medium%2520%2528water%2529.%2520Unfortunately%252C%2520these%250Amethods%2520are%2520slow%2520to%2520train%2520and%2520do%2520not%2520offer%2520real-time%2520rendering.%2520More%2520recently%252C%250A3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520method%2520offered%2520a%2520fast%2520alternative%2520to%2520NeRFs.%250AHowever%252C%2520because%2520it%2520is%2520an%2520explicit%2520method%2520that%2520renders%2520only%2520the%2520geometry%252C%2520it%250Acannot%2520render%2520the%2520medium%2520and%2520is%2520therefore%2520unsuited%2520for%2520underwater%250Areconstruction.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520fuses%2520volumetric%250Arendering%2520with%25203DGS%2520to%2520handle%2520underwater%2520data%2520effectively.%2520Our%2520method%2520employs%250A3DGS%2520for%2520explicit%2520geometry%2520representation%2520and%2520a%2520separate%2520volumetric%2520field%250A%2528queried%2520once%2520per%2520pixel%2529%2520for%2520capturing%2520the%2520scattering%2520medium.%2520This%2520dual%250Arepresentation%2520further%2520allows%2520the%2520restoration%2520of%2520the%2520scenes%2520by%2520removing%2520the%250Ascattering%2520medium.%2520Our%2520method%2520outperforms%2520state-of-the-art%2520NeRF-based%2520methods%250Ain%2520rendering%2520quality%2520on%2520the%2520underwater%2520SeaThru-NeRF%2520dataset.%2520Furthermore%252C%2520it%250Adoes%2520so%2520while%2520offering%2520real-time%2520rendering%2520performance%252C%2520addressing%2520the%250Aefficiency%2520limitations%2520of%2520existing%2520methods.%2520Web%253A%250Ahttps%253A//water-splatting.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaterSplatting%3A%20Fast%20Underwater%203D%20Scene%20Reconstruction%20Using%20Gaussian%0A%20%20Splatting&entry.906535625=Huapeng%20Li%20and%20Wenxuan%20Song%20and%20Tianao%20Xu%20and%20Alexandre%20Elsig%20and%20Jonas%20Kulhanek&entry.1292438233=%20%20The%20underwater%203D%20scene%20reconstruction%20is%20a%20challenging%2C%20yet%20interesting%0Aproblem%20with%20applications%20ranging%20from%20naval%20robots%20to%20VR%20experiences.%20The%0Aproblem%20was%20successfully%20tackled%20by%20fully%20volumetric%20NeRF-based%20methods%20which%0Acan%20model%20both%20the%20geometry%20and%20the%20medium%20%28water%29.%20Unfortunately%2C%20these%0Amethods%20are%20slow%20to%20train%20and%20do%20not%20offer%20real-time%20rendering.%20More%20recently%2C%0A3D%20Gaussian%20Splatting%20%283DGS%29%20method%20offered%20a%20fast%20alternative%20to%20NeRFs.%0AHowever%2C%20because%20it%20is%20an%20explicit%20method%20that%20renders%20only%20the%20geometry%2C%20it%0Acannot%20render%20the%20medium%20and%20is%20therefore%20unsuited%20for%20underwater%0Areconstruction.%20Therefore%2C%20we%20propose%20a%20novel%20approach%20that%20fuses%20volumetric%0Arendering%20with%203DGS%20to%20handle%20underwater%20data%20effectively.%20Our%20method%20employs%0A3DGS%20for%20explicit%20geometry%20representation%20and%20a%20separate%20volumetric%20field%0A%28queried%20once%20per%20pixel%29%20for%20capturing%20the%20scattering%20medium.%20This%20dual%0Arepresentation%20further%20allows%20the%20restoration%20of%20the%20scenes%20by%20removing%20the%0Ascattering%20medium.%20Our%20method%20outperforms%20state-of-the-art%20NeRF-based%20methods%0Ain%20rendering%20quality%20on%20the%20underwater%20SeaThru-NeRF%20dataset.%20Furthermore%2C%20it%0Adoes%20so%20while%20offering%20real-time%20rendering%20performance%2C%20addressing%20the%0Aefficiency%20limitations%20of%20existing%20methods.%20Web%3A%0Ahttps%3A//water-splatting.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08206v1&entry.124074799=Read"},
{"title": "Masked Generative Extractor for Synergistic Representation and 3D\n  Generation of Point Clouds", "author": "Hongliang Zeng and Ping Zhang and Fang Li and Jiahua Wang and Tingyu Ye and Pengteng Guo", "abstract": "  Representation and generative learning, as reconstruction-based methods, have\ndemonstrated their potential for mutual reinforcement across various domains.\nIn the field of point cloud processing, although existing studies have adopted\ntraining strategies from generative models to enhance representational\ncapabilities, these methods are limited by their inability to genuinely\ngenerate 3D shapes. To explore the benefits of deeply integrating 3D\nrepresentation learning and generative learning, we propose an innovative\nframework called \\textit{Point-MGE}. Specifically, this framework first\nutilizes a vector quantized variational autoencoder to reconstruct a neural\nfield representation of 3D shapes, thereby learning discrete semantic features\nof point patches. Subsequently, we design a sliding masking ratios to smooth\nthe transition from representation learning to generative learning. Moreover,\nour method demonstrates strong generalization capability in learning\nhigh-capacity models, achieving new state-of-the-art performance across\nmultiple downstream tasks. In shape classification, Point-MGE achieved an\naccuracy of 94.2% (+1.0%) on the ModelNet40 dataset and 92.9% (+5.5%) on the\nScanObjectNN dataset. Experimental results also confirmed that Point-MGE can\ngenerate high-quality 3D shapes in both unconditional and conditional settings.\n", "link": "http://arxiv.org/abs/2406.17342v2", "date": "2024-08-15", "relevancy": 3.0225, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6213}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5977}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Generative%20Extractor%20for%20Synergistic%20Representation%20and%203D%0A%20%20Generation%20of%20Point%20Clouds&body=Title%3A%20Masked%20Generative%20Extractor%20for%20Synergistic%20Representation%20and%203D%0A%20%20Generation%20of%20Point%20Clouds%0AAuthor%3A%20Hongliang%20Zeng%20and%20Ping%20Zhang%20and%20Fang%20Li%20and%20Jiahua%20Wang%20and%20Tingyu%20Ye%20and%20Pengteng%20Guo%0AAbstract%3A%20%20%20Representation%20and%20generative%20learning%2C%20as%20reconstruction-based%20methods%2C%20have%0Ademonstrated%20their%20potential%20for%20mutual%20reinforcement%20across%20various%20domains.%0AIn%20the%20field%20of%20point%20cloud%20processing%2C%20although%20existing%20studies%20have%20adopted%0Atraining%20strategies%20from%20generative%20models%20to%20enhance%20representational%0Acapabilities%2C%20these%20methods%20are%20limited%20by%20their%20inability%20to%20genuinely%0Agenerate%203D%20shapes.%20To%20explore%20the%20benefits%20of%20deeply%20integrating%203D%0Arepresentation%20learning%20and%20generative%20learning%2C%20we%20propose%20an%20innovative%0Aframework%20called%20%5Ctextit%7BPoint-MGE%7D.%20Specifically%2C%20this%20framework%20first%0Autilizes%20a%20vector%20quantized%20variational%20autoencoder%20to%20reconstruct%20a%20neural%0Afield%20representation%20of%203D%20shapes%2C%20thereby%20learning%20discrete%20semantic%20features%0Aof%20point%20patches.%20Subsequently%2C%20we%20design%20a%20sliding%20masking%20ratios%20to%20smooth%0Athe%20transition%20from%20representation%20learning%20to%20generative%20learning.%20Moreover%2C%0Aour%20method%20demonstrates%20strong%20generalization%20capability%20in%20learning%0Ahigh-capacity%20models%2C%20achieving%20new%20state-of-the-art%20performance%20across%0Amultiple%20downstream%20tasks.%20In%20shape%20classification%2C%20Point-MGE%20achieved%20an%0Aaccuracy%20of%2094.2%25%20%28%2B1.0%25%29%20on%20the%20ModelNet40%20dataset%20and%2092.9%25%20%28%2B5.5%25%29%20on%20the%0AScanObjectNN%20dataset.%20Experimental%20results%20also%20confirmed%20that%20Point-MGE%20can%0Agenerate%20high-quality%203D%20shapes%20in%20both%20unconditional%20and%20conditional%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Generative%2520Extractor%2520for%2520Synergistic%2520Representation%2520and%25203D%250A%2520%2520Generation%2520of%2520Point%2520Clouds%26entry.906535625%3DHongliang%2520Zeng%2520and%2520Ping%2520Zhang%2520and%2520Fang%2520Li%2520and%2520Jiahua%2520Wang%2520and%2520Tingyu%2520Ye%2520and%2520Pengteng%2520Guo%26entry.1292438233%3D%2520%2520Representation%2520and%2520generative%2520learning%252C%2520as%2520reconstruction-based%2520methods%252C%2520have%250Ademonstrated%2520their%2520potential%2520for%2520mutual%2520reinforcement%2520across%2520various%2520domains.%250AIn%2520the%2520field%2520of%2520point%2520cloud%2520processing%252C%2520although%2520existing%2520studies%2520have%2520adopted%250Atraining%2520strategies%2520from%2520generative%2520models%2520to%2520enhance%2520representational%250Acapabilities%252C%2520these%2520methods%2520are%2520limited%2520by%2520their%2520inability%2520to%2520genuinely%250Agenerate%25203D%2520shapes.%2520To%2520explore%2520the%2520benefits%2520of%2520deeply%2520integrating%25203D%250Arepresentation%2520learning%2520and%2520generative%2520learning%252C%2520we%2520propose%2520an%2520innovative%250Aframework%2520called%2520%255Ctextit%257BPoint-MGE%257D.%2520Specifically%252C%2520this%2520framework%2520first%250Autilizes%2520a%2520vector%2520quantized%2520variational%2520autoencoder%2520to%2520reconstruct%2520a%2520neural%250Afield%2520representation%2520of%25203D%2520shapes%252C%2520thereby%2520learning%2520discrete%2520semantic%2520features%250Aof%2520point%2520patches.%2520Subsequently%252C%2520we%2520design%2520a%2520sliding%2520masking%2520ratios%2520to%2520smooth%250Athe%2520transition%2520from%2520representation%2520learning%2520to%2520generative%2520learning.%2520Moreover%252C%250Aour%2520method%2520demonstrates%2520strong%2520generalization%2520capability%2520in%2520learning%250Ahigh-capacity%2520models%252C%2520achieving%2520new%2520state-of-the-art%2520performance%2520across%250Amultiple%2520downstream%2520tasks.%2520In%2520shape%2520classification%252C%2520Point-MGE%2520achieved%2520an%250Aaccuracy%2520of%252094.2%2525%2520%2528%252B1.0%2525%2529%2520on%2520the%2520ModelNet40%2520dataset%2520and%252092.9%2525%2520%2528%252B5.5%2525%2529%2520on%2520the%250AScanObjectNN%2520dataset.%2520Experimental%2520results%2520also%2520confirmed%2520that%2520Point-MGE%2520can%250Agenerate%2520high-quality%25203D%2520shapes%2520in%2520both%2520unconditional%2520and%2520conditional%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Generative%20Extractor%20for%20Synergistic%20Representation%20and%203D%0A%20%20Generation%20of%20Point%20Clouds&entry.906535625=Hongliang%20Zeng%20and%20Ping%20Zhang%20and%20Fang%20Li%20and%20Jiahua%20Wang%20and%20Tingyu%20Ye%20and%20Pengteng%20Guo&entry.1292438233=%20%20Representation%20and%20generative%20learning%2C%20as%20reconstruction-based%20methods%2C%20have%0Ademonstrated%20their%20potential%20for%20mutual%20reinforcement%20across%20various%20domains.%0AIn%20the%20field%20of%20point%20cloud%20processing%2C%20although%20existing%20studies%20have%20adopted%0Atraining%20strategies%20from%20generative%20models%20to%20enhance%20representational%0Acapabilities%2C%20these%20methods%20are%20limited%20by%20their%20inability%20to%20genuinely%0Agenerate%203D%20shapes.%20To%20explore%20the%20benefits%20of%20deeply%20integrating%203D%0Arepresentation%20learning%20and%20generative%20learning%2C%20we%20propose%20an%20innovative%0Aframework%20called%20%5Ctextit%7BPoint-MGE%7D.%20Specifically%2C%20this%20framework%20first%0Autilizes%20a%20vector%20quantized%20variational%20autoencoder%20to%20reconstruct%20a%20neural%0Afield%20representation%20of%203D%20shapes%2C%20thereby%20learning%20discrete%20semantic%20features%0Aof%20point%20patches.%20Subsequently%2C%20we%20design%20a%20sliding%20masking%20ratios%20to%20smooth%0Athe%20transition%20from%20representation%20learning%20to%20generative%20learning.%20Moreover%2C%0Aour%20method%20demonstrates%20strong%20generalization%20capability%20in%20learning%0Ahigh-capacity%20models%2C%20achieving%20new%20state-of-the-art%20performance%20across%0Amultiple%20downstream%20tasks.%20In%20shape%20classification%2C%20Point-MGE%20achieved%20an%0Aaccuracy%20of%2094.2%25%20%28%2B1.0%25%29%20on%20the%20ModelNet40%20dataset%20and%2092.9%25%20%28%2B5.5%25%29%20on%20the%0AScanObjectNN%20dataset.%20Experimental%20results%20also%20confirmed%20that%20Point-MGE%20can%0Agenerate%20high-quality%203D%20shapes%20in%20both%20unconditional%20and%20conditional%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17342v2&entry.124074799=Read"},
{"title": "Single-image coherent reconstruction of objects and humans", "author": "Sarthak Batra and Partha P. Chakrabarti and Simon Hadfield and Armin Mustafa", "abstract": "  Existing methods for reconstructing objects and humans from a monocular image\nsuffer from severe mesh collisions and performance limitations for interacting\noccluding objects. This paper introduces a method to obtain a globally\nconsistent 3D reconstruction of interacting objects and people from a single\nimage. Our contributions include: 1) an optimization framework, featuring a\ncollision loss, tailored to handle human-object and human-human interactions,\nensuring spatially coherent scene reconstruction; and 2) a novel technique to\nrobustly estimate 6 degrees of freedom (DOF) poses, specifically for heavily\noccluded objects, exploiting image inpainting. Notably, our proposed method\noperates effectively on images from real-world scenarios, without necessitating\nscene or object-level 3D supervision. Extensive qualitative and quantitative\nevaluation against existing methods demonstrates a significant reduction in\ncollisions in the final reconstructions of scenes with multiple interacting\nhumans and objects and a more coherent scene reconstruction.\n", "link": "http://arxiv.org/abs/2408.08086v1", "date": "2024-08-15", "relevancy": 2.982, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6008}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5942}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-image%20coherent%20reconstruction%20of%20objects%20and%20humans&body=Title%3A%20Single-image%20coherent%20reconstruction%20of%20objects%20and%20humans%0AAuthor%3A%20Sarthak%20Batra%20and%20Partha%20P.%20Chakrabarti%20and%20Simon%20Hadfield%20and%20Armin%20Mustafa%0AAbstract%3A%20%20%20Existing%20methods%20for%20reconstructing%20objects%20and%20humans%20from%20a%20monocular%20image%0Asuffer%20from%20severe%20mesh%20collisions%20and%20performance%20limitations%20for%20interacting%0Aoccluding%20objects.%20This%20paper%20introduces%20a%20method%20to%20obtain%20a%20globally%0Aconsistent%203D%20reconstruction%20of%20interacting%20objects%20and%20people%20from%20a%20single%0Aimage.%20Our%20contributions%20include%3A%201%29%20an%20optimization%20framework%2C%20featuring%20a%0Acollision%20loss%2C%20tailored%20to%20handle%20human-object%20and%20human-human%20interactions%2C%0Aensuring%20spatially%20coherent%20scene%20reconstruction%3B%20and%202%29%20a%20novel%20technique%20to%0Arobustly%20estimate%206%20degrees%20of%20freedom%20%28DOF%29%20poses%2C%20specifically%20for%20heavily%0Aoccluded%20objects%2C%20exploiting%20image%20inpainting.%20Notably%2C%20our%20proposed%20method%0Aoperates%20effectively%20on%20images%20from%20real-world%20scenarios%2C%20without%20necessitating%0Ascene%20or%20object-level%203D%20supervision.%20Extensive%20qualitative%20and%20quantitative%0Aevaluation%20against%20existing%20methods%20demonstrates%20a%20significant%20reduction%20in%0Acollisions%20in%20the%20final%20reconstructions%20of%20scenes%20with%20multiple%20interacting%0Ahumans%20and%20objects%20and%20a%20more%20coherent%20scene%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-image%2520coherent%2520reconstruction%2520of%2520objects%2520and%2520humans%26entry.906535625%3DSarthak%2520Batra%2520and%2520Partha%2520P.%2520Chakrabarti%2520and%2520Simon%2520Hadfield%2520and%2520Armin%2520Mustafa%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520reconstructing%2520objects%2520and%2520humans%2520from%2520a%2520monocular%2520image%250Asuffer%2520from%2520severe%2520mesh%2520collisions%2520and%2520performance%2520limitations%2520for%2520interacting%250Aoccluding%2520objects.%2520This%2520paper%2520introduces%2520a%2520method%2520to%2520obtain%2520a%2520globally%250Aconsistent%25203D%2520reconstruction%2520of%2520interacting%2520objects%2520and%2520people%2520from%2520a%2520single%250Aimage.%2520Our%2520contributions%2520include%253A%25201%2529%2520an%2520optimization%2520framework%252C%2520featuring%2520a%250Acollision%2520loss%252C%2520tailored%2520to%2520handle%2520human-object%2520and%2520human-human%2520interactions%252C%250Aensuring%2520spatially%2520coherent%2520scene%2520reconstruction%253B%2520and%25202%2529%2520a%2520novel%2520technique%2520to%250Arobustly%2520estimate%25206%2520degrees%2520of%2520freedom%2520%2528DOF%2529%2520poses%252C%2520specifically%2520for%2520heavily%250Aoccluded%2520objects%252C%2520exploiting%2520image%2520inpainting.%2520Notably%252C%2520our%2520proposed%2520method%250Aoperates%2520effectively%2520on%2520images%2520from%2520real-world%2520scenarios%252C%2520without%2520necessitating%250Ascene%2520or%2520object-level%25203D%2520supervision.%2520Extensive%2520qualitative%2520and%2520quantitative%250Aevaluation%2520against%2520existing%2520methods%2520demonstrates%2520a%2520significant%2520reduction%2520in%250Acollisions%2520in%2520the%2520final%2520reconstructions%2520of%2520scenes%2520with%2520multiple%2520interacting%250Ahumans%2520and%2520objects%2520and%2520a%2520more%2520coherent%2520scene%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-image%20coherent%20reconstruction%20of%20objects%20and%20humans&entry.906535625=Sarthak%20Batra%20and%20Partha%20P.%20Chakrabarti%20and%20Simon%20Hadfield%20and%20Armin%20Mustafa&entry.1292438233=%20%20Existing%20methods%20for%20reconstructing%20objects%20and%20humans%20from%20a%20monocular%20image%0Asuffer%20from%20severe%20mesh%20collisions%20and%20performance%20limitations%20for%20interacting%0Aoccluding%20objects.%20This%20paper%20introduces%20a%20method%20to%20obtain%20a%20globally%0Aconsistent%203D%20reconstruction%20of%20interacting%20objects%20and%20people%20from%20a%20single%0Aimage.%20Our%20contributions%20include%3A%201%29%20an%20optimization%20framework%2C%20featuring%20a%0Acollision%20loss%2C%20tailored%20to%20handle%20human-object%20and%20human-human%20interactions%2C%0Aensuring%20spatially%20coherent%20scene%20reconstruction%3B%20and%202%29%20a%20novel%20technique%20to%0Arobustly%20estimate%206%20degrees%20of%20freedom%20%28DOF%29%20poses%2C%20specifically%20for%20heavily%0Aoccluded%20objects%2C%20exploiting%20image%20inpainting.%20Notably%2C%20our%20proposed%20method%0Aoperates%20effectively%20on%20images%20from%20real-world%20scenarios%2C%20without%20necessitating%0Ascene%20or%20object-level%203D%20supervision.%20Extensive%20qualitative%20and%20quantitative%0Aevaluation%20against%20existing%20methods%20demonstrates%20a%20significant%20reduction%20in%0Acollisions%20in%20the%20final%20reconstructions%20of%20scenes%20with%20multiple%20interacting%0Ahumans%20and%20objects%20and%20a%20more%20coherent%20scene%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08086v1&entry.124074799=Read"},
{"title": "DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction", "author": "Yanlong Li and Chamara Madarasingha and Kanchana Thilakarathna", "abstract": "  Point cloud streaming is increasingly getting popular, evolving into the norm\nfor interactive service delivery and the future Metaverse. However, the\nsubstantial volume of data associated with point clouds presents numerous\nchallenges, particularly in terms of high bandwidth consumption and large\nstorage capacity. Despite various solutions proposed thus far, with a focus on\npoint cloud compression, upsampling, and completion, these\nreconstruction-related methods continue to fall short in delivering high\nfidelity point cloud output. As a solution, in DiffPMAE, we propose an\neffective point cloud reconstruction architecture. Inspired by self-supervised\nlearning concepts, we combine Masked Auto-Encoding and Diffusion Model\nmechanism to remotely reconstruct point cloud data. By the nature of this\nreconstruction process, DiffPMAE can be extended to many related downstream\ntasks including point cloud compression, upsampling and completion. Leveraging\nShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the\nperformance of DiffPMAE exceeding many state-of-the-art methods in-terms of\nauto-encoding and downstream tasks considered.\n", "link": "http://arxiv.org/abs/2312.03298v3", "date": "2024-08-15", "relevancy": 2.9562, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.67}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5533}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffPMAE%3A%20Diffusion%20Masked%20Autoencoders%20for%20Point%20Cloud%20Reconstruction&body=Title%3A%20DiffPMAE%3A%20Diffusion%20Masked%20Autoencoders%20for%20Point%20Cloud%20Reconstruction%0AAuthor%3A%20Yanlong%20Li%20and%20Chamara%20Madarasingha%20and%20Kanchana%20Thilakarathna%0AAbstract%3A%20%20%20Point%20cloud%20streaming%20is%20increasingly%20getting%20popular%2C%20evolving%20into%20the%20norm%0Afor%20interactive%20service%20delivery%20and%20the%20future%20Metaverse.%20However%2C%20the%0Asubstantial%20volume%20of%20data%20associated%20with%20point%20clouds%20presents%20numerous%0Achallenges%2C%20particularly%20in%20terms%20of%20high%20bandwidth%20consumption%20and%20large%0Astorage%20capacity.%20Despite%20various%20solutions%20proposed%20thus%20far%2C%20with%20a%20focus%20on%0Apoint%20cloud%20compression%2C%20upsampling%2C%20and%20completion%2C%20these%0Areconstruction-related%20methods%20continue%20to%20fall%20short%20in%20delivering%20high%0Afidelity%20point%20cloud%20output.%20As%20a%20solution%2C%20in%20DiffPMAE%2C%20we%20propose%20an%0Aeffective%20point%20cloud%20reconstruction%20architecture.%20Inspired%20by%20self-supervised%0Alearning%20concepts%2C%20we%20combine%20Masked%20Auto-Encoding%20and%20Diffusion%20Model%0Amechanism%20to%20remotely%20reconstruct%20point%20cloud%20data.%20By%20the%20nature%20of%20this%0Areconstruction%20process%2C%20DiffPMAE%20can%20be%20extended%20to%20many%20related%20downstream%0Atasks%20including%20point%20cloud%20compression%2C%20upsampling%20and%20completion.%20Leveraging%0AShapeNet-55%20and%20ModelNet%20datasets%20with%20over%2060000%20objects%2C%20we%20validate%20the%0Aperformance%20of%20DiffPMAE%20exceeding%20many%20state-of-the-art%20methods%20in-terms%20of%0Aauto-encoding%20and%20downstream%20tasks%20considered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03298v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffPMAE%253A%2520Diffusion%2520Masked%2520Autoencoders%2520for%2520Point%2520Cloud%2520Reconstruction%26entry.906535625%3DYanlong%2520Li%2520and%2520Chamara%2520Madarasingha%2520and%2520Kanchana%2520Thilakarathna%26entry.1292438233%3D%2520%2520Point%2520cloud%2520streaming%2520is%2520increasingly%2520getting%2520popular%252C%2520evolving%2520into%2520the%2520norm%250Afor%2520interactive%2520service%2520delivery%2520and%2520the%2520future%2520Metaverse.%2520However%252C%2520the%250Asubstantial%2520volume%2520of%2520data%2520associated%2520with%2520point%2520clouds%2520presents%2520numerous%250Achallenges%252C%2520particularly%2520in%2520terms%2520of%2520high%2520bandwidth%2520consumption%2520and%2520large%250Astorage%2520capacity.%2520Despite%2520various%2520solutions%2520proposed%2520thus%2520far%252C%2520with%2520a%2520focus%2520on%250Apoint%2520cloud%2520compression%252C%2520upsampling%252C%2520and%2520completion%252C%2520these%250Areconstruction-related%2520methods%2520continue%2520to%2520fall%2520short%2520in%2520delivering%2520high%250Afidelity%2520point%2520cloud%2520output.%2520As%2520a%2520solution%252C%2520in%2520DiffPMAE%252C%2520we%2520propose%2520an%250Aeffective%2520point%2520cloud%2520reconstruction%2520architecture.%2520Inspired%2520by%2520self-supervised%250Alearning%2520concepts%252C%2520we%2520combine%2520Masked%2520Auto-Encoding%2520and%2520Diffusion%2520Model%250Amechanism%2520to%2520remotely%2520reconstruct%2520point%2520cloud%2520data.%2520By%2520the%2520nature%2520of%2520this%250Areconstruction%2520process%252C%2520DiffPMAE%2520can%2520be%2520extended%2520to%2520many%2520related%2520downstream%250Atasks%2520including%2520point%2520cloud%2520compression%252C%2520upsampling%2520and%2520completion.%2520Leveraging%250AShapeNet-55%2520and%2520ModelNet%2520datasets%2520with%2520over%252060000%2520objects%252C%2520we%2520validate%2520the%250Aperformance%2520of%2520DiffPMAE%2520exceeding%2520many%2520state-of-the-art%2520methods%2520in-terms%2520of%250Aauto-encoding%2520and%2520downstream%2520tasks%2520considered.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03298v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffPMAE%3A%20Diffusion%20Masked%20Autoencoders%20for%20Point%20Cloud%20Reconstruction&entry.906535625=Yanlong%20Li%20and%20Chamara%20Madarasingha%20and%20Kanchana%20Thilakarathna&entry.1292438233=%20%20Point%20cloud%20streaming%20is%20increasingly%20getting%20popular%2C%20evolving%20into%20the%20norm%0Afor%20interactive%20service%20delivery%20and%20the%20future%20Metaverse.%20However%2C%20the%0Asubstantial%20volume%20of%20data%20associated%20with%20point%20clouds%20presents%20numerous%0Achallenges%2C%20particularly%20in%20terms%20of%20high%20bandwidth%20consumption%20and%20large%0Astorage%20capacity.%20Despite%20various%20solutions%20proposed%20thus%20far%2C%20with%20a%20focus%20on%0Apoint%20cloud%20compression%2C%20upsampling%2C%20and%20completion%2C%20these%0Areconstruction-related%20methods%20continue%20to%20fall%20short%20in%20delivering%20high%0Afidelity%20point%20cloud%20output.%20As%20a%20solution%2C%20in%20DiffPMAE%2C%20we%20propose%20an%0Aeffective%20point%20cloud%20reconstruction%20architecture.%20Inspired%20by%20self-supervised%0Alearning%20concepts%2C%20we%20combine%20Masked%20Auto-Encoding%20and%20Diffusion%20Model%0Amechanism%20to%20remotely%20reconstruct%20point%20cloud%20data.%20By%20the%20nature%20of%20this%0Areconstruction%20process%2C%20DiffPMAE%20can%20be%20extended%20to%20many%20related%20downstream%0Atasks%20including%20point%20cloud%20compression%2C%20upsampling%20and%20completion.%20Leveraging%0AShapeNet-55%20and%20ModelNet%20datasets%20with%20over%2060000%20objects%2C%20we%20validate%20the%0Aperformance%20of%20DiffPMAE%20exceeding%20many%20state-of-the-art%20methods%20in-terms%20of%0Aauto-encoding%20and%20downstream%20tasks%20considered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03298v3&entry.124074799=Read"},
{"title": "Category-Prompt Refined Feature Learning for Long-Tailed Multi-Label\n  Image Classification", "author": "Jiexuan Yan and Sheng Huang and Nankun Mu and Luwen Huangfu and Bo Liu", "abstract": "  Real-world data consistently exhibits a long-tailed distribution, often\nspanning multiple categories. This complexity underscores the challenge of\ncontent comprehension, particularly in scenarios requiring Long-Tailed\nMulti-Label image Classification (LTMLC). In such contexts, imbalanced data\ndistribution and multi-object recognition pose significant hurdles. To address\nthis issue, we propose a novel and effective approach for LTMLC, termed\nCategory-Prompt Refined Feature Learning (CPRFL), utilizing semantic\ncorrelations between different categories and decoupling category-specific\nvisual representations for each category. Specifically, CPRFL initializes\ncategory-prompts from the pretrained CLIP's embeddings and decouples\ncategory-specific visual representations through interaction with visual\nfeatures, thereby facilitating the establishment of semantic correlations\nbetween the head and tail classes. To mitigate the visual-semantic domain bias,\nwe design a progressive Dual-Path Back-Propagation mechanism to refine the\nprompts by progressively incorporating context-related visual information into\nprompts. Simultaneously, the refinement process facilitates the progressive\npurification of the category-specific visual representations under the guidance\nof the refined prompts. Furthermore, taking into account the negative-positive\nsample imbalance, we adopt the Asymmetric Loss as our optimization objective to\nsuppress negative samples across all classes and potentially enhance the\nhead-to-tail recognition performance. We validate the effectiveness of our\nmethod on two LTMLC benchmarks and extensive experiments demonstrate the\nsuperiority of our work over baselines.\n  The code is available at https://github.com/jiexuanyan/CPRFL.\n", "link": "http://arxiv.org/abs/2408.08125v1", "date": "2024-08-15", "relevancy": 2.9418, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5949}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5945}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category-Prompt%20Refined%20Feature%20Learning%20for%20Long-Tailed%20Multi-Label%0A%20%20Image%20Classification&body=Title%3A%20Category-Prompt%20Refined%20Feature%20Learning%20for%20Long-Tailed%20Multi-Label%0A%20%20Image%20Classification%0AAuthor%3A%20Jiexuan%20Yan%20and%20Sheng%20Huang%20and%20Nankun%20Mu%20and%20Luwen%20Huangfu%20and%20Bo%20Liu%0AAbstract%3A%20%20%20Real-world%20data%20consistently%20exhibits%20a%20long-tailed%20distribution%2C%20often%0Aspanning%20multiple%20categories.%20This%20complexity%20underscores%20the%20challenge%20of%0Acontent%20comprehension%2C%20particularly%20in%20scenarios%20requiring%20Long-Tailed%0AMulti-Label%20image%20Classification%20%28LTMLC%29.%20In%20such%20contexts%2C%20imbalanced%20data%0Adistribution%20and%20multi-object%20recognition%20pose%20significant%20hurdles.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20novel%20and%20effective%20approach%20for%20LTMLC%2C%20termed%0ACategory-Prompt%20Refined%20Feature%20Learning%20%28CPRFL%29%2C%20utilizing%20semantic%0Acorrelations%20between%20different%20categories%20and%20decoupling%20category-specific%0Avisual%20representations%20for%20each%20category.%20Specifically%2C%20CPRFL%20initializes%0Acategory-prompts%20from%20the%20pretrained%20CLIP%27s%20embeddings%20and%20decouples%0Acategory-specific%20visual%20representations%20through%20interaction%20with%20visual%0Afeatures%2C%20thereby%20facilitating%20the%20establishment%20of%20semantic%20correlations%0Abetween%20the%20head%20and%20tail%20classes.%20To%20mitigate%20the%20visual-semantic%20domain%20bias%2C%0Awe%20design%20a%20progressive%20Dual-Path%20Back-Propagation%20mechanism%20to%20refine%20the%0Aprompts%20by%20progressively%20incorporating%20context-related%20visual%20information%20into%0Aprompts.%20Simultaneously%2C%20the%20refinement%20process%20facilitates%20the%20progressive%0Apurification%20of%20the%20category-specific%20visual%20representations%20under%20the%20guidance%0Aof%20the%20refined%20prompts.%20Furthermore%2C%20taking%20into%20account%20the%20negative-positive%0Asample%20imbalance%2C%20we%20adopt%20the%20Asymmetric%20Loss%20as%20our%20optimization%20objective%20to%0Asuppress%20negative%20samples%20across%20all%20classes%20and%20potentially%20enhance%20the%0Ahead-to-tail%20recognition%20performance.%20We%20validate%20the%20effectiveness%20of%20our%0Amethod%20on%20two%20LTMLC%20benchmarks%20and%20extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20work%20over%20baselines.%0A%20%20The%20code%20is%20available%20at%20https%3A//github.com/jiexuanyan/CPRFL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory-Prompt%2520Refined%2520Feature%2520Learning%2520for%2520Long-Tailed%2520Multi-Label%250A%2520%2520Image%2520Classification%26entry.906535625%3DJiexuan%2520Yan%2520and%2520Sheng%2520Huang%2520and%2520Nankun%2520Mu%2520and%2520Luwen%2520Huangfu%2520and%2520Bo%2520Liu%26entry.1292438233%3D%2520%2520Real-world%2520data%2520consistently%2520exhibits%2520a%2520long-tailed%2520distribution%252C%2520often%250Aspanning%2520multiple%2520categories.%2520This%2520complexity%2520underscores%2520the%2520challenge%2520of%250Acontent%2520comprehension%252C%2520particularly%2520in%2520scenarios%2520requiring%2520Long-Tailed%250AMulti-Label%2520image%2520Classification%2520%2528LTMLC%2529.%2520In%2520such%2520contexts%252C%2520imbalanced%2520data%250Adistribution%2520and%2520multi-object%2520recognition%2520pose%2520significant%2520hurdles.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520a%2520novel%2520and%2520effective%2520approach%2520for%2520LTMLC%252C%2520termed%250ACategory-Prompt%2520Refined%2520Feature%2520Learning%2520%2528CPRFL%2529%252C%2520utilizing%2520semantic%250Acorrelations%2520between%2520different%2520categories%2520and%2520decoupling%2520category-specific%250Avisual%2520representations%2520for%2520each%2520category.%2520Specifically%252C%2520CPRFL%2520initializes%250Acategory-prompts%2520from%2520the%2520pretrained%2520CLIP%2527s%2520embeddings%2520and%2520decouples%250Acategory-specific%2520visual%2520representations%2520through%2520interaction%2520with%2520visual%250Afeatures%252C%2520thereby%2520facilitating%2520the%2520establishment%2520of%2520semantic%2520correlations%250Abetween%2520the%2520head%2520and%2520tail%2520classes.%2520To%2520mitigate%2520the%2520visual-semantic%2520domain%2520bias%252C%250Awe%2520design%2520a%2520progressive%2520Dual-Path%2520Back-Propagation%2520mechanism%2520to%2520refine%2520the%250Aprompts%2520by%2520progressively%2520incorporating%2520context-related%2520visual%2520information%2520into%250Aprompts.%2520Simultaneously%252C%2520the%2520refinement%2520process%2520facilitates%2520the%2520progressive%250Apurification%2520of%2520the%2520category-specific%2520visual%2520representations%2520under%2520the%2520guidance%250Aof%2520the%2520refined%2520prompts.%2520Furthermore%252C%2520taking%2520into%2520account%2520the%2520negative-positive%250Asample%2520imbalance%252C%2520we%2520adopt%2520the%2520Asymmetric%2520Loss%2520as%2520our%2520optimization%2520objective%2520to%250Asuppress%2520negative%2520samples%2520across%2520all%2520classes%2520and%2520potentially%2520enhance%2520the%250Ahead-to-tail%2520recognition%2520performance.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%250Amethod%2520on%2520two%2520LTMLC%2520benchmarks%2520and%2520extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520work%2520over%2520baselines.%250A%2520%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/jiexuanyan/CPRFL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category-Prompt%20Refined%20Feature%20Learning%20for%20Long-Tailed%20Multi-Label%0A%20%20Image%20Classification&entry.906535625=Jiexuan%20Yan%20and%20Sheng%20Huang%20and%20Nankun%20Mu%20and%20Luwen%20Huangfu%20and%20Bo%20Liu&entry.1292438233=%20%20Real-world%20data%20consistently%20exhibits%20a%20long-tailed%20distribution%2C%20often%0Aspanning%20multiple%20categories.%20This%20complexity%20underscores%20the%20challenge%20of%0Acontent%20comprehension%2C%20particularly%20in%20scenarios%20requiring%20Long-Tailed%0AMulti-Label%20image%20Classification%20%28LTMLC%29.%20In%20such%20contexts%2C%20imbalanced%20data%0Adistribution%20and%20multi-object%20recognition%20pose%20significant%20hurdles.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20novel%20and%20effective%20approach%20for%20LTMLC%2C%20termed%0ACategory-Prompt%20Refined%20Feature%20Learning%20%28CPRFL%29%2C%20utilizing%20semantic%0Acorrelations%20between%20different%20categories%20and%20decoupling%20category-specific%0Avisual%20representations%20for%20each%20category.%20Specifically%2C%20CPRFL%20initializes%0Acategory-prompts%20from%20the%20pretrained%20CLIP%27s%20embeddings%20and%20decouples%0Acategory-specific%20visual%20representations%20through%20interaction%20with%20visual%0Afeatures%2C%20thereby%20facilitating%20the%20establishment%20of%20semantic%20correlations%0Abetween%20the%20head%20and%20tail%20classes.%20To%20mitigate%20the%20visual-semantic%20domain%20bias%2C%0Awe%20design%20a%20progressive%20Dual-Path%20Back-Propagation%20mechanism%20to%20refine%20the%0Aprompts%20by%20progressively%20incorporating%20context-related%20visual%20information%20into%0Aprompts.%20Simultaneously%2C%20the%20refinement%20process%20facilitates%20the%20progressive%0Apurification%20of%20the%20category-specific%20visual%20representations%20under%20the%20guidance%0Aof%20the%20refined%20prompts.%20Furthermore%2C%20taking%20into%20account%20the%20negative-positive%0Asample%20imbalance%2C%20we%20adopt%20the%20Asymmetric%20Loss%20as%20our%20optimization%20objective%20to%0Asuppress%20negative%20samples%20across%20all%20classes%20and%20potentially%20enhance%20the%0Ahead-to-tail%20recognition%20performance.%20We%20validate%20the%20effectiveness%20of%20our%0Amethod%20on%20two%20LTMLC%20benchmarks%20and%20extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20work%20over%20baselines.%0A%20%20The%20code%20is%20available%20at%20https%3A//github.com/jiexuanyan/CPRFL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08125v1&entry.124074799=Read"},
{"title": "Unsupervised Part Discovery via Dual Representation Alignment", "author": "Jiahao Xia and Wenjian Huang and Min Xu and Jianguo Zhang and Haimin Zhang and Ziyu Sheng and Dong Xu", "abstract": "  Object parts serve as crucial intermediate representations in various\ndownstream tasks, but part-level representation learning still has not received\nas much attention as other vision tasks. Previous research has established that\nVision Transformer can learn instance-level attention without labels,\nextracting high-quality instance-level representations for boosting downstream\ntasks. In this paper, we achieve unsupervised part-specific attention learning\nusing a novel paradigm and further employ the part representations to improve\npart discovery performance. Specifically, paired images are generated from the\nsame image with different geometric transformations, and multiple part\nrepresentations are extracted from these paired images using a novel module,\nnamed PartFormer. These part representations from the paired images are then\nexchanged to improve geometric transformation invariance. Subsequently, the\npart representations are aligned with the feature map extracted by a feature\nmap encoder, achieving high similarity with the pixel representations of the\ncorresponding part regions and low similarity in irrelevant regions. Finally,\nthe geometric and semantic constraints are applied to the part representations\nthrough the intermediate results in alignment for part-specific attention\nlearning, encouraging the PartFormer to focus locally and the part\nrepresentations to explicitly include the information of the corresponding\nparts. Moreover, the aligned part representations can further serve as a series\nof reliable detectors in the testing phase, predicting pixel masks for part\ndiscovery. Extensive experiments are carried out on four widely used datasets,\nand our results demonstrate that the proposed method achieves competitive\nperformance and robustness due to its part-specific attention.\n", "link": "http://arxiv.org/abs/2408.08108v1", "date": "2024-08-15", "relevancy": 2.8824, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5793}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5759}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Part%20Discovery%20via%20Dual%20Representation%20Alignment&body=Title%3A%20Unsupervised%20Part%20Discovery%20via%20Dual%20Representation%20Alignment%0AAuthor%3A%20Jiahao%20Xia%20and%20Wenjian%20Huang%20and%20Min%20Xu%20and%20Jianguo%20Zhang%20and%20Haimin%20Zhang%20and%20Ziyu%20Sheng%20and%20Dong%20Xu%0AAbstract%3A%20%20%20Object%20parts%20serve%20as%20crucial%20intermediate%20representations%20in%20various%0Adownstream%20tasks%2C%20but%20part-level%20representation%20learning%20still%20has%20not%20received%0Aas%20much%20attention%20as%20other%20vision%20tasks.%20Previous%20research%20has%20established%20that%0AVision%20Transformer%20can%20learn%20instance-level%20attention%20without%20labels%2C%0Aextracting%20high-quality%20instance-level%20representations%20for%20boosting%20downstream%0Atasks.%20In%20this%20paper%2C%20we%20achieve%20unsupervised%20part-specific%20attention%20learning%0Ausing%20a%20novel%20paradigm%20and%20further%20employ%20the%20part%20representations%20to%20improve%0Apart%20discovery%20performance.%20Specifically%2C%20paired%20images%20are%20generated%20from%20the%0Asame%20image%20with%20different%20geometric%20transformations%2C%20and%20multiple%20part%0Arepresentations%20are%20extracted%20from%20these%20paired%20images%20using%20a%20novel%20module%2C%0Anamed%20PartFormer.%20These%20part%20representations%20from%20the%20paired%20images%20are%20then%0Aexchanged%20to%20improve%20geometric%20transformation%20invariance.%20Subsequently%2C%20the%0Apart%20representations%20are%20aligned%20with%20the%20feature%20map%20extracted%20by%20a%20feature%0Amap%20encoder%2C%20achieving%20high%20similarity%20with%20the%20pixel%20representations%20of%20the%0Acorresponding%20part%20regions%20and%20low%20similarity%20in%20irrelevant%20regions.%20Finally%2C%0Athe%20geometric%20and%20semantic%20constraints%20are%20applied%20to%20the%20part%20representations%0Athrough%20the%20intermediate%20results%20in%20alignment%20for%20part-specific%20attention%0Alearning%2C%20encouraging%20the%20PartFormer%20to%20focus%20locally%20and%20the%20part%0Arepresentations%20to%20explicitly%20include%20the%20information%20of%20the%20corresponding%0Aparts.%20Moreover%2C%20the%20aligned%20part%20representations%20can%20further%20serve%20as%20a%20series%0Aof%20reliable%20detectors%20in%20the%20testing%20phase%2C%20predicting%20pixel%20masks%20for%20part%0Adiscovery.%20Extensive%20experiments%20are%20carried%20out%20on%20four%20widely%20used%20datasets%2C%0Aand%20our%20results%20demonstrate%20that%20the%20proposed%20method%20achieves%20competitive%0Aperformance%20and%20robustness%20due%20to%20its%20part-specific%20attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08108v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Part%2520Discovery%2520via%2520Dual%2520Representation%2520Alignment%26entry.906535625%3DJiahao%2520Xia%2520and%2520Wenjian%2520Huang%2520and%2520Min%2520Xu%2520and%2520Jianguo%2520Zhang%2520and%2520Haimin%2520Zhang%2520and%2520Ziyu%2520Sheng%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520Object%2520parts%2520serve%2520as%2520crucial%2520intermediate%2520representations%2520in%2520various%250Adownstream%2520tasks%252C%2520but%2520part-level%2520representation%2520learning%2520still%2520has%2520not%2520received%250Aas%2520much%2520attention%2520as%2520other%2520vision%2520tasks.%2520Previous%2520research%2520has%2520established%2520that%250AVision%2520Transformer%2520can%2520learn%2520instance-level%2520attention%2520without%2520labels%252C%250Aextracting%2520high-quality%2520instance-level%2520representations%2520for%2520boosting%2520downstream%250Atasks.%2520In%2520this%2520paper%252C%2520we%2520achieve%2520unsupervised%2520part-specific%2520attention%2520learning%250Ausing%2520a%2520novel%2520paradigm%2520and%2520further%2520employ%2520the%2520part%2520representations%2520to%2520improve%250Apart%2520discovery%2520performance.%2520Specifically%252C%2520paired%2520images%2520are%2520generated%2520from%2520the%250Asame%2520image%2520with%2520different%2520geometric%2520transformations%252C%2520and%2520multiple%2520part%250Arepresentations%2520are%2520extracted%2520from%2520these%2520paired%2520images%2520using%2520a%2520novel%2520module%252C%250Anamed%2520PartFormer.%2520These%2520part%2520representations%2520from%2520the%2520paired%2520images%2520are%2520then%250Aexchanged%2520to%2520improve%2520geometric%2520transformation%2520invariance.%2520Subsequently%252C%2520the%250Apart%2520representations%2520are%2520aligned%2520with%2520the%2520feature%2520map%2520extracted%2520by%2520a%2520feature%250Amap%2520encoder%252C%2520achieving%2520high%2520similarity%2520with%2520the%2520pixel%2520representations%2520of%2520the%250Acorresponding%2520part%2520regions%2520and%2520low%2520similarity%2520in%2520irrelevant%2520regions.%2520Finally%252C%250Athe%2520geometric%2520and%2520semantic%2520constraints%2520are%2520applied%2520to%2520the%2520part%2520representations%250Athrough%2520the%2520intermediate%2520results%2520in%2520alignment%2520for%2520part-specific%2520attention%250Alearning%252C%2520encouraging%2520the%2520PartFormer%2520to%2520focus%2520locally%2520and%2520the%2520part%250Arepresentations%2520to%2520explicitly%2520include%2520the%2520information%2520of%2520the%2520corresponding%250Aparts.%2520Moreover%252C%2520the%2520aligned%2520part%2520representations%2520can%2520further%2520serve%2520as%2520a%2520series%250Aof%2520reliable%2520detectors%2520in%2520the%2520testing%2520phase%252C%2520predicting%2520pixel%2520masks%2520for%2520part%250Adiscovery.%2520Extensive%2520experiments%2520are%2520carried%2520out%2520on%2520four%2520widely%2520used%2520datasets%252C%250Aand%2520our%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520competitive%250Aperformance%2520and%2520robustness%2520due%2520to%2520its%2520part-specific%2520attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08108v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Part%20Discovery%20via%20Dual%20Representation%20Alignment&entry.906535625=Jiahao%20Xia%20and%20Wenjian%20Huang%20and%20Min%20Xu%20and%20Jianguo%20Zhang%20and%20Haimin%20Zhang%20and%20Ziyu%20Sheng%20and%20Dong%20Xu&entry.1292438233=%20%20Object%20parts%20serve%20as%20crucial%20intermediate%20representations%20in%20various%0Adownstream%20tasks%2C%20but%20part-level%20representation%20learning%20still%20has%20not%20received%0Aas%20much%20attention%20as%20other%20vision%20tasks.%20Previous%20research%20has%20established%20that%0AVision%20Transformer%20can%20learn%20instance-level%20attention%20without%20labels%2C%0Aextracting%20high-quality%20instance-level%20representations%20for%20boosting%20downstream%0Atasks.%20In%20this%20paper%2C%20we%20achieve%20unsupervised%20part-specific%20attention%20learning%0Ausing%20a%20novel%20paradigm%20and%20further%20employ%20the%20part%20representations%20to%20improve%0Apart%20discovery%20performance.%20Specifically%2C%20paired%20images%20are%20generated%20from%20the%0Asame%20image%20with%20different%20geometric%20transformations%2C%20and%20multiple%20part%0Arepresentations%20are%20extracted%20from%20these%20paired%20images%20using%20a%20novel%20module%2C%0Anamed%20PartFormer.%20These%20part%20representations%20from%20the%20paired%20images%20are%20then%0Aexchanged%20to%20improve%20geometric%20transformation%20invariance.%20Subsequently%2C%20the%0Apart%20representations%20are%20aligned%20with%20the%20feature%20map%20extracted%20by%20a%20feature%0Amap%20encoder%2C%20achieving%20high%20similarity%20with%20the%20pixel%20representations%20of%20the%0Acorresponding%20part%20regions%20and%20low%20similarity%20in%20irrelevant%20regions.%20Finally%2C%0Athe%20geometric%20and%20semantic%20constraints%20are%20applied%20to%20the%20part%20representations%0Athrough%20the%20intermediate%20results%20in%20alignment%20for%20part-specific%20attention%0Alearning%2C%20encouraging%20the%20PartFormer%20to%20focus%20locally%20and%20the%20part%0Arepresentations%20to%20explicitly%20include%20the%20information%20of%20the%20corresponding%0Aparts.%20Moreover%2C%20the%20aligned%20part%20representations%20can%20further%20serve%20as%20a%20series%0Aof%20reliable%20detectors%20in%20the%20testing%20phase%2C%20predicting%20pixel%20masks%20for%20part%0Adiscovery.%20Extensive%20experiments%20are%20carried%20out%20on%20four%20widely%20used%20datasets%2C%0Aand%20our%20results%20demonstrate%20that%20the%20proposed%20method%20achieves%20competitive%0Aperformance%20and%20robustness%20due%20to%20its%20part-specific%20attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08108v1&entry.124074799=Read"},
{"title": "MLAAN: Scaling Supervised Local Learning with Multilaminar Leap\n  Augmented Auxiliary Network", "author": "Yuming Zhang and Shouxin Zhang and Peizhe Wang and Feiyu Zhu and Dongzhi Guan and Junhao Su and Jiabin Liu and Changpeng Cai", "abstract": "  Deep neural networks (DNNs) typically employ an end-to-end (E2E) training\nparadigm which presents several challenges, including high GPU memory\nconsumption, inefficiency, and difficulties in model parallelization during\ntraining. Recent research has sought to address these issues, with one\npromising approach being local learning. This method involves partitioning the\nbackbone network into gradient-isolated modules and manually designing\nauxiliary networks to train these local modules. Existing methods often neglect\nthe interaction of information between local modules, leading to myopic issues\nand a performance gap compared to E2E training. To address these limitations,\nwe propose the Multilaminar Leap Augmented Auxiliary Network (MLAAN).\nSpecifically, MLAAN comprises Multilaminar Local Modules (MLM) and Leap\nAugmented Modules (LAM). MLM captures both local and global features through\nindependent and cascaded auxiliary networks, alleviating performance issues\ncaused by insufficient global features. However, overly simplistic auxiliary\nnetworks can impede MLM's ability to capture global information. To address\nthis, we further design LAM, an enhanced auxiliary network that uses the\nExponential Moving Average (EMA) method to facilitate information exchange\nbetween local modules, thereby mitigating the shortsightedness resulting from\ninadequate interaction. The synergy between MLM and LAM has demonstrated\nexcellent performance. Our experiments on the CIFAR-10, STL-10, SVHN, and\nImageNet datasets show that MLAAN can be seamlessly integrated into existing\nlocal learning frameworks, significantly enhancing their performance and even\nsurpassing end-to-end (E2E) training methods, while also reducing GPU memory\nconsumption.\n", "link": "http://arxiv.org/abs/2406.16633v4", "date": "2024-08-15", "relevancy": 2.8742, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5926}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5737}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&body=Title%3A%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network%0AAuthor%3A%20Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Junhao%20Su%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20typically%20employ%20an%20end-to-end%20%28E2E%29%20training%0Aparadigm%20which%20presents%20several%20challenges%2C%20including%20high%20GPU%20memory%0Aconsumption%2C%20inefficiency%2C%20and%20difficulties%20in%20model%20parallelization%20during%0Atraining.%20Recent%20research%20has%20sought%20to%20address%20these%20issues%2C%20with%20one%0Apromising%20approach%20being%20local%20learning.%20This%20method%20involves%20partitioning%20the%0Abackbone%20network%20into%20gradient-isolated%20modules%20and%20manually%20designing%0Aauxiliary%20networks%20to%20train%20these%20local%20modules.%20Existing%20methods%20often%20neglect%0Athe%20interaction%20of%20information%20between%20local%20modules%2C%20leading%20to%20myopic%20issues%0Aand%20a%20performance%20gap%20compared%20to%20E2E%20training.%20To%20address%20these%20limitations%2C%0Awe%20propose%20the%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%0ASpecifically%2C%20MLAAN%20comprises%20Multilaminar%20Local%20Modules%20%28MLM%29%20and%20Leap%0AAugmented%20Modules%20%28LAM%29.%20MLM%20captures%20both%20local%20and%20global%20features%20through%0Aindependent%20and%20cascaded%20auxiliary%20networks%2C%20alleviating%20performance%20issues%0Acaused%20by%20insufficient%20global%20features.%20However%2C%20overly%20simplistic%20auxiliary%0Anetworks%20can%20impede%20MLM%27s%20ability%20to%20capture%20global%20information.%20To%20address%0Athis%2C%20we%20further%20design%20LAM%2C%20an%20enhanced%20auxiliary%20network%20that%20uses%20the%0AExponential%20Moving%20Average%20%28EMA%29%20method%20to%20facilitate%20information%20exchange%0Abetween%20local%20modules%2C%20thereby%20mitigating%20the%20shortsightedness%20resulting%20from%0Ainadequate%20interaction.%20The%20synergy%20between%20MLM%20and%20LAM%20has%20demonstrated%0Aexcellent%20performance.%20Our%20experiments%20on%20the%20CIFAR-10%2C%20STL-10%2C%20SVHN%2C%20and%0AImageNet%20datasets%20show%20that%20MLAAN%20can%20be%20seamlessly%20integrated%20into%20existing%0Alocal%20learning%20frameworks%2C%20significantly%20enhancing%20their%20performance%20and%20even%0Asurpassing%20end-to-end%20%28E2E%29%20training%20methods%2C%20while%20also%20reducing%20GPU%20memory%0Aconsumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16633v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLAAN%253A%2520Scaling%2520Supervised%2520Local%2520Learning%2520with%2520Multilaminar%2520Leap%250A%2520%2520Augmented%2520Auxiliary%2520Network%26entry.906535625%3DYuming%2520Zhang%2520and%2520Shouxin%2520Zhang%2520and%2520Peizhe%2520Wang%2520and%2520Feiyu%2520Zhu%2520and%2520Dongzhi%2520Guan%2520and%2520Junhao%2520Su%2520and%2520Jiabin%2520Liu%2520and%2520Changpeng%2520Cai%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520typically%2520employ%2520an%2520end-to-end%2520%2528E2E%2529%2520training%250Aparadigm%2520which%2520presents%2520several%2520challenges%252C%2520including%2520high%2520GPU%2520memory%250Aconsumption%252C%2520inefficiency%252C%2520and%2520difficulties%2520in%2520model%2520parallelization%2520during%250Atraining.%2520Recent%2520research%2520has%2520sought%2520to%2520address%2520these%2520issues%252C%2520with%2520one%250Apromising%2520approach%2520being%2520local%2520learning.%2520This%2520method%2520involves%2520partitioning%2520the%250Abackbone%2520network%2520into%2520gradient-isolated%2520modules%2520and%2520manually%2520designing%250Aauxiliary%2520networks%2520to%2520train%2520these%2520local%2520modules.%2520Existing%2520methods%2520often%2520neglect%250Athe%2520interaction%2520of%2520information%2520between%2520local%2520modules%252C%2520leading%2520to%2520myopic%2520issues%250Aand%2520a%2520performance%2520gap%2520compared%2520to%2520E2E%2520training.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520propose%2520the%2520Multilaminar%2520Leap%2520Augmented%2520Auxiliary%2520Network%2520%2528MLAAN%2529.%250ASpecifically%252C%2520MLAAN%2520comprises%2520Multilaminar%2520Local%2520Modules%2520%2528MLM%2529%2520and%2520Leap%250AAugmented%2520Modules%2520%2528LAM%2529.%2520MLM%2520captures%2520both%2520local%2520and%2520global%2520features%2520through%250Aindependent%2520and%2520cascaded%2520auxiliary%2520networks%252C%2520alleviating%2520performance%2520issues%250Acaused%2520by%2520insufficient%2520global%2520features.%2520However%252C%2520overly%2520simplistic%2520auxiliary%250Anetworks%2520can%2520impede%2520MLM%2527s%2520ability%2520to%2520capture%2520global%2520information.%2520To%2520address%250Athis%252C%2520we%2520further%2520design%2520LAM%252C%2520an%2520enhanced%2520auxiliary%2520network%2520that%2520uses%2520the%250AExponential%2520Moving%2520Average%2520%2528EMA%2529%2520method%2520to%2520facilitate%2520information%2520exchange%250Abetween%2520local%2520modules%252C%2520thereby%2520mitigating%2520the%2520shortsightedness%2520resulting%2520from%250Ainadequate%2520interaction.%2520The%2520synergy%2520between%2520MLM%2520and%2520LAM%2520has%2520demonstrated%250Aexcellent%2520performance.%2520Our%2520experiments%2520on%2520the%2520CIFAR-10%252C%2520STL-10%252C%2520SVHN%252C%2520and%250AImageNet%2520datasets%2520show%2520that%2520MLAAN%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%250Alocal%2520learning%2520frameworks%252C%2520significantly%2520enhancing%2520their%2520performance%2520and%2520even%250Asurpassing%2520end-to-end%2520%2528E2E%2529%2520training%2520methods%252C%2520while%2520also%2520reducing%2520GPU%2520memory%250Aconsumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16633v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&entry.906535625=Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Junhao%20Su%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20typically%20employ%20an%20end-to-end%20%28E2E%29%20training%0Aparadigm%20which%20presents%20several%20challenges%2C%20including%20high%20GPU%20memory%0Aconsumption%2C%20inefficiency%2C%20and%20difficulties%20in%20model%20parallelization%20during%0Atraining.%20Recent%20research%20has%20sought%20to%20address%20these%20issues%2C%20with%20one%0Apromising%20approach%20being%20local%20learning.%20This%20method%20involves%20partitioning%20the%0Abackbone%20network%20into%20gradient-isolated%20modules%20and%20manually%20designing%0Aauxiliary%20networks%20to%20train%20these%20local%20modules.%20Existing%20methods%20often%20neglect%0Athe%20interaction%20of%20information%20between%20local%20modules%2C%20leading%20to%20myopic%20issues%0Aand%20a%20performance%20gap%20compared%20to%20E2E%20training.%20To%20address%20these%20limitations%2C%0Awe%20propose%20the%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%0ASpecifically%2C%20MLAAN%20comprises%20Multilaminar%20Local%20Modules%20%28MLM%29%20and%20Leap%0AAugmented%20Modules%20%28LAM%29.%20MLM%20captures%20both%20local%20and%20global%20features%20through%0Aindependent%20and%20cascaded%20auxiliary%20networks%2C%20alleviating%20performance%20issues%0Acaused%20by%20insufficient%20global%20features.%20However%2C%20overly%20simplistic%20auxiliary%0Anetworks%20can%20impede%20MLM%27s%20ability%20to%20capture%20global%20information.%20To%20address%0Athis%2C%20we%20further%20design%20LAM%2C%20an%20enhanced%20auxiliary%20network%20that%20uses%20the%0AExponential%20Moving%20Average%20%28EMA%29%20method%20to%20facilitate%20information%20exchange%0Abetween%20local%20modules%2C%20thereby%20mitigating%20the%20shortsightedness%20resulting%20from%0Ainadequate%20interaction.%20The%20synergy%20between%20MLM%20and%20LAM%20has%20demonstrated%0Aexcellent%20performance.%20Our%20experiments%20on%20the%20CIFAR-10%2C%20STL-10%2C%20SVHN%2C%20and%0AImageNet%20datasets%20show%20that%20MLAAN%20can%20be%20seamlessly%20integrated%20into%20existing%0Alocal%20learning%20frameworks%2C%20significantly%20enhancing%20their%20performance%20and%20even%0Asurpassing%20end-to-end%20%28E2E%29%20training%20methods%2C%20while%20also%20reducing%20GPU%20memory%0Aconsumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16633v4&entry.124074799=Read"},
{"title": "CorrAdaptor: Adaptive Local Context Learning for Correspondence Pruning", "author": "Wei Zhu and Yicheng Liu and Yuping He and Tangfei Liao and Kang Zheng and Xiaoqiu Xu and Tao Wang and Tong Lu", "abstract": "  In the fields of computer vision and robotics, accurate pixel-level\ncorrespondences are essential for enabling advanced tasks such as\nstructure-from-motion and simultaneous localization and mapping. Recent\ncorrespondence pruning methods usually focus on learning local consistency\nthrough k-nearest neighbors, which makes it difficult to capture robust context\nfor each correspondence. We propose CorrAdaptor, a novel architecture that\nintroduces a dual-branch structure capable of adaptively adjusting local\ncontexts through both explicit and implicit local graph learning. Specifically,\nthe explicit branch uses KNN-based graphs tailored for initial neighborhood\nidentification, while the implicit branch leverages a learnable matrix to\nsoftly assign neighbors and adaptively expand the local context scope,\nsignificantly enhancing the model's robustness and adaptability to complex\nimage variations. Moreover, we design a motion injection module to integrate\nmotion consistency into the network to suppress the impact of outliers and\nrefine local context learning, resulting in substantial performance\nimprovements. The experimental results on extensive correspondence-based tasks\nindicate that our CorrAdaptor achieves state-of-the-art performance both\nqualitatively and quantitatively. The code and pre-trained models are available\nat https://github.com/TaoWangzj/CorrAdaptor.\n", "link": "http://arxiv.org/abs/2408.08134v1", "date": "2024-08-15", "relevancy": 2.8722, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5972}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5665}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CorrAdaptor%3A%20Adaptive%20Local%20Context%20Learning%20for%20Correspondence%20Pruning&body=Title%3A%20CorrAdaptor%3A%20Adaptive%20Local%20Context%20Learning%20for%20Correspondence%20Pruning%0AAuthor%3A%20Wei%20Zhu%20and%20Yicheng%20Liu%20and%20Yuping%20He%20and%20Tangfei%20Liao%20and%20Kang%20Zheng%20and%20Xiaoqiu%20Xu%20and%20Tao%20Wang%20and%20Tong%20Lu%0AAbstract%3A%20%20%20In%20the%20fields%20of%20computer%20vision%20and%20robotics%2C%20accurate%20pixel-level%0Acorrespondences%20are%20essential%20for%20enabling%20advanced%20tasks%20such%20as%0Astructure-from-motion%20and%20simultaneous%20localization%20and%20mapping.%20Recent%0Acorrespondence%20pruning%20methods%20usually%20focus%20on%20learning%20local%20consistency%0Athrough%20k-nearest%20neighbors%2C%20which%20makes%20it%20difficult%20to%20capture%20robust%20context%0Afor%20each%20correspondence.%20We%20propose%20CorrAdaptor%2C%20a%20novel%20architecture%20that%0Aintroduces%20a%20dual-branch%20structure%20capable%20of%20adaptively%20adjusting%20local%0Acontexts%20through%20both%20explicit%20and%20implicit%20local%20graph%20learning.%20Specifically%2C%0Athe%20explicit%20branch%20uses%20KNN-based%20graphs%20tailored%20for%20initial%20neighborhood%0Aidentification%2C%20while%20the%20implicit%20branch%20leverages%20a%20learnable%20matrix%20to%0Asoftly%20assign%20neighbors%20and%20adaptively%20expand%20the%20local%20context%20scope%2C%0Asignificantly%20enhancing%20the%20model%27s%20robustness%20and%20adaptability%20to%20complex%0Aimage%20variations.%20Moreover%2C%20we%20design%20a%20motion%20injection%20module%20to%20integrate%0Amotion%20consistency%20into%20the%20network%20to%20suppress%20the%20impact%20of%20outliers%20and%0Arefine%20local%20context%20learning%2C%20resulting%20in%20substantial%20performance%0Aimprovements.%20The%20experimental%20results%20on%20extensive%20correspondence-based%20tasks%0Aindicate%20that%20our%20CorrAdaptor%20achieves%20state-of-the-art%20performance%20both%0Aqualitatively%20and%20quantitatively.%20The%20code%20and%20pre-trained%20models%20are%20available%0Aat%20https%3A//github.com/TaoWangzj/CorrAdaptor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrAdaptor%253A%2520Adaptive%2520Local%2520Context%2520Learning%2520for%2520Correspondence%2520Pruning%26entry.906535625%3DWei%2520Zhu%2520and%2520Yicheng%2520Liu%2520and%2520Yuping%2520He%2520and%2520Tangfei%2520Liao%2520and%2520Kang%2520Zheng%2520and%2520Xiaoqiu%2520Xu%2520and%2520Tao%2520Wang%2520and%2520Tong%2520Lu%26entry.1292438233%3D%2520%2520In%2520the%2520fields%2520of%2520computer%2520vision%2520and%2520robotics%252C%2520accurate%2520pixel-level%250Acorrespondences%2520are%2520essential%2520for%2520enabling%2520advanced%2520tasks%2520such%2520as%250Astructure-from-motion%2520and%2520simultaneous%2520localization%2520and%2520mapping.%2520Recent%250Acorrespondence%2520pruning%2520methods%2520usually%2520focus%2520on%2520learning%2520local%2520consistency%250Athrough%2520k-nearest%2520neighbors%252C%2520which%2520makes%2520it%2520difficult%2520to%2520capture%2520robust%2520context%250Afor%2520each%2520correspondence.%2520We%2520propose%2520CorrAdaptor%252C%2520a%2520novel%2520architecture%2520that%250Aintroduces%2520a%2520dual-branch%2520structure%2520capable%2520of%2520adaptively%2520adjusting%2520local%250Acontexts%2520through%2520both%2520explicit%2520and%2520implicit%2520local%2520graph%2520learning.%2520Specifically%252C%250Athe%2520explicit%2520branch%2520uses%2520KNN-based%2520graphs%2520tailored%2520for%2520initial%2520neighborhood%250Aidentification%252C%2520while%2520the%2520implicit%2520branch%2520leverages%2520a%2520learnable%2520matrix%2520to%250Asoftly%2520assign%2520neighbors%2520and%2520adaptively%2520expand%2520the%2520local%2520context%2520scope%252C%250Asignificantly%2520enhancing%2520the%2520model%2527s%2520robustness%2520and%2520adaptability%2520to%2520complex%250Aimage%2520variations.%2520Moreover%252C%2520we%2520design%2520a%2520motion%2520injection%2520module%2520to%2520integrate%250Amotion%2520consistency%2520into%2520the%2520network%2520to%2520suppress%2520the%2520impact%2520of%2520outliers%2520and%250Arefine%2520local%2520context%2520learning%252C%2520resulting%2520in%2520substantial%2520performance%250Aimprovements.%2520The%2520experimental%2520results%2520on%2520extensive%2520correspondence-based%2520tasks%250Aindicate%2520that%2520our%2520CorrAdaptor%2520achieves%2520state-of-the-art%2520performance%2520both%250Aqualitatively%2520and%2520quantitatively.%2520The%2520code%2520and%2520pre-trained%2520models%2520are%2520available%250Aat%2520https%253A//github.com/TaoWangzj/CorrAdaptor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CorrAdaptor%3A%20Adaptive%20Local%20Context%20Learning%20for%20Correspondence%20Pruning&entry.906535625=Wei%20Zhu%20and%20Yicheng%20Liu%20and%20Yuping%20He%20and%20Tangfei%20Liao%20and%20Kang%20Zheng%20and%20Xiaoqiu%20Xu%20and%20Tao%20Wang%20and%20Tong%20Lu&entry.1292438233=%20%20In%20the%20fields%20of%20computer%20vision%20and%20robotics%2C%20accurate%20pixel-level%0Acorrespondences%20are%20essential%20for%20enabling%20advanced%20tasks%20such%20as%0Astructure-from-motion%20and%20simultaneous%20localization%20and%20mapping.%20Recent%0Acorrespondence%20pruning%20methods%20usually%20focus%20on%20learning%20local%20consistency%0Athrough%20k-nearest%20neighbors%2C%20which%20makes%20it%20difficult%20to%20capture%20robust%20context%0Afor%20each%20correspondence.%20We%20propose%20CorrAdaptor%2C%20a%20novel%20architecture%20that%0Aintroduces%20a%20dual-branch%20structure%20capable%20of%20adaptively%20adjusting%20local%0Acontexts%20through%20both%20explicit%20and%20implicit%20local%20graph%20learning.%20Specifically%2C%0Athe%20explicit%20branch%20uses%20KNN-based%20graphs%20tailored%20for%20initial%20neighborhood%0Aidentification%2C%20while%20the%20implicit%20branch%20leverages%20a%20learnable%20matrix%20to%0Asoftly%20assign%20neighbors%20and%20adaptively%20expand%20the%20local%20context%20scope%2C%0Asignificantly%20enhancing%20the%20model%27s%20robustness%20and%20adaptability%20to%20complex%0Aimage%20variations.%20Moreover%2C%20we%20design%20a%20motion%20injection%20module%20to%20integrate%0Amotion%20consistency%20into%20the%20network%20to%20suppress%20the%20impact%20of%20outliers%20and%0Arefine%20local%20context%20learning%2C%20resulting%20in%20substantial%20performance%0Aimprovements.%20The%20experimental%20results%20on%20extensive%20correspondence-based%20tasks%0Aindicate%20that%20our%20CorrAdaptor%20achieves%20state-of-the-art%20performance%20both%0Aqualitatively%20and%20quantitatively.%20The%20code%20and%20pre-trained%20models%20are%20available%0Aat%20https%3A//github.com/TaoWangzj/CorrAdaptor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08134v1&entry.124074799=Read"},
{"title": "GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization", "author": "Yahao Shi and Yanmin Wu and Chenming Wu and Xing Liu and Chen Zhao and Haocheng Feng and Jian Zhang and Bin Zhou and Errui Ding and Jingdong Wang", "abstract": "  This paper presents a 3D Gaussian Inverse Rendering (GIR) method, employing\n3D Gaussian representations to effectively factorize the scene into material\nproperties, light, and geometry. The key contributions lie in three-fold. We\ncompute the normal of each 3D Gaussian using the shortest eigenvector, with a\ndirectional masking scheme forcing accurate normal estimation without external\nsupervision. We adopt an efficient voxel-based indirect illumination tracing\nscheme that stores direction-aware outgoing radiance in each 3D Gaussian to\ndisentangle secondary illumination for approximating multi-bounce light\ntransport. To further enhance the illumination disentanglement, we represent a\nhigh-resolution environmental map with a learnable low-resolution map and a\nlightweight, fully convolutional network. Our method achieves state-of-the-art\nperformance in both relighting and novel view synthesis tasks among the\nrecently proposed inverse rendering methods while achieving real-time\nrendering. This substantiates our proposed method's efficacy and broad\napplicability, highlighting its potential as an influential tool in various\nreal-time interactive graphics applications such as material editing and\nrelighting. The code will be released at https://github.com/guduxiaolang/GIR.\n", "link": "http://arxiv.org/abs/2312.05133v2", "date": "2024-08-15", "relevancy": 2.8519, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6038}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5624}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIR%3A%203D%20Gaussian%20Inverse%20Rendering%20for%20Relightable%20Scene%20Factorization&body=Title%3A%20GIR%3A%203D%20Gaussian%20Inverse%20Rendering%20for%20Relightable%20Scene%20Factorization%0AAuthor%3A%20Yahao%20Shi%20and%20Yanmin%20Wu%20and%20Chenming%20Wu%20and%20Xing%20Liu%20and%20Chen%20Zhao%20and%20Haocheng%20Feng%20and%20Jian%20Zhang%20and%20Bin%20Zhou%20and%20Errui%20Ding%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%203D%20Gaussian%20Inverse%20Rendering%20%28GIR%29%20method%2C%20employing%0A3D%20Gaussian%20representations%20to%20effectively%20factorize%20the%20scene%20into%20material%0Aproperties%2C%20light%2C%20and%20geometry.%20The%20key%20contributions%20lie%20in%20three-fold.%20We%0Acompute%20the%20normal%20of%20each%203D%20Gaussian%20using%20the%20shortest%20eigenvector%2C%20with%20a%0Adirectional%20masking%20scheme%20forcing%20accurate%20normal%20estimation%20without%20external%0Asupervision.%20We%20adopt%20an%20efficient%20voxel-based%20indirect%20illumination%20tracing%0Ascheme%20that%20stores%20direction-aware%20outgoing%20radiance%20in%20each%203D%20Gaussian%20to%0Adisentangle%20secondary%20illumination%20for%20approximating%20multi-bounce%20light%0Atransport.%20To%20further%20enhance%20the%20illumination%20disentanglement%2C%20we%20represent%20a%0Ahigh-resolution%20environmental%20map%20with%20a%20learnable%20low-resolution%20map%20and%20a%0Alightweight%2C%20fully%20convolutional%20network.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%20in%20both%20relighting%20and%20novel%20view%20synthesis%20tasks%20among%20the%0Arecently%20proposed%20inverse%20rendering%20methods%20while%20achieving%20real-time%0Arendering.%20This%20substantiates%20our%20proposed%20method%27s%20efficacy%20and%20broad%0Aapplicability%2C%20highlighting%20its%20potential%20as%20an%20influential%20tool%20in%20various%0Areal-time%20interactive%20graphics%20applications%20such%20as%20material%20editing%20and%0Arelighting.%20The%20code%20will%20be%20released%20at%20https%3A//github.com/guduxiaolang/GIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05133v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIR%253A%25203D%2520Gaussian%2520Inverse%2520Rendering%2520for%2520Relightable%2520Scene%2520Factorization%26entry.906535625%3DYahao%2520Shi%2520and%2520Yanmin%2520Wu%2520and%2520Chenming%2520Wu%2520and%2520Xing%2520Liu%2520and%2520Chen%2520Zhao%2520and%2520Haocheng%2520Feng%2520and%2520Jian%2520Zhang%2520and%2520Bin%2520Zhou%2520and%2520Errui%2520Ding%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%25203D%2520Gaussian%2520Inverse%2520Rendering%2520%2528GIR%2529%2520method%252C%2520employing%250A3D%2520Gaussian%2520representations%2520to%2520effectively%2520factorize%2520the%2520scene%2520into%2520material%250Aproperties%252C%2520light%252C%2520and%2520geometry.%2520The%2520key%2520contributions%2520lie%2520in%2520three-fold.%2520We%250Acompute%2520the%2520normal%2520of%2520each%25203D%2520Gaussian%2520using%2520the%2520shortest%2520eigenvector%252C%2520with%2520a%250Adirectional%2520masking%2520scheme%2520forcing%2520accurate%2520normal%2520estimation%2520without%2520external%250Asupervision.%2520We%2520adopt%2520an%2520efficient%2520voxel-based%2520indirect%2520illumination%2520tracing%250Ascheme%2520that%2520stores%2520direction-aware%2520outgoing%2520radiance%2520in%2520each%25203D%2520Gaussian%2520to%250Adisentangle%2520secondary%2520illumination%2520for%2520approximating%2520multi-bounce%2520light%250Atransport.%2520To%2520further%2520enhance%2520the%2520illumination%2520disentanglement%252C%2520we%2520represent%2520a%250Ahigh-resolution%2520environmental%2520map%2520with%2520a%2520learnable%2520low-resolution%2520map%2520and%2520a%250Alightweight%252C%2520fully%2520convolutional%2520network.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520both%2520relighting%2520and%2520novel%2520view%2520synthesis%2520tasks%2520among%2520the%250Arecently%2520proposed%2520inverse%2520rendering%2520methods%2520while%2520achieving%2520real-time%250Arendering.%2520This%2520substantiates%2520our%2520proposed%2520method%2527s%2520efficacy%2520and%2520broad%250Aapplicability%252C%2520highlighting%2520its%2520potential%2520as%2520an%2520influential%2520tool%2520in%2520various%250Areal-time%2520interactive%2520graphics%2520applications%2520such%2520as%2520material%2520editing%2520and%250Arelighting.%2520The%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/guduxiaolang/GIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05133v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIR%3A%203D%20Gaussian%20Inverse%20Rendering%20for%20Relightable%20Scene%20Factorization&entry.906535625=Yahao%20Shi%20and%20Yanmin%20Wu%20and%20Chenming%20Wu%20and%20Xing%20Liu%20and%20Chen%20Zhao%20and%20Haocheng%20Feng%20and%20Jian%20Zhang%20and%20Bin%20Zhou%20and%20Errui%20Ding%20and%20Jingdong%20Wang&entry.1292438233=%20%20This%20paper%20presents%20a%203D%20Gaussian%20Inverse%20Rendering%20%28GIR%29%20method%2C%20employing%0A3D%20Gaussian%20representations%20to%20effectively%20factorize%20the%20scene%20into%20material%0Aproperties%2C%20light%2C%20and%20geometry.%20The%20key%20contributions%20lie%20in%20three-fold.%20We%0Acompute%20the%20normal%20of%20each%203D%20Gaussian%20using%20the%20shortest%20eigenvector%2C%20with%20a%0Adirectional%20masking%20scheme%20forcing%20accurate%20normal%20estimation%20without%20external%0Asupervision.%20We%20adopt%20an%20efficient%20voxel-based%20indirect%20illumination%20tracing%0Ascheme%20that%20stores%20direction-aware%20outgoing%20radiance%20in%20each%203D%20Gaussian%20to%0Adisentangle%20secondary%20illumination%20for%20approximating%20multi-bounce%20light%0Atransport.%20To%20further%20enhance%20the%20illumination%20disentanglement%2C%20we%20represent%20a%0Ahigh-resolution%20environmental%20map%20with%20a%20learnable%20low-resolution%20map%20and%20a%0Alightweight%2C%20fully%20convolutional%20network.%20Our%20method%20achieves%20state-of-the-art%0Aperformance%20in%20both%20relighting%20and%20novel%20view%20synthesis%20tasks%20among%20the%0Arecently%20proposed%20inverse%20rendering%20methods%20while%20achieving%20real-time%0Arendering.%20This%20substantiates%20our%20proposed%20method%27s%20efficacy%20and%20broad%0Aapplicability%2C%20highlighting%20its%20potential%20as%20an%20influential%20tool%20in%20various%0Areal-time%20interactive%20graphics%20applications%20such%20as%20material%20editing%20and%0Arelighting.%20The%20code%20will%20be%20released%20at%20https%3A//github.com/guduxiaolang/GIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05133v2&entry.124074799=Read"},
{"title": "REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices", "author": "Chaojie Ji and Yufeng Li and Yiyi Liao", "abstract": "  This work tackles the challenging task of achieving real-time novel view\nsynthesis for reflective surfaces across various scenes. Existing real-time\nrendering methods, especially those based on meshes, often have subpar\nperformance in modeling surfaces with rich view-dependent appearances. Our key\nidea lies in leveraging meshes for rendering acceleration while incorporating a\nnovel approach to parameterize view-dependent information. We decompose the\ncolor into diffuse and specular, and model the specular color in the reflected\ndirection based on a neural environment map. Our experiments demonstrate that\nour method achieves comparable reconstruction quality for highly reflective\nsurfaces compared to state-of-the-art offline methods, while also efficiently\nenabling real-time rendering on edge devices such as smartphones.\n", "link": "http://arxiv.org/abs/2403.16481v2", "date": "2024-08-15", "relevancy": 2.7497, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5735}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5382}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REFRAME%3A%20Reflective%20Surface%20Real-Time%20Rendering%20for%20Mobile%20Devices&body=Title%3A%20REFRAME%3A%20Reflective%20Surface%20Real-Time%20Rendering%20for%20Mobile%20Devices%0AAuthor%3A%20Chaojie%20Ji%20and%20Yufeng%20Li%20and%20Yiyi%20Liao%0AAbstract%3A%20%20%20This%20work%20tackles%20the%20challenging%20task%20of%20achieving%20real-time%20novel%20view%0Asynthesis%20for%20reflective%20surfaces%20across%20various%20scenes.%20Existing%20real-time%0Arendering%20methods%2C%20especially%20those%20based%20on%20meshes%2C%20often%20have%20subpar%0Aperformance%20in%20modeling%20surfaces%20with%20rich%20view-dependent%20appearances.%20Our%20key%0Aidea%20lies%20in%20leveraging%20meshes%20for%20rendering%20acceleration%20while%20incorporating%20a%0Anovel%20approach%20to%20parameterize%20view-dependent%20information.%20We%20decompose%20the%0Acolor%20into%20diffuse%20and%20specular%2C%20and%20model%20the%20specular%20color%20in%20the%20reflected%0Adirection%20based%20on%20a%20neural%20environment%20map.%20Our%20experiments%20demonstrate%20that%0Aour%20method%20achieves%20comparable%20reconstruction%20quality%20for%20highly%20reflective%0Asurfaces%20compared%20to%20state-of-the-art%20offline%20methods%2C%20while%20also%20efficiently%0Aenabling%20real-time%20rendering%20on%20edge%20devices%20such%20as%20smartphones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16481v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREFRAME%253A%2520Reflective%2520Surface%2520Real-Time%2520Rendering%2520for%2520Mobile%2520Devices%26entry.906535625%3DChaojie%2520Ji%2520and%2520Yufeng%2520Li%2520and%2520Yiyi%2520Liao%26entry.1292438233%3D%2520%2520This%2520work%2520tackles%2520the%2520challenging%2520task%2520of%2520achieving%2520real-time%2520novel%2520view%250Asynthesis%2520for%2520reflective%2520surfaces%2520across%2520various%2520scenes.%2520Existing%2520real-time%250Arendering%2520methods%252C%2520especially%2520those%2520based%2520on%2520meshes%252C%2520often%2520have%2520subpar%250Aperformance%2520in%2520modeling%2520surfaces%2520with%2520rich%2520view-dependent%2520appearances.%2520Our%2520key%250Aidea%2520lies%2520in%2520leveraging%2520meshes%2520for%2520rendering%2520acceleration%2520while%2520incorporating%2520a%250Anovel%2520approach%2520to%2520parameterize%2520view-dependent%2520information.%2520We%2520decompose%2520the%250Acolor%2520into%2520diffuse%2520and%2520specular%252C%2520and%2520model%2520the%2520specular%2520color%2520in%2520the%2520reflected%250Adirection%2520based%2520on%2520a%2520neural%2520environment%2520map.%2520Our%2520experiments%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520comparable%2520reconstruction%2520quality%2520for%2520highly%2520reflective%250Asurfaces%2520compared%2520to%2520state-of-the-art%2520offline%2520methods%252C%2520while%2520also%2520efficiently%250Aenabling%2520real-time%2520rendering%2520on%2520edge%2520devices%2520such%2520as%2520smartphones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16481v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REFRAME%3A%20Reflective%20Surface%20Real-Time%20Rendering%20for%20Mobile%20Devices&entry.906535625=Chaojie%20Ji%20and%20Yufeng%20Li%20and%20Yiyi%20Liao&entry.1292438233=%20%20This%20work%20tackles%20the%20challenging%20task%20of%20achieving%20real-time%20novel%20view%0Asynthesis%20for%20reflective%20surfaces%20across%20various%20scenes.%20Existing%20real-time%0Arendering%20methods%2C%20especially%20those%20based%20on%20meshes%2C%20often%20have%20subpar%0Aperformance%20in%20modeling%20surfaces%20with%20rich%20view-dependent%20appearances.%20Our%20key%0Aidea%20lies%20in%20leveraging%20meshes%20for%20rendering%20acceleration%20while%20incorporating%20a%0Anovel%20approach%20to%20parameterize%20view-dependent%20information.%20We%20decompose%20the%0Acolor%20into%20diffuse%20and%20specular%2C%20and%20model%20the%20specular%20color%20in%20the%20reflected%0Adirection%20based%20on%20a%20neural%20environment%20map.%20Our%20experiments%20demonstrate%20that%0Aour%20method%20achieves%20comparable%20reconstruction%20quality%20for%20highly%20reflective%0Asurfaces%20compared%20to%20state-of-the-art%20offline%20methods%2C%20while%20also%20efficiently%0Aenabling%20real-time%20rendering%20on%20edge%20devices%20such%20as%20smartphones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16481v2&entry.124074799=Read"},
{"title": "Capturing Human Motion from Monocular Images in World Space with\n  Weak-supervised Calibration", "author": "Wei Yao and Hongwen Zhang and Yunlian Sun and Jinhui Tang", "abstract": "  Previous methods for 3D human motion recovery from monocular images often\nfall short due to reliance on camera coordinates, leading to inaccuracies in\nreal-world applications where complex shooting conditions are prevalent. The\nlimited availability and diversity of focal length labels further exacerbate\nmisalignment issues in reconstructed 3D human bodies. To address these\nchallenges, we introduce W-HMR, a weak-supervised calibration method that\npredicts \"reasonable\" focal lengths based on body distortion information,\neliminating the need for precise focal length labels. Our approach enhances 2D\nsupervision precision and recovery accuracy. Additionally, we present the\nOrientCorrect module, which corrects body orientation for plausible\nreconstructions in world space, avoiding the error accumulation associated with\ninaccurate camera rotation predictions. Our contributions include a novel\nweak-supervised camera calibration technique, an effective orientation\ncorrection module, and a decoupling strategy that significantly improves the\ngeneralizability and accuracy of human motion recovery in both camera and world\ncoordinates. The robustness of W-HMR is validated through extensive experiments\non various datasets, showcasing its superiority over existing methods. Codes\nand demos have been released on the project page\nhttps://yw0208.github.io/w-hmr/.\n", "link": "http://arxiv.org/abs/2311.17460v4", "date": "2024-08-15", "relevancy": 2.7334, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5775}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capturing%20Human%20Motion%20from%20Monocular%20Images%20in%20World%20Space%20with%0A%20%20Weak-supervised%20Calibration&body=Title%3A%20Capturing%20Human%20Motion%20from%20Monocular%20Images%20in%20World%20Space%20with%0A%20%20Weak-supervised%20Calibration%0AAuthor%3A%20Wei%20Yao%20and%20Hongwen%20Zhang%20and%20Yunlian%20Sun%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Previous%20methods%20for%203D%20human%20motion%20recovery%20from%20monocular%20images%20often%0Afall%20short%20due%20to%20reliance%20on%20camera%20coordinates%2C%20leading%20to%20inaccuracies%20in%0Areal-world%20applications%20where%20complex%20shooting%20conditions%20are%20prevalent.%20The%0Alimited%20availability%20and%20diversity%20of%20focal%20length%20labels%20further%20exacerbate%0Amisalignment%20issues%20in%20reconstructed%203D%20human%20bodies.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20W-HMR%2C%20a%20weak-supervised%20calibration%20method%20that%0Apredicts%20%22reasonable%22%20focal%20lengths%20based%20on%20body%20distortion%20information%2C%0Aeliminating%20the%20need%20for%20precise%20focal%20length%20labels.%20Our%20approach%20enhances%202D%0Asupervision%20precision%20and%20recovery%20accuracy.%20Additionally%2C%20we%20present%20the%0AOrientCorrect%20module%2C%20which%20corrects%20body%20orientation%20for%20plausible%0Areconstructions%20in%20world%20space%2C%20avoiding%20the%20error%20accumulation%20associated%20with%0Ainaccurate%20camera%20rotation%20predictions.%20Our%20contributions%20include%20a%20novel%0Aweak-supervised%20camera%20calibration%20technique%2C%20an%20effective%20orientation%0Acorrection%20module%2C%20and%20a%20decoupling%20strategy%20that%20significantly%20improves%20the%0Ageneralizability%20and%20accuracy%20of%20human%20motion%20recovery%20in%20both%20camera%20and%20world%0Acoordinates.%20The%20robustness%20of%20W-HMR%20is%20validated%20through%20extensive%20experiments%0Aon%20various%20datasets%2C%20showcasing%20its%20superiority%20over%20existing%20methods.%20Codes%0Aand%20demos%20have%20been%20released%20on%20the%20project%20page%0Ahttps%3A//yw0208.github.io/w-hmr/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17460v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapturing%2520Human%2520Motion%2520from%2520Monocular%2520Images%2520in%2520World%2520Space%2520with%250A%2520%2520Weak-supervised%2520Calibration%26entry.906535625%3DWei%2520Yao%2520and%2520Hongwen%2520Zhang%2520and%2520Yunlian%2520Sun%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Previous%2520methods%2520for%25203D%2520human%2520motion%2520recovery%2520from%2520monocular%2520images%2520often%250Afall%2520short%2520due%2520to%2520reliance%2520on%2520camera%2520coordinates%252C%2520leading%2520to%2520inaccuracies%2520in%250Areal-world%2520applications%2520where%2520complex%2520shooting%2520conditions%2520are%2520prevalent.%2520The%250Alimited%2520availability%2520and%2520diversity%2520of%2520focal%2520length%2520labels%2520further%2520exacerbate%250Amisalignment%2520issues%2520in%2520reconstructed%25203D%2520human%2520bodies.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520W-HMR%252C%2520a%2520weak-supervised%2520calibration%2520method%2520that%250Apredicts%2520%2522reasonable%2522%2520focal%2520lengths%2520based%2520on%2520body%2520distortion%2520information%252C%250Aeliminating%2520the%2520need%2520for%2520precise%2520focal%2520length%2520labels.%2520Our%2520approach%2520enhances%25202D%250Asupervision%2520precision%2520and%2520recovery%2520accuracy.%2520Additionally%252C%2520we%2520present%2520the%250AOrientCorrect%2520module%252C%2520which%2520corrects%2520body%2520orientation%2520for%2520plausible%250Areconstructions%2520in%2520world%2520space%252C%2520avoiding%2520the%2520error%2520accumulation%2520associated%2520with%250Ainaccurate%2520camera%2520rotation%2520predictions.%2520Our%2520contributions%2520include%2520a%2520novel%250Aweak-supervised%2520camera%2520calibration%2520technique%252C%2520an%2520effective%2520orientation%250Acorrection%2520module%252C%2520and%2520a%2520decoupling%2520strategy%2520that%2520significantly%2520improves%2520the%250Ageneralizability%2520and%2520accuracy%2520of%2520human%2520motion%2520recovery%2520in%2520both%2520camera%2520and%2520world%250Acoordinates.%2520The%2520robustness%2520of%2520W-HMR%2520is%2520validated%2520through%2520extensive%2520experiments%250Aon%2520various%2520datasets%252C%2520showcasing%2520its%2520superiority%2520over%2520existing%2520methods.%2520Codes%250Aand%2520demos%2520have%2520been%2520released%2520on%2520the%2520project%2520page%250Ahttps%253A//yw0208.github.io/w-hmr/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17460v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capturing%20Human%20Motion%20from%20Monocular%20Images%20in%20World%20Space%20with%0A%20%20Weak-supervised%20Calibration&entry.906535625=Wei%20Yao%20and%20Hongwen%20Zhang%20and%20Yunlian%20Sun%20and%20Jinhui%20Tang&entry.1292438233=%20%20Previous%20methods%20for%203D%20human%20motion%20recovery%20from%20monocular%20images%20often%0Afall%20short%20due%20to%20reliance%20on%20camera%20coordinates%2C%20leading%20to%20inaccuracies%20in%0Areal-world%20applications%20where%20complex%20shooting%20conditions%20are%20prevalent.%20The%0Alimited%20availability%20and%20diversity%20of%20focal%20length%20labels%20further%20exacerbate%0Amisalignment%20issues%20in%20reconstructed%203D%20human%20bodies.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20W-HMR%2C%20a%20weak-supervised%20calibration%20method%20that%0Apredicts%20%22reasonable%22%20focal%20lengths%20based%20on%20body%20distortion%20information%2C%0Aeliminating%20the%20need%20for%20precise%20focal%20length%20labels.%20Our%20approach%20enhances%202D%0Asupervision%20precision%20and%20recovery%20accuracy.%20Additionally%2C%20we%20present%20the%0AOrientCorrect%20module%2C%20which%20corrects%20body%20orientation%20for%20plausible%0Areconstructions%20in%20world%20space%2C%20avoiding%20the%20error%20accumulation%20associated%20with%0Ainaccurate%20camera%20rotation%20predictions.%20Our%20contributions%20include%20a%20novel%0Aweak-supervised%20camera%20calibration%20technique%2C%20an%20effective%20orientation%0Acorrection%20module%2C%20and%20a%20decoupling%20strategy%20that%20significantly%20improves%20the%0Ageneralizability%20and%20accuracy%20of%20human%20motion%20recovery%20in%20both%20camera%20and%20world%0Acoordinates.%20The%20robustness%20of%20W-HMR%20is%20validated%20through%20extensive%20experiments%0Aon%20various%20datasets%2C%20showcasing%20its%20superiority%20over%20existing%20methods.%20Codes%0Aand%20demos%20have%20been%20released%20on%20the%20project%20page%0Ahttps%3A//yw0208.github.io/w-hmr/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17460v4&entry.124074799=Read"},
{"title": "FancyVideo: Towards Dynamic and Consistent Video Generation via\n  Cross-frame Textual Guidance", "author": "Jiasong Feng and Ao Ma and Jing Wang and Bo Cheng and Xiaodan Liang and Dawei Leng and Yuhui Yin", "abstract": "  Synthesizing motion-rich and temporally consistent videos remains a challenge\nin artificial intelligence, especially when dealing with extended durations.\nExisting text-to-video (T2V) models commonly employ spatial cross-attention for\ntext control, equivalently guiding different frame generations without\nframe-specific textual guidance. Thus, the model's capacity to comprehend the\ntemporal logic conveyed in prompts and generate videos with coherent motion is\nrestricted. To tackle this limitation, we introduce FancyVideo, an innovative\nvideo generator that improves the existing text-control mechanism with the\nwell-designed Cross-frame Textual Guidance Module (CTGM). Specifically, CTGM\nincorporates the Temporal Information Injector (TII), Temporal Affinity Refiner\n(TAR), and Temporal Feature Booster (TFB) at the beginning, middle, and end of\ncross-attention, respectively, to achieve frame-specific textual guidance.\nFirstly, TII injects frame-specific information from latent features into text\nconditions, thereby obtaining cross-frame textual conditions. Then, TAR refines\nthe correlation matrix between cross-frame textual conditions and latent\nfeatures along the time dimension. Lastly, TFB boosts the temporal consistency\nof latent features. Extensive experiments comprising both quantitative and\nqualitative evaluations demonstrate the effectiveness of FancyVideo. Our\napproach achieves state-of-the-art T2V generation results on the EvalCrafter\nbenchmark and facilitates the synthesis of dynamic and consistent videos. The\nvideo show results can be available at https://fancyvideo.github.io/, and we\nwill make our code and model weights publicly available.\n", "link": "http://arxiv.org/abs/2408.08189v1", "date": "2024-08-15", "relevancy": 2.674, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7018}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6929}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FancyVideo%3A%20Towards%20Dynamic%20and%20Consistent%20Video%20Generation%20via%0A%20%20Cross-frame%20Textual%20Guidance&body=Title%3A%20FancyVideo%3A%20Towards%20Dynamic%20and%20Consistent%20Video%20Generation%20via%0A%20%20Cross-frame%20Textual%20Guidance%0AAuthor%3A%20Jiasong%20Feng%20and%20Ao%20Ma%20and%20Jing%20Wang%20and%20Bo%20Cheng%20and%20Xiaodan%20Liang%20and%20Dawei%20Leng%20and%20Yuhui%20Yin%0AAbstract%3A%20%20%20Synthesizing%20motion-rich%20and%20temporally%20consistent%20videos%20remains%20a%20challenge%0Ain%20artificial%20intelligence%2C%20especially%20when%20dealing%20with%20extended%20durations.%0AExisting%20text-to-video%20%28T2V%29%20models%20commonly%20employ%20spatial%20cross-attention%20for%0Atext%20control%2C%20equivalently%20guiding%20different%20frame%20generations%20without%0Aframe-specific%20textual%20guidance.%20Thus%2C%20the%20model%27s%20capacity%20to%20comprehend%20the%0Atemporal%20logic%20conveyed%20in%20prompts%20and%20generate%20videos%20with%20coherent%20motion%20is%0Arestricted.%20To%20tackle%20this%20limitation%2C%20we%20introduce%20FancyVideo%2C%20an%20innovative%0Avideo%20generator%20that%20improves%20the%20existing%20text-control%20mechanism%20with%20the%0Awell-designed%20Cross-frame%20Textual%20Guidance%20Module%20%28CTGM%29.%20Specifically%2C%20CTGM%0Aincorporates%20the%20Temporal%20Information%20Injector%20%28TII%29%2C%20Temporal%20Affinity%20Refiner%0A%28TAR%29%2C%20and%20Temporal%20Feature%20Booster%20%28TFB%29%20at%20the%20beginning%2C%20middle%2C%20and%20end%20of%0Across-attention%2C%20respectively%2C%20to%20achieve%20frame-specific%20textual%20guidance.%0AFirstly%2C%20TII%20injects%20frame-specific%20information%20from%20latent%20features%20into%20text%0Aconditions%2C%20thereby%20obtaining%20cross-frame%20textual%20conditions.%20Then%2C%20TAR%20refines%0Athe%20correlation%20matrix%20between%20cross-frame%20textual%20conditions%20and%20latent%0Afeatures%20along%20the%20time%20dimension.%20Lastly%2C%20TFB%20boosts%20the%20temporal%20consistency%0Aof%20latent%20features.%20Extensive%20experiments%20comprising%20both%20quantitative%20and%0Aqualitative%20evaluations%20demonstrate%20the%20effectiveness%20of%20FancyVideo.%20Our%0Aapproach%20achieves%20state-of-the-art%20T2V%20generation%20results%20on%20the%20EvalCrafter%0Abenchmark%20and%20facilitates%20the%20synthesis%20of%20dynamic%20and%20consistent%20videos.%20The%0Avideo%20show%20results%20can%20be%20available%20at%20https%3A//fancyvideo.github.io/%2C%20and%20we%0Awill%20make%20our%20code%20and%20model%20weights%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFancyVideo%253A%2520Towards%2520Dynamic%2520and%2520Consistent%2520Video%2520Generation%2520via%250A%2520%2520Cross-frame%2520Textual%2520Guidance%26entry.906535625%3DJiasong%2520Feng%2520and%2520Ao%2520Ma%2520and%2520Jing%2520Wang%2520and%2520Bo%2520Cheng%2520and%2520Xiaodan%2520Liang%2520and%2520Dawei%2520Leng%2520and%2520Yuhui%2520Yin%26entry.1292438233%3D%2520%2520Synthesizing%2520motion-rich%2520and%2520temporally%2520consistent%2520videos%2520remains%2520a%2520challenge%250Ain%2520artificial%2520intelligence%252C%2520especially%2520when%2520dealing%2520with%2520extended%2520durations.%250AExisting%2520text-to-video%2520%2528T2V%2529%2520models%2520commonly%2520employ%2520spatial%2520cross-attention%2520for%250Atext%2520control%252C%2520equivalently%2520guiding%2520different%2520frame%2520generations%2520without%250Aframe-specific%2520textual%2520guidance.%2520Thus%252C%2520the%2520model%2527s%2520capacity%2520to%2520comprehend%2520the%250Atemporal%2520logic%2520conveyed%2520in%2520prompts%2520and%2520generate%2520videos%2520with%2520coherent%2520motion%2520is%250Arestricted.%2520To%2520tackle%2520this%2520limitation%252C%2520we%2520introduce%2520FancyVideo%252C%2520an%2520innovative%250Avideo%2520generator%2520that%2520improves%2520the%2520existing%2520text-control%2520mechanism%2520with%2520the%250Awell-designed%2520Cross-frame%2520Textual%2520Guidance%2520Module%2520%2528CTGM%2529.%2520Specifically%252C%2520CTGM%250Aincorporates%2520the%2520Temporal%2520Information%2520Injector%2520%2528TII%2529%252C%2520Temporal%2520Affinity%2520Refiner%250A%2528TAR%2529%252C%2520and%2520Temporal%2520Feature%2520Booster%2520%2528TFB%2529%2520at%2520the%2520beginning%252C%2520middle%252C%2520and%2520end%2520of%250Across-attention%252C%2520respectively%252C%2520to%2520achieve%2520frame-specific%2520textual%2520guidance.%250AFirstly%252C%2520TII%2520injects%2520frame-specific%2520information%2520from%2520latent%2520features%2520into%2520text%250Aconditions%252C%2520thereby%2520obtaining%2520cross-frame%2520textual%2520conditions.%2520Then%252C%2520TAR%2520refines%250Athe%2520correlation%2520matrix%2520between%2520cross-frame%2520textual%2520conditions%2520and%2520latent%250Afeatures%2520along%2520the%2520time%2520dimension.%2520Lastly%252C%2520TFB%2520boosts%2520the%2520temporal%2520consistency%250Aof%2520latent%2520features.%2520Extensive%2520experiments%2520comprising%2520both%2520quantitative%2520and%250Aqualitative%2520evaluations%2520demonstrate%2520the%2520effectiveness%2520of%2520FancyVideo.%2520Our%250Aapproach%2520achieves%2520state-of-the-art%2520T2V%2520generation%2520results%2520on%2520the%2520EvalCrafter%250Abenchmark%2520and%2520facilitates%2520the%2520synthesis%2520of%2520dynamic%2520and%2520consistent%2520videos.%2520The%250Avideo%2520show%2520results%2520can%2520be%2520available%2520at%2520https%253A//fancyvideo.github.io/%252C%2520and%2520we%250Awill%2520make%2520our%2520code%2520and%2520model%2520weights%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FancyVideo%3A%20Towards%20Dynamic%20and%20Consistent%20Video%20Generation%20via%0A%20%20Cross-frame%20Textual%20Guidance&entry.906535625=Jiasong%20Feng%20and%20Ao%20Ma%20and%20Jing%20Wang%20and%20Bo%20Cheng%20and%20Xiaodan%20Liang%20and%20Dawei%20Leng%20and%20Yuhui%20Yin&entry.1292438233=%20%20Synthesizing%20motion-rich%20and%20temporally%20consistent%20videos%20remains%20a%20challenge%0Ain%20artificial%20intelligence%2C%20especially%20when%20dealing%20with%20extended%20durations.%0AExisting%20text-to-video%20%28T2V%29%20models%20commonly%20employ%20spatial%20cross-attention%20for%0Atext%20control%2C%20equivalently%20guiding%20different%20frame%20generations%20without%0Aframe-specific%20textual%20guidance.%20Thus%2C%20the%20model%27s%20capacity%20to%20comprehend%20the%0Atemporal%20logic%20conveyed%20in%20prompts%20and%20generate%20videos%20with%20coherent%20motion%20is%0Arestricted.%20To%20tackle%20this%20limitation%2C%20we%20introduce%20FancyVideo%2C%20an%20innovative%0Avideo%20generator%20that%20improves%20the%20existing%20text-control%20mechanism%20with%20the%0Awell-designed%20Cross-frame%20Textual%20Guidance%20Module%20%28CTGM%29.%20Specifically%2C%20CTGM%0Aincorporates%20the%20Temporal%20Information%20Injector%20%28TII%29%2C%20Temporal%20Affinity%20Refiner%0A%28TAR%29%2C%20and%20Temporal%20Feature%20Booster%20%28TFB%29%20at%20the%20beginning%2C%20middle%2C%20and%20end%20of%0Across-attention%2C%20respectively%2C%20to%20achieve%20frame-specific%20textual%20guidance.%0AFirstly%2C%20TII%20injects%20frame-specific%20information%20from%20latent%20features%20into%20text%0Aconditions%2C%20thereby%20obtaining%20cross-frame%20textual%20conditions.%20Then%2C%20TAR%20refines%0Athe%20correlation%20matrix%20between%20cross-frame%20textual%20conditions%20and%20latent%0Afeatures%20along%20the%20time%20dimension.%20Lastly%2C%20TFB%20boosts%20the%20temporal%20consistency%0Aof%20latent%20features.%20Extensive%20experiments%20comprising%20both%20quantitative%20and%0Aqualitative%20evaluations%20demonstrate%20the%20effectiveness%20of%20FancyVideo.%20Our%0Aapproach%20achieves%20state-of-the-art%20T2V%20generation%20results%20on%20the%20EvalCrafter%0Abenchmark%20and%20facilitates%20the%20synthesis%20of%20dynamic%20and%20consistent%20videos.%20The%0Avideo%20show%20results%20can%20be%20available%20at%20https%3A//fancyvideo.github.io/%2C%20and%20we%0Awill%20make%20our%20code%20and%20model%20weights%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08189v1&entry.124074799=Read"},
{"title": "Long-Tailed Classification Based on Coarse-Grained Leading Forest and\n  Multi-Center Loss", "author": "Jinye Yang and Ji Xu and Di Wu and Jianhang Tang and Shaobo Li and Guoyin Wang", "abstract": "  Long-tailed (LT) classification is an unavoidable and challenging problem in\nthe real world. Most existing long-tailed classification methods focus only on\nsolving the class-wise imbalance while ignoring the attribute-wise imbalance.\nThe deviation of a classification model is caused by both class-wise and\nattribute-wise imbalance. Due to the fact that attributes are implicit in most\ndatasets and the combination of attributes is complex, attribute-wise imbalance\nis more difficult to handle. For this purpose, we proposed a novel long-tailed\nclassification framework, aiming to build a multi-granularity classification\nmodel by means of invariant feature learning. This method first unsupervisedly\nconstructs Coarse-Grained forest (CLF) to better characterize the distribution\nof attributes within a class. Depending on the distribution of attributes, one\ncan customize suitable sampling strategies to construct different imbalanced\ndatasets. We then introduce multi-center loss (MCL) that aims to gradually\neliminate confusing attributes during feature learning process. The proposed\nframework does not necessarily couple to a specific LT classification model\nstructure and can be integrated with any existing LT method as an independent\ncomponent. Extensive experiments show that our approach achieves\nstate-of-the-art performance on both existing benchmarks ImageNet-GLT and\nMSCOCO-GLT and can improve the performance of existing LT methods. Our codes\nare available on GitHub: \\url{https://github.com/jinyery/cognisance}\n", "link": "http://arxiv.org/abs/2310.08206v3", "date": "2024-08-15", "relevancy": 2.6533, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5845}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Tailed%20Classification%20Based%20on%20Coarse-Grained%20Leading%20Forest%20and%0A%20%20Multi-Center%20Loss&body=Title%3A%20Long-Tailed%20Classification%20Based%20on%20Coarse-Grained%20Leading%20Forest%20and%0A%20%20Multi-Center%20Loss%0AAuthor%3A%20Jinye%20Yang%20and%20Ji%20Xu%20and%20Di%20Wu%20and%20Jianhang%20Tang%20and%20Shaobo%20Li%20and%20Guoyin%20Wang%0AAbstract%3A%20%20%20Long-tailed%20%28LT%29%20classification%20is%20an%20unavoidable%20and%20challenging%20problem%20in%0Athe%20real%20world.%20Most%20existing%20long-tailed%20classification%20methods%20focus%20only%20on%0Asolving%20the%20class-wise%20imbalance%20while%20ignoring%20the%20attribute-wise%20imbalance.%0AThe%20deviation%20of%20a%20classification%20model%20is%20caused%20by%20both%20class-wise%20and%0Aattribute-wise%20imbalance.%20Due%20to%20the%20fact%20that%20attributes%20are%20implicit%20in%20most%0Adatasets%20and%20the%20combination%20of%20attributes%20is%20complex%2C%20attribute-wise%20imbalance%0Ais%20more%20difficult%20to%20handle.%20For%20this%20purpose%2C%20we%20proposed%20a%20novel%20long-tailed%0Aclassification%20framework%2C%20aiming%20to%20build%20a%20multi-granularity%20classification%0Amodel%20by%20means%20of%20invariant%20feature%20learning.%20This%20method%20first%20unsupervisedly%0Aconstructs%20Coarse-Grained%20forest%20%28CLF%29%20to%20better%20characterize%20the%20distribution%0Aof%20attributes%20within%20a%20class.%20Depending%20on%20the%20distribution%20of%20attributes%2C%20one%0Acan%20customize%20suitable%20sampling%20strategies%20to%20construct%20different%20imbalanced%0Adatasets.%20We%20then%20introduce%20multi-center%20loss%20%28MCL%29%20that%20aims%20to%20gradually%0Aeliminate%20confusing%20attributes%20during%20feature%20learning%20process.%20The%20proposed%0Aframework%20does%20not%20necessarily%20couple%20to%20a%20specific%20LT%20classification%20model%0Astructure%20and%20can%20be%20integrated%20with%20any%20existing%20LT%20method%20as%20an%20independent%0Acomponent.%20Extensive%20experiments%20show%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance%20on%20both%20existing%20benchmarks%20ImageNet-GLT%20and%0AMSCOCO-GLT%20and%20can%20improve%20the%20performance%20of%20existing%20LT%20methods.%20Our%20codes%0Aare%20available%20on%20GitHub%3A%20%5Curl%7Bhttps%3A//github.com/jinyery/cognisance%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08206v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Tailed%2520Classification%2520Based%2520on%2520Coarse-Grained%2520Leading%2520Forest%2520and%250A%2520%2520Multi-Center%2520Loss%26entry.906535625%3DJinye%2520Yang%2520and%2520Ji%2520Xu%2520and%2520Di%2520Wu%2520and%2520Jianhang%2520Tang%2520and%2520Shaobo%2520Li%2520and%2520Guoyin%2520Wang%26entry.1292438233%3D%2520%2520Long-tailed%2520%2528LT%2529%2520classification%2520is%2520an%2520unavoidable%2520and%2520challenging%2520problem%2520in%250Athe%2520real%2520world.%2520Most%2520existing%2520long-tailed%2520classification%2520methods%2520focus%2520only%2520on%250Asolving%2520the%2520class-wise%2520imbalance%2520while%2520ignoring%2520the%2520attribute-wise%2520imbalance.%250AThe%2520deviation%2520of%2520a%2520classification%2520model%2520is%2520caused%2520by%2520both%2520class-wise%2520and%250Aattribute-wise%2520imbalance.%2520Due%2520to%2520the%2520fact%2520that%2520attributes%2520are%2520implicit%2520in%2520most%250Adatasets%2520and%2520the%2520combination%2520of%2520attributes%2520is%2520complex%252C%2520attribute-wise%2520imbalance%250Ais%2520more%2520difficult%2520to%2520handle.%2520For%2520this%2520purpose%252C%2520we%2520proposed%2520a%2520novel%2520long-tailed%250Aclassification%2520framework%252C%2520aiming%2520to%2520build%2520a%2520multi-granularity%2520classification%250Amodel%2520by%2520means%2520of%2520invariant%2520feature%2520learning.%2520This%2520method%2520first%2520unsupervisedly%250Aconstructs%2520Coarse-Grained%2520forest%2520%2528CLF%2529%2520to%2520better%2520characterize%2520the%2520distribution%250Aof%2520attributes%2520within%2520a%2520class.%2520Depending%2520on%2520the%2520distribution%2520of%2520attributes%252C%2520one%250Acan%2520customize%2520suitable%2520sampling%2520strategies%2520to%2520construct%2520different%2520imbalanced%250Adatasets.%2520We%2520then%2520introduce%2520multi-center%2520loss%2520%2528MCL%2529%2520that%2520aims%2520to%2520gradually%250Aeliminate%2520confusing%2520attributes%2520during%2520feature%2520learning%2520process.%2520The%2520proposed%250Aframework%2520does%2520not%2520necessarily%2520couple%2520to%2520a%2520specific%2520LT%2520classification%2520model%250Astructure%2520and%2520can%2520be%2520integrated%2520with%2520any%2520existing%2520LT%2520method%2520as%2520an%2520independent%250Acomponent.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%2520achieves%250Astate-of-the-art%2520performance%2520on%2520both%2520existing%2520benchmarks%2520ImageNet-GLT%2520and%250AMSCOCO-GLT%2520and%2520can%2520improve%2520the%2520performance%2520of%2520existing%2520LT%2520methods.%2520Our%2520codes%250Aare%2520available%2520on%2520GitHub%253A%2520%255Curl%257Bhttps%253A//github.com/jinyery/cognisance%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08206v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Tailed%20Classification%20Based%20on%20Coarse-Grained%20Leading%20Forest%20and%0A%20%20Multi-Center%20Loss&entry.906535625=Jinye%20Yang%20and%20Ji%20Xu%20and%20Di%20Wu%20and%20Jianhang%20Tang%20and%20Shaobo%20Li%20and%20Guoyin%20Wang&entry.1292438233=%20%20Long-tailed%20%28LT%29%20classification%20is%20an%20unavoidable%20and%20challenging%20problem%20in%0Athe%20real%20world.%20Most%20existing%20long-tailed%20classification%20methods%20focus%20only%20on%0Asolving%20the%20class-wise%20imbalance%20while%20ignoring%20the%20attribute-wise%20imbalance.%0AThe%20deviation%20of%20a%20classification%20model%20is%20caused%20by%20both%20class-wise%20and%0Aattribute-wise%20imbalance.%20Due%20to%20the%20fact%20that%20attributes%20are%20implicit%20in%20most%0Adatasets%20and%20the%20combination%20of%20attributes%20is%20complex%2C%20attribute-wise%20imbalance%0Ais%20more%20difficult%20to%20handle.%20For%20this%20purpose%2C%20we%20proposed%20a%20novel%20long-tailed%0Aclassification%20framework%2C%20aiming%20to%20build%20a%20multi-granularity%20classification%0Amodel%20by%20means%20of%20invariant%20feature%20learning.%20This%20method%20first%20unsupervisedly%0Aconstructs%20Coarse-Grained%20forest%20%28CLF%29%20to%20better%20characterize%20the%20distribution%0Aof%20attributes%20within%20a%20class.%20Depending%20on%20the%20distribution%20of%20attributes%2C%20one%0Acan%20customize%20suitable%20sampling%20strategies%20to%20construct%20different%20imbalanced%0Adatasets.%20We%20then%20introduce%20multi-center%20loss%20%28MCL%29%20that%20aims%20to%20gradually%0Aeliminate%20confusing%20attributes%20during%20feature%20learning%20process.%20The%20proposed%0Aframework%20does%20not%20necessarily%20couple%20to%20a%20specific%20LT%20classification%20model%0Astructure%20and%20can%20be%20integrated%20with%20any%20existing%20LT%20method%20as%20an%20independent%0Acomponent.%20Extensive%20experiments%20show%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance%20on%20both%20existing%20benchmarks%20ImageNet-GLT%20and%0AMSCOCO-GLT%20and%20can%20improve%20the%20performance%20of%20existing%20LT%20methods.%20Our%20codes%0Aare%20available%20on%20GitHub%3A%20%5Curl%7Bhttps%3A//github.com/jinyery/cognisance%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08206v3&entry.124074799=Read"},
{"title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual\n  Learning with Pre-training", "author": "Gengwei Zhang and Liyuan Wang and Guoliang Kang and Ling Chen and Yunchao Wei", "abstract": "  In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA.\n", "link": "http://arxiv.org/abs/2408.08295v1", "date": "2024-08-15", "relevancy": 2.6457, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5229}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLCA%2B%2B%3A%20Unleash%20the%20Power%20of%20Sequential%20Fine-tuning%20for%20Continual%0A%20%20Learning%20with%20Pre-training&body=Title%3A%20SLCA%2B%2B%3A%20Unleash%20the%20Power%20of%20Sequential%20Fine-tuning%20for%20Continual%0A%20%20Learning%20with%20Pre-training%0AAuthor%3A%20Gengwei%20Zhang%20and%20Liyuan%20Wang%20and%20Guoliang%20Kang%20and%20Ling%20Chen%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20In%20recent%20years%2C%20continual%20learning%20with%20pre-training%20%28CLPT%29%20has%20received%0Awidespread%20interest%2C%20instead%20of%20its%20traditional%20focus%20of%20training%20from%20scratch.%0AThe%20use%20of%20strong%20pre-trained%20models%20%28PTMs%29%20can%20greatly%20facilitate%20knowledge%0Atransfer%20and%20alleviate%20catastrophic%20forgetting%2C%20but%20also%20suffers%20from%0Aprogressive%20overfitting%20of%20pre-trained%20knowledge%20into%20specific%20downstream%0Atasks.%20A%20majority%20of%20current%20efforts%20often%20keep%20the%20PTMs%20frozen%20and%20incorporate%0Atask-specific%20prompts%20to%20instruct%20representation%20learning%2C%20coupled%20with%20a%0Aprompt%20selection%20process%20for%20inference.%20However%2C%20due%20to%20the%20limited%20capacity%20of%0Aprompt%20parameters%2C%20this%20strategy%20demonstrates%20only%20sub-optimal%20performance%20in%0Acontinual%20learning.%20In%20comparison%2C%20tuning%20all%20parameters%20of%20PTMs%20often%20provides%0Athe%20greatest%20potential%20for%20representation%20learning%2C%20making%20sequential%0Afine-tuning%20%28Seq%20FT%29%20a%20fundamental%20baseline%20that%20has%20been%20overlooked%20in%20CLPT.%0ATo%20this%20end%2C%20we%20present%20an%20in-depth%20analysis%20of%20the%20progressive%20overfitting%0Aproblem%20from%20the%20lens%20of%20Seq%20FT.%20Considering%20that%20the%20overly%20fast%0Arepresentation%20learning%20and%20the%20biased%20classification%20layer%20constitute%20this%0Aparticular%20problem%2C%20we%20introduce%20the%20advanced%20Slow%20Learner%20with%20Classifier%0AAlignment%20%28SLCA%2B%2B%29%20framework%20to%20unleash%20the%20power%20of%20Seq%20FT%2C%20serving%20as%20a%0Astrong%20baseline%20approach%20for%20CLPT.%20Our%20approach%20involves%20a%20Slow%20Learner%20to%0Aselectively%20reduce%20the%20learning%20rate%20of%20backbone%20parameters%2C%20and%20a%20Classifier%0AAlignment%20to%20align%20the%20disjoint%20classification%20layers%20in%20a%20post-hoc%20fashion.%20We%0Afurther%20enhance%20the%20efficacy%20of%20SL%20with%20a%20symmetric%20cross-entropy%20loss%2C%20as%20well%0Aas%20employ%20a%20parameter-efficient%20strategy%20to%20implement%20Seq%20FT%20with%20SLCA%2B%2B.%0AAcross%20a%20variety%20of%20continual%20learning%20scenarios%20on%20image%20classification%0Abenchmarks%2C%20our%20approach%20provides%20substantial%20improvements%20and%20outperforms%0Astate-of-the-art%20methods%20by%20a%20large%20margin.%20Code%3A%0Ahttps%3A//github.com/GengDavid/SLCA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLCA%252B%252B%253A%2520Unleash%2520the%2520Power%2520of%2520Sequential%2520Fine-tuning%2520for%2520Continual%250A%2520%2520Learning%2520with%2520Pre-training%26entry.906535625%3DGengwei%2520Zhang%2520and%2520Liyuan%2520Wang%2520and%2520Guoliang%2520Kang%2520and%2520Ling%2520Chen%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520continual%2520learning%2520with%2520pre-training%2520%2528CLPT%2529%2520has%2520received%250Awidespread%2520interest%252C%2520instead%2520of%2520its%2520traditional%2520focus%2520of%2520training%2520from%2520scratch.%250AThe%2520use%2520of%2520strong%2520pre-trained%2520models%2520%2528PTMs%2529%2520can%2520greatly%2520facilitate%2520knowledge%250Atransfer%2520and%2520alleviate%2520catastrophic%2520forgetting%252C%2520but%2520also%2520suffers%2520from%250Aprogressive%2520overfitting%2520of%2520pre-trained%2520knowledge%2520into%2520specific%2520downstream%250Atasks.%2520A%2520majority%2520of%2520current%2520efforts%2520often%2520keep%2520the%2520PTMs%2520frozen%2520and%2520incorporate%250Atask-specific%2520prompts%2520to%2520instruct%2520representation%2520learning%252C%2520coupled%2520with%2520a%250Aprompt%2520selection%2520process%2520for%2520inference.%2520However%252C%2520due%2520to%2520the%2520limited%2520capacity%2520of%250Aprompt%2520parameters%252C%2520this%2520strategy%2520demonstrates%2520only%2520sub-optimal%2520performance%2520in%250Acontinual%2520learning.%2520In%2520comparison%252C%2520tuning%2520all%2520parameters%2520of%2520PTMs%2520often%2520provides%250Athe%2520greatest%2520potential%2520for%2520representation%2520learning%252C%2520making%2520sequential%250Afine-tuning%2520%2528Seq%2520FT%2529%2520a%2520fundamental%2520baseline%2520that%2520has%2520been%2520overlooked%2520in%2520CLPT.%250ATo%2520this%2520end%252C%2520we%2520present%2520an%2520in-depth%2520analysis%2520of%2520the%2520progressive%2520overfitting%250Aproblem%2520from%2520the%2520lens%2520of%2520Seq%2520FT.%2520Considering%2520that%2520the%2520overly%2520fast%250Arepresentation%2520learning%2520and%2520the%2520biased%2520classification%2520layer%2520constitute%2520this%250Aparticular%2520problem%252C%2520we%2520introduce%2520the%2520advanced%2520Slow%2520Learner%2520with%2520Classifier%250AAlignment%2520%2528SLCA%252B%252B%2529%2520framework%2520to%2520unleash%2520the%2520power%2520of%2520Seq%2520FT%252C%2520serving%2520as%2520a%250Astrong%2520baseline%2520approach%2520for%2520CLPT.%2520Our%2520approach%2520involves%2520a%2520Slow%2520Learner%2520to%250Aselectively%2520reduce%2520the%2520learning%2520rate%2520of%2520backbone%2520parameters%252C%2520and%2520a%2520Classifier%250AAlignment%2520to%2520align%2520the%2520disjoint%2520classification%2520layers%2520in%2520a%2520post-hoc%2520fashion.%2520We%250Afurther%2520enhance%2520the%2520efficacy%2520of%2520SL%2520with%2520a%2520symmetric%2520cross-entropy%2520loss%252C%2520as%2520well%250Aas%2520employ%2520a%2520parameter-efficient%2520strategy%2520to%2520implement%2520Seq%2520FT%2520with%2520SLCA%252B%252B.%250AAcross%2520a%2520variety%2520of%2520continual%2520learning%2520scenarios%2520on%2520image%2520classification%250Abenchmarks%252C%2520our%2520approach%2520provides%2520substantial%2520improvements%2520and%2520outperforms%250Astate-of-the-art%2520methods%2520by%2520a%2520large%2520margin.%2520Code%253A%250Ahttps%253A//github.com/GengDavid/SLCA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLCA%2B%2B%3A%20Unleash%20the%20Power%20of%20Sequential%20Fine-tuning%20for%20Continual%0A%20%20Learning%20with%20Pre-training&entry.906535625=Gengwei%20Zhang%20and%20Liyuan%20Wang%20and%20Guoliang%20Kang%20and%20Ling%20Chen%20and%20Yunchao%20Wei&entry.1292438233=%20%20In%20recent%20years%2C%20continual%20learning%20with%20pre-training%20%28CLPT%29%20has%20received%0Awidespread%20interest%2C%20instead%20of%20its%20traditional%20focus%20of%20training%20from%20scratch.%0AThe%20use%20of%20strong%20pre-trained%20models%20%28PTMs%29%20can%20greatly%20facilitate%20knowledge%0Atransfer%20and%20alleviate%20catastrophic%20forgetting%2C%20but%20also%20suffers%20from%0Aprogressive%20overfitting%20of%20pre-trained%20knowledge%20into%20specific%20downstream%0Atasks.%20A%20majority%20of%20current%20efforts%20often%20keep%20the%20PTMs%20frozen%20and%20incorporate%0Atask-specific%20prompts%20to%20instruct%20representation%20learning%2C%20coupled%20with%20a%0Aprompt%20selection%20process%20for%20inference.%20However%2C%20due%20to%20the%20limited%20capacity%20of%0Aprompt%20parameters%2C%20this%20strategy%20demonstrates%20only%20sub-optimal%20performance%20in%0Acontinual%20learning.%20In%20comparison%2C%20tuning%20all%20parameters%20of%20PTMs%20often%20provides%0Athe%20greatest%20potential%20for%20representation%20learning%2C%20making%20sequential%0Afine-tuning%20%28Seq%20FT%29%20a%20fundamental%20baseline%20that%20has%20been%20overlooked%20in%20CLPT.%0ATo%20this%20end%2C%20we%20present%20an%20in-depth%20analysis%20of%20the%20progressive%20overfitting%0Aproblem%20from%20the%20lens%20of%20Seq%20FT.%20Considering%20that%20the%20overly%20fast%0Arepresentation%20learning%20and%20the%20biased%20classification%20layer%20constitute%20this%0Aparticular%20problem%2C%20we%20introduce%20the%20advanced%20Slow%20Learner%20with%20Classifier%0AAlignment%20%28SLCA%2B%2B%29%20framework%20to%20unleash%20the%20power%20of%20Seq%20FT%2C%20serving%20as%20a%0Astrong%20baseline%20approach%20for%20CLPT.%20Our%20approach%20involves%20a%20Slow%20Learner%20to%0Aselectively%20reduce%20the%20learning%20rate%20of%20backbone%20parameters%2C%20and%20a%20Classifier%0AAlignment%20to%20align%20the%20disjoint%20classification%20layers%20in%20a%20post-hoc%20fashion.%20We%0Afurther%20enhance%20the%20efficacy%20of%20SL%20with%20a%20symmetric%20cross-entropy%20loss%2C%20as%20well%0Aas%20employ%20a%20parameter-efficient%20strategy%20to%20implement%20Seq%20FT%20with%20SLCA%2B%2B.%0AAcross%20a%20variety%20of%20continual%20learning%20scenarios%20on%20image%20classification%0Abenchmarks%2C%20our%20approach%20provides%20substantial%20improvements%20and%20outperforms%0Astate-of-the-art%20methods%20by%20a%20large%20margin.%20Code%3A%0Ahttps%3A//github.com/GengDavid/SLCA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08295v1&entry.124074799=Read"},
{"title": "PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition", "author": "Chenhongyi Yang and Zehui Chen and Miguel Espinosa and Linus Ericsson and Zhenyu Wang and Jiaming Liu and Elliot J. Crowley", "abstract": "  We present PlainMamba: a simple non-hierarchical state space model (SSM)\ndesigned for general visual recognition. The recent Mamba model has shown how\nSSMs can be highly competitive with other architectures on sequential data and\ninitial attempts have been made to apply it to images. In this paper, we\nfurther adapt the selective scanning process of Mamba to the visual domain,\nenhancing its ability to learn features from two-dimensional images by (i) a\ncontinuous 2D scanning process that improves spatial continuity by ensuring\nadjacency of tokens in the scanning sequence, and (ii) direction-aware updating\nwhich enables the model to discern the spatial relations of tokens by encoding\ndirectional information. Our architecture is designed to be easy to use and\neasy to scale, formed by stacking identical PlainMamba blocks, resulting in a\nmodel with constant width throughout all layers. The architecture is further\nsimplified by removing the need for special tokens. We evaluate PlainMamba on a\nvariety of visual recognition tasks, achieving performance gains over previous\nnon-hierarchical models and is competitive with hierarchical alternatives. For\ntasks requiring high-resolution inputs, in particular, PlainMamba requires much\nless computing while maintaining high performance. Code and models are\navailable at: https://github.com/ChenhongyiYang/PlainMamba .\n", "link": "http://arxiv.org/abs/2403.17695v2", "date": "2024-08-15", "relevancy": 2.6331, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5259}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlainMamba%3A%20Improving%20Non-Hierarchical%20Mamba%20in%20Visual%20Recognition&body=Title%3A%20PlainMamba%3A%20Improving%20Non-Hierarchical%20Mamba%20in%20Visual%20Recognition%0AAuthor%3A%20Chenhongyi%20Yang%20and%20Zehui%20Chen%20and%20Miguel%20Espinosa%20and%20Linus%20Ericsson%20and%20Zhenyu%20Wang%20and%20Jiaming%20Liu%20and%20Elliot%20J.%20Crowley%0AAbstract%3A%20%20%20We%20present%20PlainMamba%3A%20a%20simple%20non-hierarchical%20state%20space%20model%20%28SSM%29%0Adesigned%20for%20general%20visual%20recognition.%20The%20recent%20Mamba%20model%20has%20shown%20how%0ASSMs%20can%20be%20highly%20competitive%20with%20other%20architectures%20on%20sequential%20data%20and%0Ainitial%20attempts%20have%20been%20made%20to%20apply%20it%20to%20images.%20In%20this%20paper%2C%20we%0Afurther%20adapt%20the%20selective%20scanning%20process%20of%20Mamba%20to%20the%20visual%20domain%2C%0Aenhancing%20its%20ability%20to%20learn%20features%20from%20two-dimensional%20images%20by%20%28i%29%20a%0Acontinuous%202D%20scanning%20process%20that%20improves%20spatial%20continuity%20by%20ensuring%0Aadjacency%20of%20tokens%20in%20the%20scanning%20sequence%2C%20and%20%28ii%29%20direction-aware%20updating%0Awhich%20enables%20the%20model%20to%20discern%20the%20spatial%20relations%20of%20tokens%20by%20encoding%0Adirectional%20information.%20Our%20architecture%20is%20designed%20to%20be%20easy%20to%20use%20and%0Aeasy%20to%20scale%2C%20formed%20by%20stacking%20identical%20PlainMamba%20blocks%2C%20resulting%20in%20a%0Amodel%20with%20constant%20width%20throughout%20all%20layers.%20The%20architecture%20is%20further%0Asimplified%20by%20removing%20the%20need%20for%20special%20tokens.%20We%20evaluate%20PlainMamba%20on%20a%0Avariety%20of%20visual%20recognition%20tasks%2C%20achieving%20performance%20gains%20over%20previous%0Anon-hierarchical%20models%20and%20is%20competitive%20with%20hierarchical%20alternatives.%20For%0Atasks%20requiring%20high-resolution%20inputs%2C%20in%20particular%2C%20PlainMamba%20requires%20much%0Aless%20computing%20while%20maintaining%20high%20performance.%20Code%20and%20models%20are%0Aavailable%20at%3A%20https%3A//github.com/ChenhongyiYang/PlainMamba%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlainMamba%253A%2520Improving%2520Non-Hierarchical%2520Mamba%2520in%2520Visual%2520Recognition%26entry.906535625%3DChenhongyi%2520Yang%2520and%2520Zehui%2520Chen%2520and%2520Miguel%2520Espinosa%2520and%2520Linus%2520Ericsson%2520and%2520Zhenyu%2520Wang%2520and%2520Jiaming%2520Liu%2520and%2520Elliot%2520J.%2520Crowley%26entry.1292438233%3D%2520%2520We%2520present%2520PlainMamba%253A%2520a%2520simple%2520non-hierarchical%2520state%2520space%2520model%2520%2528SSM%2529%250Adesigned%2520for%2520general%2520visual%2520recognition.%2520The%2520recent%2520Mamba%2520model%2520has%2520shown%2520how%250ASSMs%2520can%2520be%2520highly%2520competitive%2520with%2520other%2520architectures%2520on%2520sequential%2520data%2520and%250Ainitial%2520attempts%2520have%2520been%2520made%2520to%2520apply%2520it%2520to%2520images.%2520In%2520this%2520paper%252C%2520we%250Afurther%2520adapt%2520the%2520selective%2520scanning%2520process%2520of%2520Mamba%2520to%2520the%2520visual%2520domain%252C%250Aenhancing%2520its%2520ability%2520to%2520learn%2520features%2520from%2520two-dimensional%2520images%2520by%2520%2528i%2529%2520a%250Acontinuous%25202D%2520scanning%2520process%2520that%2520improves%2520spatial%2520continuity%2520by%2520ensuring%250Aadjacency%2520of%2520tokens%2520in%2520the%2520scanning%2520sequence%252C%2520and%2520%2528ii%2529%2520direction-aware%2520updating%250Awhich%2520enables%2520the%2520model%2520to%2520discern%2520the%2520spatial%2520relations%2520of%2520tokens%2520by%2520encoding%250Adirectional%2520information.%2520Our%2520architecture%2520is%2520designed%2520to%2520be%2520easy%2520to%2520use%2520and%250Aeasy%2520to%2520scale%252C%2520formed%2520by%2520stacking%2520identical%2520PlainMamba%2520blocks%252C%2520resulting%2520in%2520a%250Amodel%2520with%2520constant%2520width%2520throughout%2520all%2520layers.%2520The%2520architecture%2520is%2520further%250Asimplified%2520by%2520removing%2520the%2520need%2520for%2520special%2520tokens.%2520We%2520evaluate%2520PlainMamba%2520on%2520a%250Avariety%2520of%2520visual%2520recognition%2520tasks%252C%2520achieving%2520performance%2520gains%2520over%2520previous%250Anon-hierarchical%2520models%2520and%2520is%2520competitive%2520with%2520hierarchical%2520alternatives.%2520For%250Atasks%2520requiring%2520high-resolution%2520inputs%252C%2520in%2520particular%252C%2520PlainMamba%2520requires%2520much%250Aless%2520computing%2520while%2520maintaining%2520high%2520performance.%2520Code%2520and%2520models%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/ChenhongyiYang/PlainMamba%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlainMamba%3A%20Improving%20Non-Hierarchical%20Mamba%20in%20Visual%20Recognition&entry.906535625=Chenhongyi%20Yang%20and%20Zehui%20Chen%20and%20Miguel%20Espinosa%20and%20Linus%20Ericsson%20and%20Zhenyu%20Wang%20and%20Jiaming%20Liu%20and%20Elliot%20J.%20Crowley&entry.1292438233=%20%20We%20present%20PlainMamba%3A%20a%20simple%20non-hierarchical%20state%20space%20model%20%28SSM%29%0Adesigned%20for%20general%20visual%20recognition.%20The%20recent%20Mamba%20model%20has%20shown%20how%0ASSMs%20can%20be%20highly%20competitive%20with%20other%20architectures%20on%20sequential%20data%20and%0Ainitial%20attempts%20have%20been%20made%20to%20apply%20it%20to%20images.%20In%20this%20paper%2C%20we%0Afurther%20adapt%20the%20selective%20scanning%20process%20of%20Mamba%20to%20the%20visual%20domain%2C%0Aenhancing%20its%20ability%20to%20learn%20features%20from%20two-dimensional%20images%20by%20%28i%29%20a%0Acontinuous%202D%20scanning%20process%20that%20improves%20spatial%20continuity%20by%20ensuring%0Aadjacency%20of%20tokens%20in%20the%20scanning%20sequence%2C%20and%20%28ii%29%20direction-aware%20updating%0Awhich%20enables%20the%20model%20to%20discern%20the%20spatial%20relations%20of%20tokens%20by%20encoding%0Adirectional%20information.%20Our%20architecture%20is%20designed%20to%20be%20easy%20to%20use%20and%0Aeasy%20to%20scale%2C%20formed%20by%20stacking%20identical%20PlainMamba%20blocks%2C%20resulting%20in%20a%0Amodel%20with%20constant%20width%20throughout%20all%20layers.%20The%20architecture%20is%20further%0Asimplified%20by%20removing%20the%20need%20for%20special%20tokens.%20We%20evaluate%20PlainMamba%20on%20a%0Avariety%20of%20visual%20recognition%20tasks%2C%20achieving%20performance%20gains%20over%20previous%0Anon-hierarchical%20models%20and%20is%20competitive%20with%20hierarchical%20alternatives.%20For%0Atasks%20requiring%20high-resolution%20inputs%2C%20in%20particular%2C%20PlainMamba%20requires%20much%0Aless%20computing%20while%20maintaining%20high%20performance.%20Code%20and%20models%20are%0Aavailable%20at%3A%20https%3A//github.com/ChenhongyiYang/PlainMamba%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17695v2&entry.124074799=Read"},
{"title": "MambaMIM: Pre-training Mamba with State Space Token-interpolation", "author": "Fenghe Tang and Bingkun Nian and Yingtai Li and Jie Yang and Liu Wei and S. Kevin Zhou", "abstract": "  Generative self-supervised learning demonstrates outstanding representation\nlearning capabilities in both Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs). However, there are currently no generative pre-training\nmethods related to selective state space models (Mamba) that can handle\nlong-range dependencies effectively. To address this challenge, we introduce a\ngenerative self-supervised learning method for Mamba (MambaMIM) based on\nSelective Structure State Space Sequence Token-interpolation (S6T), a\ngeneral-purpose pre-training method for arbitrary Mamba architectures. Our\nmethod, MambaMIM, incorporates a bottom-up 3D hybrid masking strategy in the\nencoder to maintain masking consistency across different architectures.\nAdditionally, S6T is employed to learn causal relationships between the masked\nsequence in the state space. MambaMIM can be used on any single or hybrid Mamba\narchitectures to enhance the Mamba long-range representation capability.\nExtensive downstream experiments reveal the feasibility and advancement of\nusing Mamba for pre-training medical image tasks. The code is available at:\nhttps://github.com/FengheTan9/MambaMIM\n", "link": "http://arxiv.org/abs/2408.08070v1", "date": "2024-08-15", "relevancy": 2.6135, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5444}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5143}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaMIM%3A%20Pre-training%20Mamba%20with%20State%20Space%20Token-interpolation&body=Title%3A%20MambaMIM%3A%20Pre-training%20Mamba%20with%20State%20Space%20Token-interpolation%0AAuthor%3A%20Fenghe%20Tang%20and%20Bingkun%20Nian%20and%20Yingtai%20Li%20and%20Jie%20Yang%20and%20Liu%20Wei%20and%20S.%20Kevin%20Zhou%0AAbstract%3A%20%20%20Generative%20self-supervised%20learning%20demonstrates%20outstanding%20representation%0Alearning%20capabilities%20in%20both%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%0ATransformers%20%28ViTs%29.%20However%2C%20there%20are%20currently%20no%20generative%20pre-training%0Amethods%20related%20to%20selective%20state%20space%20models%20%28Mamba%29%20that%20can%20handle%0Along-range%20dependencies%20effectively.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Agenerative%20self-supervised%20learning%20method%20for%20Mamba%20%28MambaMIM%29%20based%20on%0ASelective%20Structure%20State%20Space%20Sequence%20Token-interpolation%20%28S6T%29%2C%20a%0Ageneral-purpose%20pre-training%20method%20for%20arbitrary%20Mamba%20architectures.%20Our%0Amethod%2C%20MambaMIM%2C%20incorporates%20a%20bottom-up%203D%20hybrid%20masking%20strategy%20in%20the%0Aencoder%20to%20maintain%20masking%20consistency%20across%20different%20architectures.%0AAdditionally%2C%20S6T%20is%20employed%20to%20learn%20causal%20relationships%20between%20the%20masked%0Asequence%20in%20the%20state%20space.%20MambaMIM%20can%20be%20used%20on%20any%20single%20or%20hybrid%20Mamba%0Aarchitectures%20to%20enhance%20the%20Mamba%20long-range%20representation%20capability.%0AExtensive%20downstream%20experiments%20reveal%20the%20feasibility%20and%20advancement%20of%0Ausing%20Mamba%20for%20pre-training%20medical%20image%20tasks.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/FengheTan9/MambaMIM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaMIM%253A%2520Pre-training%2520Mamba%2520with%2520State%2520Space%2520Token-interpolation%26entry.906535625%3DFenghe%2520Tang%2520and%2520Bingkun%2520Nian%2520and%2520Yingtai%2520Li%2520and%2520Jie%2520Yang%2520and%2520Liu%2520Wei%2520and%2520S.%2520Kevin%2520Zhou%26entry.1292438233%3D%2520%2520Generative%2520self-supervised%2520learning%2520demonstrates%2520outstanding%2520representation%250Alearning%2520capabilities%2520in%2520both%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Vision%250ATransformers%2520%2528ViTs%2529.%2520However%252C%2520there%2520are%2520currently%2520no%2520generative%2520pre-training%250Amethods%2520related%2520to%2520selective%2520state%2520space%2520models%2520%2528Mamba%2529%2520that%2520can%2520handle%250Along-range%2520dependencies%2520effectively.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%250Agenerative%2520self-supervised%2520learning%2520method%2520for%2520Mamba%2520%2528MambaMIM%2529%2520based%2520on%250ASelective%2520Structure%2520State%2520Space%2520Sequence%2520Token-interpolation%2520%2528S6T%2529%252C%2520a%250Ageneral-purpose%2520pre-training%2520method%2520for%2520arbitrary%2520Mamba%2520architectures.%2520Our%250Amethod%252C%2520MambaMIM%252C%2520incorporates%2520a%2520bottom-up%25203D%2520hybrid%2520masking%2520strategy%2520in%2520the%250Aencoder%2520to%2520maintain%2520masking%2520consistency%2520across%2520different%2520architectures.%250AAdditionally%252C%2520S6T%2520is%2520employed%2520to%2520learn%2520causal%2520relationships%2520between%2520the%2520masked%250Asequence%2520in%2520the%2520state%2520space.%2520MambaMIM%2520can%2520be%2520used%2520on%2520any%2520single%2520or%2520hybrid%2520Mamba%250Aarchitectures%2520to%2520enhance%2520the%2520Mamba%2520long-range%2520representation%2520capability.%250AExtensive%2520downstream%2520experiments%2520reveal%2520the%2520feasibility%2520and%2520advancement%2520of%250Ausing%2520Mamba%2520for%2520pre-training%2520medical%2520image%2520tasks.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/FengheTan9/MambaMIM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaMIM%3A%20Pre-training%20Mamba%20with%20State%20Space%20Token-interpolation&entry.906535625=Fenghe%20Tang%20and%20Bingkun%20Nian%20and%20Yingtai%20Li%20and%20Jie%20Yang%20and%20Liu%20Wei%20and%20S.%20Kevin%20Zhou&entry.1292438233=%20%20Generative%20self-supervised%20learning%20demonstrates%20outstanding%20representation%0Alearning%20capabilities%20in%20both%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%0ATransformers%20%28ViTs%29.%20However%2C%20there%20are%20currently%20no%20generative%20pre-training%0Amethods%20related%20to%20selective%20state%20space%20models%20%28Mamba%29%20that%20can%20handle%0Along-range%20dependencies%20effectively.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Agenerative%20self-supervised%20learning%20method%20for%20Mamba%20%28MambaMIM%29%20based%20on%0ASelective%20Structure%20State%20Space%20Sequence%20Token-interpolation%20%28S6T%29%2C%20a%0Ageneral-purpose%20pre-training%20method%20for%20arbitrary%20Mamba%20architectures.%20Our%0Amethod%2C%20MambaMIM%2C%20incorporates%20a%20bottom-up%203D%20hybrid%20masking%20strategy%20in%20the%0Aencoder%20to%20maintain%20masking%20consistency%20across%20different%20architectures.%0AAdditionally%2C%20S6T%20is%20employed%20to%20learn%20causal%20relationships%20between%20the%20masked%0Asequence%20in%20the%20state%20space.%20MambaMIM%20can%20be%20used%20on%20any%20single%20or%20hybrid%20Mamba%0Aarchitectures%20to%20enhance%20the%20Mamba%20long-range%20representation%20capability.%0AExtensive%20downstream%20experiments%20reveal%20the%20feasibility%20and%20advancement%20of%0Ausing%20Mamba%20for%20pre-training%20medical%20image%20tasks.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/FengheTan9/MambaMIM%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08070v1&entry.124074799=Read"},
{"title": "BAM! Just Like That: Simple and Efficient Parameter Upcycling for\n  Mixture of Experts", "author": "Qizhen Zhang and Nikolas Gritsch and Dwaraknath Gnaneshwar and Simon Guo and David Cairuz and Bharat Venkitesh and Jakob Foerster and Phil Blunsom and Sebastian Ruder and Ahmet Ustun and Acyr Locatelli", "abstract": "  The Mixture of Experts (MoE) framework has become a popular architecture for\nlarge language models due to its superior performance over dense models.\nHowever, training MoEs from scratch in a large-scale regime is prohibitively\nexpensive. Existing methods mitigate this by pre-training multiple dense expert\nmodels independently and using them to initialize an MoE. This is done by using\nexperts' feed-forward network (FFN) to initialize the MoE's experts while\nmerging other parameters. However, this method limits the reuse of dense model\nparameters to only the FFN layers, thereby constraining the advantages when\n\"upcycling\" these models into MoEs. We propose BAM (Branch-Attend-Mix), a\nsimple yet effective method that addresses this shortcoming. BAM makes full use\nof specialized dense models by not only using their FFN to initialize the MoE\nlayers but also leveraging experts' attention parameters fully by initializing\nthem into a soft-variant of Mixture of Attention (MoA) layers. We explore two\nmethods for upcycling attention parameters: 1) initializing separate attention\nexperts from dense models including all attention parameters for the best model\nperformance; and 2) sharing key and value parameters across all experts to\nfacilitate for better inference efficiency. To further improve efficiency, we\nadopt a parallel attention transformer architecture to MoEs, which allows the\nattention experts and FFN experts to be computed concurrently. Our experiments\non seed models ranging from 590 million to 2 billion parameters demonstrate\nthat BAM surpasses baselines in both perplexity and downstream task\nperformance, within the same computational and data constraints.\n", "link": "http://arxiv.org/abs/2408.08274v1", "date": "2024-08-15", "relevancy": 2.564, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5201}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5166}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAM%21%20Just%20Like%20That%3A%20Simple%20and%20Efficient%20Parameter%20Upcycling%20for%0A%20%20Mixture%20of%20Experts&body=Title%3A%20BAM%21%20Just%20Like%20That%3A%20Simple%20and%20Efficient%20Parameter%20Upcycling%20for%0A%20%20Mixture%20of%20Experts%0AAuthor%3A%20Qizhen%20Zhang%20and%20Nikolas%20Gritsch%20and%20Dwaraknath%20Gnaneshwar%20and%20Simon%20Guo%20and%20David%20Cairuz%20and%20Bharat%20Venkitesh%20and%20Jakob%20Foerster%20and%20Phil%20Blunsom%20and%20Sebastian%20Ruder%20and%20Ahmet%20Ustun%20and%20Acyr%20Locatelli%0AAbstract%3A%20%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20framework%20has%20become%20a%20popular%20architecture%20for%0Alarge%20language%20models%20due%20to%20its%20superior%20performance%20over%20dense%20models.%0AHowever%2C%20training%20MoEs%20from%20scratch%20in%20a%20large-scale%20regime%20is%20prohibitively%0Aexpensive.%20Existing%20methods%20mitigate%20this%20by%20pre-training%20multiple%20dense%20expert%0Amodels%20independently%20and%20using%20them%20to%20initialize%20an%20MoE.%20This%20is%20done%20by%20using%0Aexperts%27%20feed-forward%20network%20%28FFN%29%20to%20initialize%20the%20MoE%27s%20experts%20while%0Amerging%20other%20parameters.%20However%2C%20this%20method%20limits%20the%20reuse%20of%20dense%20model%0Aparameters%20to%20only%20the%20FFN%20layers%2C%20thereby%20constraining%20the%20advantages%20when%0A%22upcycling%22%20these%20models%20into%20MoEs.%20We%20propose%20BAM%20%28Branch-Attend-Mix%29%2C%20a%0Asimple%20yet%20effective%20method%20that%20addresses%20this%20shortcoming.%20BAM%20makes%20full%20use%0Aof%20specialized%20dense%20models%20by%20not%20only%20using%20their%20FFN%20to%20initialize%20the%20MoE%0Alayers%20but%20also%20leveraging%20experts%27%20attention%20parameters%20fully%20by%20initializing%0Athem%20into%20a%20soft-variant%20of%20Mixture%20of%20Attention%20%28MoA%29%20layers.%20We%20explore%20two%0Amethods%20for%20upcycling%20attention%20parameters%3A%201%29%20initializing%20separate%20attention%0Aexperts%20from%20dense%20models%20including%20all%20attention%20parameters%20for%20the%20best%20model%0Aperformance%3B%20and%202%29%20sharing%20key%20and%20value%20parameters%20across%20all%20experts%20to%0Afacilitate%20for%20better%20inference%20efficiency.%20To%20further%20improve%20efficiency%2C%20we%0Aadopt%20a%20parallel%20attention%20transformer%20architecture%20to%20MoEs%2C%20which%20allows%20the%0Aattention%20experts%20and%20FFN%20experts%20to%20be%20computed%20concurrently.%20Our%20experiments%0Aon%20seed%20models%20ranging%20from%20590%20million%20to%202%20billion%20parameters%20demonstrate%0Athat%20BAM%20surpasses%20baselines%20in%20both%20perplexity%20and%20downstream%20task%0Aperformance%2C%20within%20the%20same%20computational%20and%20data%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAM%2521%2520Just%2520Like%2520That%253A%2520Simple%2520and%2520Efficient%2520Parameter%2520Upcycling%2520for%250A%2520%2520Mixture%2520of%2520Experts%26entry.906535625%3DQizhen%2520Zhang%2520and%2520Nikolas%2520Gritsch%2520and%2520Dwaraknath%2520Gnaneshwar%2520and%2520Simon%2520Guo%2520and%2520David%2520Cairuz%2520and%2520Bharat%2520Venkitesh%2520and%2520Jakob%2520Foerster%2520and%2520Phil%2520Blunsom%2520and%2520Sebastian%2520Ruder%2520and%2520Ahmet%2520Ustun%2520and%2520Acyr%2520Locatelli%26entry.1292438233%3D%2520%2520The%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520framework%2520has%2520become%2520a%2520popular%2520architecture%2520for%250Alarge%2520language%2520models%2520due%2520to%2520its%2520superior%2520performance%2520over%2520dense%2520models.%250AHowever%252C%2520training%2520MoEs%2520from%2520scratch%2520in%2520a%2520large-scale%2520regime%2520is%2520prohibitively%250Aexpensive.%2520Existing%2520methods%2520mitigate%2520this%2520by%2520pre-training%2520multiple%2520dense%2520expert%250Amodels%2520independently%2520and%2520using%2520them%2520to%2520initialize%2520an%2520MoE.%2520This%2520is%2520done%2520by%2520using%250Aexperts%2527%2520feed-forward%2520network%2520%2528FFN%2529%2520to%2520initialize%2520the%2520MoE%2527s%2520experts%2520while%250Amerging%2520other%2520parameters.%2520However%252C%2520this%2520method%2520limits%2520the%2520reuse%2520of%2520dense%2520model%250Aparameters%2520to%2520only%2520the%2520FFN%2520layers%252C%2520thereby%2520constraining%2520the%2520advantages%2520when%250A%2522upcycling%2522%2520these%2520models%2520into%2520MoEs.%2520We%2520propose%2520BAM%2520%2528Branch-Attend-Mix%2529%252C%2520a%250Asimple%2520yet%2520effective%2520method%2520that%2520addresses%2520this%2520shortcoming.%2520BAM%2520makes%2520full%2520use%250Aof%2520specialized%2520dense%2520models%2520by%2520not%2520only%2520using%2520their%2520FFN%2520to%2520initialize%2520the%2520MoE%250Alayers%2520but%2520also%2520leveraging%2520experts%2527%2520attention%2520parameters%2520fully%2520by%2520initializing%250Athem%2520into%2520a%2520soft-variant%2520of%2520Mixture%2520of%2520Attention%2520%2528MoA%2529%2520layers.%2520We%2520explore%2520two%250Amethods%2520for%2520upcycling%2520attention%2520parameters%253A%25201%2529%2520initializing%2520separate%2520attention%250Aexperts%2520from%2520dense%2520models%2520including%2520all%2520attention%2520parameters%2520for%2520the%2520best%2520model%250Aperformance%253B%2520and%25202%2529%2520sharing%2520key%2520and%2520value%2520parameters%2520across%2520all%2520experts%2520to%250Afacilitate%2520for%2520better%2520inference%2520efficiency.%2520To%2520further%2520improve%2520efficiency%252C%2520we%250Aadopt%2520a%2520parallel%2520attention%2520transformer%2520architecture%2520to%2520MoEs%252C%2520which%2520allows%2520the%250Aattention%2520experts%2520and%2520FFN%2520experts%2520to%2520be%2520computed%2520concurrently.%2520Our%2520experiments%250Aon%2520seed%2520models%2520ranging%2520from%2520590%2520million%2520to%25202%2520billion%2520parameters%2520demonstrate%250Athat%2520BAM%2520surpasses%2520baselines%2520in%2520both%2520perplexity%2520and%2520downstream%2520task%250Aperformance%252C%2520within%2520the%2520same%2520computational%2520and%2520data%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAM%21%20Just%20Like%20That%3A%20Simple%20and%20Efficient%20Parameter%20Upcycling%20for%0A%20%20Mixture%20of%20Experts&entry.906535625=Qizhen%20Zhang%20and%20Nikolas%20Gritsch%20and%20Dwaraknath%20Gnaneshwar%20and%20Simon%20Guo%20and%20David%20Cairuz%20and%20Bharat%20Venkitesh%20and%20Jakob%20Foerster%20and%20Phil%20Blunsom%20and%20Sebastian%20Ruder%20and%20Ahmet%20Ustun%20and%20Acyr%20Locatelli&entry.1292438233=%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20framework%20has%20become%20a%20popular%20architecture%20for%0Alarge%20language%20models%20due%20to%20its%20superior%20performance%20over%20dense%20models.%0AHowever%2C%20training%20MoEs%20from%20scratch%20in%20a%20large-scale%20regime%20is%20prohibitively%0Aexpensive.%20Existing%20methods%20mitigate%20this%20by%20pre-training%20multiple%20dense%20expert%0Amodels%20independently%20and%20using%20them%20to%20initialize%20an%20MoE.%20This%20is%20done%20by%20using%0Aexperts%27%20feed-forward%20network%20%28FFN%29%20to%20initialize%20the%20MoE%27s%20experts%20while%0Amerging%20other%20parameters.%20However%2C%20this%20method%20limits%20the%20reuse%20of%20dense%20model%0Aparameters%20to%20only%20the%20FFN%20layers%2C%20thereby%20constraining%20the%20advantages%20when%0A%22upcycling%22%20these%20models%20into%20MoEs.%20We%20propose%20BAM%20%28Branch-Attend-Mix%29%2C%20a%0Asimple%20yet%20effective%20method%20that%20addresses%20this%20shortcoming.%20BAM%20makes%20full%20use%0Aof%20specialized%20dense%20models%20by%20not%20only%20using%20their%20FFN%20to%20initialize%20the%20MoE%0Alayers%20but%20also%20leveraging%20experts%27%20attention%20parameters%20fully%20by%20initializing%0Athem%20into%20a%20soft-variant%20of%20Mixture%20of%20Attention%20%28MoA%29%20layers.%20We%20explore%20two%0Amethods%20for%20upcycling%20attention%20parameters%3A%201%29%20initializing%20separate%20attention%0Aexperts%20from%20dense%20models%20including%20all%20attention%20parameters%20for%20the%20best%20model%0Aperformance%3B%20and%202%29%20sharing%20key%20and%20value%20parameters%20across%20all%20experts%20to%0Afacilitate%20for%20better%20inference%20efficiency.%20To%20further%20improve%20efficiency%2C%20we%0Aadopt%20a%20parallel%20attention%20transformer%20architecture%20to%20MoEs%2C%20which%20allows%20the%0Aattention%20experts%20and%20FFN%20experts%20to%20be%20computed%20concurrently.%20Our%20experiments%0Aon%20seed%20models%20ranging%20from%20590%20million%20to%202%20billion%20parameters%20demonstrate%0Athat%20BAM%20surpasses%20baselines%20in%20both%20perplexity%20and%20downstream%20task%0Aperformance%2C%20within%20the%20same%20computational%20and%20data%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08274v1&entry.124074799=Read"},
{"title": "Conditional Fairness for Generative AIs", "author": "Chih-Hong Cheng and Harald Ruess and Changshun Wu and Xingyu Zhao", "abstract": "  The deployment of generative AI (GenAI) models raises significant fairness\nconcerns, addressed in this paper through novel characterization and\nenforcement techniques specific to GenAI. Unlike standard AI performing\nspecific tasks, GenAI's broad functionality requires \"conditional fairness\"\ntailored to the context being generated, such as demographic fairness in\ngenerating images of poor people versus successful business leaders. We define\ntwo fairness levels: the first evaluates fairness in generated outputs,\nindependent of prompts and models; the second assesses inherent fairness with\nneutral prompts. Given the complexity of GenAI and challenges in fairness\nspecifications, we focus on bounding the worst case, considering a GenAI system\nunfair if the distance between appearances of a specific group exceeds preset\nthresholds. We also explore combinatorial testing for accessing relative\ncompleteness in intersectional fairness. By bounding the worst case, we develop\na prompt injection scheme within an agent-based framework to enforce\nconditional fairness with minimal intervention, validated on state-of-the-art\nGenAI systems.\n", "link": "http://arxiv.org/abs/2404.16663v4", "date": "2024-08-15", "relevancy": 2.5555, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5774}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4795}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Fairness%20for%20Generative%20AIs&body=Title%3A%20Conditional%20Fairness%20for%20Generative%20AIs%0AAuthor%3A%20Chih-Hong%20Cheng%20and%20Harald%20Ruess%20and%20Changshun%20Wu%20and%20Xingyu%20Zhao%0AAbstract%3A%20%20%20The%20deployment%20of%20generative%20AI%20%28GenAI%29%20models%20raises%20significant%20fairness%0Aconcerns%2C%20addressed%20in%20this%20paper%20through%20novel%20characterization%20and%0Aenforcement%20techniques%20specific%20to%20GenAI.%20Unlike%20standard%20AI%20performing%0Aspecific%20tasks%2C%20GenAI%27s%20broad%20functionality%20requires%20%22conditional%20fairness%22%0Atailored%20to%20the%20context%20being%20generated%2C%20such%20as%20demographic%20fairness%20in%0Agenerating%20images%20of%20poor%20people%20versus%20successful%20business%20leaders.%20We%20define%0Atwo%20fairness%20levels%3A%20the%20first%20evaluates%20fairness%20in%20generated%20outputs%2C%0Aindependent%20of%20prompts%20and%20models%3B%20the%20second%20assesses%20inherent%20fairness%20with%0Aneutral%20prompts.%20Given%20the%20complexity%20of%20GenAI%20and%20challenges%20in%20fairness%0Aspecifications%2C%20we%20focus%20on%20bounding%20the%20worst%20case%2C%20considering%20a%20GenAI%20system%0Aunfair%20if%20the%20distance%20between%20appearances%20of%20a%20specific%20group%20exceeds%20preset%0Athresholds.%20We%20also%20explore%20combinatorial%20testing%20for%20accessing%20relative%0Acompleteness%20in%20intersectional%20fairness.%20By%20bounding%20the%20worst%20case%2C%20we%20develop%0Aa%20prompt%20injection%20scheme%20within%20an%20agent-based%20framework%20to%20enforce%0Aconditional%20fairness%20with%20minimal%20intervention%2C%20validated%20on%20state-of-the-art%0AGenAI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16663v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Fairness%2520for%2520Generative%2520AIs%26entry.906535625%3DChih-Hong%2520Cheng%2520and%2520Harald%2520Ruess%2520and%2520Changshun%2520Wu%2520and%2520Xingyu%2520Zhao%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520generative%2520AI%2520%2528GenAI%2529%2520models%2520raises%2520significant%2520fairness%250Aconcerns%252C%2520addressed%2520in%2520this%2520paper%2520through%2520novel%2520characterization%2520and%250Aenforcement%2520techniques%2520specific%2520to%2520GenAI.%2520Unlike%2520standard%2520AI%2520performing%250Aspecific%2520tasks%252C%2520GenAI%2527s%2520broad%2520functionality%2520requires%2520%2522conditional%2520fairness%2522%250Atailored%2520to%2520the%2520context%2520being%2520generated%252C%2520such%2520as%2520demographic%2520fairness%2520in%250Agenerating%2520images%2520of%2520poor%2520people%2520versus%2520successful%2520business%2520leaders.%2520We%2520define%250Atwo%2520fairness%2520levels%253A%2520the%2520first%2520evaluates%2520fairness%2520in%2520generated%2520outputs%252C%250Aindependent%2520of%2520prompts%2520and%2520models%253B%2520the%2520second%2520assesses%2520inherent%2520fairness%2520with%250Aneutral%2520prompts.%2520Given%2520the%2520complexity%2520of%2520GenAI%2520and%2520challenges%2520in%2520fairness%250Aspecifications%252C%2520we%2520focus%2520on%2520bounding%2520the%2520worst%2520case%252C%2520considering%2520a%2520GenAI%2520system%250Aunfair%2520if%2520the%2520distance%2520between%2520appearances%2520of%2520a%2520specific%2520group%2520exceeds%2520preset%250Athresholds.%2520We%2520also%2520explore%2520combinatorial%2520testing%2520for%2520accessing%2520relative%250Acompleteness%2520in%2520intersectional%2520fairness.%2520By%2520bounding%2520the%2520worst%2520case%252C%2520we%2520develop%250Aa%2520prompt%2520injection%2520scheme%2520within%2520an%2520agent-based%2520framework%2520to%2520enforce%250Aconditional%2520fairness%2520with%2520minimal%2520intervention%252C%2520validated%2520on%2520state-of-the-art%250AGenAI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16663v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Fairness%20for%20Generative%20AIs&entry.906535625=Chih-Hong%20Cheng%20and%20Harald%20Ruess%20and%20Changshun%20Wu%20and%20Xingyu%20Zhao&entry.1292438233=%20%20The%20deployment%20of%20generative%20AI%20%28GenAI%29%20models%20raises%20significant%20fairness%0Aconcerns%2C%20addressed%20in%20this%20paper%20through%20novel%20characterization%20and%0Aenforcement%20techniques%20specific%20to%20GenAI.%20Unlike%20standard%20AI%20performing%0Aspecific%20tasks%2C%20GenAI%27s%20broad%20functionality%20requires%20%22conditional%20fairness%22%0Atailored%20to%20the%20context%20being%20generated%2C%20such%20as%20demographic%20fairness%20in%0Agenerating%20images%20of%20poor%20people%20versus%20successful%20business%20leaders.%20We%20define%0Atwo%20fairness%20levels%3A%20the%20first%20evaluates%20fairness%20in%20generated%20outputs%2C%0Aindependent%20of%20prompts%20and%20models%3B%20the%20second%20assesses%20inherent%20fairness%20with%0Aneutral%20prompts.%20Given%20the%20complexity%20of%20GenAI%20and%20challenges%20in%20fairness%0Aspecifications%2C%20we%20focus%20on%20bounding%20the%20worst%20case%2C%20considering%20a%20GenAI%20system%0Aunfair%20if%20the%20distance%20between%20appearances%20of%20a%20specific%20group%20exceeds%20preset%0Athresholds.%20We%20also%20explore%20combinatorial%20testing%20for%20accessing%20relative%0Acompleteness%20in%20intersectional%20fairness.%20By%20bounding%20the%20worst%20case%2C%20we%20develop%0Aa%20prompt%20injection%20scheme%20within%20an%20agent-based%20framework%20to%20enforce%0Aconditional%20fairness%20with%20minimal%20intervention%2C%20validated%20on%20state-of-the-art%0AGenAI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16663v4&entry.124074799=Read"},
{"title": "Self-Supervised Video Desmoking for Laparoscopic Surgery", "author": "Renlong Wu and Zhilu Zhang and Shuohao Zhang and Longfei Gou and Haobin Chen and Lei Zhang and Hao Chen and Wangmeng Zuo", "abstract": "  Due to the difficulty of collecting real paired data, most existing desmoking\nmethods train the models by synthesizing smoke, generalizing poorly to real\nsurgical scenarios. Although a few works have explored single-image real-world\ndesmoking in unpaired learning manners, they still encounter challenges in\nhandling dense smoke. In this work, we address these issues together by\nintroducing the self-supervised surgery video desmoking (SelfSVD). On the one\nhand, we observe that the frame captured before the activation of high-energy\ndevices is generally clear (named pre-smoke frame, PS frame), thus it can serve\nas supervision for other smoky frames, making real-world self-supervised video\ndesmoking practically feasible. On the other hand, in order to enhance the\ndesmoking performance, we further feed the valuable information from PS frame\ninto models, where a masking strategy and a regularization term are presented\nto avoid trivial solutions. In addition, we construct a real surgery video\ndataset for desmoking, which covers a variety of smoky scenes. Extensive\nexperiments on the dataset show that our SelfSVD can remove smoke more\neffectively and efficiently while recovering more photo-realistic details than\nthe state-of-the-art methods. The dataset, codes, and pre-trained models are\navailable at \\url{https://github.com/ZcsrenlongZ/SelfSVD}.\n", "link": "http://arxiv.org/abs/2403.11192v2", "date": "2024-08-15", "relevancy": 2.5479, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5168}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.507}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Video%20Desmoking%20for%20Laparoscopic%20Surgery&body=Title%3A%20Self-Supervised%20Video%20Desmoking%20for%20Laparoscopic%20Surgery%0AAuthor%3A%20Renlong%20Wu%20and%20Zhilu%20Zhang%20and%20Shuohao%20Zhang%20and%20Longfei%20Gou%20and%20Haobin%20Chen%20and%20Lei%20Zhang%20and%20Hao%20Chen%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Due%20to%20the%20difficulty%20of%20collecting%20real%20paired%20data%2C%20most%20existing%20desmoking%0Amethods%20train%20the%20models%20by%20synthesizing%20smoke%2C%20generalizing%20poorly%20to%20real%0Asurgical%20scenarios.%20Although%20a%20few%20works%20have%20explored%20single-image%20real-world%0Adesmoking%20in%20unpaired%20learning%20manners%2C%20they%20still%20encounter%20challenges%20in%0Ahandling%20dense%20smoke.%20In%20this%20work%2C%20we%20address%20these%20issues%20together%20by%0Aintroducing%20the%20self-supervised%20surgery%20video%20desmoking%20%28SelfSVD%29.%20On%20the%20one%0Ahand%2C%20we%20observe%20that%20the%20frame%20captured%20before%20the%20activation%20of%20high-energy%0Adevices%20is%20generally%20clear%20%28named%20pre-smoke%20frame%2C%20PS%20frame%29%2C%20thus%20it%20can%20serve%0Aas%20supervision%20for%20other%20smoky%20frames%2C%20making%20real-world%20self-supervised%20video%0Adesmoking%20practically%20feasible.%20On%20the%20other%20hand%2C%20in%20order%20to%20enhance%20the%0Adesmoking%20performance%2C%20we%20further%20feed%20the%20valuable%20information%20from%20PS%20frame%0Ainto%20models%2C%20where%20a%20masking%20strategy%20and%20a%20regularization%20term%20are%20presented%0Ato%20avoid%20trivial%20solutions.%20In%20addition%2C%20we%20construct%20a%20real%20surgery%20video%0Adataset%20for%20desmoking%2C%20which%20covers%20a%20variety%20of%20smoky%20scenes.%20Extensive%0Aexperiments%20on%20the%20dataset%20show%20that%20our%20SelfSVD%20can%20remove%20smoke%20more%0Aeffectively%20and%20efficiently%20while%20recovering%20more%20photo-realistic%20details%20than%0Athe%20state-of-the-art%20methods.%20The%20dataset%2C%20codes%2C%20and%20pre-trained%20models%20are%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/ZcsrenlongZ/SelfSVD%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11192v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Video%2520Desmoking%2520for%2520Laparoscopic%2520Surgery%26entry.906535625%3DRenlong%2520Wu%2520and%2520Zhilu%2520Zhang%2520and%2520Shuohao%2520Zhang%2520and%2520Longfei%2520Gou%2520and%2520Haobin%2520Chen%2520and%2520Lei%2520Zhang%2520and%2520Hao%2520Chen%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520difficulty%2520of%2520collecting%2520real%2520paired%2520data%252C%2520most%2520existing%2520desmoking%250Amethods%2520train%2520the%2520models%2520by%2520synthesizing%2520smoke%252C%2520generalizing%2520poorly%2520to%2520real%250Asurgical%2520scenarios.%2520Although%2520a%2520few%2520works%2520have%2520explored%2520single-image%2520real-world%250Adesmoking%2520in%2520unpaired%2520learning%2520manners%252C%2520they%2520still%2520encounter%2520challenges%2520in%250Ahandling%2520dense%2520smoke.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520issues%2520together%2520by%250Aintroducing%2520the%2520self-supervised%2520surgery%2520video%2520desmoking%2520%2528SelfSVD%2529.%2520On%2520the%2520one%250Ahand%252C%2520we%2520observe%2520that%2520the%2520frame%2520captured%2520before%2520the%2520activation%2520of%2520high-energy%250Adevices%2520is%2520generally%2520clear%2520%2528named%2520pre-smoke%2520frame%252C%2520PS%2520frame%2529%252C%2520thus%2520it%2520can%2520serve%250Aas%2520supervision%2520for%2520other%2520smoky%2520frames%252C%2520making%2520real-world%2520self-supervised%2520video%250Adesmoking%2520practically%2520feasible.%2520On%2520the%2520other%2520hand%252C%2520in%2520order%2520to%2520enhance%2520the%250Adesmoking%2520performance%252C%2520we%2520further%2520feed%2520the%2520valuable%2520information%2520from%2520PS%2520frame%250Ainto%2520models%252C%2520where%2520a%2520masking%2520strategy%2520and%2520a%2520regularization%2520term%2520are%2520presented%250Ato%2520avoid%2520trivial%2520solutions.%2520In%2520addition%252C%2520we%2520construct%2520a%2520real%2520surgery%2520video%250Adataset%2520for%2520desmoking%252C%2520which%2520covers%2520a%2520variety%2520of%2520smoky%2520scenes.%2520Extensive%250Aexperiments%2520on%2520the%2520dataset%2520show%2520that%2520our%2520SelfSVD%2520can%2520remove%2520smoke%2520more%250Aeffectively%2520and%2520efficiently%2520while%2520recovering%2520more%2520photo-realistic%2520details%2520than%250Athe%2520state-of-the-art%2520methods.%2520The%2520dataset%252C%2520codes%252C%2520and%2520pre-trained%2520models%2520are%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/ZcsrenlongZ/SelfSVD%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11192v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Video%20Desmoking%20for%20Laparoscopic%20Surgery&entry.906535625=Renlong%20Wu%20and%20Zhilu%20Zhang%20and%20Shuohao%20Zhang%20and%20Longfei%20Gou%20and%20Haobin%20Chen%20and%20Lei%20Zhang%20and%20Hao%20Chen%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Due%20to%20the%20difficulty%20of%20collecting%20real%20paired%20data%2C%20most%20existing%20desmoking%0Amethods%20train%20the%20models%20by%20synthesizing%20smoke%2C%20generalizing%20poorly%20to%20real%0Asurgical%20scenarios.%20Although%20a%20few%20works%20have%20explored%20single-image%20real-world%0Adesmoking%20in%20unpaired%20learning%20manners%2C%20they%20still%20encounter%20challenges%20in%0Ahandling%20dense%20smoke.%20In%20this%20work%2C%20we%20address%20these%20issues%20together%20by%0Aintroducing%20the%20self-supervised%20surgery%20video%20desmoking%20%28SelfSVD%29.%20On%20the%20one%0Ahand%2C%20we%20observe%20that%20the%20frame%20captured%20before%20the%20activation%20of%20high-energy%0Adevices%20is%20generally%20clear%20%28named%20pre-smoke%20frame%2C%20PS%20frame%29%2C%20thus%20it%20can%20serve%0Aas%20supervision%20for%20other%20smoky%20frames%2C%20making%20real-world%20self-supervised%20video%0Adesmoking%20practically%20feasible.%20On%20the%20other%20hand%2C%20in%20order%20to%20enhance%20the%0Adesmoking%20performance%2C%20we%20further%20feed%20the%20valuable%20information%20from%20PS%20frame%0Ainto%20models%2C%20where%20a%20masking%20strategy%20and%20a%20regularization%20term%20are%20presented%0Ato%20avoid%20trivial%20solutions.%20In%20addition%2C%20we%20construct%20a%20real%20surgery%20video%0Adataset%20for%20desmoking%2C%20which%20covers%20a%20variety%20of%20smoky%20scenes.%20Extensive%0Aexperiments%20on%20the%20dataset%20show%20that%20our%20SelfSVD%20can%20remove%20smoke%20more%0Aeffectively%20and%20efficiently%20while%20recovering%20more%20photo-realistic%20details%20than%0Athe%20state-of-the-art%20methods.%20The%20dataset%2C%20codes%2C%20and%20pre-trained%20models%20are%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/ZcsrenlongZ/SelfSVD%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11192v2&entry.124074799=Read"},
{"title": "The Dawn of KAN in Image-to-Image (I2I) Translation: Integrating\n  Kolmogorov-Arnold Networks with GANs for Unpaired I2I Translation", "author": "Arpan Mahara and Naphtali D. Rishe and Liangdong Deng", "abstract": "  Image-to-Image translation in Generative Artificial Intelligence (Generative\nAI) has been a central focus of research, with applications spanning\nhealthcare, remote sensing, physics, chemistry, photography, and more. Among\nthe numerous methodologies, Generative Adversarial Networks (GANs) with\ncontrastive learning have been particularly successful. This study aims to\ndemonstrate that the Kolmogorov-Arnold Network (KAN) can effectively replace\nthe Multi-layer Perceptron (MLP) method in generative AI, particularly in the\nsubdomain of image-to-image translation, to achieve better generative quality.\nOur novel approach replaces the two-layer MLP with a two-layer KAN in the\nexisting Contrastive Unpaired Image-to-Image Translation (CUT) model,\ndeveloping the KAN-CUT model. This substitution favors the generation of more\ninformative features in low-dimensional vector representations, which\ncontrastive learning can utilize more effectively to produce high-quality\nimages in the target domain. Extensive experiments, detailed in the results\nsection, demonstrate the applicability of KAN in conjunction with contrastive\nlearning and GANs in Generative AI, particularly for image-to-image\ntranslation. This work suggests that KAN could be a valuable component in the\nbroader generative AI domain.\n", "link": "http://arxiv.org/abs/2408.08216v1", "date": "2024-08-15", "relevancy": 2.5276, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5194}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5037}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Dawn%20of%20KAN%20in%20Image-to-Image%20%28I2I%29%20Translation%3A%20Integrating%0A%20%20Kolmogorov-Arnold%20Networks%20with%20GANs%20for%20Unpaired%20I2I%20Translation&body=Title%3A%20The%20Dawn%20of%20KAN%20in%20Image-to-Image%20%28I2I%29%20Translation%3A%20Integrating%0A%20%20Kolmogorov-Arnold%20Networks%20with%20GANs%20for%20Unpaired%20I2I%20Translation%0AAuthor%3A%20Arpan%20Mahara%20and%20Naphtali%20D.%20Rishe%20and%20Liangdong%20Deng%0AAbstract%3A%20%20%20Image-to-Image%20translation%20in%20Generative%20Artificial%20Intelligence%20%28Generative%0AAI%29%20has%20been%20a%20central%20focus%20of%20research%2C%20with%20applications%20spanning%0Ahealthcare%2C%20remote%20sensing%2C%20physics%2C%20chemistry%2C%20photography%2C%20and%20more.%20Among%0Athe%20numerous%20methodologies%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%20with%0Acontrastive%20learning%20have%20been%20particularly%20successful.%20This%20study%20aims%20to%0Ademonstrate%20that%20the%20Kolmogorov-Arnold%20Network%20%28KAN%29%20can%20effectively%20replace%0Athe%20Multi-layer%20Perceptron%20%28MLP%29%20method%20in%20generative%20AI%2C%20particularly%20in%20the%0Asubdomain%20of%20image-to-image%20translation%2C%20to%20achieve%20better%20generative%20quality.%0AOur%20novel%20approach%20replaces%20the%20two-layer%20MLP%20with%20a%20two-layer%20KAN%20in%20the%0Aexisting%20Contrastive%20Unpaired%20Image-to-Image%20Translation%20%28CUT%29%20model%2C%0Adeveloping%20the%20KAN-CUT%20model.%20This%20substitution%20favors%20the%20generation%20of%20more%0Ainformative%20features%20in%20low-dimensional%20vector%20representations%2C%20which%0Acontrastive%20learning%20can%20utilize%20more%20effectively%20to%20produce%20high-quality%0Aimages%20in%20the%20target%20domain.%20Extensive%20experiments%2C%20detailed%20in%20the%20results%0Asection%2C%20demonstrate%20the%20applicability%20of%20KAN%20in%20conjunction%20with%20contrastive%0Alearning%20and%20GANs%20in%20Generative%20AI%2C%20particularly%20for%20image-to-image%0Atranslation.%20This%20work%20suggests%20that%20KAN%20could%20be%20a%20valuable%20component%20in%20the%0Abroader%20generative%20AI%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Dawn%2520of%2520KAN%2520in%2520Image-to-Image%2520%2528I2I%2529%2520Translation%253A%2520Integrating%250A%2520%2520Kolmogorov-Arnold%2520Networks%2520with%2520GANs%2520for%2520Unpaired%2520I2I%2520Translation%26entry.906535625%3DArpan%2520Mahara%2520and%2520Naphtali%2520D.%2520Rishe%2520and%2520Liangdong%2520Deng%26entry.1292438233%3D%2520%2520Image-to-Image%2520translation%2520in%2520Generative%2520Artificial%2520Intelligence%2520%2528Generative%250AAI%2529%2520has%2520been%2520a%2520central%2520focus%2520of%2520research%252C%2520with%2520applications%2520spanning%250Ahealthcare%252C%2520remote%2520sensing%252C%2520physics%252C%2520chemistry%252C%2520photography%252C%2520and%2520more.%2520Among%250Athe%2520numerous%2520methodologies%252C%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520with%250Acontrastive%2520learning%2520have%2520been%2520particularly%2520successful.%2520This%2520study%2520aims%2520to%250Ademonstrate%2520that%2520the%2520Kolmogorov-Arnold%2520Network%2520%2528KAN%2529%2520can%2520effectively%2520replace%250Athe%2520Multi-layer%2520Perceptron%2520%2528MLP%2529%2520method%2520in%2520generative%2520AI%252C%2520particularly%2520in%2520the%250Asubdomain%2520of%2520image-to-image%2520translation%252C%2520to%2520achieve%2520better%2520generative%2520quality.%250AOur%2520novel%2520approach%2520replaces%2520the%2520two-layer%2520MLP%2520with%2520a%2520two-layer%2520KAN%2520in%2520the%250Aexisting%2520Contrastive%2520Unpaired%2520Image-to-Image%2520Translation%2520%2528CUT%2529%2520model%252C%250Adeveloping%2520the%2520KAN-CUT%2520model.%2520This%2520substitution%2520favors%2520the%2520generation%2520of%2520more%250Ainformative%2520features%2520in%2520low-dimensional%2520vector%2520representations%252C%2520which%250Acontrastive%2520learning%2520can%2520utilize%2520more%2520effectively%2520to%2520produce%2520high-quality%250Aimages%2520in%2520the%2520target%2520domain.%2520Extensive%2520experiments%252C%2520detailed%2520in%2520the%2520results%250Asection%252C%2520demonstrate%2520the%2520applicability%2520of%2520KAN%2520in%2520conjunction%2520with%2520contrastive%250Alearning%2520and%2520GANs%2520in%2520Generative%2520AI%252C%2520particularly%2520for%2520image-to-image%250Atranslation.%2520This%2520work%2520suggests%2520that%2520KAN%2520could%2520be%2520a%2520valuable%2520component%2520in%2520the%250Abroader%2520generative%2520AI%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Dawn%20of%20KAN%20in%20Image-to-Image%20%28I2I%29%20Translation%3A%20Integrating%0A%20%20Kolmogorov-Arnold%20Networks%20with%20GANs%20for%20Unpaired%20I2I%20Translation&entry.906535625=Arpan%20Mahara%20and%20Naphtali%20D.%20Rishe%20and%20Liangdong%20Deng&entry.1292438233=%20%20Image-to-Image%20translation%20in%20Generative%20Artificial%20Intelligence%20%28Generative%0AAI%29%20has%20been%20a%20central%20focus%20of%20research%2C%20with%20applications%20spanning%0Ahealthcare%2C%20remote%20sensing%2C%20physics%2C%20chemistry%2C%20photography%2C%20and%20more.%20Among%0Athe%20numerous%20methodologies%2C%20Generative%20Adversarial%20Networks%20%28GANs%29%20with%0Acontrastive%20learning%20have%20been%20particularly%20successful.%20This%20study%20aims%20to%0Ademonstrate%20that%20the%20Kolmogorov-Arnold%20Network%20%28KAN%29%20can%20effectively%20replace%0Athe%20Multi-layer%20Perceptron%20%28MLP%29%20method%20in%20generative%20AI%2C%20particularly%20in%20the%0Asubdomain%20of%20image-to-image%20translation%2C%20to%20achieve%20better%20generative%20quality.%0AOur%20novel%20approach%20replaces%20the%20two-layer%20MLP%20with%20a%20two-layer%20KAN%20in%20the%0Aexisting%20Contrastive%20Unpaired%20Image-to-Image%20Translation%20%28CUT%29%20model%2C%0Adeveloping%20the%20KAN-CUT%20model.%20This%20substitution%20favors%20the%20generation%20of%20more%0Ainformative%20features%20in%20low-dimensional%20vector%20representations%2C%20which%0Acontrastive%20learning%20can%20utilize%20more%20effectively%20to%20produce%20high-quality%0Aimages%20in%20the%20target%20domain.%20Extensive%20experiments%2C%20detailed%20in%20the%20results%0Asection%2C%20demonstrate%20the%20applicability%20of%20KAN%20in%20conjunction%20with%20contrastive%0Alearning%20and%20GANs%20in%20Generative%20AI%2C%20particularly%20for%20image-to-image%0Atranslation.%20This%20work%20suggests%20that%20KAN%20could%20be%20a%20valuable%20component%20in%20the%0Abroader%20generative%20AI%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08216v1&entry.124074799=Read"},
{"title": "Fully Test-Time rPPG Estimation via Synthetic Signal-Guided Feature\n  Learning", "author": "Pei-Kai Huang and Tzu-Hsien Chen and Ya-Ting Chan and Kuan-Wen Chen and Chiou-Ting Hsu", "abstract": "  Many remote photoplethysmography (rPPG) estimation models have achieved\npromising performance in the training domain but often fail to accurately\nestimate physiological signals or heart rates (HR) in the target domains.\nDomain generalization (DG) or domain adaptation (DA) techniques are therefore\nadopted during the offline training stage to adapt the model to either\nunobserved or observed target domains by utilizing all available source domain\ndata. However, in rPPG estimation problems, the adapted model usually\nencounters challenges in estimating target data with significant domain\nvariation. In contrast, Test-Time Adaptation (TTA) enables the model to\nadaptively estimate rPPG signals in various unseen domains by online adapting\nto unlabeled target data without referring to any source data. In this paper,\nwe first establish a new TTA-rPPG benchmark that encompasses various domain\ninformation and HR distributions to simulate the challenges encountered in\nreal-world rPPG estimation. Next, we propose a novel synthetic signal-guided\nrPPG estimation framework to address the forgetting issue during the TTA stage\nand to enhance the adaptation capability of the pre-trained rPPG model. To this\nend, we develop a synthetic signal-guided feature learning method by\nsynthesizing pseudo rPPG signals as pseudo ground truths to guide a conditional\ngenerator in generating latent rPPG features. In addition, we design an\neffective spectral-based entropy minimization technique to encourage the rPPG\nmodel to learn new target domain information. Both the generated rPPG features\nand synthesized rPPG signals prevent the rPPG model from overfitting to target\ndata and forgetting previously acquired knowledge, while also broadly covering\nvarious heart rate (HR) distributions. Our extensive experiments on the\nTTA-rPPG benchmark show that the proposed method achieves superior performance.\n", "link": "http://arxiv.org/abs/2407.13322v3", "date": "2024-08-15", "relevancy": 2.4228, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4936}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4813}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Test-Time%20rPPG%20Estimation%20via%20Synthetic%20Signal-Guided%20Feature%0A%20%20Learning&body=Title%3A%20Fully%20Test-Time%20rPPG%20Estimation%20via%20Synthetic%20Signal-Guided%20Feature%0A%20%20Learning%0AAuthor%3A%20Pei-Kai%20Huang%20and%20Tzu-Hsien%20Chen%20and%20Ya-Ting%20Chan%20and%20Kuan-Wen%20Chen%20and%20Chiou-Ting%20Hsu%0AAbstract%3A%20%20%20Many%20remote%20photoplethysmography%20%28rPPG%29%20estimation%20models%20have%20achieved%0Apromising%20performance%20in%20the%20training%20domain%20but%20often%20fail%20to%20accurately%0Aestimate%20physiological%20signals%20or%20heart%20rates%20%28HR%29%20in%20the%20target%20domains.%0ADomain%20generalization%20%28DG%29%20or%20domain%20adaptation%20%28DA%29%20techniques%20are%20therefore%0Aadopted%20during%20the%20offline%20training%20stage%20to%20adapt%20the%20model%20to%20either%0Aunobserved%20or%20observed%20target%20domains%20by%20utilizing%20all%20available%20source%20domain%0Adata.%20However%2C%20in%20rPPG%20estimation%20problems%2C%20the%20adapted%20model%20usually%0Aencounters%20challenges%20in%20estimating%20target%20data%20with%20significant%20domain%0Avariation.%20In%20contrast%2C%20Test-Time%20Adaptation%20%28TTA%29%20enables%20the%20model%20to%0Aadaptively%20estimate%20rPPG%20signals%20in%20various%20unseen%20domains%20by%20online%20adapting%0Ato%20unlabeled%20target%20data%20without%20referring%20to%20any%20source%20data.%20In%20this%20paper%2C%0Awe%20first%20establish%20a%20new%20TTA-rPPG%20benchmark%20that%20encompasses%20various%20domain%0Ainformation%20and%20HR%20distributions%20to%20simulate%20the%20challenges%20encountered%20in%0Areal-world%20rPPG%20estimation.%20Next%2C%20we%20propose%20a%20novel%20synthetic%20signal-guided%0ArPPG%20estimation%20framework%20to%20address%20the%20forgetting%20issue%20during%20the%20TTA%20stage%0Aand%20to%20enhance%20the%20adaptation%20capability%20of%20the%20pre-trained%20rPPG%20model.%20To%20this%0Aend%2C%20we%20develop%20a%20synthetic%20signal-guided%20feature%20learning%20method%20by%0Asynthesizing%20pseudo%20rPPG%20signals%20as%20pseudo%20ground%20truths%20to%20guide%20a%20conditional%0Agenerator%20in%20generating%20latent%20rPPG%20features.%20In%20addition%2C%20we%20design%20an%0Aeffective%20spectral-based%20entropy%20minimization%20technique%20to%20encourage%20the%20rPPG%0Amodel%20to%20learn%20new%20target%20domain%20information.%20Both%20the%20generated%20rPPG%20features%0Aand%20synthesized%20rPPG%20signals%20prevent%20the%20rPPG%20model%20from%20overfitting%20to%20target%0Adata%20and%20forgetting%20previously%20acquired%20knowledge%2C%20while%20also%20broadly%20covering%0Avarious%20heart%20rate%20%28HR%29%20distributions.%20Our%20extensive%20experiments%20on%20the%0ATTA-rPPG%20benchmark%20show%20that%20the%20proposed%20method%20achieves%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13322v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Test-Time%2520rPPG%2520Estimation%2520via%2520Synthetic%2520Signal-Guided%2520Feature%250A%2520%2520Learning%26entry.906535625%3DPei-Kai%2520Huang%2520and%2520Tzu-Hsien%2520Chen%2520and%2520Ya-Ting%2520Chan%2520and%2520Kuan-Wen%2520Chen%2520and%2520Chiou-Ting%2520Hsu%26entry.1292438233%3D%2520%2520Many%2520remote%2520photoplethysmography%2520%2528rPPG%2529%2520estimation%2520models%2520have%2520achieved%250Apromising%2520performance%2520in%2520the%2520training%2520domain%2520but%2520often%2520fail%2520to%2520accurately%250Aestimate%2520physiological%2520signals%2520or%2520heart%2520rates%2520%2528HR%2529%2520in%2520the%2520target%2520domains.%250ADomain%2520generalization%2520%2528DG%2529%2520or%2520domain%2520adaptation%2520%2528DA%2529%2520techniques%2520are%2520therefore%250Aadopted%2520during%2520the%2520offline%2520training%2520stage%2520to%2520adapt%2520the%2520model%2520to%2520either%250Aunobserved%2520or%2520observed%2520target%2520domains%2520by%2520utilizing%2520all%2520available%2520source%2520domain%250Adata.%2520However%252C%2520in%2520rPPG%2520estimation%2520problems%252C%2520the%2520adapted%2520model%2520usually%250Aencounters%2520challenges%2520in%2520estimating%2520target%2520data%2520with%2520significant%2520domain%250Avariation.%2520In%2520contrast%252C%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520enables%2520the%2520model%2520to%250Aadaptively%2520estimate%2520rPPG%2520signals%2520in%2520various%2520unseen%2520domains%2520by%2520online%2520adapting%250Ato%2520unlabeled%2520target%2520data%2520without%2520referring%2520to%2520any%2520source%2520data.%2520In%2520this%2520paper%252C%250Awe%2520first%2520establish%2520a%2520new%2520TTA-rPPG%2520benchmark%2520that%2520encompasses%2520various%2520domain%250Ainformation%2520and%2520HR%2520distributions%2520to%2520simulate%2520the%2520challenges%2520encountered%2520in%250Areal-world%2520rPPG%2520estimation.%2520Next%252C%2520we%2520propose%2520a%2520novel%2520synthetic%2520signal-guided%250ArPPG%2520estimation%2520framework%2520to%2520address%2520the%2520forgetting%2520issue%2520during%2520the%2520TTA%2520stage%250Aand%2520to%2520enhance%2520the%2520adaptation%2520capability%2520of%2520the%2520pre-trained%2520rPPG%2520model.%2520To%2520this%250Aend%252C%2520we%2520develop%2520a%2520synthetic%2520signal-guided%2520feature%2520learning%2520method%2520by%250Asynthesizing%2520pseudo%2520rPPG%2520signals%2520as%2520pseudo%2520ground%2520truths%2520to%2520guide%2520a%2520conditional%250Agenerator%2520in%2520generating%2520latent%2520rPPG%2520features.%2520In%2520addition%252C%2520we%2520design%2520an%250Aeffective%2520spectral-based%2520entropy%2520minimization%2520technique%2520to%2520encourage%2520the%2520rPPG%250Amodel%2520to%2520learn%2520new%2520target%2520domain%2520information.%2520Both%2520the%2520generated%2520rPPG%2520features%250Aand%2520synthesized%2520rPPG%2520signals%2520prevent%2520the%2520rPPG%2520model%2520from%2520overfitting%2520to%2520target%250Adata%2520and%2520forgetting%2520previously%2520acquired%2520knowledge%252C%2520while%2520also%2520broadly%2520covering%250Avarious%2520heart%2520rate%2520%2528HR%2529%2520distributions.%2520Our%2520extensive%2520experiments%2520on%2520the%250ATTA-rPPG%2520benchmark%2520show%2520that%2520the%2520proposed%2520method%2520achieves%2520superior%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13322v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Test-Time%20rPPG%20Estimation%20via%20Synthetic%20Signal-Guided%20Feature%0A%20%20Learning&entry.906535625=Pei-Kai%20Huang%20and%20Tzu-Hsien%20Chen%20and%20Ya-Ting%20Chan%20and%20Kuan-Wen%20Chen%20and%20Chiou-Ting%20Hsu&entry.1292438233=%20%20Many%20remote%20photoplethysmography%20%28rPPG%29%20estimation%20models%20have%20achieved%0Apromising%20performance%20in%20the%20training%20domain%20but%20often%20fail%20to%20accurately%0Aestimate%20physiological%20signals%20or%20heart%20rates%20%28HR%29%20in%20the%20target%20domains.%0ADomain%20generalization%20%28DG%29%20or%20domain%20adaptation%20%28DA%29%20techniques%20are%20therefore%0Aadopted%20during%20the%20offline%20training%20stage%20to%20adapt%20the%20model%20to%20either%0Aunobserved%20or%20observed%20target%20domains%20by%20utilizing%20all%20available%20source%20domain%0Adata.%20However%2C%20in%20rPPG%20estimation%20problems%2C%20the%20adapted%20model%20usually%0Aencounters%20challenges%20in%20estimating%20target%20data%20with%20significant%20domain%0Avariation.%20In%20contrast%2C%20Test-Time%20Adaptation%20%28TTA%29%20enables%20the%20model%20to%0Aadaptively%20estimate%20rPPG%20signals%20in%20various%20unseen%20domains%20by%20online%20adapting%0Ato%20unlabeled%20target%20data%20without%20referring%20to%20any%20source%20data.%20In%20this%20paper%2C%0Awe%20first%20establish%20a%20new%20TTA-rPPG%20benchmark%20that%20encompasses%20various%20domain%0Ainformation%20and%20HR%20distributions%20to%20simulate%20the%20challenges%20encountered%20in%0Areal-world%20rPPG%20estimation.%20Next%2C%20we%20propose%20a%20novel%20synthetic%20signal-guided%0ArPPG%20estimation%20framework%20to%20address%20the%20forgetting%20issue%20during%20the%20TTA%20stage%0Aand%20to%20enhance%20the%20adaptation%20capability%20of%20the%20pre-trained%20rPPG%20model.%20To%20this%0Aend%2C%20we%20develop%20a%20synthetic%20signal-guided%20feature%20learning%20method%20by%0Asynthesizing%20pseudo%20rPPG%20signals%20as%20pseudo%20ground%20truths%20to%20guide%20a%20conditional%0Agenerator%20in%20generating%20latent%20rPPG%20features.%20In%20addition%2C%20we%20design%20an%0Aeffective%20spectral-based%20entropy%20minimization%20technique%20to%20encourage%20the%20rPPG%0Amodel%20to%20learn%20new%20target%20domain%20information.%20Both%20the%20generated%20rPPG%20features%0Aand%20synthesized%20rPPG%20signals%20prevent%20the%20rPPG%20model%20from%20overfitting%20to%20target%0Adata%20and%20forgetting%20previously%20acquired%20knowledge%2C%20while%20also%20broadly%20covering%0Avarious%20heart%20rate%20%28HR%29%20distributions.%20Our%20extensive%20experiments%20on%20the%0ATTA-rPPG%20benchmark%20show%20that%20the%20proposed%20method%20achieves%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13322v3&entry.124074799=Read"},
{"title": "An Efficient Replay for Class-Incremental Learning with Pre-trained\n  Models", "author": "Weimin Yin and Bin Chen adn Chunzhao Xie and Zhenhao Tan", "abstract": "  In general class-incremental learning, researchers typically use sample sets\nas a tool to avoid catastrophic forgetting during continuous learning. At the\nsame time, researchers have also noted the differences between\nclass-incremental learning and Oracle training and have attempted to make\ncorrections. In recent years, researchers have begun to develop\nclass-incremental learning algorithms utilizing pre-trained models, achieving\nsignificant results. This paper observes that in class-incremental learning,\nthe steady state among the weight guided by each class center is disrupted,\nwhich is significantly correlated with catastrophic forgetting. Based on this,\nwe propose a new method to overcoming forgetting . In some cases, by retaining\nonly a single sample unit of each class in memory for replay and applying\nsimple gradient constraints, very good results can be achieved. Experimental\nresults indicate that under the condition of pre-trained models, our method can\nachieve competitive performance with very low computational cost and by simply\nusing the cross-entropy loss.\n", "link": "http://arxiv.org/abs/2408.08084v1", "date": "2024-08-15", "relevancy": 2.4165, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4887}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4859}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Replay%20for%20Class-Incremental%20Learning%20with%20Pre-trained%0A%20%20Models&body=Title%3A%20An%20Efficient%20Replay%20for%20Class-Incremental%20Learning%20with%20Pre-trained%0A%20%20Models%0AAuthor%3A%20Weimin%20Yin%20and%20Bin%20Chen%20adn%20Chunzhao%20Xie%20and%20Zhenhao%20Tan%0AAbstract%3A%20%20%20In%20general%20class-incremental%20learning%2C%20researchers%20typically%20use%20sample%20sets%0Aas%20a%20tool%20to%20avoid%20catastrophic%20forgetting%20during%20continuous%20learning.%20At%20the%0Asame%20time%2C%20researchers%20have%20also%20noted%20the%20differences%20between%0Aclass-incremental%20learning%20and%20Oracle%20training%20and%20have%20attempted%20to%20make%0Acorrections.%20In%20recent%20years%2C%20researchers%20have%20begun%20to%20develop%0Aclass-incremental%20learning%20algorithms%20utilizing%20pre-trained%20models%2C%20achieving%0Asignificant%20results.%20This%20paper%20observes%20that%20in%20class-incremental%20learning%2C%0Athe%20steady%20state%20among%20the%20weight%20guided%20by%20each%20class%20center%20is%20disrupted%2C%0Awhich%20is%20significantly%20correlated%20with%20catastrophic%20forgetting.%20Based%20on%20this%2C%0Awe%20propose%20a%20new%20method%20to%20overcoming%20forgetting%20.%20In%20some%20cases%2C%20by%20retaining%0Aonly%20a%20single%20sample%20unit%20of%20each%20class%20in%20memory%20for%20replay%20and%20applying%0Asimple%20gradient%20constraints%2C%20very%20good%20results%20can%20be%20achieved.%20Experimental%0Aresults%20indicate%20that%20under%20the%20condition%20of%20pre-trained%20models%2C%20our%20method%20can%0Aachieve%20competitive%20performance%20with%20very%20low%20computational%20cost%20and%20by%20simply%0Ausing%20the%20cross-entropy%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Replay%2520for%2520Class-Incremental%2520Learning%2520with%2520Pre-trained%250A%2520%2520Models%26entry.906535625%3DWeimin%2520Yin%2520and%2520Bin%2520Chen%2520adn%2520Chunzhao%2520Xie%2520and%2520Zhenhao%2520Tan%26entry.1292438233%3D%2520%2520In%2520general%2520class-incremental%2520learning%252C%2520researchers%2520typically%2520use%2520sample%2520sets%250Aas%2520a%2520tool%2520to%2520avoid%2520catastrophic%2520forgetting%2520during%2520continuous%2520learning.%2520At%2520the%250Asame%2520time%252C%2520researchers%2520have%2520also%2520noted%2520the%2520differences%2520between%250Aclass-incremental%2520learning%2520and%2520Oracle%2520training%2520and%2520have%2520attempted%2520to%2520make%250Acorrections.%2520In%2520recent%2520years%252C%2520researchers%2520have%2520begun%2520to%2520develop%250Aclass-incremental%2520learning%2520algorithms%2520utilizing%2520pre-trained%2520models%252C%2520achieving%250Asignificant%2520results.%2520This%2520paper%2520observes%2520that%2520in%2520class-incremental%2520learning%252C%250Athe%2520steady%2520state%2520among%2520the%2520weight%2520guided%2520by%2520each%2520class%2520center%2520is%2520disrupted%252C%250Awhich%2520is%2520significantly%2520correlated%2520with%2520catastrophic%2520forgetting.%2520Based%2520on%2520this%252C%250Awe%2520propose%2520a%2520new%2520method%2520to%2520overcoming%2520forgetting%2520.%2520In%2520some%2520cases%252C%2520by%2520retaining%250Aonly%2520a%2520single%2520sample%2520unit%2520of%2520each%2520class%2520in%2520memory%2520for%2520replay%2520and%2520applying%250Asimple%2520gradient%2520constraints%252C%2520very%2520good%2520results%2520can%2520be%2520achieved.%2520Experimental%250Aresults%2520indicate%2520that%2520under%2520the%2520condition%2520of%2520pre-trained%2520models%252C%2520our%2520method%2520can%250Aachieve%2520competitive%2520performance%2520with%2520very%2520low%2520computational%2520cost%2520and%2520by%2520simply%250Ausing%2520the%2520cross-entropy%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Replay%20for%20Class-Incremental%20Learning%20with%20Pre-trained%0A%20%20Models&entry.906535625=Weimin%20Yin%20and%20Bin%20Chen%20adn%20Chunzhao%20Xie%20and%20Zhenhao%20Tan&entry.1292438233=%20%20In%20general%20class-incremental%20learning%2C%20researchers%20typically%20use%20sample%20sets%0Aas%20a%20tool%20to%20avoid%20catastrophic%20forgetting%20during%20continuous%20learning.%20At%20the%0Asame%20time%2C%20researchers%20have%20also%20noted%20the%20differences%20between%0Aclass-incremental%20learning%20and%20Oracle%20training%20and%20have%20attempted%20to%20make%0Acorrections.%20In%20recent%20years%2C%20researchers%20have%20begun%20to%20develop%0Aclass-incremental%20learning%20algorithms%20utilizing%20pre-trained%20models%2C%20achieving%0Asignificant%20results.%20This%20paper%20observes%20that%20in%20class-incremental%20learning%2C%0Athe%20steady%20state%20among%20the%20weight%20guided%20by%20each%20class%20center%20is%20disrupted%2C%0Awhich%20is%20significantly%20correlated%20with%20catastrophic%20forgetting.%20Based%20on%20this%2C%0Awe%20propose%20a%20new%20method%20to%20overcoming%20forgetting%20.%20In%20some%20cases%2C%20by%20retaining%0Aonly%20a%20single%20sample%20unit%20of%20each%20class%20in%20memory%20for%20replay%20and%20applying%0Asimple%20gradient%20constraints%2C%20very%20good%20results%20can%20be%20achieved.%20Experimental%0Aresults%20indicate%20that%20under%20the%20condition%20of%20pre-trained%20models%2C%20our%20method%20can%0Aachieve%20competitive%20performance%20with%20very%20low%20computational%20cost%20and%20by%20simply%0Ausing%20the%20cross-entropy%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08084v1&entry.124074799=Read"},
{"title": "VLPG-Nav: Object Navigation Using Visual Language Pose Graph and Object\n  Localization Probability Maps", "author": "Senthil Hariharan Arul and Dhruva Kumar and Vivek Sugirtharaj and Richard Kim and  Xuewei and  Qi and Rajasimman Madhivanan and Arnie Sen and Dinesh Manocha", "abstract": "  We present VLPG-Nav, a visual language navigation method for guiding robots\nto specified objects within household scenes. Unlike existing methods primarily\nfocused on navigating the robot toward objects, our approach considers the\nadditional challenge of centering the object within the robot's camera view.\nOur method builds a visual language pose graph (VLPG) that functions as a\nspatial map of VL embeddings. Given an open vocabulary object query, we plan a\nviewpoint for object navigation using the VLPG. Despite navigating to the\nviewpoint, real-world challenges like object occlusion, displacement, and the\nrobot's localization error can prevent visibility. We build an object\nlocalization probability map that leverages the robot's current observations\nand prior VLPG. When the object isn't visible, the probability map is updated\nand an alternate viewpoint is computed. In addition, we propose an\nobject-centering formulation that locally adjusts the robot's pose to center\nthe object in the camera view. We evaluate the effectiveness of our approach\nthrough simulations and real-world experiments, evaluating its ability to\nsuccessfully view and center the object within the camera field of view.\nVLPG-Nav demonstrates improved performance in locating the object, navigating\naround occlusions, and centering the object within the robot's camera view,\noutperforming the selected baselines in the evaluation metrics.\n", "link": "http://arxiv.org/abs/2408.08301v1", "date": "2024-08-15", "relevancy": 2.3758, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6049}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5893}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLPG-Nav%3A%20Object%20Navigation%20Using%20Visual%20Language%20Pose%20Graph%20and%20Object%0A%20%20Localization%20Probability%20Maps&body=Title%3A%20VLPG-Nav%3A%20Object%20Navigation%20Using%20Visual%20Language%20Pose%20Graph%20and%20Object%0A%20%20Localization%20Probability%20Maps%0AAuthor%3A%20Senthil%20Hariharan%20Arul%20and%20Dhruva%20Kumar%20and%20Vivek%20Sugirtharaj%20and%20Richard%20Kim%20and%20%20Xuewei%20and%20%20Qi%20and%20Rajasimman%20Madhivanan%20and%20Arnie%20Sen%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20VLPG-Nav%2C%20a%20visual%20language%20navigation%20method%20for%20guiding%20robots%0Ato%20specified%20objects%20within%20household%20scenes.%20Unlike%20existing%20methods%20primarily%0Afocused%20on%20navigating%20the%20robot%20toward%20objects%2C%20our%20approach%20considers%20the%0Aadditional%20challenge%20of%20centering%20the%20object%20within%20the%20robot%27s%20camera%20view.%0AOur%20method%20builds%20a%20visual%20language%20pose%20graph%20%28VLPG%29%20that%20functions%20as%20a%0Aspatial%20map%20of%20VL%20embeddings.%20Given%20an%20open%20vocabulary%20object%20query%2C%20we%20plan%20a%0Aviewpoint%20for%20object%20navigation%20using%20the%20VLPG.%20Despite%20navigating%20to%20the%0Aviewpoint%2C%20real-world%20challenges%20like%20object%20occlusion%2C%20displacement%2C%20and%20the%0Arobot%27s%20localization%20error%20can%20prevent%20visibility.%20We%20build%20an%20object%0Alocalization%20probability%20map%20that%20leverages%20the%20robot%27s%20current%20observations%0Aand%20prior%20VLPG.%20When%20the%20object%20isn%27t%20visible%2C%20the%20probability%20map%20is%20updated%0Aand%20an%20alternate%20viewpoint%20is%20computed.%20In%20addition%2C%20we%20propose%20an%0Aobject-centering%20formulation%20that%20locally%20adjusts%20the%20robot%27s%20pose%20to%20center%0Athe%20object%20in%20the%20camera%20view.%20We%20evaluate%20the%20effectiveness%20of%20our%20approach%0Athrough%20simulations%20and%20real-world%20experiments%2C%20evaluating%20its%20ability%20to%0Asuccessfully%20view%20and%20center%20the%20object%20within%20the%20camera%20field%20of%20view.%0AVLPG-Nav%20demonstrates%20improved%20performance%20in%20locating%20the%20object%2C%20navigating%0Aaround%20occlusions%2C%20and%20centering%20the%20object%20within%20the%20robot%27s%20camera%20view%2C%0Aoutperforming%20the%20selected%20baselines%20in%20the%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLPG-Nav%253A%2520Object%2520Navigation%2520Using%2520Visual%2520Language%2520Pose%2520Graph%2520and%2520Object%250A%2520%2520Localization%2520Probability%2520Maps%26entry.906535625%3DSenthil%2520Hariharan%2520Arul%2520and%2520Dhruva%2520Kumar%2520and%2520Vivek%2520Sugirtharaj%2520and%2520Richard%2520Kim%2520and%2520%2520Xuewei%2520and%2520%2520Qi%2520and%2520Rajasimman%2520Madhivanan%2520and%2520Arnie%2520Sen%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520VLPG-Nav%252C%2520a%2520visual%2520language%2520navigation%2520method%2520for%2520guiding%2520robots%250Ato%2520specified%2520objects%2520within%2520household%2520scenes.%2520Unlike%2520existing%2520methods%2520primarily%250Afocused%2520on%2520navigating%2520the%2520robot%2520toward%2520objects%252C%2520our%2520approach%2520considers%2520the%250Aadditional%2520challenge%2520of%2520centering%2520the%2520object%2520within%2520the%2520robot%2527s%2520camera%2520view.%250AOur%2520method%2520builds%2520a%2520visual%2520language%2520pose%2520graph%2520%2528VLPG%2529%2520that%2520functions%2520as%2520a%250Aspatial%2520map%2520of%2520VL%2520embeddings.%2520Given%2520an%2520open%2520vocabulary%2520object%2520query%252C%2520we%2520plan%2520a%250Aviewpoint%2520for%2520object%2520navigation%2520using%2520the%2520VLPG.%2520Despite%2520navigating%2520to%2520the%250Aviewpoint%252C%2520real-world%2520challenges%2520like%2520object%2520occlusion%252C%2520displacement%252C%2520and%2520the%250Arobot%2527s%2520localization%2520error%2520can%2520prevent%2520visibility.%2520We%2520build%2520an%2520object%250Alocalization%2520probability%2520map%2520that%2520leverages%2520the%2520robot%2527s%2520current%2520observations%250Aand%2520prior%2520VLPG.%2520When%2520the%2520object%2520isn%2527t%2520visible%252C%2520the%2520probability%2520map%2520is%2520updated%250Aand%2520an%2520alternate%2520viewpoint%2520is%2520computed.%2520In%2520addition%252C%2520we%2520propose%2520an%250Aobject-centering%2520formulation%2520that%2520locally%2520adjusts%2520the%2520robot%2527s%2520pose%2520to%2520center%250Athe%2520object%2520in%2520the%2520camera%2520view.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520approach%250Athrough%2520simulations%2520and%2520real-world%2520experiments%252C%2520evaluating%2520its%2520ability%2520to%250Asuccessfully%2520view%2520and%2520center%2520the%2520object%2520within%2520the%2520camera%2520field%2520of%2520view.%250AVLPG-Nav%2520demonstrates%2520improved%2520performance%2520in%2520locating%2520the%2520object%252C%2520navigating%250Aaround%2520occlusions%252C%2520and%2520centering%2520the%2520object%2520within%2520the%2520robot%2527s%2520camera%2520view%252C%250Aoutperforming%2520the%2520selected%2520baselines%2520in%2520the%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLPG-Nav%3A%20Object%20Navigation%20Using%20Visual%20Language%20Pose%20Graph%20and%20Object%0A%20%20Localization%20Probability%20Maps&entry.906535625=Senthil%20Hariharan%20Arul%20and%20Dhruva%20Kumar%20and%20Vivek%20Sugirtharaj%20and%20Richard%20Kim%20and%20%20Xuewei%20and%20%20Qi%20and%20Rajasimman%20Madhivanan%20and%20Arnie%20Sen%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20VLPG-Nav%2C%20a%20visual%20language%20navigation%20method%20for%20guiding%20robots%0Ato%20specified%20objects%20within%20household%20scenes.%20Unlike%20existing%20methods%20primarily%0Afocused%20on%20navigating%20the%20robot%20toward%20objects%2C%20our%20approach%20considers%20the%0Aadditional%20challenge%20of%20centering%20the%20object%20within%20the%20robot%27s%20camera%20view.%0AOur%20method%20builds%20a%20visual%20language%20pose%20graph%20%28VLPG%29%20that%20functions%20as%20a%0Aspatial%20map%20of%20VL%20embeddings.%20Given%20an%20open%20vocabulary%20object%20query%2C%20we%20plan%20a%0Aviewpoint%20for%20object%20navigation%20using%20the%20VLPG.%20Despite%20navigating%20to%20the%0Aviewpoint%2C%20real-world%20challenges%20like%20object%20occlusion%2C%20displacement%2C%20and%20the%0Arobot%27s%20localization%20error%20can%20prevent%20visibility.%20We%20build%20an%20object%0Alocalization%20probability%20map%20that%20leverages%20the%20robot%27s%20current%20observations%0Aand%20prior%20VLPG.%20When%20the%20object%20isn%27t%20visible%2C%20the%20probability%20map%20is%20updated%0Aand%20an%20alternate%20viewpoint%20is%20computed.%20In%20addition%2C%20we%20propose%20an%0Aobject-centering%20formulation%20that%20locally%20adjusts%20the%20robot%27s%20pose%20to%20center%0Athe%20object%20in%20the%20camera%20view.%20We%20evaluate%20the%20effectiveness%20of%20our%20approach%0Athrough%20simulations%20and%20real-world%20experiments%2C%20evaluating%20its%20ability%20to%0Asuccessfully%20view%20and%20center%20the%20object%20within%20the%20camera%20field%20of%20view.%0AVLPG-Nav%20demonstrates%20improved%20performance%20in%20locating%20the%20object%2C%20navigating%0Aaround%20occlusions%2C%20and%20centering%20the%20object%20within%20the%20robot%27s%20camera%20view%2C%0Aoutperforming%20the%20selected%20baselines%20in%20the%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08301v1&entry.124074799=Read"},
{"title": "Nearest Neighbor Classification for Classical Image Upsampling", "author": "Evan Matthews and Nicolas Prate", "abstract": "  Given a set of ordered pixel data in the form of an image, our goal is to\nperform upsampling on the data such that: the resulting resolution is improved\nby some factor, the final result passes the human test, having added new,\nbelievable, and realistic information and detail to the image, the time\ncomplexity for upscaling is relatively close to that of lossy upscaling\nimplementations.\n", "link": "http://arxiv.org/abs/2403.19611v2", "date": "2024-08-15", "relevancy": 2.3713, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4971}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4753}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nearest%20Neighbor%20Classification%20for%20Classical%20Image%20Upsampling&body=Title%3A%20Nearest%20Neighbor%20Classification%20for%20Classical%20Image%20Upsampling%0AAuthor%3A%20Evan%20Matthews%20and%20Nicolas%20Prate%0AAbstract%3A%20%20%20Given%20a%20set%20of%20ordered%20pixel%20data%20in%20the%20form%20of%20an%20image%2C%20our%20goal%20is%20to%0Aperform%20upsampling%20on%20the%20data%20such%20that%3A%20the%20resulting%20resolution%20is%20improved%0Aby%20some%20factor%2C%20the%20final%20result%20passes%20the%20human%20test%2C%20having%20added%20new%2C%0Abelievable%2C%20and%20realistic%20information%20and%20detail%20to%20the%20image%2C%20the%20time%0Acomplexity%20for%20upscaling%20is%20relatively%20close%20to%20that%20of%20lossy%20upscaling%0Aimplementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNearest%2520Neighbor%2520Classification%2520for%2520Classical%2520Image%2520Upsampling%26entry.906535625%3DEvan%2520Matthews%2520and%2520Nicolas%2520Prate%26entry.1292438233%3D%2520%2520Given%2520a%2520set%2520of%2520ordered%2520pixel%2520data%2520in%2520the%2520form%2520of%2520an%2520image%252C%2520our%2520goal%2520is%2520to%250Aperform%2520upsampling%2520on%2520the%2520data%2520such%2520that%253A%2520the%2520resulting%2520resolution%2520is%2520improved%250Aby%2520some%2520factor%252C%2520the%2520final%2520result%2520passes%2520the%2520human%2520test%252C%2520having%2520added%2520new%252C%250Abelievable%252C%2520and%2520realistic%2520information%2520and%2520detail%2520to%2520the%2520image%252C%2520the%2520time%250Acomplexity%2520for%2520upscaling%2520is%2520relatively%2520close%2520to%2520that%2520of%2520lossy%2520upscaling%250Aimplementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nearest%20Neighbor%20Classification%20for%20Classical%20Image%20Upsampling&entry.906535625=Evan%20Matthews%20and%20Nicolas%20Prate&entry.1292438233=%20%20Given%20a%20set%20of%20ordered%20pixel%20data%20in%20the%20form%20of%20an%20image%2C%20our%20goal%20is%20to%0Aperform%20upsampling%20on%20the%20data%20such%20that%3A%20the%20resulting%20resolution%20is%20improved%0Aby%20some%20factor%2C%20the%20final%20result%20passes%20the%20human%20test%2C%20having%20added%20new%2C%0Abelievable%2C%20and%20realistic%20information%20and%20detail%20to%20the%20image%2C%20the%20time%0Acomplexity%20for%20upscaling%20is%20relatively%20close%20to%20that%20of%20lossy%20upscaling%0Aimplementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19611v2&entry.124074799=Read"},
{"title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in\n  Stable Diffusion", "author": "Adi Haviv and Shahar Sarfaty and Uri Hacohen and Niva Elkin-Koren and Roi Livni and Amit H Bermano", "abstract": "  This work addresses the challenge of quantifying originality in text-to-image\n(T2I) generative diffusion models, with a focus on copyright originality. We\nbegin by evaluating T2I models' ability to innovate and generalize through\ncontrolled experiments, revealing that stable diffusion models can effectively\nrecreate unseen elements with sufficiently diverse training data. Then, our key\ninsight is that concepts and combinations of image elements the model is\nfamiliar with, and saw more during training, are more concisly represented in\nthe model's latent space. We hence propose a method that leverages textual\ninversion to measure the originality of an image based on the number of tokens\nrequired for its reconstruction by the model. Our approach is inspired by legal\ndefinitions of originality and aims to assess whether a model can produce\noriginal content without relying on specific prompts or having the training\ndata of the model. We demonstrate our method using both a pre-trained stable\ndiffusion model and a synthetic dataset, showing a correlation between the\nnumber of tokens and image originality. This work contributes to the\nunderstanding of originality in generative models and has implications for\ncopyright infringement cases.\n", "link": "http://arxiv.org/abs/2408.08184v1", "date": "2024-08-15", "relevancy": 2.3707, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6012}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5935}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20Every%20Image%20is%20Worth%20a%20Thousand%20Words%3A%20Quantifying%20Originality%20in%0A%20%20Stable%20Diffusion&body=Title%3A%20Not%20Every%20Image%20is%20Worth%20a%20Thousand%20Words%3A%20Quantifying%20Originality%20in%0A%20%20Stable%20Diffusion%0AAuthor%3A%20Adi%20Haviv%20and%20Shahar%20Sarfaty%20and%20Uri%20Hacohen%20and%20Niva%20Elkin-Koren%20and%20Roi%20Livni%20and%20Amit%20H%20Bermano%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20challenge%20of%20quantifying%20originality%20in%20text-to-image%0A%28T2I%29%20generative%20diffusion%20models%2C%20with%20a%20focus%20on%20copyright%20originality.%20We%0Abegin%20by%20evaluating%20T2I%20models%27%20ability%20to%20innovate%20and%20generalize%20through%0Acontrolled%20experiments%2C%20revealing%20that%20stable%20diffusion%20models%20can%20effectively%0Arecreate%20unseen%20elements%20with%20sufficiently%20diverse%20training%20data.%20Then%2C%20our%20key%0Ainsight%20is%20that%20concepts%20and%20combinations%20of%20image%20elements%20the%20model%20is%0Afamiliar%20with%2C%20and%20saw%20more%20during%20training%2C%20are%20more%20concisly%20represented%20in%0Athe%20model%27s%20latent%20space.%20We%20hence%20propose%20a%20method%20that%20leverages%20textual%0Ainversion%20to%20measure%20the%20originality%20of%20an%20image%20based%20on%20the%20number%20of%20tokens%0Arequired%20for%20its%20reconstruction%20by%20the%20model.%20Our%20approach%20is%20inspired%20by%20legal%0Adefinitions%20of%20originality%20and%20aims%20to%20assess%20whether%20a%20model%20can%20produce%0Aoriginal%20content%20without%20relying%20on%20specific%20prompts%20or%20having%20the%20training%0Adata%20of%20the%20model.%20We%20demonstrate%20our%20method%20using%20both%20a%20pre-trained%20stable%0Adiffusion%20model%20and%20a%20synthetic%20dataset%2C%20showing%20a%20correlation%20between%20the%0Anumber%20of%20tokens%20and%20image%20originality.%20This%20work%20contributes%20to%20the%0Aunderstanding%20of%20originality%20in%20generative%20models%20and%20has%20implications%20for%0Acopyright%20infringement%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520Every%2520Image%2520is%2520Worth%2520a%2520Thousand%2520Words%253A%2520Quantifying%2520Originality%2520in%250A%2520%2520Stable%2520Diffusion%26entry.906535625%3DAdi%2520Haviv%2520and%2520Shahar%2520Sarfaty%2520and%2520Uri%2520Hacohen%2520and%2520Niva%2520Elkin-Koren%2520and%2520Roi%2520Livni%2520and%2520Amit%2520H%2520Bermano%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520challenge%2520of%2520quantifying%2520originality%2520in%2520text-to-image%250A%2528T2I%2529%2520generative%2520diffusion%2520models%252C%2520with%2520a%2520focus%2520on%2520copyright%2520originality.%2520We%250Abegin%2520by%2520evaluating%2520T2I%2520models%2527%2520ability%2520to%2520innovate%2520and%2520generalize%2520through%250Acontrolled%2520experiments%252C%2520revealing%2520that%2520stable%2520diffusion%2520models%2520can%2520effectively%250Arecreate%2520unseen%2520elements%2520with%2520sufficiently%2520diverse%2520training%2520data.%2520Then%252C%2520our%2520key%250Ainsight%2520is%2520that%2520concepts%2520and%2520combinations%2520of%2520image%2520elements%2520the%2520model%2520is%250Afamiliar%2520with%252C%2520and%2520saw%2520more%2520during%2520training%252C%2520are%2520more%2520concisly%2520represented%2520in%250Athe%2520model%2527s%2520latent%2520space.%2520We%2520hence%2520propose%2520a%2520method%2520that%2520leverages%2520textual%250Ainversion%2520to%2520measure%2520the%2520originality%2520of%2520an%2520image%2520based%2520on%2520the%2520number%2520of%2520tokens%250Arequired%2520for%2520its%2520reconstruction%2520by%2520the%2520model.%2520Our%2520approach%2520is%2520inspired%2520by%2520legal%250Adefinitions%2520of%2520originality%2520and%2520aims%2520to%2520assess%2520whether%2520a%2520model%2520can%2520produce%250Aoriginal%2520content%2520without%2520relying%2520on%2520specific%2520prompts%2520or%2520having%2520the%2520training%250Adata%2520of%2520the%2520model.%2520We%2520demonstrate%2520our%2520method%2520using%2520both%2520a%2520pre-trained%2520stable%250Adiffusion%2520model%2520and%2520a%2520synthetic%2520dataset%252C%2520showing%2520a%2520correlation%2520between%2520the%250Anumber%2520of%2520tokens%2520and%2520image%2520originality.%2520This%2520work%2520contributes%2520to%2520the%250Aunderstanding%2520of%2520originality%2520in%2520generative%2520models%2520and%2520has%2520implications%2520for%250Acopyright%2520infringement%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20Every%20Image%20is%20Worth%20a%20Thousand%20Words%3A%20Quantifying%20Originality%20in%0A%20%20Stable%20Diffusion&entry.906535625=Adi%20Haviv%20and%20Shahar%20Sarfaty%20and%20Uri%20Hacohen%20and%20Niva%20Elkin-Koren%20and%20Roi%20Livni%20and%20Amit%20H%20Bermano&entry.1292438233=%20%20This%20work%20addresses%20the%20challenge%20of%20quantifying%20originality%20in%20text-to-image%0A%28T2I%29%20generative%20diffusion%20models%2C%20with%20a%20focus%20on%20copyright%20originality.%20We%0Abegin%20by%20evaluating%20T2I%20models%27%20ability%20to%20innovate%20and%20generalize%20through%0Acontrolled%20experiments%2C%20revealing%20that%20stable%20diffusion%20models%20can%20effectively%0Arecreate%20unseen%20elements%20with%20sufficiently%20diverse%20training%20data.%20Then%2C%20our%20key%0Ainsight%20is%20that%20concepts%20and%20combinations%20of%20image%20elements%20the%20model%20is%0Afamiliar%20with%2C%20and%20saw%20more%20during%20training%2C%20are%20more%20concisly%20represented%20in%0Athe%20model%27s%20latent%20space.%20We%20hence%20propose%20a%20method%20that%20leverages%20textual%0Ainversion%20to%20measure%20the%20originality%20of%20an%20image%20based%20on%20the%20number%20of%20tokens%0Arequired%20for%20its%20reconstruction%20by%20the%20model.%20Our%20approach%20is%20inspired%20by%20legal%0Adefinitions%20of%20originality%20and%20aims%20to%20assess%20whether%20a%20model%20can%20produce%0Aoriginal%20content%20without%20relying%20on%20specific%20prompts%20or%20having%20the%20training%0Adata%20of%20the%20model.%20We%20demonstrate%20our%20method%20using%20both%20a%20pre-trained%20stable%0Adiffusion%20model%20and%20a%20synthetic%20dataset%2C%20showing%20a%20correlation%20between%20the%0Anumber%20of%20tokens%20and%20image%20originality.%20This%20work%20contributes%20to%20the%0Aunderstanding%20of%20originality%20in%20generative%20models%20and%20has%20implications%20for%0Acopyright%20infringement%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08184v1&entry.124074799=Read"},
{"title": "DeepInteraction++: Multi-Modality Interaction for Autonomous Driving", "author": "Zeyu Yang and Nan Song and Wei Li and Xiatian Zhu and Li Zhang and Philip H. S. Torr", "abstract": "  Existing top-performance autonomous driving systems typically rely on the\nmulti-modal fusion strategy for reliable scene understanding. This design is\nhowever fundamentally restricted due to overlooking the modality-specific\nstrengths and finally hampering the model performance. To address this\nlimitation, in this work, we introduce a novel modality interaction strategy\nthat allows individual per-modality representations to be learned and\nmaintained throughout, enabling their unique characteristics to be exploited\nduring the whole perception pipeline. To demonstrate the effectiveness of the\nproposed strategy, we design DeepInteraction++, a multi-modal interaction\nframework characterized by a multi-modal representational interaction encoder\nand a multi-modal predictive interaction decoder. Specifically, the encoder is\nimplemented as a dual-stream Transformer with specialized attention operation\nfor information exchange and integration between separate modality-specific\nrepresentations. Our multi-modal representational learning incorporates both\nobject-centric, precise sampling-based feature alignment and global dense\ninformation spreading, essential for the more challenging planning task. The\ndecoder is designed to iteratively refine the predictions by alternately\naggregating information from separate representations in a unified\nmodality-agnostic manner, realizing multi-modal predictive interaction.\nExtensive experiments demonstrate the superior performance of the proposed\nframework on both 3D object detection and end-to-end autonomous driving tasks.\nOur code is available at https://github.com/fudan-zvg/DeepInteraction.\n", "link": "http://arxiv.org/abs/2408.05075v2", "date": "2024-08-15", "relevancy": 2.3527, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5985}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving&body=Title%3A%20DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving%0AAuthor%3A%20Zeyu%20Yang%20and%20Nan%20Song%20and%20Wei%20Li%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%20and%20Philip%20H.%20S.%20Torr%0AAbstract%3A%20%20%20Existing%20top-performance%20autonomous%20driving%20systems%20typically%20rely%20on%20the%0Amulti-modal%20fusion%20strategy%20for%20reliable%20scene%20understanding.%20This%20design%20is%0Ahowever%20fundamentally%20restricted%20due%20to%20overlooking%20the%20modality-specific%0Astrengths%20and%20finally%20hampering%20the%20model%20performance.%20To%20address%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20introduce%20a%20novel%20modality%20interaction%20strategy%0Athat%20allows%20individual%20per-modality%20representations%20to%20be%20learned%20and%0Amaintained%20throughout%2C%20enabling%20their%20unique%20characteristics%20to%20be%20exploited%0Aduring%20the%20whole%20perception%20pipeline.%20To%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20strategy%2C%20we%20design%20DeepInteraction%2B%2B%2C%20a%20multi-modal%20interaction%0Aframework%20characterized%20by%20a%20multi-modal%20representational%20interaction%20encoder%0Aand%20a%20multi-modal%20predictive%20interaction%20decoder.%20Specifically%2C%20the%20encoder%20is%0Aimplemented%20as%20a%20dual-stream%20Transformer%20with%20specialized%20attention%20operation%0Afor%20information%20exchange%20and%20integration%20between%20separate%20modality-specific%0Arepresentations.%20Our%20multi-modal%20representational%20learning%20incorporates%20both%0Aobject-centric%2C%20precise%20sampling-based%20feature%20alignment%20and%20global%20dense%0Ainformation%20spreading%2C%20essential%20for%20the%20more%20challenging%20planning%20task.%20The%0Adecoder%20is%20designed%20to%20iteratively%20refine%20the%20predictions%20by%20alternately%0Aaggregating%20information%20from%20separate%20representations%20in%20a%20unified%0Amodality-agnostic%20manner%2C%20realizing%20multi-modal%20predictive%20interaction.%0AExtensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%0Aframework%20on%20both%203D%20object%20detection%20and%20end-to-end%20autonomous%20driving%20tasks.%0AOur%20code%20is%20available%20at%20https%3A//github.com/fudan-zvg/DeepInteraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepInteraction%252B%252B%253A%2520Multi-Modality%2520Interaction%2520for%2520Autonomous%2520Driving%26entry.906535625%3DZeyu%2520Yang%2520and%2520Nan%2520Song%2520and%2520Wei%2520Li%2520and%2520Xiatian%2520Zhu%2520and%2520Li%2520Zhang%2520and%2520Philip%2520H.%2520S.%2520Torr%26entry.1292438233%3D%2520%2520Existing%2520top-performance%2520autonomous%2520driving%2520systems%2520typically%2520rely%2520on%2520the%250Amulti-modal%2520fusion%2520strategy%2520for%2520reliable%2520scene%2520understanding.%2520This%2520design%2520is%250Ahowever%2520fundamentally%2520restricted%2520due%2520to%2520overlooking%2520the%2520modality-specific%250Astrengths%2520and%2520finally%2520hampering%2520the%2520model%2520performance.%2520To%2520address%2520this%250Alimitation%252C%2520in%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520modality%2520interaction%2520strategy%250Athat%2520allows%2520individual%2520per-modality%2520representations%2520to%2520be%2520learned%2520and%250Amaintained%2520throughout%252C%2520enabling%2520their%2520unique%2520characteristics%2520to%2520be%2520exploited%250Aduring%2520the%2520whole%2520perception%2520pipeline.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520strategy%252C%2520we%2520design%2520DeepInteraction%252B%252B%252C%2520a%2520multi-modal%2520interaction%250Aframework%2520characterized%2520by%2520a%2520multi-modal%2520representational%2520interaction%2520encoder%250Aand%2520a%2520multi-modal%2520predictive%2520interaction%2520decoder.%2520Specifically%252C%2520the%2520encoder%2520is%250Aimplemented%2520as%2520a%2520dual-stream%2520Transformer%2520with%2520specialized%2520attention%2520operation%250Afor%2520information%2520exchange%2520and%2520integration%2520between%2520separate%2520modality-specific%250Arepresentations.%2520Our%2520multi-modal%2520representational%2520learning%2520incorporates%2520both%250Aobject-centric%252C%2520precise%2520sampling-based%2520feature%2520alignment%2520and%2520global%2520dense%250Ainformation%2520spreading%252C%2520essential%2520for%2520the%2520more%2520challenging%2520planning%2520task.%2520The%250Adecoder%2520is%2520designed%2520to%2520iteratively%2520refine%2520the%2520predictions%2520by%2520alternately%250Aaggregating%2520information%2520from%2520separate%2520representations%2520in%2520a%2520unified%250Amodality-agnostic%2520manner%252C%2520realizing%2520multi-modal%2520predictive%2520interaction.%250AExtensive%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520proposed%250Aframework%2520on%2520both%25203D%2520object%2520detection%2520and%2520end-to-end%2520autonomous%2520driving%2520tasks.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/fudan-zvg/DeepInteraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepInteraction%2B%2B%3A%20Multi-Modality%20Interaction%20for%20Autonomous%20Driving&entry.906535625=Zeyu%20Yang%20and%20Nan%20Song%20and%20Wei%20Li%20and%20Xiatian%20Zhu%20and%20Li%20Zhang%20and%20Philip%20H.%20S.%20Torr&entry.1292438233=%20%20Existing%20top-performance%20autonomous%20driving%20systems%20typically%20rely%20on%20the%0Amulti-modal%20fusion%20strategy%20for%20reliable%20scene%20understanding.%20This%20design%20is%0Ahowever%20fundamentally%20restricted%20due%20to%20overlooking%20the%20modality-specific%0Astrengths%20and%20finally%20hampering%20the%20model%20performance.%20To%20address%20this%0Alimitation%2C%20in%20this%20work%2C%20we%20introduce%20a%20novel%20modality%20interaction%20strategy%0Athat%20allows%20individual%20per-modality%20representations%20to%20be%20learned%20and%0Amaintained%20throughout%2C%20enabling%20their%20unique%20characteristics%20to%20be%20exploited%0Aduring%20the%20whole%20perception%20pipeline.%20To%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20strategy%2C%20we%20design%20DeepInteraction%2B%2B%2C%20a%20multi-modal%20interaction%0Aframework%20characterized%20by%20a%20multi-modal%20representational%20interaction%20encoder%0Aand%20a%20multi-modal%20predictive%20interaction%20decoder.%20Specifically%2C%20the%20encoder%20is%0Aimplemented%20as%20a%20dual-stream%20Transformer%20with%20specialized%20attention%20operation%0Afor%20information%20exchange%20and%20integration%20between%20separate%20modality-specific%0Arepresentations.%20Our%20multi-modal%20representational%20learning%20incorporates%20both%0Aobject-centric%2C%20precise%20sampling-based%20feature%20alignment%20and%20global%20dense%0Ainformation%20spreading%2C%20essential%20for%20the%20more%20challenging%20planning%20task.%20The%0Adecoder%20is%20designed%20to%20iteratively%20refine%20the%20predictions%20by%20alternately%0Aaggregating%20information%20from%20separate%20representations%20in%20a%20unified%0Amodality-agnostic%20manner%2C%20realizing%20multi-modal%20predictive%20interaction.%0AExtensive%20experiments%20demonstrate%20the%20superior%20performance%20of%20the%20proposed%0Aframework%20on%20both%203D%20object%20detection%20and%20end-to-end%20autonomous%20driving%20tasks.%0AOur%20code%20is%20available%20at%20https%3A//github.com/fudan-zvg/DeepInteraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05075v2&entry.124074799=Read"},
{"title": "Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition", "author": "Jinfu Liu and Chen Chen and Mengyuan Liu", "abstract": "  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n", "link": "http://arxiv.org/abs/2407.15706v6", "date": "2024-08-15", "relevancy": 2.3402, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6208}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5714}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition&body=Title%3A%20Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition%0AAuthor%3A%20Jinfu%20Liu%20and%20Chen%20Chen%20and%20Mengyuan%20Liu%0AAbstract%3A%20%20%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20due%20to%0Athe%20utilization%20of%20concise%20and%20resilient%20skeletons.%20Nevertheless%2C%20the%20absence%0Aof%20detailed%20body%20information%20in%20skeletons%20restricts%20performance%2C%20while%20other%0Amultimodal%20methods%20require%20substantial%20inference%20resources%20and%20are%20inefficient%0Awhen%20using%20multimodal%20data%20during%20both%20training%20and%20inference%20stages.%20To%0Aaddress%20this%20and%20fully%20harness%20the%20complementary%20multimodal%20features%2C%20we%0Apropose%20a%20novel%20multi-modality%20co-learning%20%28MMCL%29%20framework%20by%20leveraging%20the%0Amultimodal%20large%20language%20models%20%28LLMs%29%20as%20auxiliary%20networks%20for%20efficient%0Askeleton-based%20action%20recognition%2C%20which%20engages%20in%20multi-modality%20co-learning%0Aduring%20the%20training%20stage%20and%20keeps%20efficiency%20by%20employing%20only%20concise%0Askeletons%20in%20inference.%20Our%20MMCL%20framework%20primarily%20consists%20of%20two%20modules.%0AFirst%2C%20the%20Feature%20Alignment%20Module%20%28FAM%29%20extracts%20rich%20RGB%20features%20from%20video%0Aframes%20and%20aligns%20them%20with%20global%20skeleton%20features%20via%20contrastive%20learning.%0ASecond%2C%20the%20Feature%20Refinement%20Module%20%28FRM%29%20uses%20RGB%20images%20with%20temporal%0Ainformation%20and%20text%20instruction%20to%20generate%20instructive%20features%20based%20on%20the%0Apowerful%20generalization%20of%20multimodal%20LLMs.%20These%20instructive%20text%20features%0Awill%20further%20refine%20the%20classification%20scores%20and%20the%20refined%20scores%20will%0Aenhance%20the%20model%27s%20robustness%20and%20generalization%20in%20a%20manner%20similar%20to%20soft%0Alabels.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20Northwestern-UCLA%0Abenchmarks%20consistently%20verify%20the%20effectiveness%20of%20our%20MMCL%2C%20which%20outperforms%0Athe%20existing%20skeleton-based%20action%20recognition%20methods.%20Meanwhile%2C%20experiments%0Aon%20UTD-MHAD%20and%20SYSU-Action%20datasets%20demonstrate%20the%20commendable%20generalization%0Aof%20our%20MMCL%20in%20zero-shot%20and%20domain-adaptive%20action%20recognition.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/liujf69/MMCL-Action.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15706v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modality%2520Co-Learning%2520for%2520Efficient%2520Skeleton-based%2520Action%250A%2520%2520Recognition%26entry.906535625%3DJinfu%2520Liu%2520and%2520Chen%2520Chen%2520and%2520Mengyuan%2520Liu%26entry.1292438233%3D%2520%2520Skeleton-based%2520action%2520recognition%2520has%2520garnered%2520significant%2520attention%2520due%2520to%250Athe%2520utilization%2520of%2520concise%2520and%2520resilient%2520skeletons.%2520Nevertheless%252C%2520the%2520absence%250Aof%2520detailed%2520body%2520information%2520in%2520skeletons%2520restricts%2520performance%252C%2520while%2520other%250Amultimodal%2520methods%2520require%2520substantial%2520inference%2520resources%2520and%2520are%2520inefficient%250Awhen%2520using%2520multimodal%2520data%2520during%2520both%2520training%2520and%2520inference%2520stages.%2520To%250Aaddress%2520this%2520and%2520fully%2520harness%2520the%2520complementary%2520multimodal%2520features%252C%2520we%250Apropose%2520a%2520novel%2520multi-modality%2520co-learning%2520%2528MMCL%2529%2520framework%2520by%2520leveraging%2520the%250Amultimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520auxiliary%2520networks%2520for%2520efficient%250Askeleton-based%2520action%2520recognition%252C%2520which%2520engages%2520in%2520multi-modality%2520co-learning%250Aduring%2520the%2520training%2520stage%2520and%2520keeps%2520efficiency%2520by%2520employing%2520only%2520concise%250Askeletons%2520in%2520inference.%2520Our%2520MMCL%2520framework%2520primarily%2520consists%2520of%2520two%2520modules.%250AFirst%252C%2520the%2520Feature%2520Alignment%2520Module%2520%2528FAM%2529%2520extracts%2520rich%2520RGB%2520features%2520from%2520video%250Aframes%2520and%2520aligns%2520them%2520with%2520global%2520skeleton%2520features%2520via%2520contrastive%2520learning.%250ASecond%252C%2520the%2520Feature%2520Refinement%2520Module%2520%2528FRM%2529%2520uses%2520RGB%2520images%2520with%2520temporal%250Ainformation%2520and%2520text%2520instruction%2520to%2520generate%2520instructive%2520features%2520based%2520on%2520the%250Apowerful%2520generalization%2520of%2520multimodal%2520LLMs.%2520These%2520instructive%2520text%2520features%250Awill%2520further%2520refine%2520the%2520classification%2520scores%2520and%2520the%2520refined%2520scores%2520will%250Aenhance%2520the%2520model%2527s%2520robustness%2520and%2520generalization%2520in%2520a%2520manner%2520similar%2520to%2520soft%250Alabels.%2520Extensive%2520experiments%2520on%2520NTU%2520RGB%252BD%252C%2520NTU%2520RGB%252BD%2520120%2520and%2520Northwestern-UCLA%250Abenchmarks%2520consistently%2520verify%2520the%2520effectiveness%2520of%2520our%2520MMCL%252C%2520which%2520outperforms%250Athe%2520existing%2520skeleton-based%2520action%2520recognition%2520methods.%2520Meanwhile%252C%2520experiments%250Aon%2520UTD-MHAD%2520and%2520SYSU-Action%2520datasets%2520demonstrate%2520the%2520commendable%2520generalization%250Aof%2520our%2520MMCL%2520in%2520zero-shot%2520and%2520domain-adaptive%2520action%2520recognition.%2520Our%2520code%2520is%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/liujf69/MMCL-Action.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15706v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modality%20Co-Learning%20for%20Efficient%20Skeleton-based%20Action%0A%20%20Recognition&entry.906535625=Jinfu%20Liu%20and%20Chen%20Chen%20and%20Mengyuan%20Liu&entry.1292438233=%20%20Skeleton-based%20action%20recognition%20has%20garnered%20significant%20attention%20due%20to%0Athe%20utilization%20of%20concise%20and%20resilient%20skeletons.%20Nevertheless%2C%20the%20absence%0Aof%20detailed%20body%20information%20in%20skeletons%20restricts%20performance%2C%20while%20other%0Amultimodal%20methods%20require%20substantial%20inference%20resources%20and%20are%20inefficient%0Awhen%20using%20multimodal%20data%20during%20both%20training%20and%20inference%20stages.%20To%0Aaddress%20this%20and%20fully%20harness%20the%20complementary%20multimodal%20features%2C%20we%0Apropose%20a%20novel%20multi-modality%20co-learning%20%28MMCL%29%20framework%20by%20leveraging%20the%0Amultimodal%20large%20language%20models%20%28LLMs%29%20as%20auxiliary%20networks%20for%20efficient%0Askeleton-based%20action%20recognition%2C%20which%20engages%20in%20multi-modality%20co-learning%0Aduring%20the%20training%20stage%20and%20keeps%20efficiency%20by%20employing%20only%20concise%0Askeletons%20in%20inference.%20Our%20MMCL%20framework%20primarily%20consists%20of%20two%20modules.%0AFirst%2C%20the%20Feature%20Alignment%20Module%20%28FAM%29%20extracts%20rich%20RGB%20features%20from%20video%0Aframes%20and%20aligns%20them%20with%20global%20skeleton%20features%20via%20contrastive%20learning.%0ASecond%2C%20the%20Feature%20Refinement%20Module%20%28FRM%29%20uses%20RGB%20images%20with%20temporal%0Ainformation%20and%20text%20instruction%20to%20generate%20instructive%20features%20based%20on%20the%0Apowerful%20generalization%20of%20multimodal%20LLMs.%20These%20instructive%20text%20features%0Awill%20further%20refine%20the%20classification%20scores%20and%20the%20refined%20scores%20will%0Aenhance%20the%20model%27s%20robustness%20and%20generalization%20in%20a%20manner%20similar%20to%20soft%0Alabels.%20Extensive%20experiments%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%20and%20Northwestern-UCLA%0Abenchmarks%20consistently%20verify%20the%20effectiveness%20of%20our%20MMCL%2C%20which%20outperforms%0Athe%20existing%20skeleton-based%20action%20recognition%20methods.%20Meanwhile%2C%20experiments%0Aon%20UTD-MHAD%20and%20SYSU-Action%20datasets%20demonstrate%20the%20commendable%20generalization%0Aof%20our%20MMCL%20in%20zero-shot%20and%20domain-adaptive%20action%20recognition.%20Our%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/liujf69/MMCL-Action.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15706v6&entry.124074799=Read"},
{"title": "A Spitting Image: Modular Superpixel Tokenization in Vision Transformers", "author": "Marius Aasan and Odd Kolbj\u00f8rnsen and Anne Schistad Solberg and Ad\u00edn Ramirez Rivera", "abstract": "  Vision Transformer (ViT) architectures traditionally employ a grid-based\napproach to tokenization independent of the semantic content of an image. We\npropose a modular superpixel tokenization strategy which decouples tokenization\nand feature extraction; a shift from contemporary approaches where these are\ntreated as an undifferentiated whole. Using on-line content-aware tokenization\nand scale- and shape-invariant positional embeddings, we perform experiments\nand ablations that contrast our approach with patch-based tokenization and\nrandomized partitions as baselines. We show that our method significantly\nimproves the faithfulness of attributions, gives pixel-level granularity on\nzero-shot unsupervised dense prediction tasks, while maintaining predictive\nperformance in classification tasks. Our approach provides a modular\ntokenization framework commensurable with standard architectures, extending the\nspace of ViTs to a larger class of semantically-rich models.\n", "link": "http://arxiv.org/abs/2408.07680v2", "date": "2024-08-15", "relevancy": 2.3204, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6052}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.565}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Spitting%20Image%3A%20Modular%20Superpixel%20Tokenization%20in%20Vision%20Transformers&body=Title%3A%20A%20Spitting%20Image%3A%20Modular%20Superpixel%20Tokenization%20in%20Vision%20Transformers%0AAuthor%3A%20Marius%20Aasan%20and%20Odd%20Kolbj%C3%B8rnsen%20and%20Anne%20Schistad%20Solberg%20and%20Ad%C3%ADn%20Ramirez%20Rivera%0AAbstract%3A%20%20%20Vision%20Transformer%20%28ViT%29%20architectures%20traditionally%20employ%20a%20grid-based%0Aapproach%20to%20tokenization%20independent%20of%20the%20semantic%20content%20of%20an%20image.%20We%0Apropose%20a%20modular%20superpixel%20tokenization%20strategy%20which%20decouples%20tokenization%0Aand%20feature%20extraction%3B%20a%20shift%20from%20contemporary%20approaches%20where%20these%20are%0Atreated%20as%20an%20undifferentiated%20whole.%20Using%20on-line%20content-aware%20tokenization%0Aand%20scale-%20and%20shape-invariant%20positional%20embeddings%2C%20we%20perform%20experiments%0Aand%20ablations%20that%20contrast%20our%20approach%20with%20patch-based%20tokenization%20and%0Arandomized%20partitions%20as%20baselines.%20We%20show%20that%20our%20method%20significantly%0Aimproves%20the%20faithfulness%20of%20attributions%2C%20gives%20pixel-level%20granularity%20on%0Azero-shot%20unsupervised%20dense%20prediction%20tasks%2C%20while%20maintaining%20predictive%0Aperformance%20in%20classification%20tasks.%20Our%20approach%20provides%20a%20modular%0Atokenization%20framework%20commensurable%20with%20standard%20architectures%2C%20extending%20the%0Aspace%20of%20ViTs%20to%20a%20larger%20class%20of%20semantically-rich%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07680v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Spitting%2520Image%253A%2520Modular%2520Superpixel%2520Tokenization%2520in%2520Vision%2520Transformers%26entry.906535625%3DMarius%2520Aasan%2520and%2520Odd%2520Kolbj%25C3%25B8rnsen%2520and%2520Anne%2520Schistad%2520Solberg%2520and%2520Ad%25C3%25ADn%2520Ramirez%2520Rivera%26entry.1292438233%3D%2520%2520Vision%2520Transformer%2520%2528ViT%2529%2520architectures%2520traditionally%2520employ%2520a%2520grid-based%250Aapproach%2520to%2520tokenization%2520independent%2520of%2520the%2520semantic%2520content%2520of%2520an%2520image.%2520We%250Apropose%2520a%2520modular%2520superpixel%2520tokenization%2520strategy%2520which%2520decouples%2520tokenization%250Aand%2520feature%2520extraction%253B%2520a%2520shift%2520from%2520contemporary%2520approaches%2520where%2520these%2520are%250Atreated%2520as%2520an%2520undifferentiated%2520whole.%2520Using%2520on-line%2520content-aware%2520tokenization%250Aand%2520scale-%2520and%2520shape-invariant%2520positional%2520embeddings%252C%2520we%2520perform%2520experiments%250Aand%2520ablations%2520that%2520contrast%2520our%2520approach%2520with%2520patch-based%2520tokenization%2520and%250Arandomized%2520partitions%2520as%2520baselines.%2520We%2520show%2520that%2520our%2520method%2520significantly%250Aimproves%2520the%2520faithfulness%2520of%2520attributions%252C%2520gives%2520pixel-level%2520granularity%2520on%250Azero-shot%2520unsupervised%2520dense%2520prediction%2520tasks%252C%2520while%2520maintaining%2520predictive%250Aperformance%2520in%2520classification%2520tasks.%2520Our%2520approach%2520provides%2520a%2520modular%250Atokenization%2520framework%2520commensurable%2520with%2520standard%2520architectures%252C%2520extending%2520the%250Aspace%2520of%2520ViTs%2520to%2520a%2520larger%2520class%2520of%2520semantically-rich%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07680v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Spitting%20Image%3A%20Modular%20Superpixel%20Tokenization%20in%20Vision%20Transformers&entry.906535625=Marius%20Aasan%20and%20Odd%20Kolbj%C3%B8rnsen%20and%20Anne%20Schistad%20Solberg%20and%20Ad%C3%ADn%20Ramirez%20Rivera&entry.1292438233=%20%20Vision%20Transformer%20%28ViT%29%20architectures%20traditionally%20employ%20a%20grid-based%0Aapproach%20to%20tokenization%20independent%20of%20the%20semantic%20content%20of%20an%20image.%20We%0Apropose%20a%20modular%20superpixel%20tokenization%20strategy%20which%20decouples%20tokenization%0Aand%20feature%20extraction%3B%20a%20shift%20from%20contemporary%20approaches%20where%20these%20are%0Atreated%20as%20an%20undifferentiated%20whole.%20Using%20on-line%20content-aware%20tokenization%0Aand%20scale-%20and%20shape-invariant%20positional%20embeddings%2C%20we%20perform%20experiments%0Aand%20ablations%20that%20contrast%20our%20approach%20with%20patch-based%20tokenization%20and%0Arandomized%20partitions%20as%20baselines.%20We%20show%20that%20our%20method%20significantly%0Aimproves%20the%20faithfulness%20of%20attributions%2C%20gives%20pixel-level%20granularity%20on%0Azero-shot%20unsupervised%20dense%20prediction%20tasks%2C%20while%20maintaining%20predictive%0Aperformance%20in%20classification%20tasks.%20Our%20approach%20provides%20a%20modular%0Atokenization%20framework%20commensurable%20with%20standard%20architectures%2C%20extending%20the%0Aspace%20of%20ViTs%20to%20a%20larger%20class%20of%20semantically-rich%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07680v2&entry.124074799=Read"},
{"title": "Learned Multimodal Compression for Autonomous Driving", "author": "Hadi Hadizadeh and Ivan V. Baji\u0107", "abstract": "  Autonomous driving sensors generate an enormous amount of data. In this\npaper, we explore learned multimodal compression for autonomous driving,\nspecifically targeted at 3D object detection. We focus on camera and LiDAR\nmodalities and explore several coding approaches. One approach involves joint\ncoding of fused modalities, while others involve coding one modality first,\nfollowed by conditional coding of the other modality. We evaluate the\nperformance of these coding schemes on the nuScenes dataset. Our experimental\nresults indicate that joint coding of fused modalities yields better results\ncompared to the alternatives.\n", "link": "http://arxiv.org/abs/2408.08211v1", "date": "2024-08-15", "relevancy": 2.3123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6019}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5849}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Multimodal%20Compression%20for%20Autonomous%20Driving&body=Title%3A%20Learned%20Multimodal%20Compression%20for%20Autonomous%20Driving%0AAuthor%3A%20Hadi%20Hadizadeh%20and%20Ivan%20V.%20Baji%C4%87%0AAbstract%3A%20%20%20Autonomous%20driving%20sensors%20generate%20an%20enormous%20amount%20of%20data.%20In%20this%0Apaper%2C%20we%20explore%20learned%20multimodal%20compression%20for%20autonomous%20driving%2C%0Aspecifically%20targeted%20at%203D%20object%20detection.%20We%20focus%20on%20camera%20and%20LiDAR%0Amodalities%20and%20explore%20several%20coding%20approaches.%20One%20approach%20involves%20joint%0Acoding%20of%20fused%20modalities%2C%20while%20others%20involve%20coding%20one%20modality%20first%2C%0Afollowed%20by%20conditional%20coding%20of%20the%20other%20modality.%20We%20evaluate%20the%0Aperformance%20of%20these%20coding%20schemes%20on%20the%20nuScenes%20dataset.%20Our%20experimental%0Aresults%20indicate%20that%20joint%20coding%20of%20fused%20modalities%20yields%20better%20results%0Acompared%20to%20the%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Multimodal%2520Compression%2520for%2520Autonomous%2520Driving%26entry.906535625%3DHadi%2520Hadizadeh%2520and%2520Ivan%2520V.%2520Baji%25C4%2587%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520sensors%2520generate%2520an%2520enormous%2520amount%2520of%2520data.%2520In%2520this%250Apaper%252C%2520we%2520explore%2520learned%2520multimodal%2520compression%2520for%2520autonomous%2520driving%252C%250Aspecifically%2520targeted%2520at%25203D%2520object%2520detection.%2520We%2520focus%2520on%2520camera%2520and%2520LiDAR%250Amodalities%2520and%2520explore%2520several%2520coding%2520approaches.%2520One%2520approach%2520involves%2520joint%250Acoding%2520of%2520fused%2520modalities%252C%2520while%2520others%2520involve%2520coding%2520one%2520modality%2520first%252C%250Afollowed%2520by%2520conditional%2520coding%2520of%2520the%2520other%2520modality.%2520We%2520evaluate%2520the%250Aperformance%2520of%2520these%2520coding%2520schemes%2520on%2520the%2520nuScenes%2520dataset.%2520Our%2520experimental%250Aresults%2520indicate%2520that%2520joint%2520coding%2520of%2520fused%2520modalities%2520yields%2520better%2520results%250Acompared%2520to%2520the%2520alternatives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Multimodal%20Compression%20for%20Autonomous%20Driving&entry.906535625=Hadi%20Hadizadeh%20and%20Ivan%20V.%20Baji%C4%87&entry.1292438233=%20%20Autonomous%20driving%20sensors%20generate%20an%20enormous%20amount%20of%20data.%20In%20this%0Apaper%2C%20we%20explore%20learned%20multimodal%20compression%20for%20autonomous%20driving%2C%0Aspecifically%20targeted%20at%203D%20object%20detection.%20We%20focus%20on%20camera%20and%20LiDAR%0Amodalities%20and%20explore%20several%20coding%20approaches.%20One%20approach%20involves%20joint%0Acoding%20of%20fused%20modalities%2C%20while%20others%20involve%20coding%20one%20modality%20first%2C%0Afollowed%20by%20conditional%20coding%20of%20the%20other%20modality.%20We%20evaluate%20the%0Aperformance%20of%20these%20coding%20schemes%20on%20the%20nuScenes%20dataset.%20Our%20experimental%0Aresults%20indicate%20that%20joint%20coding%20of%20fused%20modalities%20yields%20better%20results%0Acompared%20to%20the%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08211v1&entry.124074799=Read"},
{"title": "OC3D: Weakly Supervised Outdoor 3D Object Detection with Only Coarse\n  Click Annotation", "author": "Qiming Xia and Hongwei Lin and Wei Ye and Hai Wu and Yadan Luo and Shijia Zhao and Xin Li and Chenglu Wen", "abstract": "  LiDAR-based outdoor 3D object detection has received widespread attention.\nHowever, training 3D detectors from the LiDAR point cloud typically relies on\nexpensive bounding box annotations. This paper presents OC3D, an innovative\nweakly supervised method requiring only coarse clicks on the bird' s eye view\nof the 3D point cloud. A key challenge here is the absence of complete\ngeometric descriptions of the target objects from such simple click\nannotations. To address this problem, our proposed OC3D adopts a two-stage\nstrategy. In the first stage, we initially design a novel dynamic and static\nclassification strategy and then propose the Click2Box and Click2Mask modules\nto generate box-level and mask-level pseudo-labels for static and dynamic\ninstances, respectively. In the second stage, we design a Mask2Box module,\nleveraging the learning capabilities of neural networks to update mask-level\npseudo-labels, which contain less information, to box level pseudo-labels.\nExperimental results on the widely used KITTI and nuScenes datasets demonstrate\nthat our OC3D with only coarse clicks achieves state-of-the-art performance\ncompared to weakly-supervised 3D detection methods. Combining OC3D with a\nmissing click mining strategy, we propose a OC3D++ pipeline, which requires\nonly 0.2% annotation cost in the KITTI dataset to achieve performance\ncomparable to fully supervised methods.\n", "link": "http://arxiv.org/abs/2408.08092v1", "date": "2024-08-15", "relevancy": 2.3049, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5994}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5881}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OC3D%3A%20Weakly%20Supervised%20Outdoor%203D%20Object%20Detection%20with%20Only%20Coarse%0A%20%20Click%20Annotation&body=Title%3A%20OC3D%3A%20Weakly%20Supervised%20Outdoor%203D%20Object%20Detection%20with%20Only%20Coarse%0A%20%20Click%20Annotation%0AAuthor%3A%20Qiming%20Xia%20and%20Hongwei%20Lin%20and%20Wei%20Ye%20and%20Hai%20Wu%20and%20Yadan%20Luo%20and%20Shijia%20Zhao%20and%20Xin%20Li%20and%20Chenglu%20Wen%0AAbstract%3A%20%20%20LiDAR-based%20outdoor%203D%20object%20detection%20has%20received%20widespread%20attention.%0AHowever%2C%20training%203D%20detectors%20from%20the%20LiDAR%20point%20cloud%20typically%20relies%20on%0Aexpensive%20bounding%20box%20annotations.%20This%20paper%20presents%20OC3D%2C%20an%20innovative%0Aweakly%20supervised%20method%20requiring%20only%20coarse%20clicks%20on%20the%20bird%27%20s%20eye%20view%0Aof%20the%203D%20point%20cloud.%20A%20key%20challenge%20here%20is%20the%20absence%20of%20complete%0Ageometric%20descriptions%20of%20the%20target%20objects%20from%20such%20simple%20click%0Aannotations.%20To%20address%20this%20problem%2C%20our%20proposed%20OC3D%20adopts%20a%20two-stage%0Astrategy.%20In%20the%20first%20stage%2C%20we%20initially%20design%20a%20novel%20dynamic%20and%20static%0Aclassification%20strategy%20and%20then%20propose%20the%20Click2Box%20and%20Click2Mask%20modules%0Ato%20generate%20box-level%20and%20mask-level%20pseudo-labels%20for%20static%20and%20dynamic%0Ainstances%2C%20respectively.%20In%20the%20second%20stage%2C%20we%20design%20a%20Mask2Box%20module%2C%0Aleveraging%20the%20learning%20capabilities%20of%20neural%20networks%20to%20update%20mask-level%0Apseudo-labels%2C%20which%20contain%20less%20information%2C%20to%20box%20level%20pseudo-labels.%0AExperimental%20results%20on%20the%20widely%20used%20KITTI%20and%20nuScenes%20datasets%20demonstrate%0Athat%20our%20OC3D%20with%20only%20coarse%20clicks%20achieves%20state-of-the-art%20performance%0Acompared%20to%20weakly-supervised%203D%20detection%20methods.%20Combining%20OC3D%20with%20a%0Amissing%20click%20mining%20strategy%2C%20we%20propose%20a%20OC3D%2B%2B%20pipeline%2C%20which%20requires%0Aonly%200.2%25%20annotation%20cost%20in%20the%20KITTI%20dataset%20to%20achieve%20performance%0Acomparable%20to%20fully%20supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOC3D%253A%2520Weakly%2520Supervised%2520Outdoor%25203D%2520Object%2520Detection%2520with%2520Only%2520Coarse%250A%2520%2520Click%2520Annotation%26entry.906535625%3DQiming%2520Xia%2520and%2520Hongwei%2520Lin%2520and%2520Wei%2520Ye%2520and%2520Hai%2520Wu%2520and%2520Yadan%2520Luo%2520and%2520Shijia%2520Zhao%2520and%2520Xin%2520Li%2520and%2520Chenglu%2520Wen%26entry.1292438233%3D%2520%2520LiDAR-based%2520outdoor%25203D%2520object%2520detection%2520has%2520received%2520widespread%2520attention.%250AHowever%252C%2520training%25203D%2520detectors%2520from%2520the%2520LiDAR%2520point%2520cloud%2520typically%2520relies%2520on%250Aexpensive%2520bounding%2520box%2520annotations.%2520This%2520paper%2520presents%2520OC3D%252C%2520an%2520innovative%250Aweakly%2520supervised%2520method%2520requiring%2520only%2520coarse%2520clicks%2520on%2520the%2520bird%2527%2520s%2520eye%2520view%250Aof%2520the%25203D%2520point%2520cloud.%2520A%2520key%2520challenge%2520here%2520is%2520the%2520absence%2520of%2520complete%250Ageometric%2520descriptions%2520of%2520the%2520target%2520objects%2520from%2520such%2520simple%2520click%250Aannotations.%2520To%2520address%2520this%2520problem%252C%2520our%2520proposed%2520OC3D%2520adopts%2520a%2520two-stage%250Astrategy.%2520In%2520the%2520first%2520stage%252C%2520we%2520initially%2520design%2520a%2520novel%2520dynamic%2520and%2520static%250Aclassification%2520strategy%2520and%2520then%2520propose%2520the%2520Click2Box%2520and%2520Click2Mask%2520modules%250Ato%2520generate%2520box-level%2520and%2520mask-level%2520pseudo-labels%2520for%2520static%2520and%2520dynamic%250Ainstances%252C%2520respectively.%2520In%2520the%2520second%2520stage%252C%2520we%2520design%2520a%2520Mask2Box%2520module%252C%250Aleveraging%2520the%2520learning%2520capabilities%2520of%2520neural%2520networks%2520to%2520update%2520mask-level%250Apseudo-labels%252C%2520which%2520contain%2520less%2520information%252C%2520to%2520box%2520level%2520pseudo-labels.%250AExperimental%2520results%2520on%2520the%2520widely%2520used%2520KITTI%2520and%2520nuScenes%2520datasets%2520demonstrate%250Athat%2520our%2520OC3D%2520with%2520only%2520coarse%2520clicks%2520achieves%2520state-of-the-art%2520performance%250Acompared%2520to%2520weakly-supervised%25203D%2520detection%2520methods.%2520Combining%2520OC3D%2520with%2520a%250Amissing%2520click%2520mining%2520strategy%252C%2520we%2520propose%2520a%2520OC3D%252B%252B%2520pipeline%252C%2520which%2520requires%250Aonly%25200.2%2525%2520annotation%2520cost%2520in%2520the%2520KITTI%2520dataset%2520to%2520achieve%2520performance%250Acomparable%2520to%2520fully%2520supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OC3D%3A%20Weakly%20Supervised%20Outdoor%203D%20Object%20Detection%20with%20Only%20Coarse%0A%20%20Click%20Annotation&entry.906535625=Qiming%20Xia%20and%20Hongwei%20Lin%20and%20Wei%20Ye%20and%20Hai%20Wu%20and%20Yadan%20Luo%20and%20Shijia%20Zhao%20and%20Xin%20Li%20and%20Chenglu%20Wen&entry.1292438233=%20%20LiDAR-based%20outdoor%203D%20object%20detection%20has%20received%20widespread%20attention.%0AHowever%2C%20training%203D%20detectors%20from%20the%20LiDAR%20point%20cloud%20typically%20relies%20on%0Aexpensive%20bounding%20box%20annotations.%20This%20paper%20presents%20OC3D%2C%20an%20innovative%0Aweakly%20supervised%20method%20requiring%20only%20coarse%20clicks%20on%20the%20bird%27%20s%20eye%20view%0Aof%20the%203D%20point%20cloud.%20A%20key%20challenge%20here%20is%20the%20absence%20of%20complete%0Ageometric%20descriptions%20of%20the%20target%20objects%20from%20such%20simple%20click%0Aannotations.%20To%20address%20this%20problem%2C%20our%20proposed%20OC3D%20adopts%20a%20two-stage%0Astrategy.%20In%20the%20first%20stage%2C%20we%20initially%20design%20a%20novel%20dynamic%20and%20static%0Aclassification%20strategy%20and%20then%20propose%20the%20Click2Box%20and%20Click2Mask%20modules%0Ato%20generate%20box-level%20and%20mask-level%20pseudo-labels%20for%20static%20and%20dynamic%0Ainstances%2C%20respectively.%20In%20the%20second%20stage%2C%20we%20design%20a%20Mask2Box%20module%2C%0Aleveraging%20the%20learning%20capabilities%20of%20neural%20networks%20to%20update%20mask-level%0Apseudo-labels%2C%20which%20contain%20less%20information%2C%20to%20box%20level%20pseudo-labels.%0AExperimental%20results%20on%20the%20widely%20used%20KITTI%20and%20nuScenes%20datasets%20demonstrate%0Athat%20our%20OC3D%20with%20only%20coarse%20clicks%20achieves%20state-of-the-art%20performance%0Acompared%20to%20weakly-supervised%203D%20detection%20methods.%20Combining%20OC3D%20with%20a%0Amissing%20click%20mining%20strategy%2C%20we%20propose%20a%20OC3D%2B%2B%20pipeline%2C%20which%20requires%0Aonly%200.2%25%20annotation%20cost%20in%20the%20KITTI%20dataset%20to%20achieve%20performance%0Acomparable%20to%20fully%20supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08092v1&entry.124074799=Read"},
{"title": "Dual-Camera Smooth Zoom on Mobile Phones", "author": "Renlong Wu and Zhilu Zhang and Yu Yang and Wangmeng Zuo", "abstract": "  When zooming between dual cameras on a mobile, noticeable jumps in geometric\ncontent and image color occur in the preview, inevitably affecting the user's\nzoom experience. In this work, we introduce a new task, ie, dual-camera smooth\nzoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI)\ntechnique is a potential solution but struggles with ground-truth collection.\nTo address the issue, we suggest a data factory solution where continuous\nvirtual cameras are assembled to generate DCSZ data by rendering reconstructed\n3D models of the scene. In particular, we propose a novel dual-camera smooth\nzoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is\nintroduced to construct a specific 3D model for each virtual camera. With the\nproposed data factory, we construct a synthetic dataset for DCSZ, and we\nutilize it to fine-tune FI models. In addition, we collect real-world dual-zoom\nimages without ground-truth for evaluation. Extensive experiments are conducted\nwith multiple FI methods. The results show that the fine-tuned FI models\nachieve a significant performance improvement over the original ones on DCSZ\ntask. The datasets, codes, and pre-trained models will are available at\nhttps://github.com/ZcsrenlongZ/ZoomGS.\n", "link": "http://arxiv.org/abs/2404.04908v2", "date": "2024-08-15", "relevancy": 2.29, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5841}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5711}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Camera%20Smooth%20Zoom%20on%20Mobile%20Phones&body=Title%3A%20Dual-Camera%20Smooth%20Zoom%20on%20Mobile%20Phones%0AAuthor%3A%20Renlong%20Wu%20and%20Zhilu%20Zhang%20and%20Yu%20Yang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20When%20zooming%20between%20dual%20cameras%20on%20a%20mobile%2C%20noticeable%20jumps%20in%20geometric%0Acontent%20and%20image%20color%20occur%20in%20the%20preview%2C%20inevitably%20affecting%20the%20user%27s%0Azoom%20experience.%20In%20this%20work%2C%20we%20introduce%20a%20new%20task%2C%20ie%2C%20dual-camera%20smooth%0Azoom%20%28DCSZ%29%20to%20achieve%20a%20smooth%20zoom%20preview.%20The%20frame%20interpolation%20%28FI%29%0Atechnique%20is%20a%20potential%20solution%20but%20struggles%20with%20ground-truth%20collection.%0ATo%20address%20the%20issue%2C%20we%20suggest%20a%20data%20factory%20solution%20where%20continuous%0Avirtual%20cameras%20are%20assembled%20to%20generate%20DCSZ%20data%20by%20rendering%20reconstructed%0A3D%20models%20of%20the%20scene.%20In%20particular%2C%20we%20propose%20a%20novel%20dual-camera%20smooth%0Azoom%20Gaussian%20Splatting%20%28ZoomGS%29%2C%20where%20a%20camera-specific%20encoding%20is%0Aintroduced%20to%20construct%20a%20specific%203D%20model%20for%20each%20virtual%20camera.%20With%20the%0Aproposed%20data%20factory%2C%20we%20construct%20a%20synthetic%20dataset%20for%20DCSZ%2C%20and%20we%0Autilize%20it%20to%20fine-tune%20FI%20models.%20In%20addition%2C%20we%20collect%20real-world%20dual-zoom%0Aimages%20without%20ground-truth%20for%20evaluation.%20Extensive%20experiments%20are%20conducted%0Awith%20multiple%20FI%20methods.%20The%20results%20show%20that%20the%20fine-tuned%20FI%20models%0Aachieve%20a%20significant%20performance%20improvement%20over%20the%20original%20ones%20on%20DCSZ%0Atask.%20The%20datasets%2C%20codes%2C%20and%20pre-trained%20models%20will%20are%20available%20at%0Ahttps%3A//github.com/ZcsrenlongZ/ZoomGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04908v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Camera%2520Smooth%2520Zoom%2520on%2520Mobile%2520Phones%26entry.906535625%3DRenlong%2520Wu%2520and%2520Zhilu%2520Zhang%2520and%2520Yu%2520Yang%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520When%2520zooming%2520between%2520dual%2520cameras%2520on%2520a%2520mobile%252C%2520noticeable%2520jumps%2520in%2520geometric%250Acontent%2520and%2520image%2520color%2520occur%2520in%2520the%2520preview%252C%2520inevitably%2520affecting%2520the%2520user%2527s%250Azoom%2520experience.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520task%252C%2520ie%252C%2520dual-camera%2520smooth%250Azoom%2520%2528DCSZ%2529%2520to%2520achieve%2520a%2520smooth%2520zoom%2520preview.%2520The%2520frame%2520interpolation%2520%2528FI%2529%250Atechnique%2520is%2520a%2520potential%2520solution%2520but%2520struggles%2520with%2520ground-truth%2520collection.%250ATo%2520address%2520the%2520issue%252C%2520we%2520suggest%2520a%2520data%2520factory%2520solution%2520where%2520continuous%250Avirtual%2520cameras%2520are%2520assembled%2520to%2520generate%2520DCSZ%2520data%2520by%2520rendering%2520reconstructed%250A3D%2520models%2520of%2520the%2520scene.%2520In%2520particular%252C%2520we%2520propose%2520a%2520novel%2520dual-camera%2520smooth%250Azoom%2520Gaussian%2520Splatting%2520%2528ZoomGS%2529%252C%2520where%2520a%2520camera-specific%2520encoding%2520is%250Aintroduced%2520to%2520construct%2520a%2520specific%25203D%2520model%2520for%2520each%2520virtual%2520camera.%2520With%2520the%250Aproposed%2520data%2520factory%252C%2520we%2520construct%2520a%2520synthetic%2520dataset%2520for%2520DCSZ%252C%2520and%2520we%250Autilize%2520it%2520to%2520fine-tune%2520FI%2520models.%2520In%2520addition%252C%2520we%2520collect%2520real-world%2520dual-zoom%250Aimages%2520without%2520ground-truth%2520for%2520evaluation.%2520Extensive%2520experiments%2520are%2520conducted%250Awith%2520multiple%2520FI%2520methods.%2520The%2520results%2520show%2520that%2520the%2520fine-tuned%2520FI%2520models%250Aachieve%2520a%2520significant%2520performance%2520improvement%2520over%2520the%2520original%2520ones%2520on%2520DCSZ%250Atask.%2520The%2520datasets%252C%2520codes%252C%2520and%2520pre-trained%2520models%2520will%2520are%2520available%2520at%250Ahttps%253A//github.com/ZcsrenlongZ/ZoomGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04908v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Camera%20Smooth%20Zoom%20on%20Mobile%20Phones&entry.906535625=Renlong%20Wu%20and%20Zhilu%20Zhang%20and%20Yu%20Yang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20When%20zooming%20between%20dual%20cameras%20on%20a%20mobile%2C%20noticeable%20jumps%20in%20geometric%0Acontent%20and%20image%20color%20occur%20in%20the%20preview%2C%20inevitably%20affecting%20the%20user%27s%0Azoom%20experience.%20In%20this%20work%2C%20we%20introduce%20a%20new%20task%2C%20ie%2C%20dual-camera%20smooth%0Azoom%20%28DCSZ%29%20to%20achieve%20a%20smooth%20zoom%20preview.%20The%20frame%20interpolation%20%28FI%29%0Atechnique%20is%20a%20potential%20solution%20but%20struggles%20with%20ground-truth%20collection.%0ATo%20address%20the%20issue%2C%20we%20suggest%20a%20data%20factory%20solution%20where%20continuous%0Avirtual%20cameras%20are%20assembled%20to%20generate%20DCSZ%20data%20by%20rendering%20reconstructed%0A3D%20models%20of%20the%20scene.%20In%20particular%2C%20we%20propose%20a%20novel%20dual-camera%20smooth%0Azoom%20Gaussian%20Splatting%20%28ZoomGS%29%2C%20where%20a%20camera-specific%20encoding%20is%0Aintroduced%20to%20construct%20a%20specific%203D%20model%20for%20each%20virtual%20camera.%20With%20the%0Aproposed%20data%20factory%2C%20we%20construct%20a%20synthetic%20dataset%20for%20DCSZ%2C%20and%20we%0Autilize%20it%20to%20fine-tune%20FI%20models.%20In%20addition%2C%20we%20collect%20real-world%20dual-zoom%0Aimages%20without%20ground-truth%20for%20evaluation.%20Extensive%20experiments%20are%20conducted%0Awith%20multiple%20FI%20methods.%20The%20results%20show%20that%20the%20fine-tuned%20FI%20models%0Aachieve%20a%20significant%20performance%20improvement%20over%20the%20original%20ones%20on%20DCSZ%0Atask.%20The%20datasets%2C%20codes%2C%20and%20pre-trained%20models%20will%20are%20available%20at%0Ahttps%3A//github.com/ZcsrenlongZ/ZoomGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04908v2&entry.124074799=Read"},
{"title": "Accurate and efficient structure elucidation from routine\n  one-dimensional NMR spectra using multitask machine learning", "author": "Frank Hu and Michael S. Chen and Grant M. Rotskoff and Matthew W. Kanan and Thomas E. Markland", "abstract": "  Rapid determination of molecular structures can greatly accelerate workflows\nacross many chemical disciplines. However, elucidating structure using only\none-dimensional (1D) NMR spectra, the most readily accessible data, remains an\nextremely challenging problem because of the combinatorial explosion of the\nnumber of possible molecules as the number of constituent atoms is increased.\nHere, we introduce a multitask machine learning framework that predicts the\nmolecular structure (formula and connectivity) of an unknown compound solely\nbased on its 1D 1H and/or 13C NMR spectra. First, we show how a transformer\narchitecture can be constructed to efficiently solve the task, traditionally\nperformed by chemists, of assembling large numbers of molecular fragments into\nmolecular structures. Integrating this capability with a convolutional neural\nnetwork (CNN), we build an end-to-end model for predicting structure from\nspectra that is fast and accurate. We demonstrate the effectiveness of this\nframework on molecules with up to 19 heavy (non-hydrogen) atoms, a size for\nwhich there are trillions of possible structures. Without relying on any prior\nchemical knowledge such as the molecular formula, we show that our approach\npredicts the exact molecule 69.6% of the time within the first 15 predictions,\nreducing the search space by up to 11 orders of magnitude.\n", "link": "http://arxiv.org/abs/2408.08284v1", "date": "2024-08-15", "relevancy": 2.2788, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4673}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.45}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accurate%20and%20efficient%20structure%20elucidation%20from%20routine%0A%20%20one-dimensional%20NMR%20spectra%20using%20multitask%20machine%20learning&body=Title%3A%20Accurate%20and%20efficient%20structure%20elucidation%20from%20routine%0A%20%20one-dimensional%20NMR%20spectra%20using%20multitask%20machine%20learning%0AAuthor%3A%20Frank%20Hu%20and%20Michael%20S.%20Chen%20and%20Grant%20M.%20Rotskoff%20and%20Matthew%20W.%20Kanan%20and%20Thomas%20E.%20Markland%0AAbstract%3A%20%20%20Rapid%20determination%20of%20molecular%20structures%20can%20greatly%20accelerate%20workflows%0Aacross%20many%20chemical%20disciplines.%20However%2C%20elucidating%20structure%20using%20only%0Aone-dimensional%20%281D%29%20NMR%20spectra%2C%20the%20most%20readily%20accessible%20data%2C%20remains%20an%0Aextremely%20challenging%20problem%20because%20of%20the%20combinatorial%20explosion%20of%20the%0Anumber%20of%20possible%20molecules%20as%20the%20number%20of%20constituent%20atoms%20is%20increased.%0AHere%2C%20we%20introduce%20a%20multitask%20machine%20learning%20framework%20that%20predicts%20the%0Amolecular%20structure%20%28formula%20and%20connectivity%29%20of%20an%20unknown%20compound%20solely%0Abased%20on%20its%201D%201H%20and/or%2013C%20NMR%20spectra.%20First%2C%20we%20show%20how%20a%20transformer%0Aarchitecture%20can%20be%20constructed%20to%20efficiently%20solve%20the%20task%2C%20traditionally%0Aperformed%20by%20chemists%2C%20of%20assembling%20large%20numbers%20of%20molecular%20fragments%20into%0Amolecular%20structures.%20Integrating%20this%20capability%20with%20a%20convolutional%20neural%0Anetwork%20%28CNN%29%2C%20we%20build%20an%20end-to-end%20model%20for%20predicting%20structure%20from%0Aspectra%20that%20is%20fast%20and%20accurate.%20We%20demonstrate%20the%20effectiveness%20of%20this%0Aframework%20on%20molecules%20with%20up%20to%2019%20heavy%20%28non-hydrogen%29%20atoms%2C%20a%20size%20for%0Awhich%20there%20are%20trillions%20of%20possible%20structures.%20Without%20relying%20on%20any%20prior%0Achemical%20knowledge%20such%20as%20the%20molecular%20formula%2C%20we%20show%20that%20our%20approach%0Apredicts%20the%20exact%20molecule%2069.6%25%20of%20the%20time%20within%20the%20first%2015%20predictions%2C%0Areducing%20the%20search%20space%20by%20up%20to%2011%20orders%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccurate%2520and%2520efficient%2520structure%2520elucidation%2520from%2520routine%250A%2520%2520one-dimensional%2520NMR%2520spectra%2520using%2520multitask%2520machine%2520learning%26entry.906535625%3DFrank%2520Hu%2520and%2520Michael%2520S.%2520Chen%2520and%2520Grant%2520M.%2520Rotskoff%2520and%2520Matthew%2520W.%2520Kanan%2520and%2520Thomas%2520E.%2520Markland%26entry.1292438233%3D%2520%2520Rapid%2520determination%2520of%2520molecular%2520structures%2520can%2520greatly%2520accelerate%2520workflows%250Aacross%2520many%2520chemical%2520disciplines.%2520However%252C%2520elucidating%2520structure%2520using%2520only%250Aone-dimensional%2520%25281D%2529%2520NMR%2520spectra%252C%2520the%2520most%2520readily%2520accessible%2520data%252C%2520remains%2520an%250Aextremely%2520challenging%2520problem%2520because%2520of%2520the%2520combinatorial%2520explosion%2520of%2520the%250Anumber%2520of%2520possible%2520molecules%2520as%2520the%2520number%2520of%2520constituent%2520atoms%2520is%2520increased.%250AHere%252C%2520we%2520introduce%2520a%2520multitask%2520machine%2520learning%2520framework%2520that%2520predicts%2520the%250Amolecular%2520structure%2520%2528formula%2520and%2520connectivity%2529%2520of%2520an%2520unknown%2520compound%2520solely%250Abased%2520on%2520its%25201D%25201H%2520and/or%252013C%2520NMR%2520spectra.%2520First%252C%2520we%2520show%2520how%2520a%2520transformer%250Aarchitecture%2520can%2520be%2520constructed%2520to%2520efficiently%2520solve%2520the%2520task%252C%2520traditionally%250Aperformed%2520by%2520chemists%252C%2520of%2520assembling%2520large%2520numbers%2520of%2520molecular%2520fragments%2520into%250Amolecular%2520structures.%2520Integrating%2520this%2520capability%2520with%2520a%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529%252C%2520we%2520build%2520an%2520end-to-end%2520model%2520for%2520predicting%2520structure%2520from%250Aspectra%2520that%2520is%2520fast%2520and%2520accurate.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%250Aframework%2520on%2520molecules%2520with%2520up%2520to%252019%2520heavy%2520%2528non-hydrogen%2529%2520atoms%252C%2520a%2520size%2520for%250Awhich%2520there%2520are%2520trillions%2520of%2520possible%2520structures.%2520Without%2520relying%2520on%2520any%2520prior%250Achemical%2520knowledge%2520such%2520as%2520the%2520molecular%2520formula%252C%2520we%2520show%2520that%2520our%2520approach%250Apredicts%2520the%2520exact%2520molecule%252069.6%2525%2520of%2520the%2520time%2520within%2520the%2520first%252015%2520predictions%252C%250Areducing%2520the%2520search%2520space%2520by%2520up%2520to%252011%2520orders%2520of%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accurate%20and%20efficient%20structure%20elucidation%20from%20routine%0A%20%20one-dimensional%20NMR%20spectra%20using%20multitask%20machine%20learning&entry.906535625=Frank%20Hu%20and%20Michael%20S.%20Chen%20and%20Grant%20M.%20Rotskoff%20and%20Matthew%20W.%20Kanan%20and%20Thomas%20E.%20Markland&entry.1292438233=%20%20Rapid%20determination%20of%20molecular%20structures%20can%20greatly%20accelerate%20workflows%0Aacross%20many%20chemical%20disciplines.%20However%2C%20elucidating%20structure%20using%20only%0Aone-dimensional%20%281D%29%20NMR%20spectra%2C%20the%20most%20readily%20accessible%20data%2C%20remains%20an%0Aextremely%20challenging%20problem%20because%20of%20the%20combinatorial%20explosion%20of%20the%0Anumber%20of%20possible%20molecules%20as%20the%20number%20of%20constituent%20atoms%20is%20increased.%0AHere%2C%20we%20introduce%20a%20multitask%20machine%20learning%20framework%20that%20predicts%20the%0Amolecular%20structure%20%28formula%20and%20connectivity%29%20of%20an%20unknown%20compound%20solely%0Abased%20on%20its%201D%201H%20and/or%2013C%20NMR%20spectra.%20First%2C%20we%20show%20how%20a%20transformer%0Aarchitecture%20can%20be%20constructed%20to%20efficiently%20solve%20the%20task%2C%20traditionally%0Aperformed%20by%20chemists%2C%20of%20assembling%20large%20numbers%20of%20molecular%20fragments%20into%0Amolecular%20structures.%20Integrating%20this%20capability%20with%20a%20convolutional%20neural%0Anetwork%20%28CNN%29%2C%20we%20build%20an%20end-to-end%20model%20for%20predicting%20structure%20from%0Aspectra%20that%20is%20fast%20and%20accurate.%20We%20demonstrate%20the%20effectiveness%20of%20this%0Aframework%20on%20molecules%20with%20up%20to%2019%20heavy%20%28non-hydrogen%29%20atoms%2C%20a%20size%20for%0Awhich%20there%20are%20trillions%20of%20possible%20structures.%20Without%20relying%20on%20any%20prior%0Achemical%20knowledge%20such%20as%20the%20molecular%20formula%2C%20we%20show%20that%20our%20approach%0Apredicts%20the%20exact%20molecule%2069.6%25%20of%20the%20time%20within%20the%20first%2015%20predictions%2C%0Areducing%20the%20search%20space%20by%20up%20to%2011%20orders%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08284v1&entry.124074799=Read"},
{"title": "When Video Coding Meets Multimodal Large Language Models: A Unified\n  Paradigm for Video Coding", "author": "Pingping Zhang and Jinlong Li and Meng Wang and Nicu Sebe and Sam Kwong and Shiqi Wang", "abstract": "  Existing codecs are designed to eliminate intrinsic redundancies to create a\ncompact representation for compression. However, strong external priors from\nMultimodal Large Language Models (MLLMs) have not been explicitly explored in\nvideo compression. Herein, we introduce a unified paradigm for Cross-Modality\nVideo Coding (CMVC), which is a pioneering approach to explore multimodality\nrepresentation and video generative models in video coding. Specifically, on\nthe encoder side, we disentangle a video into spatial content and motion\ncomponents, which are subsequently transformed into distinct modalities to\nachieve very compact representation by leveraging MLLMs. During decoding,\npreviously encoded components and video generation models are leveraged to\ncreate multiple encoding-decoding modes that optimize video reconstruction\nquality for specific decoding requirements, including Text-Text-to-Video (TT2V)\nmode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)\nmode to achieve superb perceptual consistency. In addition, we propose an\nefficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)\ntuning to guarantee perceptual quality, which allows the generated motion cues\nto behave smoothly. Experiments on benchmarks indicate that TT2V achieves\neffective semantic reconstruction, while IT2V exhibits competitive perceptual\nconsistency. These results highlight potential directions for future research\nin video coding.\n", "link": "http://arxiv.org/abs/2408.08093v1", "date": "2024-08-15", "relevancy": 2.2733, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5826}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Video%20Coding%20Meets%20Multimodal%20Large%20Language%20Models%3A%20A%20Unified%0A%20%20Paradigm%20for%20Video%20Coding&body=Title%3A%20When%20Video%20Coding%20Meets%20Multimodal%20Large%20Language%20Models%3A%20A%20Unified%0A%20%20Paradigm%20for%20Video%20Coding%0AAuthor%3A%20Pingping%20Zhang%20and%20Jinlong%20Li%20and%20Meng%20Wang%20and%20Nicu%20Sebe%20and%20Sam%20Kwong%20and%20Shiqi%20Wang%0AAbstract%3A%20%20%20Existing%20codecs%20are%20designed%20to%20eliminate%20intrinsic%20redundancies%20to%20create%20a%0Acompact%20representation%20for%20compression.%20However%2C%20strong%20external%20priors%20from%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20not%20been%20explicitly%20explored%20in%0Avideo%20compression.%20Herein%2C%20we%20introduce%20a%20unified%20paradigm%20for%20Cross-Modality%0AVideo%20Coding%20%28CMVC%29%2C%20which%20is%20a%20pioneering%20approach%20to%20explore%20multimodality%0Arepresentation%20and%20video%20generative%20models%20in%20video%20coding.%20Specifically%2C%20on%0Athe%20encoder%20side%2C%20we%20disentangle%20a%20video%20into%20spatial%20content%20and%20motion%0Acomponents%2C%20which%20are%20subsequently%20transformed%20into%20distinct%20modalities%20to%0Aachieve%20very%20compact%20representation%20by%20leveraging%20MLLMs.%20During%20decoding%2C%0Apreviously%20encoded%20components%20and%20video%20generation%20models%20are%20leveraged%20to%0Acreate%20multiple%20encoding-decoding%20modes%20that%20optimize%20video%20reconstruction%0Aquality%20for%20specific%20decoding%20requirements%2C%20including%20Text-Text-to-Video%20%28TT2V%29%0Amode%20to%20ensure%20high-quality%20semantic%20information%20and%20Image-Text-to-Video%20%28IT2V%29%0Amode%20to%20achieve%20superb%20perceptual%20consistency.%20In%20addition%2C%20we%20propose%20an%0Aefficient%20frame%20interpolation%20model%20for%20IT2V%20mode%20via%20Low-Rank%20Adaption%20%28LoRA%29%0Atuning%20to%20guarantee%20perceptual%20quality%2C%20which%20allows%20the%20generated%20motion%20cues%0Ato%20behave%20smoothly.%20Experiments%20on%20benchmarks%20indicate%20that%20TT2V%20achieves%0Aeffective%20semantic%20reconstruction%2C%20while%20IT2V%20exhibits%20competitive%20perceptual%0Aconsistency.%20These%20results%20highlight%20potential%20directions%20for%20future%20research%0Ain%20video%20coding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Video%2520Coding%2520Meets%2520Multimodal%2520Large%2520Language%2520Models%253A%2520A%2520Unified%250A%2520%2520Paradigm%2520for%2520Video%2520Coding%26entry.906535625%3DPingping%2520Zhang%2520and%2520Jinlong%2520Li%2520and%2520Meng%2520Wang%2520and%2520Nicu%2520Sebe%2520and%2520Sam%2520Kwong%2520and%2520Shiqi%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520codecs%2520are%2520designed%2520to%2520eliminate%2520intrinsic%2520redundancies%2520to%2520create%2520a%250Acompact%2520representation%2520for%2520compression.%2520However%252C%2520strong%2520external%2520priors%2520from%250AMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520not%2520been%2520explicitly%2520explored%2520in%250Avideo%2520compression.%2520Herein%252C%2520we%2520introduce%2520a%2520unified%2520paradigm%2520for%2520Cross-Modality%250AVideo%2520Coding%2520%2528CMVC%2529%252C%2520which%2520is%2520a%2520pioneering%2520approach%2520to%2520explore%2520multimodality%250Arepresentation%2520and%2520video%2520generative%2520models%2520in%2520video%2520coding.%2520Specifically%252C%2520on%250Athe%2520encoder%2520side%252C%2520we%2520disentangle%2520a%2520video%2520into%2520spatial%2520content%2520and%2520motion%250Acomponents%252C%2520which%2520are%2520subsequently%2520transformed%2520into%2520distinct%2520modalities%2520to%250Aachieve%2520very%2520compact%2520representation%2520by%2520leveraging%2520MLLMs.%2520During%2520decoding%252C%250Apreviously%2520encoded%2520components%2520and%2520video%2520generation%2520models%2520are%2520leveraged%2520to%250Acreate%2520multiple%2520encoding-decoding%2520modes%2520that%2520optimize%2520video%2520reconstruction%250Aquality%2520for%2520specific%2520decoding%2520requirements%252C%2520including%2520Text-Text-to-Video%2520%2528TT2V%2529%250Amode%2520to%2520ensure%2520high-quality%2520semantic%2520information%2520and%2520Image-Text-to-Video%2520%2528IT2V%2529%250Amode%2520to%2520achieve%2520superb%2520perceptual%2520consistency.%2520In%2520addition%252C%2520we%2520propose%2520an%250Aefficient%2520frame%2520interpolation%2520model%2520for%2520IT2V%2520mode%2520via%2520Low-Rank%2520Adaption%2520%2528LoRA%2529%250Atuning%2520to%2520guarantee%2520perceptual%2520quality%252C%2520which%2520allows%2520the%2520generated%2520motion%2520cues%250Ato%2520behave%2520smoothly.%2520Experiments%2520on%2520benchmarks%2520indicate%2520that%2520TT2V%2520achieves%250Aeffective%2520semantic%2520reconstruction%252C%2520while%2520IT2V%2520exhibits%2520competitive%2520perceptual%250Aconsistency.%2520These%2520results%2520highlight%2520potential%2520directions%2520for%2520future%2520research%250Ain%2520video%2520coding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Video%20Coding%20Meets%20Multimodal%20Large%20Language%20Models%3A%20A%20Unified%0A%20%20Paradigm%20for%20Video%20Coding&entry.906535625=Pingping%20Zhang%20and%20Jinlong%20Li%20and%20Meng%20Wang%20and%20Nicu%20Sebe%20and%20Sam%20Kwong%20and%20Shiqi%20Wang&entry.1292438233=%20%20Existing%20codecs%20are%20designed%20to%20eliminate%20intrinsic%20redundancies%20to%20create%20a%0Acompact%20representation%20for%20compression.%20However%2C%20strong%20external%20priors%20from%0AMultimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20not%20been%20explicitly%20explored%20in%0Avideo%20compression.%20Herein%2C%20we%20introduce%20a%20unified%20paradigm%20for%20Cross-Modality%0AVideo%20Coding%20%28CMVC%29%2C%20which%20is%20a%20pioneering%20approach%20to%20explore%20multimodality%0Arepresentation%20and%20video%20generative%20models%20in%20video%20coding.%20Specifically%2C%20on%0Athe%20encoder%20side%2C%20we%20disentangle%20a%20video%20into%20spatial%20content%20and%20motion%0Acomponents%2C%20which%20are%20subsequently%20transformed%20into%20distinct%20modalities%20to%0Aachieve%20very%20compact%20representation%20by%20leveraging%20MLLMs.%20During%20decoding%2C%0Apreviously%20encoded%20components%20and%20video%20generation%20models%20are%20leveraged%20to%0Acreate%20multiple%20encoding-decoding%20modes%20that%20optimize%20video%20reconstruction%0Aquality%20for%20specific%20decoding%20requirements%2C%20including%20Text-Text-to-Video%20%28TT2V%29%0Amode%20to%20ensure%20high-quality%20semantic%20information%20and%20Image-Text-to-Video%20%28IT2V%29%0Amode%20to%20achieve%20superb%20perceptual%20consistency.%20In%20addition%2C%20we%20propose%20an%0Aefficient%20frame%20interpolation%20model%20for%20IT2V%20mode%20via%20Low-Rank%20Adaption%20%28LoRA%29%0Atuning%20to%20guarantee%20perceptual%20quality%2C%20which%20allows%20the%20generated%20motion%20cues%0Ato%20behave%20smoothly.%20Experiments%20on%20benchmarks%20indicate%20that%20TT2V%20achieves%0Aeffective%20semantic%20reconstruction%2C%20while%20IT2V%20exhibits%20competitive%20perceptual%0Aconsistency.%20These%20results%20highlight%20potential%20directions%20for%20future%20research%0Ain%20video%20coding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08093v1&entry.124074799=Read"},
{"title": "Unsupervised Variational Translator for Bridging Image Restoration and\n  High-Level Vision Tasks", "author": "Jiawei Wu and Zhi Jin", "abstract": "  Recent research tries to extend image restoration capabilities from human\nperception to machine perception, thereby enhancing the performance of\nhigh-level vision tasks in degraded environments. These methods, primarily\nbased on supervised learning, typically involve the retraining of restoration\nnetworks or high-level vision networks. However, collecting paired data in\nreal-world scenarios and retraining large-scale models are challenge. To this\nend, we propose an unsupervised learning method called \\textbf{Va}riational\n\\textbf{T}ranslator (VaT), which does not require retraining existing\nrestoration and high-level vision networks. Instead, it establishes a\nlightweight network that serves as an intermediate bridge between them. By\nvariational inference, VaT approximates the joint distribution of restoration\noutput and high-level vision input, dividing the optimization objective into\npreserving content and maximizing marginal likelihood associated with\nhigh-level vision tasks. By cleverly leveraging self-training paradigms, VaT\nachieves the above optimization objective without requiring labels. As a\nresult, the translated images maintain a close resemblance to their original\ncontent while also demonstrating exceptional performance on high-level vision\ntasks. Extensive experiments in dehazing and low-light enhancement for\ndetection and classification show the superiority of our method over other\nstate-of-the-art unsupervised counterparts, even significantly surpassing\nsupervised methods in some complex real-world scenarios.\n", "link": "http://arxiv.org/abs/2408.08149v1", "date": "2024-08-15", "relevancy": 2.2635, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5715}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.569}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Variational%20Translator%20for%20Bridging%20Image%20Restoration%20and%0A%20%20High-Level%20Vision%20Tasks&body=Title%3A%20Unsupervised%20Variational%20Translator%20for%20Bridging%20Image%20Restoration%20and%0A%20%20High-Level%20Vision%20Tasks%0AAuthor%3A%20Jiawei%20Wu%20and%20Zhi%20Jin%0AAbstract%3A%20%20%20Recent%20research%20tries%20to%20extend%20image%20restoration%20capabilities%20from%20human%0Aperception%20to%20machine%20perception%2C%20thereby%20enhancing%20the%20performance%20of%0Ahigh-level%20vision%20tasks%20in%20degraded%20environments.%20These%20methods%2C%20primarily%0Abased%20on%20supervised%20learning%2C%20typically%20involve%20the%20retraining%20of%20restoration%0Anetworks%20or%20high-level%20vision%20networks.%20However%2C%20collecting%20paired%20data%20in%0Areal-world%20scenarios%20and%20retraining%20large-scale%20models%20are%20challenge.%20To%20this%0Aend%2C%20we%20propose%20an%20unsupervised%20learning%20method%20called%20%5Ctextbf%7BVa%7Driational%0A%5Ctextbf%7BT%7Dranslator%20%28VaT%29%2C%20which%20does%20not%20require%20retraining%20existing%0Arestoration%20and%20high-level%20vision%20networks.%20Instead%2C%20it%20establishes%20a%0Alightweight%20network%20that%20serves%20as%20an%20intermediate%20bridge%20between%20them.%20By%0Avariational%20inference%2C%20VaT%20approximates%20the%20joint%20distribution%20of%20restoration%0Aoutput%20and%20high-level%20vision%20input%2C%20dividing%20the%20optimization%20objective%20into%0Apreserving%20content%20and%20maximizing%20marginal%20likelihood%20associated%20with%0Ahigh-level%20vision%20tasks.%20By%20cleverly%20leveraging%20self-training%20paradigms%2C%20VaT%0Aachieves%20the%20above%20optimization%20objective%20without%20requiring%20labels.%20As%20a%0Aresult%2C%20the%20translated%20images%20maintain%20a%20close%20resemblance%20to%20their%20original%0Acontent%20while%20also%20demonstrating%20exceptional%20performance%20on%20high-level%20vision%0Atasks.%20Extensive%20experiments%20in%20dehazing%20and%20low-light%20enhancement%20for%0Adetection%20and%20classification%20show%20the%20superiority%20of%20our%20method%20over%20other%0Astate-of-the-art%20unsupervised%20counterparts%2C%20even%20significantly%20surpassing%0Asupervised%20methods%20in%20some%20complex%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Variational%2520Translator%2520for%2520Bridging%2520Image%2520Restoration%2520and%250A%2520%2520High-Level%2520Vision%2520Tasks%26entry.906535625%3DJiawei%2520Wu%2520and%2520Zhi%2520Jin%26entry.1292438233%3D%2520%2520Recent%2520research%2520tries%2520to%2520extend%2520image%2520restoration%2520capabilities%2520from%2520human%250Aperception%2520to%2520machine%2520perception%252C%2520thereby%2520enhancing%2520the%2520performance%2520of%250Ahigh-level%2520vision%2520tasks%2520in%2520degraded%2520environments.%2520These%2520methods%252C%2520primarily%250Abased%2520on%2520supervised%2520learning%252C%2520typically%2520involve%2520the%2520retraining%2520of%2520restoration%250Anetworks%2520or%2520high-level%2520vision%2520networks.%2520However%252C%2520collecting%2520paired%2520data%2520in%250Areal-world%2520scenarios%2520and%2520retraining%2520large-scale%2520models%2520are%2520challenge.%2520To%2520this%250Aend%252C%2520we%2520propose%2520an%2520unsupervised%2520learning%2520method%2520called%2520%255Ctextbf%257BVa%257Driational%250A%255Ctextbf%257BT%257Dranslator%2520%2528VaT%2529%252C%2520which%2520does%2520not%2520require%2520retraining%2520existing%250Arestoration%2520and%2520high-level%2520vision%2520networks.%2520Instead%252C%2520it%2520establishes%2520a%250Alightweight%2520network%2520that%2520serves%2520as%2520an%2520intermediate%2520bridge%2520between%2520them.%2520By%250Avariational%2520inference%252C%2520VaT%2520approximates%2520the%2520joint%2520distribution%2520of%2520restoration%250Aoutput%2520and%2520high-level%2520vision%2520input%252C%2520dividing%2520the%2520optimization%2520objective%2520into%250Apreserving%2520content%2520and%2520maximizing%2520marginal%2520likelihood%2520associated%2520with%250Ahigh-level%2520vision%2520tasks.%2520By%2520cleverly%2520leveraging%2520self-training%2520paradigms%252C%2520VaT%250Aachieves%2520the%2520above%2520optimization%2520objective%2520without%2520requiring%2520labels.%2520As%2520a%250Aresult%252C%2520the%2520translated%2520images%2520maintain%2520a%2520close%2520resemblance%2520to%2520their%2520original%250Acontent%2520while%2520also%2520demonstrating%2520exceptional%2520performance%2520on%2520high-level%2520vision%250Atasks.%2520Extensive%2520experiments%2520in%2520dehazing%2520and%2520low-light%2520enhancement%2520for%250Adetection%2520and%2520classification%2520show%2520the%2520superiority%2520of%2520our%2520method%2520over%2520other%250Astate-of-the-art%2520unsupervised%2520counterparts%252C%2520even%2520significantly%2520surpassing%250Asupervised%2520methods%2520in%2520some%2520complex%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Variational%20Translator%20for%20Bridging%20Image%20Restoration%20and%0A%20%20High-Level%20Vision%20Tasks&entry.906535625=Jiawei%20Wu%20and%20Zhi%20Jin&entry.1292438233=%20%20Recent%20research%20tries%20to%20extend%20image%20restoration%20capabilities%20from%20human%0Aperception%20to%20machine%20perception%2C%20thereby%20enhancing%20the%20performance%20of%0Ahigh-level%20vision%20tasks%20in%20degraded%20environments.%20These%20methods%2C%20primarily%0Abased%20on%20supervised%20learning%2C%20typically%20involve%20the%20retraining%20of%20restoration%0Anetworks%20or%20high-level%20vision%20networks.%20However%2C%20collecting%20paired%20data%20in%0Areal-world%20scenarios%20and%20retraining%20large-scale%20models%20are%20challenge.%20To%20this%0Aend%2C%20we%20propose%20an%20unsupervised%20learning%20method%20called%20%5Ctextbf%7BVa%7Driational%0A%5Ctextbf%7BT%7Dranslator%20%28VaT%29%2C%20which%20does%20not%20require%20retraining%20existing%0Arestoration%20and%20high-level%20vision%20networks.%20Instead%2C%20it%20establishes%20a%0Alightweight%20network%20that%20serves%20as%20an%20intermediate%20bridge%20between%20them.%20By%0Avariational%20inference%2C%20VaT%20approximates%20the%20joint%20distribution%20of%20restoration%0Aoutput%20and%20high-level%20vision%20input%2C%20dividing%20the%20optimization%20objective%20into%0Apreserving%20content%20and%20maximizing%20marginal%20likelihood%20associated%20with%0Ahigh-level%20vision%20tasks.%20By%20cleverly%20leveraging%20self-training%20paradigms%2C%20VaT%0Aachieves%20the%20above%20optimization%20objective%20without%20requiring%20labels.%20As%20a%0Aresult%2C%20the%20translated%20images%20maintain%20a%20close%20resemblance%20to%20their%20original%0Acontent%20while%20also%20demonstrating%20exceptional%20performance%20on%20high-level%20vision%0Atasks.%20Extensive%20experiments%20in%20dehazing%20and%20low-light%20enhancement%20for%0Adetection%20and%20classification%20show%20the%20superiority%20of%20our%20method%20over%20other%0Astate-of-the-art%20unsupervised%20counterparts%2C%20even%20significantly%20surpassing%0Asupervised%20methods%20in%20some%20complex%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08149v1&entry.124074799=Read"},
{"title": "Towards Practical Human Motion Prediction with LiDAR Point Clouds", "author": "Xiao Han and Yiming Ren and Yichen Yao and Yujing Sun and Yuexin Ma", "abstract": "  Human motion prediction is crucial for human-centric multimedia understanding\nand interacting. Current methods typically rely on ground truth human poses as\nobserved input, which is not practical for real-world scenarios where only raw\nvisual sensor data is available. To implement these methods in practice, a\npre-phrase of pose estimation is essential. However, such two-stage approaches\noften lead to performance degradation due to the accumulation of errors.\nMoreover, reducing raw visual data to sparse keypoint representations\nsignificantly diminishes the density of information, resulting in the loss of\nfine-grained features. In this paper, we propose \\textit{LiDAR-HMP}, the first\nsingle-LiDAR-based 3D human motion prediction approach, which receives the raw\nLiDAR point cloud as input and forecasts future 3D human poses directly.\nBuilding upon our novel structure-aware body feature descriptor, LiDAR-HMP\nadaptively maps the observed motion manifold to future poses and effectively\nmodels the spatial-temporal correlations of human motions for further\nrefinement of prediction results. Extensive experiments show that our method\nachieves state-of-the-art performance on two public benchmarks and demonstrates\nremarkable robustness and efficacy in real-world deployments.\n", "link": "http://arxiv.org/abs/2408.08202v1", "date": "2024-08-15", "relevancy": 2.2579, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6031}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5751}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Practical%20Human%20Motion%20Prediction%20with%20LiDAR%20Point%20Clouds&body=Title%3A%20Towards%20Practical%20Human%20Motion%20Prediction%20with%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Xiao%20Han%20and%20Yiming%20Ren%20and%20Yichen%20Yao%20and%20Yujing%20Sun%20and%20Yuexin%20Ma%0AAbstract%3A%20%20%20Human%20motion%20prediction%20is%20crucial%20for%20human-centric%20multimedia%20understanding%0Aand%20interacting.%20Current%20methods%20typically%20rely%20on%20ground%20truth%20human%20poses%20as%0Aobserved%20input%2C%20which%20is%20not%20practical%20for%20real-world%20scenarios%20where%20only%20raw%0Avisual%20sensor%20data%20is%20available.%20To%20implement%20these%20methods%20in%20practice%2C%20a%0Apre-phrase%20of%20pose%20estimation%20is%20essential.%20However%2C%20such%20two-stage%20approaches%0Aoften%20lead%20to%20performance%20degradation%20due%20to%20the%20accumulation%20of%20errors.%0AMoreover%2C%20reducing%20raw%20visual%20data%20to%20sparse%20keypoint%20representations%0Asignificantly%20diminishes%20the%20density%20of%20information%2C%20resulting%20in%20the%20loss%20of%0Afine-grained%20features.%20In%20this%20paper%2C%20we%20propose%20%5Ctextit%7BLiDAR-HMP%7D%2C%20the%20first%0Asingle-LiDAR-based%203D%20human%20motion%20prediction%20approach%2C%20which%20receives%20the%20raw%0ALiDAR%20point%20cloud%20as%20input%20and%20forecasts%20future%203D%20human%20poses%20directly.%0ABuilding%20upon%20our%20novel%20structure-aware%20body%20feature%20descriptor%2C%20LiDAR-HMP%0Aadaptively%20maps%20the%20observed%20motion%20manifold%20to%20future%20poses%20and%20effectively%0Amodels%20the%20spatial-temporal%20correlations%20of%20human%20motions%20for%20further%0Arefinement%20of%20prediction%20results.%20Extensive%20experiments%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20two%20public%20benchmarks%20and%20demonstrates%0Aremarkable%20robustness%20and%20efficacy%20in%20real-world%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Practical%2520Human%2520Motion%2520Prediction%2520with%2520LiDAR%2520Point%2520Clouds%26entry.906535625%3DXiao%2520Han%2520and%2520Yiming%2520Ren%2520and%2520Yichen%2520Yao%2520and%2520Yujing%2520Sun%2520and%2520Yuexin%2520Ma%26entry.1292438233%3D%2520%2520Human%2520motion%2520prediction%2520is%2520crucial%2520for%2520human-centric%2520multimedia%2520understanding%250Aand%2520interacting.%2520Current%2520methods%2520typically%2520rely%2520on%2520ground%2520truth%2520human%2520poses%2520as%250Aobserved%2520input%252C%2520which%2520is%2520not%2520practical%2520for%2520real-world%2520scenarios%2520where%2520only%2520raw%250Avisual%2520sensor%2520data%2520is%2520available.%2520To%2520implement%2520these%2520methods%2520in%2520practice%252C%2520a%250Apre-phrase%2520of%2520pose%2520estimation%2520is%2520essential.%2520However%252C%2520such%2520two-stage%2520approaches%250Aoften%2520lead%2520to%2520performance%2520degradation%2520due%2520to%2520the%2520accumulation%2520of%2520errors.%250AMoreover%252C%2520reducing%2520raw%2520visual%2520data%2520to%2520sparse%2520keypoint%2520representations%250Asignificantly%2520diminishes%2520the%2520density%2520of%2520information%252C%2520resulting%2520in%2520the%2520loss%2520of%250Afine-grained%2520features.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextit%257BLiDAR-HMP%257D%252C%2520the%2520first%250Asingle-LiDAR-based%25203D%2520human%2520motion%2520prediction%2520approach%252C%2520which%2520receives%2520the%2520raw%250ALiDAR%2520point%2520cloud%2520as%2520input%2520and%2520forecasts%2520future%25203D%2520human%2520poses%2520directly.%250ABuilding%2520upon%2520our%2520novel%2520structure-aware%2520body%2520feature%2520descriptor%252C%2520LiDAR-HMP%250Aadaptively%2520maps%2520the%2520observed%2520motion%2520manifold%2520to%2520future%2520poses%2520and%2520effectively%250Amodels%2520the%2520spatial-temporal%2520correlations%2520of%2520human%2520motions%2520for%2520further%250Arefinement%2520of%2520prediction%2520results.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520on%2520two%2520public%2520benchmarks%2520and%2520demonstrates%250Aremarkable%2520robustness%2520and%2520efficacy%2520in%2520real-world%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Practical%20Human%20Motion%20Prediction%20with%20LiDAR%20Point%20Clouds&entry.906535625=Xiao%20Han%20and%20Yiming%20Ren%20and%20Yichen%20Yao%20and%20Yujing%20Sun%20and%20Yuexin%20Ma&entry.1292438233=%20%20Human%20motion%20prediction%20is%20crucial%20for%20human-centric%20multimedia%20understanding%0Aand%20interacting.%20Current%20methods%20typically%20rely%20on%20ground%20truth%20human%20poses%20as%0Aobserved%20input%2C%20which%20is%20not%20practical%20for%20real-world%20scenarios%20where%20only%20raw%0Avisual%20sensor%20data%20is%20available.%20To%20implement%20these%20methods%20in%20practice%2C%20a%0Apre-phrase%20of%20pose%20estimation%20is%20essential.%20However%2C%20such%20two-stage%20approaches%0Aoften%20lead%20to%20performance%20degradation%20due%20to%20the%20accumulation%20of%20errors.%0AMoreover%2C%20reducing%20raw%20visual%20data%20to%20sparse%20keypoint%20representations%0Asignificantly%20diminishes%20the%20density%20of%20information%2C%20resulting%20in%20the%20loss%20of%0Afine-grained%20features.%20In%20this%20paper%2C%20we%20propose%20%5Ctextit%7BLiDAR-HMP%7D%2C%20the%20first%0Asingle-LiDAR-based%203D%20human%20motion%20prediction%20approach%2C%20which%20receives%20the%20raw%0ALiDAR%20point%20cloud%20as%20input%20and%20forecasts%20future%203D%20human%20poses%20directly.%0ABuilding%20upon%20our%20novel%20structure-aware%20body%20feature%20descriptor%2C%20LiDAR-HMP%0Aadaptively%20maps%20the%20observed%20motion%20manifold%20to%20future%20poses%20and%20effectively%0Amodels%20the%20spatial-temporal%20correlations%20of%20human%20motions%20for%20further%0Arefinement%20of%20prediction%20results.%20Extensive%20experiments%20show%20that%20our%20method%0Aachieves%20state-of-the-art%20performance%20on%20two%20public%20benchmarks%20and%20demonstrates%0Aremarkable%20robustness%20and%20efficacy%20in%20real-world%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08202v1&entry.124074799=Read"},
{"title": "HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals Through\n  Contrastive Learning", "author": "Hongyu Li and Snehal Dikhale and Jinda Cui and Soshi Iba and Nawid Jamali", "abstract": "  To achieve dexterity comparable to that of humans, robots must intelligently\nprocess tactile sensor data. Taxel-based tactile signals often have low\nspatial-resolution, with non-standardized representations. In this paper, we\npropose a novel framework, HyperTaxel, for learning a geometrically-informed\nrepresentation of taxel-based tactile signals to address challenges associated\nwith their spatial resolution. We use this representation and a contrastive\nlearning objective to encode and map sparse low-resolution taxel signals to\nhigh-resolution contact surfaces. To address the uncertainty inherent in these\nsignals, we leverage joint probability distributions across multiple\nsimultaneous contacts to improve taxel hyper-resolution. We evaluate our\nrepresentation by comparing it with two baselines and present results that\nsuggest our representation outperforms the baselines. Furthermore, we present\nqualitative results that demonstrate the learned representation captures the\ngeometric features of the contact surface, such as flatness, curvature, and\nedges, and generalizes across different objects and sensor configurations.\nMoreover, we present results that suggest our representation improves the\nperformance of various downstream tasks, such as surface classification, 6D\nin-hand pose estimation, and sim-to-real transfer.\n", "link": "http://arxiv.org/abs/2408.08312v1", "date": "2024-08-15", "relevancy": 2.2507, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5908}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5767}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperTaxel%3A%20Hyper-Resolution%20for%20Taxel-Based%20Tactile%20Signals%20Through%0A%20%20Contrastive%20Learning&body=Title%3A%20HyperTaxel%3A%20Hyper-Resolution%20for%20Taxel-Based%20Tactile%20Signals%20Through%0A%20%20Contrastive%20Learning%0AAuthor%3A%20Hongyu%20Li%20and%20Snehal%20Dikhale%20and%20Jinda%20Cui%20and%20Soshi%20Iba%20and%20Nawid%20Jamali%0AAbstract%3A%20%20%20To%20achieve%20dexterity%20comparable%20to%20that%20of%20humans%2C%20robots%20must%20intelligently%0Aprocess%20tactile%20sensor%20data.%20Taxel-based%20tactile%20signals%20often%20have%20low%0Aspatial-resolution%2C%20with%20non-standardized%20representations.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20framework%2C%20HyperTaxel%2C%20for%20learning%20a%20geometrically-informed%0Arepresentation%20of%20taxel-based%20tactile%20signals%20to%20address%20challenges%20associated%0Awith%20their%20spatial%20resolution.%20We%20use%20this%20representation%20and%20a%20contrastive%0Alearning%20objective%20to%20encode%20and%20map%20sparse%20low-resolution%20taxel%20signals%20to%0Ahigh-resolution%20contact%20surfaces.%20To%20address%20the%20uncertainty%20inherent%20in%20these%0Asignals%2C%20we%20leverage%20joint%20probability%20distributions%20across%20multiple%0Asimultaneous%20contacts%20to%20improve%20taxel%20hyper-resolution.%20We%20evaluate%20our%0Arepresentation%20by%20comparing%20it%20with%20two%20baselines%20and%20present%20results%20that%0Asuggest%20our%20representation%20outperforms%20the%20baselines.%20Furthermore%2C%20we%20present%0Aqualitative%20results%20that%20demonstrate%20the%20learned%20representation%20captures%20the%0Ageometric%20features%20of%20the%20contact%20surface%2C%20such%20as%20flatness%2C%20curvature%2C%20and%0Aedges%2C%20and%20generalizes%20across%20different%20objects%20and%20sensor%20configurations.%0AMoreover%2C%20we%20present%20results%20that%20suggest%20our%20representation%20improves%20the%0Aperformance%20of%20various%20downstream%20tasks%2C%20such%20as%20surface%20classification%2C%206D%0Ain-hand%20pose%20estimation%2C%20and%20sim-to-real%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperTaxel%253A%2520Hyper-Resolution%2520for%2520Taxel-Based%2520Tactile%2520Signals%2520Through%250A%2520%2520Contrastive%2520Learning%26entry.906535625%3DHongyu%2520Li%2520and%2520Snehal%2520Dikhale%2520and%2520Jinda%2520Cui%2520and%2520Soshi%2520Iba%2520and%2520Nawid%2520Jamali%26entry.1292438233%3D%2520%2520To%2520achieve%2520dexterity%2520comparable%2520to%2520that%2520of%2520humans%252C%2520robots%2520must%2520intelligently%250Aprocess%2520tactile%2520sensor%2520data.%2520Taxel-based%2520tactile%2520signals%2520often%2520have%2520low%250Aspatial-resolution%252C%2520with%2520non-standardized%2520representations.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520framework%252C%2520HyperTaxel%252C%2520for%2520learning%2520a%2520geometrically-informed%250Arepresentation%2520of%2520taxel-based%2520tactile%2520signals%2520to%2520address%2520challenges%2520associated%250Awith%2520their%2520spatial%2520resolution.%2520We%2520use%2520this%2520representation%2520and%2520a%2520contrastive%250Alearning%2520objective%2520to%2520encode%2520and%2520map%2520sparse%2520low-resolution%2520taxel%2520signals%2520to%250Ahigh-resolution%2520contact%2520surfaces.%2520To%2520address%2520the%2520uncertainty%2520inherent%2520in%2520these%250Asignals%252C%2520we%2520leverage%2520joint%2520probability%2520distributions%2520across%2520multiple%250Asimultaneous%2520contacts%2520to%2520improve%2520taxel%2520hyper-resolution.%2520We%2520evaluate%2520our%250Arepresentation%2520by%2520comparing%2520it%2520with%2520two%2520baselines%2520and%2520present%2520results%2520that%250Asuggest%2520our%2520representation%2520outperforms%2520the%2520baselines.%2520Furthermore%252C%2520we%2520present%250Aqualitative%2520results%2520that%2520demonstrate%2520the%2520learned%2520representation%2520captures%2520the%250Ageometric%2520features%2520of%2520the%2520contact%2520surface%252C%2520such%2520as%2520flatness%252C%2520curvature%252C%2520and%250Aedges%252C%2520and%2520generalizes%2520across%2520different%2520objects%2520and%2520sensor%2520configurations.%250AMoreover%252C%2520we%2520present%2520results%2520that%2520suggest%2520our%2520representation%2520improves%2520the%250Aperformance%2520of%2520various%2520downstream%2520tasks%252C%2520such%2520as%2520surface%2520classification%252C%25206D%250Ain-hand%2520pose%2520estimation%252C%2520and%2520sim-to-real%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperTaxel%3A%20Hyper-Resolution%20for%20Taxel-Based%20Tactile%20Signals%20Through%0A%20%20Contrastive%20Learning&entry.906535625=Hongyu%20Li%20and%20Snehal%20Dikhale%20and%20Jinda%20Cui%20and%20Soshi%20Iba%20and%20Nawid%20Jamali&entry.1292438233=%20%20To%20achieve%20dexterity%20comparable%20to%20that%20of%20humans%2C%20robots%20must%20intelligently%0Aprocess%20tactile%20sensor%20data.%20Taxel-based%20tactile%20signals%20often%20have%20low%0Aspatial-resolution%2C%20with%20non-standardized%20representations.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20framework%2C%20HyperTaxel%2C%20for%20learning%20a%20geometrically-informed%0Arepresentation%20of%20taxel-based%20tactile%20signals%20to%20address%20challenges%20associated%0Awith%20their%20spatial%20resolution.%20We%20use%20this%20representation%20and%20a%20contrastive%0Alearning%20objective%20to%20encode%20and%20map%20sparse%20low-resolution%20taxel%20signals%20to%0Ahigh-resolution%20contact%20surfaces.%20To%20address%20the%20uncertainty%20inherent%20in%20these%0Asignals%2C%20we%20leverage%20joint%20probability%20distributions%20across%20multiple%0Asimultaneous%20contacts%20to%20improve%20taxel%20hyper-resolution.%20We%20evaluate%20our%0Arepresentation%20by%20comparing%20it%20with%20two%20baselines%20and%20present%20results%20that%0Asuggest%20our%20representation%20outperforms%20the%20baselines.%20Furthermore%2C%20we%20present%0Aqualitative%20results%20that%20demonstrate%20the%20learned%20representation%20captures%20the%0Ageometric%20features%20of%20the%20contact%20surface%2C%20such%20as%20flatness%2C%20curvature%2C%20and%0Aedges%2C%20and%20generalizes%20across%20different%20objects%20and%20sensor%20configurations.%0AMoreover%2C%20we%20present%20results%20that%20suggest%20our%20representation%20improves%20the%0Aperformance%20of%20various%20downstream%20tasks%2C%20such%20as%20surface%20classification%2C%206D%0Ain-hand%20pose%20estimation%2C%20and%20sim-to-real%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08312v1&entry.124074799=Read"},
{"title": "Comparative Evaluation of 3D Reconstruction Methods for Object Pose\n  Estimation", "author": "Varun Burde and Assia Benbihi and Pavel Burget and Torsten Sattler", "abstract": "  Object pose estimation is essential to many industrial applications involving\nrobotic manipulation, navigation, and augmented reality. Current generalizable\nobject pose estimators, i.e., approaches that do not need to be trained per\nobject, rely on accurate 3D models. Predominantly, CAD models are used, which\ncan be hard to obtain in practice. At the same time, it is often possible to\nacquire images of an object. Naturally, this leads to the question whether 3D\nmodels reconstructed from images are sufficient to facilitate accurate object\npose estimation. We aim to answer this question by proposing a novel benchmark\nfor measuring the impact of 3D reconstruction quality on pose estimation\naccuracy. Our benchmark provides calibrated images for object reconstruction\nregistered with the test images of the YCB-V dataset for pose evaluation under\nthe BOP benchmark format. Detailed experiments with multiple state-of-the-art\n3D reconstruction and object pose estimation approaches show that the geometry\nproduced by modern reconstruction methods is often sufficient for accurate pose\nestimation. Our experiments lead to interesting observations: (1) Standard\nmetrics for measuring 3D reconstruction quality are not necessarily indicative\nof pose estimation accuracy, which shows the need for dedicated benchmarks such\nas ours. (2) Classical, non-learning-based approaches can perform on par with\nmodern learning-based reconstruction techniques and can even offer a better\nreconstruction time-pose accuracy tradeoff. (3) There is still a sizable gap\nbetween performance with reconstructed and with CAD models. To foster research\non closing this gap, our benchmark is publicly available at\nhttps://github.com/VarunBurde/reconstruction_pose_benchmark}.\n", "link": "http://arxiv.org/abs/2408.08234v1", "date": "2024-08-15", "relevancy": 2.2416, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5657}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5657}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Evaluation%20of%203D%20Reconstruction%20Methods%20for%20Object%20Pose%0A%20%20Estimation&body=Title%3A%20Comparative%20Evaluation%20of%203D%20Reconstruction%20Methods%20for%20Object%20Pose%0A%20%20Estimation%0AAuthor%3A%20Varun%20Burde%20and%20Assia%20Benbihi%20and%20Pavel%20Burget%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20Object%20pose%20estimation%20is%20essential%20to%20many%20industrial%20applications%20involving%0Arobotic%20manipulation%2C%20navigation%2C%20and%20augmented%20reality.%20Current%20generalizable%0Aobject%20pose%20estimators%2C%20i.e.%2C%20approaches%20that%20do%20not%20need%20to%20be%20trained%20per%0Aobject%2C%20rely%20on%20accurate%203D%20models.%20Predominantly%2C%20CAD%20models%20are%20used%2C%20which%0Acan%20be%20hard%20to%20obtain%20in%20practice.%20At%20the%20same%20time%2C%20it%20is%20often%20possible%20to%0Aacquire%20images%20of%20an%20object.%20Naturally%2C%20this%20leads%20to%20the%20question%20whether%203D%0Amodels%20reconstructed%20from%20images%20are%20sufficient%20to%20facilitate%20accurate%20object%0Apose%20estimation.%20We%20aim%20to%20answer%20this%20question%20by%20proposing%20a%20novel%20benchmark%0Afor%20measuring%20the%20impact%20of%203D%20reconstruction%20quality%20on%20pose%20estimation%0Aaccuracy.%20Our%20benchmark%20provides%20calibrated%20images%20for%20object%20reconstruction%0Aregistered%20with%20the%20test%20images%20of%20the%20YCB-V%20dataset%20for%20pose%20evaluation%20under%0Athe%20BOP%20benchmark%20format.%20Detailed%20experiments%20with%20multiple%20state-of-the-art%0A3D%20reconstruction%20and%20object%20pose%20estimation%20approaches%20show%20that%20the%20geometry%0Aproduced%20by%20modern%20reconstruction%20methods%20is%20often%20sufficient%20for%20accurate%20pose%0Aestimation.%20Our%20experiments%20lead%20to%20interesting%20observations%3A%20%281%29%20Standard%0Ametrics%20for%20measuring%203D%20reconstruction%20quality%20are%20not%20necessarily%20indicative%0Aof%20pose%20estimation%20accuracy%2C%20which%20shows%20the%20need%20for%20dedicated%20benchmarks%20such%0Aas%20ours.%20%282%29%20Classical%2C%20non-learning-based%20approaches%20can%20perform%20on%20par%20with%0Amodern%20learning-based%20reconstruction%20techniques%20and%20can%20even%20offer%20a%20better%0Areconstruction%20time-pose%20accuracy%20tradeoff.%20%283%29%20There%20is%20still%20a%20sizable%20gap%0Abetween%20performance%20with%20reconstructed%20and%20with%20CAD%20models.%20To%20foster%20research%0Aon%20closing%20this%20gap%2C%20our%20benchmark%20is%20publicly%20available%20at%0Ahttps%3A//github.com/VarunBurde/reconstruction_pose_benchmark%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Evaluation%2520of%25203D%2520Reconstruction%2520Methods%2520for%2520Object%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DVarun%2520Burde%2520and%2520Assia%2520Benbihi%2520and%2520Pavel%2520Burget%2520and%2520Torsten%2520Sattler%26entry.1292438233%3D%2520%2520Object%2520pose%2520estimation%2520is%2520essential%2520to%2520many%2520industrial%2520applications%2520involving%250Arobotic%2520manipulation%252C%2520navigation%252C%2520and%2520augmented%2520reality.%2520Current%2520generalizable%250Aobject%2520pose%2520estimators%252C%2520i.e.%252C%2520approaches%2520that%2520do%2520not%2520need%2520to%2520be%2520trained%2520per%250Aobject%252C%2520rely%2520on%2520accurate%25203D%2520models.%2520Predominantly%252C%2520CAD%2520models%2520are%2520used%252C%2520which%250Acan%2520be%2520hard%2520to%2520obtain%2520in%2520practice.%2520At%2520the%2520same%2520time%252C%2520it%2520is%2520often%2520possible%2520to%250Aacquire%2520images%2520of%2520an%2520object.%2520Naturally%252C%2520this%2520leads%2520to%2520the%2520question%2520whether%25203D%250Amodels%2520reconstructed%2520from%2520images%2520are%2520sufficient%2520to%2520facilitate%2520accurate%2520object%250Apose%2520estimation.%2520We%2520aim%2520to%2520answer%2520this%2520question%2520by%2520proposing%2520a%2520novel%2520benchmark%250Afor%2520measuring%2520the%2520impact%2520of%25203D%2520reconstruction%2520quality%2520on%2520pose%2520estimation%250Aaccuracy.%2520Our%2520benchmark%2520provides%2520calibrated%2520images%2520for%2520object%2520reconstruction%250Aregistered%2520with%2520the%2520test%2520images%2520of%2520the%2520YCB-V%2520dataset%2520for%2520pose%2520evaluation%2520under%250Athe%2520BOP%2520benchmark%2520format.%2520Detailed%2520experiments%2520with%2520multiple%2520state-of-the-art%250A3D%2520reconstruction%2520and%2520object%2520pose%2520estimation%2520approaches%2520show%2520that%2520the%2520geometry%250Aproduced%2520by%2520modern%2520reconstruction%2520methods%2520is%2520often%2520sufficient%2520for%2520accurate%2520pose%250Aestimation.%2520Our%2520experiments%2520lead%2520to%2520interesting%2520observations%253A%2520%25281%2529%2520Standard%250Ametrics%2520for%2520measuring%25203D%2520reconstruction%2520quality%2520are%2520not%2520necessarily%2520indicative%250Aof%2520pose%2520estimation%2520accuracy%252C%2520which%2520shows%2520the%2520need%2520for%2520dedicated%2520benchmarks%2520such%250Aas%2520ours.%2520%25282%2529%2520Classical%252C%2520non-learning-based%2520approaches%2520can%2520perform%2520on%2520par%2520with%250Amodern%2520learning-based%2520reconstruction%2520techniques%2520and%2520can%2520even%2520offer%2520a%2520better%250Areconstruction%2520time-pose%2520accuracy%2520tradeoff.%2520%25283%2529%2520There%2520is%2520still%2520a%2520sizable%2520gap%250Abetween%2520performance%2520with%2520reconstructed%2520and%2520with%2520CAD%2520models.%2520To%2520foster%2520research%250Aon%2520closing%2520this%2520gap%252C%2520our%2520benchmark%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/VarunBurde/reconstruction_pose_benchmark%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Evaluation%20of%203D%20Reconstruction%20Methods%20for%20Object%20Pose%0A%20%20Estimation&entry.906535625=Varun%20Burde%20and%20Assia%20Benbihi%20and%20Pavel%20Burget%20and%20Torsten%20Sattler&entry.1292438233=%20%20Object%20pose%20estimation%20is%20essential%20to%20many%20industrial%20applications%20involving%0Arobotic%20manipulation%2C%20navigation%2C%20and%20augmented%20reality.%20Current%20generalizable%0Aobject%20pose%20estimators%2C%20i.e.%2C%20approaches%20that%20do%20not%20need%20to%20be%20trained%20per%0Aobject%2C%20rely%20on%20accurate%203D%20models.%20Predominantly%2C%20CAD%20models%20are%20used%2C%20which%0Acan%20be%20hard%20to%20obtain%20in%20practice.%20At%20the%20same%20time%2C%20it%20is%20often%20possible%20to%0Aacquire%20images%20of%20an%20object.%20Naturally%2C%20this%20leads%20to%20the%20question%20whether%203D%0Amodels%20reconstructed%20from%20images%20are%20sufficient%20to%20facilitate%20accurate%20object%0Apose%20estimation.%20We%20aim%20to%20answer%20this%20question%20by%20proposing%20a%20novel%20benchmark%0Afor%20measuring%20the%20impact%20of%203D%20reconstruction%20quality%20on%20pose%20estimation%0Aaccuracy.%20Our%20benchmark%20provides%20calibrated%20images%20for%20object%20reconstruction%0Aregistered%20with%20the%20test%20images%20of%20the%20YCB-V%20dataset%20for%20pose%20evaluation%20under%0Athe%20BOP%20benchmark%20format.%20Detailed%20experiments%20with%20multiple%20state-of-the-art%0A3D%20reconstruction%20and%20object%20pose%20estimation%20approaches%20show%20that%20the%20geometry%0Aproduced%20by%20modern%20reconstruction%20methods%20is%20often%20sufficient%20for%20accurate%20pose%0Aestimation.%20Our%20experiments%20lead%20to%20interesting%20observations%3A%20%281%29%20Standard%0Ametrics%20for%20measuring%203D%20reconstruction%20quality%20are%20not%20necessarily%20indicative%0Aof%20pose%20estimation%20accuracy%2C%20which%20shows%20the%20need%20for%20dedicated%20benchmarks%20such%0Aas%20ours.%20%282%29%20Classical%2C%20non-learning-based%20approaches%20can%20perform%20on%20par%20with%0Amodern%20learning-based%20reconstruction%20techniques%20and%20can%20even%20offer%20a%20better%0Areconstruction%20time-pose%20accuracy%20tradeoff.%20%283%29%20There%20is%20still%20a%20sizable%20gap%0Abetween%20performance%20with%20reconstructed%20and%20with%20CAD%20models.%20To%20foster%20research%0Aon%20closing%20this%20gap%2C%20our%20benchmark%20is%20publicly%20available%20at%0Ahttps%3A//github.com/VarunBurde/reconstruction_pose_benchmark%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08234v1&entry.124074799=Read"},
{"title": "End-to-end Autonomous Driving: Challenges and Frontiers", "author": "Li Chen and Penghao Wu and Kashyap Chitta and Bernhard Jaeger and Andreas Geiger and Hongyang Li", "abstract": "  The autonomous driving community has witnessed a rapid growth in approaches\nthat embrace an end-to-end algorithm framework, utilizing raw sensor input to\ngenerate vehicle motion plans, instead of concentrating on individual tasks\nsuch as detection and motion prediction. End-to-end systems, in comparison to\nmodular pipelines, benefit from joint feature optimization for perception and\nplanning. This field has flourished due to the availability of large-scale\ndatasets, closed-loop evaluation, and the increasing need for autonomous\ndriving algorithms to perform effectively in challenging scenarios. In this\nsurvey, we provide a comprehensive analysis of more than 270 papers, covering\nthe motivation, roadmap, methodology, challenges, and future trends in\nend-to-end autonomous driving. We delve into several critical challenges,\nincluding multi-modality, interpretability, causal confusion, robustness, and\nworld models, amongst others. Additionally, we discuss current advancements in\nfoundation models and visual pre-training, as well as how to incorporate these\ntechniques within the end-to-end driving framework. we maintain an active\nrepository that contains up-to-date literature and open-source projects at\nhttps://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.\n", "link": "http://arxiv.org/abs/2306.16927v3", "date": "2024-08-15", "relevancy": 2.2255, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5693}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-end%20Autonomous%20Driving%3A%20Challenges%20and%20Frontiers&body=Title%3A%20End-to-end%20Autonomous%20Driving%3A%20Challenges%20and%20Frontiers%0AAuthor%3A%20Li%20Chen%20and%20Penghao%20Wu%20and%20Kashyap%20Chitta%20and%20Bernhard%20Jaeger%20and%20Andreas%20Geiger%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20The%20autonomous%20driving%20community%20has%20witnessed%20a%20rapid%20growth%20in%20approaches%0Athat%20embrace%20an%20end-to-end%20algorithm%20framework%2C%20utilizing%20raw%20sensor%20input%20to%0Agenerate%20vehicle%20motion%20plans%2C%20instead%20of%20concentrating%20on%20individual%20tasks%0Asuch%20as%20detection%20and%20motion%20prediction.%20End-to-end%20systems%2C%20in%20comparison%20to%0Amodular%20pipelines%2C%20benefit%20from%20joint%20feature%20optimization%20for%20perception%20and%0Aplanning.%20This%20field%20has%20flourished%20due%20to%20the%20availability%20of%20large-scale%0Adatasets%2C%20closed-loop%20evaluation%2C%20and%20the%20increasing%20need%20for%20autonomous%0Adriving%20algorithms%20to%20perform%20effectively%20in%20challenging%20scenarios.%20In%20this%0Asurvey%2C%20we%20provide%20a%20comprehensive%20analysis%20of%20more%20than%20270%20papers%2C%20covering%0Athe%20motivation%2C%20roadmap%2C%20methodology%2C%20challenges%2C%20and%20future%20trends%20in%0Aend-to-end%20autonomous%20driving.%20We%20delve%20into%20several%20critical%20challenges%2C%0Aincluding%20multi-modality%2C%20interpretability%2C%20causal%20confusion%2C%20robustness%2C%20and%0Aworld%20models%2C%20amongst%20others.%20Additionally%2C%20we%20discuss%20current%20advancements%20in%0Afoundation%20models%20and%20visual%20pre-training%2C%20as%20well%20as%20how%20to%20incorporate%20these%0Atechniques%20within%20the%20end-to-end%20driving%20framework.%20we%20maintain%20an%20active%0Arepository%20that%20contains%20up-to-date%20literature%20and%20open-source%20projects%20at%0Ahttps%3A//github.com/OpenDriveLab/End-to-end-Autonomous-Driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.16927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-end%2520Autonomous%2520Driving%253A%2520Challenges%2520and%2520Frontiers%26entry.906535625%3DLi%2520Chen%2520and%2520Penghao%2520Wu%2520and%2520Kashyap%2520Chitta%2520and%2520Bernhard%2520Jaeger%2520and%2520Andreas%2520Geiger%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520The%2520autonomous%2520driving%2520community%2520has%2520witnessed%2520a%2520rapid%2520growth%2520in%2520approaches%250Athat%2520embrace%2520an%2520end-to-end%2520algorithm%2520framework%252C%2520utilizing%2520raw%2520sensor%2520input%2520to%250Agenerate%2520vehicle%2520motion%2520plans%252C%2520instead%2520of%2520concentrating%2520on%2520individual%2520tasks%250Asuch%2520as%2520detection%2520and%2520motion%2520prediction.%2520End-to-end%2520systems%252C%2520in%2520comparison%2520to%250Amodular%2520pipelines%252C%2520benefit%2520from%2520joint%2520feature%2520optimization%2520for%2520perception%2520and%250Aplanning.%2520This%2520field%2520has%2520flourished%2520due%2520to%2520the%2520availability%2520of%2520large-scale%250Adatasets%252C%2520closed-loop%2520evaluation%252C%2520and%2520the%2520increasing%2520need%2520for%2520autonomous%250Adriving%2520algorithms%2520to%2520perform%2520effectively%2520in%2520challenging%2520scenarios.%2520In%2520this%250Asurvey%252C%2520we%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520more%2520than%2520270%2520papers%252C%2520covering%250Athe%2520motivation%252C%2520roadmap%252C%2520methodology%252C%2520challenges%252C%2520and%2520future%2520trends%2520in%250Aend-to-end%2520autonomous%2520driving.%2520We%2520delve%2520into%2520several%2520critical%2520challenges%252C%250Aincluding%2520multi-modality%252C%2520interpretability%252C%2520causal%2520confusion%252C%2520robustness%252C%2520and%250Aworld%2520models%252C%2520amongst%2520others.%2520Additionally%252C%2520we%2520discuss%2520current%2520advancements%2520in%250Afoundation%2520models%2520and%2520visual%2520pre-training%252C%2520as%2520well%2520as%2520how%2520to%2520incorporate%2520these%250Atechniques%2520within%2520the%2520end-to-end%2520driving%2520framework.%2520we%2520maintain%2520an%2520active%250Arepository%2520that%2520contains%2520up-to-date%2520literature%2520and%2520open-source%2520projects%2520at%250Ahttps%253A//github.com/OpenDriveLab/End-to-end-Autonomous-Driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.16927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-end%20Autonomous%20Driving%3A%20Challenges%20and%20Frontiers&entry.906535625=Li%20Chen%20and%20Penghao%20Wu%20and%20Kashyap%20Chitta%20and%20Bernhard%20Jaeger%20and%20Andreas%20Geiger%20and%20Hongyang%20Li&entry.1292438233=%20%20The%20autonomous%20driving%20community%20has%20witnessed%20a%20rapid%20growth%20in%20approaches%0Athat%20embrace%20an%20end-to-end%20algorithm%20framework%2C%20utilizing%20raw%20sensor%20input%20to%0Agenerate%20vehicle%20motion%20plans%2C%20instead%20of%20concentrating%20on%20individual%20tasks%0Asuch%20as%20detection%20and%20motion%20prediction.%20End-to-end%20systems%2C%20in%20comparison%20to%0Amodular%20pipelines%2C%20benefit%20from%20joint%20feature%20optimization%20for%20perception%20and%0Aplanning.%20This%20field%20has%20flourished%20due%20to%20the%20availability%20of%20large-scale%0Adatasets%2C%20closed-loop%20evaluation%2C%20and%20the%20increasing%20need%20for%20autonomous%0Adriving%20algorithms%20to%20perform%20effectively%20in%20challenging%20scenarios.%20In%20this%0Asurvey%2C%20we%20provide%20a%20comprehensive%20analysis%20of%20more%20than%20270%20papers%2C%20covering%0Athe%20motivation%2C%20roadmap%2C%20methodology%2C%20challenges%2C%20and%20future%20trends%20in%0Aend-to-end%20autonomous%20driving.%20We%20delve%20into%20several%20critical%20challenges%2C%0Aincluding%20multi-modality%2C%20interpretability%2C%20causal%20confusion%2C%20robustness%2C%20and%0Aworld%20models%2C%20amongst%20others.%20Additionally%2C%20we%20discuss%20current%20advancements%20in%0Afoundation%20models%20and%20visual%20pre-training%2C%20as%20well%20as%20how%20to%20incorporate%20these%0Atechniques%20within%20the%20end-to-end%20driving%20framework.%20we%20maintain%20an%20active%0Arepository%20that%20contains%20up-to-date%20literature%20and%20open-source%20projects%20at%0Ahttps%3A//github.com/OpenDriveLab/End-to-end-Autonomous-Driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16927v3&entry.124074799=Read"},
{"title": "Towards flexible perception with visual memory", "author": "Robert Geirhos and Priyank Jaini and Austin Stone and Sourabh Medapati and Xi Yi and George Toderici and Abhijit Ogale and Jonathon Shlens", "abstract": "  Training a neural network is a monolithic endeavor, akin to carving knowledge\ninto stone: once the process is completed, editing the knowledge in a network\nis nearly impossible, since all information is distributed across the network's\nweights. We here explore a simple, compelling alternative by marrying the\nrepresentational power of deep neural networks with the flexibility of a\ndatabase. Decomposing the task of image classification into image similarity\n(from a pre-trained embedding) and search (via fast nearest neighbor retrieval\nfrom a knowledge database), we build a simple and flexible visual memory that\nhas the following key capabilities: (1.) The ability to flexibly add data\nacross scales: from individual samples all the way to entire classes and\nbillion-scale data; (2.) The ability to remove data through unlearning and\nmemory pruning; (3.) An interpretable decision-mechanism on which we can\nintervene to control its behavior. Taken together, these capabilities\ncomprehensively demonstrate the benefits of an explicit visual memory. We hope\nthat it might contribute to a conversation on how knowledge should be\nrepresented in deep vision models -- beyond carving it in ``stone'' weights.\n", "link": "http://arxiv.org/abs/2408.08172v1", "date": "2024-08-15", "relevancy": 2.2228, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5831}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5402}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20flexible%20perception%20with%20visual%20memory&body=Title%3A%20Towards%20flexible%20perception%20with%20visual%20memory%0AAuthor%3A%20Robert%20Geirhos%20and%20Priyank%20Jaini%20and%20Austin%20Stone%20and%20Sourabh%20Medapati%20and%20Xi%20Yi%20and%20George%20Toderici%20and%20Abhijit%20Ogale%20and%20Jonathon%20Shlens%0AAbstract%3A%20%20%20Training%20a%20neural%20network%20is%20a%20monolithic%20endeavor%2C%20akin%20to%20carving%20knowledge%0Ainto%20stone%3A%20once%20the%20process%20is%20completed%2C%20editing%20the%20knowledge%20in%20a%20network%0Ais%20nearly%20impossible%2C%20since%20all%20information%20is%20distributed%20across%20the%20network%27s%0Aweights.%20We%20here%20explore%20a%20simple%2C%20compelling%20alternative%20by%20marrying%20the%0Arepresentational%20power%20of%20deep%20neural%20networks%20with%20the%20flexibility%20of%20a%0Adatabase.%20Decomposing%20the%20task%20of%20image%20classification%20into%20image%20similarity%0A%28from%20a%20pre-trained%20embedding%29%20and%20search%20%28via%20fast%20nearest%20neighbor%20retrieval%0Afrom%20a%20knowledge%20database%29%2C%20we%20build%20a%20simple%20and%20flexible%20visual%20memory%20that%0Ahas%20the%20following%20key%20capabilities%3A%20%281.%29%20The%20ability%20to%20flexibly%20add%20data%0Aacross%20scales%3A%20from%20individual%20samples%20all%20the%20way%20to%20entire%20classes%20and%0Abillion-scale%20data%3B%20%282.%29%20The%20ability%20to%20remove%20data%20through%20unlearning%20and%0Amemory%20pruning%3B%20%283.%29%20An%20interpretable%20decision-mechanism%20on%20which%20we%20can%0Aintervene%20to%20control%20its%20behavior.%20Taken%20together%2C%20these%20capabilities%0Acomprehensively%20demonstrate%20the%20benefits%20of%20an%20explicit%20visual%20memory.%20We%20hope%0Athat%20it%20might%20contribute%20to%20a%20conversation%20on%20how%20knowledge%20should%20be%0Arepresented%20in%20deep%20vision%20models%20--%20beyond%20carving%20it%20in%20%60%60stone%27%27%20weights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520flexible%2520perception%2520with%2520visual%2520memory%26entry.906535625%3DRobert%2520Geirhos%2520and%2520Priyank%2520Jaini%2520and%2520Austin%2520Stone%2520and%2520Sourabh%2520Medapati%2520and%2520Xi%2520Yi%2520and%2520George%2520Toderici%2520and%2520Abhijit%2520Ogale%2520and%2520Jonathon%2520Shlens%26entry.1292438233%3D%2520%2520Training%2520a%2520neural%2520network%2520is%2520a%2520monolithic%2520endeavor%252C%2520akin%2520to%2520carving%2520knowledge%250Ainto%2520stone%253A%2520once%2520the%2520process%2520is%2520completed%252C%2520editing%2520the%2520knowledge%2520in%2520a%2520network%250Ais%2520nearly%2520impossible%252C%2520since%2520all%2520information%2520is%2520distributed%2520across%2520the%2520network%2527s%250Aweights.%2520We%2520here%2520explore%2520a%2520simple%252C%2520compelling%2520alternative%2520by%2520marrying%2520the%250Arepresentational%2520power%2520of%2520deep%2520neural%2520networks%2520with%2520the%2520flexibility%2520of%2520a%250Adatabase.%2520Decomposing%2520the%2520task%2520of%2520image%2520classification%2520into%2520image%2520similarity%250A%2528from%2520a%2520pre-trained%2520embedding%2529%2520and%2520search%2520%2528via%2520fast%2520nearest%2520neighbor%2520retrieval%250Afrom%2520a%2520knowledge%2520database%2529%252C%2520we%2520build%2520a%2520simple%2520and%2520flexible%2520visual%2520memory%2520that%250Ahas%2520the%2520following%2520key%2520capabilities%253A%2520%25281.%2529%2520The%2520ability%2520to%2520flexibly%2520add%2520data%250Aacross%2520scales%253A%2520from%2520individual%2520samples%2520all%2520the%2520way%2520to%2520entire%2520classes%2520and%250Abillion-scale%2520data%253B%2520%25282.%2529%2520The%2520ability%2520to%2520remove%2520data%2520through%2520unlearning%2520and%250Amemory%2520pruning%253B%2520%25283.%2529%2520An%2520interpretable%2520decision-mechanism%2520on%2520which%2520we%2520can%250Aintervene%2520to%2520control%2520its%2520behavior.%2520Taken%2520together%252C%2520these%2520capabilities%250Acomprehensively%2520demonstrate%2520the%2520benefits%2520of%2520an%2520explicit%2520visual%2520memory.%2520We%2520hope%250Athat%2520it%2520might%2520contribute%2520to%2520a%2520conversation%2520on%2520how%2520knowledge%2520should%2520be%250Arepresented%2520in%2520deep%2520vision%2520models%2520--%2520beyond%2520carving%2520it%2520in%2520%2560%2560stone%2527%2527%2520weights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20flexible%20perception%20with%20visual%20memory&entry.906535625=Robert%20Geirhos%20and%20Priyank%20Jaini%20and%20Austin%20Stone%20and%20Sourabh%20Medapati%20and%20Xi%20Yi%20and%20George%20Toderici%20and%20Abhijit%20Ogale%20and%20Jonathon%20Shlens&entry.1292438233=%20%20Training%20a%20neural%20network%20is%20a%20monolithic%20endeavor%2C%20akin%20to%20carving%20knowledge%0Ainto%20stone%3A%20once%20the%20process%20is%20completed%2C%20editing%20the%20knowledge%20in%20a%20network%0Ais%20nearly%20impossible%2C%20since%20all%20information%20is%20distributed%20across%20the%20network%27s%0Aweights.%20We%20here%20explore%20a%20simple%2C%20compelling%20alternative%20by%20marrying%20the%0Arepresentational%20power%20of%20deep%20neural%20networks%20with%20the%20flexibility%20of%20a%0Adatabase.%20Decomposing%20the%20task%20of%20image%20classification%20into%20image%20similarity%0A%28from%20a%20pre-trained%20embedding%29%20and%20search%20%28via%20fast%20nearest%20neighbor%20retrieval%0Afrom%20a%20knowledge%20database%29%2C%20we%20build%20a%20simple%20and%20flexible%20visual%20memory%20that%0Ahas%20the%20following%20key%20capabilities%3A%20%281.%29%20The%20ability%20to%20flexibly%20add%20data%0Aacross%20scales%3A%20from%20individual%20samples%20all%20the%20way%20to%20entire%20classes%20and%0Abillion-scale%20data%3B%20%282.%29%20The%20ability%20to%20remove%20data%20through%20unlearning%20and%0Amemory%20pruning%3B%20%283.%29%20An%20interpretable%20decision-mechanism%20on%20which%20we%20can%0Aintervene%20to%20control%20its%20behavior.%20Taken%20together%2C%20these%20capabilities%0Acomprehensively%20demonstrate%20the%20benefits%20of%20an%20explicit%20visual%20memory.%20We%20hope%0Athat%20it%20might%20contribute%20to%20a%20conversation%20on%20how%20knowledge%20should%20be%0Arepresented%20in%20deep%20vision%20models%20--%20beyond%20carving%20it%20in%20%60%60stone%27%27%20weights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08172v1&entry.124074799=Read"},
{"title": "Treat Stillness with Movement: Remote Sensing Change Detection via\n  Coarse-grained Temporal Foregrounds Mining", "author": "Xixi Wang and Zitian Wang and Jingtao Jiang and Lan Chen and Xiao Wang and Bo Jiang", "abstract": "  Current works focus on addressing the remote sensing change detection task\nusing bi-temporal images. Although good performance can be achieved, however,\nseldom of they consider the motion cues which may also be vital. In this work,\nwe revisit the widely adopted bi-temporal images-based framework and propose a\nnovel Coarse-grained Temporal Mining Augmented (CTMA) framework. To be\nspecific, given the bi-temporal images, we first transform them into a video\nusing interpolation operations. Then, a set of temporal encoders is adopted to\nextract the motion features from the obtained video for coarse-grained changed\nregion prediction. Subsequently, we design a novel Coarse-grained Foregrounds\nAugmented Spatial Encoder module to integrate both global and local\ninformation. We also introduce a motion augmented strategy that leverages\nmotion cues as an additional output to aggregate with the spatial features for\nimproved results. Meanwhile, we feed the input image pairs into the ResNet to\nget the different features and also the spatial blocks for fine-grained feature\nlearning. More importantly, we propose a mask augmented strategy that utilizes\ncoarse-grained changed regions, incorporating them into the decoder blocks to\nenhance the final changed prediction. Extensive experiments conducted on\nmultiple benchmark datasets fully validated the effectiveness of our proposed\nframework for remote sensing image change detection. The source code of this\npaper will be released on\nhttps://github.com/Event-AHU/CTM_Remote_Sensing_Change_Detection\n", "link": "http://arxiv.org/abs/2408.08078v1", "date": "2024-08-15", "relevancy": 2.2047, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.585}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.536}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Treat%20Stillness%20with%20Movement%3A%20Remote%20Sensing%20Change%20Detection%20via%0A%20%20Coarse-grained%20Temporal%20Foregrounds%20Mining&body=Title%3A%20Treat%20Stillness%20with%20Movement%3A%20Remote%20Sensing%20Change%20Detection%20via%0A%20%20Coarse-grained%20Temporal%20Foregrounds%20Mining%0AAuthor%3A%20Xixi%20Wang%20and%20Zitian%20Wang%20and%20Jingtao%20Jiang%20and%20Lan%20Chen%20and%20Xiao%20Wang%20and%20Bo%20Jiang%0AAbstract%3A%20%20%20Current%20works%20focus%20on%20addressing%20the%20remote%20sensing%20change%20detection%20task%0Ausing%20bi-temporal%20images.%20Although%20good%20performance%20can%20be%20achieved%2C%20however%2C%0Aseldom%20of%20they%20consider%20the%20motion%20cues%20which%20may%20also%20be%20vital.%20In%20this%20work%2C%0Awe%20revisit%20the%20widely%20adopted%20bi-temporal%20images-based%20framework%20and%20propose%20a%0Anovel%20Coarse-grained%20Temporal%20Mining%20Augmented%20%28CTMA%29%20framework.%20To%20be%0Aspecific%2C%20given%20the%20bi-temporal%20images%2C%20we%20first%20transform%20them%20into%20a%20video%0Ausing%20interpolation%20operations.%20Then%2C%20a%20set%20of%20temporal%20encoders%20is%20adopted%20to%0Aextract%20the%20motion%20features%20from%20the%20obtained%20video%20for%20coarse-grained%20changed%0Aregion%20prediction.%20Subsequently%2C%20we%20design%20a%20novel%20Coarse-grained%20Foregrounds%0AAugmented%20Spatial%20Encoder%20module%20to%20integrate%20both%20global%20and%20local%0Ainformation.%20We%20also%20introduce%20a%20motion%20augmented%20strategy%20that%20leverages%0Amotion%20cues%20as%20an%20additional%20output%20to%20aggregate%20with%20the%20spatial%20features%20for%0Aimproved%20results.%20Meanwhile%2C%20we%20feed%20the%20input%20image%20pairs%20into%20the%20ResNet%20to%0Aget%20the%20different%20features%20and%20also%20the%20spatial%20blocks%20for%20fine-grained%20feature%0Alearning.%20More%20importantly%2C%20we%20propose%20a%20mask%20augmented%20strategy%20that%20utilizes%0Acoarse-grained%20changed%20regions%2C%20incorporating%20them%20into%20the%20decoder%20blocks%20to%0Aenhance%20the%20final%20changed%20prediction.%20Extensive%20experiments%20conducted%20on%0Amultiple%20benchmark%20datasets%20fully%20validated%20the%20effectiveness%20of%20our%20proposed%0Aframework%20for%20remote%20sensing%20image%20change%20detection.%20The%20source%20code%20of%20this%0Apaper%20will%20be%20released%20on%0Ahttps%3A//github.com/Event-AHU/CTM_Remote_Sensing_Change_Detection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreat%2520Stillness%2520with%2520Movement%253A%2520Remote%2520Sensing%2520Change%2520Detection%2520via%250A%2520%2520Coarse-grained%2520Temporal%2520Foregrounds%2520Mining%26entry.906535625%3DXixi%2520Wang%2520and%2520Zitian%2520Wang%2520and%2520Jingtao%2520Jiang%2520and%2520Lan%2520Chen%2520and%2520Xiao%2520Wang%2520and%2520Bo%2520Jiang%26entry.1292438233%3D%2520%2520Current%2520works%2520focus%2520on%2520addressing%2520the%2520remote%2520sensing%2520change%2520detection%2520task%250Ausing%2520bi-temporal%2520images.%2520Although%2520good%2520performance%2520can%2520be%2520achieved%252C%2520however%252C%250Aseldom%2520of%2520they%2520consider%2520the%2520motion%2520cues%2520which%2520may%2520also%2520be%2520vital.%2520In%2520this%2520work%252C%250Awe%2520revisit%2520the%2520widely%2520adopted%2520bi-temporal%2520images-based%2520framework%2520and%2520propose%2520a%250Anovel%2520Coarse-grained%2520Temporal%2520Mining%2520Augmented%2520%2528CTMA%2529%2520framework.%2520To%2520be%250Aspecific%252C%2520given%2520the%2520bi-temporal%2520images%252C%2520we%2520first%2520transform%2520them%2520into%2520a%2520video%250Ausing%2520interpolation%2520operations.%2520Then%252C%2520a%2520set%2520of%2520temporal%2520encoders%2520is%2520adopted%2520to%250Aextract%2520the%2520motion%2520features%2520from%2520the%2520obtained%2520video%2520for%2520coarse-grained%2520changed%250Aregion%2520prediction.%2520Subsequently%252C%2520we%2520design%2520a%2520novel%2520Coarse-grained%2520Foregrounds%250AAugmented%2520Spatial%2520Encoder%2520module%2520to%2520integrate%2520both%2520global%2520and%2520local%250Ainformation.%2520We%2520also%2520introduce%2520a%2520motion%2520augmented%2520strategy%2520that%2520leverages%250Amotion%2520cues%2520as%2520an%2520additional%2520output%2520to%2520aggregate%2520with%2520the%2520spatial%2520features%2520for%250Aimproved%2520results.%2520Meanwhile%252C%2520we%2520feed%2520the%2520input%2520image%2520pairs%2520into%2520the%2520ResNet%2520to%250Aget%2520the%2520different%2520features%2520and%2520also%2520the%2520spatial%2520blocks%2520for%2520fine-grained%2520feature%250Alearning.%2520More%2520importantly%252C%2520we%2520propose%2520a%2520mask%2520augmented%2520strategy%2520that%2520utilizes%250Acoarse-grained%2520changed%2520regions%252C%2520incorporating%2520them%2520into%2520the%2520decoder%2520blocks%2520to%250Aenhance%2520the%2520final%2520changed%2520prediction.%2520Extensive%2520experiments%2520conducted%2520on%250Amultiple%2520benchmark%2520datasets%2520fully%2520validated%2520the%2520effectiveness%2520of%2520our%2520proposed%250Aframework%2520for%2520remote%2520sensing%2520image%2520change%2520detection.%2520The%2520source%2520code%2520of%2520this%250Apaper%2520will%2520be%2520released%2520on%250Ahttps%253A//github.com/Event-AHU/CTM_Remote_Sensing_Change_Detection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Treat%20Stillness%20with%20Movement%3A%20Remote%20Sensing%20Change%20Detection%20via%0A%20%20Coarse-grained%20Temporal%20Foregrounds%20Mining&entry.906535625=Xixi%20Wang%20and%20Zitian%20Wang%20and%20Jingtao%20Jiang%20and%20Lan%20Chen%20and%20Xiao%20Wang%20and%20Bo%20Jiang&entry.1292438233=%20%20Current%20works%20focus%20on%20addressing%20the%20remote%20sensing%20change%20detection%20task%0Ausing%20bi-temporal%20images.%20Although%20good%20performance%20can%20be%20achieved%2C%20however%2C%0Aseldom%20of%20they%20consider%20the%20motion%20cues%20which%20may%20also%20be%20vital.%20In%20this%20work%2C%0Awe%20revisit%20the%20widely%20adopted%20bi-temporal%20images-based%20framework%20and%20propose%20a%0Anovel%20Coarse-grained%20Temporal%20Mining%20Augmented%20%28CTMA%29%20framework.%20To%20be%0Aspecific%2C%20given%20the%20bi-temporal%20images%2C%20we%20first%20transform%20them%20into%20a%20video%0Ausing%20interpolation%20operations.%20Then%2C%20a%20set%20of%20temporal%20encoders%20is%20adopted%20to%0Aextract%20the%20motion%20features%20from%20the%20obtained%20video%20for%20coarse-grained%20changed%0Aregion%20prediction.%20Subsequently%2C%20we%20design%20a%20novel%20Coarse-grained%20Foregrounds%0AAugmented%20Spatial%20Encoder%20module%20to%20integrate%20both%20global%20and%20local%0Ainformation.%20We%20also%20introduce%20a%20motion%20augmented%20strategy%20that%20leverages%0Amotion%20cues%20as%20an%20additional%20output%20to%20aggregate%20with%20the%20spatial%20features%20for%0Aimproved%20results.%20Meanwhile%2C%20we%20feed%20the%20input%20image%20pairs%20into%20the%20ResNet%20to%0Aget%20the%20different%20features%20and%20also%20the%20spatial%20blocks%20for%20fine-grained%20feature%0Alearning.%20More%20importantly%2C%20we%20propose%20a%20mask%20augmented%20strategy%20that%20utilizes%0Acoarse-grained%20changed%20regions%2C%20incorporating%20them%20into%20the%20decoder%20blocks%20to%0Aenhance%20the%20final%20changed%20prediction.%20Extensive%20experiments%20conducted%20on%0Amultiple%20benchmark%20datasets%20fully%20validated%20the%20effectiveness%20of%20our%20proposed%0Aframework%20for%20remote%20sensing%20image%20change%20detection.%20The%20source%20code%20of%20this%0Apaper%20will%20be%20released%20on%0Ahttps%3A//github.com/Event-AHU/CTM_Remote_Sensing_Change_Detection%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08078v1&entry.124074799=Read"},
{"title": "GSVD-NMF: Recovering Missing Features in Non-negative Matrix\n  Factorization", "author": "Youdong Guo and Timothy E. Holy", "abstract": "  Non-negative matrix factorization (NMF) is an important tool in signal\nprocessing and widely used to separate mixed sources into their components.\nHowever, NMF is NP-hard and thus may fail to discover the ideal factorization;\nmoreover, the number of components may not be known in advance and thus\nfeatures may be missed or incompletely separated. To recover missing components\nfrom under-complete NMF, we introduce GSVD-NMF, which proposes new components\nbased on the generalized singular value decomposition (GSVD) between\npreliminary NMF results and the SVD of the original matrix. Simulation and\nexperimental results demonstrate that GSVD-NMF often recovers missing features\nfrom under-complete NMF and helps NMF achieve better local optima.\n", "link": "http://arxiv.org/abs/2408.08260v1", "date": "2024-08-15", "relevancy": 2.1993, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4634}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4292}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSVD-NMF%3A%20Recovering%20Missing%20Features%20in%20Non-negative%20Matrix%0A%20%20Factorization&body=Title%3A%20GSVD-NMF%3A%20Recovering%20Missing%20Features%20in%20Non-negative%20Matrix%0A%20%20Factorization%0AAuthor%3A%20Youdong%20Guo%20and%20Timothy%20E.%20Holy%0AAbstract%3A%20%20%20Non-negative%20matrix%20factorization%20%28NMF%29%20is%20an%20important%20tool%20in%20signal%0Aprocessing%20and%20widely%20used%20to%20separate%20mixed%20sources%20into%20their%20components.%0AHowever%2C%20NMF%20is%20NP-hard%20and%20thus%20may%20fail%20to%20discover%20the%20ideal%20factorization%3B%0Amoreover%2C%20the%20number%20of%20components%20may%20not%20be%20known%20in%20advance%20and%20thus%0Afeatures%20may%20be%20missed%20or%20incompletely%20separated.%20To%20recover%20missing%20components%0Afrom%20under-complete%20NMF%2C%20we%20introduce%20GSVD-NMF%2C%20which%20proposes%20new%20components%0Abased%20on%20the%20generalized%20singular%20value%20decomposition%20%28GSVD%29%20between%0Apreliminary%20NMF%20results%20and%20the%20SVD%20of%20the%20original%20matrix.%20Simulation%20and%0Aexperimental%20results%20demonstrate%20that%20GSVD-NMF%20often%20recovers%20missing%20features%0Afrom%20under-complete%20NMF%20and%20helps%20NMF%20achieve%20better%20local%20optima.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSVD-NMF%253A%2520Recovering%2520Missing%2520Features%2520in%2520Non-negative%2520Matrix%250A%2520%2520Factorization%26entry.906535625%3DYoudong%2520Guo%2520and%2520Timothy%2520E.%2520Holy%26entry.1292438233%3D%2520%2520Non-negative%2520matrix%2520factorization%2520%2528NMF%2529%2520is%2520an%2520important%2520tool%2520in%2520signal%250Aprocessing%2520and%2520widely%2520used%2520to%2520separate%2520mixed%2520sources%2520into%2520their%2520components.%250AHowever%252C%2520NMF%2520is%2520NP-hard%2520and%2520thus%2520may%2520fail%2520to%2520discover%2520the%2520ideal%2520factorization%253B%250Amoreover%252C%2520the%2520number%2520of%2520components%2520may%2520not%2520be%2520known%2520in%2520advance%2520and%2520thus%250Afeatures%2520may%2520be%2520missed%2520or%2520incompletely%2520separated.%2520To%2520recover%2520missing%2520components%250Afrom%2520under-complete%2520NMF%252C%2520we%2520introduce%2520GSVD-NMF%252C%2520which%2520proposes%2520new%2520components%250Abased%2520on%2520the%2520generalized%2520singular%2520value%2520decomposition%2520%2528GSVD%2529%2520between%250Apreliminary%2520NMF%2520results%2520and%2520the%2520SVD%2520of%2520the%2520original%2520matrix.%2520Simulation%2520and%250Aexperimental%2520results%2520demonstrate%2520that%2520GSVD-NMF%2520often%2520recovers%2520missing%2520features%250Afrom%2520under-complete%2520NMF%2520and%2520helps%2520NMF%2520achieve%2520better%2520local%2520optima.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSVD-NMF%3A%20Recovering%20Missing%20Features%20in%20Non-negative%20Matrix%0A%20%20Factorization&entry.906535625=Youdong%20Guo%20and%20Timothy%20E.%20Holy&entry.1292438233=%20%20Non-negative%20matrix%20factorization%20%28NMF%29%20is%20an%20important%20tool%20in%20signal%0Aprocessing%20and%20widely%20used%20to%20separate%20mixed%20sources%20into%20their%20components.%0AHowever%2C%20NMF%20is%20NP-hard%20and%20thus%20may%20fail%20to%20discover%20the%20ideal%20factorization%3B%0Amoreover%2C%20the%20number%20of%20components%20may%20not%20be%20known%20in%20advance%20and%20thus%0Afeatures%20may%20be%20missed%20or%20incompletely%20separated.%20To%20recover%20missing%20components%0Afrom%20under-complete%20NMF%2C%20we%20introduce%20GSVD-NMF%2C%20which%20proposes%20new%20components%0Abased%20on%20the%20generalized%20singular%20value%20decomposition%20%28GSVD%29%20between%0Apreliminary%20NMF%20results%20and%20the%20SVD%20of%20the%20original%20matrix.%20Simulation%20and%0Aexperimental%20results%20demonstrate%20that%20GSVD-NMF%20often%20recovers%20missing%20features%0Afrom%20under-complete%20NMF%20and%20helps%20NMF%20achieve%20better%20local%20optima.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08260v1&entry.124074799=Read"},
{"title": "Towards Flexible Visual Relationship Segmentation", "author": "Fangrui Zhu and Jianwei Yang and Huaizu Jiang", "abstract": "  Visual relationship understanding has been studied separately in human-object\ninteraction(HOI) detection, scene graph generation(SGG), and referring\nrelationships(RR) tasks. Given the complexity and interconnectedness of these\ntasks, it is crucial to have a flexible framework that can effectively address\nthese tasks in a cohesive manner. In this work, we propose FleVRS, a single\nmodel that seamlessly integrates the above three aspects in standard and\npromptable visual relationship segmentation, and further possesses the\ncapability for open-vocabulary segmentation to adapt to novel scenarios. FleVRS\nleverages the synergy between text and image modalities, to ground various\ntypes of relationships from images and use textual features from\nvision-language models to visual conceptual understanding. Empirical validation\nacross various datasets demonstrates that our framework outperforms existing\nmodels in standard, promptable, and open-vocabulary tasks, e.g., +1.9 $mAP$ on\nHICO-DET, +11.4 $Acc$ on VRD, +4.7 $mAP$ on unseen HICO-DET. Our FleVRS\nrepresents a significant step towards a more intuitive, comprehensive, and\nscalable understanding of visual relationships.\n", "link": "http://arxiv.org/abs/2408.08305v1", "date": "2024-08-15", "relevancy": 2.184, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5523}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5489}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Flexible%20Visual%20Relationship%20Segmentation&body=Title%3A%20Towards%20Flexible%20Visual%20Relationship%20Segmentation%0AAuthor%3A%20Fangrui%20Zhu%20and%20Jianwei%20Yang%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Visual%20relationship%20understanding%20has%20been%20studied%20separately%20in%20human-object%0Ainteraction%28HOI%29%20detection%2C%20scene%20graph%20generation%28SGG%29%2C%20and%20referring%0Arelationships%28RR%29%20tasks.%20Given%20the%20complexity%20and%20interconnectedness%20of%20these%0Atasks%2C%20it%20is%20crucial%20to%20have%20a%20flexible%20framework%20that%20can%20effectively%20address%0Athese%20tasks%20in%20a%20cohesive%20manner.%20In%20this%20work%2C%20we%20propose%20FleVRS%2C%20a%20single%0Amodel%20that%20seamlessly%20integrates%20the%20above%20three%20aspects%20in%20standard%20and%0Apromptable%20visual%20relationship%20segmentation%2C%20and%20further%20possesses%20the%0Acapability%20for%20open-vocabulary%20segmentation%20to%20adapt%20to%20novel%20scenarios.%20FleVRS%0Aleverages%20the%20synergy%20between%20text%20and%20image%20modalities%2C%20to%20ground%20various%0Atypes%20of%20relationships%20from%20images%20and%20use%20textual%20features%20from%0Avision-language%20models%20to%20visual%20conceptual%20understanding.%20Empirical%20validation%0Aacross%20various%20datasets%20demonstrates%20that%20our%20framework%20outperforms%20existing%0Amodels%20in%20standard%2C%20promptable%2C%20and%20open-vocabulary%20tasks%2C%20e.g.%2C%20%2B1.9%20%24mAP%24%20on%0AHICO-DET%2C%20%2B11.4%20%24Acc%24%20on%20VRD%2C%20%2B4.7%20%24mAP%24%20on%20unseen%20HICO-DET.%20Our%20FleVRS%0Arepresents%20a%20significant%20step%20towards%20a%20more%20intuitive%2C%20comprehensive%2C%20and%0Ascalable%20understanding%20of%20visual%20relationships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Flexible%2520Visual%2520Relationship%2520Segmentation%26entry.906535625%3DFangrui%2520Zhu%2520and%2520Jianwei%2520Yang%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3D%2520%2520Visual%2520relationship%2520understanding%2520has%2520been%2520studied%2520separately%2520in%2520human-object%250Ainteraction%2528HOI%2529%2520detection%252C%2520scene%2520graph%2520generation%2528SGG%2529%252C%2520and%2520referring%250Arelationships%2528RR%2529%2520tasks.%2520Given%2520the%2520complexity%2520and%2520interconnectedness%2520of%2520these%250Atasks%252C%2520it%2520is%2520crucial%2520to%2520have%2520a%2520flexible%2520framework%2520that%2520can%2520effectively%2520address%250Athese%2520tasks%2520in%2520a%2520cohesive%2520manner.%2520In%2520this%2520work%252C%2520we%2520propose%2520FleVRS%252C%2520a%2520single%250Amodel%2520that%2520seamlessly%2520integrates%2520the%2520above%2520three%2520aspects%2520in%2520standard%2520and%250Apromptable%2520visual%2520relationship%2520segmentation%252C%2520and%2520further%2520possesses%2520the%250Acapability%2520for%2520open-vocabulary%2520segmentation%2520to%2520adapt%2520to%2520novel%2520scenarios.%2520FleVRS%250Aleverages%2520the%2520synergy%2520between%2520text%2520and%2520image%2520modalities%252C%2520to%2520ground%2520various%250Atypes%2520of%2520relationships%2520from%2520images%2520and%2520use%2520textual%2520features%2520from%250Avision-language%2520models%2520to%2520visual%2520conceptual%2520understanding.%2520Empirical%2520validation%250Aacross%2520various%2520datasets%2520demonstrates%2520that%2520our%2520framework%2520outperforms%2520existing%250Amodels%2520in%2520standard%252C%2520promptable%252C%2520and%2520open-vocabulary%2520tasks%252C%2520e.g.%252C%2520%252B1.9%2520%2524mAP%2524%2520on%250AHICO-DET%252C%2520%252B11.4%2520%2524Acc%2524%2520on%2520VRD%252C%2520%252B4.7%2520%2524mAP%2524%2520on%2520unseen%2520HICO-DET.%2520Our%2520FleVRS%250Arepresents%2520a%2520significant%2520step%2520towards%2520a%2520more%2520intuitive%252C%2520comprehensive%252C%2520and%250Ascalable%2520understanding%2520of%2520visual%2520relationships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Flexible%20Visual%20Relationship%20Segmentation&entry.906535625=Fangrui%20Zhu%20and%20Jianwei%20Yang%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Visual%20relationship%20understanding%20has%20been%20studied%20separately%20in%20human-object%0Ainteraction%28HOI%29%20detection%2C%20scene%20graph%20generation%28SGG%29%2C%20and%20referring%0Arelationships%28RR%29%20tasks.%20Given%20the%20complexity%20and%20interconnectedness%20of%20these%0Atasks%2C%20it%20is%20crucial%20to%20have%20a%20flexible%20framework%20that%20can%20effectively%20address%0Athese%20tasks%20in%20a%20cohesive%20manner.%20In%20this%20work%2C%20we%20propose%20FleVRS%2C%20a%20single%0Amodel%20that%20seamlessly%20integrates%20the%20above%20three%20aspects%20in%20standard%20and%0Apromptable%20visual%20relationship%20segmentation%2C%20and%20further%20possesses%20the%0Acapability%20for%20open-vocabulary%20segmentation%20to%20adapt%20to%20novel%20scenarios.%20FleVRS%0Aleverages%20the%20synergy%20between%20text%20and%20image%20modalities%2C%20to%20ground%20various%0Atypes%20of%20relationships%20from%20images%20and%20use%20textual%20features%20from%0Avision-language%20models%20to%20visual%20conceptual%20understanding.%20Empirical%20validation%0Aacross%20various%20datasets%20demonstrates%20that%20our%20framework%20outperforms%20existing%0Amodels%20in%20standard%2C%20promptable%2C%20and%20open-vocabulary%20tasks%2C%20e.g.%2C%20%2B1.9%20%24mAP%24%20on%0AHICO-DET%2C%20%2B11.4%20%24Acc%24%20on%20VRD%2C%20%2B4.7%20%24mAP%24%20on%20unseen%20HICO-DET.%20Our%20FleVRS%0Arepresents%20a%20significant%20step%20towards%20a%20more%20intuitive%2C%20comprehensive%2C%20and%0Ascalable%20understanding%20of%20visual%20relationships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08305v1&entry.124074799=Read"},
{"title": "Two Completely Parameter-Free Alternating Gradient Projection Algorithms\n  for Nonconvex-(strongly) Concave Minimax Problems", "author": "Junnan Yang and Huiling Zhang and Zi Xu", "abstract": "  Due to their importance in various emerging applications, efficient\nalgorithms for solving minimax problems have recently received increasing\nattention. However, many existing algorithms require prior knowledge of the\nproblem parameters in order to achieve optimal iteration complexity. In this\npaper, we propose two completely parameter-free alternating gradient projection\nalgorithms, i.e., the PF-AGP-NSC algorithm and the PF-AGP-NC algorithm, to\nsolve the smooth nonconvex-strongly concave and nonconvex-concave minimax\nproblems respectively using a backtracking strategy, which does not require\nprior knowledge of parameters such as the Lipschtiz constant $L$ or the\nstrongly concave constant $\\mu$. Moreover, we show that the total number of\ngradient calls of the PF-AGP-NSC algorithm and the PF-AGP-NC algorithm to\nobtain an $\\varepsilon$-stationary point is upper bounded by $\\mathcal{O}\\left(\nL\\kappa^3\\varepsilon^{-2} \\right)$ and $\\mathcal{O}\\left( L^4\\varepsilon^{-4}\n\\right)$ respectively, where $\\kappa$ is the condition number. As far as we\nknow, the PF-AGP-NSC algorithm and the PF-AGP-NC algorithm are the first\ncompletely parameter-free algorithms for solving nonconvex-strongly concave\nminimax problems and nonconvex-concave minimax problems respectively. Numerical\nresults validate the efficiency of the proposed PF-AGP algorithm.\n", "link": "http://arxiv.org/abs/2407.21372v2", "date": "2024-08-15", "relevancy": 2.1639, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4402}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4359}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Completely%20Parameter-Free%20Alternating%20Gradient%20Projection%20Algorithms%0A%20%20for%20Nonconvex-%28strongly%29%20Concave%20Minimax%20Problems&body=Title%3A%20Two%20Completely%20Parameter-Free%20Alternating%20Gradient%20Projection%20Algorithms%0A%20%20for%20Nonconvex-%28strongly%29%20Concave%20Minimax%20Problems%0AAuthor%3A%20Junnan%20Yang%20and%20Huiling%20Zhang%20and%20Zi%20Xu%0AAbstract%3A%20%20%20Due%20to%20their%20importance%20in%20various%20emerging%20applications%2C%20efficient%0Aalgorithms%20for%20solving%20minimax%20problems%20have%20recently%20received%20increasing%0Aattention.%20However%2C%20many%20existing%20algorithms%20require%20prior%20knowledge%20of%20the%0Aproblem%20parameters%20in%20order%20to%20achieve%20optimal%20iteration%20complexity.%20In%20this%0Apaper%2C%20we%20propose%20two%20completely%20parameter-free%20alternating%20gradient%20projection%0Aalgorithms%2C%20i.e.%2C%20the%20PF-AGP-NSC%20algorithm%20and%20the%20PF-AGP-NC%20algorithm%2C%20to%0Asolve%20the%20smooth%20nonconvex-strongly%20concave%20and%20nonconvex-concave%20minimax%0Aproblems%20respectively%20using%20a%20backtracking%20strategy%2C%20which%20does%20not%20require%0Aprior%20knowledge%20of%20parameters%20such%20as%20the%20Lipschtiz%20constant%20%24L%24%20or%20the%0Astrongly%20concave%20constant%20%24%5Cmu%24.%20Moreover%2C%20we%20show%20that%20the%20total%20number%20of%0Agradient%20calls%20of%20the%20PF-AGP-NSC%20algorithm%20and%20the%20PF-AGP-NC%20algorithm%20to%0Aobtain%20an%20%24%5Cvarepsilon%24-stationary%20point%20is%20upper%20bounded%20by%20%24%5Cmathcal%7BO%7D%5Cleft%28%0AL%5Ckappa%5E3%5Cvarepsilon%5E%7B-2%7D%20%5Cright%29%24%20and%20%24%5Cmathcal%7BO%7D%5Cleft%28%20L%5E4%5Cvarepsilon%5E%7B-4%7D%0A%5Cright%29%24%20respectively%2C%20where%20%24%5Ckappa%24%20is%20the%20condition%20number.%20As%20far%20as%20we%0Aknow%2C%20the%20PF-AGP-NSC%20algorithm%20and%20the%20PF-AGP-NC%20algorithm%20are%20the%20first%0Acompletely%20parameter-free%20algorithms%20for%20solving%20nonconvex-strongly%20concave%0Aminimax%20problems%20and%20nonconvex-concave%20minimax%20problems%20respectively.%20Numerical%0Aresults%20validate%20the%20efficiency%20of%20the%20proposed%20PF-AGP%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Completely%2520Parameter-Free%2520Alternating%2520Gradient%2520Projection%2520Algorithms%250A%2520%2520for%2520Nonconvex-%2528strongly%2529%2520Concave%2520Minimax%2520Problems%26entry.906535625%3DJunnan%2520Yang%2520and%2520Huiling%2520Zhang%2520and%2520Zi%2520Xu%26entry.1292438233%3D%2520%2520Due%2520to%2520their%2520importance%2520in%2520various%2520emerging%2520applications%252C%2520efficient%250Aalgorithms%2520for%2520solving%2520minimax%2520problems%2520have%2520recently%2520received%2520increasing%250Aattention.%2520However%252C%2520many%2520existing%2520algorithms%2520require%2520prior%2520knowledge%2520of%2520the%250Aproblem%2520parameters%2520in%2520order%2520to%2520achieve%2520optimal%2520iteration%2520complexity.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520two%2520completely%2520parameter-free%2520alternating%2520gradient%2520projection%250Aalgorithms%252C%2520i.e.%252C%2520the%2520PF-AGP-NSC%2520algorithm%2520and%2520the%2520PF-AGP-NC%2520algorithm%252C%2520to%250Asolve%2520the%2520smooth%2520nonconvex-strongly%2520concave%2520and%2520nonconvex-concave%2520minimax%250Aproblems%2520respectively%2520using%2520a%2520backtracking%2520strategy%252C%2520which%2520does%2520not%2520require%250Aprior%2520knowledge%2520of%2520parameters%2520such%2520as%2520the%2520Lipschtiz%2520constant%2520%2524L%2524%2520or%2520the%250Astrongly%2520concave%2520constant%2520%2524%255Cmu%2524.%2520Moreover%252C%2520we%2520show%2520that%2520the%2520total%2520number%2520of%250Agradient%2520calls%2520of%2520the%2520PF-AGP-NSC%2520algorithm%2520and%2520the%2520PF-AGP-NC%2520algorithm%2520to%250Aobtain%2520an%2520%2524%255Cvarepsilon%2524-stationary%2520point%2520is%2520upper%2520bounded%2520by%2520%2524%255Cmathcal%257BO%257D%255Cleft%2528%250AL%255Ckappa%255E3%255Cvarepsilon%255E%257B-2%257D%2520%255Cright%2529%2524%2520and%2520%2524%255Cmathcal%257BO%257D%255Cleft%2528%2520L%255E4%255Cvarepsilon%255E%257B-4%257D%250A%255Cright%2529%2524%2520respectively%252C%2520where%2520%2524%255Ckappa%2524%2520is%2520the%2520condition%2520number.%2520As%2520far%2520as%2520we%250Aknow%252C%2520the%2520PF-AGP-NSC%2520algorithm%2520and%2520the%2520PF-AGP-NC%2520algorithm%2520are%2520the%2520first%250Acompletely%2520parameter-free%2520algorithms%2520for%2520solving%2520nonconvex-strongly%2520concave%250Aminimax%2520problems%2520and%2520nonconvex-concave%2520minimax%2520problems%2520respectively.%2520Numerical%250Aresults%2520validate%2520the%2520efficiency%2520of%2520the%2520proposed%2520PF-AGP%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Completely%20Parameter-Free%20Alternating%20Gradient%20Projection%20Algorithms%0A%20%20for%20Nonconvex-%28strongly%29%20Concave%20Minimax%20Problems&entry.906535625=Junnan%20Yang%20and%20Huiling%20Zhang%20and%20Zi%20Xu&entry.1292438233=%20%20Due%20to%20their%20importance%20in%20various%20emerging%20applications%2C%20efficient%0Aalgorithms%20for%20solving%20minimax%20problems%20have%20recently%20received%20increasing%0Aattention.%20However%2C%20many%20existing%20algorithms%20require%20prior%20knowledge%20of%20the%0Aproblem%20parameters%20in%20order%20to%20achieve%20optimal%20iteration%20complexity.%20In%20this%0Apaper%2C%20we%20propose%20two%20completely%20parameter-free%20alternating%20gradient%20projection%0Aalgorithms%2C%20i.e.%2C%20the%20PF-AGP-NSC%20algorithm%20and%20the%20PF-AGP-NC%20algorithm%2C%20to%0Asolve%20the%20smooth%20nonconvex-strongly%20concave%20and%20nonconvex-concave%20minimax%0Aproblems%20respectively%20using%20a%20backtracking%20strategy%2C%20which%20does%20not%20require%0Aprior%20knowledge%20of%20parameters%20such%20as%20the%20Lipschtiz%20constant%20%24L%24%20or%20the%0Astrongly%20concave%20constant%20%24%5Cmu%24.%20Moreover%2C%20we%20show%20that%20the%20total%20number%20of%0Agradient%20calls%20of%20the%20PF-AGP-NSC%20algorithm%20and%20the%20PF-AGP-NC%20algorithm%20to%0Aobtain%20an%20%24%5Cvarepsilon%24-stationary%20point%20is%20upper%20bounded%20by%20%24%5Cmathcal%7BO%7D%5Cleft%28%0AL%5Ckappa%5E3%5Cvarepsilon%5E%7B-2%7D%20%5Cright%29%24%20and%20%24%5Cmathcal%7BO%7D%5Cleft%28%20L%5E4%5Cvarepsilon%5E%7B-4%7D%0A%5Cright%29%24%20respectively%2C%20where%20%24%5Ckappa%24%20is%20the%20condition%20number.%20As%20far%20as%20we%0Aknow%2C%20the%20PF-AGP-NSC%20algorithm%20and%20the%20PF-AGP-NC%20algorithm%20are%20the%20first%0Acompletely%20parameter-free%20algorithms%20for%20solving%20nonconvex-strongly%20concave%0Aminimax%20problems%20and%20nonconvex-concave%20minimax%20problems%20respectively.%20Numerical%0Aresults%20validate%20the%20efficiency%20of%20the%20proposed%20PF-AGP%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21372v2&entry.124074799=Read"},
{"title": "EgoPoseFormer: A Simple Baseline for Stereo Egocentric 3D Human Pose\n  Estimation", "author": "Chenhongyi Yang and Anastasia Tkach and Shreyas Hampali and Linguang Zhang and Elliot J. Crowley and Cem Keskin", "abstract": "  We present EgoPoseFormer, a simple yet effective transformer-based model for\nstereo egocentric human pose estimation. The main challenge in egocentric pose\nestimation is overcoming joint invisibility, which is caused by self-occlusion\nor a limited field of view (FOV) of head-mounted cameras. Our approach\novercomes this challenge by incorporating a two-stage pose estimation paradigm:\nin the first stage, our model leverages the global information to estimate each\njoint's coarse location, then in the second stage, it employs a DETR style\ntransformer to refine the coarse locations by exploiting fine-grained stereo\nvisual features. In addition, we present a Deformable Stereo Attention\noperation to enable our transformer to effectively process multi-view features,\nwhich enables it to accurately localize each joint in the 3D world. We evaluate\nour method on the stereo UnrealEgo dataset and show it significantly\noutperforms previous approaches while being computationally efficient: it\nimproves MPJPE by 27.4mm (45% improvement) with only 7.9% model parameters and\n13.1% FLOPs compared to the state-of-the-art. Surprisingly, with proper\ntraining settings, we find that even our first-stage pose proposal network can\nachieve superior performance compared to previous arts. We also show that our\nmethod can be seamlessly extended to monocular settings, which achieves\nstate-of-the-art performance on the SceneEgo dataset, improving MPJPE by 25.5mm\n(21% improvement) compared to the best existing method with only 60.7% model\nparameters and 36.4% FLOPs. Code is available at:\nhttps://github.com/ChenhongyiYang/egoposeformer .\n", "link": "http://arxiv.org/abs/2403.18080v2", "date": "2024-08-15", "relevancy": 2.1516, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5459}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5408}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoPoseFormer%3A%20A%20Simple%20Baseline%20for%20Stereo%20Egocentric%203D%20Human%20Pose%0A%20%20Estimation&body=Title%3A%20EgoPoseFormer%3A%20A%20Simple%20Baseline%20for%20Stereo%20Egocentric%203D%20Human%20Pose%0A%20%20Estimation%0AAuthor%3A%20Chenhongyi%20Yang%20and%20Anastasia%20Tkach%20and%20Shreyas%20Hampali%20and%20Linguang%20Zhang%20and%20Elliot%20J.%20Crowley%20and%20Cem%20Keskin%0AAbstract%3A%20%20%20We%20present%20EgoPoseFormer%2C%20a%20simple%20yet%20effective%20transformer-based%20model%20for%0Astereo%20egocentric%20human%20pose%20estimation.%20The%20main%20challenge%20in%20egocentric%20pose%0Aestimation%20is%20overcoming%20joint%20invisibility%2C%20which%20is%20caused%20by%20self-occlusion%0Aor%20a%20limited%20field%20of%20view%20%28FOV%29%20of%20head-mounted%20cameras.%20Our%20approach%0Aovercomes%20this%20challenge%20by%20incorporating%20a%20two-stage%20pose%20estimation%20paradigm%3A%0Ain%20the%20first%20stage%2C%20our%20model%20leverages%20the%20global%20information%20to%20estimate%20each%0Ajoint%27s%20coarse%20location%2C%20then%20in%20the%20second%20stage%2C%20it%20employs%20a%20DETR%20style%0Atransformer%20to%20refine%20the%20coarse%20locations%20by%20exploiting%20fine-grained%20stereo%0Avisual%20features.%20In%20addition%2C%20we%20present%20a%20Deformable%20Stereo%20Attention%0Aoperation%20to%20enable%20our%20transformer%20to%20effectively%20process%20multi-view%20features%2C%0Awhich%20enables%20it%20to%20accurately%20localize%20each%20joint%20in%20the%203D%20world.%20We%20evaluate%0Aour%20method%20on%20the%20stereo%20UnrealEgo%20dataset%20and%20show%20it%20significantly%0Aoutperforms%20previous%20approaches%20while%20being%20computationally%20efficient%3A%20it%0Aimproves%20MPJPE%20by%2027.4mm%20%2845%25%20improvement%29%20with%20only%207.9%25%20model%20parameters%20and%0A13.1%25%20FLOPs%20compared%20to%20the%20state-of-the-art.%20Surprisingly%2C%20with%20proper%0Atraining%20settings%2C%20we%20find%20that%20even%20our%20first-stage%20pose%20proposal%20network%20can%0Aachieve%20superior%20performance%20compared%20to%20previous%20arts.%20We%20also%20show%20that%20our%0Amethod%20can%20be%20seamlessly%20extended%20to%20monocular%20settings%2C%20which%20achieves%0Astate-of-the-art%20performance%20on%20the%20SceneEgo%20dataset%2C%20improving%20MPJPE%20by%2025.5mm%0A%2821%25%20improvement%29%20compared%20to%20the%20best%20existing%20method%20with%20only%2060.7%25%20model%0Aparameters%20and%2036.4%25%20FLOPs.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ChenhongyiYang/egoposeformer%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoPoseFormer%253A%2520A%2520Simple%2520Baseline%2520for%2520Stereo%2520Egocentric%25203D%2520Human%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DChenhongyi%2520Yang%2520and%2520Anastasia%2520Tkach%2520and%2520Shreyas%2520Hampali%2520and%2520Linguang%2520Zhang%2520and%2520Elliot%2520J.%2520Crowley%2520and%2520Cem%2520Keskin%26entry.1292438233%3D%2520%2520We%2520present%2520EgoPoseFormer%252C%2520a%2520simple%2520yet%2520effective%2520transformer-based%2520model%2520for%250Astereo%2520egocentric%2520human%2520pose%2520estimation.%2520The%2520main%2520challenge%2520in%2520egocentric%2520pose%250Aestimation%2520is%2520overcoming%2520joint%2520invisibility%252C%2520which%2520is%2520caused%2520by%2520self-occlusion%250Aor%2520a%2520limited%2520field%2520of%2520view%2520%2528FOV%2529%2520of%2520head-mounted%2520cameras.%2520Our%2520approach%250Aovercomes%2520this%2520challenge%2520by%2520incorporating%2520a%2520two-stage%2520pose%2520estimation%2520paradigm%253A%250Ain%2520the%2520first%2520stage%252C%2520our%2520model%2520leverages%2520the%2520global%2520information%2520to%2520estimate%2520each%250Ajoint%2527s%2520coarse%2520location%252C%2520then%2520in%2520the%2520second%2520stage%252C%2520it%2520employs%2520a%2520DETR%2520style%250Atransformer%2520to%2520refine%2520the%2520coarse%2520locations%2520by%2520exploiting%2520fine-grained%2520stereo%250Avisual%2520features.%2520In%2520addition%252C%2520we%2520present%2520a%2520Deformable%2520Stereo%2520Attention%250Aoperation%2520to%2520enable%2520our%2520transformer%2520to%2520effectively%2520process%2520multi-view%2520features%252C%250Awhich%2520enables%2520it%2520to%2520accurately%2520localize%2520each%2520joint%2520in%2520the%25203D%2520world.%2520We%2520evaluate%250Aour%2520method%2520on%2520the%2520stereo%2520UnrealEgo%2520dataset%2520and%2520show%2520it%2520significantly%250Aoutperforms%2520previous%2520approaches%2520while%2520being%2520computationally%2520efficient%253A%2520it%250Aimproves%2520MPJPE%2520by%252027.4mm%2520%252845%2525%2520improvement%2529%2520with%2520only%25207.9%2525%2520model%2520parameters%2520and%250A13.1%2525%2520FLOPs%2520compared%2520to%2520the%2520state-of-the-art.%2520Surprisingly%252C%2520with%2520proper%250Atraining%2520settings%252C%2520we%2520find%2520that%2520even%2520our%2520first-stage%2520pose%2520proposal%2520network%2520can%250Aachieve%2520superior%2520performance%2520compared%2520to%2520previous%2520arts.%2520We%2520also%2520show%2520that%2520our%250Amethod%2520can%2520be%2520seamlessly%2520extended%2520to%2520monocular%2520settings%252C%2520which%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520SceneEgo%2520dataset%252C%2520improving%2520MPJPE%2520by%252025.5mm%250A%252821%2525%2520improvement%2529%2520compared%2520to%2520the%2520best%2520existing%2520method%2520with%2520only%252060.7%2525%2520model%250Aparameters%2520and%252036.4%2525%2520FLOPs.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ChenhongyiYang/egoposeformer%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoPoseFormer%3A%20A%20Simple%20Baseline%20for%20Stereo%20Egocentric%203D%20Human%20Pose%0A%20%20Estimation&entry.906535625=Chenhongyi%20Yang%20and%20Anastasia%20Tkach%20and%20Shreyas%20Hampali%20and%20Linguang%20Zhang%20and%20Elliot%20J.%20Crowley%20and%20Cem%20Keskin&entry.1292438233=%20%20We%20present%20EgoPoseFormer%2C%20a%20simple%20yet%20effective%20transformer-based%20model%20for%0Astereo%20egocentric%20human%20pose%20estimation.%20The%20main%20challenge%20in%20egocentric%20pose%0Aestimation%20is%20overcoming%20joint%20invisibility%2C%20which%20is%20caused%20by%20self-occlusion%0Aor%20a%20limited%20field%20of%20view%20%28FOV%29%20of%20head-mounted%20cameras.%20Our%20approach%0Aovercomes%20this%20challenge%20by%20incorporating%20a%20two-stage%20pose%20estimation%20paradigm%3A%0Ain%20the%20first%20stage%2C%20our%20model%20leverages%20the%20global%20information%20to%20estimate%20each%0Ajoint%27s%20coarse%20location%2C%20then%20in%20the%20second%20stage%2C%20it%20employs%20a%20DETR%20style%0Atransformer%20to%20refine%20the%20coarse%20locations%20by%20exploiting%20fine-grained%20stereo%0Avisual%20features.%20In%20addition%2C%20we%20present%20a%20Deformable%20Stereo%20Attention%0Aoperation%20to%20enable%20our%20transformer%20to%20effectively%20process%20multi-view%20features%2C%0Awhich%20enables%20it%20to%20accurately%20localize%20each%20joint%20in%20the%203D%20world.%20We%20evaluate%0Aour%20method%20on%20the%20stereo%20UnrealEgo%20dataset%20and%20show%20it%20significantly%0Aoutperforms%20previous%20approaches%20while%20being%20computationally%20efficient%3A%20it%0Aimproves%20MPJPE%20by%2027.4mm%20%2845%25%20improvement%29%20with%20only%207.9%25%20model%20parameters%20and%0A13.1%25%20FLOPs%20compared%20to%20the%20state-of-the-art.%20Surprisingly%2C%20with%20proper%0Atraining%20settings%2C%20we%20find%20that%20even%20our%20first-stage%20pose%20proposal%20network%20can%0Aachieve%20superior%20performance%20compared%20to%20previous%20arts.%20We%20also%20show%20that%20our%0Amethod%20can%20be%20seamlessly%20extended%20to%20monocular%20settings%2C%20which%20achieves%0Astate-of-the-art%20performance%20on%20the%20SceneEgo%20dataset%2C%20improving%20MPJPE%20by%2025.5mm%0A%2821%25%20improvement%29%20compared%20to%20the%20best%20existing%20method%20with%20only%2060.7%25%20model%0Aparameters%20and%2036.4%25%20FLOPs.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ChenhongyiYang/egoposeformer%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18080v2&entry.124074799=Read"},
{"title": "A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual\n  Deepfake Detection", "author": "Kyungbok Lee and You Zhang and Zhiyao Duan", "abstract": "  This paper addresses the challenge of developing a robust audio-visual\ndeepfake detection model. In practical use cases, new generation algorithms are\ncontinually emerging, and these algorithms are not encountered during the\ndevelopment of detection methods. This calls for the generalization ability of\nthe method. Additionally, to ensure the credibility of detection methods, it is\nbeneficial for the model to interpret which cues from the video indicate it is\nfake. Motivated by these considerations, we then propose a multi-stream fusion\napproach with one-class learning as a representation-level regularization\ntechnique. We study the generalization problem of audio-visual deepfake\ndetection by creating a new benchmark by extending and re-splitting the\nexisting FakeAVCeleb dataset. The benchmark contains four categories of fake\nvideos (Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,\nand Unsynchronized videos). The experimental results demonstrate that our\napproach surpasses the previous models by a large margin. Furthermore, our\nproposed framework offers interpretability, indicating which modality the model\nidentifies as more likely to be fake. The source code is released at\nhttps://github.com/bok-bok/MSOC.\n", "link": "http://arxiv.org/abs/2406.14176v2", "date": "2024-08-15", "relevancy": 2.1428, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5427}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.535}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection&body=Title%3A%20A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection%0AAuthor%3A%20Kyungbok%20Lee%20and%20You%20Zhang%20and%20Zhiyao%20Duan%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20developing%20a%20robust%20audio-visual%0Adeepfake%20detection%20model.%20In%20practical%20use%20cases%2C%20new%20generation%20algorithms%20are%0Acontinually%20emerging%2C%20and%20these%20algorithms%20are%20not%20encountered%20during%20the%0Adevelopment%20of%20detection%20methods.%20This%20calls%20for%20the%20generalization%20ability%20of%0Athe%20method.%20Additionally%2C%20to%20ensure%20the%20credibility%20of%20detection%20methods%2C%20it%20is%0Abeneficial%20for%20the%20model%20to%20interpret%20which%20cues%20from%20the%20video%20indicate%20it%20is%0Afake.%20Motivated%20by%20these%20considerations%2C%20we%20then%20propose%20a%20multi-stream%20fusion%0Aapproach%20with%20one-class%20learning%20as%20a%20representation-level%20regularization%0Atechnique.%20We%20study%20the%20generalization%20problem%20of%20audio-visual%20deepfake%0Adetection%20by%20creating%20a%20new%20benchmark%20by%20extending%20and%20re-splitting%20the%0Aexisting%20FakeAVCeleb%20dataset.%20The%20benchmark%20contains%20four%20categories%20of%20fake%0Avideos%20%28Real%20Audio-Fake%20Visual%2C%20Fake%20Audio-Fake%20Visual%2C%20Fake%20Audio-Real%20Visual%2C%0Aand%20Unsynchronized%20videos%29.%20The%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20surpasses%20the%20previous%20models%20by%20a%20large%20margin.%20Furthermore%2C%20our%0Aproposed%20framework%20offers%20interpretability%2C%20indicating%20which%20modality%20the%20model%0Aidentifies%20as%20more%20likely%20to%20be%20fake.%20The%20source%20code%20is%20released%20at%0Ahttps%3A//github.com/bok-bok/MSOC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Stream%2520Fusion%2520Approach%2520with%2520One-Class%2520Learning%2520for%2520Audio-Visual%250A%2520%2520Deepfake%2520Detection%26entry.906535625%3DKyungbok%2520Lee%2520and%2520You%2520Zhang%2520and%2520Zhiyao%2520Duan%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520developing%2520a%2520robust%2520audio-visual%250Adeepfake%2520detection%2520model.%2520In%2520practical%2520use%2520cases%252C%2520new%2520generation%2520algorithms%2520are%250Acontinually%2520emerging%252C%2520and%2520these%2520algorithms%2520are%2520not%2520encountered%2520during%2520the%250Adevelopment%2520of%2520detection%2520methods.%2520This%2520calls%2520for%2520the%2520generalization%2520ability%2520of%250Athe%2520method.%2520Additionally%252C%2520to%2520ensure%2520the%2520credibility%2520of%2520detection%2520methods%252C%2520it%2520is%250Abeneficial%2520for%2520the%2520model%2520to%2520interpret%2520which%2520cues%2520from%2520the%2520video%2520indicate%2520it%2520is%250Afake.%2520Motivated%2520by%2520these%2520considerations%252C%2520we%2520then%2520propose%2520a%2520multi-stream%2520fusion%250Aapproach%2520with%2520one-class%2520learning%2520as%2520a%2520representation-level%2520regularization%250Atechnique.%2520We%2520study%2520the%2520generalization%2520problem%2520of%2520audio-visual%2520deepfake%250Adetection%2520by%2520creating%2520a%2520new%2520benchmark%2520by%2520extending%2520and%2520re-splitting%2520the%250Aexisting%2520FakeAVCeleb%2520dataset.%2520The%2520benchmark%2520contains%2520four%2520categories%2520of%2520fake%250Avideos%2520%2528Real%2520Audio-Fake%2520Visual%252C%2520Fake%2520Audio-Fake%2520Visual%252C%2520Fake%2520Audio-Real%2520Visual%252C%250Aand%2520Unsynchronized%2520videos%2529.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%250Aapproach%2520surpasses%2520the%2520previous%2520models%2520by%2520a%2520large%2520margin.%2520Furthermore%252C%2520our%250Aproposed%2520framework%2520offers%2520interpretability%252C%2520indicating%2520which%2520modality%2520the%2520model%250Aidentifies%2520as%2520more%2520likely%2520to%2520be%2520fake.%2520The%2520source%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/bok-bok/MSOC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Stream%20Fusion%20Approach%20with%20One-Class%20Learning%20for%20Audio-Visual%0A%20%20Deepfake%20Detection&entry.906535625=Kyungbok%20Lee%20and%20You%20Zhang%20and%20Zhiyao%20Duan&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20developing%20a%20robust%20audio-visual%0Adeepfake%20detection%20model.%20In%20practical%20use%20cases%2C%20new%20generation%20algorithms%20are%0Acontinually%20emerging%2C%20and%20these%20algorithms%20are%20not%20encountered%20during%20the%0Adevelopment%20of%20detection%20methods.%20This%20calls%20for%20the%20generalization%20ability%20of%0Athe%20method.%20Additionally%2C%20to%20ensure%20the%20credibility%20of%20detection%20methods%2C%20it%20is%0Abeneficial%20for%20the%20model%20to%20interpret%20which%20cues%20from%20the%20video%20indicate%20it%20is%0Afake.%20Motivated%20by%20these%20considerations%2C%20we%20then%20propose%20a%20multi-stream%20fusion%0Aapproach%20with%20one-class%20learning%20as%20a%20representation-level%20regularization%0Atechnique.%20We%20study%20the%20generalization%20problem%20of%20audio-visual%20deepfake%0Adetection%20by%20creating%20a%20new%20benchmark%20by%20extending%20and%20re-splitting%20the%0Aexisting%20FakeAVCeleb%20dataset.%20The%20benchmark%20contains%20four%20categories%20of%20fake%0Avideos%20%28Real%20Audio-Fake%20Visual%2C%20Fake%20Audio-Fake%20Visual%2C%20Fake%20Audio-Real%20Visual%2C%0Aand%20Unsynchronized%20videos%29.%20The%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20surpasses%20the%20previous%20models%20by%20a%20large%20margin.%20Furthermore%2C%20our%0Aproposed%20framework%20offers%20interpretability%2C%20indicating%20which%20modality%20the%20model%0Aidentifies%20as%20more%20likely%20to%20be%20fake.%20The%20source%20code%20is%20released%20at%0Ahttps%3A//github.com/bok-bok/MSOC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14176v2&entry.124074799=Read"},
{"title": "ColorMamba: Towards High-quality NIR-to-RGB Spectral Translation with\n  Mamba", "author": "Huiyu Zhai and Guang Jin and Xingxing Yang and Guosheng Kang", "abstract": "  Translating NIR to the visible spectrum is challenging due to cross-domain\ncomplexities. Current models struggle to balance a broad receptive field with\ncomputational efficiency, limiting practical use. Although the Selective\nStructured State Space Model, especially the improved version, Mamba, excels in\ngenerative tasks by capturing long-range dependencies with linear complexity,\nits default approach of converting 2D images into 1D sequences neglects local\ncontext. In this work, we propose a simple but effective backbone, dubbed\nColorMamba, which first introduces Mamba into spectral translation tasks. To\nexplore global long-range dependencies and local context for efficient spectral\ntranslation, we introduce learnable padding tokens to enhance the distinction\nof image boundaries and prevent potential confusion within the sequence model.\nFurthermore, local convolutional enhancement and agent attention are designed\nto improve the vanilla Mamba. Moreover, we exploit the HSV color to provide\nmulti-scale guidance in the reconstruction process for more accurate spectral\ntranslation. Extensive experiments show that our ColorMamba achieves a 1.02\nimprovement in terms of PSNR compared with the state-of-the-art method. Our\ncode is available at https://github.com/AlexYangxx/ColorMamba.\n", "link": "http://arxiv.org/abs/2408.08087v1", "date": "2024-08-15", "relevancy": 2.1428, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5494}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.527}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ColorMamba%3A%20Towards%20High-quality%20NIR-to-RGB%20Spectral%20Translation%20with%0A%20%20Mamba&body=Title%3A%20ColorMamba%3A%20Towards%20High-quality%20NIR-to-RGB%20Spectral%20Translation%20with%0A%20%20Mamba%0AAuthor%3A%20Huiyu%20Zhai%20and%20Guang%20Jin%20and%20Xingxing%20Yang%20and%20Guosheng%20Kang%0AAbstract%3A%20%20%20Translating%20NIR%20to%20the%20visible%20spectrum%20is%20challenging%20due%20to%20cross-domain%0Acomplexities.%20Current%20models%20struggle%20to%20balance%20a%20broad%20receptive%20field%20with%0Acomputational%20efficiency%2C%20limiting%20practical%20use.%20Although%20the%20Selective%0AStructured%20State%20Space%20Model%2C%20especially%20the%20improved%20version%2C%20Mamba%2C%20excels%20in%0Agenerative%20tasks%20by%20capturing%20long-range%20dependencies%20with%20linear%20complexity%2C%0Aits%20default%20approach%20of%20converting%202D%20images%20into%201D%20sequences%20neglects%20local%0Acontext.%20In%20this%20work%2C%20we%20propose%20a%20simple%20but%20effective%20backbone%2C%20dubbed%0AColorMamba%2C%20which%20first%20introduces%20Mamba%20into%20spectral%20translation%20tasks.%20To%0Aexplore%20global%20long-range%20dependencies%20and%20local%20context%20for%20efficient%20spectral%0Atranslation%2C%20we%20introduce%20learnable%20padding%20tokens%20to%20enhance%20the%20distinction%0Aof%20image%20boundaries%20and%20prevent%20potential%20confusion%20within%20the%20sequence%20model.%0AFurthermore%2C%20local%20convolutional%20enhancement%20and%20agent%20attention%20are%20designed%0Ato%20improve%20the%20vanilla%20Mamba.%20Moreover%2C%20we%20exploit%20the%20HSV%20color%20to%20provide%0Amulti-scale%20guidance%20in%20the%20reconstruction%20process%20for%20more%20accurate%20spectral%0Atranslation.%20Extensive%20experiments%20show%20that%20our%20ColorMamba%20achieves%20a%201.02%0Aimprovement%20in%20terms%20of%20PSNR%20compared%20with%20the%20state-of-the-art%20method.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/AlexYangxx/ColorMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColorMamba%253A%2520Towards%2520High-quality%2520NIR-to-RGB%2520Spectral%2520Translation%2520with%250A%2520%2520Mamba%26entry.906535625%3DHuiyu%2520Zhai%2520and%2520Guang%2520Jin%2520and%2520Xingxing%2520Yang%2520and%2520Guosheng%2520Kang%26entry.1292438233%3D%2520%2520Translating%2520NIR%2520to%2520the%2520visible%2520spectrum%2520is%2520challenging%2520due%2520to%2520cross-domain%250Acomplexities.%2520Current%2520models%2520struggle%2520to%2520balance%2520a%2520broad%2520receptive%2520field%2520with%250Acomputational%2520efficiency%252C%2520limiting%2520practical%2520use.%2520Although%2520the%2520Selective%250AStructured%2520State%2520Space%2520Model%252C%2520especially%2520the%2520improved%2520version%252C%2520Mamba%252C%2520excels%2520in%250Agenerative%2520tasks%2520by%2520capturing%2520long-range%2520dependencies%2520with%2520linear%2520complexity%252C%250Aits%2520default%2520approach%2520of%2520converting%25202D%2520images%2520into%25201D%2520sequences%2520neglects%2520local%250Acontext.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520but%2520effective%2520backbone%252C%2520dubbed%250AColorMamba%252C%2520which%2520first%2520introduces%2520Mamba%2520into%2520spectral%2520translation%2520tasks.%2520To%250Aexplore%2520global%2520long-range%2520dependencies%2520and%2520local%2520context%2520for%2520efficient%2520spectral%250Atranslation%252C%2520we%2520introduce%2520learnable%2520padding%2520tokens%2520to%2520enhance%2520the%2520distinction%250Aof%2520image%2520boundaries%2520and%2520prevent%2520potential%2520confusion%2520within%2520the%2520sequence%2520model.%250AFurthermore%252C%2520local%2520convolutional%2520enhancement%2520and%2520agent%2520attention%2520are%2520designed%250Ato%2520improve%2520the%2520vanilla%2520Mamba.%2520Moreover%252C%2520we%2520exploit%2520the%2520HSV%2520color%2520to%2520provide%250Amulti-scale%2520guidance%2520in%2520the%2520reconstruction%2520process%2520for%2520more%2520accurate%2520spectral%250Atranslation.%2520Extensive%2520experiments%2520show%2520that%2520our%2520ColorMamba%2520achieves%2520a%25201.02%250Aimprovement%2520in%2520terms%2520of%2520PSNR%2520compared%2520with%2520the%2520state-of-the-art%2520method.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/AlexYangxx/ColorMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ColorMamba%3A%20Towards%20High-quality%20NIR-to-RGB%20Spectral%20Translation%20with%0A%20%20Mamba&entry.906535625=Huiyu%20Zhai%20and%20Guang%20Jin%20and%20Xingxing%20Yang%20and%20Guosheng%20Kang&entry.1292438233=%20%20Translating%20NIR%20to%20the%20visible%20spectrum%20is%20challenging%20due%20to%20cross-domain%0Acomplexities.%20Current%20models%20struggle%20to%20balance%20a%20broad%20receptive%20field%20with%0Acomputational%20efficiency%2C%20limiting%20practical%20use.%20Although%20the%20Selective%0AStructured%20State%20Space%20Model%2C%20especially%20the%20improved%20version%2C%20Mamba%2C%20excels%20in%0Agenerative%20tasks%20by%20capturing%20long-range%20dependencies%20with%20linear%20complexity%2C%0Aits%20default%20approach%20of%20converting%202D%20images%20into%201D%20sequences%20neglects%20local%0Acontext.%20In%20this%20work%2C%20we%20propose%20a%20simple%20but%20effective%20backbone%2C%20dubbed%0AColorMamba%2C%20which%20first%20introduces%20Mamba%20into%20spectral%20translation%20tasks.%20To%0Aexplore%20global%20long-range%20dependencies%20and%20local%20context%20for%20efficient%20spectral%0Atranslation%2C%20we%20introduce%20learnable%20padding%20tokens%20to%20enhance%20the%20distinction%0Aof%20image%20boundaries%20and%20prevent%20potential%20confusion%20within%20the%20sequence%20model.%0AFurthermore%2C%20local%20convolutional%20enhancement%20and%20agent%20attention%20are%20designed%0Ato%20improve%20the%20vanilla%20Mamba.%20Moreover%2C%20we%20exploit%20the%20HSV%20color%20to%20provide%0Amulti-scale%20guidance%20in%20the%20reconstruction%20process%20for%20more%20accurate%20spectral%0Atranslation.%20Extensive%20experiments%20show%20that%20our%20ColorMamba%20achieves%20a%201.02%0Aimprovement%20in%20terms%20of%20PSNR%20compared%20with%20the%20state-of-the-art%20method.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/AlexYangxx/ColorMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08087v1&entry.124074799=Read"},
{"title": "HeightLane: BEV Heightmap guided 3D Lane Detection", "author": "Chaesong Park and Eunbin Seo and Jongwoo Lim", "abstract": "  Accurate 3D lane detection from monocular images presents significant\nchallenges due to depth ambiguity and imperfect ground modeling. Previous\nattempts to model the ground have often used a planar ground assumption with\nlimited degrees of freedom, making them unsuitable for complex road\nenvironments with varying slopes. Our study introduces HeightLane, an\ninnovative method that predicts a height map from monocular images by creating\nanchors based on a multi-slope assumption. This approach provides a detailed\nand accurate representation of the ground. HeightLane employs the predicted\nheightmap along with a deformable attention-based spatial feature transform\nframework to efficiently convert 2D image features into 3D bird's eye view\n(BEV) features, enhancing spatial understanding and lane structure recognition.\nAdditionally, the heightmap is used for the positional encoding of BEV\nfeatures, further improving their spatial accuracy. This explicit view\ntransformation bridges the gap between front-view perceptions and spatially\naccurate BEV representations, significantly improving detection performance. To\naddress the lack of the necessary ground truth (GT) height map in the original\nOpenLane dataset, we leverage the Waymo dataset and accumulate its LiDAR data\nto generate a height map for the drivable area of each scene. The GT heightmaps\nare used to train the heightmap extraction module from monocular images.\nExtensive experiments on the OpenLane validation set show that HeightLane\nachieves state-of-the-art performance in terms of F-score, highlighting its\npotential in real-world applications.\n", "link": "http://arxiv.org/abs/2408.08270v1", "date": "2024-08-15", "relevancy": 2.133, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.535}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeightLane%3A%20BEV%20Heightmap%20guided%203D%20Lane%20Detection&body=Title%3A%20HeightLane%3A%20BEV%20Heightmap%20guided%203D%20Lane%20Detection%0AAuthor%3A%20Chaesong%20Park%20and%20Eunbin%20Seo%20and%20Jongwoo%20Lim%0AAbstract%3A%20%20%20Accurate%203D%20lane%20detection%20from%20monocular%20images%20presents%20significant%0Achallenges%20due%20to%20depth%20ambiguity%20and%20imperfect%20ground%20modeling.%20Previous%0Aattempts%20to%20model%20the%20ground%20have%20often%20used%20a%20planar%20ground%20assumption%20with%0Alimited%20degrees%20of%20freedom%2C%20making%20them%20unsuitable%20for%20complex%20road%0Aenvironments%20with%20varying%20slopes.%20Our%20study%20introduces%20HeightLane%2C%20an%0Ainnovative%20method%20that%20predicts%20a%20height%20map%20from%20monocular%20images%20by%20creating%0Aanchors%20based%20on%20a%20multi-slope%20assumption.%20This%20approach%20provides%20a%20detailed%0Aand%20accurate%20representation%20of%20the%20ground.%20HeightLane%20employs%20the%20predicted%0Aheightmap%20along%20with%20a%20deformable%20attention-based%20spatial%20feature%20transform%0Aframework%20to%20efficiently%20convert%202D%20image%20features%20into%203D%20bird%27s%20eye%20view%0A%28BEV%29%20features%2C%20enhancing%20spatial%20understanding%20and%20lane%20structure%20recognition.%0AAdditionally%2C%20the%20heightmap%20is%20used%20for%20the%20positional%20encoding%20of%20BEV%0Afeatures%2C%20further%20improving%20their%20spatial%20accuracy.%20This%20explicit%20view%0Atransformation%20bridges%20the%20gap%20between%20front-view%20perceptions%20and%20spatially%0Aaccurate%20BEV%20representations%2C%20significantly%20improving%20detection%20performance.%20To%0Aaddress%20the%20lack%20of%20the%20necessary%20ground%20truth%20%28GT%29%20height%20map%20in%20the%20original%0AOpenLane%20dataset%2C%20we%20leverage%20the%20Waymo%20dataset%20and%20accumulate%20its%20LiDAR%20data%0Ato%20generate%20a%20height%20map%20for%20the%20drivable%20area%20of%20each%20scene.%20The%20GT%20heightmaps%0Aare%20used%20to%20train%20the%20heightmap%20extraction%20module%20from%20monocular%20images.%0AExtensive%20experiments%20on%20the%20OpenLane%20validation%20set%20show%20that%20HeightLane%0Aachieves%20state-of-the-art%20performance%20in%20terms%20of%20F-score%2C%20highlighting%20its%0Apotential%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeightLane%253A%2520BEV%2520Heightmap%2520guided%25203D%2520Lane%2520Detection%26entry.906535625%3DChaesong%2520Park%2520and%2520Eunbin%2520Seo%2520and%2520Jongwoo%2520Lim%26entry.1292438233%3D%2520%2520Accurate%25203D%2520lane%2520detection%2520from%2520monocular%2520images%2520presents%2520significant%250Achallenges%2520due%2520to%2520depth%2520ambiguity%2520and%2520imperfect%2520ground%2520modeling.%2520Previous%250Aattempts%2520to%2520model%2520the%2520ground%2520have%2520often%2520used%2520a%2520planar%2520ground%2520assumption%2520with%250Alimited%2520degrees%2520of%2520freedom%252C%2520making%2520them%2520unsuitable%2520for%2520complex%2520road%250Aenvironments%2520with%2520varying%2520slopes.%2520Our%2520study%2520introduces%2520HeightLane%252C%2520an%250Ainnovative%2520method%2520that%2520predicts%2520a%2520height%2520map%2520from%2520monocular%2520images%2520by%2520creating%250Aanchors%2520based%2520on%2520a%2520multi-slope%2520assumption.%2520This%2520approach%2520provides%2520a%2520detailed%250Aand%2520accurate%2520representation%2520of%2520the%2520ground.%2520HeightLane%2520employs%2520the%2520predicted%250Aheightmap%2520along%2520with%2520a%2520deformable%2520attention-based%2520spatial%2520feature%2520transform%250Aframework%2520to%2520efficiently%2520convert%25202D%2520image%2520features%2520into%25203D%2520bird%2527s%2520eye%2520view%250A%2528BEV%2529%2520features%252C%2520enhancing%2520spatial%2520understanding%2520and%2520lane%2520structure%2520recognition.%250AAdditionally%252C%2520the%2520heightmap%2520is%2520used%2520for%2520the%2520positional%2520encoding%2520of%2520BEV%250Afeatures%252C%2520further%2520improving%2520their%2520spatial%2520accuracy.%2520This%2520explicit%2520view%250Atransformation%2520bridges%2520the%2520gap%2520between%2520front-view%2520perceptions%2520and%2520spatially%250Aaccurate%2520BEV%2520representations%252C%2520significantly%2520improving%2520detection%2520performance.%2520To%250Aaddress%2520the%2520lack%2520of%2520the%2520necessary%2520ground%2520truth%2520%2528GT%2529%2520height%2520map%2520in%2520the%2520original%250AOpenLane%2520dataset%252C%2520we%2520leverage%2520the%2520Waymo%2520dataset%2520and%2520accumulate%2520its%2520LiDAR%2520data%250Ato%2520generate%2520a%2520height%2520map%2520for%2520the%2520drivable%2520area%2520of%2520each%2520scene.%2520The%2520GT%2520heightmaps%250Aare%2520used%2520to%2520train%2520the%2520heightmap%2520extraction%2520module%2520from%2520monocular%2520images.%250AExtensive%2520experiments%2520on%2520the%2520OpenLane%2520validation%2520set%2520show%2520that%2520HeightLane%250Aachieves%2520state-of-the-art%2520performance%2520in%2520terms%2520of%2520F-score%252C%2520highlighting%2520its%250Apotential%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeightLane%3A%20BEV%20Heightmap%20guided%203D%20Lane%20Detection&entry.906535625=Chaesong%20Park%20and%20Eunbin%20Seo%20and%20Jongwoo%20Lim&entry.1292438233=%20%20Accurate%203D%20lane%20detection%20from%20monocular%20images%20presents%20significant%0Achallenges%20due%20to%20depth%20ambiguity%20and%20imperfect%20ground%20modeling.%20Previous%0Aattempts%20to%20model%20the%20ground%20have%20often%20used%20a%20planar%20ground%20assumption%20with%0Alimited%20degrees%20of%20freedom%2C%20making%20them%20unsuitable%20for%20complex%20road%0Aenvironments%20with%20varying%20slopes.%20Our%20study%20introduces%20HeightLane%2C%20an%0Ainnovative%20method%20that%20predicts%20a%20height%20map%20from%20monocular%20images%20by%20creating%0Aanchors%20based%20on%20a%20multi-slope%20assumption.%20This%20approach%20provides%20a%20detailed%0Aand%20accurate%20representation%20of%20the%20ground.%20HeightLane%20employs%20the%20predicted%0Aheightmap%20along%20with%20a%20deformable%20attention-based%20spatial%20feature%20transform%0Aframework%20to%20efficiently%20convert%202D%20image%20features%20into%203D%20bird%27s%20eye%20view%0A%28BEV%29%20features%2C%20enhancing%20spatial%20understanding%20and%20lane%20structure%20recognition.%0AAdditionally%2C%20the%20heightmap%20is%20used%20for%20the%20positional%20encoding%20of%20BEV%0Afeatures%2C%20further%20improving%20their%20spatial%20accuracy.%20This%20explicit%20view%0Atransformation%20bridges%20the%20gap%20between%20front-view%20perceptions%20and%20spatially%0Aaccurate%20BEV%20representations%2C%20significantly%20improving%20detection%20performance.%20To%0Aaddress%20the%20lack%20of%20the%20necessary%20ground%20truth%20%28GT%29%20height%20map%20in%20the%20original%0AOpenLane%20dataset%2C%20we%20leverage%20the%20Waymo%20dataset%20and%20accumulate%20its%20LiDAR%20data%0Ato%20generate%20a%20height%20map%20for%20the%20drivable%20area%20of%20each%20scene.%20The%20GT%20heightmaps%0Aare%20used%20to%20train%20the%20heightmap%20extraction%20module%20from%20monocular%20images.%0AExtensive%20experiments%20on%20the%20OpenLane%20validation%20set%20show%20that%20HeightLane%0Aachieves%20state-of-the-art%20performance%20in%20terms%20of%20F-score%2C%20highlighting%20its%0Apotential%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08270v1&entry.124074799=Read"},
{"title": "Understanding the Local Geometry of Generative Model Manifolds", "author": "Ahmed Imtiaz Humayun and Ibtihel Amara and Candice Schumann and Golnoosh Farnadi and Negar Rostamzadeh and Mohammad Havaei", "abstract": "  Deep generative models learn continuous representations of complex data\nmanifolds using a finite number of samples during training. For a pre-trained\ngenerative model, the common way to evaluate the quality of the manifold\nrepresentation learned, is by computing global metrics like Fr\\'echet Inception\nDistance using a large number of generated and real samples. However,\ngenerative model performance is not uniform across the learned manifold, e.g.,\nfor \\textit{foundation models} like Stable Diffusion generation performance can\nvary significantly based on the conditioning or initial noise vector being\ndenoised. In this paper we study the relationship between the \\textit{local\ngeometry of the learned manifold} and downstream generation. Based on the\ntheory of continuous piecewise-linear (CPWL) generators, we use three geometric\ndescriptors - scaling ($\\psi$), rank ($\\nu$), and complexity ($\\delta$) - to\ncharacterize a pre-trained generative model manifold locally. We provide\nquantitative and qualitative evidence showing that for a given latent, the\nlocal descriptors are correlated with generation aesthetics, artifacts,\nuncertainty, and even memorization. Finally we demonstrate that training a\n\\textit{reward model} on the local geometry can allow controlling the\nlikelihood of a generated sample under the learned distribution.\n", "link": "http://arxiv.org/abs/2408.08307v1", "date": "2024-08-15", "relevancy": 2.1264, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.57}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5266}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Local%20Geometry%20of%20Generative%20Model%20Manifolds&body=Title%3A%20Understanding%20the%20Local%20Geometry%20of%20Generative%20Model%20Manifolds%0AAuthor%3A%20Ahmed%20Imtiaz%20Humayun%20and%20Ibtihel%20Amara%20and%20Candice%20Schumann%20and%20Golnoosh%20Farnadi%20and%20Negar%20Rostamzadeh%20and%20Mohammad%20Havaei%0AAbstract%3A%20%20%20Deep%20generative%20models%20learn%20continuous%20representations%20of%20complex%20data%0Amanifolds%20using%20a%20finite%20number%20of%20samples%20during%20training.%20For%20a%20pre-trained%0Agenerative%20model%2C%20the%20common%20way%20to%20evaluate%20the%20quality%20of%20the%20manifold%0Arepresentation%20learned%2C%20is%20by%20computing%20global%20metrics%20like%20Fr%5C%27echet%20Inception%0ADistance%20using%20a%20large%20number%20of%20generated%20and%20real%20samples.%20However%2C%0Agenerative%20model%20performance%20is%20not%20uniform%20across%20the%20learned%20manifold%2C%20e.g.%2C%0Afor%20%5Ctextit%7Bfoundation%20models%7D%20like%20Stable%20Diffusion%20generation%20performance%20can%0Avary%20significantly%20based%20on%20the%20conditioning%20or%20initial%20noise%20vector%20being%0Adenoised.%20In%20this%20paper%20we%20study%20the%20relationship%20between%20the%20%5Ctextit%7Blocal%0Ageometry%20of%20the%20learned%20manifold%7D%20and%20downstream%20generation.%20Based%20on%20the%0Atheory%20of%20continuous%20piecewise-linear%20%28CPWL%29%20generators%2C%20we%20use%20three%20geometric%0Adescriptors%20-%20scaling%20%28%24%5Cpsi%24%29%2C%20rank%20%28%24%5Cnu%24%29%2C%20and%20complexity%20%28%24%5Cdelta%24%29%20-%20to%0Acharacterize%20a%20pre-trained%20generative%20model%20manifold%20locally.%20We%20provide%0Aquantitative%20and%20qualitative%20evidence%20showing%20that%20for%20a%20given%20latent%2C%20the%0Alocal%20descriptors%20are%20correlated%20with%20generation%20aesthetics%2C%20artifacts%2C%0Auncertainty%2C%20and%20even%20memorization.%20Finally%20we%20demonstrate%20that%20training%20a%0A%5Ctextit%7Breward%20model%7D%20on%20the%20local%20geometry%20can%20allow%20controlling%20the%0Alikelihood%20of%20a%20generated%20sample%20under%20the%20learned%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520Local%2520Geometry%2520of%2520Generative%2520Model%2520Manifolds%26entry.906535625%3DAhmed%2520Imtiaz%2520Humayun%2520and%2520Ibtihel%2520Amara%2520and%2520Candice%2520Schumann%2520and%2520Golnoosh%2520Farnadi%2520and%2520Negar%2520Rostamzadeh%2520and%2520Mohammad%2520Havaei%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520learn%2520continuous%2520representations%2520of%2520complex%2520data%250Amanifolds%2520using%2520a%2520finite%2520number%2520of%2520samples%2520during%2520training.%2520For%2520a%2520pre-trained%250Agenerative%2520model%252C%2520the%2520common%2520way%2520to%2520evaluate%2520the%2520quality%2520of%2520the%2520manifold%250Arepresentation%2520learned%252C%2520is%2520by%2520computing%2520global%2520metrics%2520like%2520Fr%255C%2527echet%2520Inception%250ADistance%2520using%2520a%2520large%2520number%2520of%2520generated%2520and%2520real%2520samples.%2520However%252C%250Agenerative%2520model%2520performance%2520is%2520not%2520uniform%2520across%2520the%2520learned%2520manifold%252C%2520e.g.%252C%250Afor%2520%255Ctextit%257Bfoundation%2520models%257D%2520like%2520Stable%2520Diffusion%2520generation%2520performance%2520can%250Avary%2520significantly%2520based%2520on%2520the%2520conditioning%2520or%2520initial%2520noise%2520vector%2520being%250Adenoised.%2520In%2520this%2520paper%2520we%2520study%2520the%2520relationship%2520between%2520the%2520%255Ctextit%257Blocal%250Ageometry%2520of%2520the%2520learned%2520manifold%257D%2520and%2520downstream%2520generation.%2520Based%2520on%2520the%250Atheory%2520of%2520continuous%2520piecewise-linear%2520%2528CPWL%2529%2520generators%252C%2520we%2520use%2520three%2520geometric%250Adescriptors%2520-%2520scaling%2520%2528%2524%255Cpsi%2524%2529%252C%2520rank%2520%2528%2524%255Cnu%2524%2529%252C%2520and%2520complexity%2520%2528%2524%255Cdelta%2524%2529%2520-%2520to%250Acharacterize%2520a%2520pre-trained%2520generative%2520model%2520manifold%2520locally.%2520We%2520provide%250Aquantitative%2520and%2520qualitative%2520evidence%2520showing%2520that%2520for%2520a%2520given%2520latent%252C%2520the%250Alocal%2520descriptors%2520are%2520correlated%2520with%2520generation%2520aesthetics%252C%2520artifacts%252C%250Auncertainty%252C%2520and%2520even%2520memorization.%2520Finally%2520we%2520demonstrate%2520that%2520training%2520a%250A%255Ctextit%257Breward%2520model%257D%2520on%2520the%2520local%2520geometry%2520can%2520allow%2520controlling%2520the%250Alikelihood%2520of%2520a%2520generated%2520sample%2520under%2520the%2520learned%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Local%20Geometry%20of%20Generative%20Model%20Manifolds&entry.906535625=Ahmed%20Imtiaz%20Humayun%20and%20Ibtihel%20Amara%20and%20Candice%20Schumann%20and%20Golnoosh%20Farnadi%20and%20Negar%20Rostamzadeh%20and%20Mohammad%20Havaei&entry.1292438233=%20%20Deep%20generative%20models%20learn%20continuous%20representations%20of%20complex%20data%0Amanifolds%20using%20a%20finite%20number%20of%20samples%20during%20training.%20For%20a%20pre-trained%0Agenerative%20model%2C%20the%20common%20way%20to%20evaluate%20the%20quality%20of%20the%20manifold%0Arepresentation%20learned%2C%20is%20by%20computing%20global%20metrics%20like%20Fr%5C%27echet%20Inception%0ADistance%20using%20a%20large%20number%20of%20generated%20and%20real%20samples.%20However%2C%0Agenerative%20model%20performance%20is%20not%20uniform%20across%20the%20learned%20manifold%2C%20e.g.%2C%0Afor%20%5Ctextit%7Bfoundation%20models%7D%20like%20Stable%20Diffusion%20generation%20performance%20can%0Avary%20significantly%20based%20on%20the%20conditioning%20or%20initial%20noise%20vector%20being%0Adenoised.%20In%20this%20paper%20we%20study%20the%20relationship%20between%20the%20%5Ctextit%7Blocal%0Ageometry%20of%20the%20learned%20manifold%7D%20and%20downstream%20generation.%20Based%20on%20the%0Atheory%20of%20continuous%20piecewise-linear%20%28CPWL%29%20generators%2C%20we%20use%20three%20geometric%0Adescriptors%20-%20scaling%20%28%24%5Cpsi%24%29%2C%20rank%20%28%24%5Cnu%24%29%2C%20and%20complexity%20%28%24%5Cdelta%24%29%20-%20to%0Acharacterize%20a%20pre-trained%20generative%20model%20manifold%20locally.%20We%20provide%0Aquantitative%20and%20qualitative%20evidence%20showing%20that%20for%20a%20given%20latent%2C%20the%0Alocal%20descriptors%20are%20correlated%20with%20generation%20aesthetics%2C%20artifacts%2C%0Auncertainty%2C%20and%20even%20memorization.%20Finally%20we%20demonstrate%20that%20training%20a%0A%5Ctextit%7Breward%20model%7D%20on%20the%20local%20geometry%20can%20allow%20controlling%20the%0Alikelihood%20of%20a%20generated%20sample%20under%20the%20learned%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08307v1&entry.124074799=Read"},
{"title": "Identifying Important Group of Pixels using Interactions", "author": "Kosuke Sumiyasu and Kazuhiko Kawamoto and Hiroshi Kera", "abstract": "  To better understand the behavior of image classifiers, it is useful to\nvisualize the contribution of individual pixels to the model prediction. In\nthis study, we propose a method, MoXI ($\\textbf{Mo}$del e$\\textbf{X}$planation\nby $\\textbf{I}$nteractions), that efficiently and accurately identifies a group\nof pixels with high prediction confidence. The proposed method employs\ngame-theoretic concepts, Shapley values and interactions, taking into account\nthe effects of individual pixels and the cooperative influence of pixels on\nmodel confidence. Theoretical analysis and experiments demonstrate that our\nmethod better identifies the pixels that are highly contributing to the model\noutputs than widely-used visualization by Grad-CAM, Attention rollout, and\nShapley value. While prior studies have suffered from the exponential\ncomputational cost in the computation of Shapley value and interactions, we\nshow that this can be reduced to quadratic cost for our task. The code is\navailable at https://github.com/KosukeSumiyasu/MoXI.\n", "link": "http://arxiv.org/abs/2401.03785v3", "date": "2024-08-15", "relevancy": 2.1, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5303}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5215}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Important%20Group%20of%20Pixels%20using%20Interactions&body=Title%3A%20Identifying%20Important%20Group%20of%20Pixels%20using%20Interactions%0AAuthor%3A%20Kosuke%20Sumiyasu%20and%20Kazuhiko%20Kawamoto%20and%20Hiroshi%20Kera%0AAbstract%3A%20%20%20To%20better%20understand%20the%20behavior%20of%20image%20classifiers%2C%20it%20is%20useful%20to%0Avisualize%20the%20contribution%20of%20individual%20pixels%20to%20the%20model%20prediction.%20In%0Athis%20study%2C%20we%20propose%20a%20method%2C%20MoXI%20%28%24%5Ctextbf%7BMo%7D%24del%20e%24%5Ctextbf%7BX%7D%24planation%0Aby%20%24%5Ctextbf%7BI%7D%24nteractions%29%2C%20that%20efficiently%20and%20accurately%20identifies%20a%20group%0Aof%20pixels%20with%20high%20prediction%20confidence.%20The%20proposed%20method%20employs%0Agame-theoretic%20concepts%2C%20Shapley%20values%20and%20interactions%2C%20taking%20into%20account%0Athe%20effects%20of%20individual%20pixels%20and%20the%20cooperative%20influence%20of%20pixels%20on%0Amodel%20confidence.%20Theoretical%20analysis%20and%20experiments%20demonstrate%20that%20our%0Amethod%20better%20identifies%20the%20pixels%20that%20are%20highly%20contributing%20to%20the%20model%0Aoutputs%20than%20widely-used%20visualization%20by%20Grad-CAM%2C%20Attention%20rollout%2C%20and%0AShapley%20value.%20While%20prior%20studies%20have%20suffered%20from%20the%20exponential%0Acomputational%20cost%20in%20the%20computation%20of%20Shapley%20value%20and%20interactions%2C%20we%0Ashow%20that%20this%20can%20be%20reduced%20to%20quadratic%20cost%20for%20our%20task.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/KosukeSumiyasu/MoXI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03785v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Important%2520Group%2520of%2520Pixels%2520using%2520Interactions%26entry.906535625%3DKosuke%2520Sumiyasu%2520and%2520Kazuhiko%2520Kawamoto%2520and%2520Hiroshi%2520Kera%26entry.1292438233%3D%2520%2520To%2520better%2520understand%2520the%2520behavior%2520of%2520image%2520classifiers%252C%2520it%2520is%2520useful%2520to%250Avisualize%2520the%2520contribution%2520of%2520individual%2520pixels%2520to%2520the%2520model%2520prediction.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520method%252C%2520MoXI%2520%2528%2524%255Ctextbf%257BMo%257D%2524del%2520e%2524%255Ctextbf%257BX%257D%2524planation%250Aby%2520%2524%255Ctextbf%257BI%257D%2524nteractions%2529%252C%2520that%2520efficiently%2520and%2520accurately%2520identifies%2520a%2520group%250Aof%2520pixels%2520with%2520high%2520prediction%2520confidence.%2520The%2520proposed%2520method%2520employs%250Agame-theoretic%2520concepts%252C%2520Shapley%2520values%2520and%2520interactions%252C%2520taking%2520into%2520account%250Athe%2520effects%2520of%2520individual%2520pixels%2520and%2520the%2520cooperative%2520influence%2520of%2520pixels%2520on%250Amodel%2520confidence.%2520Theoretical%2520analysis%2520and%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520better%2520identifies%2520the%2520pixels%2520that%2520are%2520highly%2520contributing%2520to%2520the%2520model%250Aoutputs%2520than%2520widely-used%2520visualization%2520by%2520Grad-CAM%252C%2520Attention%2520rollout%252C%2520and%250AShapley%2520value.%2520While%2520prior%2520studies%2520have%2520suffered%2520from%2520the%2520exponential%250Acomputational%2520cost%2520in%2520the%2520computation%2520of%2520Shapley%2520value%2520and%2520interactions%252C%2520we%250Ashow%2520that%2520this%2520can%2520be%2520reduced%2520to%2520quadratic%2520cost%2520for%2520our%2520task.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/KosukeSumiyasu/MoXI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03785v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Important%20Group%20of%20Pixels%20using%20Interactions&entry.906535625=Kosuke%20Sumiyasu%20and%20Kazuhiko%20Kawamoto%20and%20Hiroshi%20Kera&entry.1292438233=%20%20To%20better%20understand%20the%20behavior%20of%20image%20classifiers%2C%20it%20is%20useful%20to%0Avisualize%20the%20contribution%20of%20individual%20pixels%20to%20the%20model%20prediction.%20In%0Athis%20study%2C%20we%20propose%20a%20method%2C%20MoXI%20%28%24%5Ctextbf%7BMo%7D%24del%20e%24%5Ctextbf%7BX%7D%24planation%0Aby%20%24%5Ctextbf%7BI%7D%24nteractions%29%2C%20that%20efficiently%20and%20accurately%20identifies%20a%20group%0Aof%20pixels%20with%20high%20prediction%20confidence.%20The%20proposed%20method%20employs%0Agame-theoretic%20concepts%2C%20Shapley%20values%20and%20interactions%2C%20taking%20into%20account%0Athe%20effects%20of%20individual%20pixels%20and%20the%20cooperative%20influence%20of%20pixels%20on%0Amodel%20confidence.%20Theoretical%20analysis%20and%20experiments%20demonstrate%20that%20our%0Amethod%20better%20identifies%20the%20pixels%20that%20are%20highly%20contributing%20to%20the%20model%0Aoutputs%20than%20widely-used%20visualization%20by%20Grad-CAM%2C%20Attention%20rollout%2C%20and%0AShapley%20value.%20While%20prior%20studies%20have%20suffered%20from%20the%20exponential%0Acomputational%20cost%20in%20the%20computation%20of%20Shapley%20value%20and%20interactions%2C%20we%0Ashow%20that%20this%20can%20be%20reduced%20to%20quadratic%20cost%20for%20our%20task.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/KosukeSumiyasu/MoXI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03785v3&entry.124074799=Read"},
{"title": "HAIR: Hypernetworks-based All-in-One Image Restoration", "author": "Jin Cao and Yi Cao and Li Pang and Deyu Meng and Xiangyong Cao", "abstract": "  Image restoration involves recovering a high-quality clean image from its\ndegraded version, which is a fundamental task in computer vision. Recent\nprogress in image restoration has demonstrated the effectiveness of learning\nmodels capable of addressing various degradations simultaneously, i.e., the\nAll-in-One image restoration models. However, these existing methods typically\nutilize the same parameters facing images with different degradation types,\nwhich causes the model to be forced to trade off between degradation types,\ntherefore impair the total performance. To solve this problem, we propose HAIR,\na Hypernetworks-based plug-in-and-play method that dynamically generated\nparameters for the corresponding networks based on the contents of input\nimages. HAIR consists of 2 main components: Classifier (Cl) and Hyper Selecting\nNet (HSN). To be more specific, the Classifier is a simple image classification\nnetwork which is used to generate a Global Information Vector (GIV) that\ncontains the degradation information of the input image; And the HSNs can be\nseen as a simple Fully-connected Neural Network that receive the GIV and output\nparameters for the corresponding modules. Extensive experiments shows that\nincorporating HAIR into the architectures can significantly improve the\nperformance of different models on image restoration tasks at a low cost,\n\\textbf{although HAIR only generate parameters and haven't change these models'\nlogical structures at all.} With incorporating HAIR into the popular\narchitecture Restormer, our method obtains superior or at least comparable\nperformance to current state-of-the-art methods on a range of image restoration\ntasks.\n\\href{https://github.com/toummHus/HAIR}{\\textcolor{blue}{$\\underline{\\textbf{Code\nand pre-trained checkpoints are available here.}}$}}\n", "link": "http://arxiv.org/abs/2408.08091v1", "date": "2024-08-15", "relevancy": 2.0991, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.53}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5267}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAIR%3A%20Hypernetworks-based%20All-in-One%20Image%20Restoration&body=Title%3A%20HAIR%3A%20Hypernetworks-based%20All-in-One%20Image%20Restoration%0AAuthor%3A%20Jin%20Cao%20and%20Yi%20Cao%20and%20Li%20Pang%20and%20Deyu%20Meng%20and%20Xiangyong%20Cao%0AAbstract%3A%20%20%20Image%20restoration%20involves%20recovering%20a%20high-quality%20clean%20image%20from%20its%0Adegraded%20version%2C%20which%20is%20a%20fundamental%20task%20in%20computer%20vision.%20Recent%0Aprogress%20in%20image%20restoration%20has%20demonstrated%20the%20effectiveness%20of%20learning%0Amodels%20capable%20of%20addressing%20various%20degradations%20simultaneously%2C%20i.e.%2C%20the%0AAll-in-One%20image%20restoration%20models.%20However%2C%20these%20existing%20methods%20typically%0Autilize%20the%20same%20parameters%20facing%20images%20with%20different%20degradation%20types%2C%0Awhich%20causes%20the%20model%20to%20be%20forced%20to%20trade%20off%20between%20degradation%20types%2C%0Atherefore%20impair%20the%20total%20performance.%20To%20solve%20this%20problem%2C%20we%20propose%20HAIR%2C%0Aa%20Hypernetworks-based%20plug-in-and-play%20method%20that%20dynamically%20generated%0Aparameters%20for%20the%20corresponding%20networks%20based%20on%20the%20contents%20of%20input%0Aimages.%20HAIR%20consists%20of%202%20main%20components%3A%20Classifier%20%28Cl%29%20and%20Hyper%20Selecting%0ANet%20%28HSN%29.%20To%20be%20more%20specific%2C%20the%20Classifier%20is%20a%20simple%20image%20classification%0Anetwork%20which%20is%20used%20to%20generate%20a%20Global%20Information%20Vector%20%28GIV%29%20that%0Acontains%20the%20degradation%20information%20of%20the%20input%20image%3B%20And%20the%20HSNs%20can%20be%0Aseen%20as%20a%20simple%20Fully-connected%20Neural%20Network%20that%20receive%20the%20GIV%20and%20output%0Aparameters%20for%20the%20corresponding%20modules.%20Extensive%20experiments%20shows%20that%0Aincorporating%20HAIR%20into%20the%20architectures%20can%20significantly%20improve%20the%0Aperformance%20of%20different%20models%20on%20image%20restoration%20tasks%20at%20a%20low%20cost%2C%0A%5Ctextbf%7Balthough%20HAIR%20only%20generate%20parameters%20and%20haven%27t%20change%20these%20models%27%0Alogical%20structures%20at%20all.%7D%20With%20incorporating%20HAIR%20into%20the%20popular%0Aarchitecture%20Restormer%2C%20our%20method%20obtains%20superior%20or%20at%20least%20comparable%0Aperformance%20to%20current%20state-of-the-art%20methods%20on%20a%20range%20of%20image%20restoration%0Atasks.%0A%5Chref%7Bhttps%3A//github.com/toummHus/HAIR%7D%7B%5Ctextcolor%7Bblue%7D%7B%24%5Cunderline%7B%5Ctextbf%7BCode%0Aand%20pre-trained%20checkpoints%20are%20available%20here.%7D%7D%24%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAIR%253A%2520Hypernetworks-based%2520All-in-One%2520Image%2520Restoration%26entry.906535625%3DJin%2520Cao%2520and%2520Yi%2520Cao%2520and%2520Li%2520Pang%2520and%2520Deyu%2520Meng%2520and%2520Xiangyong%2520Cao%26entry.1292438233%3D%2520%2520Image%2520restoration%2520involves%2520recovering%2520a%2520high-quality%2520clean%2520image%2520from%2520its%250Adegraded%2520version%252C%2520which%2520is%2520a%2520fundamental%2520task%2520in%2520computer%2520vision.%2520Recent%250Aprogress%2520in%2520image%2520restoration%2520has%2520demonstrated%2520the%2520effectiveness%2520of%2520learning%250Amodels%2520capable%2520of%2520addressing%2520various%2520degradations%2520simultaneously%252C%2520i.e.%252C%2520the%250AAll-in-One%2520image%2520restoration%2520models.%2520However%252C%2520these%2520existing%2520methods%2520typically%250Autilize%2520the%2520same%2520parameters%2520facing%2520images%2520with%2520different%2520degradation%2520types%252C%250Awhich%2520causes%2520the%2520model%2520to%2520be%2520forced%2520to%2520trade%2520off%2520between%2520degradation%2520types%252C%250Atherefore%2520impair%2520the%2520total%2520performance.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520HAIR%252C%250Aa%2520Hypernetworks-based%2520plug-in-and-play%2520method%2520that%2520dynamically%2520generated%250Aparameters%2520for%2520the%2520corresponding%2520networks%2520based%2520on%2520the%2520contents%2520of%2520input%250Aimages.%2520HAIR%2520consists%2520of%25202%2520main%2520components%253A%2520Classifier%2520%2528Cl%2529%2520and%2520Hyper%2520Selecting%250ANet%2520%2528HSN%2529.%2520To%2520be%2520more%2520specific%252C%2520the%2520Classifier%2520is%2520a%2520simple%2520image%2520classification%250Anetwork%2520which%2520is%2520used%2520to%2520generate%2520a%2520Global%2520Information%2520Vector%2520%2528GIV%2529%2520that%250Acontains%2520the%2520degradation%2520information%2520of%2520the%2520input%2520image%253B%2520And%2520the%2520HSNs%2520can%2520be%250Aseen%2520as%2520a%2520simple%2520Fully-connected%2520Neural%2520Network%2520that%2520receive%2520the%2520GIV%2520and%2520output%250Aparameters%2520for%2520the%2520corresponding%2520modules.%2520Extensive%2520experiments%2520shows%2520that%250Aincorporating%2520HAIR%2520into%2520the%2520architectures%2520can%2520significantly%2520improve%2520the%250Aperformance%2520of%2520different%2520models%2520on%2520image%2520restoration%2520tasks%2520at%2520a%2520low%2520cost%252C%250A%255Ctextbf%257Balthough%2520HAIR%2520only%2520generate%2520parameters%2520and%2520haven%2527t%2520change%2520these%2520models%2527%250Alogical%2520structures%2520at%2520all.%257D%2520With%2520incorporating%2520HAIR%2520into%2520the%2520popular%250Aarchitecture%2520Restormer%252C%2520our%2520method%2520obtains%2520superior%2520or%2520at%2520least%2520comparable%250Aperformance%2520to%2520current%2520state-of-the-art%2520methods%2520on%2520a%2520range%2520of%2520image%2520restoration%250Atasks.%250A%255Chref%257Bhttps%253A//github.com/toummHus/HAIR%257D%257B%255Ctextcolor%257Bblue%257D%257B%2524%255Cunderline%257B%255Ctextbf%257BCode%250Aand%2520pre-trained%2520checkpoints%2520are%2520available%2520here.%257D%257D%2524%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAIR%3A%20Hypernetworks-based%20All-in-One%20Image%20Restoration&entry.906535625=Jin%20Cao%20and%20Yi%20Cao%20and%20Li%20Pang%20and%20Deyu%20Meng%20and%20Xiangyong%20Cao&entry.1292438233=%20%20Image%20restoration%20involves%20recovering%20a%20high-quality%20clean%20image%20from%20its%0Adegraded%20version%2C%20which%20is%20a%20fundamental%20task%20in%20computer%20vision.%20Recent%0Aprogress%20in%20image%20restoration%20has%20demonstrated%20the%20effectiveness%20of%20learning%0Amodels%20capable%20of%20addressing%20various%20degradations%20simultaneously%2C%20i.e.%2C%20the%0AAll-in-One%20image%20restoration%20models.%20However%2C%20these%20existing%20methods%20typically%0Autilize%20the%20same%20parameters%20facing%20images%20with%20different%20degradation%20types%2C%0Awhich%20causes%20the%20model%20to%20be%20forced%20to%20trade%20off%20between%20degradation%20types%2C%0Atherefore%20impair%20the%20total%20performance.%20To%20solve%20this%20problem%2C%20we%20propose%20HAIR%2C%0Aa%20Hypernetworks-based%20plug-in-and-play%20method%20that%20dynamically%20generated%0Aparameters%20for%20the%20corresponding%20networks%20based%20on%20the%20contents%20of%20input%0Aimages.%20HAIR%20consists%20of%202%20main%20components%3A%20Classifier%20%28Cl%29%20and%20Hyper%20Selecting%0ANet%20%28HSN%29.%20To%20be%20more%20specific%2C%20the%20Classifier%20is%20a%20simple%20image%20classification%0Anetwork%20which%20is%20used%20to%20generate%20a%20Global%20Information%20Vector%20%28GIV%29%20that%0Acontains%20the%20degradation%20information%20of%20the%20input%20image%3B%20And%20the%20HSNs%20can%20be%0Aseen%20as%20a%20simple%20Fully-connected%20Neural%20Network%20that%20receive%20the%20GIV%20and%20output%0Aparameters%20for%20the%20corresponding%20modules.%20Extensive%20experiments%20shows%20that%0Aincorporating%20HAIR%20into%20the%20architectures%20can%20significantly%20improve%20the%0Aperformance%20of%20different%20models%20on%20image%20restoration%20tasks%20at%20a%20low%20cost%2C%0A%5Ctextbf%7Balthough%20HAIR%20only%20generate%20parameters%20and%20haven%27t%20change%20these%20models%27%0Alogical%20structures%20at%20all.%7D%20With%20incorporating%20HAIR%20into%20the%20popular%0Aarchitecture%20Restormer%2C%20our%20method%20obtains%20superior%20or%20at%20least%20comparable%0Aperformance%20to%20current%20state-of-the-art%20methods%20on%20a%20range%20of%20image%20restoration%0Atasks.%0A%5Chref%7Bhttps%3A//github.com/toummHus/HAIR%7D%7B%5Ctextcolor%7Bblue%7D%7B%24%5Cunderline%7B%5Ctextbf%7BCode%0Aand%20pre-trained%20checkpoints%20are%20available%20here.%7D%7D%24%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08091v1&entry.124074799=Read"},
{"title": "DATTA: Towards Diversity Adaptive Test-Time Adaptation in Dynamic Wild\n  World", "author": "Chuyang Ye and Dongyan Wei and Zhendong Liu and Yuanyi Pang and Yixi Lin and Jiarong Liao and Qinting Jiang and Xianghua Fu and Qing Li and Jingyan Jiang", "abstract": "  Test-time adaptation (TTA) effectively addresses distribution shifts between\ntraining and testing data by adjusting models on test samples, which is crucial\nfor improving model inference in real-world applications. However, traditional\nTTA methods typically follow a fixed pattern to address the dynamic data\npatterns (low-diversity or high-diversity patterns) often leading to\nperformance degradation and consequently a decline in Quality of Experience\n(QoE). The primary issues we observed are:Different scenarios require different\nnormalization methods (e.g., Instance Normalization is optimal in mixed domains\nbut not in static domains). Model fine-tuning can potentially harm the model\nand waste time.Hence, it is crucial to design strategies for effectively\nmeasuring and managing distribution diversity to minimize its negative impact\non model performance. Based on these observations, this paper proposes a new\ngeneral method, named Diversity Adaptive Test-Time Adaptation (DATTA), aimed at\nimproving QoE. DATTA dynamically selects the best batch normalization methods\nand fine-tuning strategies by leveraging the Diversity Score to differentiate\nbetween high and low diversity score batches. It features three key components:\nDiversity Discrimination (DD) to assess batch diversity, Diversity Adaptive\nBatch Normalization (DABN) to tailor normalization methods based on DD\ninsights, and Diversity Adaptive Fine-Tuning (DAFT) to selectively fine-tune\nthe model. Experimental results show that our method achieves up to a 21%\nincrease in accuracy compared to state-of-the-art methodologies, indicating\nthat our method maintains good model performance while demonstrating its\nrobustness. Our code will be released soon.\n", "link": "http://arxiv.org/abs/2408.08056v1", "date": "2024-08-15", "relevancy": 2.0953, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5301}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5215}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DATTA%3A%20Towards%20Diversity%20Adaptive%20Test-Time%20Adaptation%20in%20Dynamic%20Wild%0A%20%20World&body=Title%3A%20DATTA%3A%20Towards%20Diversity%20Adaptive%20Test-Time%20Adaptation%20in%20Dynamic%20Wild%0A%20%20World%0AAuthor%3A%20Chuyang%20Ye%20and%20Dongyan%20Wei%20and%20Zhendong%20Liu%20and%20Yuanyi%20Pang%20and%20Yixi%20Lin%20and%20Jiarong%20Liao%20and%20Qinting%20Jiang%20and%20Xianghua%20Fu%20and%20Qing%20Li%20and%20Jingyan%20Jiang%0AAbstract%3A%20%20%20Test-time%20adaptation%20%28TTA%29%20effectively%20addresses%20distribution%20shifts%20between%0Atraining%20and%20testing%20data%20by%20adjusting%20models%20on%20test%20samples%2C%20which%20is%20crucial%0Afor%20improving%20model%20inference%20in%20real-world%20applications.%20However%2C%20traditional%0ATTA%20methods%20typically%20follow%20a%20fixed%20pattern%20to%20address%20the%20dynamic%20data%0Apatterns%20%28low-diversity%20or%20high-diversity%20patterns%29%20often%20leading%20to%0Aperformance%20degradation%20and%20consequently%20a%20decline%20in%20Quality%20of%20Experience%0A%28QoE%29.%20The%20primary%20issues%20we%20observed%20are%3ADifferent%20scenarios%20require%20different%0Anormalization%20methods%20%28e.g.%2C%20Instance%20Normalization%20is%20optimal%20in%20mixed%20domains%0Abut%20not%20in%20static%20domains%29.%20Model%20fine-tuning%20can%20potentially%20harm%20the%20model%0Aand%20waste%20time.Hence%2C%20it%20is%20crucial%20to%20design%20strategies%20for%20effectively%0Ameasuring%20and%20managing%20distribution%20diversity%20to%20minimize%20its%20negative%20impact%0Aon%20model%20performance.%20Based%20on%20these%20observations%2C%20this%20paper%20proposes%20a%20new%0Ageneral%20method%2C%20named%20Diversity%20Adaptive%20Test-Time%20Adaptation%20%28DATTA%29%2C%20aimed%20at%0Aimproving%20QoE.%20DATTA%20dynamically%20selects%20the%20best%20batch%20normalization%20methods%0Aand%20fine-tuning%20strategies%20by%20leveraging%20the%20Diversity%20Score%20to%20differentiate%0Abetween%20high%20and%20low%20diversity%20score%20batches.%20It%20features%20three%20key%20components%3A%0ADiversity%20Discrimination%20%28DD%29%20to%20assess%20batch%20diversity%2C%20Diversity%20Adaptive%0ABatch%20Normalization%20%28DABN%29%20to%20tailor%20normalization%20methods%20based%20on%20DD%0Ainsights%2C%20and%20Diversity%20Adaptive%20Fine-Tuning%20%28DAFT%29%20to%20selectively%20fine-tune%0Athe%20model.%20Experimental%20results%20show%20that%20our%20method%20achieves%20up%20to%20a%2021%25%0Aincrease%20in%20accuracy%20compared%20to%20state-of-the-art%20methodologies%2C%20indicating%0Athat%20our%20method%20maintains%20good%20model%20performance%20while%20demonstrating%20its%0Arobustness.%20Our%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDATTA%253A%2520Towards%2520Diversity%2520Adaptive%2520Test-Time%2520Adaptation%2520in%2520Dynamic%2520Wild%250A%2520%2520World%26entry.906535625%3DChuyang%2520Ye%2520and%2520Dongyan%2520Wei%2520and%2520Zhendong%2520Liu%2520and%2520Yuanyi%2520Pang%2520and%2520Yixi%2520Lin%2520and%2520Jiarong%2520Liao%2520and%2520Qinting%2520Jiang%2520and%2520Xianghua%2520Fu%2520and%2520Qing%2520Li%2520and%2520Jingyan%2520Jiang%26entry.1292438233%3D%2520%2520Test-time%2520adaptation%2520%2528TTA%2529%2520effectively%2520addresses%2520distribution%2520shifts%2520between%250Atraining%2520and%2520testing%2520data%2520by%2520adjusting%2520models%2520on%2520test%2520samples%252C%2520which%2520is%2520crucial%250Afor%2520improving%2520model%2520inference%2520in%2520real-world%2520applications.%2520However%252C%2520traditional%250ATTA%2520methods%2520typically%2520follow%2520a%2520fixed%2520pattern%2520to%2520address%2520the%2520dynamic%2520data%250Apatterns%2520%2528low-diversity%2520or%2520high-diversity%2520patterns%2529%2520often%2520leading%2520to%250Aperformance%2520degradation%2520and%2520consequently%2520a%2520decline%2520in%2520Quality%2520of%2520Experience%250A%2528QoE%2529.%2520The%2520primary%2520issues%2520we%2520observed%2520are%253ADifferent%2520scenarios%2520require%2520different%250Anormalization%2520methods%2520%2528e.g.%252C%2520Instance%2520Normalization%2520is%2520optimal%2520in%2520mixed%2520domains%250Abut%2520not%2520in%2520static%2520domains%2529.%2520Model%2520fine-tuning%2520can%2520potentially%2520harm%2520the%2520model%250Aand%2520waste%2520time.Hence%252C%2520it%2520is%2520crucial%2520to%2520design%2520strategies%2520for%2520effectively%250Ameasuring%2520and%2520managing%2520distribution%2520diversity%2520to%2520minimize%2520its%2520negative%2520impact%250Aon%2520model%2520performance.%2520Based%2520on%2520these%2520observations%252C%2520this%2520paper%2520proposes%2520a%2520new%250Ageneral%2520method%252C%2520named%2520Diversity%2520Adaptive%2520Test-Time%2520Adaptation%2520%2528DATTA%2529%252C%2520aimed%2520at%250Aimproving%2520QoE.%2520DATTA%2520dynamically%2520selects%2520the%2520best%2520batch%2520normalization%2520methods%250Aand%2520fine-tuning%2520strategies%2520by%2520leveraging%2520the%2520Diversity%2520Score%2520to%2520differentiate%250Abetween%2520high%2520and%2520low%2520diversity%2520score%2520batches.%2520It%2520features%2520three%2520key%2520components%253A%250ADiversity%2520Discrimination%2520%2528DD%2529%2520to%2520assess%2520batch%2520diversity%252C%2520Diversity%2520Adaptive%250ABatch%2520Normalization%2520%2528DABN%2529%2520to%2520tailor%2520normalization%2520methods%2520based%2520on%2520DD%250Ainsights%252C%2520and%2520Diversity%2520Adaptive%2520Fine-Tuning%2520%2528DAFT%2529%2520to%2520selectively%2520fine-tune%250Athe%2520model.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520achieves%2520up%2520to%2520a%252021%2525%250Aincrease%2520in%2520accuracy%2520compared%2520to%2520state-of-the-art%2520methodologies%252C%2520indicating%250Athat%2520our%2520method%2520maintains%2520good%2520model%2520performance%2520while%2520demonstrating%2520its%250Arobustness.%2520Our%2520code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DATTA%3A%20Towards%20Diversity%20Adaptive%20Test-Time%20Adaptation%20in%20Dynamic%20Wild%0A%20%20World&entry.906535625=Chuyang%20Ye%20and%20Dongyan%20Wei%20and%20Zhendong%20Liu%20and%20Yuanyi%20Pang%20and%20Yixi%20Lin%20and%20Jiarong%20Liao%20and%20Qinting%20Jiang%20and%20Xianghua%20Fu%20and%20Qing%20Li%20and%20Jingyan%20Jiang&entry.1292438233=%20%20Test-time%20adaptation%20%28TTA%29%20effectively%20addresses%20distribution%20shifts%20between%0Atraining%20and%20testing%20data%20by%20adjusting%20models%20on%20test%20samples%2C%20which%20is%20crucial%0Afor%20improving%20model%20inference%20in%20real-world%20applications.%20However%2C%20traditional%0ATTA%20methods%20typically%20follow%20a%20fixed%20pattern%20to%20address%20the%20dynamic%20data%0Apatterns%20%28low-diversity%20or%20high-diversity%20patterns%29%20often%20leading%20to%0Aperformance%20degradation%20and%20consequently%20a%20decline%20in%20Quality%20of%20Experience%0A%28QoE%29.%20The%20primary%20issues%20we%20observed%20are%3ADifferent%20scenarios%20require%20different%0Anormalization%20methods%20%28e.g.%2C%20Instance%20Normalization%20is%20optimal%20in%20mixed%20domains%0Abut%20not%20in%20static%20domains%29.%20Model%20fine-tuning%20can%20potentially%20harm%20the%20model%0Aand%20waste%20time.Hence%2C%20it%20is%20crucial%20to%20design%20strategies%20for%20effectively%0Ameasuring%20and%20managing%20distribution%20diversity%20to%20minimize%20its%20negative%20impact%0Aon%20model%20performance.%20Based%20on%20these%20observations%2C%20this%20paper%20proposes%20a%20new%0Ageneral%20method%2C%20named%20Diversity%20Adaptive%20Test-Time%20Adaptation%20%28DATTA%29%2C%20aimed%20at%0Aimproving%20QoE.%20DATTA%20dynamically%20selects%20the%20best%20batch%20normalization%20methods%0Aand%20fine-tuning%20strategies%20by%20leveraging%20the%20Diversity%20Score%20to%20differentiate%0Abetween%20high%20and%20low%20diversity%20score%20batches.%20It%20features%20three%20key%20components%3A%0ADiversity%20Discrimination%20%28DD%29%20to%20assess%20batch%20diversity%2C%20Diversity%20Adaptive%0ABatch%20Normalization%20%28DABN%29%20to%20tailor%20normalization%20methods%20based%20on%20DD%0Ainsights%2C%20and%20Diversity%20Adaptive%20Fine-Tuning%20%28DAFT%29%20to%20selectively%20fine-tune%0Athe%20model.%20Experimental%20results%20show%20that%20our%20method%20achieves%20up%20to%20a%2021%25%0Aincrease%20in%20accuracy%20compared%20to%20state-of-the-art%20methodologies%2C%20indicating%0Athat%20our%20method%20maintains%20good%20model%20performance%20while%20demonstrating%20its%0Arobustness.%20Our%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08056v1&entry.124074799=Read"},
{"title": "Label Dropout: Improved Deep Learning Echocardiography Segmentation\n  Using Multiple Datasets With Domain Shift and Partial Labelling", "author": "Iman Islam and Esther Puyol-Ant\u00f3n and Bram Ruijsink and Andrew J. Reader and Andrew P. King", "abstract": "  Echocardiography (echo) is the first imaging modality used when assessing\ncardiac function. The measurement of functional biomarkers from echo relies\nupon the segmentation of cardiac structures and deep learning models have been\nproposed to automate the segmentation process. However, in order to translate\nthese tools to widespread clinical use it is important that the segmentation\nmodels are robust to a wide variety of images (e.g. acquired from different\nscanners, by operators with different levels of expertise etc.). To achieve\nthis level of robustness it is necessary that the models are trained with\nmultiple diverse datasets. A significant challenge faced when training with\nmultiple diverse datasets is the variation in label presence, i.e. the combined\ndata are often partially-labelled. Adaptations of the cross entropy loss\nfunction have been proposed to deal with partially labelled data. In this paper\nwe show that training naively with such a loss function and multiple diverse\ndatasets can lead to a form of shortcut learning, where the model associates\nlabel presence with domain characteristics, leading to a drop in performance.\nTo address this problem, we propose a novel label dropout scheme to break the\nlink between domain characteristics and the presence or absence of labels. We\ndemonstrate that label dropout improves echo segmentation Dice score by 62% and\n25% on two cardiac structures when training using multiple diverse partially\nlabelled datasets.\n", "link": "http://arxiv.org/abs/2403.07818v2", "date": "2024-08-15", "relevancy": 2.0917, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5687}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5142}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label%20Dropout%3A%20Improved%20Deep%20Learning%20Echocardiography%20Segmentation%0A%20%20Using%20Multiple%20Datasets%20With%20Domain%20Shift%20and%20Partial%20Labelling&body=Title%3A%20Label%20Dropout%3A%20Improved%20Deep%20Learning%20Echocardiography%20Segmentation%0A%20%20Using%20Multiple%20Datasets%20With%20Domain%20Shift%20and%20Partial%20Labelling%0AAuthor%3A%20Iman%20Islam%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Bram%20Ruijsink%20and%20Andrew%20J.%20Reader%20and%20Andrew%20P.%20King%0AAbstract%3A%20%20%20Echocardiography%20%28echo%29%20is%20the%20first%20imaging%20modality%20used%20when%20assessing%0Acardiac%20function.%20The%20measurement%20of%20functional%20biomarkers%20from%20echo%20relies%0Aupon%20the%20segmentation%20of%20cardiac%20structures%20and%20deep%20learning%20models%20have%20been%0Aproposed%20to%20automate%20the%20segmentation%20process.%20However%2C%20in%20order%20to%20translate%0Athese%20tools%20to%20widespread%20clinical%20use%20it%20is%20important%20that%20the%20segmentation%0Amodels%20are%20robust%20to%20a%20wide%20variety%20of%20images%20%28e.g.%20acquired%20from%20different%0Ascanners%2C%20by%20operators%20with%20different%20levels%20of%20expertise%20etc.%29.%20To%20achieve%0Athis%20level%20of%20robustness%20it%20is%20necessary%20that%20the%20models%20are%20trained%20with%0Amultiple%20diverse%20datasets.%20A%20significant%20challenge%20faced%20when%20training%20with%0Amultiple%20diverse%20datasets%20is%20the%20variation%20in%20label%20presence%2C%20i.e.%20the%20combined%0Adata%20are%20often%20partially-labelled.%20Adaptations%20of%20the%20cross%20entropy%20loss%0Afunction%20have%20been%20proposed%20to%20deal%20with%20partially%20labelled%20data.%20In%20this%20paper%0Awe%20show%20that%20training%20naively%20with%20such%20a%20loss%20function%20and%20multiple%20diverse%0Adatasets%20can%20lead%20to%20a%20form%20of%20shortcut%20learning%2C%20where%20the%20model%20associates%0Alabel%20presence%20with%20domain%20characteristics%2C%20leading%20to%20a%20drop%20in%20performance.%0ATo%20address%20this%20problem%2C%20we%20propose%20a%20novel%20label%20dropout%20scheme%20to%20break%20the%0Alink%20between%20domain%20characteristics%20and%20the%20presence%20or%20absence%20of%20labels.%20We%0Ademonstrate%20that%20label%20dropout%20improves%20echo%20segmentation%20Dice%20score%20by%2062%25%20and%0A25%25%20on%20two%20cardiac%20structures%20when%20training%20using%20multiple%20diverse%20partially%0Alabelled%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07818v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel%2520Dropout%253A%2520Improved%2520Deep%2520Learning%2520Echocardiography%2520Segmentation%250A%2520%2520Using%2520Multiple%2520Datasets%2520With%2520Domain%2520Shift%2520and%2520Partial%2520Labelling%26entry.906535625%3DIman%2520Islam%2520and%2520Esther%2520Puyol-Ant%25C3%25B3n%2520and%2520Bram%2520Ruijsink%2520and%2520Andrew%2520J.%2520Reader%2520and%2520Andrew%2520P.%2520King%26entry.1292438233%3D%2520%2520Echocardiography%2520%2528echo%2529%2520is%2520the%2520first%2520imaging%2520modality%2520used%2520when%2520assessing%250Acardiac%2520function.%2520The%2520measurement%2520of%2520functional%2520biomarkers%2520from%2520echo%2520relies%250Aupon%2520the%2520segmentation%2520of%2520cardiac%2520structures%2520and%2520deep%2520learning%2520models%2520have%2520been%250Aproposed%2520to%2520automate%2520the%2520segmentation%2520process.%2520However%252C%2520in%2520order%2520to%2520translate%250Athese%2520tools%2520to%2520widespread%2520clinical%2520use%2520it%2520is%2520important%2520that%2520the%2520segmentation%250Amodels%2520are%2520robust%2520to%2520a%2520wide%2520variety%2520of%2520images%2520%2528e.g.%2520acquired%2520from%2520different%250Ascanners%252C%2520by%2520operators%2520with%2520different%2520levels%2520of%2520expertise%2520etc.%2529.%2520To%2520achieve%250Athis%2520level%2520of%2520robustness%2520it%2520is%2520necessary%2520that%2520the%2520models%2520are%2520trained%2520with%250Amultiple%2520diverse%2520datasets.%2520A%2520significant%2520challenge%2520faced%2520when%2520training%2520with%250Amultiple%2520diverse%2520datasets%2520is%2520the%2520variation%2520in%2520label%2520presence%252C%2520i.e.%2520the%2520combined%250Adata%2520are%2520often%2520partially-labelled.%2520Adaptations%2520of%2520the%2520cross%2520entropy%2520loss%250Afunction%2520have%2520been%2520proposed%2520to%2520deal%2520with%2520partially%2520labelled%2520data.%2520In%2520this%2520paper%250Awe%2520show%2520that%2520training%2520naively%2520with%2520such%2520a%2520loss%2520function%2520and%2520multiple%2520diverse%250Adatasets%2520can%2520lead%2520to%2520a%2520form%2520of%2520shortcut%2520learning%252C%2520where%2520the%2520model%2520associates%250Alabel%2520presence%2520with%2520domain%2520characteristics%252C%2520leading%2520to%2520a%2520drop%2520in%2520performance.%250ATo%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520label%2520dropout%2520scheme%2520to%2520break%2520the%250Alink%2520between%2520domain%2520characteristics%2520and%2520the%2520presence%2520or%2520absence%2520of%2520labels.%2520We%250Ademonstrate%2520that%2520label%2520dropout%2520improves%2520echo%2520segmentation%2520Dice%2520score%2520by%252062%2525%2520and%250A25%2525%2520on%2520two%2520cardiac%2520structures%2520when%2520training%2520using%2520multiple%2520diverse%2520partially%250Alabelled%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07818v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Dropout%3A%20Improved%20Deep%20Learning%20Echocardiography%20Segmentation%0A%20%20Using%20Multiple%20Datasets%20With%20Domain%20Shift%20and%20Partial%20Labelling&entry.906535625=Iman%20Islam%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Bram%20Ruijsink%20and%20Andrew%20J.%20Reader%20and%20Andrew%20P.%20King&entry.1292438233=%20%20Echocardiography%20%28echo%29%20is%20the%20first%20imaging%20modality%20used%20when%20assessing%0Acardiac%20function.%20The%20measurement%20of%20functional%20biomarkers%20from%20echo%20relies%0Aupon%20the%20segmentation%20of%20cardiac%20structures%20and%20deep%20learning%20models%20have%20been%0Aproposed%20to%20automate%20the%20segmentation%20process.%20However%2C%20in%20order%20to%20translate%0Athese%20tools%20to%20widespread%20clinical%20use%20it%20is%20important%20that%20the%20segmentation%0Amodels%20are%20robust%20to%20a%20wide%20variety%20of%20images%20%28e.g.%20acquired%20from%20different%0Ascanners%2C%20by%20operators%20with%20different%20levels%20of%20expertise%20etc.%29.%20To%20achieve%0Athis%20level%20of%20robustness%20it%20is%20necessary%20that%20the%20models%20are%20trained%20with%0Amultiple%20diverse%20datasets.%20A%20significant%20challenge%20faced%20when%20training%20with%0Amultiple%20diverse%20datasets%20is%20the%20variation%20in%20label%20presence%2C%20i.e.%20the%20combined%0Adata%20are%20often%20partially-labelled.%20Adaptations%20of%20the%20cross%20entropy%20loss%0Afunction%20have%20been%20proposed%20to%20deal%20with%20partially%20labelled%20data.%20In%20this%20paper%0Awe%20show%20that%20training%20naively%20with%20such%20a%20loss%20function%20and%20multiple%20diverse%0Adatasets%20can%20lead%20to%20a%20form%20of%20shortcut%20learning%2C%20where%20the%20model%20associates%0Alabel%20presence%20with%20domain%20characteristics%2C%20leading%20to%20a%20drop%20in%20performance.%0ATo%20address%20this%20problem%2C%20we%20propose%20a%20novel%20label%20dropout%20scheme%20to%20break%20the%0Alink%20between%20domain%20characteristics%20and%20the%20presence%20or%20absence%20of%20labels.%20We%0Ademonstrate%20that%20label%20dropout%20improves%20echo%20segmentation%20Dice%20score%20by%2062%25%20and%0A25%25%20on%20two%20cardiac%20structures%20when%20training%20using%20multiple%20diverse%20partially%0Alabelled%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07818v2&entry.124074799=Read"},
{"title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties", "author": "Junfei Xiao and Ziqi Zhou and Wenxuan Li and Shiyi Lan and Jieru Mei and Zhiding Yu and Alan Yuille and Yuyin Zhou and Cihang Xie", "abstract": "  This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab.\n", "link": "http://arxiv.org/abs/2312.13764v3", "date": "2024-08-15", "relevancy": 2.086, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Semantic%20Space%20is%20Worth%20256%20Language%20Descriptions%3A%20Make%20Stronger%0A%20%20Segmentation%20Models%20with%20Descriptive%20Properties&body=Title%3A%20A%20Semantic%20Space%20is%20Worth%20256%20Language%20Descriptions%3A%20Make%20Stronger%0A%20%20Segmentation%20Models%20with%20Descriptive%20Properties%0AAuthor%3A%20Junfei%20Xiao%20and%20Ziqi%20Zhou%20and%20Wenxuan%20Li%20and%20Shiyi%20Lan%20and%20Jieru%20Mei%20and%20Zhiding%20Yu%20and%20Alan%20Yuille%20and%20Yuyin%20Zhou%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20This%20paper%20introduces%20ProLab%2C%20a%20novel%20approach%20using%20property-level%20label%0Aspace%20for%20creating%20strong%20interpretable%20segmentation%20models.%20Instead%20of%20relying%0Asolely%20on%20category-specific%20annotations%2C%20ProLab%20uses%20descriptive%20properties%0Agrounded%20in%20common%20sense%20knowledge%20for%20supervising%20segmentation%20models.%20It%20is%0Abased%20on%20two%20core%20designs.%20First%2C%20we%20employ%20Large%20Language%20Models%20%28LLMs%29%20and%0Acarefully%20crafted%20prompts%20to%20generate%20descriptions%20of%20all%20involved%20categories%0Athat%20carry%20meaningful%20common%20sense%20knowledge%20and%20follow%20a%20structured%20format.%0ASecond%2C%20we%20introduce%20a%20description%20embedding%20model%20preserving%20semantic%0Acorrelation%20across%20descriptions%20and%20then%20cluster%20them%20into%20a%20set%20of%20descriptive%0Aproperties%20%28e.g.%2C%20256%29%20using%20K-Means.%20These%20properties%20are%20based%20on%0Ainterpretable%20common%20sense%20knowledge%20consistent%20with%20theories%20of%20human%0Arecognition.%20We%20empirically%20show%20that%20our%20approach%20makes%20segmentation%20models%0Aperform%20stronger%20on%20five%20classic%20benchmarks%20%28e.g.%2C%20ADE20K%2C%20COCO-Stuff%2C%20Pascal%0AContext%2C%20Cityscapes%2C%20and%20BDD%29.%20Our%20method%20also%20shows%20better%20scalability%20with%0Aextended%20training%20steps%20than%20category-level%20supervision.%20Our%20interpretable%0Asegmentation%20framework%20also%20emerges%20with%20the%20generalization%20ability%20to%20segment%0Aout-of-domain%20or%20unknown%20categories%20using%20only%20in-domain%20descriptive%0Aproperties.%20Code%20is%20available%20at%20https%3A//github.com/lambert-x/ProLab.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13764v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Semantic%2520Space%2520is%2520Worth%2520256%2520Language%2520Descriptions%253A%2520Make%2520Stronger%250A%2520%2520Segmentation%2520Models%2520with%2520Descriptive%2520Properties%26entry.906535625%3DJunfei%2520Xiao%2520and%2520Ziqi%2520Zhou%2520and%2520Wenxuan%2520Li%2520and%2520Shiyi%2520Lan%2520and%2520Jieru%2520Mei%2520and%2520Zhiding%2520Yu%2520and%2520Alan%2520Yuille%2520and%2520Yuyin%2520Zhou%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520ProLab%252C%2520a%2520novel%2520approach%2520using%2520property-level%2520label%250Aspace%2520for%2520creating%2520strong%2520interpretable%2520segmentation%2520models.%2520Instead%2520of%2520relying%250Asolely%2520on%2520category-specific%2520annotations%252C%2520ProLab%2520uses%2520descriptive%2520properties%250Agrounded%2520in%2520common%2520sense%2520knowledge%2520for%2520supervising%2520segmentation%2520models.%2520It%2520is%250Abased%2520on%2520two%2520core%2520designs.%2520First%252C%2520we%2520employ%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%250Acarefully%2520crafted%2520prompts%2520to%2520generate%2520descriptions%2520of%2520all%2520involved%2520categories%250Athat%2520carry%2520meaningful%2520common%2520sense%2520knowledge%2520and%2520follow%2520a%2520structured%2520format.%250ASecond%252C%2520we%2520introduce%2520a%2520description%2520embedding%2520model%2520preserving%2520semantic%250Acorrelation%2520across%2520descriptions%2520and%2520then%2520cluster%2520them%2520into%2520a%2520set%2520of%2520descriptive%250Aproperties%2520%2528e.g.%252C%2520256%2529%2520using%2520K-Means.%2520These%2520properties%2520are%2520based%2520on%250Ainterpretable%2520common%2520sense%2520knowledge%2520consistent%2520with%2520theories%2520of%2520human%250Arecognition.%2520We%2520empirically%2520show%2520that%2520our%2520approach%2520makes%2520segmentation%2520models%250Aperform%2520stronger%2520on%2520five%2520classic%2520benchmarks%2520%2528e.g.%252C%2520ADE20K%252C%2520COCO-Stuff%252C%2520Pascal%250AContext%252C%2520Cityscapes%252C%2520and%2520BDD%2529.%2520Our%2520method%2520also%2520shows%2520better%2520scalability%2520with%250Aextended%2520training%2520steps%2520than%2520category-level%2520supervision.%2520Our%2520interpretable%250Asegmentation%2520framework%2520also%2520emerges%2520with%2520the%2520generalization%2520ability%2520to%2520segment%250Aout-of-domain%2520or%2520unknown%2520categories%2520using%2520only%2520in-domain%2520descriptive%250Aproperties.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/lambert-x/ProLab.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13764v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Semantic%20Space%20is%20Worth%20256%20Language%20Descriptions%3A%20Make%20Stronger%0A%20%20Segmentation%20Models%20with%20Descriptive%20Properties&entry.906535625=Junfei%20Xiao%20and%20Ziqi%20Zhou%20and%20Wenxuan%20Li%20and%20Shiyi%20Lan%20and%20Jieru%20Mei%20and%20Zhiding%20Yu%20and%20Alan%20Yuille%20and%20Yuyin%20Zhou%20and%20Cihang%20Xie&entry.1292438233=%20%20This%20paper%20introduces%20ProLab%2C%20a%20novel%20approach%20using%20property-level%20label%0Aspace%20for%20creating%20strong%20interpretable%20segmentation%20models.%20Instead%20of%20relying%0Asolely%20on%20category-specific%20annotations%2C%20ProLab%20uses%20descriptive%20properties%0Agrounded%20in%20common%20sense%20knowledge%20for%20supervising%20segmentation%20models.%20It%20is%0Abased%20on%20two%20core%20designs.%20First%2C%20we%20employ%20Large%20Language%20Models%20%28LLMs%29%20and%0Acarefully%20crafted%20prompts%20to%20generate%20descriptions%20of%20all%20involved%20categories%0Athat%20carry%20meaningful%20common%20sense%20knowledge%20and%20follow%20a%20structured%20format.%0ASecond%2C%20we%20introduce%20a%20description%20embedding%20model%20preserving%20semantic%0Acorrelation%20across%20descriptions%20and%20then%20cluster%20them%20into%20a%20set%20of%20descriptive%0Aproperties%20%28e.g.%2C%20256%29%20using%20K-Means.%20These%20properties%20are%20based%20on%0Ainterpretable%20common%20sense%20knowledge%20consistent%20with%20theories%20of%20human%0Arecognition.%20We%20empirically%20show%20that%20our%20approach%20makes%20segmentation%20models%0Aperform%20stronger%20on%20five%20classic%20benchmarks%20%28e.g.%2C%20ADE20K%2C%20COCO-Stuff%2C%20Pascal%0AContext%2C%20Cityscapes%2C%20and%20BDD%29.%20Our%20method%20also%20shows%20better%20scalability%20with%0Aextended%20training%20steps%20than%20category-level%20supervision.%20Our%20interpretable%0Asegmentation%20framework%20also%20emerges%20with%20the%20generalization%20ability%20to%20segment%0Aout-of-domain%20or%20unknown%20categories%20using%20only%20in-domain%20descriptive%0Aproperties.%20Code%20is%20available%20at%20https%3A//github.com/lambert-x/ProLab.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13764v3&entry.124074799=Read"},
{"title": "The Z-Gromov-Wasserstein Distance", "author": "Martin Bauer and Facundo M\u00e9moli and Tom Needham and Mao Nishino", "abstract": "  The Gromov-Wasserstein (GW) distance is a powerful tool for comparing metric\nmeasure spaces which has found broad applications in data science and machine\nlearning. Driven by the need to analyze datasets whose objects have\nincreasingly complex structure (such as node and edge-attributed graphs),\nseveral variants of GW distance have been introduced in the recent literature.\nWith a view toward establishing a general framework for the theory of GW-like\ndistances, this paper considers a vast generalization of the notion of a metric\nmeasure space: for an arbitrary metric space $Z$, we define a $Z$-network to be\na measure space endowed with a kernel valued in $Z$. We introduce a method for\ncomparing $Z$-networks by defining a generalization of GW distance, which we\nrefer to as $Z$-Gromov-Wasserstein ($Z$-GW) distance. This construction\nsubsumes many previously known metrics and offers a unified approach to\nunderstanding their shared properties. The paper demonstrates that the $Z$-GW\ndistance defines a metric on the space of $Z$-networks which retains desirable\nproperties of $Z$, such as separability, completeness, and geodesicity. Many of\nthese properties were unknown for existing variants of GW distance that fall\nunder our framework. Our focus is on foundational theory, but our results also\ninclude computable lower bounds and approximations of the distance which will\nbe useful for practical applications.\n", "link": "http://arxiv.org/abs/2408.08233v1", "date": "2024-08-15", "relevancy": 2.0742, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4367}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4132}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Z-Gromov-Wasserstein%20Distance&body=Title%3A%20The%20Z-Gromov-Wasserstein%20Distance%0AAuthor%3A%20Martin%20Bauer%20and%20Facundo%20M%C3%A9moli%20and%20Tom%20Needham%20and%20Mao%20Nishino%0AAbstract%3A%20%20%20The%20Gromov-Wasserstein%20%28GW%29%20distance%20is%20a%20powerful%20tool%20for%20comparing%20metric%0Ameasure%20spaces%20which%20has%20found%20broad%20applications%20in%20data%20science%20and%20machine%0Alearning.%20Driven%20by%20the%20need%20to%20analyze%20datasets%20whose%20objects%20have%0Aincreasingly%20complex%20structure%20%28such%20as%20node%20and%20edge-attributed%20graphs%29%2C%0Aseveral%20variants%20of%20GW%20distance%20have%20been%20introduced%20in%20the%20recent%20literature.%0AWith%20a%20view%20toward%20establishing%20a%20general%20framework%20for%20the%20theory%20of%20GW-like%0Adistances%2C%20this%20paper%20considers%20a%20vast%20generalization%20of%20the%20notion%20of%20a%20metric%0Ameasure%20space%3A%20for%20an%20arbitrary%20metric%20space%20%24Z%24%2C%20we%20define%20a%20%24Z%24-network%20to%20be%0Aa%20measure%20space%20endowed%20with%20a%20kernel%20valued%20in%20%24Z%24.%20We%20introduce%20a%20method%20for%0Acomparing%20%24Z%24-networks%20by%20defining%20a%20generalization%20of%20GW%20distance%2C%20which%20we%0Arefer%20to%20as%20%24Z%24-Gromov-Wasserstein%20%28%24Z%24-GW%29%20distance.%20This%20construction%0Asubsumes%20many%20previously%20known%20metrics%20and%20offers%20a%20unified%20approach%20to%0Aunderstanding%20their%20shared%20properties.%20The%20paper%20demonstrates%20that%20the%20%24Z%24-GW%0Adistance%20defines%20a%20metric%20on%20the%20space%20of%20%24Z%24-networks%20which%20retains%20desirable%0Aproperties%20of%20%24Z%24%2C%20such%20as%20separability%2C%20completeness%2C%20and%20geodesicity.%20Many%20of%0Athese%20properties%20were%20unknown%20for%20existing%20variants%20of%20GW%20distance%20that%20fall%0Aunder%20our%20framework.%20Our%20focus%20is%20on%20foundational%20theory%2C%20but%20our%20results%20also%0Ainclude%20computable%20lower%20bounds%20and%20approximations%20of%20the%20distance%20which%20will%0Abe%20useful%20for%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Z-Gromov-Wasserstein%2520Distance%26entry.906535625%3DMartin%2520Bauer%2520and%2520Facundo%2520M%25C3%25A9moli%2520and%2520Tom%2520Needham%2520and%2520Mao%2520Nishino%26entry.1292438233%3D%2520%2520The%2520Gromov-Wasserstein%2520%2528GW%2529%2520distance%2520is%2520a%2520powerful%2520tool%2520for%2520comparing%2520metric%250Ameasure%2520spaces%2520which%2520has%2520found%2520broad%2520applications%2520in%2520data%2520science%2520and%2520machine%250Alearning.%2520Driven%2520by%2520the%2520need%2520to%2520analyze%2520datasets%2520whose%2520objects%2520have%250Aincreasingly%2520complex%2520structure%2520%2528such%2520as%2520node%2520and%2520edge-attributed%2520graphs%2529%252C%250Aseveral%2520variants%2520of%2520GW%2520distance%2520have%2520been%2520introduced%2520in%2520the%2520recent%2520literature.%250AWith%2520a%2520view%2520toward%2520establishing%2520a%2520general%2520framework%2520for%2520the%2520theory%2520of%2520GW-like%250Adistances%252C%2520this%2520paper%2520considers%2520a%2520vast%2520generalization%2520of%2520the%2520notion%2520of%2520a%2520metric%250Ameasure%2520space%253A%2520for%2520an%2520arbitrary%2520metric%2520space%2520%2524Z%2524%252C%2520we%2520define%2520a%2520%2524Z%2524-network%2520to%2520be%250Aa%2520measure%2520space%2520endowed%2520with%2520a%2520kernel%2520valued%2520in%2520%2524Z%2524.%2520We%2520introduce%2520a%2520method%2520for%250Acomparing%2520%2524Z%2524-networks%2520by%2520defining%2520a%2520generalization%2520of%2520GW%2520distance%252C%2520which%2520we%250Arefer%2520to%2520as%2520%2524Z%2524-Gromov-Wasserstein%2520%2528%2524Z%2524-GW%2529%2520distance.%2520This%2520construction%250Asubsumes%2520many%2520previously%2520known%2520metrics%2520and%2520offers%2520a%2520unified%2520approach%2520to%250Aunderstanding%2520their%2520shared%2520properties.%2520The%2520paper%2520demonstrates%2520that%2520the%2520%2524Z%2524-GW%250Adistance%2520defines%2520a%2520metric%2520on%2520the%2520space%2520of%2520%2524Z%2524-networks%2520which%2520retains%2520desirable%250Aproperties%2520of%2520%2524Z%2524%252C%2520such%2520as%2520separability%252C%2520completeness%252C%2520and%2520geodesicity.%2520Many%2520of%250Athese%2520properties%2520were%2520unknown%2520for%2520existing%2520variants%2520of%2520GW%2520distance%2520that%2520fall%250Aunder%2520our%2520framework.%2520Our%2520focus%2520is%2520on%2520foundational%2520theory%252C%2520but%2520our%2520results%2520also%250Ainclude%2520computable%2520lower%2520bounds%2520and%2520approximations%2520of%2520the%2520distance%2520which%2520will%250Abe%2520useful%2520for%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Z-Gromov-Wasserstein%20Distance&entry.906535625=Martin%20Bauer%20and%20Facundo%20M%C3%A9moli%20and%20Tom%20Needham%20and%20Mao%20Nishino&entry.1292438233=%20%20The%20Gromov-Wasserstein%20%28GW%29%20distance%20is%20a%20powerful%20tool%20for%20comparing%20metric%0Ameasure%20spaces%20which%20has%20found%20broad%20applications%20in%20data%20science%20and%20machine%0Alearning.%20Driven%20by%20the%20need%20to%20analyze%20datasets%20whose%20objects%20have%0Aincreasingly%20complex%20structure%20%28such%20as%20node%20and%20edge-attributed%20graphs%29%2C%0Aseveral%20variants%20of%20GW%20distance%20have%20been%20introduced%20in%20the%20recent%20literature.%0AWith%20a%20view%20toward%20establishing%20a%20general%20framework%20for%20the%20theory%20of%20GW-like%0Adistances%2C%20this%20paper%20considers%20a%20vast%20generalization%20of%20the%20notion%20of%20a%20metric%0Ameasure%20space%3A%20for%20an%20arbitrary%20metric%20space%20%24Z%24%2C%20we%20define%20a%20%24Z%24-network%20to%20be%0Aa%20measure%20space%20endowed%20with%20a%20kernel%20valued%20in%20%24Z%24.%20We%20introduce%20a%20method%20for%0Acomparing%20%24Z%24-networks%20by%20defining%20a%20generalization%20of%20GW%20distance%2C%20which%20we%0Arefer%20to%20as%20%24Z%24-Gromov-Wasserstein%20%28%24Z%24-GW%29%20distance.%20This%20construction%0Asubsumes%20many%20previously%20known%20metrics%20and%20offers%20a%20unified%20approach%20to%0Aunderstanding%20their%20shared%20properties.%20The%20paper%20demonstrates%20that%20the%20%24Z%24-GW%0Adistance%20defines%20a%20metric%20on%20the%20space%20of%20%24Z%24-networks%20which%20retains%20desirable%0Aproperties%20of%20%24Z%24%2C%20such%20as%20separability%2C%20completeness%2C%20and%20geodesicity.%20Many%20of%0Athese%20properties%20were%20unknown%20for%20existing%20variants%20of%20GW%20distance%20that%20fall%0Aunder%20our%20framework.%20Our%20focus%20is%20on%20foundational%20theory%2C%20but%20our%20results%20also%0Ainclude%20computable%20lower%20bounds%20and%20approximations%20of%20the%20distance%20which%20will%0Abe%20useful%20for%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08233v1&entry.124074799=Read"},
{"title": "Asymmetrical estimator for training encapsulated deep photonic neural\n  networks", "author": "Yizhi Wang and Minjia Chen and Chunhui Yao and Jie Ma and Ting Yan and Richard Penty and Qixiang Cheng", "abstract": "  Scalable isomorphic physical neural networks (PNNs) are emerging NN\nacceleration paradigms for their high-bandwidth, in-propagation computation.\nDespite backpropagation (BP)-based training is often the industry standard for\nits robustness and fast gradient convergences, existing BP-PNN training methods\nneed to truncate the propagation of analogue signal at each layer and acquire\naccurate hidden neuron readouts for deep networks. This compromises the\nincentive of PNN for fast in-propagation processing. In addition, the required\nreadouts introduce massive bottlenecks due to the conversions between the\nanalogue-digital interfaces to shuttle information across. These factors limit\nboth the time and energy efficiency during training. Here we introduce the\nasymmetrical training (AT) method, a BP-based method that can perform training\non an encapsulated deep network, where the information propagation is\nmaintained within the analogue domain until the output layer. AT's minimum\ninformation access bypass analogue-digital interface bottleneck wherever\npossible. For any deep network structure, AT offers significantly improved time\nand energy efficiency compared to existing BP-PNN methods, and scales well for\nlarge network sizes. We demonstrated AT's error-tolerant and calibration-free\ntraining for encapsulated integrated photonic deep networks to achieve near\nideal BP performances. AT's well-behaved training is demonstrated repeatably\nacross different datasets and network structures\n", "link": "http://arxiv.org/abs/2405.18458v2", "date": "2024-08-15", "relevancy": 2.0684, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5521}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4926}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymmetrical%20estimator%20for%20training%20encapsulated%20deep%20photonic%20neural%0A%20%20networks&body=Title%3A%20Asymmetrical%20estimator%20for%20training%20encapsulated%20deep%20photonic%20neural%0A%20%20networks%0AAuthor%3A%20Yizhi%20Wang%20and%20Minjia%20Chen%20and%20Chunhui%20Yao%20and%20Jie%20Ma%20and%20Ting%20Yan%20and%20Richard%20Penty%20and%20Qixiang%20Cheng%0AAbstract%3A%20%20%20Scalable%20isomorphic%20physical%20neural%20networks%20%28PNNs%29%20are%20emerging%20NN%0Aacceleration%20paradigms%20for%20their%20high-bandwidth%2C%20in-propagation%20computation.%0ADespite%20backpropagation%20%28BP%29-based%20training%20is%20often%20the%20industry%20standard%20for%0Aits%20robustness%20and%20fast%20gradient%20convergences%2C%20existing%20BP-PNN%20training%20methods%0Aneed%20to%20truncate%20the%20propagation%20of%20analogue%20signal%20at%20each%20layer%20and%20acquire%0Aaccurate%20hidden%20neuron%20readouts%20for%20deep%20networks.%20This%20compromises%20the%0Aincentive%20of%20PNN%20for%20fast%20in-propagation%20processing.%20In%20addition%2C%20the%20required%0Areadouts%20introduce%20massive%20bottlenecks%20due%20to%20the%20conversions%20between%20the%0Aanalogue-digital%20interfaces%20to%20shuttle%20information%20across.%20These%20factors%20limit%0Aboth%20the%20time%20and%20energy%20efficiency%20during%20training.%20Here%20we%20introduce%20the%0Aasymmetrical%20training%20%28AT%29%20method%2C%20a%20BP-based%20method%20that%20can%20perform%20training%0Aon%20an%20encapsulated%20deep%20network%2C%20where%20the%20information%20propagation%20is%0Amaintained%20within%20the%20analogue%20domain%20until%20the%20output%20layer.%20AT%27s%20minimum%0Ainformation%20access%20bypass%20analogue-digital%20interface%20bottleneck%20wherever%0Apossible.%20For%20any%20deep%20network%20structure%2C%20AT%20offers%20significantly%20improved%20time%0Aand%20energy%20efficiency%20compared%20to%20existing%20BP-PNN%20methods%2C%20and%20scales%20well%20for%0Alarge%20network%20sizes.%20We%20demonstrated%20AT%27s%20error-tolerant%20and%20calibration-free%0Atraining%20for%20encapsulated%20integrated%20photonic%20deep%20networks%20to%20achieve%20near%0Aideal%20BP%20performances.%20AT%27s%20well-behaved%20training%20is%20demonstrated%20repeatably%0Aacross%20different%20datasets%20and%20network%20structures%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymmetrical%2520estimator%2520for%2520training%2520encapsulated%2520deep%2520photonic%2520neural%250A%2520%2520networks%26entry.906535625%3DYizhi%2520Wang%2520and%2520Minjia%2520Chen%2520and%2520Chunhui%2520Yao%2520and%2520Jie%2520Ma%2520and%2520Ting%2520Yan%2520and%2520Richard%2520Penty%2520and%2520Qixiang%2520Cheng%26entry.1292438233%3D%2520%2520Scalable%2520isomorphic%2520physical%2520neural%2520networks%2520%2528PNNs%2529%2520are%2520emerging%2520NN%250Aacceleration%2520paradigms%2520for%2520their%2520high-bandwidth%252C%2520in-propagation%2520computation.%250ADespite%2520backpropagation%2520%2528BP%2529-based%2520training%2520is%2520often%2520the%2520industry%2520standard%2520for%250Aits%2520robustness%2520and%2520fast%2520gradient%2520convergences%252C%2520existing%2520BP-PNN%2520training%2520methods%250Aneed%2520to%2520truncate%2520the%2520propagation%2520of%2520analogue%2520signal%2520at%2520each%2520layer%2520and%2520acquire%250Aaccurate%2520hidden%2520neuron%2520readouts%2520for%2520deep%2520networks.%2520This%2520compromises%2520the%250Aincentive%2520of%2520PNN%2520for%2520fast%2520in-propagation%2520processing.%2520In%2520addition%252C%2520the%2520required%250Areadouts%2520introduce%2520massive%2520bottlenecks%2520due%2520to%2520the%2520conversions%2520between%2520the%250Aanalogue-digital%2520interfaces%2520to%2520shuttle%2520information%2520across.%2520These%2520factors%2520limit%250Aboth%2520the%2520time%2520and%2520energy%2520efficiency%2520during%2520training.%2520Here%2520we%2520introduce%2520the%250Aasymmetrical%2520training%2520%2528AT%2529%2520method%252C%2520a%2520BP-based%2520method%2520that%2520can%2520perform%2520training%250Aon%2520an%2520encapsulated%2520deep%2520network%252C%2520where%2520the%2520information%2520propagation%2520is%250Amaintained%2520within%2520the%2520analogue%2520domain%2520until%2520the%2520output%2520layer.%2520AT%2527s%2520minimum%250Ainformation%2520access%2520bypass%2520analogue-digital%2520interface%2520bottleneck%2520wherever%250Apossible.%2520For%2520any%2520deep%2520network%2520structure%252C%2520AT%2520offers%2520significantly%2520improved%2520time%250Aand%2520energy%2520efficiency%2520compared%2520to%2520existing%2520BP-PNN%2520methods%252C%2520and%2520scales%2520well%2520for%250Alarge%2520network%2520sizes.%2520We%2520demonstrated%2520AT%2527s%2520error-tolerant%2520and%2520calibration-free%250Atraining%2520for%2520encapsulated%2520integrated%2520photonic%2520deep%2520networks%2520to%2520achieve%2520near%250Aideal%2520BP%2520performances.%2520AT%2527s%2520well-behaved%2520training%2520is%2520demonstrated%2520repeatably%250Aacross%2520different%2520datasets%2520and%2520network%2520structures%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymmetrical%20estimator%20for%20training%20encapsulated%20deep%20photonic%20neural%0A%20%20networks&entry.906535625=Yizhi%20Wang%20and%20Minjia%20Chen%20and%20Chunhui%20Yao%20and%20Jie%20Ma%20and%20Ting%20Yan%20and%20Richard%20Penty%20and%20Qixiang%20Cheng&entry.1292438233=%20%20Scalable%20isomorphic%20physical%20neural%20networks%20%28PNNs%29%20are%20emerging%20NN%0Aacceleration%20paradigms%20for%20their%20high-bandwidth%2C%20in-propagation%20computation.%0ADespite%20backpropagation%20%28BP%29-based%20training%20is%20often%20the%20industry%20standard%20for%0Aits%20robustness%20and%20fast%20gradient%20convergences%2C%20existing%20BP-PNN%20training%20methods%0Aneed%20to%20truncate%20the%20propagation%20of%20analogue%20signal%20at%20each%20layer%20and%20acquire%0Aaccurate%20hidden%20neuron%20readouts%20for%20deep%20networks.%20This%20compromises%20the%0Aincentive%20of%20PNN%20for%20fast%20in-propagation%20processing.%20In%20addition%2C%20the%20required%0Areadouts%20introduce%20massive%20bottlenecks%20due%20to%20the%20conversions%20between%20the%0Aanalogue-digital%20interfaces%20to%20shuttle%20information%20across.%20These%20factors%20limit%0Aboth%20the%20time%20and%20energy%20efficiency%20during%20training.%20Here%20we%20introduce%20the%0Aasymmetrical%20training%20%28AT%29%20method%2C%20a%20BP-based%20method%20that%20can%20perform%20training%0Aon%20an%20encapsulated%20deep%20network%2C%20where%20the%20information%20propagation%20is%0Amaintained%20within%20the%20analogue%20domain%20until%20the%20output%20layer.%20AT%27s%20minimum%0Ainformation%20access%20bypass%20analogue-digital%20interface%20bottleneck%20wherever%0Apossible.%20For%20any%20deep%20network%20structure%2C%20AT%20offers%20significantly%20improved%20time%0Aand%20energy%20efficiency%20compared%20to%20existing%20BP-PNN%20methods%2C%20and%20scales%20well%20for%0Alarge%20network%20sizes.%20We%20demonstrated%20AT%27s%20error-tolerant%20and%20calibration-free%0Atraining%20for%20encapsulated%20integrated%20photonic%20deep%20networks%20to%20achieve%20near%0Aideal%20BP%20performances.%20AT%27s%20well-behaved%20training%20is%20demonstrated%20repeatably%0Aacross%20different%20datasets%20and%20network%20structures%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18458v2&entry.124074799=Read"},
{"title": "Few Shot Class Incremental Learning using Vision-Language models", "author": "Anurag Kumar and Chinmay Bharti and Saikat Dutta and Srikrishna Karanam and Biplab Banerjee", "abstract": "  Recent advancements in deep learning have demonstrated remarkable performance\ncomparable to human capabilities across various supervised computer vision\ntasks. However, the prevalent assumption of having an extensive pool of\ntraining data encompassing all classes prior to model training often diverges\nfrom real-world scenarios, where limited data availability for novel classes is\nthe norm. The challenge emerges in seamlessly integrating new classes with few\nsamples into the training data, demanding the model to adeptly accommodate\nthese additions without compromising its performance on base classes. To\naddress this exigency, the research community has introduced several solutions\nunder the realm of few-shot class incremental learning (FSCIL).\n  In this study, we introduce an innovative FSCIL framework that utilizes\nlanguage regularizer and subspace regularizer. During base training, the\nlanguage regularizer helps incorporate semantic information extracted from a\nVision-Language model. The subspace regularizer helps in facilitating the\nmodel's acquisition of nuanced connections between image and text semantics\ninherent to base classes during incremental training. Our proposed framework\nnot only empowers the model to embrace novel classes with limited data, but\nalso ensures the preservation of performance on base classes. To substantiate\nthe efficacy of our approach, we conduct comprehensive experiments on three\ndistinct FSCIL benchmarks, where our framework attains state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2405.01040v2", "date": "2024-08-15", "relevancy": 2.0612, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5202}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.515}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few%20Shot%20Class%20Incremental%20Learning%20using%20Vision-Language%20models&body=Title%3A%20Few%20Shot%20Class%20Incremental%20Learning%20using%20Vision-Language%20models%0AAuthor%3A%20Anurag%20Kumar%20and%20Chinmay%20Bharti%20and%20Saikat%20Dutta%20and%20Srikrishna%20Karanam%20and%20Biplab%20Banerjee%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20have%20demonstrated%20remarkable%20performance%0Acomparable%20to%20human%20capabilities%20across%20various%20supervised%20computer%20vision%0Atasks.%20However%2C%20the%20prevalent%20assumption%20of%20having%20an%20extensive%20pool%20of%0Atraining%20data%20encompassing%20all%20classes%20prior%20to%20model%20training%20often%20diverges%0Afrom%20real-world%20scenarios%2C%20where%20limited%20data%20availability%20for%20novel%20classes%20is%0Athe%20norm.%20The%20challenge%20emerges%20in%20seamlessly%20integrating%20new%20classes%20with%20few%0Asamples%20into%20the%20training%20data%2C%20demanding%20the%20model%20to%20adeptly%20accommodate%0Athese%20additions%20without%20compromising%20its%20performance%20on%20base%20classes.%20To%0Aaddress%20this%20exigency%2C%20the%20research%20community%20has%20introduced%20several%20solutions%0Aunder%20the%20realm%20of%20few-shot%20class%20incremental%20learning%20%28FSCIL%29.%0A%20%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20FSCIL%20framework%20that%20utilizes%0Alanguage%20regularizer%20and%20subspace%20regularizer.%20During%20base%20training%2C%20the%0Alanguage%20regularizer%20helps%20incorporate%20semantic%20information%20extracted%20from%20a%0AVision-Language%20model.%20The%20subspace%20regularizer%20helps%20in%20facilitating%20the%0Amodel%27s%20acquisition%20of%20nuanced%20connections%20between%20image%20and%20text%20semantics%0Ainherent%20to%20base%20classes%20during%20incremental%20training.%20Our%20proposed%20framework%0Anot%20only%20empowers%20the%20model%20to%20embrace%20novel%20classes%20with%20limited%20data%2C%20but%0Aalso%20ensures%20the%20preservation%20of%20performance%20on%20base%20classes.%20To%20substantiate%0Athe%20efficacy%20of%20our%20approach%2C%20we%20conduct%20comprehensive%20experiments%20on%20three%0Adistinct%20FSCIL%20benchmarks%2C%20where%20our%20framework%20attains%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew%2520Shot%2520Class%2520Incremental%2520Learning%2520using%2520Vision-Language%2520models%26entry.906535625%3DAnurag%2520Kumar%2520and%2520Chinmay%2520Bharti%2520and%2520Saikat%2520Dutta%2520and%2520Srikrishna%2520Karanam%2520and%2520Biplab%2520Banerjee%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520have%2520demonstrated%2520remarkable%2520performance%250Acomparable%2520to%2520human%2520capabilities%2520across%2520various%2520supervised%2520computer%2520vision%250Atasks.%2520However%252C%2520the%2520prevalent%2520assumption%2520of%2520having%2520an%2520extensive%2520pool%2520of%250Atraining%2520data%2520encompassing%2520all%2520classes%2520prior%2520to%2520model%2520training%2520often%2520diverges%250Afrom%2520real-world%2520scenarios%252C%2520where%2520limited%2520data%2520availability%2520for%2520novel%2520classes%2520is%250Athe%2520norm.%2520The%2520challenge%2520emerges%2520in%2520seamlessly%2520integrating%2520new%2520classes%2520with%2520few%250Asamples%2520into%2520the%2520training%2520data%252C%2520demanding%2520the%2520model%2520to%2520adeptly%2520accommodate%250Athese%2520additions%2520without%2520compromising%2520its%2520performance%2520on%2520base%2520classes.%2520To%250Aaddress%2520this%2520exigency%252C%2520the%2520research%2520community%2520has%2520introduced%2520several%2520solutions%250Aunder%2520the%2520realm%2520of%2520few-shot%2520class%2520incremental%2520learning%2520%2528FSCIL%2529.%250A%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520an%2520innovative%2520FSCIL%2520framework%2520that%2520utilizes%250Alanguage%2520regularizer%2520and%2520subspace%2520regularizer.%2520During%2520base%2520training%252C%2520the%250Alanguage%2520regularizer%2520helps%2520incorporate%2520semantic%2520information%2520extracted%2520from%2520a%250AVision-Language%2520model.%2520The%2520subspace%2520regularizer%2520helps%2520in%2520facilitating%2520the%250Amodel%2527s%2520acquisition%2520of%2520nuanced%2520connections%2520between%2520image%2520and%2520text%2520semantics%250Ainherent%2520to%2520base%2520classes%2520during%2520incremental%2520training.%2520Our%2520proposed%2520framework%250Anot%2520only%2520empowers%2520the%2520model%2520to%2520embrace%2520novel%2520classes%2520with%2520limited%2520data%252C%2520but%250Aalso%2520ensures%2520the%2520preservation%2520of%2520performance%2520on%2520base%2520classes.%2520To%2520substantiate%250Athe%2520efficacy%2520of%2520our%2520approach%252C%2520we%2520conduct%2520comprehensive%2520experiments%2520on%2520three%250Adistinct%2520FSCIL%2520benchmarks%252C%2520where%2520our%2520framework%2520attains%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few%20Shot%20Class%20Incremental%20Learning%20using%20Vision-Language%20models&entry.906535625=Anurag%20Kumar%20and%20Chinmay%20Bharti%20and%20Saikat%20Dutta%20and%20Srikrishna%20Karanam%20and%20Biplab%20Banerjee&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20have%20demonstrated%20remarkable%20performance%0Acomparable%20to%20human%20capabilities%20across%20various%20supervised%20computer%20vision%0Atasks.%20However%2C%20the%20prevalent%20assumption%20of%20having%20an%20extensive%20pool%20of%0Atraining%20data%20encompassing%20all%20classes%20prior%20to%20model%20training%20often%20diverges%0Afrom%20real-world%20scenarios%2C%20where%20limited%20data%20availability%20for%20novel%20classes%20is%0Athe%20norm.%20The%20challenge%20emerges%20in%20seamlessly%20integrating%20new%20classes%20with%20few%0Asamples%20into%20the%20training%20data%2C%20demanding%20the%20model%20to%20adeptly%20accommodate%0Athese%20additions%20without%20compromising%20its%20performance%20on%20base%20classes.%20To%0Aaddress%20this%20exigency%2C%20the%20research%20community%20has%20introduced%20several%20solutions%0Aunder%20the%20realm%20of%20few-shot%20class%20incremental%20learning%20%28FSCIL%29.%0A%20%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20FSCIL%20framework%20that%20utilizes%0Alanguage%20regularizer%20and%20subspace%20regularizer.%20During%20base%20training%2C%20the%0Alanguage%20regularizer%20helps%20incorporate%20semantic%20information%20extracted%20from%20a%0AVision-Language%20model.%20The%20subspace%20regularizer%20helps%20in%20facilitating%20the%0Amodel%27s%20acquisition%20of%20nuanced%20connections%20between%20image%20and%20text%20semantics%0Ainherent%20to%20base%20classes%20during%20incremental%20training.%20Our%20proposed%20framework%0Anot%20only%20empowers%20the%20model%20to%20embrace%20novel%20classes%20with%20limited%20data%2C%20but%0Aalso%20ensures%20the%20preservation%20of%20performance%20on%20base%20classes.%20To%20substantiate%0Athe%20efficacy%20of%20our%20approach%2C%20we%20conduct%20comprehensive%20experiments%20on%20three%0Adistinct%20FSCIL%20benchmarks%2C%20where%20our%20framework%20attains%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01040v2&entry.124074799=Read"},
{"title": "A Conflicts-free, Speed-lossless KAN-based Reinforcement Learning\n  Decision System for Interactive Driving in Roundabouts", "author": "Zhihao Lin and Zhen Tian and Qi Zhang and Ziyang Ye and Hanyang Zhuang and Jianglin Lan", "abstract": "  Safety and efficiency are crucial for autonomous driving in roundabouts,\nespecially in the context of mixed traffic where autonomous vehicles (AVs) and\nhuman-driven vehicles coexist. This paper introduces a learning-based algorithm\ntailored to foster safe and efficient driving behaviors across varying levels\nof traffic flows in roundabouts. The proposed algorithm employs a deep\nQ-learning network to effectively learn safe and efficient driving strategies\nin complex multi-vehicle roundabouts. Additionally, a KAN (Kolmogorov-Arnold\nnetwork) enhances the AVs' ability to learn their surroundings robustly and\nprecisely. An action inspector is integrated to replace dangerous actions to\navoid collisions when the AV interacts with the environment, and a route\nplanner is proposed to enhance the driving efficiency and safety of the AVs.\nMoreover, a model predictive control is adopted to ensure stability and\nprecision of the driving actions. The results show that our proposed system\nconsistently achieves safe and efficient driving whilst maintaining a stable\ntraining process, as evidenced by the smooth convergence of the reward function\nand the low variance in the training curves across various traffic flows.\nCompared to state-of-the-art benchmarks, the proposed algorithm achieves a\nlower number of collisions and reduced travel time to destination.\n", "link": "http://arxiv.org/abs/2408.08242v1", "date": "2024-08-15", "relevancy": 2.0562, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5168}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Conflicts-free%2C%20Speed-lossless%20KAN-based%20Reinforcement%20Learning%0A%20%20Decision%20System%20for%20Interactive%20Driving%20in%20Roundabouts&body=Title%3A%20A%20Conflicts-free%2C%20Speed-lossless%20KAN-based%20Reinforcement%20Learning%0A%20%20Decision%20System%20for%20Interactive%20Driving%20in%20Roundabouts%0AAuthor%3A%20Zhihao%20Lin%20and%20Zhen%20Tian%20and%20Qi%20Zhang%20and%20Ziyang%20Ye%20and%20Hanyang%20Zhuang%20and%20Jianglin%20Lan%0AAbstract%3A%20%20%20Safety%20and%20efficiency%20are%20crucial%20for%20autonomous%20driving%20in%20roundabouts%2C%0Aespecially%20in%20the%20context%20of%20mixed%20traffic%20where%20autonomous%20vehicles%20%28AVs%29%20and%0Ahuman-driven%20vehicles%20coexist.%20This%20paper%20introduces%20a%20learning-based%20algorithm%0Atailored%20to%20foster%20safe%20and%20efficient%20driving%20behaviors%20across%20varying%20levels%0Aof%20traffic%20flows%20in%20roundabouts.%20The%20proposed%20algorithm%20employs%20a%20deep%0AQ-learning%20network%20to%20effectively%20learn%20safe%20and%20efficient%20driving%20strategies%0Ain%20complex%20multi-vehicle%20roundabouts.%20Additionally%2C%20a%20KAN%20%28Kolmogorov-Arnold%0Anetwork%29%20enhances%20the%20AVs%27%20ability%20to%20learn%20their%20surroundings%20robustly%20and%0Aprecisely.%20An%20action%20inspector%20is%20integrated%20to%20replace%20dangerous%20actions%20to%0Aavoid%20collisions%20when%20the%20AV%20interacts%20with%20the%20environment%2C%20and%20a%20route%0Aplanner%20is%20proposed%20to%20enhance%20the%20driving%20efficiency%20and%20safety%20of%20the%20AVs.%0AMoreover%2C%20a%20model%20predictive%20control%20is%20adopted%20to%20ensure%20stability%20and%0Aprecision%20of%20the%20driving%20actions.%20The%20results%20show%20that%20our%20proposed%20system%0Aconsistently%20achieves%20safe%20and%20efficient%20driving%20whilst%20maintaining%20a%20stable%0Atraining%20process%2C%20as%20evidenced%20by%20the%20smooth%20convergence%20of%20the%20reward%20function%0Aand%20the%20low%20variance%20in%20the%20training%20curves%20across%20various%20traffic%20flows.%0ACompared%20to%20state-of-the-art%20benchmarks%2C%20the%20proposed%20algorithm%20achieves%20a%0Alower%20number%20of%20collisions%20and%20reduced%20travel%20time%20to%20destination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Conflicts-free%252C%2520Speed-lossless%2520KAN-based%2520Reinforcement%2520Learning%250A%2520%2520Decision%2520System%2520for%2520Interactive%2520Driving%2520in%2520Roundabouts%26entry.906535625%3DZhihao%2520Lin%2520and%2520Zhen%2520Tian%2520and%2520Qi%2520Zhang%2520and%2520Ziyang%2520Ye%2520and%2520Hanyang%2520Zhuang%2520and%2520Jianglin%2520Lan%26entry.1292438233%3D%2520%2520Safety%2520and%2520efficiency%2520are%2520crucial%2520for%2520autonomous%2520driving%2520in%2520roundabouts%252C%250Aespecially%2520in%2520the%2520context%2520of%2520mixed%2520traffic%2520where%2520autonomous%2520vehicles%2520%2528AVs%2529%2520and%250Ahuman-driven%2520vehicles%2520coexist.%2520This%2520paper%2520introduces%2520a%2520learning-based%2520algorithm%250Atailored%2520to%2520foster%2520safe%2520and%2520efficient%2520driving%2520behaviors%2520across%2520varying%2520levels%250Aof%2520traffic%2520flows%2520in%2520roundabouts.%2520The%2520proposed%2520algorithm%2520employs%2520a%2520deep%250AQ-learning%2520network%2520to%2520effectively%2520learn%2520safe%2520and%2520efficient%2520driving%2520strategies%250Ain%2520complex%2520multi-vehicle%2520roundabouts.%2520Additionally%252C%2520a%2520KAN%2520%2528Kolmogorov-Arnold%250Anetwork%2529%2520enhances%2520the%2520AVs%2527%2520ability%2520to%2520learn%2520their%2520surroundings%2520robustly%2520and%250Aprecisely.%2520An%2520action%2520inspector%2520is%2520integrated%2520to%2520replace%2520dangerous%2520actions%2520to%250Aavoid%2520collisions%2520when%2520the%2520AV%2520interacts%2520with%2520the%2520environment%252C%2520and%2520a%2520route%250Aplanner%2520is%2520proposed%2520to%2520enhance%2520the%2520driving%2520efficiency%2520and%2520safety%2520of%2520the%2520AVs.%250AMoreover%252C%2520a%2520model%2520predictive%2520control%2520is%2520adopted%2520to%2520ensure%2520stability%2520and%250Aprecision%2520of%2520the%2520driving%2520actions.%2520The%2520results%2520show%2520that%2520our%2520proposed%2520system%250Aconsistently%2520achieves%2520safe%2520and%2520efficient%2520driving%2520whilst%2520maintaining%2520a%2520stable%250Atraining%2520process%252C%2520as%2520evidenced%2520by%2520the%2520smooth%2520convergence%2520of%2520the%2520reward%2520function%250Aand%2520the%2520low%2520variance%2520in%2520the%2520training%2520curves%2520across%2520various%2520traffic%2520flows.%250ACompared%2520to%2520state-of-the-art%2520benchmarks%252C%2520the%2520proposed%2520algorithm%2520achieves%2520a%250Alower%2520number%2520of%2520collisions%2520and%2520reduced%2520travel%2520time%2520to%2520destination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Conflicts-free%2C%20Speed-lossless%20KAN-based%20Reinforcement%20Learning%0A%20%20Decision%20System%20for%20Interactive%20Driving%20in%20Roundabouts&entry.906535625=Zhihao%20Lin%20and%20Zhen%20Tian%20and%20Qi%20Zhang%20and%20Ziyang%20Ye%20and%20Hanyang%20Zhuang%20and%20Jianglin%20Lan&entry.1292438233=%20%20Safety%20and%20efficiency%20are%20crucial%20for%20autonomous%20driving%20in%20roundabouts%2C%0Aespecially%20in%20the%20context%20of%20mixed%20traffic%20where%20autonomous%20vehicles%20%28AVs%29%20and%0Ahuman-driven%20vehicles%20coexist.%20This%20paper%20introduces%20a%20learning-based%20algorithm%0Atailored%20to%20foster%20safe%20and%20efficient%20driving%20behaviors%20across%20varying%20levels%0Aof%20traffic%20flows%20in%20roundabouts.%20The%20proposed%20algorithm%20employs%20a%20deep%0AQ-learning%20network%20to%20effectively%20learn%20safe%20and%20efficient%20driving%20strategies%0Ain%20complex%20multi-vehicle%20roundabouts.%20Additionally%2C%20a%20KAN%20%28Kolmogorov-Arnold%0Anetwork%29%20enhances%20the%20AVs%27%20ability%20to%20learn%20their%20surroundings%20robustly%20and%0Aprecisely.%20An%20action%20inspector%20is%20integrated%20to%20replace%20dangerous%20actions%20to%0Aavoid%20collisions%20when%20the%20AV%20interacts%20with%20the%20environment%2C%20and%20a%20route%0Aplanner%20is%20proposed%20to%20enhance%20the%20driving%20efficiency%20and%20safety%20of%20the%20AVs.%0AMoreover%2C%20a%20model%20predictive%20control%20is%20adopted%20to%20ensure%20stability%20and%0Aprecision%20of%20the%20driving%20actions.%20The%20results%20show%20that%20our%20proposed%20system%0Aconsistently%20achieves%20safe%20and%20efficient%20driving%20whilst%20maintaining%20a%20stable%0Atraining%20process%2C%20as%20evidenced%20by%20the%20smooth%20convergence%20of%20the%20reward%20function%0Aand%20the%20low%20variance%20in%20the%20training%20curves%20across%20various%20traffic%20flows.%0ACompared%20to%20state-of-the-art%20benchmarks%2C%20the%20proposed%20algorithm%20achieves%20a%0Alower%20number%20of%20collisions%20and%20reduced%20travel%20time%20to%20destination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08242v1&entry.124074799=Read"},
{"title": "Your Turn: Real-World Turning Angle Estimation for Parkinson's Disease\n  Severity Assessment", "author": "Qiushuo Cheng and Catherine Morgan and Arindam Sikdar and Alessandro Masullo and Alan Whone and Majid Mirmehdi", "abstract": "  People with Parkinson's Disease (PD) often experience progressively worsening\ngait, including changes in how they turn around, as the disease progresses.\nExisting clinical rating tools are not capable of capturing hour-by-hour\nvariations of PD symptoms, as they are confined to brief assessments within\nclinic settings. Measuring real-world gait turning angles continuously and\npassively is a component step towards using gait characteristics as sensitive\nindicators of disease progression in PD. This paper presents a deep\nlearning-based approach to automatically quantify turning angles by extracting\n3D skeletons from videos and calculating the rotation of hip and knee joints.\nWe utilise state-of-the-art human pose estimation models, Fastpose and Strided\nTransformer, on a total of 1386 turning video clips from 24 subjects (12 people\nwith PD and 12 healthy control volunteers), trimmed from a PD dataset of\nunscripted free-living videos in a home-like setting (Turn-REMAP). We also\ncurate a turning video dataset, Turn-H3.6M, from the public Human3.6M human\npose benchmark with 3D ground truth, to further validate our method. Previous\ngait research has primarily taken place in clinics or laboratories evaluating\nscripted gait outcomes, but this work focuses on real-world settings where\ncomplexities exist, such as baggy clothing and poor lighting. Due to\ndifficulties in obtaining accurate ground truth data in a free-living setting,\nwe quantise the angle into the nearest bin $45^\\circ$ based on the manual\nlabelling of expert clinicians. Our method achieves a turning calculation\naccuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7{\\deg}, and a weighted\nprecision WPrec of 68.3% for Turn-REMAP. This is the first work to explore the\nuse of single monocular camera data to quantify turns by PD patients in a home\nsetting.\n", "link": "http://arxiv.org/abs/2408.08182v1", "date": "2024-08-15", "relevancy": 2.0454, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5363}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Your%20Turn%3A%20Real-World%20Turning%20Angle%20Estimation%20for%20Parkinson%27s%20Disease%0A%20%20Severity%20Assessment&body=Title%3A%20Your%20Turn%3A%20Real-World%20Turning%20Angle%20Estimation%20for%20Parkinson%27s%20Disease%0A%20%20Severity%20Assessment%0AAuthor%3A%20Qiushuo%20Cheng%20and%20Catherine%20Morgan%20and%20Arindam%20Sikdar%20and%20Alessandro%20Masullo%20and%20Alan%20Whone%20and%20Majid%20Mirmehdi%0AAbstract%3A%20%20%20People%20with%20Parkinson%27s%20Disease%20%28PD%29%20often%20experience%20progressively%20worsening%0Agait%2C%20including%20changes%20in%20how%20they%20turn%20around%2C%20as%20the%20disease%20progresses.%0AExisting%20clinical%20rating%20tools%20are%20not%20capable%20of%20capturing%20hour-by-hour%0Avariations%20of%20PD%20symptoms%2C%20as%20they%20are%20confined%20to%20brief%20assessments%20within%0Aclinic%20settings.%20Measuring%20real-world%20gait%20turning%20angles%20continuously%20and%0Apassively%20is%20a%20component%20step%20towards%20using%20gait%20characteristics%20as%20sensitive%0Aindicators%20of%20disease%20progression%20in%20PD.%20This%20paper%20presents%20a%20deep%0Alearning-based%20approach%20to%20automatically%20quantify%20turning%20angles%20by%20extracting%0A3D%20skeletons%20from%20videos%20and%20calculating%20the%20rotation%20of%20hip%20and%20knee%20joints.%0AWe%20utilise%20state-of-the-art%20human%20pose%20estimation%20models%2C%20Fastpose%20and%20Strided%0ATransformer%2C%20on%20a%20total%20of%201386%20turning%20video%20clips%20from%2024%20subjects%20%2812%20people%0Awith%20PD%20and%2012%20healthy%20control%20volunteers%29%2C%20trimmed%20from%20a%20PD%20dataset%20of%0Aunscripted%20free-living%20videos%20in%20a%20home-like%20setting%20%28Turn-REMAP%29.%20We%20also%0Acurate%20a%20turning%20video%20dataset%2C%20Turn-H3.6M%2C%20from%20the%20public%20Human3.6M%20human%0Apose%20benchmark%20with%203D%20ground%20truth%2C%20to%20further%20validate%20our%20method.%20Previous%0Agait%20research%20has%20primarily%20taken%20place%20in%20clinics%20or%20laboratories%20evaluating%0Ascripted%20gait%20outcomes%2C%20but%20this%20work%20focuses%20on%20real-world%20settings%20where%0Acomplexities%20exist%2C%20such%20as%20baggy%20clothing%20and%20poor%20lighting.%20Due%20to%0Adifficulties%20in%20obtaining%20accurate%20ground%20truth%20data%20in%20a%20free-living%20setting%2C%0Awe%20quantise%20the%20angle%20into%20the%20nearest%20bin%20%2445%5E%5Ccirc%24%20based%20on%20the%20manual%0Alabelling%20of%20expert%20clinicians.%20Our%20method%20achieves%20a%20turning%20calculation%0Aaccuracy%20of%2041.6%25%2C%20a%20Mean%20Absolute%20Error%20%28MAE%29%20of%2034.7%7B%5Cdeg%7D%2C%20and%20a%20weighted%0Aprecision%20WPrec%20of%2068.3%25%20for%20Turn-REMAP.%20This%20is%20the%20first%20work%20to%20explore%20the%0Ause%20of%20single%20monocular%20camera%20data%20to%20quantify%20turns%20by%20PD%20patients%20in%20a%20home%0Asetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYour%2520Turn%253A%2520Real-World%2520Turning%2520Angle%2520Estimation%2520for%2520Parkinson%2527s%2520Disease%250A%2520%2520Severity%2520Assessment%26entry.906535625%3DQiushuo%2520Cheng%2520and%2520Catherine%2520Morgan%2520and%2520Arindam%2520Sikdar%2520and%2520Alessandro%2520Masullo%2520and%2520Alan%2520Whone%2520and%2520Majid%2520Mirmehdi%26entry.1292438233%3D%2520%2520People%2520with%2520Parkinson%2527s%2520Disease%2520%2528PD%2529%2520often%2520experience%2520progressively%2520worsening%250Agait%252C%2520including%2520changes%2520in%2520how%2520they%2520turn%2520around%252C%2520as%2520the%2520disease%2520progresses.%250AExisting%2520clinical%2520rating%2520tools%2520are%2520not%2520capable%2520of%2520capturing%2520hour-by-hour%250Avariations%2520of%2520PD%2520symptoms%252C%2520as%2520they%2520are%2520confined%2520to%2520brief%2520assessments%2520within%250Aclinic%2520settings.%2520Measuring%2520real-world%2520gait%2520turning%2520angles%2520continuously%2520and%250Apassively%2520is%2520a%2520component%2520step%2520towards%2520using%2520gait%2520characteristics%2520as%2520sensitive%250Aindicators%2520of%2520disease%2520progression%2520in%2520PD.%2520This%2520paper%2520presents%2520a%2520deep%250Alearning-based%2520approach%2520to%2520automatically%2520quantify%2520turning%2520angles%2520by%2520extracting%250A3D%2520skeletons%2520from%2520videos%2520and%2520calculating%2520the%2520rotation%2520of%2520hip%2520and%2520knee%2520joints.%250AWe%2520utilise%2520state-of-the-art%2520human%2520pose%2520estimation%2520models%252C%2520Fastpose%2520and%2520Strided%250ATransformer%252C%2520on%2520a%2520total%2520of%25201386%2520turning%2520video%2520clips%2520from%252024%2520subjects%2520%252812%2520people%250Awith%2520PD%2520and%252012%2520healthy%2520control%2520volunteers%2529%252C%2520trimmed%2520from%2520a%2520PD%2520dataset%2520of%250Aunscripted%2520free-living%2520videos%2520in%2520a%2520home-like%2520setting%2520%2528Turn-REMAP%2529.%2520We%2520also%250Acurate%2520a%2520turning%2520video%2520dataset%252C%2520Turn-H3.6M%252C%2520from%2520the%2520public%2520Human3.6M%2520human%250Apose%2520benchmark%2520with%25203D%2520ground%2520truth%252C%2520to%2520further%2520validate%2520our%2520method.%2520Previous%250Agait%2520research%2520has%2520primarily%2520taken%2520place%2520in%2520clinics%2520or%2520laboratories%2520evaluating%250Ascripted%2520gait%2520outcomes%252C%2520but%2520this%2520work%2520focuses%2520on%2520real-world%2520settings%2520where%250Acomplexities%2520exist%252C%2520such%2520as%2520baggy%2520clothing%2520and%2520poor%2520lighting.%2520Due%2520to%250Adifficulties%2520in%2520obtaining%2520accurate%2520ground%2520truth%2520data%2520in%2520a%2520free-living%2520setting%252C%250Awe%2520quantise%2520the%2520angle%2520into%2520the%2520nearest%2520bin%2520%252445%255E%255Ccirc%2524%2520based%2520on%2520the%2520manual%250Alabelling%2520of%2520expert%2520clinicians.%2520Our%2520method%2520achieves%2520a%2520turning%2520calculation%250Aaccuracy%2520of%252041.6%2525%252C%2520a%2520Mean%2520Absolute%2520Error%2520%2528MAE%2529%2520of%252034.7%257B%255Cdeg%257D%252C%2520and%2520a%2520weighted%250Aprecision%2520WPrec%2520of%252068.3%2525%2520for%2520Turn-REMAP.%2520This%2520is%2520the%2520first%2520work%2520to%2520explore%2520the%250Ause%2520of%2520single%2520monocular%2520camera%2520data%2520to%2520quantify%2520turns%2520by%2520PD%2520patients%2520in%2520a%2520home%250Asetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20Turn%3A%20Real-World%20Turning%20Angle%20Estimation%20for%20Parkinson%27s%20Disease%0A%20%20Severity%20Assessment&entry.906535625=Qiushuo%20Cheng%20and%20Catherine%20Morgan%20and%20Arindam%20Sikdar%20and%20Alessandro%20Masullo%20and%20Alan%20Whone%20and%20Majid%20Mirmehdi&entry.1292438233=%20%20People%20with%20Parkinson%27s%20Disease%20%28PD%29%20often%20experience%20progressively%20worsening%0Agait%2C%20including%20changes%20in%20how%20they%20turn%20around%2C%20as%20the%20disease%20progresses.%0AExisting%20clinical%20rating%20tools%20are%20not%20capable%20of%20capturing%20hour-by-hour%0Avariations%20of%20PD%20symptoms%2C%20as%20they%20are%20confined%20to%20brief%20assessments%20within%0Aclinic%20settings.%20Measuring%20real-world%20gait%20turning%20angles%20continuously%20and%0Apassively%20is%20a%20component%20step%20towards%20using%20gait%20characteristics%20as%20sensitive%0Aindicators%20of%20disease%20progression%20in%20PD.%20This%20paper%20presents%20a%20deep%0Alearning-based%20approach%20to%20automatically%20quantify%20turning%20angles%20by%20extracting%0A3D%20skeletons%20from%20videos%20and%20calculating%20the%20rotation%20of%20hip%20and%20knee%20joints.%0AWe%20utilise%20state-of-the-art%20human%20pose%20estimation%20models%2C%20Fastpose%20and%20Strided%0ATransformer%2C%20on%20a%20total%20of%201386%20turning%20video%20clips%20from%2024%20subjects%20%2812%20people%0Awith%20PD%20and%2012%20healthy%20control%20volunteers%29%2C%20trimmed%20from%20a%20PD%20dataset%20of%0Aunscripted%20free-living%20videos%20in%20a%20home-like%20setting%20%28Turn-REMAP%29.%20We%20also%0Acurate%20a%20turning%20video%20dataset%2C%20Turn-H3.6M%2C%20from%20the%20public%20Human3.6M%20human%0Apose%20benchmark%20with%203D%20ground%20truth%2C%20to%20further%20validate%20our%20method.%20Previous%0Agait%20research%20has%20primarily%20taken%20place%20in%20clinics%20or%20laboratories%20evaluating%0Ascripted%20gait%20outcomes%2C%20but%20this%20work%20focuses%20on%20real-world%20settings%20where%0Acomplexities%20exist%2C%20such%20as%20baggy%20clothing%20and%20poor%20lighting.%20Due%20to%0Adifficulties%20in%20obtaining%20accurate%20ground%20truth%20data%20in%20a%20free-living%20setting%2C%0Awe%20quantise%20the%20angle%20into%20the%20nearest%20bin%20%2445%5E%5Ccirc%24%20based%20on%20the%20manual%0Alabelling%20of%20expert%20clinicians.%20Our%20method%20achieves%20a%20turning%20calculation%0Aaccuracy%20of%2041.6%25%2C%20a%20Mean%20Absolute%20Error%20%28MAE%29%20of%2034.7%7B%5Cdeg%7D%2C%20and%20a%20weighted%0Aprecision%20WPrec%20of%2068.3%25%20for%20Turn-REMAP.%20This%20is%20the%20first%20work%20to%20explore%20the%0Ause%20of%20single%20monocular%20camera%20data%20to%20quantify%20turns%20by%20PD%20patients%20in%20a%20home%0Asetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08182v1&entry.124074799=Read"},
{"title": "Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2", "author": "Osher Rafaeli and Tal Svoray and Roni Blushtein-Livnon and Ariel Nahlieli", "abstract": "  This paper provides insight into the effectiveness of zero-shot,\nprompt-based, Segment Anything Model (SAM), and its updated version, SAM 2, and\nthe non-promptable, conventional convolutional network (CNN), in segmenting\nsolar panels, in RGB aerial imagery, across lighting conditions, spatial\nresolutions, and prompt strategies. SAM 2 demonstrates improvements over SAM,\nparticularly in sub-optimal lighting conditions when prompted by points. Both\nSAMs, prompted by user-box, outperformed CNN, in all scenarios. Additionally,\nYOLOv9 prompting outperformed user points prompting. In high-resolution\nimagery, both in optimal and sub-optimal lighting conditions, Eff-UNet\noutperformed both SAM models prompted by YOLOv9 boxes, positioning Eff-UNet as\nthe appropriate model for automatic segmentation in high-resolution data. In\nlow-resolution data, user box prompts were found crucial to achieve a\nreasonable performance. This paper provides details on strengths and\nlimitations of each model and outlines robustness of user prompted image\nsegmentation models in inconsistent resolution and lighting conditions of\nremotely sensed data.\n", "link": "http://arxiv.org/abs/2408.06970v2", "date": "2024-08-15", "relevancy": 2.0447, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5436}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Based%20Segmentation%20at%20Multiple%20Resolutions%20and%20Lighting%0A%20%20Conditions%20using%20Segment%20Anything%20Model%202&body=Title%3A%20Prompt-Based%20Segmentation%20at%20Multiple%20Resolutions%20and%20Lighting%0A%20%20Conditions%20using%20Segment%20Anything%20Model%202%0AAuthor%3A%20Osher%20Rafaeli%20and%20Tal%20Svoray%20and%20Roni%20Blushtein-Livnon%20and%20Ariel%20Nahlieli%0AAbstract%3A%20%20%20This%20paper%20provides%20insight%20into%20the%20effectiveness%20of%20zero-shot%2C%0Aprompt-based%2C%20Segment%20Anything%20Model%20%28SAM%29%2C%20and%20its%20updated%20version%2C%20SAM%202%2C%20and%0Athe%20non-promptable%2C%20conventional%20convolutional%20network%20%28CNN%29%2C%20in%20segmenting%0Asolar%20panels%2C%20in%20RGB%20aerial%20imagery%2C%20across%20lighting%20conditions%2C%20spatial%0Aresolutions%2C%20and%20prompt%20strategies.%20SAM%202%20demonstrates%20improvements%20over%20SAM%2C%0Aparticularly%20in%20sub-optimal%20lighting%20conditions%20when%20prompted%20by%20points.%20Both%0ASAMs%2C%20prompted%20by%20user-box%2C%20outperformed%20CNN%2C%20in%20all%20scenarios.%20Additionally%2C%0AYOLOv9%20prompting%20outperformed%20user%20points%20prompting.%20In%20high-resolution%0Aimagery%2C%20both%20in%20optimal%20and%20sub-optimal%20lighting%20conditions%2C%20Eff-UNet%0Aoutperformed%20both%20SAM%20models%20prompted%20by%20YOLOv9%20boxes%2C%20positioning%20Eff-UNet%20as%0Athe%20appropriate%20model%20for%20automatic%20segmentation%20in%20high-resolution%20data.%20In%0Alow-resolution%20data%2C%20user%20box%20prompts%20were%20found%20crucial%20to%20achieve%20a%0Areasonable%20performance.%20This%20paper%20provides%20details%20on%20strengths%20and%0Alimitations%20of%20each%20model%20and%20outlines%20robustness%20of%20user%20prompted%20image%0Asegmentation%20models%20in%20inconsistent%20resolution%20and%20lighting%20conditions%20of%0Aremotely%20sensed%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06970v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Based%2520Segmentation%2520at%2520Multiple%2520Resolutions%2520and%2520Lighting%250A%2520%2520Conditions%2520using%2520Segment%2520Anything%2520Model%25202%26entry.906535625%3DOsher%2520Rafaeli%2520and%2520Tal%2520Svoray%2520and%2520Roni%2520Blushtein-Livnon%2520and%2520Ariel%2520Nahlieli%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520insight%2520into%2520the%2520effectiveness%2520of%2520zero-shot%252C%250Aprompt-based%252C%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520and%2520its%2520updated%2520version%252C%2520SAM%25202%252C%2520and%250Athe%2520non-promptable%252C%2520conventional%2520convolutional%2520network%2520%2528CNN%2529%252C%2520in%2520segmenting%250Asolar%2520panels%252C%2520in%2520RGB%2520aerial%2520imagery%252C%2520across%2520lighting%2520conditions%252C%2520spatial%250Aresolutions%252C%2520and%2520prompt%2520strategies.%2520SAM%25202%2520demonstrates%2520improvements%2520over%2520SAM%252C%250Aparticularly%2520in%2520sub-optimal%2520lighting%2520conditions%2520when%2520prompted%2520by%2520points.%2520Both%250ASAMs%252C%2520prompted%2520by%2520user-box%252C%2520outperformed%2520CNN%252C%2520in%2520all%2520scenarios.%2520Additionally%252C%250AYOLOv9%2520prompting%2520outperformed%2520user%2520points%2520prompting.%2520In%2520high-resolution%250Aimagery%252C%2520both%2520in%2520optimal%2520and%2520sub-optimal%2520lighting%2520conditions%252C%2520Eff-UNet%250Aoutperformed%2520both%2520SAM%2520models%2520prompted%2520by%2520YOLOv9%2520boxes%252C%2520positioning%2520Eff-UNet%2520as%250Athe%2520appropriate%2520model%2520for%2520automatic%2520segmentation%2520in%2520high-resolution%2520data.%2520In%250Alow-resolution%2520data%252C%2520user%2520box%2520prompts%2520were%2520found%2520crucial%2520to%2520achieve%2520a%250Areasonable%2520performance.%2520This%2520paper%2520provides%2520details%2520on%2520strengths%2520and%250Alimitations%2520of%2520each%2520model%2520and%2520outlines%2520robustness%2520of%2520user%2520prompted%2520image%250Asegmentation%2520models%2520in%2520inconsistent%2520resolution%2520and%2520lighting%2520conditions%2520of%250Aremotely%2520sensed%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06970v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Based%20Segmentation%20at%20Multiple%20Resolutions%20and%20Lighting%0A%20%20Conditions%20using%20Segment%20Anything%20Model%202&entry.906535625=Osher%20Rafaeli%20and%20Tal%20Svoray%20and%20Roni%20Blushtein-Livnon%20and%20Ariel%20Nahlieli&entry.1292438233=%20%20This%20paper%20provides%20insight%20into%20the%20effectiveness%20of%20zero-shot%2C%0Aprompt-based%2C%20Segment%20Anything%20Model%20%28SAM%29%2C%20and%20its%20updated%20version%2C%20SAM%202%2C%20and%0Athe%20non-promptable%2C%20conventional%20convolutional%20network%20%28CNN%29%2C%20in%20segmenting%0Asolar%20panels%2C%20in%20RGB%20aerial%20imagery%2C%20across%20lighting%20conditions%2C%20spatial%0Aresolutions%2C%20and%20prompt%20strategies.%20SAM%202%20demonstrates%20improvements%20over%20SAM%2C%0Aparticularly%20in%20sub-optimal%20lighting%20conditions%20when%20prompted%20by%20points.%20Both%0ASAMs%2C%20prompted%20by%20user-box%2C%20outperformed%20CNN%2C%20in%20all%20scenarios.%20Additionally%2C%0AYOLOv9%20prompting%20outperformed%20user%20points%20prompting.%20In%20high-resolution%0Aimagery%2C%20both%20in%20optimal%20and%20sub-optimal%20lighting%20conditions%2C%20Eff-UNet%0Aoutperformed%20both%20SAM%20models%20prompted%20by%20YOLOv9%20boxes%2C%20positioning%20Eff-UNet%20as%0Athe%20appropriate%20model%20for%20automatic%20segmentation%20in%20high-resolution%20data.%20In%0Alow-resolution%20data%2C%20user%20box%20prompts%20were%20found%20crucial%20to%20achieve%20a%0Areasonable%20performance.%20This%20paper%20provides%20details%20on%20strengths%20and%0Alimitations%20of%20each%20model%20and%20outlines%20robustness%20of%20user%20prompted%20image%0Asegmentation%20models%20in%20inconsistent%20resolution%20and%20lighting%20conditions%20of%0Aremotely%20sensed%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06970v2&entry.124074799=Read"},
{"title": "Navigating Data Scarcity using Foundation Models: A Benchmark of\n  Few-Shot and Zero-Shot Learning Approaches in Medical Imaging", "author": "Stefano Woerner and Christian F. Baumgartner", "abstract": "  Data scarcity is a major limiting factor for applying modern machine learning\ntechniques to clinical tasks. Although sufficient data exists for some\nwell-studied medical tasks, there remains a long tail of clinically relevant\ntasks with poor data availability. Recently, numerous foundation models have\ndemonstrated high suitability for few-shot learning (FSL) and zero-shot\nlearning (ZSL), potentially making them more accessible to practitioners.\nHowever, it remains unclear which foundation model performs best on FSL medical\nimage analysis tasks and what the optimal methods are for learning from limited\ndata. We conducted a comprehensive benchmark study of ZSL and FSL using 16\npretrained foundation models on 19 diverse medical imaging datasets. Our\nresults indicate that BiomedCLIP, a model pretrained exclusively on medical\ndata, performs best on average for very small training set sizes, while very\nlarge CLIP models pretrained on LAION-2B perform best with slightly more\ntraining samples. However, simply fine-tuning a ResNet-18 pretrained on\nImageNet performs similarly with more than five training examples per class.\nOur findings also highlight the need for further research on foundation models\nspecifically tailored for medical applications and the collection of more\ndatasets to train these models.\n", "link": "http://arxiv.org/abs/2408.08058v1", "date": "2024-08-15", "relevancy": 2.0406, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5237}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20Data%20Scarcity%20using%20Foundation%20Models%3A%20A%20Benchmark%20of%0A%20%20Few-Shot%20and%20Zero-Shot%20Learning%20Approaches%20in%20Medical%20Imaging&body=Title%3A%20Navigating%20Data%20Scarcity%20using%20Foundation%20Models%3A%20A%20Benchmark%20of%0A%20%20Few-Shot%20and%20Zero-Shot%20Learning%20Approaches%20in%20Medical%20Imaging%0AAuthor%3A%20Stefano%20Woerner%20and%20Christian%20F.%20Baumgartner%0AAbstract%3A%20%20%20Data%20scarcity%20is%20a%20major%20limiting%20factor%20for%20applying%20modern%20machine%20learning%0Atechniques%20to%20clinical%20tasks.%20Although%20sufficient%20data%20exists%20for%20some%0Awell-studied%20medical%20tasks%2C%20there%20remains%20a%20long%20tail%20of%20clinically%20relevant%0Atasks%20with%20poor%20data%20availability.%20Recently%2C%20numerous%20foundation%20models%20have%0Ademonstrated%20high%20suitability%20for%20few-shot%20learning%20%28FSL%29%20and%20zero-shot%0Alearning%20%28ZSL%29%2C%20potentially%20making%20them%20more%20accessible%20to%20practitioners.%0AHowever%2C%20it%20remains%20unclear%20which%20foundation%20model%20performs%20best%20on%20FSL%20medical%0Aimage%20analysis%20tasks%20and%20what%20the%20optimal%20methods%20are%20for%20learning%20from%20limited%0Adata.%20We%20conducted%20a%20comprehensive%20benchmark%20study%20of%20ZSL%20and%20FSL%20using%2016%0Apretrained%20foundation%20models%20on%2019%20diverse%20medical%20imaging%20datasets.%20Our%0Aresults%20indicate%20that%20BiomedCLIP%2C%20a%20model%20pretrained%20exclusively%20on%20medical%0Adata%2C%20performs%20best%20on%20average%20for%20very%20small%20training%20set%20sizes%2C%20while%20very%0Alarge%20CLIP%20models%20pretrained%20on%20LAION-2B%20perform%20best%20with%20slightly%20more%0Atraining%20samples.%20However%2C%20simply%20fine-tuning%20a%20ResNet-18%20pretrained%20on%0AImageNet%20performs%20similarly%20with%20more%20than%20five%20training%20examples%20per%20class.%0AOur%20findings%20also%20highlight%20the%20need%20for%20further%20research%20on%20foundation%20models%0Aspecifically%20tailored%20for%20medical%20applications%20and%20the%20collection%20of%20more%0Adatasets%20to%20train%20these%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520Data%2520Scarcity%2520using%2520Foundation%2520Models%253A%2520A%2520Benchmark%2520of%250A%2520%2520Few-Shot%2520and%2520Zero-Shot%2520Learning%2520Approaches%2520in%2520Medical%2520Imaging%26entry.906535625%3DStefano%2520Woerner%2520and%2520Christian%2520F.%2520Baumgartner%26entry.1292438233%3D%2520%2520Data%2520scarcity%2520is%2520a%2520major%2520limiting%2520factor%2520for%2520applying%2520modern%2520machine%2520learning%250Atechniques%2520to%2520clinical%2520tasks.%2520Although%2520sufficient%2520data%2520exists%2520for%2520some%250Awell-studied%2520medical%2520tasks%252C%2520there%2520remains%2520a%2520long%2520tail%2520of%2520clinically%2520relevant%250Atasks%2520with%2520poor%2520data%2520availability.%2520Recently%252C%2520numerous%2520foundation%2520models%2520have%250Ademonstrated%2520high%2520suitability%2520for%2520few-shot%2520learning%2520%2528FSL%2529%2520and%2520zero-shot%250Alearning%2520%2528ZSL%2529%252C%2520potentially%2520making%2520them%2520more%2520accessible%2520to%2520practitioners.%250AHowever%252C%2520it%2520remains%2520unclear%2520which%2520foundation%2520model%2520performs%2520best%2520on%2520FSL%2520medical%250Aimage%2520analysis%2520tasks%2520and%2520what%2520the%2520optimal%2520methods%2520are%2520for%2520learning%2520from%2520limited%250Adata.%2520We%2520conducted%2520a%2520comprehensive%2520benchmark%2520study%2520of%2520ZSL%2520and%2520FSL%2520using%252016%250Apretrained%2520foundation%2520models%2520on%252019%2520diverse%2520medical%2520imaging%2520datasets.%2520Our%250Aresults%2520indicate%2520that%2520BiomedCLIP%252C%2520a%2520model%2520pretrained%2520exclusively%2520on%2520medical%250Adata%252C%2520performs%2520best%2520on%2520average%2520for%2520very%2520small%2520training%2520set%2520sizes%252C%2520while%2520very%250Alarge%2520CLIP%2520models%2520pretrained%2520on%2520LAION-2B%2520perform%2520best%2520with%2520slightly%2520more%250Atraining%2520samples.%2520However%252C%2520simply%2520fine-tuning%2520a%2520ResNet-18%2520pretrained%2520on%250AImageNet%2520performs%2520similarly%2520with%2520more%2520than%2520five%2520training%2520examples%2520per%2520class.%250AOur%2520findings%2520also%2520highlight%2520the%2520need%2520for%2520further%2520research%2520on%2520foundation%2520models%250Aspecifically%2520tailored%2520for%2520medical%2520applications%2520and%2520the%2520collection%2520of%2520more%250Adatasets%2520to%2520train%2520these%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20Data%20Scarcity%20using%20Foundation%20Models%3A%20A%20Benchmark%20of%0A%20%20Few-Shot%20and%20Zero-Shot%20Learning%20Approaches%20in%20Medical%20Imaging&entry.906535625=Stefano%20Woerner%20and%20Christian%20F.%20Baumgartner&entry.1292438233=%20%20Data%20scarcity%20is%20a%20major%20limiting%20factor%20for%20applying%20modern%20machine%20learning%0Atechniques%20to%20clinical%20tasks.%20Although%20sufficient%20data%20exists%20for%20some%0Awell-studied%20medical%20tasks%2C%20there%20remains%20a%20long%20tail%20of%20clinically%20relevant%0Atasks%20with%20poor%20data%20availability.%20Recently%2C%20numerous%20foundation%20models%20have%0Ademonstrated%20high%20suitability%20for%20few-shot%20learning%20%28FSL%29%20and%20zero-shot%0Alearning%20%28ZSL%29%2C%20potentially%20making%20them%20more%20accessible%20to%20practitioners.%0AHowever%2C%20it%20remains%20unclear%20which%20foundation%20model%20performs%20best%20on%20FSL%20medical%0Aimage%20analysis%20tasks%20and%20what%20the%20optimal%20methods%20are%20for%20learning%20from%20limited%0Adata.%20We%20conducted%20a%20comprehensive%20benchmark%20study%20of%20ZSL%20and%20FSL%20using%2016%0Apretrained%20foundation%20models%20on%2019%20diverse%20medical%20imaging%20datasets.%20Our%0Aresults%20indicate%20that%20BiomedCLIP%2C%20a%20model%20pretrained%20exclusively%20on%20medical%0Adata%2C%20performs%20best%20on%20average%20for%20very%20small%20training%20set%20sizes%2C%20while%20very%0Alarge%20CLIP%20models%20pretrained%20on%20LAION-2B%20perform%20best%20with%20slightly%20more%0Atraining%20samples.%20However%2C%20simply%20fine-tuning%20a%20ResNet-18%20pretrained%20on%0AImageNet%20performs%20similarly%20with%20more%20than%20five%20training%20examples%20per%20class.%0AOur%20findings%20also%20highlight%20the%20need%20for%20further%20research%20on%20foundation%20models%0Aspecifically%20tailored%20for%20medical%20applications%20and%20the%20collection%20of%20more%0Adatasets%20to%20train%20these%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08058v1&entry.124074799=Read"},
{"title": "Beyond Full Label: Single-Point Prompt for Infrared Small Target Label\n  Generation", "author": "Shuai Yuan and Hanlin Qin and Renke Kou and Xiang Yan and Zechuan Li and Chenxu Peng and Abd-Krim Seghouane", "abstract": "  In this work, we make the first attempt to construct a learning-based\nsingle-point annotation paradigm for infrared small target label generation\n(IRSTLG). Our intuition is that label generation requires just one more point\nprompt than target detection: IRSTLG can be regarded as an infrared small\ntarget detection (IRSTD) task with the target location hint. Based on this\ninsight, we introduce an energy double guided single-point prompt (EDGSP)\nframework, which adeptly transforms the target detection network into a refined\nlabel generation method. Specifically, the proposed EDGSP includes: 1) target\nenergy initialization (TEI) to create a foundational outline for sufficient\nshape evolution of pseudo label, 2) double prompt embedding (DPE) for rapid\nlocalization of interested regions and reinforcement of individual differences\nto avoid label adhesion, and 3) bounding box-based matching (BBM) to eliminate\nfalse alarms. Experimental results show that pseudo labels generated by three\nbaselines equipped with EDGSP achieve 100% object-level probability of\ndetection (Pd) and 0% false-alarm rate (Fa) on SIRST, NUDT-SIRST, and IRSTD-1k\ndatasets, with a pixel-level intersection over union (IoU) improvement of\n13.28% over state-of-the-art label generation methods. Additionally, the\ndownstream detection task reveals that our centroid-annotated pseudo labels\nsurpass full labels, even with coarse single-point annotations, it still\nachieves 99.5% performance of full labeling.\n", "link": "http://arxiv.org/abs/2408.08191v1", "date": "2024-08-15", "relevancy": 2.0354, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.552}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5007}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Full%20Label%3A%20Single-Point%20Prompt%20for%20Infrared%20Small%20Target%20Label%0A%20%20Generation&body=Title%3A%20Beyond%20Full%20Label%3A%20Single-Point%20Prompt%20for%20Infrared%20Small%20Target%20Label%0A%20%20Generation%0AAuthor%3A%20Shuai%20Yuan%20and%20Hanlin%20Qin%20and%20Renke%20Kou%20and%20Xiang%20Yan%20and%20Zechuan%20Li%20and%20Chenxu%20Peng%20and%20Abd-Krim%20Seghouane%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20make%20the%20first%20attempt%20to%20construct%20a%20learning-based%0Asingle-point%20annotation%20paradigm%20for%20infrared%20small%20target%20label%20generation%0A%28IRSTLG%29.%20Our%20intuition%20is%20that%20label%20generation%20requires%20just%20one%20more%20point%0Aprompt%20than%20target%20detection%3A%20IRSTLG%20can%20be%20regarded%20as%20an%20infrared%20small%0Atarget%20detection%20%28IRSTD%29%20task%20with%20the%20target%20location%20hint.%20Based%20on%20this%0Ainsight%2C%20we%20introduce%20an%20energy%20double%20guided%20single-point%20prompt%20%28EDGSP%29%0Aframework%2C%20which%20adeptly%20transforms%20the%20target%20detection%20network%20into%20a%20refined%0Alabel%20generation%20method.%20Specifically%2C%20the%20proposed%20EDGSP%20includes%3A%201%29%20target%0Aenergy%20initialization%20%28TEI%29%20to%20create%20a%20foundational%20outline%20for%20sufficient%0Ashape%20evolution%20of%20pseudo%20label%2C%202%29%20double%20prompt%20embedding%20%28DPE%29%20for%20rapid%0Alocalization%20of%20interested%20regions%20and%20reinforcement%20of%20individual%20differences%0Ato%20avoid%20label%20adhesion%2C%20and%203%29%20bounding%20box-based%20matching%20%28BBM%29%20to%20eliminate%0Afalse%20alarms.%20Experimental%20results%20show%20that%20pseudo%20labels%20generated%20by%20three%0Abaselines%20equipped%20with%20EDGSP%20achieve%20100%25%20object-level%20probability%20of%0Adetection%20%28Pd%29%20and%200%25%20false-alarm%20rate%20%28Fa%29%20on%20SIRST%2C%20NUDT-SIRST%2C%20and%20IRSTD-1k%0Adatasets%2C%20with%20a%20pixel-level%20intersection%20over%20union%20%28IoU%29%20improvement%20of%0A13.28%25%20over%20state-of-the-art%20label%20generation%20methods.%20Additionally%2C%20the%0Adownstream%20detection%20task%20reveals%20that%20our%20centroid-annotated%20pseudo%20labels%0Asurpass%20full%20labels%2C%20even%20with%20coarse%20single-point%20annotations%2C%20it%20still%0Aachieves%2099.5%25%20performance%20of%20full%20labeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Full%2520Label%253A%2520Single-Point%2520Prompt%2520for%2520Infrared%2520Small%2520Target%2520Label%250A%2520%2520Generation%26entry.906535625%3DShuai%2520Yuan%2520and%2520Hanlin%2520Qin%2520and%2520Renke%2520Kou%2520and%2520Xiang%2520Yan%2520and%2520Zechuan%2520Li%2520and%2520Chenxu%2520Peng%2520and%2520Abd-Krim%2520Seghouane%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520make%2520the%2520first%2520attempt%2520to%2520construct%2520a%2520learning-based%250Asingle-point%2520annotation%2520paradigm%2520for%2520infrared%2520small%2520target%2520label%2520generation%250A%2528IRSTLG%2529.%2520Our%2520intuition%2520is%2520that%2520label%2520generation%2520requires%2520just%2520one%2520more%2520point%250Aprompt%2520than%2520target%2520detection%253A%2520IRSTLG%2520can%2520be%2520regarded%2520as%2520an%2520infrared%2520small%250Atarget%2520detection%2520%2528IRSTD%2529%2520task%2520with%2520the%2520target%2520location%2520hint.%2520Based%2520on%2520this%250Ainsight%252C%2520we%2520introduce%2520an%2520energy%2520double%2520guided%2520single-point%2520prompt%2520%2528EDGSP%2529%250Aframework%252C%2520which%2520adeptly%2520transforms%2520the%2520target%2520detection%2520network%2520into%2520a%2520refined%250Alabel%2520generation%2520method.%2520Specifically%252C%2520the%2520proposed%2520EDGSP%2520includes%253A%25201%2529%2520target%250Aenergy%2520initialization%2520%2528TEI%2529%2520to%2520create%2520a%2520foundational%2520outline%2520for%2520sufficient%250Ashape%2520evolution%2520of%2520pseudo%2520label%252C%25202%2529%2520double%2520prompt%2520embedding%2520%2528DPE%2529%2520for%2520rapid%250Alocalization%2520of%2520interested%2520regions%2520and%2520reinforcement%2520of%2520individual%2520differences%250Ato%2520avoid%2520label%2520adhesion%252C%2520and%25203%2529%2520bounding%2520box-based%2520matching%2520%2528BBM%2529%2520to%2520eliminate%250Afalse%2520alarms.%2520Experimental%2520results%2520show%2520that%2520pseudo%2520labels%2520generated%2520by%2520three%250Abaselines%2520equipped%2520with%2520EDGSP%2520achieve%2520100%2525%2520object-level%2520probability%2520of%250Adetection%2520%2528Pd%2529%2520and%25200%2525%2520false-alarm%2520rate%2520%2528Fa%2529%2520on%2520SIRST%252C%2520NUDT-SIRST%252C%2520and%2520IRSTD-1k%250Adatasets%252C%2520with%2520a%2520pixel-level%2520intersection%2520over%2520union%2520%2528IoU%2529%2520improvement%2520of%250A13.28%2525%2520over%2520state-of-the-art%2520label%2520generation%2520methods.%2520Additionally%252C%2520the%250Adownstream%2520detection%2520task%2520reveals%2520that%2520our%2520centroid-annotated%2520pseudo%2520labels%250Asurpass%2520full%2520labels%252C%2520even%2520with%2520coarse%2520single-point%2520annotations%252C%2520it%2520still%250Aachieves%252099.5%2525%2520performance%2520of%2520full%2520labeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Full%20Label%3A%20Single-Point%20Prompt%20for%20Infrared%20Small%20Target%20Label%0A%20%20Generation&entry.906535625=Shuai%20Yuan%20and%20Hanlin%20Qin%20and%20Renke%20Kou%20and%20Xiang%20Yan%20and%20Zechuan%20Li%20and%20Chenxu%20Peng%20and%20Abd-Krim%20Seghouane&entry.1292438233=%20%20In%20this%20work%2C%20we%20make%20the%20first%20attempt%20to%20construct%20a%20learning-based%0Asingle-point%20annotation%20paradigm%20for%20infrared%20small%20target%20label%20generation%0A%28IRSTLG%29.%20Our%20intuition%20is%20that%20label%20generation%20requires%20just%20one%20more%20point%0Aprompt%20than%20target%20detection%3A%20IRSTLG%20can%20be%20regarded%20as%20an%20infrared%20small%0Atarget%20detection%20%28IRSTD%29%20task%20with%20the%20target%20location%20hint.%20Based%20on%20this%0Ainsight%2C%20we%20introduce%20an%20energy%20double%20guided%20single-point%20prompt%20%28EDGSP%29%0Aframework%2C%20which%20adeptly%20transforms%20the%20target%20detection%20network%20into%20a%20refined%0Alabel%20generation%20method.%20Specifically%2C%20the%20proposed%20EDGSP%20includes%3A%201%29%20target%0Aenergy%20initialization%20%28TEI%29%20to%20create%20a%20foundational%20outline%20for%20sufficient%0Ashape%20evolution%20of%20pseudo%20label%2C%202%29%20double%20prompt%20embedding%20%28DPE%29%20for%20rapid%0Alocalization%20of%20interested%20regions%20and%20reinforcement%20of%20individual%20differences%0Ato%20avoid%20label%20adhesion%2C%20and%203%29%20bounding%20box-based%20matching%20%28BBM%29%20to%20eliminate%0Afalse%20alarms.%20Experimental%20results%20show%20that%20pseudo%20labels%20generated%20by%20three%0Abaselines%20equipped%20with%20EDGSP%20achieve%20100%25%20object-level%20probability%20of%0Adetection%20%28Pd%29%20and%200%25%20false-alarm%20rate%20%28Fa%29%20on%20SIRST%2C%20NUDT-SIRST%2C%20and%20IRSTD-1k%0Adatasets%2C%20with%20a%20pixel-level%20intersection%20over%20union%20%28IoU%29%20improvement%20of%0A13.28%25%20over%20state-of-the-art%20label%20generation%20methods.%20Additionally%2C%20the%0Adownstream%20detection%20task%20reveals%20that%20our%20centroid-annotated%20pseudo%20labels%0Asurpass%20full%20labels%2C%20even%20with%20coarse%20single-point%20annotations%2C%20it%20still%0Aachieves%2099.5%25%20performance%20of%20full%20labeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08191v1&entry.124074799=Read"},
{"title": "Unlearnable Examples Detection via Iterative Filtering", "author": "Yi Yu and Qichen Zheng and Siyuan Yang and Wenhan Yang and Jun Liu and Shijian Lu and Yap-Peng Tan and Kwok-Yan Lam and Alex Kot", "abstract": "  Deep neural networks are proven to be vulnerable to data poisoning attacks.\nRecently, a specific type of data poisoning attack known as availability\nattacks has led to the failure of data utilization for model learning by adding\nimperceptible perturbations to images. Consequently, it is quite beneficial and\nchallenging to detect poisoned samples, also known as Unlearnable Examples\n(UEs), from a mixed dataset. In response, we propose an Iterative Filtering\napproach for UEs identification. This method leverages the distinction between\nthe inherent semantic mapping rules and shortcuts, without the need for any\nadditional information. We verify that when training a classifier on a mixed\ndataset containing both UEs and clean data, the model tends to quickly adapt to\nthe UEs compared to the clean data. Due to the accuracy gaps between training\nwith clean/poisoned samples, we employ a model to misclassify clean samples\nwhile correctly identifying the poisoned ones. The incorporation of additional\nclasses and iterative refinement enhances the model's ability to differentiate\nbetween clean and poisoned samples. Extensive experiments demonstrate the\nsuperiority of our method over state-of-the-art detection approaches across\nvarious attacks, datasets, and poison ratios, significantly reducing the Half\nTotal Error Rate (HTER) compared to existing methods.\n", "link": "http://arxiv.org/abs/2408.08143v1", "date": "2024-08-15", "relevancy": 2.0034, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5468}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4995}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlearnable%20Examples%20Detection%20via%20Iterative%20Filtering&body=Title%3A%20Unlearnable%20Examples%20Detection%20via%20Iterative%20Filtering%0AAuthor%3A%20Yi%20Yu%20and%20Qichen%20Zheng%20and%20Siyuan%20Yang%20and%20Wenhan%20Yang%20and%20Jun%20Liu%20and%20Shijian%20Lu%20and%20Yap-Peng%20Tan%20and%20Kwok-Yan%20Lam%20and%20Alex%20Kot%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20proven%20to%20be%20vulnerable%20to%20data%20poisoning%20attacks.%0ARecently%2C%20a%20specific%20type%20of%20data%20poisoning%20attack%20known%20as%20availability%0Aattacks%20has%20led%20to%20the%20failure%20of%20data%20utilization%20for%20model%20learning%20by%20adding%0Aimperceptible%20perturbations%20to%20images.%20Consequently%2C%20it%20is%20quite%20beneficial%20and%0Achallenging%20to%20detect%20poisoned%20samples%2C%20also%20known%20as%20Unlearnable%20Examples%0A%28UEs%29%2C%20from%20a%20mixed%20dataset.%20In%20response%2C%20we%20propose%20an%20Iterative%20Filtering%0Aapproach%20for%20UEs%20identification.%20This%20method%20leverages%20the%20distinction%20between%0Athe%20inherent%20semantic%20mapping%20rules%20and%20shortcuts%2C%20without%20the%20need%20for%20any%0Aadditional%20information.%20We%20verify%20that%20when%20training%20a%20classifier%20on%20a%20mixed%0Adataset%20containing%20both%20UEs%20and%20clean%20data%2C%20the%20model%20tends%20to%20quickly%20adapt%20to%0Athe%20UEs%20compared%20to%20the%20clean%20data.%20Due%20to%20the%20accuracy%20gaps%20between%20training%0Awith%20clean/poisoned%20samples%2C%20we%20employ%20a%20model%20to%20misclassify%20clean%20samples%0Awhile%20correctly%20identifying%20the%20poisoned%20ones.%20The%20incorporation%20of%20additional%0Aclasses%20and%20iterative%20refinement%20enhances%20the%20model%27s%20ability%20to%20differentiate%0Abetween%20clean%20and%20poisoned%20samples.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20state-of-the-art%20detection%20approaches%20across%0Avarious%20attacks%2C%20datasets%2C%20and%20poison%20ratios%2C%20significantly%20reducing%20the%20Half%0ATotal%20Error%20Rate%20%28HTER%29%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlearnable%2520Examples%2520Detection%2520via%2520Iterative%2520Filtering%26entry.906535625%3DYi%2520Yu%2520and%2520Qichen%2520Zheng%2520and%2520Siyuan%2520Yang%2520and%2520Wenhan%2520Yang%2520and%2520Jun%2520Liu%2520and%2520Shijian%2520Lu%2520and%2520Yap-Peng%2520Tan%2520and%2520Kwok-Yan%2520Lam%2520and%2520Alex%2520Kot%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520proven%2520to%2520be%2520vulnerable%2520to%2520data%2520poisoning%2520attacks.%250ARecently%252C%2520a%2520specific%2520type%2520of%2520data%2520poisoning%2520attack%2520known%2520as%2520availability%250Aattacks%2520has%2520led%2520to%2520the%2520failure%2520of%2520data%2520utilization%2520for%2520model%2520learning%2520by%2520adding%250Aimperceptible%2520perturbations%2520to%2520images.%2520Consequently%252C%2520it%2520is%2520quite%2520beneficial%2520and%250Achallenging%2520to%2520detect%2520poisoned%2520samples%252C%2520also%2520known%2520as%2520Unlearnable%2520Examples%250A%2528UEs%2529%252C%2520from%2520a%2520mixed%2520dataset.%2520In%2520response%252C%2520we%2520propose%2520an%2520Iterative%2520Filtering%250Aapproach%2520for%2520UEs%2520identification.%2520This%2520method%2520leverages%2520the%2520distinction%2520between%250Athe%2520inherent%2520semantic%2520mapping%2520rules%2520and%2520shortcuts%252C%2520without%2520the%2520need%2520for%2520any%250Aadditional%2520information.%2520We%2520verify%2520that%2520when%2520training%2520a%2520classifier%2520on%2520a%2520mixed%250Adataset%2520containing%2520both%2520UEs%2520and%2520clean%2520data%252C%2520the%2520model%2520tends%2520to%2520quickly%2520adapt%2520to%250Athe%2520UEs%2520compared%2520to%2520the%2520clean%2520data.%2520Due%2520to%2520the%2520accuracy%2520gaps%2520between%2520training%250Awith%2520clean/poisoned%2520samples%252C%2520we%2520employ%2520a%2520model%2520to%2520misclassify%2520clean%2520samples%250Awhile%2520correctly%2520identifying%2520the%2520poisoned%2520ones.%2520The%2520incorporation%2520of%2520additional%250Aclasses%2520and%2520iterative%2520refinement%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520differentiate%250Abetween%2520clean%2520and%2520poisoned%2520samples.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520method%2520over%2520state-of-the-art%2520detection%2520approaches%2520across%250Avarious%2520attacks%252C%2520datasets%252C%2520and%2520poison%2520ratios%252C%2520significantly%2520reducing%2520the%2520Half%250ATotal%2520Error%2520Rate%2520%2528HTER%2529%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlearnable%20Examples%20Detection%20via%20Iterative%20Filtering&entry.906535625=Yi%20Yu%20and%20Qichen%20Zheng%20and%20Siyuan%20Yang%20and%20Wenhan%20Yang%20and%20Jun%20Liu%20and%20Shijian%20Lu%20and%20Yap-Peng%20Tan%20and%20Kwok-Yan%20Lam%20and%20Alex%20Kot&entry.1292438233=%20%20Deep%20neural%20networks%20are%20proven%20to%20be%20vulnerable%20to%20data%20poisoning%20attacks.%0ARecently%2C%20a%20specific%20type%20of%20data%20poisoning%20attack%20known%20as%20availability%0Aattacks%20has%20led%20to%20the%20failure%20of%20data%20utilization%20for%20model%20learning%20by%20adding%0Aimperceptible%20perturbations%20to%20images.%20Consequently%2C%20it%20is%20quite%20beneficial%20and%0Achallenging%20to%20detect%20poisoned%20samples%2C%20also%20known%20as%20Unlearnable%20Examples%0A%28UEs%29%2C%20from%20a%20mixed%20dataset.%20In%20response%2C%20we%20propose%20an%20Iterative%20Filtering%0Aapproach%20for%20UEs%20identification.%20This%20method%20leverages%20the%20distinction%20between%0Athe%20inherent%20semantic%20mapping%20rules%20and%20shortcuts%2C%20without%20the%20need%20for%20any%0Aadditional%20information.%20We%20verify%20that%20when%20training%20a%20classifier%20on%20a%20mixed%0Adataset%20containing%20both%20UEs%20and%20clean%20data%2C%20the%20model%20tends%20to%20quickly%20adapt%20to%0Athe%20UEs%20compared%20to%20the%20clean%20data.%20Due%20to%20the%20accuracy%20gaps%20between%20training%0Awith%20clean/poisoned%20samples%2C%20we%20employ%20a%20model%20to%20misclassify%20clean%20samples%0Awhile%20correctly%20identifying%20the%20poisoned%20ones.%20The%20incorporation%20of%20additional%0Aclasses%20and%20iterative%20refinement%20enhances%20the%20model%27s%20ability%20to%20differentiate%0Abetween%20clean%20and%20poisoned%20samples.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20state-of-the-art%20detection%20approaches%20across%0Avarious%20attacks%2C%20datasets%2C%20and%20poison%20ratios%2C%20significantly%20reducing%20the%20Half%0ATotal%20Error%20Rate%20%28HTER%29%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08143v1&entry.124074799=Read"},
{"title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification", "author": "Guang Lin and Qibin Zhao", "abstract": "  Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.\n", "link": "http://arxiv.org/abs/2405.20770v2", "date": "2024-08-15", "relevancy": 1.9917, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Sentinel%3A%20LLM%20Agent%20for%20Adversarial%20Purification&body=Title%3A%20Large%20Language%20Model%20Sentinel%3A%20LLM%20Agent%20for%20Adversarial%20Purification%0AAuthor%3A%20Guang%20Lin%20and%20Qibin%20Zhao%0AAbstract%3A%20%20%20Over%20the%20past%20two%20years%2C%20the%20use%20of%20large%20language%20models%20%28LLMs%29%20has%20advanced%0Arapidly.%20While%20these%20LLMs%20offer%20considerable%20convenience%2C%20they%20also%20raise%0Asecurity%20concerns%2C%20as%20LLMs%20are%20vulnerable%20to%20adversarial%20attacks%20by%20some%0Awell-designed%20textual%20perturbations.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Adefense%20technique%20named%20Large%20LAnguage%20MOdel%20Sentinel%20%28LLAMOS%29%2C%20which%20is%0Adesigned%20to%20enhance%20the%20adversarial%20robustness%20of%20LLMs%20by%20purifying%20the%0Aadversarial%20textual%20examples%20before%20feeding%20them%20into%20the%20target%20LLM.%20Our%0Amethod%20comprises%20two%20main%20components%3A%20a%29%20Agent%20instruction%2C%20which%20can%20simulate%0Aa%20new%20agent%20for%20adversarial%20defense%2C%20altering%20minimal%20characters%20to%20maintain%0Athe%20original%20meaning%20of%20the%20sentence%20while%20defending%20against%20attacks%3B%20b%29%0ADefense%20guidance%2C%20which%20provides%20strategies%20for%20modifying%20clean%20or%20adversarial%0Aexamples%20to%20ensure%20effective%20defense%20and%20accurate%20outputs%20from%20the%20target%20LLMs.%0ARemarkably%2C%20the%20defense%20agent%20demonstrates%20robust%20defensive%20capabilities%20even%0Awithout%20learning%20from%20adversarial%20examples.%20Additionally%2C%20we%20conduct%20an%0Aintriguing%20adversarial%20experiment%20where%20we%20develop%20two%20agents%2C%20one%20for%20defense%0Aand%20one%20for%20attack%2C%20and%20engage%20them%20in%20mutual%20confrontation.%20During%20the%0Aadversarial%20interactions%2C%20neither%20agent%20completely%20beat%20the%20other.%20Extensive%0Aexperiments%20on%20both%20open-source%20and%20closed-source%20LLMs%20demonstrate%20that%20our%0Amethod%20effectively%20defends%20against%20adversarial%20attacks%2C%20thereby%20enhancing%0Aadversarial%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Sentinel%253A%2520LLM%2520Agent%2520for%2520Adversarial%2520Purification%26entry.906535625%3DGuang%2520Lin%2520and%2520Qibin%2520Zhao%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520two%2520years%252C%2520the%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520advanced%250Arapidly.%2520While%2520these%2520LLMs%2520offer%2520considerable%2520convenience%252C%2520they%2520also%2520raise%250Asecurity%2520concerns%252C%2520as%2520LLMs%2520are%2520vulnerable%2520to%2520adversarial%2520attacks%2520by%2520some%250Awell-designed%2520textual%2520perturbations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Adefense%2520technique%2520named%2520Large%2520LAnguage%2520MOdel%2520Sentinel%2520%2528LLAMOS%2529%252C%2520which%2520is%250Adesigned%2520to%2520enhance%2520the%2520adversarial%2520robustness%2520of%2520LLMs%2520by%2520purifying%2520the%250Aadversarial%2520textual%2520examples%2520before%2520feeding%2520them%2520into%2520the%2520target%2520LLM.%2520Our%250Amethod%2520comprises%2520two%2520main%2520components%253A%2520a%2529%2520Agent%2520instruction%252C%2520which%2520can%2520simulate%250Aa%2520new%2520agent%2520for%2520adversarial%2520defense%252C%2520altering%2520minimal%2520characters%2520to%2520maintain%250Athe%2520original%2520meaning%2520of%2520the%2520sentence%2520while%2520defending%2520against%2520attacks%253B%2520b%2529%250ADefense%2520guidance%252C%2520which%2520provides%2520strategies%2520for%2520modifying%2520clean%2520or%2520adversarial%250Aexamples%2520to%2520ensure%2520effective%2520defense%2520and%2520accurate%2520outputs%2520from%2520the%2520target%2520LLMs.%250ARemarkably%252C%2520the%2520defense%2520agent%2520demonstrates%2520robust%2520defensive%2520capabilities%2520even%250Awithout%2520learning%2520from%2520adversarial%2520examples.%2520Additionally%252C%2520we%2520conduct%2520an%250Aintriguing%2520adversarial%2520experiment%2520where%2520we%2520develop%2520two%2520agents%252C%2520one%2520for%2520defense%250Aand%2520one%2520for%2520attack%252C%2520and%2520engage%2520them%2520in%2520mutual%2520confrontation.%2520During%2520the%250Aadversarial%2520interactions%252C%2520neither%2520agent%2520completely%2520beat%2520the%2520other.%2520Extensive%250Aexperiments%2520on%2520both%2520open-source%2520and%2520closed-source%2520LLMs%2520demonstrate%2520that%2520our%250Amethod%2520effectively%2520defends%2520against%2520adversarial%2520attacks%252C%2520thereby%2520enhancing%250Aadversarial%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Sentinel%3A%20LLM%20Agent%20for%20Adversarial%20Purification&entry.906535625=Guang%20Lin%20and%20Qibin%20Zhao&entry.1292438233=%20%20Over%20the%20past%20two%20years%2C%20the%20use%20of%20large%20language%20models%20%28LLMs%29%20has%20advanced%0Arapidly.%20While%20these%20LLMs%20offer%20considerable%20convenience%2C%20they%20also%20raise%0Asecurity%20concerns%2C%20as%20LLMs%20are%20vulnerable%20to%20adversarial%20attacks%20by%20some%0Awell-designed%20textual%20perturbations.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Adefense%20technique%20named%20Large%20LAnguage%20MOdel%20Sentinel%20%28LLAMOS%29%2C%20which%20is%0Adesigned%20to%20enhance%20the%20adversarial%20robustness%20of%20LLMs%20by%20purifying%20the%0Aadversarial%20textual%20examples%20before%20feeding%20them%20into%20the%20target%20LLM.%20Our%0Amethod%20comprises%20two%20main%20components%3A%20a%29%20Agent%20instruction%2C%20which%20can%20simulate%0Aa%20new%20agent%20for%20adversarial%20defense%2C%20altering%20minimal%20characters%20to%20maintain%0Athe%20original%20meaning%20of%20the%20sentence%20while%20defending%20against%20attacks%3B%20b%29%0ADefense%20guidance%2C%20which%20provides%20strategies%20for%20modifying%20clean%20or%20adversarial%0Aexamples%20to%20ensure%20effective%20defense%20and%20accurate%20outputs%20from%20the%20target%20LLMs.%0ARemarkably%2C%20the%20defense%20agent%20demonstrates%20robust%20defensive%20capabilities%20even%0Awithout%20learning%20from%20adversarial%20examples.%20Additionally%2C%20we%20conduct%20an%0Aintriguing%20adversarial%20experiment%20where%20we%20develop%20two%20agents%2C%20one%20for%20defense%0Aand%20one%20for%20attack%2C%20and%20engage%20them%20in%20mutual%20confrontation.%20During%20the%0Aadversarial%20interactions%2C%20neither%20agent%20completely%20beat%20the%20other.%20Extensive%0Aexperiments%20on%20both%20open-source%20and%20closed-source%20LLMs%20demonstrate%20that%20our%0Amethod%20effectively%20defends%20against%20adversarial%20attacks%2C%20thereby%20enhancing%0Aadversarial%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20770v2&entry.124074799=Read"},
{"title": "Snuffy: Efficient Whole Slide Image Classifier", "author": "Hossein Jafarinia and Alireza Alipanah and Danial Hamdi and Saeed Razavi and Nahal Mirzaie and Mohammad Hossein Rohban", "abstract": "  Whole Slide Image (WSI) classification with multiple instance learning (MIL)\nin digital pathology faces significant computational challenges. Current\nmethods mostly rely on extensive self-supervised learning (SSL) for\nsatisfactory performance, requiring long training periods and considerable\ncomputational resources. At the same time, no pre-training affects performance\ndue to domain shifts from natural images to WSIs. We introduce\n\\textbf{\\textit{Snuffy}} architecture, a novel MIL-pooling method based on\nsparse transformers that mitigates performance loss with limited pre-training\nand enables continual few-shot pre-training as a competitive option. Our\nsparsity pattern is tailored for pathology and is theoretically proven to be a\nuniversal approximator with the tightest probabilistic sharp bound on the\nnumber of layers for sparse transformers, to date. We demonstrate Snuffy's\neffectiveness on CAMELYON16 and TCGA Lung cancer datasets, achieving superior\nWSI and patch-level accuracies. The code is available on\n\\url{https://github.com/jafarinia/snuffy}.\n", "link": "http://arxiv.org/abs/2408.08258v1", "date": "2024-08-15", "relevancy": 1.9868, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4899}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snuffy%3A%20Efficient%20Whole%20Slide%20Image%20Classifier&body=Title%3A%20Snuffy%3A%20Efficient%20Whole%20Slide%20Image%20Classifier%0AAuthor%3A%20Hossein%20Jafarinia%20and%20Alireza%20Alipanah%20and%20Danial%20Hamdi%20and%20Saeed%20Razavi%20and%20Nahal%20Mirzaie%20and%20Mohammad%20Hossein%20Rohban%0AAbstract%3A%20%20%20Whole%20Slide%20Image%20%28WSI%29%20classification%20with%20multiple%20instance%20learning%20%28MIL%29%0Ain%20digital%20pathology%20faces%20significant%20computational%20challenges.%20Current%0Amethods%20mostly%20rely%20on%20extensive%20self-supervised%20learning%20%28SSL%29%20for%0Asatisfactory%20performance%2C%20requiring%20long%20training%20periods%20and%20considerable%0Acomputational%20resources.%20At%20the%20same%20time%2C%20no%20pre-training%20affects%20performance%0Adue%20to%20domain%20shifts%20from%20natural%20images%20to%20WSIs.%20We%20introduce%0A%5Ctextbf%7B%5Ctextit%7BSnuffy%7D%7D%20architecture%2C%20a%20novel%20MIL-pooling%20method%20based%20on%0Asparse%20transformers%20that%20mitigates%20performance%20loss%20with%20limited%20pre-training%0Aand%20enables%20continual%20few-shot%20pre-training%20as%20a%20competitive%20option.%20Our%0Asparsity%20pattern%20is%20tailored%20for%20pathology%20and%20is%20theoretically%20proven%20to%20be%20a%0Auniversal%20approximator%20with%20the%20tightest%20probabilistic%20sharp%20bound%20on%20the%0Anumber%20of%20layers%20for%20sparse%20transformers%2C%20to%20date.%20We%20demonstrate%20Snuffy%27s%0Aeffectiveness%20on%20CAMELYON16%20and%20TCGA%20Lung%20cancer%20datasets%2C%20achieving%20superior%0AWSI%20and%20patch-level%20accuracies.%20The%20code%20is%20available%20on%0A%5Curl%7Bhttps%3A//github.com/jafarinia/snuffy%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnuffy%253A%2520Efficient%2520Whole%2520Slide%2520Image%2520Classifier%26entry.906535625%3DHossein%2520Jafarinia%2520and%2520Alireza%2520Alipanah%2520and%2520Danial%2520Hamdi%2520and%2520Saeed%2520Razavi%2520and%2520Nahal%2520Mirzaie%2520and%2520Mohammad%2520Hossein%2520Rohban%26entry.1292438233%3D%2520%2520Whole%2520Slide%2520Image%2520%2528WSI%2529%2520classification%2520with%2520multiple%2520instance%2520learning%2520%2528MIL%2529%250Ain%2520digital%2520pathology%2520faces%2520significant%2520computational%2520challenges.%2520Current%250Amethods%2520mostly%2520rely%2520on%2520extensive%2520self-supervised%2520learning%2520%2528SSL%2529%2520for%250Asatisfactory%2520performance%252C%2520requiring%2520long%2520training%2520periods%2520and%2520considerable%250Acomputational%2520resources.%2520At%2520the%2520same%2520time%252C%2520no%2520pre-training%2520affects%2520performance%250Adue%2520to%2520domain%2520shifts%2520from%2520natural%2520images%2520to%2520WSIs.%2520We%2520introduce%250A%255Ctextbf%257B%255Ctextit%257BSnuffy%257D%257D%2520architecture%252C%2520a%2520novel%2520MIL-pooling%2520method%2520based%2520on%250Asparse%2520transformers%2520that%2520mitigates%2520performance%2520loss%2520with%2520limited%2520pre-training%250Aand%2520enables%2520continual%2520few-shot%2520pre-training%2520as%2520a%2520competitive%2520option.%2520Our%250Asparsity%2520pattern%2520is%2520tailored%2520for%2520pathology%2520and%2520is%2520theoretically%2520proven%2520to%2520be%2520a%250Auniversal%2520approximator%2520with%2520the%2520tightest%2520probabilistic%2520sharp%2520bound%2520on%2520the%250Anumber%2520of%2520layers%2520for%2520sparse%2520transformers%252C%2520to%2520date.%2520We%2520demonstrate%2520Snuffy%2527s%250Aeffectiveness%2520on%2520CAMELYON16%2520and%2520TCGA%2520Lung%2520cancer%2520datasets%252C%2520achieving%2520superior%250AWSI%2520and%2520patch-level%2520accuracies.%2520The%2520code%2520is%2520available%2520on%250A%255Curl%257Bhttps%253A//github.com/jafarinia/snuffy%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snuffy%3A%20Efficient%20Whole%20Slide%20Image%20Classifier&entry.906535625=Hossein%20Jafarinia%20and%20Alireza%20Alipanah%20and%20Danial%20Hamdi%20and%20Saeed%20Razavi%20and%20Nahal%20Mirzaie%20and%20Mohammad%20Hossein%20Rohban&entry.1292438233=%20%20Whole%20Slide%20Image%20%28WSI%29%20classification%20with%20multiple%20instance%20learning%20%28MIL%29%0Ain%20digital%20pathology%20faces%20significant%20computational%20challenges.%20Current%0Amethods%20mostly%20rely%20on%20extensive%20self-supervised%20learning%20%28SSL%29%20for%0Asatisfactory%20performance%2C%20requiring%20long%20training%20periods%20and%20considerable%0Acomputational%20resources.%20At%20the%20same%20time%2C%20no%20pre-training%20affects%20performance%0Adue%20to%20domain%20shifts%20from%20natural%20images%20to%20WSIs.%20We%20introduce%0A%5Ctextbf%7B%5Ctextit%7BSnuffy%7D%7D%20architecture%2C%20a%20novel%20MIL-pooling%20method%20based%20on%0Asparse%20transformers%20that%20mitigates%20performance%20loss%20with%20limited%20pre-training%0Aand%20enables%20continual%20few-shot%20pre-training%20as%20a%20competitive%20option.%20Our%0Asparsity%20pattern%20is%20tailored%20for%20pathology%20and%20is%20theoretically%20proven%20to%20be%20a%0Auniversal%20approximator%20with%20the%20tightest%20probabilistic%20sharp%20bound%20on%20the%0Anumber%20of%20layers%20for%20sparse%20transformers%2C%20to%20date.%20We%20demonstrate%20Snuffy%27s%0Aeffectiveness%20on%20CAMELYON16%20and%20TCGA%20Lung%20cancer%20datasets%2C%20achieving%20superior%0AWSI%20and%20patch-level%20accuracies.%20The%20code%20is%20available%20on%0A%5Curl%7Bhttps%3A//github.com/jafarinia/snuffy%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08258v1&entry.124074799=Read"},
{"title": "Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple\n  Matching with Entity and Relation Texts", "author": "Soojin Yoon and Sungho Ko and Tongyoung Kim and SeongKu Kang and Jinyoung Yeo and Dongha Lee", "abstract": "  Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge. Existing methods,\nmostly supervised, face challenges in obtaining labeled entity pairs. To\naddress this, recent studies have shifted towards self-supervised and\nunsupervised frameworks. Despite their effectiveness, these approaches have\nlimitations: (1) Relation passing: mainly focusing on the entity while\nneglecting the semantic information of relations, (2) Isomorphic assumption:\nassuming isomorphism between source and target graphs, which leads to noise and\nreduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise\nin the textual features, especially when encountering inconsistent translations\nor Out-Of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an\nunsupervised and robust cross-lingual EA pipeline that jointly performs\nEntity-level and Relation-level Alignment by neighbor triple matching strategy\nusing semantic textual features of relations and entities. Its refinement step\niteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification step\nexamines the entities' neighbor triples as the linearized text. This\nAlign-then-Verify pipeline rigorously assesses alignment results, achieving\nnear-perfect alignment even in the presence of noisy textual features of\nentities. Our extensive experiments demonstrate that the robustness and general\napplicability of ERAlign improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.\n", "link": "http://arxiv.org/abs/2407.15588v2", "date": "2024-08-15", "relevancy": 1.9856, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5197}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5103}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Robust%20Cross-Lingual%20Entity%20Alignment%20via%20Neighbor%20Triple%0A%20%20Matching%20with%20Entity%20and%20Relation%20Texts&body=Title%3A%20Unsupervised%20Robust%20Cross-Lingual%20Entity%20Alignment%20via%20Neighbor%20Triple%0A%20%20Matching%20with%20Entity%20and%20Relation%20Texts%0AAuthor%3A%20Soojin%20Yoon%20and%20Sungho%20Ko%20and%20Tongyoung%20Kim%20and%20SeongKu%20Kang%20and%20Jinyoung%20Yeo%20and%20Dongha%20Lee%0AAbstract%3A%20%20%20Cross-lingual%20entity%20alignment%20%28EA%29%20enables%20the%20integration%20of%20multiple%0Aknowledge%20graphs%20%28KGs%29%20across%20different%20languages%2C%20providing%20users%20with%0Aseamless%20access%20to%20diverse%20and%20comprehensive%20knowledge.%20Existing%20methods%2C%0Amostly%20supervised%2C%20face%20challenges%20in%20obtaining%20labeled%20entity%20pairs.%20To%0Aaddress%20this%2C%20recent%20studies%20have%20shifted%20towards%20self-supervised%20and%0Aunsupervised%20frameworks.%20Despite%20their%20effectiveness%2C%20these%20approaches%20have%0Alimitations%3A%20%281%29%20Relation%20passing%3A%20mainly%20focusing%20on%20the%20entity%20while%0Aneglecting%20the%20semantic%20information%20of%20relations%2C%20%282%29%20Isomorphic%20assumption%3A%0Aassuming%20isomorphism%20between%20source%20and%20target%20graphs%2C%20which%20leads%20to%20noise%20and%0Areduced%20alignment%20accuracy%2C%20and%20%283%29%20Noise%20vulnerability%3A%20susceptible%20to%20noise%0Ain%20the%20textual%20features%2C%20especially%20when%20encountering%20inconsistent%20translations%0Aor%20Out-Of-Vocabulary%20%28OOV%29%20problems.%20In%20this%20paper%2C%20we%20propose%20ERAlign%2C%20an%0Aunsupervised%20and%20robust%20cross-lingual%20EA%20pipeline%20that%20jointly%20performs%0AEntity-level%20and%20Relation-level%20Alignment%20by%20neighbor%20triple%20matching%20strategy%0Ausing%20semantic%20textual%20features%20of%20relations%20and%20entities.%20Its%20refinement%20step%0Aiteratively%20enhances%20results%20by%20fusing%20entity-level%20and%20relation-level%0Aalignments%20based%20on%20neighbor%20triple%20matching.%20The%20additional%20verification%20step%0Aexamines%20the%20entities%27%20neighbor%20triples%20as%20the%20linearized%20text.%20This%0AAlign-then-Verify%20pipeline%20rigorously%20assesses%20alignment%20results%2C%20achieving%0Anear-perfect%20alignment%20even%20in%20the%20presence%20of%20noisy%20textual%20features%20of%0Aentities.%20Our%20extensive%20experiments%20demonstrate%20that%20the%20robustness%20and%20general%0Aapplicability%20of%20ERAlign%20improved%20the%20accuracy%20and%20effectiveness%20of%20EA%20tasks%2C%0Acontributing%20significantly%20to%20knowledge-oriented%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15588v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Robust%2520Cross-Lingual%2520Entity%2520Alignment%2520via%2520Neighbor%2520Triple%250A%2520%2520Matching%2520with%2520Entity%2520and%2520Relation%2520Texts%26entry.906535625%3DSoojin%2520Yoon%2520and%2520Sungho%2520Ko%2520and%2520Tongyoung%2520Kim%2520and%2520SeongKu%2520Kang%2520and%2520Jinyoung%2520Yeo%2520and%2520Dongha%2520Lee%26entry.1292438233%3D%2520%2520Cross-lingual%2520entity%2520alignment%2520%2528EA%2529%2520enables%2520the%2520integration%2520of%2520multiple%250Aknowledge%2520graphs%2520%2528KGs%2529%2520across%2520different%2520languages%252C%2520providing%2520users%2520with%250Aseamless%2520access%2520to%2520diverse%2520and%2520comprehensive%2520knowledge.%2520Existing%2520methods%252C%250Amostly%2520supervised%252C%2520face%2520challenges%2520in%2520obtaining%2520labeled%2520entity%2520pairs.%2520To%250Aaddress%2520this%252C%2520recent%2520studies%2520have%2520shifted%2520towards%2520self-supervised%2520and%250Aunsupervised%2520frameworks.%2520Despite%2520their%2520effectiveness%252C%2520these%2520approaches%2520have%250Alimitations%253A%2520%25281%2529%2520Relation%2520passing%253A%2520mainly%2520focusing%2520on%2520the%2520entity%2520while%250Aneglecting%2520the%2520semantic%2520information%2520of%2520relations%252C%2520%25282%2529%2520Isomorphic%2520assumption%253A%250Aassuming%2520isomorphism%2520between%2520source%2520and%2520target%2520graphs%252C%2520which%2520leads%2520to%2520noise%2520and%250Areduced%2520alignment%2520accuracy%252C%2520and%2520%25283%2529%2520Noise%2520vulnerability%253A%2520susceptible%2520to%2520noise%250Ain%2520the%2520textual%2520features%252C%2520especially%2520when%2520encountering%2520inconsistent%2520translations%250Aor%2520Out-Of-Vocabulary%2520%2528OOV%2529%2520problems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ERAlign%252C%2520an%250Aunsupervised%2520and%2520robust%2520cross-lingual%2520EA%2520pipeline%2520that%2520jointly%2520performs%250AEntity-level%2520and%2520Relation-level%2520Alignment%2520by%2520neighbor%2520triple%2520matching%2520strategy%250Ausing%2520semantic%2520textual%2520features%2520of%2520relations%2520and%2520entities.%2520Its%2520refinement%2520step%250Aiteratively%2520enhances%2520results%2520by%2520fusing%2520entity-level%2520and%2520relation-level%250Aalignments%2520based%2520on%2520neighbor%2520triple%2520matching.%2520The%2520additional%2520verification%2520step%250Aexamines%2520the%2520entities%2527%2520neighbor%2520triples%2520as%2520the%2520linearized%2520text.%2520This%250AAlign-then-Verify%2520pipeline%2520rigorously%2520assesses%2520alignment%2520results%252C%2520achieving%250Anear-perfect%2520alignment%2520even%2520in%2520the%2520presence%2520of%2520noisy%2520textual%2520features%2520of%250Aentities.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%2520the%2520robustness%2520and%2520general%250Aapplicability%2520of%2520ERAlign%2520improved%2520the%2520accuracy%2520and%2520effectiveness%2520of%2520EA%2520tasks%252C%250Acontributing%2520significantly%2520to%2520knowledge-oriented%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15588v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Robust%20Cross-Lingual%20Entity%20Alignment%20via%20Neighbor%20Triple%0A%20%20Matching%20with%20Entity%20and%20Relation%20Texts&entry.906535625=Soojin%20Yoon%20and%20Sungho%20Ko%20and%20Tongyoung%20Kim%20and%20SeongKu%20Kang%20and%20Jinyoung%20Yeo%20and%20Dongha%20Lee&entry.1292438233=%20%20Cross-lingual%20entity%20alignment%20%28EA%29%20enables%20the%20integration%20of%20multiple%0Aknowledge%20graphs%20%28KGs%29%20across%20different%20languages%2C%20providing%20users%20with%0Aseamless%20access%20to%20diverse%20and%20comprehensive%20knowledge.%20Existing%20methods%2C%0Amostly%20supervised%2C%20face%20challenges%20in%20obtaining%20labeled%20entity%20pairs.%20To%0Aaddress%20this%2C%20recent%20studies%20have%20shifted%20towards%20self-supervised%20and%0Aunsupervised%20frameworks.%20Despite%20their%20effectiveness%2C%20these%20approaches%20have%0Alimitations%3A%20%281%29%20Relation%20passing%3A%20mainly%20focusing%20on%20the%20entity%20while%0Aneglecting%20the%20semantic%20information%20of%20relations%2C%20%282%29%20Isomorphic%20assumption%3A%0Aassuming%20isomorphism%20between%20source%20and%20target%20graphs%2C%20which%20leads%20to%20noise%20and%0Areduced%20alignment%20accuracy%2C%20and%20%283%29%20Noise%20vulnerability%3A%20susceptible%20to%20noise%0Ain%20the%20textual%20features%2C%20especially%20when%20encountering%20inconsistent%20translations%0Aor%20Out-Of-Vocabulary%20%28OOV%29%20problems.%20In%20this%20paper%2C%20we%20propose%20ERAlign%2C%20an%0Aunsupervised%20and%20robust%20cross-lingual%20EA%20pipeline%20that%20jointly%20performs%0AEntity-level%20and%20Relation-level%20Alignment%20by%20neighbor%20triple%20matching%20strategy%0Ausing%20semantic%20textual%20features%20of%20relations%20and%20entities.%20Its%20refinement%20step%0Aiteratively%20enhances%20results%20by%20fusing%20entity-level%20and%20relation-level%0Aalignments%20based%20on%20neighbor%20triple%20matching.%20The%20additional%20verification%20step%0Aexamines%20the%20entities%27%20neighbor%20triples%20as%20the%20linearized%20text.%20This%0AAlign-then-Verify%20pipeline%20rigorously%20assesses%20alignment%20results%2C%20achieving%0Anear-perfect%20alignment%20even%20in%20the%20presence%20of%20noisy%20textual%20features%20of%0Aentities.%20Our%20extensive%20experiments%20demonstrate%20that%20the%20robustness%20and%20general%0Aapplicability%20of%20ERAlign%20improved%20the%20accuracy%20and%20effectiveness%20of%20EA%20tasks%2C%0Acontributing%20significantly%20to%20knowledge-oriented%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15588v2&entry.124074799=Read"},
{"title": "A Distributed Privacy Preserving Model for the Detection of Alzheimer's\n  Disease", "author": "Paul K. Mandal", "abstract": "  BACKGROUND: Segmentation of medical data, concerns about personal health\ninformation (PHI) breaches, and the direct and indirect costs of consolidating\nand managing such segmented date should motivate diagnostic machine learning\n(DML) researchers to identify privacy-preserving machine learning algorithms\nthat can train on distributed or decentralized datasets of different\nmodalities. Federated learning models provide such a decentralized machine\nlearning framework in which multiple investigators in possession of disparate\ndatasets and working on different devices or servers can train collaboratively\na global machine learning models without ever having to exchange local data and\nthus can meet statutory PHI protections. To this end, a vertical federate\nlearning model is devised and tested for efficacy in the detection of\nAlzheimer's Disease (AD).\n  METHODS: The second version of Open Access Series of Imaging Studies -- with\nits panoply of demographic, imaging, and clinical assessment datasets -- was\nused to test a multimodal vertical federated learning (VFL) model for AD\ndetection.\n  RESULTS: By training and validating this VFL model on the demographic,\nclinical, and MRI data in OASIS-2, an 82.9\\% accuracy rate is achieved,\nconsistent with previously reported results.\n  CONCLUSIONS: The VFL architecture proposed herein offers a novel distributed\narchitecture, enabling collaborative learning across diverse sources of medical\ndata while respecting statutory privacy constraints. By leveraging multiple\nmodalities of data, the robustness and accuracy of AD detection can be\nenhanced. This model not only contributes to the advancement of federated\nlearning techniques but also holds promise for overcoming the hurdles posed by\ndata segmentation in medical research.\n", "link": "http://arxiv.org/abs/2312.10237v3", "date": "2024-08-15", "relevancy": 1.9741, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5021}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4965}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Distributed%20Privacy%20Preserving%20Model%20for%20the%20Detection%20of%20Alzheimer%27s%0A%20%20Disease&body=Title%3A%20A%20Distributed%20Privacy%20Preserving%20Model%20for%20the%20Detection%20of%20Alzheimer%27s%0A%20%20Disease%0AAuthor%3A%20Paul%20K.%20Mandal%0AAbstract%3A%20%20%20BACKGROUND%3A%20Segmentation%20of%20medical%20data%2C%20concerns%20about%20personal%20health%0Ainformation%20%28PHI%29%20breaches%2C%20and%20the%20direct%20and%20indirect%20costs%20of%20consolidating%0Aand%20managing%20such%20segmented%20date%20should%20motivate%20diagnostic%20machine%20learning%0A%28DML%29%20researchers%20to%20identify%20privacy-preserving%20machine%20learning%20algorithms%0Athat%20can%20train%20on%20distributed%20or%20decentralized%20datasets%20of%20different%0Amodalities.%20Federated%20learning%20models%20provide%20such%20a%20decentralized%20machine%0Alearning%20framework%20in%20which%20multiple%20investigators%20in%20possession%20of%20disparate%0Adatasets%20and%20working%20on%20different%20devices%20or%20servers%20can%20train%20collaboratively%0Aa%20global%20machine%20learning%20models%20without%20ever%20having%20to%20exchange%20local%20data%20and%0Athus%20can%20meet%20statutory%20PHI%20protections.%20To%20this%20end%2C%20a%20vertical%20federate%0Alearning%20model%20is%20devised%20and%20tested%20for%20efficacy%20in%20the%20detection%20of%0AAlzheimer%27s%20Disease%20%28AD%29.%0A%20%20METHODS%3A%20The%20second%20version%20of%20Open%20Access%20Series%20of%20Imaging%20Studies%20--%20with%0Aits%20panoply%20of%20demographic%2C%20imaging%2C%20and%20clinical%20assessment%20datasets%20--%20was%0Aused%20to%20test%20a%20multimodal%20vertical%20federated%20learning%20%28VFL%29%20model%20for%20AD%0Adetection.%0A%20%20RESULTS%3A%20By%20training%20and%20validating%20this%20VFL%20model%20on%20the%20demographic%2C%0Aclinical%2C%20and%20MRI%20data%20in%20OASIS-2%2C%20an%2082.9%5C%25%20accuracy%20rate%20is%20achieved%2C%0Aconsistent%20with%20previously%20reported%20results.%0A%20%20CONCLUSIONS%3A%20The%20VFL%20architecture%20proposed%20herein%20offers%20a%20novel%20distributed%0Aarchitecture%2C%20enabling%20collaborative%20learning%20across%20diverse%20sources%20of%20medical%0Adata%20while%20respecting%20statutory%20privacy%20constraints.%20By%20leveraging%20multiple%0Amodalities%20of%20data%2C%20the%20robustness%20and%20accuracy%20of%20AD%20detection%20can%20be%0Aenhanced.%20This%20model%20not%20only%20contributes%20to%20the%20advancement%20of%20federated%0Alearning%20techniques%20but%20also%20holds%20promise%20for%20overcoming%20the%20hurdles%20posed%20by%0Adata%20segmentation%20in%20medical%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10237v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Distributed%2520Privacy%2520Preserving%2520Model%2520for%2520the%2520Detection%2520of%2520Alzheimer%2527s%250A%2520%2520Disease%26entry.906535625%3DPaul%2520K.%2520Mandal%26entry.1292438233%3D%2520%2520BACKGROUND%253A%2520Segmentation%2520of%2520medical%2520data%252C%2520concerns%2520about%2520personal%2520health%250Ainformation%2520%2528PHI%2529%2520breaches%252C%2520and%2520the%2520direct%2520and%2520indirect%2520costs%2520of%2520consolidating%250Aand%2520managing%2520such%2520segmented%2520date%2520should%2520motivate%2520diagnostic%2520machine%2520learning%250A%2528DML%2529%2520researchers%2520to%2520identify%2520privacy-preserving%2520machine%2520learning%2520algorithms%250Athat%2520can%2520train%2520on%2520distributed%2520or%2520decentralized%2520datasets%2520of%2520different%250Amodalities.%2520Federated%2520learning%2520models%2520provide%2520such%2520a%2520decentralized%2520machine%250Alearning%2520framework%2520in%2520which%2520multiple%2520investigators%2520in%2520possession%2520of%2520disparate%250Adatasets%2520and%2520working%2520on%2520different%2520devices%2520or%2520servers%2520can%2520train%2520collaboratively%250Aa%2520global%2520machine%2520learning%2520models%2520without%2520ever%2520having%2520to%2520exchange%2520local%2520data%2520and%250Athus%2520can%2520meet%2520statutory%2520PHI%2520protections.%2520To%2520this%2520end%252C%2520a%2520vertical%2520federate%250Alearning%2520model%2520is%2520devised%2520and%2520tested%2520for%2520efficacy%2520in%2520the%2520detection%2520of%250AAlzheimer%2527s%2520Disease%2520%2528AD%2529.%250A%2520%2520METHODS%253A%2520The%2520second%2520version%2520of%2520Open%2520Access%2520Series%2520of%2520Imaging%2520Studies%2520--%2520with%250Aits%2520panoply%2520of%2520demographic%252C%2520imaging%252C%2520and%2520clinical%2520assessment%2520datasets%2520--%2520was%250Aused%2520to%2520test%2520a%2520multimodal%2520vertical%2520federated%2520learning%2520%2528VFL%2529%2520model%2520for%2520AD%250Adetection.%250A%2520%2520RESULTS%253A%2520By%2520training%2520and%2520validating%2520this%2520VFL%2520model%2520on%2520the%2520demographic%252C%250Aclinical%252C%2520and%2520MRI%2520data%2520in%2520OASIS-2%252C%2520an%252082.9%255C%2525%2520accuracy%2520rate%2520is%2520achieved%252C%250Aconsistent%2520with%2520previously%2520reported%2520results.%250A%2520%2520CONCLUSIONS%253A%2520The%2520VFL%2520architecture%2520proposed%2520herein%2520offers%2520a%2520novel%2520distributed%250Aarchitecture%252C%2520enabling%2520collaborative%2520learning%2520across%2520diverse%2520sources%2520of%2520medical%250Adata%2520while%2520respecting%2520statutory%2520privacy%2520constraints.%2520By%2520leveraging%2520multiple%250Amodalities%2520of%2520data%252C%2520the%2520robustness%2520and%2520accuracy%2520of%2520AD%2520detection%2520can%2520be%250Aenhanced.%2520This%2520model%2520not%2520only%2520contributes%2520to%2520the%2520advancement%2520of%2520federated%250Alearning%2520techniques%2520but%2520also%2520holds%2520promise%2520for%2520overcoming%2520the%2520hurdles%2520posed%2520by%250Adata%2520segmentation%2520in%2520medical%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10237v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Distributed%20Privacy%20Preserving%20Model%20for%20the%20Detection%20of%20Alzheimer%27s%0A%20%20Disease&entry.906535625=Paul%20K.%20Mandal&entry.1292438233=%20%20BACKGROUND%3A%20Segmentation%20of%20medical%20data%2C%20concerns%20about%20personal%20health%0Ainformation%20%28PHI%29%20breaches%2C%20and%20the%20direct%20and%20indirect%20costs%20of%20consolidating%0Aand%20managing%20such%20segmented%20date%20should%20motivate%20diagnostic%20machine%20learning%0A%28DML%29%20researchers%20to%20identify%20privacy-preserving%20machine%20learning%20algorithms%0Athat%20can%20train%20on%20distributed%20or%20decentralized%20datasets%20of%20different%0Amodalities.%20Federated%20learning%20models%20provide%20such%20a%20decentralized%20machine%0Alearning%20framework%20in%20which%20multiple%20investigators%20in%20possession%20of%20disparate%0Adatasets%20and%20working%20on%20different%20devices%20or%20servers%20can%20train%20collaboratively%0Aa%20global%20machine%20learning%20models%20without%20ever%20having%20to%20exchange%20local%20data%20and%0Athus%20can%20meet%20statutory%20PHI%20protections.%20To%20this%20end%2C%20a%20vertical%20federate%0Alearning%20model%20is%20devised%20and%20tested%20for%20efficacy%20in%20the%20detection%20of%0AAlzheimer%27s%20Disease%20%28AD%29.%0A%20%20METHODS%3A%20The%20second%20version%20of%20Open%20Access%20Series%20of%20Imaging%20Studies%20--%20with%0Aits%20panoply%20of%20demographic%2C%20imaging%2C%20and%20clinical%20assessment%20datasets%20--%20was%0Aused%20to%20test%20a%20multimodal%20vertical%20federated%20learning%20%28VFL%29%20model%20for%20AD%0Adetection.%0A%20%20RESULTS%3A%20By%20training%20and%20validating%20this%20VFL%20model%20on%20the%20demographic%2C%0Aclinical%2C%20and%20MRI%20data%20in%20OASIS-2%2C%20an%2082.9%5C%25%20accuracy%20rate%20is%20achieved%2C%0Aconsistent%20with%20previously%20reported%20results.%0A%20%20CONCLUSIONS%3A%20The%20VFL%20architecture%20proposed%20herein%20offers%20a%20novel%20distributed%0Aarchitecture%2C%20enabling%20collaborative%20learning%20across%20diverse%20sources%20of%20medical%0Adata%20while%20respecting%20statutory%20privacy%20constraints.%20By%20leveraging%20multiple%0Amodalities%20of%20data%2C%20the%20robustness%20and%20accuracy%20of%20AD%20detection%20can%20be%0Aenhanced.%20This%20model%20not%20only%20contributes%20to%20the%20advancement%20of%20federated%0Alearning%20techniques%20but%20also%20holds%20promise%20for%20overcoming%20the%20hurdles%20posed%20by%0Adata%20segmentation%20in%20medical%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10237v3&entry.124074799=Read"},
{"title": "Characterizing Multimodal Long-form Summarization: A Case Study on\n  Financial Reports", "author": "Tianyu Cao and Natraj Raman and Danial Dervovic and Chenhao Tan", "abstract": "  As large language models (LLMs) expand the power of natural language\nprocessing to handle long inputs, rigorous and systematic analyses are\nnecessary to understand their abilities and behavior. A salient application is\nsummarization, due to its ubiquity and controversy (e.g., researchers have\ndeclared the death of summarization). In this paper, we use financial report\nsummarization as a case study because financial reports are not only long but\nalso use numbers and tables extensively. We propose a computational framework\nfor characterizing multimodal long-form summarization and investigate the\nbehavior of Claude 2.0/2.1, GPT-4/3.5, and Cohere. We find that GPT-3.5 and\nCohere fail to perform this summarization task meaningfully. For Claude 2 and\nGPT-4, we analyze the extractiveness of the summary and identify a position\nbias in LLMs. This position bias disappears after shuffling the input for\nClaude, which suggests that Claude seems to recognize important information. We\nalso conduct a comprehensive investigation on the use of numeric data in\nLLM-generated summaries and offer a taxonomy of numeric hallucination. We\nemploy prompt engineering to improve GPT-4's use of numbers with limited\nsuccess. Overall, our analyses highlight the strong capability of Claude 2 in\nhandling long multimodal inputs compared to GPT-4. The generated summaries and\nevaluation code are available at\nhttps://github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization.\n", "link": "http://arxiv.org/abs/2404.06162v3", "date": "2024-08-15", "relevancy": 1.9686, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5036}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Multimodal%20Long-form%20Summarization%3A%20A%20Case%20Study%20on%0A%20%20Financial%20Reports&body=Title%3A%20Characterizing%20Multimodal%20Long-form%20Summarization%3A%20A%20Case%20Study%20on%0A%20%20Financial%20Reports%0AAuthor%3A%20Tianyu%20Cao%20and%20Natraj%20Raman%20and%20Danial%20Dervovic%20and%20Chenhao%20Tan%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20expand%20the%20power%20of%20natural%20language%0Aprocessing%20to%20handle%20long%20inputs%2C%20rigorous%20and%20systematic%20analyses%20are%0Anecessary%20to%20understand%20their%20abilities%20and%20behavior.%20A%20salient%20application%20is%0Asummarization%2C%20due%20to%20its%20ubiquity%20and%20controversy%20%28e.g.%2C%20researchers%20have%0Adeclared%20the%20death%20of%20summarization%29.%20In%20this%20paper%2C%20we%20use%20financial%20report%0Asummarization%20as%20a%20case%20study%20because%20financial%20reports%20are%20not%20only%20long%20but%0Aalso%20use%20numbers%20and%20tables%20extensively.%20We%20propose%20a%20computational%20framework%0Afor%20characterizing%20multimodal%20long-form%20summarization%20and%20investigate%20the%0Abehavior%20of%20Claude%202.0/2.1%2C%20GPT-4/3.5%2C%20and%20Cohere.%20We%20find%20that%20GPT-3.5%20and%0ACohere%20fail%20to%20perform%20this%20summarization%20task%20meaningfully.%20For%20Claude%202%20and%0AGPT-4%2C%20we%20analyze%20the%20extractiveness%20of%20the%20summary%20and%20identify%20a%20position%0Abias%20in%20LLMs.%20This%20position%20bias%20disappears%20after%20shuffling%20the%20input%20for%0AClaude%2C%20which%20suggests%20that%20Claude%20seems%20to%20recognize%20important%20information.%20We%0Aalso%20conduct%20a%20comprehensive%20investigation%20on%20the%20use%20of%20numeric%20data%20in%0ALLM-generated%20summaries%20and%20offer%20a%20taxonomy%20of%20numeric%20hallucination.%20We%0Aemploy%20prompt%20engineering%20to%20improve%20GPT-4%27s%20use%20of%20numbers%20with%20limited%0Asuccess.%20Overall%2C%20our%20analyses%20highlight%20the%20strong%20capability%20of%20Claude%202%20in%0Ahandling%20long%20multimodal%20inputs%20compared%20to%20GPT-4.%20The%20generated%20summaries%20and%0Aevaluation%20code%20are%20available%20at%0Ahttps%3A//github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06162v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Multimodal%2520Long-form%2520Summarization%253A%2520A%2520Case%2520Study%2520on%250A%2520%2520Financial%2520Reports%26entry.906535625%3DTianyu%2520Cao%2520and%2520Natraj%2520Raman%2520and%2520Danial%2520Dervovic%2520and%2520Chenhao%2520Tan%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520expand%2520the%2520power%2520of%2520natural%2520language%250Aprocessing%2520to%2520handle%2520long%2520inputs%252C%2520rigorous%2520and%2520systematic%2520analyses%2520are%250Anecessary%2520to%2520understand%2520their%2520abilities%2520and%2520behavior.%2520A%2520salient%2520application%2520is%250Asummarization%252C%2520due%2520to%2520its%2520ubiquity%2520and%2520controversy%2520%2528e.g.%252C%2520researchers%2520have%250Adeclared%2520the%2520death%2520of%2520summarization%2529.%2520In%2520this%2520paper%252C%2520we%2520use%2520financial%2520report%250Asummarization%2520as%2520a%2520case%2520study%2520because%2520financial%2520reports%2520are%2520not%2520only%2520long%2520but%250Aalso%2520use%2520numbers%2520and%2520tables%2520extensively.%2520We%2520propose%2520a%2520computational%2520framework%250Afor%2520characterizing%2520multimodal%2520long-form%2520summarization%2520and%2520investigate%2520the%250Abehavior%2520of%2520Claude%25202.0/2.1%252C%2520GPT-4/3.5%252C%2520and%2520Cohere.%2520We%2520find%2520that%2520GPT-3.5%2520and%250ACohere%2520fail%2520to%2520perform%2520this%2520summarization%2520task%2520meaningfully.%2520For%2520Claude%25202%2520and%250AGPT-4%252C%2520we%2520analyze%2520the%2520extractiveness%2520of%2520the%2520summary%2520and%2520identify%2520a%2520position%250Abias%2520in%2520LLMs.%2520This%2520position%2520bias%2520disappears%2520after%2520shuffling%2520the%2520input%2520for%250AClaude%252C%2520which%2520suggests%2520that%2520Claude%2520seems%2520to%2520recognize%2520important%2520information.%2520We%250Aalso%2520conduct%2520a%2520comprehensive%2520investigation%2520on%2520the%2520use%2520of%2520numeric%2520data%2520in%250ALLM-generated%2520summaries%2520and%2520offer%2520a%2520taxonomy%2520of%2520numeric%2520hallucination.%2520We%250Aemploy%2520prompt%2520engineering%2520to%2520improve%2520GPT-4%2527s%2520use%2520of%2520numbers%2520with%2520limited%250Asuccess.%2520Overall%252C%2520our%2520analyses%2520highlight%2520the%2520strong%2520capability%2520of%2520Claude%25202%2520in%250Ahandling%2520long%2520multimodal%2520inputs%2520compared%2520to%2520GPT-4.%2520The%2520generated%2520summaries%2520and%250Aevaluation%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06162v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Multimodal%20Long-form%20Summarization%3A%20A%20Case%20Study%20on%0A%20%20Financial%20Reports&entry.906535625=Tianyu%20Cao%20and%20Natraj%20Raman%20and%20Danial%20Dervovic%20and%20Chenhao%20Tan&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20expand%20the%20power%20of%20natural%20language%0Aprocessing%20to%20handle%20long%20inputs%2C%20rigorous%20and%20systematic%20analyses%20are%0Anecessary%20to%20understand%20their%20abilities%20and%20behavior.%20A%20salient%20application%20is%0Asummarization%2C%20due%20to%20its%20ubiquity%20and%20controversy%20%28e.g.%2C%20researchers%20have%0Adeclared%20the%20death%20of%20summarization%29.%20In%20this%20paper%2C%20we%20use%20financial%20report%0Asummarization%20as%20a%20case%20study%20because%20financial%20reports%20are%20not%20only%20long%20but%0Aalso%20use%20numbers%20and%20tables%20extensively.%20We%20propose%20a%20computational%20framework%0Afor%20characterizing%20multimodal%20long-form%20summarization%20and%20investigate%20the%0Abehavior%20of%20Claude%202.0/2.1%2C%20GPT-4/3.5%2C%20and%20Cohere.%20We%20find%20that%20GPT-3.5%20and%0ACohere%20fail%20to%20perform%20this%20summarization%20task%20meaningfully.%20For%20Claude%202%20and%0AGPT-4%2C%20we%20analyze%20the%20extractiveness%20of%20the%20summary%20and%20identify%20a%20position%0Abias%20in%20LLMs.%20This%20position%20bias%20disappears%20after%20shuffling%20the%20input%20for%0AClaude%2C%20which%20suggests%20that%20Claude%20seems%20to%20recognize%20important%20information.%20We%0Aalso%20conduct%20a%20comprehensive%20investigation%20on%20the%20use%20of%20numeric%20data%20in%0ALLM-generated%20summaries%20and%20offer%20a%20taxonomy%20of%20numeric%20hallucination.%20We%0Aemploy%20prompt%20engineering%20to%20improve%20GPT-4%27s%20use%20of%20numbers%20with%20limited%0Asuccess.%20Overall%2C%20our%20analyses%20highlight%20the%20strong%20capability%20of%20Claude%202%20in%0Ahandling%20long%20multimodal%20inputs%20compared%20to%20GPT-4.%20The%20generated%20summaries%20and%0Aevaluation%20code%20are%20available%20at%0Ahttps%3A//github.com/ChicagoHAI/characterizing-multimodal-long-form-summarization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06162v3&entry.124074799=Read"},
{"title": "SPEED: Scalable Preprocessing of EEG Data for Self-Supervised Learning", "author": "Anders Gj\u00f8lbye and Lina Skerath and William Lehn-Schi\u00f8ler and Nicolas Langer and Lars Kai Hansen", "abstract": "  Electroencephalography (EEG) research typically focuses on tasks with\nnarrowly defined objectives, but recent studies are expanding into the use of\nunlabeled data within larger models, aiming for a broader range of\napplications. This addresses a critical challenge in EEG research. For example,\nKostas et al. (2021) show that self-supervised learning (SSL) outperforms\ntraditional supervised methods. Given the high noise levels in EEG data, we\nargue that further improvements are possible with additional preprocessing.\nCurrent preprocessing methods often fail to efficiently manage the large data\nvolumes required for SSL, due to their lack of optimization, reliance on\nsubjective manual corrections, and validation processes or inflexible protocols\nthat limit SSL. We propose a Python-based EEG preprocessing pipeline optimized\nfor self-supervised learning, designed to efficiently process large-scale data.\nThis optimization not only stabilizes self-supervised training but also\nenhances performance on downstream tasks compared to training with raw data.\n", "link": "http://arxiv.org/abs/2408.08065v1", "date": "2024-08-15", "relevancy": 1.9676, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5045}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4942}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPEED%3A%20Scalable%20Preprocessing%20of%20EEG%20Data%20for%20Self-Supervised%20Learning&body=Title%3A%20SPEED%3A%20Scalable%20Preprocessing%20of%20EEG%20Data%20for%20Self-Supervised%20Learning%0AAuthor%3A%20Anders%20Gj%C3%B8lbye%20and%20Lina%20Skerath%20and%20William%20Lehn-Schi%C3%B8ler%20and%20Nicolas%20Langer%20and%20Lars%20Kai%20Hansen%0AAbstract%3A%20%20%20Electroencephalography%20%28EEG%29%20research%20typically%20focuses%20on%20tasks%20with%0Anarrowly%20defined%20objectives%2C%20but%20recent%20studies%20are%20expanding%20into%20the%20use%20of%0Aunlabeled%20data%20within%20larger%20models%2C%20aiming%20for%20a%20broader%20range%20of%0Aapplications.%20This%20addresses%20a%20critical%20challenge%20in%20EEG%20research.%20For%20example%2C%0AKostas%20et%20al.%20%282021%29%20show%20that%20self-supervised%20learning%20%28SSL%29%20outperforms%0Atraditional%20supervised%20methods.%20Given%20the%20high%20noise%20levels%20in%20EEG%20data%2C%20we%0Aargue%20that%20further%20improvements%20are%20possible%20with%20additional%20preprocessing.%0ACurrent%20preprocessing%20methods%20often%20fail%20to%20efficiently%20manage%20the%20large%20data%0Avolumes%20required%20for%20SSL%2C%20due%20to%20their%20lack%20of%20optimization%2C%20reliance%20on%0Asubjective%20manual%20corrections%2C%20and%20validation%20processes%20or%20inflexible%20protocols%0Athat%20limit%20SSL.%20We%20propose%20a%20Python-based%20EEG%20preprocessing%20pipeline%20optimized%0Afor%20self-supervised%20learning%2C%20designed%20to%20efficiently%20process%20large-scale%20data.%0AThis%20optimization%20not%20only%20stabilizes%20self-supervised%20training%20but%20also%0Aenhances%20performance%20on%20downstream%20tasks%20compared%20to%20training%20with%20raw%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPEED%253A%2520Scalable%2520Preprocessing%2520of%2520EEG%2520Data%2520for%2520Self-Supervised%2520Learning%26entry.906535625%3DAnders%2520Gj%25C3%25B8lbye%2520and%2520Lina%2520Skerath%2520and%2520William%2520Lehn-Schi%25C3%25B8ler%2520and%2520Nicolas%2520Langer%2520and%2520Lars%2520Kai%2520Hansen%26entry.1292438233%3D%2520%2520Electroencephalography%2520%2528EEG%2529%2520research%2520typically%2520focuses%2520on%2520tasks%2520with%250Anarrowly%2520defined%2520objectives%252C%2520but%2520recent%2520studies%2520are%2520expanding%2520into%2520the%2520use%2520of%250Aunlabeled%2520data%2520within%2520larger%2520models%252C%2520aiming%2520for%2520a%2520broader%2520range%2520of%250Aapplications.%2520This%2520addresses%2520a%2520critical%2520challenge%2520in%2520EEG%2520research.%2520For%2520example%252C%250AKostas%2520et%2520al.%2520%25282021%2529%2520show%2520that%2520self-supervised%2520learning%2520%2528SSL%2529%2520outperforms%250Atraditional%2520supervised%2520methods.%2520Given%2520the%2520high%2520noise%2520levels%2520in%2520EEG%2520data%252C%2520we%250Aargue%2520that%2520further%2520improvements%2520are%2520possible%2520with%2520additional%2520preprocessing.%250ACurrent%2520preprocessing%2520methods%2520often%2520fail%2520to%2520efficiently%2520manage%2520the%2520large%2520data%250Avolumes%2520required%2520for%2520SSL%252C%2520due%2520to%2520their%2520lack%2520of%2520optimization%252C%2520reliance%2520on%250Asubjective%2520manual%2520corrections%252C%2520and%2520validation%2520processes%2520or%2520inflexible%2520protocols%250Athat%2520limit%2520SSL.%2520We%2520propose%2520a%2520Python-based%2520EEG%2520preprocessing%2520pipeline%2520optimized%250Afor%2520self-supervised%2520learning%252C%2520designed%2520to%2520efficiently%2520process%2520large-scale%2520data.%250AThis%2520optimization%2520not%2520only%2520stabilizes%2520self-supervised%2520training%2520but%2520also%250Aenhances%2520performance%2520on%2520downstream%2520tasks%2520compared%2520to%2520training%2520with%2520raw%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPEED%3A%20Scalable%20Preprocessing%20of%20EEG%20Data%20for%20Self-Supervised%20Learning&entry.906535625=Anders%20Gj%C3%B8lbye%20and%20Lina%20Skerath%20and%20William%20Lehn-Schi%C3%B8ler%20and%20Nicolas%20Langer%20and%20Lars%20Kai%20Hansen&entry.1292438233=%20%20Electroencephalography%20%28EEG%29%20research%20typically%20focuses%20on%20tasks%20with%0Anarrowly%20defined%20objectives%2C%20but%20recent%20studies%20are%20expanding%20into%20the%20use%20of%0Aunlabeled%20data%20within%20larger%20models%2C%20aiming%20for%20a%20broader%20range%20of%0Aapplications.%20This%20addresses%20a%20critical%20challenge%20in%20EEG%20research.%20For%20example%2C%0AKostas%20et%20al.%20%282021%29%20show%20that%20self-supervised%20learning%20%28SSL%29%20outperforms%0Atraditional%20supervised%20methods.%20Given%20the%20high%20noise%20levels%20in%20EEG%20data%2C%20we%0Aargue%20that%20further%20improvements%20are%20possible%20with%20additional%20preprocessing.%0ACurrent%20preprocessing%20methods%20often%20fail%20to%20efficiently%20manage%20the%20large%20data%0Avolumes%20required%20for%20SSL%2C%20due%20to%20their%20lack%20of%20optimization%2C%20reliance%20on%0Asubjective%20manual%20corrections%2C%20and%20validation%20processes%20or%20inflexible%20protocols%0Athat%20limit%20SSL.%20We%20propose%20a%20Python-based%20EEG%20preprocessing%20pipeline%20optimized%0Afor%20self-supervised%20learning%2C%20designed%20to%20efficiently%20process%20large-scale%20data.%0AThis%20optimization%20not%20only%20stabilizes%20self-supervised%20training%20but%20also%0Aenhances%20performance%20on%20downstream%20tasks%20compared%20to%20training%20with%20raw%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08065v1&entry.124074799=Read"},
{"title": "A Multi-task Adversarial Attack Against Face Authentication", "author": "Hanrui Wang and Shuo Wang and Cunjian Chen and Massimo Tistarelli and Zhe Jin", "abstract": "  Deep-learning-based identity management systems, such as face authentication\nsystems, are vulnerable to adversarial attacks. However, existing attacks are\ntypically designed for single-task purposes, which means they are tailored to\nexploit vulnerabilities unique to the individual target rather than being\nadaptable for multiple users or systems. This limitation makes them unsuitable\nfor certain attack scenarios, such as morphing, universal, transferable, and\ncounter attacks. In this paper, we propose a multi-task adversarial attack\nalgorithm called MTADV that are adaptable for multiple users or systems. By\ninterpreting these scenarios as multi-task attacks, MTADV is applicable to both\nsingle- and multi-task attacks, and feasible in the white- and gray-box\nsettings. Furthermore, MTADV is effective against various face datasets,\nincluding LFW, CelebA, and CelebA-HQ, and can work with different deep learning\nmodels, such as FaceNet, InsightFace, and CurricularFace. Importantly, MTADV\nretains its feasibility as a single-task attack targeting a single user/system.\nTo the best of our knowledge, MTADV is the first adversarial attack method that\ncan target all of the aforementioned scenarios in one algorithm.\n", "link": "http://arxiv.org/abs/2408.08205v1", "date": "2024-08-15", "relevancy": 1.9591, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5133}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4787}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-task%20Adversarial%20Attack%20Against%20Face%20Authentication&body=Title%3A%20A%20Multi-task%20Adversarial%20Attack%20Against%20Face%20Authentication%0AAuthor%3A%20Hanrui%20Wang%20and%20Shuo%20Wang%20and%20Cunjian%20Chen%20and%20Massimo%20Tistarelli%20and%20Zhe%20Jin%0AAbstract%3A%20%20%20Deep-learning-based%20identity%20management%20systems%2C%20such%20as%20face%20authentication%0Asystems%2C%20are%20vulnerable%20to%20adversarial%20attacks.%20However%2C%20existing%20attacks%20are%0Atypically%20designed%20for%20single-task%20purposes%2C%20which%20means%20they%20are%20tailored%20to%0Aexploit%20vulnerabilities%20unique%20to%20the%20individual%20target%20rather%20than%20being%0Aadaptable%20for%20multiple%20users%20or%20systems.%20This%20limitation%20makes%20them%20unsuitable%0Afor%20certain%20attack%20scenarios%2C%20such%20as%20morphing%2C%20universal%2C%20transferable%2C%20and%0Acounter%20attacks.%20In%20this%20paper%2C%20we%20propose%20a%20multi-task%20adversarial%20attack%0Aalgorithm%20called%20MTADV%20that%20are%20adaptable%20for%20multiple%20users%20or%20systems.%20By%0Ainterpreting%20these%20scenarios%20as%20multi-task%20attacks%2C%20MTADV%20is%20applicable%20to%20both%0Asingle-%20and%20multi-task%20attacks%2C%20and%20feasible%20in%20the%20white-%20and%20gray-box%0Asettings.%20Furthermore%2C%20MTADV%20is%20effective%20against%20various%20face%20datasets%2C%0Aincluding%20LFW%2C%20CelebA%2C%20and%20CelebA-HQ%2C%20and%20can%20work%20with%20different%20deep%20learning%0Amodels%2C%20such%20as%20FaceNet%2C%20InsightFace%2C%20and%20CurricularFace.%20Importantly%2C%20MTADV%0Aretains%20its%20feasibility%20as%20a%20single-task%20attack%20targeting%20a%20single%20user/system.%0ATo%20the%20best%20of%20our%20knowledge%2C%20MTADV%20is%20the%20first%20adversarial%20attack%20method%20that%0Acan%20target%20all%20of%20the%20aforementioned%20scenarios%20in%20one%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-task%2520Adversarial%2520Attack%2520Against%2520Face%2520Authentication%26entry.906535625%3DHanrui%2520Wang%2520and%2520Shuo%2520Wang%2520and%2520Cunjian%2520Chen%2520and%2520Massimo%2520Tistarelli%2520and%2520Zhe%2520Jin%26entry.1292438233%3D%2520%2520Deep-learning-based%2520identity%2520management%2520systems%252C%2520such%2520as%2520face%2520authentication%250Asystems%252C%2520are%2520vulnerable%2520to%2520adversarial%2520attacks.%2520However%252C%2520existing%2520attacks%2520are%250Atypically%2520designed%2520for%2520single-task%2520purposes%252C%2520which%2520means%2520they%2520are%2520tailored%2520to%250Aexploit%2520vulnerabilities%2520unique%2520to%2520the%2520individual%2520target%2520rather%2520than%2520being%250Aadaptable%2520for%2520multiple%2520users%2520or%2520systems.%2520This%2520limitation%2520makes%2520them%2520unsuitable%250Afor%2520certain%2520attack%2520scenarios%252C%2520such%2520as%2520morphing%252C%2520universal%252C%2520transferable%252C%2520and%250Acounter%2520attacks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520multi-task%2520adversarial%2520attack%250Aalgorithm%2520called%2520MTADV%2520that%2520are%2520adaptable%2520for%2520multiple%2520users%2520or%2520systems.%2520By%250Ainterpreting%2520these%2520scenarios%2520as%2520multi-task%2520attacks%252C%2520MTADV%2520is%2520applicable%2520to%2520both%250Asingle-%2520and%2520multi-task%2520attacks%252C%2520and%2520feasible%2520in%2520the%2520white-%2520and%2520gray-box%250Asettings.%2520Furthermore%252C%2520MTADV%2520is%2520effective%2520against%2520various%2520face%2520datasets%252C%250Aincluding%2520LFW%252C%2520CelebA%252C%2520and%2520CelebA-HQ%252C%2520and%2520can%2520work%2520with%2520different%2520deep%2520learning%250Amodels%252C%2520such%2520as%2520FaceNet%252C%2520InsightFace%252C%2520and%2520CurricularFace.%2520Importantly%252C%2520MTADV%250Aretains%2520its%2520feasibility%2520as%2520a%2520single-task%2520attack%2520targeting%2520a%2520single%2520user/system.%250ATo%2520the%2520best%2520of%2520our%2520knowledge%252C%2520MTADV%2520is%2520the%2520first%2520adversarial%2520attack%2520method%2520that%250Acan%2520target%2520all%2520of%2520the%2520aforementioned%2520scenarios%2520in%2520one%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-task%20Adversarial%20Attack%20Against%20Face%20Authentication&entry.906535625=Hanrui%20Wang%20and%20Shuo%20Wang%20and%20Cunjian%20Chen%20and%20Massimo%20Tistarelli%20and%20Zhe%20Jin&entry.1292438233=%20%20Deep-learning-based%20identity%20management%20systems%2C%20such%20as%20face%20authentication%0Asystems%2C%20are%20vulnerable%20to%20adversarial%20attacks.%20However%2C%20existing%20attacks%20are%0Atypically%20designed%20for%20single-task%20purposes%2C%20which%20means%20they%20are%20tailored%20to%0Aexploit%20vulnerabilities%20unique%20to%20the%20individual%20target%20rather%20than%20being%0Aadaptable%20for%20multiple%20users%20or%20systems.%20This%20limitation%20makes%20them%20unsuitable%0Afor%20certain%20attack%20scenarios%2C%20such%20as%20morphing%2C%20universal%2C%20transferable%2C%20and%0Acounter%20attacks.%20In%20this%20paper%2C%20we%20propose%20a%20multi-task%20adversarial%20attack%0Aalgorithm%20called%20MTADV%20that%20are%20adaptable%20for%20multiple%20users%20or%20systems.%20By%0Ainterpreting%20these%20scenarios%20as%20multi-task%20attacks%2C%20MTADV%20is%20applicable%20to%20both%0Asingle-%20and%20multi-task%20attacks%2C%20and%20feasible%20in%20the%20white-%20and%20gray-box%0Asettings.%20Furthermore%2C%20MTADV%20is%20effective%20against%20various%20face%20datasets%2C%0Aincluding%20LFW%2C%20CelebA%2C%20and%20CelebA-HQ%2C%20and%20can%20work%20with%20different%20deep%20learning%0Amodels%2C%20such%20as%20FaceNet%2C%20InsightFace%2C%20and%20CurricularFace.%20Importantly%2C%20MTADV%0Aretains%20its%20feasibility%20as%20a%20single-task%20attack%20targeting%20a%20single%20user/system.%0ATo%20the%20best%20of%20our%20knowledge%2C%20MTADV%20is%20the%20first%20adversarial%20attack%20method%20that%0Acan%20target%20all%20of%20the%20aforementioned%20scenarios%20in%20one%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08205v1&entry.124074799=Read"},
{"title": "Can Large Language Models Understand Symbolic Graphics Programs?", "author": "Zeju Qiu and Weiyang Liu and Haiwen Feng and Zhen Liu and Tim Z. Xiao and Katherine M. Collins and Joshua B. Tenenbaum and Adrian Weller and Michael J. Black and Bernhard Sch\u00f6lkopf", "abstract": "  Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.\n", "link": "http://arxiv.org/abs/2408.08313v1", "date": "2024-08-15", "relevancy": 1.9484, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4941}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4863}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Understand%20Symbolic%20Graphics%20Programs%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Understand%20Symbolic%20Graphics%20Programs%3F%0AAuthor%3A%20Zeju%20Qiu%20and%20Weiyang%20Liu%20and%20Haiwen%20Feng%20and%20Zhen%20Liu%20and%20Tim%20Z.%20Xiao%20and%20Katherine%20M.%20Collins%20and%20Joshua%20B.%20Tenenbaum%20and%20Adrian%20Weller%20and%20Michael%20J.%20Black%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20Assessing%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20is%20often%0Achallenging%2C%20in%20part%2C%20because%20it%20is%20hard%20to%20find%20tasks%20to%20which%20they%20have%20not%0Abeen%20exposed%20during%20training.%20We%20take%20one%20step%20to%20address%20this%20challenge%20by%0Aturning%20to%20a%20new%20task%3A%20focusing%20on%20symbolic%20graphics%20programs%2C%20which%20are%20a%0Apopular%20representation%20for%20graphics%20content%20that%20procedurally%20generates%20visual%0Adata.%20LLMs%20have%20shown%20exciting%20promise%20towards%20program%20synthesis%2C%20but%20do%20they%0Aunderstand%20symbolic%20graphics%20programs%3F%20Unlike%20conventional%20programs%2C%20symbolic%0Agraphics%20programs%20can%20be%20translated%20to%20graphics%20content.%20Here%2C%20we%20characterize%0Aan%20LLM%27s%20understanding%20of%20symbolic%20programs%20in%20terms%20of%20their%20ability%20to%20answer%0Aquestions%20related%20to%20the%20graphics%20content.%20This%20task%20is%20challenging%20as%20the%0Aquestions%20are%20difficult%20to%20answer%20from%20the%20symbolic%20programs%20alone%20--%20yet%2C%20they%0Awould%20be%20easy%20to%20answer%20from%20the%20corresponding%20graphics%20content%20as%20we%20verify%0Athrough%20a%20human%20experiment.%20To%20understand%20symbolic%20programs%2C%20LLMs%20may%20need%20to%0Apossess%20the%20ability%20to%20imagine%20how%20the%20corresponding%20graphics%20content%20would%0Alook%20without%20directly%20accessing%20the%20rendered%20visual%20content.%20We%20use%20this%20task%0Ato%20evaluate%20LLMs%20by%20creating%20a%20large%20benchmark%20for%20the%20semantic%20understanding%0Aof%20symbolic%20graphics%20programs.%20This%20benchmark%20is%20built%20via%20program-graphics%0Acorrespondence%2C%20hence%20requiring%20minimal%20human%20efforts.%20We%20evaluate%20current%20LLMs%0Aon%20our%20benchmark%20to%20elucidate%20a%20preliminary%20assessment%20of%20their%20ability%20to%0Areason%20about%20visual%20scenes%20from%20programs.%20We%20find%20that%20this%20task%20distinguishes%0Aexisting%20LLMs%20and%20models%20considered%20good%20at%20reasoning%20perform%20better.%20Lastly%2C%0Awe%20introduce%20Symbolic%20Instruction%20Tuning%20%28SIT%29%20to%20improve%20this%20ability.%0ASpecifically%2C%20we%20query%20GPT4-o%20with%20questions%20and%20images%20generated%20by%20symbolic%0Aprograms.%20Such%20data%20are%20then%20used%20to%20finetune%20an%20LLM.%20We%20also%20find%20that%20SIT%0Adata%20can%20improve%20the%20general%20instruction%20following%20ability%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Understand%2520Symbolic%2520Graphics%2520Programs%253F%26entry.906535625%3DZeju%2520Qiu%2520and%2520Weiyang%2520Liu%2520and%2520Haiwen%2520Feng%2520and%2520Zhen%2520Liu%2520and%2520Tim%2520Z.%2520Xiao%2520and%2520Katherine%2520M.%2520Collins%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Adrian%2520Weller%2520and%2520Michael%2520J.%2520Black%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3D%2520%2520Assessing%2520the%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520often%250Achallenging%252C%2520in%2520part%252C%2520because%2520it%2520is%2520hard%2520to%2520find%2520tasks%2520to%2520which%2520they%2520have%2520not%250Abeen%2520exposed%2520during%2520training.%2520We%2520take%2520one%2520step%2520to%2520address%2520this%2520challenge%2520by%250Aturning%2520to%2520a%2520new%2520task%253A%2520focusing%2520on%2520symbolic%2520graphics%2520programs%252C%2520which%2520are%2520a%250Apopular%2520representation%2520for%2520graphics%2520content%2520that%2520procedurally%2520generates%2520visual%250Adata.%2520LLMs%2520have%2520shown%2520exciting%2520promise%2520towards%2520program%2520synthesis%252C%2520but%2520do%2520they%250Aunderstand%2520symbolic%2520graphics%2520programs%253F%2520Unlike%2520conventional%2520programs%252C%2520symbolic%250Agraphics%2520programs%2520can%2520be%2520translated%2520to%2520graphics%2520content.%2520Here%252C%2520we%2520characterize%250Aan%2520LLM%2527s%2520understanding%2520of%2520symbolic%2520programs%2520in%2520terms%2520of%2520their%2520ability%2520to%2520answer%250Aquestions%2520related%2520to%2520the%2520graphics%2520content.%2520This%2520task%2520is%2520challenging%2520as%2520the%250Aquestions%2520are%2520difficult%2520to%2520answer%2520from%2520the%2520symbolic%2520programs%2520alone%2520--%2520yet%252C%2520they%250Awould%2520be%2520easy%2520to%2520answer%2520from%2520the%2520corresponding%2520graphics%2520content%2520as%2520we%2520verify%250Athrough%2520a%2520human%2520experiment.%2520To%2520understand%2520symbolic%2520programs%252C%2520LLMs%2520may%2520need%2520to%250Apossess%2520the%2520ability%2520to%2520imagine%2520how%2520the%2520corresponding%2520graphics%2520content%2520would%250Alook%2520without%2520directly%2520accessing%2520the%2520rendered%2520visual%2520content.%2520We%2520use%2520this%2520task%250Ato%2520evaluate%2520LLMs%2520by%2520creating%2520a%2520large%2520benchmark%2520for%2520the%2520semantic%2520understanding%250Aof%2520symbolic%2520graphics%2520programs.%2520This%2520benchmark%2520is%2520built%2520via%2520program-graphics%250Acorrespondence%252C%2520hence%2520requiring%2520minimal%2520human%2520efforts.%2520We%2520evaluate%2520current%2520LLMs%250Aon%2520our%2520benchmark%2520to%2520elucidate%2520a%2520preliminary%2520assessment%2520of%2520their%2520ability%2520to%250Areason%2520about%2520visual%2520scenes%2520from%2520programs.%2520We%2520find%2520that%2520this%2520task%2520distinguishes%250Aexisting%2520LLMs%2520and%2520models%2520considered%2520good%2520at%2520reasoning%2520perform%2520better.%2520Lastly%252C%250Awe%2520introduce%2520Symbolic%2520Instruction%2520Tuning%2520%2528SIT%2529%2520to%2520improve%2520this%2520ability.%250ASpecifically%252C%2520we%2520query%2520GPT4-o%2520with%2520questions%2520and%2520images%2520generated%2520by%2520symbolic%250Aprograms.%2520Such%2520data%2520are%2520then%2520used%2520to%2520finetune%2520an%2520LLM.%2520We%2520also%2520find%2520that%2520SIT%250Adata%2520can%2520improve%2520the%2520general%2520instruction%2520following%2520ability%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Understand%20Symbolic%20Graphics%20Programs%3F&entry.906535625=Zeju%20Qiu%20and%20Weiyang%20Liu%20and%20Haiwen%20Feng%20and%20Zhen%20Liu%20and%20Tim%20Z.%20Xiao%20and%20Katherine%20M.%20Collins%20and%20Joshua%20B.%20Tenenbaum%20and%20Adrian%20Weller%20and%20Michael%20J.%20Black%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20Assessing%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20is%20often%0Achallenging%2C%20in%20part%2C%20because%20it%20is%20hard%20to%20find%20tasks%20to%20which%20they%20have%20not%0Abeen%20exposed%20during%20training.%20We%20take%20one%20step%20to%20address%20this%20challenge%20by%0Aturning%20to%20a%20new%20task%3A%20focusing%20on%20symbolic%20graphics%20programs%2C%20which%20are%20a%0Apopular%20representation%20for%20graphics%20content%20that%20procedurally%20generates%20visual%0Adata.%20LLMs%20have%20shown%20exciting%20promise%20towards%20program%20synthesis%2C%20but%20do%20they%0Aunderstand%20symbolic%20graphics%20programs%3F%20Unlike%20conventional%20programs%2C%20symbolic%0Agraphics%20programs%20can%20be%20translated%20to%20graphics%20content.%20Here%2C%20we%20characterize%0Aan%20LLM%27s%20understanding%20of%20symbolic%20programs%20in%20terms%20of%20their%20ability%20to%20answer%0Aquestions%20related%20to%20the%20graphics%20content.%20This%20task%20is%20challenging%20as%20the%0Aquestions%20are%20difficult%20to%20answer%20from%20the%20symbolic%20programs%20alone%20--%20yet%2C%20they%0Awould%20be%20easy%20to%20answer%20from%20the%20corresponding%20graphics%20content%20as%20we%20verify%0Athrough%20a%20human%20experiment.%20To%20understand%20symbolic%20programs%2C%20LLMs%20may%20need%20to%0Apossess%20the%20ability%20to%20imagine%20how%20the%20corresponding%20graphics%20content%20would%0Alook%20without%20directly%20accessing%20the%20rendered%20visual%20content.%20We%20use%20this%20task%0Ato%20evaluate%20LLMs%20by%20creating%20a%20large%20benchmark%20for%20the%20semantic%20understanding%0Aof%20symbolic%20graphics%20programs.%20This%20benchmark%20is%20built%20via%20program-graphics%0Acorrespondence%2C%20hence%20requiring%20minimal%20human%20efforts.%20We%20evaluate%20current%20LLMs%0Aon%20our%20benchmark%20to%20elucidate%20a%20preliminary%20assessment%20of%20their%20ability%20to%0Areason%20about%20visual%20scenes%20from%20programs.%20We%20find%20that%20this%20task%20distinguishes%0Aexisting%20LLMs%20and%20models%20considered%20good%20at%20reasoning%20perform%20better.%20Lastly%2C%0Awe%20introduce%20Symbolic%20Instruction%20Tuning%20%28SIT%29%20to%20improve%20this%20ability.%0ASpecifically%2C%20we%20query%20GPT4-o%20with%20questions%20and%20images%20generated%20by%20symbolic%0Aprograms.%20Such%20data%20are%20then%20used%20to%20finetune%20an%20LLM.%20We%20also%20find%20that%20SIT%0Adata%20can%20improve%20the%20general%20instruction%20following%20ability%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08313v1&entry.124074799=Read"},
{"title": "The Llama 3 Herd of Models", "author": "Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Olivier Duchenne and Onur \u00c7elebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm\u00e1n and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Govind Thattai and Grant Herman and Grigory Sizov and  Guangyi and  Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Maria Tsimpoukelli and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and V\u00edtor Albiero and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and  Yu and  Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao", "abstract": "  Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.\n", "link": "http://arxiv.org/abs/2407.21783v2", "date": "2024-08-15", "relevancy": 1.9476, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.469}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Llama%203%20Herd%20of%20Models&body=Title%3A%20The%20Llama%203%20Herd%20of%20Models%0AAuthor%3A%20Abhimanyu%20Dubey%20and%20Abhinav%20Jauhri%20and%20Abhinav%20Pandey%20and%20Abhishek%20Kadian%20and%20Ahmad%20Al-Dahle%20and%20Aiesha%20Letman%20and%20Akhil%20Mathur%20and%20Alan%20Schelten%20and%20Amy%20Yang%20and%20Angela%20Fan%20and%20Anirudh%20Goyal%20and%20Anthony%20Hartshorn%20and%20Aobo%20Yang%20and%20Archi%20Mitra%20and%20Archie%20Sravankumar%20and%20Artem%20Korenev%20and%20Arthur%20Hinsvark%20and%20Arun%20Rao%20and%20Aston%20Zhang%20and%20Aurelien%20Rodriguez%20and%20Austen%20Gregerson%20and%20Ava%20Spataru%20and%20Baptiste%20Roziere%20and%20Bethany%20Biron%20and%20Binh%20Tang%20and%20Bobbie%20Chern%20and%20Charlotte%20Caucheteux%20and%20Chaya%20Nayak%20and%20Chloe%20Bi%20and%20Chris%20Marra%20and%20Chris%20McConnell%20and%20Christian%20Keller%20and%20Christophe%20Touret%20and%20Chunyang%20Wu%20and%20Corinne%20Wong%20and%20Cristian%20Canton%20Ferrer%20and%20Cyrus%20Nikolaidis%20and%20Damien%20Allonsius%20and%20Daniel%20Song%20and%20Danielle%20Pintz%20and%20Danny%20Livshits%20and%20David%20Esiobu%20and%20Dhruv%20Choudhary%20and%20Dhruv%20Mahajan%20and%20Diego%20Garcia-Olano%20and%20Diego%20Perino%20and%20Dieuwke%20Hupkes%20and%20Egor%20Lakomkin%20and%20Ehab%20AlBadawy%20and%20Elina%20Lobanova%20and%20Emily%20Dinan%20and%20Eric%20Michael%20Smith%20and%20Filip%20Radenovic%20and%20Frank%20Zhang%20and%20Gabriel%20Synnaeve%20and%20Gabrielle%20Lee%20and%20Georgia%20Lewis%20Anderson%20and%20Graeme%20Nail%20and%20Gregoire%20Mialon%20and%20Guan%20Pang%20and%20Guillem%20Cucurell%20and%20Hailey%20Nguyen%20and%20Hannah%20Korevaar%20and%20Hu%20Xu%20and%20Hugo%20Touvron%20and%20Iliyan%20Zarov%20and%20Imanol%20Arrieta%20Ibarra%20and%20Isabel%20Kloumann%20and%20Ishan%20Misra%20and%20Ivan%20Evtimov%20and%20Jade%20Copet%20and%20Jaewon%20Lee%20and%20Jan%20Geffert%20and%20Jana%20Vranes%20and%20Jason%20Park%20and%20Jay%20Mahadeokar%20and%20Jeet%20Shah%20and%20Jelmer%20van%20der%20Linde%20and%20Jennifer%20Billock%20and%20Jenny%20Hong%20and%20Jenya%20Lee%20and%20Jeremy%20Fu%20and%20Jianfeng%20Chi%20and%20Jianyu%20Huang%20and%20Jiawen%20Liu%20and%20Jie%20Wang%20and%20Jiecao%20Yu%20and%20Joanna%20Bitton%20and%20Joe%20Spisak%20and%20Jongsoo%20Park%20and%20Joseph%20Rocca%20and%20Joshua%20Johnstun%20and%20Joshua%20Saxe%20and%20Junteng%20Jia%20and%20Kalyan%20Vasuden%20Alwala%20and%20Kartikeya%20Upasani%20and%20Kate%20Plawiak%20and%20Ke%20Li%20and%20Kenneth%20Heafield%20and%20Kevin%20Stone%20and%20Khalid%20El-Arini%20and%20Krithika%20Iyer%20and%20Kshitiz%20Malik%20and%20Kuenley%20Chiu%20and%20Kunal%20Bhalla%20and%20Lauren%20Rantala-Yeary%20and%20Laurens%20van%20der%20Maaten%20and%20Lawrence%20Chen%20and%20Liang%20Tan%20and%20Liz%20Jenkins%20and%20Louis%20Martin%20and%20Lovish%20Madaan%20and%20Lubo%20Malo%20and%20Lukas%20Blecher%20and%20Lukas%20Landzaat%20and%20Luke%20de%20Oliveira%20and%20Madeline%20Muzzi%20and%20Mahesh%20Pasupuleti%20and%20Mannat%20Singh%20and%20Manohar%20Paluri%20and%20Marcin%20Kardas%20and%20Mathew%20Oldham%20and%20Mathieu%20Rita%20and%20Maya%20Pavlova%20and%20Melanie%20Kambadur%20and%20Mike%20Lewis%20and%20Min%20Si%20and%20Mitesh%20Kumar%20Singh%20and%20Mona%20Hassan%20and%20Naman%20Goyal%20and%20Narjes%20Torabi%20and%20Nikolay%20Bashlykov%20and%20Nikolay%20Bogoychev%20and%20Niladri%20Chatterji%20and%20Olivier%20Duchenne%20and%20Onur%20%C3%87elebi%20and%20Patrick%20Alrassy%20and%20Pengchuan%20Zhang%20and%20Pengwei%20Li%20and%20Petar%20Vasic%20and%20Peter%20Weng%20and%20Prajjwal%20Bhargava%20and%20Pratik%20Dubal%20and%20Praveen%20Krishnan%20and%20Punit%20Singh%20Koura%20and%20Puxin%20Xu%20and%20Qing%20He%20and%20Qingxiao%20Dong%20and%20Ragavan%20Srinivasan%20and%20Raj%20Ganapathy%20and%20Ramon%20Calderer%20and%20Ricardo%20Silveira%20Cabral%20and%20Robert%20Stojnic%20and%20Roberta%20Raileanu%20and%20Rohit%20Girdhar%20and%20Rohit%20Patel%20and%20Romain%20Sauvestre%20and%20Ronnie%20Polidoro%20and%20Roshan%20Sumbaly%20and%20Ross%20Taylor%20and%20Ruan%20Silva%20and%20Rui%20Hou%20and%20Rui%20Wang%20and%20Saghar%20Hosseini%20and%20Sahana%20Chennabasappa%20and%20Sanjay%20Singh%20and%20Sean%20Bell%20and%20Seohyun%20Sonia%20Kim%20and%20Sergey%20Edunov%20and%20Shaoliang%20Nie%20and%20Sharan%20Narang%20and%20Sharath%20Raparthy%20and%20Sheng%20Shen%20and%20Shengye%20Wan%20and%20Shruti%20Bhosale%20and%20Shun%20Zhang%20and%20Simon%20Vandenhende%20and%20Soumya%20Batra%20and%20Spencer%20Whitman%20and%20Sten%20Sootla%20and%20Stephane%20Collot%20and%20Suchin%20Gururangan%20and%20Sydney%20Borodinsky%20and%20Tamar%20Herman%20and%20Tara%20Fowler%20and%20Tarek%20Sheasha%20and%20Thomas%20Georgiou%20and%20Thomas%20Scialom%20and%20Tobias%20Speckbacher%20and%20Todor%20Mihaylov%20and%20Tong%20Xiao%20and%20Ujjwal%20Karn%20and%20Vedanuj%20Goswami%20and%20Vibhor%20Gupta%20and%20Vignesh%20Ramanathan%20and%20Viktor%20Kerkez%20and%20Vincent%20Gonguet%20and%20Virginie%20Do%20and%20Vish%20Vogeti%20and%20Vladan%20Petrovic%20and%20Weiwei%20Chu%20and%20Wenhan%20Xiong%20and%20Wenyin%20Fu%20and%20Whitney%20Meers%20and%20Xavier%20Martinet%20and%20Xiaodong%20Wang%20and%20Xiaoqing%20Ellen%20Tan%20and%20Xinfeng%20Xie%20and%20Xuchao%20Jia%20and%20Xuewei%20Wang%20and%20Yaelle%20Goldschlag%20and%20Yashesh%20Gaur%20and%20Yasmine%20Babaei%20and%20Yi%20Wen%20and%20Yiwen%20Song%20and%20Yuchen%20Zhang%20and%20Yue%20Li%20and%20Yuning%20Mao%20and%20Zacharie%20Delpierre%20Coudert%20and%20Zheng%20Yan%20and%20Zhengxing%20Chen%20and%20Zoe%20Papakipos%20and%20Aaditya%20Singh%20and%20Aaron%20Grattafiori%20and%20Abha%20Jain%20and%20Adam%20Kelsey%20and%20Adam%20Shajnfeld%20and%20Adithya%20Gangidi%20and%20Adolfo%20Victoria%20and%20Ahuva%20Goldstand%20and%20Ajay%20Menon%20and%20Ajay%20Sharma%20and%20Alex%20Boesenberg%20and%20Alex%20Vaughan%20and%20Alexei%20Baevski%20and%20Allie%20Feinstein%20and%20Amanda%20Kallet%20and%20Amit%20Sangani%20and%20Anam%20Yunus%20and%20Andrei%20Lupu%20and%20Andres%20Alvarado%20and%20Andrew%20Caples%20and%20Andrew%20Gu%20and%20Andrew%20Ho%20and%20Andrew%20Poulton%20and%20Andrew%20Ryan%20and%20Ankit%20Ramchandani%20and%20Annie%20Franco%20and%20Aparajita%20Saraf%20and%20Arkabandhu%20Chowdhury%20and%20Ashley%20Gabriel%20and%20Ashwin%20Bharambe%20and%20Assaf%20Eisenman%20and%20Azadeh%20Yazdan%20and%20Beau%20James%20and%20Ben%20Maurer%20and%20Benjamin%20Leonhardi%20and%20Bernie%20Huang%20and%20Beth%20Loyd%20and%20Beto%20De%20Paola%20and%20Bhargavi%20Paranjape%20and%20Bing%20Liu%20and%20Bo%20Wu%20and%20Boyu%20Ni%20and%20Braden%20Hancock%20and%20Bram%20Wasti%20and%20Brandon%20Spence%20and%20Brani%20Stojkovic%20and%20Brian%20Gamido%20and%20Britt%20Montalvo%20and%20Carl%20Parker%20and%20Carly%20Burton%20and%20Catalina%20Mejia%20and%20Changhan%20Wang%20and%20Changkyu%20Kim%20and%20Chao%20Zhou%20and%20Chester%20Hu%20and%20Ching-Hsiang%20Chu%20and%20Chris%20Cai%20and%20Chris%20Tindal%20and%20Christoph%20Feichtenhofer%20and%20Damon%20Civin%20and%20Dana%20Beaty%20and%20Daniel%20Kreymer%20and%20Daniel%20Li%20and%20Danny%20Wyatt%20and%20David%20Adkins%20and%20David%20Xu%20and%20Davide%20Testuggine%20and%20Delia%20David%20and%20Devi%20Parikh%20and%20Diana%20Liskovich%20and%20Didem%20Foss%20and%20Dingkang%20Wang%20and%20Duc%20Le%20and%20Dustin%20Holland%20and%20Edward%20Dowling%20and%20Eissa%20Jamil%20and%20Elaine%20Montgomery%20and%20Eleonora%20Presani%20and%20Emily%20Hahn%20and%20Emily%20Wood%20and%20Erik%20Brinkman%20and%20Esteban%20Arcaute%20and%20Evan%20Dunbar%20and%20Evan%20Smothers%20and%20Fei%20Sun%20and%20Felix%20Kreuk%20and%20Feng%20Tian%20and%20Firat%20Ozgenel%20and%20Francesco%20Caggioni%20and%20Francisco%20Guzm%C3%A1n%20and%20Frank%20Kanayet%20and%20Frank%20Seide%20and%20Gabriela%20Medina%20Florez%20and%20Gabriella%20Schwarz%20and%20Gada%20Badeer%20and%20Georgia%20Swee%20and%20Gil%20Halpern%20and%20Govind%20Thattai%20and%20Grant%20Herman%20and%20Grigory%20Sizov%20and%20%20Guangyi%20and%20%20Zhang%20and%20Guna%20Lakshminarayanan%20and%20Hamid%20Shojanazeri%20and%20Han%20Zou%20and%20Hannah%20Wang%20and%20Hanwen%20Zha%20and%20Haroun%20Habeeb%20and%20Harrison%20Rudolph%20and%20Helen%20Suk%20and%20Henry%20Aspegren%20and%20Hunter%20Goldman%20and%20Ibrahim%20Damlaj%20and%20Igor%20Molybog%20and%20Igor%20Tufanov%20and%20Irina-Elena%20Veliche%20and%20Itai%20Gat%20and%20Jake%20Weissman%20and%20James%20Geboski%20and%20James%20Kohli%20and%20Japhet%20Asher%20and%20Jean-Baptiste%20Gaya%20and%20Jeff%20Marcus%20and%20Jeff%20Tang%20and%20Jennifer%20Chan%20and%20Jenny%20Zhen%20and%20Jeremy%20Reizenstein%20and%20Jeremy%20Teboul%20and%20Jessica%20Zhong%20and%20Jian%20Jin%20and%20Jingyi%20Yang%20and%20Joe%20Cummings%20and%20Jon%20Carvill%20and%20Jon%20Shepard%20and%20Jonathan%20McPhie%20and%20Jonathan%20Torres%20and%20Josh%20Ginsburg%20and%20Junjie%20Wang%20and%20Kai%20Wu%20and%20Kam%20Hou%20U%20and%20Karan%20Saxena%20and%20Karthik%20Prasad%20and%20Kartikay%20Khandelwal%20and%20Katayoun%20Zand%20and%20Kathy%20Matosich%20and%20Kaushik%20Veeraraghavan%20and%20Kelly%20Michelena%20and%20Keqian%20Li%20and%20Kun%20Huang%20and%20Kunal%20Chawla%20and%20Kushal%20Lakhotia%20and%20Kyle%20Huang%20and%20Lailin%20Chen%20and%20Lakshya%20Garg%20and%20Lavender%20A%20and%20Leandro%20Silva%20and%20Lee%20Bell%20and%20Lei%20Zhang%20and%20Liangpeng%20Guo%20and%20Licheng%20Yu%20and%20Liron%20Moshkovich%20and%20Luca%20Wehrstedt%20and%20Madian%20Khabsa%20and%20Manav%20Avalani%20and%20Manish%20Bhatt%20and%20Maria%20Tsimpoukelli%20and%20Martynas%20Mankus%20and%20Matan%20Hasson%20and%20Matthew%20Lennie%20and%20Matthias%20Reso%20and%20Maxim%20Groshev%20and%20Maxim%20Naumov%20and%20Maya%20Lathi%20and%20Meghan%20Keneally%20and%20Michael%20L.%20Seltzer%20and%20Michal%20Valko%20and%20Michelle%20Restrepo%20and%20Mihir%20Patel%20and%20Mik%20Vyatskov%20and%20Mikayel%20Samvelyan%20and%20Mike%20Clark%20and%20Mike%20Macey%20and%20Mike%20Wang%20and%20Miquel%20Jubert%20Hermoso%20and%20Mo%20Metanat%20and%20Mohammad%20Rastegari%20and%20Munish%20Bansal%20and%20Nandhini%20Santhanam%20and%20Natascha%20Parks%20and%20Natasha%20White%20and%20Navyata%20Bawa%20and%20Nayan%20Singhal%20and%20Nick%20Egebo%20and%20Nicolas%20Usunier%20and%20Nikolay%20Pavlovich%20Laptev%20and%20Ning%20Dong%20and%20Ning%20Zhang%20and%20Norman%20Cheng%20and%20Oleg%20Chernoguz%20and%20Olivia%20Hart%20and%20Omkar%20Salpekar%20and%20Ozlem%20Kalinli%20and%20Parkin%20Kent%20and%20Parth%20Parekh%20and%20Paul%20Saab%20and%20Pavan%20Balaji%20and%20Pedro%20Rittner%20and%20Philip%20Bontrager%20and%20Pierre%20Roux%20and%20Piotr%20Dollar%20and%20Polina%20Zvyagina%20and%20Prashant%20Ratanchandani%20and%20Pritish%20Yuvraj%20and%20Qian%20Liang%20and%20Rachad%20Alao%20and%20Rachel%20Rodriguez%20and%20Rafi%20Ayub%20and%20Raghotham%20Murthy%20and%20Raghu%20Nayani%20and%20Rahul%20Mitra%20and%20Raymond%20Li%20and%20Rebekkah%20Hogan%20and%20Robin%20Battey%20and%20Rocky%20Wang%20and%20Rohan%20Maheswari%20and%20Russ%20Howes%20and%20Ruty%20Rinott%20and%20Sai%20Jayesh%20Bondu%20and%20Samyak%20Datta%20and%20Sara%20Chugh%20and%20Sara%20Hunt%20and%20Sargun%20Dhillon%20and%20Sasha%20Sidorov%20and%20Satadru%20Pan%20and%20Saurabh%20Verma%20and%20Seiji%20Yamamoto%20and%20Sharadh%20Ramaswamy%20and%20Shaun%20Lindsay%20and%20Shaun%20Lindsay%20and%20Sheng%20Feng%20and%20Shenghao%20Lin%20and%20Shengxin%20Cindy%20Zha%20and%20Shiva%20Shankar%20and%20Shuqiang%20Zhang%20and%20Shuqiang%20Zhang%20and%20Sinong%20Wang%20and%20Sneha%20Agarwal%20and%20Soji%20Sajuyigbe%20and%20Soumith%20Chintala%20and%20Stephanie%20Max%20and%20Stephen%20Chen%20and%20Steve%20Kehoe%20and%20Steve%20Satterfield%20and%20Sudarshan%20Govindaprasad%20and%20Sumit%20Gupta%20and%20Sungmin%20Cho%20and%20Sunny%20Virk%20and%20Suraj%20Subramanian%20and%20Sy%20Choudhury%20and%20Sydney%20Goldman%20and%20Tal%20Remez%20and%20Tamar%20Glaser%20and%20Tamara%20Best%20and%20Thilo%20Kohler%20and%20Thomas%20Robinson%20and%20Tianhe%20Li%20and%20Tianjun%20Zhang%20and%20Tim%20Matthews%20and%20Timothy%20Chou%20and%20Tzook%20Shaked%20and%20Varun%20Vontimitta%20and%20Victoria%20Ajayi%20and%20Victoria%20Montanez%20and%20Vijai%20Mohan%20and%20Vinay%20Satish%20Kumar%20and%20Vishal%20Mangla%20and%20V%C3%ADtor%20Albiero%20and%20Vlad%20Ionescu%20and%20Vlad%20Poenaru%20and%20Vlad%20Tiberiu%20Mihailescu%20and%20Vladimir%20Ivanov%20and%20Wei%20Li%20and%20Wenchen%20Wang%20and%20Wenwen%20Jiang%20and%20Wes%20Bouaziz%20and%20Will%20Constable%20and%20Xiaocheng%20Tang%20and%20Xiaofang%20Wang%20and%20Xiaojian%20Wu%20and%20Xiaolan%20Wang%20and%20Xide%20Xia%20and%20Xilun%20Wu%20and%20Xinbo%20Gao%20and%20Yanjun%20Chen%20and%20Ye%20Hu%20and%20Ye%20Jia%20and%20Ye%20Qi%20and%20Yenda%20Li%20and%20Yilin%20Zhang%20and%20Ying%20Zhang%20and%20Yossi%20Adi%20and%20Youngjin%20Nam%20and%20%20Yu%20and%20%20Wang%20and%20Yuchen%20Hao%20and%20Yundi%20Qian%20and%20Yuzi%20He%20and%20Zach%20Rait%20and%20Zachary%20DeVito%20and%20Zef%20Rosnbrick%20and%20Zhaoduo%20Wen%20and%20Zhenyu%20Yang%20and%20Zhiwei%20Zhao%0AAbstract%3A%20%20%20Modern%20artificial%20intelligence%20%28AI%29%20systems%20are%20powered%20by%20foundation%20models.%0AThis%20paper%20presents%20a%20new%20set%20of%20foundation%20models%2C%20called%20Llama%203.%20It%20is%20a%0Aherd%20of%20language%20models%20that%20natively%20support%20multilinguality%2C%20coding%2C%0Areasoning%2C%20and%20tool%20usage.%20Our%20largest%20model%20is%20a%20dense%20Transformer%20with%20405B%0Aparameters%20and%20a%20context%20window%20of%20up%20to%20128K%20tokens.%20This%20paper%20presents%20an%0Aextensive%20empirical%20evaluation%20of%20Llama%203.%20We%20find%20that%20Llama%203%20delivers%0Acomparable%20quality%20to%20leading%20language%20models%20such%20as%20GPT-4%20on%20a%20plethora%20of%0Atasks.%20We%20publicly%20release%20Llama%203%2C%20including%20pre-trained%20and%20post-trained%0Aversions%20of%20the%20405B%20parameter%20language%20model%20and%20our%20Llama%20Guard%203%20model%20for%0Ainput%20and%20output%20safety.%20The%20paper%20also%20presents%20the%20results%20of%20experiments%20in%0Awhich%20we%20integrate%20image%2C%20video%2C%20and%20speech%20capabilities%20into%20Llama%203%20via%20a%0Acompositional%20approach.%20We%20observe%20this%20approach%20performs%20competitively%20with%0Athe%20state-of-the-art%20on%20image%2C%20video%2C%20and%20speech%20recognition%20tasks.%20The%0Aresulting%20models%20are%20not%20yet%20being%20broadly%20released%20as%20they%20are%20still%20under%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21783v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Llama%25203%2520Herd%2520of%2520Models%26entry.906535625%3DAbhimanyu%2520Dubey%2520and%2520Abhinav%2520Jauhri%2520and%2520Abhinav%2520Pandey%2520and%2520Abhishek%2520Kadian%2520and%2520Ahmad%2520Al-Dahle%2520and%2520Aiesha%2520Letman%2520and%2520Akhil%2520Mathur%2520and%2520Alan%2520Schelten%2520and%2520Amy%2520Yang%2520and%2520Angela%2520Fan%2520and%2520Anirudh%2520Goyal%2520and%2520Anthony%2520Hartshorn%2520and%2520Aobo%2520Yang%2520and%2520Archi%2520Mitra%2520and%2520Archie%2520Sravankumar%2520and%2520Artem%2520Korenev%2520and%2520Arthur%2520Hinsvark%2520and%2520Arun%2520Rao%2520and%2520Aston%2520Zhang%2520and%2520Aurelien%2520Rodriguez%2520and%2520Austen%2520Gregerson%2520and%2520Ava%2520Spataru%2520and%2520Baptiste%2520Roziere%2520and%2520Bethany%2520Biron%2520and%2520Binh%2520Tang%2520and%2520Bobbie%2520Chern%2520and%2520Charlotte%2520Caucheteux%2520and%2520Chaya%2520Nayak%2520and%2520Chloe%2520Bi%2520and%2520Chris%2520Marra%2520and%2520Chris%2520McConnell%2520and%2520Christian%2520Keller%2520and%2520Christophe%2520Touret%2520and%2520Chunyang%2520Wu%2520and%2520Corinne%2520Wong%2520and%2520Cristian%2520Canton%2520Ferrer%2520and%2520Cyrus%2520Nikolaidis%2520and%2520Damien%2520Allonsius%2520and%2520Daniel%2520Song%2520and%2520Danielle%2520Pintz%2520and%2520Danny%2520Livshits%2520and%2520David%2520Esiobu%2520and%2520Dhruv%2520Choudhary%2520and%2520Dhruv%2520Mahajan%2520and%2520Diego%2520Garcia-Olano%2520and%2520Diego%2520Perino%2520and%2520Dieuwke%2520Hupkes%2520and%2520Egor%2520Lakomkin%2520and%2520Ehab%2520AlBadawy%2520and%2520Elina%2520Lobanova%2520and%2520Emily%2520Dinan%2520and%2520Eric%2520Michael%2520Smith%2520and%2520Filip%2520Radenovic%2520and%2520Frank%2520Zhang%2520and%2520Gabriel%2520Synnaeve%2520and%2520Gabrielle%2520Lee%2520and%2520Georgia%2520Lewis%2520Anderson%2520and%2520Graeme%2520Nail%2520and%2520Gregoire%2520Mialon%2520and%2520Guan%2520Pang%2520and%2520Guillem%2520Cucurell%2520and%2520Hailey%2520Nguyen%2520and%2520Hannah%2520Korevaar%2520and%2520Hu%2520Xu%2520and%2520Hugo%2520Touvron%2520and%2520Iliyan%2520Zarov%2520and%2520Imanol%2520Arrieta%2520Ibarra%2520and%2520Isabel%2520Kloumann%2520and%2520Ishan%2520Misra%2520and%2520Ivan%2520Evtimov%2520and%2520Jade%2520Copet%2520and%2520Jaewon%2520Lee%2520and%2520Jan%2520Geffert%2520and%2520Jana%2520Vranes%2520and%2520Jason%2520Park%2520and%2520Jay%2520Mahadeokar%2520and%2520Jeet%2520Shah%2520and%2520Jelmer%2520van%2520der%2520Linde%2520and%2520Jennifer%2520Billock%2520and%2520Jenny%2520Hong%2520and%2520Jenya%2520Lee%2520and%2520Jeremy%2520Fu%2520and%2520Jianfeng%2520Chi%2520and%2520Jianyu%2520Huang%2520and%2520Jiawen%2520Liu%2520and%2520Jie%2520Wang%2520and%2520Jiecao%2520Yu%2520and%2520Joanna%2520Bitton%2520and%2520Joe%2520Spisak%2520and%2520Jongsoo%2520Park%2520and%2520Joseph%2520Rocca%2520and%2520Joshua%2520Johnstun%2520and%2520Joshua%2520Saxe%2520and%2520Junteng%2520Jia%2520and%2520Kalyan%2520Vasuden%2520Alwala%2520and%2520Kartikeya%2520Upasani%2520and%2520Kate%2520Plawiak%2520and%2520Ke%2520Li%2520and%2520Kenneth%2520Heafield%2520and%2520Kevin%2520Stone%2520and%2520Khalid%2520El-Arini%2520and%2520Krithika%2520Iyer%2520and%2520Kshitiz%2520Malik%2520and%2520Kuenley%2520Chiu%2520and%2520Kunal%2520Bhalla%2520and%2520Lauren%2520Rantala-Yeary%2520and%2520Laurens%2520van%2520der%2520Maaten%2520and%2520Lawrence%2520Chen%2520and%2520Liang%2520Tan%2520and%2520Liz%2520Jenkins%2520and%2520Louis%2520Martin%2520and%2520Lovish%2520Madaan%2520and%2520Lubo%2520Malo%2520and%2520Lukas%2520Blecher%2520and%2520Lukas%2520Landzaat%2520and%2520Luke%2520de%2520Oliveira%2520and%2520Madeline%2520Muzzi%2520and%2520Mahesh%2520Pasupuleti%2520and%2520Mannat%2520Singh%2520and%2520Manohar%2520Paluri%2520and%2520Marcin%2520Kardas%2520and%2520Mathew%2520Oldham%2520and%2520Mathieu%2520Rita%2520and%2520Maya%2520Pavlova%2520and%2520Melanie%2520Kambadur%2520and%2520Mike%2520Lewis%2520and%2520Min%2520Si%2520and%2520Mitesh%2520Kumar%2520Singh%2520and%2520Mona%2520Hassan%2520and%2520Naman%2520Goyal%2520and%2520Narjes%2520Torabi%2520and%2520Nikolay%2520Bashlykov%2520and%2520Nikolay%2520Bogoychev%2520and%2520Niladri%2520Chatterji%2520and%2520Olivier%2520Duchenne%2520and%2520Onur%2520%25C3%2587elebi%2520and%2520Patrick%2520Alrassy%2520and%2520Pengchuan%2520Zhang%2520and%2520Pengwei%2520Li%2520and%2520Petar%2520Vasic%2520and%2520Peter%2520Weng%2520and%2520Prajjwal%2520Bhargava%2520and%2520Pratik%2520Dubal%2520and%2520Praveen%2520Krishnan%2520and%2520Punit%2520Singh%2520Koura%2520and%2520Puxin%2520Xu%2520and%2520Qing%2520He%2520and%2520Qingxiao%2520Dong%2520and%2520Ragavan%2520Srinivasan%2520and%2520Raj%2520Ganapathy%2520and%2520Ramon%2520Calderer%2520and%2520Ricardo%2520Silveira%2520Cabral%2520and%2520Robert%2520Stojnic%2520and%2520Roberta%2520Raileanu%2520and%2520Rohit%2520Girdhar%2520and%2520Rohit%2520Patel%2520and%2520Romain%2520Sauvestre%2520and%2520Ronnie%2520Polidoro%2520and%2520Roshan%2520Sumbaly%2520and%2520Ross%2520Taylor%2520and%2520Ruan%2520Silva%2520and%2520Rui%2520Hou%2520and%2520Rui%2520Wang%2520and%2520Saghar%2520Hosseini%2520and%2520Sahana%2520Chennabasappa%2520and%2520Sanjay%2520Singh%2520and%2520Sean%2520Bell%2520and%2520Seohyun%2520Sonia%2520Kim%2520and%2520Sergey%2520Edunov%2520and%2520Shaoliang%2520Nie%2520and%2520Sharan%2520Narang%2520and%2520Sharath%2520Raparthy%2520and%2520Sheng%2520Shen%2520and%2520Shengye%2520Wan%2520and%2520Shruti%2520Bhosale%2520and%2520Shun%2520Zhang%2520and%2520Simon%2520Vandenhende%2520and%2520Soumya%2520Batra%2520and%2520Spencer%2520Whitman%2520and%2520Sten%2520Sootla%2520and%2520Stephane%2520Collot%2520and%2520Suchin%2520Gururangan%2520and%2520Sydney%2520Borodinsky%2520and%2520Tamar%2520Herman%2520and%2520Tara%2520Fowler%2520and%2520Tarek%2520Sheasha%2520and%2520Thomas%2520Georgiou%2520and%2520Thomas%2520Scialom%2520and%2520Tobias%2520Speckbacher%2520and%2520Todor%2520Mihaylov%2520and%2520Tong%2520Xiao%2520and%2520Ujjwal%2520Karn%2520and%2520Vedanuj%2520Goswami%2520and%2520Vibhor%2520Gupta%2520and%2520Vignesh%2520Ramanathan%2520and%2520Viktor%2520Kerkez%2520and%2520Vincent%2520Gonguet%2520and%2520Virginie%2520Do%2520and%2520Vish%2520Vogeti%2520and%2520Vladan%2520Petrovic%2520and%2520Weiwei%2520Chu%2520and%2520Wenhan%2520Xiong%2520and%2520Wenyin%2520Fu%2520and%2520Whitney%2520Meers%2520and%2520Xavier%2520Martinet%2520and%2520Xiaodong%2520Wang%2520and%2520Xiaoqing%2520Ellen%2520Tan%2520and%2520Xinfeng%2520Xie%2520and%2520Xuchao%2520Jia%2520and%2520Xuewei%2520Wang%2520and%2520Yaelle%2520Goldschlag%2520and%2520Yashesh%2520Gaur%2520and%2520Yasmine%2520Babaei%2520and%2520Yi%2520Wen%2520and%2520Yiwen%2520Song%2520and%2520Yuchen%2520Zhang%2520and%2520Yue%2520Li%2520and%2520Yuning%2520Mao%2520and%2520Zacharie%2520Delpierre%2520Coudert%2520and%2520Zheng%2520Yan%2520and%2520Zhengxing%2520Chen%2520and%2520Zoe%2520Papakipos%2520and%2520Aaditya%2520Singh%2520and%2520Aaron%2520Grattafiori%2520and%2520Abha%2520Jain%2520and%2520Adam%2520Kelsey%2520and%2520Adam%2520Shajnfeld%2520and%2520Adithya%2520Gangidi%2520and%2520Adolfo%2520Victoria%2520and%2520Ahuva%2520Goldstand%2520and%2520Ajay%2520Menon%2520and%2520Ajay%2520Sharma%2520and%2520Alex%2520Boesenberg%2520and%2520Alex%2520Vaughan%2520and%2520Alexei%2520Baevski%2520and%2520Allie%2520Feinstein%2520and%2520Amanda%2520Kallet%2520and%2520Amit%2520Sangani%2520and%2520Anam%2520Yunus%2520and%2520Andrei%2520Lupu%2520and%2520Andres%2520Alvarado%2520and%2520Andrew%2520Caples%2520and%2520Andrew%2520Gu%2520and%2520Andrew%2520Ho%2520and%2520Andrew%2520Poulton%2520and%2520Andrew%2520Ryan%2520and%2520Ankit%2520Ramchandani%2520and%2520Annie%2520Franco%2520and%2520Aparajita%2520Saraf%2520and%2520Arkabandhu%2520Chowdhury%2520and%2520Ashley%2520Gabriel%2520and%2520Ashwin%2520Bharambe%2520and%2520Assaf%2520Eisenman%2520and%2520Azadeh%2520Yazdan%2520and%2520Beau%2520James%2520and%2520Ben%2520Maurer%2520and%2520Benjamin%2520Leonhardi%2520and%2520Bernie%2520Huang%2520and%2520Beth%2520Loyd%2520and%2520Beto%2520De%2520Paola%2520and%2520Bhargavi%2520Paranjape%2520and%2520Bing%2520Liu%2520and%2520Bo%2520Wu%2520and%2520Boyu%2520Ni%2520and%2520Braden%2520Hancock%2520and%2520Bram%2520Wasti%2520and%2520Brandon%2520Spence%2520and%2520Brani%2520Stojkovic%2520and%2520Brian%2520Gamido%2520and%2520Britt%2520Montalvo%2520and%2520Carl%2520Parker%2520and%2520Carly%2520Burton%2520and%2520Catalina%2520Mejia%2520and%2520Changhan%2520Wang%2520and%2520Changkyu%2520Kim%2520and%2520Chao%2520Zhou%2520and%2520Chester%2520Hu%2520and%2520Ching-Hsiang%2520Chu%2520and%2520Chris%2520Cai%2520and%2520Chris%2520Tindal%2520and%2520Christoph%2520Feichtenhofer%2520and%2520Damon%2520Civin%2520and%2520Dana%2520Beaty%2520and%2520Daniel%2520Kreymer%2520and%2520Daniel%2520Li%2520and%2520Danny%2520Wyatt%2520and%2520David%2520Adkins%2520and%2520David%2520Xu%2520and%2520Davide%2520Testuggine%2520and%2520Delia%2520David%2520and%2520Devi%2520Parikh%2520and%2520Diana%2520Liskovich%2520and%2520Didem%2520Foss%2520and%2520Dingkang%2520Wang%2520and%2520Duc%2520Le%2520and%2520Dustin%2520Holland%2520and%2520Edward%2520Dowling%2520and%2520Eissa%2520Jamil%2520and%2520Elaine%2520Montgomery%2520and%2520Eleonora%2520Presani%2520and%2520Emily%2520Hahn%2520and%2520Emily%2520Wood%2520and%2520Erik%2520Brinkman%2520and%2520Esteban%2520Arcaute%2520and%2520Evan%2520Dunbar%2520and%2520Evan%2520Smothers%2520and%2520Fei%2520Sun%2520and%2520Felix%2520Kreuk%2520and%2520Feng%2520Tian%2520and%2520Firat%2520Ozgenel%2520and%2520Francesco%2520Caggioni%2520and%2520Francisco%2520Guzm%25C3%25A1n%2520and%2520Frank%2520Kanayet%2520and%2520Frank%2520Seide%2520and%2520Gabriela%2520Medina%2520Florez%2520and%2520Gabriella%2520Schwarz%2520and%2520Gada%2520Badeer%2520and%2520Georgia%2520Swee%2520and%2520Gil%2520Halpern%2520and%2520Govind%2520Thattai%2520and%2520Grant%2520Herman%2520and%2520Grigory%2520Sizov%2520and%2520%2520Guangyi%2520and%2520%2520Zhang%2520and%2520Guna%2520Lakshminarayanan%2520and%2520Hamid%2520Shojanazeri%2520and%2520Han%2520Zou%2520and%2520Hannah%2520Wang%2520and%2520Hanwen%2520Zha%2520and%2520Haroun%2520Habeeb%2520and%2520Harrison%2520Rudolph%2520and%2520Helen%2520Suk%2520and%2520Henry%2520Aspegren%2520and%2520Hunter%2520Goldman%2520and%2520Ibrahim%2520Damlaj%2520and%2520Igor%2520Molybog%2520and%2520Igor%2520Tufanov%2520and%2520Irina-Elena%2520Veliche%2520and%2520Itai%2520Gat%2520and%2520Jake%2520Weissman%2520and%2520James%2520Geboski%2520and%2520James%2520Kohli%2520and%2520Japhet%2520Asher%2520and%2520Jean-Baptiste%2520Gaya%2520and%2520Jeff%2520Marcus%2520and%2520Jeff%2520Tang%2520and%2520Jennifer%2520Chan%2520and%2520Jenny%2520Zhen%2520and%2520Jeremy%2520Reizenstein%2520and%2520Jeremy%2520Teboul%2520and%2520Jessica%2520Zhong%2520and%2520Jian%2520Jin%2520and%2520Jingyi%2520Yang%2520and%2520Joe%2520Cummings%2520and%2520Jon%2520Carvill%2520and%2520Jon%2520Shepard%2520and%2520Jonathan%2520McPhie%2520and%2520Jonathan%2520Torres%2520and%2520Josh%2520Ginsburg%2520and%2520Junjie%2520Wang%2520and%2520Kai%2520Wu%2520and%2520Kam%2520Hou%2520U%2520and%2520Karan%2520Saxena%2520and%2520Karthik%2520Prasad%2520and%2520Kartikay%2520Khandelwal%2520and%2520Katayoun%2520Zand%2520and%2520Kathy%2520Matosich%2520and%2520Kaushik%2520Veeraraghavan%2520and%2520Kelly%2520Michelena%2520and%2520Keqian%2520Li%2520and%2520Kun%2520Huang%2520and%2520Kunal%2520Chawla%2520and%2520Kushal%2520Lakhotia%2520and%2520Kyle%2520Huang%2520and%2520Lailin%2520Chen%2520and%2520Lakshya%2520Garg%2520and%2520Lavender%2520A%2520and%2520Leandro%2520Silva%2520and%2520Lee%2520Bell%2520and%2520Lei%2520Zhang%2520and%2520Liangpeng%2520Guo%2520and%2520Licheng%2520Yu%2520and%2520Liron%2520Moshkovich%2520and%2520Luca%2520Wehrstedt%2520and%2520Madian%2520Khabsa%2520and%2520Manav%2520Avalani%2520and%2520Manish%2520Bhatt%2520and%2520Maria%2520Tsimpoukelli%2520and%2520Martynas%2520Mankus%2520and%2520Matan%2520Hasson%2520and%2520Matthew%2520Lennie%2520and%2520Matthias%2520Reso%2520and%2520Maxim%2520Groshev%2520and%2520Maxim%2520Naumov%2520and%2520Maya%2520Lathi%2520and%2520Meghan%2520Keneally%2520and%2520Michael%2520L.%2520Seltzer%2520and%2520Michal%2520Valko%2520and%2520Michelle%2520Restrepo%2520and%2520Mihir%2520Patel%2520and%2520Mik%2520Vyatskov%2520and%2520Mikayel%2520Samvelyan%2520and%2520Mike%2520Clark%2520and%2520Mike%2520Macey%2520and%2520Mike%2520Wang%2520and%2520Miquel%2520Jubert%2520Hermoso%2520and%2520Mo%2520Metanat%2520and%2520Mohammad%2520Rastegari%2520and%2520Munish%2520Bansal%2520and%2520Nandhini%2520Santhanam%2520and%2520Natascha%2520Parks%2520and%2520Natasha%2520White%2520and%2520Navyata%2520Bawa%2520and%2520Nayan%2520Singhal%2520and%2520Nick%2520Egebo%2520and%2520Nicolas%2520Usunier%2520and%2520Nikolay%2520Pavlovich%2520Laptev%2520and%2520Ning%2520Dong%2520and%2520Ning%2520Zhang%2520and%2520Norman%2520Cheng%2520and%2520Oleg%2520Chernoguz%2520and%2520Olivia%2520Hart%2520and%2520Omkar%2520Salpekar%2520and%2520Ozlem%2520Kalinli%2520and%2520Parkin%2520Kent%2520and%2520Parth%2520Parekh%2520and%2520Paul%2520Saab%2520and%2520Pavan%2520Balaji%2520and%2520Pedro%2520Rittner%2520and%2520Philip%2520Bontrager%2520and%2520Pierre%2520Roux%2520and%2520Piotr%2520Dollar%2520and%2520Polina%2520Zvyagina%2520and%2520Prashant%2520Ratanchandani%2520and%2520Pritish%2520Yuvraj%2520and%2520Qian%2520Liang%2520and%2520Rachad%2520Alao%2520and%2520Rachel%2520Rodriguez%2520and%2520Rafi%2520Ayub%2520and%2520Raghotham%2520Murthy%2520and%2520Raghu%2520Nayani%2520and%2520Rahul%2520Mitra%2520and%2520Raymond%2520Li%2520and%2520Rebekkah%2520Hogan%2520and%2520Robin%2520Battey%2520and%2520Rocky%2520Wang%2520and%2520Rohan%2520Maheswari%2520and%2520Russ%2520Howes%2520and%2520Ruty%2520Rinott%2520and%2520Sai%2520Jayesh%2520Bondu%2520and%2520Samyak%2520Datta%2520and%2520Sara%2520Chugh%2520and%2520Sara%2520Hunt%2520and%2520Sargun%2520Dhillon%2520and%2520Sasha%2520Sidorov%2520and%2520Satadru%2520Pan%2520and%2520Saurabh%2520Verma%2520and%2520Seiji%2520Yamamoto%2520and%2520Sharadh%2520Ramaswamy%2520and%2520Shaun%2520Lindsay%2520and%2520Shaun%2520Lindsay%2520and%2520Sheng%2520Feng%2520and%2520Shenghao%2520Lin%2520and%2520Shengxin%2520Cindy%2520Zha%2520and%2520Shiva%2520Shankar%2520and%2520Shuqiang%2520Zhang%2520and%2520Shuqiang%2520Zhang%2520and%2520Sinong%2520Wang%2520and%2520Sneha%2520Agarwal%2520and%2520Soji%2520Sajuyigbe%2520and%2520Soumith%2520Chintala%2520and%2520Stephanie%2520Max%2520and%2520Stephen%2520Chen%2520and%2520Steve%2520Kehoe%2520and%2520Steve%2520Satterfield%2520and%2520Sudarshan%2520Govindaprasad%2520and%2520Sumit%2520Gupta%2520and%2520Sungmin%2520Cho%2520and%2520Sunny%2520Virk%2520and%2520Suraj%2520Subramanian%2520and%2520Sy%2520Choudhury%2520and%2520Sydney%2520Goldman%2520and%2520Tal%2520Remez%2520and%2520Tamar%2520Glaser%2520and%2520Tamara%2520Best%2520and%2520Thilo%2520Kohler%2520and%2520Thomas%2520Robinson%2520and%2520Tianhe%2520Li%2520and%2520Tianjun%2520Zhang%2520and%2520Tim%2520Matthews%2520and%2520Timothy%2520Chou%2520and%2520Tzook%2520Shaked%2520and%2520Varun%2520Vontimitta%2520and%2520Victoria%2520Ajayi%2520and%2520Victoria%2520Montanez%2520and%2520Vijai%2520Mohan%2520and%2520Vinay%2520Satish%2520Kumar%2520and%2520Vishal%2520Mangla%2520and%2520V%25C3%25ADtor%2520Albiero%2520and%2520Vlad%2520Ionescu%2520and%2520Vlad%2520Poenaru%2520and%2520Vlad%2520Tiberiu%2520Mihailescu%2520and%2520Vladimir%2520Ivanov%2520and%2520Wei%2520Li%2520and%2520Wenchen%2520Wang%2520and%2520Wenwen%2520Jiang%2520and%2520Wes%2520Bouaziz%2520and%2520Will%2520Constable%2520and%2520Xiaocheng%2520Tang%2520and%2520Xiaofang%2520Wang%2520and%2520Xiaojian%2520Wu%2520and%2520Xiaolan%2520Wang%2520and%2520Xide%2520Xia%2520and%2520Xilun%2520Wu%2520and%2520Xinbo%2520Gao%2520and%2520Yanjun%2520Chen%2520and%2520Ye%2520Hu%2520and%2520Ye%2520Jia%2520and%2520Ye%2520Qi%2520and%2520Yenda%2520Li%2520and%2520Yilin%2520Zhang%2520and%2520Ying%2520Zhang%2520and%2520Yossi%2520Adi%2520and%2520Youngjin%2520Nam%2520and%2520%2520Yu%2520and%2520%2520Wang%2520and%2520Yuchen%2520Hao%2520and%2520Yundi%2520Qian%2520and%2520Yuzi%2520He%2520and%2520Zach%2520Rait%2520and%2520Zachary%2520DeVito%2520and%2520Zef%2520Rosnbrick%2520and%2520Zhaoduo%2520Wen%2520and%2520Zhenyu%2520Yang%2520and%2520Zhiwei%2520Zhao%26entry.1292438233%3D%2520%2520Modern%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%2520are%2520powered%2520by%2520foundation%2520models.%250AThis%2520paper%2520presents%2520a%2520new%2520set%2520of%2520foundation%2520models%252C%2520called%2520Llama%25203.%2520It%2520is%2520a%250Aherd%2520of%2520language%2520models%2520that%2520natively%2520support%2520multilinguality%252C%2520coding%252C%250Areasoning%252C%2520and%2520tool%2520usage.%2520Our%2520largest%2520model%2520is%2520a%2520dense%2520Transformer%2520with%2520405B%250Aparameters%2520and%2520a%2520context%2520window%2520of%2520up%2520to%2520128K%2520tokens.%2520This%2520paper%2520presents%2520an%250Aextensive%2520empirical%2520evaluation%2520of%2520Llama%25203.%2520We%2520find%2520that%2520Llama%25203%2520delivers%250Acomparable%2520quality%2520to%2520leading%2520language%2520models%2520such%2520as%2520GPT-4%2520on%2520a%2520plethora%2520of%250Atasks.%2520We%2520publicly%2520release%2520Llama%25203%252C%2520including%2520pre-trained%2520and%2520post-trained%250Aversions%2520of%2520the%2520405B%2520parameter%2520language%2520model%2520and%2520our%2520Llama%2520Guard%25203%2520model%2520for%250Ainput%2520and%2520output%2520safety.%2520The%2520paper%2520also%2520presents%2520the%2520results%2520of%2520experiments%2520in%250Awhich%2520we%2520integrate%2520image%252C%2520video%252C%2520and%2520speech%2520capabilities%2520into%2520Llama%25203%2520via%2520a%250Acompositional%2520approach.%2520We%2520observe%2520this%2520approach%2520performs%2520competitively%2520with%250Athe%2520state-of-the-art%2520on%2520image%252C%2520video%252C%2520and%2520speech%2520recognition%2520tasks.%2520The%250Aresulting%2520models%2520are%2520not%2520yet%2520being%2520broadly%2520released%2520as%2520they%2520are%2520still%2520under%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21783v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Llama%203%20Herd%20of%20Models&entry.906535625=Abhimanyu%20Dubey%20and%20Abhinav%20Jauhri%20and%20Abhinav%20Pandey%20and%20Abhishek%20Kadian%20and%20Ahmad%20Al-Dahle%20and%20Aiesha%20Letman%20and%20Akhil%20Mathur%20and%20Alan%20Schelten%20and%20Amy%20Yang%20and%20Angela%20Fan%20and%20Anirudh%20Goyal%20and%20Anthony%20Hartshorn%20and%20Aobo%20Yang%20and%20Archi%20Mitra%20and%20Archie%20Sravankumar%20and%20Artem%20Korenev%20and%20Arthur%20Hinsvark%20and%20Arun%20Rao%20and%20Aston%20Zhang%20and%20Aurelien%20Rodriguez%20and%20Austen%20Gregerson%20and%20Ava%20Spataru%20and%20Baptiste%20Roziere%20and%20Bethany%20Biron%20and%20Binh%20Tang%20and%20Bobbie%20Chern%20and%20Charlotte%20Caucheteux%20and%20Chaya%20Nayak%20and%20Chloe%20Bi%20and%20Chris%20Marra%20and%20Chris%20McConnell%20and%20Christian%20Keller%20and%20Christophe%20Touret%20and%20Chunyang%20Wu%20and%20Corinne%20Wong%20and%20Cristian%20Canton%20Ferrer%20and%20Cyrus%20Nikolaidis%20and%20Damien%20Allonsius%20and%20Daniel%20Song%20and%20Danielle%20Pintz%20and%20Danny%20Livshits%20and%20David%20Esiobu%20and%20Dhruv%20Choudhary%20and%20Dhruv%20Mahajan%20and%20Diego%20Garcia-Olano%20and%20Diego%20Perino%20and%20Dieuwke%20Hupkes%20and%20Egor%20Lakomkin%20and%20Ehab%20AlBadawy%20and%20Elina%20Lobanova%20and%20Emily%20Dinan%20and%20Eric%20Michael%20Smith%20and%20Filip%20Radenovic%20and%20Frank%20Zhang%20and%20Gabriel%20Synnaeve%20and%20Gabrielle%20Lee%20and%20Georgia%20Lewis%20Anderson%20and%20Graeme%20Nail%20and%20Gregoire%20Mialon%20and%20Guan%20Pang%20and%20Guillem%20Cucurell%20and%20Hailey%20Nguyen%20and%20Hannah%20Korevaar%20and%20Hu%20Xu%20and%20Hugo%20Touvron%20and%20Iliyan%20Zarov%20and%20Imanol%20Arrieta%20Ibarra%20and%20Isabel%20Kloumann%20and%20Ishan%20Misra%20and%20Ivan%20Evtimov%20and%20Jade%20Copet%20and%20Jaewon%20Lee%20and%20Jan%20Geffert%20and%20Jana%20Vranes%20and%20Jason%20Park%20and%20Jay%20Mahadeokar%20and%20Jeet%20Shah%20and%20Jelmer%20van%20der%20Linde%20and%20Jennifer%20Billock%20and%20Jenny%20Hong%20and%20Jenya%20Lee%20and%20Jeremy%20Fu%20and%20Jianfeng%20Chi%20and%20Jianyu%20Huang%20and%20Jiawen%20Liu%20and%20Jie%20Wang%20and%20Jiecao%20Yu%20and%20Joanna%20Bitton%20and%20Joe%20Spisak%20and%20Jongsoo%20Park%20and%20Joseph%20Rocca%20and%20Joshua%20Johnstun%20and%20Joshua%20Saxe%20and%20Junteng%20Jia%20and%20Kalyan%20Vasuden%20Alwala%20and%20Kartikeya%20Upasani%20and%20Kate%20Plawiak%20and%20Ke%20Li%20and%20Kenneth%20Heafield%20and%20Kevin%20Stone%20and%20Khalid%20El-Arini%20and%20Krithika%20Iyer%20and%20Kshitiz%20Malik%20and%20Kuenley%20Chiu%20and%20Kunal%20Bhalla%20and%20Lauren%20Rantala-Yeary%20and%20Laurens%20van%20der%20Maaten%20and%20Lawrence%20Chen%20and%20Liang%20Tan%20and%20Liz%20Jenkins%20and%20Louis%20Martin%20and%20Lovish%20Madaan%20and%20Lubo%20Malo%20and%20Lukas%20Blecher%20and%20Lukas%20Landzaat%20and%20Luke%20de%20Oliveira%20and%20Madeline%20Muzzi%20and%20Mahesh%20Pasupuleti%20and%20Mannat%20Singh%20and%20Manohar%20Paluri%20and%20Marcin%20Kardas%20and%20Mathew%20Oldham%20and%20Mathieu%20Rita%20and%20Maya%20Pavlova%20and%20Melanie%20Kambadur%20and%20Mike%20Lewis%20and%20Min%20Si%20and%20Mitesh%20Kumar%20Singh%20and%20Mona%20Hassan%20and%20Naman%20Goyal%20and%20Narjes%20Torabi%20and%20Nikolay%20Bashlykov%20and%20Nikolay%20Bogoychev%20and%20Niladri%20Chatterji%20and%20Olivier%20Duchenne%20and%20Onur%20%C3%87elebi%20and%20Patrick%20Alrassy%20and%20Pengchuan%20Zhang%20and%20Pengwei%20Li%20and%20Petar%20Vasic%20and%20Peter%20Weng%20and%20Prajjwal%20Bhargava%20and%20Pratik%20Dubal%20and%20Praveen%20Krishnan%20and%20Punit%20Singh%20Koura%20and%20Puxin%20Xu%20and%20Qing%20He%20and%20Qingxiao%20Dong%20and%20Ragavan%20Srinivasan%20and%20Raj%20Ganapathy%20and%20Ramon%20Calderer%20and%20Ricardo%20Silveira%20Cabral%20and%20Robert%20Stojnic%20and%20Roberta%20Raileanu%20and%20Rohit%20Girdhar%20and%20Rohit%20Patel%20and%20Romain%20Sauvestre%20and%20Ronnie%20Polidoro%20and%20Roshan%20Sumbaly%20and%20Ross%20Taylor%20and%20Ruan%20Silva%20and%20Rui%20Hou%20and%20Rui%20Wang%20and%20Saghar%20Hosseini%20and%20Sahana%20Chennabasappa%20and%20Sanjay%20Singh%20and%20Sean%20Bell%20and%20Seohyun%20Sonia%20Kim%20and%20Sergey%20Edunov%20and%20Shaoliang%20Nie%20and%20Sharan%20Narang%20and%20Sharath%20Raparthy%20and%20Sheng%20Shen%20and%20Shengye%20Wan%20and%20Shruti%20Bhosale%20and%20Shun%20Zhang%20and%20Simon%20Vandenhende%20and%20Soumya%20Batra%20and%20Spencer%20Whitman%20and%20Sten%20Sootla%20and%20Stephane%20Collot%20and%20Suchin%20Gururangan%20and%20Sydney%20Borodinsky%20and%20Tamar%20Herman%20and%20Tara%20Fowler%20and%20Tarek%20Sheasha%20and%20Thomas%20Georgiou%20and%20Thomas%20Scialom%20and%20Tobias%20Speckbacher%20and%20Todor%20Mihaylov%20and%20Tong%20Xiao%20and%20Ujjwal%20Karn%20and%20Vedanuj%20Goswami%20and%20Vibhor%20Gupta%20and%20Vignesh%20Ramanathan%20and%20Viktor%20Kerkez%20and%20Vincent%20Gonguet%20and%20Virginie%20Do%20and%20Vish%20Vogeti%20and%20Vladan%20Petrovic%20and%20Weiwei%20Chu%20and%20Wenhan%20Xiong%20and%20Wenyin%20Fu%20and%20Whitney%20Meers%20and%20Xavier%20Martinet%20and%20Xiaodong%20Wang%20and%20Xiaoqing%20Ellen%20Tan%20and%20Xinfeng%20Xie%20and%20Xuchao%20Jia%20and%20Xuewei%20Wang%20and%20Yaelle%20Goldschlag%20and%20Yashesh%20Gaur%20and%20Yasmine%20Babaei%20and%20Yi%20Wen%20and%20Yiwen%20Song%20and%20Yuchen%20Zhang%20and%20Yue%20Li%20and%20Yuning%20Mao%20and%20Zacharie%20Delpierre%20Coudert%20and%20Zheng%20Yan%20and%20Zhengxing%20Chen%20and%20Zoe%20Papakipos%20and%20Aaditya%20Singh%20and%20Aaron%20Grattafiori%20and%20Abha%20Jain%20and%20Adam%20Kelsey%20and%20Adam%20Shajnfeld%20and%20Adithya%20Gangidi%20and%20Adolfo%20Victoria%20and%20Ahuva%20Goldstand%20and%20Ajay%20Menon%20and%20Ajay%20Sharma%20and%20Alex%20Boesenberg%20and%20Alex%20Vaughan%20and%20Alexei%20Baevski%20and%20Allie%20Feinstein%20and%20Amanda%20Kallet%20and%20Amit%20Sangani%20and%20Anam%20Yunus%20and%20Andrei%20Lupu%20and%20Andres%20Alvarado%20and%20Andrew%20Caples%20and%20Andrew%20Gu%20and%20Andrew%20Ho%20and%20Andrew%20Poulton%20and%20Andrew%20Ryan%20and%20Ankit%20Ramchandani%20and%20Annie%20Franco%20and%20Aparajita%20Saraf%20and%20Arkabandhu%20Chowdhury%20and%20Ashley%20Gabriel%20and%20Ashwin%20Bharambe%20and%20Assaf%20Eisenman%20and%20Azadeh%20Yazdan%20and%20Beau%20James%20and%20Ben%20Maurer%20and%20Benjamin%20Leonhardi%20and%20Bernie%20Huang%20and%20Beth%20Loyd%20and%20Beto%20De%20Paola%20and%20Bhargavi%20Paranjape%20and%20Bing%20Liu%20and%20Bo%20Wu%20and%20Boyu%20Ni%20and%20Braden%20Hancock%20and%20Bram%20Wasti%20and%20Brandon%20Spence%20and%20Brani%20Stojkovic%20and%20Brian%20Gamido%20and%20Britt%20Montalvo%20and%20Carl%20Parker%20and%20Carly%20Burton%20and%20Catalina%20Mejia%20and%20Changhan%20Wang%20and%20Changkyu%20Kim%20and%20Chao%20Zhou%20and%20Chester%20Hu%20and%20Ching-Hsiang%20Chu%20and%20Chris%20Cai%20and%20Chris%20Tindal%20and%20Christoph%20Feichtenhofer%20and%20Damon%20Civin%20and%20Dana%20Beaty%20and%20Daniel%20Kreymer%20and%20Daniel%20Li%20and%20Danny%20Wyatt%20and%20David%20Adkins%20and%20David%20Xu%20and%20Davide%20Testuggine%20and%20Delia%20David%20and%20Devi%20Parikh%20and%20Diana%20Liskovich%20and%20Didem%20Foss%20and%20Dingkang%20Wang%20and%20Duc%20Le%20and%20Dustin%20Holland%20and%20Edward%20Dowling%20and%20Eissa%20Jamil%20and%20Elaine%20Montgomery%20and%20Eleonora%20Presani%20and%20Emily%20Hahn%20and%20Emily%20Wood%20and%20Erik%20Brinkman%20and%20Esteban%20Arcaute%20and%20Evan%20Dunbar%20and%20Evan%20Smothers%20and%20Fei%20Sun%20and%20Felix%20Kreuk%20and%20Feng%20Tian%20and%20Firat%20Ozgenel%20and%20Francesco%20Caggioni%20and%20Francisco%20Guzm%C3%A1n%20and%20Frank%20Kanayet%20and%20Frank%20Seide%20and%20Gabriela%20Medina%20Florez%20and%20Gabriella%20Schwarz%20and%20Gada%20Badeer%20and%20Georgia%20Swee%20and%20Gil%20Halpern%20and%20Govind%20Thattai%20and%20Grant%20Herman%20and%20Grigory%20Sizov%20and%20%20Guangyi%20and%20%20Zhang%20and%20Guna%20Lakshminarayanan%20and%20Hamid%20Shojanazeri%20and%20Han%20Zou%20and%20Hannah%20Wang%20and%20Hanwen%20Zha%20and%20Haroun%20Habeeb%20and%20Harrison%20Rudolph%20and%20Helen%20Suk%20and%20Henry%20Aspegren%20and%20Hunter%20Goldman%20and%20Ibrahim%20Damlaj%20and%20Igor%20Molybog%20and%20Igor%20Tufanov%20and%20Irina-Elena%20Veliche%20and%20Itai%20Gat%20and%20Jake%20Weissman%20and%20James%20Geboski%20and%20James%20Kohli%20and%20Japhet%20Asher%20and%20Jean-Baptiste%20Gaya%20and%20Jeff%20Marcus%20and%20Jeff%20Tang%20and%20Jennifer%20Chan%20and%20Jenny%20Zhen%20and%20Jeremy%20Reizenstein%20and%20Jeremy%20Teboul%20and%20Jessica%20Zhong%20and%20Jian%20Jin%20and%20Jingyi%20Yang%20and%20Joe%20Cummings%20and%20Jon%20Carvill%20and%20Jon%20Shepard%20and%20Jonathan%20McPhie%20and%20Jonathan%20Torres%20and%20Josh%20Ginsburg%20and%20Junjie%20Wang%20and%20Kai%20Wu%20and%20Kam%20Hou%20U%20and%20Karan%20Saxena%20and%20Karthik%20Prasad%20and%20Kartikay%20Khandelwal%20and%20Katayoun%20Zand%20and%20Kathy%20Matosich%20and%20Kaushik%20Veeraraghavan%20and%20Kelly%20Michelena%20and%20Keqian%20Li%20and%20Kun%20Huang%20and%20Kunal%20Chawla%20and%20Kushal%20Lakhotia%20and%20Kyle%20Huang%20and%20Lailin%20Chen%20and%20Lakshya%20Garg%20and%20Lavender%20A%20and%20Leandro%20Silva%20and%20Lee%20Bell%20and%20Lei%20Zhang%20and%20Liangpeng%20Guo%20and%20Licheng%20Yu%20and%20Liron%20Moshkovich%20and%20Luca%20Wehrstedt%20and%20Madian%20Khabsa%20and%20Manav%20Avalani%20and%20Manish%20Bhatt%20and%20Maria%20Tsimpoukelli%20and%20Martynas%20Mankus%20and%20Matan%20Hasson%20and%20Matthew%20Lennie%20and%20Matthias%20Reso%20and%20Maxim%20Groshev%20and%20Maxim%20Naumov%20and%20Maya%20Lathi%20and%20Meghan%20Keneally%20and%20Michael%20L.%20Seltzer%20and%20Michal%20Valko%20and%20Michelle%20Restrepo%20and%20Mihir%20Patel%20and%20Mik%20Vyatskov%20and%20Mikayel%20Samvelyan%20and%20Mike%20Clark%20and%20Mike%20Macey%20and%20Mike%20Wang%20and%20Miquel%20Jubert%20Hermoso%20and%20Mo%20Metanat%20and%20Mohammad%20Rastegari%20and%20Munish%20Bansal%20and%20Nandhini%20Santhanam%20and%20Natascha%20Parks%20and%20Natasha%20White%20and%20Navyata%20Bawa%20and%20Nayan%20Singhal%20and%20Nick%20Egebo%20and%20Nicolas%20Usunier%20and%20Nikolay%20Pavlovich%20Laptev%20and%20Ning%20Dong%20and%20Ning%20Zhang%20and%20Norman%20Cheng%20and%20Oleg%20Chernoguz%20and%20Olivia%20Hart%20and%20Omkar%20Salpekar%20and%20Ozlem%20Kalinli%20and%20Parkin%20Kent%20and%20Parth%20Parekh%20and%20Paul%20Saab%20and%20Pavan%20Balaji%20and%20Pedro%20Rittner%20and%20Philip%20Bontrager%20and%20Pierre%20Roux%20and%20Piotr%20Dollar%20and%20Polina%20Zvyagina%20and%20Prashant%20Ratanchandani%20and%20Pritish%20Yuvraj%20and%20Qian%20Liang%20and%20Rachad%20Alao%20and%20Rachel%20Rodriguez%20and%20Rafi%20Ayub%20and%20Raghotham%20Murthy%20and%20Raghu%20Nayani%20and%20Rahul%20Mitra%20and%20Raymond%20Li%20and%20Rebekkah%20Hogan%20and%20Robin%20Battey%20and%20Rocky%20Wang%20and%20Rohan%20Maheswari%20and%20Russ%20Howes%20and%20Ruty%20Rinott%20and%20Sai%20Jayesh%20Bondu%20and%20Samyak%20Datta%20and%20Sara%20Chugh%20and%20Sara%20Hunt%20and%20Sargun%20Dhillon%20and%20Sasha%20Sidorov%20and%20Satadru%20Pan%20and%20Saurabh%20Verma%20and%20Seiji%20Yamamoto%20and%20Sharadh%20Ramaswamy%20and%20Shaun%20Lindsay%20and%20Shaun%20Lindsay%20and%20Sheng%20Feng%20and%20Shenghao%20Lin%20and%20Shengxin%20Cindy%20Zha%20and%20Shiva%20Shankar%20and%20Shuqiang%20Zhang%20and%20Shuqiang%20Zhang%20and%20Sinong%20Wang%20and%20Sneha%20Agarwal%20and%20Soji%20Sajuyigbe%20and%20Soumith%20Chintala%20and%20Stephanie%20Max%20and%20Stephen%20Chen%20and%20Steve%20Kehoe%20and%20Steve%20Satterfield%20and%20Sudarshan%20Govindaprasad%20and%20Sumit%20Gupta%20and%20Sungmin%20Cho%20and%20Sunny%20Virk%20and%20Suraj%20Subramanian%20and%20Sy%20Choudhury%20and%20Sydney%20Goldman%20and%20Tal%20Remez%20and%20Tamar%20Glaser%20and%20Tamara%20Best%20and%20Thilo%20Kohler%20and%20Thomas%20Robinson%20and%20Tianhe%20Li%20and%20Tianjun%20Zhang%20and%20Tim%20Matthews%20and%20Timothy%20Chou%20and%20Tzook%20Shaked%20and%20Varun%20Vontimitta%20and%20Victoria%20Ajayi%20and%20Victoria%20Montanez%20and%20Vijai%20Mohan%20and%20Vinay%20Satish%20Kumar%20and%20Vishal%20Mangla%20and%20V%C3%ADtor%20Albiero%20and%20Vlad%20Ionescu%20and%20Vlad%20Poenaru%20and%20Vlad%20Tiberiu%20Mihailescu%20and%20Vladimir%20Ivanov%20and%20Wei%20Li%20and%20Wenchen%20Wang%20and%20Wenwen%20Jiang%20and%20Wes%20Bouaziz%20and%20Will%20Constable%20and%20Xiaocheng%20Tang%20and%20Xiaofang%20Wang%20and%20Xiaojian%20Wu%20and%20Xiaolan%20Wang%20and%20Xide%20Xia%20and%20Xilun%20Wu%20and%20Xinbo%20Gao%20and%20Yanjun%20Chen%20and%20Ye%20Hu%20and%20Ye%20Jia%20and%20Ye%20Qi%20and%20Yenda%20Li%20and%20Yilin%20Zhang%20and%20Ying%20Zhang%20and%20Yossi%20Adi%20and%20Youngjin%20Nam%20and%20%20Yu%20and%20%20Wang%20and%20Yuchen%20Hao%20and%20Yundi%20Qian%20and%20Yuzi%20He%20and%20Zach%20Rait%20and%20Zachary%20DeVito%20and%20Zef%20Rosnbrick%20and%20Zhaoduo%20Wen%20and%20Zhenyu%20Yang%20and%20Zhiwei%20Zhao&entry.1292438233=%20%20Modern%20artificial%20intelligence%20%28AI%29%20systems%20are%20powered%20by%20foundation%20models.%0AThis%20paper%20presents%20a%20new%20set%20of%20foundation%20models%2C%20called%20Llama%203.%20It%20is%20a%0Aherd%20of%20language%20models%20that%20natively%20support%20multilinguality%2C%20coding%2C%0Areasoning%2C%20and%20tool%20usage.%20Our%20largest%20model%20is%20a%20dense%20Transformer%20with%20405B%0Aparameters%20and%20a%20context%20window%20of%20up%20to%20128K%20tokens.%20This%20paper%20presents%20an%0Aextensive%20empirical%20evaluation%20of%20Llama%203.%20We%20find%20that%20Llama%203%20delivers%0Acomparable%20quality%20to%20leading%20language%20models%20such%20as%20GPT-4%20on%20a%20plethora%20of%0Atasks.%20We%20publicly%20release%20Llama%203%2C%20including%20pre-trained%20and%20post-trained%0Aversions%20of%20the%20405B%20parameter%20language%20model%20and%20our%20Llama%20Guard%203%20model%20for%0Ainput%20and%20output%20safety.%20The%20paper%20also%20presents%20the%20results%20of%20experiments%20in%0Awhich%20we%20integrate%20image%2C%20video%2C%20and%20speech%20capabilities%20into%20Llama%203%20via%20a%0Acompositional%20approach.%20We%20observe%20this%20approach%20performs%20competitively%20with%0Athe%20state-of-the-art%20on%20image%2C%20video%2C%20and%20speech%20recognition%20tasks.%20The%0Aresulting%20models%20are%20not%20yet%20being%20broadly%20released%20as%20they%20are%20still%20under%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21783v2&entry.124074799=Read"},
{"title": "Problem Solving Through Human-AI Preference-Based Cooperation", "author": "Subhabrata Dutta and Timo Kaufmann and Goran Glava\u0161 and Ivan Habernal and Kristian Kersting and Frauke Kreuter and Mira Mezini and Iryna Gurevych and Eyke H\u00fcllermeier and Hinrich Schuetze", "abstract": "  While there is a widespread belief that artificial general intelligence (AGI)\n-- or even superhuman AI -- is imminent, complex problems in expert domains are\nfar from being solved. We argue that such problems require human-AI cooperation\nand that the current state of the art in generative AI is unable to play the\nrole of a reliable partner due to a multitude of shortcomings, including\ninability to keep track of a complex solution artifact (e.g., a software\nprogram), limited support for versatile human preference expression and lack of\nadapting to human preference in an interactive setting. To address these\nchallenges, we propose HAI-Co2, a novel human-AI co-construction framework. We\nformalize HAI-Co2 and discuss the difficult open research problems that it\nfaces. Finally, we present a case study of HAI-Co2 and demonstrate its efficacy\ncompared to monolithic generative AI models.\n", "link": "http://arxiv.org/abs/2408.07461v2", "date": "2024-08-15", "relevancy": 1.9439, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4998}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4949}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Problem%20Solving%20Through%20Human-AI%20Preference-Based%20Cooperation&body=Title%3A%20Problem%20Solving%20Through%20Human-AI%20Preference-Based%20Cooperation%0AAuthor%3A%20Subhabrata%20Dutta%20and%20Timo%20Kaufmann%20and%20Goran%20Glava%C5%A1%20and%20Ivan%20Habernal%20and%20Kristian%20Kersting%20and%20Frauke%20Kreuter%20and%20Mira%20Mezini%20and%20Iryna%20Gurevych%20and%20Eyke%20H%C3%BCllermeier%20and%20Hinrich%20Schuetze%0AAbstract%3A%20%20%20While%20there%20is%20a%20widespread%20belief%20that%20artificial%20general%20intelligence%20%28AGI%29%0A--%20or%20even%20superhuman%20AI%20--%20is%20imminent%2C%20complex%20problems%20in%20expert%20domains%20are%0Afar%20from%20being%20solved.%20We%20argue%20that%20such%20problems%20require%20human-AI%20cooperation%0Aand%20that%20the%20current%20state%20of%20the%20art%20in%20generative%20AI%20is%20unable%20to%20play%20the%0Arole%20of%20a%20reliable%20partner%20due%20to%20a%20multitude%20of%20shortcomings%2C%20including%0Ainability%20to%20keep%20track%20of%20a%20complex%20solution%20artifact%20%28e.g.%2C%20a%20software%0Aprogram%29%2C%20limited%20support%20for%20versatile%20human%20preference%20expression%20and%20lack%20of%0Aadapting%20to%20human%20preference%20in%20an%20interactive%20setting.%20To%20address%20these%0Achallenges%2C%20we%20propose%20HAI-Co2%2C%20a%20novel%20human-AI%20co-construction%20framework.%20We%0Aformalize%20HAI-Co2%20and%20discuss%20the%20difficult%20open%20research%20problems%20that%20it%0Afaces.%20Finally%2C%20we%20present%20a%20case%20study%20of%20HAI-Co2%20and%20demonstrate%20its%20efficacy%0Acompared%20to%20monolithic%20generative%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProblem%2520Solving%2520Through%2520Human-AI%2520Preference-Based%2520Cooperation%26entry.906535625%3DSubhabrata%2520Dutta%2520and%2520Timo%2520Kaufmann%2520and%2520Goran%2520Glava%25C5%25A1%2520and%2520Ivan%2520Habernal%2520and%2520Kristian%2520Kersting%2520and%2520Frauke%2520Kreuter%2520and%2520Mira%2520Mezini%2520and%2520Iryna%2520Gurevych%2520and%2520Eyke%2520H%25C3%25BCllermeier%2520and%2520Hinrich%2520Schuetze%26entry.1292438233%3D%2520%2520While%2520there%2520is%2520a%2520widespread%2520belief%2520that%2520artificial%2520general%2520intelligence%2520%2528AGI%2529%250A--%2520or%2520even%2520superhuman%2520AI%2520--%2520is%2520imminent%252C%2520complex%2520problems%2520in%2520expert%2520domains%2520are%250Afar%2520from%2520being%2520solved.%2520We%2520argue%2520that%2520such%2520problems%2520require%2520human-AI%2520cooperation%250Aand%2520that%2520the%2520current%2520state%2520of%2520the%2520art%2520in%2520generative%2520AI%2520is%2520unable%2520to%2520play%2520the%250Arole%2520of%2520a%2520reliable%2520partner%2520due%2520to%2520a%2520multitude%2520of%2520shortcomings%252C%2520including%250Ainability%2520to%2520keep%2520track%2520of%2520a%2520complex%2520solution%2520artifact%2520%2528e.g.%252C%2520a%2520software%250Aprogram%2529%252C%2520limited%2520support%2520for%2520versatile%2520human%2520preference%2520expression%2520and%2520lack%2520of%250Aadapting%2520to%2520human%2520preference%2520in%2520an%2520interactive%2520setting.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520HAI-Co2%252C%2520a%2520novel%2520human-AI%2520co-construction%2520framework.%2520We%250Aformalize%2520HAI-Co2%2520and%2520discuss%2520the%2520difficult%2520open%2520research%2520problems%2520that%2520it%250Afaces.%2520Finally%252C%2520we%2520present%2520a%2520case%2520study%2520of%2520HAI-Co2%2520and%2520demonstrate%2520its%2520efficacy%250Acompared%2520to%2520monolithic%2520generative%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Problem%20Solving%20Through%20Human-AI%20Preference-Based%20Cooperation&entry.906535625=Subhabrata%20Dutta%20and%20Timo%20Kaufmann%20and%20Goran%20Glava%C5%A1%20and%20Ivan%20Habernal%20and%20Kristian%20Kersting%20and%20Frauke%20Kreuter%20and%20Mira%20Mezini%20and%20Iryna%20Gurevych%20and%20Eyke%20H%C3%BCllermeier%20and%20Hinrich%20Schuetze&entry.1292438233=%20%20While%20there%20is%20a%20widespread%20belief%20that%20artificial%20general%20intelligence%20%28AGI%29%0A--%20or%20even%20superhuman%20AI%20--%20is%20imminent%2C%20complex%20problems%20in%20expert%20domains%20are%0Afar%20from%20being%20solved.%20We%20argue%20that%20such%20problems%20require%20human-AI%20cooperation%0Aand%20that%20the%20current%20state%20of%20the%20art%20in%20generative%20AI%20is%20unable%20to%20play%20the%0Arole%20of%20a%20reliable%20partner%20due%20to%20a%20multitude%20of%20shortcomings%2C%20including%0Ainability%20to%20keep%20track%20of%20a%20complex%20solution%20artifact%20%28e.g.%2C%20a%20software%0Aprogram%29%2C%20limited%20support%20for%20versatile%20human%20preference%20expression%20and%20lack%20of%0Aadapting%20to%20human%20preference%20in%20an%20interactive%20setting.%20To%20address%20these%0Achallenges%2C%20we%20propose%20HAI-Co2%2C%20a%20novel%20human-AI%20co-construction%20framework.%20We%0Aformalize%20HAI-Co2%20and%20discuss%20the%20difficult%20open%20research%20problems%20that%20it%0Afaces.%20Finally%2C%20we%20present%20a%20case%20study%20of%20HAI-Co2%20and%20demonstrate%20its%20efficacy%0Acompared%20to%20monolithic%20generative%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07461v2&entry.124074799=Read"},
{"title": "Rethinking Medical Anomaly Detection in Brain MRI: An Image Quality\n  Assessment Perspective", "author": "Zixuan Pan and Jun Xia and Zheyu Yan and Guoyue Xu and Yawen Wu and Zhenge Jia and Jianxu Chen and Yiyu Shi", "abstract": "  Reconstruction-based methods, particularly those leveraging autoencoders,\nhave been widely adopted to perform anomaly detection in brain MRI. While most\nexisting works try to improve detection accuracy by proposing new model\nstructures or algorithms, we tackle the problem through image quality\nassessment, an underexplored perspective in the field. We propose a fusion\nquality loss function that combines Structural Similarity Index Measure loss\nwith l1 loss, offering a more comprehensive evaluation of reconstruction\nquality. Additionally, we introduce a data pre-processing strategy that\nenhances the average intensity ratio (AIR) between normal and abnormal regions,\nfurther improving the distinction of anomalies. By fusing the aforementioned\ntwo methods, we devise the image quality assessment (IQA) approach. The\nproposed IQA approach achieves significant improvements (>10%) in terms of Dice\ncoefficient (DICE) and Area Under the Precision-Recall Curve (AUPRC) on the\nBraTS21 (T2, FLAIR) and MSULB datasets when compared with state-of-the-art\nmethods. These results highlight the importance of invoking the comprehensive\nimage quality assessment in medical anomaly detection and provide a new\nperspective for future research in this field.\n", "link": "http://arxiv.org/abs/2408.08228v1", "date": "2024-08-15", "relevancy": 1.9421, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5237}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4917}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Medical%20Anomaly%20Detection%20in%20Brain%20MRI%3A%20An%20Image%20Quality%0A%20%20Assessment%20Perspective&body=Title%3A%20Rethinking%20Medical%20Anomaly%20Detection%20in%20Brain%20MRI%3A%20An%20Image%20Quality%0A%20%20Assessment%20Perspective%0AAuthor%3A%20Zixuan%20Pan%20and%20Jun%20Xia%20and%20Zheyu%20Yan%20and%20Guoyue%20Xu%20and%20Yawen%20Wu%20and%20Zhenge%20Jia%20and%20Jianxu%20Chen%20and%20Yiyu%20Shi%0AAbstract%3A%20%20%20Reconstruction-based%20methods%2C%20particularly%20those%20leveraging%20autoencoders%2C%0Ahave%20been%20widely%20adopted%20to%20perform%20anomaly%20detection%20in%20brain%20MRI.%20While%20most%0Aexisting%20works%20try%20to%20improve%20detection%20accuracy%20by%20proposing%20new%20model%0Astructures%20or%20algorithms%2C%20we%20tackle%20the%20problem%20through%20image%20quality%0Aassessment%2C%20an%20underexplored%20perspective%20in%20the%20field.%20We%20propose%20a%20fusion%0Aquality%20loss%20function%20that%20combines%20Structural%20Similarity%20Index%20Measure%20loss%0Awith%20l1%20loss%2C%20offering%20a%20more%20comprehensive%20evaluation%20of%20reconstruction%0Aquality.%20Additionally%2C%20we%20introduce%20a%20data%20pre-processing%20strategy%20that%0Aenhances%20the%20average%20intensity%20ratio%20%28AIR%29%20between%20normal%20and%20abnormal%20regions%2C%0Afurther%20improving%20the%20distinction%20of%20anomalies.%20By%20fusing%20the%20aforementioned%0Atwo%20methods%2C%20we%20devise%20the%20image%20quality%20assessment%20%28IQA%29%20approach.%20The%0Aproposed%20IQA%20approach%20achieves%20significant%20improvements%20%28%3E10%25%29%20in%20terms%20of%20Dice%0Acoefficient%20%28DICE%29%20and%20Area%20Under%20the%20Precision-Recall%20Curve%20%28AUPRC%29%20on%20the%0ABraTS21%20%28T2%2C%20FLAIR%29%20and%20MSULB%20datasets%20when%20compared%20with%20state-of-the-art%0Amethods.%20These%20results%20highlight%20the%20importance%20of%20invoking%20the%20comprehensive%0Aimage%20quality%20assessment%20in%20medical%20anomaly%20detection%20and%20provide%20a%20new%0Aperspective%20for%20future%20research%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Medical%2520Anomaly%2520Detection%2520in%2520Brain%2520MRI%253A%2520An%2520Image%2520Quality%250A%2520%2520Assessment%2520Perspective%26entry.906535625%3DZixuan%2520Pan%2520and%2520Jun%2520Xia%2520and%2520Zheyu%2520Yan%2520and%2520Guoyue%2520Xu%2520and%2520Yawen%2520Wu%2520and%2520Zhenge%2520Jia%2520and%2520Jianxu%2520Chen%2520and%2520Yiyu%2520Shi%26entry.1292438233%3D%2520%2520Reconstruction-based%2520methods%252C%2520particularly%2520those%2520leveraging%2520autoencoders%252C%250Ahave%2520been%2520widely%2520adopted%2520to%2520perform%2520anomaly%2520detection%2520in%2520brain%2520MRI.%2520While%2520most%250Aexisting%2520works%2520try%2520to%2520improve%2520detection%2520accuracy%2520by%2520proposing%2520new%2520model%250Astructures%2520or%2520algorithms%252C%2520we%2520tackle%2520the%2520problem%2520through%2520image%2520quality%250Aassessment%252C%2520an%2520underexplored%2520perspective%2520in%2520the%2520field.%2520We%2520propose%2520a%2520fusion%250Aquality%2520loss%2520function%2520that%2520combines%2520Structural%2520Similarity%2520Index%2520Measure%2520loss%250Awith%2520l1%2520loss%252C%2520offering%2520a%2520more%2520comprehensive%2520evaluation%2520of%2520reconstruction%250Aquality.%2520Additionally%252C%2520we%2520introduce%2520a%2520data%2520pre-processing%2520strategy%2520that%250Aenhances%2520the%2520average%2520intensity%2520ratio%2520%2528AIR%2529%2520between%2520normal%2520and%2520abnormal%2520regions%252C%250Afurther%2520improving%2520the%2520distinction%2520of%2520anomalies.%2520By%2520fusing%2520the%2520aforementioned%250Atwo%2520methods%252C%2520we%2520devise%2520the%2520image%2520quality%2520assessment%2520%2528IQA%2529%2520approach.%2520The%250Aproposed%2520IQA%2520approach%2520achieves%2520significant%2520improvements%2520%2528%253E10%2525%2529%2520in%2520terms%2520of%2520Dice%250Acoefficient%2520%2528DICE%2529%2520and%2520Area%2520Under%2520the%2520Precision-Recall%2520Curve%2520%2528AUPRC%2529%2520on%2520the%250ABraTS21%2520%2528T2%252C%2520FLAIR%2529%2520and%2520MSULB%2520datasets%2520when%2520compared%2520with%2520state-of-the-art%250Amethods.%2520These%2520results%2520highlight%2520the%2520importance%2520of%2520invoking%2520the%2520comprehensive%250Aimage%2520quality%2520assessment%2520in%2520medical%2520anomaly%2520detection%2520and%2520provide%2520a%2520new%250Aperspective%2520for%2520future%2520research%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Medical%20Anomaly%20Detection%20in%20Brain%20MRI%3A%20An%20Image%20Quality%0A%20%20Assessment%20Perspective&entry.906535625=Zixuan%20Pan%20and%20Jun%20Xia%20and%20Zheyu%20Yan%20and%20Guoyue%20Xu%20and%20Yawen%20Wu%20and%20Zhenge%20Jia%20and%20Jianxu%20Chen%20and%20Yiyu%20Shi&entry.1292438233=%20%20Reconstruction-based%20methods%2C%20particularly%20those%20leveraging%20autoencoders%2C%0Ahave%20been%20widely%20adopted%20to%20perform%20anomaly%20detection%20in%20brain%20MRI.%20While%20most%0Aexisting%20works%20try%20to%20improve%20detection%20accuracy%20by%20proposing%20new%20model%0Astructures%20or%20algorithms%2C%20we%20tackle%20the%20problem%20through%20image%20quality%0Aassessment%2C%20an%20underexplored%20perspective%20in%20the%20field.%20We%20propose%20a%20fusion%0Aquality%20loss%20function%20that%20combines%20Structural%20Similarity%20Index%20Measure%20loss%0Awith%20l1%20loss%2C%20offering%20a%20more%20comprehensive%20evaluation%20of%20reconstruction%0Aquality.%20Additionally%2C%20we%20introduce%20a%20data%20pre-processing%20strategy%20that%0Aenhances%20the%20average%20intensity%20ratio%20%28AIR%29%20between%20normal%20and%20abnormal%20regions%2C%0Afurther%20improving%20the%20distinction%20of%20anomalies.%20By%20fusing%20the%20aforementioned%0Atwo%20methods%2C%20we%20devise%20the%20image%20quality%20assessment%20%28IQA%29%20approach.%20The%0Aproposed%20IQA%20approach%20achieves%20significant%20improvements%20%28%3E10%25%29%20in%20terms%20of%20Dice%0Acoefficient%20%28DICE%29%20and%20Area%20Under%20the%20Precision-Recall%20Curve%20%28AUPRC%29%20on%20the%0ABraTS21%20%28T2%2C%20FLAIR%29%20and%20MSULB%20datasets%20when%20compared%20with%20state-of-the-art%0Amethods.%20These%20results%20highlight%20the%20importance%20of%20invoking%20the%20comprehensive%0Aimage%20quality%20assessment%20in%20medical%20anomaly%20detection%20and%20provide%20a%20new%0Aperspective%20for%20future%20research%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08228v1&entry.124074799=Read"},
{"title": "Explaining an Agent's Future Beliefs through Temporally Decomposing\n  Future Reward Estimators", "author": "Mark Towers and Yali Du and Christopher Freeman and Timothy J. Norman", "abstract": "  Future reward estimation is a core component of reinforcement learning\nagents; i.e., Q-value and state-value functions, predicting an agent's sum of\nfuture rewards. Their scalar output, however, obfuscates when or what\nindividual future rewards an agent may expect to receive. We address this by\nmodifying an agent's future reward estimator to predict their next N expected\nrewards, referred to as Temporal Reward Decomposition (TRD). This unlocks novel\nexplanations of agent behaviour. Through TRD we can: estimate when an agent may\nexpect to receive a reward, the value of the reward and the agent's confidence\nin receiving it; measure an input feature's temporal importance to the agent's\naction decisions; and predict the influence of different actions on future\nrewards. Furthermore, we show that DQN agents trained on Atari environments can\nbe efficiently retrained to incorporate TRD with minimal impact on performance.\n", "link": "http://arxiv.org/abs/2408.08230v1", "date": "2024-08-15", "relevancy": 1.9365, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5066}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4863}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20an%20Agent%27s%20Future%20Beliefs%20through%20Temporally%20Decomposing%0A%20%20Future%20Reward%20Estimators&body=Title%3A%20Explaining%20an%20Agent%27s%20Future%20Beliefs%20through%20Temporally%20Decomposing%0A%20%20Future%20Reward%20Estimators%0AAuthor%3A%20Mark%20Towers%20and%20Yali%20Du%20and%20Christopher%20Freeman%20and%20Timothy%20J.%20Norman%0AAbstract%3A%20%20%20Future%20reward%20estimation%20is%20a%20core%20component%20of%20reinforcement%20learning%0Aagents%3B%20i.e.%2C%20Q-value%20and%20state-value%20functions%2C%20predicting%20an%20agent%27s%20sum%20of%0Afuture%20rewards.%20Their%20scalar%20output%2C%20however%2C%20obfuscates%20when%20or%20what%0Aindividual%20future%20rewards%20an%20agent%20may%20expect%20to%20receive.%20We%20address%20this%20by%0Amodifying%20an%20agent%27s%20future%20reward%20estimator%20to%20predict%20their%20next%20N%20expected%0Arewards%2C%20referred%20to%20as%20Temporal%20Reward%20Decomposition%20%28TRD%29.%20This%20unlocks%20novel%0Aexplanations%20of%20agent%20behaviour.%20Through%20TRD%20we%20can%3A%20estimate%20when%20an%20agent%20may%0Aexpect%20to%20receive%20a%20reward%2C%20the%20value%20of%20the%20reward%20and%20the%20agent%27s%20confidence%0Ain%20receiving%20it%3B%20measure%20an%20input%20feature%27s%20temporal%20importance%20to%20the%20agent%27s%0Aaction%20decisions%3B%20and%20predict%20the%20influence%20of%20different%20actions%20on%20future%0Arewards.%20Furthermore%2C%20we%20show%20that%20DQN%20agents%20trained%20on%20Atari%20environments%20can%0Abe%20efficiently%20retrained%20to%20incorporate%20TRD%20with%20minimal%20impact%20on%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520an%2520Agent%2527s%2520Future%2520Beliefs%2520through%2520Temporally%2520Decomposing%250A%2520%2520Future%2520Reward%2520Estimators%26entry.906535625%3DMark%2520Towers%2520and%2520Yali%2520Du%2520and%2520Christopher%2520Freeman%2520and%2520Timothy%2520J.%2520Norman%26entry.1292438233%3D%2520%2520Future%2520reward%2520estimation%2520is%2520a%2520core%2520component%2520of%2520reinforcement%2520learning%250Aagents%253B%2520i.e.%252C%2520Q-value%2520and%2520state-value%2520functions%252C%2520predicting%2520an%2520agent%2527s%2520sum%2520of%250Afuture%2520rewards.%2520Their%2520scalar%2520output%252C%2520however%252C%2520obfuscates%2520when%2520or%2520what%250Aindividual%2520future%2520rewards%2520an%2520agent%2520may%2520expect%2520to%2520receive.%2520We%2520address%2520this%2520by%250Amodifying%2520an%2520agent%2527s%2520future%2520reward%2520estimator%2520to%2520predict%2520their%2520next%2520N%2520expected%250Arewards%252C%2520referred%2520to%2520as%2520Temporal%2520Reward%2520Decomposition%2520%2528TRD%2529.%2520This%2520unlocks%2520novel%250Aexplanations%2520of%2520agent%2520behaviour.%2520Through%2520TRD%2520we%2520can%253A%2520estimate%2520when%2520an%2520agent%2520may%250Aexpect%2520to%2520receive%2520a%2520reward%252C%2520the%2520value%2520of%2520the%2520reward%2520and%2520the%2520agent%2527s%2520confidence%250Ain%2520receiving%2520it%253B%2520measure%2520an%2520input%2520feature%2527s%2520temporal%2520importance%2520to%2520the%2520agent%2527s%250Aaction%2520decisions%253B%2520and%2520predict%2520the%2520influence%2520of%2520different%2520actions%2520on%2520future%250Arewards.%2520Furthermore%252C%2520we%2520show%2520that%2520DQN%2520agents%2520trained%2520on%2520Atari%2520environments%2520can%250Abe%2520efficiently%2520retrained%2520to%2520incorporate%2520TRD%2520with%2520minimal%2520impact%2520on%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20an%20Agent%27s%20Future%20Beliefs%20through%20Temporally%20Decomposing%0A%20%20Future%20Reward%20Estimators&entry.906535625=Mark%20Towers%20and%20Yali%20Du%20and%20Christopher%20Freeman%20and%20Timothy%20J.%20Norman&entry.1292438233=%20%20Future%20reward%20estimation%20is%20a%20core%20component%20of%20reinforcement%20learning%0Aagents%3B%20i.e.%2C%20Q-value%20and%20state-value%20functions%2C%20predicting%20an%20agent%27s%20sum%20of%0Afuture%20rewards.%20Their%20scalar%20output%2C%20however%2C%20obfuscates%20when%20or%20what%0Aindividual%20future%20rewards%20an%20agent%20may%20expect%20to%20receive.%20We%20address%20this%20by%0Amodifying%20an%20agent%27s%20future%20reward%20estimator%20to%20predict%20their%20next%20N%20expected%0Arewards%2C%20referred%20to%20as%20Temporal%20Reward%20Decomposition%20%28TRD%29.%20This%20unlocks%20novel%0Aexplanations%20of%20agent%20behaviour.%20Through%20TRD%20we%20can%3A%20estimate%20when%20an%20agent%20may%0Aexpect%20to%20receive%20a%20reward%2C%20the%20value%20of%20the%20reward%20and%20the%20agent%27s%20confidence%0Ain%20receiving%20it%3B%20measure%20an%20input%20feature%27s%20temporal%20importance%20to%20the%20agent%27s%0Aaction%20decisions%3B%20and%20predict%20the%20influence%20of%20different%20actions%20on%20future%0Arewards.%20Furthermore%2C%20we%20show%20that%20DQN%20agents%20trained%20on%20Atari%20environments%20can%0Abe%20efficiently%20retrained%20to%20incorporate%20TRD%20with%20minimal%20impact%20on%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08230v1&entry.124074799=Read"},
{"title": "Deep Learning Innovations for Underwater Waste Detection: An In-Depth\n  Analysis", "author": "Jaskaran Singh Walia and Pavithra L K", "abstract": "  Addressing the issue of submerged underwater trash is crucial for\nsafeguarding aquatic ecosystems and preserving marine life. While identifying\ndebris present on the surface of water bodies is straightforward, assessing the\nunderwater submerged waste is a challenge due to the image distortions caused\nby factors such as light refraction, absorption, suspended particles, color\nshifts, and occlusion. This paper conducts a comprehensive review of\nstate-of-the-art architectures and on the existing datasets to establish a\nbaseline for submerged waste and trash detection. The primary goal remains to\nestablish the benchmark of the object localization techniques to be leveraged\nby advanced underwater sensors and autonomous underwater vehicles. The ultimate\nobjective is to explore the underwater environment, to identify, and remove\nunderwater debris. The absence of benchmarks (dataset or algorithm) in many\nresearches emphasizes the need for a more robust algorithmic solution. Through\nthis research, we aim to give performance comparative analysis of various\nunderwater trash detection algorithms.\n", "link": "http://arxiv.org/abs/2405.18299v2", "date": "2024-08-15", "relevancy": 1.9261, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.521}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4907}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Innovations%20for%20Underwater%20Waste%20Detection%3A%20An%20In-Depth%0A%20%20Analysis&body=Title%3A%20Deep%20Learning%20Innovations%20for%20Underwater%20Waste%20Detection%3A%20An%20In-Depth%0A%20%20Analysis%0AAuthor%3A%20Jaskaran%20Singh%20Walia%20and%20Pavithra%20L%20K%0AAbstract%3A%20%20%20Addressing%20the%20issue%20of%20submerged%20underwater%20trash%20is%20crucial%20for%0Asafeguarding%20aquatic%20ecosystems%20and%20preserving%20marine%20life.%20While%20identifying%0Adebris%20present%20on%20the%20surface%20of%20water%20bodies%20is%20straightforward%2C%20assessing%20the%0Aunderwater%20submerged%20waste%20is%20a%20challenge%20due%20to%20the%20image%20distortions%20caused%0Aby%20factors%20such%20as%20light%20refraction%2C%20absorption%2C%20suspended%20particles%2C%20color%0Ashifts%2C%20and%20occlusion.%20This%20paper%20conducts%20a%20comprehensive%20review%20of%0Astate-of-the-art%20architectures%20and%20on%20the%20existing%20datasets%20to%20establish%20a%0Abaseline%20for%20submerged%20waste%20and%20trash%20detection.%20The%20primary%20goal%20remains%20to%0Aestablish%20the%20benchmark%20of%20the%20object%20localization%20techniques%20to%20be%20leveraged%0Aby%20advanced%20underwater%20sensors%20and%20autonomous%20underwater%20vehicles.%20The%20ultimate%0Aobjective%20is%20to%20explore%20the%20underwater%20environment%2C%20to%20identify%2C%20and%20remove%0Aunderwater%20debris.%20The%20absence%20of%20benchmarks%20%28dataset%20or%20algorithm%29%20in%20many%0Aresearches%20emphasizes%20the%20need%20for%20a%20more%20robust%20algorithmic%20solution.%20Through%0Athis%20research%2C%20we%20aim%20to%20give%20performance%20comparative%20analysis%20of%20various%0Aunderwater%20trash%20detection%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Innovations%2520for%2520Underwater%2520Waste%2520Detection%253A%2520An%2520In-Depth%250A%2520%2520Analysis%26entry.906535625%3DJaskaran%2520Singh%2520Walia%2520and%2520Pavithra%2520L%2520K%26entry.1292438233%3D%2520%2520Addressing%2520the%2520issue%2520of%2520submerged%2520underwater%2520trash%2520is%2520crucial%2520for%250Asafeguarding%2520aquatic%2520ecosystems%2520and%2520preserving%2520marine%2520life.%2520While%2520identifying%250Adebris%2520present%2520on%2520the%2520surface%2520of%2520water%2520bodies%2520is%2520straightforward%252C%2520assessing%2520the%250Aunderwater%2520submerged%2520waste%2520is%2520a%2520challenge%2520due%2520to%2520the%2520image%2520distortions%2520caused%250Aby%2520factors%2520such%2520as%2520light%2520refraction%252C%2520absorption%252C%2520suspended%2520particles%252C%2520color%250Ashifts%252C%2520and%2520occlusion.%2520This%2520paper%2520conducts%2520a%2520comprehensive%2520review%2520of%250Astate-of-the-art%2520architectures%2520and%2520on%2520the%2520existing%2520datasets%2520to%2520establish%2520a%250Abaseline%2520for%2520submerged%2520waste%2520and%2520trash%2520detection.%2520The%2520primary%2520goal%2520remains%2520to%250Aestablish%2520the%2520benchmark%2520of%2520the%2520object%2520localization%2520techniques%2520to%2520be%2520leveraged%250Aby%2520advanced%2520underwater%2520sensors%2520and%2520autonomous%2520underwater%2520vehicles.%2520The%2520ultimate%250Aobjective%2520is%2520to%2520explore%2520the%2520underwater%2520environment%252C%2520to%2520identify%252C%2520and%2520remove%250Aunderwater%2520debris.%2520The%2520absence%2520of%2520benchmarks%2520%2528dataset%2520or%2520algorithm%2529%2520in%2520many%250Aresearches%2520emphasizes%2520the%2520need%2520for%2520a%2520more%2520robust%2520algorithmic%2520solution.%2520Through%250Athis%2520research%252C%2520we%2520aim%2520to%2520give%2520performance%2520comparative%2520analysis%2520of%2520various%250Aunderwater%2520trash%2520detection%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Innovations%20for%20Underwater%20Waste%20Detection%3A%20An%20In-Depth%0A%20%20Analysis&entry.906535625=Jaskaran%20Singh%20Walia%20and%20Pavithra%20L%20K&entry.1292438233=%20%20Addressing%20the%20issue%20of%20submerged%20underwater%20trash%20is%20crucial%20for%0Asafeguarding%20aquatic%20ecosystems%20and%20preserving%20marine%20life.%20While%20identifying%0Adebris%20present%20on%20the%20surface%20of%20water%20bodies%20is%20straightforward%2C%20assessing%20the%0Aunderwater%20submerged%20waste%20is%20a%20challenge%20due%20to%20the%20image%20distortions%20caused%0Aby%20factors%20such%20as%20light%20refraction%2C%20absorption%2C%20suspended%20particles%2C%20color%0Ashifts%2C%20and%20occlusion.%20This%20paper%20conducts%20a%20comprehensive%20review%20of%0Astate-of-the-art%20architectures%20and%20on%20the%20existing%20datasets%20to%20establish%20a%0Abaseline%20for%20submerged%20waste%20and%20trash%20detection.%20The%20primary%20goal%20remains%20to%0Aestablish%20the%20benchmark%20of%20the%20object%20localization%20techniques%20to%20be%20leveraged%0Aby%20advanced%20underwater%20sensors%20and%20autonomous%20underwater%20vehicles.%20The%20ultimate%0Aobjective%20is%20to%20explore%20the%20underwater%20environment%2C%20to%20identify%2C%20and%20remove%0Aunderwater%20debris.%20The%20absence%20of%20benchmarks%20%28dataset%20or%20algorithm%29%20in%20many%0Aresearches%20emphasizes%20the%20need%20for%20a%20more%20robust%20algorithmic%20solution.%20Through%0Athis%20research%2C%20we%20aim%20to%20give%20performance%20comparative%20analysis%20of%20various%0Aunderwater%20trash%20detection%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18299v2&entry.124074799=Read"},
{"title": "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented\n  Generation", "author": "Dongyu Ru and Lin Qiu and Xiangkun Hu and Tianhang Zhang and Peng Shi and Shuaichen Chang and Jiayang Cheng and Cunxiang Wang and Shichao Sun and Huanyu Li and Zizhao Zhang and Binjie Wang and Jiarong Jiang and Tong He and Zhiguo Wang and Pengfei Liu and Yue Zhang and Zheng Zhang", "abstract": "  Despite Retrieval-Augmented Generation (RAG) has shown promising capability\nin leveraging external knowledge, a comprehensive evaluation of RAG systems is\nstill challenging due to the modular nature of RAG, evaluation of long-form\nresponses and reliability of measurements. In this paper, we propose a\nfine-grained evaluation framework, RAGChecker, that incorporates a suite of\ndiagnostic metrics for both the retrieval and generation modules. Meta\nevaluation verifies that RAGChecker has significantly better correlations with\nhuman judgments than other evaluation metrics. Using RAGChecker, we evaluate 8\nRAG systems and conduct an in-depth analysis of their performance, revealing\ninsightful patterns and trade-offs in the design choices of RAG architectures.\nThe metrics of RAGChecker can guide researchers and practitioners in developing\nmore effective RAG systems.\n", "link": "http://arxiv.org/abs/2408.08067v1", "date": "2024-08-15", "relevancy": 1.9053, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4837}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4741}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAGChecker%3A%20A%20Fine-grained%20Framework%20for%20Diagnosing%20Retrieval-Augmented%0A%20%20Generation&body=Title%3A%20RAGChecker%3A%20A%20Fine-grained%20Framework%20for%20Diagnosing%20Retrieval-Augmented%0A%20%20Generation%0AAuthor%3A%20Dongyu%20Ru%20and%20Lin%20Qiu%20and%20Xiangkun%20Hu%20and%20Tianhang%20Zhang%20and%20Peng%20Shi%20and%20Shuaichen%20Chang%20and%20Jiayang%20Cheng%20and%20Cunxiang%20Wang%20and%20Shichao%20Sun%20and%20Huanyu%20Li%20and%20Zizhao%20Zhang%20and%20Binjie%20Wang%20and%20Jiarong%20Jiang%20and%20Tong%20He%20and%20Zhiguo%20Wang%20and%20Pengfei%20Liu%20and%20Yue%20Zhang%20and%20Zheng%20Zhang%0AAbstract%3A%20%20%20Despite%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20shown%20promising%20capability%0Ain%20leveraging%20external%20knowledge%2C%20a%20comprehensive%20evaluation%20of%20RAG%20systems%20is%0Astill%20challenging%20due%20to%20the%20modular%20nature%20of%20RAG%2C%20evaluation%20of%20long-form%0Aresponses%20and%20reliability%20of%20measurements.%20In%20this%20paper%2C%20we%20propose%20a%0Afine-grained%20evaluation%20framework%2C%20RAGChecker%2C%20that%20incorporates%20a%20suite%20of%0Adiagnostic%20metrics%20for%20both%20the%20retrieval%20and%20generation%20modules.%20Meta%0Aevaluation%20verifies%20that%20RAGChecker%20has%20significantly%20better%20correlations%20with%0Ahuman%20judgments%20than%20other%20evaluation%20metrics.%20Using%20RAGChecker%2C%20we%20evaluate%208%0ARAG%20systems%20and%20conduct%20an%20in-depth%20analysis%20of%20their%20performance%2C%20revealing%0Ainsightful%20patterns%20and%20trade-offs%20in%20the%20design%20choices%20of%20RAG%20architectures.%0AThe%20metrics%20of%20RAGChecker%20can%20guide%20researchers%20and%20practitioners%20in%20developing%0Amore%20effective%20RAG%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAGChecker%253A%2520A%2520Fine-grained%2520Framework%2520for%2520Diagnosing%2520Retrieval-Augmented%250A%2520%2520Generation%26entry.906535625%3DDongyu%2520Ru%2520and%2520Lin%2520Qiu%2520and%2520Xiangkun%2520Hu%2520and%2520Tianhang%2520Zhang%2520and%2520Peng%2520Shi%2520and%2520Shuaichen%2520Chang%2520and%2520Jiayang%2520Cheng%2520and%2520Cunxiang%2520Wang%2520and%2520Shichao%2520Sun%2520and%2520Huanyu%2520Li%2520and%2520Zizhao%2520Zhang%2520and%2520Binjie%2520Wang%2520and%2520Jiarong%2520Jiang%2520and%2520Tong%2520He%2520and%2520Zhiguo%2520Wang%2520and%2520Pengfei%2520Liu%2520and%2520Yue%2520Zhang%2520and%2520Zheng%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520shown%2520promising%2520capability%250Ain%2520leveraging%2520external%2520knowledge%252C%2520a%2520comprehensive%2520evaluation%2520of%2520RAG%2520systems%2520is%250Astill%2520challenging%2520due%2520to%2520the%2520modular%2520nature%2520of%2520RAG%252C%2520evaluation%2520of%2520long-form%250Aresponses%2520and%2520reliability%2520of%2520measurements.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Afine-grained%2520evaluation%2520framework%252C%2520RAGChecker%252C%2520that%2520incorporates%2520a%2520suite%2520of%250Adiagnostic%2520metrics%2520for%2520both%2520the%2520retrieval%2520and%2520generation%2520modules.%2520Meta%250Aevaluation%2520verifies%2520that%2520RAGChecker%2520has%2520significantly%2520better%2520correlations%2520with%250Ahuman%2520judgments%2520than%2520other%2520evaluation%2520metrics.%2520Using%2520RAGChecker%252C%2520we%2520evaluate%25208%250ARAG%2520systems%2520and%2520conduct%2520an%2520in-depth%2520analysis%2520of%2520their%2520performance%252C%2520revealing%250Ainsightful%2520patterns%2520and%2520trade-offs%2520in%2520the%2520design%2520choices%2520of%2520RAG%2520architectures.%250AThe%2520metrics%2520of%2520RAGChecker%2520can%2520guide%2520researchers%2520and%2520practitioners%2520in%2520developing%250Amore%2520effective%2520RAG%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAGChecker%3A%20A%20Fine-grained%20Framework%20for%20Diagnosing%20Retrieval-Augmented%0A%20%20Generation&entry.906535625=Dongyu%20Ru%20and%20Lin%20Qiu%20and%20Xiangkun%20Hu%20and%20Tianhang%20Zhang%20and%20Peng%20Shi%20and%20Shuaichen%20Chang%20and%20Jiayang%20Cheng%20and%20Cunxiang%20Wang%20and%20Shichao%20Sun%20and%20Huanyu%20Li%20and%20Zizhao%20Zhang%20and%20Binjie%20Wang%20and%20Jiarong%20Jiang%20and%20Tong%20He%20and%20Zhiguo%20Wang%20and%20Pengfei%20Liu%20and%20Yue%20Zhang%20and%20Zheng%20Zhang&entry.1292438233=%20%20Despite%20Retrieval-Augmented%20Generation%20%28RAG%29%20has%20shown%20promising%20capability%0Ain%20leveraging%20external%20knowledge%2C%20a%20comprehensive%20evaluation%20of%20RAG%20systems%20is%0Astill%20challenging%20due%20to%20the%20modular%20nature%20of%20RAG%2C%20evaluation%20of%20long-form%0Aresponses%20and%20reliability%20of%20measurements.%20In%20this%20paper%2C%20we%20propose%20a%0Afine-grained%20evaluation%20framework%2C%20RAGChecker%2C%20that%20incorporates%20a%20suite%20of%0Adiagnostic%20metrics%20for%20both%20the%20retrieval%20and%20generation%20modules.%20Meta%0Aevaluation%20verifies%20that%20RAGChecker%20has%20significantly%20better%20correlations%20with%0Ahuman%20judgments%20than%20other%20evaluation%20metrics.%20Using%20RAGChecker%2C%20we%20evaluate%208%0ARAG%20systems%20and%20conduct%20an%20in-depth%20analysis%20of%20their%20performance%2C%20revealing%0Ainsightful%20patterns%20and%20trade-offs%20in%20the%20design%20choices%20of%20RAG%20architectures.%0AThe%20metrics%20of%20RAGChecker%20can%20guide%20researchers%20and%20practitioners%20in%20developing%0Amore%20effective%20RAG%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08067v1&entry.124074799=Read"},
{"title": "Towards a Science Exocortex", "author": "Kevin G. Yager", "abstract": "  Artificial intelligence (AI) methods are poised to revolutionize intellectual\nwork, with generative AI enabling automation of text analysis, text generation,\nand simple decision making or reasoning. The impact to science is only just\nbeginning, but the opportunity is significant since scientific research relies\nfundamentally on extended chains of cognitive work. Here, we review the state\nof the art in agentic AI systems, and discuss how these methods could be\nextended to have even greater impact on science. We propose the development of\nan exocortex, a synthetic extension of a person's cognition. A science\nexocortex could be designed as a swarm of AI agents, with each agent\nindividually streamlining specific researcher tasks, and whose\ninter-communication leads to emergent behavior that greatly extend the\nresearcher's cognition and volition.\n", "link": "http://arxiv.org/abs/2406.17809v2", "date": "2024-08-15", "relevancy": 1.901, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5538}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4926}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Science%20Exocortex&body=Title%3A%20Towards%20a%20Science%20Exocortex%0AAuthor%3A%20Kevin%20G.%20Yager%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20methods%20are%20poised%20to%20revolutionize%20intellectual%0Awork%2C%20with%20generative%20AI%20enabling%20automation%20of%20text%20analysis%2C%20text%20generation%2C%0Aand%20simple%20decision%20making%20or%20reasoning.%20The%20impact%20to%20science%20is%20only%20just%0Abeginning%2C%20but%20the%20opportunity%20is%20significant%20since%20scientific%20research%20relies%0Afundamentally%20on%20extended%20chains%20of%20cognitive%20work.%20Here%2C%20we%20review%20the%20state%0Aof%20the%20art%20in%20agentic%20AI%20systems%2C%20and%20discuss%20how%20these%20methods%20could%20be%0Aextended%20to%20have%20even%20greater%20impact%20on%20science.%20We%20propose%20the%20development%20of%0Aan%20exocortex%2C%20a%20synthetic%20extension%20of%20a%20person%27s%20cognition.%20A%20science%0Aexocortex%20could%20be%20designed%20as%20a%20swarm%20of%20AI%20agents%2C%20with%20each%20agent%0Aindividually%20streamlining%20specific%20researcher%20tasks%2C%20and%20whose%0Ainter-communication%20leads%20to%20emergent%20behavior%20that%20greatly%20extend%20the%0Aresearcher%27s%20cognition%20and%20volition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Science%2520Exocortex%26entry.906535625%3DKevin%2520G.%2520Yager%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520methods%2520are%2520poised%2520to%2520revolutionize%2520intellectual%250Awork%252C%2520with%2520generative%2520AI%2520enabling%2520automation%2520of%2520text%2520analysis%252C%2520text%2520generation%252C%250Aand%2520simple%2520decision%2520making%2520or%2520reasoning.%2520The%2520impact%2520to%2520science%2520is%2520only%2520just%250Abeginning%252C%2520but%2520the%2520opportunity%2520is%2520significant%2520since%2520scientific%2520research%2520relies%250Afundamentally%2520on%2520extended%2520chains%2520of%2520cognitive%2520work.%2520Here%252C%2520we%2520review%2520the%2520state%250Aof%2520the%2520art%2520in%2520agentic%2520AI%2520systems%252C%2520and%2520discuss%2520how%2520these%2520methods%2520could%2520be%250Aextended%2520to%2520have%2520even%2520greater%2520impact%2520on%2520science.%2520We%2520propose%2520the%2520development%2520of%250Aan%2520exocortex%252C%2520a%2520synthetic%2520extension%2520of%2520a%2520person%2527s%2520cognition.%2520A%2520science%250Aexocortex%2520could%2520be%2520designed%2520as%2520a%2520swarm%2520of%2520AI%2520agents%252C%2520with%2520each%2520agent%250Aindividually%2520streamlining%2520specific%2520researcher%2520tasks%252C%2520and%2520whose%250Ainter-communication%2520leads%2520to%2520emergent%2520behavior%2520that%2520greatly%2520extend%2520the%250Aresearcher%2527s%2520cognition%2520and%2520volition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Science%20Exocortex&entry.906535625=Kevin%20G.%20Yager&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20methods%20are%20poised%20to%20revolutionize%20intellectual%0Awork%2C%20with%20generative%20AI%20enabling%20automation%20of%20text%20analysis%2C%20text%20generation%2C%0Aand%20simple%20decision%20making%20or%20reasoning.%20The%20impact%20to%20science%20is%20only%20just%0Abeginning%2C%20but%20the%20opportunity%20is%20significant%20since%20scientific%20research%20relies%0Afundamentally%20on%20extended%20chains%20of%20cognitive%20work.%20Here%2C%20we%20review%20the%20state%0Aof%20the%20art%20in%20agentic%20AI%20systems%2C%20and%20discuss%20how%20these%20methods%20could%20be%0Aextended%20to%20have%20even%20greater%20impact%20on%20science.%20We%20propose%20the%20development%20of%0Aan%20exocortex%2C%20a%20synthetic%20extension%20of%20a%20person%27s%20cognition.%20A%20science%0Aexocortex%20could%20be%20designed%20as%20a%20swarm%20of%20AI%20agents%2C%20with%20each%20agent%0Aindividually%20streamlining%20specific%20researcher%20tasks%2C%20and%20whose%0Ainter-communication%20leads%20to%20emergent%20behavior%20that%20greatly%20extend%20the%0Aresearcher%27s%20cognition%20and%20volition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17809v2&entry.124074799=Read"},
{"title": "Detector Collapse: Physical-World Backdooring Object Detection to\n  Catastrophic Overload or Blindness in Autonomous Driving", "author": "Hangtao Zhang and Shengshan Hu and Yichen Wang and Leo Yu Zhang and Ziqi Zhou and Xianlong Wang and Yanjun Zhang and Chao Chen", "abstract": "  Object detection tasks, crucial in safety-critical systems like autonomous\ndriving, focus on pinpointing object locations. These detectors are known to be\nsusceptible to backdoor attacks. However, existing backdoor techniques have\nprimarily been adapted from classification tasks, overlooking deeper\nvulnerabilities specific to object detection. This paper is dedicated to\nbridging this gap by introducing Detector Collapse} (DC), a brand-new backdoor\nattack paradigm tailored for object detection. DC is designed to instantly\nincapacitate detectors (i.e., severely impairing detector's performance and\nculminating in a denial-of-service). To this end, we develop two innovative\nattack schemes: Sponge for triggering widespread misidentifications and\nBlinding for rendering objects invisible. Remarkably, we introduce a novel\npoisoning strategy exploiting natural objects, enabling DC to act as a\npractical backdoor in real-world environments. Our experiments on different\ndetectors across several benchmarks show a significant improvement\n($\\sim$10\\%-60\\% absolute and $\\sim$2-7$\\times$ relative) in attack efficacy\nover state-of-the-art attacks.\n", "link": "http://arxiv.org/abs/2404.11357v2", "date": "2024-08-15", "relevancy": 1.8899, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4803}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4731}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detector%20Collapse%3A%20Physical-World%20Backdooring%20Object%20Detection%20to%0A%20%20Catastrophic%20Overload%20or%20Blindness%20in%20Autonomous%20Driving&body=Title%3A%20Detector%20Collapse%3A%20Physical-World%20Backdooring%20Object%20Detection%20to%0A%20%20Catastrophic%20Overload%20or%20Blindness%20in%20Autonomous%20Driving%0AAuthor%3A%20Hangtao%20Zhang%20and%20Shengshan%20Hu%20and%20Yichen%20Wang%20and%20Leo%20Yu%20Zhang%20and%20Ziqi%20Zhou%20and%20Xianlong%20Wang%20and%20Yanjun%20Zhang%20and%20Chao%20Chen%0AAbstract%3A%20%20%20Object%20detection%20tasks%2C%20crucial%20in%20safety-critical%20systems%20like%20autonomous%0Adriving%2C%20focus%20on%20pinpointing%20object%20locations.%20These%20detectors%20are%20known%20to%20be%0Asusceptible%20to%20backdoor%20attacks.%20However%2C%20existing%20backdoor%20techniques%20have%0Aprimarily%20been%20adapted%20from%20classification%20tasks%2C%20overlooking%20deeper%0Avulnerabilities%20specific%20to%20object%20detection.%20This%20paper%20is%20dedicated%20to%0Abridging%20this%20gap%20by%20introducing%20Detector%20Collapse%7D%20%28DC%29%2C%20a%20brand-new%20backdoor%0Aattack%20paradigm%20tailored%20for%20object%20detection.%20DC%20is%20designed%20to%20instantly%0Aincapacitate%20detectors%20%28i.e.%2C%20severely%20impairing%20detector%27s%20performance%20and%0Aculminating%20in%20a%20denial-of-service%29.%20To%20this%20end%2C%20we%20develop%20two%20innovative%0Aattack%20schemes%3A%20Sponge%20for%20triggering%20widespread%20misidentifications%20and%0ABlinding%20for%20rendering%20objects%20invisible.%20Remarkably%2C%20we%20introduce%20a%20novel%0Apoisoning%20strategy%20exploiting%20natural%20objects%2C%20enabling%20DC%20to%20act%20as%20a%0Apractical%20backdoor%20in%20real-world%20environments.%20Our%20experiments%20on%20different%0Adetectors%20across%20several%20benchmarks%20show%20a%20significant%20improvement%0A%28%24%5Csim%2410%5C%25-60%5C%25%20absolute%20and%20%24%5Csim%242-7%24%5Ctimes%24%20relative%29%20in%20attack%20efficacy%0Aover%20state-of-the-art%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11357v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetector%2520Collapse%253A%2520Physical-World%2520Backdooring%2520Object%2520Detection%2520to%250A%2520%2520Catastrophic%2520Overload%2520or%2520Blindness%2520in%2520Autonomous%2520Driving%26entry.906535625%3DHangtao%2520Zhang%2520and%2520Shengshan%2520Hu%2520and%2520Yichen%2520Wang%2520and%2520Leo%2520Yu%2520Zhang%2520and%2520Ziqi%2520Zhou%2520and%2520Xianlong%2520Wang%2520and%2520Yanjun%2520Zhang%2520and%2520Chao%2520Chen%26entry.1292438233%3D%2520%2520Object%2520detection%2520tasks%252C%2520crucial%2520in%2520safety-critical%2520systems%2520like%2520autonomous%250Adriving%252C%2520focus%2520on%2520pinpointing%2520object%2520locations.%2520These%2520detectors%2520are%2520known%2520to%2520be%250Asusceptible%2520to%2520backdoor%2520attacks.%2520However%252C%2520existing%2520backdoor%2520techniques%2520have%250Aprimarily%2520been%2520adapted%2520from%2520classification%2520tasks%252C%2520overlooking%2520deeper%250Avulnerabilities%2520specific%2520to%2520object%2520detection.%2520This%2520paper%2520is%2520dedicated%2520to%250Abridging%2520this%2520gap%2520by%2520introducing%2520Detector%2520Collapse%257D%2520%2528DC%2529%252C%2520a%2520brand-new%2520backdoor%250Aattack%2520paradigm%2520tailored%2520for%2520object%2520detection.%2520DC%2520is%2520designed%2520to%2520instantly%250Aincapacitate%2520detectors%2520%2528i.e.%252C%2520severely%2520impairing%2520detector%2527s%2520performance%2520and%250Aculminating%2520in%2520a%2520denial-of-service%2529.%2520To%2520this%2520end%252C%2520we%2520develop%2520two%2520innovative%250Aattack%2520schemes%253A%2520Sponge%2520for%2520triggering%2520widespread%2520misidentifications%2520and%250ABlinding%2520for%2520rendering%2520objects%2520invisible.%2520Remarkably%252C%2520we%2520introduce%2520a%2520novel%250Apoisoning%2520strategy%2520exploiting%2520natural%2520objects%252C%2520enabling%2520DC%2520to%2520act%2520as%2520a%250Apractical%2520backdoor%2520in%2520real-world%2520environments.%2520Our%2520experiments%2520on%2520different%250Adetectors%2520across%2520several%2520benchmarks%2520show%2520a%2520significant%2520improvement%250A%2528%2524%255Csim%252410%255C%2525-60%255C%2525%2520absolute%2520and%2520%2524%255Csim%25242-7%2524%255Ctimes%2524%2520relative%2529%2520in%2520attack%2520efficacy%250Aover%2520state-of-the-art%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11357v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detector%20Collapse%3A%20Physical-World%20Backdooring%20Object%20Detection%20to%0A%20%20Catastrophic%20Overload%20or%20Blindness%20in%20Autonomous%20Driving&entry.906535625=Hangtao%20Zhang%20and%20Shengshan%20Hu%20and%20Yichen%20Wang%20and%20Leo%20Yu%20Zhang%20and%20Ziqi%20Zhou%20and%20Xianlong%20Wang%20and%20Yanjun%20Zhang%20and%20Chao%20Chen&entry.1292438233=%20%20Object%20detection%20tasks%2C%20crucial%20in%20safety-critical%20systems%20like%20autonomous%0Adriving%2C%20focus%20on%20pinpointing%20object%20locations.%20These%20detectors%20are%20known%20to%20be%0Asusceptible%20to%20backdoor%20attacks.%20However%2C%20existing%20backdoor%20techniques%20have%0Aprimarily%20been%20adapted%20from%20classification%20tasks%2C%20overlooking%20deeper%0Avulnerabilities%20specific%20to%20object%20detection.%20This%20paper%20is%20dedicated%20to%0Abridging%20this%20gap%20by%20introducing%20Detector%20Collapse%7D%20%28DC%29%2C%20a%20brand-new%20backdoor%0Aattack%20paradigm%20tailored%20for%20object%20detection.%20DC%20is%20designed%20to%20instantly%0Aincapacitate%20detectors%20%28i.e.%2C%20severely%20impairing%20detector%27s%20performance%20and%0Aculminating%20in%20a%20denial-of-service%29.%20To%20this%20end%2C%20we%20develop%20two%20innovative%0Aattack%20schemes%3A%20Sponge%20for%20triggering%20widespread%20misidentifications%20and%0ABlinding%20for%20rendering%20objects%20invisible.%20Remarkably%2C%20we%20introduce%20a%20novel%0Apoisoning%20strategy%20exploiting%20natural%20objects%2C%20enabling%20DC%20to%20act%20as%20a%0Apractical%20backdoor%20in%20real-world%20environments.%20Our%20experiments%20on%20different%0Adetectors%20across%20several%20benchmarks%20show%20a%20significant%20improvement%0A%28%24%5Csim%2410%5C%25-60%5C%25%20absolute%20and%20%24%5Csim%242-7%24%5Ctimes%24%20relative%29%20in%20attack%20efficacy%0Aover%20state-of-the-art%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11357v2&entry.124074799=Read"},
{"title": "Chance-Constrained Information-Theoretic Stochastic Model Predictive\n  Control with Safety Shielding", "author": "Ji Yin and Panagiotis Tsiotras and Karl Berntorp", "abstract": "  This paper introduces a novel nonlinear stochastic model predictive control\npath integral (MPPI) method, which considers chance constraints on system\nstates. The proposed belief-space stochastic MPPI (BSS-MPPI) applies\nMonte-Carlo sampling to evaluate state distributions resulting from underlying\nsystematic disturbances, and utilizes a Control Barrier Function (CBF) inspired\nheuristic in belief space to fulfill the specified chance constraints. Compared\nto several previous stochastic predictive control methods, our approach applies\nto general nonlinear dynamics without requiring the computationally expensive\nsystem linearization step. Moreover, the BSS-MPPI controller can solve\noptimization problems without limiting the form of the objective function and\nchance constraints. By multi-threading the sampling process using a GPU, we can\nachieve fast real-time planning for time- and safety-critical tasks such as\nautonomous racing. Our results on a realistic race-car simulation study show\nsignificant reductions in constraint violation compared to some of the prior\nMPPI approaches, while being comparable in computation times.\n", "link": "http://arxiv.org/abs/2408.00494v2", "date": "2024-08-15", "relevancy": 1.8873, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4805}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chance-Constrained%20Information-Theoretic%20Stochastic%20Model%20Predictive%0A%20%20Control%20with%20Safety%20Shielding&body=Title%3A%20Chance-Constrained%20Information-Theoretic%20Stochastic%20Model%20Predictive%0A%20%20Control%20with%20Safety%20Shielding%0AAuthor%3A%20Ji%20Yin%20and%20Panagiotis%20Tsiotras%20and%20Karl%20Berntorp%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20nonlinear%20stochastic%20model%20predictive%20control%0Apath%20integral%20%28MPPI%29%20method%2C%20which%20considers%20chance%20constraints%20on%20system%0Astates.%20The%20proposed%20belief-space%20stochastic%20MPPI%20%28BSS-MPPI%29%20applies%0AMonte-Carlo%20sampling%20to%20evaluate%20state%20distributions%20resulting%20from%20underlying%0Asystematic%20disturbances%2C%20and%20utilizes%20a%20Control%20Barrier%20Function%20%28CBF%29%20inspired%0Aheuristic%20in%20belief%20space%20to%20fulfill%20the%20specified%20chance%20constraints.%20Compared%0Ato%20several%20previous%20stochastic%20predictive%20control%20methods%2C%20our%20approach%20applies%0Ato%20general%20nonlinear%20dynamics%20without%20requiring%20the%20computationally%20expensive%0Asystem%20linearization%20step.%20Moreover%2C%20the%20BSS-MPPI%20controller%20can%20solve%0Aoptimization%20problems%20without%20limiting%20the%20form%20of%20the%20objective%20function%20and%0Achance%20constraints.%20By%20multi-threading%20the%20sampling%20process%20using%20a%20GPU%2C%20we%20can%0Aachieve%20fast%20real-time%20planning%20for%20time-%20and%20safety-critical%20tasks%20such%20as%0Aautonomous%20racing.%20Our%20results%20on%20a%20realistic%20race-car%20simulation%20study%20show%0Asignificant%20reductions%20in%20constraint%20violation%20compared%20to%20some%20of%20the%20prior%0AMPPI%20approaches%2C%20while%20being%20comparable%20in%20computation%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChance-Constrained%2520Information-Theoretic%2520Stochastic%2520Model%2520Predictive%250A%2520%2520Control%2520with%2520Safety%2520Shielding%26entry.906535625%3DJi%2520Yin%2520and%2520Panagiotis%2520Tsiotras%2520and%2520Karl%2520Berntorp%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520nonlinear%2520stochastic%2520model%2520predictive%2520control%250Apath%2520integral%2520%2528MPPI%2529%2520method%252C%2520which%2520considers%2520chance%2520constraints%2520on%2520system%250Astates.%2520The%2520proposed%2520belief-space%2520stochastic%2520MPPI%2520%2528BSS-MPPI%2529%2520applies%250AMonte-Carlo%2520sampling%2520to%2520evaluate%2520state%2520distributions%2520resulting%2520from%2520underlying%250Asystematic%2520disturbances%252C%2520and%2520utilizes%2520a%2520Control%2520Barrier%2520Function%2520%2528CBF%2529%2520inspired%250Aheuristic%2520in%2520belief%2520space%2520to%2520fulfill%2520the%2520specified%2520chance%2520constraints.%2520Compared%250Ato%2520several%2520previous%2520stochastic%2520predictive%2520control%2520methods%252C%2520our%2520approach%2520applies%250Ato%2520general%2520nonlinear%2520dynamics%2520without%2520requiring%2520the%2520computationally%2520expensive%250Asystem%2520linearization%2520step.%2520Moreover%252C%2520the%2520BSS-MPPI%2520controller%2520can%2520solve%250Aoptimization%2520problems%2520without%2520limiting%2520the%2520form%2520of%2520the%2520objective%2520function%2520and%250Achance%2520constraints.%2520By%2520multi-threading%2520the%2520sampling%2520process%2520using%2520a%2520GPU%252C%2520we%2520can%250Aachieve%2520fast%2520real-time%2520planning%2520for%2520time-%2520and%2520safety-critical%2520tasks%2520such%2520as%250Aautonomous%2520racing.%2520Our%2520results%2520on%2520a%2520realistic%2520race-car%2520simulation%2520study%2520show%250Asignificant%2520reductions%2520in%2520constraint%2520violation%2520compared%2520to%2520some%2520of%2520the%2520prior%250AMPPI%2520approaches%252C%2520while%2520being%2520comparable%2520in%2520computation%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chance-Constrained%20Information-Theoretic%20Stochastic%20Model%20Predictive%0A%20%20Control%20with%20Safety%20Shielding&entry.906535625=Ji%20Yin%20and%20Panagiotis%20Tsiotras%20and%20Karl%20Berntorp&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20nonlinear%20stochastic%20model%20predictive%20control%0Apath%20integral%20%28MPPI%29%20method%2C%20which%20considers%20chance%20constraints%20on%20system%0Astates.%20The%20proposed%20belief-space%20stochastic%20MPPI%20%28BSS-MPPI%29%20applies%0AMonte-Carlo%20sampling%20to%20evaluate%20state%20distributions%20resulting%20from%20underlying%0Asystematic%20disturbances%2C%20and%20utilizes%20a%20Control%20Barrier%20Function%20%28CBF%29%20inspired%0Aheuristic%20in%20belief%20space%20to%20fulfill%20the%20specified%20chance%20constraints.%20Compared%0Ato%20several%20previous%20stochastic%20predictive%20control%20methods%2C%20our%20approach%20applies%0Ato%20general%20nonlinear%20dynamics%20without%20requiring%20the%20computationally%20expensive%0Asystem%20linearization%20step.%20Moreover%2C%20the%20BSS-MPPI%20controller%20can%20solve%0Aoptimization%20problems%20without%20limiting%20the%20form%20of%20the%20objective%20function%20and%0Achance%20constraints.%20By%20multi-threading%20the%20sampling%20process%20using%20a%20GPU%2C%20we%20can%0Aachieve%20fast%20real-time%20planning%20for%20time-%20and%20safety-critical%20tasks%20such%20as%0Aautonomous%20racing.%20Our%20results%20on%20a%20realistic%20race-car%20simulation%20study%20show%0Asignificant%20reductions%20in%20constraint%20violation%20compared%20to%20some%20of%20the%20prior%0AMPPI%20approaches%2C%20while%20being%20comparable%20in%20computation%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00494v2&entry.124074799=Read"},
{"title": "DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for\n  Fingerprint Presentation Attack Detection", "author": "Anuj Rai and Parsheel Kumar Tiwari and Jyotishna Baishya and Ram Prakash Sharma and Somnath Dey", "abstract": "  Automatic fingerprint recognition systems suffer from the threat of\npresentation attacks due to their wide range of deployment in areas including\nnational borders and commercial applications. A presentation attack can be\nperformed by creating a spoof of a user's fingerprint with or without their\nconsent. This paper presents a dynamic ensemble of deep CNN and handcrafted\nfeatures to detect presentation attacks in known-material and unknown-material\nprotocols of the livness detection competition. The proposed presentation\nattack detection model, in this way, utilizes the capabilities of both deep CNN\nand handcrafted features techniques and exhibits better performance than their\nindividual performances. We have validated our proposed method on benchmark\ndatabases from the Liveness Detection Competition in 2015, 2017, and 2019,\nyielding overall accuracy of 96.10\\%, 96.49\\%, and 94.99\\% on them,\nrespectively. The proposed method outperforms state-of-the-art methods in terms\nof classification accuracy.\n", "link": "http://arxiv.org/abs/2308.10015v3", "date": "2024-08-15", "relevancy": 1.8834, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4878}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4696}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyFFPAD%3A%20Dynamic%20Fusion%20of%20Convolutional%20and%20Handcrafted%20Features%20for%0A%20%20Fingerprint%20Presentation%20Attack%20Detection&body=Title%3A%20DyFFPAD%3A%20Dynamic%20Fusion%20of%20Convolutional%20and%20Handcrafted%20Features%20for%0A%20%20Fingerprint%20Presentation%20Attack%20Detection%0AAuthor%3A%20Anuj%20Rai%20and%20Parsheel%20Kumar%20Tiwari%20and%20Jyotishna%20Baishya%20and%20Ram%20Prakash%20Sharma%20and%20Somnath%20Dey%0AAbstract%3A%20%20%20Automatic%20fingerprint%20recognition%20systems%20suffer%20from%20the%20threat%20of%0Apresentation%20attacks%20due%20to%20their%20wide%20range%20of%20deployment%20in%20areas%20including%0Anational%20borders%20and%20commercial%20applications.%20A%20presentation%20attack%20can%20be%0Aperformed%20by%20creating%20a%20spoof%20of%20a%20user%27s%20fingerprint%20with%20or%20without%20their%0Aconsent.%20This%20paper%20presents%20a%20dynamic%20ensemble%20of%20deep%20CNN%20and%20handcrafted%0Afeatures%20to%20detect%20presentation%20attacks%20in%20known-material%20and%20unknown-material%0Aprotocols%20of%20the%20livness%20detection%20competition.%20The%20proposed%20presentation%0Aattack%20detection%20model%2C%20in%20this%20way%2C%20utilizes%20the%20capabilities%20of%20both%20deep%20CNN%0Aand%20handcrafted%20features%20techniques%20and%20exhibits%20better%20performance%20than%20their%0Aindividual%20performances.%20We%20have%20validated%20our%20proposed%20method%20on%20benchmark%0Adatabases%20from%20the%20Liveness%20Detection%20Competition%20in%202015%2C%202017%2C%20and%202019%2C%0Ayielding%20overall%20accuracy%20of%2096.10%5C%25%2C%2096.49%5C%25%2C%20and%2094.99%5C%25%20on%20them%2C%0Arespectively.%20The%20proposed%20method%20outperforms%20state-of-the-art%20methods%20in%20terms%0Aof%20classification%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10015v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyFFPAD%253A%2520Dynamic%2520Fusion%2520of%2520Convolutional%2520and%2520Handcrafted%2520Features%2520for%250A%2520%2520Fingerprint%2520Presentation%2520Attack%2520Detection%26entry.906535625%3DAnuj%2520Rai%2520and%2520Parsheel%2520Kumar%2520Tiwari%2520and%2520Jyotishna%2520Baishya%2520and%2520Ram%2520Prakash%2520Sharma%2520and%2520Somnath%2520Dey%26entry.1292438233%3D%2520%2520Automatic%2520fingerprint%2520recognition%2520systems%2520suffer%2520from%2520the%2520threat%2520of%250Apresentation%2520attacks%2520due%2520to%2520their%2520wide%2520range%2520of%2520deployment%2520in%2520areas%2520including%250Anational%2520borders%2520and%2520commercial%2520applications.%2520A%2520presentation%2520attack%2520can%2520be%250Aperformed%2520by%2520creating%2520a%2520spoof%2520of%2520a%2520user%2527s%2520fingerprint%2520with%2520or%2520without%2520their%250Aconsent.%2520This%2520paper%2520presents%2520a%2520dynamic%2520ensemble%2520of%2520deep%2520CNN%2520and%2520handcrafted%250Afeatures%2520to%2520detect%2520presentation%2520attacks%2520in%2520known-material%2520and%2520unknown-material%250Aprotocols%2520of%2520the%2520livness%2520detection%2520competition.%2520The%2520proposed%2520presentation%250Aattack%2520detection%2520model%252C%2520in%2520this%2520way%252C%2520utilizes%2520the%2520capabilities%2520of%2520both%2520deep%2520CNN%250Aand%2520handcrafted%2520features%2520techniques%2520and%2520exhibits%2520better%2520performance%2520than%2520their%250Aindividual%2520performances.%2520We%2520have%2520validated%2520our%2520proposed%2520method%2520on%2520benchmark%250Adatabases%2520from%2520the%2520Liveness%2520Detection%2520Competition%2520in%25202015%252C%25202017%252C%2520and%25202019%252C%250Ayielding%2520overall%2520accuracy%2520of%252096.10%255C%2525%252C%252096.49%255C%2525%252C%2520and%252094.99%255C%2525%2520on%2520them%252C%250Arespectively.%2520The%2520proposed%2520method%2520outperforms%2520state-of-the-art%2520methods%2520in%2520terms%250Aof%2520classification%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.10015v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyFFPAD%3A%20Dynamic%20Fusion%20of%20Convolutional%20and%20Handcrafted%20Features%20for%0A%20%20Fingerprint%20Presentation%20Attack%20Detection&entry.906535625=Anuj%20Rai%20and%20Parsheel%20Kumar%20Tiwari%20and%20Jyotishna%20Baishya%20and%20Ram%20Prakash%20Sharma%20and%20Somnath%20Dey&entry.1292438233=%20%20Automatic%20fingerprint%20recognition%20systems%20suffer%20from%20the%20threat%20of%0Apresentation%20attacks%20due%20to%20their%20wide%20range%20of%20deployment%20in%20areas%20including%0Anational%20borders%20and%20commercial%20applications.%20A%20presentation%20attack%20can%20be%0Aperformed%20by%20creating%20a%20spoof%20of%20a%20user%27s%20fingerprint%20with%20or%20without%20their%0Aconsent.%20This%20paper%20presents%20a%20dynamic%20ensemble%20of%20deep%20CNN%20and%20handcrafted%0Afeatures%20to%20detect%20presentation%20attacks%20in%20known-material%20and%20unknown-material%0Aprotocols%20of%20the%20livness%20detection%20competition.%20The%20proposed%20presentation%0Aattack%20detection%20model%2C%20in%20this%20way%2C%20utilizes%20the%20capabilities%20of%20both%20deep%20CNN%0Aand%20handcrafted%20features%20techniques%20and%20exhibits%20better%20performance%20than%20their%0Aindividual%20performances.%20We%20have%20validated%20our%20proposed%20method%20on%20benchmark%0Adatabases%20from%20the%20Liveness%20Detection%20Competition%20in%202015%2C%202017%2C%20and%202019%2C%0Ayielding%20overall%20accuracy%20of%2096.10%5C%25%2C%2096.49%5C%25%2C%20and%2094.99%5C%25%20on%20them%2C%0Arespectively.%20The%20proposed%20method%20outperforms%20state-of-the-art%20methods%20in%20terms%0Aof%20classification%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10015v3&entry.124074799=Read"},
{"title": "MagicFace: Training-free Universal-Style Human Image Customized\n  Synthesis", "author": "Yibin Wang and Weizhong Zhang and Cheng Jin", "abstract": "  Existing human image personalized generation methods often require tedious\ntraining: either fine-tuning with a few images or retraining on large-scale\ndatasets. In such cases, these methods are prone to overfitting and encounter\ndifficulties when personalizing individuals of diverse styles. Moreover, these\ntraining-based approaches also struggle with multi-concept human image\ncustomizing. To this end, we propose MagicFace, the first method for\nuniversal-style human image personalized synthesis that enables\nsingle/multi-concept customization for humans of any style in a training-free\nmanner. MagicFace introduces a coarse-to-fine generation pipeline, involving\ntwo sequential stages: semantic scene construction and concept feature\ninjection. This is achieved by our Reference-aware Self-Attention (RSA) and\nRegion-grouped Blend Attention (RBA) mechanisms. Specifically, in the first\nstage, RSA enables the latent image to query features from reference concepts\nsimultaneously, extracting the coarse-grained overall semantic understanding to\nfacilitate the initial semantic layout establishment. In the second stage, we\nemploy an attention-based semantic segmentation method to pinpoint the\ngenerated regions of all concepts in the latent image at each step. Following\nthis, RBA divides the pixels of the latent image into semantic groups, with\neach group querying fine-grained features from its reference concept, which\nensures precise attribute alignment and feature injection. Throughout the\ntwo-stage process, a weight mask strategy is employed to ensure the model\nfocuses more on the reference concepts. Extensive experiments demonstrate our\nsuperiority in both human-centric subject-to-image synthesis and multi-concept\nhuman image customization. Our approach also can be applied to texture\ntransformation, further enhancing its versatility and applicability.\n", "link": "http://arxiv.org/abs/2408.07433v2", "date": "2024-08-15", "relevancy": 1.882, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6615}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5905}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis&body=Title%3A%20MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis%0AAuthor%3A%20Yibin%20Wang%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin%0AAbstract%3A%20%20%20Existing%20human%20image%20personalized%20generation%20methods%20often%20require%20tedious%0Atraining%3A%20either%20fine-tuning%20with%20a%20few%20images%20or%20retraining%20on%20large-scale%0Adatasets.%20In%20such%20cases%2C%20these%20methods%20are%20prone%20to%20overfitting%20and%20encounter%0Adifficulties%20when%20personalizing%20individuals%20of%20diverse%20styles.%20Moreover%2C%20these%0Atraining-based%20approaches%20also%20struggle%20with%20multi-concept%20human%20image%0Acustomizing.%20To%20this%20end%2C%20we%20propose%20MagicFace%2C%20the%20first%20method%20for%0Auniversal-style%20human%20image%20personalized%20synthesis%20that%20enables%0Asingle/multi-concept%20customization%20for%20humans%20of%20any%20style%20in%20a%20training-free%0Amanner.%20MagicFace%20introduces%20a%20coarse-to-fine%20generation%20pipeline%2C%20involving%0Atwo%20sequential%20stages%3A%20semantic%20scene%20construction%20and%20concept%20feature%0Ainjection.%20This%20is%20achieved%20by%20our%20Reference-aware%20Self-Attention%20%28RSA%29%20and%0ARegion-grouped%20Blend%20Attention%20%28RBA%29%20mechanisms.%20Specifically%2C%20in%20the%20first%0Astage%2C%20RSA%20enables%20the%20latent%20image%20to%20query%20features%20from%20reference%20concepts%0Asimultaneously%2C%20extracting%20the%20coarse-grained%20overall%20semantic%20understanding%20to%0Afacilitate%20the%20initial%20semantic%20layout%20establishment.%20In%20the%20second%20stage%2C%20we%0Aemploy%20an%20attention-based%20semantic%20segmentation%20method%20to%20pinpoint%20the%0Agenerated%20regions%20of%20all%20concepts%20in%20the%20latent%20image%20at%20each%20step.%20Following%0Athis%2C%20RBA%20divides%20the%20pixels%20of%20the%20latent%20image%20into%20semantic%20groups%2C%20with%0Aeach%20group%20querying%20fine-grained%20features%20from%20its%20reference%20concept%2C%20which%0Aensures%20precise%20attribute%20alignment%20and%20feature%20injection.%20Throughout%20the%0Atwo-stage%20process%2C%20a%20weight%20mask%20strategy%20is%20employed%20to%20ensure%20the%20model%0Afocuses%20more%20on%20the%20reference%20concepts.%20Extensive%20experiments%20demonstrate%20our%0Asuperiority%20in%20both%20human-centric%20subject-to-image%20synthesis%20and%20multi-concept%0Ahuman%20image%20customization.%20Our%20approach%20also%20can%20be%20applied%20to%20texture%0Atransformation%2C%20further%20enhancing%20its%20versatility%20and%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07433v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicFace%253A%2520Training-free%2520Universal-Style%2520Human%2520Image%2520Customized%250A%2520%2520Synthesis%26entry.906535625%3DYibin%2520Wang%2520and%2520Weizhong%2520Zhang%2520and%2520Cheng%2520Jin%26entry.1292438233%3D%2520%2520Existing%2520human%2520image%2520personalized%2520generation%2520methods%2520often%2520require%2520tedious%250Atraining%253A%2520either%2520fine-tuning%2520with%2520a%2520few%2520images%2520or%2520retraining%2520on%2520large-scale%250Adatasets.%2520In%2520such%2520cases%252C%2520these%2520methods%2520are%2520prone%2520to%2520overfitting%2520and%2520encounter%250Adifficulties%2520when%2520personalizing%2520individuals%2520of%2520diverse%2520styles.%2520Moreover%252C%2520these%250Atraining-based%2520approaches%2520also%2520struggle%2520with%2520multi-concept%2520human%2520image%250Acustomizing.%2520To%2520this%2520end%252C%2520we%2520propose%2520MagicFace%252C%2520the%2520first%2520method%2520for%250Auniversal-style%2520human%2520image%2520personalized%2520synthesis%2520that%2520enables%250Asingle/multi-concept%2520customization%2520for%2520humans%2520of%2520any%2520style%2520in%2520a%2520training-free%250Amanner.%2520MagicFace%2520introduces%2520a%2520coarse-to-fine%2520generation%2520pipeline%252C%2520involving%250Atwo%2520sequential%2520stages%253A%2520semantic%2520scene%2520construction%2520and%2520concept%2520feature%250Ainjection.%2520This%2520is%2520achieved%2520by%2520our%2520Reference-aware%2520Self-Attention%2520%2528RSA%2529%2520and%250ARegion-grouped%2520Blend%2520Attention%2520%2528RBA%2529%2520mechanisms.%2520Specifically%252C%2520in%2520the%2520first%250Astage%252C%2520RSA%2520enables%2520the%2520latent%2520image%2520to%2520query%2520features%2520from%2520reference%2520concepts%250Asimultaneously%252C%2520extracting%2520the%2520coarse-grained%2520overall%2520semantic%2520understanding%2520to%250Afacilitate%2520the%2520initial%2520semantic%2520layout%2520establishment.%2520In%2520the%2520second%2520stage%252C%2520we%250Aemploy%2520an%2520attention-based%2520semantic%2520segmentation%2520method%2520to%2520pinpoint%2520the%250Agenerated%2520regions%2520of%2520all%2520concepts%2520in%2520the%2520latent%2520image%2520at%2520each%2520step.%2520Following%250Athis%252C%2520RBA%2520divides%2520the%2520pixels%2520of%2520the%2520latent%2520image%2520into%2520semantic%2520groups%252C%2520with%250Aeach%2520group%2520querying%2520fine-grained%2520features%2520from%2520its%2520reference%2520concept%252C%2520which%250Aensures%2520precise%2520attribute%2520alignment%2520and%2520feature%2520injection.%2520Throughout%2520the%250Atwo-stage%2520process%252C%2520a%2520weight%2520mask%2520strategy%2520is%2520employed%2520to%2520ensure%2520the%2520model%250Afocuses%2520more%2520on%2520the%2520reference%2520concepts.%2520Extensive%2520experiments%2520demonstrate%2520our%250Asuperiority%2520in%2520both%2520human-centric%2520subject-to-image%2520synthesis%2520and%2520multi-concept%250Ahuman%2520image%2520customization.%2520Our%2520approach%2520also%2520can%2520be%2520applied%2520to%2520texture%250Atransformation%252C%2520further%2520enhancing%2520its%2520versatility%2520and%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07433v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis&entry.906535625=Yibin%20Wang%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin&entry.1292438233=%20%20Existing%20human%20image%20personalized%20generation%20methods%20often%20require%20tedious%0Atraining%3A%20either%20fine-tuning%20with%20a%20few%20images%20or%20retraining%20on%20large-scale%0Adatasets.%20In%20such%20cases%2C%20these%20methods%20are%20prone%20to%20overfitting%20and%20encounter%0Adifficulties%20when%20personalizing%20individuals%20of%20diverse%20styles.%20Moreover%2C%20these%0Atraining-based%20approaches%20also%20struggle%20with%20multi-concept%20human%20image%0Acustomizing.%20To%20this%20end%2C%20we%20propose%20MagicFace%2C%20the%20first%20method%20for%0Auniversal-style%20human%20image%20personalized%20synthesis%20that%20enables%0Asingle/multi-concept%20customization%20for%20humans%20of%20any%20style%20in%20a%20training-free%0Amanner.%20MagicFace%20introduces%20a%20coarse-to-fine%20generation%20pipeline%2C%20involving%0Atwo%20sequential%20stages%3A%20semantic%20scene%20construction%20and%20concept%20feature%0Ainjection.%20This%20is%20achieved%20by%20our%20Reference-aware%20Self-Attention%20%28RSA%29%20and%0ARegion-grouped%20Blend%20Attention%20%28RBA%29%20mechanisms.%20Specifically%2C%20in%20the%20first%0Astage%2C%20RSA%20enables%20the%20latent%20image%20to%20query%20features%20from%20reference%20concepts%0Asimultaneously%2C%20extracting%20the%20coarse-grained%20overall%20semantic%20understanding%20to%0Afacilitate%20the%20initial%20semantic%20layout%20establishment.%20In%20the%20second%20stage%2C%20we%0Aemploy%20an%20attention-based%20semantic%20segmentation%20method%20to%20pinpoint%20the%0Agenerated%20regions%20of%20all%20concepts%20in%20the%20latent%20image%20at%20each%20step.%20Following%0Athis%2C%20RBA%20divides%20the%20pixels%20of%20the%20latent%20image%20into%20semantic%20groups%2C%20with%0Aeach%20group%20querying%20fine-grained%20features%20from%20its%20reference%20concept%2C%20which%0Aensures%20precise%20attribute%20alignment%20and%20feature%20injection.%20Throughout%20the%0Atwo-stage%20process%2C%20a%20weight%20mask%20strategy%20is%20employed%20to%20ensure%20the%20model%0Afocuses%20more%20on%20the%20reference%20concepts.%20Extensive%20experiments%20demonstrate%20our%0Asuperiority%20in%20both%20human-centric%20subject-to-image%20synthesis%20and%20multi-concept%0Ahuman%20image%20customization.%20Our%20approach%20also%20can%20be%20applied%20to%20texture%0Atransformation%2C%20further%20enhancing%20its%20versatility%20and%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07433v2&entry.124074799=Read"},
{"title": "Benchmarking the Capabilities of Large Language Models in Transportation\n  System Engineering: Accuracy, Consistency, and Reasoning Behaviors", "author": "Usman Syed and Ethan Light and Xingang Guo and Huan Zhang and Lianhui Qin and Yanfeng Ouyang and Bin Hu", "abstract": "  In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges.\n", "link": "http://arxiv.org/abs/2408.08302v1", "date": "2024-08-15", "relevancy": 1.8675, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4786}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4701}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20Capabilities%20of%20Large%20Language%20Models%20in%20Transportation%0A%20%20System%20Engineering%3A%20Accuracy%2C%20Consistency%2C%20and%20Reasoning%20Behaviors&body=Title%3A%20Benchmarking%20the%20Capabilities%20of%20Large%20Language%20Models%20in%20Transportation%0A%20%20System%20Engineering%3A%20Accuracy%2C%20Consistency%2C%20and%20Reasoning%20Behaviors%0AAuthor%3A%20Usman%20Syed%20and%20Ethan%20Light%20and%20Xingang%20Guo%20and%20Huan%20Zhang%20and%20Lianhui%20Qin%20and%20Yanfeng%20Ouyang%20and%20Bin%20Hu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explore%20the%20capabilities%20of%20state-of-the-art%20large%20language%0Amodels%20%28LLMs%29%20such%20as%20GPT-4%2C%20GPT-4o%2C%20Claude%203.5%20Sonnet%2C%20Claude%203%20Opus%2C%20Gemini%0A1.5%20Pro%2C%20Llama%203%2C%20and%20Llama%203.1%20in%20solving%20some%20selected%20undergraduate-level%0Atransportation%20engineering%20problems.%20We%20introduce%20TransportBench%2C%20a%20benchmark%0Adataset%20that%20includes%20a%20sample%20of%20transportation%20engineering%20problems%20on%20a%20wide%0Arange%20of%20subjects%20in%20the%20context%20of%20planning%2C%20design%2C%20management%2C%20and%20control%0Aof%20transportation%20systems.%20This%20dataset%20is%20used%20by%20human%20experts%20to%20evaluate%0Athe%20capabilities%20of%20various%20commercial%20and%20open-sourced%20LLMs%2C%20especially%20their%0Aaccuracy%2C%20consistency%2C%20and%20reasoning%20behaviors%2C%20in%20solving%20transportation%0Aengineering%20problems.%20Our%20comprehensive%20analysis%20uncovers%20the%20unique%20strengths%0Aand%20limitations%20of%20each%20LLM%2C%20e.g.%20our%20analysis%20shows%20the%20impressive%20accuracy%0Aand%20some%20unexpected%20inconsistent%20behaviors%20of%20Claude%203.5%20Sonnet%20in%20solving%0ATransportBench%20problems.%20Our%20study%20marks%20a%20thrilling%20first%20step%20toward%0Aharnessing%20artificial%20general%20intelligence%20for%20complex%20transportation%0Achallenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520the%2520Capabilities%2520of%2520Large%2520Language%2520Models%2520in%2520Transportation%250A%2520%2520System%2520Engineering%253A%2520Accuracy%252C%2520Consistency%252C%2520and%2520Reasoning%2520Behaviors%26entry.906535625%3DUsman%2520Syed%2520and%2520Ethan%2520Light%2520and%2520Xingang%2520Guo%2520and%2520Huan%2520Zhang%2520and%2520Lianhui%2520Qin%2520and%2520Yanfeng%2520Ouyang%2520and%2520Bin%2520Hu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520capabilities%2520of%2520state-of-the-art%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520such%2520as%2520GPT-4%252C%2520GPT-4o%252C%2520Claude%25203.5%2520Sonnet%252C%2520Claude%25203%2520Opus%252C%2520Gemini%250A1.5%2520Pro%252C%2520Llama%25203%252C%2520and%2520Llama%25203.1%2520in%2520solving%2520some%2520selected%2520undergraduate-level%250Atransportation%2520engineering%2520problems.%2520We%2520introduce%2520TransportBench%252C%2520a%2520benchmark%250Adataset%2520that%2520includes%2520a%2520sample%2520of%2520transportation%2520engineering%2520problems%2520on%2520a%2520wide%250Arange%2520of%2520subjects%2520in%2520the%2520context%2520of%2520planning%252C%2520design%252C%2520management%252C%2520and%2520control%250Aof%2520transportation%2520systems.%2520This%2520dataset%2520is%2520used%2520by%2520human%2520experts%2520to%2520evaluate%250Athe%2520capabilities%2520of%2520various%2520commercial%2520and%2520open-sourced%2520LLMs%252C%2520especially%2520their%250Aaccuracy%252C%2520consistency%252C%2520and%2520reasoning%2520behaviors%252C%2520in%2520solving%2520transportation%250Aengineering%2520problems.%2520Our%2520comprehensive%2520analysis%2520uncovers%2520the%2520unique%2520strengths%250Aand%2520limitations%2520of%2520each%2520LLM%252C%2520e.g.%2520our%2520analysis%2520shows%2520the%2520impressive%2520accuracy%250Aand%2520some%2520unexpected%2520inconsistent%2520behaviors%2520of%2520Claude%25203.5%2520Sonnet%2520in%2520solving%250ATransportBench%2520problems.%2520Our%2520study%2520marks%2520a%2520thrilling%2520first%2520step%2520toward%250Aharnessing%2520artificial%2520general%2520intelligence%2520for%2520complex%2520transportation%250Achallenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20Capabilities%20of%20Large%20Language%20Models%20in%20Transportation%0A%20%20System%20Engineering%3A%20Accuracy%2C%20Consistency%2C%20and%20Reasoning%20Behaviors&entry.906535625=Usman%20Syed%20and%20Ethan%20Light%20and%20Xingang%20Guo%20and%20Huan%20Zhang%20and%20Lianhui%20Qin%20and%20Yanfeng%20Ouyang%20and%20Bin%20Hu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explore%20the%20capabilities%20of%20state-of-the-art%20large%20language%0Amodels%20%28LLMs%29%20such%20as%20GPT-4%2C%20GPT-4o%2C%20Claude%203.5%20Sonnet%2C%20Claude%203%20Opus%2C%20Gemini%0A1.5%20Pro%2C%20Llama%203%2C%20and%20Llama%203.1%20in%20solving%20some%20selected%20undergraduate-level%0Atransportation%20engineering%20problems.%20We%20introduce%20TransportBench%2C%20a%20benchmark%0Adataset%20that%20includes%20a%20sample%20of%20transportation%20engineering%20problems%20on%20a%20wide%0Arange%20of%20subjects%20in%20the%20context%20of%20planning%2C%20design%2C%20management%2C%20and%20control%0Aof%20transportation%20systems.%20This%20dataset%20is%20used%20by%20human%20experts%20to%20evaluate%0Athe%20capabilities%20of%20various%20commercial%20and%20open-sourced%20LLMs%2C%20especially%20their%0Aaccuracy%2C%20consistency%2C%20and%20reasoning%20behaviors%2C%20in%20solving%20transportation%0Aengineering%20problems.%20Our%20comprehensive%20analysis%20uncovers%20the%20unique%20strengths%0Aand%20limitations%20of%20each%20LLM%2C%20e.g.%20our%20analysis%20shows%20the%20impressive%20accuracy%0Aand%20some%20unexpected%20inconsistent%20behaviors%20of%20Claude%203.5%20Sonnet%20in%20solving%0ATransportBench%20problems.%20Our%20study%20marks%20a%20thrilling%20first%20step%20toward%0Aharnessing%20artificial%20general%20intelligence%20for%20complex%20transportation%0Achallenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08302v1&entry.124074799=Read"},
{"title": "BINDy -- Bayesian identification of nonlinear dynamics with\n  reversible-jump Markov-chain Monte-Carlo", "author": "Max D. Champneys and Timothy J. Rogers", "abstract": "  Model parsimony is an important \\emph{cognitive bias} in data-driven\nmodelling that aids interpretability and helps to prevent over-fitting. Sparse\nidentification of nonlinear dynamics (SINDy) methods are able to learn sparse\nrepresentations of complex dynamics directly from data, given a basis of\nlibrary functions. In this work, a novel Bayesian treatment of dictionary\nlearning system identification, as an alternative to SINDy, is envisaged. The\nproposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is\ndistinct from previous approaches in that it targets the full joint posterior\ndistribution over both the terms in the library and their parameterisation in\nthe model. This formulation confers the advantage that an arbitrary prior may\nbe placed over the model structure to produce models that are sparse in the\nmodel space rather than in parameter space. Because this posterior is defined\nover parameter vectors that can change in dimension, the inference cannot be\nperformed by standard techniques. Instead, a Gibbs sampler based on\nreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare\nfavourably to ensemble SINDy in three benchmark case-studies. In particular, it\nis seen that the proposed method is better able to assign high probability to\ncorrect model terms.\n", "link": "http://arxiv.org/abs/2408.08062v1", "date": "2024-08-15", "relevancy": 1.8564, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5426}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.452}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BINDy%20--%20Bayesian%20identification%20of%20nonlinear%20dynamics%20with%0A%20%20reversible-jump%20Markov-chain%20Monte-Carlo&body=Title%3A%20BINDy%20--%20Bayesian%20identification%20of%20nonlinear%20dynamics%20with%0A%20%20reversible-jump%20Markov-chain%20Monte-Carlo%0AAuthor%3A%20Max%20D.%20Champneys%20and%20Timothy%20J.%20Rogers%0AAbstract%3A%20%20%20Model%20parsimony%20is%20an%20important%20%5Cemph%7Bcognitive%20bias%7D%20in%20data-driven%0Amodelling%20that%20aids%20interpretability%20and%20helps%20to%20prevent%20over-fitting.%20Sparse%0Aidentification%20of%20nonlinear%20dynamics%20%28SINDy%29%20methods%20are%20able%20to%20learn%20sparse%0Arepresentations%20of%20complex%20dynamics%20directly%20from%20data%2C%20given%20a%20basis%20of%0Alibrary%20functions.%20In%20this%20work%2C%20a%20novel%20Bayesian%20treatment%20of%20dictionary%0Alearning%20system%20identification%2C%20as%20an%20alternative%20to%20SINDy%2C%20is%20envisaged.%20The%0Aproposed%20method%20--%20Bayesian%20identification%20of%20nonlinear%20dynamics%20%28BINDy%29%20--%20is%0Adistinct%20from%20previous%20approaches%20in%20that%20it%20targets%20the%20full%20joint%20posterior%0Adistribution%20over%20both%20the%20terms%20in%20the%20library%20and%20their%20parameterisation%20in%0Athe%20model.%20This%20formulation%20confers%20the%20advantage%20that%20an%20arbitrary%20prior%20may%0Abe%20placed%20over%20the%20model%20structure%20to%20produce%20models%20that%20are%20sparse%20in%20the%0Amodel%20space%20rather%20than%20in%20parameter%20space.%20Because%20this%20posterior%20is%20defined%0Aover%20parameter%20vectors%20that%20can%20change%20in%20dimension%2C%20the%20inference%20cannot%20be%0Aperformed%20by%20standard%20techniques.%20Instead%2C%20a%20Gibbs%20sampler%20based%20on%0Areversible-jump%20Markov-chain%20Monte-Carlo%20is%20proposed.%20BINDy%20is%20shown%20to%20compare%0Afavourably%20to%20ensemble%20SINDy%20in%20three%20benchmark%20case-studies.%20In%20particular%2C%20it%0Ais%20seen%20that%20the%20proposed%20method%20is%20better%20able%20to%20assign%20high%20probability%20to%0Acorrect%20model%20terms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08062v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBINDy%2520--%2520Bayesian%2520identification%2520of%2520nonlinear%2520dynamics%2520with%250A%2520%2520reversible-jump%2520Markov-chain%2520Monte-Carlo%26entry.906535625%3DMax%2520D.%2520Champneys%2520and%2520Timothy%2520J.%2520Rogers%26entry.1292438233%3D%2520%2520Model%2520parsimony%2520is%2520an%2520important%2520%255Cemph%257Bcognitive%2520bias%257D%2520in%2520data-driven%250Amodelling%2520that%2520aids%2520interpretability%2520and%2520helps%2520to%2520prevent%2520over-fitting.%2520Sparse%250Aidentification%2520of%2520nonlinear%2520dynamics%2520%2528SINDy%2529%2520methods%2520are%2520able%2520to%2520learn%2520sparse%250Arepresentations%2520of%2520complex%2520dynamics%2520directly%2520from%2520data%252C%2520given%2520a%2520basis%2520of%250Alibrary%2520functions.%2520In%2520this%2520work%252C%2520a%2520novel%2520Bayesian%2520treatment%2520of%2520dictionary%250Alearning%2520system%2520identification%252C%2520as%2520an%2520alternative%2520to%2520SINDy%252C%2520is%2520envisaged.%2520The%250Aproposed%2520method%2520--%2520Bayesian%2520identification%2520of%2520nonlinear%2520dynamics%2520%2528BINDy%2529%2520--%2520is%250Adistinct%2520from%2520previous%2520approaches%2520in%2520that%2520it%2520targets%2520the%2520full%2520joint%2520posterior%250Adistribution%2520over%2520both%2520the%2520terms%2520in%2520the%2520library%2520and%2520their%2520parameterisation%2520in%250Athe%2520model.%2520This%2520formulation%2520confers%2520the%2520advantage%2520that%2520an%2520arbitrary%2520prior%2520may%250Abe%2520placed%2520over%2520the%2520model%2520structure%2520to%2520produce%2520models%2520that%2520are%2520sparse%2520in%2520the%250Amodel%2520space%2520rather%2520than%2520in%2520parameter%2520space.%2520Because%2520this%2520posterior%2520is%2520defined%250Aover%2520parameter%2520vectors%2520that%2520can%2520change%2520in%2520dimension%252C%2520the%2520inference%2520cannot%2520be%250Aperformed%2520by%2520standard%2520techniques.%2520Instead%252C%2520a%2520Gibbs%2520sampler%2520based%2520on%250Areversible-jump%2520Markov-chain%2520Monte-Carlo%2520is%2520proposed.%2520BINDy%2520is%2520shown%2520to%2520compare%250Afavourably%2520to%2520ensemble%2520SINDy%2520in%2520three%2520benchmark%2520case-studies.%2520In%2520particular%252C%2520it%250Ais%2520seen%2520that%2520the%2520proposed%2520method%2520is%2520better%2520able%2520to%2520assign%2520high%2520probability%2520to%250Acorrect%2520model%2520terms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08062v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BINDy%20--%20Bayesian%20identification%20of%20nonlinear%20dynamics%20with%0A%20%20reversible-jump%20Markov-chain%20Monte-Carlo&entry.906535625=Max%20D.%20Champneys%20and%20Timothy%20J.%20Rogers&entry.1292438233=%20%20Model%20parsimony%20is%20an%20important%20%5Cemph%7Bcognitive%20bias%7D%20in%20data-driven%0Amodelling%20that%20aids%20interpretability%20and%20helps%20to%20prevent%20over-fitting.%20Sparse%0Aidentification%20of%20nonlinear%20dynamics%20%28SINDy%29%20methods%20are%20able%20to%20learn%20sparse%0Arepresentations%20of%20complex%20dynamics%20directly%20from%20data%2C%20given%20a%20basis%20of%0Alibrary%20functions.%20In%20this%20work%2C%20a%20novel%20Bayesian%20treatment%20of%20dictionary%0Alearning%20system%20identification%2C%20as%20an%20alternative%20to%20SINDy%2C%20is%20envisaged.%20The%0Aproposed%20method%20--%20Bayesian%20identification%20of%20nonlinear%20dynamics%20%28BINDy%29%20--%20is%0Adistinct%20from%20previous%20approaches%20in%20that%20it%20targets%20the%20full%20joint%20posterior%0Adistribution%20over%20both%20the%20terms%20in%20the%20library%20and%20their%20parameterisation%20in%0Athe%20model.%20This%20formulation%20confers%20the%20advantage%20that%20an%20arbitrary%20prior%20may%0Abe%20placed%20over%20the%20model%20structure%20to%20produce%20models%20that%20are%20sparse%20in%20the%0Amodel%20space%20rather%20than%20in%20parameter%20space.%20Because%20this%20posterior%20is%20defined%0Aover%20parameter%20vectors%20that%20can%20change%20in%20dimension%2C%20the%20inference%20cannot%20be%0Aperformed%20by%20standard%20techniques.%20Instead%2C%20a%20Gibbs%20sampler%20based%20on%0Areversible-jump%20Markov-chain%20Monte-Carlo%20is%20proposed.%20BINDy%20is%20shown%20to%20compare%0Afavourably%20to%20ensemble%20SINDy%20in%20three%20benchmark%20case-studies.%20In%20particular%2C%20it%0Ais%20seen%20that%20the%20proposed%20method%20is%20better%20able%20to%20assign%20high%20probability%20to%0Acorrect%20model%20terms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08062v1&entry.124074799=Read"},
{"title": "Exact Tensor Completion Powered by Slim Transforms", "author": "Li Ge and Lin Chen and Yudong Chen and Xue Jiang", "abstract": "  In this work, a tensor completion problem is studied, which aims to perfectly\nrecover the tensor from partial observations. The existing theoretical\nguarantee requires the involved transform to be orthogonal, which hinders its\napplications. In this paper, jumping out of the constraints of isotropy and\nself-adjointness, the theoretical guarantee of exact tensor completion with\narbitrary linear transforms is established by directly operating the tensors in\nthe transform domain. With the enriched choices of transforms, a new analysis\nobtained by the proof discloses why slim transforms outperform their square\ncounterparts from a theoretical level. Our model and proof greatly enhance the\nflexibility of tensor completion and extensive experiments validate the\nsuperiority of the proposed method.\n", "link": "http://arxiv.org/abs/2402.03468v2", "date": "2024-08-15", "relevancy": 1.8547, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4766}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4624}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%20Tensor%20Completion%20Powered%20by%20Slim%20Transforms&body=Title%3A%20Exact%20Tensor%20Completion%20Powered%20by%20Slim%20Transforms%0AAuthor%3A%20Li%20Ge%20and%20Lin%20Chen%20and%20Yudong%20Chen%20and%20Xue%20Jiang%0AAbstract%3A%20%20%20In%20this%20work%2C%20a%20tensor%20completion%20problem%20is%20studied%2C%20which%20aims%20to%20perfectly%0Arecover%20the%20tensor%20from%20partial%20observations.%20The%20existing%20theoretical%0Aguarantee%20requires%20the%20involved%20transform%20to%20be%20orthogonal%2C%20which%20hinders%20its%0Aapplications.%20In%20this%20paper%2C%20jumping%20out%20of%20the%20constraints%20of%20isotropy%20and%0Aself-adjointness%2C%20the%20theoretical%20guarantee%20of%20exact%20tensor%20completion%20with%0Aarbitrary%20linear%20transforms%20is%20established%20by%20directly%20operating%20the%20tensors%20in%0Athe%20transform%20domain.%20With%20the%20enriched%20choices%20of%20transforms%2C%20a%20new%20analysis%0Aobtained%20by%20the%20proof%20discloses%20why%20slim%20transforms%20outperform%20their%20square%0Acounterparts%20from%20a%20theoretical%20level.%20Our%20model%20and%20proof%20greatly%20enhance%20the%0Aflexibility%20of%20tensor%20completion%20and%20extensive%20experiments%20validate%20the%0Asuperiority%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%2520Tensor%2520Completion%2520Powered%2520by%2520Slim%2520Transforms%26entry.906535625%3DLi%2520Ge%2520and%2520Lin%2520Chen%2520and%2520Yudong%2520Chen%2520and%2520Xue%2520Jiang%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520a%2520tensor%2520completion%2520problem%2520is%2520studied%252C%2520which%2520aims%2520to%2520perfectly%250Arecover%2520the%2520tensor%2520from%2520partial%2520observations.%2520The%2520existing%2520theoretical%250Aguarantee%2520requires%2520the%2520involved%2520transform%2520to%2520be%2520orthogonal%252C%2520which%2520hinders%2520its%250Aapplications.%2520In%2520this%2520paper%252C%2520jumping%2520out%2520of%2520the%2520constraints%2520of%2520isotropy%2520and%250Aself-adjointness%252C%2520the%2520theoretical%2520guarantee%2520of%2520exact%2520tensor%2520completion%2520with%250Aarbitrary%2520linear%2520transforms%2520is%2520established%2520by%2520directly%2520operating%2520the%2520tensors%2520in%250Athe%2520transform%2520domain.%2520With%2520the%2520enriched%2520choices%2520of%2520transforms%252C%2520a%2520new%2520analysis%250Aobtained%2520by%2520the%2520proof%2520discloses%2520why%2520slim%2520transforms%2520outperform%2520their%2520square%250Acounterparts%2520from%2520a%2520theoretical%2520level.%2520Our%2520model%2520and%2520proof%2520greatly%2520enhance%2520the%250Aflexibility%2520of%2520tensor%2520completion%2520and%2520extensive%2520experiments%2520validate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%20Tensor%20Completion%20Powered%20by%20Slim%20Transforms&entry.906535625=Li%20Ge%20and%20Lin%20Chen%20and%20Yudong%20Chen%20and%20Xue%20Jiang&entry.1292438233=%20%20In%20this%20work%2C%20a%20tensor%20completion%20problem%20is%20studied%2C%20which%20aims%20to%20perfectly%0Arecover%20the%20tensor%20from%20partial%20observations.%20The%20existing%20theoretical%0Aguarantee%20requires%20the%20involved%20transform%20to%20be%20orthogonal%2C%20which%20hinders%20its%0Aapplications.%20In%20this%20paper%2C%20jumping%20out%20of%20the%20constraints%20of%20isotropy%20and%0Aself-adjointness%2C%20the%20theoretical%20guarantee%20of%20exact%20tensor%20completion%20with%0Aarbitrary%20linear%20transforms%20is%20established%20by%20directly%20operating%20the%20tensors%20in%0Athe%20transform%20domain.%20With%20the%20enriched%20choices%20of%20transforms%2C%20a%20new%20analysis%0Aobtained%20by%20the%20proof%20discloses%20why%20slim%20transforms%20outperform%20their%20square%0Acounterparts%20from%20a%20theoretical%20level.%20Our%20model%20and%20proof%20greatly%20enhance%20the%0Aflexibility%20of%20tensor%20completion%20and%20extensive%20experiments%20validate%20the%0Asuperiority%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03468v2&entry.124074799=Read"},
{"title": "Stochastic Semi-Gradient Descent for Learning Mean Field Games with\n  Population-Aware Function Approximation", "author": "Chenyu Zhang and Xu Chen and Xuan Di", "abstract": "  Mean field games (MFGs) model the interactions within a large-population\nmulti-agent system using the population distribution. Traditional learning\nmethods for MFGs are based on fixed-point iteration (FPI), which calculates\nbest responses and induced population distribution separately and sequentially.\nHowever, FPI-type methods suffer from inefficiency and instability, due to\noscillations caused by the forward-backward procedure. This paper considers an\nonline learning method for MFGs, where an agent updates its policy and\npopulation estimates simultaneously and fully asynchronously, resulting in a\nsimple stochastic gradient descent (SGD) type method called SemiSGD. Not only\ndoes SemiSGD exhibit numerical stability and efficiency, but it also provides a\nnovel perspective by treating the value function and population distribution as\na unified parameter. We theoretically show that SemiSGD directs this unified\nparameter along a descent direction to the mean field equilibrium. Motivated by\nthis perspective, we develop a linear function approximation (LFA) for both the\nvalue function and the population distribution, resulting in the first\npopulation-aware LFA for MFGs on continuous state-action space. Finite-time\nconvergence and approximation error analysis are provided for SemiSGD equipped\nwith population-aware LFA.\n", "link": "http://arxiv.org/abs/2408.08192v1", "date": "2024-08-15", "relevancy": 1.8485, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.506}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4559}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Semi-Gradient%20Descent%20for%20Learning%20Mean%20Field%20Games%20with%0A%20%20Population-Aware%20Function%20Approximation&body=Title%3A%20Stochastic%20Semi-Gradient%20Descent%20for%20Learning%20Mean%20Field%20Games%20with%0A%20%20Population-Aware%20Function%20Approximation%0AAuthor%3A%20Chenyu%20Zhang%20and%20Xu%20Chen%20and%20Xuan%20Di%0AAbstract%3A%20%20%20Mean%20field%20games%20%28MFGs%29%20model%20the%20interactions%20within%20a%20large-population%0Amulti-agent%20system%20using%20the%20population%20distribution.%20Traditional%20learning%0Amethods%20for%20MFGs%20are%20based%20on%20fixed-point%20iteration%20%28FPI%29%2C%20which%20calculates%0Abest%20responses%20and%20induced%20population%20distribution%20separately%20and%20sequentially.%0AHowever%2C%20FPI-type%20methods%20suffer%20from%20inefficiency%20and%20instability%2C%20due%20to%0Aoscillations%20caused%20by%20the%20forward-backward%20procedure.%20This%20paper%20considers%20an%0Aonline%20learning%20method%20for%20MFGs%2C%20where%20an%20agent%20updates%20its%20policy%20and%0Apopulation%20estimates%20simultaneously%20and%20fully%20asynchronously%2C%20resulting%20in%20a%0Asimple%20stochastic%20gradient%20descent%20%28SGD%29%20type%20method%20called%20SemiSGD.%20Not%20only%0Adoes%20SemiSGD%20exhibit%20numerical%20stability%20and%20efficiency%2C%20but%20it%20also%20provides%20a%0Anovel%20perspective%20by%20treating%20the%20value%20function%20and%20population%20distribution%20as%0Aa%20unified%20parameter.%20We%20theoretically%20show%20that%20SemiSGD%20directs%20this%20unified%0Aparameter%20along%20a%20descent%20direction%20to%20the%20mean%20field%20equilibrium.%20Motivated%20by%0Athis%20perspective%2C%20we%20develop%20a%20linear%20function%20approximation%20%28LFA%29%20for%20both%20the%0Avalue%20function%20and%20the%20population%20distribution%2C%20resulting%20in%20the%20first%0Apopulation-aware%20LFA%20for%20MFGs%20on%20continuous%20state-action%20space.%20Finite-time%0Aconvergence%20and%20approximation%20error%20analysis%20are%20provided%20for%20SemiSGD%20equipped%0Awith%20population-aware%20LFA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Semi-Gradient%2520Descent%2520for%2520Learning%2520Mean%2520Field%2520Games%2520with%250A%2520%2520Population-Aware%2520Function%2520Approximation%26entry.906535625%3DChenyu%2520Zhang%2520and%2520Xu%2520Chen%2520and%2520Xuan%2520Di%26entry.1292438233%3D%2520%2520Mean%2520field%2520games%2520%2528MFGs%2529%2520model%2520the%2520interactions%2520within%2520a%2520large-population%250Amulti-agent%2520system%2520using%2520the%2520population%2520distribution.%2520Traditional%2520learning%250Amethods%2520for%2520MFGs%2520are%2520based%2520on%2520fixed-point%2520iteration%2520%2528FPI%2529%252C%2520which%2520calculates%250Abest%2520responses%2520and%2520induced%2520population%2520distribution%2520separately%2520and%2520sequentially.%250AHowever%252C%2520FPI-type%2520methods%2520suffer%2520from%2520inefficiency%2520and%2520instability%252C%2520due%2520to%250Aoscillations%2520caused%2520by%2520the%2520forward-backward%2520procedure.%2520This%2520paper%2520considers%2520an%250Aonline%2520learning%2520method%2520for%2520MFGs%252C%2520where%2520an%2520agent%2520updates%2520its%2520policy%2520and%250Apopulation%2520estimates%2520simultaneously%2520and%2520fully%2520asynchronously%252C%2520resulting%2520in%2520a%250Asimple%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520type%2520method%2520called%2520SemiSGD.%2520Not%2520only%250Adoes%2520SemiSGD%2520exhibit%2520numerical%2520stability%2520and%2520efficiency%252C%2520but%2520it%2520also%2520provides%2520a%250Anovel%2520perspective%2520by%2520treating%2520the%2520value%2520function%2520and%2520population%2520distribution%2520as%250Aa%2520unified%2520parameter.%2520We%2520theoretically%2520show%2520that%2520SemiSGD%2520directs%2520this%2520unified%250Aparameter%2520along%2520a%2520descent%2520direction%2520to%2520the%2520mean%2520field%2520equilibrium.%2520Motivated%2520by%250Athis%2520perspective%252C%2520we%2520develop%2520a%2520linear%2520function%2520approximation%2520%2528LFA%2529%2520for%2520both%2520the%250Avalue%2520function%2520and%2520the%2520population%2520distribution%252C%2520resulting%2520in%2520the%2520first%250Apopulation-aware%2520LFA%2520for%2520MFGs%2520on%2520continuous%2520state-action%2520space.%2520Finite-time%250Aconvergence%2520and%2520approximation%2520error%2520analysis%2520are%2520provided%2520for%2520SemiSGD%2520equipped%250Awith%2520population-aware%2520LFA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Semi-Gradient%20Descent%20for%20Learning%20Mean%20Field%20Games%20with%0A%20%20Population-Aware%20Function%20Approximation&entry.906535625=Chenyu%20Zhang%20and%20Xu%20Chen%20and%20Xuan%20Di&entry.1292438233=%20%20Mean%20field%20games%20%28MFGs%29%20model%20the%20interactions%20within%20a%20large-population%0Amulti-agent%20system%20using%20the%20population%20distribution.%20Traditional%20learning%0Amethods%20for%20MFGs%20are%20based%20on%20fixed-point%20iteration%20%28FPI%29%2C%20which%20calculates%0Abest%20responses%20and%20induced%20population%20distribution%20separately%20and%20sequentially.%0AHowever%2C%20FPI-type%20methods%20suffer%20from%20inefficiency%20and%20instability%2C%20due%20to%0Aoscillations%20caused%20by%20the%20forward-backward%20procedure.%20This%20paper%20considers%20an%0Aonline%20learning%20method%20for%20MFGs%2C%20where%20an%20agent%20updates%20its%20policy%20and%0Apopulation%20estimates%20simultaneously%20and%20fully%20asynchronously%2C%20resulting%20in%20a%0Asimple%20stochastic%20gradient%20descent%20%28SGD%29%20type%20method%20called%20SemiSGD.%20Not%20only%0Adoes%20SemiSGD%20exhibit%20numerical%20stability%20and%20efficiency%2C%20but%20it%20also%20provides%20a%0Anovel%20perspective%20by%20treating%20the%20value%20function%20and%20population%20distribution%20as%0Aa%20unified%20parameter.%20We%20theoretically%20show%20that%20SemiSGD%20directs%20this%20unified%0Aparameter%20along%20a%20descent%20direction%20to%20the%20mean%20field%20equilibrium.%20Motivated%20by%0Athis%20perspective%2C%20we%20develop%20a%20linear%20function%20approximation%20%28LFA%29%20for%20both%20the%0Avalue%20function%20and%20the%20population%20distribution%2C%20resulting%20in%20the%20first%0Apopulation-aware%20LFA%20for%20MFGs%20on%20continuous%20state-action%20space.%20Finite-time%0Aconvergence%20and%20approximation%20error%20analysis%20are%20provided%20for%20SemiSGD%20equipped%0Awith%20population-aware%20LFA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08192v1&entry.124074799=Read"},
{"title": "Enhancing Sharpness-Aware Minimization by Learning Perturbation Radius", "author": "Xuehao Wang and Weisen Jiang and Shuai Fu and Yu Zhang", "abstract": "  Sharpness-aware minimization (SAM) is to improve model generalization by\nsearching for flat minima in the loss landscape. The SAM update consists of one\nstep for computing the perturbation and the other for computing the update\ngradient. Within the two steps, the choice of the perturbation radius is\ncrucial to the performance of SAM, but finding an appropriate perturbation\nradius is challenging. In this paper, we propose a bilevel optimization\nframework called LEarning the perTurbation radiuS (LETS) to learn the\nperturbation radius for sharpness-aware minimization algorithms. Specifically,\nin the proposed LETS method, the upper-level problem aims at seeking a good\nperturbation radius by minimizing the squared generalization gap between the\ntraining and validation losses, while the lower-level problem is the SAM\noptimization problem. Moreover, the LETS method can be combined with any\nvariant of SAM. Experimental results on various architectures and benchmark\ndatasets in computer vision and natural language processing demonstrate the\neffectiveness of the proposed LETS method in improving the performance of SAM.\n", "link": "http://arxiv.org/abs/2408.08222v1", "date": "2024-08-15", "relevancy": 1.8378, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4653}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.458}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Sharpness-Aware%20Minimization%20by%20Learning%20Perturbation%20Radius&body=Title%3A%20Enhancing%20Sharpness-Aware%20Minimization%20by%20Learning%20Perturbation%20Radius%0AAuthor%3A%20Xuehao%20Wang%20and%20Weisen%20Jiang%20and%20Shuai%20Fu%20and%20Yu%20Zhang%0AAbstract%3A%20%20%20Sharpness-aware%20minimization%20%28SAM%29%20is%20to%20improve%20model%20generalization%20by%0Asearching%20for%20flat%20minima%20in%20the%20loss%20landscape.%20The%20SAM%20update%20consists%20of%20one%0Astep%20for%20computing%20the%20perturbation%20and%20the%20other%20for%20computing%20the%20update%0Agradient.%20Within%20the%20two%20steps%2C%20the%20choice%20of%20the%20perturbation%20radius%20is%0Acrucial%20to%20the%20performance%20of%20SAM%2C%20but%20finding%20an%20appropriate%20perturbation%0Aradius%20is%20challenging.%20In%20this%20paper%2C%20we%20propose%20a%20bilevel%20optimization%0Aframework%20called%20LEarning%20the%20perTurbation%20radiuS%20%28LETS%29%20to%20learn%20the%0Aperturbation%20radius%20for%20sharpness-aware%20minimization%20algorithms.%20Specifically%2C%0Ain%20the%20proposed%20LETS%20method%2C%20the%20upper-level%20problem%20aims%20at%20seeking%20a%20good%0Aperturbation%20radius%20by%20minimizing%20the%20squared%20generalization%20gap%20between%20the%0Atraining%20and%20validation%20losses%2C%20while%20the%20lower-level%20problem%20is%20the%20SAM%0Aoptimization%20problem.%20Moreover%2C%20the%20LETS%20method%20can%20be%20combined%20with%20any%0Avariant%20of%20SAM.%20Experimental%20results%20on%20various%20architectures%20and%20benchmark%0Adatasets%20in%20computer%20vision%20and%20natural%20language%20processing%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20LETS%20method%20in%20improving%20the%20performance%20of%20SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Sharpness-Aware%2520Minimization%2520by%2520Learning%2520Perturbation%2520Radius%26entry.906535625%3DXuehao%2520Wang%2520and%2520Weisen%2520Jiang%2520and%2520Shuai%2520Fu%2520and%2520Yu%2520Zhang%26entry.1292438233%3D%2520%2520Sharpness-aware%2520minimization%2520%2528SAM%2529%2520is%2520to%2520improve%2520model%2520generalization%2520by%250Asearching%2520for%2520flat%2520minima%2520in%2520the%2520loss%2520landscape.%2520The%2520SAM%2520update%2520consists%2520of%2520one%250Astep%2520for%2520computing%2520the%2520perturbation%2520and%2520the%2520other%2520for%2520computing%2520the%2520update%250Agradient.%2520Within%2520the%2520two%2520steps%252C%2520the%2520choice%2520of%2520the%2520perturbation%2520radius%2520is%250Acrucial%2520to%2520the%2520performance%2520of%2520SAM%252C%2520but%2520finding%2520an%2520appropriate%2520perturbation%250Aradius%2520is%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520bilevel%2520optimization%250Aframework%2520called%2520LEarning%2520the%2520perTurbation%2520radiuS%2520%2528LETS%2529%2520to%2520learn%2520the%250Aperturbation%2520radius%2520for%2520sharpness-aware%2520minimization%2520algorithms.%2520Specifically%252C%250Ain%2520the%2520proposed%2520LETS%2520method%252C%2520the%2520upper-level%2520problem%2520aims%2520at%2520seeking%2520a%2520good%250Aperturbation%2520radius%2520by%2520minimizing%2520the%2520squared%2520generalization%2520gap%2520between%2520the%250Atraining%2520and%2520validation%2520losses%252C%2520while%2520the%2520lower-level%2520problem%2520is%2520the%2520SAM%250Aoptimization%2520problem.%2520Moreover%252C%2520the%2520LETS%2520method%2520can%2520be%2520combined%2520with%2520any%250Avariant%2520of%2520SAM.%2520Experimental%2520results%2520on%2520various%2520architectures%2520and%2520benchmark%250Adatasets%2520in%2520computer%2520vision%2520and%2520natural%2520language%2520processing%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520LETS%2520method%2520in%2520improving%2520the%2520performance%2520of%2520SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Sharpness-Aware%20Minimization%20by%20Learning%20Perturbation%20Radius&entry.906535625=Xuehao%20Wang%20and%20Weisen%20Jiang%20and%20Shuai%20Fu%20and%20Yu%20Zhang&entry.1292438233=%20%20Sharpness-aware%20minimization%20%28SAM%29%20is%20to%20improve%20model%20generalization%20by%0Asearching%20for%20flat%20minima%20in%20the%20loss%20landscape.%20The%20SAM%20update%20consists%20of%20one%0Astep%20for%20computing%20the%20perturbation%20and%20the%20other%20for%20computing%20the%20update%0Agradient.%20Within%20the%20two%20steps%2C%20the%20choice%20of%20the%20perturbation%20radius%20is%0Acrucial%20to%20the%20performance%20of%20SAM%2C%20but%20finding%20an%20appropriate%20perturbation%0Aradius%20is%20challenging.%20In%20this%20paper%2C%20we%20propose%20a%20bilevel%20optimization%0Aframework%20called%20LEarning%20the%20perTurbation%20radiuS%20%28LETS%29%20to%20learn%20the%0Aperturbation%20radius%20for%20sharpness-aware%20minimization%20algorithms.%20Specifically%2C%0Ain%20the%20proposed%20LETS%20method%2C%20the%20upper-level%20problem%20aims%20at%20seeking%20a%20good%0Aperturbation%20radius%20by%20minimizing%20the%20squared%20generalization%20gap%20between%20the%0Atraining%20and%20validation%20losses%2C%20while%20the%20lower-level%20problem%20is%20the%20SAM%0Aoptimization%20problem.%20Moreover%2C%20the%20LETS%20method%20can%20be%20combined%20with%20any%0Avariant%20of%20SAM.%20Experimental%20results%20on%20various%20architectures%20and%20benchmark%0Adatasets%20in%20computer%20vision%20and%20natural%20language%20processing%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20LETS%20method%20in%20improving%20the%20performance%20of%20SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08222v1&entry.124074799=Read"},
{"title": "Extracting Sentence Embeddings from Pretrained Transformer Models", "author": "Lukas Stankevi\u010dius and Mantas Luko\u0161evi\u010dius", "abstract": "  Background/introduction: Pre-trained transformer models shine in many natural\nlanguage processing tasks and therefore are expected to bear the representation\nof the input sentence or text meaning. These sentence-level embeddings are also\nimportant in retrieval-augmented generation. But do commonly used plain\naveraging or prompt templates surface it enough?\n  Methods: Given 110M parameters BERT's hidden representations from multiple\nlayers and multiple tokens we tried various ways to extract optimal sentence\nrepresentations. We tested various token aggregation and representation\npost-processing techniques. We also tested multiple ways of using a general\nWikitext dataset to complement BERTs sentence representations. All methods were\ntested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12\nclassification tasks. We also evaluated our representation-shaping techniques\non other static models, including random token representations.\n  Results: Proposed representation extraction methods improved the performance\non STS and clustering tasks for all models considered. Very high improvements\nfor static token-based models, especially random embeddings for STS tasks\nalmost reach the performance of BERT-derived representations.\n  Conclusions: Our work shows that for multiple tasks simple baselines with\nrepresentation shaping techniques reach or even outperform more complex\nBERT-based models or are able to contribute to their performance.\n", "link": "http://arxiv.org/abs/2408.08073v1", "date": "2024-08-15", "relevancy": 1.8312, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20Sentence%20Embeddings%20from%20Pretrained%20Transformer%20Models&body=Title%3A%20Extracting%20Sentence%20Embeddings%20from%20Pretrained%20Transformer%20Models%0AAuthor%3A%20Lukas%20Stankevi%C4%8Dius%20and%20Mantas%20Luko%C5%A1evi%C4%8Dius%0AAbstract%3A%20%20%20Background/introduction%3A%20Pre-trained%20transformer%20models%20shine%20in%20many%20natural%0Alanguage%20processing%20tasks%20and%20therefore%20are%20expected%20to%20bear%20the%20representation%0Aof%20the%20input%20sentence%20or%20text%20meaning.%20These%20sentence-level%20embeddings%20are%20also%0Aimportant%20in%20retrieval-augmented%20generation.%20But%20do%20commonly%20used%20plain%0Aaveraging%20or%20prompt%20templates%20surface%20it%20enough%3F%0A%20%20Methods%3A%20Given%20110M%20parameters%20BERT%27s%20hidden%20representations%20from%20multiple%0Alayers%20and%20multiple%20tokens%20we%20tried%20various%20ways%20to%20extract%20optimal%20sentence%0Arepresentations.%20We%20tested%20various%20token%20aggregation%20and%20representation%0Apost-processing%20techniques.%20We%20also%20tested%20multiple%20ways%20of%20using%20a%20general%0AWikitext%20dataset%20to%20complement%20BERTs%20sentence%20representations.%20All%20methods%20were%0Atested%20on%208%20Semantic%20Textual%20Similarity%20%28STS%29%2C%206%20short%20text%20clustering%2C%20and%2012%0Aclassification%20tasks.%20We%20also%20evaluated%20our%20representation-shaping%20techniques%0Aon%20other%20static%20models%2C%20including%20random%20token%20representations.%0A%20%20Results%3A%20Proposed%20representation%20extraction%20methods%20improved%20the%20performance%0Aon%20STS%20and%20clustering%20tasks%20for%20all%20models%20considered.%20Very%20high%20improvements%0Afor%20static%20token-based%20models%2C%20especially%20random%20embeddings%20for%20STS%20tasks%0Aalmost%20reach%20the%20performance%20of%20BERT-derived%20representations.%0A%20%20Conclusions%3A%20Our%20work%20shows%20that%20for%20multiple%20tasks%20simple%20baselines%20with%0Arepresentation%20shaping%20techniques%20reach%20or%20even%20outperform%20more%20complex%0ABERT-based%20models%20or%20are%20able%20to%20contribute%20to%20their%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520Sentence%2520Embeddings%2520from%2520Pretrained%2520Transformer%2520Models%26entry.906535625%3DLukas%2520Stankevi%25C4%258Dius%2520and%2520Mantas%2520Luko%25C5%25A1evi%25C4%258Dius%26entry.1292438233%3D%2520%2520Background/introduction%253A%2520Pre-trained%2520transformer%2520models%2520shine%2520in%2520many%2520natural%250Alanguage%2520processing%2520tasks%2520and%2520therefore%2520are%2520expected%2520to%2520bear%2520the%2520representation%250Aof%2520the%2520input%2520sentence%2520or%2520text%2520meaning.%2520These%2520sentence-level%2520embeddings%2520are%2520also%250Aimportant%2520in%2520retrieval-augmented%2520generation.%2520But%2520do%2520commonly%2520used%2520plain%250Aaveraging%2520or%2520prompt%2520templates%2520surface%2520it%2520enough%253F%250A%2520%2520Methods%253A%2520Given%2520110M%2520parameters%2520BERT%2527s%2520hidden%2520representations%2520from%2520multiple%250Alayers%2520and%2520multiple%2520tokens%2520we%2520tried%2520various%2520ways%2520to%2520extract%2520optimal%2520sentence%250Arepresentations.%2520We%2520tested%2520various%2520token%2520aggregation%2520and%2520representation%250Apost-processing%2520techniques.%2520We%2520also%2520tested%2520multiple%2520ways%2520of%2520using%2520a%2520general%250AWikitext%2520dataset%2520to%2520complement%2520BERTs%2520sentence%2520representations.%2520All%2520methods%2520were%250Atested%2520on%25208%2520Semantic%2520Textual%2520Similarity%2520%2528STS%2529%252C%25206%2520short%2520text%2520clustering%252C%2520and%252012%250Aclassification%2520tasks.%2520We%2520also%2520evaluated%2520our%2520representation-shaping%2520techniques%250Aon%2520other%2520static%2520models%252C%2520including%2520random%2520token%2520representations.%250A%2520%2520Results%253A%2520Proposed%2520representation%2520extraction%2520methods%2520improved%2520the%2520performance%250Aon%2520STS%2520and%2520clustering%2520tasks%2520for%2520all%2520models%2520considered.%2520Very%2520high%2520improvements%250Afor%2520static%2520token-based%2520models%252C%2520especially%2520random%2520embeddings%2520for%2520STS%2520tasks%250Aalmost%2520reach%2520the%2520performance%2520of%2520BERT-derived%2520representations.%250A%2520%2520Conclusions%253A%2520Our%2520work%2520shows%2520that%2520for%2520multiple%2520tasks%2520simple%2520baselines%2520with%250Arepresentation%2520shaping%2520techniques%2520reach%2520or%2520even%2520outperform%2520more%2520complex%250ABERT-based%2520models%2520or%2520are%2520able%2520to%2520contribute%2520to%2520their%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20Sentence%20Embeddings%20from%20Pretrained%20Transformer%20Models&entry.906535625=Lukas%20Stankevi%C4%8Dius%20and%20Mantas%20Luko%C5%A1evi%C4%8Dius&entry.1292438233=%20%20Background/introduction%3A%20Pre-trained%20transformer%20models%20shine%20in%20many%20natural%0Alanguage%20processing%20tasks%20and%20therefore%20are%20expected%20to%20bear%20the%20representation%0Aof%20the%20input%20sentence%20or%20text%20meaning.%20These%20sentence-level%20embeddings%20are%20also%0Aimportant%20in%20retrieval-augmented%20generation.%20But%20do%20commonly%20used%20plain%0Aaveraging%20or%20prompt%20templates%20surface%20it%20enough%3F%0A%20%20Methods%3A%20Given%20110M%20parameters%20BERT%27s%20hidden%20representations%20from%20multiple%0Alayers%20and%20multiple%20tokens%20we%20tried%20various%20ways%20to%20extract%20optimal%20sentence%0Arepresentations.%20We%20tested%20various%20token%20aggregation%20and%20representation%0Apost-processing%20techniques.%20We%20also%20tested%20multiple%20ways%20of%20using%20a%20general%0AWikitext%20dataset%20to%20complement%20BERTs%20sentence%20representations.%20All%20methods%20were%0Atested%20on%208%20Semantic%20Textual%20Similarity%20%28STS%29%2C%206%20short%20text%20clustering%2C%20and%2012%0Aclassification%20tasks.%20We%20also%20evaluated%20our%20representation-shaping%20techniques%0Aon%20other%20static%20models%2C%20including%20random%20token%20representations.%0A%20%20Results%3A%20Proposed%20representation%20extraction%20methods%20improved%20the%20performance%0Aon%20STS%20and%20clustering%20tasks%20for%20all%20models%20considered.%20Very%20high%20improvements%0Afor%20static%20token-based%20models%2C%20especially%20random%20embeddings%20for%20STS%20tasks%0Aalmost%20reach%20the%20performance%20of%20BERT-derived%20representations.%0A%20%20Conclusions%3A%20Our%20work%20shows%20that%20for%20multiple%20tasks%20simple%20baselines%20with%0Arepresentation%20shaping%20techniques%20reach%20or%20even%20outperform%20more%20complex%0ABERT-based%20models%20or%20are%20able%20to%20contribute%20to%20their%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08073v1&entry.124074799=Read"},
{"title": "Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature\n  Attribution Explainability", "author": "Joakim Edin and Andreas Geert Motzfeldt and Casper L. Christensen and Tuukka Ruotsalo and Lars Maal\u00f8e and Maria Maistro", "abstract": "  Deep neural network predictions are notoriously difficult to interpret.\nFeature attribution methods aim to explain these predictions by identifying the\ncontribution of each input feature. Faithfulness, often evaluated using the\narea over the perturbation curve (AOPC), reflects feature attributions'\naccuracy in describing the internal mechanisms of deep neural networks.\nHowever, many studies rely on AOPC to compare faithfulness across different\nmodels, which we show can lead to false conclusions about models' faithfulness.\nSpecifically, we find that AOPC is sensitive to variations in the model,\nresulting in unreliable cross-model comparisons. Moreover, AOPC scores are\ndifficult to interpret in isolation without knowing the model-specific lower\nand upper limits. To address these issues, we propose a normalization approach,\nNormalized AOPC (NAOPC), enabling consistent cross-model evaluations and more\nmeaningful interpretation of individual scores. Our experiments demonstrate\nthat this normalization can radically change AOPC results, questioning the\nconclusions of earlier studies and offering a more robust framework for\nassessing feature attribution faithfulness.\n", "link": "http://arxiv.org/abs/2408.08137v1", "date": "2024-08-15", "relevancy": 1.8178, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4708}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4518}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalized%20AOPC%3A%20Fixing%20Misleading%20Faithfulness%20Metrics%20for%20Feature%0A%20%20Attribution%20Explainability&body=Title%3A%20Normalized%20AOPC%3A%20Fixing%20Misleading%20Faithfulness%20Metrics%20for%20Feature%0A%20%20Attribution%20Explainability%0AAuthor%3A%20Joakim%20Edin%20and%20Andreas%20Geert%20Motzfeldt%20and%20Casper%20L.%20Christensen%20and%20Tuukka%20Ruotsalo%20and%20Lars%20Maal%C3%B8e%20and%20Maria%20Maistro%0AAbstract%3A%20%20%20Deep%20neural%20network%20predictions%20are%20notoriously%20difficult%20to%20interpret.%0AFeature%20attribution%20methods%20aim%20to%20explain%20these%20predictions%20by%20identifying%20the%0Acontribution%20of%20each%20input%20feature.%20Faithfulness%2C%20often%20evaluated%20using%20the%0Aarea%20over%20the%20perturbation%20curve%20%28AOPC%29%2C%20reflects%20feature%20attributions%27%0Aaccuracy%20in%20describing%20the%20internal%20mechanisms%20of%20deep%20neural%20networks.%0AHowever%2C%20many%20studies%20rely%20on%20AOPC%20to%20compare%20faithfulness%20across%20different%0Amodels%2C%20which%20we%20show%20can%20lead%20to%20false%20conclusions%20about%20models%27%20faithfulness.%0ASpecifically%2C%20we%20find%20that%20AOPC%20is%20sensitive%20to%20variations%20in%20the%20model%2C%0Aresulting%20in%20unreliable%20cross-model%20comparisons.%20Moreover%2C%20AOPC%20scores%20are%0Adifficult%20to%20interpret%20in%20isolation%20without%20knowing%20the%20model-specific%20lower%0Aand%20upper%20limits.%20To%20address%20these%20issues%2C%20we%20propose%20a%20normalization%20approach%2C%0ANormalized%20AOPC%20%28NAOPC%29%2C%20enabling%20consistent%20cross-model%20evaluations%20and%20more%0Ameaningful%20interpretation%20of%20individual%20scores.%20Our%20experiments%20demonstrate%0Athat%20this%20normalization%20can%20radically%20change%20AOPC%20results%2C%20questioning%20the%0Aconclusions%20of%20earlier%20studies%20and%20offering%20a%20more%20robust%20framework%20for%0Aassessing%20feature%20attribution%20faithfulness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalized%2520AOPC%253A%2520Fixing%2520Misleading%2520Faithfulness%2520Metrics%2520for%2520Feature%250A%2520%2520Attribution%2520Explainability%26entry.906535625%3DJoakim%2520Edin%2520and%2520Andreas%2520Geert%2520Motzfeldt%2520and%2520Casper%2520L.%2520Christensen%2520and%2520Tuukka%2520Ruotsalo%2520and%2520Lars%2520Maal%25C3%25B8e%2520and%2520Maria%2520Maistro%26entry.1292438233%3D%2520%2520Deep%2520neural%2520network%2520predictions%2520are%2520notoriously%2520difficult%2520to%2520interpret.%250AFeature%2520attribution%2520methods%2520aim%2520to%2520explain%2520these%2520predictions%2520by%2520identifying%2520the%250Acontribution%2520of%2520each%2520input%2520feature.%2520Faithfulness%252C%2520often%2520evaluated%2520using%2520the%250Aarea%2520over%2520the%2520perturbation%2520curve%2520%2528AOPC%2529%252C%2520reflects%2520feature%2520attributions%2527%250Aaccuracy%2520in%2520describing%2520the%2520internal%2520mechanisms%2520of%2520deep%2520neural%2520networks.%250AHowever%252C%2520many%2520studies%2520rely%2520on%2520AOPC%2520to%2520compare%2520faithfulness%2520across%2520different%250Amodels%252C%2520which%2520we%2520show%2520can%2520lead%2520to%2520false%2520conclusions%2520about%2520models%2527%2520faithfulness.%250ASpecifically%252C%2520we%2520find%2520that%2520AOPC%2520is%2520sensitive%2520to%2520variations%2520in%2520the%2520model%252C%250Aresulting%2520in%2520unreliable%2520cross-model%2520comparisons.%2520Moreover%252C%2520AOPC%2520scores%2520are%250Adifficult%2520to%2520interpret%2520in%2520isolation%2520without%2520knowing%2520the%2520model-specific%2520lower%250Aand%2520upper%2520limits.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520normalization%2520approach%252C%250ANormalized%2520AOPC%2520%2528NAOPC%2529%252C%2520enabling%2520consistent%2520cross-model%2520evaluations%2520and%2520more%250Ameaningful%2520interpretation%2520of%2520individual%2520scores.%2520Our%2520experiments%2520demonstrate%250Athat%2520this%2520normalization%2520can%2520radically%2520change%2520AOPC%2520results%252C%2520questioning%2520the%250Aconclusions%2520of%2520earlier%2520studies%2520and%2520offering%2520a%2520more%2520robust%2520framework%2520for%250Aassessing%2520feature%2520attribution%2520faithfulness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalized%20AOPC%3A%20Fixing%20Misleading%20Faithfulness%20Metrics%20for%20Feature%0A%20%20Attribution%20Explainability&entry.906535625=Joakim%20Edin%20and%20Andreas%20Geert%20Motzfeldt%20and%20Casper%20L.%20Christensen%20and%20Tuukka%20Ruotsalo%20and%20Lars%20Maal%C3%B8e%20and%20Maria%20Maistro&entry.1292438233=%20%20Deep%20neural%20network%20predictions%20are%20notoriously%20difficult%20to%20interpret.%0AFeature%20attribution%20methods%20aim%20to%20explain%20these%20predictions%20by%20identifying%20the%0Acontribution%20of%20each%20input%20feature.%20Faithfulness%2C%20often%20evaluated%20using%20the%0Aarea%20over%20the%20perturbation%20curve%20%28AOPC%29%2C%20reflects%20feature%20attributions%27%0Aaccuracy%20in%20describing%20the%20internal%20mechanisms%20of%20deep%20neural%20networks.%0AHowever%2C%20many%20studies%20rely%20on%20AOPC%20to%20compare%20faithfulness%20across%20different%0Amodels%2C%20which%20we%20show%20can%20lead%20to%20false%20conclusions%20about%20models%27%20faithfulness.%0ASpecifically%2C%20we%20find%20that%20AOPC%20is%20sensitive%20to%20variations%20in%20the%20model%2C%0Aresulting%20in%20unreliable%20cross-model%20comparisons.%20Moreover%2C%20AOPC%20scores%20are%0Adifficult%20to%20interpret%20in%20isolation%20without%20knowing%20the%20model-specific%20lower%0Aand%20upper%20limits.%20To%20address%20these%20issues%2C%20we%20propose%20a%20normalization%20approach%2C%0ANormalized%20AOPC%20%28NAOPC%29%2C%20enabling%20consistent%20cross-model%20evaluations%20and%20more%0Ameaningful%20interpretation%20of%20individual%20scores.%20Our%20experiments%20demonstrate%0Athat%20this%20normalization%20can%20radically%20change%20AOPC%20results%2C%20questioning%20the%0Aconclusions%20of%20earlier%20studies%20and%20offering%20a%20more%20robust%20framework%20for%0Aassessing%20feature%20attribution%20faithfulness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08137v1&entry.124074799=Read"},
{"title": "Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and\n  Outlook", "author": "Ziying Song and Lin Liu and Feiyang Jia and Yadan Luo and Guoxin Zhang and Lei Yang and Li Wang and Caiyan Jia", "abstract": "  In the realm of modern autonomous driving, the perception system is\nindispensable for accurately assessing the state of the surrounding\nenvironment, thereby enabling informed prediction and planning. The key step to\nthis system is related to 3D object detection that utilizes vehicle-mounted\nsensors such as LiDAR and cameras to identify the size, the category, and the\nlocation of nearby objects. Despite the surge in 3D object detection methods\naimed at enhancing detection precision and efficiency, there is a gap in the\nliterature that systematically examines their resilience against environmental\nvariations, noise, and weather changes. This study emphasizes the importance of\nrobustness, alongside accuracy and latency, in evaluating perception systems\nunder practical scenarios. Our work presents an extensive survey of\ncamera-only, LiDAR-only, and multi-modal 3D object detection algorithms,\nthoroughly evaluating their trade-off between accuracy, latency, and\nrobustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair\ncomparisons. Among these, multi-modal 3D detection approaches exhibit superior\nrobustness, and a novel taxonomy is introduced to reorganize the literature for\nenhanced clarity. This survey aims to offer a more practical perspective on the\ncurrent capabilities and the constraints of 3D object detection algorithms in\nreal-world applications, thus steering future research towards\nrobustness-centric advancements.\n", "link": "http://arxiv.org/abs/2401.06542v3", "date": "2024-08-15", "relevancy": 1.8067, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6158}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5925}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robustness-Aware%203D%20Object%20Detection%20in%20Autonomous%20Driving%3A%20A%20Review%20and%0A%20%20Outlook&body=Title%3A%20Robustness-Aware%203D%20Object%20Detection%20in%20Autonomous%20Driving%3A%20A%20Review%20and%0A%20%20Outlook%0AAuthor%3A%20Ziying%20Song%20and%20Lin%20Liu%20and%20Feiyang%20Jia%20and%20Yadan%20Luo%20and%20Guoxin%20Zhang%20and%20Lei%20Yang%20and%20Li%20Wang%20and%20Caiyan%20Jia%0AAbstract%3A%20%20%20In%20the%20realm%20of%20modern%20autonomous%20driving%2C%20the%20perception%20system%20is%0Aindispensable%20for%20accurately%20assessing%20the%20state%20of%20the%20surrounding%0Aenvironment%2C%20thereby%20enabling%20informed%20prediction%20and%20planning.%20The%20key%20step%20to%0Athis%20system%20is%20related%20to%203D%20object%20detection%20that%20utilizes%20vehicle-mounted%0Asensors%20such%20as%20LiDAR%20and%20cameras%20to%20identify%20the%20size%2C%20the%20category%2C%20and%20the%0Alocation%20of%20nearby%20objects.%20Despite%20the%20surge%20in%203D%20object%20detection%20methods%0Aaimed%20at%20enhancing%20detection%20precision%20and%20efficiency%2C%20there%20is%20a%20gap%20in%20the%0Aliterature%20that%20systematically%20examines%20their%20resilience%20against%20environmental%0Avariations%2C%20noise%2C%20and%20weather%20changes.%20This%20study%20emphasizes%20the%20importance%20of%0Arobustness%2C%20alongside%20accuracy%20and%20latency%2C%20in%20evaluating%20perception%20systems%0Aunder%20practical%20scenarios.%20Our%20work%20presents%20an%20extensive%20survey%20of%0Acamera-only%2C%20LiDAR-only%2C%20and%20multi-modal%203D%20object%20detection%20algorithms%2C%0Athoroughly%20evaluating%20their%20trade-off%20between%20accuracy%2C%20latency%2C%20and%0Arobustness%2C%20particularly%20on%20datasets%20like%20KITTI-C%20and%20nuScenes-C%20to%20ensure%20fair%0Acomparisons.%20Among%20these%2C%20multi-modal%203D%20detection%20approaches%20exhibit%20superior%0Arobustness%2C%20and%20a%20novel%20taxonomy%20is%20introduced%20to%20reorganize%20the%20literature%20for%0Aenhanced%20clarity.%20This%20survey%20aims%20to%20offer%20a%20more%20practical%20perspective%20on%20the%0Acurrent%20capabilities%20and%20the%20constraints%20of%203D%20object%20detection%20algorithms%20in%0Areal-world%20applications%2C%20thus%20steering%20future%20research%20towards%0Arobustness-centric%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06542v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustness-Aware%25203D%2520Object%2520Detection%2520in%2520Autonomous%2520Driving%253A%2520A%2520Review%2520and%250A%2520%2520Outlook%26entry.906535625%3DZiying%2520Song%2520and%2520Lin%2520Liu%2520and%2520Feiyang%2520Jia%2520and%2520Yadan%2520Luo%2520and%2520Guoxin%2520Zhang%2520and%2520Lei%2520Yang%2520and%2520Li%2520Wang%2520and%2520Caiyan%2520Jia%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520modern%2520autonomous%2520driving%252C%2520the%2520perception%2520system%2520is%250Aindispensable%2520for%2520accurately%2520assessing%2520the%2520state%2520of%2520the%2520surrounding%250Aenvironment%252C%2520thereby%2520enabling%2520informed%2520prediction%2520and%2520planning.%2520The%2520key%2520step%2520to%250Athis%2520system%2520is%2520related%2520to%25203D%2520object%2520detection%2520that%2520utilizes%2520vehicle-mounted%250Asensors%2520such%2520as%2520LiDAR%2520and%2520cameras%2520to%2520identify%2520the%2520size%252C%2520the%2520category%252C%2520and%2520the%250Alocation%2520of%2520nearby%2520objects.%2520Despite%2520the%2520surge%2520in%25203D%2520object%2520detection%2520methods%250Aaimed%2520at%2520enhancing%2520detection%2520precision%2520and%2520efficiency%252C%2520there%2520is%2520a%2520gap%2520in%2520the%250Aliterature%2520that%2520systematically%2520examines%2520their%2520resilience%2520against%2520environmental%250Avariations%252C%2520noise%252C%2520and%2520weather%2520changes.%2520This%2520study%2520emphasizes%2520the%2520importance%2520of%250Arobustness%252C%2520alongside%2520accuracy%2520and%2520latency%252C%2520in%2520evaluating%2520perception%2520systems%250Aunder%2520practical%2520scenarios.%2520Our%2520work%2520presents%2520an%2520extensive%2520survey%2520of%250Acamera-only%252C%2520LiDAR-only%252C%2520and%2520multi-modal%25203D%2520object%2520detection%2520algorithms%252C%250Athoroughly%2520evaluating%2520their%2520trade-off%2520between%2520accuracy%252C%2520latency%252C%2520and%250Arobustness%252C%2520particularly%2520on%2520datasets%2520like%2520KITTI-C%2520and%2520nuScenes-C%2520to%2520ensure%2520fair%250Acomparisons.%2520Among%2520these%252C%2520multi-modal%25203D%2520detection%2520approaches%2520exhibit%2520superior%250Arobustness%252C%2520and%2520a%2520novel%2520taxonomy%2520is%2520introduced%2520to%2520reorganize%2520the%2520literature%2520for%250Aenhanced%2520clarity.%2520This%2520survey%2520aims%2520to%2520offer%2520a%2520more%2520practical%2520perspective%2520on%2520the%250Acurrent%2520capabilities%2520and%2520the%2520constraints%2520of%25203D%2520object%2520detection%2520algorithms%2520in%250Areal-world%2520applications%252C%2520thus%2520steering%2520future%2520research%2520towards%250Arobustness-centric%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06542v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robustness-Aware%203D%20Object%20Detection%20in%20Autonomous%20Driving%3A%20A%20Review%20and%0A%20%20Outlook&entry.906535625=Ziying%20Song%20and%20Lin%20Liu%20and%20Feiyang%20Jia%20and%20Yadan%20Luo%20and%20Guoxin%20Zhang%20and%20Lei%20Yang%20and%20Li%20Wang%20and%20Caiyan%20Jia&entry.1292438233=%20%20In%20the%20realm%20of%20modern%20autonomous%20driving%2C%20the%20perception%20system%20is%0Aindispensable%20for%20accurately%20assessing%20the%20state%20of%20the%20surrounding%0Aenvironment%2C%20thereby%20enabling%20informed%20prediction%20and%20planning.%20The%20key%20step%20to%0Athis%20system%20is%20related%20to%203D%20object%20detection%20that%20utilizes%20vehicle-mounted%0Asensors%20such%20as%20LiDAR%20and%20cameras%20to%20identify%20the%20size%2C%20the%20category%2C%20and%20the%0Alocation%20of%20nearby%20objects.%20Despite%20the%20surge%20in%203D%20object%20detection%20methods%0Aaimed%20at%20enhancing%20detection%20precision%20and%20efficiency%2C%20there%20is%20a%20gap%20in%20the%0Aliterature%20that%20systematically%20examines%20their%20resilience%20against%20environmental%0Avariations%2C%20noise%2C%20and%20weather%20changes.%20This%20study%20emphasizes%20the%20importance%20of%0Arobustness%2C%20alongside%20accuracy%20and%20latency%2C%20in%20evaluating%20perception%20systems%0Aunder%20practical%20scenarios.%20Our%20work%20presents%20an%20extensive%20survey%20of%0Acamera-only%2C%20LiDAR-only%2C%20and%20multi-modal%203D%20object%20detection%20algorithms%2C%0Athoroughly%20evaluating%20their%20trade-off%20between%20accuracy%2C%20latency%2C%20and%0Arobustness%2C%20particularly%20on%20datasets%20like%20KITTI-C%20and%20nuScenes-C%20to%20ensure%20fair%0Acomparisons.%20Among%20these%2C%20multi-modal%203D%20detection%20approaches%20exhibit%20superior%0Arobustness%2C%20and%20a%20novel%20taxonomy%20is%20introduced%20to%20reorganize%20the%20literature%20for%0Aenhanced%20clarity.%20This%20survey%20aims%20to%20offer%20a%20more%20practical%20perspective%20on%20the%0Acurrent%20capabilities%20and%20the%20constraints%20of%203D%20object%20detection%20algorithms%20in%0Areal-world%20applications%2C%20thus%20steering%20future%20research%20towards%0Arobustness-centric%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06542v3&entry.124074799=Read"},
{"title": "Diagnosis extraction from unstructured Dutch echocardiogram reports\n  using span- and document-level characteristic classification", "author": "Bauke Arends and Melle Vessies and Dirk van Osch and Arco Teske and Pim van der Harst and Ren\u00e9 van Es and Bram van Es", "abstract": "  Clinical machine learning research and AI driven clinical decision support\nmodels rely on clinically accurate labels. Manually extracting these labels\nwith the help of clinical specialists is often time-consuming and expensive.\nThis study tests the feasibility of automatic span- and document-level\ndiagnosis extraction from unstructured Dutch echocardiogram reports. We\nincluded 115,692 unstructured echocardiogram reports from the UMCU a large\nuniversity hospital in the Netherlands. A randomly selected subset was manually\nannotated for the occurrence and severity of eleven commonly described cardiac\ncharacteristics. We developed and tested several automatic labelling techniques\nat both span and document levels, using weighted and macro F1-score, precision,\nand recall for performance evaluation. We compared the performance of span\nlabelling against document labelling methods, which included both direct\ndocument classifiers and indirect document classifiers that rely on span\nclassification results. The SpanCategorizer and MedRoBERTa$.$nl models\noutperformed all other span and document classifiers, respectively. The\nweighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in\nSpanCategorizer and 0.96 to 0.98 in MedRoBERTa$.$nl. Direct document\nclassification was superior to indirect document classification using span\nclassifiers. SetFit achieved competitive document classification performance\nusing only 10% of the training data. Utilizing a reduced label set yielded\nnear-perfect document classification results. We recommend using our published\nSpanCategorizer and MedRoBERTa$.$nl models for span- and document-level\ndiagnosis extraction from Dutch echocardiography reports. For settings with\nlimited training data, SetFit may be a promising alternative for document\nclassification.\n", "link": "http://arxiv.org/abs/2408.06930v2", "date": "2024-08-15", "relevancy": 1.7847, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4917}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4387}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports%0A%20%20using%20span-%20and%20document-level%20characteristic%20classification&body=Title%3A%20Diagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports%0A%20%20using%20span-%20and%20document-level%20characteristic%20classification%0AAuthor%3A%20Bauke%20Arends%20and%20Melle%20Vessies%20and%20Dirk%20van%20Osch%20and%20Arco%20Teske%20and%20Pim%20van%20der%20Harst%20and%20Ren%C3%A9%20van%20Es%20and%20Bram%20van%20Es%0AAbstract%3A%20%20%20Clinical%20machine%20learning%20research%20and%20AI%20driven%20clinical%20decision%20support%0Amodels%20rely%20on%20clinically%20accurate%20labels.%20Manually%20extracting%20these%20labels%0Awith%20the%20help%20of%20clinical%20specialists%20is%20often%20time-consuming%20and%20expensive.%0AThis%20study%20tests%20the%20feasibility%20of%20automatic%20span-%20and%20document-level%0Adiagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports.%20We%0Aincluded%20115%2C692%20unstructured%20echocardiogram%20reports%20from%20the%20UMCU%20a%20large%0Auniversity%20hospital%20in%20the%20Netherlands.%20A%20randomly%20selected%20subset%20was%20manually%0Aannotated%20for%20the%20occurrence%20and%20severity%20of%20eleven%20commonly%20described%20cardiac%0Acharacteristics.%20We%20developed%20and%20tested%20several%20automatic%20labelling%20techniques%0Aat%20both%20span%20and%20document%20levels%2C%20using%20weighted%20and%20macro%20F1-score%2C%20precision%2C%0Aand%20recall%20for%20performance%20evaluation.%20We%20compared%20the%20performance%20of%20span%0Alabelling%20against%20document%20labelling%20methods%2C%20which%20included%20both%20direct%0Adocument%20classifiers%20and%20indirect%20document%20classifiers%20that%20rely%20on%20span%0Aclassification%20results.%20The%20SpanCategorizer%20and%20MedRoBERTa%24.%24nl%20models%0Aoutperformed%20all%20other%20span%20and%20document%20classifiers%2C%20respectively.%20The%0Aweighted%20F1-score%20varied%20between%20characteristics%2C%20ranging%20from%200.60%20to%200.93%20in%0ASpanCategorizer%20and%200.96%20to%200.98%20in%20MedRoBERTa%24.%24nl.%20Direct%20document%0Aclassification%20was%20superior%20to%20indirect%20document%20classification%20using%20span%0Aclassifiers.%20SetFit%20achieved%20competitive%20document%20classification%20performance%0Ausing%20only%2010%25%20of%20the%20training%20data.%20Utilizing%20a%20reduced%20label%20set%20yielded%0Anear-perfect%20document%20classification%20results.%20We%20recommend%20using%20our%20published%0ASpanCategorizer%20and%20MedRoBERTa%24.%24nl%20models%20for%20span-%20and%20document-level%0Adiagnosis%20extraction%20from%20Dutch%20echocardiography%20reports.%20For%20settings%20with%0Alimited%20training%20data%2C%20SetFit%20may%20be%20a%20promising%20alternative%20for%20document%0Aclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06930v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagnosis%2520extraction%2520from%2520unstructured%2520Dutch%2520echocardiogram%2520reports%250A%2520%2520using%2520span-%2520and%2520document-level%2520characteristic%2520classification%26entry.906535625%3DBauke%2520Arends%2520and%2520Melle%2520Vessies%2520and%2520Dirk%2520van%2520Osch%2520and%2520Arco%2520Teske%2520and%2520Pim%2520van%2520der%2520Harst%2520and%2520Ren%25C3%25A9%2520van%2520Es%2520and%2520Bram%2520van%2520Es%26entry.1292438233%3D%2520%2520Clinical%2520machine%2520learning%2520research%2520and%2520AI%2520driven%2520clinical%2520decision%2520support%250Amodels%2520rely%2520on%2520clinically%2520accurate%2520labels.%2520Manually%2520extracting%2520these%2520labels%250Awith%2520the%2520help%2520of%2520clinical%2520specialists%2520is%2520often%2520time-consuming%2520and%2520expensive.%250AThis%2520study%2520tests%2520the%2520feasibility%2520of%2520automatic%2520span-%2520and%2520document-level%250Adiagnosis%2520extraction%2520from%2520unstructured%2520Dutch%2520echocardiogram%2520reports.%2520We%250Aincluded%2520115%252C692%2520unstructured%2520echocardiogram%2520reports%2520from%2520the%2520UMCU%2520a%2520large%250Auniversity%2520hospital%2520in%2520the%2520Netherlands.%2520A%2520randomly%2520selected%2520subset%2520was%2520manually%250Aannotated%2520for%2520the%2520occurrence%2520and%2520severity%2520of%2520eleven%2520commonly%2520described%2520cardiac%250Acharacteristics.%2520We%2520developed%2520and%2520tested%2520several%2520automatic%2520labelling%2520techniques%250Aat%2520both%2520span%2520and%2520document%2520levels%252C%2520using%2520weighted%2520and%2520macro%2520F1-score%252C%2520precision%252C%250Aand%2520recall%2520for%2520performance%2520evaluation.%2520We%2520compared%2520the%2520performance%2520of%2520span%250Alabelling%2520against%2520document%2520labelling%2520methods%252C%2520which%2520included%2520both%2520direct%250Adocument%2520classifiers%2520and%2520indirect%2520document%2520classifiers%2520that%2520rely%2520on%2520span%250Aclassification%2520results.%2520The%2520SpanCategorizer%2520and%2520MedRoBERTa%2524.%2524nl%2520models%250Aoutperformed%2520all%2520other%2520span%2520and%2520document%2520classifiers%252C%2520respectively.%2520The%250Aweighted%2520F1-score%2520varied%2520between%2520characteristics%252C%2520ranging%2520from%25200.60%2520to%25200.93%2520in%250ASpanCategorizer%2520and%25200.96%2520to%25200.98%2520in%2520MedRoBERTa%2524.%2524nl.%2520Direct%2520document%250Aclassification%2520was%2520superior%2520to%2520indirect%2520document%2520classification%2520using%2520span%250Aclassifiers.%2520SetFit%2520achieved%2520competitive%2520document%2520classification%2520performance%250Ausing%2520only%252010%2525%2520of%2520the%2520training%2520data.%2520Utilizing%2520a%2520reduced%2520label%2520set%2520yielded%250Anear-perfect%2520document%2520classification%2520results.%2520We%2520recommend%2520using%2520our%2520published%250ASpanCategorizer%2520and%2520MedRoBERTa%2524.%2524nl%2520models%2520for%2520span-%2520and%2520document-level%250Adiagnosis%2520extraction%2520from%2520Dutch%2520echocardiography%2520reports.%2520For%2520settings%2520with%250Alimited%2520training%2520data%252C%2520SetFit%2520may%2520be%2520a%2520promising%2520alternative%2520for%2520document%250Aclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06930v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports%0A%20%20using%20span-%20and%20document-level%20characteristic%20classification&entry.906535625=Bauke%20Arends%20and%20Melle%20Vessies%20and%20Dirk%20van%20Osch%20and%20Arco%20Teske%20and%20Pim%20van%20der%20Harst%20and%20Ren%C3%A9%20van%20Es%20and%20Bram%20van%20Es&entry.1292438233=%20%20Clinical%20machine%20learning%20research%20and%20AI%20driven%20clinical%20decision%20support%0Amodels%20rely%20on%20clinically%20accurate%20labels.%20Manually%20extracting%20these%20labels%0Awith%20the%20help%20of%20clinical%20specialists%20is%20often%20time-consuming%20and%20expensive.%0AThis%20study%20tests%20the%20feasibility%20of%20automatic%20span-%20and%20document-level%0Adiagnosis%20extraction%20from%20unstructured%20Dutch%20echocardiogram%20reports.%20We%0Aincluded%20115%2C692%20unstructured%20echocardiogram%20reports%20from%20the%20UMCU%20a%20large%0Auniversity%20hospital%20in%20the%20Netherlands.%20A%20randomly%20selected%20subset%20was%20manually%0Aannotated%20for%20the%20occurrence%20and%20severity%20of%20eleven%20commonly%20described%20cardiac%0Acharacteristics.%20We%20developed%20and%20tested%20several%20automatic%20labelling%20techniques%0Aat%20both%20span%20and%20document%20levels%2C%20using%20weighted%20and%20macro%20F1-score%2C%20precision%2C%0Aand%20recall%20for%20performance%20evaluation.%20We%20compared%20the%20performance%20of%20span%0Alabelling%20against%20document%20labelling%20methods%2C%20which%20included%20both%20direct%0Adocument%20classifiers%20and%20indirect%20document%20classifiers%20that%20rely%20on%20span%0Aclassification%20results.%20The%20SpanCategorizer%20and%20MedRoBERTa%24.%24nl%20models%0Aoutperformed%20all%20other%20span%20and%20document%20classifiers%2C%20respectively.%20The%0Aweighted%20F1-score%20varied%20between%20characteristics%2C%20ranging%20from%200.60%20to%200.93%20in%0ASpanCategorizer%20and%200.96%20to%200.98%20in%20MedRoBERTa%24.%24nl.%20Direct%20document%0Aclassification%20was%20superior%20to%20indirect%20document%20classification%20using%20span%0Aclassifiers.%20SetFit%20achieved%20competitive%20document%20classification%20performance%0Ausing%20only%2010%25%20of%20the%20training%20data.%20Utilizing%20a%20reduced%20label%20set%20yielded%0Anear-perfect%20document%20classification%20results.%20We%20recommend%20using%20our%20published%0ASpanCategorizer%20and%20MedRoBERTa%24.%24nl%20models%20for%20span-%20and%20document-level%0Adiagnosis%20extraction%20from%20Dutch%20echocardiography%20reports.%20For%20settings%20with%0Alimited%20training%20data%2C%20SetFit%20may%20be%20a%20promising%20alternative%20for%20document%0Aclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06930v2&entry.124074799=Read"},
{"title": "Conformalized Answer Set Prediction for Knowledge Graph Embedding", "author": "Yuqicheng Zhu and Nico Potyka and Jiarong Pan and Bo Xiong and Yunjie He and Evgeny Kharlamov and Steffen Staab", "abstract": "  Knowledge graph embeddings (KGE) apply machine learning methods on knowledge\ngraphs (KGs) to provide non-classical reasoning capabilities based on\nsimilarities and analogies. The learned KG embeddings are typically used to\nanswer queries by ranking all potential answers, but rankings often lack a\nmeaningful probabilistic interpretation - lower-ranked answers do not\nnecessarily have a lower probability of being true. This limitation makes it\ndifficult to distinguish plausible from implausible answers, posing challenges\nfor the application of KGE methods in high-stakes domains like medicine. We\naddress this issue by applying the theory of conformal prediction that allows\ngenerating answer sets, which contain the correct answer with probabilistic\nguarantees. We explain how conformal prediction can be used to generate such\nanswer sets for link prediction tasks. Our empirical evaluation on four\nbenchmark datasets using six representative KGE methods validates that the\ngenerated answer sets satisfy the probabilistic guarantees given by the theory\nof conformal prediction. We also demonstrate that the generated answer sets\noften have a sensible size and that the size adapts well with respect to the\ndifficulty of the query.\n", "link": "http://arxiv.org/abs/2408.08248v1", "date": "2024-08-15", "relevancy": 1.3581, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4865}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4585}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformalized%20Answer%20Set%20Prediction%20for%20Knowledge%20Graph%20Embedding&body=Title%3A%20Conformalized%20Answer%20Set%20Prediction%20for%20Knowledge%20Graph%20Embedding%0AAuthor%3A%20Yuqicheng%20Zhu%20and%20Nico%20Potyka%20and%20Jiarong%20Pan%20and%20Bo%20Xiong%20and%20Yunjie%20He%20and%20Evgeny%20Kharlamov%20and%20Steffen%20Staab%0AAbstract%3A%20%20%20Knowledge%20graph%20embeddings%20%28KGE%29%20apply%20machine%20learning%20methods%20on%20knowledge%0Agraphs%20%28KGs%29%20to%20provide%20non-classical%20reasoning%20capabilities%20based%20on%0Asimilarities%20and%20analogies.%20The%20learned%20KG%20embeddings%20are%20typically%20used%20to%0Aanswer%20queries%20by%20ranking%20all%20potential%20answers%2C%20but%20rankings%20often%20lack%20a%0Ameaningful%20probabilistic%20interpretation%20-%20lower-ranked%20answers%20do%20not%0Anecessarily%20have%20a%20lower%20probability%20of%20being%20true.%20This%20limitation%20makes%20it%0Adifficult%20to%20distinguish%20plausible%20from%20implausible%20answers%2C%20posing%20challenges%0Afor%20the%20application%20of%20KGE%20methods%20in%20high-stakes%20domains%20like%20medicine.%20We%0Aaddress%20this%20issue%20by%20applying%20the%20theory%20of%20conformal%20prediction%20that%20allows%0Agenerating%20answer%20sets%2C%20which%20contain%20the%20correct%20answer%20with%20probabilistic%0Aguarantees.%20We%20explain%20how%20conformal%20prediction%20can%20be%20used%20to%20generate%20such%0Aanswer%20sets%20for%20link%20prediction%20tasks.%20Our%20empirical%20evaluation%20on%20four%0Abenchmark%20datasets%20using%20six%20representative%20KGE%20methods%20validates%20that%20the%0Agenerated%20answer%20sets%20satisfy%20the%20probabilistic%20guarantees%20given%20by%20the%20theory%0Aof%20conformal%20prediction.%20We%20also%20demonstrate%20that%20the%20generated%20answer%20sets%0Aoften%20have%20a%20sensible%20size%20and%20that%20the%20size%20adapts%20well%20with%20respect%20to%20the%0Adifficulty%20of%20the%20query.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformalized%2520Answer%2520Set%2520Prediction%2520for%2520Knowledge%2520Graph%2520Embedding%26entry.906535625%3DYuqicheng%2520Zhu%2520and%2520Nico%2520Potyka%2520and%2520Jiarong%2520Pan%2520and%2520Bo%2520Xiong%2520and%2520Yunjie%2520He%2520and%2520Evgeny%2520Kharlamov%2520and%2520Steffen%2520Staab%26entry.1292438233%3D%2520%2520Knowledge%2520graph%2520embeddings%2520%2528KGE%2529%2520apply%2520machine%2520learning%2520methods%2520on%2520knowledge%250Agraphs%2520%2528KGs%2529%2520to%2520provide%2520non-classical%2520reasoning%2520capabilities%2520based%2520on%250Asimilarities%2520and%2520analogies.%2520The%2520learned%2520KG%2520embeddings%2520are%2520typically%2520used%2520to%250Aanswer%2520queries%2520by%2520ranking%2520all%2520potential%2520answers%252C%2520but%2520rankings%2520often%2520lack%2520a%250Ameaningful%2520probabilistic%2520interpretation%2520-%2520lower-ranked%2520answers%2520do%2520not%250Anecessarily%2520have%2520a%2520lower%2520probability%2520of%2520being%2520true.%2520This%2520limitation%2520makes%2520it%250Adifficult%2520to%2520distinguish%2520plausible%2520from%2520implausible%2520answers%252C%2520posing%2520challenges%250Afor%2520the%2520application%2520of%2520KGE%2520methods%2520in%2520high-stakes%2520domains%2520like%2520medicine.%2520We%250Aaddress%2520this%2520issue%2520by%2520applying%2520the%2520theory%2520of%2520conformal%2520prediction%2520that%2520allows%250Agenerating%2520answer%2520sets%252C%2520which%2520contain%2520the%2520correct%2520answer%2520with%2520probabilistic%250Aguarantees.%2520We%2520explain%2520how%2520conformal%2520prediction%2520can%2520be%2520used%2520to%2520generate%2520such%250Aanswer%2520sets%2520for%2520link%2520prediction%2520tasks.%2520Our%2520empirical%2520evaluation%2520on%2520four%250Abenchmark%2520datasets%2520using%2520six%2520representative%2520KGE%2520methods%2520validates%2520that%2520the%250Agenerated%2520answer%2520sets%2520satisfy%2520the%2520probabilistic%2520guarantees%2520given%2520by%2520the%2520theory%250Aof%2520conformal%2520prediction.%2520We%2520also%2520demonstrate%2520that%2520the%2520generated%2520answer%2520sets%250Aoften%2520have%2520a%2520sensible%2520size%2520and%2520that%2520the%2520size%2520adapts%2520well%2520with%2520respect%2520to%2520the%250Adifficulty%2520of%2520the%2520query.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformalized%20Answer%20Set%20Prediction%20for%20Knowledge%20Graph%20Embedding&entry.906535625=Yuqicheng%20Zhu%20and%20Nico%20Potyka%20and%20Jiarong%20Pan%20and%20Bo%20Xiong%20and%20Yunjie%20He%20and%20Evgeny%20Kharlamov%20and%20Steffen%20Staab&entry.1292438233=%20%20Knowledge%20graph%20embeddings%20%28KGE%29%20apply%20machine%20learning%20methods%20on%20knowledge%0Agraphs%20%28KGs%29%20to%20provide%20non-classical%20reasoning%20capabilities%20based%20on%0Asimilarities%20and%20analogies.%20The%20learned%20KG%20embeddings%20are%20typically%20used%20to%0Aanswer%20queries%20by%20ranking%20all%20potential%20answers%2C%20but%20rankings%20often%20lack%20a%0Ameaningful%20probabilistic%20interpretation%20-%20lower-ranked%20answers%20do%20not%0Anecessarily%20have%20a%20lower%20probability%20of%20being%20true.%20This%20limitation%20makes%20it%0Adifficult%20to%20distinguish%20plausible%20from%20implausible%20answers%2C%20posing%20challenges%0Afor%20the%20application%20of%20KGE%20methods%20in%20high-stakes%20domains%20like%20medicine.%20We%0Aaddress%20this%20issue%20by%20applying%20the%20theory%20of%20conformal%20prediction%20that%20allows%0Agenerating%20answer%20sets%2C%20which%20contain%20the%20correct%20answer%20with%20probabilistic%0Aguarantees.%20We%20explain%20how%20conformal%20prediction%20can%20be%20used%20to%20generate%20such%0Aanswer%20sets%20for%20link%20prediction%20tasks.%20Our%20empirical%20evaluation%20on%20four%0Abenchmark%20datasets%20using%20six%20representative%20KGE%20methods%20validates%20that%20the%0Agenerated%20answer%20sets%20satisfy%20the%20probabilistic%20guarantees%20given%20by%20the%20theory%0Aof%20conformal%20prediction.%20We%20also%20demonstrate%20that%20the%20generated%20answer%20sets%0Aoften%20have%20a%20sensible%20size%20and%20that%20the%20size%20adapts%20well%20with%20respect%20to%20the%0Adifficulty%20of%20the%20query.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08248v1&entry.124074799=Read"},
{"title": "Hearing Your Blood Sugar: Non-Invasive Glucose Measurement Through\n  Simple Vocal Signals, Transforming any Speech into a Sensor with Machine\n  Learning", "author": "Nihat Ahmadli and Mehmet Ali Sarsil and Onur Ergen", "abstract": "  Effective diabetes management relies heavily on the continuous monitoring of\nblood glucose levels, traditionally achieved through invasive and uncomfortable\nmethods. While various non-invasive techniques have been explored, such as\noptical, microwave, and electrochemical approaches, none have effectively\nsupplanted these invasive technologies due to issues related to complexity,\naccuracy, and cost. In this study, we present a transformative and\nstraightforward method that utilizes voice analysis to predict blood glucose\nlevels. Our research investigates the relationship between fluctuations in\nblood glucose and vocal characteristics, highlighting the influence of blood\nvessel dynamics during voice production. By applying advanced machine learning\nalgorithms, we analyzed vocal signal variations and established a significant\ncorrelation with blood glucose levels. We developed a predictive model using\nartificial intelligence, based on voice recordings and corresponding glucose\nmeasurements from participants, utilizing logistic regression and Ridge\nregularization. Our findings indicate that voice analysis may serve as a viable\nnon-invasive alternative for glucose monitoring. This innovative approach not\nonly has the potential to streamline and reduce the costs associated with\ndiabetes management but also aims to enhance the quality of life for\nindividuals living with diabetes by providing a painless and user-friendly\nmethod for monitoring blood sugar levels.\n", "link": "http://arxiv.org/abs/2408.08109v1", "date": "2024-08-15", "relevancy": 1.3037, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4648}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4355}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hearing%20Your%20Blood%20Sugar%3A%20Non-Invasive%20Glucose%20Measurement%20Through%0A%20%20Simple%20Vocal%20Signals%2C%20Transforming%20any%20Speech%20into%20a%20Sensor%20with%20Machine%0A%20%20Learning&body=Title%3A%20Hearing%20Your%20Blood%20Sugar%3A%20Non-Invasive%20Glucose%20Measurement%20Through%0A%20%20Simple%20Vocal%20Signals%2C%20Transforming%20any%20Speech%20into%20a%20Sensor%20with%20Machine%0A%20%20Learning%0AAuthor%3A%20Nihat%20Ahmadli%20and%20Mehmet%20Ali%20Sarsil%20and%20Onur%20Ergen%0AAbstract%3A%20%20%20Effective%20diabetes%20management%20relies%20heavily%20on%20the%20continuous%20monitoring%20of%0Ablood%20glucose%20levels%2C%20traditionally%20achieved%20through%20invasive%20and%20uncomfortable%0Amethods.%20While%20various%20non-invasive%20techniques%20have%20been%20explored%2C%20such%20as%0Aoptical%2C%20microwave%2C%20and%20electrochemical%20approaches%2C%20none%20have%20effectively%0Asupplanted%20these%20invasive%20technologies%20due%20to%20issues%20related%20to%20complexity%2C%0Aaccuracy%2C%20and%20cost.%20In%20this%20study%2C%20we%20present%20a%20transformative%20and%0Astraightforward%20method%20that%20utilizes%20voice%20analysis%20to%20predict%20blood%20glucose%0Alevels.%20Our%20research%20investigates%20the%20relationship%20between%20fluctuations%20in%0Ablood%20glucose%20and%20vocal%20characteristics%2C%20highlighting%20the%20influence%20of%20blood%0Avessel%20dynamics%20during%20voice%20production.%20By%20applying%20advanced%20machine%20learning%0Aalgorithms%2C%20we%20analyzed%20vocal%20signal%20variations%20and%20established%20a%20significant%0Acorrelation%20with%20blood%20glucose%20levels.%20We%20developed%20a%20predictive%20model%20using%0Aartificial%20intelligence%2C%20based%20on%20voice%20recordings%20and%20corresponding%20glucose%0Ameasurements%20from%20participants%2C%20utilizing%20logistic%20regression%20and%20Ridge%0Aregularization.%20Our%20findings%20indicate%20that%20voice%20analysis%20may%20serve%20as%20a%20viable%0Anon-invasive%20alternative%20for%20glucose%20monitoring.%20This%20innovative%20approach%20not%0Aonly%20has%20the%20potential%20to%20streamline%20and%20reduce%20the%20costs%20associated%20with%0Adiabetes%20management%20but%20also%20aims%20to%20enhance%20the%20quality%20of%20life%20for%0Aindividuals%20living%20with%20diabetes%20by%20providing%20a%20painless%20and%20user-friendly%0Amethod%20for%20monitoring%20blood%20sugar%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHearing%2520Your%2520Blood%2520Sugar%253A%2520Non-Invasive%2520Glucose%2520Measurement%2520Through%250A%2520%2520Simple%2520Vocal%2520Signals%252C%2520Transforming%2520any%2520Speech%2520into%2520a%2520Sensor%2520with%2520Machine%250A%2520%2520Learning%26entry.906535625%3DNihat%2520Ahmadli%2520and%2520Mehmet%2520Ali%2520Sarsil%2520and%2520Onur%2520Ergen%26entry.1292438233%3D%2520%2520Effective%2520diabetes%2520management%2520relies%2520heavily%2520on%2520the%2520continuous%2520monitoring%2520of%250Ablood%2520glucose%2520levels%252C%2520traditionally%2520achieved%2520through%2520invasive%2520and%2520uncomfortable%250Amethods.%2520While%2520various%2520non-invasive%2520techniques%2520have%2520been%2520explored%252C%2520such%2520as%250Aoptical%252C%2520microwave%252C%2520and%2520electrochemical%2520approaches%252C%2520none%2520have%2520effectively%250Asupplanted%2520these%2520invasive%2520technologies%2520due%2520to%2520issues%2520related%2520to%2520complexity%252C%250Aaccuracy%252C%2520and%2520cost.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520transformative%2520and%250Astraightforward%2520method%2520that%2520utilizes%2520voice%2520analysis%2520to%2520predict%2520blood%2520glucose%250Alevels.%2520Our%2520research%2520investigates%2520the%2520relationship%2520between%2520fluctuations%2520in%250Ablood%2520glucose%2520and%2520vocal%2520characteristics%252C%2520highlighting%2520the%2520influence%2520of%2520blood%250Avessel%2520dynamics%2520during%2520voice%2520production.%2520By%2520applying%2520advanced%2520machine%2520learning%250Aalgorithms%252C%2520we%2520analyzed%2520vocal%2520signal%2520variations%2520and%2520established%2520a%2520significant%250Acorrelation%2520with%2520blood%2520glucose%2520levels.%2520We%2520developed%2520a%2520predictive%2520model%2520using%250Aartificial%2520intelligence%252C%2520based%2520on%2520voice%2520recordings%2520and%2520corresponding%2520glucose%250Ameasurements%2520from%2520participants%252C%2520utilizing%2520logistic%2520regression%2520and%2520Ridge%250Aregularization.%2520Our%2520findings%2520indicate%2520that%2520voice%2520analysis%2520may%2520serve%2520as%2520a%2520viable%250Anon-invasive%2520alternative%2520for%2520glucose%2520monitoring.%2520This%2520innovative%2520approach%2520not%250Aonly%2520has%2520the%2520potential%2520to%2520streamline%2520and%2520reduce%2520the%2520costs%2520associated%2520with%250Adiabetes%2520management%2520but%2520also%2520aims%2520to%2520enhance%2520the%2520quality%2520of%2520life%2520for%250Aindividuals%2520living%2520with%2520diabetes%2520by%2520providing%2520a%2520painless%2520and%2520user-friendly%250Amethod%2520for%2520monitoring%2520blood%2520sugar%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hearing%20Your%20Blood%20Sugar%3A%20Non-Invasive%20Glucose%20Measurement%20Through%0A%20%20Simple%20Vocal%20Signals%2C%20Transforming%20any%20Speech%20into%20a%20Sensor%20with%20Machine%0A%20%20Learning&entry.906535625=Nihat%20Ahmadli%20and%20Mehmet%20Ali%20Sarsil%20and%20Onur%20Ergen&entry.1292438233=%20%20Effective%20diabetes%20management%20relies%20heavily%20on%20the%20continuous%20monitoring%20of%0Ablood%20glucose%20levels%2C%20traditionally%20achieved%20through%20invasive%20and%20uncomfortable%0Amethods.%20While%20various%20non-invasive%20techniques%20have%20been%20explored%2C%20such%20as%0Aoptical%2C%20microwave%2C%20and%20electrochemical%20approaches%2C%20none%20have%20effectively%0Asupplanted%20these%20invasive%20technologies%20due%20to%20issues%20related%20to%20complexity%2C%0Aaccuracy%2C%20and%20cost.%20In%20this%20study%2C%20we%20present%20a%20transformative%20and%0Astraightforward%20method%20that%20utilizes%20voice%20analysis%20to%20predict%20blood%20glucose%0Alevels.%20Our%20research%20investigates%20the%20relationship%20between%20fluctuations%20in%0Ablood%20glucose%20and%20vocal%20characteristics%2C%20highlighting%20the%20influence%20of%20blood%0Avessel%20dynamics%20during%20voice%20production.%20By%20applying%20advanced%20machine%20learning%0Aalgorithms%2C%20we%20analyzed%20vocal%20signal%20variations%20and%20established%20a%20significant%0Acorrelation%20with%20blood%20glucose%20levels.%20We%20developed%20a%20predictive%20model%20using%0Aartificial%20intelligence%2C%20based%20on%20voice%20recordings%20and%20corresponding%20glucose%0Ameasurements%20from%20participants%2C%20utilizing%20logistic%20regression%20and%20Ridge%0Aregularization.%20Our%20findings%20indicate%20that%20voice%20analysis%20may%20serve%20as%20a%20viable%0Anon-invasive%20alternative%20for%20glucose%20monitoring.%20This%20innovative%20approach%20not%0Aonly%20has%20the%20potential%20to%20streamline%20and%20reduce%20the%20costs%20associated%20with%0Adiabetes%20management%20but%20also%20aims%20to%20enhance%20the%20quality%20of%20life%20for%0Aindividuals%20living%20with%20diabetes%20by%20providing%20a%20painless%20and%20user-friendly%0Amethod%20for%20monitoring%20blood%20sugar%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08109v1&entry.124074799=Read"},
{"title": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through\n  Grounded Language Model", "author": "Jin Wang and Arturo Laurenzi and Nikos Tsagarakis", "abstract": "  Enabling humanoid robots to perform autonomously loco-manipulation in\nunstructured environments is crucial and highly challenging for achieving\nembodied intelligence. This involves robots being able to plan their actions\nand behaviors in long-horizon tasks while using multi-modality to perceive\ndeviations between task execution and high-level planning. Recently, large\nlanguage models (LLMs) have demonstrated powerful planning and reasoning\ncapabilities for comprehension and processing of semantic information through\nrobot control tasks, as well as the usability of analytical judgment and\ndecision-making for multi-modal inputs. To leverage the power of LLMs towards\nhumanoid loco-manipulation, we propose a novel language-model based framework\nthat enables robots to autonomously plan behaviors and low-level execution\nunder given textual instructions, while observing and correcting failures that\nmay occur during task execution. To systematically evaluate this framework in\ngrounding LLMs, we created the robot 'action' and 'sensing' behavior library\nfor task planning, and conducted mobile manipulation tasks and experiments in\nboth simulated and real environments using the CENTAURO robot, and verified the\neffectiveness and application of this approach in robotic tasks with autonomous\nbehavioral planning.\n", "link": "http://arxiv.org/abs/2408.08282v1", "date": "2024-08-15", "relevancy": 1.7293, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6237}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5798}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Behavior%20Planning%20For%20Humanoid%20Loco-manipulation%20Through%0A%20%20Grounded%20Language%20Model&body=Title%3A%20Autonomous%20Behavior%20Planning%20For%20Humanoid%20Loco-manipulation%20Through%0A%20%20Grounded%20Language%20Model%0AAuthor%3A%20Jin%20Wang%20and%20Arturo%20Laurenzi%20and%20Nikos%20Tsagarakis%0AAbstract%3A%20%20%20Enabling%20humanoid%20robots%20to%20perform%20autonomously%20loco-manipulation%20in%0Aunstructured%20environments%20is%20crucial%20and%20highly%20challenging%20for%20achieving%0Aembodied%20intelligence.%20This%20involves%20robots%20being%20able%20to%20plan%20their%20actions%0Aand%20behaviors%20in%20long-horizon%20tasks%20while%20using%20multi-modality%20to%20perceive%0Adeviations%20between%20task%20execution%20and%20high-level%20planning.%20Recently%2C%20large%0Alanguage%20models%20%28LLMs%29%20have%20demonstrated%20powerful%20planning%20and%20reasoning%0Acapabilities%20for%20comprehension%20and%20processing%20of%20semantic%20information%20through%0Arobot%20control%20tasks%2C%20as%20well%20as%20the%20usability%20of%20analytical%20judgment%20and%0Adecision-making%20for%20multi-modal%20inputs.%20To%20leverage%20the%20power%20of%20LLMs%20towards%0Ahumanoid%20loco-manipulation%2C%20we%20propose%20a%20novel%20language-model%20based%20framework%0Athat%20enables%20robots%20to%20autonomously%20plan%20behaviors%20and%20low-level%20execution%0Aunder%20given%20textual%20instructions%2C%20while%20observing%20and%20correcting%20failures%20that%0Amay%20occur%20during%20task%20execution.%20To%20systematically%20evaluate%20this%20framework%20in%0Agrounding%20LLMs%2C%20we%20created%20the%20robot%20%27action%27%20and%20%27sensing%27%20behavior%20library%0Afor%20task%20planning%2C%20and%20conducted%20mobile%20manipulation%20tasks%20and%20experiments%20in%0Aboth%20simulated%20and%20real%20environments%20using%20the%20CENTAURO%20robot%2C%20and%20verified%20the%0Aeffectiveness%20and%20application%20of%20this%20approach%20in%20robotic%20tasks%20with%20autonomous%0Abehavioral%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Behavior%2520Planning%2520For%2520Humanoid%2520Loco-manipulation%2520Through%250A%2520%2520Grounded%2520Language%2520Model%26entry.906535625%3DJin%2520Wang%2520and%2520Arturo%2520Laurenzi%2520and%2520Nikos%2520Tsagarakis%26entry.1292438233%3D%2520%2520Enabling%2520humanoid%2520robots%2520to%2520perform%2520autonomously%2520loco-manipulation%2520in%250Aunstructured%2520environments%2520is%2520crucial%2520and%2520highly%2520challenging%2520for%2520achieving%250Aembodied%2520intelligence.%2520This%2520involves%2520robots%2520being%2520able%2520to%2520plan%2520their%2520actions%250Aand%2520behaviors%2520in%2520long-horizon%2520tasks%2520while%2520using%2520multi-modality%2520to%2520perceive%250Adeviations%2520between%2520task%2520execution%2520and%2520high-level%2520planning.%2520Recently%252C%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520powerful%2520planning%2520and%2520reasoning%250Acapabilities%2520for%2520comprehension%2520and%2520processing%2520of%2520semantic%2520information%2520through%250Arobot%2520control%2520tasks%252C%2520as%2520well%2520as%2520the%2520usability%2520of%2520analytical%2520judgment%2520and%250Adecision-making%2520for%2520multi-modal%2520inputs.%2520To%2520leverage%2520the%2520power%2520of%2520LLMs%2520towards%250Ahumanoid%2520loco-manipulation%252C%2520we%2520propose%2520a%2520novel%2520language-model%2520based%2520framework%250Athat%2520enables%2520robots%2520to%2520autonomously%2520plan%2520behaviors%2520and%2520low-level%2520execution%250Aunder%2520given%2520textual%2520instructions%252C%2520while%2520observing%2520and%2520correcting%2520failures%2520that%250Amay%2520occur%2520during%2520task%2520execution.%2520To%2520systematically%2520evaluate%2520this%2520framework%2520in%250Agrounding%2520LLMs%252C%2520we%2520created%2520the%2520robot%2520%2527action%2527%2520and%2520%2527sensing%2527%2520behavior%2520library%250Afor%2520task%2520planning%252C%2520and%2520conducted%2520mobile%2520manipulation%2520tasks%2520and%2520experiments%2520in%250Aboth%2520simulated%2520and%2520real%2520environments%2520using%2520the%2520CENTAURO%2520robot%252C%2520and%2520verified%2520the%250Aeffectiveness%2520and%2520application%2520of%2520this%2520approach%2520in%2520robotic%2520tasks%2520with%2520autonomous%250Abehavioral%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Behavior%20Planning%20For%20Humanoid%20Loco-manipulation%20Through%0A%20%20Grounded%20Language%20Model&entry.906535625=Jin%20Wang%20and%20Arturo%20Laurenzi%20and%20Nikos%20Tsagarakis&entry.1292438233=%20%20Enabling%20humanoid%20robots%20to%20perform%20autonomously%20loco-manipulation%20in%0Aunstructured%20environments%20is%20crucial%20and%20highly%20challenging%20for%20achieving%0Aembodied%20intelligence.%20This%20involves%20robots%20being%20able%20to%20plan%20their%20actions%0Aand%20behaviors%20in%20long-horizon%20tasks%20while%20using%20multi-modality%20to%20perceive%0Adeviations%20between%20task%20execution%20and%20high-level%20planning.%20Recently%2C%20large%0Alanguage%20models%20%28LLMs%29%20have%20demonstrated%20powerful%20planning%20and%20reasoning%0Acapabilities%20for%20comprehension%20and%20processing%20of%20semantic%20information%20through%0Arobot%20control%20tasks%2C%20as%20well%20as%20the%20usability%20of%20analytical%20judgment%20and%0Adecision-making%20for%20multi-modal%20inputs.%20To%20leverage%20the%20power%20of%20LLMs%20towards%0Ahumanoid%20loco-manipulation%2C%20we%20propose%20a%20novel%20language-model%20based%20framework%0Athat%20enables%20robots%20to%20autonomously%20plan%20behaviors%20and%20low-level%20execution%0Aunder%20given%20textual%20instructions%2C%20while%20observing%20and%20correcting%20failures%20that%0Amay%20occur%20during%20task%20execution.%20To%20systematically%20evaluate%20this%20framework%20in%0Agrounding%20LLMs%2C%20we%20created%20the%20robot%20%27action%27%20and%20%27sensing%27%20behavior%20library%0Afor%20task%20planning%2C%20and%20conducted%20mobile%20manipulation%20tasks%20and%20experiments%20in%0Aboth%20simulated%20and%20real%20environments%20using%20the%20CENTAURO%20robot%2C%20and%20verified%20the%0Aeffectiveness%20and%20application%20of%20this%20approach%20in%20robotic%20tasks%20with%20autonomous%0Abehavioral%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08282v1&entry.124074799=Read"},
{"title": "Winning Snake: Design Choices in Multi-Shot ASP", "author": "Elisa B\u00f6hl and Stefan Ellmauthaler and Sarah Alice Gaggl", "abstract": "  Answer set programming is a well-understood and established problem-solving\nand knowledge representation paradigm. It has become more prominent amongst a\nwider audience due to its multiple applications in science and industry. The\nconstant development of advanced programming and modeling techniques extends\nthe toolset for developers and users regularly. This paper demonstrates\ndifferent techniques to reuse logic program parts (multi-shot) by solving the\narcade game snake. This game is particularly interesting because a victory can\nbe assured by solving the underlying NP-hard problem of Hamiltonian Cycles. We\nwill demonstrate five hands-on implementations in clingo and compare their\nperformance in an empirical evaluation. In addition, our implementation\nutilizes clingraph to generate a simple yet informative image representation of\nthe game's progress.\n", "link": "http://arxiv.org/abs/2408.08150v1", "date": "2024-08-15", "relevancy": 1.3223, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.493}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4275}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Winning%20Snake%3A%20Design%20Choices%20in%20Multi-Shot%20ASP&body=Title%3A%20Winning%20Snake%3A%20Design%20Choices%20in%20Multi-Shot%20ASP%0AAuthor%3A%20Elisa%20B%C3%B6hl%20and%20Stefan%20Ellmauthaler%20and%20Sarah%20Alice%20Gaggl%0AAbstract%3A%20%20%20Answer%20set%20programming%20is%20a%20well-understood%20and%20established%20problem-solving%0Aand%20knowledge%20representation%20paradigm.%20It%20has%20become%20more%20prominent%20amongst%20a%0Awider%20audience%20due%20to%20its%20multiple%20applications%20in%20science%20and%20industry.%20The%0Aconstant%20development%20of%20advanced%20programming%20and%20modeling%20techniques%20extends%0Athe%20toolset%20for%20developers%20and%20users%20regularly.%20This%20paper%20demonstrates%0Adifferent%20techniques%20to%20reuse%20logic%20program%20parts%20%28multi-shot%29%20by%20solving%20the%0Aarcade%20game%20snake.%20This%20game%20is%20particularly%20interesting%20because%20a%20victory%20can%0Abe%20assured%20by%20solving%20the%20underlying%20NP-hard%20problem%20of%20Hamiltonian%20Cycles.%20We%0Awill%20demonstrate%20five%20hands-on%20implementations%20in%20clingo%20and%20compare%20their%0Aperformance%20in%20an%20empirical%20evaluation.%20In%20addition%2C%20our%20implementation%0Autilizes%20clingraph%20to%20generate%20a%20simple%20yet%20informative%20image%20representation%20of%0Athe%20game%27s%20progress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWinning%2520Snake%253A%2520Design%2520Choices%2520in%2520Multi-Shot%2520ASP%26entry.906535625%3DElisa%2520B%25C3%25B6hl%2520and%2520Stefan%2520Ellmauthaler%2520and%2520Sarah%2520Alice%2520Gaggl%26entry.1292438233%3D%2520%2520Answer%2520set%2520programming%2520is%2520a%2520well-understood%2520and%2520established%2520problem-solving%250Aand%2520knowledge%2520representation%2520paradigm.%2520It%2520has%2520become%2520more%2520prominent%2520amongst%2520a%250Awider%2520audience%2520due%2520to%2520its%2520multiple%2520applications%2520in%2520science%2520and%2520industry.%2520The%250Aconstant%2520development%2520of%2520advanced%2520programming%2520and%2520modeling%2520techniques%2520extends%250Athe%2520toolset%2520for%2520developers%2520and%2520users%2520regularly.%2520This%2520paper%2520demonstrates%250Adifferent%2520techniques%2520to%2520reuse%2520logic%2520program%2520parts%2520%2528multi-shot%2529%2520by%2520solving%2520the%250Aarcade%2520game%2520snake.%2520This%2520game%2520is%2520particularly%2520interesting%2520because%2520a%2520victory%2520can%250Abe%2520assured%2520by%2520solving%2520the%2520underlying%2520NP-hard%2520problem%2520of%2520Hamiltonian%2520Cycles.%2520We%250Awill%2520demonstrate%2520five%2520hands-on%2520implementations%2520in%2520clingo%2520and%2520compare%2520their%250Aperformance%2520in%2520an%2520empirical%2520evaluation.%2520In%2520addition%252C%2520our%2520implementation%250Autilizes%2520clingraph%2520to%2520generate%2520a%2520simple%2520yet%2520informative%2520image%2520representation%2520of%250Athe%2520game%2527s%2520progress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Winning%20Snake%3A%20Design%20Choices%20in%20Multi-Shot%20ASP&entry.906535625=Elisa%20B%C3%B6hl%20and%20Stefan%20Ellmauthaler%20and%20Sarah%20Alice%20Gaggl&entry.1292438233=%20%20Answer%20set%20programming%20is%20a%20well-understood%20and%20established%20problem-solving%0Aand%20knowledge%20representation%20paradigm.%20It%20has%20become%20more%20prominent%20amongst%20a%0Awider%20audience%20due%20to%20its%20multiple%20applications%20in%20science%20and%20industry.%20The%0Aconstant%20development%20of%20advanced%20programming%20and%20modeling%20techniques%20extends%0Athe%20toolset%20for%20developers%20and%20users%20regularly.%20This%20paper%20demonstrates%0Adifferent%20techniques%20to%20reuse%20logic%20program%20parts%20%28multi-shot%29%20by%20solving%20the%0Aarcade%20game%20snake.%20This%20game%20is%20particularly%20interesting%20because%20a%20victory%20can%0Abe%20assured%20by%20solving%20the%20underlying%20NP-hard%20problem%20of%20Hamiltonian%20Cycles.%20We%0Awill%20demonstrate%20five%20hands-on%20implementations%20in%20clingo%20and%20compare%20their%0Aperformance%20in%20an%20empirical%20evaluation.%20In%20addition%2C%20our%20implementation%0Autilizes%20clingraph%20to%20generate%20a%20simple%20yet%20informative%20image%20representation%20of%0Athe%20game%27s%20progress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08150v1&entry.124074799=Read"},
{"title": "On the Impact of Uncertainty and Calibration on Likelihood-Ratio\n  Membership Inference Attacks", "author": "Meiyi Zhu and Caili Guo and Chunyan Feng and Osvaldo Simeone", "abstract": "  In a membership inference attack (MIA), an attacker exploits the\noverconfidence exhibited by typical machine learning models to determine\nwhether a specific data point was used to train a target model. In this paper,\nwe analyze the performance of the state-of-the-art likelihood ratio attack\n(LiRA) within an information-theoretical framework that allows the\ninvestigation of the impact of the aleatoric uncertainty in the true data\ngeneration process, of the epistemic uncertainty caused by a limited training\ndata set, and of the calibration level of the target model. We compare three\ndifferent settings, in which the attacker receives decreasingly informative\nfeedback from the target model: confidence vector (CV) disclosure, in which the\noutput probability vector is released; true label confidence (TLC) disclosure,\nin which only the probability assigned to the true label is made available by\nthe model; and decision set (DS) disclosure, in which an adaptive prediction\nset is produced as in conformal prediction. We derive bounds on the advantage\nof an MIA adversary with the aim of offering insights into the impact of\nuncertainty and calibration on the effectiveness of MIAs. Simulation results\ndemonstrate that the derived analytical bounds predict well the effectiveness\nof MIAs.\n", "link": "http://arxiv.org/abs/2402.10686v2", "date": "2024-08-15", "relevancy": 1.4506, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5035}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4844}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Impact%20of%20Uncertainty%20and%20Calibration%20on%20Likelihood-Ratio%0A%20%20Membership%20Inference%20Attacks&body=Title%3A%20On%20the%20Impact%20of%20Uncertainty%20and%20Calibration%20on%20Likelihood-Ratio%0A%20%20Membership%20Inference%20Attacks%0AAuthor%3A%20Meiyi%20Zhu%20and%20Caili%20Guo%20and%20Chunyan%20Feng%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20In%20a%20membership%20inference%20attack%20%28MIA%29%2C%20an%20attacker%20exploits%20the%0Aoverconfidence%20exhibited%20by%20typical%20machine%20learning%20models%20to%20determine%0Awhether%20a%20specific%20data%20point%20was%20used%20to%20train%20a%20target%20model.%20In%20this%20paper%2C%0Awe%20analyze%20the%20performance%20of%20the%20state-of-the-art%20likelihood%20ratio%20attack%0A%28LiRA%29%20within%20an%20information-theoretical%20framework%20that%20allows%20the%0Ainvestigation%20of%20the%20impact%20of%20the%20aleatoric%20uncertainty%20in%20the%20true%20data%0Ageneration%20process%2C%20of%20the%20epistemic%20uncertainty%20caused%20by%20a%20limited%20training%0Adata%20set%2C%20and%20of%20the%20calibration%20level%20of%20the%20target%20model.%20We%20compare%20three%0Adifferent%20settings%2C%20in%20which%20the%20attacker%20receives%20decreasingly%20informative%0Afeedback%20from%20the%20target%20model%3A%20confidence%20vector%20%28CV%29%20disclosure%2C%20in%20which%20the%0Aoutput%20probability%20vector%20is%20released%3B%20true%20label%20confidence%20%28TLC%29%20disclosure%2C%0Ain%20which%20only%20the%20probability%20assigned%20to%20the%20true%20label%20is%20made%20available%20by%0Athe%20model%3B%20and%20decision%20set%20%28DS%29%20disclosure%2C%20in%20which%20an%20adaptive%20prediction%0Aset%20is%20produced%20as%20in%20conformal%20prediction.%20We%20derive%20bounds%20on%20the%20advantage%0Aof%20an%20MIA%20adversary%20with%20the%20aim%20of%20offering%20insights%20into%20the%20impact%20of%0Auncertainty%20and%20calibration%20on%20the%20effectiveness%20of%20MIAs.%20Simulation%20results%0Ademonstrate%20that%20the%20derived%20analytical%20bounds%20predict%20well%20the%20effectiveness%0Aof%20MIAs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Impact%2520of%2520Uncertainty%2520and%2520Calibration%2520on%2520Likelihood-Ratio%250A%2520%2520Membership%2520Inference%2520Attacks%26entry.906535625%3DMeiyi%2520Zhu%2520and%2520Caili%2520Guo%2520and%2520Chunyan%2520Feng%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520In%2520a%2520membership%2520inference%2520attack%2520%2528MIA%2529%252C%2520an%2520attacker%2520exploits%2520the%250Aoverconfidence%2520exhibited%2520by%2520typical%2520machine%2520learning%2520models%2520to%2520determine%250Awhether%2520a%2520specific%2520data%2520point%2520was%2520used%2520to%2520train%2520a%2520target%2520model.%2520In%2520this%2520paper%252C%250Awe%2520analyze%2520the%2520performance%2520of%2520the%2520state-of-the-art%2520likelihood%2520ratio%2520attack%250A%2528LiRA%2529%2520within%2520an%2520information-theoretical%2520framework%2520that%2520allows%2520the%250Ainvestigation%2520of%2520the%2520impact%2520of%2520the%2520aleatoric%2520uncertainty%2520in%2520the%2520true%2520data%250Ageneration%2520process%252C%2520of%2520the%2520epistemic%2520uncertainty%2520caused%2520by%2520a%2520limited%2520training%250Adata%2520set%252C%2520and%2520of%2520the%2520calibration%2520level%2520of%2520the%2520target%2520model.%2520We%2520compare%2520three%250Adifferent%2520settings%252C%2520in%2520which%2520the%2520attacker%2520receives%2520decreasingly%2520informative%250Afeedback%2520from%2520the%2520target%2520model%253A%2520confidence%2520vector%2520%2528CV%2529%2520disclosure%252C%2520in%2520which%2520the%250Aoutput%2520probability%2520vector%2520is%2520released%253B%2520true%2520label%2520confidence%2520%2528TLC%2529%2520disclosure%252C%250Ain%2520which%2520only%2520the%2520probability%2520assigned%2520to%2520the%2520true%2520label%2520is%2520made%2520available%2520by%250Athe%2520model%253B%2520and%2520decision%2520set%2520%2528DS%2529%2520disclosure%252C%2520in%2520which%2520an%2520adaptive%2520prediction%250Aset%2520is%2520produced%2520as%2520in%2520conformal%2520prediction.%2520We%2520derive%2520bounds%2520on%2520the%2520advantage%250Aof%2520an%2520MIA%2520adversary%2520with%2520the%2520aim%2520of%2520offering%2520insights%2520into%2520the%2520impact%2520of%250Auncertainty%2520and%2520calibration%2520on%2520the%2520effectiveness%2520of%2520MIAs.%2520Simulation%2520results%250Ademonstrate%2520that%2520the%2520derived%2520analytical%2520bounds%2520predict%2520well%2520the%2520effectiveness%250Aof%2520MIAs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Impact%20of%20Uncertainty%20and%20Calibration%20on%20Likelihood-Ratio%0A%20%20Membership%20Inference%20Attacks&entry.906535625=Meiyi%20Zhu%20and%20Caili%20Guo%20and%20Chunyan%20Feng%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20In%20a%20membership%20inference%20attack%20%28MIA%29%2C%20an%20attacker%20exploits%20the%0Aoverconfidence%20exhibited%20by%20typical%20machine%20learning%20models%20to%20determine%0Awhether%20a%20specific%20data%20point%20was%20used%20to%20train%20a%20target%20model.%20In%20this%20paper%2C%0Awe%20analyze%20the%20performance%20of%20the%20state-of-the-art%20likelihood%20ratio%20attack%0A%28LiRA%29%20within%20an%20information-theoretical%20framework%20that%20allows%20the%0Ainvestigation%20of%20the%20impact%20of%20the%20aleatoric%20uncertainty%20in%20the%20true%20data%0Ageneration%20process%2C%20of%20the%20epistemic%20uncertainty%20caused%20by%20a%20limited%20training%0Adata%20set%2C%20and%20of%20the%20calibration%20level%20of%20the%20target%20model.%20We%20compare%20three%0Adifferent%20settings%2C%20in%20which%20the%20attacker%20receives%20decreasingly%20informative%0Afeedback%20from%20the%20target%20model%3A%20confidence%20vector%20%28CV%29%20disclosure%2C%20in%20which%20the%0Aoutput%20probability%20vector%20is%20released%3B%20true%20label%20confidence%20%28TLC%29%20disclosure%2C%0Ain%20which%20only%20the%20probability%20assigned%20to%20the%20true%20label%20is%20made%20available%20by%0Athe%20model%3B%20and%20decision%20set%20%28DS%29%20disclosure%2C%20in%20which%20an%20adaptive%20prediction%0Aset%20is%20produced%20as%20in%20conformal%20prediction.%20We%20derive%20bounds%20on%20the%20advantage%0Aof%20an%20MIA%20adversary%20with%20the%20aim%20of%20offering%20insights%20into%20the%20impact%20of%0Auncertainty%20and%20calibration%20on%20the%20effectiveness%20of%20MIAs.%20Simulation%20results%0Ademonstrate%20that%20the%20derived%20analytical%20bounds%20predict%20well%20the%20effectiveness%0Aof%20MIAs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10686v2&entry.124074799=Read"},
{"title": "Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners", "author": "Yifei Gao and Jie Ou and Lei Wang and Fanhua Shang and Jaji Wu and Jun Cheng", "abstract": "  Large Language Models (LLMs) showcase remarkable performance and robust\ndeductive capabilities, yet their expansive size complicates deployment and\nraises environmental concerns due to substantial resource consumption. The\nrecent development of a quantization technique known as Learnable\nSingular-value Increment (LSI) has addressed some of these quantization\nchallenges. Leveraging insights from LSI and our extensive research, we have\ndeveloped innovative methods that enhance the performance of quantized LLMs,\nparticularly in low-bit settings. Our methods consistently deliver\nstate-of-the-art results across various quantization scenarios and offer deep\ntheoretical insights into the quantization process, elucidating the potential\nof quantized models for widespread application.\n", "link": "http://arxiv.org/abs/2407.15508v2", "date": "2024-08-15", "relevancy": 0.9271, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4755}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4578}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compensate%20Quantization%20Errors%2B%3A%20Quantized%20Models%20Are%20Inquisitive%0A%20%20Learners&body=Title%3A%20Compensate%20Quantization%20Errors%2B%3A%20Quantized%20Models%20Are%20Inquisitive%0A%20%20Learners%0AAuthor%3A%20Yifei%20Gao%20and%20Jie%20Ou%20and%20Lei%20Wang%20and%20Fanhua%20Shang%20and%20Jaji%20Wu%20and%20Jun%20Cheng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20showcase%20remarkable%20performance%20and%20robust%0Adeductive%20capabilities%2C%20yet%20their%20expansive%20size%20complicates%20deployment%20and%0Araises%20environmental%20concerns%20due%20to%20substantial%20resource%20consumption.%20The%0Arecent%20development%20of%20a%20quantization%20technique%20known%20as%20Learnable%0ASingular-value%20Increment%20%28LSI%29%20has%20addressed%20some%20of%20these%20quantization%0Achallenges.%20Leveraging%20insights%20from%20LSI%20and%20our%20extensive%20research%2C%20we%20have%0Adeveloped%20innovative%20methods%20that%20enhance%20the%20performance%20of%20quantized%20LLMs%2C%0Aparticularly%20in%20low-bit%20settings.%20Our%20methods%20consistently%20deliver%0Astate-of-the-art%20results%20across%20various%20quantization%20scenarios%20and%20offer%20deep%0Atheoretical%20insights%20into%20the%20quantization%20process%2C%20elucidating%20the%20potential%0Aof%20quantized%20models%20for%20widespread%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompensate%2520Quantization%2520Errors%252B%253A%2520Quantized%2520Models%2520Are%2520Inquisitive%250A%2520%2520Learners%26entry.906535625%3DYifei%2520Gao%2520and%2520Jie%2520Ou%2520and%2520Lei%2520Wang%2520and%2520Fanhua%2520Shang%2520and%2520Jaji%2520Wu%2520and%2520Jun%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520showcase%2520remarkable%2520performance%2520and%2520robust%250Adeductive%2520capabilities%252C%2520yet%2520their%2520expansive%2520size%2520complicates%2520deployment%2520and%250Araises%2520environmental%2520concerns%2520due%2520to%2520substantial%2520resource%2520consumption.%2520The%250Arecent%2520development%2520of%2520a%2520quantization%2520technique%2520known%2520as%2520Learnable%250ASingular-value%2520Increment%2520%2528LSI%2529%2520has%2520addressed%2520some%2520of%2520these%2520quantization%250Achallenges.%2520Leveraging%2520insights%2520from%2520LSI%2520and%2520our%2520extensive%2520research%252C%2520we%2520have%250Adeveloped%2520innovative%2520methods%2520that%2520enhance%2520the%2520performance%2520of%2520quantized%2520LLMs%252C%250Aparticularly%2520in%2520low-bit%2520settings.%2520Our%2520methods%2520consistently%2520deliver%250Astate-of-the-art%2520results%2520across%2520various%2520quantization%2520scenarios%2520and%2520offer%2520deep%250Atheoretical%2520insights%2520into%2520the%2520quantization%2520process%252C%2520elucidating%2520the%2520potential%250Aof%2520quantized%2520models%2520for%2520widespread%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compensate%20Quantization%20Errors%2B%3A%20Quantized%20Models%20Are%20Inquisitive%0A%20%20Learners&entry.906535625=Yifei%20Gao%20and%20Jie%20Ou%20and%20Lei%20Wang%20and%20Fanhua%20Shang%20and%20Jaji%20Wu%20and%20Jun%20Cheng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20showcase%20remarkable%20performance%20and%20robust%0Adeductive%20capabilities%2C%20yet%20their%20expansive%20size%20complicates%20deployment%20and%0Araises%20environmental%20concerns%20due%20to%20substantial%20resource%20consumption.%20The%0Arecent%20development%20of%20a%20quantization%20technique%20known%20as%20Learnable%0ASingular-value%20Increment%20%28LSI%29%20has%20addressed%20some%20of%20these%20quantization%0Achallenges.%20Leveraging%20insights%20from%20LSI%20and%20our%20extensive%20research%2C%20we%20have%0Adeveloped%20innovative%20methods%20that%20enhance%20the%20performance%20of%20quantized%20LLMs%2C%0Aparticularly%20in%20low-bit%20settings.%20Our%20methods%20consistently%20deliver%0Astate-of-the-art%20results%20across%20various%20quantization%20scenarios%20and%20offer%20deep%0Atheoretical%20insights%20into%20the%20quantization%20process%2C%20elucidating%20the%20potential%0Aof%20quantized%20models%20for%20widespread%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15508v2&entry.124074799=Read"},
{"title": "Examining Common Paradigms in Multi-Task Learning", "author": "Cathrin Elich and Lukas Kirchdorfer and Jan M. K\u00f6hler and Lukas Schott", "abstract": "  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we investigate paradigms in MTL in\nthe context of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Overall,\nwe find surprising similarities between STL and MTL suggesting to consider\nmethods from both fields in a broader context.\n", "link": "http://arxiv.org/abs/2311.04698v5", "date": "2024-08-15", "relevancy": 1.4752, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5247}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4549}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Examining%20Common%20Paradigms%20in%20Multi-Task%20Learning&body=Title%3A%20Examining%20Common%20Paradigms%20in%20Multi-Task%20Learning%0AAuthor%3A%20Cathrin%20Elich%20and%20Lukas%20Kirchdorfer%20and%20Jan%20M.%20K%C3%B6hler%20and%20Lukas%20Schott%0AAbstract%3A%20%20%20While%20multi-task%20learning%20%28MTL%29%20has%20gained%20significant%20attention%20in%20recent%0Ayears%2C%20its%20underlying%20mechanisms%20remain%20poorly%20understood.%20Recent%20methods%20did%0Anot%20yield%20consistent%20performance%20improvements%20over%20single%20task%20learning%20%28STL%29%0Abaselines%2C%20underscoring%20the%20importance%20of%20gaining%20more%20profound%20insights%20about%0Achallenges%20specific%20to%20MTL.%20In%20our%20study%2C%20we%20investigate%20paradigms%20in%20MTL%20in%0Athe%20context%20of%20STL%3A%20First%2C%20the%20impact%20of%20the%20choice%20of%20optimizer%20has%20only%20been%0Amildly%20investigated%20in%20MTL.%20We%20show%20the%20pivotal%20role%20of%20common%20STL%20tools%20such%0Aas%20the%20Adam%20optimizer%20in%20MTL%20empirically%20in%20various%20experiments.%20To%20further%0Ainvestigate%20Adam%27s%20effectiveness%2C%20we%20theoretical%20derive%20a%20partial%20loss-scale%0Ainvariance%20under%20mild%20assumptions.%20Second%2C%20the%20notion%20of%20gradient%20conflicts%20has%0Aoften%20been%20phrased%20as%20a%20specific%20problem%20in%20MTL.%20We%20delve%20into%20the%20role%20of%0Agradient%20conflicts%20in%20MTL%20and%20compare%20it%20to%20STL.%20For%20angular%20gradient%20alignment%0Awe%20find%20no%20evidence%20that%20this%20is%20a%20unique%20problem%20in%20MTL.%20We%20emphasize%0Adifferences%20in%20gradient%20magnitude%20as%20the%20main%20distinguishing%20factor.%20Overall%2C%0Awe%20find%20surprising%20similarities%20between%20STL%20and%20MTL%20suggesting%20to%20consider%0Amethods%20from%20both%20fields%20in%20a%20broader%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04698v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExamining%2520Common%2520Paradigms%2520in%2520Multi-Task%2520Learning%26entry.906535625%3DCathrin%2520Elich%2520and%2520Lukas%2520Kirchdorfer%2520and%2520Jan%2520M.%2520K%25C3%25B6hler%2520and%2520Lukas%2520Schott%26entry.1292438233%3D%2520%2520While%2520multi-task%2520learning%2520%2528MTL%2529%2520has%2520gained%2520significant%2520attention%2520in%2520recent%250Ayears%252C%2520its%2520underlying%2520mechanisms%2520remain%2520poorly%2520understood.%2520Recent%2520methods%2520did%250Anot%2520yield%2520consistent%2520performance%2520improvements%2520over%2520single%2520task%2520learning%2520%2528STL%2529%250Abaselines%252C%2520underscoring%2520the%2520importance%2520of%2520gaining%2520more%2520profound%2520insights%2520about%250Achallenges%2520specific%2520to%2520MTL.%2520In%2520our%2520study%252C%2520we%2520investigate%2520paradigms%2520in%2520MTL%2520in%250Athe%2520context%2520of%2520STL%253A%2520First%252C%2520the%2520impact%2520of%2520the%2520choice%2520of%2520optimizer%2520has%2520only%2520been%250Amildly%2520investigated%2520in%2520MTL.%2520We%2520show%2520the%2520pivotal%2520role%2520of%2520common%2520STL%2520tools%2520such%250Aas%2520the%2520Adam%2520optimizer%2520in%2520MTL%2520empirically%2520in%2520various%2520experiments.%2520To%2520further%250Ainvestigate%2520Adam%2527s%2520effectiveness%252C%2520we%2520theoretical%2520derive%2520a%2520partial%2520loss-scale%250Ainvariance%2520under%2520mild%2520assumptions.%2520Second%252C%2520the%2520notion%2520of%2520gradient%2520conflicts%2520has%250Aoften%2520been%2520phrased%2520as%2520a%2520specific%2520problem%2520in%2520MTL.%2520We%2520delve%2520into%2520the%2520role%2520of%250Agradient%2520conflicts%2520in%2520MTL%2520and%2520compare%2520it%2520to%2520STL.%2520For%2520angular%2520gradient%2520alignment%250Awe%2520find%2520no%2520evidence%2520that%2520this%2520is%2520a%2520unique%2520problem%2520in%2520MTL.%2520We%2520emphasize%250Adifferences%2520in%2520gradient%2520magnitude%2520as%2520the%2520main%2520distinguishing%2520factor.%2520Overall%252C%250Awe%2520find%2520surprising%2520similarities%2520between%2520STL%2520and%2520MTL%2520suggesting%2520to%2520consider%250Amethods%2520from%2520both%2520fields%2520in%2520a%2520broader%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04698v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Examining%20Common%20Paradigms%20in%20Multi-Task%20Learning&entry.906535625=Cathrin%20Elich%20and%20Lukas%20Kirchdorfer%20and%20Jan%20M.%20K%C3%B6hler%20and%20Lukas%20Schott&entry.1292438233=%20%20While%20multi-task%20learning%20%28MTL%29%20has%20gained%20significant%20attention%20in%20recent%0Ayears%2C%20its%20underlying%20mechanisms%20remain%20poorly%20understood.%20Recent%20methods%20did%0Anot%20yield%20consistent%20performance%20improvements%20over%20single%20task%20learning%20%28STL%29%0Abaselines%2C%20underscoring%20the%20importance%20of%20gaining%20more%20profound%20insights%20about%0Achallenges%20specific%20to%20MTL.%20In%20our%20study%2C%20we%20investigate%20paradigms%20in%20MTL%20in%0Athe%20context%20of%20STL%3A%20First%2C%20the%20impact%20of%20the%20choice%20of%20optimizer%20has%20only%20been%0Amildly%20investigated%20in%20MTL.%20We%20show%20the%20pivotal%20role%20of%20common%20STL%20tools%20such%0Aas%20the%20Adam%20optimizer%20in%20MTL%20empirically%20in%20various%20experiments.%20To%20further%0Ainvestigate%20Adam%27s%20effectiveness%2C%20we%20theoretical%20derive%20a%20partial%20loss-scale%0Ainvariance%20under%20mild%20assumptions.%20Second%2C%20the%20notion%20of%20gradient%20conflicts%20has%0Aoften%20been%20phrased%20as%20a%20specific%20problem%20in%20MTL.%20We%20delve%20into%20the%20role%20of%0Agradient%20conflicts%20in%20MTL%20and%20compare%20it%20to%20STL.%20For%20angular%20gradient%20alignment%0Awe%20find%20no%20evidence%20that%20this%20is%20a%20unique%20problem%20in%20MTL.%20We%20emphasize%0Adifferences%20in%20gradient%20magnitude%20as%20the%20main%20distinguishing%20factor.%20Overall%2C%0Awe%20find%20surprising%20similarities%20between%20STL%20and%20MTL%20suggesting%20to%20consider%0Amethods%20from%20both%20fields%20in%20a%20broader%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04698v5&entry.124074799=Read"},
{"title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language\n  Models to Infer Causal Links Between Siamese Images", "author": "Zhiyuan Li and Heng Wang and Dongnan Liu and Chaoyi Zhang and Ao Ma and Jieting Long and Weidong Cai", "abstract": "  Large Language Models (LLMs) have showcased exceptional ability in causal\nreasoning from textual information. However, will these causalities remain\nstraightforward for Vision Large Language Models (VLLMs) when only visual hints\nare provided? Motivated by this, we propose a novel Multimodal Causal Reasoning\nbenchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect\nrelationship when solely relying on visual cues such as action, appearance,\nclothing, and environment. Specifically, we introduce a prompt-driven image\nsynthesis approach to create siamese images with embedded semantic causality\nand visual cues, which can effectively evaluate VLLMs' causal reasoning\ncapabilities. Additionally, we develop tailored metrics from multiple\nperspectives, including image-level match, phrase-level understanding, and\nsentence-level explanation, to comprehensively assess VLLMs' comprehension\nabilities. Our extensive experiments reveal that the current state-of-the-art\nVLLMs are not as skilled at multimodal causal reasoning as we might have hoped.\nFurthermore, we perform a comprehensive analysis to understand these models'\nshortcomings from different views and suggest directions for future research.\nWe hope MuCR can serve as a valuable resource and foundational benchmark in\nmultimodal causal reasoning research. The project is available at:\nhttps://github.com/Zhiyuan-Li-John/MuCR\n", "link": "http://arxiv.org/abs/2408.08105v1", "date": "2024-08-15", "relevancy": 1.5955, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Causal%20Reasoning%20Benchmark%3A%20Challenging%20Vision%20Large%20Language%0A%20%20Models%20to%20Infer%20Causal%20Links%20Between%20Siamese%20Images&body=Title%3A%20Multimodal%20Causal%20Reasoning%20Benchmark%3A%20Challenging%20Vision%20Large%20Language%0A%20%20Models%20to%20Infer%20Causal%20Links%20Between%20Siamese%20Images%0AAuthor%3A%20Zhiyuan%20Li%20and%20Heng%20Wang%20and%20Dongnan%20Liu%20and%20Chaoyi%20Zhang%20and%20Ao%20Ma%20and%20Jieting%20Long%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20showcased%20exceptional%20ability%20in%20causal%0Areasoning%20from%20textual%20information.%20However%2C%20will%20these%20causalities%20remain%0Astraightforward%20for%20Vision%20Large%20Language%20Models%20%28VLLMs%29%20when%20only%20visual%20hints%0Aare%20provided%3F%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%20Multimodal%20Causal%20Reasoning%0Abenchmark%2C%20namely%20MuCR%2C%20to%20challenge%20VLLMs%20to%20infer%20semantic%20cause-and-effect%0Arelationship%20when%20solely%20relying%20on%20visual%20cues%20such%20as%20action%2C%20appearance%2C%0Aclothing%2C%20and%20environment.%20Specifically%2C%20we%20introduce%20a%20prompt-driven%20image%0Asynthesis%20approach%20to%20create%20siamese%20images%20with%20embedded%20semantic%20causality%0Aand%20visual%20cues%2C%20which%20can%20effectively%20evaluate%20VLLMs%27%20causal%20reasoning%0Acapabilities.%20Additionally%2C%20we%20develop%20tailored%20metrics%20from%20multiple%0Aperspectives%2C%20including%20image-level%20match%2C%20phrase-level%20understanding%2C%20and%0Asentence-level%20explanation%2C%20to%20comprehensively%20assess%20VLLMs%27%20comprehension%0Aabilities.%20Our%20extensive%20experiments%20reveal%20that%20the%20current%20state-of-the-art%0AVLLMs%20are%20not%20as%20skilled%20at%20multimodal%20causal%20reasoning%20as%20we%20might%20have%20hoped.%0AFurthermore%2C%20we%20perform%20a%20comprehensive%20analysis%20to%20understand%20these%20models%27%0Ashortcomings%20from%20different%20views%20and%20suggest%20directions%20for%20future%20research.%0AWe%20hope%20MuCR%20can%20serve%20as%20a%20valuable%20resource%20and%20foundational%20benchmark%20in%0Amultimodal%20causal%20reasoning%20research.%20The%20project%20is%20available%20at%3A%0Ahttps%3A//github.com/Zhiyuan-Li-John/MuCR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Causal%2520Reasoning%2520Benchmark%253A%2520Challenging%2520Vision%2520Large%2520Language%250A%2520%2520Models%2520to%2520Infer%2520Causal%2520Links%2520Between%2520Siamese%2520Images%26entry.906535625%3DZhiyuan%2520Li%2520and%2520Heng%2520Wang%2520and%2520Dongnan%2520Liu%2520and%2520Chaoyi%2520Zhang%2520and%2520Ao%2520Ma%2520and%2520Jieting%2520Long%2520and%2520Weidong%2520Cai%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520showcased%2520exceptional%2520ability%2520in%2520causal%250Areasoning%2520from%2520textual%2520information.%2520However%252C%2520will%2520these%2520causalities%2520remain%250Astraightforward%2520for%2520Vision%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520when%2520only%2520visual%2520hints%250Aare%2520provided%253F%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520novel%2520Multimodal%2520Causal%2520Reasoning%250Abenchmark%252C%2520namely%2520MuCR%252C%2520to%2520challenge%2520VLLMs%2520to%2520infer%2520semantic%2520cause-and-effect%250Arelationship%2520when%2520solely%2520relying%2520on%2520visual%2520cues%2520such%2520as%2520action%252C%2520appearance%252C%250Aclothing%252C%2520and%2520environment.%2520Specifically%252C%2520we%2520introduce%2520a%2520prompt-driven%2520image%250Asynthesis%2520approach%2520to%2520create%2520siamese%2520images%2520with%2520embedded%2520semantic%2520causality%250Aand%2520visual%2520cues%252C%2520which%2520can%2520effectively%2520evaluate%2520VLLMs%2527%2520causal%2520reasoning%250Acapabilities.%2520Additionally%252C%2520we%2520develop%2520tailored%2520metrics%2520from%2520multiple%250Aperspectives%252C%2520including%2520image-level%2520match%252C%2520phrase-level%2520understanding%252C%2520and%250Asentence-level%2520explanation%252C%2520to%2520comprehensively%2520assess%2520VLLMs%2527%2520comprehension%250Aabilities.%2520Our%2520extensive%2520experiments%2520reveal%2520that%2520the%2520current%2520state-of-the-art%250AVLLMs%2520are%2520not%2520as%2520skilled%2520at%2520multimodal%2520causal%2520reasoning%2520as%2520we%2520might%2520have%2520hoped.%250AFurthermore%252C%2520we%2520perform%2520a%2520comprehensive%2520analysis%2520to%2520understand%2520these%2520models%2527%250Ashortcomings%2520from%2520different%2520views%2520and%2520suggest%2520directions%2520for%2520future%2520research.%250AWe%2520hope%2520MuCR%2520can%2520serve%2520as%2520a%2520valuable%2520resource%2520and%2520foundational%2520benchmark%2520in%250Amultimodal%2520causal%2520reasoning%2520research.%2520The%2520project%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Zhiyuan-Li-John/MuCR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Causal%20Reasoning%20Benchmark%3A%20Challenging%20Vision%20Large%20Language%0A%20%20Models%20to%20Infer%20Causal%20Links%20Between%20Siamese%20Images&entry.906535625=Zhiyuan%20Li%20and%20Heng%20Wang%20and%20Dongnan%20Liu%20and%20Chaoyi%20Zhang%20and%20Ao%20Ma%20and%20Jieting%20Long%20and%20Weidong%20Cai&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20showcased%20exceptional%20ability%20in%20causal%0Areasoning%20from%20textual%20information.%20However%2C%20will%20these%20causalities%20remain%0Astraightforward%20for%20Vision%20Large%20Language%20Models%20%28VLLMs%29%20when%20only%20visual%20hints%0Aare%20provided%3F%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%20Multimodal%20Causal%20Reasoning%0Abenchmark%2C%20namely%20MuCR%2C%20to%20challenge%20VLLMs%20to%20infer%20semantic%20cause-and-effect%0Arelationship%20when%20solely%20relying%20on%20visual%20cues%20such%20as%20action%2C%20appearance%2C%0Aclothing%2C%20and%20environment.%20Specifically%2C%20we%20introduce%20a%20prompt-driven%20image%0Asynthesis%20approach%20to%20create%20siamese%20images%20with%20embedded%20semantic%20causality%0Aand%20visual%20cues%2C%20which%20can%20effectively%20evaluate%20VLLMs%27%20causal%20reasoning%0Acapabilities.%20Additionally%2C%20we%20develop%20tailored%20metrics%20from%20multiple%0Aperspectives%2C%20including%20image-level%20match%2C%20phrase-level%20understanding%2C%20and%0Asentence-level%20explanation%2C%20to%20comprehensively%20assess%20VLLMs%27%20comprehension%0Aabilities.%20Our%20extensive%20experiments%20reveal%20that%20the%20current%20state-of-the-art%0AVLLMs%20are%20not%20as%20skilled%20at%20multimodal%20causal%20reasoning%20as%20we%20might%20have%20hoped.%0AFurthermore%2C%20we%20perform%20a%20comprehensive%20analysis%20to%20understand%20these%20models%27%0Ashortcomings%20from%20different%20views%20and%20suggest%20directions%20for%20future%20research.%0AWe%20hope%20MuCR%20can%20serve%20as%20a%20valuable%20resource%20and%20foundational%20benchmark%20in%0Amultimodal%20causal%20reasoning%20research.%20The%20project%20is%20available%20at%3A%0Ahttps%3A//github.com/Zhiyuan-Li-John/MuCR%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08105v1&entry.124074799=Read"},
{"title": "On Model Compression for Neural Networks: Framework, Algorithm, and\n  Convergence Guarantee", "author": "Chenyang Li and Jihoon Chung and Mengnan Du and Haimin Wang and Xianlian Zhou and Bo Shen", "abstract": "  Model compression is a crucial part of deploying neural networks (NNs),\nespecially when the memory and storage of computing devices are limited in many\napplications. This paper focuses on two model compression techniques: low-rank\napproximation and weight pruning in neural networks, which are very popular\nnowadays. However, training NN with low-rank approximation and weight pruning\nalways suffers significant accuracy loss and convergence issues. In this paper,\na holistic framework is proposed for model compression from a novel perspective\nof nonconvex optimization by designing an appropriate objective function. Then,\nwe introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the\nnonconvex optimization. One advantage of our algorithm is that an efficient\niteration scheme can be derived with closed-form, which is gradient-free.\nTherefore, our algorithm will not suffer from vanishing/exploding gradient\nproblems. Furthermore, with the Kurdyka-{\\L}ojasiewicz (K{\\L}) property of our\nobjective function, we show that our algorithm globally converges to a critical\npoint at the rate of O(1/k), where k denotes the number of iterations. Lastly,\nextensive experiments with tensor train decomposition and weight pruning\ndemonstrate the efficiency and superior performance of the proposed framework.\nOur code implementation is available at https://github.com/ChenyangLi-97/NN-BCD\n", "link": "http://arxiv.org/abs/2303.06815v3", "date": "2024-08-15", "relevancy": 1.5213, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5241}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.496}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Model%20Compression%20for%20Neural%20Networks%3A%20Framework%2C%20Algorithm%2C%20and%0A%20%20Convergence%20Guarantee&body=Title%3A%20On%20Model%20Compression%20for%20Neural%20Networks%3A%20Framework%2C%20Algorithm%2C%20and%0A%20%20Convergence%20Guarantee%0AAuthor%3A%20Chenyang%20Li%20and%20Jihoon%20Chung%20and%20Mengnan%20Du%20and%20Haimin%20Wang%20and%20Xianlian%20Zhou%20and%20Bo%20Shen%0AAbstract%3A%20%20%20Model%20compression%20is%20a%20crucial%20part%20of%20deploying%20neural%20networks%20%28NNs%29%2C%0Aespecially%20when%20the%20memory%20and%20storage%20of%20computing%20devices%20are%20limited%20in%20many%0Aapplications.%20This%20paper%20focuses%20on%20two%20model%20compression%20techniques%3A%20low-rank%0Aapproximation%20and%20weight%20pruning%20in%20neural%20networks%2C%20which%20are%20very%20popular%0Anowadays.%20However%2C%20training%20NN%20with%20low-rank%20approximation%20and%20weight%20pruning%0Aalways%20suffers%20significant%20accuracy%20loss%20and%20convergence%20issues.%20In%20this%20paper%2C%0Aa%20holistic%20framework%20is%20proposed%20for%20model%20compression%20from%20a%20novel%20perspective%0Aof%20nonconvex%20optimization%20by%20designing%20an%20appropriate%20objective%20function.%20Then%2C%0Awe%20introduce%20NN-BCD%2C%20a%20block%20coordinate%20descent%20%28BCD%29%20algorithm%20to%20solve%20the%0Anonconvex%20optimization.%20One%20advantage%20of%20our%20algorithm%20is%20that%20an%20efficient%0Aiteration%20scheme%20can%20be%20derived%20with%20closed-form%2C%20which%20is%20gradient-free.%0ATherefore%2C%20our%20algorithm%20will%20not%20suffer%20from%20vanishing/exploding%20gradient%0Aproblems.%20Furthermore%2C%20with%20the%20Kurdyka-%7B%5CL%7Dojasiewicz%20%28K%7B%5CL%7D%29%20property%20of%20our%0Aobjective%20function%2C%20we%20show%20that%20our%20algorithm%20globally%20converges%20to%20a%20critical%0Apoint%20at%20the%20rate%20of%20O%281/k%29%2C%20where%20k%20denotes%20the%20number%20of%20iterations.%20Lastly%2C%0Aextensive%20experiments%20with%20tensor%20train%20decomposition%20and%20weight%20pruning%0Ademonstrate%20the%20efficiency%20and%20superior%20performance%20of%20the%20proposed%20framework.%0AOur%20code%20implementation%20is%20available%20at%20https%3A//github.com/ChenyangLi-97/NN-BCD%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06815v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Model%2520Compression%2520for%2520Neural%2520Networks%253A%2520Framework%252C%2520Algorithm%252C%2520and%250A%2520%2520Convergence%2520Guarantee%26entry.906535625%3DChenyang%2520Li%2520and%2520Jihoon%2520Chung%2520and%2520Mengnan%2520Du%2520and%2520Haimin%2520Wang%2520and%2520Xianlian%2520Zhou%2520and%2520Bo%2520Shen%26entry.1292438233%3D%2520%2520Model%2520compression%2520is%2520a%2520crucial%2520part%2520of%2520deploying%2520neural%2520networks%2520%2528NNs%2529%252C%250Aespecially%2520when%2520the%2520memory%2520and%2520storage%2520of%2520computing%2520devices%2520are%2520limited%2520in%2520many%250Aapplications.%2520This%2520paper%2520focuses%2520on%2520two%2520model%2520compression%2520techniques%253A%2520low-rank%250Aapproximation%2520and%2520weight%2520pruning%2520in%2520neural%2520networks%252C%2520which%2520are%2520very%2520popular%250Anowadays.%2520However%252C%2520training%2520NN%2520with%2520low-rank%2520approximation%2520and%2520weight%2520pruning%250Aalways%2520suffers%2520significant%2520accuracy%2520loss%2520and%2520convergence%2520issues.%2520In%2520this%2520paper%252C%250Aa%2520holistic%2520framework%2520is%2520proposed%2520for%2520model%2520compression%2520from%2520a%2520novel%2520perspective%250Aof%2520nonconvex%2520optimization%2520by%2520designing%2520an%2520appropriate%2520objective%2520function.%2520Then%252C%250Awe%2520introduce%2520NN-BCD%252C%2520a%2520block%2520coordinate%2520descent%2520%2528BCD%2529%2520algorithm%2520to%2520solve%2520the%250Anonconvex%2520optimization.%2520One%2520advantage%2520of%2520our%2520algorithm%2520is%2520that%2520an%2520efficient%250Aiteration%2520scheme%2520can%2520be%2520derived%2520with%2520closed-form%252C%2520which%2520is%2520gradient-free.%250ATherefore%252C%2520our%2520algorithm%2520will%2520not%2520suffer%2520from%2520vanishing/exploding%2520gradient%250Aproblems.%2520Furthermore%252C%2520with%2520the%2520Kurdyka-%257B%255CL%257Dojasiewicz%2520%2528K%257B%255CL%257D%2529%2520property%2520of%2520our%250Aobjective%2520function%252C%2520we%2520show%2520that%2520our%2520algorithm%2520globally%2520converges%2520to%2520a%2520critical%250Apoint%2520at%2520the%2520rate%2520of%2520O%25281/k%2529%252C%2520where%2520k%2520denotes%2520the%2520number%2520of%2520iterations.%2520Lastly%252C%250Aextensive%2520experiments%2520with%2520tensor%2520train%2520decomposition%2520and%2520weight%2520pruning%250Ademonstrate%2520the%2520efficiency%2520and%2520superior%2520performance%2520of%2520the%2520proposed%2520framework.%250AOur%2520code%2520implementation%2520is%2520available%2520at%2520https%253A//github.com/ChenyangLi-97/NN-BCD%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.06815v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Model%20Compression%20for%20Neural%20Networks%3A%20Framework%2C%20Algorithm%2C%20and%0A%20%20Convergence%20Guarantee&entry.906535625=Chenyang%20Li%20and%20Jihoon%20Chung%20and%20Mengnan%20Du%20and%20Haimin%20Wang%20and%20Xianlian%20Zhou%20and%20Bo%20Shen&entry.1292438233=%20%20Model%20compression%20is%20a%20crucial%20part%20of%20deploying%20neural%20networks%20%28NNs%29%2C%0Aespecially%20when%20the%20memory%20and%20storage%20of%20computing%20devices%20are%20limited%20in%20many%0Aapplications.%20This%20paper%20focuses%20on%20two%20model%20compression%20techniques%3A%20low-rank%0Aapproximation%20and%20weight%20pruning%20in%20neural%20networks%2C%20which%20are%20very%20popular%0Anowadays.%20However%2C%20training%20NN%20with%20low-rank%20approximation%20and%20weight%20pruning%0Aalways%20suffers%20significant%20accuracy%20loss%20and%20convergence%20issues.%20In%20this%20paper%2C%0Aa%20holistic%20framework%20is%20proposed%20for%20model%20compression%20from%20a%20novel%20perspective%0Aof%20nonconvex%20optimization%20by%20designing%20an%20appropriate%20objective%20function.%20Then%2C%0Awe%20introduce%20NN-BCD%2C%20a%20block%20coordinate%20descent%20%28BCD%29%20algorithm%20to%20solve%20the%0Anonconvex%20optimization.%20One%20advantage%20of%20our%20algorithm%20is%20that%20an%20efficient%0Aiteration%20scheme%20can%20be%20derived%20with%20closed-form%2C%20which%20is%20gradient-free.%0ATherefore%2C%20our%20algorithm%20will%20not%20suffer%20from%20vanishing/exploding%20gradient%0Aproblems.%20Furthermore%2C%20with%20the%20Kurdyka-%7B%5CL%7Dojasiewicz%20%28K%7B%5CL%7D%29%20property%20of%20our%0Aobjective%20function%2C%20we%20show%20that%20our%20algorithm%20globally%20converges%20to%20a%20critical%0Apoint%20at%20the%20rate%20of%20O%281/k%29%2C%20where%20k%20denotes%20the%20number%20of%20iterations.%20Lastly%2C%0Aextensive%20experiments%20with%20tensor%20train%20decomposition%20and%20weight%20pruning%0Ademonstrate%20the%20efficiency%20and%20superior%20performance%20of%20the%20proposed%20framework.%0AOur%20code%20implementation%20is%20available%20at%20https%3A//github.com/ChenyangLi-97/NN-BCD%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06815v3&entry.124074799=Read"},
{"title": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential\n  Recommendation", "author": "Bohao Wang and Feng Liu and Jiawei Chen and Yudi Wu and Xingyu Lou and Jun Wang and Yan Feng and Chun Chen and Can Wang", "abstract": "  Sequential recommendation systems fundamentally rely on users' historical\ninteraction sequences, which are often contaminated by noisy interactions.\nIdentifying these noisy interactions accurately without additional information\nis particularly difficult due to the lack of explicit supervisory signals to\ndenote noise. Large Language Models (LLMs), equipped with extensive open\nknowledge and semantic reasoning abilities, present a promising avenue to\nbridge this information gap. However, employing LLMs for denoising in\nsequential recommendation introduces notable challenges: 1) Direct application\nof pretrained LLMs may not be competent for the denoising task, frequently\ngenerating nonsensical responses; 2) Even after fine-tuning, the reliability of\nLLM outputs remains questionable, especially given the complexity of the task\nand th inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing the\ncorrected sequences to be flexibly applied across various recommendation\nmodels. Extensive experiments validate the superiority of LLM4DSR over existing\nmethods across three datasets and three recommendation backbones.\n", "link": "http://arxiv.org/abs/2408.08208v1", "date": "2024-08-15", "relevancy": 1.4469, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5229}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4728}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM4DSR%3A%20Leveraing%20Large%20Language%20Model%20for%20Denoising%20Sequential%0A%20%20Recommendation&body=Title%3A%20LLM4DSR%3A%20Leveraing%20Large%20Language%20Model%20for%20Denoising%20Sequential%0A%20%20Recommendation%0AAuthor%3A%20Bohao%20Wang%20and%20Feng%20Liu%20and%20Jiawei%20Chen%20and%20Yudi%20Wu%20and%20Xingyu%20Lou%20and%20Jun%20Wang%20and%20Yan%20Feng%20and%20Chun%20Chen%20and%20Can%20Wang%0AAbstract%3A%20%20%20Sequential%20recommendation%20systems%20fundamentally%20rely%20on%20users%27%20historical%0Ainteraction%20sequences%2C%20which%20are%20often%20contaminated%20by%20noisy%20interactions.%0AIdentifying%20these%20noisy%20interactions%20accurately%20without%20additional%20information%0Ais%20particularly%20difficult%20due%20to%20the%20lack%20of%20explicit%20supervisory%20signals%20to%0Adenote%20noise.%20Large%20Language%20Models%20%28LLMs%29%2C%20equipped%20with%20extensive%20open%0Aknowledge%20and%20semantic%20reasoning%20abilities%2C%20present%20a%20promising%20avenue%20to%0Abridge%20this%20information%20gap.%20However%2C%20employing%20LLMs%20for%20denoising%20in%0Asequential%20recommendation%20introduces%20notable%20challenges%3A%201%29%20Direct%20application%0Aof%20pretrained%20LLMs%20may%20not%20be%20competent%20for%20the%20denoising%20task%2C%20frequently%0Agenerating%20nonsensical%20responses%3B%202%29%20Even%20after%20fine-tuning%2C%20the%20reliability%20of%0ALLM%20outputs%20remains%20questionable%2C%20especially%20given%20the%20complexity%20of%20the%20task%0Aand%20th%20inherent%20hallucinatory%20issue%20of%20LLMs.%0A%20%20To%20tackle%20these%20challenges%2C%20we%20propose%20LLM4DSR%2C%20a%20tailored%20approach%20for%0Adenoising%20sequential%20recommendation%20using%20LLMs.%20We%20constructed%20a%0Aself-supervised%20fine-tuning%20task%20to%20activate%20LLMs%27%20capabilities%20to%20identify%0Anoisy%20items%20and%20suggest%20replacements.%20Furthermore%2C%20we%20developed%20an%20uncertainty%0Aestimation%20module%20that%20ensures%20only%20high-confidence%20responses%20are%20utilized%20for%0Asequence%20corrections.%20Remarkably%2C%20LLM4DSR%20is%20model-agnostic%2C%20allowing%20the%0Acorrected%20sequences%20to%20be%20flexibly%20applied%20across%20various%20recommendation%0Amodels.%20Extensive%20experiments%20validate%20the%20superiority%20of%20LLM4DSR%20over%20existing%0Amethods%20across%20three%20datasets%20and%20three%20recommendation%20backbones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM4DSR%253A%2520Leveraing%2520Large%2520Language%2520Model%2520for%2520Denoising%2520Sequential%250A%2520%2520Recommendation%26entry.906535625%3DBohao%2520Wang%2520and%2520Feng%2520Liu%2520and%2520Jiawei%2520Chen%2520and%2520Yudi%2520Wu%2520and%2520Xingyu%2520Lou%2520and%2520Jun%2520Wang%2520and%2520Yan%2520Feng%2520and%2520Chun%2520Chen%2520and%2520Can%2520Wang%26entry.1292438233%3D%2520%2520Sequential%2520recommendation%2520systems%2520fundamentally%2520rely%2520on%2520users%2527%2520historical%250Ainteraction%2520sequences%252C%2520which%2520are%2520often%2520contaminated%2520by%2520noisy%2520interactions.%250AIdentifying%2520these%2520noisy%2520interactions%2520accurately%2520without%2520additional%2520information%250Ais%2520particularly%2520difficult%2520due%2520to%2520the%2520lack%2520of%2520explicit%2520supervisory%2520signals%2520to%250Adenote%2520noise.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520equipped%2520with%2520extensive%2520open%250Aknowledge%2520and%2520semantic%2520reasoning%2520abilities%252C%2520present%2520a%2520promising%2520avenue%2520to%250Abridge%2520this%2520information%2520gap.%2520However%252C%2520employing%2520LLMs%2520for%2520denoising%2520in%250Asequential%2520recommendation%2520introduces%2520notable%2520challenges%253A%25201%2529%2520Direct%2520application%250Aof%2520pretrained%2520LLMs%2520may%2520not%2520be%2520competent%2520for%2520the%2520denoising%2520task%252C%2520frequently%250Agenerating%2520nonsensical%2520responses%253B%25202%2529%2520Even%2520after%2520fine-tuning%252C%2520the%2520reliability%2520of%250ALLM%2520outputs%2520remains%2520questionable%252C%2520especially%2520given%2520the%2520complexity%2520of%2520the%2520task%250Aand%2520th%2520inherent%2520hallucinatory%2520issue%2520of%2520LLMs.%250A%2520%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520LLM4DSR%252C%2520a%2520tailored%2520approach%2520for%250Adenoising%2520sequential%2520recommendation%2520using%2520LLMs.%2520We%2520constructed%2520a%250Aself-supervised%2520fine-tuning%2520task%2520to%2520activate%2520LLMs%2527%2520capabilities%2520to%2520identify%250Anoisy%2520items%2520and%2520suggest%2520replacements.%2520Furthermore%252C%2520we%2520developed%2520an%2520uncertainty%250Aestimation%2520module%2520that%2520ensures%2520only%2520high-confidence%2520responses%2520are%2520utilized%2520for%250Asequence%2520corrections.%2520Remarkably%252C%2520LLM4DSR%2520is%2520model-agnostic%252C%2520allowing%2520the%250Acorrected%2520sequences%2520to%2520be%2520flexibly%2520applied%2520across%2520various%2520recommendation%250Amodels.%2520Extensive%2520experiments%2520validate%2520the%2520superiority%2520of%2520LLM4DSR%2520over%2520existing%250Amethods%2520across%2520three%2520datasets%2520and%2520three%2520recommendation%2520backbones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM4DSR%3A%20Leveraing%20Large%20Language%20Model%20for%20Denoising%20Sequential%0A%20%20Recommendation&entry.906535625=Bohao%20Wang%20and%20Feng%20Liu%20and%20Jiawei%20Chen%20and%20Yudi%20Wu%20and%20Xingyu%20Lou%20and%20Jun%20Wang%20and%20Yan%20Feng%20and%20Chun%20Chen%20and%20Can%20Wang&entry.1292438233=%20%20Sequential%20recommendation%20systems%20fundamentally%20rely%20on%20users%27%20historical%0Ainteraction%20sequences%2C%20which%20are%20often%20contaminated%20by%20noisy%20interactions.%0AIdentifying%20these%20noisy%20interactions%20accurately%20without%20additional%20information%0Ais%20particularly%20difficult%20due%20to%20the%20lack%20of%20explicit%20supervisory%20signals%20to%0Adenote%20noise.%20Large%20Language%20Models%20%28LLMs%29%2C%20equipped%20with%20extensive%20open%0Aknowledge%20and%20semantic%20reasoning%20abilities%2C%20present%20a%20promising%20avenue%20to%0Abridge%20this%20information%20gap.%20However%2C%20employing%20LLMs%20for%20denoising%20in%0Asequential%20recommendation%20introduces%20notable%20challenges%3A%201%29%20Direct%20application%0Aof%20pretrained%20LLMs%20may%20not%20be%20competent%20for%20the%20denoising%20task%2C%20frequently%0Agenerating%20nonsensical%20responses%3B%202%29%20Even%20after%20fine-tuning%2C%20the%20reliability%20of%0ALLM%20outputs%20remains%20questionable%2C%20especially%20given%20the%20complexity%20of%20the%20task%0Aand%20th%20inherent%20hallucinatory%20issue%20of%20LLMs.%0A%20%20To%20tackle%20these%20challenges%2C%20we%20propose%20LLM4DSR%2C%20a%20tailored%20approach%20for%0Adenoising%20sequential%20recommendation%20using%20LLMs.%20We%20constructed%20a%0Aself-supervised%20fine-tuning%20task%20to%20activate%20LLMs%27%20capabilities%20to%20identify%0Anoisy%20items%20and%20suggest%20replacements.%20Furthermore%2C%20we%20developed%20an%20uncertainty%0Aestimation%20module%20that%20ensures%20only%20high-confidence%20responses%20are%20utilized%20for%0Asequence%20corrections.%20Remarkably%2C%20LLM4DSR%20is%20model-agnostic%2C%20allowing%20the%0Acorrected%20sequences%20to%20be%20flexibly%20applied%20across%20various%20recommendation%0Amodels.%20Extensive%20experiments%20validate%20the%20superiority%20of%20LLM4DSR%20over%20existing%0Amethods%20across%20three%20datasets%20and%20three%20recommendation%20backbones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08208v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


