<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="#"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D\n  Reconstruction from Unposed Sparse Views", "author": "Songchun Zhang and Chunhui Zhao", "abstract": "  Inferring 3D structures from sparse, unposed observations is challenging due\nto its unconstrained nature. Recent methods propose to predict implicit\nrepresentations directly from unposed inputs in a data-driven manner, achieving\npromising results. However, these methods do not utilize geometric priors and\ncannot hallucinate the appearance of unseen regions, thus making it challenging\nto reconstruct fine geometric and textural details. To tackle this challenge,\nour key idea is to reformulate this ill-posed problem as conditional novel view\nsynthesis, aiming to generate complete observations from limited input views to\nfacilitate reconstruction. With complete observations, the poses of the input\nviews can be easily recovered and further used to optimize the reconstructed\nobject. To this end, we propose a novel pipeline Pragmatist. First, we generate\na complete observation of the object via a multiview conditional diffusion\nmodel. Then, we use a feed-forward large reconstruction model to obtain the\nreconstructed mesh. To further improve the reconstruction quality, we recover\nthe poses of input views by inverting the obtained 3D representations and\nfurther optimize the texture using detailed input views. Unlike previous\napproaches, our pipeline improves reconstruction by efficiently leveraging\nunposed inputs and generative priors, circumventing the direct resolution of\nhighly ill-posed problems. Extensive experiments show that our approach\nachieves promising performance in several benchmarks.\n", "link": "http://arxiv.org/abs/2412.08412v1", "date": "2024-12-11", "relevancy": 3.4708, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7256}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7256}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pragmatist%3A%20Multiview%20Conditional%20Diffusion%20Models%20for%20High-Fidelity%203D%0A%20%20Reconstruction%20from%20Unposed%20Sparse%20Views&body=Title%3A%20Pragmatist%3A%20Multiview%20Conditional%20Diffusion%20Models%20for%20High-Fidelity%203D%0A%20%20Reconstruction%20from%20Unposed%20Sparse%20Views%0AAuthor%3A%20Songchun%20Zhang%20and%20Chunhui%20Zhao%0AAbstract%3A%20%20%20Inferring%203D%20structures%20from%20sparse%2C%20unposed%20observations%20is%20challenging%20due%0Ato%20its%20unconstrained%20nature.%20Recent%20methods%20propose%20to%20predict%20implicit%0Arepresentations%20directly%20from%20unposed%20inputs%20in%20a%20data-driven%20manner%2C%20achieving%0Apromising%20results.%20However%2C%20these%20methods%20do%20not%20utilize%20geometric%20priors%20and%0Acannot%20hallucinate%20the%20appearance%20of%20unseen%20regions%2C%20thus%20making%20it%20challenging%0Ato%20reconstruct%20fine%20geometric%20and%20textural%20details.%20To%20tackle%20this%20challenge%2C%0Aour%20key%20idea%20is%20to%20reformulate%20this%20ill-posed%20problem%20as%20conditional%20novel%20view%0Asynthesis%2C%20aiming%20to%20generate%20complete%20observations%20from%20limited%20input%20views%20to%0Afacilitate%20reconstruction.%20With%20complete%20observations%2C%20the%20poses%20of%20the%20input%0Aviews%20can%20be%20easily%20recovered%20and%20further%20used%20to%20optimize%20the%20reconstructed%0Aobject.%20To%20this%20end%2C%20we%20propose%20a%20novel%20pipeline%20Pragmatist.%20First%2C%20we%20generate%0Aa%20complete%20observation%20of%20the%20object%20via%20a%20multiview%20conditional%20diffusion%0Amodel.%20Then%2C%20we%20use%20a%20feed-forward%20large%20reconstruction%20model%20to%20obtain%20the%0Areconstructed%20mesh.%20To%20further%20improve%20the%20reconstruction%20quality%2C%20we%20recover%0Athe%20poses%20of%20input%20views%20by%20inverting%20the%20obtained%203D%20representations%20and%0Afurther%20optimize%20the%20texture%20using%20detailed%20input%20views.%20Unlike%20previous%0Aapproaches%2C%20our%20pipeline%20improves%20reconstruction%20by%20efficiently%20leveraging%0Aunposed%20inputs%20and%20generative%20priors%2C%20circumventing%20the%20direct%20resolution%20of%0Ahighly%20ill-posed%20problems.%20Extensive%20experiments%20show%20that%20our%20approach%0Aachieves%20promising%20performance%20in%20several%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPragmatist%253A%2520Multiview%2520Conditional%2520Diffusion%2520Models%2520for%2520High-Fidelity%25203D%250A%2520%2520Reconstruction%2520from%2520Unposed%2520Sparse%2520Views%26entry.906535625%3DSongchun%2520Zhang%2520and%2520Chunhui%2520Zhao%26entry.1292438233%3D%2520%2520Inferring%25203D%2520structures%2520from%2520sparse%252C%2520unposed%2520observations%2520is%2520challenging%2520due%250Ato%2520its%2520unconstrained%2520nature.%2520Recent%2520methods%2520propose%2520to%2520predict%2520implicit%250Arepresentations%2520directly%2520from%2520unposed%2520inputs%2520in%2520a%2520data-driven%2520manner%252C%2520achieving%250Apromising%2520results.%2520However%252C%2520these%2520methods%2520do%2520not%2520utilize%2520geometric%2520priors%2520and%250Acannot%2520hallucinate%2520the%2520appearance%2520of%2520unseen%2520regions%252C%2520thus%2520making%2520it%2520challenging%250Ato%2520reconstruct%2520fine%2520geometric%2520and%2520textural%2520details.%2520To%2520tackle%2520this%2520challenge%252C%250Aour%2520key%2520idea%2520is%2520to%2520reformulate%2520this%2520ill-posed%2520problem%2520as%2520conditional%2520novel%2520view%250Asynthesis%252C%2520aiming%2520to%2520generate%2520complete%2520observations%2520from%2520limited%2520input%2520views%2520to%250Afacilitate%2520reconstruction.%2520With%2520complete%2520observations%252C%2520the%2520poses%2520of%2520the%2520input%250Aviews%2520can%2520be%2520easily%2520recovered%2520and%2520further%2520used%2520to%2520optimize%2520the%2520reconstructed%250Aobject.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520pipeline%2520Pragmatist.%2520First%252C%2520we%2520generate%250Aa%2520complete%2520observation%2520of%2520the%2520object%2520via%2520a%2520multiview%2520conditional%2520diffusion%250Amodel.%2520Then%252C%2520we%2520use%2520a%2520feed-forward%2520large%2520reconstruction%2520model%2520to%2520obtain%2520the%250Areconstructed%2520mesh.%2520To%2520further%2520improve%2520the%2520reconstruction%2520quality%252C%2520we%2520recover%250Athe%2520poses%2520of%2520input%2520views%2520by%2520inverting%2520the%2520obtained%25203D%2520representations%2520and%250Afurther%2520optimize%2520the%2520texture%2520using%2520detailed%2520input%2520views.%2520Unlike%2520previous%250Aapproaches%252C%2520our%2520pipeline%2520improves%2520reconstruction%2520by%2520efficiently%2520leveraging%250Aunposed%2520inputs%2520and%2520generative%2520priors%252C%2520circumventing%2520the%2520direct%2520resolution%2520of%250Ahighly%2520ill-posed%2520problems.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%250Aachieves%2520promising%2520performance%2520in%2520several%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pragmatist%3A%20Multiview%20Conditional%20Diffusion%20Models%20for%20High-Fidelity%203D%0A%20%20Reconstruction%20from%20Unposed%20Sparse%20Views&entry.906535625=Songchun%20Zhang%20and%20Chunhui%20Zhao&entry.1292438233=%20%20Inferring%203D%20structures%20from%20sparse%2C%20unposed%20observations%20is%20challenging%20due%0Ato%20its%20unconstrained%20nature.%20Recent%20methods%20propose%20to%20predict%20implicit%0Arepresentations%20directly%20from%20unposed%20inputs%20in%20a%20data-driven%20manner%2C%20achieving%0Apromising%20results.%20However%2C%20these%20methods%20do%20not%20utilize%20geometric%20priors%20and%0Acannot%20hallucinate%20the%20appearance%20of%20unseen%20regions%2C%20thus%20making%20it%20challenging%0Ato%20reconstruct%20fine%20geometric%20and%20textural%20details.%20To%20tackle%20this%20challenge%2C%0Aour%20key%20idea%20is%20to%20reformulate%20this%20ill-posed%20problem%20as%20conditional%20novel%20view%0Asynthesis%2C%20aiming%20to%20generate%20complete%20observations%20from%20limited%20input%20views%20to%0Afacilitate%20reconstruction.%20With%20complete%20observations%2C%20the%20poses%20of%20the%20input%0Aviews%20can%20be%20easily%20recovered%20and%20further%20used%20to%20optimize%20the%20reconstructed%0Aobject.%20To%20this%20end%2C%20we%20propose%20a%20novel%20pipeline%20Pragmatist.%20First%2C%20we%20generate%0Aa%20complete%20observation%20of%20the%20object%20via%20a%20multiview%20conditional%20diffusion%0Amodel.%20Then%2C%20we%20use%20a%20feed-forward%20large%20reconstruction%20model%20to%20obtain%20the%0Areconstructed%20mesh.%20To%20further%20improve%20the%20reconstruction%20quality%2C%20we%20recover%0Athe%20poses%20of%20input%20views%20by%20inverting%20the%20obtained%203D%20representations%20and%0Afurther%20optimize%20the%20texture%20using%20detailed%20input%20views.%20Unlike%20previous%0Aapproaches%2C%20our%20pipeline%20improves%20reconstruction%20by%20efficiently%20leveraging%0Aunposed%20inputs%20and%20generative%20priors%2C%20circumventing%20the%20direct%20resolution%20of%0Ahighly%20ill-posed%20problems.%20Extensive%20experiments%20show%20that%20our%20approach%0Aachieves%20promising%20performance%20in%20several%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08412v1&entry.124074799=Read"},
{"title": "LineGS : 3D Line Segment Representation on 3D Gaussian Splatting", "author": "Chenggang Yang and Yuang Shi and Wei Tsang Ooi", "abstract": "  Abstract representations of 3D scenes play a crucial role in computer vision,\nenabling a wide range of applications such as mapping, localization, surface\nreconstruction, and even advanced tasks like SLAM and rendering. Among these\nrepresentations, line segments are widely used because of their ability to\nsuccinctly capture the structural features of a scene. However, existing 3D\nreconstruction methods often face significant challenges. Methods relying on 2D\nprojections suffer from instability caused by errors in multi-view matching and\nocclusions, while direct 3D approaches are hampered by noise and sparsity in 3D\npoint cloud data. This paper introduces LineGS, a novel method that combines\ngeometry-guided 3D line reconstruction with a 3D Gaussian splatting model to\naddress these challenges and improve representation ability. The method\nleverages the high-density Gaussian point distributions along the edge of the\nscene to refine and optimize initial line segments generated from traditional\ngeometric approaches. By aligning these segments with the underlying geometric\nfeatures of the scene, LineGS achieves a more precise and reliable\nrepresentation of 3D structures. The results show significant improvements in\nboth geometric accuracy and model compactness compared to baseline methods.\n", "link": "http://arxiv.org/abs/2412.00477v2", "date": "2024-12-11", "relevancy": 3.4478, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7237}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7045}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LineGS%20%3A%203D%20Line%20Segment%20Representation%20on%203D%20Gaussian%20Splatting&body=Title%3A%20LineGS%20%3A%203D%20Line%20Segment%20Representation%20on%203D%20Gaussian%20Splatting%0AAuthor%3A%20Chenggang%20Yang%20and%20Yuang%20Shi%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%20Abstract%20representations%20of%203D%20scenes%20play%20a%20crucial%20role%20in%20computer%20vision%2C%0Aenabling%20a%20wide%20range%20of%20applications%20such%20as%20mapping%2C%20localization%2C%20surface%0Areconstruction%2C%20and%20even%20advanced%20tasks%20like%20SLAM%20and%20rendering.%20Among%20these%0Arepresentations%2C%20line%20segments%20are%20widely%20used%20because%20of%20their%20ability%20to%0Asuccinctly%20capture%20the%20structural%20features%20of%20a%20scene.%20However%2C%20existing%203D%0Areconstruction%20methods%20often%20face%20significant%20challenges.%20Methods%20relying%20on%202D%0Aprojections%20suffer%20from%20instability%20caused%20by%20errors%20in%20multi-view%20matching%20and%0Aocclusions%2C%20while%20direct%203D%20approaches%20are%20hampered%20by%20noise%20and%20sparsity%20in%203D%0Apoint%20cloud%20data.%20This%20paper%20introduces%20LineGS%2C%20a%20novel%20method%20that%20combines%0Ageometry-guided%203D%20line%20reconstruction%20with%20a%203D%20Gaussian%20splatting%20model%20to%0Aaddress%20these%20challenges%20and%20improve%20representation%20ability.%20The%20method%0Aleverages%20the%20high-density%20Gaussian%20point%20distributions%20along%20the%20edge%20of%20the%0Ascene%20to%20refine%20and%20optimize%20initial%20line%20segments%20generated%20from%20traditional%0Ageometric%20approaches.%20By%20aligning%20these%20segments%20with%20the%20underlying%20geometric%0Afeatures%20of%20the%20scene%2C%20LineGS%20achieves%20a%20more%20precise%20and%20reliable%0Arepresentation%20of%203D%20structures.%20The%20results%20show%20significant%20improvements%20in%0Aboth%20geometric%20accuracy%20and%20model%20compactness%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLineGS%2520%253A%25203D%2520Line%2520Segment%2520Representation%2520on%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DChenggang%2520Yang%2520and%2520Yuang%2520Shi%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%2520Abstract%2520representations%2520of%25203D%2520scenes%2520play%2520a%2520crucial%2520role%2520in%2520computer%2520vision%252C%250Aenabling%2520a%2520wide%2520range%2520of%2520applications%2520such%2520as%2520mapping%252C%2520localization%252C%2520surface%250Areconstruction%252C%2520and%2520even%2520advanced%2520tasks%2520like%2520SLAM%2520and%2520rendering.%2520Among%2520these%250Arepresentations%252C%2520line%2520segments%2520are%2520widely%2520used%2520because%2520of%2520their%2520ability%2520to%250Asuccinctly%2520capture%2520the%2520structural%2520features%2520of%2520a%2520scene.%2520However%252C%2520existing%25203D%250Areconstruction%2520methods%2520often%2520face%2520significant%2520challenges.%2520Methods%2520relying%2520on%25202D%250Aprojections%2520suffer%2520from%2520instability%2520caused%2520by%2520errors%2520in%2520multi-view%2520matching%2520and%250Aocclusions%252C%2520while%2520direct%25203D%2520approaches%2520are%2520hampered%2520by%2520noise%2520and%2520sparsity%2520in%25203D%250Apoint%2520cloud%2520data.%2520This%2520paper%2520introduces%2520LineGS%252C%2520a%2520novel%2520method%2520that%2520combines%250Ageometry-guided%25203D%2520line%2520reconstruction%2520with%2520a%25203D%2520Gaussian%2520splatting%2520model%2520to%250Aaddress%2520these%2520challenges%2520and%2520improve%2520representation%2520ability.%2520The%2520method%250Aleverages%2520the%2520high-density%2520Gaussian%2520point%2520distributions%2520along%2520the%2520edge%2520of%2520the%250Ascene%2520to%2520refine%2520and%2520optimize%2520initial%2520line%2520segments%2520generated%2520from%2520traditional%250Ageometric%2520approaches.%2520By%2520aligning%2520these%2520segments%2520with%2520the%2520underlying%2520geometric%250Afeatures%2520of%2520the%2520scene%252C%2520LineGS%2520achieves%2520a%2520more%2520precise%2520and%2520reliable%250Arepresentation%2520of%25203D%2520structures.%2520The%2520results%2520show%2520significant%2520improvements%2520in%250Aboth%2520geometric%2520accuracy%2520and%2520model%2520compactness%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LineGS%20%3A%203D%20Line%20Segment%20Representation%20on%203D%20Gaussian%20Splatting&entry.906535625=Chenggang%20Yang%20and%20Yuang%20Shi%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%20Abstract%20representations%20of%203D%20scenes%20play%20a%20crucial%20role%20in%20computer%20vision%2C%0Aenabling%20a%20wide%20range%20of%20applications%20such%20as%20mapping%2C%20localization%2C%20surface%0Areconstruction%2C%20and%20even%20advanced%20tasks%20like%20SLAM%20and%20rendering.%20Among%20these%0Arepresentations%2C%20line%20segments%20are%20widely%20used%20because%20of%20their%20ability%20to%0Asuccinctly%20capture%20the%20structural%20features%20of%20a%20scene.%20However%2C%20existing%203D%0Areconstruction%20methods%20often%20face%20significant%20challenges.%20Methods%20relying%20on%202D%0Aprojections%20suffer%20from%20instability%20caused%20by%20errors%20in%20multi-view%20matching%20and%0Aocclusions%2C%20while%20direct%203D%20approaches%20are%20hampered%20by%20noise%20and%20sparsity%20in%203D%0Apoint%20cloud%20data.%20This%20paper%20introduces%20LineGS%2C%20a%20novel%20method%20that%20combines%0Ageometry-guided%203D%20line%20reconstruction%20with%20a%203D%20Gaussian%20splatting%20model%20to%0Aaddress%20these%20challenges%20and%20improve%20representation%20ability.%20The%20method%0Aleverages%20the%20high-density%20Gaussian%20point%20distributions%20along%20the%20edge%20of%20the%0Ascene%20to%20refine%20and%20optimize%20initial%20line%20segments%20generated%20from%20traditional%0Ageometric%20approaches.%20By%20aligning%20these%20segments%20with%20the%20underlying%20geometric%0Afeatures%20of%20the%20scene%2C%20LineGS%20achieves%20a%20more%20precise%20and%20reliable%0Arepresentation%20of%203D%20structures.%20The%20results%20show%20significant%20improvements%20in%0Aboth%20geometric%20accuracy%20and%20model%20compactness%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00477v2&entry.124074799=Read"},
{"title": "SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors", "author": "Yijia Guo and Liwen Hu and Yuanxi Bai and Lei Ma and Tiejun Huang", "abstract": "  3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance\nin 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.\nFulfilling this requirement can be challenging in real-world scenarios\nespecially when the camera moves fast, which severely limits the application of\n3DGS. To address these challenges, we proposed Spike Gausian Splatting\n(SpikeGS), the first framework that integrates the spike streams into 3DGS\npipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With\naccumulation rasterization, interval supervision, and a specially designed\npipeline, SpikeGS extracts detailed geometry and texture from high temporal\nresolution but texture lacking spike stream, reconstructs 3D scenes captured in\n1 second. Extensive experiments on multiple synthetic and real-world datasets\ndemonstrate the superiority of SpikeGS compared with existing spike-based and\ndeblur 3D scene reconstruction methods. Codes and data will be released soon.\n", "link": "http://arxiv.org/abs/2407.03771v3", "date": "2024-12-11", "relevancy": 3.4071, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7261}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6646}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeGS%3A%20Reconstruct%203D%20scene%20via%20fast-moving%20bio-inspired%20sensors&body=Title%3A%20SpikeGS%3A%20Reconstruct%203D%20scene%20via%20fast-moving%20bio-inspired%20sensors%0AAuthor%3A%20Yijia%20Guo%20and%20Liwen%20Hu%20and%20Yuanxi%20Bai%20and%20Lei%20Ma%20and%20Tiejun%20Huang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20demonstrates%20unparalleled%20superior%20performance%0Ain%203D%20scene%20reconstruction.%20However%2C%203DGS%20heavily%20relies%20on%20the%20sharp%20images.%0AFulfilling%20this%20requirement%20can%20be%20challenging%20in%20real-world%20scenarios%0Aespecially%20when%20the%20camera%20moves%20fast%2C%20which%20severely%20limits%20the%20application%20of%0A3DGS.%20To%20address%20these%20challenges%2C%20we%20proposed%20Spike%20Gausian%20Splatting%0A%28SpikeGS%29%2C%20the%20first%20framework%20that%20integrates%20the%20spike%20streams%20into%203DGS%0Apipeline%20to%20reconstruct%203D%20scenes%20via%20a%20fast-moving%20bio-inspired%20camera.%20With%0Aaccumulation%20rasterization%2C%20interval%20supervision%2C%20and%20a%20specially%20designed%0Apipeline%2C%20SpikeGS%20extracts%20detailed%20geometry%20and%20texture%20from%20high%20temporal%0Aresolution%20but%20texture%20lacking%20spike%20stream%2C%20reconstructs%203D%20scenes%20captured%20in%0A1%20second.%20Extensive%20experiments%20on%20multiple%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20the%20superiority%20of%20SpikeGS%20compared%20with%20existing%20spike-based%20and%0Adeblur%203D%20scene%20reconstruction%20methods.%20Codes%20and%20data%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03771v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeGS%253A%2520Reconstruct%25203D%2520scene%2520via%2520fast-moving%2520bio-inspired%2520sensors%26entry.906535625%3DYijia%2520Guo%2520and%2520Liwen%2520Hu%2520and%2520Yuanxi%2520Bai%2520and%2520Lei%2520Ma%2520and%2520Tiejun%2520Huang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520demonstrates%2520unparalleled%2520superior%2520performance%250Ain%25203D%2520scene%2520reconstruction.%2520However%252C%25203DGS%2520heavily%2520relies%2520on%2520the%2520sharp%2520images.%250AFulfilling%2520this%2520requirement%2520can%2520be%2520challenging%2520in%2520real-world%2520scenarios%250Aespecially%2520when%2520the%2520camera%2520moves%2520fast%252C%2520which%2520severely%2520limits%2520the%2520application%2520of%250A3DGS.%2520To%2520address%2520these%2520challenges%252C%2520we%2520proposed%2520Spike%2520Gausian%2520Splatting%250A%2528SpikeGS%2529%252C%2520the%2520first%2520framework%2520that%2520integrates%2520the%2520spike%2520streams%2520into%25203DGS%250Apipeline%2520to%2520reconstruct%25203D%2520scenes%2520via%2520a%2520fast-moving%2520bio-inspired%2520camera.%2520With%250Aaccumulation%2520rasterization%252C%2520interval%2520supervision%252C%2520and%2520a%2520specially%2520designed%250Apipeline%252C%2520SpikeGS%2520extracts%2520detailed%2520geometry%2520and%2520texture%2520from%2520high%2520temporal%250Aresolution%2520but%2520texture%2520lacking%2520spike%2520stream%252C%2520reconstructs%25203D%2520scenes%2520captured%2520in%250A1%2520second.%2520Extensive%2520experiments%2520on%2520multiple%2520synthetic%2520and%2520real-world%2520datasets%250Ademonstrate%2520the%2520superiority%2520of%2520SpikeGS%2520compared%2520with%2520existing%2520spike-based%2520and%250Adeblur%25203D%2520scene%2520reconstruction%2520methods.%2520Codes%2520and%2520data%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03771v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeGS%3A%20Reconstruct%203D%20scene%20via%20fast-moving%20bio-inspired%20sensors&entry.906535625=Yijia%20Guo%20and%20Liwen%20Hu%20and%20Yuanxi%20Bai%20and%20Lei%20Ma%20and%20Tiejun%20Huang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20demonstrates%20unparalleled%20superior%20performance%0Ain%203D%20scene%20reconstruction.%20However%2C%203DGS%20heavily%20relies%20on%20the%20sharp%20images.%0AFulfilling%20this%20requirement%20can%20be%20challenging%20in%20real-world%20scenarios%0Aespecially%20when%20the%20camera%20moves%20fast%2C%20which%20severely%20limits%20the%20application%20of%0A3DGS.%20To%20address%20these%20challenges%2C%20we%20proposed%20Spike%20Gausian%20Splatting%0A%28SpikeGS%29%2C%20the%20first%20framework%20that%20integrates%20the%20spike%20streams%20into%203DGS%0Apipeline%20to%20reconstruct%203D%20scenes%20via%20a%20fast-moving%20bio-inspired%20camera.%20With%0Aaccumulation%20rasterization%2C%20interval%20supervision%2C%20and%20a%20specially%20designed%0Apipeline%2C%20SpikeGS%20extracts%20detailed%20geometry%20and%20texture%20from%20high%20temporal%0Aresolution%20but%20texture%20lacking%20spike%20stream%2C%20reconstructs%203D%20scenes%20captured%20in%0A1%20second.%20Extensive%20experiments%20on%20multiple%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20the%20superiority%20of%20SpikeGS%20compared%20with%20existing%20spike-based%20and%0Adeblur%203D%20scene%20reconstruction%20methods.%20Codes%20and%20data%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03771v3&entry.124074799=Read"},
{"title": "SLGaussian: Fast Language Gaussian Splatting in Sparse Views", "author": "Kangjie Chen and BingQuan Dai and Minghan Qin and Dongbin Zhang and Peihao Li and Yingshuang Zou and Haoqian Wang", "abstract": "  3D semantic field learning is crucial for applications like autonomous\nnavigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from\nlimited viewpoints is essential. Existing methods struggle under sparse view\nconditions, relying on inefficient per-scene multi-view optimizations, which\nare impractical for many real-world tasks. To address this, we propose\nSLGaussian, a feed-forward method for constructing 3D semantic fields from\nsparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring\nconsistent SAM segmentations through video tracking and using low-dimensional\nindexing for high-dimensional CLIP features, SLGaussian efficiently embeds\nlanguage information in 3D space, offering a robust solution for accurate 3D\nscene understanding under sparse view conditions. In experiments on two-view\nsparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,\nSLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,\nand mIoU. Moreover, our model achieves scene inference in under 30 seconds and\nopen-vocabulary querying in just 0.011 seconds per query.\n", "link": "http://arxiv.org/abs/2412.08331v1", "date": "2024-12-11", "relevancy": 3.3714, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7255}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.669}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLGaussian%3A%20Fast%20Language%20Gaussian%20Splatting%20in%20Sparse%20Views&body=Title%3A%20SLGaussian%3A%20Fast%20Language%20Gaussian%20Splatting%20in%20Sparse%20Views%0AAuthor%3A%20Kangjie%20Chen%20and%20BingQuan%20Dai%20and%20Minghan%20Qin%20and%20Dongbin%20Zhang%20and%20Peihao%20Li%20and%20Yingshuang%20Zou%20and%20Haoqian%20Wang%0AAbstract%3A%20%20%203D%20semantic%20field%20learning%20is%20crucial%20for%20applications%20like%20autonomous%0Anavigation%2C%20AR/VR%2C%20and%20robotics%2C%20where%20accurate%20comprehension%20of%203D%20scenes%20from%0Alimited%20viewpoints%20is%20essential.%20Existing%20methods%20struggle%20under%20sparse%20view%0Aconditions%2C%20relying%20on%20inefficient%20per-scene%20multi-view%20optimizations%2C%20which%0Aare%20impractical%20for%20many%20real-world%20tasks.%20To%20address%20this%2C%20we%20propose%0ASLGaussian%2C%20a%20feed-forward%20method%20for%20constructing%203D%20semantic%20fields%20from%0Asparse%20viewpoints%2C%20allowing%20direct%20inference%20of%203DGS-based%20scenes.%20By%20ensuring%0Aconsistent%20SAM%20segmentations%20through%20video%20tracking%20and%20using%20low-dimensional%0Aindexing%20for%20high-dimensional%20CLIP%20features%2C%20SLGaussian%20efficiently%20embeds%0Alanguage%20information%20in%203D%20space%2C%20offering%20a%20robust%20solution%20for%20accurate%203D%0Ascene%20understanding%20under%20sparse%20view%20conditions.%20In%20experiments%20on%20two-view%0Asparse%203D%20object%20querying%20and%20segmentation%20in%20the%20LERF%20and%203D-OVS%20datasets%2C%0ASLGaussian%20outperforms%20existing%20methods%20in%20chosen%20IoU%2C%20Localization%20Accuracy%2C%0Aand%20mIoU.%20Moreover%2C%20our%20model%20achieves%20scene%20inference%20in%20under%2030%20seconds%20and%0Aopen-vocabulary%20querying%20in%20just%200.011%20seconds%20per%20query.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLGaussian%253A%2520Fast%2520Language%2520Gaussian%2520Splatting%2520in%2520Sparse%2520Views%26entry.906535625%3DKangjie%2520Chen%2520and%2520BingQuan%2520Dai%2520and%2520Minghan%2520Qin%2520and%2520Dongbin%2520Zhang%2520and%2520Peihao%2520Li%2520and%2520Yingshuang%2520Zou%2520and%2520Haoqian%2520Wang%26entry.1292438233%3D%2520%25203D%2520semantic%2520field%2520learning%2520is%2520crucial%2520for%2520applications%2520like%2520autonomous%250Anavigation%252C%2520AR/VR%252C%2520and%2520robotics%252C%2520where%2520accurate%2520comprehension%2520of%25203D%2520scenes%2520from%250Alimited%2520viewpoints%2520is%2520essential.%2520Existing%2520methods%2520struggle%2520under%2520sparse%2520view%250Aconditions%252C%2520relying%2520on%2520inefficient%2520per-scene%2520multi-view%2520optimizations%252C%2520which%250Aare%2520impractical%2520for%2520many%2520real-world%2520tasks.%2520To%2520address%2520this%252C%2520we%2520propose%250ASLGaussian%252C%2520a%2520feed-forward%2520method%2520for%2520constructing%25203D%2520semantic%2520fields%2520from%250Asparse%2520viewpoints%252C%2520allowing%2520direct%2520inference%2520of%25203DGS-based%2520scenes.%2520By%2520ensuring%250Aconsistent%2520SAM%2520segmentations%2520through%2520video%2520tracking%2520and%2520using%2520low-dimensional%250Aindexing%2520for%2520high-dimensional%2520CLIP%2520features%252C%2520SLGaussian%2520efficiently%2520embeds%250Alanguage%2520information%2520in%25203D%2520space%252C%2520offering%2520a%2520robust%2520solution%2520for%2520accurate%25203D%250Ascene%2520understanding%2520under%2520sparse%2520view%2520conditions.%2520In%2520experiments%2520on%2520two-view%250Asparse%25203D%2520object%2520querying%2520and%2520segmentation%2520in%2520the%2520LERF%2520and%25203D-OVS%2520datasets%252C%250ASLGaussian%2520outperforms%2520existing%2520methods%2520in%2520chosen%2520IoU%252C%2520Localization%2520Accuracy%252C%250Aand%2520mIoU.%2520Moreover%252C%2520our%2520model%2520achieves%2520scene%2520inference%2520in%2520under%252030%2520seconds%2520and%250Aopen-vocabulary%2520querying%2520in%2520just%25200.011%2520seconds%2520per%2520query.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLGaussian%3A%20Fast%20Language%20Gaussian%20Splatting%20in%20Sparse%20Views&entry.906535625=Kangjie%20Chen%20and%20BingQuan%20Dai%20and%20Minghan%20Qin%20and%20Dongbin%20Zhang%20and%20Peihao%20Li%20and%20Yingshuang%20Zou%20and%20Haoqian%20Wang&entry.1292438233=%20%203D%20semantic%20field%20learning%20is%20crucial%20for%20applications%20like%20autonomous%0Anavigation%2C%20AR/VR%2C%20and%20robotics%2C%20where%20accurate%20comprehension%20of%203D%20scenes%20from%0Alimited%20viewpoints%20is%20essential.%20Existing%20methods%20struggle%20under%20sparse%20view%0Aconditions%2C%20relying%20on%20inefficient%20per-scene%20multi-view%20optimizations%2C%20which%0Aare%20impractical%20for%20many%20real-world%20tasks.%20To%20address%20this%2C%20we%20propose%0ASLGaussian%2C%20a%20feed-forward%20method%20for%20constructing%203D%20semantic%20fields%20from%0Asparse%20viewpoints%2C%20allowing%20direct%20inference%20of%203DGS-based%20scenes.%20By%20ensuring%0Aconsistent%20SAM%20segmentations%20through%20video%20tracking%20and%20using%20low-dimensional%0Aindexing%20for%20high-dimensional%20CLIP%20features%2C%20SLGaussian%20efficiently%20embeds%0Alanguage%20information%20in%203D%20space%2C%20offering%20a%20robust%20solution%20for%20accurate%203D%0Ascene%20understanding%20under%20sparse%20view%20conditions.%20In%20experiments%20on%20two-view%0Asparse%203D%20object%20querying%20and%20segmentation%20in%20the%20LERF%20and%203D-OVS%20datasets%2C%0ASLGaussian%20outperforms%20existing%20methods%20in%20chosen%20IoU%2C%20Localization%20Accuracy%2C%0Aand%20mIoU.%20Moreover%2C%20our%20model%20achieves%20scene%20inference%20in%20under%2030%20seconds%20and%0Aopen-vocabulary%20querying%20in%20just%200.011%20seconds%20per%20query.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08331v1&entry.124074799=Read"},
{"title": "SuperGS: Super-Resolution 3D Gaussian Splatting Enhanced by Variational\n  Residual Features and Uncertainty-Augmented Learning", "author": "Shiyun Xie and Zhiru Wang and Xu Wang and Yinghao Zhu and Chengwei Pan and Xiwang Dong", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis\n(NVS) with its real-time rendering capabilities and superior quality. However,\nit faces challenges for high-resolution novel view synthesis (HRNVS) due to the\ncoarse nature of primitives derived from low-resolution input views. To address\nthis issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion\nof 3DGS designed with a two-stage coarse-to-fine training framework. In this\nframework, we use a latent feature field to represent the low-resolution scene,\nserving as both the initialization and foundational information for\nsuper-resolution optimization. Additionally, we introduce variational residual\nfeatures to enhance high-resolution details, using their variance as\nuncertainty estimates to guide the densification process and loss computation.\nFurthermore, the introduction of a multi-view joint learning approach helps\nmitigate ambiguities caused by multi-view inconsistencies in the pseudo labels.\nExtensive experiments demonstrate that SuperGS surpasses state-of-the-art HRNVS\nmethods on both real-world and synthetic datasets using only low-resolution\ninputs. Code is available at https://github.com/SYXieee/SuperGS.\n", "link": "http://arxiv.org/abs/2410.02571v3", "date": "2024-12-11", "relevancy": 3.2739, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6857}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6612}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperGS%3A%20Super-Resolution%203D%20Gaussian%20Splatting%20Enhanced%20by%20Variational%0A%20%20Residual%20Features%20and%20Uncertainty-Augmented%20Learning&body=Title%3A%20SuperGS%3A%20Super-Resolution%203D%20Gaussian%20Splatting%20Enhanced%20by%20Variational%0A%20%20Residual%20Features%20and%20Uncertainty-Augmented%20Learning%0AAuthor%3A%20Shiyun%20Xie%20and%20Zhiru%20Wang%20and%20Xu%20Wang%20and%20Yinghao%20Zhu%20and%20Chengwei%20Pan%20and%20Xiwang%20Dong%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20exceled%20in%20novel%20view%20synthesis%0A%28NVS%29%20with%20its%20real-time%20rendering%20capabilities%20and%20superior%20quality.%20However%2C%0Ait%20faces%20challenges%20for%20high-resolution%20novel%20view%20synthesis%20%28HRNVS%29%20due%20to%20the%0Acoarse%20nature%20of%20primitives%20derived%20from%20low-resolution%20input%20views.%20To%20address%0Athis%20issue%2C%20we%20propose%20Super-Resolution%203DGS%20%28SuperGS%29%2C%20which%20is%20an%20expansion%0Aof%203DGS%20designed%20with%20a%20two-stage%20coarse-to-fine%20training%20framework.%20In%20this%0Aframework%2C%20we%20use%20a%20latent%20feature%20field%20to%20represent%20the%20low-resolution%20scene%2C%0Aserving%20as%20both%20the%20initialization%20and%20foundational%20information%20for%0Asuper-resolution%20optimization.%20Additionally%2C%20we%20introduce%20variational%20residual%0Afeatures%20to%20enhance%20high-resolution%20details%2C%20using%20their%20variance%20as%0Auncertainty%20estimates%20to%20guide%20the%20densification%20process%20and%20loss%20computation.%0AFurthermore%2C%20the%20introduction%20of%20a%20multi-view%20joint%20learning%20approach%20helps%0Amitigate%20ambiguities%20caused%20by%20multi-view%20inconsistencies%20in%20the%20pseudo%20labels.%0AExtensive%20experiments%20demonstrate%20that%20SuperGS%20surpasses%20state-of-the-art%20HRNVS%0Amethods%20on%20both%20real-world%20and%20synthetic%20datasets%20using%20only%20low-resolution%0Ainputs.%20Code%20is%20available%20at%20https%3A//github.com/SYXieee/SuperGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02571v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperGS%253A%2520Super-Resolution%25203D%2520Gaussian%2520Splatting%2520Enhanced%2520by%2520Variational%250A%2520%2520Residual%2520Features%2520and%2520Uncertainty-Augmented%2520Learning%26entry.906535625%3DShiyun%2520Xie%2520and%2520Zhiru%2520Wang%2520and%2520Xu%2520Wang%2520and%2520Yinghao%2520Zhu%2520and%2520Chengwei%2520Pan%2520and%2520Xiwang%2520Dong%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520exceled%2520in%2520novel%2520view%2520synthesis%250A%2528NVS%2529%2520with%2520its%2520real-time%2520rendering%2520capabilities%2520and%2520superior%2520quality.%2520However%252C%250Ait%2520faces%2520challenges%2520for%2520high-resolution%2520novel%2520view%2520synthesis%2520%2528HRNVS%2529%2520due%2520to%2520the%250Acoarse%2520nature%2520of%2520primitives%2520derived%2520from%2520low-resolution%2520input%2520views.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520Super-Resolution%25203DGS%2520%2528SuperGS%2529%252C%2520which%2520is%2520an%2520expansion%250Aof%25203DGS%2520designed%2520with%2520a%2520two-stage%2520coarse-to-fine%2520training%2520framework.%2520In%2520this%250Aframework%252C%2520we%2520use%2520a%2520latent%2520feature%2520field%2520to%2520represent%2520the%2520low-resolution%2520scene%252C%250Aserving%2520as%2520both%2520the%2520initialization%2520and%2520foundational%2520information%2520for%250Asuper-resolution%2520optimization.%2520Additionally%252C%2520we%2520introduce%2520variational%2520residual%250Afeatures%2520to%2520enhance%2520high-resolution%2520details%252C%2520using%2520their%2520variance%2520as%250Auncertainty%2520estimates%2520to%2520guide%2520the%2520densification%2520process%2520and%2520loss%2520computation.%250AFurthermore%252C%2520the%2520introduction%2520of%2520a%2520multi-view%2520joint%2520learning%2520approach%2520helps%250Amitigate%2520ambiguities%2520caused%2520by%2520multi-view%2520inconsistencies%2520in%2520the%2520pseudo%2520labels.%250AExtensive%2520experiments%2520demonstrate%2520that%2520SuperGS%2520surpasses%2520state-of-the-art%2520HRNVS%250Amethods%2520on%2520both%2520real-world%2520and%2520synthetic%2520datasets%2520using%2520only%2520low-resolution%250Ainputs.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/SYXieee/SuperGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02571v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperGS%3A%20Super-Resolution%203D%20Gaussian%20Splatting%20Enhanced%20by%20Variational%0A%20%20Residual%20Features%20and%20Uncertainty-Augmented%20Learning&entry.906535625=Shiyun%20Xie%20and%20Zhiru%20Wang%20and%20Xu%20Wang%20and%20Yinghao%20Zhu%20and%20Chengwei%20Pan%20and%20Xiwang%20Dong&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20exceled%20in%20novel%20view%20synthesis%0A%28NVS%29%20with%20its%20real-time%20rendering%20capabilities%20and%20superior%20quality.%20However%2C%0Ait%20faces%20challenges%20for%20high-resolution%20novel%20view%20synthesis%20%28HRNVS%29%20due%20to%20the%0Acoarse%20nature%20of%20primitives%20derived%20from%20low-resolution%20input%20views.%20To%20address%0Athis%20issue%2C%20we%20propose%20Super-Resolution%203DGS%20%28SuperGS%29%2C%20which%20is%20an%20expansion%0Aof%203DGS%20designed%20with%20a%20two-stage%20coarse-to-fine%20training%20framework.%20In%20this%0Aframework%2C%20we%20use%20a%20latent%20feature%20field%20to%20represent%20the%20low-resolution%20scene%2C%0Aserving%20as%20both%20the%20initialization%20and%20foundational%20information%20for%0Asuper-resolution%20optimization.%20Additionally%2C%20we%20introduce%20variational%20residual%0Afeatures%20to%20enhance%20high-resolution%20details%2C%20using%20their%20variance%20as%0Auncertainty%20estimates%20to%20guide%20the%20densification%20process%20and%20loss%20computation.%0AFurthermore%2C%20the%20introduction%20of%20a%20multi-view%20joint%20learning%20approach%20helps%0Amitigate%20ambiguities%20caused%20by%20multi-view%20inconsistencies%20in%20the%20pseudo%20labels.%0AExtensive%20experiments%20demonstrate%20that%20SuperGS%20surpasses%20state-of-the-art%20HRNVS%0Amethods%20on%20both%20real-world%20and%20synthetic%20datasets%20using%20only%20low-resolution%0Ainputs.%20Code%20is%20available%20at%20https%3A//github.com/SYXieee/SuperGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02571v3&entry.124074799=Read"},
{"title": "Discretized Gaussian Representation for Tomographic Reconstruction", "author": "Shaokai Wu and Yuxiang Lu and Wei Ji and Suizhi Huang and Fengyu Yang and Shalayiding Sirejiding and Qichen He and Jing Tong and Yanbiao Ji and Yue Ding and Hongtao Lu", "abstract": "  Computed Tomography (CT) is a widely used imaging technique that provides\ndetailed cross-sectional views of objects. Over the past decade, Deep\nLearning-based Reconstruction (DLR) methods have led efforts to enhance image\nquality and reduce noise, yet they often require large amounts of data and are\ncomputationally intensive. Inspired by recent advancements in scene\nreconstruction, some approaches have adapted NeRF and 3D Gaussian Splatting\n(3DGS) techniques for CT reconstruction. However, these methods are not ideal\nfor direct 3D volume reconstruction. In this paper, we reconsider the\nrepresentation of CT reconstruction and propose a novel Discretized Gaussian\nRepresentation (DGR) specifically designed for CT. Unlike the popular 3D\nGaussian Splatting, our representation directly reconstructs the 3D volume\nusing a set of discretized Gaussian functions in an end-to-end manner.\nAdditionally, we introduce a Fast Volume Reconstruction technique that\nefficiently aggregates the contributions of these Gaussians into a discretized\nvolume. Extensive experiments on both real-world and synthetic datasets\ndemonstrate the effectiveness of our method in improving reconstruction quality\nand computational efficiency. Our code has been provided for review purposes\nand will be made publicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2411.04844v2", "date": "2024-12-11", "relevancy": 3.2332, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6948}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6372}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discretized%20Gaussian%20Representation%20for%20Tomographic%20Reconstruction&body=Title%3A%20Discretized%20Gaussian%20Representation%20for%20Tomographic%20Reconstruction%0AAuthor%3A%20Shaokai%20Wu%20and%20Yuxiang%20Lu%20and%20Wei%20Ji%20and%20Suizhi%20Huang%20and%20Fengyu%20Yang%20and%20Shalayiding%20Sirejiding%20and%20Qichen%20He%20and%20Jing%20Tong%20and%20Yanbiao%20Ji%20and%20Yue%20Ding%20and%20Hongtao%20Lu%0AAbstract%3A%20%20%20Computed%20Tomography%20%28CT%29%20is%20a%20widely%20used%20imaging%20technique%20that%20provides%0Adetailed%20cross-sectional%20views%20of%20objects.%20Over%20the%20past%20decade%2C%20Deep%0ALearning-based%20Reconstruction%20%28DLR%29%20methods%20have%20led%20efforts%20to%20enhance%20image%0Aquality%20and%20reduce%20noise%2C%20yet%20they%20often%20require%20large%20amounts%20of%20data%20and%20are%0Acomputationally%20intensive.%20Inspired%20by%20recent%20advancements%20in%20scene%0Areconstruction%2C%20some%20approaches%20have%20adapted%20NeRF%20and%203D%20Gaussian%20Splatting%0A%283DGS%29%20techniques%20for%20CT%20reconstruction.%20However%2C%20these%20methods%20are%20not%20ideal%0Afor%20direct%203D%20volume%20reconstruction.%20In%20this%20paper%2C%20we%20reconsider%20the%0Arepresentation%20of%20CT%20reconstruction%20and%20propose%20a%20novel%20Discretized%20Gaussian%0ARepresentation%20%28DGR%29%20specifically%20designed%20for%20CT.%20Unlike%20the%20popular%203D%0AGaussian%20Splatting%2C%20our%20representation%20directly%20reconstructs%20the%203D%20volume%0Ausing%20a%20set%20of%20discretized%20Gaussian%20functions%20in%20an%20end-to-end%20manner.%0AAdditionally%2C%20we%20introduce%20a%20Fast%20Volume%20Reconstruction%20technique%20that%0Aefficiently%20aggregates%20the%20contributions%20of%20these%20Gaussians%20into%20a%20discretized%0Avolume.%20Extensive%20experiments%20on%20both%20real-world%20and%20synthetic%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20in%20improving%20reconstruction%20quality%0Aand%20computational%20efficiency.%20Our%20code%20has%20been%20provided%20for%20review%20purposes%0Aand%20will%20be%20made%20publicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscretized%2520Gaussian%2520Representation%2520for%2520Tomographic%2520Reconstruction%26entry.906535625%3DShaokai%2520Wu%2520and%2520Yuxiang%2520Lu%2520and%2520Wei%2520Ji%2520and%2520Suizhi%2520Huang%2520and%2520Fengyu%2520Yang%2520and%2520Shalayiding%2520Sirejiding%2520and%2520Qichen%2520He%2520and%2520Jing%2520Tong%2520and%2520Yanbiao%2520Ji%2520and%2520Yue%2520Ding%2520and%2520Hongtao%2520Lu%26entry.1292438233%3D%2520%2520Computed%2520Tomography%2520%2528CT%2529%2520is%2520a%2520widely%2520used%2520imaging%2520technique%2520that%2520provides%250Adetailed%2520cross-sectional%2520views%2520of%2520objects.%2520Over%2520the%2520past%2520decade%252C%2520Deep%250ALearning-based%2520Reconstruction%2520%2528DLR%2529%2520methods%2520have%2520led%2520efforts%2520to%2520enhance%2520image%250Aquality%2520and%2520reduce%2520noise%252C%2520yet%2520they%2520often%2520require%2520large%2520amounts%2520of%2520data%2520and%2520are%250Acomputationally%2520intensive.%2520Inspired%2520by%2520recent%2520advancements%2520in%2520scene%250Areconstruction%252C%2520some%2520approaches%2520have%2520adapted%2520NeRF%2520and%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%2520techniques%2520for%2520CT%2520reconstruction.%2520However%252C%2520these%2520methods%2520are%2520not%2520ideal%250Afor%2520direct%25203D%2520volume%2520reconstruction.%2520In%2520this%2520paper%252C%2520we%2520reconsider%2520the%250Arepresentation%2520of%2520CT%2520reconstruction%2520and%2520propose%2520a%2520novel%2520Discretized%2520Gaussian%250ARepresentation%2520%2528DGR%2529%2520specifically%2520designed%2520for%2520CT.%2520Unlike%2520the%2520popular%25203D%250AGaussian%2520Splatting%252C%2520our%2520representation%2520directly%2520reconstructs%2520the%25203D%2520volume%250Ausing%2520a%2520set%2520of%2520discretized%2520Gaussian%2520functions%2520in%2520an%2520end-to-end%2520manner.%250AAdditionally%252C%2520we%2520introduce%2520a%2520Fast%2520Volume%2520Reconstruction%2520technique%2520that%250Aefficiently%2520aggregates%2520the%2520contributions%2520of%2520these%2520Gaussians%2520into%2520a%2520discretized%250Avolume.%2520Extensive%2520experiments%2520on%2520both%2520real-world%2520and%2520synthetic%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520improving%2520reconstruction%2520quality%250Aand%2520computational%2520efficiency.%2520Our%2520code%2520has%2520been%2520provided%2520for%2520review%2520purposes%250Aand%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discretized%20Gaussian%20Representation%20for%20Tomographic%20Reconstruction&entry.906535625=Shaokai%20Wu%20and%20Yuxiang%20Lu%20and%20Wei%20Ji%20and%20Suizhi%20Huang%20and%20Fengyu%20Yang%20and%20Shalayiding%20Sirejiding%20and%20Qichen%20He%20and%20Jing%20Tong%20and%20Yanbiao%20Ji%20and%20Yue%20Ding%20and%20Hongtao%20Lu&entry.1292438233=%20%20Computed%20Tomography%20%28CT%29%20is%20a%20widely%20used%20imaging%20technique%20that%20provides%0Adetailed%20cross-sectional%20views%20of%20objects.%20Over%20the%20past%20decade%2C%20Deep%0ALearning-based%20Reconstruction%20%28DLR%29%20methods%20have%20led%20efforts%20to%20enhance%20image%0Aquality%20and%20reduce%20noise%2C%20yet%20they%20often%20require%20large%20amounts%20of%20data%20and%20are%0Acomputationally%20intensive.%20Inspired%20by%20recent%20advancements%20in%20scene%0Areconstruction%2C%20some%20approaches%20have%20adapted%20NeRF%20and%203D%20Gaussian%20Splatting%0A%283DGS%29%20techniques%20for%20CT%20reconstruction.%20However%2C%20these%20methods%20are%20not%20ideal%0Afor%20direct%203D%20volume%20reconstruction.%20In%20this%20paper%2C%20we%20reconsider%20the%0Arepresentation%20of%20CT%20reconstruction%20and%20propose%20a%20novel%20Discretized%20Gaussian%0ARepresentation%20%28DGR%29%20specifically%20designed%20for%20CT.%20Unlike%20the%20popular%203D%0AGaussian%20Splatting%2C%20our%20representation%20directly%20reconstructs%20the%203D%20volume%0Ausing%20a%20set%20of%20discretized%20Gaussian%20functions%20in%20an%20end-to-end%20manner.%0AAdditionally%2C%20we%20introduce%20a%20Fast%20Volume%20Reconstruction%20technique%20that%0Aefficiently%20aggregates%20the%20contributions%20of%20these%20Gaussians%20into%20a%20discretized%0Avolume.%20Extensive%20experiments%20on%20both%20real-world%20and%20synthetic%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20method%20in%20improving%20reconstruction%20quality%0Aand%20computational%20efficiency.%20Our%20code%20has%20been%20provided%20for%20review%20purposes%0Aand%20will%20be%20made%20publicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04844v2&entry.124074799=Read"},
{"title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for\n  Multimodal Language Models", "author": "Jieyu Zhang and Le Xue and Linxin Song and Jun Wang and Weikai Huang and Manli Shu and An Yan and Zixian Ma and Juan Carlos Niebles and silvio savarese and Caiming Xiong and Zeyuan Chen and Ranjay Krishna and Ran Xu", "abstract": "  With the rise of multimodal applications, instruction data has become\ncritical for training multimodal language models capable of understanding\ncomplex image-based queries. Existing practices rely on powerful but costly\nlarge language models (LLMs) or multimodal language models (MLMs) to produce\ninstruction data. These are often prone to hallucinations, licensing issues and\nthe generation process is often hard to scale and interpret. In this work, we\npresent a programmatic approach that employs scene graphs as symbolic\nrepresentations of images and human-written programs to systematically\nsynthesize vision-centric instruction data. Our approach ensures the\ninterpretability and controllability of the data generation process and scales\nefficiently while maintaining factual accuracy. By implementing a suite of 24\nsingle-image, 14 multi-image instruction generators, and a scene graph\ngeneration pipeline, we build a scalable, cost-effective system: ProVision\nwhich produces diverse question-answer pairs concerning objects, attributes,\nrelations, depth, etc., for any given image. Applied to Visual Genome and\nDataComp datasets, we generate over 10 million instruction data points,\nProVision-10M, and leverage them in both pretraining and instruction tuning\nstages of MLMs. When adopted in the instruction tuning stage, our single-image\ninstruction data yields up to a 7% improvement on the 2D split and 8% on the 3D\nsplit of CVBench, along with a 3% increase in performance on QBench2,\nRealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%\nimprovement on Mantis-Eval. Incorporation of our data in both pre-training and\nfine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%\nacross 11 benchmarks.\n", "link": "http://arxiv.org/abs/2412.07012v2", "date": "2024-12-11", "relevancy": 3.135, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6522}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6522}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProVision%3A%20Programmatically%20Scaling%20Vision-centric%20Instruction%20Data%20for%0A%20%20Multimodal%20Language%20Models&body=Title%3A%20ProVision%3A%20Programmatically%20Scaling%20Vision-centric%20Instruction%20Data%20for%0A%20%20Multimodal%20Language%20Models%0AAuthor%3A%20Jieyu%20Zhang%20and%20Le%20Xue%20and%20Linxin%20Song%20and%20Jun%20Wang%20and%20Weikai%20Huang%20and%20Manli%20Shu%20and%20An%20Yan%20and%20Zixian%20Ma%20and%20Juan%20Carlos%20Niebles%20and%20silvio%20savarese%20and%20Caiming%20Xiong%20and%20Zeyuan%20Chen%20and%20Ranjay%20Krishna%20and%20Ran%20Xu%0AAbstract%3A%20%20%20With%20the%20rise%20of%20multimodal%20applications%2C%20instruction%20data%20has%20become%0Acritical%20for%20training%20multimodal%20language%20models%20capable%20of%20understanding%0Acomplex%20image-based%20queries.%20Existing%20practices%20rely%20on%20powerful%20but%20costly%0Alarge%20language%20models%20%28LLMs%29%20or%20multimodal%20language%20models%20%28MLMs%29%20to%20produce%0Ainstruction%20data.%20These%20are%20often%20prone%20to%20hallucinations%2C%20licensing%20issues%20and%0Athe%20generation%20process%20is%20often%20hard%20to%20scale%20and%20interpret.%20In%20this%20work%2C%20we%0Apresent%20a%20programmatic%20approach%20that%20employs%20scene%20graphs%20as%20symbolic%0Arepresentations%20of%20images%20and%20human-written%20programs%20to%20systematically%0Asynthesize%20vision-centric%20instruction%20data.%20Our%20approach%20ensures%20the%0Ainterpretability%20and%20controllability%20of%20the%20data%20generation%20process%20and%20scales%0Aefficiently%20while%20maintaining%20factual%20accuracy.%20By%20implementing%20a%20suite%20of%2024%0Asingle-image%2C%2014%20multi-image%20instruction%20generators%2C%20and%20a%20scene%20graph%0Ageneration%20pipeline%2C%20we%20build%20a%20scalable%2C%20cost-effective%20system%3A%20ProVision%0Awhich%20produces%20diverse%20question-answer%20pairs%20concerning%20objects%2C%20attributes%2C%0Arelations%2C%20depth%2C%20etc.%2C%20for%20any%20given%20image.%20Applied%20to%20Visual%20Genome%20and%0ADataComp%20datasets%2C%20we%20generate%20over%2010%20million%20instruction%20data%20points%2C%0AProVision-10M%2C%20and%20leverage%20them%20in%20both%20pretraining%20and%20instruction%20tuning%0Astages%20of%20MLMs.%20When%20adopted%20in%20the%20instruction%20tuning%20stage%2C%20our%20single-image%0Ainstruction%20data%20yields%20up%20to%20a%207%25%20improvement%20on%20the%202D%20split%20and%208%25%20on%20the%203D%0Asplit%20of%20CVBench%2C%20along%20with%20a%203%25%20increase%20in%20performance%20on%20QBench2%2C%0ARealWorldQA%2C%20and%20MMMU.%20Our%20multi-image%20instruction%20data%20leads%20to%20an%208%25%0Aimprovement%20on%20Mantis-Eval.%20Incorporation%20of%20our%20data%20in%20both%20pre-training%20and%0Afine-tuning%20stages%20of%20xGen-MM-4B%20leads%20to%20an%20averaged%20improvement%20of%201.6%25%0Aacross%2011%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProVision%253A%2520Programmatically%2520Scaling%2520Vision-centric%2520Instruction%2520Data%2520for%250A%2520%2520Multimodal%2520Language%2520Models%26entry.906535625%3DJieyu%2520Zhang%2520and%2520Le%2520Xue%2520and%2520Linxin%2520Song%2520and%2520Jun%2520Wang%2520and%2520Weikai%2520Huang%2520and%2520Manli%2520Shu%2520and%2520An%2520Yan%2520and%2520Zixian%2520Ma%2520and%2520Juan%2520Carlos%2520Niebles%2520and%2520silvio%2520savarese%2520and%2520Caiming%2520Xiong%2520and%2520Zeyuan%2520Chen%2520and%2520Ranjay%2520Krishna%2520and%2520Ran%2520Xu%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520multimodal%2520applications%252C%2520instruction%2520data%2520has%2520become%250Acritical%2520for%2520training%2520multimodal%2520language%2520models%2520capable%2520of%2520understanding%250Acomplex%2520image-based%2520queries.%2520Existing%2520practices%2520rely%2520on%2520powerful%2520but%2520costly%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520or%2520multimodal%2520language%2520models%2520%2528MLMs%2529%2520to%2520produce%250Ainstruction%2520data.%2520These%2520are%2520often%2520prone%2520to%2520hallucinations%252C%2520licensing%2520issues%2520and%250Athe%2520generation%2520process%2520is%2520often%2520hard%2520to%2520scale%2520and%2520interpret.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520programmatic%2520approach%2520that%2520employs%2520scene%2520graphs%2520as%2520symbolic%250Arepresentations%2520of%2520images%2520and%2520human-written%2520programs%2520to%2520systematically%250Asynthesize%2520vision-centric%2520instruction%2520data.%2520Our%2520approach%2520ensures%2520the%250Ainterpretability%2520and%2520controllability%2520of%2520the%2520data%2520generation%2520process%2520and%2520scales%250Aefficiently%2520while%2520maintaining%2520factual%2520accuracy.%2520By%2520implementing%2520a%2520suite%2520of%252024%250Asingle-image%252C%252014%2520multi-image%2520instruction%2520generators%252C%2520and%2520a%2520scene%2520graph%250Ageneration%2520pipeline%252C%2520we%2520build%2520a%2520scalable%252C%2520cost-effective%2520system%253A%2520ProVision%250Awhich%2520produces%2520diverse%2520question-answer%2520pairs%2520concerning%2520objects%252C%2520attributes%252C%250Arelations%252C%2520depth%252C%2520etc.%252C%2520for%2520any%2520given%2520image.%2520Applied%2520to%2520Visual%2520Genome%2520and%250ADataComp%2520datasets%252C%2520we%2520generate%2520over%252010%2520million%2520instruction%2520data%2520points%252C%250AProVision-10M%252C%2520and%2520leverage%2520them%2520in%2520both%2520pretraining%2520and%2520instruction%2520tuning%250Astages%2520of%2520MLMs.%2520When%2520adopted%2520in%2520the%2520instruction%2520tuning%2520stage%252C%2520our%2520single-image%250Ainstruction%2520data%2520yields%2520up%2520to%2520a%25207%2525%2520improvement%2520on%2520the%25202D%2520split%2520and%25208%2525%2520on%2520the%25203D%250Asplit%2520of%2520CVBench%252C%2520along%2520with%2520a%25203%2525%2520increase%2520in%2520performance%2520on%2520QBench2%252C%250ARealWorldQA%252C%2520and%2520MMMU.%2520Our%2520multi-image%2520instruction%2520data%2520leads%2520to%2520an%25208%2525%250Aimprovement%2520on%2520Mantis-Eval.%2520Incorporation%2520of%2520our%2520data%2520in%2520both%2520pre-training%2520and%250Afine-tuning%2520stages%2520of%2520xGen-MM-4B%2520leads%2520to%2520an%2520averaged%2520improvement%2520of%25201.6%2525%250Aacross%252011%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProVision%3A%20Programmatically%20Scaling%20Vision-centric%20Instruction%20Data%20for%0A%20%20Multimodal%20Language%20Models&entry.906535625=Jieyu%20Zhang%20and%20Le%20Xue%20and%20Linxin%20Song%20and%20Jun%20Wang%20and%20Weikai%20Huang%20and%20Manli%20Shu%20and%20An%20Yan%20and%20Zixian%20Ma%20and%20Juan%20Carlos%20Niebles%20and%20silvio%20savarese%20and%20Caiming%20Xiong%20and%20Zeyuan%20Chen%20and%20Ranjay%20Krishna%20and%20Ran%20Xu&entry.1292438233=%20%20With%20the%20rise%20of%20multimodal%20applications%2C%20instruction%20data%20has%20become%0Acritical%20for%20training%20multimodal%20language%20models%20capable%20of%20understanding%0Acomplex%20image-based%20queries.%20Existing%20practices%20rely%20on%20powerful%20but%20costly%0Alarge%20language%20models%20%28LLMs%29%20or%20multimodal%20language%20models%20%28MLMs%29%20to%20produce%0Ainstruction%20data.%20These%20are%20often%20prone%20to%20hallucinations%2C%20licensing%20issues%20and%0Athe%20generation%20process%20is%20often%20hard%20to%20scale%20and%20interpret.%20In%20this%20work%2C%20we%0Apresent%20a%20programmatic%20approach%20that%20employs%20scene%20graphs%20as%20symbolic%0Arepresentations%20of%20images%20and%20human-written%20programs%20to%20systematically%0Asynthesize%20vision-centric%20instruction%20data.%20Our%20approach%20ensures%20the%0Ainterpretability%20and%20controllability%20of%20the%20data%20generation%20process%20and%20scales%0Aefficiently%20while%20maintaining%20factual%20accuracy.%20By%20implementing%20a%20suite%20of%2024%0Asingle-image%2C%2014%20multi-image%20instruction%20generators%2C%20and%20a%20scene%20graph%0Ageneration%20pipeline%2C%20we%20build%20a%20scalable%2C%20cost-effective%20system%3A%20ProVision%0Awhich%20produces%20diverse%20question-answer%20pairs%20concerning%20objects%2C%20attributes%2C%0Arelations%2C%20depth%2C%20etc.%2C%20for%20any%20given%20image.%20Applied%20to%20Visual%20Genome%20and%0ADataComp%20datasets%2C%20we%20generate%20over%2010%20million%20instruction%20data%20points%2C%0AProVision-10M%2C%20and%20leverage%20them%20in%20both%20pretraining%20and%20instruction%20tuning%0Astages%20of%20MLMs.%20When%20adopted%20in%20the%20instruction%20tuning%20stage%2C%20our%20single-image%0Ainstruction%20data%20yields%20up%20to%20a%207%25%20improvement%20on%20the%202D%20split%20and%208%25%20on%20the%203D%0Asplit%20of%20CVBench%2C%20along%20with%20a%203%25%20increase%20in%20performance%20on%20QBench2%2C%0ARealWorldQA%2C%20and%20MMMU.%20Our%20multi-image%20instruction%20data%20leads%20to%20an%208%25%0Aimprovement%20on%20Mantis-Eval.%20Incorporation%20of%20our%20data%20in%20both%20pre-training%20and%0Afine-tuning%20stages%20of%20xGen-MM-4B%20leads%20to%20an%20averaged%20improvement%20of%201.6%25%0Aacross%2011%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07012v2&entry.124074799=Read"},
{"title": "PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based\n  Talking Head Synthesis", "author": "Yifan Xie and Tao Feng and Xin Zhang and Xiangyang Luo and Zixuan Guo and Weijiang Yu and Heng Chang and Fei Ma and Fei Richard Yu", "abstract": "  Talking head synthesis with arbitrary speech audio is a crucial challenge in\nthe field of digital humans. Recently, methods based on radiance fields have\nreceived increasing attention due to their ability to synthesize high-fidelity\nand identity-consistent talking heads from just a few minutes of training\nvideo. However, due to the limited scale of the training data, these methods\noften exhibit poor performance in audio-lip synchronization and visual quality.\nIn this paper, we propose a novel 3D Gaussian-based method called PointTalk,\nwhich constructs a static 3D Gaussian field of the head and deforms it in sync\nwith the audio. It also incorporates an audio-driven dynamic lip point cloud as\na critical component of the conditional information, thereby facilitating the\neffective synthesis of talking heads. Specifically, the initial step involves\ngenerating the corresponding lip point cloud from the audio signal and\ncapturing its topological structure. The design of the dynamic difference\nencoder aims to capture the subtle nuances inherent in dynamic lip movements\nmore effectively. Furthermore, we integrate the audio-point enhancement module,\nwhich not only ensures the synchronization of the audio signal with the\ncorresponding lip point cloud within the feature space, but also facilitates a\ndeeper understanding of the interrelations among cross-modal conditional\nfeatures. Extensive experiments demonstrate that our method achieves superior\nhigh-fidelity and audio-lip synchronization in talking head synthesis compared\nto previous methods.\n", "link": "http://arxiv.org/abs/2412.08504v1", "date": "2024-12-11", "relevancy": 3.122, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6485}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6485}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointTalk%3A%20Audio-Driven%20Dynamic%20Lip%20Point%20Cloud%20for%203D%20Gaussian-based%0A%20%20Talking%20Head%20Synthesis&body=Title%3A%20PointTalk%3A%20Audio-Driven%20Dynamic%20Lip%20Point%20Cloud%20for%203D%20Gaussian-based%0A%20%20Talking%20Head%20Synthesis%0AAuthor%3A%20Yifan%20Xie%20and%20Tao%20Feng%20and%20Xin%20Zhang%20and%20Xiangyang%20Luo%20and%20Zixuan%20Guo%20and%20Weijiang%20Yu%20and%20Heng%20Chang%20and%20Fei%20Ma%20and%20Fei%20Richard%20Yu%0AAbstract%3A%20%20%20Talking%20head%20synthesis%20with%20arbitrary%20speech%20audio%20is%20a%20crucial%20challenge%20in%0Athe%20field%20of%20digital%20humans.%20Recently%2C%20methods%20based%20on%20radiance%20fields%20have%0Areceived%20increasing%20attention%20due%20to%20their%20ability%20to%20synthesize%20high-fidelity%0Aand%20identity-consistent%20talking%20heads%20from%20just%20a%20few%20minutes%20of%20training%0Avideo.%20However%2C%20due%20to%20the%20limited%20scale%20of%20the%20training%20data%2C%20these%20methods%0Aoften%20exhibit%20poor%20performance%20in%20audio-lip%20synchronization%20and%20visual%20quality.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%203D%20Gaussian-based%20method%20called%20PointTalk%2C%0Awhich%20constructs%20a%20static%203D%20Gaussian%20field%20of%20the%20head%20and%20deforms%20it%20in%20sync%0Awith%20the%20audio.%20It%20also%20incorporates%20an%20audio-driven%20dynamic%20lip%20point%20cloud%20as%0Aa%20critical%20component%20of%20the%20conditional%20information%2C%20thereby%20facilitating%20the%0Aeffective%20synthesis%20of%20talking%20heads.%20Specifically%2C%20the%20initial%20step%20involves%0Agenerating%20the%20corresponding%20lip%20point%20cloud%20from%20the%20audio%20signal%20and%0Acapturing%20its%20topological%20structure.%20The%20design%20of%20the%20dynamic%20difference%0Aencoder%20aims%20to%20capture%20the%20subtle%20nuances%20inherent%20in%20dynamic%20lip%20movements%0Amore%20effectively.%20Furthermore%2C%20we%20integrate%20the%20audio-point%20enhancement%20module%2C%0Awhich%20not%20only%20ensures%20the%20synchronization%20of%20the%20audio%20signal%20with%20the%0Acorresponding%20lip%20point%20cloud%20within%20the%20feature%20space%2C%20but%20also%20facilitates%20a%0Adeeper%20understanding%20of%20the%20interrelations%20among%20cross-modal%20conditional%0Afeatures.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20superior%0Ahigh-fidelity%20and%20audio-lip%20synchronization%20in%20talking%20head%20synthesis%20compared%0Ato%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointTalk%253A%2520Audio-Driven%2520Dynamic%2520Lip%2520Point%2520Cloud%2520for%25203D%2520Gaussian-based%250A%2520%2520Talking%2520Head%2520Synthesis%26entry.906535625%3DYifan%2520Xie%2520and%2520Tao%2520Feng%2520and%2520Xin%2520Zhang%2520and%2520Xiangyang%2520Luo%2520and%2520Zixuan%2520Guo%2520and%2520Weijiang%2520Yu%2520and%2520Heng%2520Chang%2520and%2520Fei%2520Ma%2520and%2520Fei%2520Richard%2520Yu%26entry.1292438233%3D%2520%2520Talking%2520head%2520synthesis%2520with%2520arbitrary%2520speech%2520audio%2520is%2520a%2520crucial%2520challenge%2520in%250Athe%2520field%2520of%2520digital%2520humans.%2520Recently%252C%2520methods%2520based%2520on%2520radiance%2520fields%2520have%250Areceived%2520increasing%2520attention%2520due%2520to%2520their%2520ability%2520to%2520synthesize%2520high-fidelity%250Aand%2520identity-consistent%2520talking%2520heads%2520from%2520just%2520a%2520few%2520minutes%2520of%2520training%250Avideo.%2520However%252C%2520due%2520to%2520the%2520limited%2520scale%2520of%2520the%2520training%2520data%252C%2520these%2520methods%250Aoften%2520exhibit%2520poor%2520performance%2520in%2520audio-lip%2520synchronization%2520and%2520visual%2520quality.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%25203D%2520Gaussian-based%2520method%2520called%2520PointTalk%252C%250Awhich%2520constructs%2520a%2520static%25203D%2520Gaussian%2520field%2520of%2520the%2520head%2520and%2520deforms%2520it%2520in%2520sync%250Awith%2520the%2520audio.%2520It%2520also%2520incorporates%2520an%2520audio-driven%2520dynamic%2520lip%2520point%2520cloud%2520as%250Aa%2520critical%2520component%2520of%2520the%2520conditional%2520information%252C%2520thereby%2520facilitating%2520the%250Aeffective%2520synthesis%2520of%2520talking%2520heads.%2520Specifically%252C%2520the%2520initial%2520step%2520involves%250Agenerating%2520the%2520corresponding%2520lip%2520point%2520cloud%2520from%2520the%2520audio%2520signal%2520and%250Acapturing%2520its%2520topological%2520structure.%2520The%2520design%2520of%2520the%2520dynamic%2520difference%250Aencoder%2520aims%2520to%2520capture%2520the%2520subtle%2520nuances%2520inherent%2520in%2520dynamic%2520lip%2520movements%250Amore%2520effectively.%2520Furthermore%252C%2520we%2520integrate%2520the%2520audio-point%2520enhancement%2520module%252C%250Awhich%2520not%2520only%2520ensures%2520the%2520synchronization%2520of%2520the%2520audio%2520signal%2520with%2520the%250Acorresponding%2520lip%2520point%2520cloud%2520within%2520the%2520feature%2520space%252C%2520but%2520also%2520facilitates%2520a%250Adeeper%2520understanding%2520of%2520the%2520interrelations%2520among%2520cross-modal%2520conditional%250Afeatures.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%250Ahigh-fidelity%2520and%2520audio-lip%2520synchronization%2520in%2520talking%2520head%2520synthesis%2520compared%250Ato%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointTalk%3A%20Audio-Driven%20Dynamic%20Lip%20Point%20Cloud%20for%203D%20Gaussian-based%0A%20%20Talking%20Head%20Synthesis&entry.906535625=Yifan%20Xie%20and%20Tao%20Feng%20and%20Xin%20Zhang%20and%20Xiangyang%20Luo%20and%20Zixuan%20Guo%20and%20Weijiang%20Yu%20and%20Heng%20Chang%20and%20Fei%20Ma%20and%20Fei%20Richard%20Yu&entry.1292438233=%20%20Talking%20head%20synthesis%20with%20arbitrary%20speech%20audio%20is%20a%20crucial%20challenge%20in%0Athe%20field%20of%20digital%20humans.%20Recently%2C%20methods%20based%20on%20radiance%20fields%20have%0Areceived%20increasing%20attention%20due%20to%20their%20ability%20to%20synthesize%20high-fidelity%0Aand%20identity-consistent%20talking%20heads%20from%20just%20a%20few%20minutes%20of%20training%0Avideo.%20However%2C%20due%20to%20the%20limited%20scale%20of%20the%20training%20data%2C%20these%20methods%0Aoften%20exhibit%20poor%20performance%20in%20audio-lip%20synchronization%20and%20visual%20quality.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%203D%20Gaussian-based%20method%20called%20PointTalk%2C%0Awhich%20constructs%20a%20static%203D%20Gaussian%20field%20of%20the%20head%20and%20deforms%20it%20in%20sync%0Awith%20the%20audio.%20It%20also%20incorporates%20an%20audio-driven%20dynamic%20lip%20point%20cloud%20as%0Aa%20critical%20component%20of%20the%20conditional%20information%2C%20thereby%20facilitating%20the%0Aeffective%20synthesis%20of%20talking%20heads.%20Specifically%2C%20the%20initial%20step%20involves%0Agenerating%20the%20corresponding%20lip%20point%20cloud%20from%20the%20audio%20signal%20and%0Acapturing%20its%20topological%20structure.%20The%20design%20of%20the%20dynamic%20difference%0Aencoder%20aims%20to%20capture%20the%20subtle%20nuances%20inherent%20in%20dynamic%20lip%20movements%0Amore%20effectively.%20Furthermore%2C%20we%20integrate%20the%20audio-point%20enhancement%20module%2C%0Awhich%20not%20only%20ensures%20the%20synchronization%20of%20the%20audio%20signal%20with%20the%0Acorresponding%20lip%20point%20cloud%20within%20the%20feature%20space%2C%20but%20also%20facilitates%20a%0Adeeper%20understanding%20of%20the%20interrelations%20among%20cross-modal%20conditional%0Afeatures.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20superior%0Ahigh-fidelity%20and%20audio-lip%20synchronization%20in%20talking%20head%20synthesis%20compared%0Ato%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08504v1&entry.124074799=Read"},
{"title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for\n  Comprehensive Image Captioning", "author": "Fan Lu and Wei Wu and Kecheng Zheng and Shuailei Ma and Biao Gong and Jiawei Liu and Wei Zhai and Yang Cao and Yujun Shen and Zheng-Jun Zha", "abstract": "  Generating detailed captions comprehending text-rich visual content in images\nhas received growing attention for Large Vision-Language Models (LVLMs).\nHowever, few studies have developed benchmarks specifically tailored for\ndetailed captions to measure their accuracy and comprehensiveness. In this\npaper, we introduce a detailed caption benchmark, termed as CompreCap, to\nevaluate the visual context from a directed scene graph view. Concretely, we\nfirst manually segment the image into semantically meaningful regions (i.e.,\nsemantic segmentation mask) according to common-object vocabulary, while also\ndistinguishing attributes of objects within all those regions. Then directional\nrelation labels of these objects are annotated to compose a directed scene\ngraph that can well encode rich compositional information of the image. Based\non our directed scene graph, we develop a pipeline to assess the generated\ndetailed captions from LVLMs on multiple levels, including the object-level\ncoverage, the accuracy of attribute descriptions, the score of key\nrelationships, etc. Experimental results on the CompreCap dataset confirm that\nour evaluation method aligns closely with human evaluation scores across LVLMs.\n", "link": "http://arxiv.org/abs/2412.08614v1", "date": "2024-12-11", "relevancy": 3.069, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6307}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Large%20Vision-Language%20Models%20via%20Directed%20Scene%20Graph%20for%0A%20%20Comprehensive%20Image%20Captioning&body=Title%3A%20Benchmarking%20Large%20Vision-Language%20Models%20via%20Directed%20Scene%20Graph%20for%0A%20%20Comprehensive%20Image%20Captioning%0AAuthor%3A%20Fan%20Lu%20and%20Wei%20Wu%20and%20Kecheng%20Zheng%20and%20Shuailei%20Ma%20and%20Biao%20Gong%20and%20Jiawei%20Liu%20and%20Wei%20Zhai%20and%20Yang%20Cao%20and%20Yujun%20Shen%20and%20Zheng-Jun%20Zha%0AAbstract%3A%20%20%20Generating%20detailed%20captions%20comprehending%20text-rich%20visual%20content%20in%20images%0Ahas%20received%20growing%20attention%20for%20Large%20Vision-Language%20Models%20%28LVLMs%29.%0AHowever%2C%20few%20studies%20have%20developed%20benchmarks%20specifically%20tailored%20for%0Adetailed%20captions%20to%20measure%20their%20accuracy%20and%20comprehensiveness.%20In%20this%0Apaper%2C%20we%20introduce%20a%20detailed%20caption%20benchmark%2C%20termed%20as%20CompreCap%2C%20to%0Aevaluate%20the%20visual%20context%20from%20a%20directed%20scene%20graph%20view.%20Concretely%2C%20we%0Afirst%20manually%20segment%20the%20image%20into%20semantically%20meaningful%20regions%20%28i.e.%2C%0Asemantic%20segmentation%20mask%29%20according%20to%20common-object%20vocabulary%2C%20while%20also%0Adistinguishing%20attributes%20of%20objects%20within%20all%20those%20regions.%20Then%20directional%0Arelation%20labels%20of%20these%20objects%20are%20annotated%20to%20compose%20a%20directed%20scene%0Agraph%20that%20can%20well%20encode%20rich%20compositional%20information%20of%20the%20image.%20Based%0Aon%20our%20directed%20scene%20graph%2C%20we%20develop%20a%20pipeline%20to%20assess%20the%20generated%0Adetailed%20captions%20from%20LVLMs%20on%20multiple%20levels%2C%20including%20the%20object-level%0Acoverage%2C%20the%20accuracy%20of%20attribute%20descriptions%2C%20the%20score%20of%20key%0Arelationships%2C%20etc.%20Experimental%20results%20on%20the%20CompreCap%20dataset%20confirm%20that%0Aour%20evaluation%20method%20aligns%20closely%20with%20human%20evaluation%20scores%20across%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Large%2520Vision-Language%2520Models%2520via%2520Directed%2520Scene%2520Graph%2520for%250A%2520%2520Comprehensive%2520Image%2520Captioning%26entry.906535625%3DFan%2520Lu%2520and%2520Wei%2520Wu%2520and%2520Kecheng%2520Zheng%2520and%2520Shuailei%2520Ma%2520and%2520Biao%2520Gong%2520and%2520Jiawei%2520Liu%2520and%2520Wei%2520Zhai%2520and%2520Yang%2520Cao%2520and%2520Yujun%2520Shen%2520and%2520Zheng-Jun%2520Zha%26entry.1292438233%3D%2520%2520Generating%2520detailed%2520captions%2520comprehending%2520text-rich%2520visual%2520content%2520in%2520images%250Ahas%2520received%2520growing%2520attention%2520for%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529.%250AHowever%252C%2520few%2520studies%2520have%2520developed%2520benchmarks%2520specifically%2520tailored%2520for%250Adetailed%2520captions%2520to%2520measure%2520their%2520accuracy%2520and%2520comprehensiveness.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520detailed%2520caption%2520benchmark%252C%2520termed%2520as%2520CompreCap%252C%2520to%250Aevaluate%2520the%2520visual%2520context%2520from%2520a%2520directed%2520scene%2520graph%2520view.%2520Concretely%252C%2520we%250Afirst%2520manually%2520segment%2520the%2520image%2520into%2520semantically%2520meaningful%2520regions%2520%2528i.e.%252C%250Asemantic%2520segmentation%2520mask%2529%2520according%2520to%2520common-object%2520vocabulary%252C%2520while%2520also%250Adistinguishing%2520attributes%2520of%2520objects%2520within%2520all%2520those%2520regions.%2520Then%2520directional%250Arelation%2520labels%2520of%2520these%2520objects%2520are%2520annotated%2520to%2520compose%2520a%2520directed%2520scene%250Agraph%2520that%2520can%2520well%2520encode%2520rich%2520compositional%2520information%2520of%2520the%2520image.%2520Based%250Aon%2520our%2520directed%2520scene%2520graph%252C%2520we%2520develop%2520a%2520pipeline%2520to%2520assess%2520the%2520generated%250Adetailed%2520captions%2520from%2520LVLMs%2520on%2520multiple%2520levels%252C%2520including%2520the%2520object-level%250Acoverage%252C%2520the%2520accuracy%2520of%2520attribute%2520descriptions%252C%2520the%2520score%2520of%2520key%250Arelationships%252C%2520etc.%2520Experimental%2520results%2520on%2520the%2520CompreCap%2520dataset%2520confirm%2520that%250Aour%2520evaluation%2520method%2520aligns%2520closely%2520with%2520human%2520evaluation%2520scores%2520across%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Large%20Vision-Language%20Models%20via%20Directed%20Scene%20Graph%20for%0A%20%20Comprehensive%20Image%20Captioning&entry.906535625=Fan%20Lu%20and%20Wei%20Wu%20and%20Kecheng%20Zheng%20and%20Shuailei%20Ma%20and%20Biao%20Gong%20and%20Jiawei%20Liu%20and%20Wei%20Zhai%20and%20Yang%20Cao%20and%20Yujun%20Shen%20and%20Zheng-Jun%20Zha&entry.1292438233=%20%20Generating%20detailed%20captions%20comprehending%20text-rich%20visual%20content%20in%20images%0Ahas%20received%20growing%20attention%20for%20Large%20Vision-Language%20Models%20%28LVLMs%29.%0AHowever%2C%20few%20studies%20have%20developed%20benchmarks%20specifically%20tailored%20for%0Adetailed%20captions%20to%20measure%20their%20accuracy%20and%20comprehensiveness.%20In%20this%0Apaper%2C%20we%20introduce%20a%20detailed%20caption%20benchmark%2C%20termed%20as%20CompreCap%2C%20to%0Aevaluate%20the%20visual%20context%20from%20a%20directed%20scene%20graph%20view.%20Concretely%2C%20we%0Afirst%20manually%20segment%20the%20image%20into%20semantically%20meaningful%20regions%20%28i.e.%2C%0Asemantic%20segmentation%20mask%29%20according%20to%20common-object%20vocabulary%2C%20while%20also%0Adistinguishing%20attributes%20of%20objects%20within%20all%20those%20regions.%20Then%20directional%0Arelation%20labels%20of%20these%20objects%20are%20annotated%20to%20compose%20a%20directed%20scene%0Agraph%20that%20can%20well%20encode%20rich%20compositional%20information%20of%20the%20image.%20Based%0Aon%20our%20directed%20scene%20graph%2C%20we%20develop%20a%20pipeline%20to%20assess%20the%20generated%0Adetailed%20captions%20from%20LVLMs%20on%20multiple%20levels%2C%20including%20the%20object-level%0Acoverage%2C%20the%20accuracy%20of%20attribute%20descriptions%2C%20the%20score%20of%20key%0Arelationships%2C%20etc.%20Experimental%20results%20on%20the%20CompreCap%20dataset%20confirm%20that%0Aour%20evaluation%20method%20aligns%20closely%20with%20human%20evaluation%20scores%20across%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08614v1&entry.124074799=Read"},
{"title": "LOMA: Language-assisted Semantic Occupancy Network via Triplane Mamba", "author": "Yubo Cui and Zhiheng Li and Jiaqiang Wang and Zheng Fang", "abstract": "  Vision-based 3D occupancy prediction has become a popular research task due\nto its versatility and affordability. Nowadays, conventional methods usually\nproject the image-based vision features to 3D space and learn the geometric\ninformation through the attention mechanism, enabling the 3D semantic occupancy\nprediction. However, these works usually face two main challenges: 1) Limited\ngeometric information. Due to the lack of geometric information in the image\nitself, it is challenging to directly predict 3D space information, especially\nin large-scale outdoor scenes. 2) Local restricted interaction. Due to the\nquadratic complexity of the attention mechanism, they often use modified local\nattention to fuse features, resulting in a restricted fusion. To address these\nproblems, in this paper, we propose a language-assisted 3D semantic occupancy\nprediction network, named LOMA. In the proposed vision-language framework, we\nfirst introduce a VL-aware Scene Generator (VSG) module to generate the 3D\nlanguage feature of the scene. By leveraging the vision-language model, this\nmodule provides implicit geometric knowledge and explicit semantic information\nfrom the language. Furthermore, we present a Tri-plane Fusion Mamba (TFM) block\nto efficiently fuse the 3D language feature and 3D vision feature. The proposed\nmodule not only fuses the two features with global modeling but also avoids too\nmuch computation costs. Experiments on the SemanticKITTI and SSCBench-KITTI360\ndatasets show that our algorithm achieves new state-of-the-art performances in\nboth geometric and semantic completion tasks. Our code will be open soon.\n", "link": "http://arxiv.org/abs/2412.08388v1", "date": "2024-12-11", "relevancy": 3.0317, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6076}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOMA%3A%20Language-assisted%20Semantic%20Occupancy%20Network%20via%20Triplane%20Mamba&body=Title%3A%20LOMA%3A%20Language-assisted%20Semantic%20Occupancy%20Network%20via%20Triplane%20Mamba%0AAuthor%3A%20Yubo%20Cui%20and%20Zhiheng%20Li%20and%20Jiaqiang%20Wang%20and%20Zheng%20Fang%0AAbstract%3A%20%20%20Vision-based%203D%20occupancy%20prediction%20has%20become%20a%20popular%20research%20task%20due%0Ato%20its%20versatility%20and%20affordability.%20Nowadays%2C%20conventional%20methods%20usually%0Aproject%20the%20image-based%20vision%20features%20to%203D%20space%20and%20learn%20the%20geometric%0Ainformation%20through%20the%20attention%20mechanism%2C%20enabling%20the%203D%20semantic%20occupancy%0Aprediction.%20However%2C%20these%20works%20usually%20face%20two%20main%20challenges%3A%201%29%20Limited%0Ageometric%20information.%20Due%20to%20the%20lack%20of%20geometric%20information%20in%20the%20image%0Aitself%2C%20it%20is%20challenging%20to%20directly%20predict%203D%20space%20information%2C%20especially%0Ain%20large-scale%20outdoor%20scenes.%202%29%20Local%20restricted%20interaction.%20Due%20to%20the%0Aquadratic%20complexity%20of%20the%20attention%20mechanism%2C%20they%20often%20use%20modified%20local%0Aattention%20to%20fuse%20features%2C%20resulting%20in%20a%20restricted%20fusion.%20To%20address%20these%0Aproblems%2C%20in%20this%20paper%2C%20we%20propose%20a%20language-assisted%203D%20semantic%20occupancy%0Aprediction%20network%2C%20named%20LOMA.%20In%20the%20proposed%20vision-language%20framework%2C%20we%0Afirst%20introduce%20a%20VL-aware%20Scene%20Generator%20%28VSG%29%20module%20to%20generate%20the%203D%0Alanguage%20feature%20of%20the%20scene.%20By%20leveraging%20the%20vision-language%20model%2C%20this%0Amodule%20provides%20implicit%20geometric%20knowledge%20and%20explicit%20semantic%20information%0Afrom%20the%20language.%20Furthermore%2C%20we%20present%20a%20Tri-plane%20Fusion%20Mamba%20%28TFM%29%20block%0Ato%20efficiently%20fuse%20the%203D%20language%20feature%20and%203D%20vision%20feature.%20The%20proposed%0Amodule%20not%20only%20fuses%20the%20two%20features%20with%20global%20modeling%20but%20also%20avoids%20too%0Amuch%20computation%20costs.%20Experiments%20on%20the%20SemanticKITTI%20and%20SSCBench-KITTI360%0Adatasets%20show%20that%20our%20algorithm%20achieves%20new%20state-of-the-art%20performances%20in%0Aboth%20geometric%20and%20semantic%20completion%20tasks.%20Our%20code%20will%20be%20open%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOMA%253A%2520Language-assisted%2520Semantic%2520Occupancy%2520Network%2520via%2520Triplane%2520Mamba%26entry.906535625%3DYubo%2520Cui%2520and%2520Zhiheng%2520Li%2520and%2520Jiaqiang%2520Wang%2520and%2520Zheng%2520Fang%26entry.1292438233%3D%2520%2520Vision-based%25203D%2520occupancy%2520prediction%2520has%2520become%2520a%2520popular%2520research%2520task%2520due%250Ato%2520its%2520versatility%2520and%2520affordability.%2520Nowadays%252C%2520conventional%2520methods%2520usually%250Aproject%2520the%2520image-based%2520vision%2520features%2520to%25203D%2520space%2520and%2520learn%2520the%2520geometric%250Ainformation%2520through%2520the%2520attention%2520mechanism%252C%2520enabling%2520the%25203D%2520semantic%2520occupancy%250Aprediction.%2520However%252C%2520these%2520works%2520usually%2520face%2520two%2520main%2520challenges%253A%25201%2529%2520Limited%250Ageometric%2520information.%2520Due%2520to%2520the%2520lack%2520of%2520geometric%2520information%2520in%2520the%2520image%250Aitself%252C%2520it%2520is%2520challenging%2520to%2520directly%2520predict%25203D%2520space%2520information%252C%2520especially%250Ain%2520large-scale%2520outdoor%2520scenes.%25202%2529%2520Local%2520restricted%2520interaction.%2520Due%2520to%2520the%250Aquadratic%2520complexity%2520of%2520the%2520attention%2520mechanism%252C%2520they%2520often%2520use%2520modified%2520local%250Aattention%2520to%2520fuse%2520features%252C%2520resulting%2520in%2520a%2520restricted%2520fusion.%2520To%2520address%2520these%250Aproblems%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520language-assisted%25203D%2520semantic%2520occupancy%250Aprediction%2520network%252C%2520named%2520LOMA.%2520In%2520the%2520proposed%2520vision-language%2520framework%252C%2520we%250Afirst%2520introduce%2520a%2520VL-aware%2520Scene%2520Generator%2520%2528VSG%2529%2520module%2520to%2520generate%2520the%25203D%250Alanguage%2520feature%2520of%2520the%2520scene.%2520By%2520leveraging%2520the%2520vision-language%2520model%252C%2520this%250Amodule%2520provides%2520implicit%2520geometric%2520knowledge%2520and%2520explicit%2520semantic%2520information%250Afrom%2520the%2520language.%2520Furthermore%252C%2520we%2520present%2520a%2520Tri-plane%2520Fusion%2520Mamba%2520%2528TFM%2529%2520block%250Ato%2520efficiently%2520fuse%2520the%25203D%2520language%2520feature%2520and%25203D%2520vision%2520feature.%2520The%2520proposed%250Amodule%2520not%2520only%2520fuses%2520the%2520two%2520features%2520with%2520global%2520modeling%2520but%2520also%2520avoids%2520too%250Amuch%2520computation%2520costs.%2520Experiments%2520on%2520the%2520SemanticKITTI%2520and%2520SSCBench-KITTI360%250Adatasets%2520show%2520that%2520our%2520algorithm%2520achieves%2520new%2520state-of-the-art%2520performances%2520in%250Aboth%2520geometric%2520and%2520semantic%2520completion%2520tasks.%2520Our%2520code%2520will%2520be%2520open%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOMA%3A%20Language-assisted%20Semantic%20Occupancy%20Network%20via%20Triplane%20Mamba&entry.906535625=Yubo%20Cui%20and%20Zhiheng%20Li%20and%20Jiaqiang%20Wang%20and%20Zheng%20Fang&entry.1292438233=%20%20Vision-based%203D%20occupancy%20prediction%20has%20become%20a%20popular%20research%20task%20due%0Ato%20its%20versatility%20and%20affordability.%20Nowadays%2C%20conventional%20methods%20usually%0Aproject%20the%20image-based%20vision%20features%20to%203D%20space%20and%20learn%20the%20geometric%0Ainformation%20through%20the%20attention%20mechanism%2C%20enabling%20the%203D%20semantic%20occupancy%0Aprediction.%20However%2C%20these%20works%20usually%20face%20two%20main%20challenges%3A%201%29%20Limited%0Ageometric%20information.%20Due%20to%20the%20lack%20of%20geometric%20information%20in%20the%20image%0Aitself%2C%20it%20is%20challenging%20to%20directly%20predict%203D%20space%20information%2C%20especially%0Ain%20large-scale%20outdoor%20scenes.%202%29%20Local%20restricted%20interaction.%20Due%20to%20the%0Aquadratic%20complexity%20of%20the%20attention%20mechanism%2C%20they%20often%20use%20modified%20local%0Aattention%20to%20fuse%20features%2C%20resulting%20in%20a%20restricted%20fusion.%20To%20address%20these%0Aproblems%2C%20in%20this%20paper%2C%20we%20propose%20a%20language-assisted%203D%20semantic%20occupancy%0Aprediction%20network%2C%20named%20LOMA.%20In%20the%20proposed%20vision-language%20framework%2C%20we%0Afirst%20introduce%20a%20VL-aware%20Scene%20Generator%20%28VSG%29%20module%20to%20generate%20the%203D%0Alanguage%20feature%20of%20the%20scene.%20By%20leveraging%20the%20vision-language%20model%2C%20this%0Amodule%20provides%20implicit%20geometric%20knowledge%20and%20explicit%20semantic%20information%0Afrom%20the%20language.%20Furthermore%2C%20we%20present%20a%20Tri-plane%20Fusion%20Mamba%20%28TFM%29%20block%0Ato%20efficiently%20fuse%20the%203D%20language%20feature%20and%203D%20vision%20feature.%20The%20proposed%0Amodule%20not%20only%20fuses%20the%20two%20features%20with%20global%20modeling%20but%20also%20avoids%20too%0Amuch%20computation%20costs.%20Experiments%20on%20the%20SemanticKITTI%20and%20SSCBench-KITTI360%0Adatasets%20show%20that%20our%20algorithm%20achieves%20new%20state-of-the-art%20performances%20in%0Aboth%20geometric%20and%20semantic%20completion%20tasks.%20Our%20code%20will%20be%20open%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08388v1&entry.124074799=Read"},
{"title": "Towards Long Video Understanding via Fine-detailed Video Story\n  Generation", "author": "Zeng You and Zhiquan Wen and Yaofo Chen and Xin Li and Runhao Zeng and Yaowei Wang and Mingkui Tan", "abstract": "  Long video understanding has become a critical task in computer vision,\ndriving advancements across numerous applications from surveillance to content\nretrieval. Existing video understanding methods suffer from two challenges when\ndealing with long video understanding: intricate long-context relationship\nmodeling and interference from redundancy. To tackle these challenges, we\nintroduce Fine-Detailed Video Story generation (FDVS), which interprets long\nvideos into detailed textual representations. Specifically, to achieve\nfine-grained modeling of long-temporal content, we propose a Bottom-up Video\nInterpretation Mechanism that progressively interprets video content from clips\nto video. To avoid interference from redundant information in videos, we\nintroduce a Semantic Redundancy Reduction mechanism that removes redundancy at\nboth the visual and textual levels. Our method transforms long videos into\nhierarchical textual representations that contain multi-granularity information\nof the video. With these representations, FDVS is applicable to various tasks\nwithout any fine-tuning. We evaluate the proposed method across eight datasets\nspanning three tasks. The performance demonstrates the effectiveness and\nversatility of our method.\n", "link": "http://arxiv.org/abs/2412.06182v2", "date": "2024-12-11", "relevancy": 3.0125, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Long%20Video%20Understanding%20via%20Fine-detailed%20Video%20Story%0A%20%20Generation&body=Title%3A%20Towards%20Long%20Video%20Understanding%20via%20Fine-detailed%20Video%20Story%0A%20%20Generation%0AAuthor%3A%20Zeng%20You%20and%20Zhiquan%20Wen%20and%20Yaofo%20Chen%20and%20Xin%20Li%20and%20Runhao%20Zeng%20and%20Yaowei%20Wang%20and%20Mingkui%20Tan%0AAbstract%3A%20%20%20Long%20video%20understanding%20has%20become%20a%20critical%20task%20in%20computer%20vision%2C%0Adriving%20advancements%20across%20numerous%20applications%20from%20surveillance%20to%20content%0Aretrieval.%20Existing%20video%20understanding%20methods%20suffer%20from%20two%20challenges%20when%0Adealing%20with%20long%20video%20understanding%3A%20intricate%20long-context%20relationship%0Amodeling%20and%20interference%20from%20redundancy.%20To%20tackle%20these%20challenges%2C%20we%0Aintroduce%20Fine-Detailed%20Video%20Story%20generation%20%28FDVS%29%2C%20which%20interprets%20long%0Avideos%20into%20detailed%20textual%20representations.%20Specifically%2C%20to%20achieve%0Afine-grained%20modeling%20of%20long-temporal%20content%2C%20we%20propose%20a%20Bottom-up%20Video%0AInterpretation%20Mechanism%20that%20progressively%20interprets%20video%20content%20from%20clips%0Ato%20video.%20To%20avoid%20interference%20from%20redundant%20information%20in%20videos%2C%20we%0Aintroduce%20a%20Semantic%20Redundancy%20Reduction%20mechanism%20that%20removes%20redundancy%20at%0Aboth%20the%20visual%20and%20textual%20levels.%20Our%20method%20transforms%20long%20videos%20into%0Ahierarchical%20textual%20representations%20that%20contain%20multi-granularity%20information%0Aof%20the%20video.%20With%20these%20representations%2C%20FDVS%20is%20applicable%20to%20various%20tasks%0Awithout%20any%20fine-tuning.%20We%20evaluate%20the%20proposed%20method%20across%20eight%20datasets%0Aspanning%20three%20tasks.%20The%20performance%20demonstrates%20the%20effectiveness%20and%0Aversatility%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Long%2520Video%2520Understanding%2520via%2520Fine-detailed%2520Video%2520Story%250A%2520%2520Generation%26entry.906535625%3DZeng%2520You%2520and%2520Zhiquan%2520Wen%2520and%2520Yaofo%2520Chen%2520and%2520Xin%2520Li%2520and%2520Runhao%2520Zeng%2520and%2520Yaowei%2520Wang%2520and%2520Mingkui%2520Tan%26entry.1292438233%3D%2520%2520Long%2520video%2520understanding%2520has%2520become%2520a%2520critical%2520task%2520in%2520computer%2520vision%252C%250Adriving%2520advancements%2520across%2520numerous%2520applications%2520from%2520surveillance%2520to%2520content%250Aretrieval.%2520Existing%2520video%2520understanding%2520methods%2520suffer%2520from%2520two%2520challenges%2520when%250Adealing%2520with%2520long%2520video%2520understanding%253A%2520intricate%2520long-context%2520relationship%250Amodeling%2520and%2520interference%2520from%2520redundancy.%2520To%2520tackle%2520these%2520challenges%252C%2520we%250Aintroduce%2520Fine-Detailed%2520Video%2520Story%2520generation%2520%2528FDVS%2529%252C%2520which%2520interprets%2520long%250Avideos%2520into%2520detailed%2520textual%2520representations.%2520Specifically%252C%2520to%2520achieve%250Afine-grained%2520modeling%2520of%2520long-temporal%2520content%252C%2520we%2520propose%2520a%2520Bottom-up%2520Video%250AInterpretation%2520Mechanism%2520that%2520progressively%2520interprets%2520video%2520content%2520from%2520clips%250Ato%2520video.%2520To%2520avoid%2520interference%2520from%2520redundant%2520information%2520in%2520videos%252C%2520we%250Aintroduce%2520a%2520Semantic%2520Redundancy%2520Reduction%2520mechanism%2520that%2520removes%2520redundancy%2520at%250Aboth%2520the%2520visual%2520and%2520textual%2520levels.%2520Our%2520method%2520transforms%2520long%2520videos%2520into%250Ahierarchical%2520textual%2520representations%2520that%2520contain%2520multi-granularity%2520information%250Aof%2520the%2520video.%2520With%2520these%2520representations%252C%2520FDVS%2520is%2520applicable%2520to%2520various%2520tasks%250Awithout%2520any%2520fine-tuning.%2520We%2520evaluate%2520the%2520proposed%2520method%2520across%2520eight%2520datasets%250Aspanning%2520three%2520tasks.%2520The%2520performance%2520demonstrates%2520the%2520effectiveness%2520and%250Aversatility%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Long%20Video%20Understanding%20via%20Fine-detailed%20Video%20Story%0A%20%20Generation&entry.906535625=Zeng%20You%20and%20Zhiquan%20Wen%20and%20Yaofo%20Chen%20and%20Xin%20Li%20and%20Runhao%20Zeng%20and%20Yaowei%20Wang%20and%20Mingkui%20Tan&entry.1292438233=%20%20Long%20video%20understanding%20has%20become%20a%20critical%20task%20in%20computer%20vision%2C%0Adriving%20advancements%20across%20numerous%20applications%20from%20surveillance%20to%20content%0Aretrieval.%20Existing%20video%20understanding%20methods%20suffer%20from%20two%20challenges%20when%0Adealing%20with%20long%20video%20understanding%3A%20intricate%20long-context%20relationship%0Amodeling%20and%20interference%20from%20redundancy.%20To%20tackle%20these%20challenges%2C%20we%0Aintroduce%20Fine-Detailed%20Video%20Story%20generation%20%28FDVS%29%2C%20which%20interprets%20long%0Avideos%20into%20detailed%20textual%20representations.%20Specifically%2C%20to%20achieve%0Afine-grained%20modeling%20of%20long-temporal%20content%2C%20we%20propose%20a%20Bottom-up%20Video%0AInterpretation%20Mechanism%20that%20progressively%20interprets%20video%20content%20from%20clips%0Ato%20video.%20To%20avoid%20interference%20from%20redundant%20information%20in%20videos%2C%20we%0Aintroduce%20a%20Semantic%20Redundancy%20Reduction%20mechanism%20that%20removes%20redundancy%20at%0Aboth%20the%20visual%20and%20textual%20levels.%20Our%20method%20transforms%20long%20videos%20into%0Ahierarchical%20textual%20representations%20that%20contain%20multi-granularity%20information%0Aof%20the%20video.%20With%20these%20representations%2C%20FDVS%20is%20applicable%20to%20various%20tasks%0Awithout%20any%20fine-tuning.%20We%20evaluate%20the%20proposed%20method%20across%20eight%20datasets%0Aspanning%20three%20tasks.%20The%20performance%20demonstrates%20the%20effectiveness%20and%0Aversatility%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06182v2&entry.124074799=Read"},
{"title": "Reloc3r: Large-Scale Training of Relative Camera Pose Regression for\n  Generalizable, Fast, and Accurate Visual Localization", "author": "Siyan Dong and Shuzhe Wang and Shaohui Liu and Lulu Cai and Qingnan Fan and Juho Kannala and Yanchao Yang", "abstract": "  Visual localization aims to determine the camera pose of a query image\nrelative to a database of posed images. In recent years, deep neural networks\nthat directly regress camera poses have gained popularity due to their fast\ninference capabilities. However, existing methods struggle to either generalize\nwell to new scenes or provide accurate camera pose estimates. To address these\nissues, we present \\textbf{Reloc3r}, a simple yet effective visual localization\nframework. It consists of an elegantly designed relative pose regression\nnetwork, and a minimalist motion averaging module for absolute pose estimation.\nTrained on approximately 8 million posed image pairs, Reloc3r achieves\nsurprisingly good performance and generalization ability. We conduct extensive\nexperiments on 6 public datasets, consistently demonstrating the effectiveness\nand efficiency of the proposed method. It provides high-quality camera pose\nestimates in real time and generalizes to novel scenes. Code, weights, and data\nat: \\url{https://github.com/ffrivera0/reloc3r}.\n", "link": "http://arxiv.org/abs/2412.08376v1", "date": "2024-12-11", "relevancy": 2.9938, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.628}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5959}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reloc3r%3A%20Large-Scale%20Training%20of%20Relative%20Camera%20Pose%20Regression%20for%0A%20%20Generalizable%2C%20Fast%2C%20and%20Accurate%20Visual%20Localization&body=Title%3A%20Reloc3r%3A%20Large-Scale%20Training%20of%20Relative%20Camera%20Pose%20Regression%20for%0A%20%20Generalizable%2C%20Fast%2C%20and%20Accurate%20Visual%20Localization%0AAuthor%3A%20Siyan%20Dong%20and%20Shuzhe%20Wang%20and%20Shaohui%20Liu%20and%20Lulu%20Cai%20and%20Qingnan%20Fan%20and%20Juho%20Kannala%20and%20Yanchao%20Yang%0AAbstract%3A%20%20%20Visual%20localization%20aims%20to%20determine%20the%20camera%20pose%20of%20a%20query%20image%0Arelative%20to%20a%20database%20of%20posed%20images.%20In%20recent%20years%2C%20deep%20neural%20networks%0Athat%20directly%20regress%20camera%20poses%20have%20gained%20popularity%20due%20to%20their%20fast%0Ainference%20capabilities.%20However%2C%20existing%20methods%20struggle%20to%20either%20generalize%0Awell%20to%20new%20scenes%20or%20provide%20accurate%20camera%20pose%20estimates.%20To%20address%20these%0Aissues%2C%20we%20present%20%5Ctextbf%7BReloc3r%7D%2C%20a%20simple%20yet%20effective%20visual%20localization%0Aframework.%20It%20consists%20of%20an%20elegantly%20designed%20relative%20pose%20regression%0Anetwork%2C%20and%20a%20minimalist%20motion%20averaging%20module%20for%20absolute%20pose%20estimation.%0ATrained%20on%20approximately%208%20million%20posed%20image%20pairs%2C%20Reloc3r%20achieves%0Asurprisingly%20good%20performance%20and%20generalization%20ability.%20We%20conduct%20extensive%0Aexperiments%20on%206%20public%20datasets%2C%20consistently%20demonstrating%20the%20effectiveness%0Aand%20efficiency%20of%20the%20proposed%20method.%20It%20provides%20high-quality%20camera%20pose%0Aestimates%20in%20real%20time%20and%20generalizes%20to%20novel%20scenes.%20Code%2C%20weights%2C%20and%20data%0Aat%3A%20%5Curl%7Bhttps%3A//github.com/ffrivera0/reloc3r%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReloc3r%253A%2520Large-Scale%2520Training%2520of%2520Relative%2520Camera%2520Pose%2520Regression%2520for%250A%2520%2520Generalizable%252C%2520Fast%252C%2520and%2520Accurate%2520Visual%2520Localization%26entry.906535625%3DSiyan%2520Dong%2520and%2520Shuzhe%2520Wang%2520and%2520Shaohui%2520Liu%2520and%2520Lulu%2520Cai%2520and%2520Qingnan%2520Fan%2520and%2520Juho%2520Kannala%2520and%2520Yanchao%2520Yang%26entry.1292438233%3D%2520%2520Visual%2520localization%2520aims%2520to%2520determine%2520the%2520camera%2520pose%2520of%2520a%2520query%2520image%250Arelative%2520to%2520a%2520database%2520of%2520posed%2520images.%2520In%2520recent%2520years%252C%2520deep%2520neural%2520networks%250Athat%2520directly%2520regress%2520camera%2520poses%2520have%2520gained%2520popularity%2520due%2520to%2520their%2520fast%250Ainference%2520capabilities.%2520However%252C%2520existing%2520methods%2520struggle%2520to%2520either%2520generalize%250Awell%2520to%2520new%2520scenes%2520or%2520provide%2520accurate%2520camera%2520pose%2520estimates.%2520To%2520address%2520these%250Aissues%252C%2520we%2520present%2520%255Ctextbf%257BReloc3r%257D%252C%2520a%2520simple%2520yet%2520effective%2520visual%2520localization%250Aframework.%2520It%2520consists%2520of%2520an%2520elegantly%2520designed%2520relative%2520pose%2520regression%250Anetwork%252C%2520and%2520a%2520minimalist%2520motion%2520averaging%2520module%2520for%2520absolute%2520pose%2520estimation.%250ATrained%2520on%2520approximately%25208%2520million%2520posed%2520image%2520pairs%252C%2520Reloc3r%2520achieves%250Asurprisingly%2520good%2520performance%2520and%2520generalization%2520ability.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%25206%2520public%2520datasets%252C%2520consistently%2520demonstrating%2520the%2520effectiveness%250Aand%2520efficiency%2520of%2520the%2520proposed%2520method.%2520It%2520provides%2520high-quality%2520camera%2520pose%250Aestimates%2520in%2520real%2520time%2520and%2520generalizes%2520to%2520novel%2520scenes.%2520Code%252C%2520weights%252C%2520and%2520data%250Aat%253A%2520%255Curl%257Bhttps%253A//github.com/ffrivera0/reloc3r%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reloc3r%3A%20Large-Scale%20Training%20of%20Relative%20Camera%20Pose%20Regression%20for%0A%20%20Generalizable%2C%20Fast%2C%20and%20Accurate%20Visual%20Localization&entry.906535625=Siyan%20Dong%20and%20Shuzhe%20Wang%20and%20Shaohui%20Liu%20and%20Lulu%20Cai%20and%20Qingnan%20Fan%20and%20Juho%20Kannala%20and%20Yanchao%20Yang&entry.1292438233=%20%20Visual%20localization%20aims%20to%20determine%20the%20camera%20pose%20of%20a%20query%20image%0Arelative%20to%20a%20database%20of%20posed%20images.%20In%20recent%20years%2C%20deep%20neural%20networks%0Athat%20directly%20regress%20camera%20poses%20have%20gained%20popularity%20due%20to%20their%20fast%0Ainference%20capabilities.%20However%2C%20existing%20methods%20struggle%20to%20either%20generalize%0Awell%20to%20new%20scenes%20or%20provide%20accurate%20camera%20pose%20estimates.%20To%20address%20these%0Aissues%2C%20we%20present%20%5Ctextbf%7BReloc3r%7D%2C%20a%20simple%20yet%20effective%20visual%20localization%0Aframework.%20It%20consists%20of%20an%20elegantly%20designed%20relative%20pose%20regression%0Anetwork%2C%20and%20a%20minimalist%20motion%20averaging%20module%20for%20absolute%20pose%20estimation.%0ATrained%20on%20approximately%208%20million%20posed%20image%20pairs%2C%20Reloc3r%20achieves%0Asurprisingly%20good%20performance%20and%20generalization%20ability.%20We%20conduct%20extensive%0Aexperiments%20on%206%20public%20datasets%2C%20consistently%20demonstrating%20the%20effectiveness%0Aand%20efficiency%20of%20the%20proposed%20method.%20It%20provides%20high-quality%20camera%20pose%0Aestimates%20in%20real%20time%20and%20generalizes%20to%20novel%20scenes.%20Code%2C%20weights%2C%20and%20data%0Aat%3A%20%5Curl%7Bhttps%3A//github.com/ffrivera0/reloc3r%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08376v1&entry.124074799=Read"},
{"title": "Drift-free Visual SLAM using Digital Twins", "author": "Roxane Merat and Giovanni Cioffi and Leonard Bauersfeld and Davide Scaramuzza", "abstract": "  Globally-consistent localization in urban environments is crucial for\nautonomous systems such as self-driving vehicles and drones, as well as\nassistive technologies for visually impaired people. Traditional\nVisual-Inertial Odometry (VIO) and Visual Simultaneous Localization and Mapping\n(VSLAM) methods, though adequate for local pose estimation, suffer from drift\nin the long term due to reliance on local sensor data. While GPS counteracts\nthis drift, it is unavailable indoors and often unreliable in urban areas. An\nalternative is to localize the camera to an existing 3D map using\nvisual-feature matching. This can provide centimeter-level accurate\nlocalization but is limited by the visual similarities between the current view\nand the map. This paper introduces a novel approach that achieves accurate and\nglobally-consistent localization by aligning the sparse 3D point cloud\ngenerated by the VIO/VSLAM system to a digital twin using point-to-plane\nmatching; no visual data association is needed. The proposed method provides a\n6-DoF global measurement tightly integrated into the VIO/VSLAM system.\nExperiments run on a high-fidelity GPS simulator and real-world data collected\nfrom a drone demonstrate that our approach outperforms state-of-the-art VIO-GPS\nsystems and offers superior robustness against viewpoint changes compared to\nthe state-of-the-art Visual SLAM systems.\n", "link": "http://arxiv.org/abs/2412.08496v1", "date": "2024-12-11", "relevancy": 2.9807, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6037}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5976}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drift-free%20Visual%20SLAM%20using%20Digital%20Twins&body=Title%3A%20Drift-free%20Visual%20SLAM%20using%20Digital%20Twins%0AAuthor%3A%20Roxane%20Merat%20and%20Giovanni%20Cioffi%20and%20Leonard%20Bauersfeld%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Globally-consistent%20localization%20in%20urban%20environments%20is%20crucial%20for%0Aautonomous%20systems%20such%20as%20self-driving%20vehicles%20and%20drones%2C%20as%20well%20as%0Aassistive%20technologies%20for%20visually%20impaired%20people.%20Traditional%0AVisual-Inertial%20Odometry%20%28VIO%29%20and%20Visual%20Simultaneous%20Localization%20and%20Mapping%0A%28VSLAM%29%20methods%2C%20though%20adequate%20for%20local%20pose%20estimation%2C%20suffer%20from%20drift%0Ain%20the%20long%20term%20due%20to%20reliance%20on%20local%20sensor%20data.%20While%20GPS%20counteracts%0Athis%20drift%2C%20it%20is%20unavailable%20indoors%20and%20often%20unreliable%20in%20urban%20areas.%20An%0Aalternative%20is%20to%20localize%20the%20camera%20to%20an%20existing%203D%20map%20using%0Avisual-feature%20matching.%20This%20can%20provide%20centimeter-level%20accurate%0Alocalization%20but%20is%20limited%20by%20the%20visual%20similarities%20between%20the%20current%20view%0Aand%20the%20map.%20This%20paper%20introduces%20a%20novel%20approach%20that%20achieves%20accurate%20and%0Aglobally-consistent%20localization%20by%20aligning%20the%20sparse%203D%20point%20cloud%0Agenerated%20by%20the%20VIO/VSLAM%20system%20to%20a%20digital%20twin%20using%20point-to-plane%0Amatching%3B%20no%20visual%20data%20association%20is%20needed.%20The%20proposed%20method%20provides%20a%0A6-DoF%20global%20measurement%20tightly%20integrated%20into%20the%20VIO/VSLAM%20system.%0AExperiments%20run%20on%20a%20high-fidelity%20GPS%20simulator%20and%20real-world%20data%20collected%0Afrom%20a%20drone%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20VIO-GPS%0Asystems%20and%20offers%20superior%20robustness%20against%20viewpoint%20changes%20compared%20to%0Athe%20state-of-the-art%20Visual%20SLAM%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrift-free%2520Visual%2520SLAM%2520using%2520Digital%2520Twins%26entry.906535625%3DRoxane%2520Merat%2520and%2520Giovanni%2520Cioffi%2520and%2520Leonard%2520Bauersfeld%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Globally-consistent%2520localization%2520in%2520urban%2520environments%2520is%2520crucial%2520for%250Aautonomous%2520systems%2520such%2520as%2520self-driving%2520vehicles%2520and%2520drones%252C%2520as%2520well%2520as%250Aassistive%2520technologies%2520for%2520visually%2520impaired%2520people.%2520Traditional%250AVisual-Inertial%2520Odometry%2520%2528VIO%2529%2520and%2520Visual%2520Simultaneous%2520Localization%2520and%2520Mapping%250A%2528VSLAM%2529%2520methods%252C%2520though%2520adequate%2520for%2520local%2520pose%2520estimation%252C%2520suffer%2520from%2520drift%250Ain%2520the%2520long%2520term%2520due%2520to%2520reliance%2520on%2520local%2520sensor%2520data.%2520While%2520GPS%2520counteracts%250Athis%2520drift%252C%2520it%2520is%2520unavailable%2520indoors%2520and%2520often%2520unreliable%2520in%2520urban%2520areas.%2520An%250Aalternative%2520is%2520to%2520localize%2520the%2520camera%2520to%2520an%2520existing%25203D%2520map%2520using%250Avisual-feature%2520matching.%2520This%2520can%2520provide%2520centimeter-level%2520accurate%250Alocalization%2520but%2520is%2520limited%2520by%2520the%2520visual%2520similarities%2520between%2520the%2520current%2520view%250Aand%2520the%2520map.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520that%2520achieves%2520accurate%2520and%250Aglobally-consistent%2520localization%2520by%2520aligning%2520the%2520sparse%25203D%2520point%2520cloud%250Agenerated%2520by%2520the%2520VIO/VSLAM%2520system%2520to%2520a%2520digital%2520twin%2520using%2520point-to-plane%250Amatching%253B%2520no%2520visual%2520data%2520association%2520is%2520needed.%2520The%2520proposed%2520method%2520provides%2520a%250A6-DoF%2520global%2520measurement%2520tightly%2520integrated%2520into%2520the%2520VIO/VSLAM%2520system.%250AExperiments%2520run%2520on%2520a%2520high-fidelity%2520GPS%2520simulator%2520and%2520real-world%2520data%2520collected%250Afrom%2520a%2520drone%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%2520VIO-GPS%250Asystems%2520and%2520offers%2520superior%2520robustness%2520against%2520viewpoint%2520changes%2520compared%2520to%250Athe%2520state-of-the-art%2520Visual%2520SLAM%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drift-free%20Visual%20SLAM%20using%20Digital%20Twins&entry.906535625=Roxane%20Merat%20and%20Giovanni%20Cioffi%20and%20Leonard%20Bauersfeld%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Globally-consistent%20localization%20in%20urban%20environments%20is%20crucial%20for%0Aautonomous%20systems%20such%20as%20self-driving%20vehicles%20and%20drones%2C%20as%20well%20as%0Aassistive%20technologies%20for%20visually%20impaired%20people.%20Traditional%0AVisual-Inertial%20Odometry%20%28VIO%29%20and%20Visual%20Simultaneous%20Localization%20and%20Mapping%0A%28VSLAM%29%20methods%2C%20though%20adequate%20for%20local%20pose%20estimation%2C%20suffer%20from%20drift%0Ain%20the%20long%20term%20due%20to%20reliance%20on%20local%20sensor%20data.%20While%20GPS%20counteracts%0Athis%20drift%2C%20it%20is%20unavailable%20indoors%20and%20often%20unreliable%20in%20urban%20areas.%20An%0Aalternative%20is%20to%20localize%20the%20camera%20to%20an%20existing%203D%20map%20using%0Avisual-feature%20matching.%20This%20can%20provide%20centimeter-level%20accurate%0Alocalization%20but%20is%20limited%20by%20the%20visual%20similarities%20between%20the%20current%20view%0Aand%20the%20map.%20This%20paper%20introduces%20a%20novel%20approach%20that%20achieves%20accurate%20and%0Aglobally-consistent%20localization%20by%20aligning%20the%20sparse%203D%20point%20cloud%0Agenerated%20by%20the%20VIO/VSLAM%20system%20to%20a%20digital%20twin%20using%20point-to-plane%0Amatching%3B%20no%20visual%20data%20association%20is%20needed.%20The%20proposed%20method%20provides%20a%0A6-DoF%20global%20measurement%20tightly%20integrated%20into%20the%20VIO/VSLAM%20system.%0AExperiments%20run%20on%20a%20high-fidelity%20GPS%20simulator%20and%20real-world%20data%20collected%0Afrom%20a%20drone%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20VIO-GPS%0Asystems%20and%20offers%20superior%20robustness%20against%20viewpoint%20changes%20compared%20to%0Athe%20state-of-the-art%20Visual%20SLAM%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08496v1&entry.124074799=Read"},
{"title": "Position-aware Guided Point Cloud Completion with CLIP Model", "author": "Feng Zhou and Qi Zhang and Ju Dai and Lei Li and Qing Fan and Junliang Xing", "abstract": "  Point cloud completion aims to recover partial geometric and topological\nshapes caused by equipment defects or limited viewpoints. Current methods\neither solely rely on the 3D coordinates of the point cloud to complete it or\nincorporate additional images with well-calibrated intrinsic parameters to\nguide the geometric estimation of the missing parts. Although these methods\nhave achieved excellent performance by directly predicting the location of\ncomplete points, the extracted features lack fine-grained information regarding\nthe location of the missing area. To address this issue, we propose a rapid and\nefficient method to expand an unimodal framework into a multimodal framework.\nThis approach incorporates a position-aware module designed to enhance the\nspatial information of the missing parts through a weighted map learning\nmechanism. In addition, we establish a Point-Text-Image triplet corpus PCI-TI\nand MVP-TI based on the existing unimodal point cloud completion dataset and\nuse the pre-trained vision-language model CLIP to provide richer detail\ninformation for 3D shapes, thereby enhancing performance. Extensive\nquantitative and qualitative experiments demonstrate that our method\noutperforms state-of-the-art point cloud completion methods.\n", "link": "http://arxiv.org/abs/2412.08271v1", "date": "2024-12-11", "relevancy": 2.9747, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6082}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5918}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position-aware%20Guided%20Point%20Cloud%20Completion%20with%20CLIP%20Model&body=Title%3A%20Position-aware%20Guided%20Point%20Cloud%20Completion%20with%20CLIP%20Model%0AAuthor%3A%20Feng%20Zhou%20and%20Qi%20Zhang%20and%20Ju%20Dai%20and%20Lei%20Li%20and%20Qing%20Fan%20and%20Junliang%20Xing%0AAbstract%3A%20%20%20Point%20cloud%20completion%20aims%20to%20recover%20partial%20geometric%20and%20topological%0Ashapes%20caused%20by%20equipment%20defects%20or%20limited%20viewpoints.%20Current%20methods%0Aeither%20solely%20rely%20on%20the%203D%20coordinates%20of%20the%20point%20cloud%20to%20complete%20it%20or%0Aincorporate%20additional%20images%20with%20well-calibrated%20intrinsic%20parameters%20to%0Aguide%20the%20geometric%20estimation%20of%20the%20missing%20parts.%20Although%20these%20methods%0Ahave%20achieved%20excellent%20performance%20by%20directly%20predicting%20the%20location%20of%0Acomplete%20points%2C%20the%20extracted%20features%20lack%20fine-grained%20information%20regarding%0Athe%20location%20of%20the%20missing%20area.%20To%20address%20this%20issue%2C%20we%20propose%20a%20rapid%20and%0Aefficient%20method%20to%20expand%20an%20unimodal%20framework%20into%20a%20multimodal%20framework.%0AThis%20approach%20incorporates%20a%20position-aware%20module%20designed%20to%20enhance%20the%0Aspatial%20information%20of%20the%20missing%20parts%20through%20a%20weighted%20map%20learning%0Amechanism.%20In%20addition%2C%20we%20establish%20a%20Point-Text-Image%20triplet%20corpus%20PCI-TI%0Aand%20MVP-TI%20based%20on%20the%20existing%20unimodal%20point%20cloud%20completion%20dataset%20and%0Ause%20the%20pre-trained%20vision-language%20model%20CLIP%20to%20provide%20richer%20detail%0Ainformation%20for%203D%20shapes%2C%20thereby%20enhancing%20performance.%20Extensive%0Aquantitative%20and%20qualitative%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20point%20cloud%20completion%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition-aware%2520Guided%2520Point%2520Cloud%2520Completion%2520with%2520CLIP%2520Model%26entry.906535625%3DFeng%2520Zhou%2520and%2520Qi%2520Zhang%2520and%2520Ju%2520Dai%2520and%2520Lei%2520Li%2520and%2520Qing%2520Fan%2520and%2520Junliang%2520Xing%26entry.1292438233%3D%2520%2520Point%2520cloud%2520completion%2520aims%2520to%2520recover%2520partial%2520geometric%2520and%2520topological%250Ashapes%2520caused%2520by%2520equipment%2520defects%2520or%2520limited%2520viewpoints.%2520Current%2520methods%250Aeither%2520solely%2520rely%2520on%2520the%25203D%2520coordinates%2520of%2520the%2520point%2520cloud%2520to%2520complete%2520it%2520or%250Aincorporate%2520additional%2520images%2520with%2520well-calibrated%2520intrinsic%2520parameters%2520to%250Aguide%2520the%2520geometric%2520estimation%2520of%2520the%2520missing%2520parts.%2520Although%2520these%2520methods%250Ahave%2520achieved%2520excellent%2520performance%2520by%2520directly%2520predicting%2520the%2520location%2520of%250Acomplete%2520points%252C%2520the%2520extracted%2520features%2520lack%2520fine-grained%2520information%2520regarding%250Athe%2520location%2520of%2520the%2520missing%2520area.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520rapid%2520and%250Aefficient%2520method%2520to%2520expand%2520an%2520unimodal%2520framework%2520into%2520a%2520multimodal%2520framework.%250AThis%2520approach%2520incorporates%2520a%2520position-aware%2520module%2520designed%2520to%2520enhance%2520the%250Aspatial%2520information%2520of%2520the%2520missing%2520parts%2520through%2520a%2520weighted%2520map%2520learning%250Amechanism.%2520In%2520addition%252C%2520we%2520establish%2520a%2520Point-Text-Image%2520triplet%2520corpus%2520PCI-TI%250Aand%2520MVP-TI%2520based%2520on%2520the%2520existing%2520unimodal%2520point%2520cloud%2520completion%2520dataset%2520and%250Ause%2520the%2520pre-trained%2520vision-language%2520model%2520CLIP%2520to%2520provide%2520richer%2520detail%250Ainformation%2520for%25203D%2520shapes%252C%2520thereby%2520enhancing%2520performance.%2520Extensive%250Aquantitative%2520and%2520qualitative%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520state-of-the-art%2520point%2520cloud%2520completion%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position-aware%20Guided%20Point%20Cloud%20Completion%20with%20CLIP%20Model&entry.906535625=Feng%20Zhou%20and%20Qi%20Zhang%20and%20Ju%20Dai%20and%20Lei%20Li%20and%20Qing%20Fan%20and%20Junliang%20Xing&entry.1292438233=%20%20Point%20cloud%20completion%20aims%20to%20recover%20partial%20geometric%20and%20topological%0Ashapes%20caused%20by%20equipment%20defects%20or%20limited%20viewpoints.%20Current%20methods%0Aeither%20solely%20rely%20on%20the%203D%20coordinates%20of%20the%20point%20cloud%20to%20complete%20it%20or%0Aincorporate%20additional%20images%20with%20well-calibrated%20intrinsic%20parameters%20to%0Aguide%20the%20geometric%20estimation%20of%20the%20missing%20parts.%20Although%20these%20methods%0Ahave%20achieved%20excellent%20performance%20by%20directly%20predicting%20the%20location%20of%0Acomplete%20points%2C%20the%20extracted%20features%20lack%20fine-grained%20information%20regarding%0Athe%20location%20of%20the%20missing%20area.%20To%20address%20this%20issue%2C%20we%20propose%20a%20rapid%20and%0Aefficient%20method%20to%20expand%20an%20unimodal%20framework%20into%20a%20multimodal%20framework.%0AThis%20approach%20incorporates%20a%20position-aware%20module%20designed%20to%20enhance%20the%0Aspatial%20information%20of%20the%20missing%20parts%20through%20a%20weighted%20map%20learning%0Amechanism.%20In%20addition%2C%20we%20establish%20a%20Point-Text-Image%20triplet%20corpus%20PCI-TI%0Aand%20MVP-TI%20based%20on%20the%20existing%20unimodal%20point%20cloud%20completion%20dataset%20and%0Ause%20the%20pre-trained%20vision-language%20model%20CLIP%20to%20provide%20richer%20detail%0Ainformation%20for%203D%20shapes%2C%20thereby%20enhancing%20performance.%20Extensive%0Aquantitative%20and%20qualitative%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20point%20cloud%20completion%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08271v1&entry.124074799=Read"},
{"title": "RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied\n  Navigation", "author": "Mingfei Han and Liang Ma and Kamila Zhumakhanova and Ekaterina Radionova and Jingyi Zhang and Xiaojun Chang and Xiaodan Liang and Ivan Laptev", "abstract": "  Vision-and-Language Navigation (VLN) suffers from the limited diversity and\nscale of training data, primarily constrained by the manual curation of\nexisting simulators. To address this, we introduce RoomTour3D, a\nvideo-instruction dataset derived from web-based room tour videos that capture\nreal-world indoor spaces and human walking demonstrations. Unlike existing VLN\ndatasets, RoomTour3D leverages the scale and diversity of online videos to\ngenerate open-ended human walking trajectories and open-world navigable\ninstructions. To compensate for the lack of navigation data in online videos,\nwe perform 3D reconstruction and obtain 3D trajectories of walking paths\naugmented with additional information on the room types, object locations and\n3D shape of surrounding scenes. Our dataset includes $\\sim$100K open-ended\ndescription-enriched trajectories with $\\sim$200K instructions, and 17K\naction-enriched trajectories from 1847 room tour environments. We demonstrate\nexperimentally that RoomTour3D enables significant improvements across multiple\nVLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D\nfacilitates the development of trainable zero-shot VLN agents, showcasing the\npotential and challenges of advancing towards open-world navigation.\n", "link": "http://arxiv.org/abs/2412.08591v1", "date": "2024-12-11", "relevancy": 2.9308, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5908}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoomTour3D%3A%20Geometry-Aware%20Video-Instruction%20Tuning%20for%20Embodied%0A%20%20Navigation&body=Title%3A%20RoomTour3D%3A%20Geometry-Aware%20Video-Instruction%20Tuning%20for%20Embodied%0A%20%20Navigation%0AAuthor%3A%20Mingfei%20Han%20and%20Liang%20Ma%20and%20Kamila%20Zhumakhanova%20and%20Ekaterina%20Radionova%20and%20Jingyi%20Zhang%20and%20Xiaojun%20Chang%20and%20Xiaodan%20Liang%20and%20Ivan%20Laptev%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20suffers%20from%20the%20limited%20diversity%20and%0Ascale%20of%20training%20data%2C%20primarily%20constrained%20by%20the%20manual%20curation%20of%0Aexisting%20simulators.%20To%20address%20this%2C%20we%20introduce%20RoomTour3D%2C%20a%0Avideo-instruction%20dataset%20derived%20from%20web-based%20room%20tour%20videos%20that%20capture%0Areal-world%20indoor%20spaces%20and%20human%20walking%20demonstrations.%20Unlike%20existing%20VLN%0Adatasets%2C%20RoomTour3D%20leverages%20the%20scale%20and%20diversity%20of%20online%20videos%20to%0Agenerate%20open-ended%20human%20walking%20trajectories%20and%20open-world%20navigable%0Ainstructions.%20To%20compensate%20for%20the%20lack%20of%20navigation%20data%20in%20online%20videos%2C%0Awe%20perform%203D%20reconstruction%20and%20obtain%203D%20trajectories%20of%20walking%20paths%0Aaugmented%20with%20additional%20information%20on%20the%20room%20types%2C%20object%20locations%20and%0A3D%20shape%20of%20surrounding%20scenes.%20Our%20dataset%20includes%20%24%5Csim%24100K%20open-ended%0Adescription-enriched%20trajectories%20with%20%24%5Csim%24200K%20instructions%2C%20and%2017K%0Aaction-enriched%20trajectories%20from%201847%20room%20tour%20environments.%20We%20demonstrate%0Aexperimentally%20that%20RoomTour3D%20enables%20significant%20improvements%20across%20multiple%0AVLN%20tasks%20including%20CVDN%2C%20SOON%2C%20R2R%2C%20and%20REVERIE.%20Moreover%2C%20RoomTour3D%0Afacilitates%20the%20development%20of%20trainable%20zero-shot%20VLN%20agents%2C%20showcasing%20the%0Apotential%20and%20challenges%20of%20advancing%20towards%20open-world%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoomTour3D%253A%2520Geometry-Aware%2520Video-Instruction%2520Tuning%2520for%2520Embodied%250A%2520%2520Navigation%26entry.906535625%3DMingfei%2520Han%2520and%2520Liang%2520Ma%2520and%2520Kamila%2520Zhumakhanova%2520and%2520Ekaterina%2520Radionova%2520and%2520Jingyi%2520Zhang%2520and%2520Xiaojun%2520Chang%2520and%2520Xiaodan%2520Liang%2520and%2520Ivan%2520Laptev%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520suffers%2520from%2520the%2520limited%2520diversity%2520and%250Ascale%2520of%2520training%2520data%252C%2520primarily%2520constrained%2520by%2520the%2520manual%2520curation%2520of%250Aexisting%2520simulators.%2520To%2520address%2520this%252C%2520we%2520introduce%2520RoomTour3D%252C%2520a%250Avideo-instruction%2520dataset%2520derived%2520from%2520web-based%2520room%2520tour%2520videos%2520that%2520capture%250Areal-world%2520indoor%2520spaces%2520and%2520human%2520walking%2520demonstrations.%2520Unlike%2520existing%2520VLN%250Adatasets%252C%2520RoomTour3D%2520leverages%2520the%2520scale%2520and%2520diversity%2520of%2520online%2520videos%2520to%250Agenerate%2520open-ended%2520human%2520walking%2520trajectories%2520and%2520open-world%2520navigable%250Ainstructions.%2520To%2520compensate%2520for%2520the%2520lack%2520of%2520navigation%2520data%2520in%2520online%2520videos%252C%250Awe%2520perform%25203D%2520reconstruction%2520and%2520obtain%25203D%2520trajectories%2520of%2520walking%2520paths%250Aaugmented%2520with%2520additional%2520information%2520on%2520the%2520room%2520types%252C%2520object%2520locations%2520and%250A3D%2520shape%2520of%2520surrounding%2520scenes.%2520Our%2520dataset%2520includes%2520%2524%255Csim%2524100K%2520open-ended%250Adescription-enriched%2520trajectories%2520with%2520%2524%255Csim%2524200K%2520instructions%252C%2520and%252017K%250Aaction-enriched%2520trajectories%2520from%25201847%2520room%2520tour%2520environments.%2520We%2520demonstrate%250Aexperimentally%2520that%2520RoomTour3D%2520enables%2520significant%2520improvements%2520across%2520multiple%250AVLN%2520tasks%2520including%2520CVDN%252C%2520SOON%252C%2520R2R%252C%2520and%2520REVERIE.%2520Moreover%252C%2520RoomTour3D%250Afacilitates%2520the%2520development%2520of%2520trainable%2520zero-shot%2520VLN%2520agents%252C%2520showcasing%2520the%250Apotential%2520and%2520challenges%2520of%2520advancing%2520towards%2520open-world%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoomTour3D%3A%20Geometry-Aware%20Video-Instruction%20Tuning%20for%20Embodied%0A%20%20Navigation&entry.906535625=Mingfei%20Han%20and%20Liang%20Ma%20and%20Kamila%20Zhumakhanova%20and%20Ekaterina%20Radionova%20and%20Jingyi%20Zhang%20and%20Xiaojun%20Chang%20and%20Xiaodan%20Liang%20and%20Ivan%20Laptev&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20suffers%20from%20the%20limited%20diversity%20and%0Ascale%20of%20training%20data%2C%20primarily%20constrained%20by%20the%20manual%20curation%20of%0Aexisting%20simulators.%20To%20address%20this%2C%20we%20introduce%20RoomTour3D%2C%20a%0Avideo-instruction%20dataset%20derived%20from%20web-based%20room%20tour%20videos%20that%20capture%0Areal-world%20indoor%20spaces%20and%20human%20walking%20demonstrations.%20Unlike%20existing%20VLN%0Adatasets%2C%20RoomTour3D%20leverages%20the%20scale%20and%20diversity%20of%20online%20videos%20to%0Agenerate%20open-ended%20human%20walking%20trajectories%20and%20open-world%20navigable%0Ainstructions.%20To%20compensate%20for%20the%20lack%20of%20navigation%20data%20in%20online%20videos%2C%0Awe%20perform%203D%20reconstruction%20and%20obtain%203D%20trajectories%20of%20walking%20paths%0Aaugmented%20with%20additional%20information%20on%20the%20room%20types%2C%20object%20locations%20and%0A3D%20shape%20of%20surrounding%20scenes.%20Our%20dataset%20includes%20%24%5Csim%24100K%20open-ended%0Adescription-enriched%20trajectories%20with%20%24%5Csim%24200K%20instructions%2C%20and%2017K%0Aaction-enriched%20trajectories%20from%201847%20room%20tour%20environments.%20We%20demonstrate%0Aexperimentally%20that%20RoomTour3D%20enables%20significant%20improvements%20across%20multiple%0AVLN%20tasks%20including%20CVDN%2C%20SOON%2C%20R2R%2C%20and%20REVERIE.%20Moreover%2C%20RoomTour3D%0Afacilitates%20the%20development%20of%20trainable%20zero-shot%20VLN%20agents%2C%20showcasing%20the%0Apotential%20and%20challenges%20of%20advancing%20towards%20open-world%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08591v1&entry.124074799=Read"},
{"title": "BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation", "author": "Shengze Wang and Jiefeng Li and Tianye Li and Ye Yuan and Henry Fuchs and Koki Nagano and Shalini De Mello and Michael Stengel", "abstract": "  Single-image human mesh recovery is a challenging task due to the ill-posed\nnature of simultaneous body shape, pose, and camera estimation. Existing\nestimators work well on images taken from afar, but they break down as the\nperson moves close to the camera. Moreover, current methods fail to achieve\nboth accurate 3D pose and 2D alignment at the same time. Error is mainly\nintroduced by inaccurate perspective projection heuristically derived from\northographic parameters. To resolve this long-standing challenge, we present\nour method BLADE which accurately recovers perspective parameters from a single\nimage without heuristic assumptions. We start from the inverse relationship\nbetween perspective distortion and the person's Z-translation Tz, and we show\nthat Tz can be reliably estimated from the image. We then discuss the important\nrole of Tz for accurate human mesh recovery estimated from close-range images.\nFinally, we show that, once Tz and the 3D human mesh are estimated, one can\naccurately recover the focal length and full 3D translation. Extensive\nexperiments on standard benchmarks and real-world close-range images show that\nour method is the first to accurately recover projection parameters from a\nsingle image, and consequently attain state-of-the-art accuracy on 3D pose\nestimation and 2D alignment for a wide range of images.\nhttps://research.nvidia.com/labs/amri/projects/blade/\n", "link": "http://arxiv.org/abs/2412.08640v1", "date": "2024-12-11", "relevancy": 2.923, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.609}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLADE%3A%20Single-view%20Body%20Mesh%20Learning%20through%20Accurate%20Depth%20Estimation&body=Title%3A%20BLADE%3A%20Single-view%20Body%20Mesh%20Learning%20through%20Accurate%20Depth%20Estimation%0AAuthor%3A%20Shengze%20Wang%20and%20Jiefeng%20Li%20and%20Tianye%20Li%20and%20Ye%20Yuan%20and%20Henry%20Fuchs%20and%20Koki%20Nagano%20and%20Shalini%20De%20Mello%20and%20Michael%20Stengel%0AAbstract%3A%20%20%20Single-image%20human%20mesh%20recovery%20is%20a%20challenging%20task%20due%20to%20the%20ill-posed%0Anature%20of%20simultaneous%20body%20shape%2C%20pose%2C%20and%20camera%20estimation.%20Existing%0Aestimators%20work%20well%20on%20images%20taken%20from%20afar%2C%20but%20they%20break%20down%20as%20the%0Aperson%20moves%20close%20to%20the%20camera.%20Moreover%2C%20current%20methods%20fail%20to%20achieve%0Aboth%20accurate%203D%20pose%20and%202D%20alignment%20at%20the%20same%20time.%20Error%20is%20mainly%0Aintroduced%20by%20inaccurate%20perspective%20projection%20heuristically%20derived%20from%0Aorthographic%20parameters.%20To%20resolve%20this%20long-standing%20challenge%2C%20we%20present%0Aour%20method%20BLADE%20which%20accurately%20recovers%20perspective%20parameters%20from%20a%20single%0Aimage%20without%20heuristic%20assumptions.%20We%20start%20from%20the%20inverse%20relationship%0Abetween%20perspective%20distortion%20and%20the%20person%27s%20Z-translation%20Tz%2C%20and%20we%20show%0Athat%20Tz%20can%20be%20reliably%20estimated%20from%20the%20image.%20We%20then%20discuss%20the%20important%0Arole%20of%20Tz%20for%20accurate%20human%20mesh%20recovery%20estimated%20from%20close-range%20images.%0AFinally%2C%20we%20show%20that%2C%20once%20Tz%20and%20the%203D%20human%20mesh%20are%20estimated%2C%20one%20can%0Aaccurately%20recover%20the%20focal%20length%20and%20full%203D%20translation.%20Extensive%0Aexperiments%20on%20standard%20benchmarks%20and%20real-world%20close-range%20images%20show%20that%0Aour%20method%20is%20the%20first%20to%20accurately%20recover%20projection%20parameters%20from%20a%0Asingle%20image%2C%20and%20consequently%20attain%20state-of-the-art%20accuracy%20on%203D%20pose%0Aestimation%20and%202D%20alignment%20for%20a%20wide%20range%20of%20images.%0Ahttps%3A//research.nvidia.com/labs/amri/projects/blade/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLADE%253A%2520Single-view%2520Body%2520Mesh%2520Learning%2520through%2520Accurate%2520Depth%2520Estimation%26entry.906535625%3DShengze%2520Wang%2520and%2520Jiefeng%2520Li%2520and%2520Tianye%2520Li%2520and%2520Ye%2520Yuan%2520and%2520Henry%2520Fuchs%2520and%2520Koki%2520Nagano%2520and%2520Shalini%2520De%2520Mello%2520and%2520Michael%2520Stengel%26entry.1292438233%3D%2520%2520Single-image%2520human%2520mesh%2520recovery%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%2520ill-posed%250Anature%2520of%2520simultaneous%2520body%2520shape%252C%2520pose%252C%2520and%2520camera%2520estimation.%2520Existing%250Aestimators%2520work%2520well%2520on%2520images%2520taken%2520from%2520afar%252C%2520but%2520they%2520break%2520down%2520as%2520the%250Aperson%2520moves%2520close%2520to%2520the%2520camera.%2520Moreover%252C%2520current%2520methods%2520fail%2520to%2520achieve%250Aboth%2520accurate%25203D%2520pose%2520and%25202D%2520alignment%2520at%2520the%2520same%2520time.%2520Error%2520is%2520mainly%250Aintroduced%2520by%2520inaccurate%2520perspective%2520projection%2520heuristically%2520derived%2520from%250Aorthographic%2520parameters.%2520To%2520resolve%2520this%2520long-standing%2520challenge%252C%2520we%2520present%250Aour%2520method%2520BLADE%2520which%2520accurately%2520recovers%2520perspective%2520parameters%2520from%2520a%2520single%250Aimage%2520without%2520heuristic%2520assumptions.%2520We%2520start%2520from%2520the%2520inverse%2520relationship%250Abetween%2520perspective%2520distortion%2520and%2520the%2520person%2527s%2520Z-translation%2520Tz%252C%2520and%2520we%2520show%250Athat%2520Tz%2520can%2520be%2520reliably%2520estimated%2520from%2520the%2520image.%2520We%2520then%2520discuss%2520the%2520important%250Arole%2520of%2520Tz%2520for%2520accurate%2520human%2520mesh%2520recovery%2520estimated%2520from%2520close-range%2520images.%250AFinally%252C%2520we%2520show%2520that%252C%2520once%2520Tz%2520and%2520the%25203D%2520human%2520mesh%2520are%2520estimated%252C%2520one%2520can%250Aaccurately%2520recover%2520the%2520focal%2520length%2520and%2520full%25203D%2520translation.%2520Extensive%250Aexperiments%2520on%2520standard%2520benchmarks%2520and%2520real-world%2520close-range%2520images%2520show%2520that%250Aour%2520method%2520is%2520the%2520first%2520to%2520accurately%2520recover%2520projection%2520parameters%2520from%2520a%250Asingle%2520image%252C%2520and%2520consequently%2520attain%2520state-of-the-art%2520accuracy%2520on%25203D%2520pose%250Aestimation%2520and%25202D%2520alignment%2520for%2520a%2520wide%2520range%2520of%2520images.%250Ahttps%253A//research.nvidia.com/labs/amri/projects/blade/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLADE%3A%20Single-view%20Body%20Mesh%20Learning%20through%20Accurate%20Depth%20Estimation&entry.906535625=Shengze%20Wang%20and%20Jiefeng%20Li%20and%20Tianye%20Li%20and%20Ye%20Yuan%20and%20Henry%20Fuchs%20and%20Koki%20Nagano%20and%20Shalini%20De%20Mello%20and%20Michael%20Stengel&entry.1292438233=%20%20Single-image%20human%20mesh%20recovery%20is%20a%20challenging%20task%20due%20to%20the%20ill-posed%0Anature%20of%20simultaneous%20body%20shape%2C%20pose%2C%20and%20camera%20estimation.%20Existing%0Aestimators%20work%20well%20on%20images%20taken%20from%20afar%2C%20but%20they%20break%20down%20as%20the%0Aperson%20moves%20close%20to%20the%20camera.%20Moreover%2C%20current%20methods%20fail%20to%20achieve%0Aboth%20accurate%203D%20pose%20and%202D%20alignment%20at%20the%20same%20time.%20Error%20is%20mainly%0Aintroduced%20by%20inaccurate%20perspective%20projection%20heuristically%20derived%20from%0Aorthographic%20parameters.%20To%20resolve%20this%20long-standing%20challenge%2C%20we%20present%0Aour%20method%20BLADE%20which%20accurately%20recovers%20perspective%20parameters%20from%20a%20single%0Aimage%20without%20heuristic%20assumptions.%20We%20start%20from%20the%20inverse%20relationship%0Abetween%20perspective%20distortion%20and%20the%20person%27s%20Z-translation%20Tz%2C%20and%20we%20show%0Athat%20Tz%20can%20be%20reliably%20estimated%20from%20the%20image.%20We%20then%20discuss%20the%20important%0Arole%20of%20Tz%20for%20accurate%20human%20mesh%20recovery%20estimated%20from%20close-range%20images.%0AFinally%2C%20we%20show%20that%2C%20once%20Tz%20and%20the%203D%20human%20mesh%20are%20estimated%2C%20one%20can%0Aaccurately%20recover%20the%20focal%20length%20and%20full%203D%20translation.%20Extensive%0Aexperiments%20on%20standard%20benchmarks%20and%20real-world%20close-range%20images%20show%20that%0Aour%20method%20is%20the%20first%20to%20accurately%20recover%20projection%20parameters%20from%20a%0Asingle%20image%2C%20and%20consequently%20attain%20state-of-the-art%20accuracy%20on%203D%20pose%0Aestimation%20and%202D%20alignment%20for%20a%20wide%20range%20of%20images.%0Ahttps%3A//research.nvidia.com/labs/amri/projects/blade/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08640v1&entry.124074799=Read"},
{"title": "HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for\n  Vision-Language Models", "author": "Shiding Zhu and Wenhui Dong and Jun Song and Yanan Guo and Bo Zheng", "abstract": "  Recently, there has been growing interest in the capability of multimodal\nlarge language models (MLLMs) to process high-resolution images. A common\napproach currently involves dynamically cropping the original high-resolution\nimage into smaller sub-images, which are then fed into a vision encoder that\nwas pre-trained on lower-resolution images. However, this cropping approach\noften truncates objects and connected areas in the original image, causing\nsemantic breaks. To address this limitation, we introduce HyViLM, designed to\nprocess images of any resolution while retaining the overall context during\nencoding. Specifically, we: (i) Design a new visual encoder called Hybrid\nEncoder that not only encodes individual sub-images but also interacts with\ndetailed global visual features, significantly improving the model's ability to\nencode high-resolution images. (ii) Propose an optimal feature fusion strategy\nfor the dynamic cropping approach, effectively leveraging information from\ndifferent layers of the vision encoder. Compared with the state-of-the-art\nMLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out\nof ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance\non the TextVQA task and a 6.9% enhancement on the DocVQA task.\n", "link": "http://arxiv.org/abs/2412.08378v1", "date": "2024-12-11", "relevancy": 2.9097, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyViLM%3A%20Enhancing%20Fine-Grained%20Recognition%20with%20a%20Hybrid%20Encoder%20for%0A%20%20Vision-Language%20Models&body=Title%3A%20HyViLM%3A%20Enhancing%20Fine-Grained%20Recognition%20with%20a%20Hybrid%20Encoder%20for%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Shiding%20Zhu%20and%20Wenhui%20Dong%20and%20Jun%20Song%20and%20Yanan%20Guo%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20the%20capability%20of%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20to%20process%20high-resolution%20images.%20A%20common%0Aapproach%20currently%20involves%20dynamically%20cropping%20the%20original%20high-resolution%0Aimage%20into%20smaller%20sub-images%2C%20which%20are%20then%20fed%20into%20a%20vision%20encoder%20that%0Awas%20pre-trained%20on%20lower-resolution%20images.%20However%2C%20this%20cropping%20approach%0Aoften%20truncates%20objects%20and%20connected%20areas%20in%20the%20original%20image%2C%20causing%0Asemantic%20breaks.%20To%20address%20this%20limitation%2C%20we%20introduce%20HyViLM%2C%20designed%20to%0Aprocess%20images%20of%20any%20resolution%20while%20retaining%20the%20overall%20context%20during%0Aencoding.%20Specifically%2C%20we%3A%20%28i%29%20Design%20a%20new%20visual%20encoder%20called%20Hybrid%0AEncoder%20that%20not%20only%20encodes%20individual%20sub-images%20but%20also%20interacts%20with%0Adetailed%20global%20visual%20features%2C%20significantly%20improving%20the%20model%27s%20ability%20to%0Aencode%20high-resolution%20images.%20%28ii%29%20Propose%20an%20optimal%20feature%20fusion%20strategy%0Afor%20the%20dynamic%20cropping%20approach%2C%20effectively%20leveraging%20information%20from%0Adifferent%20layers%20of%20the%20vision%20encoder.%20Compared%20with%20the%20state-of-the-art%0AMLLMs%20under%20the%20same%20setting%2C%20our%20HyViLM%20outperforms%20existing%20MLLMs%20in%20nine%20out%0Aof%20ten%20tasks.%20Specifically%2C%20HyViLM%20achieves%20a%209.6%25%20improvement%20in%20performance%0Aon%20the%20TextVQA%20task%20and%20a%206.9%25%20enhancement%20on%20the%20DocVQA%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyViLM%253A%2520Enhancing%2520Fine-Grained%2520Recognition%2520with%2520a%2520Hybrid%2520Encoder%2520for%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DShiding%2520Zhu%2520and%2520Wenhui%2520Dong%2520and%2520Jun%2520Song%2520and%2520Yanan%2520Guo%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520growing%2520interest%2520in%2520the%2520capability%2520of%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520to%2520process%2520high-resolution%2520images.%2520A%2520common%250Aapproach%2520currently%2520involves%2520dynamically%2520cropping%2520the%2520original%2520high-resolution%250Aimage%2520into%2520smaller%2520sub-images%252C%2520which%2520are%2520then%2520fed%2520into%2520a%2520vision%2520encoder%2520that%250Awas%2520pre-trained%2520on%2520lower-resolution%2520images.%2520However%252C%2520this%2520cropping%2520approach%250Aoften%2520truncates%2520objects%2520and%2520connected%2520areas%2520in%2520the%2520original%2520image%252C%2520causing%250Asemantic%2520breaks.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520HyViLM%252C%2520designed%2520to%250Aprocess%2520images%2520of%2520any%2520resolution%2520while%2520retaining%2520the%2520overall%2520context%2520during%250Aencoding.%2520Specifically%252C%2520we%253A%2520%2528i%2529%2520Design%2520a%2520new%2520visual%2520encoder%2520called%2520Hybrid%250AEncoder%2520that%2520not%2520only%2520encodes%2520individual%2520sub-images%2520but%2520also%2520interacts%2520with%250Adetailed%2520global%2520visual%2520features%252C%2520significantly%2520improving%2520the%2520model%2527s%2520ability%2520to%250Aencode%2520high-resolution%2520images.%2520%2528ii%2529%2520Propose%2520an%2520optimal%2520feature%2520fusion%2520strategy%250Afor%2520the%2520dynamic%2520cropping%2520approach%252C%2520effectively%2520leveraging%2520information%2520from%250Adifferent%2520layers%2520of%2520the%2520vision%2520encoder.%2520Compared%2520with%2520the%2520state-of-the-art%250AMLLMs%2520under%2520the%2520same%2520setting%252C%2520our%2520HyViLM%2520outperforms%2520existing%2520MLLMs%2520in%2520nine%2520out%250Aof%2520ten%2520tasks.%2520Specifically%252C%2520HyViLM%2520achieves%2520a%25209.6%2525%2520improvement%2520in%2520performance%250Aon%2520the%2520TextVQA%2520task%2520and%2520a%25206.9%2525%2520enhancement%2520on%2520the%2520DocVQA%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyViLM%3A%20Enhancing%20Fine-Grained%20Recognition%20with%20a%20Hybrid%20Encoder%20for%0A%20%20Vision-Language%20Models&entry.906535625=Shiding%20Zhu%20and%20Wenhui%20Dong%20and%20Jun%20Song%20and%20Yanan%20Guo%20and%20Bo%20Zheng&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20the%20capability%20of%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20to%20process%20high-resolution%20images.%20A%20common%0Aapproach%20currently%20involves%20dynamically%20cropping%20the%20original%20high-resolution%0Aimage%20into%20smaller%20sub-images%2C%20which%20are%20then%20fed%20into%20a%20vision%20encoder%20that%0Awas%20pre-trained%20on%20lower-resolution%20images.%20However%2C%20this%20cropping%20approach%0Aoften%20truncates%20objects%20and%20connected%20areas%20in%20the%20original%20image%2C%20causing%0Asemantic%20breaks.%20To%20address%20this%20limitation%2C%20we%20introduce%20HyViLM%2C%20designed%20to%0Aprocess%20images%20of%20any%20resolution%20while%20retaining%20the%20overall%20context%20during%0Aencoding.%20Specifically%2C%20we%3A%20%28i%29%20Design%20a%20new%20visual%20encoder%20called%20Hybrid%0AEncoder%20that%20not%20only%20encodes%20individual%20sub-images%20but%20also%20interacts%20with%0Adetailed%20global%20visual%20features%2C%20significantly%20improving%20the%20model%27s%20ability%20to%0Aencode%20high-resolution%20images.%20%28ii%29%20Propose%20an%20optimal%20feature%20fusion%20strategy%0Afor%20the%20dynamic%20cropping%20approach%2C%20effectively%20leveraging%20information%20from%0Adifferent%20layers%20of%20the%20vision%20encoder.%20Compared%20with%20the%20state-of-the-art%0AMLLMs%20under%20the%20same%20setting%2C%20our%20HyViLM%20outperforms%20existing%20MLLMs%20in%20nine%20out%0Aof%20ten%20tasks.%20Specifically%2C%20HyViLM%20achieves%20a%209.6%25%20improvement%20in%20performance%0Aon%20the%20TextVQA%20task%20and%20a%206.9%25%20enhancement%20on%20the%20DocVQA%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08378v1&entry.124074799=Read"},
{"title": "DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays with\n  Generative Adversarial Networks", "author": "Zhaoxi Zhang and Yueliang Ying", "abstract": "  Computed tomography (CT) provides highly detailed three-dimensional (3D)\nmedical images but is costly, time-consuming, and often inaccessible in\nintraoperative settings (Organization et al. 2011). Recent advancements have\nexplored reconstructing 3D chest volumes from sparse 2D X-rays, such as\nsingle-view or orthogonal double-view images. However, current models tend to\nprocess 2D images in a planar manner, prioritizing visual realism over\nstructural accuracy. In this work, we introduce DuoLift Generative Adversarial\nNetworks (DuoLift-GAN), a novel architecture with dual branches that\nindependently elevate 2D images and their features into 3D representations.\nThese 3D outputs are merged into a unified 3D feature map and decoded into a\ncomplete 3D chest volume, enabling richer 3D information capture. We also\npresent a masked loss function that directs reconstruction towards critical\nanatomical regions, improving structural accuracy and visual quality. This\npaper demonstrates that DuoLift-GAN significantly enhances reconstruction\naccuracy while achieving superior visual realism compared to existing methods.\n", "link": "http://arxiv.org/abs/2411.07941v2", "date": "2024-12-11", "relevancy": 2.9046, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5882}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5882}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DuoLift-GAN%3AReconstructing%20CT%20from%20Single-view%20and%20Biplanar%20X-Rays%20with%0A%20%20Generative%20Adversarial%20Networks&body=Title%3A%20DuoLift-GAN%3AReconstructing%20CT%20from%20Single-view%20and%20Biplanar%20X-Rays%20with%0A%20%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Zhaoxi%20Zhang%20and%20Yueliang%20Ying%0AAbstract%3A%20%20%20Computed%20tomography%20%28CT%29%20provides%20highly%20detailed%20three-dimensional%20%283D%29%0Amedical%20images%20but%20is%20costly%2C%20time-consuming%2C%20and%20often%20inaccessible%20in%0Aintraoperative%20settings%20%28Organization%20et%20al.%202011%29.%20Recent%20advancements%20have%0Aexplored%20reconstructing%203D%20chest%20volumes%20from%20sparse%202D%20X-rays%2C%20such%20as%0Asingle-view%20or%20orthogonal%20double-view%20images.%20However%2C%20current%20models%20tend%20to%0Aprocess%202D%20images%20in%20a%20planar%20manner%2C%20prioritizing%20visual%20realism%20over%0Astructural%20accuracy.%20In%20this%20work%2C%20we%20introduce%20DuoLift%20Generative%20Adversarial%0ANetworks%20%28DuoLift-GAN%29%2C%20a%20novel%20architecture%20with%20dual%20branches%20that%0Aindependently%20elevate%202D%20images%20and%20their%20features%20into%203D%20representations.%0AThese%203D%20outputs%20are%20merged%20into%20a%20unified%203D%20feature%20map%20and%20decoded%20into%20a%0Acomplete%203D%20chest%20volume%2C%20enabling%20richer%203D%20information%20capture.%20We%20also%0Apresent%20a%20masked%20loss%20function%20that%20directs%20reconstruction%20towards%20critical%0Aanatomical%20regions%2C%20improving%20structural%20accuracy%20and%20visual%20quality.%20This%0Apaper%20demonstrates%20that%20DuoLift-GAN%20significantly%20enhances%20reconstruction%0Aaccuracy%20while%20achieving%20superior%20visual%20realism%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07941v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuoLift-GAN%253AReconstructing%2520CT%2520from%2520Single-view%2520and%2520Biplanar%2520X-Rays%2520with%250A%2520%2520Generative%2520Adversarial%2520Networks%26entry.906535625%3DZhaoxi%2520Zhang%2520and%2520Yueliang%2520Ying%26entry.1292438233%3D%2520%2520Computed%2520tomography%2520%2528CT%2529%2520provides%2520highly%2520detailed%2520three-dimensional%2520%25283D%2529%250Amedical%2520images%2520but%2520is%2520costly%252C%2520time-consuming%252C%2520and%2520often%2520inaccessible%2520in%250Aintraoperative%2520settings%2520%2528Organization%2520et%2520al.%25202011%2529.%2520Recent%2520advancements%2520have%250Aexplored%2520reconstructing%25203D%2520chest%2520volumes%2520from%2520sparse%25202D%2520X-rays%252C%2520such%2520as%250Asingle-view%2520or%2520orthogonal%2520double-view%2520images.%2520However%252C%2520current%2520models%2520tend%2520to%250Aprocess%25202D%2520images%2520in%2520a%2520planar%2520manner%252C%2520prioritizing%2520visual%2520realism%2520over%250Astructural%2520accuracy.%2520In%2520this%2520work%252C%2520we%2520introduce%2520DuoLift%2520Generative%2520Adversarial%250ANetworks%2520%2528DuoLift-GAN%2529%252C%2520a%2520novel%2520architecture%2520with%2520dual%2520branches%2520that%250Aindependently%2520elevate%25202D%2520images%2520and%2520their%2520features%2520into%25203D%2520representations.%250AThese%25203D%2520outputs%2520are%2520merged%2520into%2520a%2520unified%25203D%2520feature%2520map%2520and%2520decoded%2520into%2520a%250Acomplete%25203D%2520chest%2520volume%252C%2520enabling%2520richer%25203D%2520information%2520capture.%2520We%2520also%250Apresent%2520a%2520masked%2520loss%2520function%2520that%2520directs%2520reconstruction%2520towards%2520critical%250Aanatomical%2520regions%252C%2520improving%2520structural%2520accuracy%2520and%2520visual%2520quality.%2520This%250Apaper%2520demonstrates%2520that%2520DuoLift-GAN%2520significantly%2520enhances%2520reconstruction%250Aaccuracy%2520while%2520achieving%2520superior%2520visual%2520realism%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07941v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DuoLift-GAN%3AReconstructing%20CT%20from%20Single-view%20and%20Biplanar%20X-Rays%20with%0A%20%20Generative%20Adversarial%20Networks&entry.906535625=Zhaoxi%20Zhang%20and%20Yueliang%20Ying&entry.1292438233=%20%20Computed%20tomography%20%28CT%29%20provides%20highly%20detailed%20three-dimensional%20%283D%29%0Amedical%20images%20but%20is%20costly%2C%20time-consuming%2C%20and%20often%20inaccessible%20in%0Aintraoperative%20settings%20%28Organization%20et%20al.%202011%29.%20Recent%20advancements%20have%0Aexplored%20reconstructing%203D%20chest%20volumes%20from%20sparse%202D%20X-rays%2C%20such%20as%0Asingle-view%20or%20orthogonal%20double-view%20images.%20However%2C%20current%20models%20tend%20to%0Aprocess%202D%20images%20in%20a%20planar%20manner%2C%20prioritizing%20visual%20realism%20over%0Astructural%20accuracy.%20In%20this%20work%2C%20we%20introduce%20DuoLift%20Generative%20Adversarial%0ANetworks%20%28DuoLift-GAN%29%2C%20a%20novel%20architecture%20with%20dual%20branches%20that%0Aindependently%20elevate%202D%20images%20and%20their%20features%20into%203D%20representations.%0AThese%203D%20outputs%20are%20merged%20into%20a%20unified%203D%20feature%20map%20and%20decoded%20into%20a%0Acomplete%203D%20chest%20volume%2C%20enabling%20richer%203D%20information%20capture.%20We%20also%0Apresent%20a%20masked%20loss%20function%20that%20directs%20reconstruction%20towards%20critical%0Aanatomical%20regions%2C%20improving%20structural%20accuracy%20and%20visual%20quality.%20This%0Apaper%20demonstrates%20that%20DuoLift-GAN%20significantly%20enhances%20reconstruction%0Aaccuracy%20while%20achieving%20superior%20visual%20realism%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07941v2&entry.124074799=Read"},
{"title": "Embedding and Enriching Explicit Semantics for Visible-Infrared Person\n  Re-Identification", "author": "Neng Dong and Shuanglin Yan and Liyan Zhang and Jinhui Tang", "abstract": "  Visible-infrared person re-identification (VIReID) retrieves pedestrian\nimages with the same identity across different modalities. Existing methods\nlearn visual content solely from images, lacking the capability to sense\nhigh-level semantics. In this paper, we propose an Embedding and Enriching\nExplicit Semantics (EEES) framework to learn semantically rich cross-modality\npedestrian representations. Our method offers several contributions. First,\nwith the collaboration of multiple large language-vision models, we develop\nExplicit Semantics Embedding (ESE), which automatically supplements language\ndescriptions for pedestrians and aligns image-text pairs into a common space,\nthereby learning visual content associated with explicit semantics. Second,\nrecognizing the complementarity of multi-view information, we present\nCross-View Semantics Compensation (CVSC), which constructs multi-view\nimage-text pair representations, establishes their many-to-many matching, and\npropagates knowledge to single-view representations, thus compensating visual\ncontent with its missing cross-view semantics. Third, to eliminate noisy\nsemantics such as conflicting color attributes in different modalities, we\ndesign Cross-Modality Semantics Purification (CMSP), which constrains the\ndistance between inter-modality image-text pair representations to be close to\nthat between intra-modality image-text pair representations, further enhancing\nthe modality-invariance of visual content. Finally, experimental results\ndemonstrate the effectiveness and superiority of the proposed EEES.\n", "link": "http://arxiv.org/abs/2412.08406v1", "date": "2024-12-11", "relevancy": 2.8554, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5757}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20and%20Enriching%20Explicit%20Semantics%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification&body=Title%3A%20Embedding%20and%20Enriching%20Explicit%20Semantics%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Neng%20Dong%20and%20Shuanglin%20Yan%20and%20Liyan%20Zhang%20and%20Jinhui%20Tang%0AAbstract%3A%20%20%20Visible-infrared%20person%20re-identification%20%28VIReID%29%20retrieves%20pedestrian%0Aimages%20with%20the%20same%20identity%20across%20different%20modalities.%20Existing%20methods%0Alearn%20visual%20content%20solely%20from%20images%2C%20lacking%20the%20capability%20to%20sense%0Ahigh-level%20semantics.%20In%20this%20paper%2C%20we%20propose%20an%20Embedding%20and%20Enriching%0AExplicit%20Semantics%20%28EEES%29%20framework%20to%20learn%20semantically%20rich%20cross-modality%0Apedestrian%20representations.%20Our%20method%20offers%20several%20contributions.%20First%2C%0Awith%20the%20collaboration%20of%20multiple%20large%20language-vision%20models%2C%20we%20develop%0AExplicit%20Semantics%20Embedding%20%28ESE%29%2C%20which%20automatically%20supplements%20language%0Adescriptions%20for%20pedestrians%20and%20aligns%20image-text%20pairs%20into%20a%20common%20space%2C%0Athereby%20learning%20visual%20content%20associated%20with%20explicit%20semantics.%20Second%2C%0Arecognizing%20the%20complementarity%20of%20multi-view%20information%2C%20we%20present%0ACross-View%20Semantics%20Compensation%20%28CVSC%29%2C%20which%20constructs%20multi-view%0Aimage-text%20pair%20representations%2C%20establishes%20their%20many-to-many%20matching%2C%20and%0Apropagates%20knowledge%20to%20single-view%20representations%2C%20thus%20compensating%20visual%0Acontent%20with%20its%20missing%20cross-view%20semantics.%20Third%2C%20to%20eliminate%20noisy%0Asemantics%20such%20as%20conflicting%20color%20attributes%20in%20different%20modalities%2C%20we%0Adesign%20Cross-Modality%20Semantics%20Purification%20%28CMSP%29%2C%20which%20constrains%20the%0Adistance%20between%20inter-modality%20image-text%20pair%20representations%20to%20be%20close%20to%0Athat%20between%20intra-modality%20image-text%20pair%20representations%2C%20further%20enhancing%0Athe%20modality-invariance%20of%20visual%20content.%20Finally%2C%20experimental%20results%0Ademonstrate%20the%20effectiveness%20and%20superiority%20of%20the%20proposed%20EEES.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520and%2520Enriching%2520Explicit%2520Semantics%2520for%2520Visible-Infrared%2520Person%250A%2520%2520Re-Identification%26entry.906535625%3DNeng%2520Dong%2520and%2520Shuanglin%2520Yan%2520and%2520Liyan%2520Zhang%2520and%2520Jinhui%2520Tang%26entry.1292438233%3D%2520%2520Visible-infrared%2520person%2520re-identification%2520%2528VIReID%2529%2520retrieves%2520pedestrian%250Aimages%2520with%2520the%2520same%2520identity%2520across%2520different%2520modalities.%2520Existing%2520methods%250Alearn%2520visual%2520content%2520solely%2520from%2520images%252C%2520lacking%2520the%2520capability%2520to%2520sense%250Ahigh-level%2520semantics.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520Embedding%2520and%2520Enriching%250AExplicit%2520Semantics%2520%2528EEES%2529%2520framework%2520to%2520learn%2520semantically%2520rich%2520cross-modality%250Apedestrian%2520representations.%2520Our%2520method%2520offers%2520several%2520contributions.%2520First%252C%250Awith%2520the%2520collaboration%2520of%2520multiple%2520large%2520language-vision%2520models%252C%2520we%2520develop%250AExplicit%2520Semantics%2520Embedding%2520%2528ESE%2529%252C%2520which%2520automatically%2520supplements%2520language%250Adescriptions%2520for%2520pedestrians%2520and%2520aligns%2520image-text%2520pairs%2520into%2520a%2520common%2520space%252C%250Athereby%2520learning%2520visual%2520content%2520associated%2520with%2520explicit%2520semantics.%2520Second%252C%250Arecognizing%2520the%2520complementarity%2520of%2520multi-view%2520information%252C%2520we%2520present%250ACross-View%2520Semantics%2520Compensation%2520%2528CVSC%2529%252C%2520which%2520constructs%2520multi-view%250Aimage-text%2520pair%2520representations%252C%2520establishes%2520their%2520many-to-many%2520matching%252C%2520and%250Apropagates%2520knowledge%2520to%2520single-view%2520representations%252C%2520thus%2520compensating%2520visual%250Acontent%2520with%2520its%2520missing%2520cross-view%2520semantics.%2520Third%252C%2520to%2520eliminate%2520noisy%250Asemantics%2520such%2520as%2520conflicting%2520color%2520attributes%2520in%2520different%2520modalities%252C%2520we%250Adesign%2520Cross-Modality%2520Semantics%2520Purification%2520%2528CMSP%2529%252C%2520which%2520constrains%2520the%250Adistance%2520between%2520inter-modality%2520image-text%2520pair%2520representations%2520to%2520be%2520close%2520to%250Athat%2520between%2520intra-modality%2520image-text%2520pair%2520representations%252C%2520further%2520enhancing%250Athe%2520modality-invariance%2520of%2520visual%2520content.%2520Finally%252C%2520experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520the%2520proposed%2520EEES.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20and%20Enriching%20Explicit%20Semantics%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification&entry.906535625=Neng%20Dong%20and%20Shuanglin%20Yan%20and%20Liyan%20Zhang%20and%20Jinhui%20Tang&entry.1292438233=%20%20Visible-infrared%20person%20re-identification%20%28VIReID%29%20retrieves%20pedestrian%0Aimages%20with%20the%20same%20identity%20across%20different%20modalities.%20Existing%20methods%0Alearn%20visual%20content%20solely%20from%20images%2C%20lacking%20the%20capability%20to%20sense%0Ahigh-level%20semantics.%20In%20this%20paper%2C%20we%20propose%20an%20Embedding%20and%20Enriching%0AExplicit%20Semantics%20%28EEES%29%20framework%20to%20learn%20semantically%20rich%20cross-modality%0Apedestrian%20representations.%20Our%20method%20offers%20several%20contributions.%20First%2C%0Awith%20the%20collaboration%20of%20multiple%20large%20language-vision%20models%2C%20we%20develop%0AExplicit%20Semantics%20Embedding%20%28ESE%29%2C%20which%20automatically%20supplements%20language%0Adescriptions%20for%20pedestrians%20and%20aligns%20image-text%20pairs%20into%20a%20common%20space%2C%0Athereby%20learning%20visual%20content%20associated%20with%20explicit%20semantics.%20Second%2C%0Arecognizing%20the%20complementarity%20of%20multi-view%20information%2C%20we%20present%0ACross-View%20Semantics%20Compensation%20%28CVSC%29%2C%20which%20constructs%20multi-view%0Aimage-text%20pair%20representations%2C%20establishes%20their%20many-to-many%20matching%2C%20and%0Apropagates%20knowledge%20to%20single-view%20representations%2C%20thus%20compensating%20visual%0Acontent%20with%20its%20missing%20cross-view%20semantics.%20Third%2C%20to%20eliminate%20noisy%0Asemantics%20such%20as%20conflicting%20color%20attributes%20in%20different%20modalities%2C%20we%0Adesign%20Cross-Modality%20Semantics%20Purification%20%28CMSP%29%2C%20which%20constrains%20the%0Adistance%20between%20inter-modality%20image-text%20pair%20representations%20to%20be%20close%20to%0Athat%20between%20intra-modality%20image-text%20pair%20representations%2C%20further%20enhancing%0Athe%20modality-invariance%20of%20visual%20content.%20Finally%2C%20experimental%20results%0Ademonstrate%20the%20effectiveness%20and%20superiority%20of%20the%20proposed%20EEES.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08406v1&entry.124074799=Read"},
{"title": "Digging into Intrinsic Contextual Information for High-fidelity 3D Point\n  Cloud Completion", "author": "Jisheng Chu and Wenrui Li and Xingtao Wang and Kanglin Ning and Yidan Lu and Xiaopeng Fan", "abstract": "  The common occurrence of occlusion-induced incompleteness in point clouds has\nmade point cloud completion (PCC) a highly-concerned task in the field of\ngeometric processing. Existing PCC methods typically produce complete point\nclouds from partial point clouds in a coarse-to-fine paradigm, with the coarse\nstage generating entire shapes and the fine stage improving texture details.\nThough diffusion models have demonstrated effectiveness in the coarse stage,\nthe fine stage still faces challenges in producing high-fidelity results due to\nthe ill-posed nature of PCC. The intrinsic contextual information for texture\ndetails in partial point clouds is the key to solving the challenge. In this\npaper, we propose a high-fidelity PCC method that digs into both short and\nlong-range contextual information from the partial point cloud in the fine\nstage. Specifically, after generating the coarse point cloud via a\ndiffusion-based coarse generator, a mixed sampling module introduces\nshort-range contextual information from partial point clouds into the fine\nstage. A surface freezing modules safeguards points from noise-free partial\npoint clouds against disruption. As for the long-range contextual information,\nwe design a similarity modeling module to derive similarity with rigid\ntransformation invariance between points, conducting effective matching of\ngeometric manifold features globally. In this way, the high-quality components\npresent in the partial point cloud serve as valuable references for refining\nthe coarse point cloud with high fidelity. Extensive experiments have\ndemonstrated the superiority of the proposed method over SOTA competitors. Our\ncode is available at https://github.com/JS-CHU/ContextualCompletion.\n", "link": "http://arxiv.org/abs/2412.08326v1", "date": "2024-12-11", "relevancy": 2.8461, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5702}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5702}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digging%20into%20Intrinsic%20Contextual%20Information%20for%20High-fidelity%203D%20Point%0A%20%20Cloud%20Completion&body=Title%3A%20Digging%20into%20Intrinsic%20Contextual%20Information%20for%20High-fidelity%203D%20Point%0A%20%20Cloud%20Completion%0AAuthor%3A%20Jisheng%20Chu%20and%20Wenrui%20Li%20and%20Xingtao%20Wang%20and%20Kanglin%20Ning%20and%20Yidan%20Lu%20and%20Xiaopeng%20Fan%0AAbstract%3A%20%20%20The%20common%20occurrence%20of%20occlusion-induced%20incompleteness%20in%20point%20clouds%20has%0Amade%20point%20cloud%20completion%20%28PCC%29%20a%20highly-concerned%20task%20in%20the%20field%20of%0Ageometric%20processing.%20Existing%20PCC%20methods%20typically%20produce%20complete%20point%0Aclouds%20from%20partial%20point%20clouds%20in%20a%20coarse-to-fine%20paradigm%2C%20with%20the%20coarse%0Astage%20generating%20entire%20shapes%20and%20the%20fine%20stage%20improving%20texture%20details.%0AThough%20diffusion%20models%20have%20demonstrated%20effectiveness%20in%20the%20coarse%20stage%2C%0Athe%20fine%20stage%20still%20faces%20challenges%20in%20producing%20high-fidelity%20results%20due%20to%0Athe%20ill-posed%20nature%20of%20PCC.%20The%20intrinsic%20contextual%20information%20for%20texture%0Adetails%20in%20partial%20point%20clouds%20is%20the%20key%20to%20solving%20the%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20high-fidelity%20PCC%20method%20that%20digs%20into%20both%20short%20and%0Along-range%20contextual%20information%20from%20the%20partial%20point%20cloud%20in%20the%20fine%0Astage.%20Specifically%2C%20after%20generating%20the%20coarse%20point%20cloud%20via%20a%0Adiffusion-based%20coarse%20generator%2C%20a%20mixed%20sampling%20module%20introduces%0Ashort-range%20contextual%20information%20from%20partial%20point%20clouds%20into%20the%20fine%0Astage.%20A%20surface%20freezing%20modules%20safeguards%20points%20from%20noise-free%20partial%0Apoint%20clouds%20against%20disruption.%20As%20for%20the%20long-range%20contextual%20information%2C%0Awe%20design%20a%20similarity%20modeling%20module%20to%20derive%20similarity%20with%20rigid%0Atransformation%20invariance%20between%20points%2C%20conducting%20effective%20matching%20of%0Ageometric%20manifold%20features%20globally.%20In%20this%20way%2C%20the%20high-quality%20components%0Apresent%20in%20the%20partial%20point%20cloud%20serve%20as%20valuable%20references%20for%20refining%0Athe%20coarse%20point%20cloud%20with%20high%20fidelity.%20Extensive%20experiments%20have%0Ademonstrated%20the%20superiority%20of%20the%20proposed%20method%20over%20SOTA%20competitors.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/JS-CHU/ContextualCompletion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigging%2520into%2520Intrinsic%2520Contextual%2520Information%2520for%2520High-fidelity%25203D%2520Point%250A%2520%2520Cloud%2520Completion%26entry.906535625%3DJisheng%2520Chu%2520and%2520Wenrui%2520Li%2520and%2520Xingtao%2520Wang%2520and%2520Kanglin%2520Ning%2520and%2520Yidan%2520Lu%2520and%2520Xiaopeng%2520Fan%26entry.1292438233%3D%2520%2520The%2520common%2520occurrence%2520of%2520occlusion-induced%2520incompleteness%2520in%2520point%2520clouds%2520has%250Amade%2520point%2520cloud%2520completion%2520%2528PCC%2529%2520a%2520highly-concerned%2520task%2520in%2520the%2520field%2520of%250Ageometric%2520processing.%2520Existing%2520PCC%2520methods%2520typically%2520produce%2520complete%2520point%250Aclouds%2520from%2520partial%2520point%2520clouds%2520in%2520a%2520coarse-to-fine%2520paradigm%252C%2520with%2520the%2520coarse%250Astage%2520generating%2520entire%2520shapes%2520and%2520the%2520fine%2520stage%2520improving%2520texture%2520details.%250AThough%2520diffusion%2520models%2520have%2520demonstrated%2520effectiveness%2520in%2520the%2520coarse%2520stage%252C%250Athe%2520fine%2520stage%2520still%2520faces%2520challenges%2520in%2520producing%2520high-fidelity%2520results%2520due%2520to%250Athe%2520ill-posed%2520nature%2520of%2520PCC.%2520The%2520intrinsic%2520contextual%2520information%2520for%2520texture%250Adetails%2520in%2520partial%2520point%2520clouds%2520is%2520the%2520key%2520to%2520solving%2520the%2520challenge.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520high-fidelity%2520PCC%2520method%2520that%2520digs%2520into%2520both%2520short%2520and%250Along-range%2520contextual%2520information%2520from%2520the%2520partial%2520point%2520cloud%2520in%2520the%2520fine%250Astage.%2520Specifically%252C%2520after%2520generating%2520the%2520coarse%2520point%2520cloud%2520via%2520a%250Adiffusion-based%2520coarse%2520generator%252C%2520a%2520mixed%2520sampling%2520module%2520introduces%250Ashort-range%2520contextual%2520information%2520from%2520partial%2520point%2520clouds%2520into%2520the%2520fine%250Astage.%2520A%2520surface%2520freezing%2520modules%2520safeguards%2520points%2520from%2520noise-free%2520partial%250Apoint%2520clouds%2520against%2520disruption.%2520As%2520for%2520the%2520long-range%2520contextual%2520information%252C%250Awe%2520design%2520a%2520similarity%2520modeling%2520module%2520to%2520derive%2520similarity%2520with%2520rigid%250Atransformation%2520invariance%2520between%2520points%252C%2520conducting%2520effective%2520matching%2520of%250Ageometric%2520manifold%2520features%2520globally.%2520In%2520this%2520way%252C%2520the%2520high-quality%2520components%250Apresent%2520in%2520the%2520partial%2520point%2520cloud%2520serve%2520as%2520valuable%2520references%2520for%2520refining%250Athe%2520coarse%2520point%2520cloud%2520with%2520high%2520fidelity.%2520Extensive%2520experiments%2520have%250Ademonstrated%2520the%2520superiority%2520of%2520the%2520proposed%2520method%2520over%2520SOTA%2520competitors.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/JS-CHU/ContextualCompletion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digging%20into%20Intrinsic%20Contextual%20Information%20for%20High-fidelity%203D%20Point%0A%20%20Cloud%20Completion&entry.906535625=Jisheng%20Chu%20and%20Wenrui%20Li%20and%20Xingtao%20Wang%20and%20Kanglin%20Ning%20and%20Yidan%20Lu%20and%20Xiaopeng%20Fan&entry.1292438233=%20%20The%20common%20occurrence%20of%20occlusion-induced%20incompleteness%20in%20point%20clouds%20has%0Amade%20point%20cloud%20completion%20%28PCC%29%20a%20highly-concerned%20task%20in%20the%20field%20of%0Ageometric%20processing.%20Existing%20PCC%20methods%20typically%20produce%20complete%20point%0Aclouds%20from%20partial%20point%20clouds%20in%20a%20coarse-to-fine%20paradigm%2C%20with%20the%20coarse%0Astage%20generating%20entire%20shapes%20and%20the%20fine%20stage%20improving%20texture%20details.%0AThough%20diffusion%20models%20have%20demonstrated%20effectiveness%20in%20the%20coarse%20stage%2C%0Athe%20fine%20stage%20still%20faces%20challenges%20in%20producing%20high-fidelity%20results%20due%20to%0Athe%20ill-posed%20nature%20of%20PCC.%20The%20intrinsic%20contextual%20information%20for%20texture%0Adetails%20in%20partial%20point%20clouds%20is%20the%20key%20to%20solving%20the%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20high-fidelity%20PCC%20method%20that%20digs%20into%20both%20short%20and%0Along-range%20contextual%20information%20from%20the%20partial%20point%20cloud%20in%20the%20fine%0Astage.%20Specifically%2C%20after%20generating%20the%20coarse%20point%20cloud%20via%20a%0Adiffusion-based%20coarse%20generator%2C%20a%20mixed%20sampling%20module%20introduces%0Ashort-range%20contextual%20information%20from%20partial%20point%20clouds%20into%20the%20fine%0Astage.%20A%20surface%20freezing%20modules%20safeguards%20points%20from%20noise-free%20partial%0Apoint%20clouds%20against%20disruption.%20As%20for%20the%20long-range%20contextual%20information%2C%0Awe%20design%20a%20similarity%20modeling%20module%20to%20derive%20similarity%20with%20rigid%0Atransformation%20invariance%20between%20points%2C%20conducting%20effective%20matching%20of%0Ageometric%20manifold%20features%20globally.%20In%20this%20way%2C%20the%20high-quality%20components%0Apresent%20in%20the%20partial%20point%20cloud%20serve%20as%20valuable%20references%20for%20refining%0Athe%20coarse%20point%20cloud%20with%20high%20fidelity.%20Extensive%20experiments%20have%0Ademonstrated%20the%20superiority%20of%20the%20proposed%20method%20over%20SOTA%20competitors.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/JS-CHU/ContextualCompletion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08326v1&entry.124074799=Read"},
{"title": "Recoverable Compression: A Multimodal Vision Token Recovery Mechanism\n  Guided by Text Information", "author": "Yi Chen and Jian Xu and Xu-Yao Zhang and Wen-Zhuo Liu and Yang-Yang Liu and Cheng-Lin Liu", "abstract": "  With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance.\n", "link": "http://arxiv.org/abs/2409.01179v2", "date": "2024-12-11", "relevancy": 2.8275, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recoverable%20Compression%3A%20A%20Multimodal%20Vision%20Token%20Recovery%20Mechanism%0A%20%20Guided%20by%20Text%20Information&body=Title%3A%20Recoverable%20Compression%3A%20A%20Multimodal%20Vision%20Token%20Recovery%20Mechanism%0A%20%20Guided%20by%20Text%20Information%0AAuthor%3A%20Yi%20Chen%20and%20Jian%20Xu%20and%20Xu-Yao%20Zhang%20and%20Wen-Zhuo%20Liu%20and%20Yang-Yang%20Liu%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20large-scale%20language%20modeling%20techniques%2C%20large%0Amultimodal%20models%20combining%20visual%20encoders%20with%20large%20language%20models%20have%0Ademonstrated%20exceptional%20performance%20in%20various%20visual%20tasks.%20Most%20of%20the%0Acurrent%20large-scale%20multimodal%20models%20achieve%20this%20by%20mapping%20visual%20features%0Aobtained%20from%20the%20visual%20encoder%20into%20a%20large%20language%20model%20and%20using%20them%20as%0Ainputs%20alongside%20text%20for%20downstream%20tasks.%20Therefore%2C%20the%20number%20of%20visual%0Atokens%20directly%20affects%20the%20training%20and%20inference%20speed%20of%20the%20model.%20There%0Ahas%20been%20significant%20work%20on%20token%20pruning%20for%20visual%20transformers%2C%20but%20for%0Alarge%20multimodal%20models%2C%20only%20relying%20on%20visual%20information%20for%20token%20pruning%0Aor%20compression%20may%20lead%20to%20significant%20loss%20of%20important%20information.%20On%20the%0Aother%20hand%2C%20the%20textual%20input%20in%20the%20form%20of%20a%20question%20may%20contain%20valuable%0Ainformation%20that%20can%20aid%20in%20answering%20the%20question%2C%20providing%20additional%0Aknowledge%20to%20the%20model.%20To%20address%20the%20potential%20oversimplification%20and%0Aexcessive%20pruning%20that%20can%20occur%20with%20most%20purely%20visual%20token%20pruning%20methods%2C%0Awe%20propose%20a%20text%20information-guided%20dynamic%20visual%20token%20recovery%20mechanism%0Athat%20does%20not%20require%20training.%20This%20mechanism%20leverages%20the%20similarity%20between%0Athe%20question%20text%20and%20visual%20tokens%20to%20recover%20visually%20meaningful%20tokens%20with%0Aimportant%20text%20information%20while%20merging%20other%20less%20important%20tokens.%0AExperimental%20results%20demonstrate%20that%20our%20proposed%20method%20achieves%20comparable%0Aperformance%20to%20the%20original%20approach%20while%20compressing%20the%20visual%20tokens%20to%20an%0Aaverage%20of%2010%25%20of%20the%20original%20quantity.%20Our%20source%20code%20will%20be%20made%20publicly%0Aavailable%20following%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecoverable%2520Compression%253A%2520A%2520Multimodal%2520Vision%2520Token%2520Recovery%2520Mechanism%250A%2520%2520Guided%2520by%2520Text%2520Information%26entry.906535625%3DYi%2520Chen%2520and%2520Jian%2520Xu%2520and%2520Xu-Yao%2520Zhang%2520and%2520Wen-Zhuo%2520Liu%2520and%2520Yang-Yang%2520Liu%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520large-scale%2520language%2520modeling%2520techniques%252C%2520large%250Amultimodal%2520models%2520combining%2520visual%2520encoders%2520with%2520large%2520language%2520models%2520have%250Ademonstrated%2520exceptional%2520performance%2520in%2520various%2520visual%2520tasks.%2520Most%2520of%2520the%250Acurrent%2520large-scale%2520multimodal%2520models%2520achieve%2520this%2520by%2520mapping%2520visual%2520features%250Aobtained%2520from%2520the%2520visual%2520encoder%2520into%2520a%2520large%2520language%2520model%2520and%2520using%2520them%2520as%250Ainputs%2520alongside%2520text%2520for%2520downstream%2520tasks.%2520Therefore%252C%2520the%2520number%2520of%2520visual%250Atokens%2520directly%2520affects%2520the%2520training%2520and%2520inference%2520speed%2520of%2520the%2520model.%2520There%250Ahas%2520been%2520significant%2520work%2520on%2520token%2520pruning%2520for%2520visual%2520transformers%252C%2520but%2520for%250Alarge%2520multimodal%2520models%252C%2520only%2520relying%2520on%2520visual%2520information%2520for%2520token%2520pruning%250Aor%2520compression%2520may%2520lead%2520to%2520significant%2520loss%2520of%2520important%2520information.%2520On%2520the%250Aother%2520hand%252C%2520the%2520textual%2520input%2520in%2520the%2520form%2520of%2520a%2520question%2520may%2520contain%2520valuable%250Ainformation%2520that%2520can%2520aid%2520in%2520answering%2520the%2520question%252C%2520providing%2520additional%250Aknowledge%2520to%2520the%2520model.%2520To%2520address%2520the%2520potential%2520oversimplification%2520and%250Aexcessive%2520pruning%2520that%2520can%2520occur%2520with%2520most%2520purely%2520visual%2520token%2520pruning%2520methods%252C%250Awe%2520propose%2520a%2520text%2520information-guided%2520dynamic%2520visual%2520token%2520recovery%2520mechanism%250Athat%2520does%2520not%2520require%2520training.%2520This%2520mechanism%2520leverages%2520the%2520similarity%2520between%250Athe%2520question%2520text%2520and%2520visual%2520tokens%2520to%2520recover%2520visually%2520meaningful%2520tokens%2520with%250Aimportant%2520text%2520information%2520while%2520merging%2520other%2520less%2520important%2520tokens.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520method%2520achieves%2520comparable%250Aperformance%2520to%2520the%2520original%2520approach%2520while%2520compressing%2520the%2520visual%2520tokens%2520to%2520an%250Aaverage%2520of%252010%2525%2520of%2520the%2520original%2520quantity.%2520Our%2520source%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520following%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recoverable%20Compression%3A%20A%20Multimodal%20Vision%20Token%20Recovery%20Mechanism%0A%20%20Guided%20by%20Text%20Information&entry.906535625=Yi%20Chen%20and%20Jian%20Xu%20and%20Xu-Yao%20Zhang%20and%20Wen-Zhuo%20Liu%20and%20Yang-Yang%20Liu%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20With%20the%20advancement%20of%20large-scale%20language%20modeling%20techniques%2C%20large%0Amultimodal%20models%20combining%20visual%20encoders%20with%20large%20language%20models%20have%0Ademonstrated%20exceptional%20performance%20in%20various%20visual%20tasks.%20Most%20of%20the%0Acurrent%20large-scale%20multimodal%20models%20achieve%20this%20by%20mapping%20visual%20features%0Aobtained%20from%20the%20visual%20encoder%20into%20a%20large%20language%20model%20and%20using%20them%20as%0Ainputs%20alongside%20text%20for%20downstream%20tasks.%20Therefore%2C%20the%20number%20of%20visual%0Atokens%20directly%20affects%20the%20training%20and%20inference%20speed%20of%20the%20model.%20There%0Ahas%20been%20significant%20work%20on%20token%20pruning%20for%20visual%20transformers%2C%20but%20for%0Alarge%20multimodal%20models%2C%20only%20relying%20on%20visual%20information%20for%20token%20pruning%0Aor%20compression%20may%20lead%20to%20significant%20loss%20of%20important%20information.%20On%20the%0Aother%20hand%2C%20the%20textual%20input%20in%20the%20form%20of%20a%20question%20may%20contain%20valuable%0Ainformation%20that%20can%20aid%20in%20answering%20the%20question%2C%20providing%20additional%0Aknowledge%20to%20the%20model.%20To%20address%20the%20potential%20oversimplification%20and%0Aexcessive%20pruning%20that%20can%20occur%20with%20most%20purely%20visual%20token%20pruning%20methods%2C%0Awe%20propose%20a%20text%20information-guided%20dynamic%20visual%20token%20recovery%20mechanism%0Athat%20does%20not%20require%20training.%20This%20mechanism%20leverages%20the%20similarity%20between%0Athe%20question%20text%20and%20visual%20tokens%20to%20recover%20visually%20meaningful%20tokens%20with%0Aimportant%20text%20information%20while%20merging%20other%20less%20important%20tokens.%0AExperimental%20results%20demonstrate%20that%20our%20proposed%20method%20achieves%20comparable%0Aperformance%20to%20the%20original%20approach%20while%20compressing%20the%20visual%20tokens%20to%20an%0Aaverage%20of%2010%25%20of%20the%20original%20quantity.%20Our%20source%20code%20will%20be%20made%20publicly%0Aavailable%20following%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01179v2&entry.124074799=Read"},
{"title": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos", "author": "Lishuai Gao and Yujie Zhong and Yingsen Zeng and Haoxian Tan and Dengjie Li and Zheng Zhao", "abstract": "  Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding.\n", "link": "http://arxiv.org/abs/2412.05185v2", "date": "2024-12-11", "relevancy": 2.8145, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LinVT%3A%20Empower%20Your%20Image-level%20Large%20Language%20Model%20to%20Understand%0A%20%20Videos&body=Title%3A%20LinVT%3A%20Empower%20Your%20Image-level%20Large%20Language%20Model%20to%20Understand%0A%20%20Videos%0AAuthor%3A%20Lishuai%20Gao%20and%20Yujie%20Zhong%20and%20Yingsen%20Zeng%20and%20Haoxian%20Tan%20and%20Dengjie%20Li%20and%20Zheng%20Zhao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20widely%20used%20in%20various%20tasks%2C%0Amotivating%20us%20to%20develop%20an%20LLM-based%20assistant%20for%20videos.%20Instead%20of%20training%0Afrom%20scratch%2C%20we%20propose%20a%20module%20to%20transform%20arbitrary%20well-trained%0Aimage-based%20LLMs%20into%20video-LLMs%20%28after%20being%20trained%20on%20video%20data%29.%20To%20better%0Aadapt%20image-LLMs%20for%20processing%20videos%2C%20we%20introduce%20two%20design%20principles%3A%0Alinear%20transformation%20to%20preserve%20the%20original%20visual-language%20alignment%20and%0Arepresentative%20information%20condensation%20from%20redundant%20video%20content.%20Guided%20by%0Athese%20principles%2C%20we%20propose%20a%20plug-and-play%20Linear%20Video%20Tokenizer%28LinVT%29%2C%0Awhich%20enables%20existing%20image-LLMs%20to%20understand%20videos.%20We%20benchmark%20LinVT%20with%0Asix%20recent%20visual%20LLMs%3A%20Aquila%2C%20Blip-3%2C%20InternVL2%2C%20Mipha%2C%20Molmo%20and%20Qwen2-VL%2C%0Ashowcasing%20the%20high%20compatibility%20of%20LinVT.%20LinVT-based%20LLMs%20achieve%0Astate-of-the-art%20performance%20across%20various%20video%20benchmarks%2C%20illustrating%20the%0Aeffectiveness%20of%20LinVT%20in%20multi-modal%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05185v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinVT%253A%2520Empower%2520Your%2520Image-level%2520Large%2520Language%2520Model%2520to%2520Understand%250A%2520%2520Videos%26entry.906535625%3DLishuai%2520Gao%2520and%2520Yujie%2520Zhong%2520and%2520Yingsen%2520Zeng%2520and%2520Haoxian%2520Tan%2520and%2520Dengjie%2520Li%2520and%2520Zheng%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520used%2520in%2520various%2520tasks%252C%250Amotivating%2520us%2520to%2520develop%2520an%2520LLM-based%2520assistant%2520for%2520videos.%2520Instead%2520of%2520training%250Afrom%2520scratch%252C%2520we%2520propose%2520a%2520module%2520to%2520transform%2520arbitrary%2520well-trained%250Aimage-based%2520LLMs%2520into%2520video-LLMs%2520%2528after%2520being%2520trained%2520on%2520video%2520data%2529.%2520To%2520better%250Aadapt%2520image-LLMs%2520for%2520processing%2520videos%252C%2520we%2520introduce%2520two%2520design%2520principles%253A%250Alinear%2520transformation%2520to%2520preserve%2520the%2520original%2520visual-language%2520alignment%2520and%250Arepresentative%2520information%2520condensation%2520from%2520redundant%2520video%2520content.%2520Guided%2520by%250Athese%2520principles%252C%2520we%2520propose%2520a%2520plug-and-play%2520Linear%2520Video%2520Tokenizer%2528LinVT%2529%252C%250Awhich%2520enables%2520existing%2520image-LLMs%2520to%2520understand%2520videos.%2520We%2520benchmark%2520LinVT%2520with%250Asix%2520recent%2520visual%2520LLMs%253A%2520Aquila%252C%2520Blip-3%252C%2520InternVL2%252C%2520Mipha%252C%2520Molmo%2520and%2520Qwen2-VL%252C%250Ashowcasing%2520the%2520high%2520compatibility%2520of%2520LinVT.%2520LinVT-based%2520LLMs%2520achieve%250Astate-of-the-art%2520performance%2520across%2520various%2520video%2520benchmarks%252C%2520illustrating%2520the%250Aeffectiveness%2520of%2520LinVT%2520in%2520multi-modal%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05185v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LinVT%3A%20Empower%20Your%20Image-level%20Large%20Language%20Model%20to%20Understand%0A%20%20Videos&entry.906535625=Lishuai%20Gao%20and%20Yujie%20Zhong%20and%20Yingsen%20Zeng%20and%20Haoxian%20Tan%20and%20Dengjie%20Li%20and%20Zheng%20Zhao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20widely%20used%20in%20various%20tasks%2C%0Amotivating%20us%20to%20develop%20an%20LLM-based%20assistant%20for%20videos.%20Instead%20of%20training%0Afrom%20scratch%2C%20we%20propose%20a%20module%20to%20transform%20arbitrary%20well-trained%0Aimage-based%20LLMs%20into%20video-LLMs%20%28after%20being%20trained%20on%20video%20data%29.%20To%20better%0Aadapt%20image-LLMs%20for%20processing%20videos%2C%20we%20introduce%20two%20design%20principles%3A%0Alinear%20transformation%20to%20preserve%20the%20original%20visual-language%20alignment%20and%0Arepresentative%20information%20condensation%20from%20redundant%20video%20content.%20Guided%20by%0Athese%20principles%2C%20we%20propose%20a%20plug-and-play%20Linear%20Video%20Tokenizer%28LinVT%29%2C%0Awhich%20enables%20existing%20image-LLMs%20to%20understand%20videos.%20We%20benchmark%20LinVT%20with%0Asix%20recent%20visual%20LLMs%3A%20Aquila%2C%20Blip-3%2C%20InternVL2%2C%20Mipha%2C%20Molmo%20and%20Qwen2-VL%2C%0Ashowcasing%20the%20high%20compatibility%20of%20LinVT.%20LinVT-based%20LLMs%20achieve%0Astate-of-the-art%20performance%20across%20various%20video%20benchmarks%2C%20illustrating%20the%0Aeffectiveness%20of%20LinVT%20in%20multi-modal%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05185v2&entry.124074799=Read"},
{"title": "3D Mesh Editing using Masked LRMs", "author": "Will Gao and Dilin Wang and Yuchen Fan and Aljaz Bozic and Tuur Stuyck and Zhengqin Li and Zhao Dong and Rakesh Ranjan and Nikolaos Sarafianos", "abstract": "  We present a novel approach to mesh shape editing, building on recent\nprogress in 3D reconstruction from multi-view images. We formulate shape\nediting as a conditional reconstruction problem, where the model must\nreconstruct the input shape with the exception of a specified 3D region, in\nwhich the geometry should be generated from the conditional signal. To this\nend, we train a conditional Large Reconstruction Model (LRM) for masked\nreconstruction, using multi-view consistent masks rendered from a randomly\ngenerated 3D occlusion, and using one clean viewpoint as the conditional\nsignal. During inference, we manually define a 3D region to edit and provide an\nedited image from a canonical viewpoint to fill in that region. We demonstrate\nthat, in just a single forward pass, our method not only preserves the input\ngeometry in the unmasked region through reconstruction capabilities on par with\nSoTA, but is also expressive enough to perform a variety of mesh edits from a\nsingle image guidance that past works struggle with, while being 10x faster\nthan the top-performing competing prior work.\n", "link": "http://arxiv.org/abs/2412.08641v1", "date": "2024-12-11", "relevancy": 2.8109, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5853}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5506}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Mesh%20Editing%20using%20Masked%20LRMs&body=Title%3A%203D%20Mesh%20Editing%20using%20Masked%20LRMs%0AAuthor%3A%20Will%20Gao%20and%20Dilin%20Wang%20and%20Yuchen%20Fan%20and%20Aljaz%20Bozic%20and%20Tuur%20Stuyck%20and%20Zhengqin%20Li%20and%20Zhao%20Dong%20and%20Rakesh%20Ranjan%20and%20Nikolaos%20Sarafianos%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20mesh%20shape%20editing%2C%20building%20on%20recent%0Aprogress%20in%203D%20reconstruction%20from%20multi-view%20images.%20We%20formulate%20shape%0Aediting%20as%20a%20conditional%20reconstruction%20problem%2C%20where%20the%20model%20must%0Areconstruct%20the%20input%20shape%20with%20the%20exception%20of%20a%20specified%203D%20region%2C%20in%0Awhich%20the%20geometry%20should%20be%20generated%20from%20the%20conditional%20signal.%20To%20this%0Aend%2C%20we%20train%20a%20conditional%20Large%20Reconstruction%20Model%20%28LRM%29%20for%20masked%0Areconstruction%2C%20using%20multi-view%20consistent%20masks%20rendered%20from%20a%20randomly%0Agenerated%203D%20occlusion%2C%20and%20using%20one%20clean%20viewpoint%20as%20the%20conditional%0Asignal.%20During%20inference%2C%20we%20manually%20define%20a%203D%20region%20to%20edit%20and%20provide%20an%0Aedited%20image%20from%20a%20canonical%20viewpoint%20to%20fill%20in%20that%20region.%20We%20demonstrate%0Athat%2C%20in%20just%20a%20single%20forward%20pass%2C%20our%20method%20not%20only%20preserves%20the%20input%0Ageometry%20in%20the%20unmasked%20region%20through%20reconstruction%20capabilities%20on%20par%20with%0ASoTA%2C%20but%20is%20also%20expressive%20enough%20to%20perform%20a%20variety%20of%20mesh%20edits%20from%20a%0Asingle%20image%20guidance%20that%20past%20works%20struggle%20with%2C%20while%20being%2010x%20faster%0Athan%20the%20top-performing%20competing%20prior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Mesh%2520Editing%2520using%2520Masked%2520LRMs%26entry.906535625%3DWill%2520Gao%2520and%2520Dilin%2520Wang%2520and%2520Yuchen%2520Fan%2520and%2520Aljaz%2520Bozic%2520and%2520Tuur%2520Stuyck%2520and%2520Zhengqin%2520Li%2520and%2520Zhao%2520Dong%2520and%2520Rakesh%2520Ranjan%2520and%2520Nikolaos%2520Sarafianos%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520mesh%2520shape%2520editing%252C%2520building%2520on%2520recent%250Aprogress%2520in%25203D%2520reconstruction%2520from%2520multi-view%2520images.%2520We%2520formulate%2520shape%250Aediting%2520as%2520a%2520conditional%2520reconstruction%2520problem%252C%2520where%2520the%2520model%2520must%250Areconstruct%2520the%2520input%2520shape%2520with%2520the%2520exception%2520of%2520a%2520specified%25203D%2520region%252C%2520in%250Awhich%2520the%2520geometry%2520should%2520be%2520generated%2520from%2520the%2520conditional%2520signal.%2520To%2520this%250Aend%252C%2520we%2520train%2520a%2520conditional%2520Large%2520Reconstruction%2520Model%2520%2528LRM%2529%2520for%2520masked%250Areconstruction%252C%2520using%2520multi-view%2520consistent%2520masks%2520rendered%2520from%2520a%2520randomly%250Agenerated%25203D%2520occlusion%252C%2520and%2520using%2520one%2520clean%2520viewpoint%2520as%2520the%2520conditional%250Asignal.%2520During%2520inference%252C%2520we%2520manually%2520define%2520a%25203D%2520region%2520to%2520edit%2520and%2520provide%2520an%250Aedited%2520image%2520from%2520a%2520canonical%2520viewpoint%2520to%2520fill%2520in%2520that%2520region.%2520We%2520demonstrate%250Athat%252C%2520in%2520just%2520a%2520single%2520forward%2520pass%252C%2520our%2520method%2520not%2520only%2520preserves%2520the%2520input%250Ageometry%2520in%2520the%2520unmasked%2520region%2520through%2520reconstruction%2520capabilities%2520on%2520par%2520with%250ASoTA%252C%2520but%2520is%2520also%2520expressive%2520enough%2520to%2520perform%2520a%2520variety%2520of%2520mesh%2520edits%2520from%2520a%250Asingle%2520image%2520guidance%2520that%2520past%2520works%2520struggle%2520with%252C%2520while%2520being%252010x%2520faster%250Athan%2520the%2520top-performing%2520competing%2520prior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Mesh%20Editing%20using%20Masked%20LRMs&entry.906535625=Will%20Gao%20and%20Dilin%20Wang%20and%20Yuchen%20Fan%20and%20Aljaz%20Bozic%20and%20Tuur%20Stuyck%20and%20Zhengqin%20Li%20and%20Zhao%20Dong%20and%20Rakesh%20Ranjan%20and%20Nikolaos%20Sarafianos&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20mesh%20shape%20editing%2C%20building%20on%20recent%0Aprogress%20in%203D%20reconstruction%20from%20multi-view%20images.%20We%20formulate%20shape%0Aediting%20as%20a%20conditional%20reconstruction%20problem%2C%20where%20the%20model%20must%0Areconstruct%20the%20input%20shape%20with%20the%20exception%20of%20a%20specified%203D%20region%2C%20in%0Awhich%20the%20geometry%20should%20be%20generated%20from%20the%20conditional%20signal.%20To%20this%0Aend%2C%20we%20train%20a%20conditional%20Large%20Reconstruction%20Model%20%28LRM%29%20for%20masked%0Areconstruction%2C%20using%20multi-view%20consistent%20masks%20rendered%20from%20a%20randomly%0Agenerated%203D%20occlusion%2C%20and%20using%20one%20clean%20viewpoint%20as%20the%20conditional%0Asignal.%20During%20inference%2C%20we%20manually%20define%20a%203D%20region%20to%20edit%20and%20provide%20an%0Aedited%20image%20from%20a%20canonical%20viewpoint%20to%20fill%20in%20that%20region.%20We%20demonstrate%0Athat%2C%20in%20just%20a%20single%20forward%20pass%2C%20our%20method%20not%20only%20preserves%20the%20input%0Ageometry%20in%20the%20unmasked%20region%20through%20reconstruction%20capabilities%20on%20par%20with%0ASoTA%2C%20but%20is%20also%20expressive%20enough%20to%20perform%20a%20variety%20of%20mesh%20edits%20from%20a%0Asingle%20image%20guidance%20that%20past%20works%20struggle%20with%2C%20while%20being%2010x%20faster%0Athan%20the%20top-performing%20competing%20prior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08641v1&entry.124074799=Read"},
{"title": "Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion\n  Reconstruction from Partial Data", "author": "Aymen Merrouche and Stefanie Wuhrer and Edmond Boyer", "abstract": "  We introduce a novel, data-driven approach for reconstructing temporally\ncoherent 3D motion from unstructured and potentially partial observations of\nnon-rigidly deforming shapes. Our goal is to achieve high-fidelity motion\nreconstructions for shapes that undergo near-isometric deformations, such as\nhumans wearing loose clothing. The key novelty of our work lies in its ability\nto combine implicit shape representations with explicit mesh-based deformation\nmodels, enabling detailed and temporally coherent motion reconstructions\nwithout relying on parametric shape models or decoupling shape and motion. Each\nframe is represented as a neural field decoded from a feature space where\nobservations over time are fused, hence preserving geometric details present in\nthe input data. Temporal coherence is enforced with a near-isometric\ndeformation constraint between adjacent frames that applies to the underlying\nsurface in the neural field. Our method outperforms state-of-the-art\napproaches, as demonstrated by its application to human and animal motion\nsequences reconstructed from monocular depth videos.\n", "link": "http://arxiv.org/abs/2412.08511v1", "date": "2024-12-11", "relevancy": 2.7942, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6053}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5357}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Neural%20Fields%20and%20Deformation%20Models%20for%20Non-Rigid%203D%20Motion%0A%20%20Reconstruction%20from%20Partial%20Data&body=Title%3A%20Combining%20Neural%20Fields%20and%20Deformation%20Models%20for%20Non-Rigid%203D%20Motion%0A%20%20Reconstruction%20from%20Partial%20Data%0AAuthor%3A%20Aymen%20Merrouche%20and%20Stefanie%20Wuhrer%20and%20Edmond%20Boyer%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%2C%20data-driven%20approach%20for%20reconstructing%20temporally%0Acoherent%203D%20motion%20from%20unstructured%20and%20potentially%20partial%20observations%20of%0Anon-rigidly%20deforming%20shapes.%20Our%20goal%20is%20to%20achieve%20high-fidelity%20motion%0Areconstructions%20for%20shapes%20that%20undergo%20near-isometric%20deformations%2C%20such%20as%0Ahumans%20wearing%20loose%20clothing.%20The%20key%20novelty%20of%20our%20work%20lies%20in%20its%20ability%0Ato%20combine%20implicit%20shape%20representations%20with%20explicit%20mesh-based%20deformation%0Amodels%2C%20enabling%20detailed%20and%20temporally%20coherent%20motion%20reconstructions%0Awithout%20relying%20on%20parametric%20shape%20models%20or%20decoupling%20shape%20and%20motion.%20Each%0Aframe%20is%20represented%20as%20a%20neural%20field%20decoded%20from%20a%20feature%20space%20where%0Aobservations%20over%20time%20are%20fused%2C%20hence%20preserving%20geometric%20details%20present%20in%0Athe%20input%20data.%20Temporal%20coherence%20is%20enforced%20with%20a%20near-isometric%0Adeformation%20constraint%20between%20adjacent%20frames%20that%20applies%20to%20the%20underlying%0Asurface%20in%20the%20neural%20field.%20Our%20method%20outperforms%20state-of-the-art%0Aapproaches%2C%20as%20demonstrated%20by%20its%20application%20to%20human%20and%20animal%20motion%0Asequences%20reconstructed%20from%20monocular%20depth%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Neural%2520Fields%2520and%2520Deformation%2520Models%2520for%2520Non-Rigid%25203D%2520Motion%250A%2520%2520Reconstruction%2520from%2520Partial%2520Data%26entry.906535625%3DAymen%2520Merrouche%2520and%2520Stefanie%2520Wuhrer%2520and%2520Edmond%2520Boyer%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%252C%2520data-driven%2520approach%2520for%2520reconstructing%2520temporally%250Acoherent%25203D%2520motion%2520from%2520unstructured%2520and%2520potentially%2520partial%2520observations%2520of%250Anon-rigidly%2520deforming%2520shapes.%2520Our%2520goal%2520is%2520to%2520achieve%2520high-fidelity%2520motion%250Areconstructions%2520for%2520shapes%2520that%2520undergo%2520near-isometric%2520deformations%252C%2520such%2520as%250Ahumans%2520wearing%2520loose%2520clothing.%2520The%2520key%2520novelty%2520of%2520our%2520work%2520lies%2520in%2520its%2520ability%250Ato%2520combine%2520implicit%2520shape%2520representations%2520with%2520explicit%2520mesh-based%2520deformation%250Amodels%252C%2520enabling%2520detailed%2520and%2520temporally%2520coherent%2520motion%2520reconstructions%250Awithout%2520relying%2520on%2520parametric%2520shape%2520models%2520or%2520decoupling%2520shape%2520and%2520motion.%2520Each%250Aframe%2520is%2520represented%2520as%2520a%2520neural%2520field%2520decoded%2520from%2520a%2520feature%2520space%2520where%250Aobservations%2520over%2520time%2520are%2520fused%252C%2520hence%2520preserving%2520geometric%2520details%2520present%2520in%250Athe%2520input%2520data.%2520Temporal%2520coherence%2520is%2520enforced%2520with%2520a%2520near-isometric%250Adeformation%2520constraint%2520between%2520adjacent%2520frames%2520that%2520applies%2520to%2520the%2520underlying%250Asurface%2520in%2520the%2520neural%2520field.%2520Our%2520method%2520outperforms%2520state-of-the-art%250Aapproaches%252C%2520as%2520demonstrated%2520by%2520its%2520application%2520to%2520human%2520and%2520animal%2520motion%250Asequences%2520reconstructed%2520from%2520monocular%2520depth%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Neural%20Fields%20and%20Deformation%20Models%20for%20Non-Rigid%203D%20Motion%0A%20%20Reconstruction%20from%20Partial%20Data&entry.906535625=Aymen%20Merrouche%20and%20Stefanie%20Wuhrer%20and%20Edmond%20Boyer&entry.1292438233=%20%20We%20introduce%20a%20novel%2C%20data-driven%20approach%20for%20reconstructing%20temporally%0Acoherent%203D%20motion%20from%20unstructured%20and%20potentially%20partial%20observations%20of%0Anon-rigidly%20deforming%20shapes.%20Our%20goal%20is%20to%20achieve%20high-fidelity%20motion%0Areconstructions%20for%20shapes%20that%20undergo%20near-isometric%20deformations%2C%20such%20as%0Ahumans%20wearing%20loose%20clothing.%20The%20key%20novelty%20of%20our%20work%20lies%20in%20its%20ability%0Ato%20combine%20implicit%20shape%20representations%20with%20explicit%20mesh-based%20deformation%0Amodels%2C%20enabling%20detailed%20and%20temporally%20coherent%20motion%20reconstructions%0Awithout%20relying%20on%20parametric%20shape%20models%20or%20decoupling%20shape%20and%20motion.%20Each%0Aframe%20is%20represented%20as%20a%20neural%20field%20decoded%20from%20a%20feature%20space%20where%0Aobservations%20over%20time%20are%20fused%2C%20hence%20preserving%20geometric%20details%20present%20in%0Athe%20input%20data.%20Temporal%20coherence%20is%20enforced%20with%20a%20near-isometric%0Adeformation%20constraint%20between%20adjacent%20frames%20that%20applies%20to%20the%20underlying%0Asurface%20in%20the%20neural%20field.%20Our%20method%20outperforms%20state-of-the-art%0Aapproaches%2C%20as%20demonstrated%20by%20its%20application%20to%20human%20and%20animal%20motion%0Asequences%20reconstructed%20from%20monocular%20depth%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08511v1&entry.124074799=Read"},
{"title": "Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification", "author": "Filippos Gouidis and Katerina Papantoniou and Konstantinos Papoutsakis and Theodore Patkos and Antonis Argyros and Dimitris Plexousakis", "abstract": "  Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.\n", "link": "http://arxiv.org/abs/2403.12151v3", "date": "2024-12-11", "relevancy": 2.7819, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusing%20Domain-Specific%20Content%20from%20Large%20Language%20Models%20into%20Knowledge%0A%20%20Graphs%20for%20Enhanced%20Zero%20Shot%20Object%20State%20Classification&body=Title%3A%20Fusing%20Domain-Specific%20Content%20from%20Large%20Language%20Models%20into%20Knowledge%0A%20%20Graphs%20for%20Enhanced%20Zero%20Shot%20Object%20State%20Classification%0AAuthor%3A%20Filippos%20Gouidis%20and%20Katerina%20Papantoniou%20and%20Konstantinos%20Papoutsakis%20and%20Theodore%20Patkos%20and%20Antonis%20Argyros%20and%20Dimitris%20Plexousakis%0AAbstract%3A%20%20%20Domain-specific%20knowledge%20can%20significantly%20contribute%20to%20addressing%20a%20wide%0Avariety%20of%20vision%20tasks.%20However%2C%20the%20generation%20of%20such%20knowledge%20entails%0Aconsiderable%20human%20labor%20and%20time%20costs.%20This%20study%20investigates%20the%20potential%0Aof%20Large%20Language%20Models%20%28LLMs%29%20in%20generating%20and%20providing%20domain-specific%0Ainformation%20through%20semantic%20embeddings.%20To%20achieve%20this%2C%20an%20LLM%20is%20integrated%0Ainto%20a%20pipeline%20that%20utilizes%20Knowledge%20Graphs%20and%20pre-trained%20semantic%20vectors%0Ain%20the%20context%20of%20the%20Vision-based%20Zero-shot%20Object%20State%20Classification%20task.%0AWe%20thoroughly%20examine%20the%20behavior%20of%20the%20LLM%20through%20an%20extensive%20ablation%0Astudy.%20Our%20findings%20reveal%20that%20the%20integration%20of%20LLM-based%20embeddings%2C%20in%0Acombination%20with%20general-purpose%20pre-trained%20embeddings%2C%20leads%20to%20substantial%0Aperformance%20improvements.%20Drawing%20insights%20from%20this%20ablation%20study%2C%20we%20conduct%0Aa%20comparative%20analysis%20against%20competing%20models%2C%20thereby%20highlighting%20the%0Astate-of-the-art%20performance%20achieved%20by%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12151v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusing%2520Domain-Specific%2520Content%2520from%2520Large%2520Language%2520Models%2520into%2520Knowledge%250A%2520%2520Graphs%2520for%2520Enhanced%2520Zero%2520Shot%2520Object%2520State%2520Classification%26entry.906535625%3DFilippos%2520Gouidis%2520and%2520Katerina%2520Papantoniou%2520and%2520Konstantinos%2520Papoutsakis%2520and%2520Theodore%2520Patkos%2520and%2520Antonis%2520Argyros%2520and%2520Dimitris%2520Plexousakis%26entry.1292438233%3D%2520%2520Domain-specific%2520knowledge%2520can%2520significantly%2520contribute%2520to%2520addressing%2520a%2520wide%250Avariety%2520of%2520vision%2520tasks.%2520However%252C%2520the%2520generation%2520of%2520such%2520knowledge%2520entails%250Aconsiderable%2520human%2520labor%2520and%2520time%2520costs.%2520This%2520study%2520investigates%2520the%2520potential%250Aof%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520generating%2520and%2520providing%2520domain-specific%250Ainformation%2520through%2520semantic%2520embeddings.%2520To%2520achieve%2520this%252C%2520an%2520LLM%2520is%2520integrated%250Ainto%2520a%2520pipeline%2520that%2520utilizes%2520Knowledge%2520Graphs%2520and%2520pre-trained%2520semantic%2520vectors%250Ain%2520the%2520context%2520of%2520the%2520Vision-based%2520Zero-shot%2520Object%2520State%2520Classification%2520task.%250AWe%2520thoroughly%2520examine%2520the%2520behavior%2520of%2520the%2520LLM%2520through%2520an%2520extensive%2520ablation%250Astudy.%2520Our%2520findings%2520reveal%2520that%2520the%2520integration%2520of%2520LLM-based%2520embeddings%252C%2520in%250Acombination%2520with%2520general-purpose%2520pre-trained%2520embeddings%252C%2520leads%2520to%2520substantial%250Aperformance%2520improvements.%2520Drawing%2520insights%2520from%2520this%2520ablation%2520study%252C%2520we%2520conduct%250Aa%2520comparative%2520analysis%2520against%2520competing%2520models%252C%2520thereby%2520highlighting%2520the%250Astate-of-the-art%2520performance%2520achieved%2520by%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12151v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusing%20Domain-Specific%20Content%20from%20Large%20Language%20Models%20into%20Knowledge%0A%20%20Graphs%20for%20Enhanced%20Zero%20Shot%20Object%20State%20Classification&entry.906535625=Filippos%20Gouidis%20and%20Katerina%20Papantoniou%20and%20Konstantinos%20Papoutsakis%20and%20Theodore%20Patkos%20and%20Antonis%20Argyros%20and%20Dimitris%20Plexousakis&entry.1292438233=%20%20Domain-specific%20knowledge%20can%20significantly%20contribute%20to%20addressing%20a%20wide%0Avariety%20of%20vision%20tasks.%20However%2C%20the%20generation%20of%20such%20knowledge%20entails%0Aconsiderable%20human%20labor%20and%20time%20costs.%20This%20study%20investigates%20the%20potential%0Aof%20Large%20Language%20Models%20%28LLMs%29%20in%20generating%20and%20providing%20domain-specific%0Ainformation%20through%20semantic%20embeddings.%20To%20achieve%20this%2C%20an%20LLM%20is%20integrated%0Ainto%20a%20pipeline%20that%20utilizes%20Knowledge%20Graphs%20and%20pre-trained%20semantic%20vectors%0Ain%20the%20context%20of%20the%20Vision-based%20Zero-shot%20Object%20State%20Classification%20task.%0AWe%20thoroughly%20examine%20the%20behavior%20of%20the%20LLM%20through%20an%20extensive%20ablation%0Astudy.%20Our%20findings%20reveal%20that%20the%20integration%20of%20LLM-based%20embeddings%2C%20in%0Acombination%20with%20general-purpose%20pre-trained%20embeddings%2C%20leads%20to%20substantial%0Aperformance%20improvements.%20Drawing%20insights%20from%20this%20ablation%20study%2C%20we%20conduct%0Aa%20comparative%20analysis%20against%20competing%20models%2C%20thereby%20highlighting%20the%0Astate-of-the-art%20performance%20achieved%20by%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12151v3&entry.124074799=Read"},
{"title": "TryOffAnyone: Tiled Cloth Generation from a Dressed Person", "author": "Ioannis Xarchakos and Theodoros Koukopoulos", "abstract": "  The fashion industry is increasingly leveraging computer vision and deep\nlearning technologies to enhance online shopping experiences and operational\nefficiencies. In this paper, we address the challenge of generating\nhigh-fidelity tiled garment images essential for personalized recommendations,\noutfit composition, and virtual try-on systems from photos of garments worn by\nmodels. Inspired by the success of Latent Diffusion Models (LDMs) in\nimage-to-image translation, we propose a novel approach utilizing a fine-tuned\nStableDiffusion model. Our method features a streamlined single-stage network\ndesign, which integrates garmentspecific masks to isolate and process target\nclothing items effectively. By simplifying the network architecture through\nselective training of transformer blocks and removing unnecessary\ncrossattention layers, we significantly reduce computational complexity while\nachieving state-of-the-art performance on benchmark datasets like VITON-HD.\nExperimental results demonstrate the effectiveness of our approach in producing\nhigh-quality tiled garment images for both full-body and half-body inputs. Code\nand model are available at: https://github.com/ixarchakos/try-off-anyone\n", "link": "http://arxiv.org/abs/2412.08573v1", "date": "2024-12-11", "relevancy": 2.7811, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7233}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6894}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TryOffAnyone%3A%20Tiled%20Cloth%20Generation%20from%20a%20Dressed%20Person&body=Title%3A%20TryOffAnyone%3A%20Tiled%20Cloth%20Generation%20from%20a%20Dressed%20Person%0AAuthor%3A%20Ioannis%20Xarchakos%20and%20Theodoros%20Koukopoulos%0AAbstract%3A%20%20%20The%20fashion%20industry%20is%20increasingly%20leveraging%20computer%20vision%20and%20deep%0Alearning%20technologies%20to%20enhance%20online%20shopping%20experiences%20and%20operational%0Aefficiencies.%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20generating%0Ahigh-fidelity%20tiled%20garment%20images%20essential%20for%20personalized%20recommendations%2C%0Aoutfit%20composition%2C%20and%20virtual%20try-on%20systems%20from%20photos%20of%20garments%20worn%20by%0Amodels.%20Inspired%20by%20the%20success%20of%20Latent%20Diffusion%20Models%20%28LDMs%29%20in%0Aimage-to-image%20translation%2C%20we%20propose%20a%20novel%20approach%20utilizing%20a%20fine-tuned%0AStableDiffusion%20model.%20Our%20method%20features%20a%20streamlined%20single-stage%20network%0Adesign%2C%20which%20integrates%20garmentspecific%20masks%20to%20isolate%20and%20process%20target%0Aclothing%20items%20effectively.%20By%20simplifying%20the%20network%20architecture%20through%0Aselective%20training%20of%20transformer%20blocks%20and%20removing%20unnecessary%0Acrossattention%20layers%2C%20we%20significantly%20reduce%20computational%20complexity%20while%0Aachieving%20state-of-the-art%20performance%20on%20benchmark%20datasets%20like%20VITON-HD.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20producing%0Ahigh-quality%20tiled%20garment%20images%20for%20both%20full-body%20and%20half-body%20inputs.%20Code%0Aand%20model%20are%20available%20at%3A%20https%3A//github.com/ixarchakos/try-off-anyone%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTryOffAnyone%253A%2520Tiled%2520Cloth%2520Generation%2520from%2520a%2520Dressed%2520Person%26entry.906535625%3DIoannis%2520Xarchakos%2520and%2520Theodoros%2520Koukopoulos%26entry.1292438233%3D%2520%2520The%2520fashion%2520industry%2520is%2520increasingly%2520leveraging%2520computer%2520vision%2520and%2520deep%250Alearning%2520technologies%2520to%2520enhance%2520online%2520shopping%2520experiences%2520and%2520operational%250Aefficiencies.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenge%2520of%2520generating%250Ahigh-fidelity%2520tiled%2520garment%2520images%2520essential%2520for%2520personalized%2520recommendations%252C%250Aoutfit%2520composition%252C%2520and%2520virtual%2520try-on%2520systems%2520from%2520photos%2520of%2520garments%2520worn%2520by%250Amodels.%2520Inspired%2520by%2520the%2520success%2520of%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529%2520in%250Aimage-to-image%2520translation%252C%2520we%2520propose%2520a%2520novel%2520approach%2520utilizing%2520a%2520fine-tuned%250AStableDiffusion%2520model.%2520Our%2520method%2520features%2520a%2520streamlined%2520single-stage%2520network%250Adesign%252C%2520which%2520integrates%2520garmentspecific%2520masks%2520to%2520isolate%2520and%2520process%2520target%250Aclothing%2520items%2520effectively.%2520By%2520simplifying%2520the%2520network%2520architecture%2520through%250Aselective%2520training%2520of%2520transformer%2520blocks%2520and%2520removing%2520unnecessary%250Acrossattention%2520layers%252C%2520we%2520significantly%2520reduce%2520computational%2520complexity%2520while%250Aachieving%2520state-of-the-art%2520performance%2520on%2520benchmark%2520datasets%2520like%2520VITON-HD.%250AExperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520producing%250Ahigh-quality%2520tiled%2520garment%2520images%2520for%2520both%2520full-body%2520and%2520half-body%2520inputs.%2520Code%250Aand%2520model%2520are%2520available%2520at%253A%2520https%253A//github.com/ixarchakos/try-off-anyone%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TryOffAnyone%3A%20Tiled%20Cloth%20Generation%20from%20a%20Dressed%20Person&entry.906535625=Ioannis%20Xarchakos%20and%20Theodoros%20Koukopoulos&entry.1292438233=%20%20The%20fashion%20industry%20is%20increasingly%20leveraging%20computer%20vision%20and%20deep%0Alearning%20technologies%20to%20enhance%20online%20shopping%20experiences%20and%20operational%0Aefficiencies.%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20generating%0Ahigh-fidelity%20tiled%20garment%20images%20essential%20for%20personalized%20recommendations%2C%0Aoutfit%20composition%2C%20and%20virtual%20try-on%20systems%20from%20photos%20of%20garments%20worn%20by%0Amodels.%20Inspired%20by%20the%20success%20of%20Latent%20Diffusion%20Models%20%28LDMs%29%20in%0Aimage-to-image%20translation%2C%20we%20propose%20a%20novel%20approach%20utilizing%20a%20fine-tuned%0AStableDiffusion%20model.%20Our%20method%20features%20a%20streamlined%20single-stage%20network%0Adesign%2C%20which%20integrates%20garmentspecific%20masks%20to%20isolate%20and%20process%20target%0Aclothing%20items%20effectively.%20By%20simplifying%20the%20network%20architecture%20through%0Aselective%20training%20of%20transformer%20blocks%20and%20removing%20unnecessary%0Acrossattention%20layers%2C%20we%20significantly%20reduce%20computational%20complexity%20while%0Aachieving%20state-of-the-art%20performance%20on%20benchmark%20datasets%20like%20VITON-HD.%0AExperimental%20results%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%20producing%0Ahigh-quality%20tiled%20garment%20images%20for%20both%20full-body%20and%20half-body%20inputs.%20Code%0Aand%20model%20are%20available%20at%3A%20https%3A//github.com/ixarchakos/try-off-anyone%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08573v1&entry.124074799=Read"},
{"title": "Standing on the Shoulders of Giants: Reprogramming Visual-Language Model\n  for General Deepfake Detection", "author": "Kaiqing Lin and Yuzhen Lin and Weixiang Li and Taiping Yao and Bin Li", "abstract": "  The proliferation of deepfake faces poses huge potential negative impacts on\nour daily lives. Despite substantial advancements in deepfake detection over\nthese years, the generalizability of existing methods against forgeries from\nunseen datasets or created by emerging generative models remains constrained.\nIn this paper, inspired by the zero-shot advantages of Vision-Language Models\n(VLMs), we propose a novel approach that repurposes a well-trained VLM for\ngeneral deepfake detection. Motivated by the model reprogramming paradigm that\nmanipulates the model prediction via data perturbations, our method can\nreprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its\ninput without tuning the inner parameters. Furthermore, we insert a pseudo-word\nguided by facial identity into the text prompt. Extensive experiments on\nseveral popular benchmarks demonstrate that (1) the cross-dataset and\ncross-manipulation performances of deepfake detection can be significantly and\nconsistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to\nWildDeepfake) using a pre-trained CLIP model with our proposed reprogramming\nmethod; (2) our superior performances are at less cost of trainable parameters,\nmaking it a promising approach for real-world applications.\n", "link": "http://arxiv.org/abs/2409.02664v2", "date": "2024-12-11", "relevancy": 2.7488, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Standing%20on%20the%20Shoulders%20of%20Giants%3A%20Reprogramming%20Visual-Language%20Model%0A%20%20for%20General%20Deepfake%20Detection&body=Title%3A%20Standing%20on%20the%20Shoulders%20of%20Giants%3A%20Reprogramming%20Visual-Language%20Model%0A%20%20for%20General%20Deepfake%20Detection%0AAuthor%3A%20Kaiqing%20Lin%20and%20Yuzhen%20Lin%20and%20Weixiang%20Li%20and%20Taiping%20Yao%20and%20Bin%20Li%0AAbstract%3A%20%20%20The%20proliferation%20of%20deepfake%20faces%20poses%20huge%20potential%20negative%20impacts%20on%0Aour%20daily%20lives.%20Despite%20substantial%20advancements%20in%20deepfake%20detection%20over%0Athese%20years%2C%20the%20generalizability%20of%20existing%20methods%20against%20forgeries%20from%0Aunseen%20datasets%20or%20created%20by%20emerging%20generative%20models%20remains%20constrained.%0AIn%20this%20paper%2C%20inspired%20by%20the%20zero-shot%20advantages%20of%20Vision-Language%20Models%0A%28VLMs%29%2C%20we%20propose%20a%20novel%20approach%20that%20repurposes%20a%20well-trained%20VLM%20for%0Ageneral%20deepfake%20detection.%20Motivated%20by%20the%20model%20reprogramming%20paradigm%20that%0Amanipulates%20the%20model%20prediction%20via%20data%20perturbations%2C%20our%20method%20can%0Areprogram%20a%20pretrained%20VLM%20model%20%28e.g.%2C%20CLIP%29%20solely%20based%20on%20manipulating%20its%0Ainput%20without%20tuning%20the%20inner%20parameters.%20Furthermore%2C%20we%20insert%20a%20pseudo-word%0Aguided%20by%20facial%20identity%20into%20the%20text%20prompt.%20Extensive%20experiments%20on%0Aseveral%20popular%20benchmarks%20demonstrate%20that%20%281%29%20the%20cross-dataset%20and%0Across-manipulation%20performances%20of%20deepfake%20detection%20can%20be%20significantly%20and%0Aconsistently%20improved%20%28e.g.%2C%20over%2088%25%20AUC%20in%20cross-dataset%20setting%20from%20FF%2B%2B%20to%0AWildDeepfake%29%20using%20a%20pre-trained%20CLIP%20model%20with%20our%20proposed%20reprogramming%0Amethod%3B%20%282%29%20our%20superior%20performances%20are%20at%20less%20cost%20of%20trainable%20parameters%2C%0Amaking%20it%20a%20promising%20approach%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStanding%2520on%2520the%2520Shoulders%2520of%2520Giants%253A%2520Reprogramming%2520Visual-Language%2520Model%250A%2520%2520for%2520General%2520Deepfake%2520Detection%26entry.906535625%3DKaiqing%2520Lin%2520and%2520Yuzhen%2520Lin%2520and%2520Weixiang%2520Li%2520and%2520Taiping%2520Yao%2520and%2520Bin%2520Li%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520deepfake%2520faces%2520poses%2520huge%2520potential%2520negative%2520impacts%2520on%250Aour%2520daily%2520lives.%2520Despite%2520substantial%2520advancements%2520in%2520deepfake%2520detection%2520over%250Athese%2520years%252C%2520the%2520generalizability%2520of%2520existing%2520methods%2520against%2520forgeries%2520from%250Aunseen%2520datasets%2520or%2520created%2520by%2520emerging%2520generative%2520models%2520remains%2520constrained.%250AIn%2520this%2520paper%252C%2520inspired%2520by%2520the%2520zero-shot%2520advantages%2520of%2520Vision-Language%2520Models%250A%2528VLMs%2529%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520repurposes%2520a%2520well-trained%2520VLM%2520for%250Ageneral%2520deepfake%2520detection.%2520Motivated%2520by%2520the%2520model%2520reprogramming%2520paradigm%2520that%250Amanipulates%2520the%2520model%2520prediction%2520via%2520data%2520perturbations%252C%2520our%2520method%2520can%250Areprogram%2520a%2520pretrained%2520VLM%2520model%2520%2528e.g.%252C%2520CLIP%2529%2520solely%2520based%2520on%2520manipulating%2520its%250Ainput%2520without%2520tuning%2520the%2520inner%2520parameters.%2520Furthermore%252C%2520we%2520insert%2520a%2520pseudo-word%250Aguided%2520by%2520facial%2520identity%2520into%2520the%2520text%2520prompt.%2520Extensive%2520experiments%2520on%250Aseveral%2520popular%2520benchmarks%2520demonstrate%2520that%2520%25281%2529%2520the%2520cross-dataset%2520and%250Across-manipulation%2520performances%2520of%2520deepfake%2520detection%2520can%2520be%2520significantly%2520and%250Aconsistently%2520improved%2520%2528e.g.%252C%2520over%252088%2525%2520AUC%2520in%2520cross-dataset%2520setting%2520from%2520FF%252B%252B%2520to%250AWildDeepfake%2529%2520using%2520a%2520pre-trained%2520CLIP%2520model%2520with%2520our%2520proposed%2520reprogramming%250Amethod%253B%2520%25282%2529%2520our%2520superior%2520performances%2520are%2520at%2520less%2520cost%2520of%2520trainable%2520parameters%252C%250Amaking%2520it%2520a%2520promising%2520approach%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Standing%20on%20the%20Shoulders%20of%20Giants%3A%20Reprogramming%20Visual-Language%20Model%0A%20%20for%20General%20Deepfake%20Detection&entry.906535625=Kaiqing%20Lin%20and%20Yuzhen%20Lin%20and%20Weixiang%20Li%20and%20Taiping%20Yao%20and%20Bin%20Li&entry.1292438233=%20%20The%20proliferation%20of%20deepfake%20faces%20poses%20huge%20potential%20negative%20impacts%20on%0Aour%20daily%20lives.%20Despite%20substantial%20advancements%20in%20deepfake%20detection%20over%0Athese%20years%2C%20the%20generalizability%20of%20existing%20methods%20against%20forgeries%20from%0Aunseen%20datasets%20or%20created%20by%20emerging%20generative%20models%20remains%20constrained.%0AIn%20this%20paper%2C%20inspired%20by%20the%20zero-shot%20advantages%20of%20Vision-Language%20Models%0A%28VLMs%29%2C%20we%20propose%20a%20novel%20approach%20that%20repurposes%20a%20well-trained%20VLM%20for%0Ageneral%20deepfake%20detection.%20Motivated%20by%20the%20model%20reprogramming%20paradigm%20that%0Amanipulates%20the%20model%20prediction%20via%20data%20perturbations%2C%20our%20method%20can%0Areprogram%20a%20pretrained%20VLM%20model%20%28e.g.%2C%20CLIP%29%20solely%20based%20on%20manipulating%20its%0Ainput%20without%20tuning%20the%20inner%20parameters.%20Furthermore%2C%20we%20insert%20a%20pseudo-word%0Aguided%20by%20facial%20identity%20into%20the%20text%20prompt.%20Extensive%20experiments%20on%0Aseveral%20popular%20benchmarks%20demonstrate%20that%20%281%29%20the%20cross-dataset%20and%0Across-manipulation%20performances%20of%20deepfake%20detection%20can%20be%20significantly%20and%0Aconsistently%20improved%20%28e.g.%2C%20over%2088%25%20AUC%20in%20cross-dataset%20setting%20from%20FF%2B%2B%20to%0AWildDeepfake%29%20using%20a%20pre-trained%20CLIP%20model%20with%20our%20proposed%20reprogramming%0Amethod%3B%20%282%29%20our%20superior%20performances%20are%20at%20less%20cost%20of%20trainable%20parameters%2C%0Amaking%20it%20a%20promising%20approach%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02664v2&entry.124074799=Read"},
{"title": "Local-to-Global Self-Supervised Representation Learning for Diabetic\n  Retinopathy Grading", "author": "Mostafa Hajighasemlou and Samad Sheikhaei and Hamid Soltanian-Zadeh", "abstract": "  Artificial intelligence algorithms have demonstrated their image\nclassification and segmentation ability in the past decade. However, artificial\nintelligence algorithms perform less for actual clinical data than those used\nfor simulations. This research aims to present a novel hybrid learning model\nusing self-supervised learning and knowledge distillation, which can achieve\nsufficient generalization and robustness. The self-attention mechanism and\ntokens employed in ViT, besides the local-to-global learning approach used in\nthe hybrid model, enable the proposed algorithm to extract a high-dimensional\nand high-quality feature space from images. To demonstrate the proposed neural\nnetwork's capability in classifying and extracting feature spaces from medical\nimages, we use it on a dataset of Diabetic Retinopathy images, specifically the\nEyePACS dataset. This dataset is more complex structurally and challenging\nregarding damaged areas than other medical images. For the first time in this\nstudy, self-supervised learning and knowledge distillation are used to classify\nthis dataset. In our algorithm, for the first time among all self-supervised\nlearning and knowledge distillation models, the test dataset is 50% larger than\nthe training dataset. Unlike many studies, we have not removed any images from\nthe dataset. Finally, our algorithm achieved an accuracy of 79.1% in the linear\nclassifier and 74.36% in the k-NN algorithm for multiclass classification.\nCompared to a similar state-of-the-art model, our results achieved higher\naccuracy and more effective representation spaces.\n", "link": "http://arxiv.org/abs/2410.00779v3", "date": "2024-12-11", "relevancy": 2.7217, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5888}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5243}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local-to-Global%20Self-Supervised%20Representation%20Learning%20for%20Diabetic%0A%20%20Retinopathy%20Grading&body=Title%3A%20Local-to-Global%20Self-Supervised%20Representation%20Learning%20for%20Diabetic%0A%20%20Retinopathy%20Grading%0AAuthor%3A%20Mostafa%20Hajighasemlou%20and%20Samad%20Sheikhaei%20and%20Hamid%20Soltanian-Zadeh%0AAbstract%3A%20%20%20Artificial%20intelligence%20algorithms%20have%20demonstrated%20their%20image%0Aclassification%20and%20segmentation%20ability%20in%20the%20past%20decade.%20However%2C%20artificial%0Aintelligence%20algorithms%20perform%20less%20for%20actual%20clinical%20data%20than%20those%20used%0Afor%20simulations.%20This%20research%20aims%20to%20present%20a%20novel%20hybrid%20learning%20model%0Ausing%20self-supervised%20learning%20and%20knowledge%20distillation%2C%20which%20can%20achieve%0Asufficient%20generalization%20and%20robustness.%20The%20self-attention%20mechanism%20and%0Atokens%20employed%20in%20ViT%2C%20besides%20the%20local-to-global%20learning%20approach%20used%20in%0Athe%20hybrid%20model%2C%20enable%20the%20proposed%20algorithm%20to%20extract%20a%20high-dimensional%0Aand%20high-quality%20feature%20space%20from%20images.%20To%20demonstrate%20the%20proposed%20neural%0Anetwork%27s%20capability%20in%20classifying%20and%20extracting%20feature%20spaces%20from%20medical%0Aimages%2C%20we%20use%20it%20on%20a%20dataset%20of%20Diabetic%20Retinopathy%20images%2C%20specifically%20the%0AEyePACS%20dataset.%20This%20dataset%20is%20more%20complex%20structurally%20and%20challenging%0Aregarding%20damaged%20areas%20than%20other%20medical%20images.%20For%20the%20first%20time%20in%20this%0Astudy%2C%20self-supervised%20learning%20and%20knowledge%20distillation%20are%20used%20to%20classify%0Athis%20dataset.%20In%20our%20algorithm%2C%20for%20the%20first%20time%20among%20all%20self-supervised%0Alearning%20and%20knowledge%20distillation%20models%2C%20the%20test%20dataset%20is%2050%25%20larger%20than%0Athe%20training%20dataset.%20Unlike%20many%20studies%2C%20we%20have%20not%20removed%20any%20images%20from%0Athe%20dataset.%20Finally%2C%20our%20algorithm%20achieved%20an%20accuracy%20of%2079.1%25%20in%20the%20linear%0Aclassifier%20and%2074.36%25%20in%20the%20k-NN%20algorithm%20for%20multiclass%20classification.%0ACompared%20to%20a%20similar%20state-of-the-art%20model%2C%20our%20results%20achieved%20higher%0Aaccuracy%20and%20more%20effective%20representation%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00779v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal-to-Global%2520Self-Supervised%2520Representation%2520Learning%2520for%2520Diabetic%250A%2520%2520Retinopathy%2520Grading%26entry.906535625%3DMostafa%2520Hajighasemlou%2520and%2520Samad%2520Sheikhaei%2520and%2520Hamid%2520Soltanian-Zadeh%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520algorithms%2520have%2520demonstrated%2520their%2520image%250Aclassification%2520and%2520segmentation%2520ability%2520in%2520the%2520past%2520decade.%2520However%252C%2520artificial%250Aintelligence%2520algorithms%2520perform%2520less%2520for%2520actual%2520clinical%2520data%2520than%2520those%2520used%250Afor%2520simulations.%2520This%2520research%2520aims%2520to%2520present%2520a%2520novel%2520hybrid%2520learning%2520model%250Ausing%2520self-supervised%2520learning%2520and%2520knowledge%2520distillation%252C%2520which%2520can%2520achieve%250Asufficient%2520generalization%2520and%2520robustness.%2520The%2520self-attention%2520mechanism%2520and%250Atokens%2520employed%2520in%2520ViT%252C%2520besides%2520the%2520local-to-global%2520learning%2520approach%2520used%2520in%250Athe%2520hybrid%2520model%252C%2520enable%2520the%2520proposed%2520algorithm%2520to%2520extract%2520a%2520high-dimensional%250Aand%2520high-quality%2520feature%2520space%2520from%2520images.%2520To%2520demonstrate%2520the%2520proposed%2520neural%250Anetwork%2527s%2520capability%2520in%2520classifying%2520and%2520extracting%2520feature%2520spaces%2520from%2520medical%250Aimages%252C%2520we%2520use%2520it%2520on%2520a%2520dataset%2520of%2520Diabetic%2520Retinopathy%2520images%252C%2520specifically%2520the%250AEyePACS%2520dataset.%2520This%2520dataset%2520is%2520more%2520complex%2520structurally%2520and%2520challenging%250Aregarding%2520damaged%2520areas%2520than%2520other%2520medical%2520images.%2520For%2520the%2520first%2520time%2520in%2520this%250Astudy%252C%2520self-supervised%2520learning%2520and%2520knowledge%2520distillation%2520are%2520used%2520to%2520classify%250Athis%2520dataset.%2520In%2520our%2520algorithm%252C%2520for%2520the%2520first%2520time%2520among%2520all%2520self-supervised%250Alearning%2520and%2520knowledge%2520distillation%2520models%252C%2520the%2520test%2520dataset%2520is%252050%2525%2520larger%2520than%250Athe%2520training%2520dataset.%2520Unlike%2520many%2520studies%252C%2520we%2520have%2520not%2520removed%2520any%2520images%2520from%250Athe%2520dataset.%2520Finally%252C%2520our%2520algorithm%2520achieved%2520an%2520accuracy%2520of%252079.1%2525%2520in%2520the%2520linear%250Aclassifier%2520and%252074.36%2525%2520in%2520the%2520k-NN%2520algorithm%2520for%2520multiclass%2520classification.%250ACompared%2520to%2520a%2520similar%2520state-of-the-art%2520model%252C%2520our%2520results%2520achieved%2520higher%250Aaccuracy%2520and%2520more%2520effective%2520representation%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00779v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local-to-Global%20Self-Supervised%20Representation%20Learning%20for%20Diabetic%0A%20%20Retinopathy%20Grading&entry.906535625=Mostafa%20Hajighasemlou%20and%20Samad%20Sheikhaei%20and%20Hamid%20Soltanian-Zadeh&entry.1292438233=%20%20Artificial%20intelligence%20algorithms%20have%20demonstrated%20their%20image%0Aclassification%20and%20segmentation%20ability%20in%20the%20past%20decade.%20However%2C%20artificial%0Aintelligence%20algorithms%20perform%20less%20for%20actual%20clinical%20data%20than%20those%20used%0Afor%20simulations.%20This%20research%20aims%20to%20present%20a%20novel%20hybrid%20learning%20model%0Ausing%20self-supervised%20learning%20and%20knowledge%20distillation%2C%20which%20can%20achieve%0Asufficient%20generalization%20and%20robustness.%20The%20self-attention%20mechanism%20and%0Atokens%20employed%20in%20ViT%2C%20besides%20the%20local-to-global%20learning%20approach%20used%20in%0Athe%20hybrid%20model%2C%20enable%20the%20proposed%20algorithm%20to%20extract%20a%20high-dimensional%0Aand%20high-quality%20feature%20space%20from%20images.%20To%20demonstrate%20the%20proposed%20neural%0Anetwork%27s%20capability%20in%20classifying%20and%20extracting%20feature%20spaces%20from%20medical%0Aimages%2C%20we%20use%20it%20on%20a%20dataset%20of%20Diabetic%20Retinopathy%20images%2C%20specifically%20the%0AEyePACS%20dataset.%20This%20dataset%20is%20more%20complex%20structurally%20and%20challenging%0Aregarding%20damaged%20areas%20than%20other%20medical%20images.%20For%20the%20first%20time%20in%20this%0Astudy%2C%20self-supervised%20learning%20and%20knowledge%20distillation%20are%20used%20to%20classify%0Athis%20dataset.%20In%20our%20algorithm%2C%20for%20the%20first%20time%20among%20all%20self-supervised%0Alearning%20and%20knowledge%20distillation%20models%2C%20the%20test%20dataset%20is%2050%25%20larger%20than%0Athe%20training%20dataset.%20Unlike%20many%20studies%2C%20we%20have%20not%20removed%20any%20images%20from%0Athe%20dataset.%20Finally%2C%20our%20algorithm%20achieved%20an%20accuracy%20of%2079.1%25%20in%20the%20linear%0Aclassifier%20and%2074.36%25%20in%20the%20k-NN%20algorithm%20for%20multiclass%20classification.%0ACompared%20to%20a%20similar%20state-of-the-art%20model%2C%20our%20results%20achieved%20higher%0Aaccuracy%20and%20more%20effective%20representation%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00779v3&entry.124074799=Read"},
{"title": "OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong\n  Baseline", "author": "Xianda Guo and Chenming Zhang and Juntao Lu and Yiqun Duan and Yiqi Wang and Tian Yang and Zheng Zhu and Long Chen", "abstract": "  Stereo matching aims to estimate the disparity between matching pixels in a\nstereo image pair, which is important to robotics, autonomous driving, and\nother computer vision tasks. Despite the development of numerous impressive\nmethods in recent years, determining the most suitable architecture for\npractical application remains challenging. Addressing this gap, our paper\nintroduces a comprehensive benchmark focusing on practical applicability rather\nthan solely on individual models for optimized performance. Specifically, we\ndevelop a flexible and efficient stereo matching codebase, called OpenStereo.\nOpenStereo includes training and inference codes of more than 10 network\nmodels, making it, to our knowledge, the most complete stereo matching toolbox\navailable. Based on OpenStereo, we conducted experiments and have achieved or\nsurpassed the performance metrics reported in the original paper. Additionally,\nwe conduct an exhaustive analysis and deconstruction of recent developments in\nstereo matching through comprehensive ablative experiments. These\ninvestigations inspired the creation of StereoBase, a strong baseline model.\nOur StereoBase ranks 1st on SceneFlow, KITTI 2015, 2012 (Reflective) among\npublished methods and achieves the best performance across all metrics. In\naddition, StereoBase has strong cross-dataset generalization. Code is available\nat \\url{https://github.com/XiandaGuo/OpenStereo}.\n", "link": "http://arxiv.org/abs/2312.00343v8", "date": "2024-12-11", "relevancy": 2.7105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5506}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenStereo%3A%20A%20Comprehensive%20Benchmark%20for%20Stereo%20Matching%20and%20Strong%0A%20%20Baseline&body=Title%3A%20OpenStereo%3A%20A%20Comprehensive%20Benchmark%20for%20Stereo%20Matching%20and%20Strong%0A%20%20Baseline%0AAuthor%3A%20Xianda%20Guo%20and%20Chenming%20Zhang%20and%20Juntao%20Lu%20and%20Yiqun%20Duan%20and%20Yiqi%20Wang%20and%20Tian%20Yang%20and%20Zheng%20Zhu%20and%20Long%20Chen%0AAbstract%3A%20%20%20Stereo%20matching%20aims%20to%20estimate%20the%20disparity%20between%20matching%20pixels%20in%20a%0Astereo%20image%20pair%2C%20which%20is%20important%20to%20robotics%2C%20autonomous%20driving%2C%20and%0Aother%20computer%20vision%20tasks.%20Despite%20the%20development%20of%20numerous%20impressive%0Amethods%20in%20recent%20years%2C%20determining%20the%20most%20suitable%20architecture%20for%0Apractical%20application%20remains%20challenging.%20Addressing%20this%20gap%2C%20our%20paper%0Aintroduces%20a%20comprehensive%20benchmark%20focusing%20on%20practical%20applicability%20rather%0Athan%20solely%20on%20individual%20models%20for%20optimized%20performance.%20Specifically%2C%20we%0Adevelop%20a%20flexible%20and%20efficient%20stereo%20matching%20codebase%2C%20called%20OpenStereo.%0AOpenStereo%20includes%20training%20and%20inference%20codes%20of%20more%20than%2010%20network%0Amodels%2C%20making%20it%2C%20to%20our%20knowledge%2C%20the%20most%20complete%20stereo%20matching%20toolbox%0Aavailable.%20Based%20on%20OpenStereo%2C%20we%20conducted%20experiments%20and%20have%20achieved%20or%0Asurpassed%20the%20performance%20metrics%20reported%20in%20the%20original%20paper.%20Additionally%2C%0Awe%20conduct%20an%20exhaustive%20analysis%20and%20deconstruction%20of%20recent%20developments%20in%0Astereo%20matching%20through%20comprehensive%20ablative%20experiments.%20These%0Ainvestigations%20inspired%20the%20creation%20of%20StereoBase%2C%20a%20strong%20baseline%20model.%0AOur%20StereoBase%20ranks%201st%20on%20SceneFlow%2C%20KITTI%202015%2C%202012%20%28Reflective%29%20among%0Apublished%20methods%20and%20achieves%20the%20best%20performance%20across%20all%20metrics.%20In%0Aaddition%2C%20StereoBase%20has%20strong%20cross-dataset%20generalization.%20Code%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/XiandaGuo/OpenStereo%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00343v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenStereo%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Stereo%2520Matching%2520and%2520Strong%250A%2520%2520Baseline%26entry.906535625%3DXianda%2520Guo%2520and%2520Chenming%2520Zhang%2520and%2520Juntao%2520Lu%2520and%2520Yiqun%2520Duan%2520and%2520Yiqi%2520Wang%2520and%2520Tian%2520Yang%2520and%2520Zheng%2520Zhu%2520and%2520Long%2520Chen%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520aims%2520to%2520estimate%2520the%2520disparity%2520between%2520matching%2520pixels%2520in%2520a%250Astereo%2520image%2520pair%252C%2520which%2520is%2520important%2520to%2520robotics%252C%2520autonomous%2520driving%252C%2520and%250Aother%2520computer%2520vision%2520tasks.%2520Despite%2520the%2520development%2520of%2520numerous%2520impressive%250Amethods%2520in%2520recent%2520years%252C%2520determining%2520the%2520most%2520suitable%2520architecture%2520for%250Apractical%2520application%2520remains%2520challenging.%2520Addressing%2520this%2520gap%252C%2520our%2520paper%250Aintroduces%2520a%2520comprehensive%2520benchmark%2520focusing%2520on%2520practical%2520applicability%2520rather%250Athan%2520solely%2520on%2520individual%2520models%2520for%2520optimized%2520performance.%2520Specifically%252C%2520we%250Adevelop%2520a%2520flexible%2520and%2520efficient%2520stereo%2520matching%2520codebase%252C%2520called%2520OpenStereo.%250AOpenStereo%2520includes%2520training%2520and%2520inference%2520codes%2520of%2520more%2520than%252010%2520network%250Amodels%252C%2520making%2520it%252C%2520to%2520our%2520knowledge%252C%2520the%2520most%2520complete%2520stereo%2520matching%2520toolbox%250Aavailable.%2520Based%2520on%2520OpenStereo%252C%2520we%2520conducted%2520experiments%2520and%2520have%2520achieved%2520or%250Asurpassed%2520the%2520performance%2520metrics%2520reported%2520in%2520the%2520original%2520paper.%2520Additionally%252C%250Awe%2520conduct%2520an%2520exhaustive%2520analysis%2520and%2520deconstruction%2520of%2520recent%2520developments%2520in%250Astereo%2520matching%2520through%2520comprehensive%2520ablative%2520experiments.%2520These%250Ainvestigations%2520inspired%2520the%2520creation%2520of%2520StereoBase%252C%2520a%2520strong%2520baseline%2520model.%250AOur%2520StereoBase%2520ranks%25201st%2520on%2520SceneFlow%252C%2520KITTI%25202015%252C%25202012%2520%2528Reflective%2529%2520among%250Apublished%2520methods%2520and%2520achieves%2520the%2520best%2520performance%2520across%2520all%2520metrics.%2520In%250Aaddition%252C%2520StereoBase%2520has%2520strong%2520cross-dataset%2520generalization.%2520Code%2520is%2520available%250Aat%2520%255Curl%257Bhttps%253A//github.com/XiandaGuo/OpenStereo%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00343v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenStereo%3A%20A%20Comprehensive%20Benchmark%20for%20Stereo%20Matching%20and%20Strong%0A%20%20Baseline&entry.906535625=Xianda%20Guo%20and%20Chenming%20Zhang%20and%20Juntao%20Lu%20and%20Yiqun%20Duan%20and%20Yiqi%20Wang%20and%20Tian%20Yang%20and%20Zheng%20Zhu%20and%20Long%20Chen&entry.1292438233=%20%20Stereo%20matching%20aims%20to%20estimate%20the%20disparity%20between%20matching%20pixels%20in%20a%0Astereo%20image%20pair%2C%20which%20is%20important%20to%20robotics%2C%20autonomous%20driving%2C%20and%0Aother%20computer%20vision%20tasks.%20Despite%20the%20development%20of%20numerous%20impressive%0Amethods%20in%20recent%20years%2C%20determining%20the%20most%20suitable%20architecture%20for%0Apractical%20application%20remains%20challenging.%20Addressing%20this%20gap%2C%20our%20paper%0Aintroduces%20a%20comprehensive%20benchmark%20focusing%20on%20practical%20applicability%20rather%0Athan%20solely%20on%20individual%20models%20for%20optimized%20performance.%20Specifically%2C%20we%0Adevelop%20a%20flexible%20and%20efficient%20stereo%20matching%20codebase%2C%20called%20OpenStereo.%0AOpenStereo%20includes%20training%20and%20inference%20codes%20of%20more%20than%2010%20network%0Amodels%2C%20making%20it%2C%20to%20our%20knowledge%2C%20the%20most%20complete%20stereo%20matching%20toolbox%0Aavailable.%20Based%20on%20OpenStereo%2C%20we%20conducted%20experiments%20and%20have%20achieved%20or%0Asurpassed%20the%20performance%20metrics%20reported%20in%20the%20original%20paper.%20Additionally%2C%0Awe%20conduct%20an%20exhaustive%20analysis%20and%20deconstruction%20of%20recent%20developments%20in%0Astereo%20matching%20through%20comprehensive%20ablative%20experiments.%20These%0Ainvestigations%20inspired%20the%20creation%20of%20StereoBase%2C%20a%20strong%20baseline%20model.%0AOur%20StereoBase%20ranks%201st%20on%20SceneFlow%2C%20KITTI%202015%2C%202012%20%28Reflective%29%20among%0Apublished%20methods%20and%20achieves%20the%20best%20performance%20across%20all%20metrics.%20In%0Aaddition%2C%20StereoBase%20has%20strong%20cross-dataset%20generalization.%20Code%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/XiandaGuo/OpenStereo%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00343v8&entry.124074799=Read"},
{"title": "Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse\n  Brain Tumors in MRI from Gliomas to Pediatric Tumors", "author": "Ramy A. Zeineldin and Franziska Mathis-Ullrich", "abstract": "  Accurate segmentation of brain tumors from 3D multimodal MRI is vital for\ndiagnosis and treatment planning across diverse brain tumors. This paper\naddresses the challenges posed by the BraTS 2023, presenting a unified transfer\nlearning approach that applies to a broader spectrum of brain tumors. We\nintroduce HT-CNNs, an ensemble of Hybrid Transformers and Convolutional Neural\nNetworks optimized through transfer learning for varied brain tumor\nsegmentation. This method captures spatial and contextual details from MRI\ndata, fine-tuned on diverse datasets representing common tumor types. Through\ntransfer learning, HT-CNNs utilize the learned representations from one task to\nimprove generalization in another, harnessing the power of pre-trained models\non large datasets and fine-tuning them on specific tumor types. We preprocess\ndiverse datasets from multiple international distributions, ensuring\nrepresentativeness for the most common brain tumors. Our rigorous evaluation\nemploys standardized quantitative metrics across all tumor types, ensuring\nrobustness and generalizability. The proposed ensemble model achieves superior\nsegmentation results across the BraTS validation datasets over the previous\nwinning methods. Comprehensive quantitative evaluations using the DSC and HD95\ndemonstrate the effectiveness of our approach. Qualitative segmentation\npredictions further validate the high-quality outputs produced by our model.\nOur findings underscore the potential of transfer learning and ensemble\napproaches in medical image segmentation, indicating a substantial enhancement\nin clinical decision-making and patient care. Despite facing challenges related\nto post-processing and domain gaps, our study sets a new precedent for future\nresearch for brain tumor segmentation. The docker image for the code and models\nhas been made publicly available, https://hub.docker.com/r/razeineldin/ht-cnns.\n", "link": "http://arxiv.org/abs/2412.08240v1", "date": "2024-12-11", "relevancy": 2.71, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5726}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20HT-CNNs%20Architecture%3A%20Transfer%20Learning%20for%20Segmenting%20Diverse%0A%20%20Brain%20Tumors%20in%20MRI%20from%20Gliomas%20to%20Pediatric%20Tumors&body=Title%3A%20Unified%20HT-CNNs%20Architecture%3A%20Transfer%20Learning%20for%20Segmenting%20Diverse%0A%20%20Brain%20Tumors%20in%20MRI%20from%20Gliomas%20to%20Pediatric%20Tumors%0AAuthor%3A%20Ramy%20A.%20Zeineldin%20and%20Franziska%20Mathis-Ullrich%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20brain%20tumors%20from%203D%20multimodal%20MRI%20is%20vital%20for%0Adiagnosis%20and%20treatment%20planning%20across%20diverse%20brain%20tumors.%20This%20paper%0Aaddresses%20the%20challenges%20posed%20by%20the%20BraTS%202023%2C%20presenting%20a%20unified%20transfer%0Alearning%20approach%20that%20applies%20to%20a%20broader%20spectrum%20of%20brain%20tumors.%20We%0Aintroduce%20HT-CNNs%2C%20an%20ensemble%20of%20Hybrid%20Transformers%20and%20Convolutional%20Neural%0ANetworks%20optimized%20through%20transfer%20learning%20for%20varied%20brain%20tumor%0Asegmentation.%20This%20method%20captures%20spatial%20and%20contextual%20details%20from%20MRI%0Adata%2C%20fine-tuned%20on%20diverse%20datasets%20representing%20common%20tumor%20types.%20Through%0Atransfer%20learning%2C%20HT-CNNs%20utilize%20the%20learned%20representations%20from%20one%20task%20to%0Aimprove%20generalization%20in%20another%2C%20harnessing%20the%20power%20of%20pre-trained%20models%0Aon%20large%20datasets%20and%20fine-tuning%20them%20on%20specific%20tumor%20types.%20We%20preprocess%0Adiverse%20datasets%20from%20multiple%20international%20distributions%2C%20ensuring%0Arepresentativeness%20for%20the%20most%20common%20brain%20tumors.%20Our%20rigorous%20evaluation%0Aemploys%20standardized%20quantitative%20metrics%20across%20all%20tumor%20types%2C%20ensuring%0Arobustness%20and%20generalizability.%20The%20proposed%20ensemble%20model%20achieves%20superior%0Asegmentation%20results%20across%20the%20BraTS%20validation%20datasets%20over%20the%20previous%0Awinning%20methods.%20Comprehensive%20quantitative%20evaluations%20using%20the%20DSC%20and%20HD95%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%20Qualitative%20segmentation%0Apredictions%20further%20validate%20the%20high-quality%20outputs%20produced%20by%20our%20model.%0AOur%20findings%20underscore%20the%20potential%20of%20transfer%20learning%20and%20ensemble%0Aapproaches%20in%20medical%20image%20segmentation%2C%20indicating%20a%20substantial%20enhancement%0Ain%20clinical%20decision-making%20and%20patient%20care.%20Despite%20facing%20challenges%20related%0Ato%20post-processing%20and%20domain%20gaps%2C%20our%20study%20sets%20a%20new%20precedent%20for%20future%0Aresearch%20for%20brain%20tumor%20segmentation.%20The%20docker%20image%20for%20the%20code%20and%20models%0Ahas%20been%20made%20publicly%20available%2C%20https%3A//hub.docker.com/r/razeineldin/ht-cnns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520HT-CNNs%2520Architecture%253A%2520Transfer%2520Learning%2520for%2520Segmenting%2520Diverse%250A%2520%2520Brain%2520Tumors%2520in%2520MRI%2520from%2520Gliomas%2520to%2520Pediatric%2520Tumors%26entry.906535625%3DRamy%2520A.%2520Zeineldin%2520and%2520Franziska%2520Mathis-Ullrich%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520brain%2520tumors%2520from%25203D%2520multimodal%2520MRI%2520is%2520vital%2520for%250Adiagnosis%2520and%2520treatment%2520planning%2520across%2520diverse%2520brain%2520tumors.%2520This%2520paper%250Aaddresses%2520the%2520challenges%2520posed%2520by%2520the%2520BraTS%25202023%252C%2520presenting%2520a%2520unified%2520transfer%250Alearning%2520approach%2520that%2520applies%2520to%2520a%2520broader%2520spectrum%2520of%2520brain%2520tumors.%2520We%250Aintroduce%2520HT-CNNs%252C%2520an%2520ensemble%2520of%2520Hybrid%2520Transformers%2520and%2520Convolutional%2520Neural%250ANetworks%2520optimized%2520through%2520transfer%2520learning%2520for%2520varied%2520brain%2520tumor%250Asegmentation.%2520This%2520method%2520captures%2520spatial%2520and%2520contextual%2520details%2520from%2520MRI%250Adata%252C%2520fine-tuned%2520on%2520diverse%2520datasets%2520representing%2520common%2520tumor%2520types.%2520Through%250Atransfer%2520learning%252C%2520HT-CNNs%2520utilize%2520the%2520learned%2520representations%2520from%2520one%2520task%2520to%250Aimprove%2520generalization%2520in%2520another%252C%2520harnessing%2520the%2520power%2520of%2520pre-trained%2520models%250Aon%2520large%2520datasets%2520and%2520fine-tuning%2520them%2520on%2520specific%2520tumor%2520types.%2520We%2520preprocess%250Adiverse%2520datasets%2520from%2520multiple%2520international%2520distributions%252C%2520ensuring%250Arepresentativeness%2520for%2520the%2520most%2520common%2520brain%2520tumors.%2520Our%2520rigorous%2520evaluation%250Aemploys%2520standardized%2520quantitative%2520metrics%2520across%2520all%2520tumor%2520types%252C%2520ensuring%250Arobustness%2520and%2520generalizability.%2520The%2520proposed%2520ensemble%2520model%2520achieves%2520superior%250Asegmentation%2520results%2520across%2520the%2520BraTS%2520validation%2520datasets%2520over%2520the%2520previous%250Awinning%2520methods.%2520Comprehensive%2520quantitative%2520evaluations%2520using%2520the%2520DSC%2520and%2520HD95%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Qualitative%2520segmentation%250Apredictions%2520further%2520validate%2520the%2520high-quality%2520outputs%2520produced%2520by%2520our%2520model.%250AOur%2520findings%2520underscore%2520the%2520potential%2520of%2520transfer%2520learning%2520and%2520ensemble%250Aapproaches%2520in%2520medical%2520image%2520segmentation%252C%2520indicating%2520a%2520substantial%2520enhancement%250Ain%2520clinical%2520decision-making%2520and%2520patient%2520care.%2520Despite%2520facing%2520challenges%2520related%250Ato%2520post-processing%2520and%2520domain%2520gaps%252C%2520our%2520study%2520sets%2520a%2520new%2520precedent%2520for%2520future%250Aresearch%2520for%2520brain%2520tumor%2520segmentation.%2520The%2520docker%2520image%2520for%2520the%2520code%2520and%2520models%250Ahas%2520been%2520made%2520publicly%2520available%252C%2520https%253A//hub.docker.com/r/razeineldin/ht-cnns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20HT-CNNs%20Architecture%3A%20Transfer%20Learning%20for%20Segmenting%20Diverse%0A%20%20Brain%20Tumors%20in%20MRI%20from%20Gliomas%20to%20Pediatric%20Tumors&entry.906535625=Ramy%20A.%20Zeineldin%20and%20Franziska%20Mathis-Ullrich&entry.1292438233=%20%20Accurate%20segmentation%20of%20brain%20tumors%20from%203D%20multimodal%20MRI%20is%20vital%20for%0Adiagnosis%20and%20treatment%20planning%20across%20diverse%20brain%20tumors.%20This%20paper%0Aaddresses%20the%20challenges%20posed%20by%20the%20BraTS%202023%2C%20presenting%20a%20unified%20transfer%0Alearning%20approach%20that%20applies%20to%20a%20broader%20spectrum%20of%20brain%20tumors.%20We%0Aintroduce%20HT-CNNs%2C%20an%20ensemble%20of%20Hybrid%20Transformers%20and%20Convolutional%20Neural%0ANetworks%20optimized%20through%20transfer%20learning%20for%20varied%20brain%20tumor%0Asegmentation.%20This%20method%20captures%20spatial%20and%20contextual%20details%20from%20MRI%0Adata%2C%20fine-tuned%20on%20diverse%20datasets%20representing%20common%20tumor%20types.%20Through%0Atransfer%20learning%2C%20HT-CNNs%20utilize%20the%20learned%20representations%20from%20one%20task%20to%0Aimprove%20generalization%20in%20another%2C%20harnessing%20the%20power%20of%20pre-trained%20models%0Aon%20large%20datasets%20and%20fine-tuning%20them%20on%20specific%20tumor%20types.%20We%20preprocess%0Adiverse%20datasets%20from%20multiple%20international%20distributions%2C%20ensuring%0Arepresentativeness%20for%20the%20most%20common%20brain%20tumors.%20Our%20rigorous%20evaluation%0Aemploys%20standardized%20quantitative%20metrics%20across%20all%20tumor%20types%2C%20ensuring%0Arobustness%20and%20generalizability.%20The%20proposed%20ensemble%20model%20achieves%20superior%0Asegmentation%20results%20across%20the%20BraTS%20validation%20datasets%20over%20the%20previous%0Awinning%20methods.%20Comprehensive%20quantitative%20evaluations%20using%20the%20DSC%20and%20HD95%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%20Qualitative%20segmentation%0Apredictions%20further%20validate%20the%20high-quality%20outputs%20produced%20by%20our%20model.%0AOur%20findings%20underscore%20the%20potential%20of%20transfer%20learning%20and%20ensemble%0Aapproaches%20in%20medical%20image%20segmentation%2C%20indicating%20a%20substantial%20enhancement%0Ain%20clinical%20decision-making%20and%20patient%20care.%20Despite%20facing%20challenges%20related%0Ato%20post-processing%20and%20domain%20gaps%2C%20our%20study%20sets%20a%20new%20precedent%20for%20future%0Aresearch%20for%20brain%20tumor%20segmentation.%20The%20docker%20image%20for%20the%20code%20and%20models%0Ahas%20been%20made%20publicly%20available%2C%20https%3A//hub.docker.com/r/razeineldin/ht-cnns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08240v1&entry.124074799=Read"},
{"title": "Advancing Single- and Multi-task Text Classification through Large\n  Language Model Fine-tuning", "author": "Hang Zhao and Qile P. Chen and Yijing Barry Zhang and Gang Yang", "abstract": "  Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance.\n", "link": "http://arxiv.org/abs/2412.08587v1", "date": "2024-12-11", "relevancy": 2.6945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5393}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Single-%20and%20Multi-task%20Text%20Classification%20through%20Large%0A%20%20Language%20Model%20Fine-tuning&body=Title%3A%20Advancing%20Single-%20and%20Multi-task%20Text%20Classification%20through%20Large%0A%20%20Language%20Model%20Fine-tuning%0AAuthor%3A%20Hang%20Zhao%20and%20Qile%20P.%20Chen%20and%20Yijing%20Barry%20Zhang%20and%20Gang%20Yang%0AAbstract%3A%20%20%20Both%20encoder-only%20models%20%28e.g.%2C%20BERT%2C%20RoBERTa%29%20and%20large%20language%20models%0A%28LLMs%2C%20e.g.%2C%20Llama3%29%20have%20been%20widely%20used%20for%20text%20classification%20tasks.%0AHowever%2C%20there%20is%20a%20lack%20of%20systematic%20studies%20comparing%20the%20performance%20of%0Aencoder-based%20models%20and%20LLMs%20in%20text%20classification%2C%20particularly%20when%0Afine-tuning%20is%20involved.%20This%20study%20employed%20a%20diverse%20range%20of%20models%20and%0Amethods%2C%20varying%20in%20size%20and%20architecture%2C%20and%20including%20both%20fine-tuned%20and%0Apre-trained%20approaches.%20We%20first%20assessed%20the%20performances%20of%20these%20LLMs%20on%20the%0A20%20Newsgroups%20%2820NG%29%20and%20MASSIVE%20datasets%2C%20comparing%20them%20to%20encoder-only%0ARoBERTa%20models.%20Additionally%2C%20we%20explored%20the%20multi-task%20capabilities%20of%20both%0Amodel%20types%20by%20combining%20multiple%20classification%20tasks%2C%20including%20intent%0Adetection%20and%20slot-filling%2C%20into%20a%20single%20model%20using%20data%20from%20both%20datasets.%0AOur%20results%20indicate%20that%20fully%20fine-tuned%20Llama3-70B%20models%20outperform%0ARoBERTa-large%20and%20other%20decoder%20LLMs%20across%20various%20classification%20tasks%20and%0Adatasets.%20Moreover%2C%20the%20consolidated%20multi-task%20fine-tuned%20LLMs%20matched%20the%0Aperformance%20of%20dual-model%20setups%20in%20both%20tasks%20across%20both%20datasets.%20Overall%2C%0Aour%20study%20provides%20a%20comprehensive%20benchmark%20of%20encoder-only%20and%20LLM%20models%20on%0Atext%20classification%20tasks%20and%20demonstrates%20a%20method%20to%20combine%20two%20or%20more%0Afully%20fine-tuned%20decoder%20LLMs%20for%20reduced%20latency%20and%20equivalent%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Single-%2520and%2520Multi-task%2520Text%2520Classification%2520through%2520Large%250A%2520%2520Language%2520Model%2520Fine-tuning%26entry.906535625%3DHang%2520Zhao%2520and%2520Qile%2520P.%2520Chen%2520and%2520Yijing%2520Barry%2520Zhang%2520and%2520Gang%2520Yang%26entry.1292438233%3D%2520%2520Both%2520encoder-only%2520models%2520%2528e.g.%252C%2520BERT%252C%2520RoBERTa%2529%2520and%2520large%2520language%2520models%250A%2528LLMs%252C%2520e.g.%252C%2520Llama3%2529%2520have%2520been%2520widely%2520used%2520for%2520text%2520classification%2520tasks.%250AHowever%252C%2520there%2520is%2520a%2520lack%2520of%2520systematic%2520studies%2520comparing%2520the%2520performance%2520of%250Aencoder-based%2520models%2520and%2520LLMs%2520in%2520text%2520classification%252C%2520particularly%2520when%250Afine-tuning%2520is%2520involved.%2520This%2520study%2520employed%2520a%2520diverse%2520range%2520of%2520models%2520and%250Amethods%252C%2520varying%2520in%2520size%2520and%2520architecture%252C%2520and%2520including%2520both%2520fine-tuned%2520and%250Apre-trained%2520approaches.%2520We%2520first%2520assessed%2520the%2520performances%2520of%2520these%2520LLMs%2520on%2520the%250A20%2520Newsgroups%2520%252820NG%2529%2520and%2520MASSIVE%2520datasets%252C%2520comparing%2520them%2520to%2520encoder-only%250ARoBERTa%2520models.%2520Additionally%252C%2520we%2520explored%2520the%2520multi-task%2520capabilities%2520of%2520both%250Amodel%2520types%2520by%2520combining%2520multiple%2520classification%2520tasks%252C%2520including%2520intent%250Adetection%2520and%2520slot-filling%252C%2520into%2520a%2520single%2520model%2520using%2520data%2520from%2520both%2520datasets.%250AOur%2520results%2520indicate%2520that%2520fully%2520fine-tuned%2520Llama3-70B%2520models%2520outperform%250ARoBERTa-large%2520and%2520other%2520decoder%2520LLMs%2520across%2520various%2520classification%2520tasks%2520and%250Adatasets.%2520Moreover%252C%2520the%2520consolidated%2520multi-task%2520fine-tuned%2520LLMs%2520matched%2520the%250Aperformance%2520of%2520dual-model%2520setups%2520in%2520both%2520tasks%2520across%2520both%2520datasets.%2520Overall%252C%250Aour%2520study%2520provides%2520a%2520comprehensive%2520benchmark%2520of%2520encoder-only%2520and%2520LLM%2520models%2520on%250Atext%2520classification%2520tasks%2520and%2520demonstrates%2520a%2520method%2520to%2520combine%2520two%2520or%2520more%250Afully%2520fine-tuned%2520decoder%2520LLMs%2520for%2520reduced%2520latency%2520and%2520equivalent%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Single-%20and%20Multi-task%20Text%20Classification%20through%20Large%0A%20%20Language%20Model%20Fine-tuning&entry.906535625=Hang%20Zhao%20and%20Qile%20P.%20Chen%20and%20Yijing%20Barry%20Zhang%20and%20Gang%20Yang&entry.1292438233=%20%20Both%20encoder-only%20models%20%28e.g.%2C%20BERT%2C%20RoBERTa%29%20and%20large%20language%20models%0A%28LLMs%2C%20e.g.%2C%20Llama3%29%20have%20been%20widely%20used%20for%20text%20classification%20tasks.%0AHowever%2C%20there%20is%20a%20lack%20of%20systematic%20studies%20comparing%20the%20performance%20of%0Aencoder-based%20models%20and%20LLMs%20in%20text%20classification%2C%20particularly%20when%0Afine-tuning%20is%20involved.%20This%20study%20employed%20a%20diverse%20range%20of%20models%20and%0Amethods%2C%20varying%20in%20size%20and%20architecture%2C%20and%20including%20both%20fine-tuned%20and%0Apre-trained%20approaches.%20We%20first%20assessed%20the%20performances%20of%20these%20LLMs%20on%20the%0A20%20Newsgroups%20%2820NG%29%20and%20MASSIVE%20datasets%2C%20comparing%20them%20to%20encoder-only%0ARoBERTa%20models.%20Additionally%2C%20we%20explored%20the%20multi-task%20capabilities%20of%20both%0Amodel%20types%20by%20combining%20multiple%20classification%20tasks%2C%20including%20intent%0Adetection%20and%20slot-filling%2C%20into%20a%20single%20model%20using%20data%20from%20both%20datasets.%0AOur%20results%20indicate%20that%20fully%20fine-tuned%20Llama3-70B%20models%20outperform%0ARoBERTa-large%20and%20other%20decoder%20LLMs%20across%20various%20classification%20tasks%20and%0Adatasets.%20Moreover%2C%20the%20consolidated%20multi-task%20fine-tuned%20LLMs%20matched%20the%0Aperformance%20of%20dual-model%20setups%20in%20both%20tasks%20across%20both%20datasets.%20Overall%2C%0Aour%20study%20provides%20a%20comprehensive%20benchmark%20of%20encoder-only%20and%20LLM%20models%20on%0Atext%20classification%20tasks%20and%20demonstrates%20a%20method%20to%20combine%20two%20or%20more%0Afully%20fine-tuned%20decoder%20LLMs%20for%20reduced%20latency%20and%20equivalent%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08587v1&entry.124074799=Read"},
{"title": "SegFace: Face Segmentation of Long-Tail Classes", "author": "Kartik Narayan and Vibashan VS and Vishal M. Patel", "abstract": "  Face parsing refers to the semantic segmentation of human faces into key\nfacial regions such as eyes, nose, hair, etc. It serves as a prerequisite for\nvarious advanced applications, including face editing, face swapping, and\nfacial makeup, which often require segmentation masks for classes like\neyeglasses, hats, earrings, and necklaces. These infrequently occurring classes\nare called long-tail classes, which are overshadowed by more frequently\noccurring classes known as head classes. Existing methods, primarily CNN-based,\ntend to be dominated by head classes during training, resulting in suboptimal\nrepresentation for long-tail classes. Previous works have largely overlooked\nthe problem of poor segmentation performance of long-tail classes. To address\nthis issue, we propose SegFace, a simple and efficient approach that uses a\nlightweight transformer-based model which utilizes learnable class-specific\ntokens. The transformer decoder leverages class-specific tokens, allowing each\ntoken to focus on its corresponding class, thereby enabling independent\nmodeling of each class. The proposed approach improves the performance of\nlong-tail classes, thereby boosting overall performance. To the best of our\nknowledge, SegFace is the first work to employ transformer models for face\nparsing. Moreover, our approach can be adapted for low-compute edge devices,\nachieving 95.96 FPS. We conduct extensive experiments demonstrating that\nSegFace significantly outperforms previous state-of-the-art models, achieving a\nmean F1 score of 88.96 (+2.82) on the CelebAMask-HQ dataset and 93.03 (+0.65)\non the LaPa dataset. Code: https://github.com/Kartik-3004/SegFace\n", "link": "http://arxiv.org/abs/2412.08647v1", "date": "2024-12-11", "relevancy": 2.6888, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.544}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5435}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegFace%3A%20Face%20Segmentation%20of%20Long-Tail%20Classes&body=Title%3A%20SegFace%3A%20Face%20Segmentation%20of%20Long-Tail%20Classes%0AAuthor%3A%20Kartik%20Narayan%20and%20Vibashan%20VS%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20Face%20parsing%20refers%20to%20the%20semantic%20segmentation%20of%20human%20faces%20into%20key%0Afacial%20regions%20such%20as%20eyes%2C%20nose%2C%20hair%2C%20etc.%20It%20serves%20as%20a%20prerequisite%20for%0Avarious%20advanced%20applications%2C%20including%20face%20editing%2C%20face%20swapping%2C%20and%0Afacial%20makeup%2C%20which%20often%20require%20segmentation%20masks%20for%20classes%20like%0Aeyeglasses%2C%20hats%2C%20earrings%2C%20and%20necklaces.%20These%20infrequently%20occurring%20classes%0Aare%20called%20long-tail%20classes%2C%20which%20are%20overshadowed%20by%20more%20frequently%0Aoccurring%20classes%20known%20as%20head%20classes.%20Existing%20methods%2C%20primarily%20CNN-based%2C%0Atend%20to%20be%20dominated%20by%20head%20classes%20during%20training%2C%20resulting%20in%20suboptimal%0Arepresentation%20for%20long-tail%20classes.%20Previous%20works%20have%20largely%20overlooked%0Athe%20problem%20of%20poor%20segmentation%20performance%20of%20long-tail%20classes.%20To%20address%0Athis%20issue%2C%20we%20propose%20SegFace%2C%20a%20simple%20and%20efficient%20approach%20that%20uses%20a%0Alightweight%20transformer-based%20model%20which%20utilizes%20learnable%20class-specific%0Atokens.%20The%20transformer%20decoder%20leverages%20class-specific%20tokens%2C%20allowing%20each%0Atoken%20to%20focus%20on%20its%20corresponding%20class%2C%20thereby%20enabling%20independent%0Amodeling%20of%20each%20class.%20The%20proposed%20approach%20improves%20the%20performance%20of%0Along-tail%20classes%2C%20thereby%20boosting%20overall%20performance.%20To%20the%20best%20of%20our%0Aknowledge%2C%20SegFace%20is%20the%20first%20work%20to%20employ%20transformer%20models%20for%20face%0Aparsing.%20Moreover%2C%20our%20approach%20can%20be%20adapted%20for%20low-compute%20edge%20devices%2C%0Aachieving%2095.96%20FPS.%20We%20conduct%20extensive%20experiments%20demonstrating%20that%0ASegFace%20significantly%20outperforms%20previous%20state-of-the-art%20models%2C%20achieving%20a%0Amean%20F1%20score%20of%2088.96%20%28%2B2.82%29%20on%20the%20CelebAMask-HQ%20dataset%20and%2093.03%20%28%2B0.65%29%0Aon%20the%20LaPa%20dataset.%20Code%3A%20https%3A//github.com/Kartik-3004/SegFace%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegFace%253A%2520Face%2520Segmentation%2520of%2520Long-Tail%2520Classes%26entry.906535625%3DKartik%2520Narayan%2520and%2520Vibashan%2520VS%2520and%2520Vishal%2520M.%2520Patel%26entry.1292438233%3D%2520%2520Face%2520parsing%2520refers%2520to%2520the%2520semantic%2520segmentation%2520of%2520human%2520faces%2520into%2520key%250Afacial%2520regions%2520such%2520as%2520eyes%252C%2520nose%252C%2520hair%252C%2520etc.%2520It%2520serves%2520as%2520a%2520prerequisite%2520for%250Avarious%2520advanced%2520applications%252C%2520including%2520face%2520editing%252C%2520face%2520swapping%252C%2520and%250Afacial%2520makeup%252C%2520which%2520often%2520require%2520segmentation%2520masks%2520for%2520classes%2520like%250Aeyeglasses%252C%2520hats%252C%2520earrings%252C%2520and%2520necklaces.%2520These%2520infrequently%2520occurring%2520classes%250Aare%2520called%2520long-tail%2520classes%252C%2520which%2520are%2520overshadowed%2520by%2520more%2520frequently%250Aoccurring%2520classes%2520known%2520as%2520head%2520classes.%2520Existing%2520methods%252C%2520primarily%2520CNN-based%252C%250Atend%2520to%2520be%2520dominated%2520by%2520head%2520classes%2520during%2520training%252C%2520resulting%2520in%2520suboptimal%250Arepresentation%2520for%2520long-tail%2520classes.%2520Previous%2520works%2520have%2520largely%2520overlooked%250Athe%2520problem%2520of%2520poor%2520segmentation%2520performance%2520of%2520long-tail%2520classes.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520SegFace%252C%2520a%2520simple%2520and%2520efficient%2520approach%2520that%2520uses%2520a%250Alightweight%2520transformer-based%2520model%2520which%2520utilizes%2520learnable%2520class-specific%250Atokens.%2520The%2520transformer%2520decoder%2520leverages%2520class-specific%2520tokens%252C%2520allowing%2520each%250Atoken%2520to%2520focus%2520on%2520its%2520corresponding%2520class%252C%2520thereby%2520enabling%2520independent%250Amodeling%2520of%2520each%2520class.%2520The%2520proposed%2520approach%2520improves%2520the%2520performance%2520of%250Along-tail%2520classes%252C%2520thereby%2520boosting%2520overall%2520performance.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520SegFace%2520is%2520the%2520first%2520work%2520to%2520employ%2520transformer%2520models%2520for%2520face%250Aparsing.%2520Moreover%252C%2520our%2520approach%2520can%2520be%2520adapted%2520for%2520low-compute%2520edge%2520devices%252C%250Aachieving%252095.96%2520FPS.%2520We%2520conduct%2520extensive%2520experiments%2520demonstrating%2520that%250ASegFace%2520significantly%2520outperforms%2520previous%2520state-of-the-art%2520models%252C%2520achieving%2520a%250Amean%2520F1%2520score%2520of%252088.96%2520%2528%252B2.82%2529%2520on%2520the%2520CelebAMask-HQ%2520dataset%2520and%252093.03%2520%2528%252B0.65%2529%250Aon%2520the%2520LaPa%2520dataset.%2520Code%253A%2520https%253A//github.com/Kartik-3004/SegFace%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegFace%3A%20Face%20Segmentation%20of%20Long-Tail%20Classes&entry.906535625=Kartik%20Narayan%20and%20Vibashan%20VS%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20Face%20parsing%20refers%20to%20the%20semantic%20segmentation%20of%20human%20faces%20into%20key%0Afacial%20regions%20such%20as%20eyes%2C%20nose%2C%20hair%2C%20etc.%20It%20serves%20as%20a%20prerequisite%20for%0Avarious%20advanced%20applications%2C%20including%20face%20editing%2C%20face%20swapping%2C%20and%0Afacial%20makeup%2C%20which%20often%20require%20segmentation%20masks%20for%20classes%20like%0Aeyeglasses%2C%20hats%2C%20earrings%2C%20and%20necklaces.%20These%20infrequently%20occurring%20classes%0Aare%20called%20long-tail%20classes%2C%20which%20are%20overshadowed%20by%20more%20frequently%0Aoccurring%20classes%20known%20as%20head%20classes.%20Existing%20methods%2C%20primarily%20CNN-based%2C%0Atend%20to%20be%20dominated%20by%20head%20classes%20during%20training%2C%20resulting%20in%20suboptimal%0Arepresentation%20for%20long-tail%20classes.%20Previous%20works%20have%20largely%20overlooked%0Athe%20problem%20of%20poor%20segmentation%20performance%20of%20long-tail%20classes.%20To%20address%0Athis%20issue%2C%20we%20propose%20SegFace%2C%20a%20simple%20and%20efficient%20approach%20that%20uses%20a%0Alightweight%20transformer-based%20model%20which%20utilizes%20learnable%20class-specific%0Atokens.%20The%20transformer%20decoder%20leverages%20class-specific%20tokens%2C%20allowing%20each%0Atoken%20to%20focus%20on%20its%20corresponding%20class%2C%20thereby%20enabling%20independent%0Amodeling%20of%20each%20class.%20The%20proposed%20approach%20improves%20the%20performance%20of%0Along-tail%20classes%2C%20thereby%20boosting%20overall%20performance.%20To%20the%20best%20of%20our%0Aknowledge%2C%20SegFace%20is%20the%20first%20work%20to%20employ%20transformer%20models%20for%20face%0Aparsing.%20Moreover%2C%20our%20approach%20can%20be%20adapted%20for%20low-compute%20edge%20devices%2C%0Aachieving%2095.96%20FPS.%20We%20conduct%20extensive%20experiments%20demonstrating%20that%0ASegFace%20significantly%20outperforms%20previous%20state-of-the-art%20models%2C%20achieving%20a%0Amean%20F1%20score%20of%2088.96%20%28%2B2.82%29%20on%20the%20CelebAMask-HQ%20dataset%20and%2093.03%20%28%2B0.65%29%0Aon%20the%20LaPa%20dataset.%20Code%3A%20https%3A//github.com/Kartik-3004/SegFace%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08647v1&entry.124074799=Read"},
{"title": "SenCLIP: Enhancing zero-shot land-use mapping for Sentinel-2 with\n  ground-level prompting", "author": "Pallavi Jain and Dino Ienco and Roberto Interdonato and Tristan Berchoux and Diego Marcos", "abstract": "  Pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive zero-shot classification capabilities with free-form prompts and\neven show some generalization in specialized domains. However, their\nperformance on satellite imagery is limited due to the underrepresentation of\nsuch data in their training sets, which predominantly consist of ground-level\nimages. Existing prompting techniques for satellite imagery are often\nrestricted to generic phrases like a satellite image of ..., limiting their\neffectiveness for zero-shot land-use and land-cover (LULC) mapping. To address\nthese challenges, we introduce SenCLIP, which transfers CLIPs representation to\nSentinel-2 imagery by leveraging a large dataset of Sentinel-2 images paired\nwith geotagged ground-level photos from across Europe. We evaluate SenCLIP\nalongside other SOTA remote sensing VLMs on zero-shot LULC mapping tasks using\nthe EuroSAT and BigEarthNet datasets with both aerial and ground-level\nprompting styles. Our approach, which aligns ground-level representations with\nsatellite imagery, demonstrates significant improvements in classification\naccuracy across both prompt styles, opening new possibilities for applying\nfree-form textual descriptions in zero-shot LULC mapping.\n", "link": "http://arxiv.org/abs/2412.08536v1", "date": "2024-12-11", "relevancy": 2.6871, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SenCLIP%3A%20Enhancing%20zero-shot%20land-use%20mapping%20for%20Sentinel-2%20with%0A%20%20ground-level%20prompting&body=Title%3A%20SenCLIP%3A%20Enhancing%20zero-shot%20land-use%20mapping%20for%20Sentinel-2%20with%0A%20%20ground-level%20prompting%0AAuthor%3A%20Pallavi%20Jain%20and%20Dino%20Ienco%20and%20Roberto%20Interdonato%20and%20Tristan%20Berchoux%20and%20Diego%20Marcos%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20demonstrate%0Aimpressive%20zero-shot%20classification%20capabilities%20with%20free-form%20prompts%20and%0Aeven%20show%20some%20generalization%20in%20specialized%20domains.%20However%2C%20their%0Aperformance%20on%20satellite%20imagery%20is%20limited%20due%20to%20the%20underrepresentation%20of%0Asuch%20data%20in%20their%20training%20sets%2C%20which%20predominantly%20consist%20of%20ground-level%0Aimages.%20Existing%20prompting%20techniques%20for%20satellite%20imagery%20are%20often%0Arestricted%20to%20generic%20phrases%20like%20a%20satellite%20image%20of%20...%2C%20limiting%20their%0Aeffectiveness%20for%20zero-shot%20land-use%20and%20land-cover%20%28LULC%29%20mapping.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20SenCLIP%2C%20which%20transfers%20CLIPs%20representation%20to%0ASentinel-2%20imagery%20by%20leveraging%20a%20large%20dataset%20of%20Sentinel-2%20images%20paired%0Awith%20geotagged%20ground-level%20photos%20from%20across%20Europe.%20We%20evaluate%20SenCLIP%0Aalongside%20other%20SOTA%20remote%20sensing%20VLMs%20on%20zero-shot%20LULC%20mapping%20tasks%20using%0Athe%20EuroSAT%20and%20BigEarthNet%20datasets%20with%20both%20aerial%20and%20ground-level%0Aprompting%20styles.%20Our%20approach%2C%20which%20aligns%20ground-level%20representations%20with%0Asatellite%20imagery%2C%20demonstrates%20significant%20improvements%20in%20classification%0Aaccuracy%20across%20both%20prompt%20styles%2C%20opening%20new%20possibilities%20for%20applying%0Afree-form%20textual%20descriptions%20in%20zero-shot%20LULC%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSenCLIP%253A%2520Enhancing%2520zero-shot%2520land-use%2520mapping%2520for%2520Sentinel-2%2520with%250A%2520%2520ground-level%2520prompting%26entry.906535625%3DPallavi%2520Jain%2520and%2520Dino%2520Ienco%2520and%2520Roberto%2520Interdonato%2520and%2520Tristan%2520Berchoux%2520and%2520Diego%2520Marcos%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520demonstrate%250Aimpressive%2520zero-shot%2520classification%2520capabilities%2520with%2520free-form%2520prompts%2520and%250Aeven%2520show%2520some%2520generalization%2520in%2520specialized%2520domains.%2520However%252C%2520their%250Aperformance%2520on%2520satellite%2520imagery%2520is%2520limited%2520due%2520to%2520the%2520underrepresentation%2520of%250Asuch%2520data%2520in%2520their%2520training%2520sets%252C%2520which%2520predominantly%2520consist%2520of%2520ground-level%250Aimages.%2520Existing%2520prompting%2520techniques%2520for%2520satellite%2520imagery%2520are%2520often%250Arestricted%2520to%2520generic%2520phrases%2520like%2520a%2520satellite%2520image%2520of%2520...%252C%2520limiting%2520their%250Aeffectiveness%2520for%2520zero-shot%2520land-use%2520and%2520land-cover%2520%2528LULC%2529%2520mapping.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520SenCLIP%252C%2520which%2520transfers%2520CLIPs%2520representation%2520to%250ASentinel-2%2520imagery%2520by%2520leveraging%2520a%2520large%2520dataset%2520of%2520Sentinel-2%2520images%2520paired%250Awith%2520geotagged%2520ground-level%2520photos%2520from%2520across%2520Europe.%2520We%2520evaluate%2520SenCLIP%250Aalongside%2520other%2520SOTA%2520remote%2520sensing%2520VLMs%2520on%2520zero-shot%2520LULC%2520mapping%2520tasks%2520using%250Athe%2520EuroSAT%2520and%2520BigEarthNet%2520datasets%2520with%2520both%2520aerial%2520and%2520ground-level%250Aprompting%2520styles.%2520Our%2520approach%252C%2520which%2520aligns%2520ground-level%2520representations%2520with%250Asatellite%2520imagery%252C%2520demonstrates%2520significant%2520improvements%2520in%2520classification%250Aaccuracy%2520across%2520both%2520prompt%2520styles%252C%2520opening%2520new%2520possibilities%2520for%2520applying%250Afree-form%2520textual%2520descriptions%2520in%2520zero-shot%2520LULC%2520mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SenCLIP%3A%20Enhancing%20zero-shot%20land-use%20mapping%20for%20Sentinel-2%20with%0A%20%20ground-level%20prompting&entry.906535625=Pallavi%20Jain%20and%20Dino%20Ienco%20and%20Roberto%20Interdonato%20and%20Tristan%20Berchoux%20and%20Diego%20Marcos&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20demonstrate%0Aimpressive%20zero-shot%20classification%20capabilities%20with%20free-form%20prompts%20and%0Aeven%20show%20some%20generalization%20in%20specialized%20domains.%20However%2C%20their%0Aperformance%20on%20satellite%20imagery%20is%20limited%20due%20to%20the%20underrepresentation%20of%0Asuch%20data%20in%20their%20training%20sets%2C%20which%20predominantly%20consist%20of%20ground-level%0Aimages.%20Existing%20prompting%20techniques%20for%20satellite%20imagery%20are%20often%0Arestricted%20to%20generic%20phrases%20like%20a%20satellite%20image%20of%20...%2C%20limiting%20their%0Aeffectiveness%20for%20zero-shot%20land-use%20and%20land-cover%20%28LULC%29%20mapping.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20SenCLIP%2C%20which%20transfers%20CLIPs%20representation%20to%0ASentinel-2%20imagery%20by%20leveraging%20a%20large%20dataset%20of%20Sentinel-2%20images%20paired%0Awith%20geotagged%20ground-level%20photos%20from%20across%20Europe.%20We%20evaluate%20SenCLIP%0Aalongside%20other%20SOTA%20remote%20sensing%20VLMs%20on%20zero-shot%20LULC%20mapping%20tasks%20using%0Athe%20EuroSAT%20and%20BigEarthNet%20datasets%20with%20both%20aerial%20and%20ground-level%0Aprompting%20styles.%20Our%20approach%2C%20which%20aligns%20ground-level%20representations%20with%0Asatellite%20imagery%2C%20demonstrates%20significant%20improvements%20in%20classification%0Aaccuracy%20across%20both%20prompt%20styles%2C%20opening%20new%20possibilities%20for%20applying%0Afree-form%20textual%20descriptions%20in%20zero-shot%20LULC%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08536v1&entry.124074799=Read"},
{"title": "Monocular Lane Detection Based on Deep Learning: A Survey", "author": "Xin He and Haiyun Guo and Kuan Zhu and Bingke Zhu and Xu Zhao and Jianwu Fang and Jinqiao Wang", "abstract": "  Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.\n", "link": "http://arxiv.org/abs/2411.16316v6", "date": "2024-12-11", "relevancy": 2.6726, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5408}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey&body=Title%3A%20Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey%0AAuthor%3A%20Xin%20He%20and%20Haiyun%20Guo%20and%20Kuan%20Zhu%20and%20Bingke%20Zhu%20and%20Xu%20Zhao%20and%20Jianwu%20Fang%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Lane%20detection%20plays%20an%20important%20role%20in%20autonomous%20driving%20perception%0Asystems.%20As%20deep%20learning%20algorithms%20gain%20popularity%2C%20monocular%20lane%20detection%0Amethods%20based%20on%20them%20have%20demonstrated%20superior%20performance%20and%20emerged%20as%20a%0Akey%20research%20direction%20in%20autonomous%20driving%20perception.%20The%20core%20designs%20of%0Athese%20algorithmic%20frameworks%20can%20be%20summarized%20as%20follows%3A%20%281%29%20Task%20paradigm%2C%0Afocusing%20on%20lane%20instance-level%20discrimination%3B%20%282%29%20Lane%20modeling%2C%20representing%0Alanes%20as%20a%20set%20of%20learnable%20parameters%20in%20the%20neural%20network%3B%20%283%29%20Global%0Acontext%20supplementation%2C%20enhancing%20inference%20on%20the%20obscure%20lanes%3B%20%284%29%0APerspective%20effect%20elimination%2C%20providing%20accurate%203D%20lanes%20for%20downstream%0Aapplications.%20From%20these%20perspectives%2C%20this%20paper%20presents%20a%20comprehensive%0Aoverview%20of%20existing%20methods%2C%20encompassing%20both%20the%20increasingly%20mature%202D%20lane%0Adetection%20approaches%20and%20the%20developing%203D%20lane%20detection%20works.%20Besides%2C%20this%0Apaper%20compares%20the%20performance%20of%20mainstream%20methods%20on%20different%20benchmarks%0Aand%20investigates%20their%20inference%20speed%20under%20a%20unified%20setting%20for%20fair%0Acomparison.%20Moreover%2C%20we%20present%20some%20extended%20works%20on%20lane%20detection%2C%0Aincluding%20multi-task%20perception%2C%20video%20lane%20detection%2C%20online%20high-definition%0Amap%20construction%2C%20and%20lane%20topology%20reasoning%2C%20to%20offer%20readers%20a%20comprehensive%0Aroadmap%20for%20the%20evolution%20of%20lane%20detection.%20Finally%2C%20we%20point%20out%20some%0Apotential%20future%20research%20directions%20in%20this%20field.%20We%20exhaustively%20collect%20the%0Apapers%20and%20codes%20of%20existing%20works%20at%0Ahttps%3A//github.com/Core9724/Awesome-Lane-Detection%20and%20will%20keep%20tracing%20the%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16316v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonocular%2520Lane%2520Detection%2520Based%2520on%2520Deep%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DXin%2520He%2520and%2520Haiyun%2520Guo%2520and%2520Kuan%2520Zhu%2520and%2520Bingke%2520Zhu%2520and%2520Xu%2520Zhao%2520and%2520Jianwu%2520Fang%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Lane%2520detection%2520plays%2520an%2520important%2520role%2520in%2520autonomous%2520driving%2520perception%250Asystems.%2520As%2520deep%2520learning%2520algorithms%2520gain%2520popularity%252C%2520monocular%2520lane%2520detection%250Amethods%2520based%2520on%2520them%2520have%2520demonstrated%2520superior%2520performance%2520and%2520emerged%2520as%2520a%250Akey%2520research%2520direction%2520in%2520autonomous%2520driving%2520perception.%2520The%2520core%2520designs%2520of%250Athese%2520algorithmic%2520frameworks%2520can%2520be%2520summarized%2520as%2520follows%253A%2520%25281%2529%2520Task%2520paradigm%252C%250Afocusing%2520on%2520lane%2520instance-level%2520discrimination%253B%2520%25282%2529%2520Lane%2520modeling%252C%2520representing%250Alanes%2520as%2520a%2520set%2520of%2520learnable%2520parameters%2520in%2520the%2520neural%2520network%253B%2520%25283%2529%2520Global%250Acontext%2520supplementation%252C%2520enhancing%2520inference%2520on%2520the%2520obscure%2520lanes%253B%2520%25284%2529%250APerspective%2520effect%2520elimination%252C%2520providing%2520accurate%25203D%2520lanes%2520for%2520downstream%250Aapplications.%2520From%2520these%2520perspectives%252C%2520this%2520paper%2520presents%2520a%2520comprehensive%250Aoverview%2520of%2520existing%2520methods%252C%2520encompassing%2520both%2520the%2520increasingly%2520mature%25202D%2520lane%250Adetection%2520approaches%2520and%2520the%2520developing%25203D%2520lane%2520detection%2520works.%2520Besides%252C%2520this%250Apaper%2520compares%2520the%2520performance%2520of%2520mainstream%2520methods%2520on%2520different%2520benchmarks%250Aand%2520investigates%2520their%2520inference%2520speed%2520under%2520a%2520unified%2520setting%2520for%2520fair%250Acomparison.%2520Moreover%252C%2520we%2520present%2520some%2520extended%2520works%2520on%2520lane%2520detection%252C%250Aincluding%2520multi-task%2520perception%252C%2520video%2520lane%2520detection%252C%2520online%2520high-definition%250Amap%2520construction%252C%2520and%2520lane%2520topology%2520reasoning%252C%2520to%2520offer%2520readers%2520a%2520comprehensive%250Aroadmap%2520for%2520the%2520evolution%2520of%2520lane%2520detection.%2520Finally%252C%2520we%2520point%2520out%2520some%250Apotential%2520future%2520research%2520directions%2520in%2520this%2520field.%2520We%2520exhaustively%2520collect%2520the%250Apapers%2520and%2520codes%2520of%2520existing%2520works%2520at%250Ahttps%253A//github.com/Core9724/Awesome-Lane-Detection%2520and%2520will%2520keep%2520tracing%2520the%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16316v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monocular%20Lane%20Detection%20Based%20on%20Deep%20Learning%3A%20A%20Survey&entry.906535625=Xin%20He%20and%20Haiyun%20Guo%20and%20Kuan%20Zhu%20and%20Bingke%20Zhu%20and%20Xu%20Zhao%20and%20Jianwu%20Fang%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Lane%20detection%20plays%20an%20important%20role%20in%20autonomous%20driving%20perception%0Asystems.%20As%20deep%20learning%20algorithms%20gain%20popularity%2C%20monocular%20lane%20detection%0Amethods%20based%20on%20them%20have%20demonstrated%20superior%20performance%20and%20emerged%20as%20a%0Akey%20research%20direction%20in%20autonomous%20driving%20perception.%20The%20core%20designs%20of%0Athese%20algorithmic%20frameworks%20can%20be%20summarized%20as%20follows%3A%20%281%29%20Task%20paradigm%2C%0Afocusing%20on%20lane%20instance-level%20discrimination%3B%20%282%29%20Lane%20modeling%2C%20representing%0Alanes%20as%20a%20set%20of%20learnable%20parameters%20in%20the%20neural%20network%3B%20%283%29%20Global%0Acontext%20supplementation%2C%20enhancing%20inference%20on%20the%20obscure%20lanes%3B%20%284%29%0APerspective%20effect%20elimination%2C%20providing%20accurate%203D%20lanes%20for%20downstream%0Aapplications.%20From%20these%20perspectives%2C%20this%20paper%20presents%20a%20comprehensive%0Aoverview%20of%20existing%20methods%2C%20encompassing%20both%20the%20increasingly%20mature%202D%20lane%0Adetection%20approaches%20and%20the%20developing%203D%20lane%20detection%20works.%20Besides%2C%20this%0Apaper%20compares%20the%20performance%20of%20mainstream%20methods%20on%20different%20benchmarks%0Aand%20investigates%20their%20inference%20speed%20under%20a%20unified%20setting%20for%20fair%0Acomparison.%20Moreover%2C%20we%20present%20some%20extended%20works%20on%20lane%20detection%2C%0Aincluding%20multi-task%20perception%2C%20video%20lane%20detection%2C%20online%20high-definition%0Amap%20construction%2C%20and%20lane%20topology%20reasoning%2C%20to%20offer%20readers%20a%20comprehensive%0Aroadmap%20for%20the%20evolution%20of%20lane%20detection.%20Finally%2C%20we%20point%20out%20some%0Apotential%20future%20research%20directions%20in%20this%20field.%20We%20exhaustively%20collect%20the%0Apapers%20and%20codes%20of%20existing%20works%20at%0Ahttps%3A//github.com/Core9724/Awesome-Lane-Detection%20and%20will%20keep%20tracing%20the%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16316v6&entry.124074799=Read"},
{"title": "Adversarial Purification by Consistency-aware Latent Space Optimization\n  on Data Manifolds", "author": "Shuhai Zhang and Jiahao Yang and Hui Luo and Jie Chen and Li Wang and Feng Liu and Bo Han and Mingkui Tan", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial samples crafted by\nadding imperceptible perturbations to clean data, potentially leading to\nincorrect and dangerous predictions. Adversarial purification has been an\neffective means to improve DNNs robustness by removing these perturbations\nbefore feeding the data into the model. However, it faces significant\nchallenges in preserving key structural and semantic information of data, as\nthe imperceptible nature of adversarial perturbations makes it hard to avoid\nover-correcting, which can destroy important information and degrade model\nperformance. In this paper, we break away from traditional adversarial\npurification methods by focusing on the clean data manifold. To this end, we\nreveal that samples generated by a well-trained generative model are close to\nclean ones but far from adversarial ones. Leveraging this insight, we propose\nConsistency Model-based Adversarial Purification (CMAP), which optimizes\nvectors within the latent space of a pre-trained consistency model to generate\nsamples for restoring clean data. Specifically, 1) we propose a\n\\textit{Perceptual consistency restoration} mechanism by minimizing the\ndiscrepancy between generated samples and input samples in both pixel and\nperceptual spaces. 2) To maintain the optimized latent vectors within the valid\ndata manifold, we introduce a \\textit{Latent distribution consistency\nconstraint} strategy to align generated samples with the clean data\ndistribution. 3) We also apply a \\textit{Latent vector consistency prediction}\nscheme via an ensemble approach to enhance prediction reliability. CMAP\nfundamentally addresses adversarial perturbations at their source, providing a\nrobust purification. Extensive experiments on CIFAR-10 and ImageNet-100 show\nthat our CMAP significantly enhances robustness against strong adversarial\nattacks while preserving high natural accuracy.\n", "link": "http://arxiv.org/abs/2412.08394v1", "date": "2024-12-11", "relevancy": 2.6506, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5378}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5372}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Purification%20by%20Consistency-aware%20Latent%20Space%20Optimization%0A%20%20on%20Data%20Manifolds&body=Title%3A%20Adversarial%20Purification%20by%20Consistency-aware%20Latent%20Space%20Optimization%0A%20%20on%20Data%20Manifolds%0AAuthor%3A%20Shuhai%20Zhang%20and%20Jiahao%20Yang%20and%20Hui%20Luo%20and%20Jie%20Chen%20and%20Li%20Wang%20and%20Feng%20Liu%20and%20Bo%20Han%20and%20Mingkui%20Tan%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%20to%20adversarial%20samples%20crafted%20by%0Aadding%20imperceptible%20perturbations%20to%20clean%20data%2C%20potentially%20leading%20to%0Aincorrect%20and%20dangerous%20predictions.%20Adversarial%20purification%20has%20been%20an%0Aeffective%20means%20to%20improve%20DNNs%20robustness%20by%20removing%20these%20perturbations%0Abefore%20feeding%20the%20data%20into%20the%20model.%20However%2C%20it%20faces%20significant%0Achallenges%20in%20preserving%20key%20structural%20and%20semantic%20information%20of%20data%2C%20as%0Athe%20imperceptible%20nature%20of%20adversarial%20perturbations%20makes%20it%20hard%20to%20avoid%0Aover-correcting%2C%20which%20can%20destroy%20important%20information%20and%20degrade%20model%0Aperformance.%20In%20this%20paper%2C%20we%20break%20away%20from%20traditional%20adversarial%0Apurification%20methods%20by%20focusing%20on%20the%20clean%20data%20manifold.%20To%20this%20end%2C%20we%0Areveal%20that%20samples%20generated%20by%20a%20well-trained%20generative%20model%20are%20close%20to%0Aclean%20ones%20but%20far%20from%20adversarial%20ones.%20Leveraging%20this%20insight%2C%20we%20propose%0AConsistency%20Model-based%20Adversarial%20Purification%20%28CMAP%29%2C%20which%20optimizes%0Avectors%20within%20the%20latent%20space%20of%20a%20pre-trained%20consistency%20model%20to%20generate%0Asamples%20for%20restoring%20clean%20data.%20Specifically%2C%201%29%20we%20propose%20a%0A%5Ctextit%7BPerceptual%20consistency%20restoration%7D%20mechanism%20by%20minimizing%20the%0Adiscrepancy%20between%20generated%20samples%20and%20input%20samples%20in%20both%20pixel%20and%0Aperceptual%20spaces.%202%29%20To%20maintain%20the%20optimized%20latent%20vectors%20within%20the%20valid%0Adata%20manifold%2C%20we%20introduce%20a%20%5Ctextit%7BLatent%20distribution%20consistency%0Aconstraint%7D%20strategy%20to%20align%20generated%20samples%20with%20the%20clean%20data%0Adistribution.%203%29%20We%20also%20apply%20a%20%5Ctextit%7BLatent%20vector%20consistency%20prediction%7D%0Ascheme%20via%20an%20ensemble%20approach%20to%20enhance%20prediction%20reliability.%20CMAP%0Afundamentally%20addresses%20adversarial%20perturbations%20at%20their%20source%2C%20providing%20a%0Arobust%20purification.%20Extensive%20experiments%20on%20CIFAR-10%20and%20ImageNet-100%20show%0Athat%20our%20CMAP%20significantly%20enhances%20robustness%20against%20strong%20adversarial%0Aattacks%20while%20preserving%20high%20natural%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Purification%2520by%2520Consistency-aware%2520Latent%2520Space%2520Optimization%250A%2520%2520on%2520Data%2520Manifolds%26entry.906535625%3DShuhai%2520Zhang%2520and%2520Jiahao%2520Yang%2520and%2520Hui%2520Luo%2520and%2520Jie%2520Chen%2520and%2520Li%2520Wang%2520and%2520Feng%2520Liu%2520and%2520Bo%2520Han%2520and%2520Mingkui%2520Tan%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520vulnerable%2520to%2520adversarial%2520samples%2520crafted%2520by%250Aadding%2520imperceptible%2520perturbations%2520to%2520clean%2520data%252C%2520potentially%2520leading%2520to%250Aincorrect%2520and%2520dangerous%2520predictions.%2520Adversarial%2520purification%2520has%2520been%2520an%250Aeffective%2520means%2520to%2520improve%2520DNNs%2520robustness%2520by%2520removing%2520these%2520perturbations%250Abefore%2520feeding%2520the%2520data%2520into%2520the%2520model.%2520However%252C%2520it%2520faces%2520significant%250Achallenges%2520in%2520preserving%2520key%2520structural%2520and%2520semantic%2520information%2520of%2520data%252C%2520as%250Athe%2520imperceptible%2520nature%2520of%2520adversarial%2520perturbations%2520makes%2520it%2520hard%2520to%2520avoid%250Aover-correcting%252C%2520which%2520can%2520destroy%2520important%2520information%2520and%2520degrade%2520model%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520break%2520away%2520from%2520traditional%2520adversarial%250Apurification%2520methods%2520by%2520focusing%2520on%2520the%2520clean%2520data%2520manifold.%2520To%2520this%2520end%252C%2520we%250Areveal%2520that%2520samples%2520generated%2520by%2520a%2520well-trained%2520generative%2520model%2520are%2520close%2520to%250Aclean%2520ones%2520but%2520far%2520from%2520adversarial%2520ones.%2520Leveraging%2520this%2520insight%252C%2520we%2520propose%250AConsistency%2520Model-based%2520Adversarial%2520Purification%2520%2528CMAP%2529%252C%2520which%2520optimizes%250Avectors%2520within%2520the%2520latent%2520space%2520of%2520a%2520pre-trained%2520consistency%2520model%2520to%2520generate%250Asamples%2520for%2520restoring%2520clean%2520data.%2520Specifically%252C%25201%2529%2520we%2520propose%2520a%250A%255Ctextit%257BPerceptual%2520consistency%2520restoration%257D%2520mechanism%2520by%2520minimizing%2520the%250Adiscrepancy%2520between%2520generated%2520samples%2520and%2520input%2520samples%2520in%2520both%2520pixel%2520and%250Aperceptual%2520spaces.%25202%2529%2520To%2520maintain%2520the%2520optimized%2520latent%2520vectors%2520within%2520the%2520valid%250Adata%2520manifold%252C%2520we%2520introduce%2520a%2520%255Ctextit%257BLatent%2520distribution%2520consistency%250Aconstraint%257D%2520strategy%2520to%2520align%2520generated%2520samples%2520with%2520the%2520clean%2520data%250Adistribution.%25203%2529%2520We%2520also%2520apply%2520a%2520%255Ctextit%257BLatent%2520vector%2520consistency%2520prediction%257D%250Ascheme%2520via%2520an%2520ensemble%2520approach%2520to%2520enhance%2520prediction%2520reliability.%2520CMAP%250Afundamentally%2520addresses%2520adversarial%2520perturbations%2520at%2520their%2520source%252C%2520providing%2520a%250Arobust%2520purification.%2520Extensive%2520experiments%2520on%2520CIFAR-10%2520and%2520ImageNet-100%2520show%250Athat%2520our%2520CMAP%2520significantly%2520enhances%2520robustness%2520against%2520strong%2520adversarial%250Aattacks%2520while%2520preserving%2520high%2520natural%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Purification%20by%20Consistency-aware%20Latent%20Space%20Optimization%0A%20%20on%20Data%20Manifolds&entry.906535625=Shuhai%20Zhang%20and%20Jiahao%20Yang%20and%20Hui%20Luo%20and%20Jie%20Chen%20and%20Li%20Wang%20and%20Feng%20Liu%20and%20Bo%20Han%20and%20Mingkui%20Tan&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%20to%20adversarial%20samples%20crafted%20by%0Aadding%20imperceptible%20perturbations%20to%20clean%20data%2C%20potentially%20leading%20to%0Aincorrect%20and%20dangerous%20predictions.%20Adversarial%20purification%20has%20been%20an%0Aeffective%20means%20to%20improve%20DNNs%20robustness%20by%20removing%20these%20perturbations%0Abefore%20feeding%20the%20data%20into%20the%20model.%20However%2C%20it%20faces%20significant%0Achallenges%20in%20preserving%20key%20structural%20and%20semantic%20information%20of%20data%2C%20as%0Athe%20imperceptible%20nature%20of%20adversarial%20perturbations%20makes%20it%20hard%20to%20avoid%0Aover-correcting%2C%20which%20can%20destroy%20important%20information%20and%20degrade%20model%0Aperformance.%20In%20this%20paper%2C%20we%20break%20away%20from%20traditional%20adversarial%0Apurification%20methods%20by%20focusing%20on%20the%20clean%20data%20manifold.%20To%20this%20end%2C%20we%0Areveal%20that%20samples%20generated%20by%20a%20well-trained%20generative%20model%20are%20close%20to%0Aclean%20ones%20but%20far%20from%20adversarial%20ones.%20Leveraging%20this%20insight%2C%20we%20propose%0AConsistency%20Model-based%20Adversarial%20Purification%20%28CMAP%29%2C%20which%20optimizes%0Avectors%20within%20the%20latent%20space%20of%20a%20pre-trained%20consistency%20model%20to%20generate%0Asamples%20for%20restoring%20clean%20data.%20Specifically%2C%201%29%20we%20propose%20a%0A%5Ctextit%7BPerceptual%20consistency%20restoration%7D%20mechanism%20by%20minimizing%20the%0Adiscrepancy%20between%20generated%20samples%20and%20input%20samples%20in%20both%20pixel%20and%0Aperceptual%20spaces.%202%29%20To%20maintain%20the%20optimized%20latent%20vectors%20within%20the%20valid%0Adata%20manifold%2C%20we%20introduce%20a%20%5Ctextit%7BLatent%20distribution%20consistency%0Aconstraint%7D%20strategy%20to%20align%20generated%20samples%20with%20the%20clean%20data%0Adistribution.%203%29%20We%20also%20apply%20a%20%5Ctextit%7BLatent%20vector%20consistency%20prediction%7D%0Ascheme%20via%20an%20ensemble%20approach%20to%20enhance%20prediction%20reliability.%20CMAP%0Afundamentally%20addresses%20adversarial%20perturbations%20at%20their%20source%2C%20providing%20a%0Arobust%20purification.%20Extensive%20experiments%20on%20CIFAR-10%20and%20ImageNet-100%20show%0Athat%20our%20CMAP%20significantly%20enhances%20robustness%20against%20strong%20adversarial%0Aattacks%20while%20preserving%20high%20natural%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08394v1&entry.124074799=Read"},
{"title": "Adaptive Principal Components Allocation with the\n  $\\ell_{2,g}$-regularized Gaussian Graphical Model for Efficient Fine-Tuning\n  Large Models", "author": "Jingjing Zheng and Yankai Cao", "abstract": "  In this work, we propose a novel Parameter-Efficient Fine-Tuning (PEFT)\napproach based on Gaussian Graphical Models (GGMs), marking the first\napplication of GGMs to PEFT tasks, to the best of our knowledge. The proposed\nmethod utilizes the $\\ell_{2,g}$-norm to effectively select critical parameters\nand capture global dependencies. The resulting non-convex optimization problem\nis efficiently solved using a Block Coordinate Descent (BCD) algorithm.\nExperimental results on the GLUE benchmark [24] for fine-tuning RoBERTa-Base\n[18] demonstrate the effectiveness of the proposed approach, achieving\ncompetitive performance with significantly fewer trainable parameters. The code\nfor this work is available at: https://github.com/jzheng20/Course projects.git.\n", "link": "http://arxiv.org/abs/2412.08592v1", "date": "2024-12-11", "relevancy": 2.636, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5498}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5167}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Principal%20Components%20Allocation%20with%20the%0A%20%20%24%5Cell_%7B2%2Cg%7D%24-regularized%20Gaussian%20Graphical%20Model%20for%20Efficient%20Fine-Tuning%0A%20%20Large%20Models&body=Title%3A%20Adaptive%20Principal%20Components%20Allocation%20with%20the%0A%20%20%24%5Cell_%7B2%2Cg%7D%24-regularized%20Gaussian%20Graphical%20Model%20for%20Efficient%20Fine-Tuning%0A%20%20Large%20Models%0AAuthor%3A%20Jingjing%20Zheng%20and%20Yankai%20Cao%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%0Aapproach%20based%20on%20Gaussian%20Graphical%20Models%20%28GGMs%29%2C%20marking%20the%20first%0Aapplication%20of%20GGMs%20to%20PEFT%20tasks%2C%20to%20the%20best%20of%20our%20knowledge.%20The%20proposed%0Amethod%20utilizes%20the%20%24%5Cell_%7B2%2Cg%7D%24-norm%20to%20effectively%20select%20critical%20parameters%0Aand%20capture%20global%20dependencies.%20The%20resulting%20non-convex%20optimization%20problem%0Ais%20efficiently%20solved%20using%20a%20Block%20Coordinate%20Descent%20%28BCD%29%20algorithm.%0AExperimental%20results%20on%20the%20GLUE%20benchmark%20%5B24%5D%20for%20fine-tuning%20RoBERTa-Base%0A%5B18%5D%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%2C%20achieving%0Acompetitive%20performance%20with%20significantly%20fewer%20trainable%20parameters.%20The%20code%0Afor%20this%20work%20is%20available%20at%3A%20https%3A//github.com/jzheng20/Course%20projects.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Principal%2520Components%2520Allocation%2520with%2520the%250A%2520%2520%2524%255Cell_%257B2%252Cg%257D%2524-regularized%2520Gaussian%2520Graphical%2520Model%2520for%2520Efficient%2520Fine-Tuning%250A%2520%2520Large%2520Models%26entry.906535625%3DJingjing%2520Zheng%2520and%2520Yankai%2520Cao%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%250Aapproach%2520based%2520on%2520Gaussian%2520Graphical%2520Models%2520%2528GGMs%2529%252C%2520marking%2520the%2520first%250Aapplication%2520of%2520GGMs%2520to%2520PEFT%2520tasks%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge.%2520The%2520proposed%250Amethod%2520utilizes%2520the%2520%2524%255Cell_%257B2%252Cg%257D%2524-norm%2520to%2520effectively%2520select%2520critical%2520parameters%250Aand%2520capture%2520global%2520dependencies.%2520The%2520resulting%2520non-convex%2520optimization%2520problem%250Ais%2520efficiently%2520solved%2520using%2520a%2520Block%2520Coordinate%2520Descent%2520%2528BCD%2529%2520algorithm.%250AExperimental%2520results%2520on%2520the%2520GLUE%2520benchmark%2520%255B24%255D%2520for%2520fine-tuning%2520RoBERTa-Base%250A%255B18%255D%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%252C%2520achieving%250Acompetitive%2520performance%2520with%2520significantly%2520fewer%2520trainable%2520parameters.%2520The%2520code%250Afor%2520this%2520work%2520is%2520available%2520at%253A%2520https%253A//github.com/jzheng20/Course%2520projects.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Principal%20Components%20Allocation%20with%20the%0A%20%20%24%5Cell_%7B2%2Cg%7D%24-regularized%20Gaussian%20Graphical%20Model%20for%20Efficient%20Fine-Tuning%0A%20%20Large%20Models&entry.906535625=Jingjing%20Zheng%20and%20Yankai%20Cao&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%0Aapproach%20based%20on%20Gaussian%20Graphical%20Models%20%28GGMs%29%2C%20marking%20the%20first%0Aapplication%20of%20GGMs%20to%20PEFT%20tasks%2C%20to%20the%20best%20of%20our%20knowledge.%20The%20proposed%0Amethod%20utilizes%20the%20%24%5Cell_%7B2%2Cg%7D%24-norm%20to%20effectively%20select%20critical%20parameters%0Aand%20capture%20global%20dependencies.%20The%20resulting%20non-convex%20optimization%20problem%0Ais%20efficiently%20solved%20using%20a%20Block%20Coordinate%20Descent%20%28BCD%29%20algorithm.%0AExperimental%20results%20on%20the%20GLUE%20benchmark%20%5B24%5D%20for%20fine-tuning%20RoBERTa-Base%0A%5B18%5D%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%2C%20achieving%0Acompetitive%20performance%20with%20significantly%20fewer%20trainable%20parameters.%20The%20code%0Afor%20this%20work%20is%20available%20at%3A%20https%3A//github.com/jzheng20/Course%20projects.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08592v1&entry.124074799=Read"},
{"title": "SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp\n  Segmentation", "author": "Tapas Kumar Dutta and Snehashis Majhi and Deepak Ranjan Nayak and Debesh Jha", "abstract": "  Polyp segmentation in colonoscopy is crucial for detecting colorectal cancer.\nHowever, it is challenging due to variations in the structure, color, and size\nof polyps, as well as the lack of clear boundaries with surrounding tissues.\nTraditional segmentation models based on Convolutional Neural Networks (CNNs)\nstruggle to capture detailed patterns and global context, limiting their\nperformance. Vision Transformer (ViT)-based models address some of these issues\nbut have difficulties in capturing local context and lack strong zero-shot\ngeneralization. To this end, we propose the Mamba-guided Segment Anything Model\n(SAM-Mamba) for efficient polyp segmentation. Our approach introduces a\nMamba-Prior module in the encoder to bridge the gap between the general\npre-trained representation of SAM and polyp-relevant trivial clues. It injects\nsalient cues of polyp images into the SAM image encoder as a domain prior while\ncapturing global dependencies at various scales, leading to more accurate\nsegmentation results. Extensive experiments on five benchmark datasets show\nthat SAM-Mamba outperforms traditional CNN, ViT, and Adapter-based models in\nboth quantitative and qualitative measures. Additionally, SAM-Mamba\ndemonstrates excellent adaptability to unseen datasets, making it highly\nsuitable for real-time clinical use.\n", "link": "http://arxiv.org/abs/2412.08482v1", "date": "2024-12-11", "relevancy": 2.6169, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-Mamba%3A%20Mamba%20Guided%20SAM%20Architecture%20for%20Generalized%20Zero-Shot%20Polyp%0A%20%20Segmentation&body=Title%3A%20SAM-Mamba%3A%20Mamba%20Guided%20SAM%20Architecture%20for%20Generalized%20Zero-Shot%20Polyp%0A%20%20Segmentation%0AAuthor%3A%20Tapas%20Kumar%20Dutta%20and%20Snehashis%20Majhi%20and%20Deepak%20Ranjan%20Nayak%20and%20Debesh%20Jha%0AAbstract%3A%20%20%20Polyp%20segmentation%20in%20colonoscopy%20is%20crucial%20for%20detecting%20colorectal%20cancer.%0AHowever%2C%20it%20is%20challenging%20due%20to%20variations%20in%20the%20structure%2C%20color%2C%20and%20size%0Aof%20polyps%2C%20as%20well%20as%20the%20lack%20of%20clear%20boundaries%20with%20surrounding%20tissues.%0ATraditional%20segmentation%20models%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Astruggle%20to%20capture%20detailed%20patterns%20and%20global%20context%2C%20limiting%20their%0Aperformance.%20Vision%20Transformer%20%28ViT%29-based%20models%20address%20some%20of%20these%20issues%0Abut%20have%20difficulties%20in%20capturing%20local%20context%20and%20lack%20strong%20zero-shot%0Ageneralization.%20To%20this%20end%2C%20we%20propose%20the%20Mamba-guided%20Segment%20Anything%20Model%0A%28SAM-Mamba%29%20for%20efficient%20polyp%20segmentation.%20Our%20approach%20introduces%20a%0AMamba-Prior%20module%20in%20the%20encoder%20to%20bridge%20the%20gap%20between%20the%20general%0Apre-trained%20representation%20of%20SAM%20and%20polyp-relevant%20trivial%20clues.%20It%20injects%0Asalient%20cues%20of%20polyp%20images%20into%20the%20SAM%20image%20encoder%20as%20a%20domain%20prior%20while%0Acapturing%20global%20dependencies%20at%20various%20scales%2C%20leading%20to%20more%20accurate%0Asegmentation%20results.%20Extensive%20experiments%20on%20five%20benchmark%20datasets%20show%0Athat%20SAM-Mamba%20outperforms%20traditional%20CNN%2C%20ViT%2C%20and%20Adapter-based%20models%20in%0Aboth%20quantitative%20and%20qualitative%20measures.%20Additionally%2C%20SAM-Mamba%0Ademonstrates%20excellent%20adaptability%20to%20unseen%20datasets%2C%20making%20it%20highly%0Asuitable%20for%20real-time%20clinical%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-Mamba%253A%2520Mamba%2520Guided%2520SAM%2520Architecture%2520for%2520Generalized%2520Zero-Shot%2520Polyp%250A%2520%2520Segmentation%26entry.906535625%3DTapas%2520Kumar%2520Dutta%2520and%2520Snehashis%2520Majhi%2520and%2520Deepak%2520Ranjan%2520Nayak%2520and%2520Debesh%2520Jha%26entry.1292438233%3D%2520%2520Polyp%2520segmentation%2520in%2520colonoscopy%2520is%2520crucial%2520for%2520detecting%2520colorectal%2520cancer.%250AHowever%252C%2520it%2520is%2520challenging%2520due%2520to%2520variations%2520in%2520the%2520structure%252C%2520color%252C%2520and%2520size%250Aof%2520polyps%252C%2520as%2520well%2520as%2520the%2520lack%2520of%2520clear%2520boundaries%2520with%2520surrounding%2520tissues.%250ATraditional%2520segmentation%2520models%2520based%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%250Astruggle%2520to%2520capture%2520detailed%2520patterns%2520and%2520global%2520context%252C%2520limiting%2520their%250Aperformance.%2520Vision%2520Transformer%2520%2528ViT%2529-based%2520models%2520address%2520some%2520of%2520these%2520issues%250Abut%2520have%2520difficulties%2520in%2520capturing%2520local%2520context%2520and%2520lack%2520strong%2520zero-shot%250Ageneralization.%2520To%2520this%2520end%252C%2520we%2520propose%2520the%2520Mamba-guided%2520Segment%2520Anything%2520Model%250A%2528SAM-Mamba%2529%2520for%2520efficient%2520polyp%2520segmentation.%2520Our%2520approach%2520introduces%2520a%250AMamba-Prior%2520module%2520in%2520the%2520encoder%2520to%2520bridge%2520the%2520gap%2520between%2520the%2520general%250Apre-trained%2520representation%2520of%2520SAM%2520and%2520polyp-relevant%2520trivial%2520clues.%2520It%2520injects%250Asalient%2520cues%2520of%2520polyp%2520images%2520into%2520the%2520SAM%2520image%2520encoder%2520as%2520a%2520domain%2520prior%2520while%250Acapturing%2520global%2520dependencies%2520at%2520various%2520scales%252C%2520leading%2520to%2520more%2520accurate%250Asegmentation%2520results.%2520Extensive%2520experiments%2520on%2520five%2520benchmark%2520datasets%2520show%250Athat%2520SAM-Mamba%2520outperforms%2520traditional%2520CNN%252C%2520ViT%252C%2520and%2520Adapter-based%2520models%2520in%250Aboth%2520quantitative%2520and%2520qualitative%2520measures.%2520Additionally%252C%2520SAM-Mamba%250Ademonstrates%2520excellent%2520adaptability%2520to%2520unseen%2520datasets%252C%2520making%2520it%2520highly%250Asuitable%2520for%2520real-time%2520clinical%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-Mamba%3A%20Mamba%20Guided%20SAM%20Architecture%20for%20Generalized%20Zero-Shot%20Polyp%0A%20%20Segmentation&entry.906535625=Tapas%20Kumar%20Dutta%20and%20Snehashis%20Majhi%20and%20Deepak%20Ranjan%20Nayak%20and%20Debesh%20Jha&entry.1292438233=%20%20Polyp%20segmentation%20in%20colonoscopy%20is%20crucial%20for%20detecting%20colorectal%20cancer.%0AHowever%2C%20it%20is%20challenging%20due%20to%20variations%20in%20the%20structure%2C%20color%2C%20and%20size%0Aof%20polyps%2C%20as%20well%20as%20the%20lack%20of%20clear%20boundaries%20with%20surrounding%20tissues.%0ATraditional%20segmentation%20models%20based%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%0Astruggle%20to%20capture%20detailed%20patterns%20and%20global%20context%2C%20limiting%20their%0Aperformance.%20Vision%20Transformer%20%28ViT%29-based%20models%20address%20some%20of%20these%20issues%0Abut%20have%20difficulties%20in%20capturing%20local%20context%20and%20lack%20strong%20zero-shot%0Ageneralization.%20To%20this%20end%2C%20we%20propose%20the%20Mamba-guided%20Segment%20Anything%20Model%0A%28SAM-Mamba%29%20for%20efficient%20polyp%20segmentation.%20Our%20approach%20introduces%20a%0AMamba-Prior%20module%20in%20the%20encoder%20to%20bridge%20the%20gap%20between%20the%20general%0Apre-trained%20representation%20of%20SAM%20and%20polyp-relevant%20trivial%20clues.%20It%20injects%0Asalient%20cues%20of%20polyp%20images%20into%20the%20SAM%20image%20encoder%20as%20a%20domain%20prior%20while%0Acapturing%20global%20dependencies%20at%20various%20scales%2C%20leading%20to%20more%20accurate%0Asegmentation%20results.%20Extensive%20experiments%20on%20five%20benchmark%20datasets%20show%0Athat%20SAM-Mamba%20outperforms%20traditional%20CNN%2C%20ViT%2C%20and%20Adapter-based%20models%20in%0Aboth%20quantitative%20and%20qualitative%20measures.%20Additionally%2C%20SAM-Mamba%0Ademonstrates%20excellent%20adaptability%20to%20unseen%20datasets%2C%20making%20it%20highly%0Asuitable%20for%20real-time%20clinical%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08482v1&entry.124074799=Read"},
{"title": "Design2GarmentCode: Turning Design Concepts to Tangible Garments Through\n  Program Synthesis", "author": "Feng Zhou and Ruiyang Liu and Chen Liu and Gaofeng He and Yong-Lu Li and Xiaogang Jin and Huamin Wang", "abstract": "  Sewing patterns, the essential blueprints for fabric cutting and tailoring,\nact as a crucial bridge between design concepts and producible garments.\nHowever, existing uni-modal sewing pattern generation models struggle to\neffectively encode complex design concepts with a multi-modal nature and\ncorrelate them with vectorized sewing patterns that possess precise geometric\nstructures and intricate sewing relations. In this work, we propose a novel\nsewing pattern generation approach Design2GarmentCode based on Large Multimodal\nModels (LMMs), to generate parametric pattern-making programs from multi-modal\ndesign concepts. LMM offers an intuitive interface for interpreting diverse\ndesign inputs, while pattern-making programs could serve as well-structured and\nsemantically meaningful representations of sewing patterns, and act as a robust\nbridge connecting the cross-domain pattern-making knowledge embedded in LMMs\nwith vectorized sewing patterns. Experimental results demonstrate that our\nmethod can flexibly handle various complex design expressions such as images,\ntextual descriptions, designer sketches, or their combinations, and convert\nthem into size-precise sewing patterns with correct stitches. Compared to\nprevious methods, our approach significantly enhances training efficiency,\ngeneration quality, and authoring flexibility. Our code and data will be\npublicly available.\n", "link": "http://arxiv.org/abs/2412.08603v1", "date": "2024-12-11", "relevancy": 2.6148, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7414}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6039}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design2GarmentCode%3A%20Turning%20Design%20Concepts%20to%20Tangible%20Garments%20Through%0A%20%20Program%20Synthesis&body=Title%3A%20Design2GarmentCode%3A%20Turning%20Design%20Concepts%20to%20Tangible%20Garments%20Through%0A%20%20Program%20Synthesis%0AAuthor%3A%20Feng%20Zhou%20and%20Ruiyang%20Liu%20and%20Chen%20Liu%20and%20Gaofeng%20He%20and%20Yong-Lu%20Li%20and%20Xiaogang%20Jin%20and%20Huamin%20Wang%0AAbstract%3A%20%20%20Sewing%20patterns%2C%20the%20essential%20blueprints%20for%20fabric%20cutting%20and%20tailoring%2C%0Aact%20as%20a%20crucial%20bridge%20between%20design%20concepts%20and%20producible%20garments.%0AHowever%2C%20existing%20uni-modal%20sewing%20pattern%20generation%20models%20struggle%20to%0Aeffectively%20encode%20complex%20design%20concepts%20with%20a%20multi-modal%20nature%20and%0Acorrelate%20them%20with%20vectorized%20sewing%20patterns%20that%20possess%20precise%20geometric%0Astructures%20and%20intricate%20sewing%20relations.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Asewing%20pattern%20generation%20approach%20Design2GarmentCode%20based%20on%20Large%20Multimodal%0AModels%20%28LMMs%29%2C%20to%20generate%20parametric%20pattern-making%20programs%20from%20multi-modal%0Adesign%20concepts.%20LMM%20offers%20an%20intuitive%20interface%20for%20interpreting%20diverse%0Adesign%20inputs%2C%20while%20pattern-making%20programs%20could%20serve%20as%20well-structured%20and%0Asemantically%20meaningful%20representations%20of%20sewing%20patterns%2C%20and%20act%20as%20a%20robust%0Abridge%20connecting%20the%20cross-domain%20pattern-making%20knowledge%20embedded%20in%20LMMs%0Awith%20vectorized%20sewing%20patterns.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20can%20flexibly%20handle%20various%20complex%20design%20expressions%20such%20as%20images%2C%0Atextual%20descriptions%2C%20designer%20sketches%2C%20or%20their%20combinations%2C%20and%20convert%0Athem%20into%20size-precise%20sewing%20patterns%20with%20correct%20stitches.%20Compared%20to%0Aprevious%20methods%2C%20our%20approach%20significantly%20enhances%20training%20efficiency%2C%0Ageneration%20quality%2C%20and%20authoring%20flexibility.%20Our%20code%20and%20data%20will%20be%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign2GarmentCode%253A%2520Turning%2520Design%2520Concepts%2520to%2520Tangible%2520Garments%2520Through%250A%2520%2520Program%2520Synthesis%26entry.906535625%3DFeng%2520Zhou%2520and%2520Ruiyang%2520Liu%2520and%2520Chen%2520Liu%2520and%2520Gaofeng%2520He%2520and%2520Yong-Lu%2520Li%2520and%2520Xiaogang%2520Jin%2520and%2520Huamin%2520Wang%26entry.1292438233%3D%2520%2520Sewing%2520patterns%252C%2520the%2520essential%2520blueprints%2520for%2520fabric%2520cutting%2520and%2520tailoring%252C%250Aact%2520as%2520a%2520crucial%2520bridge%2520between%2520design%2520concepts%2520and%2520producible%2520garments.%250AHowever%252C%2520existing%2520uni-modal%2520sewing%2520pattern%2520generation%2520models%2520struggle%2520to%250Aeffectively%2520encode%2520complex%2520design%2520concepts%2520with%2520a%2520multi-modal%2520nature%2520and%250Acorrelate%2520them%2520with%2520vectorized%2520sewing%2520patterns%2520that%2520possess%2520precise%2520geometric%250Astructures%2520and%2520intricate%2520sewing%2520relations.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Asewing%2520pattern%2520generation%2520approach%2520Design2GarmentCode%2520based%2520on%2520Large%2520Multimodal%250AModels%2520%2528LMMs%2529%252C%2520to%2520generate%2520parametric%2520pattern-making%2520programs%2520from%2520multi-modal%250Adesign%2520concepts.%2520LMM%2520offers%2520an%2520intuitive%2520interface%2520for%2520interpreting%2520diverse%250Adesign%2520inputs%252C%2520while%2520pattern-making%2520programs%2520could%2520serve%2520as%2520well-structured%2520and%250Asemantically%2520meaningful%2520representations%2520of%2520sewing%2520patterns%252C%2520and%2520act%2520as%2520a%2520robust%250Abridge%2520connecting%2520the%2520cross-domain%2520pattern-making%2520knowledge%2520embedded%2520in%2520LMMs%250Awith%2520vectorized%2520sewing%2520patterns.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520can%2520flexibly%2520handle%2520various%2520complex%2520design%2520expressions%2520such%2520as%2520images%252C%250Atextual%2520descriptions%252C%2520designer%2520sketches%252C%2520or%2520their%2520combinations%252C%2520and%2520convert%250Athem%2520into%2520size-precise%2520sewing%2520patterns%2520with%2520correct%2520stitches.%2520Compared%2520to%250Aprevious%2520methods%252C%2520our%2520approach%2520significantly%2520enhances%2520training%2520efficiency%252C%250Ageneration%2520quality%252C%2520and%2520authoring%2520flexibility.%2520Our%2520code%2520and%2520data%2520will%2520be%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design2GarmentCode%3A%20Turning%20Design%20Concepts%20to%20Tangible%20Garments%20Through%0A%20%20Program%20Synthesis&entry.906535625=Feng%20Zhou%20and%20Ruiyang%20Liu%20and%20Chen%20Liu%20and%20Gaofeng%20He%20and%20Yong-Lu%20Li%20and%20Xiaogang%20Jin%20and%20Huamin%20Wang&entry.1292438233=%20%20Sewing%20patterns%2C%20the%20essential%20blueprints%20for%20fabric%20cutting%20and%20tailoring%2C%0Aact%20as%20a%20crucial%20bridge%20between%20design%20concepts%20and%20producible%20garments.%0AHowever%2C%20existing%20uni-modal%20sewing%20pattern%20generation%20models%20struggle%20to%0Aeffectively%20encode%20complex%20design%20concepts%20with%20a%20multi-modal%20nature%20and%0Acorrelate%20them%20with%20vectorized%20sewing%20patterns%20that%20possess%20precise%20geometric%0Astructures%20and%20intricate%20sewing%20relations.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Asewing%20pattern%20generation%20approach%20Design2GarmentCode%20based%20on%20Large%20Multimodal%0AModels%20%28LMMs%29%2C%20to%20generate%20parametric%20pattern-making%20programs%20from%20multi-modal%0Adesign%20concepts.%20LMM%20offers%20an%20intuitive%20interface%20for%20interpreting%20diverse%0Adesign%20inputs%2C%20while%20pattern-making%20programs%20could%20serve%20as%20well-structured%20and%0Asemantically%20meaningful%20representations%20of%20sewing%20patterns%2C%20and%20act%20as%20a%20robust%0Abridge%20connecting%20the%20cross-domain%20pattern-making%20knowledge%20embedded%20in%20LMMs%0Awith%20vectorized%20sewing%20patterns.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20can%20flexibly%20handle%20various%20complex%20design%20expressions%20such%20as%20images%2C%0Atextual%20descriptions%2C%20designer%20sketches%2C%20or%20their%20combinations%2C%20and%20convert%0Athem%20into%20size-precise%20sewing%20patterns%20with%20correct%20stitches.%20Compared%20to%0Aprevious%20methods%2C%20our%20approach%20significantly%20enhances%20training%20efficiency%2C%0Ageneration%20quality%2C%20and%20authoring%20flexibility.%20Our%20code%20and%20data%20will%20be%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08603v1&entry.124074799=Read"},
{"title": "Can We Generate Visual Programs Without Prompting LLMs?", "author": "Michal Shlapentokh-Rothman and Yu-Xiong Wang and Derek Hoiem", "abstract": "  Visual programming prompts LLMs (large language mod-els) to generate\nexecutable code for visual tasks like visual question answering (VQA).\nPrompt-based methods are difficult to improve while also being unreliable and\ncostly in both time and money. Our goal is to develop an efficient visual\nprogramming system without 1) using prompt-based LLMs at inference time and 2)\na large set of program and answer annotations. We develop a synthetic data\naugmentation approach and alternative program generation method based on\ndecoupling programs into higher-level skills called templates and the\ncorresponding arguments. Our results show that with data augmentation,\nprompt-free smaller LLMs ($\\approx$ 1B parameters) are competitive with\nstate-of-the art models with the added benefit of much faster inference\n", "link": "http://arxiv.org/abs/2412.08564v1", "date": "2024-12-11", "relevancy": 2.613, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Generate%20Visual%20Programs%20Without%20Prompting%20LLMs%3F&body=Title%3A%20Can%20We%20Generate%20Visual%20Programs%20Without%20Prompting%20LLMs%3F%0AAuthor%3A%20Michal%20Shlapentokh-Rothman%20and%20Yu-Xiong%20Wang%20and%20Derek%20Hoiem%0AAbstract%3A%20%20%20Visual%20programming%20prompts%20LLMs%20%28large%20language%20mod-els%29%20to%20generate%0Aexecutable%20code%20for%20visual%20tasks%20like%20visual%20question%20answering%20%28VQA%29.%0APrompt-based%20methods%20are%20difficult%20to%20improve%20while%20also%20being%20unreliable%20and%0Acostly%20in%20both%20time%20and%20money.%20Our%20goal%20is%20to%20develop%20an%20efficient%20visual%0Aprogramming%20system%20without%201%29%20using%20prompt-based%20LLMs%20at%20inference%20time%20and%202%29%0Aa%20large%20set%20of%20program%20and%20answer%20annotations.%20We%20develop%20a%20synthetic%20data%0Aaugmentation%20approach%20and%20alternative%20program%20generation%20method%20based%20on%0Adecoupling%20programs%20into%20higher-level%20skills%20called%20templates%20and%20the%0Acorresponding%20arguments.%20Our%20results%20show%20that%20with%20data%20augmentation%2C%0Aprompt-free%20smaller%20LLMs%20%28%24%5Capprox%24%201B%20parameters%29%20are%20competitive%20with%0Astate-of-the%20art%20models%20with%20the%20added%20benefit%20of%20much%20faster%20inference%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520We%2520Generate%2520Visual%2520Programs%2520Without%2520Prompting%2520LLMs%253F%26entry.906535625%3DMichal%2520Shlapentokh-Rothman%2520and%2520Yu-Xiong%2520Wang%2520and%2520Derek%2520Hoiem%26entry.1292438233%3D%2520%2520Visual%2520programming%2520prompts%2520LLMs%2520%2528large%2520language%2520mod-els%2529%2520to%2520generate%250Aexecutable%2520code%2520for%2520visual%2520tasks%2520like%2520visual%2520question%2520answering%2520%2528VQA%2529.%250APrompt-based%2520methods%2520are%2520difficult%2520to%2520improve%2520while%2520also%2520being%2520unreliable%2520and%250Acostly%2520in%2520both%2520time%2520and%2520money.%2520Our%2520goal%2520is%2520to%2520develop%2520an%2520efficient%2520visual%250Aprogramming%2520system%2520without%25201%2529%2520using%2520prompt-based%2520LLMs%2520at%2520inference%2520time%2520and%25202%2529%250Aa%2520large%2520set%2520of%2520program%2520and%2520answer%2520annotations.%2520We%2520develop%2520a%2520synthetic%2520data%250Aaugmentation%2520approach%2520and%2520alternative%2520program%2520generation%2520method%2520based%2520on%250Adecoupling%2520programs%2520into%2520higher-level%2520skills%2520called%2520templates%2520and%2520the%250Acorresponding%2520arguments.%2520Our%2520results%2520show%2520that%2520with%2520data%2520augmentation%252C%250Aprompt-free%2520smaller%2520LLMs%2520%2528%2524%255Capprox%2524%25201B%2520parameters%2529%2520are%2520competitive%2520with%250Astate-of-the%2520art%2520models%2520with%2520the%2520added%2520benefit%2520of%2520much%2520faster%2520inference%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Generate%20Visual%20Programs%20Without%20Prompting%20LLMs%3F&entry.906535625=Michal%20Shlapentokh-Rothman%20and%20Yu-Xiong%20Wang%20and%20Derek%20Hoiem&entry.1292438233=%20%20Visual%20programming%20prompts%20LLMs%20%28large%20language%20mod-els%29%20to%20generate%0Aexecutable%20code%20for%20visual%20tasks%20like%20visual%20question%20answering%20%28VQA%29.%0APrompt-based%20methods%20are%20difficult%20to%20improve%20while%20also%20being%20unreliable%20and%0Acostly%20in%20both%20time%20and%20money.%20Our%20goal%20is%20to%20develop%20an%20efficient%20visual%0Aprogramming%20system%20without%201%29%20using%20prompt-based%20LLMs%20at%20inference%20time%20and%202%29%0Aa%20large%20set%20of%20program%20and%20answer%20annotations.%20We%20develop%20a%20synthetic%20data%0Aaugmentation%20approach%20and%20alternative%20program%20generation%20method%20based%20on%0Adecoupling%20programs%20into%20higher-level%20skills%20called%20templates%20and%20the%0Acorresponding%20arguments.%20Our%20results%20show%20that%20with%20data%20augmentation%2C%0Aprompt-free%20smaller%20LLMs%20%28%24%5Capprox%24%201B%20parameters%29%20are%20competitive%20with%0Astate-of-the%20art%20models%20with%20the%20added%20benefit%20of%20much%20faster%20inference%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08564v1&entry.124074799=Read"},
{"title": "CAT: Class Aware Adaptive Thresholding for Semi-Supervised Domain\n  Generalization", "author": "Sumaiya Zoha and Jeong-Gun Lee and Young-Woong Ko", "abstract": "  Domain Generalization (DG) seeks to transfer knowledge from multiple source\ndomains to unseen target domains, even in the presence of domain shifts.\nAchieving effective generalization typically requires a large and diverse set\nof labeled source data to learn robust representations that can generalize to\nnew, unseen domains. However, obtaining such high-quality labeled data is often\ncostly and labor-intensive, limiting the practical applicability of DG. To\naddress this, we investigate a more practical and challenging problem:\nsemi-supervised domain generalization (SSDG) under a label-efficient paradigm.\nIn this paper, we propose a novel method, CAT, which leverages semi-supervised\nlearning with limited labeled data to achieve competitive generalization\nperformance under domain shifts. Our method addresses key limitations of\nprevious approaches, such as reliance on fixed thresholds and sensitivity to\nnoisy pseudo-labels. CAT combines adaptive thresholding with noisy label\nrefinement techniques, creating a straightforward yet highly effective solution\nfor SSDG tasks. Specifically, our approach uses flexible thresholding to\ngenerate high-quality pseudo-labels with higher class diversity while refining\nnoisy pseudo-labels to improve their reliability. Extensive experiments across\nmultiple benchmark datasets demonstrate the superior performance of our method,\nhighlighting its effectiveness in achieving robust generalization under domain\nshift.\n", "link": "http://arxiv.org/abs/2412.08479v1", "date": "2024-12-11", "relevancy": 2.6057, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5215}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT%3A%20Class%20Aware%20Adaptive%20Thresholding%20for%20Semi-Supervised%20Domain%0A%20%20Generalization&body=Title%3A%20CAT%3A%20Class%20Aware%20Adaptive%20Thresholding%20for%20Semi-Supervised%20Domain%0A%20%20Generalization%0AAuthor%3A%20Sumaiya%20Zoha%20and%20Jeong-Gun%20Lee%20and%20Young-Woong%20Ko%0AAbstract%3A%20%20%20Domain%20Generalization%20%28DG%29%20seeks%20to%20transfer%20knowledge%20from%20multiple%20source%0Adomains%20to%20unseen%20target%20domains%2C%20even%20in%20the%20presence%20of%20domain%20shifts.%0AAchieving%20effective%20generalization%20typically%20requires%20a%20large%20and%20diverse%20set%0Aof%20labeled%20source%20data%20to%20learn%20robust%20representations%20that%20can%20generalize%20to%0Anew%2C%20unseen%20domains.%20However%2C%20obtaining%20such%20high-quality%20labeled%20data%20is%20often%0Acostly%20and%20labor-intensive%2C%20limiting%20the%20practical%20applicability%20of%20DG.%20To%0Aaddress%20this%2C%20we%20investigate%20a%20more%20practical%20and%20challenging%20problem%3A%0Asemi-supervised%20domain%20generalization%20%28SSDG%29%20under%20a%20label-efficient%20paradigm.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%20CAT%2C%20which%20leverages%20semi-supervised%0Alearning%20with%20limited%20labeled%20data%20to%20achieve%20competitive%20generalization%0Aperformance%20under%20domain%20shifts.%20Our%20method%20addresses%20key%20limitations%20of%0Aprevious%20approaches%2C%20such%20as%20reliance%20on%20fixed%20thresholds%20and%20sensitivity%20to%0Anoisy%20pseudo-labels.%20CAT%20combines%20adaptive%20thresholding%20with%20noisy%20label%0Arefinement%20techniques%2C%20creating%20a%20straightforward%20yet%20highly%20effective%20solution%0Afor%20SSDG%20tasks.%20Specifically%2C%20our%20approach%20uses%20flexible%20thresholding%20to%0Agenerate%20high-quality%20pseudo-labels%20with%20higher%20class%20diversity%20while%20refining%0Anoisy%20pseudo-labels%20to%20improve%20their%20reliability.%20Extensive%20experiments%20across%0Amultiple%20benchmark%20datasets%20demonstrate%20the%20superior%20performance%20of%20our%20method%2C%0Ahighlighting%20its%20effectiveness%20in%20achieving%20robust%20generalization%20under%20domain%0Ashift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT%253A%2520Class%2520Aware%2520Adaptive%2520Thresholding%2520for%2520Semi-Supervised%2520Domain%250A%2520%2520Generalization%26entry.906535625%3DSumaiya%2520Zoha%2520and%2520Jeong-Gun%2520Lee%2520and%2520Young-Woong%2520Ko%26entry.1292438233%3D%2520%2520Domain%2520Generalization%2520%2528DG%2529%2520seeks%2520to%2520transfer%2520knowledge%2520from%2520multiple%2520source%250Adomains%2520to%2520unseen%2520target%2520domains%252C%2520even%2520in%2520the%2520presence%2520of%2520domain%2520shifts.%250AAchieving%2520effective%2520generalization%2520typically%2520requires%2520a%2520large%2520and%2520diverse%2520set%250Aof%2520labeled%2520source%2520data%2520to%2520learn%2520robust%2520representations%2520that%2520can%2520generalize%2520to%250Anew%252C%2520unseen%2520domains.%2520However%252C%2520obtaining%2520such%2520high-quality%2520labeled%2520data%2520is%2520often%250Acostly%2520and%2520labor-intensive%252C%2520limiting%2520the%2520practical%2520applicability%2520of%2520DG.%2520To%250Aaddress%2520this%252C%2520we%2520investigate%2520a%2520more%2520practical%2520and%2520challenging%2520problem%253A%250Asemi-supervised%2520domain%2520generalization%2520%2528SSDG%2529%2520under%2520a%2520label-efficient%2520paradigm.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520CAT%252C%2520which%2520leverages%2520semi-supervised%250Alearning%2520with%2520limited%2520labeled%2520data%2520to%2520achieve%2520competitive%2520generalization%250Aperformance%2520under%2520domain%2520shifts.%2520Our%2520method%2520addresses%2520key%2520limitations%2520of%250Aprevious%2520approaches%252C%2520such%2520as%2520reliance%2520on%2520fixed%2520thresholds%2520and%2520sensitivity%2520to%250Anoisy%2520pseudo-labels.%2520CAT%2520combines%2520adaptive%2520thresholding%2520with%2520noisy%2520label%250Arefinement%2520techniques%252C%2520creating%2520a%2520straightforward%2520yet%2520highly%2520effective%2520solution%250Afor%2520SSDG%2520tasks.%2520Specifically%252C%2520our%2520approach%2520uses%2520flexible%2520thresholding%2520to%250Agenerate%2520high-quality%2520pseudo-labels%2520with%2520higher%2520class%2520diversity%2520while%2520refining%250Anoisy%2520pseudo-labels%2520to%2520improve%2520their%2520reliability.%2520Extensive%2520experiments%2520across%250Amultiple%2520benchmark%2520datasets%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%2520method%252C%250Ahighlighting%2520its%2520effectiveness%2520in%2520achieving%2520robust%2520generalization%2520under%2520domain%250Ashift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT%3A%20Class%20Aware%20Adaptive%20Thresholding%20for%20Semi-Supervised%20Domain%0A%20%20Generalization&entry.906535625=Sumaiya%20Zoha%20and%20Jeong-Gun%20Lee%20and%20Young-Woong%20Ko&entry.1292438233=%20%20Domain%20Generalization%20%28DG%29%20seeks%20to%20transfer%20knowledge%20from%20multiple%20source%0Adomains%20to%20unseen%20target%20domains%2C%20even%20in%20the%20presence%20of%20domain%20shifts.%0AAchieving%20effective%20generalization%20typically%20requires%20a%20large%20and%20diverse%20set%0Aof%20labeled%20source%20data%20to%20learn%20robust%20representations%20that%20can%20generalize%20to%0Anew%2C%20unseen%20domains.%20However%2C%20obtaining%20such%20high-quality%20labeled%20data%20is%20often%0Acostly%20and%20labor-intensive%2C%20limiting%20the%20practical%20applicability%20of%20DG.%20To%0Aaddress%20this%2C%20we%20investigate%20a%20more%20practical%20and%20challenging%20problem%3A%0Asemi-supervised%20domain%20generalization%20%28SSDG%29%20under%20a%20label-efficient%20paradigm.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%20CAT%2C%20which%20leverages%20semi-supervised%0Alearning%20with%20limited%20labeled%20data%20to%20achieve%20competitive%20generalization%0Aperformance%20under%20domain%20shifts.%20Our%20method%20addresses%20key%20limitations%20of%0Aprevious%20approaches%2C%20such%20as%20reliance%20on%20fixed%20thresholds%20and%20sensitivity%20to%0Anoisy%20pseudo-labels.%20CAT%20combines%20adaptive%20thresholding%20with%20noisy%20label%0Arefinement%20techniques%2C%20creating%20a%20straightforward%20yet%20highly%20effective%20solution%0Afor%20SSDG%20tasks.%20Specifically%2C%20our%20approach%20uses%20flexible%20thresholding%20to%0Agenerate%20high-quality%20pseudo-labels%20with%20higher%20class%20diversity%20while%20refining%0Anoisy%20pseudo-labels%20to%20improve%20their%20reliability.%20Extensive%20experiments%20across%0Amultiple%20benchmark%20datasets%20demonstrate%20the%20superior%20performance%20of%20our%20method%2C%0Ahighlighting%20its%20effectiveness%20in%20achieving%20robust%20generalization%20under%20domain%0Ashift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08479v1&entry.124074799=Read"},
{"title": "DistrictNet: Decision-aware learning for geographical districting", "author": "Cheikh Ahmed and Alexandre Forel and Axel Parmentier and Thibaut Vidal", "abstract": "  Districting is a complex combinatorial problem that consists in partitioning\na geographical area into small districts. In logistics, it is a major strategic\ndecision determining operating costs for several years. Solving districting\nproblems using traditional methods is intractable even for small geographical\nareas and existing heuristics often provide sub-optimal results. We present a\nstructured learning approach to find high-quality solutions to real-world\ndistricting problems in a few minutes. It is based on integrating a\ncombinatorial optimization layer, the capacitated minimum spanning tree\nproblem, into a graph neural network architecture. To train this pipeline in a\ndecision-aware fashion, we show how to construct target solutions embedded in a\nsuitable space and learn from target solutions. Experiments show that our\napproach outperforms existing methods as it can significantly reduce costs on\nreal-world cities.\n", "link": "http://arxiv.org/abs/2412.08287v1", "date": "2024-12-11", "relevancy": 2.557, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5409}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.512}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DistrictNet%3A%20Decision-aware%20learning%20for%20geographical%20districting&body=Title%3A%20DistrictNet%3A%20Decision-aware%20learning%20for%20geographical%20districting%0AAuthor%3A%20Cheikh%20Ahmed%20and%20Alexandre%20Forel%20and%20Axel%20Parmentier%20and%20Thibaut%20Vidal%0AAbstract%3A%20%20%20Districting%20is%20a%20complex%20combinatorial%20problem%20that%20consists%20in%20partitioning%0Aa%20geographical%20area%20into%20small%20districts.%20In%20logistics%2C%20it%20is%20a%20major%20strategic%0Adecision%20determining%20operating%20costs%20for%20several%20years.%20Solving%20districting%0Aproblems%20using%20traditional%20methods%20is%20intractable%20even%20for%20small%20geographical%0Aareas%20and%20existing%20heuristics%20often%20provide%20sub-optimal%20results.%20We%20present%20a%0Astructured%20learning%20approach%20to%20find%20high-quality%20solutions%20to%20real-world%0Adistricting%20problems%20in%20a%20few%20minutes.%20It%20is%20based%20on%20integrating%20a%0Acombinatorial%20optimization%20layer%2C%20the%20capacitated%20minimum%20spanning%20tree%0Aproblem%2C%20into%20a%20graph%20neural%20network%20architecture.%20To%20train%20this%20pipeline%20in%20a%0Adecision-aware%20fashion%2C%20we%20show%20how%20to%20construct%20target%20solutions%20embedded%20in%20a%0Asuitable%20space%20and%20learn%20from%20target%20solutions.%20Experiments%20show%20that%20our%0Aapproach%20outperforms%20existing%20methods%20as%20it%20can%20significantly%20reduce%20costs%20on%0Areal-world%20cities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistrictNet%253A%2520Decision-aware%2520learning%2520for%2520geographical%2520districting%26entry.906535625%3DCheikh%2520Ahmed%2520and%2520Alexandre%2520Forel%2520and%2520Axel%2520Parmentier%2520and%2520Thibaut%2520Vidal%26entry.1292438233%3D%2520%2520Districting%2520is%2520a%2520complex%2520combinatorial%2520problem%2520that%2520consists%2520in%2520partitioning%250Aa%2520geographical%2520area%2520into%2520small%2520districts.%2520In%2520logistics%252C%2520it%2520is%2520a%2520major%2520strategic%250Adecision%2520determining%2520operating%2520costs%2520for%2520several%2520years.%2520Solving%2520districting%250Aproblems%2520using%2520traditional%2520methods%2520is%2520intractable%2520even%2520for%2520small%2520geographical%250Aareas%2520and%2520existing%2520heuristics%2520often%2520provide%2520sub-optimal%2520results.%2520We%2520present%2520a%250Astructured%2520learning%2520approach%2520to%2520find%2520high-quality%2520solutions%2520to%2520real-world%250Adistricting%2520problems%2520in%2520a%2520few%2520minutes.%2520It%2520is%2520based%2520on%2520integrating%2520a%250Acombinatorial%2520optimization%2520layer%252C%2520the%2520capacitated%2520minimum%2520spanning%2520tree%250Aproblem%252C%2520into%2520a%2520graph%2520neural%2520network%2520architecture.%2520To%2520train%2520this%2520pipeline%2520in%2520a%250Adecision-aware%2520fashion%252C%2520we%2520show%2520how%2520to%2520construct%2520target%2520solutions%2520embedded%2520in%2520a%250Asuitable%2520space%2520and%2520learn%2520from%2520target%2520solutions.%2520Experiments%2520show%2520that%2520our%250Aapproach%2520outperforms%2520existing%2520methods%2520as%2520it%2520can%2520significantly%2520reduce%2520costs%2520on%250Areal-world%2520cities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DistrictNet%3A%20Decision-aware%20learning%20for%20geographical%20districting&entry.906535625=Cheikh%20Ahmed%20and%20Alexandre%20Forel%20and%20Axel%20Parmentier%20and%20Thibaut%20Vidal&entry.1292438233=%20%20Districting%20is%20a%20complex%20combinatorial%20problem%20that%20consists%20in%20partitioning%0Aa%20geographical%20area%20into%20small%20districts.%20In%20logistics%2C%20it%20is%20a%20major%20strategic%0Adecision%20determining%20operating%20costs%20for%20several%20years.%20Solving%20districting%0Aproblems%20using%20traditional%20methods%20is%20intractable%20even%20for%20small%20geographical%0Aareas%20and%20existing%20heuristics%20often%20provide%20sub-optimal%20results.%20We%20present%20a%0Astructured%20learning%20approach%20to%20find%20high-quality%20solutions%20to%20real-world%0Adistricting%20problems%20in%20a%20few%20minutes.%20It%20is%20based%20on%20integrating%20a%0Acombinatorial%20optimization%20layer%2C%20the%20capacitated%20minimum%20spanning%20tree%0Aproblem%2C%20into%20a%20graph%20neural%20network%20architecture.%20To%20train%20this%20pipeline%20in%20a%0Adecision-aware%20fashion%2C%20we%20show%20how%20to%20construct%20target%20solutions%20embedded%20in%20a%0Asuitable%20space%20and%20learn%20from%20target%20solutions.%20Experiments%20show%20that%20our%0Aapproach%20outperforms%20existing%20methods%20as%20it%20can%20significantly%20reduce%20costs%20on%0Areal-world%20cities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08287v1&entry.124074799=Read"},
{"title": "Understanding Token Probability Encoding in Output Embeddings", "author": "Hakaze Cho and Yoshihiro Sakai and Kenshiro Tanaka and Mariko Kato and Naoya Inoue", "abstract": "  In this paper, we investigate the output token probability information in the\noutput embedding of language models. We find an approximate common log-linear\nencoding of output token probabilities within the output embedding vectors and\nempirically demonstrate that it is accurate and sparse. As a causality\nexamination, we steer the encoding in output embedding to modify the output\nprobability distribution accurately. Moreover, the sparsity we find in output\nprobability encoding suggests that a large number of dimensions in the output\nembedding do not contribute to causal language modeling. Therefore, we attempt\nto delete the output-unrelated dimensions and find more than 30% of the\ndimensions can be deleted without significant movement in output distribution\nand sequence generation. Additionally, in the pre-training dynamics of language\nmodels, we find that the output embeddings capture the corpus token frequency\ninformation in early steps, even before an obvious convergence of parameters\nstarts.\n", "link": "http://arxiv.org/abs/2406.01468v2", "date": "2024-12-11", "relevancy": 2.5252, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Token%20Probability%20Encoding%20in%20Output%20Embeddings&body=Title%3A%20Understanding%20Token%20Probability%20Encoding%20in%20Output%20Embeddings%0AAuthor%3A%20Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Kenshiro%20Tanaka%20and%20Mariko%20Kato%20and%20Naoya%20Inoue%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20output%20token%20probability%20information%20in%20the%0Aoutput%20embedding%20of%20language%20models.%20We%20find%20an%20approximate%20common%20log-linear%0Aencoding%20of%20output%20token%20probabilities%20within%20the%20output%20embedding%20vectors%20and%0Aempirically%20demonstrate%20that%20it%20is%20accurate%20and%20sparse.%20As%20a%20causality%0Aexamination%2C%20we%20steer%20the%20encoding%20in%20output%20embedding%20to%20modify%20the%20output%0Aprobability%20distribution%20accurately.%20Moreover%2C%20the%20sparsity%20we%20find%20in%20output%0Aprobability%20encoding%20suggests%20that%20a%20large%20number%20of%20dimensions%20in%20the%20output%0Aembedding%20do%20not%20contribute%20to%20causal%20language%20modeling.%20Therefore%2C%20we%20attempt%0Ato%20delete%20the%20output-unrelated%20dimensions%20and%20find%20more%20than%2030%25%20of%20the%0Adimensions%20can%20be%20deleted%20without%20significant%20movement%20in%20output%20distribution%0Aand%20sequence%20generation.%20Additionally%2C%20in%20the%20pre-training%20dynamics%20of%20language%0Amodels%2C%20we%20find%20that%20the%20output%20embeddings%20capture%20the%20corpus%20token%20frequency%0Ainformation%20in%20early%20steps%2C%20even%20before%20an%20obvious%20convergence%20of%20parameters%0Astarts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Token%2520Probability%2520Encoding%2520in%2520Output%2520Embeddings%26entry.906535625%3DHakaze%2520Cho%2520and%2520Yoshihiro%2520Sakai%2520and%2520Kenshiro%2520Tanaka%2520and%2520Mariko%2520Kato%2520and%2520Naoya%2520Inoue%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520output%2520token%2520probability%2520information%2520in%2520the%250Aoutput%2520embedding%2520of%2520language%2520models.%2520We%2520find%2520an%2520approximate%2520common%2520log-linear%250Aencoding%2520of%2520output%2520token%2520probabilities%2520within%2520the%2520output%2520embedding%2520vectors%2520and%250Aempirically%2520demonstrate%2520that%2520it%2520is%2520accurate%2520and%2520sparse.%2520As%2520a%2520causality%250Aexamination%252C%2520we%2520steer%2520the%2520encoding%2520in%2520output%2520embedding%2520to%2520modify%2520the%2520output%250Aprobability%2520distribution%2520accurately.%2520Moreover%252C%2520the%2520sparsity%2520we%2520find%2520in%2520output%250Aprobability%2520encoding%2520suggests%2520that%2520a%2520large%2520number%2520of%2520dimensions%2520in%2520the%2520output%250Aembedding%2520do%2520not%2520contribute%2520to%2520causal%2520language%2520modeling.%2520Therefore%252C%2520we%2520attempt%250Ato%2520delete%2520the%2520output-unrelated%2520dimensions%2520and%2520find%2520more%2520than%252030%2525%2520of%2520the%250Adimensions%2520can%2520be%2520deleted%2520without%2520significant%2520movement%2520in%2520output%2520distribution%250Aand%2520sequence%2520generation.%2520Additionally%252C%2520in%2520the%2520pre-training%2520dynamics%2520of%2520language%250Amodels%252C%2520we%2520find%2520that%2520the%2520output%2520embeddings%2520capture%2520the%2520corpus%2520token%2520frequency%250Ainformation%2520in%2520early%2520steps%252C%2520even%2520before%2520an%2520obvious%2520convergence%2520of%2520parameters%250Astarts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Token%20Probability%20Encoding%20in%20Output%20Embeddings&entry.906535625=Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Kenshiro%20Tanaka%20and%20Mariko%20Kato%20and%20Naoya%20Inoue&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20output%20token%20probability%20information%20in%20the%0Aoutput%20embedding%20of%20language%20models.%20We%20find%20an%20approximate%20common%20log-linear%0Aencoding%20of%20output%20token%20probabilities%20within%20the%20output%20embedding%20vectors%20and%0Aempirically%20demonstrate%20that%20it%20is%20accurate%20and%20sparse.%20As%20a%20causality%0Aexamination%2C%20we%20steer%20the%20encoding%20in%20output%20embedding%20to%20modify%20the%20output%0Aprobability%20distribution%20accurately.%20Moreover%2C%20the%20sparsity%20we%20find%20in%20output%0Aprobability%20encoding%20suggests%20that%20a%20large%20number%20of%20dimensions%20in%20the%20output%0Aembedding%20do%20not%20contribute%20to%20causal%20language%20modeling.%20Therefore%2C%20we%20attempt%0Ato%20delete%20the%20output-unrelated%20dimensions%20and%20find%20more%20than%2030%25%20of%20the%0Adimensions%20can%20be%20deleted%20without%20significant%20movement%20in%20output%20distribution%0Aand%20sequence%20generation.%20Additionally%2C%20in%20the%20pre-training%20dynamics%20of%20language%0Amodels%2C%20we%20find%20that%20the%20output%20embeddings%20capture%20the%20corpus%20token%20frequency%0Ainformation%20in%20early%20steps%2C%20even%20before%20an%20obvious%20convergence%20of%20parameters%0Astarts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01468v2&entry.124074799=Read"},
{"title": "Local Features Meet Stochastic Anonymization: Revolutionizing\n  Privacy-Preserving Face Recognition for Black-Box Models", "author": "Yuanwei Liu and Chengyu Jia and Ruqi Xiao and Xuemai Jia and Hui Wei and Kui Jiang and Zheng Wang", "abstract": "  The task of privacy-preserving face recognition (PPFR) currently faces two\nmajor unsolved challenges: (1) existing methods are typically effective only on\nspecific face recognition models and struggle to generalize to black-box face\nrecognition models; (2) current methods employ data-driven reversible\nrepresentation encoding for privacy protection, making them susceptible to\nadversarial learning and reconstruction of the original image. We observe that\nface recognition models primarily rely on local features ({e.g., face contour,\nskin texture, and so on) for identification. Thus, by disrupting global\nfeatures while enhancing local features, we achieve effective recognition even\nin black-box environments. Additionally, to prevent adversarial models from\nlearning and reversing the anonymization process, we adopt an adversarial\nlearning-based approach with irreversible stochastic injection to ensure the\nstochastic nature of the anonymization. Experimental results demonstrate that\nour method achieves an average recognition accuracy of 94.21\\% on black-box\nmodels, outperforming existing methods in both privacy protection and\nanti-reconstruction capabilities.\n", "link": "http://arxiv.org/abs/2412.08276v1", "date": "2024-12-11", "relevancy": 2.5186, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5129}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5082}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Features%20Meet%20Stochastic%20Anonymization%3A%20Revolutionizing%0A%20%20Privacy-Preserving%20Face%20Recognition%20for%20Black-Box%20Models&body=Title%3A%20Local%20Features%20Meet%20Stochastic%20Anonymization%3A%20Revolutionizing%0A%20%20Privacy-Preserving%20Face%20Recognition%20for%20Black-Box%20Models%0AAuthor%3A%20Yuanwei%20Liu%20and%20Chengyu%20Jia%20and%20Ruqi%20Xiao%20and%20Xuemai%20Jia%20and%20Hui%20Wei%20and%20Kui%20Jiang%20and%20Zheng%20Wang%0AAbstract%3A%20%20%20The%20task%20of%20privacy-preserving%20face%20recognition%20%28PPFR%29%20currently%20faces%20two%0Amajor%20unsolved%20challenges%3A%20%281%29%20existing%20methods%20are%20typically%20effective%20only%20on%0Aspecific%20face%20recognition%20models%20and%20struggle%20to%20generalize%20to%20black-box%20face%0Arecognition%20models%3B%20%282%29%20current%20methods%20employ%20data-driven%20reversible%0Arepresentation%20encoding%20for%20privacy%20protection%2C%20making%20them%20susceptible%20to%0Aadversarial%20learning%20and%20reconstruction%20of%20the%20original%20image.%20We%20observe%20that%0Aface%20recognition%20models%20primarily%20rely%20on%20local%20features%20%28%7Be.g.%2C%20face%20contour%2C%0Askin%20texture%2C%20and%20so%20on%29%20for%20identification.%20Thus%2C%20by%20disrupting%20global%0Afeatures%20while%20enhancing%20local%20features%2C%20we%20achieve%20effective%20recognition%20even%0Ain%20black-box%20environments.%20Additionally%2C%20to%20prevent%20adversarial%20models%20from%0Alearning%20and%20reversing%20the%20anonymization%20process%2C%20we%20adopt%20an%20adversarial%0Alearning-based%20approach%20with%20irreversible%20stochastic%20injection%20to%20ensure%20the%0Astochastic%20nature%20of%20the%20anonymization.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20achieves%20an%20average%20recognition%20accuracy%20of%2094.21%5C%25%20on%20black-box%0Amodels%2C%20outperforming%20existing%20methods%20in%20both%20privacy%20protection%20and%0Aanti-reconstruction%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Features%2520Meet%2520Stochastic%2520Anonymization%253A%2520Revolutionizing%250A%2520%2520Privacy-Preserving%2520Face%2520Recognition%2520for%2520Black-Box%2520Models%26entry.906535625%3DYuanwei%2520Liu%2520and%2520Chengyu%2520Jia%2520and%2520Ruqi%2520Xiao%2520and%2520Xuemai%2520Jia%2520and%2520Hui%2520Wei%2520and%2520Kui%2520Jiang%2520and%2520Zheng%2520Wang%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520privacy-preserving%2520face%2520recognition%2520%2528PPFR%2529%2520currently%2520faces%2520two%250Amajor%2520unsolved%2520challenges%253A%2520%25281%2529%2520existing%2520methods%2520are%2520typically%2520effective%2520only%2520on%250Aspecific%2520face%2520recognition%2520models%2520and%2520struggle%2520to%2520generalize%2520to%2520black-box%2520face%250Arecognition%2520models%253B%2520%25282%2529%2520current%2520methods%2520employ%2520data-driven%2520reversible%250Arepresentation%2520encoding%2520for%2520privacy%2520protection%252C%2520making%2520them%2520susceptible%2520to%250Aadversarial%2520learning%2520and%2520reconstruction%2520of%2520the%2520original%2520image.%2520We%2520observe%2520that%250Aface%2520recognition%2520models%2520primarily%2520rely%2520on%2520local%2520features%2520%2528%257Be.g.%252C%2520face%2520contour%252C%250Askin%2520texture%252C%2520and%2520so%2520on%2529%2520for%2520identification.%2520Thus%252C%2520by%2520disrupting%2520global%250Afeatures%2520while%2520enhancing%2520local%2520features%252C%2520we%2520achieve%2520effective%2520recognition%2520even%250Ain%2520black-box%2520environments.%2520Additionally%252C%2520to%2520prevent%2520adversarial%2520models%2520from%250Alearning%2520and%2520reversing%2520the%2520anonymization%2520process%252C%2520we%2520adopt%2520an%2520adversarial%250Alearning-based%2520approach%2520with%2520irreversible%2520stochastic%2520injection%2520to%2520ensure%2520the%250Astochastic%2520nature%2520of%2520the%2520anonymization.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520an%2520average%2520recognition%2520accuracy%2520of%252094.21%255C%2525%2520on%2520black-box%250Amodels%252C%2520outperforming%2520existing%2520methods%2520in%2520both%2520privacy%2520protection%2520and%250Aanti-reconstruction%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Features%20Meet%20Stochastic%20Anonymization%3A%20Revolutionizing%0A%20%20Privacy-Preserving%20Face%20Recognition%20for%20Black-Box%20Models&entry.906535625=Yuanwei%20Liu%20and%20Chengyu%20Jia%20and%20Ruqi%20Xiao%20and%20Xuemai%20Jia%20and%20Hui%20Wei%20and%20Kui%20Jiang%20and%20Zheng%20Wang&entry.1292438233=%20%20The%20task%20of%20privacy-preserving%20face%20recognition%20%28PPFR%29%20currently%20faces%20two%0Amajor%20unsolved%20challenges%3A%20%281%29%20existing%20methods%20are%20typically%20effective%20only%20on%0Aspecific%20face%20recognition%20models%20and%20struggle%20to%20generalize%20to%20black-box%20face%0Arecognition%20models%3B%20%282%29%20current%20methods%20employ%20data-driven%20reversible%0Arepresentation%20encoding%20for%20privacy%20protection%2C%20making%20them%20susceptible%20to%0Aadversarial%20learning%20and%20reconstruction%20of%20the%20original%20image.%20We%20observe%20that%0Aface%20recognition%20models%20primarily%20rely%20on%20local%20features%20%28%7Be.g.%2C%20face%20contour%2C%0Askin%20texture%2C%20and%20so%20on%29%20for%20identification.%20Thus%2C%20by%20disrupting%20global%0Afeatures%20while%20enhancing%20local%20features%2C%20we%20achieve%20effective%20recognition%20even%0Ain%20black-box%20environments.%20Additionally%2C%20to%20prevent%20adversarial%20models%20from%0Alearning%20and%20reversing%20the%20anonymization%20process%2C%20we%20adopt%20an%20adversarial%0Alearning-based%20approach%20with%20irreversible%20stochastic%20injection%20to%20ensure%20the%0Astochastic%20nature%20of%20the%20anonymization.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20achieves%20an%20average%20recognition%20accuracy%20of%2094.21%5C%25%20on%20black-box%0Amodels%2C%20outperforming%20existing%20methods%20in%20both%20privacy%20protection%20and%0Aanti-reconstruction%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08276v1&entry.124074799=Read"},
{"title": "Open-Canopy: Towards Very High Resolution Forest Monitoring", "author": "Fajwel Fogel and Yohann Perron and Nikola Besic and Laurent Saint-Andr\u00e9 and Agn\u00e8s Pellissier-Tanon and Martin Schwartz and Thomas Boudras and Ibrahim Fayad and Alexandre d'Aspremont and Loic Landrieu and Philippe Ciais", "abstract": "  Estimating canopy height and its changes at meter resolution from satellite\nimagery is a significant challenge in computer vision with critical\nenvironmental applications. However, the lack of open-access datasets at this\nresolution hinders the reproducibility and evaluation of models. We introduce\nOpen-Canopy, the first open-access, country-scale benchmark for very\nhigh-resolution (1.5 m) canopy height estimation, covering over 87,000 km$^2$\nacross France with 1.5 m resolution satellite imagery and aerial LiDAR data.\nAdditionally, we present Open-Canopy-$\\Delta$, a benchmark for canopy height\nchange detection between images from different years at tree level-a\nchallenging task for current computer vision models. We evaluate\nstate-of-the-art architectures on these benchmarks, highlighting significant\nchallenges and opportunities for improvement. Our datasets and code are\npublicly available at https://github.com/fajwel/Open-Canopy.\n", "link": "http://arxiv.org/abs/2407.09392v4", "date": "2024-12-11", "relevancy": 2.5004, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Canopy%3A%20Towards%20Very%20High%20Resolution%20Forest%20Monitoring&body=Title%3A%20Open-Canopy%3A%20Towards%20Very%20High%20Resolution%20Forest%20Monitoring%0AAuthor%3A%20Fajwel%20Fogel%20and%20Yohann%20Perron%20and%20Nikola%20Besic%20and%20Laurent%20Saint-Andr%C3%A9%20and%20Agn%C3%A8s%20Pellissier-Tanon%20and%20Martin%20Schwartz%20and%20Thomas%20Boudras%20and%20Ibrahim%20Fayad%20and%20Alexandre%20d%27Aspremont%20and%20Loic%20Landrieu%20and%20Philippe%20Ciais%0AAbstract%3A%20%20%20Estimating%20canopy%20height%20and%20its%20changes%20at%20meter%20resolution%20from%20satellite%0Aimagery%20is%20a%20significant%20challenge%20in%20computer%20vision%20with%20critical%0Aenvironmental%20applications.%20However%2C%20the%20lack%20of%20open-access%20datasets%20at%20this%0Aresolution%20hinders%20the%20reproducibility%20and%20evaluation%20of%20models.%20We%20introduce%0AOpen-Canopy%2C%20the%20first%20open-access%2C%20country-scale%20benchmark%20for%20very%0Ahigh-resolution%20%281.5%20m%29%20canopy%20height%20estimation%2C%20covering%20over%2087%2C000%20km%24%5E2%24%0Aacross%20France%20with%201.5%20m%20resolution%20satellite%20imagery%20and%20aerial%20LiDAR%20data.%0AAdditionally%2C%20we%20present%20Open-Canopy-%24%5CDelta%24%2C%20a%20benchmark%20for%20canopy%20height%0Achange%20detection%20between%20images%20from%20different%20years%20at%20tree%20level-a%0Achallenging%20task%20for%20current%20computer%20vision%20models.%20We%20evaluate%0Astate-of-the-art%20architectures%20on%20these%20benchmarks%2C%20highlighting%20significant%0Achallenges%20and%20opportunities%20for%20improvement.%20Our%20datasets%20and%20code%20are%0Apublicly%20available%20at%20https%3A//github.com/fajwel/Open-Canopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09392v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Canopy%253A%2520Towards%2520Very%2520High%2520Resolution%2520Forest%2520Monitoring%26entry.906535625%3DFajwel%2520Fogel%2520and%2520Yohann%2520Perron%2520and%2520Nikola%2520Besic%2520and%2520Laurent%2520Saint-Andr%25C3%25A9%2520and%2520Agn%25C3%25A8s%2520Pellissier-Tanon%2520and%2520Martin%2520Schwartz%2520and%2520Thomas%2520Boudras%2520and%2520Ibrahim%2520Fayad%2520and%2520Alexandre%2520d%2527Aspremont%2520and%2520Loic%2520Landrieu%2520and%2520Philippe%2520Ciais%26entry.1292438233%3D%2520%2520Estimating%2520canopy%2520height%2520and%2520its%2520changes%2520at%2520meter%2520resolution%2520from%2520satellite%250Aimagery%2520is%2520a%2520significant%2520challenge%2520in%2520computer%2520vision%2520with%2520critical%250Aenvironmental%2520applications.%2520However%252C%2520the%2520lack%2520of%2520open-access%2520datasets%2520at%2520this%250Aresolution%2520hinders%2520the%2520reproducibility%2520and%2520evaluation%2520of%2520models.%2520We%2520introduce%250AOpen-Canopy%252C%2520the%2520first%2520open-access%252C%2520country-scale%2520benchmark%2520for%2520very%250Ahigh-resolution%2520%25281.5%2520m%2529%2520canopy%2520height%2520estimation%252C%2520covering%2520over%252087%252C000%2520km%2524%255E2%2524%250Aacross%2520France%2520with%25201.5%2520m%2520resolution%2520satellite%2520imagery%2520and%2520aerial%2520LiDAR%2520data.%250AAdditionally%252C%2520we%2520present%2520Open-Canopy-%2524%255CDelta%2524%252C%2520a%2520benchmark%2520for%2520canopy%2520height%250Achange%2520detection%2520between%2520images%2520from%2520different%2520years%2520at%2520tree%2520level-a%250Achallenging%2520task%2520for%2520current%2520computer%2520vision%2520models.%2520We%2520evaluate%250Astate-of-the-art%2520architectures%2520on%2520these%2520benchmarks%252C%2520highlighting%2520significant%250Achallenges%2520and%2520opportunities%2520for%2520improvement.%2520Our%2520datasets%2520and%2520code%2520are%250Apublicly%2520available%2520at%2520https%253A//github.com/fajwel/Open-Canopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09392v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Canopy%3A%20Towards%20Very%20High%20Resolution%20Forest%20Monitoring&entry.906535625=Fajwel%20Fogel%20and%20Yohann%20Perron%20and%20Nikola%20Besic%20and%20Laurent%20Saint-Andr%C3%A9%20and%20Agn%C3%A8s%20Pellissier-Tanon%20and%20Martin%20Schwartz%20and%20Thomas%20Boudras%20and%20Ibrahim%20Fayad%20and%20Alexandre%20d%27Aspremont%20and%20Loic%20Landrieu%20and%20Philippe%20Ciais&entry.1292438233=%20%20Estimating%20canopy%20height%20and%20its%20changes%20at%20meter%20resolution%20from%20satellite%0Aimagery%20is%20a%20significant%20challenge%20in%20computer%20vision%20with%20critical%0Aenvironmental%20applications.%20However%2C%20the%20lack%20of%20open-access%20datasets%20at%20this%0Aresolution%20hinders%20the%20reproducibility%20and%20evaluation%20of%20models.%20We%20introduce%0AOpen-Canopy%2C%20the%20first%20open-access%2C%20country-scale%20benchmark%20for%20very%0Ahigh-resolution%20%281.5%20m%29%20canopy%20height%20estimation%2C%20covering%20over%2087%2C000%20km%24%5E2%24%0Aacross%20France%20with%201.5%20m%20resolution%20satellite%20imagery%20and%20aerial%20LiDAR%20data.%0AAdditionally%2C%20we%20present%20Open-Canopy-%24%5CDelta%24%2C%20a%20benchmark%20for%20canopy%20height%0Achange%20detection%20between%20images%20from%20different%20years%20at%20tree%20level-a%0Achallenging%20task%20for%20current%20computer%20vision%20models.%20We%20evaluate%0Astate-of-the-art%20architectures%20on%20these%20benchmarks%2C%20highlighting%20significant%0Achallenges%20and%20opportunities%20for%20improvement.%20Our%20datasets%20and%20code%20are%0Apublicly%20available%20at%20https%3A//github.com/fajwel/Open-Canopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09392v4&entry.124074799=Read"},
{"title": "Orchestrating the Symphony of Prompt Distribution Learning for\n  Human-Object Interaction Detection", "author": "Mingda Jia and Liming Zhao and Ge Li and Yun Zheng", "abstract": "  Human-object interaction (HOI) detectors with popular query-transformer\narchitecture have achieved promising performance. However, accurately\nidentifying uncommon visual patterns and distinguishing between ambiguous HOIs\ncontinue to be difficult for them. We observe that these difficulties may arise\nfrom the limited capacity of traditional detector queries in representing\ndiverse intra-category patterns and inter-category dependencies. To address\nthis, we introduce the Interaction Prompt Distribution Learning (InterProDa)\napproach. InterProDa learns multiple sets of soft prompts and estimates\ncategory distributions from various prompts. It then incorporates HOI queries\nwith category distributions, making them capable of representing near-infinite\nintra-category dynamics and universal cross-category relationships. Our\nInterProDa detector demonstrates competitive performance on HICO-DET and vcoco\nbenchmarks. Additionally, our method can be integrated into most\ntransformer-based HOI detectors, significantly enhancing their performance with\nminimal additional parameters.\n", "link": "http://arxiv.org/abs/2412.08506v1", "date": "2024-12-11", "relevancy": 2.495, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5234}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orchestrating%20the%20Symphony%20of%20Prompt%20Distribution%20Learning%20for%0A%20%20Human-Object%20Interaction%20Detection&body=Title%3A%20Orchestrating%20the%20Symphony%20of%20Prompt%20Distribution%20Learning%20for%0A%20%20Human-Object%20Interaction%20Detection%0AAuthor%3A%20Mingda%20Jia%20and%20Liming%20Zhao%20and%20Ge%20Li%20and%20Yun%20Zheng%0AAbstract%3A%20%20%20Human-object%20interaction%20%28HOI%29%20detectors%20with%20popular%20query-transformer%0Aarchitecture%20have%20achieved%20promising%20performance.%20However%2C%20accurately%0Aidentifying%20uncommon%20visual%20patterns%20and%20distinguishing%20between%20ambiguous%20HOIs%0Acontinue%20to%20be%20difficult%20for%20them.%20We%20observe%20that%20these%20difficulties%20may%20arise%0Afrom%20the%20limited%20capacity%20of%20traditional%20detector%20queries%20in%20representing%0Adiverse%20intra-category%20patterns%20and%20inter-category%20dependencies.%20To%20address%0Athis%2C%20we%20introduce%20the%20Interaction%20Prompt%20Distribution%20Learning%20%28InterProDa%29%0Aapproach.%20InterProDa%20learns%20multiple%20sets%20of%20soft%20prompts%20and%20estimates%0Acategory%20distributions%20from%20various%20prompts.%20It%20then%20incorporates%20HOI%20queries%0Awith%20category%20distributions%2C%20making%20them%20capable%20of%20representing%20near-infinite%0Aintra-category%20dynamics%20and%20universal%20cross-category%20relationships.%20Our%0AInterProDa%20detector%20demonstrates%20competitive%20performance%20on%20HICO-DET%20and%20vcoco%0Abenchmarks.%20Additionally%2C%20our%20method%20can%20be%20integrated%20into%20most%0Atransformer-based%20HOI%20detectors%2C%20significantly%20enhancing%20their%20performance%20with%0Aminimal%20additional%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrchestrating%2520the%2520Symphony%2520of%2520Prompt%2520Distribution%2520Learning%2520for%250A%2520%2520Human-Object%2520Interaction%2520Detection%26entry.906535625%3DMingda%2520Jia%2520and%2520Liming%2520Zhao%2520and%2520Ge%2520Li%2520and%2520Yun%2520Zheng%26entry.1292438233%3D%2520%2520Human-object%2520interaction%2520%2528HOI%2529%2520detectors%2520with%2520popular%2520query-transformer%250Aarchitecture%2520have%2520achieved%2520promising%2520performance.%2520However%252C%2520accurately%250Aidentifying%2520uncommon%2520visual%2520patterns%2520and%2520distinguishing%2520between%2520ambiguous%2520HOIs%250Acontinue%2520to%2520be%2520difficult%2520for%2520them.%2520We%2520observe%2520that%2520these%2520difficulties%2520may%2520arise%250Afrom%2520the%2520limited%2520capacity%2520of%2520traditional%2520detector%2520queries%2520in%2520representing%250Adiverse%2520intra-category%2520patterns%2520and%2520inter-category%2520dependencies.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520the%2520Interaction%2520Prompt%2520Distribution%2520Learning%2520%2528InterProDa%2529%250Aapproach.%2520InterProDa%2520learns%2520multiple%2520sets%2520of%2520soft%2520prompts%2520and%2520estimates%250Acategory%2520distributions%2520from%2520various%2520prompts.%2520It%2520then%2520incorporates%2520HOI%2520queries%250Awith%2520category%2520distributions%252C%2520making%2520them%2520capable%2520of%2520representing%2520near-infinite%250Aintra-category%2520dynamics%2520and%2520universal%2520cross-category%2520relationships.%2520Our%250AInterProDa%2520detector%2520demonstrates%2520competitive%2520performance%2520on%2520HICO-DET%2520and%2520vcoco%250Abenchmarks.%2520Additionally%252C%2520our%2520method%2520can%2520be%2520integrated%2520into%2520most%250Atransformer-based%2520HOI%2520detectors%252C%2520significantly%2520enhancing%2520their%2520performance%2520with%250Aminimal%2520additional%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orchestrating%20the%20Symphony%20of%20Prompt%20Distribution%20Learning%20for%0A%20%20Human-Object%20Interaction%20Detection&entry.906535625=Mingda%20Jia%20and%20Liming%20Zhao%20and%20Ge%20Li%20and%20Yun%20Zheng&entry.1292438233=%20%20Human-object%20interaction%20%28HOI%29%20detectors%20with%20popular%20query-transformer%0Aarchitecture%20have%20achieved%20promising%20performance.%20However%2C%20accurately%0Aidentifying%20uncommon%20visual%20patterns%20and%20distinguishing%20between%20ambiguous%20HOIs%0Acontinue%20to%20be%20difficult%20for%20them.%20We%20observe%20that%20these%20difficulties%20may%20arise%0Afrom%20the%20limited%20capacity%20of%20traditional%20detector%20queries%20in%20representing%0Adiverse%20intra-category%20patterns%20and%20inter-category%20dependencies.%20To%20address%0Athis%2C%20we%20introduce%20the%20Interaction%20Prompt%20Distribution%20Learning%20%28InterProDa%29%0Aapproach.%20InterProDa%20learns%20multiple%20sets%20of%20soft%20prompts%20and%20estimates%0Acategory%20distributions%20from%20various%20prompts.%20It%20then%20incorporates%20HOI%20queries%0Awith%20category%20distributions%2C%20making%20them%20capable%20of%20representing%20near-infinite%0Aintra-category%20dynamics%20and%20universal%20cross-category%20relationships.%20Our%0AInterProDa%20detector%20demonstrates%20competitive%20performance%20on%20HICO-DET%20and%20vcoco%0Abenchmarks.%20Additionally%2C%20our%20method%20can%20be%20integrated%20into%20most%0Atransformer-based%20HOI%20detectors%2C%20significantly%20enhancing%20their%20performance%20with%0Aminimal%20additional%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08506v1&entry.124074799=Read"},
{"title": "Social Recommendation through Heterogeneous Graph Modeling of the\n  Long-term and Short-term Preference Defined by Dynamic Time Spans", "author": "Behafarid Mohammad Jafari and Xiao Luo and Ali Jafari", "abstract": "  Social recommendations have been widely adopted in substantial domains.\nRecently, graph neural networks (GNN) have been employed in recommender systems\ndue to their success in graph representation learning. However, dealing with\nthe dynamic property of social network data is a challenge. This research\npresents a novel method that provides social recommendations by incorporating\nthe dynamic property of social network data in a heterogeneous graph. The model\naims to capture user preference over time without going through the\ncomplexities of a dynamic graph by adding period nodes to define users'\nlong-term and short-term preferences and aggregating assigned edge weights. The\nmodel is applied to real-world data to argue its superior performance.\nPromising results demonstrate the effectiveness of this model.\n", "link": "http://arxiv.org/abs/2312.14306v2", "date": "2024-12-11", "relevancy": 2.4793, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5347}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4851}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Social%20Recommendation%20through%20Heterogeneous%20Graph%20Modeling%20of%20the%0A%20%20Long-term%20and%20Short-term%20Preference%20Defined%20by%20Dynamic%20Time%20Spans&body=Title%3A%20Social%20Recommendation%20through%20Heterogeneous%20Graph%20Modeling%20of%20the%0A%20%20Long-term%20and%20Short-term%20Preference%20Defined%20by%20Dynamic%20Time%20Spans%0AAuthor%3A%20Behafarid%20Mohammad%20Jafari%20and%20Xiao%20Luo%20and%20Ali%20Jafari%0AAbstract%3A%20%20%20Social%20recommendations%20have%20been%20widely%20adopted%20in%20substantial%20domains.%0ARecently%2C%20graph%20neural%20networks%20%28GNN%29%20have%20been%20employed%20in%20recommender%20systems%0Adue%20to%20their%20success%20in%20graph%20representation%20learning.%20However%2C%20dealing%20with%0Athe%20dynamic%20property%20of%20social%20network%20data%20is%20a%20challenge.%20This%20research%0Apresents%20a%20novel%20method%20that%20provides%20social%20recommendations%20by%20incorporating%0Athe%20dynamic%20property%20of%20social%20network%20data%20in%20a%20heterogeneous%20graph.%20The%20model%0Aaims%20to%20capture%20user%20preference%20over%20time%20without%20going%20through%20the%0Acomplexities%20of%20a%20dynamic%20graph%20by%20adding%20period%20nodes%20to%20define%20users%27%0Along-term%20and%20short-term%20preferences%20and%20aggregating%20assigned%20edge%20weights.%20The%0Amodel%20is%20applied%20to%20real-world%20data%20to%20argue%20its%20superior%20performance.%0APromising%20results%20demonstrate%20the%20effectiveness%20of%20this%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14306v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocial%2520Recommendation%2520through%2520Heterogeneous%2520Graph%2520Modeling%2520of%2520the%250A%2520%2520Long-term%2520and%2520Short-term%2520Preference%2520Defined%2520by%2520Dynamic%2520Time%2520Spans%26entry.906535625%3DBehafarid%2520Mohammad%2520Jafari%2520and%2520Xiao%2520Luo%2520and%2520Ali%2520Jafari%26entry.1292438233%3D%2520%2520Social%2520recommendations%2520have%2520been%2520widely%2520adopted%2520in%2520substantial%2520domains.%250ARecently%252C%2520graph%2520neural%2520networks%2520%2528GNN%2529%2520have%2520been%2520employed%2520in%2520recommender%2520systems%250Adue%2520to%2520their%2520success%2520in%2520graph%2520representation%2520learning.%2520However%252C%2520dealing%2520with%250Athe%2520dynamic%2520property%2520of%2520social%2520network%2520data%2520is%2520a%2520challenge.%2520This%2520research%250Apresents%2520a%2520novel%2520method%2520that%2520provides%2520social%2520recommendations%2520by%2520incorporating%250Athe%2520dynamic%2520property%2520of%2520social%2520network%2520data%2520in%2520a%2520heterogeneous%2520graph.%2520The%2520model%250Aaims%2520to%2520capture%2520user%2520preference%2520over%2520time%2520without%2520going%2520through%2520the%250Acomplexities%2520of%2520a%2520dynamic%2520graph%2520by%2520adding%2520period%2520nodes%2520to%2520define%2520users%2527%250Along-term%2520and%2520short-term%2520preferences%2520and%2520aggregating%2520assigned%2520edge%2520weights.%2520The%250Amodel%2520is%2520applied%2520to%2520real-world%2520data%2520to%2520argue%2520its%2520superior%2520performance.%250APromising%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14306v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social%20Recommendation%20through%20Heterogeneous%20Graph%20Modeling%20of%20the%0A%20%20Long-term%20and%20Short-term%20Preference%20Defined%20by%20Dynamic%20Time%20Spans&entry.906535625=Behafarid%20Mohammad%20Jafari%20and%20Xiao%20Luo%20and%20Ali%20Jafari&entry.1292438233=%20%20Social%20recommendations%20have%20been%20widely%20adopted%20in%20substantial%20domains.%0ARecently%2C%20graph%20neural%20networks%20%28GNN%29%20have%20been%20employed%20in%20recommender%20systems%0Adue%20to%20their%20success%20in%20graph%20representation%20learning.%20However%2C%20dealing%20with%0Athe%20dynamic%20property%20of%20social%20network%20data%20is%20a%20challenge.%20This%20research%0Apresents%20a%20novel%20method%20that%20provides%20social%20recommendations%20by%20incorporating%0Athe%20dynamic%20property%20of%20social%20network%20data%20in%20a%20heterogeneous%20graph.%20The%20model%0Aaims%20to%20capture%20user%20preference%20over%20time%20without%20going%20through%20the%0Acomplexities%20of%20a%20dynamic%20graph%20by%20adding%20period%20nodes%20to%20define%20users%27%0Along-term%20and%20short-term%20preferences%20and%20aggregating%20assigned%20edge%20weights.%20The%0Amodel%20is%20applied%20to%20real-world%20data%20to%20argue%20its%20superior%20performance.%0APromising%20results%20demonstrate%20the%20effectiveness%20of%20this%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14306v2&entry.124074799=Read"},
{"title": "ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven\n  Generation", "author": "Daniel Winter and Asaf Shul and Matan Cohen and Dana Berman and Yael Pritch and Alex Rav-Acha and Yedid Hoshen", "abstract": "  This paper introduces a tuning-free method for both object insertion and\nsubject-driven generation. The task involves composing an object, given\nmultiple views, into a scene specified by either an image or text. Existing\nmethods struggle to fully meet the task's challenging objectives: (i)\nseamlessly composing the object into the scene with photorealistic pose and\nlighting, and (ii) preserving the object's identity. We hypothesize that\nachieving these goals requires large scale supervision, but manually collecting\nsufficient data is simply too expensive. The key observation in this paper is\nthat many mass-produced objects recur across multiple images of large unlabeled\ndatasets, in different scenes, poses, and lighting conditions. We use this\nobservation to create massive supervision by retrieving sets of diverse views\nof the same object. This powerful paired dataset enables us to train a\nstraightforward text-to-image diffusion architecture to map the object and\nscene descriptions to the composited image. We compare our method, ObjectMate,\nwith state-of-the-art methods for object insertion and subject-driven\ngeneration, using a single or multiple references. Empirically, ObjectMate\nachieves superior identity preservation and more photorealistic composition.\nDifferently from many other multi-reference methods, ObjectMate does not\nrequire slow test-time tuning.\n", "link": "http://arxiv.org/abs/2412.08645v1", "date": "2024-12-11", "relevancy": 2.4656, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6791}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6284}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObjectMate%3A%20A%20Recurrence%20Prior%20for%20Object%20Insertion%20and%20Subject-Driven%0A%20%20Generation&body=Title%3A%20ObjectMate%3A%20A%20Recurrence%20Prior%20for%20Object%20Insertion%20and%20Subject-Driven%0A%20%20Generation%0AAuthor%3A%20Daniel%20Winter%20and%20Asaf%20Shul%20and%20Matan%20Cohen%20and%20Dana%20Berman%20and%20Yael%20Pritch%20and%20Alex%20Rav-Acha%20and%20Yedid%20Hoshen%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20tuning-free%20method%20for%20both%20object%20insertion%20and%0Asubject-driven%20generation.%20The%20task%20involves%20composing%20an%20object%2C%20given%0Amultiple%20views%2C%20into%20a%20scene%20specified%20by%20either%20an%20image%20or%20text.%20Existing%0Amethods%20struggle%20to%20fully%20meet%20the%20task%27s%20challenging%20objectives%3A%20%28i%29%0Aseamlessly%20composing%20the%20object%20into%20the%20scene%20with%20photorealistic%20pose%20and%0Alighting%2C%20and%20%28ii%29%20preserving%20the%20object%27s%20identity.%20We%20hypothesize%20that%0Aachieving%20these%20goals%20requires%20large%20scale%20supervision%2C%20but%20manually%20collecting%0Asufficient%20data%20is%20simply%20too%20expensive.%20The%20key%20observation%20in%20this%20paper%20is%0Athat%20many%20mass-produced%20objects%20recur%20across%20multiple%20images%20of%20large%20unlabeled%0Adatasets%2C%20in%20different%20scenes%2C%20poses%2C%20and%20lighting%20conditions.%20We%20use%20this%0Aobservation%20to%20create%20massive%20supervision%20by%20retrieving%20sets%20of%20diverse%20views%0Aof%20the%20same%20object.%20This%20powerful%20paired%20dataset%20enables%20us%20to%20train%20a%0Astraightforward%20text-to-image%20diffusion%20architecture%20to%20map%20the%20object%20and%0Ascene%20descriptions%20to%20the%20composited%20image.%20We%20compare%20our%20method%2C%20ObjectMate%2C%0Awith%20state-of-the-art%20methods%20for%20object%20insertion%20and%20subject-driven%0Ageneration%2C%20using%20a%20single%20or%20multiple%20references.%20Empirically%2C%20ObjectMate%0Aachieves%20superior%20identity%20preservation%20and%20more%20photorealistic%20composition.%0ADifferently%20from%20many%20other%20multi-reference%20methods%2C%20ObjectMate%20does%20not%0Arequire%20slow%20test-time%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObjectMate%253A%2520A%2520Recurrence%2520Prior%2520for%2520Object%2520Insertion%2520and%2520Subject-Driven%250A%2520%2520Generation%26entry.906535625%3DDaniel%2520Winter%2520and%2520Asaf%2520Shul%2520and%2520Matan%2520Cohen%2520and%2520Dana%2520Berman%2520and%2520Yael%2520Pritch%2520and%2520Alex%2520Rav-Acha%2520and%2520Yedid%2520Hoshen%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520tuning-free%2520method%2520for%2520both%2520object%2520insertion%2520and%250Asubject-driven%2520generation.%2520The%2520task%2520involves%2520composing%2520an%2520object%252C%2520given%250Amultiple%2520views%252C%2520into%2520a%2520scene%2520specified%2520by%2520either%2520an%2520image%2520or%2520text.%2520Existing%250Amethods%2520struggle%2520to%2520fully%2520meet%2520the%2520task%2527s%2520challenging%2520objectives%253A%2520%2528i%2529%250Aseamlessly%2520composing%2520the%2520object%2520into%2520the%2520scene%2520with%2520photorealistic%2520pose%2520and%250Alighting%252C%2520and%2520%2528ii%2529%2520preserving%2520the%2520object%2527s%2520identity.%2520We%2520hypothesize%2520that%250Aachieving%2520these%2520goals%2520requires%2520large%2520scale%2520supervision%252C%2520but%2520manually%2520collecting%250Asufficient%2520data%2520is%2520simply%2520too%2520expensive.%2520The%2520key%2520observation%2520in%2520this%2520paper%2520is%250Athat%2520many%2520mass-produced%2520objects%2520recur%2520across%2520multiple%2520images%2520of%2520large%2520unlabeled%250Adatasets%252C%2520in%2520different%2520scenes%252C%2520poses%252C%2520and%2520lighting%2520conditions.%2520We%2520use%2520this%250Aobservation%2520to%2520create%2520massive%2520supervision%2520by%2520retrieving%2520sets%2520of%2520diverse%2520views%250Aof%2520the%2520same%2520object.%2520This%2520powerful%2520paired%2520dataset%2520enables%2520us%2520to%2520train%2520a%250Astraightforward%2520text-to-image%2520diffusion%2520architecture%2520to%2520map%2520the%2520object%2520and%250Ascene%2520descriptions%2520to%2520the%2520composited%2520image.%2520We%2520compare%2520our%2520method%252C%2520ObjectMate%252C%250Awith%2520state-of-the-art%2520methods%2520for%2520object%2520insertion%2520and%2520subject-driven%250Ageneration%252C%2520using%2520a%2520single%2520or%2520multiple%2520references.%2520Empirically%252C%2520ObjectMate%250Aachieves%2520superior%2520identity%2520preservation%2520and%2520more%2520photorealistic%2520composition.%250ADifferently%2520from%2520many%2520other%2520multi-reference%2520methods%252C%2520ObjectMate%2520does%2520not%250Arequire%2520slow%2520test-time%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObjectMate%3A%20A%20Recurrence%20Prior%20for%20Object%20Insertion%20and%20Subject-Driven%0A%20%20Generation&entry.906535625=Daniel%20Winter%20and%20Asaf%20Shul%20and%20Matan%20Cohen%20and%20Dana%20Berman%20and%20Yael%20Pritch%20and%20Alex%20Rav-Acha%20and%20Yedid%20Hoshen&entry.1292438233=%20%20This%20paper%20introduces%20a%20tuning-free%20method%20for%20both%20object%20insertion%20and%0Asubject-driven%20generation.%20The%20task%20involves%20composing%20an%20object%2C%20given%0Amultiple%20views%2C%20into%20a%20scene%20specified%20by%20either%20an%20image%20or%20text.%20Existing%0Amethods%20struggle%20to%20fully%20meet%20the%20task%27s%20challenging%20objectives%3A%20%28i%29%0Aseamlessly%20composing%20the%20object%20into%20the%20scene%20with%20photorealistic%20pose%20and%0Alighting%2C%20and%20%28ii%29%20preserving%20the%20object%27s%20identity.%20We%20hypothesize%20that%0Aachieving%20these%20goals%20requires%20large%20scale%20supervision%2C%20but%20manually%20collecting%0Asufficient%20data%20is%20simply%20too%20expensive.%20The%20key%20observation%20in%20this%20paper%20is%0Athat%20many%20mass-produced%20objects%20recur%20across%20multiple%20images%20of%20large%20unlabeled%0Adatasets%2C%20in%20different%20scenes%2C%20poses%2C%20and%20lighting%20conditions.%20We%20use%20this%0Aobservation%20to%20create%20massive%20supervision%20by%20retrieving%20sets%20of%20diverse%20views%0Aof%20the%20same%20object.%20This%20powerful%20paired%20dataset%20enables%20us%20to%20train%20a%0Astraightforward%20text-to-image%20diffusion%20architecture%20to%20map%20the%20object%20and%0Ascene%20descriptions%20to%20the%20composited%20image.%20We%20compare%20our%20method%2C%20ObjectMate%2C%0Awith%20state-of-the-art%20methods%20for%20object%20insertion%20and%20subject-driven%0Ageneration%2C%20using%20a%20single%20or%20multiple%20references.%20Empirically%2C%20ObjectMate%0Aachieves%20superior%20identity%20preservation%20and%20more%20photorealistic%20composition.%0ADifferently%20from%20many%20other%20multi-reference%20methods%2C%20ObjectMate%20does%20not%0Arequire%20slow%20test-time%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08645v1&entry.124074799=Read"},
{"title": "Lan-grasp: Using Large Language Models for Semantic Object Grasping", "author": "Reihaneh Mirjalili and Michael Krawez and Simone Silenzi and Yannik Blei and Wolfram Burgard", "abstract": "  In this paper, we propose Lan-grasp, a novel approach towards more\nappropriate semantic grasping. We use foundation models to provide the robot\nwith a deeper understanding of the objects, the right place to grasp an object,\nor even the parts to avoid. This allows our robot to grasp and utilize objects\nin a more meaningful and safe manner. We leverage the combination of a Large\nLanguage Model, a Vision Language Model, and a traditional grasp planner to\ngenerate grasps demonstrating a deeper semantic understanding of the objects.\nWe first prompt the Large Language Model about which object part is appropriate\nfor grasping. Next, the Vision Language Model identifies the corresponding part\nin the object image. Finally, we generate grasp proposals in the region\nproposed by the Vision Language Model. Building on foundation models provides\nus with a zero-shot grasp method that can handle a wide range of objects\nwithout the need for further training or fine-tuning. We evaluated our method\nin real-world experiments on a custom object data set. We present the results\nof a survey that asks the participants to choose an object part appropriate for\ngrasping. The results show that the grasps generated by our method are\nconsistently ranked higher by the participants than those generated by a\nconventional grasping planner and a recent semantic grasping approach. In\naddition, we propose a Visual Chain-of-Thought feedback loop to assess grasp\nfeasibility in complex scenarios. This mechanism enables dynamic reasoning and\ngenerates alternative grasp strategies when needed, ensuring safer and more\neffective grasping outcomes.\n", "link": "http://arxiv.org/abs/2310.05239v2", "date": "2024-12-11", "relevancy": 2.4651, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lan-grasp%3A%20Using%20Large%20Language%20Models%20for%20Semantic%20Object%20Grasping&body=Title%3A%20Lan-grasp%3A%20Using%20Large%20Language%20Models%20for%20Semantic%20Object%20Grasping%0AAuthor%3A%20Reihaneh%20Mirjalili%20and%20Michael%20Krawez%20and%20Simone%20Silenzi%20and%20Yannik%20Blei%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Lan-grasp%2C%20a%20novel%20approach%20towards%20more%0Aappropriate%20semantic%20grasping.%20We%20use%20foundation%20models%20to%20provide%20the%20robot%0Awith%20a%20deeper%20understanding%20of%20the%20objects%2C%20the%20right%20place%20to%20grasp%20an%20object%2C%0Aor%20even%20the%20parts%20to%20avoid.%20This%20allows%20our%20robot%20to%20grasp%20and%20utilize%20objects%0Ain%20a%20more%20meaningful%20and%20safe%20manner.%20We%20leverage%20the%20combination%20of%20a%20Large%0ALanguage%20Model%2C%20a%20Vision%20Language%20Model%2C%20and%20a%20traditional%20grasp%20planner%20to%0Agenerate%20grasps%20demonstrating%20a%20deeper%20semantic%20understanding%20of%20the%20objects.%0AWe%20first%20prompt%20the%20Large%20Language%20Model%20about%20which%20object%20part%20is%20appropriate%0Afor%20grasping.%20Next%2C%20the%20Vision%20Language%20Model%20identifies%20the%20corresponding%20part%0Ain%20the%20object%20image.%20Finally%2C%20we%20generate%20grasp%20proposals%20in%20the%20region%0Aproposed%20by%20the%20Vision%20Language%20Model.%20Building%20on%20foundation%20models%20provides%0Aus%20with%20a%20zero-shot%20grasp%20method%20that%20can%20handle%20a%20wide%20range%20of%20objects%0Awithout%20the%20need%20for%20further%20training%20or%20fine-tuning.%20We%20evaluated%20our%20method%0Ain%20real-world%20experiments%20on%20a%20custom%20object%20data%20set.%20We%20present%20the%20results%0Aof%20a%20survey%20that%20asks%20the%20participants%20to%20choose%20an%20object%20part%20appropriate%20for%0Agrasping.%20The%20results%20show%20that%20the%20grasps%20generated%20by%20our%20method%20are%0Aconsistently%20ranked%20higher%20by%20the%20participants%20than%20those%20generated%20by%20a%0Aconventional%20grasping%20planner%20and%20a%20recent%20semantic%20grasping%20approach.%20In%0Aaddition%2C%20we%20propose%20a%20Visual%20Chain-of-Thought%20feedback%20loop%20to%20assess%20grasp%0Afeasibility%20in%20complex%20scenarios.%20This%20mechanism%20enables%20dynamic%20reasoning%20and%0Agenerates%20alternative%20grasp%20strategies%20when%20needed%2C%20ensuring%20safer%20and%20more%0Aeffective%20grasping%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLan-grasp%253A%2520Using%2520Large%2520Language%2520Models%2520for%2520Semantic%2520Object%2520Grasping%26entry.906535625%3DReihaneh%2520Mirjalili%2520and%2520Michael%2520Krawez%2520and%2520Simone%2520Silenzi%2520and%2520Yannik%2520Blei%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Lan-grasp%252C%2520a%2520novel%2520approach%2520towards%2520more%250Aappropriate%2520semantic%2520grasping.%2520We%2520use%2520foundation%2520models%2520to%2520provide%2520the%2520robot%250Awith%2520a%2520deeper%2520understanding%2520of%2520the%2520objects%252C%2520the%2520right%2520place%2520to%2520grasp%2520an%2520object%252C%250Aor%2520even%2520the%2520parts%2520to%2520avoid.%2520This%2520allows%2520our%2520robot%2520to%2520grasp%2520and%2520utilize%2520objects%250Ain%2520a%2520more%2520meaningful%2520and%2520safe%2520manner.%2520We%2520leverage%2520the%2520combination%2520of%2520a%2520Large%250ALanguage%2520Model%252C%2520a%2520Vision%2520Language%2520Model%252C%2520and%2520a%2520traditional%2520grasp%2520planner%2520to%250Agenerate%2520grasps%2520demonstrating%2520a%2520deeper%2520semantic%2520understanding%2520of%2520the%2520objects.%250AWe%2520first%2520prompt%2520the%2520Large%2520Language%2520Model%2520about%2520which%2520object%2520part%2520is%2520appropriate%250Afor%2520grasping.%2520Next%252C%2520the%2520Vision%2520Language%2520Model%2520identifies%2520the%2520corresponding%2520part%250Ain%2520the%2520object%2520image.%2520Finally%252C%2520we%2520generate%2520grasp%2520proposals%2520in%2520the%2520region%250Aproposed%2520by%2520the%2520Vision%2520Language%2520Model.%2520Building%2520on%2520foundation%2520models%2520provides%250Aus%2520with%2520a%2520zero-shot%2520grasp%2520method%2520that%2520can%2520handle%2520a%2520wide%2520range%2520of%2520objects%250Awithout%2520the%2520need%2520for%2520further%2520training%2520or%2520fine-tuning.%2520We%2520evaluated%2520our%2520method%250Ain%2520real-world%2520experiments%2520on%2520a%2520custom%2520object%2520data%2520set.%2520We%2520present%2520the%2520results%250Aof%2520a%2520survey%2520that%2520asks%2520the%2520participants%2520to%2520choose%2520an%2520object%2520part%2520appropriate%2520for%250Agrasping.%2520The%2520results%2520show%2520that%2520the%2520grasps%2520generated%2520by%2520our%2520method%2520are%250Aconsistently%2520ranked%2520higher%2520by%2520the%2520participants%2520than%2520those%2520generated%2520by%2520a%250Aconventional%2520grasping%2520planner%2520and%2520a%2520recent%2520semantic%2520grasping%2520approach.%2520In%250Aaddition%252C%2520we%2520propose%2520a%2520Visual%2520Chain-of-Thought%2520feedback%2520loop%2520to%2520assess%2520grasp%250Afeasibility%2520in%2520complex%2520scenarios.%2520This%2520mechanism%2520enables%2520dynamic%2520reasoning%2520and%250Agenerates%2520alternative%2520grasp%2520strategies%2520when%2520needed%252C%2520ensuring%2520safer%2520and%2520more%250Aeffective%2520grasping%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lan-grasp%3A%20Using%20Large%20Language%20Models%20for%20Semantic%20Object%20Grasping&entry.906535625=Reihaneh%20Mirjalili%20and%20Michael%20Krawez%20and%20Simone%20Silenzi%20and%20Yannik%20Blei%20and%20Wolfram%20Burgard&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Lan-grasp%2C%20a%20novel%20approach%20towards%20more%0Aappropriate%20semantic%20grasping.%20We%20use%20foundation%20models%20to%20provide%20the%20robot%0Awith%20a%20deeper%20understanding%20of%20the%20objects%2C%20the%20right%20place%20to%20grasp%20an%20object%2C%0Aor%20even%20the%20parts%20to%20avoid.%20This%20allows%20our%20robot%20to%20grasp%20and%20utilize%20objects%0Ain%20a%20more%20meaningful%20and%20safe%20manner.%20We%20leverage%20the%20combination%20of%20a%20Large%0ALanguage%20Model%2C%20a%20Vision%20Language%20Model%2C%20and%20a%20traditional%20grasp%20planner%20to%0Agenerate%20grasps%20demonstrating%20a%20deeper%20semantic%20understanding%20of%20the%20objects.%0AWe%20first%20prompt%20the%20Large%20Language%20Model%20about%20which%20object%20part%20is%20appropriate%0Afor%20grasping.%20Next%2C%20the%20Vision%20Language%20Model%20identifies%20the%20corresponding%20part%0Ain%20the%20object%20image.%20Finally%2C%20we%20generate%20grasp%20proposals%20in%20the%20region%0Aproposed%20by%20the%20Vision%20Language%20Model.%20Building%20on%20foundation%20models%20provides%0Aus%20with%20a%20zero-shot%20grasp%20method%20that%20can%20handle%20a%20wide%20range%20of%20objects%0Awithout%20the%20need%20for%20further%20training%20or%20fine-tuning.%20We%20evaluated%20our%20method%0Ain%20real-world%20experiments%20on%20a%20custom%20object%20data%20set.%20We%20present%20the%20results%0Aof%20a%20survey%20that%20asks%20the%20participants%20to%20choose%20an%20object%20part%20appropriate%20for%0Agrasping.%20The%20results%20show%20that%20the%20grasps%20generated%20by%20our%20method%20are%0Aconsistently%20ranked%20higher%20by%20the%20participants%20than%20those%20generated%20by%20a%0Aconventional%20grasping%20planner%20and%20a%20recent%20semantic%20grasping%20approach.%20In%0Aaddition%2C%20we%20propose%20a%20Visual%20Chain-of-Thought%20feedback%20loop%20to%20assess%20grasp%0Afeasibility%20in%20complex%20scenarios.%20This%20mechanism%20enables%20dynamic%20reasoning%20and%0Agenerates%20alternative%20grasp%20strategies%20when%20needed%2C%20ensuring%20safer%20and%20more%0Aeffective%20grasping%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05239v2&entry.124074799=Read"},
{"title": "POINTS1.5: Building a Vision-Language Model towards Real World\n  Applications", "author": "Yuan Liu and Le Tian and Xiao Zhou and Xinyu Gao and Kavio Yu and Yang Yu and Jie Zhou", "abstract": "  Vision-language models have made significant strides recently, demonstrating\nsuperior performance across a range of tasks, e.g. optical character\nrecognition and complex diagram analysis. Building on this trend, we introduce\na new vision-language model, POINTS1.5, designed to excel in various real-world\napplications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several\nkey innovations: i) We replace the original CLIP vision encoder, which had a\nfixed image resolution, with a NaViT-style vision encoder that supports native\ndynamic high resolution. This allows POINTS1.5 to process images of any\nresolution without needing to split them into tiles. ii) We add bilingual\nsupport to POINTS1.5, significantly enhancing its capability in Chinese. Due to\nthe scarcity of open-source Chinese datasets for vision-language models, we\ncollect numerous images from the Internet and annotate them using a combination\nof manual and automatic methods. iii) We propose a set of rigorous filtering\nmethods for visual instruction tuning datasets. We comprehensively evaluate all\nthese filtering methods, and choose the most effective ones to obtain the final\nvisual instruction tuning set. Thanks to these innovations, POINTS1.5\nsignificantly outperforms POINTS1.0 and demonstrates strong performance across\na range of real-world applications. Notably, POINTS1.5-7B is trained on fewer\nthan 4 billion tokens and ranks first on the OpenCompass leaderboard among\nmodels with fewer than 10 billion parameters\n", "link": "http://arxiv.org/abs/2412.08443v1", "date": "2024-12-11", "relevancy": 2.4635, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6291}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POINTS1.5%3A%20Building%20a%20Vision-Language%20Model%20towards%20Real%20World%0A%20%20Applications&body=Title%3A%20POINTS1.5%3A%20Building%20a%20Vision-Language%20Model%20towards%20Real%20World%0A%20%20Applications%0AAuthor%3A%20Yuan%20Liu%20and%20Le%20Tian%20and%20Xiao%20Zhou%20and%20Xinyu%20Gao%20and%20Kavio%20Yu%20and%20Yang%20Yu%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Vision-language%20models%20have%20made%20significant%20strides%20recently%2C%20demonstrating%0Asuperior%20performance%20across%20a%20range%20of%20tasks%2C%20e.g.%20optical%20character%0Arecognition%20and%20complex%20diagram%20analysis.%20Building%20on%20this%20trend%2C%20we%20introduce%0Aa%20new%20vision-language%20model%2C%20POINTS1.5%2C%20designed%20to%20excel%20in%20various%20real-world%0Aapplications.%20POINTS1.5%20is%20an%20enhancement%20of%20POINTS1.0%20and%20incorporates%20several%0Akey%20innovations%3A%20i%29%20We%20replace%20the%20original%20CLIP%20vision%20encoder%2C%20which%20had%20a%0Afixed%20image%20resolution%2C%20with%20a%20NaViT-style%20vision%20encoder%20that%20supports%20native%0Adynamic%20high%20resolution.%20This%20allows%20POINTS1.5%20to%20process%20images%20of%20any%0Aresolution%20without%20needing%20to%20split%20them%20into%20tiles.%20ii%29%20We%20add%20bilingual%0Asupport%20to%20POINTS1.5%2C%20significantly%20enhancing%20its%20capability%20in%20Chinese.%20Due%20to%0Athe%20scarcity%20of%20open-source%20Chinese%20datasets%20for%20vision-language%20models%2C%20we%0Acollect%20numerous%20images%20from%20the%20Internet%20and%20annotate%20them%20using%20a%20combination%0Aof%20manual%20and%20automatic%20methods.%20iii%29%20We%20propose%20a%20set%20of%20rigorous%20filtering%0Amethods%20for%20visual%20instruction%20tuning%20datasets.%20We%20comprehensively%20evaluate%20all%0Athese%20filtering%20methods%2C%20and%20choose%20the%20most%20effective%20ones%20to%20obtain%20the%20final%0Avisual%20instruction%20tuning%20set.%20Thanks%20to%20these%20innovations%2C%20POINTS1.5%0Asignificantly%20outperforms%20POINTS1.0%20and%20demonstrates%20strong%20performance%20across%0Aa%20range%20of%20real-world%20applications.%20Notably%2C%20POINTS1.5-7B%20is%20trained%20on%20fewer%0Athan%204%20billion%20tokens%20and%20ranks%20first%20on%20the%20OpenCompass%20leaderboard%20among%0Amodels%20with%20fewer%20than%2010%20billion%20parameters%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOINTS1.5%253A%2520Building%2520a%2520Vision-Language%2520Model%2520towards%2520Real%2520World%250A%2520%2520Applications%26entry.906535625%3DYuan%2520Liu%2520and%2520Le%2520Tian%2520and%2520Xiao%2520Zhou%2520and%2520Xinyu%2520Gao%2520and%2520Kavio%2520Yu%2520and%2520Yang%2520Yu%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520have%2520made%2520significant%2520strides%2520recently%252C%2520demonstrating%250Asuperior%2520performance%2520across%2520a%2520range%2520of%2520tasks%252C%2520e.g.%2520optical%2520character%250Arecognition%2520and%2520complex%2520diagram%2520analysis.%2520Building%2520on%2520this%2520trend%252C%2520we%2520introduce%250Aa%2520new%2520vision-language%2520model%252C%2520POINTS1.5%252C%2520designed%2520to%2520excel%2520in%2520various%2520real-world%250Aapplications.%2520POINTS1.5%2520is%2520an%2520enhancement%2520of%2520POINTS1.0%2520and%2520incorporates%2520several%250Akey%2520innovations%253A%2520i%2529%2520We%2520replace%2520the%2520original%2520CLIP%2520vision%2520encoder%252C%2520which%2520had%2520a%250Afixed%2520image%2520resolution%252C%2520with%2520a%2520NaViT-style%2520vision%2520encoder%2520that%2520supports%2520native%250Adynamic%2520high%2520resolution.%2520This%2520allows%2520POINTS1.5%2520to%2520process%2520images%2520of%2520any%250Aresolution%2520without%2520needing%2520to%2520split%2520them%2520into%2520tiles.%2520ii%2529%2520We%2520add%2520bilingual%250Asupport%2520to%2520POINTS1.5%252C%2520significantly%2520enhancing%2520its%2520capability%2520in%2520Chinese.%2520Due%2520to%250Athe%2520scarcity%2520of%2520open-source%2520Chinese%2520datasets%2520for%2520vision-language%2520models%252C%2520we%250Acollect%2520numerous%2520images%2520from%2520the%2520Internet%2520and%2520annotate%2520them%2520using%2520a%2520combination%250Aof%2520manual%2520and%2520automatic%2520methods.%2520iii%2529%2520We%2520propose%2520a%2520set%2520of%2520rigorous%2520filtering%250Amethods%2520for%2520visual%2520instruction%2520tuning%2520datasets.%2520We%2520comprehensively%2520evaluate%2520all%250Athese%2520filtering%2520methods%252C%2520and%2520choose%2520the%2520most%2520effective%2520ones%2520to%2520obtain%2520the%2520final%250Avisual%2520instruction%2520tuning%2520set.%2520Thanks%2520to%2520these%2520innovations%252C%2520POINTS1.5%250Asignificantly%2520outperforms%2520POINTS1.0%2520and%2520demonstrates%2520strong%2520performance%2520across%250Aa%2520range%2520of%2520real-world%2520applications.%2520Notably%252C%2520POINTS1.5-7B%2520is%2520trained%2520on%2520fewer%250Athan%25204%2520billion%2520tokens%2520and%2520ranks%2520first%2520on%2520the%2520OpenCompass%2520leaderboard%2520among%250Amodels%2520with%2520fewer%2520than%252010%2520billion%2520parameters%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POINTS1.5%3A%20Building%20a%20Vision-Language%20Model%20towards%20Real%20World%0A%20%20Applications&entry.906535625=Yuan%20Liu%20and%20Le%20Tian%20and%20Xiao%20Zhou%20and%20Xinyu%20Gao%20and%20Kavio%20Yu%20and%20Yang%20Yu%20and%20Jie%20Zhou&entry.1292438233=%20%20Vision-language%20models%20have%20made%20significant%20strides%20recently%2C%20demonstrating%0Asuperior%20performance%20across%20a%20range%20of%20tasks%2C%20e.g.%20optical%20character%0Arecognition%20and%20complex%20diagram%20analysis.%20Building%20on%20this%20trend%2C%20we%20introduce%0Aa%20new%20vision-language%20model%2C%20POINTS1.5%2C%20designed%20to%20excel%20in%20various%20real-world%0Aapplications.%20POINTS1.5%20is%20an%20enhancement%20of%20POINTS1.0%20and%20incorporates%20several%0Akey%20innovations%3A%20i%29%20We%20replace%20the%20original%20CLIP%20vision%20encoder%2C%20which%20had%20a%0Afixed%20image%20resolution%2C%20with%20a%20NaViT-style%20vision%20encoder%20that%20supports%20native%0Adynamic%20high%20resolution.%20This%20allows%20POINTS1.5%20to%20process%20images%20of%20any%0Aresolution%20without%20needing%20to%20split%20them%20into%20tiles.%20ii%29%20We%20add%20bilingual%0Asupport%20to%20POINTS1.5%2C%20significantly%20enhancing%20its%20capability%20in%20Chinese.%20Due%20to%0Athe%20scarcity%20of%20open-source%20Chinese%20datasets%20for%20vision-language%20models%2C%20we%0Acollect%20numerous%20images%20from%20the%20Internet%20and%20annotate%20them%20using%20a%20combination%0Aof%20manual%20and%20automatic%20methods.%20iii%29%20We%20propose%20a%20set%20of%20rigorous%20filtering%0Amethods%20for%20visual%20instruction%20tuning%20datasets.%20We%20comprehensively%20evaluate%20all%0Athese%20filtering%20methods%2C%20and%20choose%20the%20most%20effective%20ones%20to%20obtain%20the%20final%0Avisual%20instruction%20tuning%20set.%20Thanks%20to%20these%20innovations%2C%20POINTS1.5%0Asignificantly%20outperforms%20POINTS1.0%20and%20demonstrates%20strong%20performance%20across%0Aa%20range%20of%20real-world%20applications.%20Notably%2C%20POINTS1.5-7B%20is%20trained%20on%20fewer%0Athan%204%20billion%20tokens%20and%20ranks%20first%20on%20the%20OpenCompass%20leaderboard%20among%0Amodels%20with%20fewer%20than%2010%20billion%20parameters%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08443v1&entry.124074799=Read"},
{"title": "Synthetic Vision: Training Vision-Language Models to Understand Physics", "author": "Vahid Balazadeh and Mohammadmehdi Ataei and Hyunmin Cheong and Amir Hosein Khasahmadi and Rahul G. Krishnan", "abstract": "  Physical reasoning, which involves the interpretation, understanding, and\nprediction of object behavior in dynamic environments, remains a significant\nchallenge for current Vision-Language Models (VLMs). In this work, we propose\ntwo methods to enhance VLMs' physical reasoning capabilities using simulated\ndata. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs\ngenerated from simulations relevant to physical reasoning tasks. Second, we\nintroduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to\ncreate scene descriptions enriched with physical properties and processes.\nDuring physical reasoning tasks, these PCBs can be leveraged as context to\nassist a Large Language Model (LLM) to improve its performance. We evaluate\nboth of our approaches using multiple benchmarks, including a new stability\ndetection QA dataset called Falling Tower, which includes both simulated and\nreal-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM\ncan significantly outperform larger state-of-the-art foundational models. We\nalso show that integrating PCBs boosts the performance of foundational LLMs on\nphysical reasoning tasks. Using the real-world scenes from the Falling Tower\ndataset, we also validate the robustness of both approaches in Sim2Real\ntransfer. Our results highlight the utility that simulated data can have in the\ncreation of learning systems capable of advanced physical reasoning.\n", "link": "http://arxiv.org/abs/2412.08619v1", "date": "2024-12-11", "relevancy": 2.4605, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.616}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Vision%3A%20Training%20Vision-Language%20Models%20to%20Understand%20Physics&body=Title%3A%20Synthetic%20Vision%3A%20Training%20Vision-Language%20Models%20to%20Understand%20Physics%0AAuthor%3A%20Vahid%20Balazadeh%20and%20Mohammadmehdi%20Ataei%20and%20Hyunmin%20Cheong%20and%20Amir%20Hosein%20Khasahmadi%20and%20Rahul%20G.%20Krishnan%0AAbstract%3A%20%20%20Physical%20reasoning%2C%20which%20involves%20the%20interpretation%2C%20understanding%2C%20and%0Aprediction%20of%20object%20behavior%20in%20dynamic%20environments%2C%20remains%20a%20significant%0Achallenge%20for%20current%20Vision-Language%20Models%20%28VLMs%29.%20In%20this%20work%2C%20we%20propose%0Atwo%20methods%20to%20enhance%20VLMs%27%20physical%20reasoning%20capabilities%20using%20simulated%0Adata.%20First%2C%20we%20fine-tune%20a%20pre-trained%20VLM%20using%20question-answer%20%28QA%29%20pairs%0Agenerated%20from%20simulations%20relevant%20to%20physical%20reasoning%20tasks.%20Second%2C%20we%0Aintroduce%20Physics%20Context%20Builders%20%28PCBs%29%2C%20specialized%20VLMs%20fine-tuned%20to%0Acreate%20scene%20descriptions%20enriched%20with%20physical%20properties%20and%20processes.%0ADuring%20physical%20reasoning%20tasks%2C%20these%20PCBs%20can%20be%20leveraged%20as%20context%20to%0Aassist%20a%20Large%20Language%20Model%20%28LLM%29%20to%20improve%20its%20performance.%20We%20evaluate%0Aboth%20of%20our%20approaches%20using%20multiple%20benchmarks%2C%20including%20a%20new%20stability%0Adetection%20QA%20dataset%20called%20Falling%20Tower%2C%20which%20includes%20both%20simulated%20and%0Areal-world%20scenes%2C%20and%20CLEVRER.%20We%20demonstrate%20that%20a%20small%20QA%20fine-tuned%20VLM%0Acan%20significantly%20outperform%20larger%20state-of-the-art%20foundational%20models.%20We%0Aalso%20show%20that%20integrating%20PCBs%20boosts%20the%20performance%20of%20foundational%20LLMs%20on%0Aphysical%20reasoning%20tasks.%20Using%20the%20real-world%20scenes%20from%20the%20Falling%20Tower%0Adataset%2C%20we%20also%20validate%20the%20robustness%20of%20both%20approaches%20in%20Sim2Real%0Atransfer.%20Our%20results%20highlight%20the%20utility%20that%20simulated%20data%20can%20have%20in%20the%0Acreation%20of%20learning%20systems%20capable%20of%20advanced%20physical%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Vision%253A%2520Training%2520Vision-Language%2520Models%2520to%2520Understand%2520Physics%26entry.906535625%3DVahid%2520Balazadeh%2520and%2520Mohammadmehdi%2520Ataei%2520and%2520Hyunmin%2520Cheong%2520and%2520Amir%2520Hosein%2520Khasahmadi%2520and%2520Rahul%2520G.%2520Krishnan%26entry.1292438233%3D%2520%2520Physical%2520reasoning%252C%2520which%2520involves%2520the%2520interpretation%252C%2520understanding%252C%2520and%250Aprediction%2520of%2520object%2520behavior%2520in%2520dynamic%2520environments%252C%2520remains%2520a%2520significant%250Achallenge%2520for%2520current%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520In%2520this%2520work%252C%2520we%2520propose%250Atwo%2520methods%2520to%2520enhance%2520VLMs%2527%2520physical%2520reasoning%2520capabilities%2520using%2520simulated%250Adata.%2520First%252C%2520we%2520fine-tune%2520a%2520pre-trained%2520VLM%2520using%2520question-answer%2520%2528QA%2529%2520pairs%250Agenerated%2520from%2520simulations%2520relevant%2520to%2520physical%2520reasoning%2520tasks.%2520Second%252C%2520we%250Aintroduce%2520Physics%2520Context%2520Builders%2520%2528PCBs%2529%252C%2520specialized%2520VLMs%2520fine-tuned%2520to%250Acreate%2520scene%2520descriptions%2520enriched%2520with%2520physical%2520properties%2520and%2520processes.%250ADuring%2520physical%2520reasoning%2520tasks%252C%2520these%2520PCBs%2520can%2520be%2520leveraged%2520as%2520context%2520to%250Aassist%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520to%2520improve%2520its%2520performance.%2520We%2520evaluate%250Aboth%2520of%2520our%2520approaches%2520using%2520multiple%2520benchmarks%252C%2520including%2520a%2520new%2520stability%250Adetection%2520QA%2520dataset%2520called%2520Falling%2520Tower%252C%2520which%2520includes%2520both%2520simulated%2520and%250Areal-world%2520scenes%252C%2520and%2520CLEVRER.%2520We%2520demonstrate%2520that%2520a%2520small%2520QA%2520fine-tuned%2520VLM%250Acan%2520significantly%2520outperform%2520larger%2520state-of-the-art%2520foundational%2520models.%2520We%250Aalso%2520show%2520that%2520integrating%2520PCBs%2520boosts%2520the%2520performance%2520of%2520foundational%2520LLMs%2520on%250Aphysical%2520reasoning%2520tasks.%2520Using%2520the%2520real-world%2520scenes%2520from%2520the%2520Falling%2520Tower%250Adataset%252C%2520we%2520also%2520validate%2520the%2520robustness%2520of%2520both%2520approaches%2520in%2520Sim2Real%250Atransfer.%2520Our%2520results%2520highlight%2520the%2520utility%2520that%2520simulated%2520data%2520can%2520have%2520in%2520the%250Acreation%2520of%2520learning%2520systems%2520capable%2520of%2520advanced%2520physical%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Vision%3A%20Training%20Vision-Language%20Models%20to%20Understand%20Physics&entry.906535625=Vahid%20Balazadeh%20and%20Mohammadmehdi%20Ataei%20and%20Hyunmin%20Cheong%20and%20Amir%20Hosein%20Khasahmadi%20and%20Rahul%20G.%20Krishnan&entry.1292438233=%20%20Physical%20reasoning%2C%20which%20involves%20the%20interpretation%2C%20understanding%2C%20and%0Aprediction%20of%20object%20behavior%20in%20dynamic%20environments%2C%20remains%20a%20significant%0Achallenge%20for%20current%20Vision-Language%20Models%20%28VLMs%29.%20In%20this%20work%2C%20we%20propose%0Atwo%20methods%20to%20enhance%20VLMs%27%20physical%20reasoning%20capabilities%20using%20simulated%0Adata.%20First%2C%20we%20fine-tune%20a%20pre-trained%20VLM%20using%20question-answer%20%28QA%29%20pairs%0Agenerated%20from%20simulations%20relevant%20to%20physical%20reasoning%20tasks.%20Second%2C%20we%0Aintroduce%20Physics%20Context%20Builders%20%28PCBs%29%2C%20specialized%20VLMs%20fine-tuned%20to%0Acreate%20scene%20descriptions%20enriched%20with%20physical%20properties%20and%20processes.%0ADuring%20physical%20reasoning%20tasks%2C%20these%20PCBs%20can%20be%20leveraged%20as%20context%20to%0Aassist%20a%20Large%20Language%20Model%20%28LLM%29%20to%20improve%20its%20performance.%20We%20evaluate%0Aboth%20of%20our%20approaches%20using%20multiple%20benchmarks%2C%20including%20a%20new%20stability%0Adetection%20QA%20dataset%20called%20Falling%20Tower%2C%20which%20includes%20both%20simulated%20and%0Areal-world%20scenes%2C%20and%20CLEVRER.%20We%20demonstrate%20that%20a%20small%20QA%20fine-tuned%20VLM%0Acan%20significantly%20outperform%20larger%20state-of-the-art%20foundational%20models.%20We%0Aalso%20show%20that%20integrating%20PCBs%20boosts%20the%20performance%20of%20foundational%20LLMs%20on%0Aphysical%20reasoning%20tasks.%20Using%20the%20real-world%20scenes%20from%20the%20Falling%20Tower%0Adataset%2C%20we%20also%20validate%20the%20robustness%20of%20both%20approaches%20in%20Sim2Real%0Atransfer.%20Our%20results%20highlight%20the%20utility%20that%20simulated%20data%20can%20have%20in%20the%0Acreation%20of%20learning%20systems%20capable%20of%20advanced%20physical%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08619v1&entry.124074799=Read"},
{"title": "Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp\n  Generation", "author": "Haosheng Li and Weixin Mao and Weipeng Deng and Chenyu Meng and Haoqiang Fan and Tiancai Wang and Ping Tan and Hongan Wang and Xiaoming Deng", "abstract": "  Multi-hand semantic grasp generation aims to generate feasible and\nsemantically appropriate grasp poses for different robotic hands based on\nnatural language instructions. Although the task is highly valuable, due to the\nlack of multi-hand grasp datasets with fine-grained contact description between\nrobotic hands and objects, it is still a long-standing difficult task. In this\npaper, we present Multi-GraspSet, the first large-scale multi-hand grasp\ndataset with automatically contact annotations. Based on Multi-GraspSet, we\npropose Multi-GraspLLM, a unified language-guided grasp generation framework.\nIt leverages large language models (LLM) to handle variable-length sequences,\ngenerating grasp poses for diverse robotic hands in a single unified\narchitecture. Multi-GraspLLM first aligns the encoded point cloud features and\ntext features into a unified semantic space. It then generates grasp bin tokens\nwhich are subsequently converted into grasp pose for each robotic hand via\nhand-aware linear mapping. The experimental results demonstrate that our\napproach significantly outperforms existing methods on Multi-GraspSet. More\ninformation can be found on our project page https://multi-graspllm.github.io.\n", "link": "http://arxiv.org/abs/2412.08468v1", "date": "2024-12-11", "relevancy": 2.4395, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6713}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-GraspLLM%3A%20A%20Multimodal%20LLM%20for%20Multi-Hand%20Semantic%20Guided%20Grasp%0A%20%20Generation&body=Title%3A%20Multi-GraspLLM%3A%20A%20Multimodal%20LLM%20for%20Multi-Hand%20Semantic%20Guided%20Grasp%0A%20%20Generation%0AAuthor%3A%20Haosheng%20Li%20and%20Weixin%20Mao%20and%20Weipeng%20Deng%20and%20Chenyu%20Meng%20and%20Haoqiang%20Fan%20and%20Tiancai%20Wang%20and%20Ping%20Tan%20and%20Hongan%20Wang%20and%20Xiaoming%20Deng%0AAbstract%3A%20%20%20Multi-hand%20semantic%20grasp%20generation%20aims%20to%20generate%20feasible%20and%0Asemantically%20appropriate%20grasp%20poses%20for%20different%20robotic%20hands%20based%20on%0Anatural%20language%20instructions.%20Although%20the%20task%20is%20highly%20valuable%2C%20due%20to%20the%0Alack%20of%20multi-hand%20grasp%20datasets%20with%20fine-grained%20contact%20description%20between%0Arobotic%20hands%20and%20objects%2C%20it%20is%20still%20a%20long-standing%20difficult%20task.%20In%20this%0Apaper%2C%20we%20present%20Multi-GraspSet%2C%20the%20first%20large-scale%20multi-hand%20grasp%0Adataset%20with%20automatically%20contact%20annotations.%20Based%20on%20Multi-GraspSet%2C%20we%0Apropose%20Multi-GraspLLM%2C%20a%20unified%20language-guided%20grasp%20generation%20framework.%0AIt%20leverages%20large%20language%20models%20%28LLM%29%20to%20handle%20variable-length%20sequences%2C%0Agenerating%20grasp%20poses%20for%20diverse%20robotic%20hands%20in%20a%20single%20unified%0Aarchitecture.%20Multi-GraspLLM%20first%20aligns%20the%20encoded%20point%20cloud%20features%20and%0Atext%20features%20into%20a%20unified%20semantic%20space.%20It%20then%20generates%20grasp%20bin%20tokens%0Awhich%20are%20subsequently%20converted%20into%20grasp%20pose%20for%20each%20robotic%20hand%20via%0Ahand-aware%20linear%20mapping.%20The%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20significantly%20outperforms%20existing%20methods%20on%20Multi-GraspSet.%20More%0Ainformation%20can%20be%20found%20on%20our%20project%20page%20https%3A//multi-graspllm.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-GraspLLM%253A%2520A%2520Multimodal%2520LLM%2520for%2520Multi-Hand%2520Semantic%2520Guided%2520Grasp%250A%2520%2520Generation%26entry.906535625%3DHaosheng%2520Li%2520and%2520Weixin%2520Mao%2520and%2520Weipeng%2520Deng%2520and%2520Chenyu%2520Meng%2520and%2520Haoqiang%2520Fan%2520and%2520Tiancai%2520Wang%2520and%2520Ping%2520Tan%2520and%2520Hongan%2520Wang%2520and%2520Xiaoming%2520Deng%26entry.1292438233%3D%2520%2520Multi-hand%2520semantic%2520grasp%2520generation%2520aims%2520to%2520generate%2520feasible%2520and%250Asemantically%2520appropriate%2520grasp%2520poses%2520for%2520different%2520robotic%2520hands%2520based%2520on%250Anatural%2520language%2520instructions.%2520Although%2520the%2520task%2520is%2520highly%2520valuable%252C%2520due%2520to%2520the%250Alack%2520of%2520multi-hand%2520grasp%2520datasets%2520with%2520fine-grained%2520contact%2520description%2520between%250Arobotic%2520hands%2520and%2520objects%252C%2520it%2520is%2520still%2520a%2520long-standing%2520difficult%2520task.%2520In%2520this%250Apaper%252C%2520we%2520present%2520Multi-GraspSet%252C%2520the%2520first%2520large-scale%2520multi-hand%2520grasp%250Adataset%2520with%2520automatically%2520contact%2520annotations.%2520Based%2520on%2520Multi-GraspSet%252C%2520we%250Apropose%2520Multi-GraspLLM%252C%2520a%2520unified%2520language-guided%2520grasp%2520generation%2520framework.%250AIt%2520leverages%2520large%2520language%2520models%2520%2528LLM%2529%2520to%2520handle%2520variable-length%2520sequences%252C%250Agenerating%2520grasp%2520poses%2520for%2520diverse%2520robotic%2520hands%2520in%2520a%2520single%2520unified%250Aarchitecture.%2520Multi-GraspLLM%2520first%2520aligns%2520the%2520encoded%2520point%2520cloud%2520features%2520and%250Atext%2520features%2520into%2520a%2520unified%2520semantic%2520space.%2520It%2520then%2520generates%2520grasp%2520bin%2520tokens%250Awhich%2520are%2520subsequently%2520converted%2520into%2520grasp%2520pose%2520for%2520each%2520robotic%2520hand%2520via%250Ahand-aware%2520linear%2520mapping.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%250Aapproach%2520significantly%2520outperforms%2520existing%2520methods%2520on%2520Multi-GraspSet.%2520More%250Ainformation%2520can%2520be%2520found%2520on%2520our%2520project%2520page%2520https%253A//multi-graspllm.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-GraspLLM%3A%20A%20Multimodal%20LLM%20for%20Multi-Hand%20Semantic%20Guided%20Grasp%0A%20%20Generation&entry.906535625=Haosheng%20Li%20and%20Weixin%20Mao%20and%20Weipeng%20Deng%20and%20Chenyu%20Meng%20and%20Haoqiang%20Fan%20and%20Tiancai%20Wang%20and%20Ping%20Tan%20and%20Hongan%20Wang%20and%20Xiaoming%20Deng&entry.1292438233=%20%20Multi-hand%20semantic%20grasp%20generation%20aims%20to%20generate%20feasible%20and%0Asemantically%20appropriate%20grasp%20poses%20for%20different%20robotic%20hands%20based%20on%0Anatural%20language%20instructions.%20Although%20the%20task%20is%20highly%20valuable%2C%20due%20to%20the%0Alack%20of%20multi-hand%20grasp%20datasets%20with%20fine-grained%20contact%20description%20between%0Arobotic%20hands%20and%20objects%2C%20it%20is%20still%20a%20long-standing%20difficult%20task.%20In%20this%0Apaper%2C%20we%20present%20Multi-GraspSet%2C%20the%20first%20large-scale%20multi-hand%20grasp%0Adataset%20with%20automatically%20contact%20annotations.%20Based%20on%20Multi-GraspSet%2C%20we%0Apropose%20Multi-GraspLLM%2C%20a%20unified%20language-guided%20grasp%20generation%20framework.%0AIt%20leverages%20large%20language%20models%20%28LLM%29%20to%20handle%20variable-length%20sequences%2C%0Agenerating%20grasp%20poses%20for%20diverse%20robotic%20hands%20in%20a%20single%20unified%0Aarchitecture.%20Multi-GraspLLM%20first%20aligns%20the%20encoded%20point%20cloud%20features%20and%0Atext%20features%20into%20a%20unified%20semantic%20space.%20It%20then%20generates%20grasp%20bin%20tokens%0Awhich%20are%20subsequently%20converted%20into%20grasp%20pose%20for%20each%20robotic%20hand%20via%0Ahand-aware%20linear%20mapping.%20The%20experimental%20results%20demonstrate%20that%20our%0Aapproach%20significantly%20outperforms%20existing%20methods%20on%20Multi-GraspSet.%20More%0Ainformation%20can%20be%20found%20on%20our%20project%20page%20https%3A//multi-graspllm.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08468v1&entry.124074799=Read"},
{"title": "Pysical Informed Driving World Model", "author": "Zhuoran Yang and Xi Guo and Chenjing Ding and Chiyu Wang and Wei Wu", "abstract": "  Autonomous driving requires robust perception models trained on high-quality,\nlarge-scale multi-view driving videos for tasks like 3D object detection,\nsegmentation and trajectory prediction. While world models provide a\ncost-effective solution for generating realistic driving videos, challenges\nremain in ensuring these videos adhere to fundamental physical principles, such\nas relative and absolute motion, spatial relationship like occlusion and\nspatial consistency, and temporal consistency. To address these, we propose\nDrivePhysica, an innovative model designed to generate realistic multi-view\ndriving videos that accurately adhere to essential physical principles through\nthree key advancements: (1) a Coordinate System Aligner module that integrates\nrelative and absolute motion features to enhance motion interpretation, (2) an\nInstance Flow Guidance module that ensures precise temporal consistency via\nefficient 3D flow extraction, and (3) a Box Coordinate Guidance module that\nimproves spatial relationship understanding and accurately resolves occlusion\nhierarchies. Grounded in physical principles, we achieve state-of-the-art\nperformance in driving video generation quality (3.96 FID and 38.06 FVD on the\nNuscenes dataset) and downstream perception tasks. Our project homepage:\nhttps://metadrivescape.github.io/papers_project/DrivePhysica/page.html\n", "link": "http://arxiv.org/abs/2412.08410v1", "date": "2024-12-11", "relevancy": 2.4271, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.61}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pysical%20Informed%20Driving%20World%20Model&body=Title%3A%20Pysical%20Informed%20Driving%20World%20Model%0AAuthor%3A%20Zhuoran%20Yang%20and%20Xi%20Guo%20and%20Chenjing%20Ding%20and%20Chiyu%20Wang%20and%20Wei%20Wu%0AAbstract%3A%20%20%20Autonomous%20driving%20requires%20robust%20perception%20models%20trained%20on%20high-quality%2C%0Alarge-scale%20multi-view%20driving%20videos%20for%20tasks%20like%203D%20object%20detection%2C%0Asegmentation%20and%20trajectory%20prediction.%20While%20world%20models%20provide%20a%0Acost-effective%20solution%20for%20generating%20realistic%20driving%20videos%2C%20challenges%0Aremain%20in%20ensuring%20these%20videos%20adhere%20to%20fundamental%20physical%20principles%2C%20such%0Aas%20relative%20and%20absolute%20motion%2C%20spatial%20relationship%20like%20occlusion%20and%0Aspatial%20consistency%2C%20and%20temporal%20consistency.%20To%20address%20these%2C%20we%20propose%0ADrivePhysica%2C%20an%20innovative%20model%20designed%20to%20generate%20realistic%20multi-view%0Adriving%20videos%20that%20accurately%20adhere%20to%20essential%20physical%20principles%20through%0Athree%20key%20advancements%3A%20%281%29%20a%20Coordinate%20System%20Aligner%20module%20that%20integrates%0Arelative%20and%20absolute%20motion%20features%20to%20enhance%20motion%20interpretation%2C%20%282%29%20an%0AInstance%20Flow%20Guidance%20module%20that%20ensures%20precise%20temporal%20consistency%20via%0Aefficient%203D%20flow%20extraction%2C%20and%20%283%29%20a%20Box%20Coordinate%20Guidance%20module%20that%0Aimproves%20spatial%20relationship%20understanding%20and%20accurately%20resolves%20occlusion%0Ahierarchies.%20Grounded%20in%20physical%20principles%2C%20we%20achieve%20state-of-the-art%0Aperformance%20in%20driving%20video%20generation%20quality%20%283.96%20FID%20and%2038.06%20FVD%20on%20the%0ANuscenes%20dataset%29%20and%20downstream%20perception%20tasks.%20Our%20project%20homepage%3A%0Ahttps%3A//metadrivescape.github.io/papers_project/DrivePhysica/page.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPysical%2520Informed%2520Driving%2520World%2520Model%26entry.906535625%3DZhuoran%2520Yang%2520and%2520Xi%2520Guo%2520and%2520Chenjing%2520Ding%2520and%2520Chiyu%2520Wang%2520and%2520Wei%2520Wu%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520requires%2520robust%2520perception%2520models%2520trained%2520on%2520high-quality%252C%250Alarge-scale%2520multi-view%2520driving%2520videos%2520for%2520tasks%2520like%25203D%2520object%2520detection%252C%250Asegmentation%2520and%2520trajectory%2520prediction.%2520While%2520world%2520models%2520provide%2520a%250Acost-effective%2520solution%2520for%2520generating%2520realistic%2520driving%2520videos%252C%2520challenges%250Aremain%2520in%2520ensuring%2520these%2520videos%2520adhere%2520to%2520fundamental%2520physical%2520principles%252C%2520such%250Aas%2520relative%2520and%2520absolute%2520motion%252C%2520spatial%2520relationship%2520like%2520occlusion%2520and%250Aspatial%2520consistency%252C%2520and%2520temporal%2520consistency.%2520To%2520address%2520these%252C%2520we%2520propose%250ADrivePhysica%252C%2520an%2520innovative%2520model%2520designed%2520to%2520generate%2520realistic%2520multi-view%250Adriving%2520videos%2520that%2520accurately%2520adhere%2520to%2520essential%2520physical%2520principles%2520through%250Athree%2520key%2520advancements%253A%2520%25281%2529%2520a%2520Coordinate%2520System%2520Aligner%2520module%2520that%2520integrates%250Arelative%2520and%2520absolute%2520motion%2520features%2520to%2520enhance%2520motion%2520interpretation%252C%2520%25282%2529%2520an%250AInstance%2520Flow%2520Guidance%2520module%2520that%2520ensures%2520precise%2520temporal%2520consistency%2520via%250Aefficient%25203D%2520flow%2520extraction%252C%2520and%2520%25283%2529%2520a%2520Box%2520Coordinate%2520Guidance%2520module%2520that%250Aimproves%2520spatial%2520relationship%2520understanding%2520and%2520accurately%2520resolves%2520occlusion%250Ahierarchies.%2520Grounded%2520in%2520physical%2520principles%252C%2520we%2520achieve%2520state-of-the-art%250Aperformance%2520in%2520driving%2520video%2520generation%2520quality%2520%25283.96%2520FID%2520and%252038.06%2520FVD%2520on%2520the%250ANuscenes%2520dataset%2529%2520and%2520downstream%2520perception%2520tasks.%2520Our%2520project%2520homepage%253A%250Ahttps%253A//metadrivescape.github.io/papers_project/DrivePhysica/page.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pysical%20Informed%20Driving%20World%20Model&entry.906535625=Zhuoran%20Yang%20and%20Xi%20Guo%20and%20Chenjing%20Ding%20and%20Chiyu%20Wang%20and%20Wei%20Wu&entry.1292438233=%20%20Autonomous%20driving%20requires%20robust%20perception%20models%20trained%20on%20high-quality%2C%0Alarge-scale%20multi-view%20driving%20videos%20for%20tasks%20like%203D%20object%20detection%2C%0Asegmentation%20and%20trajectory%20prediction.%20While%20world%20models%20provide%20a%0Acost-effective%20solution%20for%20generating%20realistic%20driving%20videos%2C%20challenges%0Aremain%20in%20ensuring%20these%20videos%20adhere%20to%20fundamental%20physical%20principles%2C%20such%0Aas%20relative%20and%20absolute%20motion%2C%20spatial%20relationship%20like%20occlusion%20and%0Aspatial%20consistency%2C%20and%20temporal%20consistency.%20To%20address%20these%2C%20we%20propose%0ADrivePhysica%2C%20an%20innovative%20model%20designed%20to%20generate%20realistic%20multi-view%0Adriving%20videos%20that%20accurately%20adhere%20to%20essential%20physical%20principles%20through%0Athree%20key%20advancements%3A%20%281%29%20a%20Coordinate%20System%20Aligner%20module%20that%20integrates%0Arelative%20and%20absolute%20motion%20features%20to%20enhance%20motion%20interpretation%2C%20%282%29%20an%0AInstance%20Flow%20Guidance%20module%20that%20ensures%20precise%20temporal%20consistency%20via%0Aefficient%203D%20flow%20extraction%2C%20and%20%283%29%20a%20Box%20Coordinate%20Guidance%20module%20that%0Aimproves%20spatial%20relationship%20understanding%20and%20accurately%20resolves%20occlusion%0Ahierarchies.%20Grounded%20in%20physical%20principles%2C%20we%20achieve%20state-of-the-art%0Aperformance%20in%20driving%20video%20generation%20quality%20%283.96%20FID%20and%2038.06%20FVD%20on%20the%0ANuscenes%20dataset%29%20and%20downstream%20perception%20tasks.%20Our%20project%20homepage%3A%0Ahttps%3A//metadrivescape.github.io/papers_project/DrivePhysica/page.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08410v1&entry.124074799=Read"},
{"title": "Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks\n  Defending against Poisoning Attacks", "author": "Ao Liu and Wenshan Li and Beibei Li and Wengang Ma and Tao Li and Pan Zhou", "abstract": "  End-to-end training with global optimization have popularized graph neural\nnetworks (GNNs) for node classification, yet inadvertently introduced\nvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit\nthe inherent opened interfaces of GNNs' input and output, perturbing critical\nedges and thus manipulating the classification results. Current defenses, due\nto their persistent utilization of global-optimization-based end-to-end\ntraining schemes, inherently encapsulate the vulnerabilities of GNNs. This is\nspecifically evidenced in their inability to defend against targeted secondary\nattacks. In this paper, we propose the Graph Agent Network (GAgN) to address\nthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent\nnetwork in which each node is designed as an 1-hop-view agent. Through the\ndecentralized interactions between agents, they can learn to infer global\nperceptions to perform tasks including inferring embeddings, degrees and\nneighbor relationships for given nodes. This empowers nodes to filtering\nadversarial edges while carrying out classification tasks. Furthermore, agents'\nlimited view prevents malicious messages from propagating globally in GAgN,\nthereby resisting global-optimization-based secondary attacks. We prove that\nsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient\nto achieve these functionalities. Experimental results show that GAgN\neffectively implements all its intended capabilities and, compared to\nstate-of-the-art defenses, achieves optimal classification accuracy on the\nperturbed datasets.\n", "link": "http://arxiv.org/abs/2412.08555v1", "date": "2024-12-11", "relevancy": 2.4054, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.499}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4796}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grimm%3A%20A%20Plug-and-Play%20Perturbation%20Rectifier%20for%20Graph%20Neural%20Networks%0A%20%20Defending%20against%20Poisoning%20Attacks&body=Title%3A%20Grimm%3A%20A%20Plug-and-Play%20Perturbation%20Rectifier%20for%20Graph%20Neural%20Networks%0A%20%20Defending%20against%20Poisoning%20Attacks%0AAuthor%3A%20Ao%20Liu%20and%20Wenshan%20Li%20and%20Beibei%20Li%20and%20Wengang%20Ma%20and%20Tao%20Li%20and%20Pan%20Zhou%0AAbstract%3A%20%20%20End-to-end%20training%20with%20global%20optimization%20have%20popularized%20graph%20neural%0Anetworks%20%28GNNs%29%20for%20node%20classification%2C%20yet%20inadvertently%20introduced%0Avulnerabilities%20to%20adversarial%20edge-perturbing%20attacks.%20Adversaries%20can%20exploit%0Athe%20inherent%20opened%20interfaces%20of%20GNNs%27%20input%20and%20output%2C%20perturbing%20critical%0Aedges%20and%20thus%20manipulating%20the%20classification%20results.%20Current%20defenses%2C%20due%0Ato%20their%20persistent%20utilization%20of%20global-optimization-based%20end-to-end%0Atraining%20schemes%2C%20inherently%20encapsulate%20the%20vulnerabilities%20of%20GNNs.%20This%20is%0Aspecifically%20evidenced%20in%20their%20inability%20to%20defend%20against%20targeted%20secondary%0Aattacks.%20In%20this%20paper%2C%20we%20propose%20the%20Graph%20Agent%20Network%20%28GAgN%29%20to%20address%0Athe%20aforementioned%20vulnerabilities%20of%20GNNs.%20GAgN%20is%20a%20graph-structured%20agent%0Anetwork%20in%20which%20each%20node%20is%20designed%20as%20an%201-hop-view%20agent.%20Through%20the%0Adecentralized%20interactions%20between%20agents%2C%20they%20can%20learn%20to%20infer%20global%0Aperceptions%20to%20perform%20tasks%20including%20inferring%20embeddings%2C%20degrees%20and%0Aneighbor%20relationships%20for%20given%20nodes.%20This%20empowers%20nodes%20to%20filtering%0Aadversarial%20edges%20while%20carrying%20out%20classification%20tasks.%20Furthermore%2C%20agents%27%0Alimited%20view%20prevents%20malicious%20messages%20from%20propagating%20globally%20in%20GAgN%2C%0Athereby%20resisting%20global-optimization-based%20secondary%20attacks.%20We%20prove%20that%0Asingle-hidden-layer%20multilayer%20perceptrons%20%28MLPs%29%20are%20theoretically%20sufficient%0Ato%20achieve%20these%20functionalities.%20Experimental%20results%20show%20that%20GAgN%0Aeffectively%20implements%20all%20its%20intended%20capabilities%20and%2C%20compared%20to%0Astate-of-the-art%20defenses%2C%20achieves%20optimal%20classification%20accuracy%20on%20the%0Aperturbed%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrimm%253A%2520A%2520Plug-and-Play%2520Perturbation%2520Rectifier%2520for%2520Graph%2520Neural%2520Networks%250A%2520%2520Defending%2520against%2520Poisoning%2520Attacks%26entry.906535625%3DAo%2520Liu%2520and%2520Wenshan%2520Li%2520and%2520Beibei%2520Li%2520and%2520Wengang%2520Ma%2520and%2520Tao%2520Li%2520and%2520Pan%2520Zhou%26entry.1292438233%3D%2520%2520End-to-end%2520training%2520with%2520global%2520optimization%2520have%2520popularized%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520for%2520node%2520classification%252C%2520yet%2520inadvertently%2520introduced%250Avulnerabilities%2520to%2520adversarial%2520edge-perturbing%2520attacks.%2520Adversaries%2520can%2520exploit%250Athe%2520inherent%2520opened%2520interfaces%2520of%2520GNNs%2527%2520input%2520and%2520output%252C%2520perturbing%2520critical%250Aedges%2520and%2520thus%2520manipulating%2520the%2520classification%2520results.%2520Current%2520defenses%252C%2520due%250Ato%2520their%2520persistent%2520utilization%2520of%2520global-optimization-based%2520end-to-end%250Atraining%2520schemes%252C%2520inherently%2520encapsulate%2520the%2520vulnerabilities%2520of%2520GNNs.%2520This%2520is%250Aspecifically%2520evidenced%2520in%2520their%2520inability%2520to%2520defend%2520against%2520targeted%2520secondary%250Aattacks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Graph%2520Agent%2520Network%2520%2528GAgN%2529%2520to%2520address%250Athe%2520aforementioned%2520vulnerabilities%2520of%2520GNNs.%2520GAgN%2520is%2520a%2520graph-structured%2520agent%250Anetwork%2520in%2520which%2520each%2520node%2520is%2520designed%2520as%2520an%25201-hop-view%2520agent.%2520Through%2520the%250Adecentralized%2520interactions%2520between%2520agents%252C%2520they%2520can%2520learn%2520to%2520infer%2520global%250Aperceptions%2520to%2520perform%2520tasks%2520including%2520inferring%2520embeddings%252C%2520degrees%2520and%250Aneighbor%2520relationships%2520for%2520given%2520nodes.%2520This%2520empowers%2520nodes%2520to%2520filtering%250Aadversarial%2520edges%2520while%2520carrying%2520out%2520classification%2520tasks.%2520Furthermore%252C%2520agents%2527%250Alimited%2520view%2520prevents%2520malicious%2520messages%2520from%2520propagating%2520globally%2520in%2520GAgN%252C%250Athereby%2520resisting%2520global-optimization-based%2520secondary%2520attacks.%2520We%2520prove%2520that%250Asingle-hidden-layer%2520multilayer%2520perceptrons%2520%2528MLPs%2529%2520are%2520theoretically%2520sufficient%250Ato%2520achieve%2520these%2520functionalities.%2520Experimental%2520results%2520show%2520that%2520GAgN%250Aeffectively%2520implements%2520all%2520its%2520intended%2520capabilities%2520and%252C%2520compared%2520to%250Astate-of-the-art%2520defenses%252C%2520achieves%2520optimal%2520classification%2520accuracy%2520on%2520the%250Aperturbed%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grimm%3A%20A%20Plug-and-Play%20Perturbation%20Rectifier%20for%20Graph%20Neural%20Networks%0A%20%20Defending%20against%20Poisoning%20Attacks&entry.906535625=Ao%20Liu%20and%20Wenshan%20Li%20and%20Beibei%20Li%20and%20Wengang%20Ma%20and%20Tao%20Li%20and%20Pan%20Zhou&entry.1292438233=%20%20End-to-end%20training%20with%20global%20optimization%20have%20popularized%20graph%20neural%0Anetworks%20%28GNNs%29%20for%20node%20classification%2C%20yet%20inadvertently%20introduced%0Avulnerabilities%20to%20adversarial%20edge-perturbing%20attacks.%20Adversaries%20can%20exploit%0Athe%20inherent%20opened%20interfaces%20of%20GNNs%27%20input%20and%20output%2C%20perturbing%20critical%0Aedges%20and%20thus%20manipulating%20the%20classification%20results.%20Current%20defenses%2C%20due%0Ato%20their%20persistent%20utilization%20of%20global-optimization-based%20end-to-end%0Atraining%20schemes%2C%20inherently%20encapsulate%20the%20vulnerabilities%20of%20GNNs.%20This%20is%0Aspecifically%20evidenced%20in%20their%20inability%20to%20defend%20against%20targeted%20secondary%0Aattacks.%20In%20this%20paper%2C%20we%20propose%20the%20Graph%20Agent%20Network%20%28GAgN%29%20to%20address%0Athe%20aforementioned%20vulnerabilities%20of%20GNNs.%20GAgN%20is%20a%20graph-structured%20agent%0Anetwork%20in%20which%20each%20node%20is%20designed%20as%20an%201-hop-view%20agent.%20Through%20the%0Adecentralized%20interactions%20between%20agents%2C%20they%20can%20learn%20to%20infer%20global%0Aperceptions%20to%20perform%20tasks%20including%20inferring%20embeddings%2C%20degrees%20and%0Aneighbor%20relationships%20for%20given%20nodes.%20This%20empowers%20nodes%20to%20filtering%0Aadversarial%20edges%20while%20carrying%20out%20classification%20tasks.%20Furthermore%2C%20agents%27%0Alimited%20view%20prevents%20malicious%20messages%20from%20propagating%20globally%20in%20GAgN%2C%0Athereby%20resisting%20global-optimization-based%20secondary%20attacks.%20We%20prove%20that%0Asingle-hidden-layer%20multilayer%20perceptrons%20%28MLPs%29%20are%20theoretically%20sufficient%0Ato%20achieve%20these%20functionalities.%20Experimental%20results%20show%20that%20GAgN%0Aeffectively%20implements%20all%20its%20intended%20capabilities%20and%2C%20compared%20to%0Astate-of-the-art%20defenses%2C%20achieves%20optimal%20classification%20accuracy%20on%20the%0Aperturbed%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08555v1&entry.124074799=Read"},
{"title": "Edge-Splitting MLP: Node Classification on Homophilic and Heterophilic\n  Graphs without Message Passing", "author": "Matthias Kohn and Marcel Hoffmann and Ansgar Scherp", "abstract": "  Message Passing Neural Networks (MPNNs) have demonstrated remarkable success\nin node classification on homophilic graphs. It has been shown that they do not\nsolely rely on homophily but on neighborhood distributions of nodes, i.e.,\nconsistency of the neighborhood label distribution within the same class.\nMLP-based models do not use message passing, \\eg Graph-MLP incorporates the\nneighborhood in a separate loss function. These models are faster and more\nrobust to edge noise. Graph-MLP maps adjacent nodes closer in the embedding\nspace but is unaware of the neighborhood pattern of the labels, i.e., relies\nsolely on homophily. Edge Splitting GNN (ES-GNN) is a model specialized for\nheterophilic graphs and splits the edges into task-relevant and\ntask-irrelevant, respectively. To mitigate the limitations of Graph-MLP on\nheterophilic graphs, we propose ES-MLP that combines Graph-MLP with an\nedge-splitting mechanism from ES-GNN. It incorporates the edge splitting into\nthe loss of Graph-MLP to learn two separate adjacency matrices based on\nrelevant and irrelevant feature pairs. Our experiments on seven datasets with\nsix baselines show that ES-MLP is on par with homophilic and heterophilic\nmodels on all datasets without using edges during inference. We show that\nES-MLP is robust to multiple types of edge noise during inference and that its\ninference time is two to five times faster than that of commonly used MPNNs.\nThe source code is available at https://github.com/MatthiasKohn/ES-MLP.\n", "link": "http://arxiv.org/abs/2412.08310v1", "date": "2024-12-11", "relevancy": 2.3934, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5309}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4611}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-Splitting%20MLP%3A%20Node%20Classification%20on%20Homophilic%20and%20Heterophilic%0A%20%20Graphs%20without%20Message%20Passing&body=Title%3A%20Edge-Splitting%20MLP%3A%20Node%20Classification%20on%20Homophilic%20and%20Heterophilic%0A%20%20Graphs%20without%20Message%20Passing%0AAuthor%3A%20Matthias%20Kohn%20and%20Marcel%20Hoffmann%20and%20Ansgar%20Scherp%0AAbstract%3A%20%20%20Message%20Passing%20Neural%20Networks%20%28MPNNs%29%20have%20demonstrated%20remarkable%20success%0Ain%20node%20classification%20on%20homophilic%20graphs.%20It%20has%20been%20shown%20that%20they%20do%20not%0Asolely%20rely%20on%20homophily%20but%20on%20neighborhood%20distributions%20of%20nodes%2C%20i.e.%2C%0Aconsistency%20of%20the%20neighborhood%20label%20distribution%20within%20the%20same%20class.%0AMLP-based%20models%20do%20not%20use%20message%20passing%2C%20%5Ceg%20Graph-MLP%20incorporates%20the%0Aneighborhood%20in%20a%20separate%20loss%20function.%20These%20models%20are%20faster%20and%20more%0Arobust%20to%20edge%20noise.%20Graph-MLP%20maps%20adjacent%20nodes%20closer%20in%20the%20embedding%0Aspace%20but%20is%20unaware%20of%20the%20neighborhood%20pattern%20of%20the%20labels%2C%20i.e.%2C%20relies%0Asolely%20on%20homophily.%20Edge%20Splitting%20GNN%20%28ES-GNN%29%20is%20a%20model%20specialized%20for%0Aheterophilic%20graphs%20and%20splits%20the%20edges%20into%20task-relevant%20and%0Atask-irrelevant%2C%20respectively.%20To%20mitigate%20the%20limitations%20of%20Graph-MLP%20on%0Aheterophilic%20graphs%2C%20we%20propose%20ES-MLP%20that%20combines%20Graph-MLP%20with%20an%0Aedge-splitting%20mechanism%20from%20ES-GNN.%20It%20incorporates%20the%20edge%20splitting%20into%0Athe%20loss%20of%20Graph-MLP%20to%20learn%20two%20separate%20adjacency%20matrices%20based%20on%0Arelevant%20and%20irrelevant%20feature%20pairs.%20Our%20experiments%20on%20seven%20datasets%20with%0Asix%20baselines%20show%20that%20ES-MLP%20is%20on%20par%20with%20homophilic%20and%20heterophilic%0Amodels%20on%20all%20datasets%20without%20using%20edges%20during%20inference.%20We%20show%20that%0AES-MLP%20is%20robust%20to%20multiple%20types%20of%20edge%20noise%20during%20inference%20and%20that%20its%0Ainference%20time%20is%20two%20to%20five%20times%20faster%20than%20that%20of%20commonly%20used%20MPNNs.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/MatthiasKohn/ES-MLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-Splitting%2520MLP%253A%2520Node%2520Classification%2520on%2520Homophilic%2520and%2520Heterophilic%250A%2520%2520Graphs%2520without%2520Message%2520Passing%26entry.906535625%3DMatthias%2520Kohn%2520and%2520Marcel%2520Hoffmann%2520and%2520Ansgar%2520Scherp%26entry.1292438233%3D%2520%2520Message%2520Passing%2520Neural%2520Networks%2520%2528MPNNs%2529%2520have%2520demonstrated%2520remarkable%2520success%250Ain%2520node%2520classification%2520on%2520homophilic%2520graphs.%2520It%2520has%2520been%2520shown%2520that%2520they%2520do%2520not%250Asolely%2520rely%2520on%2520homophily%2520but%2520on%2520neighborhood%2520distributions%2520of%2520nodes%252C%2520i.e.%252C%250Aconsistency%2520of%2520the%2520neighborhood%2520label%2520distribution%2520within%2520the%2520same%2520class.%250AMLP-based%2520models%2520do%2520not%2520use%2520message%2520passing%252C%2520%255Ceg%2520Graph-MLP%2520incorporates%2520the%250Aneighborhood%2520in%2520a%2520separate%2520loss%2520function.%2520These%2520models%2520are%2520faster%2520and%2520more%250Arobust%2520to%2520edge%2520noise.%2520Graph-MLP%2520maps%2520adjacent%2520nodes%2520closer%2520in%2520the%2520embedding%250Aspace%2520but%2520is%2520unaware%2520of%2520the%2520neighborhood%2520pattern%2520of%2520the%2520labels%252C%2520i.e.%252C%2520relies%250Asolely%2520on%2520homophily.%2520Edge%2520Splitting%2520GNN%2520%2528ES-GNN%2529%2520is%2520a%2520model%2520specialized%2520for%250Aheterophilic%2520graphs%2520and%2520splits%2520the%2520edges%2520into%2520task-relevant%2520and%250Atask-irrelevant%252C%2520respectively.%2520To%2520mitigate%2520the%2520limitations%2520of%2520Graph-MLP%2520on%250Aheterophilic%2520graphs%252C%2520we%2520propose%2520ES-MLP%2520that%2520combines%2520Graph-MLP%2520with%2520an%250Aedge-splitting%2520mechanism%2520from%2520ES-GNN.%2520It%2520incorporates%2520the%2520edge%2520splitting%2520into%250Athe%2520loss%2520of%2520Graph-MLP%2520to%2520learn%2520two%2520separate%2520adjacency%2520matrices%2520based%2520on%250Arelevant%2520and%2520irrelevant%2520feature%2520pairs.%2520Our%2520experiments%2520on%2520seven%2520datasets%2520with%250Asix%2520baselines%2520show%2520that%2520ES-MLP%2520is%2520on%2520par%2520with%2520homophilic%2520and%2520heterophilic%250Amodels%2520on%2520all%2520datasets%2520without%2520using%2520edges%2520during%2520inference.%2520We%2520show%2520that%250AES-MLP%2520is%2520robust%2520to%2520multiple%2520types%2520of%2520edge%2520noise%2520during%2520inference%2520and%2520that%2520its%250Ainference%2520time%2520is%2520two%2520to%2520five%2520times%2520faster%2520than%2520that%2520of%2520commonly%2520used%2520MPNNs.%250AThe%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/MatthiasKohn/ES-MLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-Splitting%20MLP%3A%20Node%20Classification%20on%20Homophilic%20and%20Heterophilic%0A%20%20Graphs%20without%20Message%20Passing&entry.906535625=Matthias%20Kohn%20and%20Marcel%20Hoffmann%20and%20Ansgar%20Scherp&entry.1292438233=%20%20Message%20Passing%20Neural%20Networks%20%28MPNNs%29%20have%20demonstrated%20remarkable%20success%0Ain%20node%20classification%20on%20homophilic%20graphs.%20It%20has%20been%20shown%20that%20they%20do%20not%0Asolely%20rely%20on%20homophily%20but%20on%20neighborhood%20distributions%20of%20nodes%2C%20i.e.%2C%0Aconsistency%20of%20the%20neighborhood%20label%20distribution%20within%20the%20same%20class.%0AMLP-based%20models%20do%20not%20use%20message%20passing%2C%20%5Ceg%20Graph-MLP%20incorporates%20the%0Aneighborhood%20in%20a%20separate%20loss%20function.%20These%20models%20are%20faster%20and%20more%0Arobust%20to%20edge%20noise.%20Graph-MLP%20maps%20adjacent%20nodes%20closer%20in%20the%20embedding%0Aspace%20but%20is%20unaware%20of%20the%20neighborhood%20pattern%20of%20the%20labels%2C%20i.e.%2C%20relies%0Asolely%20on%20homophily.%20Edge%20Splitting%20GNN%20%28ES-GNN%29%20is%20a%20model%20specialized%20for%0Aheterophilic%20graphs%20and%20splits%20the%20edges%20into%20task-relevant%20and%0Atask-irrelevant%2C%20respectively.%20To%20mitigate%20the%20limitations%20of%20Graph-MLP%20on%0Aheterophilic%20graphs%2C%20we%20propose%20ES-MLP%20that%20combines%20Graph-MLP%20with%20an%0Aedge-splitting%20mechanism%20from%20ES-GNN.%20It%20incorporates%20the%20edge%20splitting%20into%0Athe%20loss%20of%20Graph-MLP%20to%20learn%20two%20separate%20adjacency%20matrices%20based%20on%0Arelevant%20and%20irrelevant%20feature%20pairs.%20Our%20experiments%20on%20seven%20datasets%20with%0Asix%20baselines%20show%20that%20ES-MLP%20is%20on%20par%20with%20homophilic%20and%20heterophilic%0Amodels%20on%20all%20datasets%20without%20using%20edges%20during%20inference.%20We%20show%20that%0AES-MLP%20is%20robust%20to%20multiple%20types%20of%20edge%20noise%20during%20inference%20and%20that%20its%0Ainference%20time%20is%20two%20to%20five%20times%20faster%20than%20that%20of%20commonly%20used%20MPNNs.%0AThe%20source%20code%20is%20available%20at%20https%3A//github.com/MatthiasKohn/ES-MLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08310v1&entry.124074799=Read"},
{"title": "Graph Agent Network: Empowering Nodes with Inference Capabilities for\n  Adversarial Resilience", "author": "Ao Liu and Wenshan Li and Tao Li and Beibei Li and Guangquan Xu and Pan Zhou and Wengang Ma and Hanyuan Huang", "abstract": "  End-to-end training with global optimization have popularized graph neural\nnetworks (GNNs) for node classification, yet inadvertently introduced\nvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit\nthe inherent opened interfaces of GNNs' input and output, perturbing critical\nedges and thus manipulating the classification results. Current defenses, due\nto their persistent utilization of global-optimization-based end-to-end\ntraining schemes, inherently encapsulate the vulnerabilities of GNNs. This is\nspecifically evidenced in their inability to defend against targeted secondary\nattacks. In this paper, we propose the Graph Agent Network (GAgN) to address\nthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent\nnetwork in which each node is designed as an 1-hop-view agent. Through the\ndecentralized interactions between agents, they can learn to infer global\nperceptions to perform tasks including inferring embeddings, degrees and\nneighbor relationships for given nodes. This empowers nodes to filtering\nadversarial edges while carrying out classification tasks. Furthermore, agents'\nlimited view prevents malicious messages from propagating globally in GAgN,\nthereby resisting global-optimization-based secondary attacks. We prove that\nsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient\nto achieve these functionalities. Experimental results show that GAgN\neffectively implements all its intended capabilities and, compared to\nstate-of-the-art defenses, achieves optimal classification accuracy on the\nperturbed datasets.\n", "link": "http://arxiv.org/abs/2306.06909v5", "date": "2024-12-11", "relevancy": 2.386, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5167}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4615}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Agent%20Network%3A%20Empowering%20Nodes%20with%20Inference%20Capabilities%20for%0A%20%20Adversarial%20Resilience&body=Title%3A%20Graph%20Agent%20Network%3A%20Empowering%20Nodes%20with%20Inference%20Capabilities%20for%0A%20%20Adversarial%20Resilience%0AAuthor%3A%20Ao%20Liu%20and%20Wenshan%20Li%20and%20Tao%20Li%20and%20Beibei%20Li%20and%20Guangquan%20Xu%20and%20Pan%20Zhou%20and%20Wengang%20Ma%20and%20Hanyuan%20Huang%0AAbstract%3A%20%20%20End-to-end%20training%20with%20global%20optimization%20have%20popularized%20graph%20neural%0Anetworks%20%28GNNs%29%20for%20node%20classification%2C%20yet%20inadvertently%20introduced%0Avulnerabilities%20to%20adversarial%20edge-perturbing%20attacks.%20Adversaries%20can%20exploit%0Athe%20inherent%20opened%20interfaces%20of%20GNNs%27%20input%20and%20output%2C%20perturbing%20critical%0Aedges%20and%20thus%20manipulating%20the%20classification%20results.%20Current%20defenses%2C%20due%0Ato%20their%20persistent%20utilization%20of%20global-optimization-based%20end-to-end%0Atraining%20schemes%2C%20inherently%20encapsulate%20the%20vulnerabilities%20of%20GNNs.%20This%20is%0Aspecifically%20evidenced%20in%20their%20inability%20to%20defend%20against%20targeted%20secondary%0Aattacks.%20In%20this%20paper%2C%20we%20propose%20the%20Graph%20Agent%20Network%20%28GAgN%29%20to%20address%0Athe%20aforementioned%20vulnerabilities%20of%20GNNs.%20GAgN%20is%20a%20graph-structured%20agent%0Anetwork%20in%20which%20each%20node%20is%20designed%20as%20an%201-hop-view%20agent.%20Through%20the%0Adecentralized%20interactions%20between%20agents%2C%20they%20can%20learn%20to%20infer%20global%0Aperceptions%20to%20perform%20tasks%20including%20inferring%20embeddings%2C%20degrees%20and%0Aneighbor%20relationships%20for%20given%20nodes.%20This%20empowers%20nodes%20to%20filtering%0Aadversarial%20edges%20while%20carrying%20out%20classification%20tasks.%20Furthermore%2C%20agents%27%0Alimited%20view%20prevents%20malicious%20messages%20from%20propagating%20globally%20in%20GAgN%2C%0Athereby%20resisting%20global-optimization-based%20secondary%20attacks.%20We%20prove%20that%0Asingle-hidden-layer%20multilayer%20perceptrons%20%28MLPs%29%20are%20theoretically%20sufficient%0Ato%20achieve%20these%20functionalities.%20Experimental%20results%20show%20that%20GAgN%0Aeffectively%20implements%20all%20its%20intended%20capabilities%20and%2C%20compared%20to%0Astate-of-the-art%20defenses%2C%20achieves%20optimal%20classification%20accuracy%20on%20the%0Aperturbed%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06909v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Agent%2520Network%253A%2520Empowering%2520Nodes%2520with%2520Inference%2520Capabilities%2520for%250A%2520%2520Adversarial%2520Resilience%26entry.906535625%3DAo%2520Liu%2520and%2520Wenshan%2520Li%2520and%2520Tao%2520Li%2520and%2520Beibei%2520Li%2520and%2520Guangquan%2520Xu%2520and%2520Pan%2520Zhou%2520and%2520Wengang%2520Ma%2520and%2520Hanyuan%2520Huang%26entry.1292438233%3D%2520%2520End-to-end%2520training%2520with%2520global%2520optimization%2520have%2520popularized%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520for%2520node%2520classification%252C%2520yet%2520inadvertently%2520introduced%250Avulnerabilities%2520to%2520adversarial%2520edge-perturbing%2520attacks.%2520Adversaries%2520can%2520exploit%250Athe%2520inherent%2520opened%2520interfaces%2520of%2520GNNs%2527%2520input%2520and%2520output%252C%2520perturbing%2520critical%250Aedges%2520and%2520thus%2520manipulating%2520the%2520classification%2520results.%2520Current%2520defenses%252C%2520due%250Ato%2520their%2520persistent%2520utilization%2520of%2520global-optimization-based%2520end-to-end%250Atraining%2520schemes%252C%2520inherently%2520encapsulate%2520the%2520vulnerabilities%2520of%2520GNNs.%2520This%2520is%250Aspecifically%2520evidenced%2520in%2520their%2520inability%2520to%2520defend%2520against%2520targeted%2520secondary%250Aattacks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Graph%2520Agent%2520Network%2520%2528GAgN%2529%2520to%2520address%250Athe%2520aforementioned%2520vulnerabilities%2520of%2520GNNs.%2520GAgN%2520is%2520a%2520graph-structured%2520agent%250Anetwork%2520in%2520which%2520each%2520node%2520is%2520designed%2520as%2520an%25201-hop-view%2520agent.%2520Through%2520the%250Adecentralized%2520interactions%2520between%2520agents%252C%2520they%2520can%2520learn%2520to%2520infer%2520global%250Aperceptions%2520to%2520perform%2520tasks%2520including%2520inferring%2520embeddings%252C%2520degrees%2520and%250Aneighbor%2520relationships%2520for%2520given%2520nodes.%2520This%2520empowers%2520nodes%2520to%2520filtering%250Aadversarial%2520edges%2520while%2520carrying%2520out%2520classification%2520tasks.%2520Furthermore%252C%2520agents%2527%250Alimited%2520view%2520prevents%2520malicious%2520messages%2520from%2520propagating%2520globally%2520in%2520GAgN%252C%250Athereby%2520resisting%2520global-optimization-based%2520secondary%2520attacks.%2520We%2520prove%2520that%250Asingle-hidden-layer%2520multilayer%2520perceptrons%2520%2528MLPs%2529%2520are%2520theoretically%2520sufficient%250Ato%2520achieve%2520these%2520functionalities.%2520Experimental%2520results%2520show%2520that%2520GAgN%250Aeffectively%2520implements%2520all%2520its%2520intended%2520capabilities%2520and%252C%2520compared%2520to%250Astate-of-the-art%2520defenses%252C%2520achieves%2520optimal%2520classification%2520accuracy%2520on%2520the%250Aperturbed%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06909v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Agent%20Network%3A%20Empowering%20Nodes%20with%20Inference%20Capabilities%20for%0A%20%20Adversarial%20Resilience&entry.906535625=Ao%20Liu%20and%20Wenshan%20Li%20and%20Tao%20Li%20and%20Beibei%20Li%20and%20Guangquan%20Xu%20and%20Pan%20Zhou%20and%20Wengang%20Ma%20and%20Hanyuan%20Huang&entry.1292438233=%20%20End-to-end%20training%20with%20global%20optimization%20have%20popularized%20graph%20neural%0Anetworks%20%28GNNs%29%20for%20node%20classification%2C%20yet%20inadvertently%20introduced%0Avulnerabilities%20to%20adversarial%20edge-perturbing%20attacks.%20Adversaries%20can%20exploit%0Athe%20inherent%20opened%20interfaces%20of%20GNNs%27%20input%20and%20output%2C%20perturbing%20critical%0Aedges%20and%20thus%20manipulating%20the%20classification%20results.%20Current%20defenses%2C%20due%0Ato%20their%20persistent%20utilization%20of%20global-optimization-based%20end-to-end%0Atraining%20schemes%2C%20inherently%20encapsulate%20the%20vulnerabilities%20of%20GNNs.%20This%20is%0Aspecifically%20evidenced%20in%20their%20inability%20to%20defend%20against%20targeted%20secondary%0Aattacks.%20In%20this%20paper%2C%20we%20propose%20the%20Graph%20Agent%20Network%20%28GAgN%29%20to%20address%0Athe%20aforementioned%20vulnerabilities%20of%20GNNs.%20GAgN%20is%20a%20graph-structured%20agent%0Anetwork%20in%20which%20each%20node%20is%20designed%20as%20an%201-hop-view%20agent.%20Through%20the%0Adecentralized%20interactions%20between%20agents%2C%20they%20can%20learn%20to%20infer%20global%0Aperceptions%20to%20perform%20tasks%20including%20inferring%20embeddings%2C%20degrees%20and%0Aneighbor%20relationships%20for%20given%20nodes.%20This%20empowers%20nodes%20to%20filtering%0Aadversarial%20edges%20while%20carrying%20out%20classification%20tasks.%20Furthermore%2C%20agents%27%0Alimited%20view%20prevents%20malicious%20messages%20from%20propagating%20globally%20in%20GAgN%2C%0Athereby%20resisting%20global-optimization-based%20secondary%20attacks.%20We%20prove%20that%0Asingle-hidden-layer%20multilayer%20perceptrons%20%28MLPs%29%20are%20theoretically%20sufficient%0Ato%20achieve%20these%20functionalities.%20Experimental%20results%20show%20that%20GAgN%0Aeffectively%20implements%20all%20its%20intended%20capabilities%20and%2C%20compared%20to%0Astate-of-the-art%20defenses%2C%20achieves%20optimal%20classification%20accuracy%20on%20the%0Aperturbed%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06909v5&entry.124074799=Read"},
{"title": "Annotation-guided Protein Design with Multi-Level Domain Alignment", "author": "Chaohao Yuan and Songyou Li and Geyan Ye and Yikun Zhang and Long-Kai Huang and Wenbing Huang and Wei Liu and Jianhua Yao and Yu Rong", "abstract": "  The core challenge of de novo protein design lies in creating proteins with\nspecific functions or properties, guided by certain conditions. Current models\nexplore to generate protein using structural and evolutionary guidance, which\nonly provide indirect conditions concerning functions and properties. However,\ntextual annotations of proteins, especially the annotations for protein\ndomains, which directly describe the protein's high-level functionalities,\nproperties, and their correlation with target amino acid sequences, remain\nunexplored in the context of protein design tasks. In this paper, we propose\nProtein-Annotation Alignment Generation, PAAG, a multi-modality protein design\nframework that integrates the textual annotations extracted from protein\ndatabase for controllable generation in sequence space. Specifically, within a\nmulti-level alignment module, PAAG can explicitly generate proteins containing\nspecific domains conditioned on the corresponding domain annotations, and can\neven design novel proteins with flexible combinations of different kinds of\nannotations. Our experimental results underscore the superiority of the aligned\nprotein representations from PAAG over 7 prediction tasks. Furthermore, PAAG\ndemonstrates a significant increase in generation success rate (24.7% vs 4.7%\nin zinc finger, and 54.3% vs 22.0% in the immunoglobulin domain) in comparison\nto the existing model. We anticipate that PAAG will broaden the horizons of\nprotein design by leveraging the knowledge from between textual annotation and\nproteins.\n", "link": "http://arxiv.org/abs/2404.16866v3", "date": "2024-12-11", "relevancy": 2.3781, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4998}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annotation-guided%20Protein%20Design%20with%20Multi-Level%20Domain%20Alignment&body=Title%3A%20Annotation-guided%20Protein%20Design%20with%20Multi-Level%20Domain%20Alignment%0AAuthor%3A%20Chaohao%20Yuan%20and%20Songyou%20Li%20and%20Geyan%20Ye%20and%20Yikun%20Zhang%20and%20Long-Kai%20Huang%20and%20Wenbing%20Huang%20and%20Wei%20Liu%20and%20Jianhua%20Yao%20and%20Yu%20Rong%0AAbstract%3A%20%20%20The%20core%20challenge%20of%20de%20novo%20protein%20design%20lies%20in%20creating%20proteins%20with%0Aspecific%20functions%20or%20properties%2C%20guided%20by%20certain%20conditions.%20Current%20models%0Aexplore%20to%20generate%20protein%20using%20structural%20and%20evolutionary%20guidance%2C%20which%0Aonly%20provide%20indirect%20conditions%20concerning%20functions%20and%20properties.%20However%2C%0Atextual%20annotations%20of%20proteins%2C%20especially%20the%20annotations%20for%20protein%0Adomains%2C%20which%20directly%20describe%20the%20protein%27s%20high-level%20functionalities%2C%0Aproperties%2C%20and%20their%20correlation%20with%20target%20amino%20acid%20sequences%2C%20remain%0Aunexplored%20in%20the%20context%20of%20protein%20design%20tasks.%20In%20this%20paper%2C%20we%20propose%0AProtein-Annotation%20Alignment%20Generation%2C%20PAAG%2C%20a%20multi-modality%20protein%20design%0Aframework%20that%20integrates%20the%20textual%20annotations%20extracted%20from%20protein%0Adatabase%20for%20controllable%20generation%20in%20sequence%20space.%20Specifically%2C%20within%20a%0Amulti-level%20alignment%20module%2C%20PAAG%20can%20explicitly%20generate%20proteins%20containing%0Aspecific%20domains%20conditioned%20on%20the%20corresponding%20domain%20annotations%2C%20and%20can%0Aeven%20design%20novel%20proteins%20with%20flexible%20combinations%20of%20different%20kinds%20of%0Aannotations.%20Our%20experimental%20results%20underscore%20the%20superiority%20of%20the%20aligned%0Aprotein%20representations%20from%20PAAG%20over%207%20prediction%20tasks.%20Furthermore%2C%20PAAG%0Ademonstrates%20a%20significant%20increase%20in%20generation%20success%20rate%20%2824.7%25%20vs%204.7%25%0Ain%20zinc%20finger%2C%20and%2054.3%25%20vs%2022.0%25%20in%20the%20immunoglobulin%20domain%29%20in%20comparison%0Ato%20the%20existing%20model.%20We%20anticipate%20that%20PAAG%20will%20broaden%20the%20horizons%20of%0Aprotein%20design%20by%20leveraging%20the%20knowledge%20from%20between%20textual%20annotation%20and%0Aproteins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16866v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnotation-guided%2520Protein%2520Design%2520with%2520Multi-Level%2520Domain%2520Alignment%26entry.906535625%3DChaohao%2520Yuan%2520and%2520Songyou%2520Li%2520and%2520Geyan%2520Ye%2520and%2520Yikun%2520Zhang%2520and%2520Long-Kai%2520Huang%2520and%2520Wenbing%2520Huang%2520and%2520Wei%2520Liu%2520and%2520Jianhua%2520Yao%2520and%2520Yu%2520Rong%26entry.1292438233%3D%2520%2520The%2520core%2520challenge%2520of%2520de%2520novo%2520protein%2520design%2520lies%2520in%2520creating%2520proteins%2520with%250Aspecific%2520functions%2520or%2520properties%252C%2520guided%2520by%2520certain%2520conditions.%2520Current%2520models%250Aexplore%2520to%2520generate%2520protein%2520using%2520structural%2520and%2520evolutionary%2520guidance%252C%2520which%250Aonly%2520provide%2520indirect%2520conditions%2520concerning%2520functions%2520and%2520properties.%2520However%252C%250Atextual%2520annotations%2520of%2520proteins%252C%2520especially%2520the%2520annotations%2520for%2520protein%250Adomains%252C%2520which%2520directly%2520describe%2520the%2520protein%2527s%2520high-level%2520functionalities%252C%250Aproperties%252C%2520and%2520their%2520correlation%2520with%2520target%2520amino%2520acid%2520sequences%252C%2520remain%250Aunexplored%2520in%2520the%2520context%2520of%2520protein%2520design%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%250AProtein-Annotation%2520Alignment%2520Generation%252C%2520PAAG%252C%2520a%2520multi-modality%2520protein%2520design%250Aframework%2520that%2520integrates%2520the%2520textual%2520annotations%2520extracted%2520from%2520protein%250Adatabase%2520for%2520controllable%2520generation%2520in%2520sequence%2520space.%2520Specifically%252C%2520within%2520a%250Amulti-level%2520alignment%2520module%252C%2520PAAG%2520can%2520explicitly%2520generate%2520proteins%2520containing%250Aspecific%2520domains%2520conditioned%2520on%2520the%2520corresponding%2520domain%2520annotations%252C%2520and%2520can%250Aeven%2520design%2520novel%2520proteins%2520with%2520flexible%2520combinations%2520of%2520different%2520kinds%2520of%250Aannotations.%2520Our%2520experimental%2520results%2520underscore%2520the%2520superiority%2520of%2520the%2520aligned%250Aprotein%2520representations%2520from%2520PAAG%2520over%25207%2520prediction%2520tasks.%2520Furthermore%252C%2520PAAG%250Ademonstrates%2520a%2520significant%2520increase%2520in%2520generation%2520success%2520rate%2520%252824.7%2525%2520vs%25204.7%2525%250Ain%2520zinc%2520finger%252C%2520and%252054.3%2525%2520vs%252022.0%2525%2520in%2520the%2520immunoglobulin%2520domain%2529%2520in%2520comparison%250Ato%2520the%2520existing%2520model.%2520We%2520anticipate%2520that%2520PAAG%2520will%2520broaden%2520the%2520horizons%2520of%250Aprotein%2520design%2520by%2520leveraging%2520the%2520knowledge%2520from%2520between%2520textual%2520annotation%2520and%250Aproteins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16866v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotation-guided%20Protein%20Design%20with%20Multi-Level%20Domain%20Alignment&entry.906535625=Chaohao%20Yuan%20and%20Songyou%20Li%20and%20Geyan%20Ye%20and%20Yikun%20Zhang%20and%20Long-Kai%20Huang%20and%20Wenbing%20Huang%20and%20Wei%20Liu%20and%20Jianhua%20Yao%20and%20Yu%20Rong&entry.1292438233=%20%20The%20core%20challenge%20of%20de%20novo%20protein%20design%20lies%20in%20creating%20proteins%20with%0Aspecific%20functions%20or%20properties%2C%20guided%20by%20certain%20conditions.%20Current%20models%0Aexplore%20to%20generate%20protein%20using%20structural%20and%20evolutionary%20guidance%2C%20which%0Aonly%20provide%20indirect%20conditions%20concerning%20functions%20and%20properties.%20However%2C%0Atextual%20annotations%20of%20proteins%2C%20especially%20the%20annotations%20for%20protein%0Adomains%2C%20which%20directly%20describe%20the%20protein%27s%20high-level%20functionalities%2C%0Aproperties%2C%20and%20their%20correlation%20with%20target%20amino%20acid%20sequences%2C%20remain%0Aunexplored%20in%20the%20context%20of%20protein%20design%20tasks.%20In%20this%20paper%2C%20we%20propose%0AProtein-Annotation%20Alignment%20Generation%2C%20PAAG%2C%20a%20multi-modality%20protein%20design%0Aframework%20that%20integrates%20the%20textual%20annotations%20extracted%20from%20protein%0Adatabase%20for%20controllable%20generation%20in%20sequence%20space.%20Specifically%2C%20within%20a%0Amulti-level%20alignment%20module%2C%20PAAG%20can%20explicitly%20generate%20proteins%20containing%0Aspecific%20domains%20conditioned%20on%20the%20corresponding%20domain%20annotations%2C%20and%20can%0Aeven%20design%20novel%20proteins%20with%20flexible%20combinations%20of%20different%20kinds%20of%0Aannotations.%20Our%20experimental%20results%20underscore%20the%20superiority%20of%20the%20aligned%0Aprotein%20representations%20from%20PAAG%20over%207%20prediction%20tasks.%20Furthermore%2C%20PAAG%0Ademonstrates%20a%20significant%20increase%20in%20generation%20success%20rate%20%2824.7%25%20vs%204.7%25%0Ain%20zinc%20finger%2C%20and%2054.3%25%20vs%2022.0%25%20in%20the%20immunoglobulin%20domain%29%20in%20comparison%0Ato%20the%20existing%20model.%20We%20anticipate%20that%20PAAG%20will%20broaden%20the%20horizons%20of%0Aprotein%20design%20by%20leveraging%20the%20knowledge%20from%20between%20textual%20annotation%20and%0Aproteins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16866v3&entry.124074799=Read"},
{"title": "GPD-1: Generative Pre-training for Driving", "author": "Zixun Xie and Sicheng Zuo and Wenzhao Zheng and Yunpeng Zhang and Dalong Du and Jie Zhou and Jiwen Lu and Shanghang Zhang", "abstract": "  Modeling the evolutions of driving scenarios is important for the evaluation\nand decision-making of autonomous driving systems. Most existing methods focus\non one aspect of scene evolution such as map generation, motion prediction, and\ntrajectory planning. In this paper, we propose a unified Generative\nPre-training for Driving (GPD-1) model to accomplish all these tasks altogether\nwithout additional fine-tuning. We represent each scene with ego, agent, and\nmap tokens and formulate autonomous driving as a unified token generation\nproblem. We adopt the autoregressive transformer architecture and use a\nscene-level attention mask to enable intra-scene bi-directional interactions.\nFor the ego and agent tokens, we propose a hierarchical positional tokenizer to\neffectively encode both 2D positions and headings. For the map tokens, we train\na map vector-quantized autoencoder to efficiently compress ego-centric semantic\nmaps into discrete tokens. We pre-train our GPD-1 on the large-scale nuPlan\ndataset and conduct extensive experiments to evaluate its effectiveness. With\ndifferent prompts, our GPD-1 successfully generalizes to various tasks without\nfinetuning, including scene generation, traffic simulation, closed-loop\nsimulation, map prediction, and motion planning. Code:\nhttps://github.com/wzzheng/GPD.\n", "link": "http://arxiv.org/abs/2412.08643v1", "date": "2024-12-11", "relevancy": 2.3618, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6202}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.579}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPD-1%3A%20Generative%20Pre-training%20for%20Driving&body=Title%3A%20GPD-1%3A%20Generative%20Pre-training%20for%20Driving%0AAuthor%3A%20Zixun%20Xie%20and%20Sicheng%20Zuo%20and%20Wenzhao%20Zheng%20and%20Yunpeng%20Zhang%20and%20Dalong%20Du%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Modeling%20the%20evolutions%20of%20driving%20scenarios%20is%20important%20for%20the%20evaluation%0Aand%20decision-making%20of%20autonomous%20driving%20systems.%20Most%20existing%20methods%20focus%0Aon%20one%20aspect%20of%20scene%20evolution%20such%20as%20map%20generation%2C%20motion%20prediction%2C%20and%0Atrajectory%20planning.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20Generative%0APre-training%20for%20Driving%20%28GPD-1%29%20model%20to%20accomplish%20all%20these%20tasks%20altogether%0Awithout%20additional%20fine-tuning.%20We%20represent%20each%20scene%20with%20ego%2C%20agent%2C%20and%0Amap%20tokens%20and%20formulate%20autonomous%20driving%20as%20a%20unified%20token%20generation%0Aproblem.%20We%20adopt%20the%20autoregressive%20transformer%20architecture%20and%20use%20a%0Ascene-level%20attention%20mask%20to%20enable%20intra-scene%20bi-directional%20interactions.%0AFor%20the%20ego%20and%20agent%20tokens%2C%20we%20propose%20a%20hierarchical%20positional%20tokenizer%20to%0Aeffectively%20encode%20both%202D%20positions%20and%20headings.%20For%20the%20map%20tokens%2C%20we%20train%0Aa%20map%20vector-quantized%20autoencoder%20to%20efficiently%20compress%20ego-centric%20semantic%0Amaps%20into%20discrete%20tokens.%20We%20pre-train%20our%20GPD-1%20on%20the%20large-scale%20nuPlan%0Adataset%20and%20conduct%20extensive%20experiments%20to%20evaluate%20its%20effectiveness.%20With%0Adifferent%20prompts%2C%20our%20GPD-1%20successfully%20generalizes%20to%20various%20tasks%20without%0Afinetuning%2C%20including%20scene%20generation%2C%20traffic%20simulation%2C%20closed-loop%0Asimulation%2C%20map%20prediction%2C%20and%20motion%20planning.%20Code%3A%0Ahttps%3A//github.com/wzzheng/GPD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPD-1%253A%2520Generative%2520Pre-training%2520for%2520Driving%26entry.906535625%3DZixun%2520Xie%2520and%2520Sicheng%2520Zuo%2520and%2520Wenzhao%2520Zheng%2520and%2520Yunpeng%2520Zhang%2520and%2520Dalong%2520Du%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Modeling%2520the%2520evolutions%2520of%2520driving%2520scenarios%2520is%2520important%2520for%2520the%2520evaluation%250Aand%2520decision-making%2520of%2520autonomous%2520driving%2520systems.%2520Most%2520existing%2520methods%2520focus%250Aon%2520one%2520aspect%2520of%2520scene%2520evolution%2520such%2520as%2520map%2520generation%252C%2520motion%2520prediction%252C%2520and%250Atrajectory%2520planning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520unified%2520Generative%250APre-training%2520for%2520Driving%2520%2528GPD-1%2529%2520model%2520to%2520accomplish%2520all%2520these%2520tasks%2520altogether%250Awithout%2520additional%2520fine-tuning.%2520We%2520represent%2520each%2520scene%2520with%2520ego%252C%2520agent%252C%2520and%250Amap%2520tokens%2520and%2520formulate%2520autonomous%2520driving%2520as%2520a%2520unified%2520token%2520generation%250Aproblem.%2520We%2520adopt%2520the%2520autoregressive%2520transformer%2520architecture%2520and%2520use%2520a%250Ascene-level%2520attention%2520mask%2520to%2520enable%2520intra-scene%2520bi-directional%2520interactions.%250AFor%2520the%2520ego%2520and%2520agent%2520tokens%252C%2520we%2520propose%2520a%2520hierarchical%2520positional%2520tokenizer%2520to%250Aeffectively%2520encode%2520both%25202D%2520positions%2520and%2520headings.%2520For%2520the%2520map%2520tokens%252C%2520we%2520train%250Aa%2520map%2520vector-quantized%2520autoencoder%2520to%2520efficiently%2520compress%2520ego-centric%2520semantic%250Amaps%2520into%2520discrete%2520tokens.%2520We%2520pre-train%2520our%2520GPD-1%2520on%2520the%2520large-scale%2520nuPlan%250Adataset%2520and%2520conduct%2520extensive%2520experiments%2520to%2520evaluate%2520its%2520effectiveness.%2520With%250Adifferent%2520prompts%252C%2520our%2520GPD-1%2520successfully%2520generalizes%2520to%2520various%2520tasks%2520without%250Afinetuning%252C%2520including%2520scene%2520generation%252C%2520traffic%2520simulation%252C%2520closed-loop%250Asimulation%252C%2520map%2520prediction%252C%2520and%2520motion%2520planning.%2520Code%253A%250Ahttps%253A//github.com/wzzheng/GPD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPD-1%3A%20Generative%20Pre-training%20for%20Driving&entry.906535625=Zixun%20Xie%20and%20Sicheng%20Zuo%20and%20Wenzhao%20Zheng%20and%20Yunpeng%20Zhang%20and%20Dalong%20Du%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Modeling%20the%20evolutions%20of%20driving%20scenarios%20is%20important%20for%20the%20evaluation%0Aand%20decision-making%20of%20autonomous%20driving%20systems.%20Most%20existing%20methods%20focus%0Aon%20one%20aspect%20of%20scene%20evolution%20such%20as%20map%20generation%2C%20motion%20prediction%2C%20and%0Atrajectory%20planning.%20In%20this%20paper%2C%20we%20propose%20a%20unified%20Generative%0APre-training%20for%20Driving%20%28GPD-1%29%20model%20to%20accomplish%20all%20these%20tasks%20altogether%0Awithout%20additional%20fine-tuning.%20We%20represent%20each%20scene%20with%20ego%2C%20agent%2C%20and%0Amap%20tokens%20and%20formulate%20autonomous%20driving%20as%20a%20unified%20token%20generation%0Aproblem.%20We%20adopt%20the%20autoregressive%20transformer%20architecture%20and%20use%20a%0Ascene-level%20attention%20mask%20to%20enable%20intra-scene%20bi-directional%20interactions.%0AFor%20the%20ego%20and%20agent%20tokens%2C%20we%20propose%20a%20hierarchical%20positional%20tokenizer%20to%0Aeffectively%20encode%20both%202D%20positions%20and%20headings.%20For%20the%20map%20tokens%2C%20we%20train%0Aa%20map%20vector-quantized%20autoencoder%20to%20efficiently%20compress%20ego-centric%20semantic%0Amaps%20into%20discrete%20tokens.%20We%20pre-train%20our%20GPD-1%20on%20the%20large-scale%20nuPlan%0Adataset%20and%20conduct%20extensive%20experiments%20to%20evaluate%20its%20effectiveness.%20With%0Adifferent%20prompts%2C%20our%20GPD-1%20successfully%20generalizes%20to%20various%20tasks%20without%0Afinetuning%2C%20including%20scene%20generation%2C%20traffic%20simulation%2C%20closed-loop%0Asimulation%2C%20map%20prediction%2C%20and%20motion%20planning.%20Code%3A%0Ahttps%3A//github.com/wzzheng/GPD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08643v1&entry.124074799=Read"},
{"title": "FLIP: Flow-Centric Generative Planning for General-Purpose Manipulation\n  Tasks", "author": "Chongkai Gao and Haozhuo Zhang and Zhixuan Xu and Zhehao Cai and Lin Shao", "abstract": "  We aim to develop a model-based planning framework for world models that can\nbe scaled with increasing model and data budgets for general-purpose\nmanipulation tasks with only language and vision inputs. To this end, we\npresent FLow-centric generative Planning (FLIP), a model-based planning\nalgorithm on visual space that features three key modules: 1. a multi-modal\nflow generation model as the general-purpose action proposal module; 2. a\nflow-conditioned video generation model as the dynamics module; and 3. a\nvision-language representation learning model as the value module. Given an\ninitial image and language instruction as the goal, FLIP can progressively\nsearch for long-horizon flow and video plans that maximize the discounted\nreturn to accomplish the task. FLIP is able to synthesize long-horizon plans\nacross objects, robots, and tasks with image flows as the general action\nrepresentation, and the dense flow information also provides rich guidance for\nlong-horizon video generation. In addition, the synthesized flow and video\nplans can guide the training of low-level control policies for robot execution.\nExperiments on diverse benchmarks demonstrate that FLIP can improve both the\nsuccess rates and quality of long-horizon video plan synthesis and has the\ninteractive world model property, opening up wider applications for future\nworks.\n", "link": "http://arxiv.org/abs/2412.08261v1", "date": "2024-12-11", "relevancy": 2.35, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5897}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5883}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLIP%3A%20Flow-Centric%20Generative%20Planning%20for%20General-Purpose%20Manipulation%0A%20%20Tasks&body=Title%3A%20FLIP%3A%20Flow-Centric%20Generative%20Planning%20for%20General-Purpose%20Manipulation%0A%20%20Tasks%0AAuthor%3A%20Chongkai%20Gao%20and%20Haozhuo%20Zhang%20and%20Zhixuan%20Xu%20and%20Zhehao%20Cai%20and%20Lin%20Shao%0AAbstract%3A%20%20%20We%20aim%20to%20develop%20a%20model-based%20planning%20framework%20for%20world%20models%20that%20can%0Abe%20scaled%20with%20increasing%20model%20and%20data%20budgets%20for%20general-purpose%0Amanipulation%20tasks%20with%20only%20language%20and%20vision%20inputs.%20To%20this%20end%2C%20we%0Apresent%20FLow-centric%20generative%20Planning%20%28FLIP%29%2C%20a%20model-based%20planning%0Aalgorithm%20on%20visual%20space%20that%20features%20three%20key%20modules%3A%201.%20a%20multi-modal%0Aflow%20generation%20model%20as%20the%20general-purpose%20action%20proposal%20module%3B%202.%20a%0Aflow-conditioned%20video%20generation%20model%20as%20the%20dynamics%20module%3B%20and%203.%20a%0Avision-language%20representation%20learning%20model%20as%20the%20value%20module.%20Given%20an%0Ainitial%20image%20and%20language%20instruction%20as%20the%20goal%2C%20FLIP%20can%20progressively%0Asearch%20for%20long-horizon%20flow%20and%20video%20plans%20that%20maximize%20the%20discounted%0Areturn%20to%20accomplish%20the%20task.%20FLIP%20is%20able%20to%20synthesize%20long-horizon%20plans%0Aacross%20objects%2C%20robots%2C%20and%20tasks%20with%20image%20flows%20as%20the%20general%20action%0Arepresentation%2C%20and%20the%20dense%20flow%20information%20also%20provides%20rich%20guidance%20for%0Along-horizon%20video%20generation.%20In%20addition%2C%20the%20synthesized%20flow%20and%20video%0Aplans%20can%20guide%20the%20training%20of%20low-level%20control%20policies%20for%20robot%20execution.%0AExperiments%20on%20diverse%20benchmarks%20demonstrate%20that%20FLIP%20can%20improve%20both%20the%0Asuccess%20rates%20and%20quality%20of%20long-horizon%20video%20plan%20synthesis%20and%20has%20the%0Ainteractive%20world%20model%20property%2C%20opening%20up%20wider%20applications%20for%20future%0Aworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLIP%253A%2520Flow-Centric%2520Generative%2520Planning%2520for%2520General-Purpose%2520Manipulation%250A%2520%2520Tasks%26entry.906535625%3DChongkai%2520Gao%2520and%2520Haozhuo%2520Zhang%2520and%2520Zhixuan%2520Xu%2520and%2520Zhehao%2520Cai%2520and%2520Lin%2520Shao%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520develop%2520a%2520model-based%2520planning%2520framework%2520for%2520world%2520models%2520that%2520can%250Abe%2520scaled%2520with%2520increasing%2520model%2520and%2520data%2520budgets%2520for%2520general-purpose%250Amanipulation%2520tasks%2520with%2520only%2520language%2520and%2520vision%2520inputs.%2520To%2520this%2520end%252C%2520we%250Apresent%2520FLow-centric%2520generative%2520Planning%2520%2528FLIP%2529%252C%2520a%2520model-based%2520planning%250Aalgorithm%2520on%2520visual%2520space%2520that%2520features%2520three%2520key%2520modules%253A%25201.%2520a%2520multi-modal%250Aflow%2520generation%2520model%2520as%2520the%2520general-purpose%2520action%2520proposal%2520module%253B%25202.%2520a%250Aflow-conditioned%2520video%2520generation%2520model%2520as%2520the%2520dynamics%2520module%253B%2520and%25203.%2520a%250Avision-language%2520representation%2520learning%2520model%2520as%2520the%2520value%2520module.%2520Given%2520an%250Ainitial%2520image%2520and%2520language%2520instruction%2520as%2520the%2520goal%252C%2520FLIP%2520can%2520progressively%250Asearch%2520for%2520long-horizon%2520flow%2520and%2520video%2520plans%2520that%2520maximize%2520the%2520discounted%250Areturn%2520to%2520accomplish%2520the%2520task.%2520FLIP%2520is%2520able%2520to%2520synthesize%2520long-horizon%2520plans%250Aacross%2520objects%252C%2520robots%252C%2520and%2520tasks%2520with%2520image%2520flows%2520as%2520the%2520general%2520action%250Arepresentation%252C%2520and%2520the%2520dense%2520flow%2520information%2520also%2520provides%2520rich%2520guidance%2520for%250Along-horizon%2520video%2520generation.%2520In%2520addition%252C%2520the%2520synthesized%2520flow%2520and%2520video%250Aplans%2520can%2520guide%2520the%2520training%2520of%2520low-level%2520control%2520policies%2520for%2520robot%2520execution.%250AExperiments%2520on%2520diverse%2520benchmarks%2520demonstrate%2520that%2520FLIP%2520can%2520improve%2520both%2520the%250Asuccess%2520rates%2520and%2520quality%2520of%2520long-horizon%2520video%2520plan%2520synthesis%2520and%2520has%2520the%250Ainteractive%2520world%2520model%2520property%252C%2520opening%2520up%2520wider%2520applications%2520for%2520future%250Aworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLIP%3A%20Flow-Centric%20Generative%20Planning%20for%20General-Purpose%20Manipulation%0A%20%20Tasks&entry.906535625=Chongkai%20Gao%20and%20Haozhuo%20Zhang%20and%20Zhixuan%20Xu%20and%20Zhehao%20Cai%20and%20Lin%20Shao&entry.1292438233=%20%20We%20aim%20to%20develop%20a%20model-based%20planning%20framework%20for%20world%20models%20that%20can%0Abe%20scaled%20with%20increasing%20model%20and%20data%20budgets%20for%20general-purpose%0Amanipulation%20tasks%20with%20only%20language%20and%20vision%20inputs.%20To%20this%20end%2C%20we%0Apresent%20FLow-centric%20generative%20Planning%20%28FLIP%29%2C%20a%20model-based%20planning%0Aalgorithm%20on%20visual%20space%20that%20features%20three%20key%20modules%3A%201.%20a%20multi-modal%0Aflow%20generation%20model%20as%20the%20general-purpose%20action%20proposal%20module%3B%202.%20a%0Aflow-conditioned%20video%20generation%20model%20as%20the%20dynamics%20module%3B%20and%203.%20a%0Avision-language%20representation%20learning%20model%20as%20the%20value%20module.%20Given%20an%0Ainitial%20image%20and%20language%20instruction%20as%20the%20goal%2C%20FLIP%20can%20progressively%0Asearch%20for%20long-horizon%20flow%20and%20video%20plans%20that%20maximize%20the%20discounted%0Areturn%20to%20accomplish%20the%20task.%20FLIP%20is%20able%20to%20synthesize%20long-horizon%20plans%0Aacross%20objects%2C%20robots%2C%20and%20tasks%20with%20image%20flows%20as%20the%20general%20action%0Arepresentation%2C%20and%20the%20dense%20flow%20information%20also%20provides%20rich%20guidance%20for%0Along-horizon%20video%20generation.%20In%20addition%2C%20the%20synthesized%20flow%20and%20video%0Aplans%20can%20guide%20the%20training%20of%20low-level%20control%20policies%20for%20robot%20execution.%0AExperiments%20on%20diverse%20benchmarks%20demonstrate%20that%20FLIP%20can%20improve%20both%20the%0Asuccess%20rates%20and%20quality%20of%20long-horizon%20video%20plan%20synthesis%20and%20has%20the%0Ainteractive%20world%20model%20property%2C%20opening%20up%20wider%20applications%20for%20future%0Aworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08261v1&entry.124074799=Read"},
{"title": "Continuous Multidimensional Scaling", "author": "Michael W. Trosset and Carey E. Priebe", "abstract": "  Multidimensional scaling (MDS) is the act of embedding proximity information\nabout a set of $n$ objects in $d$-dimensional Euclidean space. As originally\nconceived by the psychometric community, MDS was concerned with embedding a\nfixed set of proximities associated with a fixed set of objects. Modern\nconcerns, e.g., that arise in developing asymptotic theories for statistical\ninference on random graphs, more typically involve studying the limiting\nbehavior of a sequence of proximities associated with an increasing set of\nobjects. Here we are concerned with embedding dissimilarities by minimizing\nKruskal's (1964) raw stress criterion. Standard results from the theory of\npoint-to-set maps can be used to establish that, if $n$ is fixed and a sequence\nof dissimilarity matrices converges, then the limit of their embedded\nstructures is the embedded structure of the limiting dissimilarity matrix. But\nwhat if $n$ increases? It then becomes necessary to reformulate MDS so that the\nentire sequence of embedding problems can be viewed as a sequence of\noptimization problems in a fixed space. We present such a reformulation, {\\em\ncontinuous MDS}. Within the continuous MDS framework, we derive two $L^p$\nconsistency results, one for embedding without constraints on the\nconfiguration, the other for embedding subject to {\\em approximate Lipschitz\nconstraints}\\/ that encourage smoothness of the embedding function. The latter\napproach, {\\em Approximate Lipschitz Embedding}\\/ (ALE) is new. Finally, we\ndemonstrate that embedded structures produced by ALE can be interpolated in a\nway that results in uniform convergence.\n", "link": "http://arxiv.org/abs/2402.04436v3", "date": "2024-12-11", "relevancy": 2.3468, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4836}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4705}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Multidimensional%20Scaling&body=Title%3A%20Continuous%20Multidimensional%20Scaling%0AAuthor%3A%20Michael%20W.%20Trosset%20and%20Carey%20E.%20Priebe%0AAbstract%3A%20%20%20Multidimensional%20scaling%20%28MDS%29%20is%20the%20act%20of%20embedding%20proximity%20information%0Aabout%20a%20set%20of%20%24n%24%20objects%20in%20%24d%24-dimensional%20Euclidean%20space.%20As%20originally%0Aconceived%20by%20the%20psychometric%20community%2C%20MDS%20was%20concerned%20with%20embedding%20a%0Afixed%20set%20of%20proximities%20associated%20with%20a%20fixed%20set%20of%20objects.%20Modern%0Aconcerns%2C%20e.g.%2C%20that%20arise%20in%20developing%20asymptotic%20theories%20for%20statistical%0Ainference%20on%20random%20graphs%2C%20more%20typically%20involve%20studying%20the%20limiting%0Abehavior%20of%20a%20sequence%20of%20proximities%20associated%20with%20an%20increasing%20set%20of%0Aobjects.%20Here%20we%20are%20concerned%20with%20embedding%20dissimilarities%20by%20minimizing%0AKruskal%27s%20%281964%29%20raw%20stress%20criterion.%20Standard%20results%20from%20the%20theory%20of%0Apoint-to-set%20maps%20can%20be%20used%20to%20establish%20that%2C%20if%20%24n%24%20is%20fixed%20and%20a%20sequence%0Aof%20dissimilarity%20matrices%20converges%2C%20then%20the%20limit%20of%20their%20embedded%0Astructures%20is%20the%20embedded%20structure%20of%20the%20limiting%20dissimilarity%20matrix.%20But%0Awhat%20if%20%24n%24%20increases%3F%20It%20then%20becomes%20necessary%20to%20reformulate%20MDS%20so%20that%20the%0Aentire%20sequence%20of%20embedding%20problems%20can%20be%20viewed%20as%20a%20sequence%20of%0Aoptimization%20problems%20in%20a%20fixed%20space.%20We%20present%20such%20a%20reformulation%2C%20%7B%5Cem%0Acontinuous%20MDS%7D.%20Within%20the%20continuous%20MDS%20framework%2C%20we%20derive%20two%20%24L%5Ep%24%0Aconsistency%20results%2C%20one%20for%20embedding%20without%20constraints%20on%20the%0Aconfiguration%2C%20the%20other%20for%20embedding%20subject%20to%20%7B%5Cem%20approximate%20Lipschitz%0Aconstraints%7D%5C/%20that%20encourage%20smoothness%20of%20the%20embedding%20function.%20The%20latter%0Aapproach%2C%20%7B%5Cem%20Approximate%20Lipschitz%20Embedding%7D%5C/%20%28ALE%29%20is%20new.%20Finally%2C%20we%0Ademonstrate%20that%20embedded%20structures%20produced%20by%20ALE%20can%20be%20interpolated%20in%20a%0Away%20that%20results%20in%20uniform%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04436v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Multidimensional%2520Scaling%26entry.906535625%3DMichael%2520W.%2520Trosset%2520and%2520Carey%2520E.%2520Priebe%26entry.1292438233%3D%2520%2520Multidimensional%2520scaling%2520%2528MDS%2529%2520is%2520the%2520act%2520of%2520embedding%2520proximity%2520information%250Aabout%2520a%2520set%2520of%2520%2524n%2524%2520objects%2520in%2520%2524d%2524-dimensional%2520Euclidean%2520space.%2520As%2520originally%250Aconceived%2520by%2520the%2520psychometric%2520community%252C%2520MDS%2520was%2520concerned%2520with%2520embedding%2520a%250Afixed%2520set%2520of%2520proximities%2520associated%2520with%2520a%2520fixed%2520set%2520of%2520objects.%2520Modern%250Aconcerns%252C%2520e.g.%252C%2520that%2520arise%2520in%2520developing%2520asymptotic%2520theories%2520for%2520statistical%250Ainference%2520on%2520random%2520graphs%252C%2520more%2520typically%2520involve%2520studying%2520the%2520limiting%250Abehavior%2520of%2520a%2520sequence%2520of%2520proximities%2520associated%2520with%2520an%2520increasing%2520set%2520of%250Aobjects.%2520Here%2520we%2520are%2520concerned%2520with%2520embedding%2520dissimilarities%2520by%2520minimizing%250AKruskal%2527s%2520%25281964%2529%2520raw%2520stress%2520criterion.%2520Standard%2520results%2520from%2520the%2520theory%2520of%250Apoint-to-set%2520maps%2520can%2520be%2520used%2520to%2520establish%2520that%252C%2520if%2520%2524n%2524%2520is%2520fixed%2520and%2520a%2520sequence%250Aof%2520dissimilarity%2520matrices%2520converges%252C%2520then%2520the%2520limit%2520of%2520their%2520embedded%250Astructures%2520is%2520the%2520embedded%2520structure%2520of%2520the%2520limiting%2520dissimilarity%2520matrix.%2520But%250Awhat%2520if%2520%2524n%2524%2520increases%253F%2520It%2520then%2520becomes%2520necessary%2520to%2520reformulate%2520MDS%2520so%2520that%2520the%250Aentire%2520sequence%2520of%2520embedding%2520problems%2520can%2520be%2520viewed%2520as%2520a%2520sequence%2520of%250Aoptimization%2520problems%2520in%2520a%2520fixed%2520space.%2520We%2520present%2520such%2520a%2520reformulation%252C%2520%257B%255Cem%250Acontinuous%2520MDS%257D.%2520Within%2520the%2520continuous%2520MDS%2520framework%252C%2520we%2520derive%2520two%2520%2524L%255Ep%2524%250Aconsistency%2520results%252C%2520one%2520for%2520embedding%2520without%2520constraints%2520on%2520the%250Aconfiguration%252C%2520the%2520other%2520for%2520embedding%2520subject%2520to%2520%257B%255Cem%2520approximate%2520Lipschitz%250Aconstraints%257D%255C/%2520that%2520encourage%2520smoothness%2520of%2520the%2520embedding%2520function.%2520The%2520latter%250Aapproach%252C%2520%257B%255Cem%2520Approximate%2520Lipschitz%2520Embedding%257D%255C/%2520%2528ALE%2529%2520is%2520new.%2520Finally%252C%2520we%250Ademonstrate%2520that%2520embedded%2520structures%2520produced%2520by%2520ALE%2520can%2520be%2520interpolated%2520in%2520a%250Away%2520that%2520results%2520in%2520uniform%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04436v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Multidimensional%20Scaling&entry.906535625=Michael%20W.%20Trosset%20and%20Carey%20E.%20Priebe&entry.1292438233=%20%20Multidimensional%20scaling%20%28MDS%29%20is%20the%20act%20of%20embedding%20proximity%20information%0Aabout%20a%20set%20of%20%24n%24%20objects%20in%20%24d%24-dimensional%20Euclidean%20space.%20As%20originally%0Aconceived%20by%20the%20psychometric%20community%2C%20MDS%20was%20concerned%20with%20embedding%20a%0Afixed%20set%20of%20proximities%20associated%20with%20a%20fixed%20set%20of%20objects.%20Modern%0Aconcerns%2C%20e.g.%2C%20that%20arise%20in%20developing%20asymptotic%20theories%20for%20statistical%0Ainference%20on%20random%20graphs%2C%20more%20typically%20involve%20studying%20the%20limiting%0Abehavior%20of%20a%20sequence%20of%20proximities%20associated%20with%20an%20increasing%20set%20of%0Aobjects.%20Here%20we%20are%20concerned%20with%20embedding%20dissimilarities%20by%20minimizing%0AKruskal%27s%20%281964%29%20raw%20stress%20criterion.%20Standard%20results%20from%20the%20theory%20of%0Apoint-to-set%20maps%20can%20be%20used%20to%20establish%20that%2C%20if%20%24n%24%20is%20fixed%20and%20a%20sequence%0Aof%20dissimilarity%20matrices%20converges%2C%20then%20the%20limit%20of%20their%20embedded%0Astructures%20is%20the%20embedded%20structure%20of%20the%20limiting%20dissimilarity%20matrix.%20But%0Awhat%20if%20%24n%24%20increases%3F%20It%20then%20becomes%20necessary%20to%20reformulate%20MDS%20so%20that%20the%0Aentire%20sequence%20of%20embedding%20problems%20can%20be%20viewed%20as%20a%20sequence%20of%0Aoptimization%20problems%20in%20a%20fixed%20space.%20We%20present%20such%20a%20reformulation%2C%20%7B%5Cem%0Acontinuous%20MDS%7D.%20Within%20the%20continuous%20MDS%20framework%2C%20we%20derive%20two%20%24L%5Ep%24%0Aconsistency%20results%2C%20one%20for%20embedding%20without%20constraints%20on%20the%0Aconfiguration%2C%20the%20other%20for%20embedding%20subject%20to%20%7B%5Cem%20approximate%20Lipschitz%0Aconstraints%7D%5C/%20that%20encourage%20smoothness%20of%20the%20embedding%20function.%20The%20latter%0Aapproach%2C%20%7B%5Cem%20Approximate%20Lipschitz%20Embedding%7D%5C/%20%28ALE%29%20is%20new.%20Finally%2C%20we%0Ademonstrate%20that%20embedded%20structures%20produced%20by%20ALE%20can%20be%20interpolated%20in%20a%0Away%20that%20results%20in%20uniform%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04436v3&entry.124074799=Read"},
{"title": "GenPlan: Generative sequence models as adaptive planners", "author": "Akash Karthikeyan and Yash Vardhan Pant", "abstract": "  Offline reinforcement learning has shown tremendous success in behavioral\nplanning by learning from previously collected demonstrations. However,\ndecision-making in multitask missions still presents significant challenges.\nFor instance, a mission might require an agent to explore an unknown\nenvironment, discover goals, and navigate to them, even if it involves\ninteracting with obstacles along the way. Such behavioral planning problems are\ndifficult to solve due to: a) agents failing to adapt beyond the single task\nlearned through their reward function, and b) the inability to generalize to\nnew environments not covered in the training demonstrations, e.g., environments\nwhere all doors were unlocked in the demonstrations. Consequently,\nstate-of-the-art decision making methods are limited to missions where the\nrequired tasks are well-represented in the training demonstrations and can be\nsolved within a short (temporal) planning horizon. To address this, we propose\nGenPlan: a stochastic and adaptive planner that leverages discrete-flow models\nfor generative sequence modeling, enabling sample-efficient exploration and\nexploitation. This framework relies on an iterative denoising procedure to\ngenerate a sequence of goals and actions. This approach captures multi-modal\naction distributions and facilitates goal and task discovery, thereby enhancing\ngeneralization to out-of-distribution tasks and environments, i.e., missions\nnot part of the training data. We demonstrate the effectiveness of our method\nthrough multiple simulation environments. Notably, GenPlan outperforms the\nstate-of-the-art methods by over 10% on adaptive planning tasks, where the\nagent adapts to multi-task missions while leveraging demonstrations on\nsingle-goal-reaching tasks.\n", "link": "http://arxiv.org/abs/2412.08565v1", "date": "2024-12-11", "relevancy": 2.3383, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6342}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5787}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenPlan%3A%20Generative%20sequence%20models%20as%20adaptive%20planners&body=Title%3A%20GenPlan%3A%20Generative%20sequence%20models%20as%20adaptive%20planners%0AAuthor%3A%20Akash%20Karthikeyan%20and%20Yash%20Vardhan%20Pant%0AAbstract%3A%20%20%20Offline%20reinforcement%20learning%20has%20shown%20tremendous%20success%20in%20behavioral%0Aplanning%20by%20learning%20from%20previously%20collected%20demonstrations.%20However%2C%0Adecision-making%20in%20multitask%20missions%20still%20presents%20significant%20challenges.%0AFor%20instance%2C%20a%20mission%20might%20require%20an%20agent%20to%20explore%20an%20unknown%0Aenvironment%2C%20discover%20goals%2C%20and%20navigate%20to%20them%2C%20even%20if%20it%20involves%0Ainteracting%20with%20obstacles%20along%20the%20way.%20Such%20behavioral%20planning%20problems%20are%0Adifficult%20to%20solve%20due%20to%3A%20a%29%20agents%20failing%20to%20adapt%20beyond%20the%20single%20task%0Alearned%20through%20their%20reward%20function%2C%20and%20b%29%20the%20inability%20to%20generalize%20to%0Anew%20environments%20not%20covered%20in%20the%20training%20demonstrations%2C%20e.g.%2C%20environments%0Awhere%20all%20doors%20were%20unlocked%20in%20the%20demonstrations.%20Consequently%2C%0Astate-of-the-art%20decision%20making%20methods%20are%20limited%20to%20missions%20where%20the%0Arequired%20tasks%20are%20well-represented%20in%20the%20training%20demonstrations%20and%20can%20be%0Asolved%20within%20a%20short%20%28temporal%29%20planning%20horizon.%20To%20address%20this%2C%20we%20propose%0AGenPlan%3A%20a%20stochastic%20and%20adaptive%20planner%20that%20leverages%20discrete-flow%20models%0Afor%20generative%20sequence%20modeling%2C%20enabling%20sample-efficient%20exploration%20and%0Aexploitation.%20This%20framework%20relies%20on%20an%20iterative%20denoising%20procedure%20to%0Agenerate%20a%20sequence%20of%20goals%20and%20actions.%20This%20approach%20captures%20multi-modal%0Aaction%20distributions%20and%20facilitates%20goal%20and%20task%20discovery%2C%20thereby%20enhancing%0Ageneralization%20to%20out-of-distribution%20tasks%20and%20environments%2C%20i.e.%2C%20missions%0Anot%20part%20of%20the%20training%20data.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%0Athrough%20multiple%20simulation%20environments.%20Notably%2C%20GenPlan%20outperforms%20the%0Astate-of-the-art%20methods%20by%20over%2010%25%20on%20adaptive%20planning%20tasks%2C%20where%20the%0Aagent%20adapts%20to%20multi-task%20missions%20while%20leveraging%20demonstrations%20on%0Asingle-goal-reaching%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenPlan%253A%2520Generative%2520sequence%2520models%2520as%2520adaptive%2520planners%26entry.906535625%3DAkash%2520Karthikeyan%2520and%2520Yash%2520Vardhan%2520Pant%26entry.1292438233%3D%2520%2520Offline%2520reinforcement%2520learning%2520has%2520shown%2520tremendous%2520success%2520in%2520behavioral%250Aplanning%2520by%2520learning%2520from%2520previously%2520collected%2520demonstrations.%2520However%252C%250Adecision-making%2520in%2520multitask%2520missions%2520still%2520presents%2520significant%2520challenges.%250AFor%2520instance%252C%2520a%2520mission%2520might%2520require%2520an%2520agent%2520to%2520explore%2520an%2520unknown%250Aenvironment%252C%2520discover%2520goals%252C%2520and%2520navigate%2520to%2520them%252C%2520even%2520if%2520it%2520involves%250Ainteracting%2520with%2520obstacles%2520along%2520the%2520way.%2520Such%2520behavioral%2520planning%2520problems%2520are%250Adifficult%2520to%2520solve%2520due%2520to%253A%2520a%2529%2520agents%2520failing%2520to%2520adapt%2520beyond%2520the%2520single%2520task%250Alearned%2520through%2520their%2520reward%2520function%252C%2520and%2520b%2529%2520the%2520inability%2520to%2520generalize%2520to%250Anew%2520environments%2520not%2520covered%2520in%2520the%2520training%2520demonstrations%252C%2520e.g.%252C%2520environments%250Awhere%2520all%2520doors%2520were%2520unlocked%2520in%2520the%2520demonstrations.%2520Consequently%252C%250Astate-of-the-art%2520decision%2520making%2520methods%2520are%2520limited%2520to%2520missions%2520where%2520the%250Arequired%2520tasks%2520are%2520well-represented%2520in%2520the%2520training%2520demonstrations%2520and%2520can%2520be%250Asolved%2520within%2520a%2520short%2520%2528temporal%2529%2520planning%2520horizon.%2520To%2520address%2520this%252C%2520we%2520propose%250AGenPlan%253A%2520a%2520stochastic%2520and%2520adaptive%2520planner%2520that%2520leverages%2520discrete-flow%2520models%250Afor%2520generative%2520sequence%2520modeling%252C%2520enabling%2520sample-efficient%2520exploration%2520and%250Aexploitation.%2520This%2520framework%2520relies%2520on%2520an%2520iterative%2520denoising%2520procedure%2520to%250Agenerate%2520a%2520sequence%2520of%2520goals%2520and%2520actions.%2520This%2520approach%2520captures%2520multi-modal%250Aaction%2520distributions%2520and%2520facilitates%2520goal%2520and%2520task%2520discovery%252C%2520thereby%2520enhancing%250Ageneralization%2520to%2520out-of-distribution%2520tasks%2520and%2520environments%252C%2520i.e.%252C%2520missions%250Anot%2520part%2520of%2520the%2520training%2520data.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%250Athrough%2520multiple%2520simulation%2520environments.%2520Notably%252C%2520GenPlan%2520outperforms%2520the%250Astate-of-the-art%2520methods%2520by%2520over%252010%2525%2520on%2520adaptive%2520planning%2520tasks%252C%2520where%2520the%250Aagent%2520adapts%2520to%2520multi-task%2520missions%2520while%2520leveraging%2520demonstrations%2520on%250Asingle-goal-reaching%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenPlan%3A%20Generative%20sequence%20models%20as%20adaptive%20planners&entry.906535625=Akash%20Karthikeyan%20and%20Yash%20Vardhan%20Pant&entry.1292438233=%20%20Offline%20reinforcement%20learning%20has%20shown%20tremendous%20success%20in%20behavioral%0Aplanning%20by%20learning%20from%20previously%20collected%20demonstrations.%20However%2C%0Adecision-making%20in%20multitask%20missions%20still%20presents%20significant%20challenges.%0AFor%20instance%2C%20a%20mission%20might%20require%20an%20agent%20to%20explore%20an%20unknown%0Aenvironment%2C%20discover%20goals%2C%20and%20navigate%20to%20them%2C%20even%20if%20it%20involves%0Ainteracting%20with%20obstacles%20along%20the%20way.%20Such%20behavioral%20planning%20problems%20are%0Adifficult%20to%20solve%20due%20to%3A%20a%29%20agents%20failing%20to%20adapt%20beyond%20the%20single%20task%0Alearned%20through%20their%20reward%20function%2C%20and%20b%29%20the%20inability%20to%20generalize%20to%0Anew%20environments%20not%20covered%20in%20the%20training%20demonstrations%2C%20e.g.%2C%20environments%0Awhere%20all%20doors%20were%20unlocked%20in%20the%20demonstrations.%20Consequently%2C%0Astate-of-the-art%20decision%20making%20methods%20are%20limited%20to%20missions%20where%20the%0Arequired%20tasks%20are%20well-represented%20in%20the%20training%20demonstrations%20and%20can%20be%0Asolved%20within%20a%20short%20%28temporal%29%20planning%20horizon.%20To%20address%20this%2C%20we%20propose%0AGenPlan%3A%20a%20stochastic%20and%20adaptive%20planner%20that%20leverages%20discrete-flow%20models%0Afor%20generative%20sequence%20modeling%2C%20enabling%20sample-efficient%20exploration%20and%0Aexploitation.%20This%20framework%20relies%20on%20an%20iterative%20denoising%20procedure%20to%0Agenerate%20a%20sequence%20of%20goals%20and%20actions.%20This%20approach%20captures%20multi-modal%0Aaction%20distributions%20and%20facilitates%20goal%20and%20task%20discovery%2C%20thereby%20enhancing%0Ageneralization%20to%20out-of-distribution%20tasks%20and%20environments%2C%20i.e.%2C%20missions%0Anot%20part%20of%20the%20training%20data.%20We%20demonstrate%20the%20effectiveness%20of%20our%20method%0Athrough%20multiple%20simulation%20environments.%20Notably%2C%20GenPlan%20outperforms%20the%0Astate-of-the-art%20methods%20by%20over%2010%25%20on%20adaptive%20planning%20tasks%2C%20where%20the%0Aagent%20adapts%20to%20multi-task%20missions%20while%20leveraging%20demonstrations%20on%0Asingle-goal-reaching%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08565v1&entry.124074799=Read"},
{"title": "An End-to-End Collaborative Learning Approach for Connected Autonomous\n  Vehicles in Occluded Scenarios", "author": "Leandro Parada and Hanlin Tian and Jose Escribano and Panagiotis Angeloudis", "abstract": "  Collaborative navigation becomes essential in situations of occluded\nscenarios in autonomous driving where independent driving policies are likely\nto lead to collisions. One promising approach to address this issue is through\nthe use of Vehicle-to-Vehicle (V2V) networks that allow for the sharing of\nperception information with nearby agents, preventing catastrophic accidents.\nIn this article, we propose a collaborative control method based on a V2V\nnetwork for sharing compressed LiDAR features and employing Proximal Policy\nOptimisation to train safe and efficient navigation policies. Unlike previous\napproaches that rely on expert data (behaviour cloning), our proposed approach\nlearns the multi-agent policies directly from experience in the occluded\nenvironment, while effectively meeting bandwidth limitations. The proposed\nmethod first prepossesses LiDAR point cloud data to obtain meaningful features\nthrough a convolutional neural network and then shares them with nearby CAVs to\nalert for potentially dangerous situations. To evaluate the proposed method, we\ndeveloped an occluded intersection gym environment based on the CARLA\nautonomous driving simulator, allowing real-time data sharing among agents. Our\nexperimental results demonstrate the consistent superiority of our\ncollaborative control method over an independent reinforcement learning method\nand a cooperative early fusion method.\n", "link": "http://arxiv.org/abs/2412.08562v1", "date": "2024-12-11", "relevancy": 2.338, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5921}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5901}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20End-to-End%20Collaborative%20Learning%20Approach%20for%20Connected%20Autonomous%0A%20%20Vehicles%20in%20Occluded%20Scenarios&body=Title%3A%20An%20End-to-End%20Collaborative%20Learning%20Approach%20for%20Connected%20Autonomous%0A%20%20Vehicles%20in%20Occluded%20Scenarios%0AAuthor%3A%20Leandro%20Parada%20and%20Hanlin%20Tian%20and%20Jose%20Escribano%20and%20Panagiotis%20Angeloudis%0AAbstract%3A%20%20%20Collaborative%20navigation%20becomes%20essential%20in%20situations%20of%20occluded%0Ascenarios%20in%20autonomous%20driving%20where%20independent%20driving%20policies%20are%20likely%0Ato%20lead%20to%20collisions.%20One%20promising%20approach%20to%20address%20this%20issue%20is%20through%0Athe%20use%20of%20Vehicle-to-Vehicle%20%28V2V%29%20networks%20that%20allow%20for%20the%20sharing%20of%0Aperception%20information%20with%20nearby%20agents%2C%20preventing%20catastrophic%20accidents.%0AIn%20this%20article%2C%20we%20propose%20a%20collaborative%20control%20method%20based%20on%20a%20V2V%0Anetwork%20for%20sharing%20compressed%20LiDAR%20features%20and%20employing%20Proximal%20Policy%0AOptimisation%20to%20train%20safe%20and%20efficient%20navigation%20policies.%20Unlike%20previous%0Aapproaches%20that%20rely%20on%20expert%20data%20%28behaviour%20cloning%29%2C%20our%20proposed%20approach%0Alearns%20the%20multi-agent%20policies%20directly%20from%20experience%20in%20the%20occluded%0Aenvironment%2C%20while%20effectively%20meeting%20bandwidth%20limitations.%20The%20proposed%0Amethod%20first%20prepossesses%20LiDAR%20point%20cloud%20data%20to%20obtain%20meaningful%20features%0Athrough%20a%20convolutional%20neural%20network%20and%20then%20shares%20them%20with%20nearby%20CAVs%20to%0Aalert%20for%20potentially%20dangerous%20situations.%20To%20evaluate%20the%20proposed%20method%2C%20we%0Adeveloped%20an%20occluded%20intersection%20gym%20environment%20based%20on%20the%20CARLA%0Aautonomous%20driving%20simulator%2C%20allowing%20real-time%20data%20sharing%20among%20agents.%20Our%0Aexperimental%20results%20demonstrate%20the%20consistent%20superiority%20of%20our%0Acollaborative%20control%20method%20over%20an%20independent%20reinforcement%20learning%20method%0Aand%20a%20cooperative%20early%20fusion%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520End-to-End%2520Collaborative%2520Learning%2520Approach%2520for%2520Connected%2520Autonomous%250A%2520%2520Vehicles%2520in%2520Occluded%2520Scenarios%26entry.906535625%3DLeandro%2520Parada%2520and%2520Hanlin%2520Tian%2520and%2520Jose%2520Escribano%2520and%2520Panagiotis%2520Angeloudis%26entry.1292438233%3D%2520%2520Collaborative%2520navigation%2520becomes%2520essential%2520in%2520situations%2520of%2520occluded%250Ascenarios%2520in%2520autonomous%2520driving%2520where%2520independent%2520driving%2520policies%2520are%2520likely%250Ato%2520lead%2520to%2520collisions.%2520One%2520promising%2520approach%2520to%2520address%2520this%2520issue%2520is%2520through%250Athe%2520use%2520of%2520Vehicle-to-Vehicle%2520%2528V2V%2529%2520networks%2520that%2520allow%2520for%2520the%2520sharing%2520of%250Aperception%2520information%2520with%2520nearby%2520agents%252C%2520preventing%2520catastrophic%2520accidents.%250AIn%2520this%2520article%252C%2520we%2520propose%2520a%2520collaborative%2520control%2520method%2520based%2520on%2520a%2520V2V%250Anetwork%2520for%2520sharing%2520compressed%2520LiDAR%2520features%2520and%2520employing%2520Proximal%2520Policy%250AOptimisation%2520to%2520train%2520safe%2520and%2520efficient%2520navigation%2520policies.%2520Unlike%2520previous%250Aapproaches%2520that%2520rely%2520on%2520expert%2520data%2520%2528behaviour%2520cloning%2529%252C%2520our%2520proposed%2520approach%250Alearns%2520the%2520multi-agent%2520policies%2520directly%2520from%2520experience%2520in%2520the%2520occluded%250Aenvironment%252C%2520while%2520effectively%2520meeting%2520bandwidth%2520limitations.%2520The%2520proposed%250Amethod%2520first%2520prepossesses%2520LiDAR%2520point%2520cloud%2520data%2520to%2520obtain%2520meaningful%2520features%250Athrough%2520a%2520convolutional%2520neural%2520network%2520and%2520then%2520shares%2520them%2520with%2520nearby%2520CAVs%2520to%250Aalert%2520for%2520potentially%2520dangerous%2520situations.%2520To%2520evaluate%2520the%2520proposed%2520method%252C%2520we%250Adeveloped%2520an%2520occluded%2520intersection%2520gym%2520environment%2520based%2520on%2520the%2520CARLA%250Aautonomous%2520driving%2520simulator%252C%2520allowing%2520real-time%2520data%2520sharing%2520among%2520agents.%2520Our%250Aexperimental%2520results%2520demonstrate%2520the%2520consistent%2520superiority%2520of%2520our%250Acollaborative%2520control%2520method%2520over%2520an%2520independent%2520reinforcement%2520learning%2520method%250Aand%2520a%2520cooperative%2520early%2520fusion%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20End-to-End%20Collaborative%20Learning%20Approach%20for%20Connected%20Autonomous%0A%20%20Vehicles%20in%20Occluded%20Scenarios&entry.906535625=Leandro%20Parada%20and%20Hanlin%20Tian%20and%20Jose%20Escribano%20and%20Panagiotis%20Angeloudis&entry.1292438233=%20%20Collaborative%20navigation%20becomes%20essential%20in%20situations%20of%20occluded%0Ascenarios%20in%20autonomous%20driving%20where%20independent%20driving%20policies%20are%20likely%0Ato%20lead%20to%20collisions.%20One%20promising%20approach%20to%20address%20this%20issue%20is%20through%0Athe%20use%20of%20Vehicle-to-Vehicle%20%28V2V%29%20networks%20that%20allow%20for%20the%20sharing%20of%0Aperception%20information%20with%20nearby%20agents%2C%20preventing%20catastrophic%20accidents.%0AIn%20this%20article%2C%20we%20propose%20a%20collaborative%20control%20method%20based%20on%20a%20V2V%0Anetwork%20for%20sharing%20compressed%20LiDAR%20features%20and%20employing%20Proximal%20Policy%0AOptimisation%20to%20train%20safe%20and%20efficient%20navigation%20policies.%20Unlike%20previous%0Aapproaches%20that%20rely%20on%20expert%20data%20%28behaviour%20cloning%29%2C%20our%20proposed%20approach%0Alearns%20the%20multi-agent%20policies%20directly%20from%20experience%20in%20the%20occluded%0Aenvironment%2C%20while%20effectively%20meeting%20bandwidth%20limitations.%20The%20proposed%0Amethod%20first%20prepossesses%20LiDAR%20point%20cloud%20data%20to%20obtain%20meaningful%20features%0Athrough%20a%20convolutional%20neural%20network%20and%20then%20shares%20them%20with%20nearby%20CAVs%20to%0Aalert%20for%20potentially%20dangerous%20situations.%20To%20evaluate%20the%20proposed%20method%2C%20we%0Adeveloped%20an%20occluded%20intersection%20gym%20environment%20based%20on%20the%20CARLA%0Aautonomous%20driving%20simulator%2C%20allowing%20real-time%20data%20sharing%20among%20agents.%20Our%0Aexperimental%20results%20demonstrate%20the%20consistent%20superiority%20of%20our%0Acollaborative%20control%20method%20over%20an%20independent%20reinforcement%20learning%20method%0Aand%20a%20cooperative%20early%20fusion%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08562v1&entry.124074799=Read"},
{"title": "SPACE-SUIT: An Artificial Intelligence based chromospheric feature\n  extractor and classifier for SUIT", "author": "Pranava Seth and Vishal Upendran and Megha Anand and Janmejoy Sarkar and Soumya Roy and Priyadarshan Chaki and Pratyay Chowdhury and Borishan Ghosh and Durgesh Tripathi", "abstract": "  The Solar Ultraviolet Imaging Telescope(SUIT) onboard Aditya-L1 is an imager\nthat observes the solar photosphere and chromosphere through observations in\nthe wavelength range of 200-400 nm. A comprehensive understanding of the plasma\nand thermodynamic properties of chromospheric and photospheric morphological\nstructures requires a large sample statistical study, necessitating the\ndevelopment of automatic feature detection methods. To this end, we develop the\nfeature detection algorithm SPACE-SUIT: Solar Phenomena Analysis and\nClassification using Enhanced vision techniques for SUIT, to detect and\nclassify the solar chromospheric features to be observed from SUIT's Mg II k\nfilter. Specifically, we target plage regions, sunspots, filaments, and\noff-limb structures. SPACE uses You Only Look Once(YOLO), a neural\nnetwork-based model to identify regions of interest. We train and validate\nSPACE using mock-SUIT images developed from Interface Region Imaging\nSpectrometer(IRIS) full-disk mosaic images in Mg II k line, while we also\nperform detection on Level-1 SUIT data. SPACE achieves an approximate precision\nof 0.788, recall 0.863 and MAP of 0.874 on the validation mock SUIT FITS\ndataset. Given the manual labeling of our dataset, we perform \"self-validation\"\nby applying statistical measures and Tamura features on the ground truth and\npredicted bounding boxes. We find the distributions of entropy, contrast,\ndissimilarity, and energy to show differences in the features. These\ndifferences are qualitatively captured by the detected regions predicted by\nSPACE and validated with the observed SUIT images, even in the absence of\nlabeled ground truth. This work not only develops a chromospheric feature\nextractor but also demonstrates the effectiveness of statistical metrics and\nTamura features for distinguishing chromospheric features, offering independent\nvalidation for future detection schemes.\n", "link": "http://arxiv.org/abs/2412.08589v1", "date": "2024-12-11", "relevancy": 2.3277, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4534}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPACE-SUIT%3A%20An%20Artificial%20Intelligence%20based%20chromospheric%20feature%0A%20%20extractor%20and%20classifier%20for%20SUIT&body=Title%3A%20SPACE-SUIT%3A%20An%20Artificial%20Intelligence%20based%20chromospheric%20feature%0A%20%20extractor%20and%20classifier%20for%20SUIT%0AAuthor%3A%20Pranava%20Seth%20and%20Vishal%20Upendran%20and%20Megha%20Anand%20and%20Janmejoy%20Sarkar%20and%20Soumya%20Roy%20and%20Priyadarshan%20Chaki%20and%20Pratyay%20Chowdhury%20and%20Borishan%20Ghosh%20and%20Durgesh%20Tripathi%0AAbstract%3A%20%20%20The%20Solar%20Ultraviolet%20Imaging%20Telescope%28SUIT%29%20onboard%20Aditya-L1%20is%20an%20imager%0Athat%20observes%20the%20solar%20photosphere%20and%20chromosphere%20through%20observations%20in%0Athe%20wavelength%20range%20of%20200-400%20nm.%20A%20comprehensive%20understanding%20of%20the%20plasma%0Aand%20thermodynamic%20properties%20of%20chromospheric%20and%20photospheric%20morphological%0Astructures%20requires%20a%20large%20sample%20statistical%20study%2C%20necessitating%20the%0Adevelopment%20of%20automatic%20feature%20detection%20methods.%20To%20this%20end%2C%20we%20develop%20the%0Afeature%20detection%20algorithm%20SPACE-SUIT%3A%20Solar%20Phenomena%20Analysis%20and%0AClassification%20using%20Enhanced%20vision%20techniques%20for%20SUIT%2C%20to%20detect%20and%0Aclassify%20the%20solar%20chromospheric%20features%20to%20be%20observed%20from%20SUIT%27s%20Mg%20II%20k%0Afilter.%20Specifically%2C%20we%20target%20plage%20regions%2C%20sunspots%2C%20filaments%2C%20and%0Aoff-limb%20structures.%20SPACE%20uses%20You%20Only%20Look%20Once%28YOLO%29%2C%20a%20neural%0Anetwork-based%20model%20to%20identify%20regions%20of%20interest.%20We%20train%20and%20validate%0ASPACE%20using%20mock-SUIT%20images%20developed%20from%20Interface%20Region%20Imaging%0ASpectrometer%28IRIS%29%20full-disk%20mosaic%20images%20in%20Mg%20II%20k%20line%2C%20while%20we%20also%0Aperform%20detection%20on%20Level-1%20SUIT%20data.%20SPACE%20achieves%20an%20approximate%20precision%0Aof%200.788%2C%20recall%200.863%20and%20MAP%20of%200.874%20on%20the%20validation%20mock%20SUIT%20FITS%0Adataset.%20Given%20the%20manual%20labeling%20of%20our%20dataset%2C%20we%20perform%20%22self-validation%22%0Aby%20applying%20statistical%20measures%20and%20Tamura%20features%20on%20the%20ground%20truth%20and%0Apredicted%20bounding%20boxes.%20We%20find%20the%20distributions%20of%20entropy%2C%20contrast%2C%0Adissimilarity%2C%20and%20energy%20to%20show%20differences%20in%20the%20features.%20These%0Adifferences%20are%20qualitatively%20captured%20by%20the%20detected%20regions%20predicted%20by%0ASPACE%20and%20validated%20with%20the%20observed%20SUIT%20images%2C%20even%20in%20the%20absence%20of%0Alabeled%20ground%20truth.%20This%20work%20not%20only%20develops%20a%20chromospheric%20feature%0Aextractor%20but%20also%20demonstrates%20the%20effectiveness%20of%20statistical%20metrics%20and%0ATamura%20features%20for%20distinguishing%20chromospheric%20features%2C%20offering%20independent%0Avalidation%20for%20future%20detection%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPACE-SUIT%253A%2520An%2520Artificial%2520Intelligence%2520based%2520chromospheric%2520feature%250A%2520%2520extractor%2520and%2520classifier%2520for%2520SUIT%26entry.906535625%3DPranava%2520Seth%2520and%2520Vishal%2520Upendran%2520and%2520Megha%2520Anand%2520and%2520Janmejoy%2520Sarkar%2520and%2520Soumya%2520Roy%2520and%2520Priyadarshan%2520Chaki%2520and%2520Pratyay%2520Chowdhury%2520and%2520Borishan%2520Ghosh%2520and%2520Durgesh%2520Tripathi%26entry.1292438233%3D%2520%2520The%2520Solar%2520Ultraviolet%2520Imaging%2520Telescope%2528SUIT%2529%2520onboard%2520Aditya-L1%2520is%2520an%2520imager%250Athat%2520observes%2520the%2520solar%2520photosphere%2520and%2520chromosphere%2520through%2520observations%2520in%250Athe%2520wavelength%2520range%2520of%2520200-400%2520nm.%2520A%2520comprehensive%2520understanding%2520of%2520the%2520plasma%250Aand%2520thermodynamic%2520properties%2520of%2520chromospheric%2520and%2520photospheric%2520morphological%250Astructures%2520requires%2520a%2520large%2520sample%2520statistical%2520study%252C%2520necessitating%2520the%250Adevelopment%2520of%2520automatic%2520feature%2520detection%2520methods.%2520To%2520this%2520end%252C%2520we%2520develop%2520the%250Afeature%2520detection%2520algorithm%2520SPACE-SUIT%253A%2520Solar%2520Phenomena%2520Analysis%2520and%250AClassification%2520using%2520Enhanced%2520vision%2520techniques%2520for%2520SUIT%252C%2520to%2520detect%2520and%250Aclassify%2520the%2520solar%2520chromospheric%2520features%2520to%2520be%2520observed%2520from%2520SUIT%2527s%2520Mg%2520II%2520k%250Afilter.%2520Specifically%252C%2520we%2520target%2520plage%2520regions%252C%2520sunspots%252C%2520filaments%252C%2520and%250Aoff-limb%2520structures.%2520SPACE%2520uses%2520You%2520Only%2520Look%2520Once%2528YOLO%2529%252C%2520a%2520neural%250Anetwork-based%2520model%2520to%2520identify%2520regions%2520of%2520interest.%2520We%2520train%2520and%2520validate%250ASPACE%2520using%2520mock-SUIT%2520images%2520developed%2520from%2520Interface%2520Region%2520Imaging%250ASpectrometer%2528IRIS%2529%2520full-disk%2520mosaic%2520images%2520in%2520Mg%2520II%2520k%2520line%252C%2520while%2520we%2520also%250Aperform%2520detection%2520on%2520Level-1%2520SUIT%2520data.%2520SPACE%2520achieves%2520an%2520approximate%2520precision%250Aof%25200.788%252C%2520recall%25200.863%2520and%2520MAP%2520of%25200.874%2520on%2520the%2520validation%2520mock%2520SUIT%2520FITS%250Adataset.%2520Given%2520the%2520manual%2520labeling%2520of%2520our%2520dataset%252C%2520we%2520perform%2520%2522self-validation%2522%250Aby%2520applying%2520statistical%2520measures%2520and%2520Tamura%2520features%2520on%2520the%2520ground%2520truth%2520and%250Apredicted%2520bounding%2520boxes.%2520We%2520find%2520the%2520distributions%2520of%2520entropy%252C%2520contrast%252C%250Adissimilarity%252C%2520and%2520energy%2520to%2520show%2520differences%2520in%2520the%2520features.%2520These%250Adifferences%2520are%2520qualitatively%2520captured%2520by%2520the%2520detected%2520regions%2520predicted%2520by%250ASPACE%2520and%2520validated%2520with%2520the%2520observed%2520SUIT%2520images%252C%2520even%2520in%2520the%2520absence%2520of%250Alabeled%2520ground%2520truth.%2520This%2520work%2520not%2520only%2520develops%2520a%2520chromospheric%2520feature%250Aextractor%2520but%2520also%2520demonstrates%2520the%2520effectiveness%2520of%2520statistical%2520metrics%2520and%250ATamura%2520features%2520for%2520distinguishing%2520chromospheric%2520features%252C%2520offering%2520independent%250Avalidation%2520for%2520future%2520detection%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPACE-SUIT%3A%20An%20Artificial%20Intelligence%20based%20chromospheric%20feature%0A%20%20extractor%20and%20classifier%20for%20SUIT&entry.906535625=Pranava%20Seth%20and%20Vishal%20Upendran%20and%20Megha%20Anand%20and%20Janmejoy%20Sarkar%20and%20Soumya%20Roy%20and%20Priyadarshan%20Chaki%20and%20Pratyay%20Chowdhury%20and%20Borishan%20Ghosh%20and%20Durgesh%20Tripathi&entry.1292438233=%20%20The%20Solar%20Ultraviolet%20Imaging%20Telescope%28SUIT%29%20onboard%20Aditya-L1%20is%20an%20imager%0Athat%20observes%20the%20solar%20photosphere%20and%20chromosphere%20through%20observations%20in%0Athe%20wavelength%20range%20of%20200-400%20nm.%20A%20comprehensive%20understanding%20of%20the%20plasma%0Aand%20thermodynamic%20properties%20of%20chromospheric%20and%20photospheric%20morphological%0Astructures%20requires%20a%20large%20sample%20statistical%20study%2C%20necessitating%20the%0Adevelopment%20of%20automatic%20feature%20detection%20methods.%20To%20this%20end%2C%20we%20develop%20the%0Afeature%20detection%20algorithm%20SPACE-SUIT%3A%20Solar%20Phenomena%20Analysis%20and%0AClassification%20using%20Enhanced%20vision%20techniques%20for%20SUIT%2C%20to%20detect%20and%0Aclassify%20the%20solar%20chromospheric%20features%20to%20be%20observed%20from%20SUIT%27s%20Mg%20II%20k%0Afilter.%20Specifically%2C%20we%20target%20plage%20regions%2C%20sunspots%2C%20filaments%2C%20and%0Aoff-limb%20structures.%20SPACE%20uses%20You%20Only%20Look%20Once%28YOLO%29%2C%20a%20neural%0Anetwork-based%20model%20to%20identify%20regions%20of%20interest.%20We%20train%20and%20validate%0ASPACE%20using%20mock-SUIT%20images%20developed%20from%20Interface%20Region%20Imaging%0ASpectrometer%28IRIS%29%20full-disk%20mosaic%20images%20in%20Mg%20II%20k%20line%2C%20while%20we%20also%0Aperform%20detection%20on%20Level-1%20SUIT%20data.%20SPACE%20achieves%20an%20approximate%20precision%0Aof%200.788%2C%20recall%200.863%20and%20MAP%20of%200.874%20on%20the%20validation%20mock%20SUIT%20FITS%0Adataset.%20Given%20the%20manual%20labeling%20of%20our%20dataset%2C%20we%20perform%20%22self-validation%22%0Aby%20applying%20statistical%20measures%20and%20Tamura%20features%20on%20the%20ground%20truth%20and%0Apredicted%20bounding%20boxes.%20We%20find%20the%20distributions%20of%20entropy%2C%20contrast%2C%0Adissimilarity%2C%20and%20energy%20to%20show%20differences%20in%20the%20features.%20These%0Adifferences%20are%20qualitatively%20captured%20by%20the%20detected%20regions%20predicted%20by%0ASPACE%20and%20validated%20with%20the%20observed%20SUIT%20images%2C%20even%20in%20the%20absence%20of%0Alabeled%20ground%20truth.%20This%20work%20not%20only%20develops%20a%20chromospheric%20feature%0Aextractor%20but%20also%20demonstrates%20the%20effectiveness%20of%20statistical%20metrics%20and%0ATamura%20features%20for%20distinguishing%20chromospheric%20features%2C%20offering%20independent%0Avalidation%20for%20future%20detection%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08589v1&entry.124074799=Read"},
{"title": "Video Summarization using Denoising Diffusion Probabilistic Model", "author": "Zirui Shang and Yubo Zhu and Hongxi Li and Shuo yang and Xinxiao Wu", "abstract": "  Video summarization aims to eliminate visual redundancy while retaining key\nparts of video to construct concise and comprehensive synopses. Most existing\nmethods use discriminative models to predict the importance scores of video\nframes. However, these methods are susceptible to annotation inconsistency\ncaused by the inherent subjectivity of different annotators when annotating the\nsame video. In this paper, we introduce a generative framework for video\nsummarization that learns how to generate summaries from a probability\ndistribution perspective, effectively reducing the interference of subjective\nannotation noise. Specifically, we propose a novel diffusion summarization\nmethod based on the Denoising Diffusion Probabilistic Model (DDPM), which\nlearns the probability distribution of training data through noise prediction,\nand generates summaries by iterative denoising. Our method is more resistant to\nsubjective annotation noise, and is less prone to overfitting the training data\nthan discriminative methods, with strong generalization ability. Moreover, to\nfacilitate training DDPM with limited data, we employ an unsupervised video\nsummarization model to implement the earlier denoising process. Extensive\nexperiments on various datasets (TVSum, SumMe, and FPVSum) demonstrate the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2412.08357v1", "date": "2024-12-11", "relevancy": 2.3237, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6011}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5673}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Summarization%20using%20Denoising%20Diffusion%20Probabilistic%20Model&body=Title%3A%20Video%20Summarization%20using%20Denoising%20Diffusion%20Probabilistic%20Model%0AAuthor%3A%20Zirui%20Shang%20and%20Yubo%20Zhu%20and%20Hongxi%20Li%20and%20Shuo%20yang%20and%20Xinxiao%20Wu%0AAbstract%3A%20%20%20Video%20summarization%20aims%20to%20eliminate%20visual%20redundancy%20while%20retaining%20key%0Aparts%20of%20video%20to%20construct%20concise%20and%20comprehensive%20synopses.%20Most%20existing%0Amethods%20use%20discriminative%20models%20to%20predict%20the%20importance%20scores%20of%20video%0Aframes.%20However%2C%20these%20methods%20are%20susceptible%20to%20annotation%20inconsistency%0Acaused%20by%20the%20inherent%20subjectivity%20of%20different%20annotators%20when%20annotating%20the%0Asame%20video.%20In%20this%20paper%2C%20we%20introduce%20a%20generative%20framework%20for%20video%0Asummarization%20that%20learns%20how%20to%20generate%20summaries%20from%20a%20probability%0Adistribution%20perspective%2C%20effectively%20reducing%20the%20interference%20of%20subjective%0Aannotation%20noise.%20Specifically%2C%20we%20propose%20a%20novel%20diffusion%20summarization%0Amethod%20based%20on%20the%20Denoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%2C%20which%0Alearns%20the%20probability%20distribution%20of%20training%20data%20through%20noise%20prediction%2C%0Aand%20generates%20summaries%20by%20iterative%20denoising.%20Our%20method%20is%20more%20resistant%20to%0Asubjective%20annotation%20noise%2C%20and%20is%20less%20prone%20to%20overfitting%20the%20training%20data%0Athan%20discriminative%20methods%2C%20with%20strong%20generalization%20ability.%20Moreover%2C%20to%0Afacilitate%20training%20DDPM%20with%20limited%20data%2C%20we%20employ%20an%20unsupervised%20video%0Asummarization%20model%20to%20implement%20the%20earlier%20denoising%20process.%20Extensive%0Aexperiments%20on%20various%20datasets%20%28TVSum%2C%20SumMe%2C%20and%20FPVSum%29%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Summarization%2520using%2520Denoising%2520Diffusion%2520Probabilistic%2520Model%26entry.906535625%3DZirui%2520Shang%2520and%2520Yubo%2520Zhu%2520and%2520Hongxi%2520Li%2520and%2520Shuo%2520yang%2520and%2520Xinxiao%2520Wu%26entry.1292438233%3D%2520%2520Video%2520summarization%2520aims%2520to%2520eliminate%2520visual%2520redundancy%2520while%2520retaining%2520key%250Aparts%2520of%2520video%2520to%2520construct%2520concise%2520and%2520comprehensive%2520synopses.%2520Most%2520existing%250Amethods%2520use%2520discriminative%2520models%2520to%2520predict%2520the%2520importance%2520scores%2520of%2520video%250Aframes.%2520However%252C%2520these%2520methods%2520are%2520susceptible%2520to%2520annotation%2520inconsistency%250Acaused%2520by%2520the%2520inherent%2520subjectivity%2520of%2520different%2520annotators%2520when%2520annotating%2520the%250Asame%2520video.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520generative%2520framework%2520for%2520video%250Asummarization%2520that%2520learns%2520how%2520to%2520generate%2520summaries%2520from%2520a%2520probability%250Adistribution%2520perspective%252C%2520effectively%2520reducing%2520the%2520interference%2520of%2520subjective%250Aannotation%2520noise.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520diffusion%2520summarization%250Amethod%2520based%2520on%2520the%2520Denoising%2520Diffusion%2520Probabilistic%2520Model%2520%2528DDPM%2529%252C%2520which%250Alearns%2520the%2520probability%2520distribution%2520of%2520training%2520data%2520through%2520noise%2520prediction%252C%250Aand%2520generates%2520summaries%2520by%2520iterative%2520denoising.%2520Our%2520method%2520is%2520more%2520resistant%2520to%250Asubjective%2520annotation%2520noise%252C%2520and%2520is%2520less%2520prone%2520to%2520overfitting%2520the%2520training%2520data%250Athan%2520discriminative%2520methods%252C%2520with%2520strong%2520generalization%2520ability.%2520Moreover%252C%2520to%250Afacilitate%2520training%2520DDPM%2520with%2520limited%2520data%252C%2520we%2520employ%2520an%2520unsupervised%2520video%250Asummarization%2520model%2520to%2520implement%2520the%2520earlier%2520denoising%2520process.%2520Extensive%250Aexperiments%2520on%2520various%2520datasets%2520%2528TVSum%252C%2520SumMe%252C%2520and%2520FPVSum%2529%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Summarization%20using%20Denoising%20Diffusion%20Probabilistic%20Model&entry.906535625=Zirui%20Shang%20and%20Yubo%20Zhu%20and%20Hongxi%20Li%20and%20Shuo%20yang%20and%20Xinxiao%20Wu&entry.1292438233=%20%20Video%20summarization%20aims%20to%20eliminate%20visual%20redundancy%20while%20retaining%20key%0Aparts%20of%20video%20to%20construct%20concise%20and%20comprehensive%20synopses.%20Most%20existing%0Amethods%20use%20discriminative%20models%20to%20predict%20the%20importance%20scores%20of%20video%0Aframes.%20However%2C%20these%20methods%20are%20susceptible%20to%20annotation%20inconsistency%0Acaused%20by%20the%20inherent%20subjectivity%20of%20different%20annotators%20when%20annotating%20the%0Asame%20video.%20In%20this%20paper%2C%20we%20introduce%20a%20generative%20framework%20for%20video%0Asummarization%20that%20learns%20how%20to%20generate%20summaries%20from%20a%20probability%0Adistribution%20perspective%2C%20effectively%20reducing%20the%20interference%20of%20subjective%0Aannotation%20noise.%20Specifically%2C%20we%20propose%20a%20novel%20diffusion%20summarization%0Amethod%20based%20on%20the%20Denoising%20Diffusion%20Probabilistic%20Model%20%28DDPM%29%2C%20which%0Alearns%20the%20probability%20distribution%20of%20training%20data%20through%20noise%20prediction%2C%0Aand%20generates%20summaries%20by%20iterative%20denoising.%20Our%20method%20is%20more%20resistant%20to%0Asubjective%20annotation%20noise%2C%20and%20is%20less%20prone%20to%20overfitting%20the%20training%20data%0Athan%20discriminative%20methods%2C%20with%20strong%20generalization%20ability.%20Moreover%2C%20to%0Afacilitate%20training%20DDPM%20with%20limited%20data%2C%20we%20employ%20an%20unsupervised%20video%0Asummarization%20model%20to%20implement%20the%20earlier%20denoising%20process.%20Extensive%0Aexperiments%20on%20various%20datasets%20%28TVSum%2C%20SumMe%2C%20and%20FPVSum%29%20demonstrate%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08357v1&entry.124074799=Read"},
{"title": "LAION-SG: An Enhanced Large-Scale Dataset for Training Complex\n  Image-Text Models with Structural Annotations", "author": "Zejian Li and Chenye Meng and Yize Li and Ling Yang and Shengyuan Zhang and Jiarui Ma and Jiayi Li and Guang Yang and Changyuan Yang and Zhiyuan Yang and Jinxiong Chang and Lingyun Sun", "abstract": "  Recent advances in text-to-image (T2I) generation have shown remarkable\nsuccess in producing high-quality images from text. However, existing T2I\nmodels show decayed performance in compositional image generation involving\nmultiple objects and intricate relationships. We attribute this problem to\nlimitations in existing datasets of image-text pairs, which lack precise\ninter-object relationship annotations with prompts only. To address this\nproblem, we construct LAION-SG, a large-scale dataset with high-quality\nstructural annotations of scene graphs (SG), which precisely describe\nattributes and relationships of multiple objects, effectively representing the\nsemantic structure in complex scenes. Based on LAION-SG, we train a new\nfoundation model SDXL-SG to incorporate structural annotation information into\nthe generation process. Extensive experiments show advanced models trained on\nour LAION-SG boast significant performance improvements in complex scene\ngeneration over models on existing datasets. We also introduce CompSG-Bench, a\nbenchmark that evaluates models on compositional image generation, establishing\na new standard for this domain.\n", "link": "http://arxiv.org/abs/2412.08580v1", "date": "2024-12-11", "relevancy": 2.3218, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5851}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAION-SG%3A%20An%20Enhanced%20Large-Scale%20Dataset%20for%20Training%20Complex%0A%20%20Image-Text%20Models%20with%20Structural%20Annotations&body=Title%3A%20LAION-SG%3A%20An%20Enhanced%20Large-Scale%20Dataset%20for%20Training%20Complex%0A%20%20Image-Text%20Models%20with%20Structural%20Annotations%0AAuthor%3A%20Zejian%20Li%20and%20Chenye%20Meng%20and%20Yize%20Li%20and%20Ling%20Yang%20and%20Shengyuan%20Zhang%20and%20Jiarui%20Ma%20and%20Jiayi%20Li%20and%20Guang%20Yang%20and%20Changyuan%20Yang%20and%20Zhiyuan%20Yang%20and%20Jinxiong%20Chang%20and%20Lingyun%20Sun%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-image%20%28T2I%29%20generation%20have%20shown%20remarkable%0Asuccess%20in%20producing%20high-quality%20images%20from%20text.%20However%2C%20existing%20T2I%0Amodels%20show%20decayed%20performance%20in%20compositional%20image%20generation%20involving%0Amultiple%20objects%20and%20intricate%20relationships.%20We%20attribute%20this%20problem%20to%0Alimitations%20in%20existing%20datasets%20of%20image-text%20pairs%2C%20which%20lack%20precise%0Ainter-object%20relationship%20annotations%20with%20prompts%20only.%20To%20address%20this%0Aproblem%2C%20we%20construct%20LAION-SG%2C%20a%20large-scale%20dataset%20with%20high-quality%0Astructural%20annotations%20of%20scene%20graphs%20%28SG%29%2C%20which%20precisely%20describe%0Aattributes%20and%20relationships%20of%20multiple%20objects%2C%20effectively%20representing%20the%0Asemantic%20structure%20in%20complex%20scenes.%20Based%20on%20LAION-SG%2C%20we%20train%20a%20new%0Afoundation%20model%20SDXL-SG%20to%20incorporate%20structural%20annotation%20information%20into%0Athe%20generation%20process.%20Extensive%20experiments%20show%20advanced%20models%20trained%20on%0Aour%20LAION-SG%20boast%20significant%20performance%20improvements%20in%20complex%20scene%0Ageneration%20over%20models%20on%20existing%20datasets.%20We%20also%20introduce%20CompSG-Bench%2C%20a%0Abenchmark%20that%20evaluates%20models%20on%20compositional%20image%20generation%2C%20establishing%0Aa%20new%20standard%20for%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAION-SG%253A%2520An%2520Enhanced%2520Large-Scale%2520Dataset%2520for%2520Training%2520Complex%250A%2520%2520Image-Text%2520Models%2520with%2520Structural%2520Annotations%26entry.906535625%3DZejian%2520Li%2520and%2520Chenye%2520Meng%2520and%2520Yize%2520Li%2520and%2520Ling%2520Yang%2520and%2520Shengyuan%2520Zhang%2520and%2520Jiarui%2520Ma%2520and%2520Jiayi%2520Li%2520and%2520Guang%2520Yang%2520and%2520Changyuan%2520Yang%2520and%2520Zhiyuan%2520Yang%2520and%2520Jinxiong%2520Chang%2520and%2520Lingyun%2520Sun%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-image%2520%2528T2I%2529%2520generation%2520have%2520shown%2520remarkable%250Asuccess%2520in%2520producing%2520high-quality%2520images%2520from%2520text.%2520However%252C%2520existing%2520T2I%250Amodels%2520show%2520decayed%2520performance%2520in%2520compositional%2520image%2520generation%2520involving%250Amultiple%2520objects%2520and%2520intricate%2520relationships.%2520We%2520attribute%2520this%2520problem%2520to%250Alimitations%2520in%2520existing%2520datasets%2520of%2520image-text%2520pairs%252C%2520which%2520lack%2520precise%250Ainter-object%2520relationship%2520annotations%2520with%2520prompts%2520only.%2520To%2520address%2520this%250Aproblem%252C%2520we%2520construct%2520LAION-SG%252C%2520a%2520large-scale%2520dataset%2520with%2520high-quality%250Astructural%2520annotations%2520of%2520scene%2520graphs%2520%2528SG%2529%252C%2520which%2520precisely%2520describe%250Aattributes%2520and%2520relationships%2520of%2520multiple%2520objects%252C%2520effectively%2520representing%2520the%250Asemantic%2520structure%2520in%2520complex%2520scenes.%2520Based%2520on%2520LAION-SG%252C%2520we%2520train%2520a%2520new%250Afoundation%2520model%2520SDXL-SG%2520to%2520incorporate%2520structural%2520annotation%2520information%2520into%250Athe%2520generation%2520process.%2520Extensive%2520experiments%2520show%2520advanced%2520models%2520trained%2520on%250Aour%2520LAION-SG%2520boast%2520significant%2520performance%2520improvements%2520in%2520complex%2520scene%250Ageneration%2520over%2520models%2520on%2520existing%2520datasets.%2520We%2520also%2520introduce%2520CompSG-Bench%252C%2520a%250Abenchmark%2520that%2520evaluates%2520models%2520on%2520compositional%2520image%2520generation%252C%2520establishing%250Aa%2520new%2520standard%2520for%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAION-SG%3A%20An%20Enhanced%20Large-Scale%20Dataset%20for%20Training%20Complex%0A%20%20Image-Text%20Models%20with%20Structural%20Annotations&entry.906535625=Zejian%20Li%20and%20Chenye%20Meng%20and%20Yize%20Li%20and%20Ling%20Yang%20and%20Shengyuan%20Zhang%20and%20Jiarui%20Ma%20and%20Jiayi%20Li%20and%20Guang%20Yang%20and%20Changyuan%20Yang%20and%20Zhiyuan%20Yang%20and%20Jinxiong%20Chang%20and%20Lingyun%20Sun&entry.1292438233=%20%20Recent%20advances%20in%20text-to-image%20%28T2I%29%20generation%20have%20shown%20remarkable%0Asuccess%20in%20producing%20high-quality%20images%20from%20text.%20However%2C%20existing%20T2I%0Amodels%20show%20decayed%20performance%20in%20compositional%20image%20generation%20involving%0Amultiple%20objects%20and%20intricate%20relationships.%20We%20attribute%20this%20problem%20to%0Alimitations%20in%20existing%20datasets%20of%20image-text%20pairs%2C%20which%20lack%20precise%0Ainter-object%20relationship%20annotations%20with%20prompts%20only.%20To%20address%20this%0Aproblem%2C%20we%20construct%20LAION-SG%2C%20a%20large-scale%20dataset%20with%20high-quality%0Astructural%20annotations%20of%20scene%20graphs%20%28SG%29%2C%20which%20precisely%20describe%0Aattributes%20and%20relationships%20of%20multiple%20objects%2C%20effectively%20representing%20the%0Asemantic%20structure%20in%20complex%20scenes.%20Based%20on%20LAION-SG%2C%20we%20train%20a%20new%0Afoundation%20model%20SDXL-SG%20to%20incorporate%20structural%20annotation%20information%20into%0Athe%20generation%20process.%20Extensive%20experiments%20show%20advanced%20models%20trained%20on%0Aour%20LAION-SG%20boast%20significant%20performance%20improvements%20in%20complex%20scene%0Ageneration%20over%20models%20on%20existing%20datasets.%20We%20also%20introduce%20CompSG-Bench%2C%20a%0Abenchmark%20that%20evaluates%20models%20on%20compositional%20image%20generation%2C%20establishing%0Aa%20new%20standard%20for%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08580v1&entry.124074799=Read"},
{"title": "Hierarchical Context Alignment with Disentangled Geometric and Temporal\n  Modeling for Semantic Occupancy Prediction", "author": "Bohan Li and Xin Jin and Jiajun Deng and Yasheng Sun and Xiaofeng Wang and Wenjun Zeng", "abstract": "  Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for\nunderstanding complex 3D scenes from limited 2D image observations. Existing\nSOP methods typically aggregate contextual features to assist the occupancy\nrepresentation learning, alleviating issues like occlusion or ambiguity.\nHowever, these solutions often face misalignment issues wherein the\ncorresponding features at the same position across different frames may have\ndifferent semantic meanings during the aggregation process, which leads to\nunreliable contextual fusion results and an unstable representation learning\nprocess. To address this problem, we introduce a new Hierarchical context\nalignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles\nthe geometric and temporal context for separate alignment, which two branches\nare then composed to enhance the reliability of SOP. This parsing of the visual\ninput into a local-global alignment hierarchy includes: (I) disentangled\ngeometric and temporal separate alignment, within each leverages depth\nconfidence and camera pose as prior for relevant feature matching respectively;\n(II) global alignment and composition of the transformed geometric and temporal\nvolumes based on semantics consistency. Our method outperforms SOTAs for\nsemantic scene completion on the SemanticKITTI & NuScenes-Occupancy datasets\nand LiDAR semantic segmentation on the NuScenes dataset.\n", "link": "http://arxiv.org/abs/2412.08243v1", "date": "2024-12-11", "relevancy": 2.3173, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Context%20Alignment%20with%20Disentangled%20Geometric%20and%20Temporal%0A%20%20Modeling%20for%20Semantic%20Occupancy%20Prediction&body=Title%3A%20Hierarchical%20Context%20Alignment%20with%20Disentangled%20Geometric%20and%20Temporal%0A%20%20Modeling%20for%20Semantic%20Occupancy%20Prediction%0AAuthor%3A%20Bohan%20Li%20and%20Xin%20Jin%20and%20Jiajun%20Deng%20and%20Yasheng%20Sun%20and%20Xiaofeng%20Wang%20and%20Wenjun%20Zeng%0AAbstract%3A%20%20%20Camera-based%203D%20Semantic%20Occupancy%20Prediction%20%28SOP%29%20is%20crucial%20for%0Aunderstanding%20complex%203D%20scenes%20from%20limited%202D%20image%20observations.%20Existing%0ASOP%20methods%20typically%20aggregate%20contextual%20features%20to%20assist%20the%20occupancy%0Arepresentation%20learning%2C%20alleviating%20issues%20like%20occlusion%20or%20ambiguity.%0AHowever%2C%20these%20solutions%20often%20face%20misalignment%20issues%20wherein%20the%0Acorresponding%20features%20at%20the%20same%20position%20across%20different%20frames%20may%20have%0Adifferent%20semantic%20meanings%20during%20the%20aggregation%20process%2C%20which%20leads%20to%0Aunreliable%20contextual%20fusion%20results%20and%20an%20unstable%20representation%20learning%0Aprocess.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20new%20Hierarchical%20context%0Aalignment%20paradigm%20for%20a%20more%20accurate%20SOP%20%28Hi-SOP%29.%20Hi-SOP%20first%20disentangles%0Athe%20geometric%20and%20temporal%20context%20for%20separate%20alignment%2C%20which%20two%20branches%0Aare%20then%20composed%20to%20enhance%20the%20reliability%20of%20SOP.%20This%20parsing%20of%20the%20visual%0Ainput%20into%20a%20local-global%20alignment%20hierarchy%20includes%3A%20%28I%29%20disentangled%0Ageometric%20and%20temporal%20separate%20alignment%2C%20within%20each%20leverages%20depth%0Aconfidence%20and%20camera%20pose%20as%20prior%20for%20relevant%20feature%20matching%20respectively%3B%0A%28II%29%20global%20alignment%20and%20composition%20of%20the%20transformed%20geometric%20and%20temporal%0Avolumes%20based%20on%20semantics%20consistency.%20Our%20method%20outperforms%20SOTAs%20for%0Asemantic%20scene%20completion%20on%20the%20SemanticKITTI%20%26%20NuScenes-Occupancy%20datasets%0Aand%20LiDAR%20semantic%20segmentation%20on%20the%20NuScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Context%2520Alignment%2520with%2520Disentangled%2520Geometric%2520and%2520Temporal%250A%2520%2520Modeling%2520for%2520Semantic%2520Occupancy%2520Prediction%26entry.906535625%3DBohan%2520Li%2520and%2520Xin%2520Jin%2520and%2520Jiajun%2520Deng%2520and%2520Yasheng%2520Sun%2520and%2520Xiaofeng%2520Wang%2520and%2520Wenjun%2520Zeng%26entry.1292438233%3D%2520%2520Camera-based%25203D%2520Semantic%2520Occupancy%2520Prediction%2520%2528SOP%2529%2520is%2520crucial%2520for%250Aunderstanding%2520complex%25203D%2520scenes%2520from%2520limited%25202D%2520image%2520observations.%2520Existing%250ASOP%2520methods%2520typically%2520aggregate%2520contextual%2520features%2520to%2520assist%2520the%2520occupancy%250Arepresentation%2520learning%252C%2520alleviating%2520issues%2520like%2520occlusion%2520or%2520ambiguity.%250AHowever%252C%2520these%2520solutions%2520often%2520face%2520misalignment%2520issues%2520wherein%2520the%250Acorresponding%2520features%2520at%2520the%2520same%2520position%2520across%2520different%2520frames%2520may%2520have%250Adifferent%2520semantic%2520meanings%2520during%2520the%2520aggregation%2520process%252C%2520which%2520leads%2520to%250Aunreliable%2520contextual%2520fusion%2520results%2520and%2520an%2520unstable%2520representation%2520learning%250Aprocess.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%2520new%2520Hierarchical%2520context%250Aalignment%2520paradigm%2520for%2520a%2520more%2520accurate%2520SOP%2520%2528Hi-SOP%2529.%2520Hi-SOP%2520first%2520disentangles%250Athe%2520geometric%2520and%2520temporal%2520context%2520for%2520separate%2520alignment%252C%2520which%2520two%2520branches%250Aare%2520then%2520composed%2520to%2520enhance%2520the%2520reliability%2520of%2520SOP.%2520This%2520parsing%2520of%2520the%2520visual%250Ainput%2520into%2520a%2520local-global%2520alignment%2520hierarchy%2520includes%253A%2520%2528I%2529%2520disentangled%250Ageometric%2520and%2520temporal%2520separate%2520alignment%252C%2520within%2520each%2520leverages%2520depth%250Aconfidence%2520and%2520camera%2520pose%2520as%2520prior%2520for%2520relevant%2520feature%2520matching%2520respectively%253B%250A%2528II%2529%2520global%2520alignment%2520and%2520composition%2520of%2520the%2520transformed%2520geometric%2520and%2520temporal%250Avolumes%2520based%2520on%2520semantics%2520consistency.%2520Our%2520method%2520outperforms%2520SOTAs%2520for%250Asemantic%2520scene%2520completion%2520on%2520the%2520SemanticKITTI%2520%2526%2520NuScenes-Occupancy%2520datasets%250Aand%2520LiDAR%2520semantic%2520segmentation%2520on%2520the%2520NuScenes%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Context%20Alignment%20with%20Disentangled%20Geometric%20and%20Temporal%0A%20%20Modeling%20for%20Semantic%20Occupancy%20Prediction&entry.906535625=Bohan%20Li%20and%20Xin%20Jin%20and%20Jiajun%20Deng%20and%20Yasheng%20Sun%20and%20Xiaofeng%20Wang%20and%20Wenjun%20Zeng&entry.1292438233=%20%20Camera-based%203D%20Semantic%20Occupancy%20Prediction%20%28SOP%29%20is%20crucial%20for%0Aunderstanding%20complex%203D%20scenes%20from%20limited%202D%20image%20observations.%20Existing%0ASOP%20methods%20typically%20aggregate%20contextual%20features%20to%20assist%20the%20occupancy%0Arepresentation%20learning%2C%20alleviating%20issues%20like%20occlusion%20or%20ambiguity.%0AHowever%2C%20these%20solutions%20often%20face%20misalignment%20issues%20wherein%20the%0Acorresponding%20features%20at%20the%20same%20position%20across%20different%20frames%20may%20have%0Adifferent%20semantic%20meanings%20during%20the%20aggregation%20process%2C%20which%20leads%20to%0Aunreliable%20contextual%20fusion%20results%20and%20an%20unstable%20representation%20learning%0Aprocess.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20new%20Hierarchical%20context%0Aalignment%20paradigm%20for%20a%20more%20accurate%20SOP%20%28Hi-SOP%29.%20Hi-SOP%20first%20disentangles%0Athe%20geometric%20and%20temporal%20context%20for%20separate%20alignment%2C%20which%20two%20branches%0Aare%20then%20composed%20to%20enhance%20the%20reliability%20of%20SOP.%20This%20parsing%20of%20the%20visual%0Ainput%20into%20a%20local-global%20alignment%20hierarchy%20includes%3A%20%28I%29%20disentangled%0Ageometric%20and%20temporal%20separate%20alignment%2C%20within%20each%20leverages%20depth%0Aconfidence%20and%20camera%20pose%20as%20prior%20for%20relevant%20feature%20matching%20respectively%3B%0A%28II%29%20global%20alignment%20and%20composition%20of%20the%20transformed%20geometric%20and%20temporal%0Avolumes%20based%20on%20semantics%20consistency.%20Our%20method%20outperforms%20SOTAs%20for%0Asemantic%20scene%20completion%20on%20the%20SemanticKITTI%20%26%20NuScenes-Occupancy%20datasets%0Aand%20LiDAR%20semantic%20segmentation%20on%20the%20NuScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08243v1&entry.124074799=Read"},
{"title": "Learning to Decouple the Lights for 3D Face Texture Modeling", "author": "Tianxin Huang and Zhenyu Zhang and Ying Tai and Gim Hee Lee", "abstract": "  Existing research has made impressive strides in reconstructing human facial\nshapes and textures from images with well-illuminated faces and minimal\nexternal occlusions. Nevertheless, it remains challenging to recover accurate\nfacial textures from scenarios with complicated illumination affected by\nexternal occlusions, e.g. a face that is partially obscured by items such as a\nhat. Existing works based on the assumption of single and uniform illumination\ncannot correctly process these data. In this work, we introduce a novel\napproach to model 3D facial textures under such unnatural illumination. Instead\nof assuming single illumination, our framework learns to imitate the unnatural\nillumination as a composition of multiple separate light conditions combined\nwith learned neural representations, named Light Decoupling. According to\nexperiments on both single images and video sequences, we demonstrate the\neffectiveness of our approach in modeling facial textures under challenging\nillumination affected by occlusions. Please check\nhttps://tianxinhuang.github.io/projects/Deface for our videos and codes.\n", "link": "http://arxiv.org/abs/2412.08524v1", "date": "2024-12-11", "relevancy": 2.3123, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Decouple%20the%20Lights%20for%203D%20Face%20Texture%20Modeling&body=Title%3A%20Learning%20to%20Decouple%20the%20Lights%20for%203D%20Face%20Texture%20Modeling%0AAuthor%3A%20Tianxin%20Huang%20and%20Zhenyu%20Zhang%20and%20Ying%20Tai%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20Existing%20research%20has%20made%20impressive%20strides%20in%20reconstructing%20human%20facial%0Ashapes%20and%20textures%20from%20images%20with%20well-illuminated%20faces%20and%20minimal%0Aexternal%20occlusions.%20Nevertheless%2C%20it%20remains%20challenging%20to%20recover%20accurate%0Afacial%20textures%20from%20scenarios%20with%20complicated%20illumination%20affected%20by%0Aexternal%20occlusions%2C%20e.g.%20a%20face%20that%20is%20partially%20obscured%20by%20items%20such%20as%20a%0Ahat.%20Existing%20works%20based%20on%20the%20assumption%20of%20single%20and%20uniform%20illumination%0Acannot%20correctly%20process%20these%20data.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aapproach%20to%20model%203D%20facial%20textures%20under%20such%20unnatural%20illumination.%20Instead%0Aof%20assuming%20single%20illumination%2C%20our%20framework%20learns%20to%20imitate%20the%20unnatural%0Aillumination%20as%20a%20composition%20of%20multiple%20separate%20light%20conditions%20combined%0Awith%20learned%20neural%20representations%2C%20named%20Light%20Decoupling.%20According%20to%0Aexperiments%20on%20both%20single%20images%20and%20video%20sequences%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20modeling%20facial%20textures%20under%20challenging%0Aillumination%20affected%20by%20occlusions.%20Please%20check%0Ahttps%3A//tianxinhuang.github.io/projects/Deface%20for%20our%20videos%20and%20codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Decouple%2520the%2520Lights%2520for%25203D%2520Face%2520Texture%2520Modeling%26entry.906535625%3DTianxin%2520Huang%2520and%2520Zhenyu%2520Zhang%2520and%2520Ying%2520Tai%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520Existing%2520research%2520has%2520made%2520impressive%2520strides%2520in%2520reconstructing%2520human%2520facial%250Ashapes%2520and%2520textures%2520from%2520images%2520with%2520well-illuminated%2520faces%2520and%2520minimal%250Aexternal%2520occlusions.%2520Nevertheless%252C%2520it%2520remains%2520challenging%2520to%2520recover%2520accurate%250Afacial%2520textures%2520from%2520scenarios%2520with%2520complicated%2520illumination%2520affected%2520by%250Aexternal%2520occlusions%252C%2520e.g.%2520a%2520face%2520that%2520is%2520partially%2520obscured%2520by%2520items%2520such%2520as%2520a%250Ahat.%2520Existing%2520works%2520based%2520on%2520the%2520assumption%2520of%2520single%2520and%2520uniform%2520illumination%250Acannot%2520correctly%2520process%2520these%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250Aapproach%2520to%2520model%25203D%2520facial%2520textures%2520under%2520such%2520unnatural%2520illumination.%2520Instead%250Aof%2520assuming%2520single%2520illumination%252C%2520our%2520framework%2520learns%2520to%2520imitate%2520the%2520unnatural%250Aillumination%2520as%2520a%2520composition%2520of%2520multiple%2520separate%2520light%2520conditions%2520combined%250Awith%2520learned%2520neural%2520representations%252C%2520named%2520Light%2520Decoupling.%2520According%2520to%250Aexperiments%2520on%2520both%2520single%2520images%2520and%2520video%2520sequences%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520modeling%2520facial%2520textures%2520under%2520challenging%250Aillumination%2520affected%2520by%2520occlusions.%2520Please%2520check%250Ahttps%253A//tianxinhuang.github.io/projects/Deface%2520for%2520our%2520videos%2520and%2520codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Decouple%20the%20Lights%20for%203D%20Face%20Texture%20Modeling&entry.906535625=Tianxin%20Huang%20and%20Zhenyu%20Zhang%20and%20Ying%20Tai%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20Existing%20research%20has%20made%20impressive%20strides%20in%20reconstructing%20human%20facial%0Ashapes%20and%20textures%20from%20images%20with%20well-illuminated%20faces%20and%20minimal%0Aexternal%20occlusions.%20Nevertheless%2C%20it%20remains%20challenging%20to%20recover%20accurate%0Afacial%20textures%20from%20scenarios%20with%20complicated%20illumination%20affected%20by%0Aexternal%20occlusions%2C%20e.g.%20a%20face%20that%20is%20partially%20obscured%20by%20items%20such%20as%20a%0Ahat.%20Existing%20works%20based%20on%20the%20assumption%20of%20single%20and%20uniform%20illumination%0Acannot%20correctly%20process%20these%20data.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aapproach%20to%20model%203D%20facial%20textures%20under%20such%20unnatural%20illumination.%20Instead%0Aof%20assuming%20single%20illumination%2C%20our%20framework%20learns%20to%20imitate%20the%20unnatural%0Aillumination%20as%20a%20composition%20of%20multiple%20separate%20light%20conditions%20combined%0Awith%20learned%20neural%20representations%2C%20named%20Light%20Decoupling.%20According%20to%0Aexperiments%20on%20both%20single%20images%20and%20video%20sequences%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20modeling%20facial%20textures%20under%20challenging%0Aillumination%20affected%20by%20occlusions.%20Please%20check%0Ahttps%3A//tianxinhuang.github.io/projects/Deface%20for%20our%20videos%20and%20codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08524v1&entry.124074799=Read"},
{"title": "Using Game Play to Investigate Multimodal and Conversational Grounding\n  in Large Multimodal Models", "author": "Sherzod Hakimov and Yerkezhan Abdullayeva and Kushal Koshti and Antonia Schmidt and Yan Weiser and Anne Beyer and David Schlangen", "abstract": "  While the situation has improved for text-only models, it again seems to be\nthe case currently that multimodal (text and image) models develop faster than\nways to evaluate them. In this paper, we bring a recently developed evaluation\nparadigm from text models to multimodal models, namely evaluation through the\ngoal-oriented game (self) play, complementing reference-based and\npreference-based evaluation. Specifically, we define games that challenge a\nmodel's capability to represent a situation from visual information and align\nsuch representations through dialogue. We find that the largest closed models\nperform rather well on the games that we define, while even the best\nopen-weight models struggle with them. On further analysis, we find that the\nexceptional deep captioning capabilities of the largest models drive some of\nthe performance. There is still room to grow for both kinds of models, ensuring\nthe continued relevance of the benchmark.\n", "link": "http://arxiv.org/abs/2406.14035v3", "date": "2024-12-11", "relevancy": 2.2937, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5814}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Game%20Play%20to%20Investigate%20Multimodal%20and%20Conversational%20Grounding%0A%20%20in%20Large%20Multimodal%20Models&body=Title%3A%20Using%20Game%20Play%20to%20Investigate%20Multimodal%20and%20Conversational%20Grounding%0A%20%20in%20Large%20Multimodal%20Models%0AAuthor%3A%20Sherzod%20Hakimov%20and%20Yerkezhan%20Abdullayeva%20and%20Kushal%20Koshti%20and%20Antonia%20Schmidt%20and%20Yan%20Weiser%20and%20Anne%20Beyer%20and%20David%20Schlangen%0AAbstract%3A%20%20%20While%20the%20situation%20has%20improved%20for%20text-only%20models%2C%20it%20again%20seems%20to%20be%0Athe%20case%20currently%20that%20multimodal%20%28text%20and%20image%29%20models%20develop%20faster%20than%0Aways%20to%20evaluate%20them.%20In%20this%20paper%2C%20we%20bring%20a%20recently%20developed%20evaluation%0Aparadigm%20from%20text%20models%20to%20multimodal%20models%2C%20namely%20evaluation%20through%20the%0Agoal-oriented%20game%20%28self%29%20play%2C%20complementing%20reference-based%20and%0Apreference-based%20evaluation.%20Specifically%2C%20we%20define%20games%20that%20challenge%20a%0Amodel%27s%20capability%20to%20represent%20a%20situation%20from%20visual%20information%20and%20align%0Asuch%20representations%20through%20dialogue.%20We%20find%20that%20the%20largest%20closed%20models%0Aperform%20rather%20well%20on%20the%20games%20that%20we%20define%2C%20while%20even%20the%20best%0Aopen-weight%20models%20struggle%20with%20them.%20On%20further%20analysis%2C%20we%20find%20that%20the%0Aexceptional%20deep%20captioning%20capabilities%20of%20the%20largest%20models%20drive%20some%20of%0Athe%20performance.%20There%20is%20still%20room%20to%20grow%20for%20both%20kinds%20of%20models%2C%20ensuring%0Athe%20continued%20relevance%20of%20the%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14035v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Game%2520Play%2520to%2520Investigate%2520Multimodal%2520and%2520Conversational%2520Grounding%250A%2520%2520in%2520Large%2520Multimodal%2520Models%26entry.906535625%3DSherzod%2520Hakimov%2520and%2520Yerkezhan%2520Abdullayeva%2520and%2520Kushal%2520Koshti%2520and%2520Antonia%2520Schmidt%2520and%2520Yan%2520Weiser%2520and%2520Anne%2520Beyer%2520and%2520David%2520Schlangen%26entry.1292438233%3D%2520%2520While%2520the%2520situation%2520has%2520improved%2520for%2520text-only%2520models%252C%2520it%2520again%2520seems%2520to%2520be%250Athe%2520case%2520currently%2520that%2520multimodal%2520%2528text%2520and%2520image%2529%2520models%2520develop%2520faster%2520than%250Aways%2520to%2520evaluate%2520them.%2520In%2520this%2520paper%252C%2520we%2520bring%2520a%2520recently%2520developed%2520evaluation%250Aparadigm%2520from%2520text%2520models%2520to%2520multimodal%2520models%252C%2520namely%2520evaluation%2520through%2520the%250Agoal-oriented%2520game%2520%2528self%2529%2520play%252C%2520complementing%2520reference-based%2520and%250Apreference-based%2520evaluation.%2520Specifically%252C%2520we%2520define%2520games%2520that%2520challenge%2520a%250Amodel%2527s%2520capability%2520to%2520represent%2520a%2520situation%2520from%2520visual%2520information%2520and%2520align%250Asuch%2520representations%2520through%2520dialogue.%2520We%2520find%2520that%2520the%2520largest%2520closed%2520models%250Aperform%2520rather%2520well%2520on%2520the%2520games%2520that%2520we%2520define%252C%2520while%2520even%2520the%2520best%250Aopen-weight%2520models%2520struggle%2520with%2520them.%2520On%2520further%2520analysis%252C%2520we%2520find%2520that%2520the%250Aexceptional%2520deep%2520captioning%2520capabilities%2520of%2520the%2520largest%2520models%2520drive%2520some%2520of%250Athe%2520performance.%2520There%2520is%2520still%2520room%2520to%2520grow%2520for%2520both%2520kinds%2520of%2520models%252C%2520ensuring%250Athe%2520continued%2520relevance%2520of%2520the%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14035v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Game%20Play%20to%20Investigate%20Multimodal%20and%20Conversational%20Grounding%0A%20%20in%20Large%20Multimodal%20Models&entry.906535625=Sherzod%20Hakimov%20and%20Yerkezhan%20Abdullayeva%20and%20Kushal%20Koshti%20and%20Antonia%20Schmidt%20and%20Yan%20Weiser%20and%20Anne%20Beyer%20and%20David%20Schlangen&entry.1292438233=%20%20While%20the%20situation%20has%20improved%20for%20text-only%20models%2C%20it%20again%20seems%20to%20be%0Athe%20case%20currently%20that%20multimodal%20%28text%20and%20image%29%20models%20develop%20faster%20than%0Aways%20to%20evaluate%20them.%20In%20this%20paper%2C%20we%20bring%20a%20recently%20developed%20evaluation%0Aparadigm%20from%20text%20models%20to%20multimodal%20models%2C%20namely%20evaluation%20through%20the%0Agoal-oriented%20game%20%28self%29%20play%2C%20complementing%20reference-based%20and%0Apreference-based%20evaluation.%20Specifically%2C%20we%20define%20games%20that%20challenge%20a%0Amodel%27s%20capability%20to%20represent%20a%20situation%20from%20visual%20information%20and%20align%0Asuch%20representations%20through%20dialogue.%20We%20find%20that%20the%20largest%20closed%20models%0Aperform%20rather%20well%20on%20the%20games%20that%20we%20define%2C%20while%20even%20the%20best%0Aopen-weight%20models%20struggle%20with%20them.%20On%20further%20analysis%2C%20we%20find%20that%20the%0Aexceptional%20deep%20captioning%20capabilities%20of%20the%20largest%20models%20drive%20some%20of%0Athe%20performance.%20There%20is%20still%20room%20to%20grow%20for%20both%20kinds%20of%20models%2C%20ensuring%0Athe%20continued%20relevance%20of%20the%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14035v3&entry.124074799=Read"},
{"title": "ConvMesh: Reimagining Mesh Quality Through Convex Optimization", "author": "Alexander Valverde", "abstract": "  Mesh generation has become a critical topic in recent years, forming the\nfoundation of all 3D objects used across various applications, such as virtual\nreality, gaming, and 3D printing. With advancements in computational resources\nand machine learning, neural networks have emerged as powerful tools for\ngenerating high-quality 3D object representations, enabling accurate scene and\nobject reconstructions. Despite these advancements, many methods produce meshes\nthat lack realism or exhibit geometric and textural flaws, necessitating\nadditional processing to improve their quality. This research introduces a\nconvex optimization programming called disciplined convex programming to\nenhance existing meshes by refining their texture and geometry with a conic\nsolver. By focusing on a sparse set of point clouds from both the original and\ntarget meshes, this method demonstrates significant improvements in mesh\nquality with minimal data requirements. To evaluate the approach, the classical\ndolphin mesh dataset from Facebook AI was used as a case study, with\noptimization performed using the CVXPY library. The results reveal promising\npotential for streamlined and effective mesh refinement.\n", "link": "http://arxiv.org/abs/2412.08484v1", "date": "2024-12-11", "relevancy": 2.2914, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.626}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5383}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConvMesh%3A%20Reimagining%20Mesh%20Quality%20Through%20Convex%20Optimization&body=Title%3A%20ConvMesh%3A%20Reimagining%20Mesh%20Quality%20Through%20Convex%20Optimization%0AAuthor%3A%20Alexander%20Valverde%0AAbstract%3A%20%20%20Mesh%20generation%20has%20become%20a%20critical%20topic%20in%20recent%20years%2C%20forming%20the%0Afoundation%20of%20all%203D%20objects%20used%20across%20various%20applications%2C%20such%20as%20virtual%0Areality%2C%20gaming%2C%20and%203D%20printing.%20With%20advancements%20in%20computational%20resources%0Aand%20machine%20learning%2C%20neural%20networks%20have%20emerged%20as%20powerful%20tools%20for%0Agenerating%20high-quality%203D%20object%20representations%2C%20enabling%20accurate%20scene%20and%0Aobject%20reconstructions.%20Despite%20these%20advancements%2C%20many%20methods%20produce%20meshes%0Athat%20lack%20realism%20or%20exhibit%20geometric%20and%20textural%20flaws%2C%20necessitating%0Aadditional%20processing%20to%20improve%20their%20quality.%20This%20research%20introduces%20a%0Aconvex%20optimization%20programming%20called%20disciplined%20convex%20programming%20to%0Aenhance%20existing%20meshes%20by%20refining%20their%20texture%20and%20geometry%20with%20a%20conic%0Asolver.%20By%20focusing%20on%20a%20sparse%20set%20of%20point%20clouds%20from%20both%20the%20original%20and%0Atarget%20meshes%2C%20this%20method%20demonstrates%20significant%20improvements%20in%20mesh%0Aquality%20with%20minimal%20data%20requirements.%20To%20evaluate%20the%20approach%2C%20the%20classical%0Adolphin%20mesh%20dataset%20from%20Facebook%20AI%20was%20used%20as%20a%20case%20study%2C%20with%0Aoptimization%20performed%20using%20the%20CVXPY%20library.%20The%20results%20reveal%20promising%0Apotential%20for%20streamlined%20and%20effective%20mesh%20refinement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvMesh%253A%2520Reimagining%2520Mesh%2520Quality%2520Through%2520Convex%2520Optimization%26entry.906535625%3DAlexander%2520Valverde%26entry.1292438233%3D%2520%2520Mesh%2520generation%2520has%2520become%2520a%2520critical%2520topic%2520in%2520recent%2520years%252C%2520forming%2520the%250Afoundation%2520of%2520all%25203D%2520objects%2520used%2520across%2520various%2520applications%252C%2520such%2520as%2520virtual%250Areality%252C%2520gaming%252C%2520and%25203D%2520printing.%2520With%2520advancements%2520in%2520computational%2520resources%250Aand%2520machine%2520learning%252C%2520neural%2520networks%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%250Agenerating%2520high-quality%25203D%2520object%2520representations%252C%2520enabling%2520accurate%2520scene%2520and%250Aobject%2520reconstructions.%2520Despite%2520these%2520advancements%252C%2520many%2520methods%2520produce%2520meshes%250Athat%2520lack%2520realism%2520or%2520exhibit%2520geometric%2520and%2520textural%2520flaws%252C%2520necessitating%250Aadditional%2520processing%2520to%2520improve%2520their%2520quality.%2520This%2520research%2520introduces%2520a%250Aconvex%2520optimization%2520programming%2520called%2520disciplined%2520convex%2520programming%2520to%250Aenhance%2520existing%2520meshes%2520by%2520refining%2520their%2520texture%2520and%2520geometry%2520with%2520a%2520conic%250Asolver.%2520By%2520focusing%2520on%2520a%2520sparse%2520set%2520of%2520point%2520clouds%2520from%2520both%2520the%2520original%2520and%250Atarget%2520meshes%252C%2520this%2520method%2520demonstrates%2520significant%2520improvements%2520in%2520mesh%250Aquality%2520with%2520minimal%2520data%2520requirements.%2520To%2520evaluate%2520the%2520approach%252C%2520the%2520classical%250Adolphin%2520mesh%2520dataset%2520from%2520Facebook%2520AI%2520was%2520used%2520as%2520a%2520case%2520study%252C%2520with%250Aoptimization%2520performed%2520using%2520the%2520CVXPY%2520library.%2520The%2520results%2520reveal%2520promising%250Apotential%2520for%2520streamlined%2520and%2520effective%2520mesh%2520refinement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConvMesh%3A%20Reimagining%20Mesh%20Quality%20Through%20Convex%20Optimization&entry.906535625=Alexander%20Valverde&entry.1292438233=%20%20Mesh%20generation%20has%20become%20a%20critical%20topic%20in%20recent%20years%2C%20forming%20the%0Afoundation%20of%20all%203D%20objects%20used%20across%20various%20applications%2C%20such%20as%20virtual%0Areality%2C%20gaming%2C%20and%203D%20printing.%20With%20advancements%20in%20computational%20resources%0Aand%20machine%20learning%2C%20neural%20networks%20have%20emerged%20as%20powerful%20tools%20for%0Agenerating%20high-quality%203D%20object%20representations%2C%20enabling%20accurate%20scene%20and%0Aobject%20reconstructions.%20Despite%20these%20advancements%2C%20many%20methods%20produce%20meshes%0Athat%20lack%20realism%20or%20exhibit%20geometric%20and%20textural%20flaws%2C%20necessitating%0Aadditional%20processing%20to%20improve%20their%20quality.%20This%20research%20introduces%20a%0Aconvex%20optimization%20programming%20called%20disciplined%20convex%20programming%20to%0Aenhance%20existing%20meshes%20by%20refining%20their%20texture%20and%20geometry%20with%20a%20conic%0Asolver.%20By%20focusing%20on%20a%20sparse%20set%20of%20point%20clouds%20from%20both%20the%20original%20and%0Atarget%20meshes%2C%20this%20method%20demonstrates%20significant%20improvements%20in%20mesh%0Aquality%20with%20minimal%20data%20requirements.%20To%20evaluate%20the%20approach%2C%20the%20classical%0Adolphin%20mesh%20dataset%20from%20Facebook%20AI%20was%20used%20as%20a%20case%20study%2C%20with%0Aoptimization%20performed%20using%20the%20CVXPY%20library.%20The%20results%20reveal%20promising%0Apotential%20for%20streamlined%20and%20effective%20mesh%20refinement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08484v1&entry.124074799=Read"},
{"title": "StreamMOS: Streaming Moving Object Segmentation with Multi-View\n  Perception and Dual-Span Memory", "author": "Zhiheng Li and Yubo Cui and Jiexi Zhong and Zheng Fang", "abstract": "  Moving object segmentation based on LiDAR is a crucial and challenging task\nfor autonomous driving and mobile robotics. Most approaches explore\nspatio-temporal information from LiDAR sequences to predict moving objects in\nthe current frame. However, they often focus on transferring temporal cues in a\nsingle inference and regard every prediction as independent of others. This may\ncause inconsistent segmentation results for the same object in different\nframes. To overcome this issue, we propose a streaming network with a memory\nmechanism, called StreamMOS, to build the association of features and\npredictions among multiple inferences. Specifically, we utilize a short-term\nmemory to convey historical features, which can be regarded as spatial prior of\nmoving objects and adopted to enhance current inference by temporal fusion.\nMeanwhile, we build a long-term memory to store previous predictions and\nexploit them to refine the present forecast at voxel and instance levels\nthrough voting. Besides, we present multi-view encoder with cascade projection\nand asymmetric convolution to extract motion feature of objects in different\nrepresentations. Extensive experiments validate that our algorithm gets\ncompetitive performance on SemanticKITTI and Sipailou Campus datasets. Code\nwill be released at https://github.com/NEU-REAL/StreamMOS.git.\n", "link": "http://arxiv.org/abs/2407.17905v2", "date": "2024-12-11", "relevancy": 2.2892, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamMOS%3A%20Streaming%20Moving%20Object%20Segmentation%20with%20Multi-View%0A%20%20Perception%20and%20Dual-Span%20Memory&body=Title%3A%20StreamMOS%3A%20Streaming%20Moving%20Object%20Segmentation%20with%20Multi-View%0A%20%20Perception%20and%20Dual-Span%20Memory%0AAuthor%3A%20Zhiheng%20Li%20and%20Yubo%20Cui%20and%20Jiexi%20Zhong%20and%20Zheng%20Fang%0AAbstract%3A%20%20%20Moving%20object%20segmentation%20based%20on%20LiDAR%20is%20a%20crucial%20and%20challenging%20task%0Afor%20autonomous%20driving%20and%20mobile%20robotics.%20Most%20approaches%20explore%0Aspatio-temporal%20information%20from%20LiDAR%20sequences%20to%20predict%20moving%20objects%20in%0Athe%20current%20frame.%20However%2C%20they%20often%20focus%20on%20transferring%20temporal%20cues%20in%20a%0Asingle%20inference%20and%20regard%20every%20prediction%20as%20independent%20of%20others.%20This%20may%0Acause%20inconsistent%20segmentation%20results%20for%20the%20same%20object%20in%20different%0Aframes.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20streaming%20network%20with%20a%20memory%0Amechanism%2C%20called%20StreamMOS%2C%20to%20build%20the%20association%20of%20features%20and%0Apredictions%20among%20multiple%20inferences.%20Specifically%2C%20we%20utilize%20a%20short-term%0Amemory%20to%20convey%20historical%20features%2C%20which%20can%20be%20regarded%20as%20spatial%20prior%20of%0Amoving%20objects%20and%20adopted%20to%20enhance%20current%20inference%20by%20temporal%20fusion.%0AMeanwhile%2C%20we%20build%20a%20long-term%20memory%20to%20store%20previous%20predictions%20and%0Aexploit%20them%20to%20refine%20the%20present%20forecast%20at%20voxel%20and%20instance%20levels%0Athrough%20voting.%20Besides%2C%20we%20present%20multi-view%20encoder%20with%20cascade%20projection%0Aand%20asymmetric%20convolution%20to%20extract%20motion%20feature%20of%20objects%20in%20different%0Arepresentations.%20Extensive%20experiments%20validate%20that%20our%20algorithm%20gets%0Acompetitive%20performance%20on%20SemanticKITTI%20and%20Sipailou%20Campus%20datasets.%20Code%0Awill%20be%20released%20at%20https%3A//github.com/NEU-REAL/StreamMOS.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamMOS%253A%2520Streaming%2520Moving%2520Object%2520Segmentation%2520with%2520Multi-View%250A%2520%2520Perception%2520and%2520Dual-Span%2520Memory%26entry.906535625%3DZhiheng%2520Li%2520and%2520Yubo%2520Cui%2520and%2520Jiexi%2520Zhong%2520and%2520Zheng%2520Fang%26entry.1292438233%3D%2520%2520Moving%2520object%2520segmentation%2520based%2520on%2520LiDAR%2520is%2520a%2520crucial%2520and%2520challenging%2520task%250Afor%2520autonomous%2520driving%2520and%2520mobile%2520robotics.%2520Most%2520approaches%2520explore%250Aspatio-temporal%2520information%2520from%2520LiDAR%2520sequences%2520to%2520predict%2520moving%2520objects%2520in%250Athe%2520current%2520frame.%2520However%252C%2520they%2520often%2520focus%2520on%2520transferring%2520temporal%2520cues%2520in%2520a%250Asingle%2520inference%2520and%2520regard%2520every%2520prediction%2520as%2520independent%2520of%2520others.%2520This%2520may%250Acause%2520inconsistent%2520segmentation%2520results%2520for%2520the%2520same%2520object%2520in%2520different%250Aframes.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520propose%2520a%2520streaming%2520network%2520with%2520a%2520memory%250Amechanism%252C%2520called%2520StreamMOS%252C%2520to%2520build%2520the%2520association%2520of%2520features%2520and%250Apredictions%2520among%2520multiple%2520inferences.%2520Specifically%252C%2520we%2520utilize%2520a%2520short-term%250Amemory%2520to%2520convey%2520historical%2520features%252C%2520which%2520can%2520be%2520regarded%2520as%2520spatial%2520prior%2520of%250Amoving%2520objects%2520and%2520adopted%2520to%2520enhance%2520current%2520inference%2520by%2520temporal%2520fusion.%250AMeanwhile%252C%2520we%2520build%2520a%2520long-term%2520memory%2520to%2520store%2520previous%2520predictions%2520and%250Aexploit%2520them%2520to%2520refine%2520the%2520present%2520forecast%2520at%2520voxel%2520and%2520instance%2520levels%250Athrough%2520voting.%2520Besides%252C%2520we%2520present%2520multi-view%2520encoder%2520with%2520cascade%2520projection%250Aand%2520asymmetric%2520convolution%2520to%2520extract%2520motion%2520feature%2520of%2520objects%2520in%2520different%250Arepresentations.%2520Extensive%2520experiments%2520validate%2520that%2520our%2520algorithm%2520gets%250Acompetitive%2520performance%2520on%2520SemanticKITTI%2520and%2520Sipailou%2520Campus%2520datasets.%2520Code%250Awill%2520be%2520released%2520at%2520https%253A//github.com/NEU-REAL/StreamMOS.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamMOS%3A%20Streaming%20Moving%20Object%20Segmentation%20with%20Multi-View%0A%20%20Perception%20and%20Dual-Span%20Memory&entry.906535625=Zhiheng%20Li%20and%20Yubo%20Cui%20and%20Jiexi%20Zhong%20and%20Zheng%20Fang&entry.1292438233=%20%20Moving%20object%20segmentation%20based%20on%20LiDAR%20is%20a%20crucial%20and%20challenging%20task%0Afor%20autonomous%20driving%20and%20mobile%20robotics.%20Most%20approaches%20explore%0Aspatio-temporal%20information%20from%20LiDAR%20sequences%20to%20predict%20moving%20objects%20in%0Athe%20current%20frame.%20However%2C%20they%20often%20focus%20on%20transferring%20temporal%20cues%20in%20a%0Asingle%20inference%20and%20regard%20every%20prediction%20as%20independent%20of%20others.%20This%20may%0Acause%20inconsistent%20segmentation%20results%20for%20the%20same%20object%20in%20different%0Aframes.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20streaming%20network%20with%20a%20memory%0Amechanism%2C%20called%20StreamMOS%2C%20to%20build%20the%20association%20of%20features%20and%0Apredictions%20among%20multiple%20inferences.%20Specifically%2C%20we%20utilize%20a%20short-term%0Amemory%20to%20convey%20historical%20features%2C%20which%20can%20be%20regarded%20as%20spatial%20prior%20of%0Amoving%20objects%20and%20adopted%20to%20enhance%20current%20inference%20by%20temporal%20fusion.%0AMeanwhile%2C%20we%20build%20a%20long-term%20memory%20to%20store%20previous%20predictions%20and%0Aexploit%20them%20to%20refine%20the%20present%20forecast%20at%20voxel%20and%20instance%20levels%0Athrough%20voting.%20Besides%2C%20we%20present%20multi-view%20encoder%20with%20cascade%20projection%0Aand%20asymmetric%20convolution%20to%20extract%20motion%20feature%20of%20objects%20in%20different%0Arepresentations.%20Extensive%20experiments%20validate%20that%20our%20algorithm%20gets%0Acompetitive%20performance%20on%20SemanticKITTI%20and%20Sipailou%20Campus%20datasets.%20Code%0Awill%20be%20released%20at%20https%3A//github.com/NEU-REAL/StreamMOS.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17905v2&entry.124074799=Read"},
{"title": "EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation", "author": "Hongwei Niu and Jie Hu and Jianghang Lin and Shengchuan Zhang", "abstract": "  Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatial-aware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.2 PQ, 31.6 mIoU, and\n12.7 FPS on the ADE20K dataset for panoptic and semantic segmentation tasks and\nthe inference time of EOV-Seg is 4-21 times faster than state-of-the-art\nmethods. Especially, equipped with ResNet-50 backbone, EOV-Seg runs 25 FPS with\nonly 71M parameters on a single RTX 3090 GPU. Code is available at\n\\url{https://github.com/nhw649/EOV-Seg}.\n", "link": "http://arxiv.org/abs/2412.08628v1", "date": "2024-12-11", "relevancy": 2.2869, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5761}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EOV-Seg%3A%20Efficient%20Open-Vocabulary%20Panoptic%20Segmentation&body=Title%3A%20EOV-Seg%3A%20Efficient%20Open-Vocabulary%20Panoptic%20Segmentation%0AAuthor%3A%20Hongwei%20Niu%20and%20Jie%20Hu%20and%20Jianghang%20Lin%20and%20Shengchuan%20Zhang%0AAbstract%3A%20%20%20Open-vocabulary%20panoptic%20segmentation%20aims%20to%20segment%20and%20classify%20everything%0Ain%20diverse%20scenes%20across%20an%20unbounded%20vocabulary.%20Existing%20methods%20typically%0Aemploy%20two-stage%20or%20single-stage%20framework.%20The%20two-stage%20framework%20involves%0Acropping%20the%20image%20multiple%20times%20using%20masks%20generated%20by%20a%20mask%20generator%2C%0Afollowed%20by%20feature%20extraction%2C%20while%20the%20single-stage%20framework%20relies%20on%20a%0Aheavyweight%20mask%20decoder%20to%20make%20up%20for%20the%20lack%20of%20spatial%20position%0Ainformation%20through%20self-attention%20and%20cross-attention%20in%20multiple%20stacked%0ATransformer%20blocks.%20Both%20methods%20incur%20substantial%20computational%20overhead%2C%0Athereby%20hindering%20the%20efficiency%20of%20model%20inference.%20To%20fill%20the%20gap%20in%0Aefficiency%2C%20we%20propose%20EOV-Seg%2C%20a%20novel%20single-stage%2C%20shared%2C%20efficient%2C%20and%0Aspatial-aware%20framework%20designed%20for%20open-vocabulary%20panoptic%20segmentation.%0ASpecifically%2C%20EOV-Seg%20innovates%20in%20two%20aspects.%20First%2C%20a%20Vocabulary-Aware%0ASelection%20%28VAS%29%20module%20is%20proposed%20to%20improve%20the%20semantic%20comprehension%20of%0Avisual%20aggregated%20features%20and%20alleviate%20the%20feature%20interaction%20burden%20on%20the%0Amask%20decoder.%20Second%2C%20we%20introduce%20a%20Two-way%20Dynamic%20Embedding%20Experts%20%28TDEE%29%2C%0Awhich%20efficiently%20utilizes%20the%20spatial%20awareness%20capabilities%20of%20ViT-based%20CLIP%0Abackbone.%20To%20the%20best%20of%20our%20knowledge%2C%20EOV-Seg%20is%20the%20first%20open-vocabulary%0Apanoptic%20segmentation%20framework%20towards%20efficiency%2C%20which%20runs%20faster%20and%0Aachieves%20competitive%20performance%20compared%20with%20state-of-the-art%20methods.%0ASpecifically%2C%20with%20COCO%20training%20only%2C%20EOV-Seg%20achieves%2024.2%20PQ%2C%2031.6%20mIoU%2C%20and%0A12.7%20FPS%20on%20the%20ADE20K%20dataset%20for%20panoptic%20and%20semantic%20segmentation%20tasks%20and%0Athe%20inference%20time%20of%20EOV-Seg%20is%204-21%20times%20faster%20than%20state-of-the-art%0Amethods.%20Especially%2C%20equipped%20with%20ResNet-50%20backbone%2C%20EOV-Seg%20runs%2025%20FPS%20with%0Aonly%2071M%20parameters%20on%20a%20single%20RTX%203090%20GPU.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/nhw649/EOV-Seg%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEOV-Seg%253A%2520Efficient%2520Open-Vocabulary%2520Panoptic%2520Segmentation%26entry.906535625%3DHongwei%2520Niu%2520and%2520Jie%2520Hu%2520and%2520Jianghang%2520Lin%2520and%2520Shengchuan%2520Zhang%26entry.1292438233%3D%2520%2520Open-vocabulary%2520panoptic%2520segmentation%2520aims%2520to%2520segment%2520and%2520classify%2520everything%250Ain%2520diverse%2520scenes%2520across%2520an%2520unbounded%2520vocabulary.%2520Existing%2520methods%2520typically%250Aemploy%2520two-stage%2520or%2520single-stage%2520framework.%2520The%2520two-stage%2520framework%2520involves%250Acropping%2520the%2520image%2520multiple%2520times%2520using%2520masks%2520generated%2520by%2520a%2520mask%2520generator%252C%250Afollowed%2520by%2520feature%2520extraction%252C%2520while%2520the%2520single-stage%2520framework%2520relies%2520on%2520a%250Aheavyweight%2520mask%2520decoder%2520to%2520make%2520up%2520for%2520the%2520lack%2520of%2520spatial%2520position%250Ainformation%2520through%2520self-attention%2520and%2520cross-attention%2520in%2520multiple%2520stacked%250ATransformer%2520blocks.%2520Both%2520methods%2520incur%2520substantial%2520computational%2520overhead%252C%250Athereby%2520hindering%2520the%2520efficiency%2520of%2520model%2520inference.%2520To%2520fill%2520the%2520gap%2520in%250Aefficiency%252C%2520we%2520propose%2520EOV-Seg%252C%2520a%2520novel%2520single-stage%252C%2520shared%252C%2520efficient%252C%2520and%250Aspatial-aware%2520framework%2520designed%2520for%2520open-vocabulary%2520panoptic%2520segmentation.%250ASpecifically%252C%2520EOV-Seg%2520innovates%2520in%2520two%2520aspects.%2520First%252C%2520a%2520Vocabulary-Aware%250ASelection%2520%2528VAS%2529%2520module%2520is%2520proposed%2520to%2520improve%2520the%2520semantic%2520comprehension%2520of%250Avisual%2520aggregated%2520features%2520and%2520alleviate%2520the%2520feature%2520interaction%2520burden%2520on%2520the%250Amask%2520decoder.%2520Second%252C%2520we%2520introduce%2520a%2520Two-way%2520Dynamic%2520Embedding%2520Experts%2520%2528TDEE%2529%252C%250Awhich%2520efficiently%2520utilizes%2520the%2520spatial%2520awareness%2520capabilities%2520of%2520ViT-based%2520CLIP%250Abackbone.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520EOV-Seg%2520is%2520the%2520first%2520open-vocabulary%250Apanoptic%2520segmentation%2520framework%2520towards%2520efficiency%252C%2520which%2520runs%2520faster%2520and%250Aachieves%2520competitive%2520performance%2520compared%2520with%2520state-of-the-art%2520methods.%250ASpecifically%252C%2520with%2520COCO%2520training%2520only%252C%2520EOV-Seg%2520achieves%252024.2%2520PQ%252C%252031.6%2520mIoU%252C%2520and%250A12.7%2520FPS%2520on%2520the%2520ADE20K%2520dataset%2520for%2520panoptic%2520and%2520semantic%2520segmentation%2520tasks%2520and%250Athe%2520inference%2520time%2520of%2520EOV-Seg%2520is%25204-21%2520times%2520faster%2520than%2520state-of-the-art%250Amethods.%2520Especially%252C%2520equipped%2520with%2520ResNet-50%2520backbone%252C%2520EOV-Seg%2520runs%252025%2520FPS%2520with%250Aonly%252071M%2520parameters%2520on%2520a%2520single%2520RTX%25203090%2520GPU.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/nhw649/EOV-Seg%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EOV-Seg%3A%20Efficient%20Open-Vocabulary%20Panoptic%20Segmentation&entry.906535625=Hongwei%20Niu%20and%20Jie%20Hu%20and%20Jianghang%20Lin%20and%20Shengchuan%20Zhang&entry.1292438233=%20%20Open-vocabulary%20panoptic%20segmentation%20aims%20to%20segment%20and%20classify%20everything%0Ain%20diverse%20scenes%20across%20an%20unbounded%20vocabulary.%20Existing%20methods%20typically%0Aemploy%20two-stage%20or%20single-stage%20framework.%20The%20two-stage%20framework%20involves%0Acropping%20the%20image%20multiple%20times%20using%20masks%20generated%20by%20a%20mask%20generator%2C%0Afollowed%20by%20feature%20extraction%2C%20while%20the%20single-stage%20framework%20relies%20on%20a%0Aheavyweight%20mask%20decoder%20to%20make%20up%20for%20the%20lack%20of%20spatial%20position%0Ainformation%20through%20self-attention%20and%20cross-attention%20in%20multiple%20stacked%0ATransformer%20blocks.%20Both%20methods%20incur%20substantial%20computational%20overhead%2C%0Athereby%20hindering%20the%20efficiency%20of%20model%20inference.%20To%20fill%20the%20gap%20in%0Aefficiency%2C%20we%20propose%20EOV-Seg%2C%20a%20novel%20single-stage%2C%20shared%2C%20efficient%2C%20and%0Aspatial-aware%20framework%20designed%20for%20open-vocabulary%20panoptic%20segmentation.%0ASpecifically%2C%20EOV-Seg%20innovates%20in%20two%20aspects.%20First%2C%20a%20Vocabulary-Aware%0ASelection%20%28VAS%29%20module%20is%20proposed%20to%20improve%20the%20semantic%20comprehension%20of%0Avisual%20aggregated%20features%20and%20alleviate%20the%20feature%20interaction%20burden%20on%20the%0Amask%20decoder.%20Second%2C%20we%20introduce%20a%20Two-way%20Dynamic%20Embedding%20Experts%20%28TDEE%29%2C%0Awhich%20efficiently%20utilizes%20the%20spatial%20awareness%20capabilities%20of%20ViT-based%20CLIP%0Abackbone.%20To%20the%20best%20of%20our%20knowledge%2C%20EOV-Seg%20is%20the%20first%20open-vocabulary%0Apanoptic%20segmentation%20framework%20towards%20efficiency%2C%20which%20runs%20faster%20and%0Aachieves%20competitive%20performance%20compared%20with%20state-of-the-art%20methods.%0ASpecifically%2C%20with%20COCO%20training%20only%2C%20EOV-Seg%20achieves%2024.2%20PQ%2C%2031.6%20mIoU%2C%20and%0A12.7%20FPS%20on%20the%20ADE20K%20dataset%20for%20panoptic%20and%20semantic%20segmentation%20tasks%20and%0Athe%20inference%20time%20of%20EOV-Seg%20is%204-21%20times%20faster%20than%20state-of-the-art%0Amethods.%20Especially%2C%20equipped%20with%20ResNet-50%20backbone%2C%20EOV-Seg%20runs%2025%20FPS%20with%0Aonly%2071M%20parameters%20on%20a%20single%20RTX%203090%20GPU.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/nhw649/EOV-Seg%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08628v1&entry.124074799=Read"},
{"title": "Physics Based Differentiable Rendering for Inverse Problems and Beyond", "author": "Preetish Kakkar and Srijani Mukherjee and Hariharan Ragothaman and Vishal Mehta", "abstract": "  Physics-based differentiable rendering (PBDR) has become an efficient method\nin computer vision, graphics, and machine learning for addressing an array of\ninverse problems. PBDR allows patterns to be generated from perceptions which\ncan be applied to enhance object attributes like geometry, substances, and\nlighting by adding physical models of light propagation and materials\ninteraction. Due to these capabilities, distinguished rendering has been\nemployed in a wider range of sectors such as autonomous navigation, scene\nreconstruction, and material design. We provide an extensive overview of PBDR\ntechniques in this study, emphasizing their creation, effectiveness, and\nlimitations while managing inverse situations. We demonstrate modern techniques\nand examine their value in everyday situations.\n", "link": "http://arxiv.org/abs/2412.08563v1", "date": "2024-12-11", "relevancy": 2.2626, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5854}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5682}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics%20Based%20Differentiable%20Rendering%20for%20Inverse%20Problems%20and%20Beyond&body=Title%3A%20Physics%20Based%20Differentiable%20Rendering%20for%20Inverse%20Problems%20and%20Beyond%0AAuthor%3A%20Preetish%20Kakkar%20and%20Srijani%20Mukherjee%20and%20Hariharan%20Ragothaman%20and%20Vishal%20Mehta%0AAbstract%3A%20%20%20Physics-based%20differentiable%20rendering%20%28PBDR%29%20has%20become%20an%20efficient%20method%0Ain%20computer%20vision%2C%20graphics%2C%20and%20machine%20learning%20for%20addressing%20an%20array%20of%0Ainverse%20problems.%20PBDR%20allows%20patterns%20to%20be%20generated%20from%20perceptions%20which%0Acan%20be%20applied%20to%20enhance%20object%20attributes%20like%20geometry%2C%20substances%2C%20and%0Alighting%20by%20adding%20physical%20models%20of%20light%20propagation%20and%20materials%0Ainteraction.%20Due%20to%20these%20capabilities%2C%20distinguished%20rendering%20has%20been%0Aemployed%20in%20a%20wider%20range%20of%20sectors%20such%20as%20autonomous%20navigation%2C%20scene%0Areconstruction%2C%20and%20material%20design.%20We%20provide%20an%20extensive%20overview%20of%20PBDR%0Atechniques%20in%20this%20study%2C%20emphasizing%20their%20creation%2C%20effectiveness%2C%20and%0Alimitations%20while%20managing%20inverse%20situations.%20We%20demonstrate%20modern%20techniques%0Aand%20examine%20their%20value%20in%20everyday%20situations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics%2520Based%2520Differentiable%2520Rendering%2520for%2520Inverse%2520Problems%2520and%2520Beyond%26entry.906535625%3DPreetish%2520Kakkar%2520and%2520Srijani%2520Mukherjee%2520and%2520Hariharan%2520Ragothaman%2520and%2520Vishal%2520Mehta%26entry.1292438233%3D%2520%2520Physics-based%2520differentiable%2520rendering%2520%2528PBDR%2529%2520has%2520become%2520an%2520efficient%2520method%250Ain%2520computer%2520vision%252C%2520graphics%252C%2520and%2520machine%2520learning%2520for%2520addressing%2520an%2520array%2520of%250Ainverse%2520problems.%2520PBDR%2520allows%2520patterns%2520to%2520be%2520generated%2520from%2520perceptions%2520which%250Acan%2520be%2520applied%2520to%2520enhance%2520object%2520attributes%2520like%2520geometry%252C%2520substances%252C%2520and%250Alighting%2520by%2520adding%2520physical%2520models%2520of%2520light%2520propagation%2520and%2520materials%250Ainteraction.%2520Due%2520to%2520these%2520capabilities%252C%2520distinguished%2520rendering%2520has%2520been%250Aemployed%2520in%2520a%2520wider%2520range%2520of%2520sectors%2520such%2520as%2520autonomous%2520navigation%252C%2520scene%250Areconstruction%252C%2520and%2520material%2520design.%2520We%2520provide%2520an%2520extensive%2520overview%2520of%2520PBDR%250Atechniques%2520in%2520this%2520study%252C%2520emphasizing%2520their%2520creation%252C%2520effectiveness%252C%2520and%250Alimitations%2520while%2520managing%2520inverse%2520situations.%2520We%2520demonstrate%2520modern%2520techniques%250Aand%2520examine%2520their%2520value%2520in%2520everyday%2520situations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics%20Based%20Differentiable%20Rendering%20for%20Inverse%20Problems%20and%20Beyond&entry.906535625=Preetish%20Kakkar%20and%20Srijani%20Mukherjee%20and%20Hariharan%20Ragothaman%20and%20Vishal%20Mehta&entry.1292438233=%20%20Physics-based%20differentiable%20rendering%20%28PBDR%29%20has%20become%20an%20efficient%20method%0Ain%20computer%20vision%2C%20graphics%2C%20and%20machine%20learning%20for%20addressing%20an%20array%20of%0Ainverse%20problems.%20PBDR%20allows%20patterns%20to%20be%20generated%20from%20perceptions%20which%0Acan%20be%20applied%20to%20enhance%20object%20attributes%20like%20geometry%2C%20substances%2C%20and%0Alighting%20by%20adding%20physical%20models%20of%20light%20propagation%20and%20materials%0Ainteraction.%20Due%20to%20these%20capabilities%2C%20distinguished%20rendering%20has%20been%0Aemployed%20in%20a%20wider%20range%20of%20sectors%20such%20as%20autonomous%20navigation%2C%20scene%0Areconstruction%2C%20and%20material%20design.%20We%20provide%20an%20extensive%20overview%20of%20PBDR%0Atechniques%20in%20this%20study%2C%20emphasizing%20their%20creation%2C%20effectiveness%2C%20and%0Alimitations%20while%20managing%20inverse%20situations.%20We%20demonstrate%20modern%20techniques%0Aand%20examine%20their%20value%20in%20everyday%20situations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08563v1&entry.124074799=Read"},
{"title": "Neural Observation Field Guided Hybrid Optimization of Camera Placement", "author": "Yihan Cao and Jiazhao Zhang and Zhinan Yu and Kai Xu", "abstract": "  Camera placement is crutial in multi-camera systems such as virtual reality,\nautonomous driving, and high-quality reconstruction. The camera placement\nchallenge lies in the nonlinear nature of high-dimensional parameters and the\nunavailability of gradients for target functions like coverage and visibility.\nConsequently, most existing methods tackle this challenge by leveraging\nnon-gradient-based optimization methods.In this work, we present a hybrid\ncamera placement optimization approach that incorporates both gradient-based\nand non-gradient-based optimization methods. This design allows our method to\nenjoy the advantages of smooth optimization convergence and robustness from\ngradient-based and non-gradient-based optimization, respectively. To bridge the\ntwo disparate optimization methods, we propose a neural observation field,\nwhich implicitly encodes the coverage and observation quality. The neural\nobservation field provides the measurements of the camera observations and\ncorresponding gradients without the assumption of target scenes, making our\nmethod applicable to diverse scenarios, including 2D planar shapes, 3D objects,\nand room-scale 3D scenes.Extensive experiments on diverse datasets demonstrate\nthat our method achieves state-of-the-art performance, while requiring only a\nfraction (8x less) of the typical computation time. Furthermore, we conducted a\nreal-world experiment using a custom-built capture system, confirming the\nresilience of our approach to real-world environmental noise.\n", "link": "http://arxiv.org/abs/2412.08266v1", "date": "2024-12-11", "relevancy": 2.2529, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5796}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.555}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Observation%20Field%20Guided%20Hybrid%20Optimization%20of%20Camera%20Placement&body=Title%3A%20Neural%20Observation%20Field%20Guided%20Hybrid%20Optimization%20of%20Camera%20Placement%0AAuthor%3A%20Yihan%20Cao%20and%20Jiazhao%20Zhang%20and%20Zhinan%20Yu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20Camera%20placement%20is%20crutial%20in%20multi-camera%20systems%20such%20as%20virtual%20reality%2C%0Aautonomous%20driving%2C%20and%20high-quality%20reconstruction.%20The%20camera%20placement%0Achallenge%20lies%20in%20the%20nonlinear%20nature%20of%20high-dimensional%20parameters%20and%20the%0Aunavailability%20of%20gradients%20for%20target%20functions%20like%20coverage%20and%20visibility.%0AConsequently%2C%20most%20existing%20methods%20tackle%20this%20challenge%20by%20leveraging%0Anon-gradient-based%20optimization%20methods.In%20this%20work%2C%20we%20present%20a%20hybrid%0Acamera%20placement%20optimization%20approach%20that%20incorporates%20both%20gradient-based%0Aand%20non-gradient-based%20optimization%20methods.%20This%20design%20allows%20our%20method%20to%0Aenjoy%20the%20advantages%20of%20smooth%20optimization%20convergence%20and%20robustness%20from%0Agradient-based%20and%20non-gradient-based%20optimization%2C%20respectively.%20To%20bridge%20the%0Atwo%20disparate%20optimization%20methods%2C%20we%20propose%20a%20neural%20observation%20field%2C%0Awhich%20implicitly%20encodes%20the%20coverage%20and%20observation%20quality.%20The%20neural%0Aobservation%20field%20provides%20the%20measurements%20of%20the%20camera%20observations%20and%0Acorresponding%20gradients%20without%20the%20assumption%20of%20target%20scenes%2C%20making%20our%0Amethod%20applicable%20to%20diverse%20scenarios%2C%20including%202D%20planar%20shapes%2C%203D%20objects%2C%0Aand%20room-scale%203D%20scenes.Extensive%20experiments%20on%20diverse%20datasets%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20performance%2C%20while%20requiring%20only%20a%0Afraction%20%288x%20less%29%20of%20the%20typical%20computation%20time.%20Furthermore%2C%20we%20conducted%20a%0Areal-world%20experiment%20using%20a%20custom-built%20capture%20system%2C%20confirming%20the%0Aresilience%20of%20our%20approach%20to%20real-world%20environmental%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Observation%2520Field%2520Guided%2520Hybrid%2520Optimization%2520of%2520Camera%2520Placement%26entry.906535625%3DYihan%2520Cao%2520and%2520Jiazhao%2520Zhang%2520and%2520Zhinan%2520Yu%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520Camera%2520placement%2520is%2520crutial%2520in%2520multi-camera%2520systems%2520such%2520as%2520virtual%2520reality%252C%250Aautonomous%2520driving%252C%2520and%2520high-quality%2520reconstruction.%2520The%2520camera%2520placement%250Achallenge%2520lies%2520in%2520the%2520nonlinear%2520nature%2520of%2520high-dimensional%2520parameters%2520and%2520the%250Aunavailability%2520of%2520gradients%2520for%2520target%2520functions%2520like%2520coverage%2520and%2520visibility.%250AConsequently%252C%2520most%2520existing%2520methods%2520tackle%2520this%2520challenge%2520by%2520leveraging%250Anon-gradient-based%2520optimization%2520methods.In%2520this%2520work%252C%2520we%2520present%2520a%2520hybrid%250Acamera%2520placement%2520optimization%2520approach%2520that%2520incorporates%2520both%2520gradient-based%250Aand%2520non-gradient-based%2520optimization%2520methods.%2520This%2520design%2520allows%2520our%2520method%2520to%250Aenjoy%2520the%2520advantages%2520of%2520smooth%2520optimization%2520convergence%2520and%2520robustness%2520from%250Agradient-based%2520and%2520non-gradient-based%2520optimization%252C%2520respectively.%2520To%2520bridge%2520the%250Atwo%2520disparate%2520optimization%2520methods%252C%2520we%2520propose%2520a%2520neural%2520observation%2520field%252C%250Awhich%2520implicitly%2520encodes%2520the%2520coverage%2520and%2520observation%2520quality.%2520The%2520neural%250Aobservation%2520field%2520provides%2520the%2520measurements%2520of%2520the%2520camera%2520observations%2520and%250Acorresponding%2520gradients%2520without%2520the%2520assumption%2520of%2520target%2520scenes%252C%2520making%2520our%250Amethod%2520applicable%2520to%2520diverse%2520scenarios%252C%2520including%25202D%2520planar%2520shapes%252C%25203D%2520objects%252C%250Aand%2520room-scale%25203D%2520scenes.Extensive%2520experiments%2520on%2520diverse%2520datasets%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%252C%2520while%2520requiring%2520only%2520a%250Afraction%2520%25288x%2520less%2529%2520of%2520the%2520typical%2520computation%2520time.%2520Furthermore%252C%2520we%2520conducted%2520a%250Areal-world%2520experiment%2520using%2520a%2520custom-built%2520capture%2520system%252C%2520confirming%2520the%250Aresilience%2520of%2520our%2520approach%2520to%2520real-world%2520environmental%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Observation%20Field%20Guided%20Hybrid%20Optimization%20of%20Camera%20Placement&entry.906535625=Yihan%20Cao%20and%20Jiazhao%20Zhang%20and%20Zhinan%20Yu%20and%20Kai%20Xu&entry.1292438233=%20%20Camera%20placement%20is%20crutial%20in%20multi-camera%20systems%20such%20as%20virtual%20reality%2C%0Aautonomous%20driving%2C%20and%20high-quality%20reconstruction.%20The%20camera%20placement%0Achallenge%20lies%20in%20the%20nonlinear%20nature%20of%20high-dimensional%20parameters%20and%20the%0Aunavailability%20of%20gradients%20for%20target%20functions%20like%20coverage%20and%20visibility.%0AConsequently%2C%20most%20existing%20methods%20tackle%20this%20challenge%20by%20leveraging%0Anon-gradient-based%20optimization%20methods.In%20this%20work%2C%20we%20present%20a%20hybrid%0Acamera%20placement%20optimization%20approach%20that%20incorporates%20both%20gradient-based%0Aand%20non-gradient-based%20optimization%20methods.%20This%20design%20allows%20our%20method%20to%0Aenjoy%20the%20advantages%20of%20smooth%20optimization%20convergence%20and%20robustness%20from%0Agradient-based%20and%20non-gradient-based%20optimization%2C%20respectively.%20To%20bridge%20the%0Atwo%20disparate%20optimization%20methods%2C%20we%20propose%20a%20neural%20observation%20field%2C%0Awhich%20implicitly%20encodes%20the%20coverage%20and%20observation%20quality.%20The%20neural%0Aobservation%20field%20provides%20the%20measurements%20of%20the%20camera%20observations%20and%0Acorresponding%20gradients%20without%20the%20assumption%20of%20target%20scenes%2C%20making%20our%0Amethod%20applicable%20to%20diverse%20scenarios%2C%20including%202D%20planar%20shapes%2C%203D%20objects%2C%0Aand%20room-scale%203D%20scenes.Extensive%20experiments%20on%20diverse%20datasets%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20performance%2C%20while%20requiring%20only%20a%0Afraction%20%288x%20less%29%20of%20the%20typical%20computation%20time.%20Furthermore%2C%20we%20conducted%20a%0Areal-world%20experiment%20using%20a%20custom-built%20capture%20system%2C%20confirming%20the%0Aresilience%20of%20our%20approach%20to%20real-world%20environmental%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08266v1&entry.124074799=Read"},
{"title": "From Logistic Regression to the Perceptron Algorithm: Exploring Gradient\n  Descent with Large Step Sizes", "author": "Alexander Tyurin", "abstract": "  We focus on the classification problem with a separable dataset, one of the\nmost important and classical problems from machine learning. The standard\napproach to this task is logistic regression with gradient descent (LR+GD).\nRecent studies have observed that LR+GD can find a solution with arbitrarily\nlarge step sizes, defying conventional optimization theory. Our work\ninvestigates this phenomenon and makes three interconnected key observations\nabout LR+GD with large step sizes. First, we find a remarkably simple\nexplanation of why LR+GD with large step sizes solves the classification\nproblem: LR+GD reduces to a batch version of the celebrated perceptron\nalgorithm when the step size $\\gamma \\to \\infty.$ Second, we observe that\nlarger step sizes lead LR+GD to higher logistic losses when it tends to the\nperceptron algorithm, but larger step sizes also lead to faster convergence to\na solution for the classification problem, meaning that logistic loss is an\nunreliable metric of the proximity to a solution. Surprisingly, high loss\nvalues can actually indicate faster convergence. Third, since the convergence\nrate in terms of loss function values of LR+GD is unreliable, we examine the\niteration complexity required by LR+GD with large step sizes to solve the\nclassification problem and prove that this complexity is suboptimal. To address\nthis, we propose a new method, Normalized LR+GD - based on the connection\nbetween LR+GD and the perceptron algorithm - with much better theoretical\nguarantees.\n", "link": "http://arxiv.org/abs/2412.08424v1", "date": "2024-12-11", "relevancy": 2.2515, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.452}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4515}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Logistic%20Regression%20to%20the%20Perceptron%20Algorithm%3A%20Exploring%20Gradient%0A%20%20Descent%20with%20Large%20Step%20Sizes&body=Title%3A%20From%20Logistic%20Regression%20to%20the%20Perceptron%20Algorithm%3A%20Exploring%20Gradient%0A%20%20Descent%20with%20Large%20Step%20Sizes%0AAuthor%3A%20Alexander%20Tyurin%0AAbstract%3A%20%20%20We%20focus%20on%20the%20classification%20problem%20with%20a%20separable%20dataset%2C%20one%20of%20the%0Amost%20important%20and%20classical%20problems%20from%20machine%20learning.%20The%20standard%0Aapproach%20to%20this%20task%20is%20logistic%20regression%20with%20gradient%20descent%20%28LR%2BGD%29.%0ARecent%20studies%20have%20observed%20that%20LR%2BGD%20can%20find%20a%20solution%20with%20arbitrarily%0Alarge%20step%20sizes%2C%20defying%20conventional%20optimization%20theory.%20Our%20work%0Ainvestigates%20this%20phenomenon%20and%20makes%20three%20interconnected%20key%20observations%0Aabout%20LR%2BGD%20with%20large%20step%20sizes.%20First%2C%20we%20find%20a%20remarkably%20simple%0Aexplanation%20of%20why%20LR%2BGD%20with%20large%20step%20sizes%20solves%20the%20classification%0Aproblem%3A%20LR%2BGD%20reduces%20to%20a%20batch%20version%20of%20the%20celebrated%20perceptron%0Aalgorithm%20when%20the%20step%20size%20%24%5Cgamma%20%5Cto%20%5Cinfty.%24%20Second%2C%20we%20observe%20that%0Alarger%20step%20sizes%20lead%20LR%2BGD%20to%20higher%20logistic%20losses%20when%20it%20tends%20to%20the%0Aperceptron%20algorithm%2C%20but%20larger%20step%20sizes%20also%20lead%20to%20faster%20convergence%20to%0Aa%20solution%20for%20the%20classification%20problem%2C%20meaning%20that%20logistic%20loss%20is%20an%0Aunreliable%20metric%20of%20the%20proximity%20to%20a%20solution.%20Surprisingly%2C%20high%20loss%0Avalues%20can%20actually%20indicate%20faster%20convergence.%20Third%2C%20since%20the%20convergence%0Arate%20in%20terms%20of%20loss%20function%20values%20of%20LR%2BGD%20is%20unreliable%2C%20we%20examine%20the%0Aiteration%20complexity%20required%20by%20LR%2BGD%20with%20large%20step%20sizes%20to%20solve%20the%0Aclassification%20problem%20and%20prove%20that%20this%20complexity%20is%20suboptimal.%20To%20address%0Athis%2C%20we%20propose%20a%20new%20method%2C%20Normalized%20LR%2BGD%20-%20based%20on%20the%20connection%0Abetween%20LR%2BGD%20and%20the%20perceptron%20algorithm%20-%20with%20much%20better%20theoretical%0Aguarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Logistic%2520Regression%2520to%2520the%2520Perceptron%2520Algorithm%253A%2520Exploring%2520Gradient%250A%2520%2520Descent%2520with%2520Large%2520Step%2520Sizes%26entry.906535625%3DAlexander%2520Tyurin%26entry.1292438233%3D%2520%2520We%2520focus%2520on%2520the%2520classification%2520problem%2520with%2520a%2520separable%2520dataset%252C%2520one%2520of%2520the%250Amost%2520important%2520and%2520classical%2520problems%2520from%2520machine%2520learning.%2520The%2520standard%250Aapproach%2520to%2520this%2520task%2520is%2520logistic%2520regression%2520with%2520gradient%2520descent%2520%2528LR%252BGD%2529.%250ARecent%2520studies%2520have%2520observed%2520that%2520LR%252BGD%2520can%2520find%2520a%2520solution%2520with%2520arbitrarily%250Alarge%2520step%2520sizes%252C%2520defying%2520conventional%2520optimization%2520theory.%2520Our%2520work%250Ainvestigates%2520this%2520phenomenon%2520and%2520makes%2520three%2520interconnected%2520key%2520observations%250Aabout%2520LR%252BGD%2520with%2520large%2520step%2520sizes.%2520First%252C%2520we%2520find%2520a%2520remarkably%2520simple%250Aexplanation%2520of%2520why%2520LR%252BGD%2520with%2520large%2520step%2520sizes%2520solves%2520the%2520classification%250Aproblem%253A%2520LR%252BGD%2520reduces%2520to%2520a%2520batch%2520version%2520of%2520the%2520celebrated%2520perceptron%250Aalgorithm%2520when%2520the%2520step%2520size%2520%2524%255Cgamma%2520%255Cto%2520%255Cinfty.%2524%2520Second%252C%2520we%2520observe%2520that%250Alarger%2520step%2520sizes%2520lead%2520LR%252BGD%2520to%2520higher%2520logistic%2520losses%2520when%2520it%2520tends%2520to%2520the%250Aperceptron%2520algorithm%252C%2520but%2520larger%2520step%2520sizes%2520also%2520lead%2520to%2520faster%2520convergence%2520to%250Aa%2520solution%2520for%2520the%2520classification%2520problem%252C%2520meaning%2520that%2520logistic%2520loss%2520is%2520an%250Aunreliable%2520metric%2520of%2520the%2520proximity%2520to%2520a%2520solution.%2520Surprisingly%252C%2520high%2520loss%250Avalues%2520can%2520actually%2520indicate%2520faster%2520convergence.%2520Third%252C%2520since%2520the%2520convergence%250Arate%2520in%2520terms%2520of%2520loss%2520function%2520values%2520of%2520LR%252BGD%2520is%2520unreliable%252C%2520we%2520examine%2520the%250Aiteration%2520complexity%2520required%2520by%2520LR%252BGD%2520with%2520large%2520step%2520sizes%2520to%2520solve%2520the%250Aclassification%2520problem%2520and%2520prove%2520that%2520this%2520complexity%2520is%2520suboptimal.%2520To%2520address%250Athis%252C%2520we%2520propose%2520a%2520new%2520method%252C%2520Normalized%2520LR%252BGD%2520-%2520based%2520on%2520the%2520connection%250Abetween%2520LR%252BGD%2520and%2520the%2520perceptron%2520algorithm%2520-%2520with%2520much%2520better%2520theoretical%250Aguarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Logistic%20Regression%20to%20the%20Perceptron%20Algorithm%3A%20Exploring%20Gradient%0A%20%20Descent%20with%20Large%20Step%20Sizes&entry.906535625=Alexander%20Tyurin&entry.1292438233=%20%20We%20focus%20on%20the%20classification%20problem%20with%20a%20separable%20dataset%2C%20one%20of%20the%0Amost%20important%20and%20classical%20problems%20from%20machine%20learning.%20The%20standard%0Aapproach%20to%20this%20task%20is%20logistic%20regression%20with%20gradient%20descent%20%28LR%2BGD%29.%0ARecent%20studies%20have%20observed%20that%20LR%2BGD%20can%20find%20a%20solution%20with%20arbitrarily%0Alarge%20step%20sizes%2C%20defying%20conventional%20optimization%20theory.%20Our%20work%0Ainvestigates%20this%20phenomenon%20and%20makes%20three%20interconnected%20key%20observations%0Aabout%20LR%2BGD%20with%20large%20step%20sizes.%20First%2C%20we%20find%20a%20remarkably%20simple%0Aexplanation%20of%20why%20LR%2BGD%20with%20large%20step%20sizes%20solves%20the%20classification%0Aproblem%3A%20LR%2BGD%20reduces%20to%20a%20batch%20version%20of%20the%20celebrated%20perceptron%0Aalgorithm%20when%20the%20step%20size%20%24%5Cgamma%20%5Cto%20%5Cinfty.%24%20Second%2C%20we%20observe%20that%0Alarger%20step%20sizes%20lead%20LR%2BGD%20to%20higher%20logistic%20losses%20when%20it%20tends%20to%20the%0Aperceptron%20algorithm%2C%20but%20larger%20step%20sizes%20also%20lead%20to%20faster%20convergence%20to%0Aa%20solution%20for%20the%20classification%20problem%2C%20meaning%20that%20logistic%20loss%20is%20an%0Aunreliable%20metric%20of%20the%20proximity%20to%20a%20solution.%20Surprisingly%2C%20high%20loss%0Avalues%20can%20actually%20indicate%20faster%20convergence.%20Third%2C%20since%20the%20convergence%0Arate%20in%20terms%20of%20loss%20function%20values%20of%20LR%2BGD%20is%20unreliable%2C%20we%20examine%20the%0Aiteration%20complexity%20required%20by%20LR%2BGD%20with%20large%20step%20sizes%20to%20solve%20the%0Aclassification%20problem%20and%20prove%20that%20this%20complexity%20is%20suboptimal.%20To%20address%0Athis%2C%20we%20propose%20a%20new%20method%2C%20Normalized%20LR%2BGD%20-%20based%20on%20the%20connection%0Abetween%20LR%2BGD%20and%20the%20perceptron%20algorithm%20-%20with%20much%20better%20theoretical%0Aguarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08424v1&entry.124074799=Read"},
{"title": "INRetouch: Context Aware Implicit Neural Representation for Photography\n  Retouching", "author": "Omar Elezabi and Marcos V. Conde and Zongwei Wu and Radu Timofte", "abstract": "  Professional photo editing remains challenging, requiring extensive knowledge\nof imaging pipelines and significant expertise. With the ubiquity of smartphone\nphotography, there is an increasing demand for accessible yet sophisticated\nimage editing solutions. While recent deep learning approaches, particularly\nstyle transfer methods, have attempted to automate this process, they often\nstruggle with output fidelity, editing control, and complex retouching\ncapabilities. We propose a novel retouch transfer approach that learns from\nprofessional edits through before-after image pairs, enabling precise\nreplication of complex editing operations. To facilitate this research\ndirection, we introduce a comprehensive Photo Retouching Dataset comprising\n100,000 high-quality images edited using over 170 professional Adobe Lightroom\npresets. We develop a context-aware Implicit Neural Representation that learns\nto apply edits adaptively based on image content and context, requiring no\npretraining and capable of learning from a single example. Our method extracts\nimplicit transformations from reference edits and adaptively applies them to\nnew images. Through extensive evaluation, we demonstrate that our approach not\nonly surpasses existing methods in photo retouching but also enhances\nperformance in related image reconstruction tasks like Gamut Mapping and Raw\nReconstruction. By bridging the gap between professional editing capabilities\nand automated solutions, our work presents a significant step toward making\nsophisticated photo editing more accessible while maintaining high-fidelity\nresults. Check the Project Page at https://omaralezaby.github.io/inretouch for\nmore Results and information about Code and Dataset availability.\n", "link": "http://arxiv.org/abs/2412.03848v2", "date": "2024-12-11", "relevancy": 2.2438, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5763}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5622}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20INRetouch%3A%20Context%20Aware%20Implicit%20Neural%20Representation%20for%20Photography%0A%20%20Retouching&body=Title%3A%20INRetouch%3A%20Context%20Aware%20Implicit%20Neural%20Representation%20for%20Photography%0A%20%20Retouching%0AAuthor%3A%20Omar%20Elezabi%20and%20Marcos%20V.%20Conde%20and%20Zongwei%20Wu%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20Professional%20photo%20editing%20remains%20challenging%2C%20requiring%20extensive%20knowledge%0Aof%20imaging%20pipelines%20and%20significant%20expertise.%20With%20the%20ubiquity%20of%20smartphone%0Aphotography%2C%20there%20is%20an%20increasing%20demand%20for%20accessible%20yet%20sophisticated%0Aimage%20editing%20solutions.%20While%20recent%20deep%20learning%20approaches%2C%20particularly%0Astyle%20transfer%20methods%2C%20have%20attempted%20to%20automate%20this%20process%2C%20they%20often%0Astruggle%20with%20output%20fidelity%2C%20editing%20control%2C%20and%20complex%20retouching%0Acapabilities.%20We%20propose%20a%20novel%20retouch%20transfer%20approach%20that%20learns%20from%0Aprofessional%20edits%20through%20before-after%20image%20pairs%2C%20enabling%20precise%0Areplication%20of%20complex%20editing%20operations.%20To%20facilitate%20this%20research%0Adirection%2C%20we%20introduce%20a%20comprehensive%20Photo%20Retouching%20Dataset%20comprising%0A100%2C000%20high-quality%20images%20edited%20using%20over%20170%20professional%20Adobe%20Lightroom%0Apresets.%20We%20develop%20a%20context-aware%20Implicit%20Neural%20Representation%20that%20learns%0Ato%20apply%20edits%20adaptively%20based%20on%20image%20content%20and%20context%2C%20requiring%20no%0Apretraining%20and%20capable%20of%20learning%20from%20a%20single%20example.%20Our%20method%20extracts%0Aimplicit%20transformations%20from%20reference%20edits%20and%20adaptively%20applies%20them%20to%0Anew%20images.%20Through%20extensive%20evaluation%2C%20we%20demonstrate%20that%20our%20approach%20not%0Aonly%20surpasses%20existing%20methods%20in%20photo%20retouching%20but%20also%20enhances%0Aperformance%20in%20related%20image%20reconstruction%20tasks%20like%20Gamut%20Mapping%20and%20Raw%0AReconstruction.%20By%20bridging%20the%20gap%20between%20professional%20editing%20capabilities%0Aand%20automated%20solutions%2C%20our%20work%20presents%20a%20significant%20step%20toward%20making%0Asophisticated%20photo%20editing%20more%20accessible%20while%20maintaining%20high-fidelity%0Aresults.%20Check%20the%20Project%20Page%20at%20https%3A//omaralezaby.github.io/inretouch%20for%0Amore%20Results%20and%20information%20about%20Code%20and%20Dataset%20availability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DINRetouch%253A%2520Context%2520Aware%2520Implicit%2520Neural%2520Representation%2520for%2520Photography%250A%2520%2520Retouching%26entry.906535625%3DOmar%2520Elezabi%2520and%2520Marcos%2520V.%2520Conde%2520and%2520Zongwei%2520Wu%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520Professional%2520photo%2520editing%2520remains%2520challenging%252C%2520requiring%2520extensive%2520knowledge%250Aof%2520imaging%2520pipelines%2520and%2520significant%2520expertise.%2520With%2520the%2520ubiquity%2520of%2520smartphone%250Aphotography%252C%2520there%2520is%2520an%2520increasing%2520demand%2520for%2520accessible%2520yet%2520sophisticated%250Aimage%2520editing%2520solutions.%2520While%2520recent%2520deep%2520learning%2520approaches%252C%2520particularly%250Astyle%2520transfer%2520methods%252C%2520have%2520attempted%2520to%2520automate%2520this%2520process%252C%2520they%2520often%250Astruggle%2520with%2520output%2520fidelity%252C%2520editing%2520control%252C%2520and%2520complex%2520retouching%250Acapabilities.%2520We%2520propose%2520a%2520novel%2520retouch%2520transfer%2520approach%2520that%2520learns%2520from%250Aprofessional%2520edits%2520through%2520before-after%2520image%2520pairs%252C%2520enabling%2520precise%250Areplication%2520of%2520complex%2520editing%2520operations.%2520To%2520facilitate%2520this%2520research%250Adirection%252C%2520we%2520introduce%2520a%2520comprehensive%2520Photo%2520Retouching%2520Dataset%2520comprising%250A100%252C000%2520high-quality%2520images%2520edited%2520using%2520over%2520170%2520professional%2520Adobe%2520Lightroom%250Apresets.%2520We%2520develop%2520a%2520context-aware%2520Implicit%2520Neural%2520Representation%2520that%2520learns%250Ato%2520apply%2520edits%2520adaptively%2520based%2520on%2520image%2520content%2520and%2520context%252C%2520requiring%2520no%250Apretraining%2520and%2520capable%2520of%2520learning%2520from%2520a%2520single%2520example.%2520Our%2520method%2520extracts%250Aimplicit%2520transformations%2520from%2520reference%2520edits%2520and%2520adaptively%2520applies%2520them%2520to%250Anew%2520images.%2520Through%2520extensive%2520evaluation%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520not%250Aonly%2520surpasses%2520existing%2520methods%2520in%2520photo%2520retouching%2520but%2520also%2520enhances%250Aperformance%2520in%2520related%2520image%2520reconstruction%2520tasks%2520like%2520Gamut%2520Mapping%2520and%2520Raw%250AReconstruction.%2520By%2520bridging%2520the%2520gap%2520between%2520professional%2520editing%2520capabilities%250Aand%2520automated%2520solutions%252C%2520our%2520work%2520presents%2520a%2520significant%2520step%2520toward%2520making%250Asophisticated%2520photo%2520editing%2520more%2520accessible%2520while%2520maintaining%2520high-fidelity%250Aresults.%2520Check%2520the%2520Project%2520Page%2520at%2520https%253A//omaralezaby.github.io/inretouch%2520for%250Amore%2520Results%2520and%2520information%2520about%2520Code%2520and%2520Dataset%2520availability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INRetouch%3A%20Context%20Aware%20Implicit%20Neural%20Representation%20for%20Photography%0A%20%20Retouching&entry.906535625=Omar%20Elezabi%20and%20Marcos%20V.%20Conde%20and%20Zongwei%20Wu%20and%20Radu%20Timofte&entry.1292438233=%20%20Professional%20photo%20editing%20remains%20challenging%2C%20requiring%20extensive%20knowledge%0Aof%20imaging%20pipelines%20and%20significant%20expertise.%20With%20the%20ubiquity%20of%20smartphone%0Aphotography%2C%20there%20is%20an%20increasing%20demand%20for%20accessible%20yet%20sophisticated%0Aimage%20editing%20solutions.%20While%20recent%20deep%20learning%20approaches%2C%20particularly%0Astyle%20transfer%20methods%2C%20have%20attempted%20to%20automate%20this%20process%2C%20they%20often%0Astruggle%20with%20output%20fidelity%2C%20editing%20control%2C%20and%20complex%20retouching%0Acapabilities.%20We%20propose%20a%20novel%20retouch%20transfer%20approach%20that%20learns%20from%0Aprofessional%20edits%20through%20before-after%20image%20pairs%2C%20enabling%20precise%0Areplication%20of%20complex%20editing%20operations.%20To%20facilitate%20this%20research%0Adirection%2C%20we%20introduce%20a%20comprehensive%20Photo%20Retouching%20Dataset%20comprising%0A100%2C000%20high-quality%20images%20edited%20using%20over%20170%20professional%20Adobe%20Lightroom%0Apresets.%20We%20develop%20a%20context-aware%20Implicit%20Neural%20Representation%20that%20learns%0Ato%20apply%20edits%20adaptively%20based%20on%20image%20content%20and%20context%2C%20requiring%20no%0Apretraining%20and%20capable%20of%20learning%20from%20a%20single%20example.%20Our%20method%20extracts%0Aimplicit%20transformations%20from%20reference%20edits%20and%20adaptively%20applies%20them%20to%0Anew%20images.%20Through%20extensive%20evaluation%2C%20we%20demonstrate%20that%20our%20approach%20not%0Aonly%20surpasses%20existing%20methods%20in%20photo%20retouching%20but%20also%20enhances%0Aperformance%20in%20related%20image%20reconstruction%20tasks%20like%20Gamut%20Mapping%20and%20Raw%0AReconstruction.%20By%20bridging%20the%20gap%20between%20professional%20editing%20capabilities%0Aand%20automated%20solutions%2C%20our%20work%20presents%20a%20significant%20step%20toward%20making%0Asophisticated%20photo%20editing%20more%20accessible%20while%20maintaining%20high-fidelity%0Aresults.%20Check%20the%20Project%20Page%20at%20https%3A//omaralezaby.github.io/inretouch%20for%0Amore%20Results%20and%20information%20about%20Code%20and%20Dataset%20availability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03848v2&entry.124074799=Read"},
{"title": "Image Retrieval Methods in the Dissimilarity Space", "author": "Madhu Kiran and Kartikey Vishnu and Rafael M. O. Cruz and Eric Granger", "abstract": "  Image retrieval methods rely on metric learning to train backbone feature\nextraction models that can extract discriminant queries and reference (gallery)\nfeature representations for similarity matching. Although state-of-the-art\naccuracy has improved considerably with the advent of deep learning (DL) models\ntrained on large datasets, image retrieval remains challenging in many\nreal-world video analytics and surveillance applications, e.g., person\nre-identification. Using the Euclidean space for matching limits the\nperformance in real-world applications due to the curse of dimensionality,\noverfitting, and sensitivity to noisy data.\n  We argue that the feature dissimilarity space is more suitable for similarity\nmatching, and propose a dichotomy transformation to project query and reference\nembeddings into a single embedding in the dissimilarity space.\n  We also advocate for end-to-end training of a backbone and binary\nclassification models for pair-wise matching. As opposed to comparing the\ndistance between queries and reference embeddings, we show the benefits of\nclassifying the single dissimilarity space embedding (as similar or\ndissimilar), especially when trained end-to-end. We propose a method to train\nthe max-margin classifier together with the backbone feature extractor by\napplying constraints to the L2 norm of the classifier weights along with the\nhinge loss.\n  Our extensive experiments on challenging image retrieval datasets and using\ndiverse feature extraction backbones highlight the benefits of similarity\nmatching in the dissimilarity space. In particular, when jointly training the\nfeature extraction backbone and regularised classifier for matching, the\ndissimilarity space provides a higher level of accuracy.\n", "link": "http://arxiv.org/abs/2412.08618v1", "date": "2024-12-11", "relevancy": 2.2378, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.583}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5479}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Retrieval%20Methods%20in%20the%20Dissimilarity%20Space&body=Title%3A%20Image%20Retrieval%20Methods%20in%20the%20Dissimilarity%20Space%0AAuthor%3A%20Madhu%20Kiran%20and%20Kartikey%20Vishnu%20and%20Rafael%20M.%20O.%20Cruz%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Image%20retrieval%20methods%20rely%20on%20metric%20learning%20to%20train%20backbone%20feature%0Aextraction%20models%20that%20can%20extract%20discriminant%20queries%20and%20reference%20%28gallery%29%0Afeature%20representations%20for%20similarity%20matching.%20Although%20state-of-the-art%0Aaccuracy%20has%20improved%20considerably%20with%20the%20advent%20of%20deep%20learning%20%28DL%29%20models%0Atrained%20on%20large%20datasets%2C%20image%20retrieval%20remains%20challenging%20in%20many%0Areal-world%20video%20analytics%20and%20surveillance%20applications%2C%20e.g.%2C%20person%0Are-identification.%20Using%20the%20Euclidean%20space%20for%20matching%20limits%20the%0Aperformance%20in%20real-world%20applications%20due%20to%20the%20curse%20of%20dimensionality%2C%0Aoverfitting%2C%20and%20sensitivity%20to%20noisy%20data.%0A%20%20We%20argue%20that%20the%20feature%20dissimilarity%20space%20is%20more%20suitable%20for%20similarity%0Amatching%2C%20and%20propose%20a%20dichotomy%20transformation%20to%20project%20query%20and%20reference%0Aembeddings%20into%20a%20single%20embedding%20in%20the%20dissimilarity%20space.%0A%20%20We%20also%20advocate%20for%20end-to-end%20training%20of%20a%20backbone%20and%20binary%0Aclassification%20models%20for%20pair-wise%20matching.%20As%20opposed%20to%20comparing%20the%0Adistance%20between%20queries%20and%20reference%20embeddings%2C%20we%20show%20the%20benefits%20of%0Aclassifying%20the%20single%20dissimilarity%20space%20embedding%20%28as%20similar%20or%0Adissimilar%29%2C%20especially%20when%20trained%20end-to-end.%20We%20propose%20a%20method%20to%20train%0Athe%20max-margin%20classifier%20together%20with%20the%20backbone%20feature%20extractor%20by%0Aapplying%20constraints%20to%20the%20L2%20norm%20of%20the%20classifier%20weights%20along%20with%20the%0Ahinge%20loss.%0A%20%20Our%20extensive%20experiments%20on%20challenging%20image%20retrieval%20datasets%20and%20using%0Adiverse%20feature%20extraction%20backbones%20highlight%20the%20benefits%20of%20similarity%0Amatching%20in%20the%20dissimilarity%20space.%20In%20particular%2C%20when%20jointly%20training%20the%0Afeature%20extraction%20backbone%20and%20regularised%20classifier%20for%20matching%2C%20the%0Adissimilarity%20space%20provides%20a%20higher%20level%20of%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Retrieval%2520Methods%2520in%2520the%2520Dissimilarity%2520Space%26entry.906535625%3DMadhu%2520Kiran%2520and%2520Kartikey%2520Vishnu%2520and%2520Rafael%2520M.%2520O.%2520Cruz%2520and%2520Eric%2520Granger%26entry.1292438233%3D%2520%2520Image%2520retrieval%2520methods%2520rely%2520on%2520metric%2520learning%2520to%2520train%2520backbone%2520feature%250Aextraction%2520models%2520that%2520can%2520extract%2520discriminant%2520queries%2520and%2520reference%2520%2528gallery%2529%250Afeature%2520representations%2520for%2520similarity%2520matching.%2520Although%2520state-of-the-art%250Aaccuracy%2520has%2520improved%2520considerably%2520with%2520the%2520advent%2520of%2520deep%2520learning%2520%2528DL%2529%2520models%250Atrained%2520on%2520large%2520datasets%252C%2520image%2520retrieval%2520remains%2520challenging%2520in%2520many%250Areal-world%2520video%2520analytics%2520and%2520surveillance%2520applications%252C%2520e.g.%252C%2520person%250Are-identification.%2520Using%2520the%2520Euclidean%2520space%2520for%2520matching%2520limits%2520the%250Aperformance%2520in%2520real-world%2520applications%2520due%2520to%2520the%2520curse%2520of%2520dimensionality%252C%250Aoverfitting%252C%2520and%2520sensitivity%2520to%2520noisy%2520data.%250A%2520%2520We%2520argue%2520that%2520the%2520feature%2520dissimilarity%2520space%2520is%2520more%2520suitable%2520for%2520similarity%250Amatching%252C%2520and%2520propose%2520a%2520dichotomy%2520transformation%2520to%2520project%2520query%2520and%2520reference%250Aembeddings%2520into%2520a%2520single%2520embedding%2520in%2520the%2520dissimilarity%2520space.%250A%2520%2520We%2520also%2520advocate%2520for%2520end-to-end%2520training%2520of%2520a%2520backbone%2520and%2520binary%250Aclassification%2520models%2520for%2520pair-wise%2520matching.%2520As%2520opposed%2520to%2520comparing%2520the%250Adistance%2520between%2520queries%2520and%2520reference%2520embeddings%252C%2520we%2520show%2520the%2520benefits%2520of%250Aclassifying%2520the%2520single%2520dissimilarity%2520space%2520embedding%2520%2528as%2520similar%2520or%250Adissimilar%2529%252C%2520especially%2520when%2520trained%2520end-to-end.%2520We%2520propose%2520a%2520method%2520to%2520train%250Athe%2520max-margin%2520classifier%2520together%2520with%2520the%2520backbone%2520feature%2520extractor%2520by%250Aapplying%2520constraints%2520to%2520the%2520L2%2520norm%2520of%2520the%2520classifier%2520weights%2520along%2520with%2520the%250Ahinge%2520loss.%250A%2520%2520Our%2520extensive%2520experiments%2520on%2520challenging%2520image%2520retrieval%2520datasets%2520and%2520using%250Adiverse%2520feature%2520extraction%2520backbones%2520highlight%2520the%2520benefits%2520of%2520similarity%250Amatching%2520in%2520the%2520dissimilarity%2520space.%2520In%2520particular%252C%2520when%2520jointly%2520training%2520the%250Afeature%2520extraction%2520backbone%2520and%2520regularised%2520classifier%2520for%2520matching%252C%2520the%250Adissimilarity%2520space%2520provides%2520a%2520higher%2520level%2520of%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Retrieval%20Methods%20in%20the%20Dissimilarity%20Space&entry.906535625=Madhu%20Kiran%20and%20Kartikey%20Vishnu%20and%20Rafael%20M.%20O.%20Cruz%20and%20Eric%20Granger&entry.1292438233=%20%20Image%20retrieval%20methods%20rely%20on%20metric%20learning%20to%20train%20backbone%20feature%0Aextraction%20models%20that%20can%20extract%20discriminant%20queries%20and%20reference%20%28gallery%29%0Afeature%20representations%20for%20similarity%20matching.%20Although%20state-of-the-art%0Aaccuracy%20has%20improved%20considerably%20with%20the%20advent%20of%20deep%20learning%20%28DL%29%20models%0Atrained%20on%20large%20datasets%2C%20image%20retrieval%20remains%20challenging%20in%20many%0Areal-world%20video%20analytics%20and%20surveillance%20applications%2C%20e.g.%2C%20person%0Are-identification.%20Using%20the%20Euclidean%20space%20for%20matching%20limits%20the%0Aperformance%20in%20real-world%20applications%20due%20to%20the%20curse%20of%20dimensionality%2C%0Aoverfitting%2C%20and%20sensitivity%20to%20noisy%20data.%0A%20%20We%20argue%20that%20the%20feature%20dissimilarity%20space%20is%20more%20suitable%20for%20similarity%0Amatching%2C%20and%20propose%20a%20dichotomy%20transformation%20to%20project%20query%20and%20reference%0Aembeddings%20into%20a%20single%20embedding%20in%20the%20dissimilarity%20space.%0A%20%20We%20also%20advocate%20for%20end-to-end%20training%20of%20a%20backbone%20and%20binary%0Aclassification%20models%20for%20pair-wise%20matching.%20As%20opposed%20to%20comparing%20the%0Adistance%20between%20queries%20and%20reference%20embeddings%2C%20we%20show%20the%20benefits%20of%0Aclassifying%20the%20single%20dissimilarity%20space%20embedding%20%28as%20similar%20or%0Adissimilar%29%2C%20especially%20when%20trained%20end-to-end.%20We%20propose%20a%20method%20to%20train%0Athe%20max-margin%20classifier%20together%20with%20the%20backbone%20feature%20extractor%20by%0Aapplying%20constraints%20to%20the%20L2%20norm%20of%20the%20classifier%20weights%20along%20with%20the%0Ahinge%20loss.%0A%20%20Our%20extensive%20experiments%20on%20challenging%20image%20retrieval%20datasets%20and%20using%0Adiverse%20feature%20extraction%20backbones%20highlight%20the%20benefits%20of%20similarity%0Amatching%20in%20the%20dissimilarity%20space.%20In%20particular%2C%20when%20jointly%20training%20the%0Afeature%20extraction%20backbone%20and%20regularised%20classifier%20for%20matching%2C%20the%0Adissimilarity%20space%20provides%20a%20higher%20level%20of%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08618v1&entry.124074799=Read"},
{"title": "EmoSpeech: A Corpus of Emotionally Rich and Contextually Detailed Speech\n  Annotations", "author": "Weizhen Bian and Yubo Zhou and Kaitai Zhang and Xiaohan Gu", "abstract": "  Advances in text-to-speech (TTS) technology have significantly improved the\nquality of generated speech, closely matching the timbre and intonation of the\ntarget speaker. However, due to the inherent complexity of human emotional\nexpression, the development of TTS systems capable of controlling subtle\nemotional differences remains a formidable challenge. Existing emotional speech\ndatabases often suffer from overly simplistic labelling schemes that fail to\ncapture a wide range of emotional states, thus limiting the effectiveness of\nemotion synthesis in TTS applications. To this end, recent efforts have\nfocussed on building databases that use natural language annotations to\ndescribe speech emotions. However, these approaches are costly and require more\nemotional depth to train robust systems. In this paper, we propose a novel\nprocess aimed at building databases by systematically extracting emotion-rich\nspeech segments and annotating them with detailed natural language descriptions\nthrough a generative model. This approach enhances the emotional granularity of\nthe database and significantly reduces the reliance on costly manual\nannotations by automatically augmenting the data with high-level language\nmodels. The resulting rich database provides a scalable and economically viable\nsolution for developing a more nuanced and dynamic basis for developing\nemotionally controlled TTS systems.\n", "link": "http://arxiv.org/abs/2412.06581v2", "date": "2024-12-11", "relevancy": 2.227, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4523}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoSpeech%3A%20A%20Corpus%20of%20Emotionally%20Rich%20and%20Contextually%20Detailed%20Speech%0A%20%20Annotations&body=Title%3A%20EmoSpeech%3A%20A%20Corpus%20of%20Emotionally%20Rich%20and%20Contextually%20Detailed%20Speech%0A%20%20Annotations%0AAuthor%3A%20Weizhen%20Bian%20and%20Yubo%20Zhou%20and%20Kaitai%20Zhang%20and%20Xiaohan%20Gu%0AAbstract%3A%20%20%20Advances%20in%20text-to-speech%20%28TTS%29%20technology%20have%20significantly%20improved%20the%0Aquality%20of%20generated%20speech%2C%20closely%20matching%20the%20timbre%20and%20intonation%20of%20the%0Atarget%20speaker.%20However%2C%20due%20to%20the%20inherent%20complexity%20of%20human%20emotional%0Aexpression%2C%20the%20development%20of%20TTS%20systems%20capable%20of%20controlling%20subtle%0Aemotional%20differences%20remains%20a%20formidable%20challenge.%20Existing%20emotional%20speech%0Adatabases%20often%20suffer%20from%20overly%20simplistic%20labelling%20schemes%20that%20fail%20to%0Acapture%20a%20wide%20range%20of%20emotional%20states%2C%20thus%20limiting%20the%20effectiveness%20of%0Aemotion%20synthesis%20in%20TTS%20applications.%20To%20this%20end%2C%20recent%20efforts%20have%0Afocussed%20on%20building%20databases%20that%20use%20natural%20language%20annotations%20to%0Adescribe%20speech%20emotions.%20However%2C%20these%20approaches%20are%20costly%20and%20require%20more%0Aemotional%20depth%20to%20train%20robust%20systems.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aprocess%20aimed%20at%20building%20databases%20by%20systematically%20extracting%20emotion-rich%0Aspeech%20segments%20and%20annotating%20them%20with%20detailed%20natural%20language%20descriptions%0Athrough%20a%20generative%20model.%20This%20approach%20enhances%20the%20emotional%20granularity%20of%0Athe%20database%20and%20significantly%20reduces%20the%20reliance%20on%20costly%20manual%0Aannotations%20by%20automatically%20augmenting%20the%20data%20with%20high-level%20language%0Amodels.%20The%20resulting%20rich%20database%20provides%20a%20scalable%20and%20economically%20viable%0Asolution%20for%20developing%20a%20more%20nuanced%20and%20dynamic%20basis%20for%20developing%0Aemotionally%20controlled%20TTS%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06581v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoSpeech%253A%2520A%2520Corpus%2520of%2520Emotionally%2520Rich%2520and%2520Contextually%2520Detailed%2520Speech%250A%2520%2520Annotations%26entry.906535625%3DWeizhen%2520Bian%2520and%2520Yubo%2520Zhou%2520and%2520Kaitai%2520Zhang%2520and%2520Xiaohan%2520Gu%26entry.1292438233%3D%2520%2520Advances%2520in%2520text-to-speech%2520%2528TTS%2529%2520technology%2520have%2520significantly%2520improved%2520the%250Aquality%2520of%2520generated%2520speech%252C%2520closely%2520matching%2520the%2520timbre%2520and%2520intonation%2520of%2520the%250Atarget%2520speaker.%2520However%252C%2520due%2520to%2520the%2520inherent%2520complexity%2520of%2520human%2520emotional%250Aexpression%252C%2520the%2520development%2520of%2520TTS%2520systems%2520capable%2520of%2520controlling%2520subtle%250Aemotional%2520differences%2520remains%2520a%2520formidable%2520challenge.%2520Existing%2520emotional%2520speech%250Adatabases%2520often%2520suffer%2520from%2520overly%2520simplistic%2520labelling%2520schemes%2520that%2520fail%2520to%250Acapture%2520a%2520wide%2520range%2520of%2520emotional%2520states%252C%2520thus%2520limiting%2520the%2520effectiveness%2520of%250Aemotion%2520synthesis%2520in%2520TTS%2520applications.%2520To%2520this%2520end%252C%2520recent%2520efforts%2520have%250Afocussed%2520on%2520building%2520databases%2520that%2520use%2520natural%2520language%2520annotations%2520to%250Adescribe%2520speech%2520emotions.%2520However%252C%2520these%2520approaches%2520are%2520costly%2520and%2520require%2520more%250Aemotional%2520depth%2520to%2520train%2520robust%2520systems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aprocess%2520aimed%2520at%2520building%2520databases%2520by%2520systematically%2520extracting%2520emotion-rich%250Aspeech%2520segments%2520and%2520annotating%2520them%2520with%2520detailed%2520natural%2520language%2520descriptions%250Athrough%2520a%2520generative%2520model.%2520This%2520approach%2520enhances%2520the%2520emotional%2520granularity%2520of%250Athe%2520database%2520and%2520significantly%2520reduces%2520the%2520reliance%2520on%2520costly%2520manual%250Aannotations%2520by%2520automatically%2520augmenting%2520the%2520data%2520with%2520high-level%2520language%250Amodels.%2520The%2520resulting%2520rich%2520database%2520provides%2520a%2520scalable%2520and%2520economically%2520viable%250Asolution%2520for%2520developing%2520a%2520more%2520nuanced%2520and%2520dynamic%2520basis%2520for%2520developing%250Aemotionally%2520controlled%2520TTS%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06581v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoSpeech%3A%20A%20Corpus%20of%20Emotionally%20Rich%20and%20Contextually%20Detailed%20Speech%0A%20%20Annotations&entry.906535625=Weizhen%20Bian%20and%20Yubo%20Zhou%20and%20Kaitai%20Zhang%20and%20Xiaohan%20Gu&entry.1292438233=%20%20Advances%20in%20text-to-speech%20%28TTS%29%20technology%20have%20significantly%20improved%20the%0Aquality%20of%20generated%20speech%2C%20closely%20matching%20the%20timbre%20and%20intonation%20of%20the%0Atarget%20speaker.%20However%2C%20due%20to%20the%20inherent%20complexity%20of%20human%20emotional%0Aexpression%2C%20the%20development%20of%20TTS%20systems%20capable%20of%20controlling%20subtle%0Aemotional%20differences%20remains%20a%20formidable%20challenge.%20Existing%20emotional%20speech%0Adatabases%20often%20suffer%20from%20overly%20simplistic%20labelling%20schemes%20that%20fail%20to%0Acapture%20a%20wide%20range%20of%20emotional%20states%2C%20thus%20limiting%20the%20effectiveness%20of%0Aemotion%20synthesis%20in%20TTS%20applications.%20To%20this%20end%2C%20recent%20efforts%20have%0Afocussed%20on%20building%20databases%20that%20use%20natural%20language%20annotations%20to%0Adescribe%20speech%20emotions.%20However%2C%20these%20approaches%20are%20costly%20and%20require%20more%0Aemotional%20depth%20to%20train%20robust%20systems.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aprocess%20aimed%20at%20building%20databases%20by%20systematically%20extracting%20emotion-rich%0Aspeech%20segments%20and%20annotating%20them%20with%20detailed%20natural%20language%20descriptions%0Athrough%20a%20generative%20model.%20This%20approach%20enhances%20the%20emotional%20granularity%20of%0Athe%20database%20and%20significantly%20reduces%20the%20reliance%20on%20costly%20manual%0Aannotations%20by%20automatically%20augmenting%20the%20data%20with%20high-level%20language%0Amodels.%20The%20resulting%20rich%20database%20provides%20a%20scalable%20and%20economically%20viable%0Asolution%20for%20developing%20a%20more%20nuanced%20and%20dynamic%20basis%20for%20developing%0Aemotionally%20controlled%20TTS%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06581v2&entry.124074799=Read"},
{"title": "Post-Hoc MOTS: Exploring the Capabilities of Time-Symmetric Multi-Object\n  Tracking", "author": "Gergely Szab\u00f3 and Zs\u00f3fia Moln\u00e1r and Andr\u00e1s Horv\u00e1th", "abstract": "  Temporal forward-tracking has been the dominant approach for multi-object\nsegmentation and tracking (MOTS). However, a novel time-symmetric tracking\nmethodology has recently been introduced for the detection, segmentation, and\ntracking of budding yeast cells in pre-recorded samples. Although this\narchitecture has demonstrated a unique perspective on stable and consistent\ntracking, as well as missed instance re-interpolation, its evaluation has so\nfar been largely confined to settings related to videomicroscopic environments.\nIn this work, we aim to reveal the broader capabilities, advantages, and\npotential challenges of this architecture across various specifically designed\nscenarios, including a pedestrian tracking dataset. We also conduct an ablation\nstudy comparing the model against its restricted variants and the widely used\nKalman filter. Furthermore, we present an attention analysis of the tracking\narchitecture for both pretrained and non-pretrained models\n", "link": "http://arxiv.org/abs/2412.08313v1", "date": "2024-12-11", "relevancy": 2.224, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5628}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5526}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-Hoc%20MOTS%3A%20Exploring%20the%20Capabilities%20of%20Time-Symmetric%20Multi-Object%0A%20%20Tracking&body=Title%3A%20Post-Hoc%20MOTS%3A%20Exploring%20the%20Capabilities%20of%20Time-Symmetric%20Multi-Object%0A%20%20Tracking%0AAuthor%3A%20Gergely%20Szab%C3%B3%20and%20Zs%C3%B3fia%20Moln%C3%A1r%20and%20Andr%C3%A1s%20Horv%C3%A1th%0AAbstract%3A%20%20%20Temporal%20forward-tracking%20has%20been%20the%20dominant%20approach%20for%20multi-object%0Asegmentation%20and%20tracking%20%28MOTS%29.%20However%2C%20a%20novel%20time-symmetric%20tracking%0Amethodology%20has%20recently%20been%20introduced%20for%20the%20detection%2C%20segmentation%2C%20and%0Atracking%20of%20budding%20yeast%20cells%20in%20pre-recorded%20samples.%20Although%20this%0Aarchitecture%20has%20demonstrated%20a%20unique%20perspective%20on%20stable%20and%20consistent%0Atracking%2C%20as%20well%20as%20missed%20instance%20re-interpolation%2C%20its%20evaluation%20has%20so%0Afar%20been%20largely%20confined%20to%20settings%20related%20to%20videomicroscopic%20environments.%0AIn%20this%20work%2C%20we%20aim%20to%20reveal%20the%20broader%20capabilities%2C%20advantages%2C%20and%0Apotential%20challenges%20of%20this%20architecture%20across%20various%20specifically%20designed%0Ascenarios%2C%20including%20a%20pedestrian%20tracking%20dataset.%20We%20also%20conduct%20an%20ablation%0Astudy%20comparing%20the%20model%20against%20its%20restricted%20variants%20and%20the%20widely%20used%0AKalman%20filter.%20Furthermore%2C%20we%20present%20an%20attention%20analysis%20of%20the%20tracking%0Aarchitecture%20for%20both%20pretrained%20and%20non-pretrained%20models%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-Hoc%2520MOTS%253A%2520Exploring%2520the%2520Capabilities%2520of%2520Time-Symmetric%2520Multi-Object%250A%2520%2520Tracking%26entry.906535625%3DGergely%2520Szab%25C3%25B3%2520and%2520Zs%25C3%25B3fia%2520Moln%25C3%25A1r%2520and%2520Andr%25C3%25A1s%2520Horv%25C3%25A1th%26entry.1292438233%3D%2520%2520Temporal%2520forward-tracking%2520has%2520been%2520the%2520dominant%2520approach%2520for%2520multi-object%250Asegmentation%2520and%2520tracking%2520%2528MOTS%2529.%2520However%252C%2520a%2520novel%2520time-symmetric%2520tracking%250Amethodology%2520has%2520recently%2520been%2520introduced%2520for%2520the%2520detection%252C%2520segmentation%252C%2520and%250Atracking%2520of%2520budding%2520yeast%2520cells%2520in%2520pre-recorded%2520samples.%2520Although%2520this%250Aarchitecture%2520has%2520demonstrated%2520a%2520unique%2520perspective%2520on%2520stable%2520and%2520consistent%250Atracking%252C%2520as%2520well%2520as%2520missed%2520instance%2520re-interpolation%252C%2520its%2520evaluation%2520has%2520so%250Afar%2520been%2520largely%2520confined%2520to%2520settings%2520related%2520to%2520videomicroscopic%2520environments.%250AIn%2520this%2520work%252C%2520we%2520aim%2520to%2520reveal%2520the%2520broader%2520capabilities%252C%2520advantages%252C%2520and%250Apotential%2520challenges%2520of%2520this%2520architecture%2520across%2520various%2520specifically%2520designed%250Ascenarios%252C%2520including%2520a%2520pedestrian%2520tracking%2520dataset.%2520We%2520also%2520conduct%2520an%2520ablation%250Astudy%2520comparing%2520the%2520model%2520against%2520its%2520restricted%2520variants%2520and%2520the%2520widely%2520used%250AKalman%2520filter.%2520Furthermore%252C%2520we%2520present%2520an%2520attention%2520analysis%2520of%2520the%2520tracking%250Aarchitecture%2520for%2520both%2520pretrained%2520and%2520non-pretrained%2520models%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-Hoc%20MOTS%3A%20Exploring%20the%20Capabilities%20of%20Time-Symmetric%20Multi-Object%0A%20%20Tracking&entry.906535625=Gergely%20Szab%C3%B3%20and%20Zs%C3%B3fia%20Moln%C3%A1r%20and%20Andr%C3%A1s%20Horv%C3%A1th&entry.1292438233=%20%20Temporal%20forward-tracking%20has%20been%20the%20dominant%20approach%20for%20multi-object%0Asegmentation%20and%20tracking%20%28MOTS%29.%20However%2C%20a%20novel%20time-symmetric%20tracking%0Amethodology%20has%20recently%20been%20introduced%20for%20the%20detection%2C%20segmentation%2C%20and%0Atracking%20of%20budding%20yeast%20cells%20in%20pre-recorded%20samples.%20Although%20this%0Aarchitecture%20has%20demonstrated%20a%20unique%20perspective%20on%20stable%20and%20consistent%0Atracking%2C%20as%20well%20as%20missed%20instance%20re-interpolation%2C%20its%20evaluation%20has%20so%0Afar%20been%20largely%20confined%20to%20settings%20related%20to%20videomicroscopic%20environments.%0AIn%20this%20work%2C%20we%20aim%20to%20reveal%20the%20broader%20capabilities%2C%20advantages%2C%20and%0Apotential%20challenges%20of%20this%20architecture%20across%20various%20specifically%20designed%0Ascenarios%2C%20including%20a%20pedestrian%20tracking%20dataset.%20We%20also%20conduct%20an%20ablation%0Astudy%20comparing%20the%20model%20against%20its%20restricted%20variants%20and%20the%20widely%20used%0AKalman%20filter.%20Furthermore%2C%20we%20present%20an%20attention%20analysis%20of%20the%20tracking%0Aarchitecture%20for%20both%20pretrained%20and%20non-pretrained%20models%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08313v1&entry.124074799=Read"},
{"title": "Benchmarking learned algorithms for computed tomography image\n  reconstruction tasks", "author": "Maximilian B. Kiss and Ander Biguri and Zakhar Shumaylov and Ferdia Sherry and K. Joost Batenburg and Carola-Bibiane Sch\u00f6nlieb and Felix Lucka", "abstract": "  Computed tomography (CT) is a widely used non-invasive diagnostic method in\nvarious fields, and recent advances in deep learning have led to significant\nprogress in CT image reconstruction. However, the lack of large-scale,\nopen-access datasets has hindered the comparison of different types of learned\nmethods. To address this gap, we use the 2DeteCT dataset, a real-world\nexperimental computed tomography dataset, for benchmarking machine learning\nbased CT image reconstruction algorithms. We categorize these methods into\npost-processing networks, learned/unrolled iterative methods, learned\nregularizer methods, and plug-and-play methods, and provide a pipeline for easy\nimplementation and evaluation. Using key performance metrics, including SSIM\nand PSNR, our benchmarking results showcase the effectiveness of various\nalgorithms on tasks such as full data reconstruction, limited-angle\nreconstruction, sparse-angle reconstruction, low-dose reconstruction, and\nbeam-hardening corrected reconstruction. With this benchmarking study, we\nprovide an evaluation of a range of algorithms representative for different\ncategories of learned reconstruction methods on a recently published dataset of\nreal-world experimental CT measurements. The reproducible setup of methods and\nCT image reconstruction tasks in an open-source toolbox enables straightforward\naddition and comparison of new methods later on. The toolbox also provides the\noption to load the 2DeteCT dataset differently for extensions to other problems\nand different CT reconstruction tasks.\n", "link": "http://arxiv.org/abs/2412.08350v1", "date": "2024-12-11", "relevancy": 2.2171, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5622}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5622}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20learned%20algorithms%20for%20computed%20tomography%20image%0A%20%20reconstruction%20tasks&body=Title%3A%20Benchmarking%20learned%20algorithms%20for%20computed%20tomography%20image%0A%20%20reconstruction%20tasks%0AAuthor%3A%20Maximilian%20B.%20Kiss%20and%20Ander%20Biguri%20and%20Zakhar%20Shumaylov%20and%20Ferdia%20Sherry%20and%20K.%20Joost%20Batenburg%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Felix%20Lucka%0AAbstract%3A%20%20%20Computed%20tomography%20%28CT%29%20is%20a%20widely%20used%20non-invasive%20diagnostic%20method%20in%0Avarious%20fields%2C%20and%20recent%20advances%20in%20deep%20learning%20have%20led%20to%20significant%0Aprogress%20in%20CT%20image%20reconstruction.%20However%2C%20the%20lack%20of%20large-scale%2C%0Aopen-access%20datasets%20has%20hindered%20the%20comparison%20of%20different%20types%20of%20learned%0Amethods.%20To%20address%20this%20gap%2C%20we%20use%20the%202DeteCT%20dataset%2C%20a%20real-world%0Aexperimental%20computed%20tomography%20dataset%2C%20for%20benchmarking%20machine%20learning%0Abased%20CT%20image%20reconstruction%20algorithms.%20We%20categorize%20these%20methods%20into%0Apost-processing%20networks%2C%20learned/unrolled%20iterative%20methods%2C%20learned%0Aregularizer%20methods%2C%20and%20plug-and-play%20methods%2C%20and%20provide%20a%20pipeline%20for%20easy%0Aimplementation%20and%20evaluation.%20Using%20key%20performance%20metrics%2C%20including%20SSIM%0Aand%20PSNR%2C%20our%20benchmarking%20results%20showcase%20the%20effectiveness%20of%20various%0Aalgorithms%20on%20tasks%20such%20as%20full%20data%20reconstruction%2C%20limited-angle%0Areconstruction%2C%20sparse-angle%20reconstruction%2C%20low-dose%20reconstruction%2C%20and%0Abeam-hardening%20corrected%20reconstruction.%20With%20this%20benchmarking%20study%2C%20we%0Aprovide%20an%20evaluation%20of%20a%20range%20of%20algorithms%20representative%20for%20different%0Acategories%20of%20learned%20reconstruction%20methods%20on%20a%20recently%20published%20dataset%20of%0Areal-world%20experimental%20CT%20measurements.%20The%20reproducible%20setup%20of%20methods%20and%0ACT%20image%20reconstruction%20tasks%20in%20an%20open-source%20toolbox%20enables%20straightforward%0Aaddition%20and%20comparison%20of%20new%20methods%20later%20on.%20The%20toolbox%20also%20provides%20the%0Aoption%20to%20load%20the%202DeteCT%20dataset%20differently%20for%20extensions%20to%20other%20problems%0Aand%20different%20CT%20reconstruction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520learned%2520algorithms%2520for%2520computed%2520tomography%2520image%250A%2520%2520reconstruction%2520tasks%26entry.906535625%3DMaximilian%2520B.%2520Kiss%2520and%2520Ander%2520Biguri%2520and%2520Zakhar%2520Shumaylov%2520and%2520Ferdia%2520Sherry%2520and%2520K.%2520Joost%2520Batenburg%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Felix%2520Lucka%26entry.1292438233%3D%2520%2520Computed%2520tomography%2520%2528CT%2529%2520is%2520a%2520widely%2520used%2520non-invasive%2520diagnostic%2520method%2520in%250Avarious%2520fields%252C%2520and%2520recent%2520advances%2520in%2520deep%2520learning%2520have%2520led%2520to%2520significant%250Aprogress%2520in%2520CT%2520image%2520reconstruction.%2520However%252C%2520the%2520lack%2520of%2520large-scale%252C%250Aopen-access%2520datasets%2520has%2520hindered%2520the%2520comparison%2520of%2520different%2520types%2520of%2520learned%250Amethods.%2520To%2520address%2520this%2520gap%252C%2520we%2520use%2520the%25202DeteCT%2520dataset%252C%2520a%2520real-world%250Aexperimental%2520computed%2520tomography%2520dataset%252C%2520for%2520benchmarking%2520machine%2520learning%250Abased%2520CT%2520image%2520reconstruction%2520algorithms.%2520We%2520categorize%2520these%2520methods%2520into%250Apost-processing%2520networks%252C%2520learned/unrolled%2520iterative%2520methods%252C%2520learned%250Aregularizer%2520methods%252C%2520and%2520plug-and-play%2520methods%252C%2520and%2520provide%2520a%2520pipeline%2520for%2520easy%250Aimplementation%2520and%2520evaluation.%2520Using%2520key%2520performance%2520metrics%252C%2520including%2520SSIM%250Aand%2520PSNR%252C%2520our%2520benchmarking%2520results%2520showcase%2520the%2520effectiveness%2520of%2520various%250Aalgorithms%2520on%2520tasks%2520such%2520as%2520full%2520data%2520reconstruction%252C%2520limited-angle%250Areconstruction%252C%2520sparse-angle%2520reconstruction%252C%2520low-dose%2520reconstruction%252C%2520and%250Abeam-hardening%2520corrected%2520reconstruction.%2520With%2520this%2520benchmarking%2520study%252C%2520we%250Aprovide%2520an%2520evaluation%2520of%2520a%2520range%2520of%2520algorithms%2520representative%2520for%2520different%250Acategories%2520of%2520learned%2520reconstruction%2520methods%2520on%2520a%2520recently%2520published%2520dataset%2520of%250Areal-world%2520experimental%2520CT%2520measurements.%2520The%2520reproducible%2520setup%2520of%2520methods%2520and%250ACT%2520image%2520reconstruction%2520tasks%2520in%2520an%2520open-source%2520toolbox%2520enables%2520straightforward%250Aaddition%2520and%2520comparison%2520of%2520new%2520methods%2520later%2520on.%2520The%2520toolbox%2520also%2520provides%2520the%250Aoption%2520to%2520load%2520the%25202DeteCT%2520dataset%2520differently%2520for%2520extensions%2520to%2520other%2520problems%250Aand%2520different%2520CT%2520reconstruction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20learned%20algorithms%20for%20computed%20tomography%20image%0A%20%20reconstruction%20tasks&entry.906535625=Maximilian%20B.%20Kiss%20and%20Ander%20Biguri%20and%20Zakhar%20Shumaylov%20and%20Ferdia%20Sherry%20and%20K.%20Joost%20Batenburg%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Felix%20Lucka&entry.1292438233=%20%20Computed%20tomography%20%28CT%29%20is%20a%20widely%20used%20non-invasive%20diagnostic%20method%20in%0Avarious%20fields%2C%20and%20recent%20advances%20in%20deep%20learning%20have%20led%20to%20significant%0Aprogress%20in%20CT%20image%20reconstruction.%20However%2C%20the%20lack%20of%20large-scale%2C%0Aopen-access%20datasets%20has%20hindered%20the%20comparison%20of%20different%20types%20of%20learned%0Amethods.%20To%20address%20this%20gap%2C%20we%20use%20the%202DeteCT%20dataset%2C%20a%20real-world%0Aexperimental%20computed%20tomography%20dataset%2C%20for%20benchmarking%20machine%20learning%0Abased%20CT%20image%20reconstruction%20algorithms.%20We%20categorize%20these%20methods%20into%0Apost-processing%20networks%2C%20learned/unrolled%20iterative%20methods%2C%20learned%0Aregularizer%20methods%2C%20and%20plug-and-play%20methods%2C%20and%20provide%20a%20pipeline%20for%20easy%0Aimplementation%20and%20evaluation.%20Using%20key%20performance%20metrics%2C%20including%20SSIM%0Aand%20PSNR%2C%20our%20benchmarking%20results%20showcase%20the%20effectiveness%20of%20various%0Aalgorithms%20on%20tasks%20such%20as%20full%20data%20reconstruction%2C%20limited-angle%0Areconstruction%2C%20sparse-angle%20reconstruction%2C%20low-dose%20reconstruction%2C%20and%0Abeam-hardening%20corrected%20reconstruction.%20With%20this%20benchmarking%20study%2C%20we%0Aprovide%20an%20evaluation%20of%20a%20range%20of%20algorithms%20representative%20for%20different%0Acategories%20of%20learned%20reconstruction%20methods%20on%20a%20recently%20published%20dataset%20of%0Areal-world%20experimental%20CT%20measurements.%20The%20reproducible%20setup%20of%20methods%20and%0ACT%20image%20reconstruction%20tasks%20in%20an%20open-source%20toolbox%20enables%20straightforward%0Aaddition%20and%20comparison%20of%20new%20methods%20later%20on.%20The%20toolbox%20also%20provides%20the%0Aoption%20to%20load%20the%202DeteCT%20dataset%20differently%20for%20extensions%20to%20other%20problems%0Aand%20different%20CT%20reconstruction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08350v1&entry.124074799=Read"},
{"title": "Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain\n  Offline Data", "author": "Zhiyuan Zhou and Andy Peng and Qiyang Li and Sergey Levine and Aviral Kumar", "abstract": "  The modern paradigm in machine learning involves pre-training on diverse\ndata, followed by task-specific fine-tuning. In reinforcement learning (RL),\nthis translates to learning via offline RL on a diverse historical dataset,\nfollowed by rapid online RL fine-tuning using interaction data. Most RL\nfine-tuning methods require continued training on offline data for stability\nand performance. However, this is undesirable because training on diverse\noffline data is slow and expensive for large datasets, and in principle, also\nlimit the performance improvement possible because of constraints or pessimism\non offline data. In this paper, we show that retaining offline data is\nunnecessary as long as we use a properly-designed online RL approach for\nfine-tuning offline RL initializations. To build this approach, we start by\nanalyzing the role of retaining offline data in online fine-tuning. We find\nthat continued training on offline data is mostly useful for preventing a\nsudden divergence in the value function at the onset of fine-tuning, caused by\na distribution mismatch between the offline data and online rollouts. This\ndivergence typically results in unlearning and forgetting the benefits of\noffline pre-training. Our approach, Warm-start RL (WSRL), mitigates the\ncatastrophic forgetting of pre-trained initializations using a very simple\nidea. WSRL employs a warmup phase that seeds the online RL run with a very\nsmall number of rollouts from the pre-trained policy to do fast online RL. The\ndata collected during warmup helps ``recalibrate'' the offline Q-function to\nthe online distribution, allowing us to completely discard offline data without\ndestabilizing the online RL fine-tuning. We show that WSRL is able to fine-tune\nwithout retaining any offline data, and is able to learn faster and attains\nhigher performance than existing algorithms irrespective of whether they retain\noffline data or not.\n", "link": "http://arxiv.org/abs/2412.07762v2", "date": "2024-12-11", "relevancy": 2.2159, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4534}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4475}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Online%20Reinforcement%20Learning%20Fine-Tuning%20Need%20Not%20Retain%0A%20%20Offline%20Data&body=Title%3A%20Efficient%20Online%20Reinforcement%20Learning%20Fine-Tuning%20Need%20Not%20Retain%0A%20%20Offline%20Data%0AAuthor%3A%20Zhiyuan%20Zhou%20and%20Andy%20Peng%20and%20Qiyang%20Li%20and%20Sergey%20Levine%20and%20Aviral%20Kumar%0AAbstract%3A%20%20%20The%20modern%20paradigm%20in%20machine%20learning%20involves%20pre-training%20on%20diverse%0Adata%2C%20followed%20by%20task-specific%20fine-tuning.%20In%20reinforcement%20learning%20%28RL%29%2C%0Athis%20translates%20to%20learning%20via%20offline%20RL%20on%20a%20diverse%20historical%20dataset%2C%0Afollowed%20by%20rapid%20online%20RL%20fine-tuning%20using%20interaction%20data.%20Most%20RL%0Afine-tuning%20methods%20require%20continued%20training%20on%20offline%20data%20for%20stability%0Aand%20performance.%20However%2C%20this%20is%20undesirable%20because%20training%20on%20diverse%0Aoffline%20data%20is%20slow%20and%20expensive%20for%20large%20datasets%2C%20and%20in%20principle%2C%20also%0Alimit%20the%20performance%20improvement%20possible%20because%20of%20constraints%20or%20pessimism%0Aon%20offline%20data.%20In%20this%20paper%2C%20we%20show%20that%20retaining%20offline%20data%20is%0Aunnecessary%20as%20long%20as%20we%20use%20a%20properly-designed%20online%20RL%20approach%20for%0Afine-tuning%20offline%20RL%20initializations.%20To%20build%20this%20approach%2C%20we%20start%20by%0Aanalyzing%20the%20role%20of%20retaining%20offline%20data%20in%20online%20fine-tuning.%20We%20find%0Athat%20continued%20training%20on%20offline%20data%20is%20mostly%20useful%20for%20preventing%20a%0Asudden%20divergence%20in%20the%20value%20function%20at%20the%20onset%20of%20fine-tuning%2C%20caused%20by%0Aa%20distribution%20mismatch%20between%20the%20offline%20data%20and%20online%20rollouts.%20This%0Adivergence%20typically%20results%20in%20unlearning%20and%20forgetting%20the%20benefits%20of%0Aoffline%20pre-training.%20Our%20approach%2C%20Warm-start%20RL%20%28WSRL%29%2C%20mitigates%20the%0Acatastrophic%20forgetting%20of%20pre-trained%20initializations%20using%20a%20very%20simple%0Aidea.%20WSRL%20employs%20a%20warmup%20phase%20that%20seeds%20the%20online%20RL%20run%20with%20a%20very%0Asmall%20number%20of%20rollouts%20from%20the%20pre-trained%20policy%20to%20do%20fast%20online%20RL.%20The%0Adata%20collected%20during%20warmup%20helps%20%60%60recalibrate%27%27%20the%20offline%20Q-function%20to%0Athe%20online%20distribution%2C%20allowing%20us%20to%20completely%20discard%20offline%20data%20without%0Adestabilizing%20the%20online%20RL%20fine-tuning.%20We%20show%20that%20WSRL%20is%20able%20to%20fine-tune%0Awithout%20retaining%20any%20offline%20data%2C%20and%20is%20able%20to%20learn%20faster%20and%20attains%0Ahigher%20performance%20than%20existing%20algorithms%20irrespective%20of%20whether%20they%20retain%0Aoffline%20data%20or%20not.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Online%2520Reinforcement%2520Learning%2520Fine-Tuning%2520Need%2520Not%2520Retain%250A%2520%2520Offline%2520Data%26entry.906535625%3DZhiyuan%2520Zhou%2520and%2520Andy%2520Peng%2520and%2520Qiyang%2520Li%2520and%2520Sergey%2520Levine%2520and%2520Aviral%2520Kumar%26entry.1292438233%3D%2520%2520The%2520modern%2520paradigm%2520in%2520machine%2520learning%2520involves%2520pre-training%2520on%2520diverse%250Adata%252C%2520followed%2520by%2520task-specific%2520fine-tuning.%2520In%2520reinforcement%2520learning%2520%2528RL%2529%252C%250Athis%2520translates%2520to%2520learning%2520via%2520offline%2520RL%2520on%2520a%2520diverse%2520historical%2520dataset%252C%250Afollowed%2520by%2520rapid%2520online%2520RL%2520fine-tuning%2520using%2520interaction%2520data.%2520Most%2520RL%250Afine-tuning%2520methods%2520require%2520continued%2520training%2520on%2520offline%2520data%2520for%2520stability%250Aand%2520performance.%2520However%252C%2520this%2520is%2520undesirable%2520because%2520training%2520on%2520diverse%250Aoffline%2520data%2520is%2520slow%2520and%2520expensive%2520for%2520large%2520datasets%252C%2520and%2520in%2520principle%252C%2520also%250Alimit%2520the%2520performance%2520improvement%2520possible%2520because%2520of%2520constraints%2520or%2520pessimism%250Aon%2520offline%2520data.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520retaining%2520offline%2520data%2520is%250Aunnecessary%2520as%2520long%2520as%2520we%2520use%2520a%2520properly-designed%2520online%2520RL%2520approach%2520for%250Afine-tuning%2520offline%2520RL%2520initializations.%2520To%2520build%2520this%2520approach%252C%2520we%2520start%2520by%250Aanalyzing%2520the%2520role%2520of%2520retaining%2520offline%2520data%2520in%2520online%2520fine-tuning.%2520We%2520find%250Athat%2520continued%2520training%2520on%2520offline%2520data%2520is%2520mostly%2520useful%2520for%2520preventing%2520a%250Asudden%2520divergence%2520in%2520the%2520value%2520function%2520at%2520the%2520onset%2520of%2520fine-tuning%252C%2520caused%2520by%250Aa%2520distribution%2520mismatch%2520between%2520the%2520offline%2520data%2520and%2520online%2520rollouts.%2520This%250Adivergence%2520typically%2520results%2520in%2520unlearning%2520and%2520forgetting%2520the%2520benefits%2520of%250Aoffline%2520pre-training.%2520Our%2520approach%252C%2520Warm-start%2520RL%2520%2528WSRL%2529%252C%2520mitigates%2520the%250Acatastrophic%2520forgetting%2520of%2520pre-trained%2520initializations%2520using%2520a%2520very%2520simple%250Aidea.%2520WSRL%2520employs%2520a%2520warmup%2520phase%2520that%2520seeds%2520the%2520online%2520RL%2520run%2520with%2520a%2520very%250Asmall%2520number%2520of%2520rollouts%2520from%2520the%2520pre-trained%2520policy%2520to%2520do%2520fast%2520online%2520RL.%2520The%250Adata%2520collected%2520during%2520warmup%2520helps%2520%2560%2560recalibrate%2527%2527%2520the%2520offline%2520Q-function%2520to%250Athe%2520online%2520distribution%252C%2520allowing%2520us%2520to%2520completely%2520discard%2520offline%2520data%2520without%250Adestabilizing%2520the%2520online%2520RL%2520fine-tuning.%2520We%2520show%2520that%2520WSRL%2520is%2520able%2520to%2520fine-tune%250Awithout%2520retaining%2520any%2520offline%2520data%252C%2520and%2520is%2520able%2520to%2520learn%2520faster%2520and%2520attains%250Ahigher%2520performance%2520than%2520existing%2520algorithms%2520irrespective%2520of%2520whether%2520they%2520retain%250Aoffline%2520data%2520or%2520not.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Online%20Reinforcement%20Learning%20Fine-Tuning%20Need%20Not%20Retain%0A%20%20Offline%20Data&entry.906535625=Zhiyuan%20Zhou%20and%20Andy%20Peng%20and%20Qiyang%20Li%20and%20Sergey%20Levine%20and%20Aviral%20Kumar&entry.1292438233=%20%20The%20modern%20paradigm%20in%20machine%20learning%20involves%20pre-training%20on%20diverse%0Adata%2C%20followed%20by%20task-specific%20fine-tuning.%20In%20reinforcement%20learning%20%28RL%29%2C%0Athis%20translates%20to%20learning%20via%20offline%20RL%20on%20a%20diverse%20historical%20dataset%2C%0Afollowed%20by%20rapid%20online%20RL%20fine-tuning%20using%20interaction%20data.%20Most%20RL%0Afine-tuning%20methods%20require%20continued%20training%20on%20offline%20data%20for%20stability%0Aand%20performance.%20However%2C%20this%20is%20undesirable%20because%20training%20on%20diverse%0Aoffline%20data%20is%20slow%20and%20expensive%20for%20large%20datasets%2C%20and%20in%20principle%2C%20also%0Alimit%20the%20performance%20improvement%20possible%20because%20of%20constraints%20or%20pessimism%0Aon%20offline%20data.%20In%20this%20paper%2C%20we%20show%20that%20retaining%20offline%20data%20is%0Aunnecessary%20as%20long%20as%20we%20use%20a%20properly-designed%20online%20RL%20approach%20for%0Afine-tuning%20offline%20RL%20initializations.%20To%20build%20this%20approach%2C%20we%20start%20by%0Aanalyzing%20the%20role%20of%20retaining%20offline%20data%20in%20online%20fine-tuning.%20We%20find%0Athat%20continued%20training%20on%20offline%20data%20is%20mostly%20useful%20for%20preventing%20a%0Asudden%20divergence%20in%20the%20value%20function%20at%20the%20onset%20of%20fine-tuning%2C%20caused%20by%0Aa%20distribution%20mismatch%20between%20the%20offline%20data%20and%20online%20rollouts.%20This%0Adivergence%20typically%20results%20in%20unlearning%20and%20forgetting%20the%20benefits%20of%0Aoffline%20pre-training.%20Our%20approach%2C%20Warm-start%20RL%20%28WSRL%29%2C%20mitigates%20the%0Acatastrophic%20forgetting%20of%20pre-trained%20initializations%20using%20a%20very%20simple%0Aidea.%20WSRL%20employs%20a%20warmup%20phase%20that%20seeds%20the%20online%20RL%20run%20with%20a%20very%0Asmall%20number%20of%20rollouts%20from%20the%20pre-trained%20policy%20to%20do%20fast%20online%20RL.%20The%0Adata%20collected%20during%20warmup%20helps%20%60%60recalibrate%27%27%20the%20offline%20Q-function%20to%0Athe%20online%20distribution%2C%20allowing%20us%20to%20completely%20discard%20offline%20data%20without%0Adestabilizing%20the%20online%20RL%20fine-tuning.%20We%20show%20that%20WSRL%20is%20able%20to%20fine-tune%0Awithout%20retaining%20any%20offline%20data%2C%20and%20is%20able%20to%20learn%20faster%20and%20attains%0Ahigher%20performance%20than%20existing%20algorithms%20irrespective%20of%20whether%20they%20retain%0Aoffline%20data%20or%20not.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07762v2&entry.124074799=Read"},
{"title": "ConDSeg: A General Medical Image Segmentation Framework via\n  Contrast-Driven Feature Enhancement", "author": "Mengqi Lei and Haochen Wu and Xinhua Lv and Xin Wang", "abstract": "  Medical image segmentation plays an important role in clinical decision\nmaking, treatment planning, and disease tracking. However, it still faces two\nmajor challenges. On the one hand, there is often a ``soft boundary'' between\nforeground and background in medical images, with poor illumination and low\ncontrast further reducing the distinguishability of foreground and background\nwithin the image. On the other hand, co-occurrence phenomena are widespread in\nmedical images, and learning these features is misleading to the model's\njudgment. To address these challenges, we propose a general framework called\nContrast-Driven Medical Image Segmentation (ConDSeg). First, we develop a\ncontrastive training strategy called Consistency Reinforcement. It is designed\nto improve the encoder's robustness in various illumination and contrast\nscenarios, enabling the model to extract high-quality features even in adverse\nenvironments. Second, we introduce a Semantic Information Decoupling module,\nwhich is able to decouple features from the encoder into foreground,\nbackground, and uncertainty regions, gradually acquiring the ability to reduce\nuncertainty during training. The Contrast-Driven Feature Aggregation module\nthen contrasts the foreground and background features to guide multi-level\nfeature fusion and key feature enhancement, further distinguishing the entities\nto be segmented. We also propose a Size-Aware Decoder to solve the scale\nsingularity of the decoder. It accurately locate entities of different sizes in\nthe image, thus avoiding erroneous learning of co-occurrence features.\nExtensive experiments on five medical image datasets across three scenarios\ndemonstrate the state-of-the-art performance of our method, proving its\nadvanced nature and general applicability to various medical image segmentation\nscenarios. Our released code is available at\n\\url{https://github.com/Mengqi-Lei/ConDSeg}.\n", "link": "http://arxiv.org/abs/2412.08345v1", "date": "2024-12-11", "relevancy": 2.2157, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5631}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConDSeg%3A%20A%20General%20Medical%20Image%20Segmentation%20Framework%20via%0A%20%20Contrast-Driven%20Feature%20Enhancement&body=Title%3A%20ConDSeg%3A%20A%20General%20Medical%20Image%20Segmentation%20Framework%20via%0A%20%20Contrast-Driven%20Feature%20Enhancement%0AAuthor%3A%20Mengqi%20Lei%20and%20Haochen%20Wu%20and%20Xinhua%20Lv%20and%20Xin%20Wang%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20plays%20an%20important%20role%20in%20clinical%20decision%0Amaking%2C%20treatment%20planning%2C%20and%20disease%20tracking.%20However%2C%20it%20still%20faces%20two%0Amajor%20challenges.%20On%20the%20one%20hand%2C%20there%20is%20often%20a%20%60%60soft%20boundary%27%27%20between%0Aforeground%20and%20background%20in%20medical%20images%2C%20with%20poor%20illumination%20and%20low%0Acontrast%20further%20reducing%20the%20distinguishability%20of%20foreground%20and%20background%0Awithin%20the%20image.%20On%20the%20other%20hand%2C%20co-occurrence%20phenomena%20are%20widespread%20in%0Amedical%20images%2C%20and%20learning%20these%20features%20is%20misleading%20to%20the%20model%27s%0Ajudgment.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20general%20framework%20called%0AContrast-Driven%20Medical%20Image%20Segmentation%20%28ConDSeg%29.%20First%2C%20we%20develop%20a%0Acontrastive%20training%20strategy%20called%20Consistency%20Reinforcement.%20It%20is%20designed%0Ato%20improve%20the%20encoder%27s%20robustness%20in%20various%20illumination%20and%20contrast%0Ascenarios%2C%20enabling%20the%20model%20to%20extract%20high-quality%20features%20even%20in%20adverse%0Aenvironments.%20Second%2C%20we%20introduce%20a%20Semantic%20Information%20Decoupling%20module%2C%0Awhich%20is%20able%20to%20decouple%20features%20from%20the%20encoder%20into%20foreground%2C%0Abackground%2C%20and%20uncertainty%20regions%2C%20gradually%20acquiring%20the%20ability%20to%20reduce%0Auncertainty%20during%20training.%20The%20Contrast-Driven%20Feature%20Aggregation%20module%0Athen%20contrasts%20the%20foreground%20and%20background%20features%20to%20guide%20multi-level%0Afeature%20fusion%20and%20key%20feature%20enhancement%2C%20further%20distinguishing%20the%20entities%0Ato%20be%20segmented.%20We%20also%20propose%20a%20Size-Aware%20Decoder%20to%20solve%20the%20scale%0Asingularity%20of%20the%20decoder.%20It%20accurately%20locate%20entities%20of%20different%20sizes%20in%0Athe%20image%2C%20thus%20avoiding%20erroneous%20learning%20of%20co-occurrence%20features.%0AExtensive%20experiments%20on%20five%20medical%20image%20datasets%20across%20three%20scenarios%0Ademonstrate%20the%20state-of-the-art%20performance%20of%20our%20method%2C%20proving%20its%0Aadvanced%20nature%20and%20general%20applicability%20to%20various%20medical%20image%20segmentation%0Ascenarios.%20Our%20released%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Mengqi-Lei/ConDSeg%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConDSeg%253A%2520A%2520General%2520Medical%2520Image%2520Segmentation%2520Framework%2520via%250A%2520%2520Contrast-Driven%2520Feature%2520Enhancement%26entry.906535625%3DMengqi%2520Lei%2520and%2520Haochen%2520Wu%2520and%2520Xinhua%2520Lv%2520and%2520Xin%2520Wang%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520plays%2520an%2520important%2520role%2520in%2520clinical%2520decision%250Amaking%252C%2520treatment%2520planning%252C%2520and%2520disease%2520tracking.%2520However%252C%2520it%2520still%2520faces%2520two%250Amajor%2520challenges.%2520On%2520the%2520one%2520hand%252C%2520there%2520is%2520often%2520a%2520%2560%2560soft%2520boundary%2527%2527%2520between%250Aforeground%2520and%2520background%2520in%2520medical%2520images%252C%2520with%2520poor%2520illumination%2520and%2520low%250Acontrast%2520further%2520reducing%2520the%2520distinguishability%2520of%2520foreground%2520and%2520background%250Awithin%2520the%2520image.%2520On%2520the%2520other%2520hand%252C%2520co-occurrence%2520phenomena%2520are%2520widespread%2520in%250Amedical%2520images%252C%2520and%2520learning%2520these%2520features%2520is%2520misleading%2520to%2520the%2520model%2527s%250Ajudgment.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520general%2520framework%2520called%250AContrast-Driven%2520Medical%2520Image%2520Segmentation%2520%2528ConDSeg%2529.%2520First%252C%2520we%2520develop%2520a%250Acontrastive%2520training%2520strategy%2520called%2520Consistency%2520Reinforcement.%2520It%2520is%2520designed%250Ato%2520improve%2520the%2520encoder%2527s%2520robustness%2520in%2520various%2520illumination%2520and%2520contrast%250Ascenarios%252C%2520enabling%2520the%2520model%2520to%2520extract%2520high-quality%2520features%2520even%2520in%2520adverse%250Aenvironments.%2520Second%252C%2520we%2520introduce%2520a%2520Semantic%2520Information%2520Decoupling%2520module%252C%250Awhich%2520is%2520able%2520to%2520decouple%2520features%2520from%2520the%2520encoder%2520into%2520foreground%252C%250Abackground%252C%2520and%2520uncertainty%2520regions%252C%2520gradually%2520acquiring%2520the%2520ability%2520to%2520reduce%250Auncertainty%2520during%2520training.%2520The%2520Contrast-Driven%2520Feature%2520Aggregation%2520module%250Athen%2520contrasts%2520the%2520foreground%2520and%2520background%2520features%2520to%2520guide%2520multi-level%250Afeature%2520fusion%2520and%2520key%2520feature%2520enhancement%252C%2520further%2520distinguishing%2520the%2520entities%250Ato%2520be%2520segmented.%2520We%2520also%2520propose%2520a%2520Size-Aware%2520Decoder%2520to%2520solve%2520the%2520scale%250Asingularity%2520of%2520the%2520decoder.%2520It%2520accurately%2520locate%2520entities%2520of%2520different%2520sizes%2520in%250Athe%2520image%252C%2520thus%2520avoiding%2520erroneous%2520learning%2520of%2520co-occurrence%2520features.%250AExtensive%2520experiments%2520on%2520five%2520medical%2520image%2520datasets%2520across%2520three%2520scenarios%250Ademonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520our%2520method%252C%2520proving%2520its%250Aadvanced%2520nature%2520and%2520general%2520applicability%2520to%2520various%2520medical%2520image%2520segmentation%250Ascenarios.%2520Our%2520released%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Mengqi-Lei/ConDSeg%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConDSeg%3A%20A%20General%20Medical%20Image%20Segmentation%20Framework%20via%0A%20%20Contrast-Driven%20Feature%20Enhancement&entry.906535625=Mengqi%20Lei%20and%20Haochen%20Wu%20and%20Xinhua%20Lv%20and%20Xin%20Wang&entry.1292438233=%20%20Medical%20image%20segmentation%20plays%20an%20important%20role%20in%20clinical%20decision%0Amaking%2C%20treatment%20planning%2C%20and%20disease%20tracking.%20However%2C%20it%20still%20faces%20two%0Amajor%20challenges.%20On%20the%20one%20hand%2C%20there%20is%20often%20a%20%60%60soft%20boundary%27%27%20between%0Aforeground%20and%20background%20in%20medical%20images%2C%20with%20poor%20illumination%20and%20low%0Acontrast%20further%20reducing%20the%20distinguishability%20of%20foreground%20and%20background%0Awithin%20the%20image.%20On%20the%20other%20hand%2C%20co-occurrence%20phenomena%20are%20widespread%20in%0Amedical%20images%2C%20and%20learning%20these%20features%20is%20misleading%20to%20the%20model%27s%0Ajudgment.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20general%20framework%20called%0AContrast-Driven%20Medical%20Image%20Segmentation%20%28ConDSeg%29.%20First%2C%20we%20develop%20a%0Acontrastive%20training%20strategy%20called%20Consistency%20Reinforcement.%20It%20is%20designed%0Ato%20improve%20the%20encoder%27s%20robustness%20in%20various%20illumination%20and%20contrast%0Ascenarios%2C%20enabling%20the%20model%20to%20extract%20high-quality%20features%20even%20in%20adverse%0Aenvironments.%20Second%2C%20we%20introduce%20a%20Semantic%20Information%20Decoupling%20module%2C%0Awhich%20is%20able%20to%20decouple%20features%20from%20the%20encoder%20into%20foreground%2C%0Abackground%2C%20and%20uncertainty%20regions%2C%20gradually%20acquiring%20the%20ability%20to%20reduce%0Auncertainty%20during%20training.%20The%20Contrast-Driven%20Feature%20Aggregation%20module%0Athen%20contrasts%20the%20foreground%20and%20background%20features%20to%20guide%20multi-level%0Afeature%20fusion%20and%20key%20feature%20enhancement%2C%20further%20distinguishing%20the%20entities%0Ato%20be%20segmented.%20We%20also%20propose%20a%20Size-Aware%20Decoder%20to%20solve%20the%20scale%0Asingularity%20of%20the%20decoder.%20It%20accurately%20locate%20entities%20of%20different%20sizes%20in%0Athe%20image%2C%20thus%20avoiding%20erroneous%20learning%20of%20co-occurrence%20features.%0AExtensive%20experiments%20on%20five%20medical%20image%20datasets%20across%20three%20scenarios%0Ademonstrate%20the%20state-of-the-art%20performance%20of%20our%20method%2C%20proving%20its%0Aadvanced%20nature%20and%20general%20applicability%20to%20various%20medical%20image%20segmentation%0Ascenarios.%20Our%20released%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Mengqi-Lei/ConDSeg%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08345v1&entry.124074799=Read"},
{"title": "MoMA: Momentum Contrastive Learning with Multi-head Attention-based\n  Knowledge Distillation for Histopathology Image Analysis", "author": "Trinh Thi Le Vuong and Jin Tae Kwak", "abstract": "  There is no doubt that advanced artificial intelligence models and high\nquality data are the keys to success in developing computational pathology\ntools. Although the overall volume of pathology data keeps increasing, a lack\nof quality data is a common issue when it comes to a specific task due to\nseveral reasons including privacy and ethical issues with patient data. In this\nwork, we propose to exploit knowledge distillation, i.e., utilize the existing\nmodel to learn a new, target model, to overcome such issues in computational\npathology. Specifically, we employ a student-teacher framework to learn a\ntarget model from a pre-trained, teacher model without direct access to source\ndata and distill relevant knowledge via momentum contrastive learning with\nmulti-head attention mechanism, which provides consistent and context-aware\nfeature representations. This enables the target model to assimilate\ninformative representations of the teacher model while seamlessly adapting to\nthe unique nuances of the target data. The proposed method is rigorously\nevaluated across different scenarios where the teacher model was trained on the\nsame, relevant, and irrelevant classification tasks with the target model.\nExperimental results demonstrate the accuracy and robustness of our approach in\ntransferring knowledge to different domains and tasks, outperforming other\nrelated methods. Moreover, the results provide a guideline on the learning\nstrategy for different types of tasks and scenarios in computational pathology.\nCode is available at: \\url{https://github.com/trinhvg/MoMA}.\n", "link": "http://arxiv.org/abs/2308.16561v2", "date": "2024-12-11", "relevancy": 2.2133, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5952}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5315}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoMA%3A%20Momentum%20Contrastive%20Learning%20with%20Multi-head%20Attention-based%0A%20%20Knowledge%20Distillation%20for%20Histopathology%20Image%20Analysis&body=Title%3A%20MoMA%3A%20Momentum%20Contrastive%20Learning%20with%20Multi-head%20Attention-based%0A%20%20Knowledge%20Distillation%20for%20Histopathology%20Image%20Analysis%0AAuthor%3A%20Trinh%20Thi%20Le%20Vuong%20and%20Jin%20Tae%20Kwak%0AAbstract%3A%20%20%20There%20is%20no%20doubt%20that%20advanced%20artificial%20intelligence%20models%20and%20high%0Aquality%20data%20are%20the%20keys%20to%20success%20in%20developing%20computational%20pathology%0Atools.%20Although%20the%20overall%20volume%20of%20pathology%20data%20keeps%20increasing%2C%20a%20lack%0Aof%20quality%20data%20is%20a%20common%20issue%20when%20it%20comes%20to%20a%20specific%20task%20due%20to%0Aseveral%20reasons%20including%20privacy%20and%20ethical%20issues%20with%20patient%20data.%20In%20this%0Awork%2C%20we%20propose%20to%20exploit%20knowledge%20distillation%2C%20i.e.%2C%20utilize%20the%20existing%0Amodel%20to%20learn%20a%20new%2C%20target%20model%2C%20to%20overcome%20such%20issues%20in%20computational%0Apathology.%20Specifically%2C%20we%20employ%20a%20student-teacher%20framework%20to%20learn%20a%0Atarget%20model%20from%20a%20pre-trained%2C%20teacher%20model%20without%20direct%20access%20to%20source%0Adata%20and%20distill%20relevant%20knowledge%20via%20momentum%20contrastive%20learning%20with%0Amulti-head%20attention%20mechanism%2C%20which%20provides%20consistent%20and%20context-aware%0Afeature%20representations.%20This%20enables%20the%20target%20model%20to%20assimilate%0Ainformative%20representations%20of%20the%20teacher%20model%20while%20seamlessly%20adapting%20to%0Athe%20unique%20nuances%20of%20the%20target%20data.%20The%20proposed%20method%20is%20rigorously%0Aevaluated%20across%20different%20scenarios%20where%20the%20teacher%20model%20was%20trained%20on%20the%0Asame%2C%20relevant%2C%20and%20irrelevant%20classification%20tasks%20with%20the%20target%20model.%0AExperimental%20results%20demonstrate%20the%20accuracy%20and%20robustness%20of%20our%20approach%20in%0Atransferring%20knowledge%20to%20different%20domains%20and%20tasks%2C%20outperforming%20other%0Arelated%20methods.%20Moreover%2C%20the%20results%20provide%20a%20guideline%20on%20the%20learning%0Astrategy%20for%20different%20types%20of%20tasks%20and%20scenarios%20in%20computational%20pathology.%0ACode%20is%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/trinhvg/MoMA%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.16561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoMA%253A%2520Momentum%2520Contrastive%2520Learning%2520with%2520Multi-head%2520Attention-based%250A%2520%2520Knowledge%2520Distillation%2520for%2520Histopathology%2520Image%2520Analysis%26entry.906535625%3DTrinh%2520Thi%2520Le%2520Vuong%2520and%2520Jin%2520Tae%2520Kwak%26entry.1292438233%3D%2520%2520There%2520is%2520no%2520doubt%2520that%2520advanced%2520artificial%2520intelligence%2520models%2520and%2520high%250Aquality%2520data%2520are%2520the%2520keys%2520to%2520success%2520in%2520developing%2520computational%2520pathology%250Atools.%2520Although%2520the%2520overall%2520volume%2520of%2520pathology%2520data%2520keeps%2520increasing%252C%2520a%2520lack%250Aof%2520quality%2520data%2520is%2520a%2520common%2520issue%2520when%2520it%2520comes%2520to%2520a%2520specific%2520task%2520due%2520to%250Aseveral%2520reasons%2520including%2520privacy%2520and%2520ethical%2520issues%2520with%2520patient%2520data.%2520In%2520this%250Awork%252C%2520we%2520propose%2520to%2520exploit%2520knowledge%2520distillation%252C%2520i.e.%252C%2520utilize%2520the%2520existing%250Amodel%2520to%2520learn%2520a%2520new%252C%2520target%2520model%252C%2520to%2520overcome%2520such%2520issues%2520in%2520computational%250Apathology.%2520Specifically%252C%2520we%2520employ%2520a%2520student-teacher%2520framework%2520to%2520learn%2520a%250Atarget%2520model%2520from%2520a%2520pre-trained%252C%2520teacher%2520model%2520without%2520direct%2520access%2520to%2520source%250Adata%2520and%2520distill%2520relevant%2520knowledge%2520via%2520momentum%2520contrastive%2520learning%2520with%250Amulti-head%2520attention%2520mechanism%252C%2520which%2520provides%2520consistent%2520and%2520context-aware%250Afeature%2520representations.%2520This%2520enables%2520the%2520target%2520model%2520to%2520assimilate%250Ainformative%2520representations%2520of%2520the%2520teacher%2520model%2520while%2520seamlessly%2520adapting%2520to%250Athe%2520unique%2520nuances%2520of%2520the%2520target%2520data.%2520The%2520proposed%2520method%2520is%2520rigorously%250Aevaluated%2520across%2520different%2520scenarios%2520where%2520the%2520teacher%2520model%2520was%2520trained%2520on%2520the%250Asame%252C%2520relevant%252C%2520and%2520irrelevant%2520classification%2520tasks%2520with%2520the%2520target%2520model.%250AExperimental%2520results%2520demonstrate%2520the%2520accuracy%2520and%2520robustness%2520of%2520our%2520approach%2520in%250Atransferring%2520knowledge%2520to%2520different%2520domains%2520and%2520tasks%252C%2520outperforming%2520other%250Arelated%2520methods.%2520Moreover%252C%2520the%2520results%2520provide%2520a%2520guideline%2520on%2520the%2520learning%250Astrategy%2520for%2520different%2520types%2520of%2520tasks%2520and%2520scenarios%2520in%2520computational%2520pathology.%250ACode%2520is%2520available%2520at%253A%2520%255Curl%257Bhttps%253A//github.com/trinhvg/MoMA%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.16561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoMA%3A%20Momentum%20Contrastive%20Learning%20with%20Multi-head%20Attention-based%0A%20%20Knowledge%20Distillation%20for%20Histopathology%20Image%20Analysis&entry.906535625=Trinh%20Thi%20Le%20Vuong%20and%20Jin%20Tae%20Kwak&entry.1292438233=%20%20There%20is%20no%20doubt%20that%20advanced%20artificial%20intelligence%20models%20and%20high%0Aquality%20data%20are%20the%20keys%20to%20success%20in%20developing%20computational%20pathology%0Atools.%20Although%20the%20overall%20volume%20of%20pathology%20data%20keeps%20increasing%2C%20a%20lack%0Aof%20quality%20data%20is%20a%20common%20issue%20when%20it%20comes%20to%20a%20specific%20task%20due%20to%0Aseveral%20reasons%20including%20privacy%20and%20ethical%20issues%20with%20patient%20data.%20In%20this%0Awork%2C%20we%20propose%20to%20exploit%20knowledge%20distillation%2C%20i.e.%2C%20utilize%20the%20existing%0Amodel%20to%20learn%20a%20new%2C%20target%20model%2C%20to%20overcome%20such%20issues%20in%20computational%0Apathology.%20Specifically%2C%20we%20employ%20a%20student-teacher%20framework%20to%20learn%20a%0Atarget%20model%20from%20a%20pre-trained%2C%20teacher%20model%20without%20direct%20access%20to%20source%0Adata%20and%20distill%20relevant%20knowledge%20via%20momentum%20contrastive%20learning%20with%0Amulti-head%20attention%20mechanism%2C%20which%20provides%20consistent%20and%20context-aware%0Afeature%20representations.%20This%20enables%20the%20target%20model%20to%20assimilate%0Ainformative%20representations%20of%20the%20teacher%20model%20while%20seamlessly%20adapting%20to%0Athe%20unique%20nuances%20of%20the%20target%20data.%20The%20proposed%20method%20is%20rigorously%0Aevaluated%20across%20different%20scenarios%20where%20the%20teacher%20model%20was%20trained%20on%20the%0Asame%2C%20relevant%2C%20and%20irrelevant%20classification%20tasks%20with%20the%20target%20model.%0AExperimental%20results%20demonstrate%20the%20accuracy%20and%20robustness%20of%20our%20approach%20in%0Atransferring%20knowledge%20to%20different%20domains%20and%20tasks%2C%20outperforming%20other%0Arelated%20methods.%20Moreover%2C%20the%20results%20provide%20a%20guideline%20on%20the%20learning%0Astrategy%20for%20different%20types%20of%20tasks%20and%20scenarios%20in%20computational%20pathology.%0ACode%20is%20available%20at%3A%20%5Curl%7Bhttps%3A//github.com/trinhvg/MoMA%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.16561v2&entry.124074799=Read"},
{"title": "Large Language Models for Scholarly Ontology Generation: An Extensive\n  Analysis in the Engineering Field", "author": "Tanay Aggarwal and Angelo Salatino and Francesco Osborne and Enrico Motta", "abstract": "  Ontologies of research topics are crucial for structuring scientific\nknowledge, enabling scientists to navigate vast amounts of research, and\nforming the backbone of intelligent systems such as search engines and\nrecommendation systems. However, manual creation of these ontologies is\nexpensive, slow, and often results in outdated and overly general\nrepresentations. As a solution, researchers have been investigating ways to\nautomate or semi-automate the process of generating these ontologies. This\npaper offers a comprehensive analysis of the ability of large language models\n(LLMs) to identify semantic relationships between different research topics,\nwhich is a critical step in the development of such ontologies. To this end, we\ndeveloped a gold standard based on the IEEE Thesaurus to evaluate the task of\nidentifying four types of relationships between pairs of topics: broader,\nnarrower, same-as, and other. Our study evaluates the performance of seventeen\nLLMs, which differ in scale, accessibility (open vs. proprietary), and model\ntype (full vs. quantised), while also assessing four zero-shot reasoning\nstrategies. Several models have achieved outstanding results, including\nMixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,\n0.920, and 0.967, respectively. Furthermore, our findings demonstrate that\nsmaller, quantised models, when optimised through prompt engineering, can\ndeliver performance comparable to much larger proprietary models, while\nrequiring significantly fewer computational resources.\n", "link": "http://arxiv.org/abs/2412.08258v1", "date": "2024-12-11", "relevancy": 2.2126, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Scholarly%20Ontology%20Generation%3A%20An%20Extensive%0A%20%20Analysis%20in%20the%20Engineering%20Field&body=Title%3A%20Large%20Language%20Models%20for%20Scholarly%20Ontology%20Generation%3A%20An%20Extensive%0A%20%20Analysis%20in%20the%20Engineering%20Field%0AAuthor%3A%20Tanay%20Aggarwal%20and%20Angelo%20Salatino%20and%20Francesco%20Osborne%20and%20Enrico%20Motta%0AAbstract%3A%20%20%20Ontologies%20of%20research%20topics%20are%20crucial%20for%20structuring%20scientific%0Aknowledge%2C%20enabling%20scientists%20to%20navigate%20vast%20amounts%20of%20research%2C%20and%0Aforming%20the%20backbone%20of%20intelligent%20systems%20such%20as%20search%20engines%20and%0Arecommendation%20systems.%20However%2C%20manual%20creation%20of%20these%20ontologies%20is%0Aexpensive%2C%20slow%2C%20and%20often%20results%20in%20outdated%20and%20overly%20general%0Arepresentations.%20As%20a%20solution%2C%20researchers%20have%20been%20investigating%20ways%20to%0Aautomate%20or%20semi-automate%20the%20process%20of%20generating%20these%20ontologies.%20This%0Apaper%20offers%20a%20comprehensive%20analysis%20of%20the%20ability%20of%20large%20language%20models%0A%28LLMs%29%20to%20identify%20semantic%20relationships%20between%20different%20research%20topics%2C%0Awhich%20is%20a%20critical%20step%20in%20the%20development%20of%20such%20ontologies.%20To%20this%20end%2C%20we%0Adeveloped%20a%20gold%20standard%20based%20on%20the%20IEEE%20Thesaurus%20to%20evaluate%20the%20task%20of%0Aidentifying%20four%20types%20of%20relationships%20between%20pairs%20of%20topics%3A%20broader%2C%0Anarrower%2C%20same-as%2C%20and%20other.%20Our%20study%20evaluates%20the%20performance%20of%20seventeen%0ALLMs%2C%20which%20differ%20in%20scale%2C%20accessibility%20%28open%20vs.%20proprietary%29%2C%20and%20model%0Atype%20%28full%20vs.%20quantised%29%2C%20while%20also%20assessing%20four%20zero-shot%20reasoning%0Astrategies.%20Several%20models%20have%20achieved%20outstanding%20results%2C%20including%0AMixtral-8x7B%2C%20Dolphin-Mistral-7B%2C%20and%20Claude%203%20Sonnet%2C%20with%20F1-scores%20of%200.847%2C%0A0.920%2C%20and%200.967%2C%20respectively.%20Furthermore%2C%20our%20findings%20demonstrate%20that%0Asmaller%2C%20quantised%20models%2C%20when%20optimised%20through%20prompt%20engineering%2C%20can%0Adeliver%20performance%20comparable%20to%20much%20larger%20proprietary%20models%2C%20while%0Arequiring%20significantly%20fewer%20computational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520for%2520Scholarly%2520Ontology%2520Generation%253A%2520An%2520Extensive%250A%2520%2520Analysis%2520in%2520the%2520Engineering%2520Field%26entry.906535625%3DTanay%2520Aggarwal%2520and%2520Angelo%2520Salatino%2520and%2520Francesco%2520Osborne%2520and%2520Enrico%2520Motta%26entry.1292438233%3D%2520%2520Ontologies%2520of%2520research%2520topics%2520are%2520crucial%2520for%2520structuring%2520scientific%250Aknowledge%252C%2520enabling%2520scientists%2520to%2520navigate%2520vast%2520amounts%2520of%2520research%252C%2520and%250Aforming%2520the%2520backbone%2520of%2520intelligent%2520systems%2520such%2520as%2520search%2520engines%2520and%250Arecommendation%2520systems.%2520However%252C%2520manual%2520creation%2520of%2520these%2520ontologies%2520is%250Aexpensive%252C%2520slow%252C%2520and%2520often%2520results%2520in%2520outdated%2520and%2520overly%2520general%250Arepresentations.%2520As%2520a%2520solution%252C%2520researchers%2520have%2520been%2520investigating%2520ways%2520to%250Aautomate%2520or%2520semi-automate%2520the%2520process%2520of%2520generating%2520these%2520ontologies.%2520This%250Apaper%2520offers%2520a%2520comprehensive%2520analysis%2520of%2520the%2520ability%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520identify%2520semantic%2520relationships%2520between%2520different%2520research%2520topics%252C%250Awhich%2520is%2520a%2520critical%2520step%2520in%2520the%2520development%2520of%2520such%2520ontologies.%2520To%2520this%2520end%252C%2520we%250Adeveloped%2520a%2520gold%2520standard%2520based%2520on%2520the%2520IEEE%2520Thesaurus%2520to%2520evaluate%2520the%2520task%2520of%250Aidentifying%2520four%2520types%2520of%2520relationships%2520between%2520pairs%2520of%2520topics%253A%2520broader%252C%250Anarrower%252C%2520same-as%252C%2520and%2520other.%2520Our%2520study%2520evaluates%2520the%2520performance%2520of%2520seventeen%250ALLMs%252C%2520which%2520differ%2520in%2520scale%252C%2520accessibility%2520%2528open%2520vs.%2520proprietary%2529%252C%2520and%2520model%250Atype%2520%2528full%2520vs.%2520quantised%2529%252C%2520while%2520also%2520assessing%2520four%2520zero-shot%2520reasoning%250Astrategies.%2520Several%2520models%2520have%2520achieved%2520outstanding%2520results%252C%2520including%250AMixtral-8x7B%252C%2520Dolphin-Mistral-7B%252C%2520and%2520Claude%25203%2520Sonnet%252C%2520with%2520F1-scores%2520of%25200.847%252C%250A0.920%252C%2520and%25200.967%252C%2520respectively.%2520Furthermore%252C%2520our%2520findings%2520demonstrate%2520that%250Asmaller%252C%2520quantised%2520models%252C%2520when%2520optimised%2520through%2520prompt%2520engineering%252C%2520can%250Adeliver%2520performance%2520comparable%2520to%2520much%2520larger%2520proprietary%2520models%252C%2520while%250Arequiring%2520significantly%2520fewer%2520computational%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Scholarly%20Ontology%20Generation%3A%20An%20Extensive%0A%20%20Analysis%20in%20the%20Engineering%20Field&entry.906535625=Tanay%20Aggarwal%20and%20Angelo%20Salatino%20and%20Francesco%20Osborne%20and%20Enrico%20Motta&entry.1292438233=%20%20Ontologies%20of%20research%20topics%20are%20crucial%20for%20structuring%20scientific%0Aknowledge%2C%20enabling%20scientists%20to%20navigate%20vast%20amounts%20of%20research%2C%20and%0Aforming%20the%20backbone%20of%20intelligent%20systems%20such%20as%20search%20engines%20and%0Arecommendation%20systems.%20However%2C%20manual%20creation%20of%20these%20ontologies%20is%0Aexpensive%2C%20slow%2C%20and%20often%20results%20in%20outdated%20and%20overly%20general%0Arepresentations.%20As%20a%20solution%2C%20researchers%20have%20been%20investigating%20ways%20to%0Aautomate%20or%20semi-automate%20the%20process%20of%20generating%20these%20ontologies.%20This%0Apaper%20offers%20a%20comprehensive%20analysis%20of%20the%20ability%20of%20large%20language%20models%0A%28LLMs%29%20to%20identify%20semantic%20relationships%20between%20different%20research%20topics%2C%0Awhich%20is%20a%20critical%20step%20in%20the%20development%20of%20such%20ontologies.%20To%20this%20end%2C%20we%0Adeveloped%20a%20gold%20standard%20based%20on%20the%20IEEE%20Thesaurus%20to%20evaluate%20the%20task%20of%0Aidentifying%20four%20types%20of%20relationships%20between%20pairs%20of%20topics%3A%20broader%2C%0Anarrower%2C%20same-as%2C%20and%20other.%20Our%20study%20evaluates%20the%20performance%20of%20seventeen%0ALLMs%2C%20which%20differ%20in%20scale%2C%20accessibility%20%28open%20vs.%20proprietary%29%2C%20and%20model%0Atype%20%28full%20vs.%20quantised%29%2C%20while%20also%20assessing%20four%20zero-shot%20reasoning%0Astrategies.%20Several%20models%20have%20achieved%20outstanding%20results%2C%20including%0AMixtral-8x7B%2C%20Dolphin-Mistral-7B%2C%20and%20Claude%203%20Sonnet%2C%20with%20F1-scores%20of%200.847%2C%0A0.920%2C%20and%200.967%2C%20respectively.%20Furthermore%2C%20our%20findings%20demonstrate%20that%0Asmaller%2C%20quantised%20models%2C%20when%20optimised%20through%20prompt%20engineering%2C%20can%0Adeliver%20performance%20comparable%20to%20much%20larger%20proprietary%20models%2C%20while%0Arequiring%20significantly%20fewer%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08258v1&entry.124074799=Read"},
{"title": "Dynamic Disentangled Fusion Network for RGBT Tracking", "author": "Chenglong Li and Tao Wang and Zhaodong Ding and Yun Xiao and Jin Tang", "abstract": "  RGBT tracking usually suffers from various challenging factors of low\nresolution, similar appearance, extreme illumination, thermal crossover and\nocclusion, to name a few. Existing works often study complex fusion models to\nhandle challenging scenarios, but can not well adapt to various challenges,\nwhich might limit tracking performance. To handle this problem, we propose a\nnovel Dynamic Disentangled Fusion Network called DDFNet, which disentangles the\nfusion process into several dynamic fusion models via the challenge attributes\nto adapt to various challenging scenarios, for robust RGBT tracking. In\nparticular, we design six attribute-based fusion models to integrate RGB and\nthermal features under the six challenging scenarios respectively.Since each\nfusion model is to deal with the corresponding challenges, such disentangled\nfusion scheme could increase the fusion capacity without the dependence on\nlarge-scale training data. Considering that every challenging scenario also has\ndifferent levels of difficulty, we propose to optimize the combination of\nmultiple fusion units to form each attribute-based fusion model in a dynamic\nmanner, which could well adapt to the difficulty of the corresponding\nchallenging scenario. To address the issue that which fusion models should be\nactivated in the tracking process, we design an adaptive aggregation fusion\nmodule to integrate all features from attribute-based fusion models in an\nadaptive manner with a three-stage training algorithm. In addition, we design\nan enhancement fusion module to further strengthen the aggregated feature and\nmodality-specific features. Experimental results on benchmark datasets\ndemonstrate the effectiveness of our DDFNet against other state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2412.08441v1", "date": "2024-12-11", "relevancy": 2.2098, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5614}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5466}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Disentangled%20Fusion%20Network%20for%20RGBT%20Tracking&body=Title%3A%20Dynamic%20Disentangled%20Fusion%20Network%20for%20RGBT%20Tracking%0AAuthor%3A%20Chenglong%20Li%20and%20Tao%20Wang%20and%20Zhaodong%20Ding%20and%20Yun%20Xiao%20and%20Jin%20Tang%0AAbstract%3A%20%20%20RGBT%20tracking%20usually%20suffers%20from%20various%20challenging%20factors%20of%20low%0Aresolution%2C%20similar%20appearance%2C%20extreme%20illumination%2C%20thermal%20crossover%20and%0Aocclusion%2C%20to%20name%20a%20few.%20Existing%20works%20often%20study%20complex%20fusion%20models%20to%0Ahandle%20challenging%20scenarios%2C%20but%20can%20not%20well%20adapt%20to%20various%20challenges%2C%0Awhich%20might%20limit%20tracking%20performance.%20To%20handle%20this%20problem%2C%20we%20propose%20a%0Anovel%20Dynamic%20Disentangled%20Fusion%20Network%20called%20DDFNet%2C%20which%20disentangles%20the%0Afusion%20process%20into%20several%20dynamic%20fusion%20models%20via%20the%20challenge%20attributes%0Ato%20adapt%20to%20various%20challenging%20scenarios%2C%20for%20robust%20RGBT%20tracking.%20In%0Aparticular%2C%20we%20design%20six%20attribute-based%20fusion%20models%20to%20integrate%20RGB%20and%0Athermal%20features%20under%20the%20six%20challenging%20scenarios%20respectively.Since%20each%0Afusion%20model%20is%20to%20deal%20with%20the%20corresponding%20challenges%2C%20such%20disentangled%0Afusion%20scheme%20could%20increase%20the%20fusion%20capacity%20without%20the%20dependence%20on%0Alarge-scale%20training%20data.%20Considering%20that%20every%20challenging%20scenario%20also%20has%0Adifferent%20levels%20of%20difficulty%2C%20we%20propose%20to%20optimize%20the%20combination%20of%0Amultiple%20fusion%20units%20to%20form%20each%20attribute-based%20fusion%20model%20in%20a%20dynamic%0Amanner%2C%20which%20could%20well%20adapt%20to%20the%20difficulty%20of%20the%20corresponding%0Achallenging%20scenario.%20To%20address%20the%20issue%20that%20which%20fusion%20models%20should%20be%0Aactivated%20in%20the%20tracking%20process%2C%20we%20design%20an%20adaptive%20aggregation%20fusion%0Amodule%20to%20integrate%20all%20features%20from%20attribute-based%20fusion%20models%20in%20an%0Aadaptive%20manner%20with%20a%20three-stage%20training%20algorithm.%20In%20addition%2C%20we%20design%0Aan%20enhancement%20fusion%20module%20to%20further%20strengthen%20the%20aggregated%20feature%20and%0Amodality-specific%20features.%20Experimental%20results%20on%20benchmark%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20DDFNet%20against%20other%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Disentangled%2520Fusion%2520Network%2520for%2520RGBT%2520Tracking%26entry.906535625%3DChenglong%2520Li%2520and%2520Tao%2520Wang%2520and%2520Zhaodong%2520Ding%2520and%2520Yun%2520Xiao%2520and%2520Jin%2520Tang%26entry.1292438233%3D%2520%2520RGBT%2520tracking%2520usually%2520suffers%2520from%2520various%2520challenging%2520factors%2520of%2520low%250Aresolution%252C%2520similar%2520appearance%252C%2520extreme%2520illumination%252C%2520thermal%2520crossover%2520and%250Aocclusion%252C%2520to%2520name%2520a%2520few.%2520Existing%2520works%2520often%2520study%2520complex%2520fusion%2520models%2520to%250Ahandle%2520challenging%2520scenarios%252C%2520but%2520can%2520not%2520well%2520adapt%2520to%2520various%2520challenges%252C%250Awhich%2520might%2520limit%2520tracking%2520performance.%2520To%2520handle%2520this%2520problem%252C%2520we%2520propose%2520a%250Anovel%2520Dynamic%2520Disentangled%2520Fusion%2520Network%2520called%2520DDFNet%252C%2520which%2520disentangles%2520the%250Afusion%2520process%2520into%2520several%2520dynamic%2520fusion%2520models%2520via%2520the%2520challenge%2520attributes%250Ato%2520adapt%2520to%2520various%2520challenging%2520scenarios%252C%2520for%2520robust%2520RGBT%2520tracking.%2520In%250Aparticular%252C%2520we%2520design%2520six%2520attribute-based%2520fusion%2520models%2520to%2520integrate%2520RGB%2520and%250Athermal%2520features%2520under%2520the%2520six%2520challenging%2520scenarios%2520respectively.Since%2520each%250Afusion%2520model%2520is%2520to%2520deal%2520with%2520the%2520corresponding%2520challenges%252C%2520such%2520disentangled%250Afusion%2520scheme%2520could%2520increase%2520the%2520fusion%2520capacity%2520without%2520the%2520dependence%2520on%250Alarge-scale%2520training%2520data.%2520Considering%2520that%2520every%2520challenging%2520scenario%2520also%2520has%250Adifferent%2520levels%2520of%2520difficulty%252C%2520we%2520propose%2520to%2520optimize%2520the%2520combination%2520of%250Amultiple%2520fusion%2520units%2520to%2520form%2520each%2520attribute-based%2520fusion%2520model%2520in%2520a%2520dynamic%250Amanner%252C%2520which%2520could%2520well%2520adapt%2520to%2520the%2520difficulty%2520of%2520the%2520corresponding%250Achallenging%2520scenario.%2520To%2520address%2520the%2520issue%2520that%2520which%2520fusion%2520models%2520should%2520be%250Aactivated%2520in%2520the%2520tracking%2520process%252C%2520we%2520design%2520an%2520adaptive%2520aggregation%2520fusion%250Amodule%2520to%2520integrate%2520all%2520features%2520from%2520attribute-based%2520fusion%2520models%2520in%2520an%250Aadaptive%2520manner%2520with%2520a%2520three-stage%2520training%2520algorithm.%2520In%2520addition%252C%2520we%2520design%250Aan%2520enhancement%2520fusion%2520module%2520to%2520further%2520strengthen%2520the%2520aggregated%2520feature%2520and%250Amodality-specific%2520features.%2520Experimental%2520results%2520on%2520benchmark%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520DDFNet%2520against%2520other%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Disentangled%20Fusion%20Network%20for%20RGBT%20Tracking&entry.906535625=Chenglong%20Li%20and%20Tao%20Wang%20and%20Zhaodong%20Ding%20and%20Yun%20Xiao%20and%20Jin%20Tang&entry.1292438233=%20%20RGBT%20tracking%20usually%20suffers%20from%20various%20challenging%20factors%20of%20low%0Aresolution%2C%20similar%20appearance%2C%20extreme%20illumination%2C%20thermal%20crossover%20and%0Aocclusion%2C%20to%20name%20a%20few.%20Existing%20works%20often%20study%20complex%20fusion%20models%20to%0Ahandle%20challenging%20scenarios%2C%20but%20can%20not%20well%20adapt%20to%20various%20challenges%2C%0Awhich%20might%20limit%20tracking%20performance.%20To%20handle%20this%20problem%2C%20we%20propose%20a%0Anovel%20Dynamic%20Disentangled%20Fusion%20Network%20called%20DDFNet%2C%20which%20disentangles%20the%0Afusion%20process%20into%20several%20dynamic%20fusion%20models%20via%20the%20challenge%20attributes%0Ato%20adapt%20to%20various%20challenging%20scenarios%2C%20for%20robust%20RGBT%20tracking.%20In%0Aparticular%2C%20we%20design%20six%20attribute-based%20fusion%20models%20to%20integrate%20RGB%20and%0Athermal%20features%20under%20the%20six%20challenging%20scenarios%20respectively.Since%20each%0Afusion%20model%20is%20to%20deal%20with%20the%20corresponding%20challenges%2C%20such%20disentangled%0Afusion%20scheme%20could%20increase%20the%20fusion%20capacity%20without%20the%20dependence%20on%0Alarge-scale%20training%20data.%20Considering%20that%20every%20challenging%20scenario%20also%20has%0Adifferent%20levels%20of%20difficulty%2C%20we%20propose%20to%20optimize%20the%20combination%20of%0Amultiple%20fusion%20units%20to%20form%20each%20attribute-based%20fusion%20model%20in%20a%20dynamic%0Amanner%2C%20which%20could%20well%20adapt%20to%20the%20difficulty%20of%20the%20corresponding%0Achallenging%20scenario.%20To%20address%20the%20issue%20that%20which%20fusion%20models%20should%20be%0Aactivated%20in%20the%20tracking%20process%2C%20we%20design%20an%20adaptive%20aggregation%20fusion%0Amodule%20to%20integrate%20all%20features%20from%20attribute-based%20fusion%20models%20in%20an%0Aadaptive%20manner%20with%20a%20three-stage%20training%20algorithm.%20In%20addition%2C%20we%20design%0Aan%20enhancement%20fusion%20module%20to%20further%20strengthen%20the%20aggregated%20feature%20and%0Amodality-specific%20features.%20Experimental%20results%20on%20benchmark%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20DDFNet%20against%20other%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08441v1&entry.124074799=Read"},
{"title": "Annotation-Efficient Task Guidance for Medical Segment Anything", "author": "Tyler Ward and Abdullah-Al-Zubaer Imran", "abstract": "  Medical image segmentation is a key task in the imaging workflow, influencing\nmany image-based decisions. Traditional, fully-supervised segmentation models\nrely on large amounts of labeled training data, typically obtained through\nmanual annotation, which can be an expensive, time-consuming, and error-prone\nprocess. This signals a need for accurate, automatic, and annotation-efficient\nmethods of training these models. We propose SAM-Mix, a novel multitask\nlearning framework for medical image segmentation that uses class activation\nmaps produced by an auxiliary classifier to guide the predictions of the\nsemi-supervised segmentation branch, which is based on the SAM framework.\nExperimental evaluations on the public LiTS dataset confirm the effectiveness\nof SAM-Mix for simultaneous classification and segmentation of the liver from\nabdominal computed tomography (CT) scans. When trained for 90% fewer epochs on\nonly 50 labeled 2D slices, representing just 0.04% of the available labeled\ntraining data, SAM-Mix achieves a Dice improvement of 5.1% over the best\nbaseline model. The generalization results for SAM-Mix are even more\nimpressive, with the same model configuration yielding a 25.4% Dice improvement\non a cross-domain segmentation task. Our code is available at\nhttps://github.com/tbwa233/SAM-Mix.\n", "link": "http://arxiv.org/abs/2412.08575v1", "date": "2024-12-11", "relevancy": 2.1917, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5655}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5621}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Annotation-Efficient%20Task%20Guidance%20for%20Medical%20Segment%20Anything&body=Title%3A%20Annotation-Efficient%20Task%20Guidance%20for%20Medical%20Segment%20Anything%0AAuthor%3A%20Tyler%20Ward%20and%20Abdullah-Al-Zubaer%20Imran%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20a%20key%20task%20in%20the%20imaging%20workflow%2C%20influencing%0Amany%20image-based%20decisions.%20Traditional%2C%20fully-supervised%20segmentation%20models%0Arely%20on%20large%20amounts%20of%20labeled%20training%20data%2C%20typically%20obtained%20through%0Amanual%20annotation%2C%20which%20can%20be%20an%20expensive%2C%20time-consuming%2C%20and%20error-prone%0Aprocess.%20This%20signals%20a%20need%20for%20accurate%2C%20automatic%2C%20and%20annotation-efficient%0Amethods%20of%20training%20these%20models.%20We%20propose%20SAM-Mix%2C%20a%20novel%20multitask%0Alearning%20framework%20for%20medical%20image%20segmentation%20that%20uses%20class%20activation%0Amaps%20produced%20by%20an%20auxiliary%20classifier%20to%20guide%20the%20predictions%20of%20the%0Asemi-supervised%20segmentation%20branch%2C%20which%20is%20based%20on%20the%20SAM%20framework.%0AExperimental%20evaluations%20on%20the%20public%20LiTS%20dataset%20confirm%20the%20effectiveness%0Aof%20SAM-Mix%20for%20simultaneous%20classification%20and%20segmentation%20of%20the%20liver%20from%0Aabdominal%20computed%20tomography%20%28CT%29%20scans.%20When%20trained%20for%2090%25%20fewer%20epochs%20on%0Aonly%2050%20labeled%202D%20slices%2C%20representing%20just%200.04%25%20of%20the%20available%20labeled%0Atraining%20data%2C%20SAM-Mix%20achieves%20a%20Dice%20improvement%20of%205.1%25%20over%20the%20best%0Abaseline%20model.%20The%20generalization%20results%20for%20SAM-Mix%20are%20even%20more%0Aimpressive%2C%20with%20the%20same%20model%20configuration%20yielding%20a%2025.4%25%20Dice%20improvement%0Aon%20a%20cross-domain%20segmentation%20task.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tbwa233/SAM-Mix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnnotation-Efficient%2520Task%2520Guidance%2520for%2520Medical%2520Segment%2520Anything%26entry.906535625%3DTyler%2520Ward%2520and%2520Abdullah-Al-Zubaer%2520Imran%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520a%2520key%2520task%2520in%2520the%2520imaging%2520workflow%252C%2520influencing%250Amany%2520image-based%2520decisions.%2520Traditional%252C%2520fully-supervised%2520segmentation%2520models%250Arely%2520on%2520large%2520amounts%2520of%2520labeled%2520training%2520data%252C%2520typically%2520obtained%2520through%250Amanual%2520annotation%252C%2520which%2520can%2520be%2520an%2520expensive%252C%2520time-consuming%252C%2520and%2520error-prone%250Aprocess.%2520This%2520signals%2520a%2520need%2520for%2520accurate%252C%2520automatic%252C%2520and%2520annotation-efficient%250Amethods%2520of%2520training%2520these%2520models.%2520We%2520propose%2520SAM-Mix%252C%2520a%2520novel%2520multitask%250Alearning%2520framework%2520for%2520medical%2520image%2520segmentation%2520that%2520uses%2520class%2520activation%250Amaps%2520produced%2520by%2520an%2520auxiliary%2520classifier%2520to%2520guide%2520the%2520predictions%2520of%2520the%250Asemi-supervised%2520segmentation%2520branch%252C%2520which%2520is%2520based%2520on%2520the%2520SAM%2520framework.%250AExperimental%2520evaluations%2520on%2520the%2520public%2520LiTS%2520dataset%2520confirm%2520the%2520effectiveness%250Aof%2520SAM-Mix%2520for%2520simultaneous%2520classification%2520and%2520segmentation%2520of%2520the%2520liver%2520from%250Aabdominal%2520computed%2520tomography%2520%2528CT%2529%2520scans.%2520When%2520trained%2520for%252090%2525%2520fewer%2520epochs%2520on%250Aonly%252050%2520labeled%25202D%2520slices%252C%2520representing%2520just%25200.04%2525%2520of%2520the%2520available%2520labeled%250Atraining%2520data%252C%2520SAM-Mix%2520achieves%2520a%2520Dice%2520improvement%2520of%25205.1%2525%2520over%2520the%2520best%250Abaseline%2520model.%2520The%2520generalization%2520results%2520for%2520SAM-Mix%2520are%2520even%2520more%250Aimpressive%252C%2520with%2520the%2520same%2520model%2520configuration%2520yielding%2520a%252025.4%2525%2520Dice%2520improvement%250Aon%2520a%2520cross-domain%2520segmentation%2520task.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/tbwa233/SAM-Mix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotation-Efficient%20Task%20Guidance%20for%20Medical%20Segment%20Anything&entry.906535625=Tyler%20Ward%20and%20Abdullah-Al-Zubaer%20Imran&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20a%20key%20task%20in%20the%20imaging%20workflow%2C%20influencing%0Amany%20image-based%20decisions.%20Traditional%2C%20fully-supervised%20segmentation%20models%0Arely%20on%20large%20amounts%20of%20labeled%20training%20data%2C%20typically%20obtained%20through%0Amanual%20annotation%2C%20which%20can%20be%20an%20expensive%2C%20time-consuming%2C%20and%20error-prone%0Aprocess.%20This%20signals%20a%20need%20for%20accurate%2C%20automatic%2C%20and%20annotation-efficient%0Amethods%20of%20training%20these%20models.%20We%20propose%20SAM-Mix%2C%20a%20novel%20multitask%0Alearning%20framework%20for%20medical%20image%20segmentation%20that%20uses%20class%20activation%0Amaps%20produced%20by%20an%20auxiliary%20classifier%20to%20guide%20the%20predictions%20of%20the%0Asemi-supervised%20segmentation%20branch%2C%20which%20is%20based%20on%20the%20SAM%20framework.%0AExperimental%20evaluations%20on%20the%20public%20LiTS%20dataset%20confirm%20the%20effectiveness%0Aof%20SAM-Mix%20for%20simultaneous%20classification%20and%20segmentation%20of%20the%20liver%20from%0Aabdominal%20computed%20tomography%20%28CT%29%20scans.%20When%20trained%20for%2090%25%20fewer%20epochs%20on%0Aonly%2050%20labeled%202D%20slices%2C%20representing%20just%200.04%25%20of%20the%20available%20labeled%0Atraining%20data%2C%20SAM-Mix%20achieves%20a%20Dice%20improvement%20of%205.1%25%20over%20the%20best%0Abaseline%20model.%20The%20generalization%20results%20for%20SAM-Mix%20are%20even%20more%0Aimpressive%2C%20with%20the%20same%20model%20configuration%20yielding%20a%2025.4%25%20Dice%20improvement%0Aon%20a%20cross-domain%20segmentation%20task.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tbwa233/SAM-Mix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08575v1&entry.124074799=Read"},
{"title": "Unseen Visual Anomaly Generation", "author": "Han Sun and Yunkang Cao and Hao Dong and Olga Fink", "abstract": "  Visual anomaly detection (AD) presents significant challenges due to the\nscarcity of anomalous data samples. While numerous works have been proposed to\nsynthesize anomalous samples, these synthetic anomalies often lack authenticity\nor require extensive training data, limiting their applicability in real-world\nscenarios. In this work, we propose Anomaly Anything (AnomalyAny), a novel\nframework that leverages Stable Diffusion (SD)'s image generation capabilities\nto generate diverse and realistic unseen anomalies. By conditioning on a single\nnormal sample during test time, AnomalyAny is able to generate unseen anomalies\nfor arbitrary object types with text descriptions. Within AnomalyAny, we\npropose attention-guided anomaly optimization to direct SD attention on\ngenerating hard anomaly concepts. Additionally, we introduce prompt-guided\nanomaly refinement, incorporating detailed descriptions to further improve the\ngeneration quality. Extensive experiments on MVTec AD and VisA datasets\ndemonstrate AnomalyAny's ability in generating high-quality unseen anomalies\nand its effectiveness in enhancing downstream AD performance.\n", "link": "http://arxiv.org/abs/2406.01078v2", "date": "2024-12-11", "relevancy": 2.1915, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5567}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5437}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unseen%20Visual%20Anomaly%20Generation&body=Title%3A%20Unseen%20Visual%20Anomaly%20Generation%0AAuthor%3A%20Han%20Sun%20and%20Yunkang%20Cao%20and%20Hao%20Dong%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Visual%20anomaly%20detection%20%28AD%29%20presents%20significant%20challenges%20due%20to%20the%0Ascarcity%20of%20anomalous%20data%20samples.%20While%20numerous%20works%20have%20been%20proposed%20to%0Asynthesize%20anomalous%20samples%2C%20these%20synthetic%20anomalies%20often%20lack%20authenticity%0Aor%20require%20extensive%20training%20data%2C%20limiting%20their%20applicability%20in%20real-world%0Ascenarios.%20In%20this%20work%2C%20we%20propose%20Anomaly%20Anything%20%28AnomalyAny%29%2C%20a%20novel%0Aframework%20that%20leverages%20Stable%20Diffusion%20%28SD%29%27s%20image%20generation%20capabilities%0Ato%20generate%20diverse%20and%20realistic%20unseen%20anomalies.%20By%20conditioning%20on%20a%20single%0Anormal%20sample%20during%20test%20time%2C%20AnomalyAny%20is%20able%20to%20generate%20unseen%20anomalies%0Afor%20arbitrary%20object%20types%20with%20text%20descriptions.%20Within%20AnomalyAny%2C%20we%0Apropose%20attention-guided%20anomaly%20optimization%20to%20direct%20SD%20attention%20on%0Agenerating%20hard%20anomaly%20concepts.%20Additionally%2C%20we%20introduce%20prompt-guided%0Aanomaly%20refinement%2C%20incorporating%20detailed%20descriptions%20to%20further%20improve%20the%0Ageneration%20quality.%20Extensive%20experiments%20on%20MVTec%20AD%20and%20VisA%20datasets%0Ademonstrate%20AnomalyAny%27s%20ability%20in%20generating%20high-quality%20unseen%20anomalies%0Aand%20its%20effectiveness%20in%20enhancing%20downstream%20AD%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnseen%2520Visual%2520Anomaly%2520Generation%26entry.906535625%3DHan%2520Sun%2520and%2520Yunkang%2520Cao%2520and%2520Hao%2520Dong%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Visual%2520anomaly%2520detection%2520%2528AD%2529%2520presents%2520significant%2520challenges%2520due%2520to%2520the%250Ascarcity%2520of%2520anomalous%2520data%2520samples.%2520While%2520numerous%2520works%2520have%2520been%2520proposed%2520to%250Asynthesize%2520anomalous%2520samples%252C%2520these%2520synthetic%2520anomalies%2520often%2520lack%2520authenticity%250Aor%2520require%2520extensive%2520training%2520data%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%250Ascenarios.%2520In%2520this%2520work%252C%2520we%2520propose%2520Anomaly%2520Anything%2520%2528AnomalyAny%2529%252C%2520a%2520novel%250Aframework%2520that%2520leverages%2520Stable%2520Diffusion%2520%2528SD%2529%2527s%2520image%2520generation%2520capabilities%250Ato%2520generate%2520diverse%2520and%2520realistic%2520unseen%2520anomalies.%2520By%2520conditioning%2520on%2520a%2520single%250Anormal%2520sample%2520during%2520test%2520time%252C%2520AnomalyAny%2520is%2520able%2520to%2520generate%2520unseen%2520anomalies%250Afor%2520arbitrary%2520object%2520types%2520with%2520text%2520descriptions.%2520Within%2520AnomalyAny%252C%2520we%250Apropose%2520attention-guided%2520anomaly%2520optimization%2520to%2520direct%2520SD%2520attention%2520on%250Agenerating%2520hard%2520anomaly%2520concepts.%2520Additionally%252C%2520we%2520introduce%2520prompt-guided%250Aanomaly%2520refinement%252C%2520incorporating%2520detailed%2520descriptions%2520to%2520further%2520improve%2520the%250Ageneration%2520quality.%2520Extensive%2520experiments%2520on%2520MVTec%2520AD%2520and%2520VisA%2520datasets%250Ademonstrate%2520AnomalyAny%2527s%2520ability%2520in%2520generating%2520high-quality%2520unseen%2520anomalies%250Aand%2520its%2520effectiveness%2520in%2520enhancing%2520downstream%2520AD%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unseen%20Visual%20Anomaly%20Generation&entry.906535625=Han%20Sun%20and%20Yunkang%20Cao%20and%20Hao%20Dong%20and%20Olga%20Fink&entry.1292438233=%20%20Visual%20anomaly%20detection%20%28AD%29%20presents%20significant%20challenges%20due%20to%20the%0Ascarcity%20of%20anomalous%20data%20samples.%20While%20numerous%20works%20have%20been%20proposed%20to%0Asynthesize%20anomalous%20samples%2C%20these%20synthetic%20anomalies%20often%20lack%20authenticity%0Aor%20require%20extensive%20training%20data%2C%20limiting%20their%20applicability%20in%20real-world%0Ascenarios.%20In%20this%20work%2C%20we%20propose%20Anomaly%20Anything%20%28AnomalyAny%29%2C%20a%20novel%0Aframework%20that%20leverages%20Stable%20Diffusion%20%28SD%29%27s%20image%20generation%20capabilities%0Ato%20generate%20diverse%20and%20realistic%20unseen%20anomalies.%20By%20conditioning%20on%20a%20single%0Anormal%20sample%20during%20test%20time%2C%20AnomalyAny%20is%20able%20to%20generate%20unseen%20anomalies%0Afor%20arbitrary%20object%20types%20with%20text%20descriptions.%20Within%20AnomalyAny%2C%20we%0Apropose%20attention-guided%20anomaly%20optimization%20to%20direct%20SD%20attention%20on%0Agenerating%20hard%20anomaly%20concepts.%20Additionally%2C%20we%20introduce%20prompt-guided%0Aanomaly%20refinement%2C%20incorporating%20detailed%20descriptions%20to%20further%20improve%20the%0Ageneration%20quality.%20Extensive%20experiments%20on%20MVTec%20AD%20and%20VisA%20datasets%0Ademonstrate%20AnomalyAny%27s%20ability%20in%20generating%20high-quality%20unseen%20anomalies%0Aand%20its%20effectiveness%20in%20enhancing%20downstream%20AD%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01078v2&entry.124074799=Read"},
{"title": "Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in\n  Remote Sensing", "author": "Jakob Hackstein and Gencer Sumbul and Kai Norman Clasen and Beg\u00fcm Demir", "abstract": "  Self-supervised learning through masked autoencoders (MAEs) has recently\nattracted great attention for remote sensing (RS) image representation\nlearning, and thus embodies a significant potential for content-based image\nretrieval (CBIR) from ever-growing RS image archives. However, the existing MAE\nbased CBIR studies in RS assume that the considered RS images are acquired by a\nsingle image sensor, and thus are only suitable for uni-modal CBIR problems.\nThe effectiveness of MAEs for cross-sensor CBIR, which aims to search\nsemantically similar images across different image modalities, has not been\nexplored yet. In this paper, we take the first step to explore the\neffectiveness of MAEs for sensor-agnostic CBIR in RS. To this end, we present a\nsystematic overview on the possible adaptations of the vanilla MAE to exploit\nmasked image modeling on multi-sensor RS image archives (denoted as\ncross-sensor masked autoencoders [CSMAEs]) in the context of CBIR. Based on\ndifferent adjustments applied to the vanilla MAE, we introduce different CSMAE\nmodels. We also provide an extensive experimental analysis of these CSMAE\nmodels. We finally derive a guideline to exploit masked image modeling for\nuni-modal and cross-modal CBIR problems in RS. The code of this work is\npublicly available at https://github.com/jakhac/CSMAE.\n", "link": "http://arxiv.org/abs/2401.07782v3", "date": "2024-12-11", "relevancy": 2.1731, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5741}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Masked%20Autoencoders%20for%20Sensor-Agnostic%20Image%20Retrieval%20in%0A%20%20Remote%20Sensing&body=Title%3A%20Exploring%20Masked%20Autoencoders%20for%20Sensor-Agnostic%20Image%20Retrieval%20in%0A%20%20Remote%20Sensing%0AAuthor%3A%20Jakob%20Hackstein%20and%20Gencer%20Sumbul%20and%20Kai%20Norman%20Clasen%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20Self-supervised%20learning%20through%20masked%20autoencoders%20%28MAEs%29%20has%20recently%0Aattracted%20great%20attention%20for%20remote%20sensing%20%28RS%29%20image%20representation%0Alearning%2C%20and%20thus%20embodies%20a%20significant%20potential%20for%20content-based%20image%0Aretrieval%20%28CBIR%29%20from%20ever-growing%20RS%20image%20archives.%20However%2C%20the%20existing%20MAE%0Abased%20CBIR%20studies%20in%20RS%20assume%20that%20the%20considered%20RS%20images%20are%20acquired%20by%20a%0Asingle%20image%20sensor%2C%20and%20thus%20are%20only%20suitable%20for%20uni-modal%20CBIR%20problems.%0AThe%20effectiveness%20of%20MAEs%20for%20cross-sensor%20CBIR%2C%20which%20aims%20to%20search%0Asemantically%20similar%20images%20across%20different%20image%20modalities%2C%20has%20not%20been%0Aexplored%20yet.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20to%20explore%20the%0Aeffectiveness%20of%20MAEs%20for%20sensor-agnostic%20CBIR%20in%20RS.%20To%20this%20end%2C%20we%20present%20a%0Asystematic%20overview%20on%20the%20possible%20adaptations%20of%20the%20vanilla%20MAE%20to%20exploit%0Amasked%20image%20modeling%20on%20multi-sensor%20RS%20image%20archives%20%28denoted%20as%0Across-sensor%20masked%20autoencoders%20%5BCSMAEs%5D%29%20in%20the%20context%20of%20CBIR.%20Based%20on%0Adifferent%20adjustments%20applied%20to%20the%20vanilla%20MAE%2C%20we%20introduce%20different%20CSMAE%0Amodels.%20We%20also%20provide%20an%20extensive%20experimental%20analysis%20of%20these%20CSMAE%0Amodels.%20We%20finally%20derive%20a%20guideline%20to%20exploit%20masked%20image%20modeling%20for%0Auni-modal%20and%20cross-modal%20CBIR%20problems%20in%20RS.%20The%20code%20of%20this%20work%20is%0Apublicly%20available%20at%20https%3A//github.com/jakhac/CSMAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07782v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Masked%2520Autoencoders%2520for%2520Sensor-Agnostic%2520Image%2520Retrieval%2520in%250A%2520%2520Remote%2520Sensing%26entry.906535625%3DJakob%2520Hackstein%2520and%2520Gencer%2520Sumbul%2520and%2520Kai%2520Norman%2520Clasen%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520through%2520masked%2520autoencoders%2520%2528MAEs%2529%2520has%2520recently%250Aattracted%2520great%2520attention%2520for%2520remote%2520sensing%2520%2528RS%2529%2520image%2520representation%250Alearning%252C%2520and%2520thus%2520embodies%2520a%2520significant%2520potential%2520for%2520content-based%2520image%250Aretrieval%2520%2528CBIR%2529%2520from%2520ever-growing%2520RS%2520image%2520archives.%2520However%252C%2520the%2520existing%2520MAE%250Abased%2520CBIR%2520studies%2520in%2520RS%2520assume%2520that%2520the%2520considered%2520RS%2520images%2520are%2520acquired%2520by%2520a%250Asingle%2520image%2520sensor%252C%2520and%2520thus%2520are%2520only%2520suitable%2520for%2520uni-modal%2520CBIR%2520problems.%250AThe%2520effectiveness%2520of%2520MAEs%2520for%2520cross-sensor%2520CBIR%252C%2520which%2520aims%2520to%2520search%250Asemantically%2520similar%2520images%2520across%2520different%2520image%2520modalities%252C%2520has%2520not%2520been%250Aexplored%2520yet.%2520In%2520this%2520paper%252C%2520we%2520take%2520the%2520first%2520step%2520to%2520explore%2520the%250Aeffectiveness%2520of%2520MAEs%2520for%2520sensor-agnostic%2520CBIR%2520in%2520RS.%2520To%2520this%2520end%252C%2520we%2520present%2520a%250Asystematic%2520overview%2520on%2520the%2520possible%2520adaptations%2520of%2520the%2520vanilla%2520MAE%2520to%2520exploit%250Amasked%2520image%2520modeling%2520on%2520multi-sensor%2520RS%2520image%2520archives%2520%2528denoted%2520as%250Across-sensor%2520masked%2520autoencoders%2520%255BCSMAEs%255D%2529%2520in%2520the%2520context%2520of%2520CBIR.%2520Based%2520on%250Adifferent%2520adjustments%2520applied%2520to%2520the%2520vanilla%2520MAE%252C%2520we%2520introduce%2520different%2520CSMAE%250Amodels.%2520We%2520also%2520provide%2520an%2520extensive%2520experimental%2520analysis%2520of%2520these%2520CSMAE%250Amodels.%2520We%2520finally%2520derive%2520a%2520guideline%2520to%2520exploit%2520masked%2520image%2520modeling%2520for%250Auni-modal%2520and%2520cross-modal%2520CBIR%2520problems%2520in%2520RS.%2520The%2520code%2520of%2520this%2520work%2520is%250Apublicly%2520available%2520at%2520https%253A//github.com/jakhac/CSMAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07782v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Masked%20Autoencoders%20for%20Sensor-Agnostic%20Image%20Retrieval%20in%0A%20%20Remote%20Sensing&entry.906535625=Jakob%20Hackstein%20and%20Gencer%20Sumbul%20and%20Kai%20Norman%20Clasen%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20Self-supervised%20learning%20through%20masked%20autoencoders%20%28MAEs%29%20has%20recently%0Aattracted%20great%20attention%20for%20remote%20sensing%20%28RS%29%20image%20representation%0Alearning%2C%20and%20thus%20embodies%20a%20significant%20potential%20for%20content-based%20image%0Aretrieval%20%28CBIR%29%20from%20ever-growing%20RS%20image%20archives.%20However%2C%20the%20existing%20MAE%0Abased%20CBIR%20studies%20in%20RS%20assume%20that%20the%20considered%20RS%20images%20are%20acquired%20by%20a%0Asingle%20image%20sensor%2C%20and%20thus%20are%20only%20suitable%20for%20uni-modal%20CBIR%20problems.%0AThe%20effectiveness%20of%20MAEs%20for%20cross-sensor%20CBIR%2C%20which%20aims%20to%20search%0Asemantically%20similar%20images%20across%20different%20image%20modalities%2C%20has%20not%20been%0Aexplored%20yet.%20In%20this%20paper%2C%20we%20take%20the%20first%20step%20to%20explore%20the%0Aeffectiveness%20of%20MAEs%20for%20sensor-agnostic%20CBIR%20in%20RS.%20To%20this%20end%2C%20we%20present%20a%0Asystematic%20overview%20on%20the%20possible%20adaptations%20of%20the%20vanilla%20MAE%20to%20exploit%0Amasked%20image%20modeling%20on%20multi-sensor%20RS%20image%20archives%20%28denoted%20as%0Across-sensor%20masked%20autoencoders%20%5BCSMAEs%5D%29%20in%20the%20context%20of%20CBIR.%20Based%20on%0Adifferent%20adjustments%20applied%20to%20the%20vanilla%20MAE%2C%20we%20introduce%20different%20CSMAE%0Amodels.%20We%20also%20provide%20an%20extensive%20experimental%20analysis%20of%20these%20CSMAE%0Amodels.%20We%20finally%20derive%20a%20guideline%20to%20exploit%20masked%20image%20modeling%20for%0Auni-modal%20and%20cross-modal%20CBIR%20problems%20in%20RS.%20The%20code%20of%20this%20work%20is%0Apublicly%20available%20at%20https%3A//github.com/jakhac/CSMAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07782v3&entry.124074799=Read"},
{"title": "Domain-Adaptive Pre-training of Self-Supervised Foundation Models for\n  Medical Image Classification in Gastrointestinal Endoscopy", "author": "Marcel Roth and Micha V. Nowak and Adrian Krenzer and Frank Puppe", "abstract": "  Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE)\ndiagnostics by offering a non-invasive method for capturing detailed images of\nthe gastrointestinal tract, enabling early disease detection. However, its\npotential is limited by the sheer volume of images generated during the imaging\nprocedure, which can take anywhere from 6-8 hours and often produce up to 1\nmillion images, necessitating automated analysis. Additionally, the variability\nof these images, combined with the need for expert annotations and the scarcity\nof large, high-quality labeled datasets, constrains the effectiveness of\ncurrent medical image analysis models. To address this, we introduce a novel\nlarge GIE dataset, called EndoExtend24, created by merging ten existing public\nand private datasets, ensuring patient integrity across splits. EndoExtend24\nincludes over 226,000 labeled images, as well as dynamic class mappings, which\nallow unified training across datasets with differing labeling granularity,\nsupporting up to 123 distinct pathological findings. Further, we propose to\nleverage domain adaptive pre-training of foundation models trained with\nself-supervision on generic image data, to adapt them to the task of GIE\nmedical image diagnosis. Specifically, the EVA-02 model, which is based on the\nViT architecture and trained on ImageNet-22k with masked image modeling (using\nEVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to\nachieve domain adaptation, and finally trained on the Capsule Endoscopy 2024\nChallenge dataset. Our model demonstrates robust performance, securing third\nplace in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762\nand a balanced accuracy of 37.1% on the test set. These results emphasize the\neffectiveness of our domain-adaptive pre-training approach and the enriched\nEndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics.\n", "link": "http://arxiv.org/abs/2410.21302v4", "date": "2024-12-11", "relevancy": 2.1725, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5594}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Adaptive%20Pre-training%20of%20Self-Supervised%20Foundation%20Models%20for%0A%20%20Medical%20Image%20Classification%20in%20Gastrointestinal%20Endoscopy&body=Title%3A%20Domain-Adaptive%20Pre-training%20of%20Self-Supervised%20Foundation%20Models%20for%0A%20%20Medical%20Image%20Classification%20in%20Gastrointestinal%20Endoscopy%0AAuthor%3A%20Marcel%20Roth%20and%20Micha%20V.%20Nowak%20and%20Adrian%20Krenzer%20and%20Frank%20Puppe%0AAbstract%3A%20%20%20Video%20capsule%20endoscopy%20has%20transformed%20gastrointestinal%20endoscopy%20%28GIE%29%0Adiagnostics%20by%20offering%20a%20non-invasive%20method%20for%20capturing%20detailed%20images%20of%0Athe%20gastrointestinal%20tract%2C%20enabling%20early%20disease%20detection.%20However%2C%20its%0Apotential%20is%20limited%20by%20the%20sheer%20volume%20of%20images%20generated%20during%20the%20imaging%0Aprocedure%2C%20which%20can%20take%20anywhere%20from%206-8%20hours%20and%20often%20produce%20up%20to%201%0Amillion%20images%2C%20necessitating%20automated%20analysis.%20Additionally%2C%20the%20variability%0Aof%20these%20images%2C%20combined%20with%20the%20need%20for%20expert%20annotations%20and%20the%20scarcity%0Aof%20large%2C%20high-quality%20labeled%20datasets%2C%20constrains%20the%20effectiveness%20of%0Acurrent%20medical%20image%20analysis%20models.%20To%20address%20this%2C%20we%20introduce%20a%20novel%0Alarge%20GIE%20dataset%2C%20called%20EndoExtend24%2C%20created%20by%20merging%20ten%20existing%20public%0Aand%20private%20datasets%2C%20ensuring%20patient%20integrity%20across%20splits.%20EndoExtend24%0Aincludes%20over%20226%2C000%20labeled%20images%2C%20as%20well%20as%20dynamic%20class%20mappings%2C%20which%0Aallow%20unified%20training%20across%20datasets%20with%20differing%20labeling%20granularity%2C%0Asupporting%20up%20to%20123%20distinct%20pathological%20findings.%20Further%2C%20we%20propose%20to%0Aleverage%20domain%20adaptive%20pre-training%20of%20foundation%20models%20trained%20with%0Aself-supervision%20on%20generic%20image%20data%2C%20to%20adapt%20them%20to%20the%20task%20of%20GIE%0Amedical%20image%20diagnosis.%20Specifically%2C%20the%20EVA-02%20model%2C%20which%20is%20based%20on%20the%0AViT%20architecture%20and%20trained%20on%20ImageNet-22k%20with%20masked%20image%20modeling%20%28using%0AEVA-CLIP%20as%20a%20MIM%20teacher%29%2C%20is%20pre-trained%20on%20the%20EndoExtend24%20dataset%20to%0Aachieve%20domain%20adaptation%2C%20and%20finally%20trained%20on%20the%20Capsule%20Endoscopy%202024%0AChallenge%20dataset.%20Our%20model%20demonstrates%20robust%20performance%2C%20securing%20third%0Aplace%20in%20the%20Capsule%20Endoscopy%202024%20Challenge.%20We%20achieved%20a%20macro%20AUC%20of%200.762%0Aand%20a%20balanced%20accuracy%20of%2037.1%25%20on%20the%20test%20set.%20These%20results%20emphasize%20the%0Aeffectiveness%20of%20our%20domain-adaptive%20pre-training%20approach%20and%20the%20enriched%0AEndoExtend24%20dataset%20in%20advancing%20gastrointestinal%20endoscopy%20diagnostics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21302v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Adaptive%2520Pre-training%2520of%2520Self-Supervised%2520Foundation%2520Models%2520for%250A%2520%2520Medical%2520Image%2520Classification%2520in%2520Gastrointestinal%2520Endoscopy%26entry.906535625%3DMarcel%2520Roth%2520and%2520Micha%2520V.%2520Nowak%2520and%2520Adrian%2520Krenzer%2520and%2520Frank%2520Puppe%26entry.1292438233%3D%2520%2520Video%2520capsule%2520endoscopy%2520has%2520transformed%2520gastrointestinal%2520endoscopy%2520%2528GIE%2529%250Adiagnostics%2520by%2520offering%2520a%2520non-invasive%2520method%2520for%2520capturing%2520detailed%2520images%2520of%250Athe%2520gastrointestinal%2520tract%252C%2520enabling%2520early%2520disease%2520detection.%2520However%252C%2520its%250Apotential%2520is%2520limited%2520by%2520the%2520sheer%2520volume%2520of%2520images%2520generated%2520during%2520the%2520imaging%250Aprocedure%252C%2520which%2520can%2520take%2520anywhere%2520from%25206-8%2520hours%2520and%2520often%2520produce%2520up%2520to%25201%250Amillion%2520images%252C%2520necessitating%2520automated%2520analysis.%2520Additionally%252C%2520the%2520variability%250Aof%2520these%2520images%252C%2520combined%2520with%2520the%2520need%2520for%2520expert%2520annotations%2520and%2520the%2520scarcity%250Aof%2520large%252C%2520high-quality%2520labeled%2520datasets%252C%2520constrains%2520the%2520effectiveness%2520of%250Acurrent%2520medical%2520image%2520analysis%2520models.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%250Alarge%2520GIE%2520dataset%252C%2520called%2520EndoExtend24%252C%2520created%2520by%2520merging%2520ten%2520existing%2520public%250Aand%2520private%2520datasets%252C%2520ensuring%2520patient%2520integrity%2520across%2520splits.%2520EndoExtend24%250Aincludes%2520over%2520226%252C000%2520labeled%2520images%252C%2520as%2520well%2520as%2520dynamic%2520class%2520mappings%252C%2520which%250Aallow%2520unified%2520training%2520across%2520datasets%2520with%2520differing%2520labeling%2520granularity%252C%250Asupporting%2520up%2520to%2520123%2520distinct%2520pathological%2520findings.%2520Further%252C%2520we%2520propose%2520to%250Aleverage%2520domain%2520adaptive%2520pre-training%2520of%2520foundation%2520models%2520trained%2520with%250Aself-supervision%2520on%2520generic%2520image%2520data%252C%2520to%2520adapt%2520them%2520to%2520the%2520task%2520of%2520GIE%250Amedical%2520image%2520diagnosis.%2520Specifically%252C%2520the%2520EVA-02%2520model%252C%2520which%2520is%2520based%2520on%2520the%250AViT%2520architecture%2520and%2520trained%2520on%2520ImageNet-22k%2520with%2520masked%2520image%2520modeling%2520%2528using%250AEVA-CLIP%2520as%2520a%2520MIM%2520teacher%2529%252C%2520is%2520pre-trained%2520on%2520the%2520EndoExtend24%2520dataset%2520to%250Aachieve%2520domain%2520adaptation%252C%2520and%2520finally%2520trained%2520on%2520the%2520Capsule%2520Endoscopy%25202024%250AChallenge%2520dataset.%2520Our%2520model%2520demonstrates%2520robust%2520performance%252C%2520securing%2520third%250Aplace%2520in%2520the%2520Capsule%2520Endoscopy%25202024%2520Challenge.%2520We%2520achieved%2520a%2520macro%2520AUC%2520of%25200.762%250Aand%2520a%2520balanced%2520accuracy%2520of%252037.1%2525%2520on%2520the%2520test%2520set.%2520These%2520results%2520emphasize%2520the%250Aeffectiveness%2520of%2520our%2520domain-adaptive%2520pre-training%2520approach%2520and%2520the%2520enriched%250AEndoExtend24%2520dataset%2520in%2520advancing%2520gastrointestinal%2520endoscopy%2520diagnostics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21302v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Adaptive%20Pre-training%20of%20Self-Supervised%20Foundation%20Models%20for%0A%20%20Medical%20Image%20Classification%20in%20Gastrointestinal%20Endoscopy&entry.906535625=Marcel%20Roth%20and%20Micha%20V.%20Nowak%20and%20Adrian%20Krenzer%20and%20Frank%20Puppe&entry.1292438233=%20%20Video%20capsule%20endoscopy%20has%20transformed%20gastrointestinal%20endoscopy%20%28GIE%29%0Adiagnostics%20by%20offering%20a%20non-invasive%20method%20for%20capturing%20detailed%20images%20of%0Athe%20gastrointestinal%20tract%2C%20enabling%20early%20disease%20detection.%20However%2C%20its%0Apotential%20is%20limited%20by%20the%20sheer%20volume%20of%20images%20generated%20during%20the%20imaging%0Aprocedure%2C%20which%20can%20take%20anywhere%20from%206-8%20hours%20and%20often%20produce%20up%20to%201%0Amillion%20images%2C%20necessitating%20automated%20analysis.%20Additionally%2C%20the%20variability%0Aof%20these%20images%2C%20combined%20with%20the%20need%20for%20expert%20annotations%20and%20the%20scarcity%0Aof%20large%2C%20high-quality%20labeled%20datasets%2C%20constrains%20the%20effectiveness%20of%0Acurrent%20medical%20image%20analysis%20models.%20To%20address%20this%2C%20we%20introduce%20a%20novel%0Alarge%20GIE%20dataset%2C%20called%20EndoExtend24%2C%20created%20by%20merging%20ten%20existing%20public%0Aand%20private%20datasets%2C%20ensuring%20patient%20integrity%20across%20splits.%20EndoExtend24%0Aincludes%20over%20226%2C000%20labeled%20images%2C%20as%20well%20as%20dynamic%20class%20mappings%2C%20which%0Aallow%20unified%20training%20across%20datasets%20with%20differing%20labeling%20granularity%2C%0Asupporting%20up%20to%20123%20distinct%20pathological%20findings.%20Further%2C%20we%20propose%20to%0Aleverage%20domain%20adaptive%20pre-training%20of%20foundation%20models%20trained%20with%0Aself-supervision%20on%20generic%20image%20data%2C%20to%20adapt%20them%20to%20the%20task%20of%20GIE%0Amedical%20image%20diagnosis.%20Specifically%2C%20the%20EVA-02%20model%2C%20which%20is%20based%20on%20the%0AViT%20architecture%20and%20trained%20on%20ImageNet-22k%20with%20masked%20image%20modeling%20%28using%0AEVA-CLIP%20as%20a%20MIM%20teacher%29%2C%20is%20pre-trained%20on%20the%20EndoExtend24%20dataset%20to%0Aachieve%20domain%20adaptation%2C%20and%20finally%20trained%20on%20the%20Capsule%20Endoscopy%202024%0AChallenge%20dataset.%20Our%20model%20demonstrates%20robust%20performance%2C%20securing%20third%0Aplace%20in%20the%20Capsule%20Endoscopy%202024%20Challenge.%20We%20achieved%20a%20macro%20AUC%20of%200.762%0Aand%20a%20balanced%20accuracy%20of%2037.1%25%20on%20the%20test%20set.%20These%20results%20emphasize%20the%0Aeffectiveness%20of%20our%20domain-adaptive%20pre-training%20approach%20and%20the%20enriched%0AEndoExtend24%20dataset%20in%20advancing%20gastrointestinal%20endoscopy%20diagnostics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21302v4&entry.124074799=Read"},
{"title": "Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks\n  and Diffusion Model", "author": "Jiahua Xu and Dawei Zhou and Lei Hu and Jianfeng Guo and Feng Yang and Zaiyi Liu and Nannan Wang and Xinbo Gao", "abstract": "  Motion artifacts present in magnetic resonance imaging (MRI) can seriously\ninterfere with clinical diagnosis. Removing motion artifacts is a\nstraightforward solution and has been extensively studied. However, paired data\nare still heavily relied on in recent works and the perturbations in k-space\n(frequency domain) are not well considered, which limits their applications in\nthe clinical field. To address these issues, we propose a novel unsupervised\npurification method which leverages pixel-frequency information of noisy MRI\nimages to guide a pre-trained diffusion model to recover clean MRI images.\nSpecifically, considering that motion artifacts are mainly concentrated in\nhigh-frequency components in k-space, we utilize the low-frequency components\nas the guide to ensure correct tissue textures. Additionally, given that\nhigh-frequency and pixel information are helpful for recovering shape and\ndetail textures, we design alternate complementary masks to simultaneously\ndestroy the artifact structure and exploit useful information. Quantitative\nexperiments are performed on datasets from different tissues and show that our\nmethod achieves superior performance on several metrics. Qualitative\nevaluations with radiologists also show that our method provides better\nclinical feedback. Our code is available at https://github.com/medcx/PFAD.\n", "link": "http://arxiv.org/abs/2412.07590v2", "date": "2024-12-11", "relevancy": 2.1684, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5581}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5334}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20Artifact%20Removal%20in%20Pixel-Frequency%20Domain%20via%20Alternate%20Masks%0A%20%20and%20Diffusion%20Model&body=Title%3A%20Motion%20Artifact%20Removal%20in%20Pixel-Frequency%20Domain%20via%20Alternate%20Masks%0A%20%20and%20Diffusion%20Model%0AAuthor%3A%20Jiahua%20Xu%20and%20Dawei%20Zhou%20and%20Lei%20Hu%20and%20Jianfeng%20Guo%20and%20Feng%20Yang%20and%20Zaiyi%20Liu%20and%20Nannan%20Wang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Motion%20artifacts%20present%20in%20magnetic%20resonance%20imaging%20%28MRI%29%20can%20seriously%0Ainterfere%20with%20clinical%20diagnosis.%20Removing%20motion%20artifacts%20is%20a%0Astraightforward%20solution%20and%20has%20been%20extensively%20studied.%20However%2C%20paired%20data%0Aare%20still%20heavily%20relied%20on%20in%20recent%20works%20and%20the%20perturbations%20in%20k-space%0A%28frequency%20domain%29%20are%20not%20well%20considered%2C%20which%20limits%20their%20applications%20in%0Athe%20clinical%20field.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20unsupervised%0Apurification%20method%20which%20leverages%20pixel-frequency%20information%20of%20noisy%20MRI%0Aimages%20to%20guide%20a%20pre-trained%20diffusion%20model%20to%20recover%20clean%20MRI%20images.%0ASpecifically%2C%20considering%20that%20motion%20artifacts%20are%20mainly%20concentrated%20in%0Ahigh-frequency%20components%20in%20k-space%2C%20we%20utilize%20the%20low-frequency%20components%0Aas%20the%20guide%20to%20ensure%20correct%20tissue%20textures.%20Additionally%2C%20given%20that%0Ahigh-frequency%20and%20pixel%20information%20are%20helpful%20for%20recovering%20shape%20and%0Adetail%20textures%2C%20we%20design%20alternate%20complementary%20masks%20to%20simultaneously%0Adestroy%20the%20artifact%20structure%20and%20exploit%20useful%20information.%20Quantitative%0Aexperiments%20are%20performed%20on%20datasets%20from%20different%20tissues%20and%20show%20that%20our%0Amethod%20achieves%20superior%20performance%20on%20several%20metrics.%20Qualitative%0Aevaluations%20with%20radiologists%20also%20show%20that%20our%20method%20provides%20better%0Aclinical%20feedback.%20Our%20code%20is%20available%20at%20https%3A//github.com/medcx/PFAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07590v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520Artifact%2520Removal%2520in%2520Pixel-Frequency%2520Domain%2520via%2520Alternate%2520Masks%250A%2520%2520and%2520Diffusion%2520Model%26entry.906535625%3DJiahua%2520Xu%2520and%2520Dawei%2520Zhou%2520and%2520Lei%2520Hu%2520and%2520Jianfeng%2520Guo%2520and%2520Feng%2520Yang%2520and%2520Zaiyi%2520Liu%2520and%2520Nannan%2520Wang%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Motion%2520artifacts%2520present%2520in%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520can%2520seriously%250Ainterfere%2520with%2520clinical%2520diagnosis.%2520Removing%2520motion%2520artifacts%2520is%2520a%250Astraightforward%2520solution%2520and%2520has%2520been%2520extensively%2520studied.%2520However%252C%2520paired%2520data%250Aare%2520still%2520heavily%2520relied%2520on%2520in%2520recent%2520works%2520and%2520the%2520perturbations%2520in%2520k-space%250A%2528frequency%2520domain%2529%2520are%2520not%2520well%2520considered%252C%2520which%2520limits%2520their%2520applications%2520in%250Athe%2520clinical%2520field.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520unsupervised%250Apurification%2520method%2520which%2520leverages%2520pixel-frequency%2520information%2520of%2520noisy%2520MRI%250Aimages%2520to%2520guide%2520a%2520pre-trained%2520diffusion%2520model%2520to%2520recover%2520clean%2520MRI%2520images.%250ASpecifically%252C%2520considering%2520that%2520motion%2520artifacts%2520are%2520mainly%2520concentrated%2520in%250Ahigh-frequency%2520components%2520in%2520k-space%252C%2520we%2520utilize%2520the%2520low-frequency%2520components%250Aas%2520the%2520guide%2520to%2520ensure%2520correct%2520tissue%2520textures.%2520Additionally%252C%2520given%2520that%250Ahigh-frequency%2520and%2520pixel%2520information%2520are%2520helpful%2520for%2520recovering%2520shape%2520and%250Adetail%2520textures%252C%2520we%2520design%2520alternate%2520complementary%2520masks%2520to%2520simultaneously%250Adestroy%2520the%2520artifact%2520structure%2520and%2520exploit%2520useful%2520information.%2520Quantitative%250Aexperiments%2520are%2520performed%2520on%2520datasets%2520from%2520different%2520tissues%2520and%2520show%2520that%2520our%250Amethod%2520achieves%2520superior%2520performance%2520on%2520several%2520metrics.%2520Qualitative%250Aevaluations%2520with%2520radiologists%2520also%2520show%2520that%2520our%2520method%2520provides%2520better%250Aclinical%2520feedback.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/medcx/PFAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07590v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20Artifact%20Removal%20in%20Pixel-Frequency%20Domain%20via%20Alternate%20Masks%0A%20%20and%20Diffusion%20Model&entry.906535625=Jiahua%20Xu%20and%20Dawei%20Zhou%20and%20Lei%20Hu%20and%20Jianfeng%20Guo%20and%20Feng%20Yang%20and%20Zaiyi%20Liu%20and%20Nannan%20Wang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Motion%20artifacts%20present%20in%20magnetic%20resonance%20imaging%20%28MRI%29%20can%20seriously%0Ainterfere%20with%20clinical%20diagnosis.%20Removing%20motion%20artifacts%20is%20a%0Astraightforward%20solution%20and%20has%20been%20extensively%20studied.%20However%2C%20paired%20data%0Aare%20still%20heavily%20relied%20on%20in%20recent%20works%20and%20the%20perturbations%20in%20k-space%0A%28frequency%20domain%29%20are%20not%20well%20considered%2C%20which%20limits%20their%20applications%20in%0Athe%20clinical%20field.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20unsupervised%0Apurification%20method%20which%20leverages%20pixel-frequency%20information%20of%20noisy%20MRI%0Aimages%20to%20guide%20a%20pre-trained%20diffusion%20model%20to%20recover%20clean%20MRI%20images.%0ASpecifically%2C%20considering%20that%20motion%20artifacts%20are%20mainly%20concentrated%20in%0Ahigh-frequency%20components%20in%20k-space%2C%20we%20utilize%20the%20low-frequency%20components%0Aas%20the%20guide%20to%20ensure%20correct%20tissue%20textures.%20Additionally%2C%20given%20that%0Ahigh-frequency%20and%20pixel%20information%20are%20helpful%20for%20recovering%20shape%20and%0Adetail%20textures%2C%20we%20design%20alternate%20complementary%20masks%20to%20simultaneously%0Adestroy%20the%20artifact%20structure%20and%20exploit%20useful%20information.%20Quantitative%0Aexperiments%20are%20performed%20on%20datasets%20from%20different%20tissues%20and%20show%20that%20our%0Amethod%20achieves%20superior%20performance%20on%20several%20metrics.%20Qualitative%0Aevaluations%20with%20radiologists%20also%20show%20that%20our%20method%20provides%20better%0Aclinical%20feedback.%20Our%20code%20is%20available%20at%20https%3A//github.com/medcx/PFAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07590v2&entry.124074799=Read"},
{"title": "REPEAT: Improving Uncertainty Estimation in Representation Learning\n  Explainability", "author": "Kristoffer K. Wickstr\u00f8m and Thea Br\u00fcsch and Michael C. Kampffmeyer and Robert Jenssen", "abstract": "  Incorporating uncertainty is crucial to provide trustworthy explanations of\ndeep learning models. Recent works have demonstrated how uncertainty modeling\ncan be particularly important in the unsupervised field of representation\nlearning explainable artificial intelligence (R-XAI). Current R-XAI methods\nprovide uncertainty by measuring variability in the importance score. However,\nthey fail to provide meaningful estimates of whether a pixel is certainly\nimportant or not. In this work, we propose a new R-XAI method called REPEAT\nthat addresses the key question of whether or not a pixel is \\textit{certainly}\nimportant. REPEAT leverages the stochasticity of current R-XAI methods to\nproduce multiple estimates of importance, thus considering each pixel in an\nimage as a Bernoulli random variable that is either important or unimportant.\nFrom these Bernoulli random variables we can directly estimate the importance\nof a pixel and its associated certainty, thus enabling users to determine\ncertainty in pixel importance. Our extensive evaluation shows that REPEAT gives\ncertainty estimates that are more intuitive, better at detecting\nout-of-distribution data, and more concise.\n", "link": "http://arxiv.org/abs/2412.08513v1", "date": "2024-12-11", "relevancy": 2.1616, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5621}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5463}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REPEAT%3A%20Improving%20Uncertainty%20Estimation%20in%20Representation%20Learning%0A%20%20Explainability&body=Title%3A%20REPEAT%3A%20Improving%20Uncertainty%20Estimation%20in%20Representation%20Learning%0A%20%20Explainability%0AAuthor%3A%20Kristoffer%20K.%20Wickstr%C3%B8m%20and%20Thea%20Br%C3%BCsch%20and%20Michael%20C.%20Kampffmeyer%20and%20Robert%20Jenssen%0AAbstract%3A%20%20%20Incorporating%20uncertainty%20is%20crucial%20to%20provide%20trustworthy%20explanations%20of%0Adeep%20learning%20models.%20Recent%20works%20have%20demonstrated%20how%20uncertainty%20modeling%0Acan%20be%20particularly%20important%20in%20the%20unsupervised%20field%20of%20representation%0Alearning%20explainable%20artificial%20intelligence%20%28R-XAI%29.%20Current%20R-XAI%20methods%0Aprovide%20uncertainty%20by%20measuring%20variability%20in%20the%20importance%20score.%20However%2C%0Athey%20fail%20to%20provide%20meaningful%20estimates%20of%20whether%20a%20pixel%20is%20certainly%0Aimportant%20or%20not.%20In%20this%20work%2C%20we%20propose%20a%20new%20R-XAI%20method%20called%20REPEAT%0Athat%20addresses%20the%20key%20question%20of%20whether%20or%20not%20a%20pixel%20is%20%5Ctextit%7Bcertainly%7D%0Aimportant.%20REPEAT%20leverages%20the%20stochasticity%20of%20current%20R-XAI%20methods%20to%0Aproduce%20multiple%20estimates%20of%20importance%2C%20thus%20considering%20each%20pixel%20in%20an%0Aimage%20as%20a%20Bernoulli%20random%20variable%20that%20is%20either%20important%20or%20unimportant.%0AFrom%20these%20Bernoulli%20random%20variables%20we%20can%20directly%20estimate%20the%20importance%0Aof%20a%20pixel%20and%20its%20associated%20certainty%2C%20thus%20enabling%20users%20to%20determine%0Acertainty%20in%20pixel%20importance.%20Our%20extensive%20evaluation%20shows%20that%20REPEAT%20gives%0Acertainty%20estimates%20that%20are%20more%20intuitive%2C%20better%20at%20detecting%0Aout-of-distribution%20data%2C%20and%20more%20concise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREPEAT%253A%2520Improving%2520Uncertainty%2520Estimation%2520in%2520Representation%2520Learning%250A%2520%2520Explainability%26entry.906535625%3DKristoffer%2520K.%2520Wickstr%25C3%25B8m%2520and%2520Thea%2520Br%25C3%25BCsch%2520and%2520Michael%2520C.%2520Kampffmeyer%2520and%2520Robert%2520Jenssen%26entry.1292438233%3D%2520%2520Incorporating%2520uncertainty%2520is%2520crucial%2520to%2520provide%2520trustworthy%2520explanations%2520of%250Adeep%2520learning%2520models.%2520Recent%2520works%2520have%2520demonstrated%2520how%2520uncertainty%2520modeling%250Acan%2520be%2520particularly%2520important%2520in%2520the%2520unsupervised%2520field%2520of%2520representation%250Alearning%2520explainable%2520artificial%2520intelligence%2520%2528R-XAI%2529.%2520Current%2520R-XAI%2520methods%250Aprovide%2520uncertainty%2520by%2520measuring%2520variability%2520in%2520the%2520importance%2520score.%2520However%252C%250Athey%2520fail%2520to%2520provide%2520meaningful%2520estimates%2520of%2520whether%2520a%2520pixel%2520is%2520certainly%250Aimportant%2520or%2520not.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520R-XAI%2520method%2520called%2520REPEAT%250Athat%2520addresses%2520the%2520key%2520question%2520of%2520whether%2520or%2520not%2520a%2520pixel%2520is%2520%255Ctextit%257Bcertainly%257D%250Aimportant.%2520REPEAT%2520leverages%2520the%2520stochasticity%2520of%2520current%2520R-XAI%2520methods%2520to%250Aproduce%2520multiple%2520estimates%2520of%2520importance%252C%2520thus%2520considering%2520each%2520pixel%2520in%2520an%250Aimage%2520as%2520a%2520Bernoulli%2520random%2520variable%2520that%2520is%2520either%2520important%2520or%2520unimportant.%250AFrom%2520these%2520Bernoulli%2520random%2520variables%2520we%2520can%2520directly%2520estimate%2520the%2520importance%250Aof%2520a%2520pixel%2520and%2520its%2520associated%2520certainty%252C%2520thus%2520enabling%2520users%2520to%2520determine%250Acertainty%2520in%2520pixel%2520importance.%2520Our%2520extensive%2520evaluation%2520shows%2520that%2520REPEAT%2520gives%250Acertainty%2520estimates%2520that%2520are%2520more%2520intuitive%252C%2520better%2520at%2520detecting%250Aout-of-distribution%2520data%252C%2520and%2520more%2520concise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REPEAT%3A%20Improving%20Uncertainty%20Estimation%20in%20Representation%20Learning%0A%20%20Explainability&entry.906535625=Kristoffer%20K.%20Wickstr%C3%B8m%20and%20Thea%20Br%C3%BCsch%20and%20Michael%20C.%20Kampffmeyer%20and%20Robert%20Jenssen&entry.1292438233=%20%20Incorporating%20uncertainty%20is%20crucial%20to%20provide%20trustworthy%20explanations%20of%0Adeep%20learning%20models.%20Recent%20works%20have%20demonstrated%20how%20uncertainty%20modeling%0Acan%20be%20particularly%20important%20in%20the%20unsupervised%20field%20of%20representation%0Alearning%20explainable%20artificial%20intelligence%20%28R-XAI%29.%20Current%20R-XAI%20methods%0Aprovide%20uncertainty%20by%20measuring%20variability%20in%20the%20importance%20score.%20However%2C%0Athey%20fail%20to%20provide%20meaningful%20estimates%20of%20whether%20a%20pixel%20is%20certainly%0Aimportant%20or%20not.%20In%20this%20work%2C%20we%20propose%20a%20new%20R-XAI%20method%20called%20REPEAT%0Athat%20addresses%20the%20key%20question%20of%20whether%20or%20not%20a%20pixel%20is%20%5Ctextit%7Bcertainly%7D%0Aimportant.%20REPEAT%20leverages%20the%20stochasticity%20of%20current%20R-XAI%20methods%20to%0Aproduce%20multiple%20estimates%20of%20importance%2C%20thus%20considering%20each%20pixel%20in%20an%0Aimage%20as%20a%20Bernoulli%20random%20variable%20that%20is%20either%20important%20or%20unimportant.%0AFrom%20these%20Bernoulli%20random%20variables%20we%20can%20directly%20estimate%20the%20importance%0Aof%20a%20pixel%20and%20its%20associated%20certainty%2C%20thus%20enabling%20users%20to%20determine%0Acertainty%20in%20pixel%20importance.%20Our%20extensive%20evaluation%20shows%20that%20REPEAT%20gives%0Acertainty%20estimates%20that%20are%20more%20intuitive%2C%20better%20at%20detecting%0Aout-of-distribution%20data%2C%20and%20more%20concise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08513v1&entry.124074799=Read"},
{"title": "CoDTS: Enhancing Sparsely Supervised Collaborative Perception with a\n  Dual Teacher-Student Framework", "author": "Yushan Han and Hui Zhang and Honglei Zhang and Jing Wang and Yidong Li", "abstract": "  Current collaborative perception methods often rely on fully annotated\ndatasets, which can be expensive to obtain in practical situations. To reduce\nannotation costs, some works adopt sparsely supervised learning techniques and\ngenerate pseudo labels for the missing instances. However, these methods fail\nto achieve an optimal confidence threshold that harmonizes the quality and\nquantity of pseudo labels. To address this issue, we propose an end-to-end\nCollaborative perception Dual Teacher-Student framework (CoDTS), which employs\nadaptive complementary learning to produce both high-quality and high-quantity\npseudo labels. Specifically, the Main Foreground Mining (MFM) module generates\nhigh-quality pseudo labels based on the prediction of the static teacher.\nSubsequently, the Supplement Foreground Mining (SFM) module ensures a balance\nbetween the quality and quantity of pseudo labels by adaptively identifying\nmissing instances based on the prediction of the dynamic teacher. Additionally,\nthe Neighbor Anchor Sampling (NAS) module is incorporated to enhance the\nrepresentation of pseudo labels. To promote the adaptive complementary\nlearning, we implement a staged training strategy that trains the student and\ndynamic teacher in a mutually beneficial manner. Extensive experiments\ndemonstrate that the CoDTS effectively ensures an optimal balance of pseudo\nlabels in both quality and quantity, establishing a new state-of-the-art in\nsparsely supervised collaborative perception.\n", "link": "http://arxiv.org/abs/2412.08344v1", "date": "2024-12-11", "relevancy": 2.1598, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5435}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoDTS%3A%20Enhancing%20Sparsely%20Supervised%20Collaborative%20Perception%20with%20a%0A%20%20Dual%20Teacher-Student%20Framework&body=Title%3A%20CoDTS%3A%20Enhancing%20Sparsely%20Supervised%20Collaborative%20Perception%20with%20a%0A%20%20Dual%20Teacher-Student%20Framework%0AAuthor%3A%20Yushan%20Han%20and%20Hui%20Zhang%20and%20Honglei%20Zhang%20and%20Jing%20Wang%20and%20Yidong%20Li%0AAbstract%3A%20%20%20Current%20collaborative%20perception%20methods%20often%20rely%20on%20fully%20annotated%0Adatasets%2C%20which%20can%20be%20expensive%20to%20obtain%20in%20practical%20situations.%20To%20reduce%0Aannotation%20costs%2C%20some%20works%20adopt%20sparsely%20supervised%20learning%20techniques%20and%0Agenerate%20pseudo%20labels%20for%20the%20missing%20instances.%20However%2C%20these%20methods%20fail%0Ato%20achieve%20an%20optimal%20confidence%20threshold%20that%20harmonizes%20the%20quality%20and%0Aquantity%20of%20pseudo%20labels.%20To%20address%20this%20issue%2C%20we%20propose%20an%20end-to-end%0ACollaborative%20perception%20Dual%20Teacher-Student%20framework%20%28CoDTS%29%2C%20which%20employs%0Aadaptive%20complementary%20learning%20to%20produce%20both%20high-quality%20and%20high-quantity%0Apseudo%20labels.%20Specifically%2C%20the%20Main%20Foreground%20Mining%20%28MFM%29%20module%20generates%0Ahigh-quality%20pseudo%20labels%20based%20on%20the%20prediction%20of%20the%20static%20teacher.%0ASubsequently%2C%20the%20Supplement%20Foreground%20Mining%20%28SFM%29%20module%20ensures%20a%20balance%0Abetween%20the%20quality%20and%20quantity%20of%20pseudo%20labels%20by%20adaptively%20identifying%0Amissing%20instances%20based%20on%20the%20prediction%20of%20the%20dynamic%20teacher.%20Additionally%2C%0Athe%20Neighbor%20Anchor%20Sampling%20%28NAS%29%20module%20is%20incorporated%20to%20enhance%20the%0Arepresentation%20of%20pseudo%20labels.%20To%20promote%20the%20adaptive%20complementary%0Alearning%2C%20we%20implement%20a%20staged%20training%20strategy%20that%20trains%20the%20student%20and%0Adynamic%20teacher%20in%20a%20mutually%20beneficial%20manner.%20Extensive%20experiments%0Ademonstrate%20that%20the%20CoDTS%20effectively%20ensures%20an%20optimal%20balance%20of%20pseudo%0Alabels%20in%20both%20quality%20and%20quantity%2C%20establishing%20a%20new%20state-of-the-art%20in%0Asparsely%20supervised%20collaborative%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08344v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoDTS%253A%2520Enhancing%2520Sparsely%2520Supervised%2520Collaborative%2520Perception%2520with%2520a%250A%2520%2520Dual%2520Teacher-Student%2520Framework%26entry.906535625%3DYushan%2520Han%2520and%2520Hui%2520Zhang%2520and%2520Honglei%2520Zhang%2520and%2520Jing%2520Wang%2520and%2520Yidong%2520Li%26entry.1292438233%3D%2520%2520Current%2520collaborative%2520perception%2520methods%2520often%2520rely%2520on%2520fully%2520annotated%250Adatasets%252C%2520which%2520can%2520be%2520expensive%2520to%2520obtain%2520in%2520practical%2520situations.%2520To%2520reduce%250Aannotation%2520costs%252C%2520some%2520works%2520adopt%2520sparsely%2520supervised%2520learning%2520techniques%2520and%250Agenerate%2520pseudo%2520labels%2520for%2520the%2520missing%2520instances.%2520However%252C%2520these%2520methods%2520fail%250Ato%2520achieve%2520an%2520optimal%2520confidence%2520threshold%2520that%2520harmonizes%2520the%2520quality%2520and%250Aquantity%2520of%2520pseudo%2520labels.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520end-to-end%250ACollaborative%2520perception%2520Dual%2520Teacher-Student%2520framework%2520%2528CoDTS%2529%252C%2520which%2520employs%250Aadaptive%2520complementary%2520learning%2520to%2520produce%2520both%2520high-quality%2520and%2520high-quantity%250Apseudo%2520labels.%2520Specifically%252C%2520the%2520Main%2520Foreground%2520Mining%2520%2528MFM%2529%2520module%2520generates%250Ahigh-quality%2520pseudo%2520labels%2520based%2520on%2520the%2520prediction%2520of%2520the%2520static%2520teacher.%250ASubsequently%252C%2520the%2520Supplement%2520Foreground%2520Mining%2520%2528SFM%2529%2520module%2520ensures%2520a%2520balance%250Abetween%2520the%2520quality%2520and%2520quantity%2520of%2520pseudo%2520labels%2520by%2520adaptively%2520identifying%250Amissing%2520instances%2520based%2520on%2520the%2520prediction%2520of%2520the%2520dynamic%2520teacher.%2520Additionally%252C%250Athe%2520Neighbor%2520Anchor%2520Sampling%2520%2528NAS%2529%2520module%2520is%2520incorporated%2520to%2520enhance%2520the%250Arepresentation%2520of%2520pseudo%2520labels.%2520To%2520promote%2520the%2520adaptive%2520complementary%250Alearning%252C%2520we%2520implement%2520a%2520staged%2520training%2520strategy%2520that%2520trains%2520the%2520student%2520and%250Adynamic%2520teacher%2520in%2520a%2520mutually%2520beneficial%2520manner.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520the%2520CoDTS%2520effectively%2520ensures%2520an%2520optimal%2520balance%2520of%2520pseudo%250Alabels%2520in%2520both%2520quality%2520and%2520quantity%252C%2520establishing%2520a%2520new%2520state-of-the-art%2520in%250Asparsely%2520supervised%2520collaborative%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08344v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoDTS%3A%20Enhancing%20Sparsely%20Supervised%20Collaborative%20Perception%20with%20a%0A%20%20Dual%20Teacher-Student%20Framework&entry.906535625=Yushan%20Han%20and%20Hui%20Zhang%20and%20Honglei%20Zhang%20and%20Jing%20Wang%20and%20Yidong%20Li&entry.1292438233=%20%20Current%20collaborative%20perception%20methods%20often%20rely%20on%20fully%20annotated%0Adatasets%2C%20which%20can%20be%20expensive%20to%20obtain%20in%20practical%20situations.%20To%20reduce%0Aannotation%20costs%2C%20some%20works%20adopt%20sparsely%20supervised%20learning%20techniques%20and%0Agenerate%20pseudo%20labels%20for%20the%20missing%20instances.%20However%2C%20these%20methods%20fail%0Ato%20achieve%20an%20optimal%20confidence%20threshold%20that%20harmonizes%20the%20quality%20and%0Aquantity%20of%20pseudo%20labels.%20To%20address%20this%20issue%2C%20we%20propose%20an%20end-to-end%0ACollaborative%20perception%20Dual%20Teacher-Student%20framework%20%28CoDTS%29%2C%20which%20employs%0Aadaptive%20complementary%20learning%20to%20produce%20both%20high-quality%20and%20high-quantity%0Apseudo%20labels.%20Specifically%2C%20the%20Main%20Foreground%20Mining%20%28MFM%29%20module%20generates%0Ahigh-quality%20pseudo%20labels%20based%20on%20the%20prediction%20of%20the%20static%20teacher.%0ASubsequently%2C%20the%20Supplement%20Foreground%20Mining%20%28SFM%29%20module%20ensures%20a%20balance%0Abetween%20the%20quality%20and%20quantity%20of%20pseudo%20labels%20by%20adaptively%20identifying%0Amissing%20instances%20based%20on%20the%20prediction%20of%20the%20dynamic%20teacher.%20Additionally%2C%0Athe%20Neighbor%20Anchor%20Sampling%20%28NAS%29%20module%20is%20incorporated%20to%20enhance%20the%0Arepresentation%20of%20pseudo%20labels.%20To%20promote%20the%20adaptive%20complementary%0Alearning%2C%20we%20implement%20a%20staged%20training%20strategy%20that%20trains%20the%20student%20and%0Adynamic%20teacher%20in%20a%20mutually%20beneficial%20manner.%20Extensive%20experiments%0Ademonstrate%20that%20the%20CoDTS%20effectively%20ensures%20an%20optimal%20balance%20of%20pseudo%0Alabels%20in%20both%20quality%20and%20quantity%2C%20establishing%20a%20new%20state-of-the-art%20in%0Asparsely%20supervised%20collaborative%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08344v1&entry.124074799=Read"},
{"title": "Benchmarking Autoregressive Conditional Diffusion Models for Turbulent\n  Flow Simulation", "author": "Georg Kohl and Li-Wei Chen and Nils Thuerey", "abstract": "  Simulating turbulent flows is crucial for a wide range of applications, and\nmachine learning-based solvers are gaining increasing relevance. However,\nachieving temporal stability when generalizing to longer rollout horizons\nremains a persistent challenge for learned PDE solvers. In this work, we\nanalyze if fully data-driven fluid solvers that utilize an autoregressive\nrollout based on conditional diffusion models are a viable option to address\nthis challenge. We investigate accuracy, posterior sampling, spectral behavior,\nand temporal stability, while requiring that methods generalize to flow\nparameters beyond the training regime. To quantitatively and qualitatively\nbenchmark the performance of various flow prediction approaches, three\nchallenging 2D scenarios including incompressible and transonic flows, as well\nas isotropic turbulence are employed. We find that even simple diffusion-based\napproaches can outperform multiple established flow prediction methods in terms\nof accuracy and temporal stability, while being on par with state-of-the-art\nstabilization techniques like unrolling at training time. Such traditional\narchitectures are superior in terms of inference speed, however, the\nprobabilistic nature of diffusion approaches allows for inferring multiple\npredictions that align with the statistics of the underlying physics. Overall,\nour benchmark contains three carefully chosen data sets that are suitable for\nprobabilistic evaluation alongside various established flow prediction\narchitectures.\n", "link": "http://arxiv.org/abs/2309.01745v3", "date": "2024-12-11", "relevancy": 1.1468, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6383}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5664}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Autoregressive%20Conditional%20Diffusion%20Models%20for%20Turbulent%0A%20%20Flow%20Simulation&body=Title%3A%20Benchmarking%20Autoregressive%20Conditional%20Diffusion%20Models%20for%20Turbulent%0A%20%20Flow%20Simulation%0AAuthor%3A%20Georg%20Kohl%20and%20Li-Wei%20Chen%20and%20Nils%20Thuerey%0AAbstract%3A%20%20%20Simulating%20turbulent%20flows%20is%20crucial%20for%20a%20wide%20range%20of%20applications%2C%20and%0Amachine%20learning-based%20solvers%20are%20gaining%20increasing%20relevance.%20However%2C%0Aachieving%20temporal%20stability%20when%20generalizing%20to%20longer%20rollout%20horizons%0Aremains%20a%20persistent%20challenge%20for%20learned%20PDE%20solvers.%20In%20this%20work%2C%20we%0Aanalyze%20if%20fully%20data-driven%20fluid%20solvers%20that%20utilize%20an%20autoregressive%0Arollout%20based%20on%20conditional%20diffusion%20models%20are%20a%20viable%20option%20to%20address%0Athis%20challenge.%20We%20investigate%20accuracy%2C%20posterior%20sampling%2C%20spectral%20behavior%2C%0Aand%20temporal%20stability%2C%20while%20requiring%20that%20methods%20generalize%20to%20flow%0Aparameters%20beyond%20the%20training%20regime.%20To%20quantitatively%20and%20qualitatively%0Abenchmark%20the%20performance%20of%20various%20flow%20prediction%20approaches%2C%20three%0Achallenging%202D%20scenarios%20including%20incompressible%20and%20transonic%20flows%2C%20as%20well%0Aas%20isotropic%20turbulence%20are%20employed.%20We%20find%20that%20even%20simple%20diffusion-based%0Aapproaches%20can%20outperform%20multiple%20established%20flow%20prediction%20methods%20in%20terms%0Aof%20accuracy%20and%20temporal%20stability%2C%20while%20being%20on%20par%20with%20state-of-the-art%0Astabilization%20techniques%20like%20unrolling%20at%20training%20time.%20Such%20traditional%0Aarchitectures%20are%20superior%20in%20terms%20of%20inference%20speed%2C%20however%2C%20the%0Aprobabilistic%20nature%20of%20diffusion%20approaches%20allows%20for%20inferring%20multiple%0Apredictions%20that%20align%20with%20the%20statistics%20of%20the%20underlying%20physics.%20Overall%2C%0Aour%20benchmark%20contains%20three%20carefully%20chosen%20data%20sets%20that%20are%20suitable%20for%0Aprobabilistic%20evaluation%20alongside%20various%20established%20flow%20prediction%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.01745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Autoregressive%2520Conditional%2520Diffusion%2520Models%2520for%2520Turbulent%250A%2520%2520Flow%2520Simulation%26entry.906535625%3DGeorg%2520Kohl%2520and%2520Li-Wei%2520Chen%2520and%2520Nils%2520Thuerey%26entry.1292438233%3D%2520%2520Simulating%2520turbulent%2520flows%2520is%2520crucial%2520for%2520a%2520wide%2520range%2520of%2520applications%252C%2520and%250Amachine%2520learning-based%2520solvers%2520are%2520gaining%2520increasing%2520relevance.%2520However%252C%250Aachieving%2520temporal%2520stability%2520when%2520generalizing%2520to%2520longer%2520rollout%2520horizons%250Aremains%2520a%2520persistent%2520challenge%2520for%2520learned%2520PDE%2520solvers.%2520In%2520this%2520work%252C%2520we%250Aanalyze%2520if%2520fully%2520data-driven%2520fluid%2520solvers%2520that%2520utilize%2520an%2520autoregressive%250Arollout%2520based%2520on%2520conditional%2520diffusion%2520models%2520are%2520a%2520viable%2520option%2520to%2520address%250Athis%2520challenge.%2520We%2520investigate%2520accuracy%252C%2520posterior%2520sampling%252C%2520spectral%2520behavior%252C%250Aand%2520temporal%2520stability%252C%2520while%2520requiring%2520that%2520methods%2520generalize%2520to%2520flow%250Aparameters%2520beyond%2520the%2520training%2520regime.%2520To%2520quantitatively%2520and%2520qualitatively%250Abenchmark%2520the%2520performance%2520of%2520various%2520flow%2520prediction%2520approaches%252C%2520three%250Achallenging%25202D%2520scenarios%2520including%2520incompressible%2520and%2520transonic%2520flows%252C%2520as%2520well%250Aas%2520isotropic%2520turbulence%2520are%2520employed.%2520We%2520find%2520that%2520even%2520simple%2520diffusion-based%250Aapproaches%2520can%2520outperform%2520multiple%2520established%2520flow%2520prediction%2520methods%2520in%2520terms%250Aof%2520accuracy%2520and%2520temporal%2520stability%252C%2520while%2520being%2520on%2520par%2520with%2520state-of-the-art%250Astabilization%2520techniques%2520like%2520unrolling%2520at%2520training%2520time.%2520Such%2520traditional%250Aarchitectures%2520are%2520superior%2520in%2520terms%2520of%2520inference%2520speed%252C%2520however%252C%2520the%250Aprobabilistic%2520nature%2520of%2520diffusion%2520approaches%2520allows%2520for%2520inferring%2520multiple%250Apredictions%2520that%2520align%2520with%2520the%2520statistics%2520of%2520the%2520underlying%2520physics.%2520Overall%252C%250Aour%2520benchmark%2520contains%2520three%2520carefully%2520chosen%2520data%2520sets%2520that%2520are%2520suitable%2520for%250Aprobabilistic%2520evaluation%2520alongside%2520various%2520established%2520flow%2520prediction%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.01745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Autoregressive%20Conditional%20Diffusion%20Models%20for%20Turbulent%0A%20%20Flow%20Simulation&entry.906535625=Georg%20Kohl%20and%20Li-Wei%20Chen%20and%20Nils%20Thuerey&entry.1292438233=%20%20Simulating%20turbulent%20flows%20is%20crucial%20for%20a%20wide%20range%20of%20applications%2C%20and%0Amachine%20learning-based%20solvers%20are%20gaining%20increasing%20relevance.%20However%2C%0Aachieving%20temporal%20stability%20when%20generalizing%20to%20longer%20rollout%20horizons%0Aremains%20a%20persistent%20challenge%20for%20learned%20PDE%20solvers.%20In%20this%20work%2C%20we%0Aanalyze%20if%20fully%20data-driven%20fluid%20solvers%20that%20utilize%20an%20autoregressive%0Arollout%20based%20on%20conditional%20diffusion%20models%20are%20a%20viable%20option%20to%20address%0Athis%20challenge.%20We%20investigate%20accuracy%2C%20posterior%20sampling%2C%20spectral%20behavior%2C%0Aand%20temporal%20stability%2C%20while%20requiring%20that%20methods%20generalize%20to%20flow%0Aparameters%20beyond%20the%20training%20regime.%20To%20quantitatively%20and%20qualitatively%0Abenchmark%20the%20performance%20of%20various%20flow%20prediction%20approaches%2C%20three%0Achallenging%202D%20scenarios%20including%20incompressible%20and%20transonic%20flows%2C%20as%20well%0Aas%20isotropic%20turbulence%20are%20employed.%20We%20find%20that%20even%20simple%20diffusion-based%0Aapproaches%20can%20outperform%20multiple%20established%20flow%20prediction%20methods%20in%20terms%0Aof%20accuracy%20and%20temporal%20stability%2C%20while%20being%20on%20par%20with%20state-of-the-art%0Astabilization%20techniques%20like%20unrolling%20at%20training%20time.%20Such%20traditional%0Aarchitectures%20are%20superior%20in%20terms%20of%20inference%20speed%2C%20however%2C%20the%0Aprobabilistic%20nature%20of%20diffusion%20approaches%20allows%20for%20inferring%20multiple%0Apredictions%20that%20align%20with%20the%20statistics%20of%20the%20underlying%20physics.%20Overall%2C%0Aour%20benchmark%20contains%20three%20carefully%20chosen%20data%20sets%20that%20are%20suitable%20for%0Aprobabilistic%20evaluation%20alongside%20various%20established%20flow%20prediction%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.01745v3&entry.124074799=Read"},
{"title": "InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models", "author": "Min Hou and Yueying Wu and Chang Xu and Yu-Hao Huang and Chenxi Bai and Le Wu and Jiang Bian", "abstract": "  As one of the most successful generative models, diffusion models have\ndemonstrated remarkable efficacy in synthesizing high-quality images. These\nmodels learn the underlying high-dimensional data distribution in an\nunsupervised manner. Despite their success, diffusion models are highly\ndata-driven and prone to inheriting the imbalances and biases present in\nreal-world data. Some studies have attempted to address these issues by\ndesigning text prompts for known biases or using bias labels to construct\nunbiased data. While these methods have shown improved results, real-world\nscenarios often contain various unknown biases, and obtaining bias labels is\nparticularly challenging. In this paper, we emphasize the necessity of\nmitigating bias in pre-trained diffusion models without relying on auxiliary\nbias annotations. To tackle this problem, we propose a framework, InvDiff,\nwhich aims to learn invariant semantic information for diffusion guidance.\nSpecifically, we propose identifying underlying biases in the training data and\ndesigning a novel debiasing training objective. Then, we employ a lightweight\ntrainable module that automatically preserves invariant semantic information\nand uses it to guide the diffusion model's sampling process toward unbiased\noutcomes simultaneously. Notably, we only need to learn a small number of\nparameters in the lightweight learnable module without altering the pre-trained\ndiffusion model. Furthermore, we provide a theoretical guarantee that the\nimplementation of InvDiff is equivalent to reducing the error upper bound of\ngeneralization. Extensive experimental results on three publicly available\nbenchmarks demonstrate that InvDiff effectively reduces biases while\nmaintaining the quality of image generation. Our code is available at\nhttps://github.com/Hundredl/InvDiff.\n", "link": "http://arxiv.org/abs/2412.08480v1", "date": "2024-12-11", "relevancy": 1.8159, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6219}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.601}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InvDiff%3A%20Invariant%20Guidance%20for%20Bias%20Mitigation%20in%20Diffusion%20Models&body=Title%3A%20InvDiff%3A%20Invariant%20Guidance%20for%20Bias%20Mitigation%20in%20Diffusion%20Models%0AAuthor%3A%20Min%20Hou%20and%20Yueying%20Wu%20and%20Chang%20Xu%20and%20Yu-Hao%20Huang%20and%20Chenxi%20Bai%20and%20Le%20Wu%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20As%20one%20of%20the%20most%20successful%20generative%20models%2C%20diffusion%20models%20have%0Ademonstrated%20remarkable%20efficacy%20in%20synthesizing%20high-quality%20images.%20These%0Amodels%20learn%20the%20underlying%20high-dimensional%20data%20distribution%20in%20an%0Aunsupervised%20manner.%20Despite%20their%20success%2C%20diffusion%20models%20are%20highly%0Adata-driven%20and%20prone%20to%20inheriting%20the%20imbalances%20and%20biases%20present%20in%0Areal-world%20data.%20Some%20studies%20have%20attempted%20to%20address%20these%20issues%20by%0Adesigning%20text%20prompts%20for%20known%20biases%20or%20using%20bias%20labels%20to%20construct%0Aunbiased%20data.%20While%20these%20methods%20have%20shown%20improved%20results%2C%20real-world%0Ascenarios%20often%20contain%20various%20unknown%20biases%2C%20and%20obtaining%20bias%20labels%20is%0Aparticularly%20challenging.%20In%20this%20paper%2C%20we%20emphasize%20the%20necessity%20of%0Amitigating%20bias%20in%20pre-trained%20diffusion%20models%20without%20relying%20on%20auxiliary%0Abias%20annotations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20framework%2C%20InvDiff%2C%0Awhich%20aims%20to%20learn%20invariant%20semantic%20information%20for%20diffusion%20guidance.%0ASpecifically%2C%20we%20propose%20identifying%20underlying%20biases%20in%20the%20training%20data%20and%0Adesigning%20a%20novel%20debiasing%20training%20objective.%20Then%2C%20we%20employ%20a%20lightweight%0Atrainable%20module%20that%20automatically%20preserves%20invariant%20semantic%20information%0Aand%20uses%20it%20to%20guide%20the%20diffusion%20model%27s%20sampling%20process%20toward%20unbiased%0Aoutcomes%20simultaneously.%20Notably%2C%20we%20only%20need%20to%20learn%20a%20small%20number%20of%0Aparameters%20in%20the%20lightweight%20learnable%20module%20without%20altering%20the%20pre-trained%0Adiffusion%20model.%20Furthermore%2C%20we%20provide%20a%20theoretical%20guarantee%20that%20the%0Aimplementation%20of%20InvDiff%20is%20equivalent%20to%20reducing%20the%20error%20upper%20bound%20of%0Ageneralization.%20Extensive%20experimental%20results%20on%20three%20publicly%20available%0Abenchmarks%20demonstrate%20that%20InvDiff%20effectively%20reduces%20biases%20while%0Amaintaining%20the%20quality%20of%20image%20generation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Hundredl/InvDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvDiff%253A%2520Invariant%2520Guidance%2520for%2520Bias%2520Mitigation%2520in%2520Diffusion%2520Models%26entry.906535625%3DMin%2520Hou%2520and%2520Yueying%2520Wu%2520and%2520Chang%2520Xu%2520and%2520Yu-Hao%2520Huang%2520and%2520Chenxi%2520Bai%2520and%2520Le%2520Wu%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520As%2520one%2520of%2520the%2520most%2520successful%2520generative%2520models%252C%2520diffusion%2520models%2520have%250Ademonstrated%2520remarkable%2520efficacy%2520in%2520synthesizing%2520high-quality%2520images.%2520These%250Amodels%2520learn%2520the%2520underlying%2520high-dimensional%2520data%2520distribution%2520in%2520an%250Aunsupervised%2520manner.%2520Despite%2520their%2520success%252C%2520diffusion%2520models%2520are%2520highly%250Adata-driven%2520and%2520prone%2520to%2520inheriting%2520the%2520imbalances%2520and%2520biases%2520present%2520in%250Areal-world%2520data.%2520Some%2520studies%2520have%2520attempted%2520to%2520address%2520these%2520issues%2520by%250Adesigning%2520text%2520prompts%2520for%2520known%2520biases%2520or%2520using%2520bias%2520labels%2520to%2520construct%250Aunbiased%2520data.%2520While%2520these%2520methods%2520have%2520shown%2520improved%2520results%252C%2520real-world%250Ascenarios%2520often%2520contain%2520various%2520unknown%2520biases%252C%2520and%2520obtaining%2520bias%2520labels%2520is%250Aparticularly%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520emphasize%2520the%2520necessity%2520of%250Amitigating%2520bias%2520in%2520pre-trained%2520diffusion%2520models%2520without%2520relying%2520on%2520auxiliary%250Abias%2520annotations.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520framework%252C%2520InvDiff%252C%250Awhich%2520aims%2520to%2520learn%2520invariant%2520semantic%2520information%2520for%2520diffusion%2520guidance.%250ASpecifically%252C%2520we%2520propose%2520identifying%2520underlying%2520biases%2520in%2520the%2520training%2520data%2520and%250Adesigning%2520a%2520novel%2520debiasing%2520training%2520objective.%2520Then%252C%2520we%2520employ%2520a%2520lightweight%250Atrainable%2520module%2520that%2520automatically%2520preserves%2520invariant%2520semantic%2520information%250Aand%2520uses%2520it%2520to%2520guide%2520the%2520diffusion%2520model%2527s%2520sampling%2520process%2520toward%2520unbiased%250Aoutcomes%2520simultaneously.%2520Notably%252C%2520we%2520only%2520need%2520to%2520learn%2520a%2520small%2520number%2520of%250Aparameters%2520in%2520the%2520lightweight%2520learnable%2520module%2520without%2520altering%2520the%2520pre-trained%250Adiffusion%2520model.%2520Furthermore%252C%2520we%2520provide%2520a%2520theoretical%2520guarantee%2520that%2520the%250Aimplementation%2520of%2520InvDiff%2520is%2520equivalent%2520to%2520reducing%2520the%2520error%2520upper%2520bound%2520of%250Ageneralization.%2520Extensive%2520experimental%2520results%2520on%2520three%2520publicly%2520available%250Abenchmarks%2520demonstrate%2520that%2520InvDiff%2520effectively%2520reduces%2520biases%2520while%250Amaintaining%2520the%2520quality%2520of%2520image%2520generation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Hundredl/InvDiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InvDiff%3A%20Invariant%20Guidance%20for%20Bias%20Mitigation%20in%20Diffusion%20Models&entry.906535625=Min%20Hou%20and%20Yueying%20Wu%20and%20Chang%20Xu%20and%20Yu-Hao%20Huang%20and%20Chenxi%20Bai%20and%20Le%20Wu%20and%20Jiang%20Bian&entry.1292438233=%20%20As%20one%20of%20the%20most%20successful%20generative%20models%2C%20diffusion%20models%20have%0Ademonstrated%20remarkable%20efficacy%20in%20synthesizing%20high-quality%20images.%20These%0Amodels%20learn%20the%20underlying%20high-dimensional%20data%20distribution%20in%20an%0Aunsupervised%20manner.%20Despite%20their%20success%2C%20diffusion%20models%20are%20highly%0Adata-driven%20and%20prone%20to%20inheriting%20the%20imbalances%20and%20biases%20present%20in%0Areal-world%20data.%20Some%20studies%20have%20attempted%20to%20address%20these%20issues%20by%0Adesigning%20text%20prompts%20for%20known%20biases%20or%20using%20bias%20labels%20to%20construct%0Aunbiased%20data.%20While%20these%20methods%20have%20shown%20improved%20results%2C%20real-world%0Ascenarios%20often%20contain%20various%20unknown%20biases%2C%20and%20obtaining%20bias%20labels%20is%0Aparticularly%20challenging.%20In%20this%20paper%2C%20we%20emphasize%20the%20necessity%20of%0Amitigating%20bias%20in%20pre-trained%20diffusion%20models%20without%20relying%20on%20auxiliary%0Abias%20annotations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20framework%2C%20InvDiff%2C%0Awhich%20aims%20to%20learn%20invariant%20semantic%20information%20for%20diffusion%20guidance.%0ASpecifically%2C%20we%20propose%20identifying%20underlying%20biases%20in%20the%20training%20data%20and%0Adesigning%20a%20novel%20debiasing%20training%20objective.%20Then%2C%20we%20employ%20a%20lightweight%0Atrainable%20module%20that%20automatically%20preserves%20invariant%20semantic%20information%0Aand%20uses%20it%20to%20guide%20the%20diffusion%20model%27s%20sampling%20process%20toward%20unbiased%0Aoutcomes%20simultaneously.%20Notably%2C%20we%20only%20need%20to%20learn%20a%20small%20number%20of%0Aparameters%20in%20the%20lightweight%20learnable%20module%20without%20altering%20the%20pre-trained%0Adiffusion%20model.%20Furthermore%2C%20we%20provide%20a%20theoretical%20guarantee%20that%20the%0Aimplementation%20of%20InvDiff%20is%20equivalent%20to%20reducing%20the%20error%20upper%20bound%20of%0Ageneralization.%20Extensive%20experimental%20results%20on%20three%20publicly%20available%0Abenchmarks%20demonstrate%20that%20InvDiff%20effectively%20reduces%20biases%20while%0Amaintaining%20the%20quality%20of%20image%20generation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Hundredl/InvDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08480v1&entry.124074799=Read"},
{"title": "SINERGYM -- A virtual testbed for building energy optimization with\n  Reinforcement Learning", "author": "Alejandro Campoy-Nieves and Antonio Manjavacas and Javier Jim\u00e9nez-Raboso and Miguel Molina-Solana and Juan G\u00f3mez-Romero", "abstract": "  Simulation has become a crucial tool for Building Energy Optimization (BEO)\nas it enables the evaluation of different design and control strategies at a\nlow cost. Machine Learning (ML) algorithms can leverage large-scale simulations\nto learn optimal control from vast amounts of data without supervision,\nparticularly under the Reinforcement Learning (RL) paradigm. Unfortunately, the\nlack of open and standardized tools has hindered the widespread application of\nML and RL to BEO. To address this issue, this paper presents Sinergym, an\nopen-source Python-based virtual testbed for large-scale building simulation,\ndata collection, continuous control, and experiment monitoring. Sinergym\nprovides a consistent interface for training and running controllers,\npredefined benchmarks, experiment visualization and replication support, and\ncomprehensive documentation in a ready-to-use software library. This paper 1)\nhighlights the main features of Sinergym in comparison to other existing\nframeworks, 2) describes its basic usage, and 3) demonstrates its applicability\nfor RL-based BEO through several representative examples. By integrating\nsimulation, data, and control, Sinergym supports the development of\nintelligent, data-driven applications for more efficient and responsive\nbuilding operations, aligning with the objectives of digital twin technology.\n", "link": "http://arxiv.org/abs/2412.08293v1", "date": "2024-12-11", "relevancy": 1.4732, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4885}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SINERGYM%20--%20A%20virtual%20testbed%20for%20building%20energy%20optimization%20with%0A%20%20Reinforcement%20Learning&body=Title%3A%20SINERGYM%20--%20A%20virtual%20testbed%20for%20building%20energy%20optimization%20with%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Alejandro%20Campoy-Nieves%20and%20Antonio%20Manjavacas%20and%20Javier%20Jim%C3%A9nez-Raboso%20and%20Miguel%20Molina-Solana%20and%20Juan%20G%C3%B3mez-Romero%0AAbstract%3A%20%20%20Simulation%20has%20become%20a%20crucial%20tool%20for%20Building%20Energy%20Optimization%20%28BEO%29%0Aas%20it%20enables%20the%20evaluation%20of%20different%20design%20and%20control%20strategies%20at%20a%0Alow%20cost.%20Machine%20Learning%20%28ML%29%20algorithms%20can%20leverage%20large-scale%20simulations%0Ato%20learn%20optimal%20control%20from%20vast%20amounts%20of%20data%20without%20supervision%2C%0Aparticularly%20under%20the%20Reinforcement%20Learning%20%28RL%29%20paradigm.%20Unfortunately%2C%20the%0Alack%20of%20open%20and%20standardized%20tools%20has%20hindered%20the%20widespread%20application%20of%0AML%20and%20RL%20to%20BEO.%20To%20address%20this%20issue%2C%20this%20paper%20presents%20Sinergym%2C%20an%0Aopen-source%20Python-based%20virtual%20testbed%20for%20large-scale%20building%20simulation%2C%0Adata%20collection%2C%20continuous%20control%2C%20and%20experiment%20monitoring.%20Sinergym%0Aprovides%20a%20consistent%20interface%20for%20training%20and%20running%20controllers%2C%0Apredefined%20benchmarks%2C%20experiment%20visualization%20and%20replication%20support%2C%20and%0Acomprehensive%20documentation%20in%20a%20ready-to-use%20software%20library.%20This%20paper%201%29%0Ahighlights%20the%20main%20features%20of%20Sinergym%20in%20comparison%20to%20other%20existing%0Aframeworks%2C%202%29%20describes%20its%20basic%20usage%2C%20and%203%29%20demonstrates%20its%20applicability%0Afor%20RL-based%20BEO%20through%20several%20representative%20examples.%20By%20integrating%0Asimulation%2C%20data%2C%20and%20control%2C%20Sinergym%20supports%20the%20development%20of%0Aintelligent%2C%20data-driven%20applications%20for%20more%20efficient%20and%20responsive%0Abuilding%20operations%2C%20aligning%20with%20the%20objectives%20of%20digital%20twin%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSINERGYM%2520--%2520A%2520virtual%2520testbed%2520for%2520building%2520energy%2520optimization%2520with%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DAlejandro%2520Campoy-Nieves%2520and%2520Antonio%2520Manjavacas%2520and%2520Javier%2520Jim%25C3%25A9nez-Raboso%2520and%2520Miguel%2520Molina-Solana%2520and%2520Juan%2520G%25C3%25B3mez-Romero%26entry.1292438233%3D%2520%2520Simulation%2520has%2520become%2520a%2520crucial%2520tool%2520for%2520Building%2520Energy%2520Optimization%2520%2528BEO%2529%250Aas%2520it%2520enables%2520the%2520evaluation%2520of%2520different%2520design%2520and%2520control%2520strategies%2520at%2520a%250Alow%2520cost.%2520Machine%2520Learning%2520%2528ML%2529%2520algorithms%2520can%2520leverage%2520large-scale%2520simulations%250Ato%2520learn%2520optimal%2520control%2520from%2520vast%2520amounts%2520of%2520data%2520without%2520supervision%252C%250Aparticularly%2520under%2520the%2520Reinforcement%2520Learning%2520%2528RL%2529%2520paradigm.%2520Unfortunately%252C%2520the%250Alack%2520of%2520open%2520and%2520standardized%2520tools%2520has%2520hindered%2520the%2520widespread%2520application%2520of%250AML%2520and%2520RL%2520to%2520BEO.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%2520presents%2520Sinergym%252C%2520an%250Aopen-source%2520Python-based%2520virtual%2520testbed%2520for%2520large-scale%2520building%2520simulation%252C%250Adata%2520collection%252C%2520continuous%2520control%252C%2520and%2520experiment%2520monitoring.%2520Sinergym%250Aprovides%2520a%2520consistent%2520interface%2520for%2520training%2520and%2520running%2520controllers%252C%250Apredefined%2520benchmarks%252C%2520experiment%2520visualization%2520and%2520replication%2520support%252C%2520and%250Acomprehensive%2520documentation%2520in%2520a%2520ready-to-use%2520software%2520library.%2520This%2520paper%25201%2529%250Ahighlights%2520the%2520main%2520features%2520of%2520Sinergym%2520in%2520comparison%2520to%2520other%2520existing%250Aframeworks%252C%25202%2529%2520describes%2520its%2520basic%2520usage%252C%2520and%25203%2529%2520demonstrates%2520its%2520applicability%250Afor%2520RL-based%2520BEO%2520through%2520several%2520representative%2520examples.%2520By%2520integrating%250Asimulation%252C%2520data%252C%2520and%2520control%252C%2520Sinergym%2520supports%2520the%2520development%2520of%250Aintelligent%252C%2520data-driven%2520applications%2520for%2520more%2520efficient%2520and%2520responsive%250Abuilding%2520operations%252C%2520aligning%2520with%2520the%2520objectives%2520of%2520digital%2520twin%2520technology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SINERGYM%20--%20A%20virtual%20testbed%20for%20building%20energy%20optimization%20with%0A%20%20Reinforcement%20Learning&entry.906535625=Alejandro%20Campoy-Nieves%20and%20Antonio%20Manjavacas%20and%20Javier%20Jim%C3%A9nez-Raboso%20and%20Miguel%20Molina-Solana%20and%20Juan%20G%C3%B3mez-Romero&entry.1292438233=%20%20Simulation%20has%20become%20a%20crucial%20tool%20for%20Building%20Energy%20Optimization%20%28BEO%29%0Aas%20it%20enables%20the%20evaluation%20of%20different%20design%20and%20control%20strategies%20at%20a%0Alow%20cost.%20Machine%20Learning%20%28ML%29%20algorithms%20can%20leverage%20large-scale%20simulations%0Ato%20learn%20optimal%20control%20from%20vast%20amounts%20of%20data%20without%20supervision%2C%0Aparticularly%20under%20the%20Reinforcement%20Learning%20%28RL%29%20paradigm.%20Unfortunately%2C%20the%0Alack%20of%20open%20and%20standardized%20tools%20has%20hindered%20the%20widespread%20application%20of%0AML%20and%20RL%20to%20BEO.%20To%20address%20this%20issue%2C%20this%20paper%20presents%20Sinergym%2C%20an%0Aopen-source%20Python-based%20virtual%20testbed%20for%20large-scale%20building%20simulation%2C%0Adata%20collection%2C%20continuous%20control%2C%20and%20experiment%20monitoring.%20Sinergym%0Aprovides%20a%20consistent%20interface%20for%20training%20and%20running%20controllers%2C%0Apredefined%20benchmarks%2C%20experiment%20visualization%20and%20replication%20support%2C%20and%0Acomprehensive%20documentation%20in%20a%20ready-to-use%20software%20library.%20This%20paper%201%29%0Ahighlights%20the%20main%20features%20of%20Sinergym%20in%20comparison%20to%20other%20existing%0Aframeworks%2C%202%29%20describes%20its%20basic%20usage%2C%20and%203%29%20demonstrates%20its%20applicability%0Afor%20RL-based%20BEO%20through%20several%20representative%20examples.%20By%20integrating%0Asimulation%2C%20data%2C%20and%20control%2C%20Sinergym%20supports%20the%20development%20of%0Aintelligent%2C%20data-driven%20applications%20for%20more%20efficient%20and%20responsive%0Abuilding%20operations%2C%20aligning%20with%20the%20objectives%20of%20digital%20twin%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08293v1&entry.124074799=Read"},
{"title": "Training Data Reconstruction: Privacy due to Uncertainty?", "author": "Christina Runkel and Kanchana Vaishnavi Gandikota and Jonas Geiping and Carola-Bibiane Sch\u00f6nlieb and Michael Moeller", "abstract": "  Being able to reconstruct training data from the parameters of a neural\nnetwork is a major privacy concern. Previous works have shown that\nreconstructing training data, under certain circumstances, is possible. In this\nwork, we analyse such reconstructions empirically and propose a new formulation\nof the reconstruction as a solution to a bilevel optimisation problem. We\ndemonstrate that our formulation as well as previous approaches highly depend\non the initialisation of the training images $x$ to reconstruct. In particular,\nwe show that a random initialisation of $x$ can lead to reconstructions that\nresemble valid training samples while not being part of the actual training\ndataset. Thus, our experiments on affine and one-hidden layer networks suggest\nthat when reconstructing natural images, yet an adversary cannot identify\nwhether reconstructed images have indeed been part of the set of training\nsamples.\n", "link": "http://arxiv.org/abs/2412.08544v1", "date": "2024-12-11", "relevancy": 1.9206, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4873}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Data%20Reconstruction%3A%20Privacy%20due%20to%20Uncertainty%3F&body=Title%3A%20Training%20Data%20Reconstruction%3A%20Privacy%20due%20to%20Uncertainty%3F%0AAuthor%3A%20Christina%20Runkel%20and%20Kanchana%20Vaishnavi%20Gandikota%20and%20Jonas%20Geiping%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Michael%20Moeller%0AAbstract%3A%20%20%20Being%20able%20to%20reconstruct%20training%20data%20from%20the%20parameters%20of%20a%20neural%0Anetwork%20is%20a%20major%20privacy%20concern.%20Previous%20works%20have%20shown%20that%0Areconstructing%20training%20data%2C%20under%20certain%20circumstances%2C%20is%20possible.%20In%20this%0Awork%2C%20we%20analyse%20such%20reconstructions%20empirically%20and%20propose%20a%20new%20formulation%0Aof%20the%20reconstruction%20as%20a%20solution%20to%20a%20bilevel%20optimisation%20problem.%20We%0Ademonstrate%20that%20our%20formulation%20as%20well%20as%20previous%20approaches%20highly%20depend%0Aon%20the%20initialisation%20of%20the%20training%20images%20%24x%24%20to%20reconstruct.%20In%20particular%2C%0Awe%20show%20that%20a%20random%20initialisation%20of%20%24x%24%20can%20lead%20to%20reconstructions%20that%0Aresemble%20valid%20training%20samples%20while%20not%20being%20part%20of%20the%20actual%20training%0Adataset.%20Thus%2C%20our%20experiments%20on%20affine%20and%20one-hidden%20layer%20networks%20suggest%0Athat%20when%20reconstructing%20natural%20images%2C%20yet%20an%20adversary%20cannot%20identify%0Awhether%20reconstructed%20images%20have%20indeed%20been%20part%20of%20the%20set%20of%20training%0Asamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Data%2520Reconstruction%253A%2520Privacy%2520due%2520to%2520Uncertainty%253F%26entry.906535625%3DChristina%2520Runkel%2520and%2520Kanchana%2520Vaishnavi%2520Gandikota%2520and%2520Jonas%2520Geiping%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Michael%2520Moeller%26entry.1292438233%3D%2520%2520Being%2520able%2520to%2520reconstruct%2520training%2520data%2520from%2520the%2520parameters%2520of%2520a%2520neural%250Anetwork%2520is%2520a%2520major%2520privacy%2520concern.%2520Previous%2520works%2520have%2520shown%2520that%250Areconstructing%2520training%2520data%252C%2520under%2520certain%2520circumstances%252C%2520is%2520possible.%2520In%2520this%250Awork%252C%2520we%2520analyse%2520such%2520reconstructions%2520empirically%2520and%2520propose%2520a%2520new%2520formulation%250Aof%2520the%2520reconstruction%2520as%2520a%2520solution%2520to%2520a%2520bilevel%2520optimisation%2520problem.%2520We%250Ademonstrate%2520that%2520our%2520formulation%2520as%2520well%2520as%2520previous%2520approaches%2520highly%2520depend%250Aon%2520the%2520initialisation%2520of%2520the%2520training%2520images%2520%2524x%2524%2520to%2520reconstruct.%2520In%2520particular%252C%250Awe%2520show%2520that%2520a%2520random%2520initialisation%2520of%2520%2524x%2524%2520can%2520lead%2520to%2520reconstructions%2520that%250Aresemble%2520valid%2520training%2520samples%2520while%2520not%2520being%2520part%2520of%2520the%2520actual%2520training%250Adataset.%2520Thus%252C%2520our%2520experiments%2520on%2520affine%2520and%2520one-hidden%2520layer%2520networks%2520suggest%250Athat%2520when%2520reconstructing%2520natural%2520images%252C%2520yet%2520an%2520adversary%2520cannot%2520identify%250Awhether%2520reconstructed%2520images%2520have%2520indeed%2520been%2520part%2520of%2520the%2520set%2520of%2520training%250Asamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Data%20Reconstruction%3A%20Privacy%20due%20to%20Uncertainty%3F&entry.906535625=Christina%20Runkel%20and%20Kanchana%20Vaishnavi%20Gandikota%20and%20Jonas%20Geiping%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Michael%20Moeller&entry.1292438233=%20%20Being%20able%20to%20reconstruct%20training%20data%20from%20the%20parameters%20of%20a%20neural%0Anetwork%20is%20a%20major%20privacy%20concern.%20Previous%20works%20have%20shown%20that%0Areconstructing%20training%20data%2C%20under%20certain%20circumstances%2C%20is%20possible.%20In%20this%0Awork%2C%20we%20analyse%20such%20reconstructions%20empirically%20and%20propose%20a%20new%20formulation%0Aof%20the%20reconstruction%20as%20a%20solution%20to%20a%20bilevel%20optimisation%20problem.%20We%0Ademonstrate%20that%20our%20formulation%20as%20well%20as%20previous%20approaches%20highly%20depend%0Aon%20the%20initialisation%20of%20the%20training%20images%20%24x%24%20to%20reconstruct.%20In%20particular%2C%0Awe%20show%20that%20a%20random%20initialisation%20of%20%24x%24%20can%20lead%20to%20reconstructions%20that%0Aresemble%20valid%20training%20samples%20while%20not%20being%20part%20of%20the%20actual%20training%0Adataset.%20Thus%2C%20our%20experiments%20on%20affine%20and%20one-hidden%20layer%20networks%20suggest%0Athat%20when%20reconstructing%20natural%20images%2C%20yet%20an%20adversary%20cannot%20identify%0Awhether%20reconstructed%20images%20have%20indeed%20been%20part%20of%20the%20set%20of%20training%0Asamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08544v1&entry.124074799=Read"},
{"title": "AgentMixer: Multi-Agent Correlated Policy Factorization", "author": "Zhiyuan Li and Wenshuai Zhao and Lijun Wu and Joni Pajarinen", "abstract": "  In multi-agent reinforcement learning, centralized training with\ndecentralized execution (CTDE) methods typically assume that agents make\ndecisions based on their local observations independently, which may not lead\nto a correlated joint policy with coordination. Coordination can be explicitly\nencouraged during training and individual policies can be trained to imitate\nthe correlated joint policy. However, this may lead to an \\textit{asymmetric\nlearning failure} due to the observation mismatch between the joint and\nindividual policies. Inspired by the concept of correlated equilibrium, we\nintroduce a \\textit{strategy modification} called AgentMixer that allows agents\nto correlate their policies. AgentMixer combines individual partially\nobservable policies into a joint fully observable policy non-linearly. To\nenable decentralized execution, we introduce\n\\textit{Individual-Global-Consistency} to guarantee mode consistency during\njoint training of the centralized and decentralized policies and prove that\nAgentMixer converges to an $\\epsilon$-approximate Correlated Equilibrium. In\nthe Multi-Agent MuJoCo, SMAC-v2, Matrix Game, and Predator-Prey benchmarks,\nAgentMixer outperforms or matches state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2401.08728v2", "date": "2024-12-11", "relevancy": 1.4494, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5042}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4834}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentMixer%3A%20Multi-Agent%20Correlated%20Policy%20Factorization&body=Title%3A%20AgentMixer%3A%20Multi-Agent%20Correlated%20Policy%20Factorization%0AAuthor%3A%20Zhiyuan%20Li%20and%20Wenshuai%20Zhao%20and%20Lijun%20Wu%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20In%20multi-agent%20reinforcement%20learning%2C%20centralized%20training%20with%0Adecentralized%20execution%20%28CTDE%29%20methods%20typically%20assume%20that%20agents%20make%0Adecisions%20based%20on%20their%20local%20observations%20independently%2C%20which%20may%20not%20lead%0Ato%20a%20correlated%20joint%20policy%20with%20coordination.%20Coordination%20can%20be%20explicitly%0Aencouraged%20during%20training%20and%20individual%20policies%20can%20be%20trained%20to%20imitate%0Athe%20correlated%20joint%20policy.%20However%2C%20this%20may%20lead%20to%20an%20%5Ctextit%7Basymmetric%0Alearning%20failure%7D%20due%20to%20the%20observation%20mismatch%20between%20the%20joint%20and%0Aindividual%20policies.%20Inspired%20by%20the%20concept%20of%20correlated%20equilibrium%2C%20we%0Aintroduce%20a%20%5Ctextit%7Bstrategy%20modification%7D%20called%20AgentMixer%20that%20allows%20agents%0Ato%20correlate%20their%20policies.%20AgentMixer%20combines%20individual%20partially%0Aobservable%20policies%20into%20a%20joint%20fully%20observable%20policy%20non-linearly.%20To%0Aenable%20decentralized%20execution%2C%20we%20introduce%0A%5Ctextit%7BIndividual-Global-Consistency%7D%20to%20guarantee%20mode%20consistency%20during%0Ajoint%20training%20of%20the%20centralized%20and%20decentralized%20policies%20and%20prove%20that%0AAgentMixer%20converges%20to%20an%20%24%5Cepsilon%24-approximate%20Correlated%20Equilibrium.%20In%0Athe%20Multi-Agent%20MuJoCo%2C%20SMAC-v2%2C%20Matrix%20Game%2C%20and%20Predator-Prey%20benchmarks%2C%0AAgentMixer%20outperforms%20or%20matches%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentMixer%253A%2520Multi-Agent%2520Correlated%2520Policy%2520Factorization%26entry.906535625%3DZhiyuan%2520Li%2520and%2520Wenshuai%2520Zhao%2520and%2520Lijun%2520Wu%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520In%2520multi-agent%2520reinforcement%2520learning%252C%2520centralized%2520training%2520with%250Adecentralized%2520execution%2520%2528CTDE%2529%2520methods%2520typically%2520assume%2520that%2520agents%2520make%250Adecisions%2520based%2520on%2520their%2520local%2520observations%2520independently%252C%2520which%2520may%2520not%2520lead%250Ato%2520a%2520correlated%2520joint%2520policy%2520with%2520coordination.%2520Coordination%2520can%2520be%2520explicitly%250Aencouraged%2520during%2520training%2520and%2520individual%2520policies%2520can%2520be%2520trained%2520to%2520imitate%250Athe%2520correlated%2520joint%2520policy.%2520However%252C%2520this%2520may%2520lead%2520to%2520an%2520%255Ctextit%257Basymmetric%250Alearning%2520failure%257D%2520due%2520to%2520the%2520observation%2520mismatch%2520between%2520the%2520joint%2520and%250Aindividual%2520policies.%2520Inspired%2520by%2520the%2520concept%2520of%2520correlated%2520equilibrium%252C%2520we%250Aintroduce%2520a%2520%255Ctextit%257Bstrategy%2520modification%257D%2520called%2520AgentMixer%2520that%2520allows%2520agents%250Ato%2520correlate%2520their%2520policies.%2520AgentMixer%2520combines%2520individual%2520partially%250Aobservable%2520policies%2520into%2520a%2520joint%2520fully%2520observable%2520policy%2520non-linearly.%2520To%250Aenable%2520decentralized%2520execution%252C%2520we%2520introduce%250A%255Ctextit%257BIndividual-Global-Consistency%257D%2520to%2520guarantee%2520mode%2520consistency%2520during%250Ajoint%2520training%2520of%2520the%2520centralized%2520and%2520decentralized%2520policies%2520and%2520prove%2520that%250AAgentMixer%2520converges%2520to%2520an%2520%2524%255Cepsilon%2524-approximate%2520Correlated%2520Equilibrium.%2520In%250Athe%2520Multi-Agent%2520MuJoCo%252C%2520SMAC-v2%252C%2520Matrix%2520Game%252C%2520and%2520Predator-Prey%2520benchmarks%252C%250AAgentMixer%2520outperforms%2520or%2520matches%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentMixer%3A%20Multi-Agent%20Correlated%20Policy%20Factorization&entry.906535625=Zhiyuan%20Li%20and%20Wenshuai%20Zhao%20and%20Lijun%20Wu%20and%20Joni%20Pajarinen&entry.1292438233=%20%20In%20multi-agent%20reinforcement%20learning%2C%20centralized%20training%20with%0Adecentralized%20execution%20%28CTDE%29%20methods%20typically%20assume%20that%20agents%20make%0Adecisions%20based%20on%20their%20local%20observations%20independently%2C%20which%20may%20not%20lead%0Ato%20a%20correlated%20joint%20policy%20with%20coordination.%20Coordination%20can%20be%20explicitly%0Aencouraged%20during%20training%20and%20individual%20policies%20can%20be%20trained%20to%20imitate%0Athe%20correlated%20joint%20policy.%20However%2C%20this%20may%20lead%20to%20an%20%5Ctextit%7Basymmetric%0Alearning%20failure%7D%20due%20to%20the%20observation%20mismatch%20between%20the%20joint%20and%0Aindividual%20policies.%20Inspired%20by%20the%20concept%20of%20correlated%20equilibrium%2C%20we%0Aintroduce%20a%20%5Ctextit%7Bstrategy%20modification%7D%20called%20AgentMixer%20that%20allows%20agents%0Ato%20correlate%20their%20policies.%20AgentMixer%20combines%20individual%20partially%0Aobservable%20policies%20into%20a%20joint%20fully%20observable%20policy%20non-linearly.%20To%0Aenable%20decentralized%20execution%2C%20we%20introduce%0A%5Ctextit%7BIndividual-Global-Consistency%7D%20to%20guarantee%20mode%20consistency%20during%0Ajoint%20training%20of%20the%20centralized%20and%20decentralized%20policies%20and%20prove%20that%0AAgentMixer%20converges%20to%20an%20%24%5Cepsilon%24-approximate%20Correlated%20Equilibrium.%20In%0Athe%20Multi-Agent%20MuJoCo%2C%20SMAC-v2%2C%20Matrix%20Game%2C%20and%20Predator-Prey%20benchmarks%2C%0AAgentMixer%20outperforms%20or%20matches%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08728v2&entry.124074799=Read"},
{"title": "Zero-Shot Mono-to-Binaural Speech Synthesis", "author": "Alon Levkovitch and Julian Salazar and Soroosh Mariooryad and RJ Skerry-Ryan and Nadav Bar and Bastiaan Kleijn and Eliya Nachmani", "abstract": "  We present ZeroBAS, a neural method to synthesize binaural audio from\nmonaural audio recordings and positional information without training on any\nbinaural data. To our knowledge, this is the first published zero-shot neural\napproach to mono-to-binaural audio synthesis. Specifically, we show that a\nparameter-free geometric time warping and amplitude scaling based on source\nlocation suffices to get an initial binaural synthesis that can be refined by\niteratively applying a pretrained denoising vocoder. Furthermore, we find this\nleads to generalization across room conditions, which we measure by introducing\na new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art\nmonaural-to-binaural synthesis methods on unseen conditions. Our zero-shot\nmethod is perceptually on-par with the performance of supervised methods on the\nstandard mono-to-binaural dataset, and even surpasses them on our\nout-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the\npotential of pretrained generative audio models and zero-shot learning to\nunlock robust binaural audio synthesis.\n", "link": "http://arxiv.org/abs/2412.08356v1", "date": "2024-12-11", "relevancy": 1.9474, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4907}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4851}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Mono-to-Binaural%20Speech%20Synthesis&body=Title%3A%20Zero-Shot%20Mono-to-Binaural%20Speech%20Synthesis%0AAuthor%3A%20Alon%20Levkovitch%20and%20Julian%20Salazar%20and%20Soroosh%20Mariooryad%20and%20RJ%20Skerry-Ryan%20and%20Nadav%20Bar%20and%20Bastiaan%20Kleijn%20and%20Eliya%20Nachmani%0AAbstract%3A%20%20%20We%20present%20ZeroBAS%2C%20a%20neural%20method%20to%20synthesize%20binaural%20audio%20from%0Amonaural%20audio%20recordings%20and%20positional%20information%20without%20training%20on%20any%0Abinaural%20data.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20published%20zero-shot%20neural%0Aapproach%20to%20mono-to-binaural%20audio%20synthesis.%20Specifically%2C%20we%20show%20that%20a%0Aparameter-free%20geometric%20time%20warping%20and%20amplitude%20scaling%20based%20on%20source%0Alocation%20suffices%20to%20get%20an%20initial%20binaural%20synthesis%20that%20can%20be%20refined%20by%0Aiteratively%20applying%20a%20pretrained%20denoising%20vocoder.%20Furthermore%2C%20we%20find%20this%0Aleads%20to%20generalization%20across%20room%20conditions%2C%20which%20we%20measure%20by%20introducing%0Aa%20new%20dataset%2C%20TUT%20Mono-to-Binaural%2C%20to%20evaluate%20state-of-the-art%0Amonaural-to-binaural%20synthesis%20methods%20on%20unseen%20conditions.%20Our%20zero-shot%0Amethod%20is%20perceptually%20on-par%20with%20the%20performance%20of%20supervised%20methods%20on%20the%0Astandard%20mono-to-binaural%20dataset%2C%20and%20even%20surpasses%20them%20on%20our%0Aout-of-distribution%20TUT%20Mono-to-Binaural%20dataset.%20Our%20results%20highlight%20the%0Apotential%20of%20pretrained%20generative%20audio%20models%20and%20zero-shot%20learning%20to%0Aunlock%20robust%20binaural%20audio%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Mono-to-Binaural%2520Speech%2520Synthesis%26entry.906535625%3DAlon%2520Levkovitch%2520and%2520Julian%2520Salazar%2520and%2520Soroosh%2520Mariooryad%2520and%2520RJ%2520Skerry-Ryan%2520and%2520Nadav%2520Bar%2520and%2520Bastiaan%2520Kleijn%2520and%2520Eliya%2520Nachmani%26entry.1292438233%3D%2520%2520We%2520present%2520ZeroBAS%252C%2520a%2520neural%2520method%2520to%2520synthesize%2520binaural%2520audio%2520from%250Amonaural%2520audio%2520recordings%2520and%2520positional%2520information%2520without%2520training%2520on%2520any%250Abinaural%2520data.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520published%2520zero-shot%2520neural%250Aapproach%2520to%2520mono-to-binaural%2520audio%2520synthesis.%2520Specifically%252C%2520we%2520show%2520that%2520a%250Aparameter-free%2520geometric%2520time%2520warping%2520and%2520amplitude%2520scaling%2520based%2520on%2520source%250Alocation%2520suffices%2520to%2520get%2520an%2520initial%2520binaural%2520synthesis%2520that%2520can%2520be%2520refined%2520by%250Aiteratively%2520applying%2520a%2520pretrained%2520denoising%2520vocoder.%2520Furthermore%252C%2520we%2520find%2520this%250Aleads%2520to%2520generalization%2520across%2520room%2520conditions%252C%2520which%2520we%2520measure%2520by%2520introducing%250Aa%2520new%2520dataset%252C%2520TUT%2520Mono-to-Binaural%252C%2520to%2520evaluate%2520state-of-the-art%250Amonaural-to-binaural%2520synthesis%2520methods%2520on%2520unseen%2520conditions.%2520Our%2520zero-shot%250Amethod%2520is%2520perceptually%2520on-par%2520with%2520the%2520performance%2520of%2520supervised%2520methods%2520on%2520the%250Astandard%2520mono-to-binaural%2520dataset%252C%2520and%2520even%2520surpasses%2520them%2520on%2520our%250Aout-of-distribution%2520TUT%2520Mono-to-Binaural%2520dataset.%2520Our%2520results%2520highlight%2520the%250Apotential%2520of%2520pretrained%2520generative%2520audio%2520models%2520and%2520zero-shot%2520learning%2520to%250Aunlock%2520robust%2520binaural%2520audio%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Mono-to-Binaural%20Speech%20Synthesis&entry.906535625=Alon%20Levkovitch%20and%20Julian%20Salazar%20and%20Soroosh%20Mariooryad%20and%20RJ%20Skerry-Ryan%20and%20Nadav%20Bar%20and%20Bastiaan%20Kleijn%20and%20Eliya%20Nachmani&entry.1292438233=%20%20We%20present%20ZeroBAS%2C%20a%20neural%20method%20to%20synthesize%20binaural%20audio%20from%0Amonaural%20audio%20recordings%20and%20positional%20information%20without%20training%20on%20any%0Abinaural%20data.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20published%20zero-shot%20neural%0Aapproach%20to%20mono-to-binaural%20audio%20synthesis.%20Specifically%2C%20we%20show%20that%20a%0Aparameter-free%20geometric%20time%20warping%20and%20amplitude%20scaling%20based%20on%20source%0Alocation%20suffices%20to%20get%20an%20initial%20binaural%20synthesis%20that%20can%20be%20refined%20by%0Aiteratively%20applying%20a%20pretrained%20denoising%20vocoder.%20Furthermore%2C%20we%20find%20this%0Aleads%20to%20generalization%20across%20room%20conditions%2C%20which%20we%20measure%20by%20introducing%0Aa%20new%20dataset%2C%20TUT%20Mono-to-Binaural%2C%20to%20evaluate%20state-of-the-art%0Amonaural-to-binaural%20synthesis%20methods%20on%20unseen%20conditions.%20Our%20zero-shot%0Amethod%20is%20perceptually%20on-par%20with%20the%20performance%20of%20supervised%20methods%20on%20the%0Astandard%20mono-to-binaural%20dataset%2C%20and%20even%20surpasses%20them%20on%20our%0Aout-of-distribution%20TUT%20Mono-to-Binaural%20dataset.%20Our%20results%20highlight%20the%0Apotential%20of%20pretrained%20generative%20audio%20models%20and%20zero-shot%20learning%20to%0Aunlock%20robust%20binaural%20audio%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08356v1&entry.124074799=Read"},
{"title": "Bilevel Learning for Dual-Quadruped Collaborative Transportation under\n  Kinematic and Anisotropic Velocity Constraints", "author": "Williard Joshua Jose and Hao Zhang", "abstract": "  Multi-robot collaborative transportation is a critical capability that has\nattracted significant attention over recent years. To reliably transport a\nkinematically constrained payload, a team of robots must closely collaborate\nand coordinate their individual velocities to achieve the desired payload\nmotion. For quadruped robots, a key challenge is caused by their anisotropic\nvelocity limits, where forward and backward movement is faster and more stable\nthan lateral motion. In order to enable dual-quadruped collaborative\ntransportation and address the above challenges, we propose a novel Bilevel\nLearning for Collaborative Transportation (BLCT) approach. In the upper-level,\nBLCT learns a team collaboration policy for the two quadruped robots to move\nthe payload to the goal position, while accounting for the kinematic\nconstraints imposed by their connection to the payload. In the lower-level,\nBLCT optimizes velocity controls of each individual robot to closely follow the\ncollaboration policy while satisfying the anisotropic velocity constraints and\navoiding obstacles. Experiments demonstrate that our BLCT approach well enables\ncollaborative transportation in challenging scenarios and outperforms baseline\napproaches.\n", "link": "http://arxiv.org/abs/2412.08644v1", "date": "2024-12-11", "relevancy": 2.1442, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5838}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5466}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bilevel%20Learning%20for%20Dual-Quadruped%20Collaborative%20Transportation%20under%0A%20%20Kinematic%20and%20Anisotropic%20Velocity%20Constraints&body=Title%3A%20Bilevel%20Learning%20for%20Dual-Quadruped%20Collaborative%20Transportation%20under%0A%20%20Kinematic%20and%20Anisotropic%20Velocity%20Constraints%0AAuthor%3A%20Williard%20Joshua%20Jose%20and%20Hao%20Zhang%0AAbstract%3A%20%20%20Multi-robot%20collaborative%20transportation%20is%20a%20critical%20capability%20that%20has%0Aattracted%20significant%20attention%20over%20recent%20years.%20To%20reliably%20transport%20a%0Akinematically%20constrained%20payload%2C%20a%20team%20of%20robots%20must%20closely%20collaborate%0Aand%20coordinate%20their%20individual%20velocities%20to%20achieve%20the%20desired%20payload%0Amotion.%20For%20quadruped%20robots%2C%20a%20key%20challenge%20is%20caused%20by%20their%20anisotropic%0Avelocity%20limits%2C%20where%20forward%20and%20backward%20movement%20is%20faster%20and%20more%20stable%0Athan%20lateral%20motion.%20In%20order%20to%20enable%20dual-quadruped%20collaborative%0Atransportation%20and%20address%20the%20above%20challenges%2C%20we%20propose%20a%20novel%20Bilevel%0ALearning%20for%20Collaborative%20Transportation%20%28BLCT%29%20approach.%20In%20the%20upper-level%2C%0ABLCT%20learns%20a%20team%20collaboration%20policy%20for%20the%20two%20quadruped%20robots%20to%20move%0Athe%20payload%20to%20the%20goal%20position%2C%20while%20accounting%20for%20the%20kinematic%0Aconstraints%20imposed%20by%20their%20connection%20to%20the%20payload.%20In%20the%20lower-level%2C%0ABLCT%20optimizes%20velocity%20controls%20of%20each%20individual%20robot%20to%20closely%20follow%20the%0Acollaboration%20policy%20while%20satisfying%20the%20anisotropic%20velocity%20constraints%20and%0Aavoiding%20obstacles.%20Experiments%20demonstrate%20that%20our%20BLCT%20approach%20well%20enables%0Acollaborative%20transportation%20in%20challenging%20scenarios%20and%20outperforms%20baseline%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBilevel%2520Learning%2520for%2520Dual-Quadruped%2520Collaborative%2520Transportation%2520under%250A%2520%2520Kinematic%2520and%2520Anisotropic%2520Velocity%2520Constraints%26entry.906535625%3DWilliard%2520Joshua%2520Jose%2520and%2520Hao%2520Zhang%26entry.1292438233%3D%2520%2520Multi-robot%2520collaborative%2520transportation%2520is%2520a%2520critical%2520capability%2520that%2520has%250Aattracted%2520significant%2520attention%2520over%2520recent%2520years.%2520To%2520reliably%2520transport%2520a%250Akinematically%2520constrained%2520payload%252C%2520a%2520team%2520of%2520robots%2520must%2520closely%2520collaborate%250Aand%2520coordinate%2520their%2520individual%2520velocities%2520to%2520achieve%2520the%2520desired%2520payload%250Amotion.%2520For%2520quadruped%2520robots%252C%2520a%2520key%2520challenge%2520is%2520caused%2520by%2520their%2520anisotropic%250Avelocity%2520limits%252C%2520where%2520forward%2520and%2520backward%2520movement%2520is%2520faster%2520and%2520more%2520stable%250Athan%2520lateral%2520motion.%2520In%2520order%2520to%2520enable%2520dual-quadruped%2520collaborative%250Atransportation%2520and%2520address%2520the%2520above%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520Bilevel%250ALearning%2520for%2520Collaborative%2520Transportation%2520%2528BLCT%2529%2520approach.%2520In%2520the%2520upper-level%252C%250ABLCT%2520learns%2520a%2520team%2520collaboration%2520policy%2520for%2520the%2520two%2520quadruped%2520robots%2520to%2520move%250Athe%2520payload%2520to%2520the%2520goal%2520position%252C%2520while%2520accounting%2520for%2520the%2520kinematic%250Aconstraints%2520imposed%2520by%2520their%2520connection%2520to%2520the%2520payload.%2520In%2520the%2520lower-level%252C%250ABLCT%2520optimizes%2520velocity%2520controls%2520of%2520each%2520individual%2520robot%2520to%2520closely%2520follow%2520the%250Acollaboration%2520policy%2520while%2520satisfying%2520the%2520anisotropic%2520velocity%2520constraints%2520and%250Aavoiding%2520obstacles.%2520Experiments%2520demonstrate%2520that%2520our%2520BLCT%2520approach%2520well%2520enables%250Acollaborative%2520transportation%2520in%2520challenging%2520scenarios%2520and%2520outperforms%2520baseline%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bilevel%20Learning%20for%20Dual-Quadruped%20Collaborative%20Transportation%20under%0A%20%20Kinematic%20and%20Anisotropic%20Velocity%20Constraints&entry.906535625=Williard%20Joshua%20Jose%20and%20Hao%20Zhang&entry.1292438233=%20%20Multi-robot%20collaborative%20transportation%20is%20a%20critical%20capability%20that%20has%0Aattracted%20significant%20attention%20over%20recent%20years.%20To%20reliably%20transport%20a%0Akinematically%20constrained%20payload%2C%20a%20team%20of%20robots%20must%20closely%20collaborate%0Aand%20coordinate%20their%20individual%20velocities%20to%20achieve%20the%20desired%20payload%0Amotion.%20For%20quadruped%20robots%2C%20a%20key%20challenge%20is%20caused%20by%20their%20anisotropic%0Avelocity%20limits%2C%20where%20forward%20and%20backward%20movement%20is%20faster%20and%20more%20stable%0Athan%20lateral%20motion.%20In%20order%20to%20enable%20dual-quadruped%20collaborative%0Atransportation%20and%20address%20the%20above%20challenges%2C%20we%20propose%20a%20novel%20Bilevel%0ALearning%20for%20Collaborative%20Transportation%20%28BLCT%29%20approach.%20In%20the%20upper-level%2C%0ABLCT%20learns%20a%20team%20collaboration%20policy%20for%20the%20two%20quadruped%20robots%20to%20move%0Athe%20payload%20to%20the%20goal%20position%2C%20while%20accounting%20for%20the%20kinematic%0Aconstraints%20imposed%20by%20their%20connection%20to%20the%20payload.%20In%20the%20lower-level%2C%0ABLCT%20optimizes%20velocity%20controls%20of%20each%20individual%20robot%20to%20closely%20follow%20the%0Acollaboration%20policy%20while%20satisfying%20the%20anisotropic%20velocity%20constraints%20and%0Aavoiding%20obstacles.%20Experiments%20demonstrate%20that%20our%20BLCT%20approach%20well%20enables%0Acollaborative%20transportation%20in%20challenging%20scenarios%20and%20outperforms%20baseline%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08644v1&entry.124074799=Read"},
{"title": "Which Augmentation Should I Use? An Empirical Investigation of\n  Augmentations for Self-Supervised Phonocardiogram Representation Learning", "author": "Aristotelis Ballas and Vasileios Papapanagiotou and Christos Diou", "abstract": "  Despite recent advancements in deep learning, its application in real-world\nmedical settings, such as phonocardiogram (PCG) classification, remains\nlimited. A significant barrier is the lack of high-quality annotated datasets,\nwhich hampers the development of robust, generalizable models that can perform\nwell on newly collected, out-of-distribution (OOD) data. Self-Supervised\nLearning (SSL) contrastive learning, has shown promise in mitigating the issue\nof data scarcity by using unlabeled data to enhance model robustness. Even\nthough SSL methods have been proposed and researched in other domains, works\nfocusing on the impact of data augmentations on model robustness for PCG\nclassification are limited. In particular, while augmentations are a key\ncomponent in SSL, selecting the most suitable policy during training is highly\nchallenging. Improper augmentations can lead to substantial performance\ndegradation and even hinder a network's ability to learn meaningful\nrepresentations. Addressing this gap, our research aims to explore and evaluate\na wide range of audio-based augmentations and uncover combinations that enhance\nSSL model performance in PCG classification. We conduct a comprehensive\ncomparative analysis across multiple datasets, assessing the impact of various\naugmentations on model performance. Our findings reveal that depending on the\ntraining distribution, augmentation choice significantly influences model\nrobustness, with fully-supervised models experiencing up to a 32\\% drop in\neffectiveness when evaluated on unseen data, while SSL models demonstrate\ngreater resilience, losing only 10\\% or even improving in some cases. This\nstudy also highlights the most promising and appropriate augmentations for PCG\nsignal processing, by calculating their effect size on training. These insights\nequip researchers with valuable guidelines for developing reliable models in\nPCG signal processing.\n", "link": "http://arxiv.org/abs/2312.00502v4", "date": "2024-12-11", "relevancy": 1.8951, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5016}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4747}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Which%20Augmentation%20Should%20I%20Use%3F%20An%20Empirical%20Investigation%20of%0A%20%20Augmentations%20for%20Self-Supervised%20Phonocardiogram%20Representation%20Learning&body=Title%3A%20Which%20Augmentation%20Should%20I%20Use%3F%20An%20Empirical%20Investigation%20of%0A%20%20Augmentations%20for%20Self-Supervised%20Phonocardiogram%20Representation%20Learning%0AAuthor%3A%20Aristotelis%20Ballas%20and%20Vasileios%20Papapanagiotou%20and%20Christos%20Diou%0AAbstract%3A%20%20%20Despite%20recent%20advancements%20in%20deep%20learning%2C%20its%20application%20in%20real-world%0Amedical%20settings%2C%20such%20as%20phonocardiogram%20%28PCG%29%20classification%2C%20remains%0Alimited.%20A%20significant%20barrier%20is%20the%20lack%20of%20high-quality%20annotated%20datasets%2C%0Awhich%20hampers%20the%20development%20of%20robust%2C%20generalizable%20models%20that%20can%20perform%0Awell%20on%20newly%20collected%2C%20out-of-distribution%20%28OOD%29%20data.%20Self-Supervised%0ALearning%20%28SSL%29%20contrastive%20learning%2C%20has%20shown%20promise%20in%20mitigating%20the%20issue%0Aof%20data%20scarcity%20by%20using%20unlabeled%20data%20to%20enhance%20model%20robustness.%20Even%0Athough%20SSL%20methods%20have%20been%20proposed%20and%20researched%20in%20other%20domains%2C%20works%0Afocusing%20on%20the%20impact%20of%20data%20augmentations%20on%20model%20robustness%20for%20PCG%0Aclassification%20are%20limited.%20In%20particular%2C%20while%20augmentations%20are%20a%20key%0Acomponent%20in%20SSL%2C%20selecting%20the%20most%20suitable%20policy%20during%20training%20is%20highly%0Achallenging.%20Improper%20augmentations%20can%20lead%20to%20substantial%20performance%0Adegradation%20and%20even%20hinder%20a%20network%27s%20ability%20to%20learn%20meaningful%0Arepresentations.%20Addressing%20this%20gap%2C%20our%20research%20aims%20to%20explore%20and%20evaluate%0Aa%20wide%20range%20of%20audio-based%20augmentations%20and%20uncover%20combinations%20that%20enhance%0ASSL%20model%20performance%20in%20PCG%20classification.%20We%20conduct%20a%20comprehensive%0Acomparative%20analysis%20across%20multiple%20datasets%2C%20assessing%20the%20impact%20of%20various%0Aaugmentations%20on%20model%20performance.%20Our%20findings%20reveal%20that%20depending%20on%20the%0Atraining%20distribution%2C%20augmentation%20choice%20significantly%20influences%20model%0Arobustness%2C%20with%20fully-supervised%20models%20experiencing%20up%20to%20a%2032%5C%25%20drop%20in%0Aeffectiveness%20when%20evaluated%20on%20unseen%20data%2C%20while%20SSL%20models%20demonstrate%0Agreater%20resilience%2C%20losing%20only%2010%5C%25%20or%20even%20improving%20in%20some%20cases.%20This%0Astudy%20also%20highlights%20the%20most%20promising%20and%20appropriate%20augmentations%20for%20PCG%0Asignal%20processing%2C%20by%20calculating%20their%20effect%20size%20on%20training.%20These%20insights%0Aequip%20researchers%20with%20valuable%20guidelines%20for%20developing%20reliable%20models%20in%0APCG%20signal%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00502v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhich%2520Augmentation%2520Should%2520I%2520Use%253F%2520An%2520Empirical%2520Investigation%2520of%250A%2520%2520Augmentations%2520for%2520Self-Supervised%2520Phonocardiogram%2520Representation%2520Learning%26entry.906535625%3DAristotelis%2520Ballas%2520and%2520Vasileios%2520Papapanagiotou%2520and%2520Christos%2520Diou%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advancements%2520in%2520deep%2520learning%252C%2520its%2520application%2520in%2520real-world%250Amedical%2520settings%252C%2520such%2520as%2520phonocardiogram%2520%2528PCG%2529%2520classification%252C%2520remains%250Alimited.%2520A%2520significant%2520barrier%2520is%2520the%2520lack%2520of%2520high-quality%2520annotated%2520datasets%252C%250Awhich%2520hampers%2520the%2520development%2520of%2520robust%252C%2520generalizable%2520models%2520that%2520can%2520perform%250Awell%2520on%2520newly%2520collected%252C%2520out-of-distribution%2520%2528OOD%2529%2520data.%2520Self-Supervised%250ALearning%2520%2528SSL%2529%2520contrastive%2520learning%252C%2520has%2520shown%2520promise%2520in%2520mitigating%2520the%2520issue%250Aof%2520data%2520scarcity%2520by%2520using%2520unlabeled%2520data%2520to%2520enhance%2520model%2520robustness.%2520Even%250Athough%2520SSL%2520methods%2520have%2520been%2520proposed%2520and%2520researched%2520in%2520other%2520domains%252C%2520works%250Afocusing%2520on%2520the%2520impact%2520of%2520data%2520augmentations%2520on%2520model%2520robustness%2520for%2520PCG%250Aclassification%2520are%2520limited.%2520In%2520particular%252C%2520while%2520augmentations%2520are%2520a%2520key%250Acomponent%2520in%2520SSL%252C%2520selecting%2520the%2520most%2520suitable%2520policy%2520during%2520training%2520is%2520highly%250Achallenging.%2520Improper%2520augmentations%2520can%2520lead%2520to%2520substantial%2520performance%250Adegradation%2520and%2520even%2520hinder%2520a%2520network%2527s%2520ability%2520to%2520learn%2520meaningful%250Arepresentations.%2520Addressing%2520this%2520gap%252C%2520our%2520research%2520aims%2520to%2520explore%2520and%2520evaluate%250Aa%2520wide%2520range%2520of%2520audio-based%2520augmentations%2520and%2520uncover%2520combinations%2520that%2520enhance%250ASSL%2520model%2520performance%2520in%2520PCG%2520classification.%2520We%2520conduct%2520a%2520comprehensive%250Acomparative%2520analysis%2520across%2520multiple%2520datasets%252C%2520assessing%2520the%2520impact%2520of%2520various%250Aaugmentations%2520on%2520model%2520performance.%2520Our%2520findings%2520reveal%2520that%2520depending%2520on%2520the%250Atraining%2520distribution%252C%2520augmentation%2520choice%2520significantly%2520influences%2520model%250Arobustness%252C%2520with%2520fully-supervised%2520models%2520experiencing%2520up%2520to%2520a%252032%255C%2525%2520drop%2520in%250Aeffectiveness%2520when%2520evaluated%2520on%2520unseen%2520data%252C%2520while%2520SSL%2520models%2520demonstrate%250Agreater%2520resilience%252C%2520losing%2520only%252010%255C%2525%2520or%2520even%2520improving%2520in%2520some%2520cases.%2520This%250Astudy%2520also%2520highlights%2520the%2520most%2520promising%2520and%2520appropriate%2520augmentations%2520for%2520PCG%250Asignal%2520processing%252C%2520by%2520calculating%2520their%2520effect%2520size%2520on%2520training.%2520These%2520insights%250Aequip%2520researchers%2520with%2520valuable%2520guidelines%2520for%2520developing%2520reliable%2520models%2520in%250APCG%2520signal%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00502v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Which%20Augmentation%20Should%20I%20Use%3F%20An%20Empirical%20Investigation%20of%0A%20%20Augmentations%20for%20Self-Supervised%20Phonocardiogram%20Representation%20Learning&entry.906535625=Aristotelis%20Ballas%20and%20Vasileios%20Papapanagiotou%20and%20Christos%20Diou&entry.1292438233=%20%20Despite%20recent%20advancements%20in%20deep%20learning%2C%20its%20application%20in%20real-world%0Amedical%20settings%2C%20such%20as%20phonocardiogram%20%28PCG%29%20classification%2C%20remains%0Alimited.%20A%20significant%20barrier%20is%20the%20lack%20of%20high-quality%20annotated%20datasets%2C%0Awhich%20hampers%20the%20development%20of%20robust%2C%20generalizable%20models%20that%20can%20perform%0Awell%20on%20newly%20collected%2C%20out-of-distribution%20%28OOD%29%20data.%20Self-Supervised%0ALearning%20%28SSL%29%20contrastive%20learning%2C%20has%20shown%20promise%20in%20mitigating%20the%20issue%0Aof%20data%20scarcity%20by%20using%20unlabeled%20data%20to%20enhance%20model%20robustness.%20Even%0Athough%20SSL%20methods%20have%20been%20proposed%20and%20researched%20in%20other%20domains%2C%20works%0Afocusing%20on%20the%20impact%20of%20data%20augmentations%20on%20model%20robustness%20for%20PCG%0Aclassification%20are%20limited.%20In%20particular%2C%20while%20augmentations%20are%20a%20key%0Acomponent%20in%20SSL%2C%20selecting%20the%20most%20suitable%20policy%20during%20training%20is%20highly%0Achallenging.%20Improper%20augmentations%20can%20lead%20to%20substantial%20performance%0Adegradation%20and%20even%20hinder%20a%20network%27s%20ability%20to%20learn%20meaningful%0Arepresentations.%20Addressing%20this%20gap%2C%20our%20research%20aims%20to%20explore%20and%20evaluate%0Aa%20wide%20range%20of%20audio-based%20augmentations%20and%20uncover%20combinations%20that%20enhance%0ASSL%20model%20performance%20in%20PCG%20classification.%20We%20conduct%20a%20comprehensive%0Acomparative%20analysis%20across%20multiple%20datasets%2C%20assessing%20the%20impact%20of%20various%0Aaugmentations%20on%20model%20performance.%20Our%20findings%20reveal%20that%20depending%20on%20the%0Atraining%20distribution%2C%20augmentation%20choice%20significantly%20influences%20model%0Arobustness%2C%20with%20fully-supervised%20models%20experiencing%20up%20to%20a%2032%5C%25%20drop%20in%0Aeffectiveness%20when%20evaluated%20on%20unseen%20data%2C%20while%20SSL%20models%20demonstrate%0Agreater%20resilience%2C%20losing%20only%2010%5C%25%20or%20even%20improving%20in%20some%20cases.%20This%0Astudy%20also%20highlights%20the%20most%20promising%20and%20appropriate%20augmentations%20for%20PCG%0Asignal%20processing%2C%20by%20calculating%20their%20effect%20size%20on%20training.%20These%20insights%0Aequip%20researchers%20with%20valuable%20guidelines%20for%20developing%20reliable%20models%20in%0APCG%20signal%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00502v4&entry.124074799=Read"},
{"title": "ALoRE: Efficient Visual Adaptation via Aggregating Low Rank Experts", "author": "Sinan Du and Guosheng Zhang and Keyao Wang and Yuanrui Wang and Haixiao Yue and Gang Zhang and Errui Ding and Jingdong Wang and Zhengzhuo Xu and Chun Yuan", "abstract": "  Parameter-efficient transfer learning (PETL) has become a promising paradigm\nfor adapting large-scale vision foundation models to downstream tasks. Typical\nmethods primarily leverage the intrinsic low rank property to make\ndecomposition, learning task-specific weights while compressing parameter size.\nHowever, such approaches predominantly manipulate within the original feature\nspace utilizing a single-branch structure, which might be suboptimal for\ndecoupling the learned representations and patterns. In this paper, we propose\nALoRE, a novel PETL method that reuses the hypercomplex parameterized space\nconstructed by Kronecker product to Aggregate Low Rank Experts using a\nmulti-branch paradigm, disentangling the learned cognitive patterns during\ntraining. Thanks to the artful design, ALoRE maintains negligible extra\nparameters and can be effortlessly merged into the frozen backbone via\nre-parameterization in a sequential manner, avoiding additional inference\nlatency. We conduct extensive experiments on 24 image classification tasks\nusing various backbone variants. Experimental results demonstrate that ALoRE\noutperforms the full fine-tuning strategy and other state-of-the-art PETL\nmethods in terms of performance and parameter efficiency. For instance, ALoRE\nobtains 3.06% and 9.97% Top-1 accuracy improvement on average compared to full\nfine-tuning on the FGVC datasets and VTAB-1k benchmark by only updating 0.15M\nparameters.\n", "link": "http://arxiv.org/abs/2412.08341v1", "date": "2024-12-11", "relevancy": 2.0822, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5122}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ALoRE%3A%20Efficient%20Visual%20Adaptation%20via%20Aggregating%20Low%20Rank%20Experts&body=Title%3A%20ALoRE%3A%20Efficient%20Visual%20Adaptation%20via%20Aggregating%20Low%20Rank%20Experts%0AAuthor%3A%20Sinan%20Du%20and%20Guosheng%20Zhang%20and%20Keyao%20Wang%20and%20Yuanrui%20Wang%20and%20Haixiao%20Yue%20and%20Gang%20Zhang%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Zhengzhuo%20Xu%20and%20Chun%20Yuan%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20has%20become%20a%20promising%20paradigm%0Afor%20adapting%20large-scale%20vision%20foundation%20models%20to%20downstream%20tasks.%20Typical%0Amethods%20primarily%20leverage%20the%20intrinsic%20low%20rank%20property%20to%20make%0Adecomposition%2C%20learning%20task-specific%20weights%20while%20compressing%20parameter%20size.%0AHowever%2C%20such%20approaches%20predominantly%20manipulate%20within%20the%20original%20feature%0Aspace%20utilizing%20a%20single-branch%20structure%2C%20which%20might%20be%20suboptimal%20for%0Adecoupling%20the%20learned%20representations%20and%20patterns.%20In%20this%20paper%2C%20we%20propose%0AALoRE%2C%20a%20novel%20PETL%20method%20that%20reuses%20the%20hypercomplex%20parameterized%20space%0Aconstructed%20by%20Kronecker%20product%20to%20Aggregate%20Low%20Rank%20Experts%20using%20a%0Amulti-branch%20paradigm%2C%20disentangling%20the%20learned%20cognitive%20patterns%20during%0Atraining.%20Thanks%20to%20the%20artful%20design%2C%20ALoRE%20maintains%20negligible%20extra%0Aparameters%20and%20can%20be%20effortlessly%20merged%20into%20the%20frozen%20backbone%20via%0Are-parameterization%20in%20a%20sequential%20manner%2C%20avoiding%20additional%20inference%0Alatency.%20We%20conduct%20extensive%20experiments%20on%2024%20image%20classification%20tasks%0Ausing%20various%20backbone%20variants.%20Experimental%20results%20demonstrate%20that%20ALoRE%0Aoutperforms%20the%20full%20fine-tuning%20strategy%20and%20other%20state-of-the-art%20PETL%0Amethods%20in%20terms%20of%20performance%20and%20parameter%20efficiency.%20For%20instance%2C%20ALoRE%0Aobtains%203.06%25%20and%209.97%25%20Top-1%20accuracy%20improvement%20on%20average%20compared%20to%20full%0Afine-tuning%20on%20the%20FGVC%20datasets%20and%20VTAB-1k%20benchmark%20by%20only%20updating%200.15M%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DALoRE%253A%2520Efficient%2520Visual%2520Adaptation%2520via%2520Aggregating%2520Low%2520Rank%2520Experts%26entry.906535625%3DSinan%2520Du%2520and%2520Guosheng%2520Zhang%2520and%2520Keyao%2520Wang%2520and%2520Yuanrui%2520Wang%2520and%2520Haixiao%2520Yue%2520and%2520Gang%2520Zhang%2520and%2520Errui%2520Ding%2520and%2520Jingdong%2520Wang%2520and%2520Zhengzhuo%2520Xu%2520and%2520Chun%2520Yuan%26entry.1292438233%3D%2520%2520Parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520has%2520become%2520a%2520promising%2520paradigm%250Afor%2520adapting%2520large-scale%2520vision%2520foundation%2520models%2520to%2520downstream%2520tasks.%2520Typical%250Amethods%2520primarily%2520leverage%2520the%2520intrinsic%2520low%2520rank%2520property%2520to%2520make%250Adecomposition%252C%2520learning%2520task-specific%2520weights%2520while%2520compressing%2520parameter%2520size.%250AHowever%252C%2520such%2520approaches%2520predominantly%2520manipulate%2520within%2520the%2520original%2520feature%250Aspace%2520utilizing%2520a%2520single-branch%2520structure%252C%2520which%2520might%2520be%2520suboptimal%2520for%250Adecoupling%2520the%2520learned%2520representations%2520and%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520propose%250AALoRE%252C%2520a%2520novel%2520PETL%2520method%2520that%2520reuses%2520the%2520hypercomplex%2520parameterized%2520space%250Aconstructed%2520by%2520Kronecker%2520product%2520to%2520Aggregate%2520Low%2520Rank%2520Experts%2520using%2520a%250Amulti-branch%2520paradigm%252C%2520disentangling%2520the%2520learned%2520cognitive%2520patterns%2520during%250Atraining.%2520Thanks%2520to%2520the%2520artful%2520design%252C%2520ALoRE%2520maintains%2520negligible%2520extra%250Aparameters%2520and%2520can%2520be%2520effortlessly%2520merged%2520into%2520the%2520frozen%2520backbone%2520via%250Are-parameterization%2520in%2520a%2520sequential%2520manner%252C%2520avoiding%2520additional%2520inference%250Alatency.%2520We%2520conduct%2520extensive%2520experiments%2520on%252024%2520image%2520classification%2520tasks%250Ausing%2520various%2520backbone%2520variants.%2520Experimental%2520results%2520demonstrate%2520that%2520ALoRE%250Aoutperforms%2520the%2520full%2520fine-tuning%2520strategy%2520and%2520other%2520state-of-the-art%2520PETL%250Amethods%2520in%2520terms%2520of%2520performance%2520and%2520parameter%2520efficiency.%2520For%2520instance%252C%2520ALoRE%250Aobtains%25203.06%2525%2520and%25209.97%2525%2520Top-1%2520accuracy%2520improvement%2520on%2520average%2520compared%2520to%2520full%250Afine-tuning%2520on%2520the%2520FGVC%2520datasets%2520and%2520VTAB-1k%2520benchmark%2520by%2520only%2520updating%25200.15M%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALoRE%3A%20Efficient%20Visual%20Adaptation%20via%20Aggregating%20Low%20Rank%20Experts&entry.906535625=Sinan%20Du%20and%20Guosheng%20Zhang%20and%20Keyao%20Wang%20and%20Yuanrui%20Wang%20and%20Haixiao%20Yue%20and%20Gang%20Zhang%20and%20Errui%20Ding%20and%20Jingdong%20Wang%20and%20Zhengzhuo%20Xu%20and%20Chun%20Yuan&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20has%20become%20a%20promising%20paradigm%0Afor%20adapting%20large-scale%20vision%20foundation%20models%20to%20downstream%20tasks.%20Typical%0Amethods%20primarily%20leverage%20the%20intrinsic%20low%20rank%20property%20to%20make%0Adecomposition%2C%20learning%20task-specific%20weights%20while%20compressing%20parameter%20size.%0AHowever%2C%20such%20approaches%20predominantly%20manipulate%20within%20the%20original%20feature%0Aspace%20utilizing%20a%20single-branch%20structure%2C%20which%20might%20be%20suboptimal%20for%0Adecoupling%20the%20learned%20representations%20and%20patterns.%20In%20this%20paper%2C%20we%20propose%0AALoRE%2C%20a%20novel%20PETL%20method%20that%20reuses%20the%20hypercomplex%20parameterized%20space%0Aconstructed%20by%20Kronecker%20product%20to%20Aggregate%20Low%20Rank%20Experts%20using%20a%0Amulti-branch%20paradigm%2C%20disentangling%20the%20learned%20cognitive%20patterns%20during%0Atraining.%20Thanks%20to%20the%20artful%20design%2C%20ALoRE%20maintains%20negligible%20extra%0Aparameters%20and%20can%20be%20effortlessly%20merged%20into%20the%20frozen%20backbone%20via%0Are-parameterization%20in%20a%20sequential%20manner%2C%20avoiding%20additional%20inference%0Alatency.%20We%20conduct%20extensive%20experiments%20on%2024%20image%20classification%20tasks%0Ausing%20various%20backbone%20variants.%20Experimental%20results%20demonstrate%20that%20ALoRE%0Aoutperforms%20the%20full%20fine-tuning%20strategy%20and%20other%20state-of-the-art%20PETL%0Amethods%20in%20terms%20of%20performance%20and%20parameter%20efficiency.%20For%20instance%2C%20ALoRE%0Aobtains%203.06%25%20and%209.97%25%20Top-1%20accuracy%20improvement%20on%20average%20compared%20to%20full%0Afine-tuning%20on%20the%20FGVC%20datasets%20and%20VTAB-1k%20benchmark%20by%20only%20updating%200.15M%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08341v1&entry.124074799=Read"},
{"title": "Physics-Inspired Synthesized Underwater Image Dataset", "author": "Reina Kaneko and Takumi Ueda and Hiroshi Higashi and Yuichi Tanaka", "abstract": "  This paper introduces the physics-inspired synthesized underwater image\ndataset (PHISWID), a dataset tailored for enhancing underwater image processing\nthrough physics-inspired image synthesis. For underwater image enhancement,\ndata-driven approaches (e.g., deep neural networks) typically demand extensive\ndatasets, yet acquiring paired clean and degraded underwater images poses\nsignificant challenges. Existing datasets have limited contributions to image\nenhancement due to lack of physics models, publicity, and ground-truth images.\nPHISWID addresses these issues by offering a set of paired ground-truth\n(atmospheric) and underwater images synthetically degraded by color degradation\nand marine snow artifacts. Generating underwater images from atmospheric RGB-D\nimages based on physical models provides pairs of real-world ground-truth and\ndegraded images. Our synthetic approach generates a large quantity of the\npairs, enabling effective training of deep neural networks and objective image\nquality assessment. Through benchmark experiment with some datasets and image\nenhance methods, we validate that our dataset can improve the image enhancement\nperformance. Our dataset, which is publicly available, contributes to the\ndevelopment in underwater image processing.\n", "link": "http://arxiv.org/abs/2404.03998v2", "date": "2024-12-11", "relevancy": 2.0875, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5561}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4982}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Inspired%20Synthesized%20Underwater%20Image%20Dataset&body=Title%3A%20Physics-Inspired%20Synthesized%20Underwater%20Image%20Dataset%0AAuthor%3A%20Reina%20Kaneko%20and%20Takumi%20Ueda%20and%20Hiroshi%20Higashi%20and%20Yuichi%20Tanaka%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20physics-inspired%20synthesized%20underwater%20image%0Adataset%20%28PHISWID%29%2C%20a%20dataset%20tailored%20for%20enhancing%20underwater%20image%20processing%0Athrough%20physics-inspired%20image%20synthesis.%20For%20underwater%20image%20enhancement%2C%0Adata-driven%20approaches%20%28e.g.%2C%20deep%20neural%20networks%29%20typically%20demand%20extensive%0Adatasets%2C%20yet%20acquiring%20paired%20clean%20and%20degraded%20underwater%20images%20poses%0Asignificant%20challenges.%20Existing%20datasets%20have%20limited%20contributions%20to%20image%0Aenhancement%20due%20to%20lack%20of%20physics%20models%2C%20publicity%2C%20and%20ground-truth%20images.%0APHISWID%20addresses%20these%20issues%20by%20offering%20a%20set%20of%20paired%20ground-truth%0A%28atmospheric%29%20and%20underwater%20images%20synthetically%20degraded%20by%20color%20degradation%0Aand%20marine%20snow%20artifacts.%20Generating%20underwater%20images%20from%20atmospheric%20RGB-D%0Aimages%20based%20on%20physical%20models%20provides%20pairs%20of%20real-world%20ground-truth%20and%0Adegraded%20images.%20Our%20synthetic%20approach%20generates%20a%20large%20quantity%20of%20the%0Apairs%2C%20enabling%20effective%20training%20of%20deep%20neural%20networks%20and%20objective%20image%0Aquality%20assessment.%20Through%20benchmark%20experiment%20with%20some%20datasets%20and%20image%0Aenhance%20methods%2C%20we%20validate%20that%20our%20dataset%20can%20improve%20the%20image%20enhancement%0Aperformance.%20Our%20dataset%2C%20which%20is%20publicly%20available%2C%20contributes%20to%20the%0Adevelopment%20in%20underwater%20image%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Inspired%2520Synthesized%2520Underwater%2520Image%2520Dataset%26entry.906535625%3DReina%2520Kaneko%2520and%2520Takumi%2520Ueda%2520and%2520Hiroshi%2520Higashi%2520and%2520Yuichi%2520Tanaka%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520physics-inspired%2520synthesized%2520underwater%2520image%250Adataset%2520%2528PHISWID%2529%252C%2520a%2520dataset%2520tailored%2520for%2520enhancing%2520underwater%2520image%2520processing%250Athrough%2520physics-inspired%2520image%2520synthesis.%2520For%2520underwater%2520image%2520enhancement%252C%250Adata-driven%2520approaches%2520%2528e.g.%252C%2520deep%2520neural%2520networks%2529%2520typically%2520demand%2520extensive%250Adatasets%252C%2520yet%2520acquiring%2520paired%2520clean%2520and%2520degraded%2520underwater%2520images%2520poses%250Asignificant%2520challenges.%2520Existing%2520datasets%2520have%2520limited%2520contributions%2520to%2520image%250Aenhancement%2520due%2520to%2520lack%2520of%2520physics%2520models%252C%2520publicity%252C%2520and%2520ground-truth%2520images.%250APHISWID%2520addresses%2520these%2520issues%2520by%2520offering%2520a%2520set%2520of%2520paired%2520ground-truth%250A%2528atmospheric%2529%2520and%2520underwater%2520images%2520synthetically%2520degraded%2520by%2520color%2520degradation%250Aand%2520marine%2520snow%2520artifacts.%2520Generating%2520underwater%2520images%2520from%2520atmospheric%2520RGB-D%250Aimages%2520based%2520on%2520physical%2520models%2520provides%2520pairs%2520of%2520real-world%2520ground-truth%2520and%250Adegraded%2520images.%2520Our%2520synthetic%2520approach%2520generates%2520a%2520large%2520quantity%2520of%2520the%250Apairs%252C%2520enabling%2520effective%2520training%2520of%2520deep%2520neural%2520networks%2520and%2520objective%2520image%250Aquality%2520assessment.%2520Through%2520benchmark%2520experiment%2520with%2520some%2520datasets%2520and%2520image%250Aenhance%2520methods%252C%2520we%2520validate%2520that%2520our%2520dataset%2520can%2520improve%2520the%2520image%2520enhancement%250Aperformance.%2520Our%2520dataset%252C%2520which%2520is%2520publicly%2520available%252C%2520contributes%2520to%2520the%250Adevelopment%2520in%2520underwater%2520image%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Inspired%20Synthesized%20Underwater%20Image%20Dataset&entry.906535625=Reina%20Kaneko%20and%20Takumi%20Ueda%20and%20Hiroshi%20Higashi%20and%20Yuichi%20Tanaka&entry.1292438233=%20%20This%20paper%20introduces%20the%20physics-inspired%20synthesized%20underwater%20image%0Adataset%20%28PHISWID%29%2C%20a%20dataset%20tailored%20for%20enhancing%20underwater%20image%20processing%0Athrough%20physics-inspired%20image%20synthesis.%20For%20underwater%20image%20enhancement%2C%0Adata-driven%20approaches%20%28e.g.%2C%20deep%20neural%20networks%29%20typically%20demand%20extensive%0Adatasets%2C%20yet%20acquiring%20paired%20clean%20and%20degraded%20underwater%20images%20poses%0Asignificant%20challenges.%20Existing%20datasets%20have%20limited%20contributions%20to%20image%0Aenhancement%20due%20to%20lack%20of%20physics%20models%2C%20publicity%2C%20and%20ground-truth%20images.%0APHISWID%20addresses%20these%20issues%20by%20offering%20a%20set%20of%20paired%20ground-truth%0A%28atmospheric%29%20and%20underwater%20images%20synthetically%20degraded%20by%20color%20degradation%0Aand%20marine%20snow%20artifacts.%20Generating%20underwater%20images%20from%20atmospheric%20RGB-D%0Aimages%20based%20on%20physical%20models%20provides%20pairs%20of%20real-world%20ground-truth%20and%0Adegraded%20images.%20Our%20synthetic%20approach%20generates%20a%20large%20quantity%20of%20the%0Apairs%2C%20enabling%20effective%20training%20of%20deep%20neural%20networks%20and%20objective%20image%0Aquality%20assessment.%20Through%20benchmark%20experiment%20with%20some%20datasets%20and%20image%0Aenhance%20methods%2C%20we%20validate%20that%20our%20dataset%20can%20improve%20the%20image%20enhancement%0Aperformance.%20Our%20dataset%2C%20which%20is%20publicly%20available%2C%20contributes%20to%20the%0Adevelopment%20in%20underwater%20image%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03998v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


