<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240910.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MVGaussian: High-Fidelity text-to-3D Content Generation with Multi-View\n  Guidance and Surface Densification", "author": "Phu Pham and Aradhya N. Mathur and Ojaswa Sharma and Aniket Bera", "abstract": "  The field of text-to-3D content generation has made significant progress in\ngenerating realistic 3D objects, with existing methodologies like Score\nDistillation Sampling (SDS) offering promising guidance. However, these methods\noften encounter the \"Janus\" problem-multi-face ambiguities due to imprecise\nguidance. Additionally, while recent advancements in 3D gaussian splitting have\nshown its efficacy in representing 3D volumes, optimization of this\nrepresentation remains largely unexplored. This paper introduces a unified\nframework for text-to-3D content generation that addresses these critical gaps.\nOur approach utilizes multi-view guidance to iteratively form the structure of\nthe 3D model, progressively enhancing detail and accuracy. We also introduce a\nnovel densification algorithm that aligns gaussians close to the surface,\noptimizing the structural integrity and fidelity of the generated models.\nExtensive experiments validate our approach, demonstrating that it produces\nhigh-quality visual outputs with minimal time cost. Notably, our method\nachieves high-quality results within half an hour of training, offering a\nsubstantial efficiency gain over most existing methods, which require hours of\ntraining time to achieve comparable results.\n", "link": "http://arxiv.org/abs/2409.06620v1", "date": "2024-09-10", "relevancy": 3.3537, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6959}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6581}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVGaussian%3A%20High-Fidelity%20text-to-3D%20Content%20Generation%20with%20Multi-View%0A%20%20Guidance%20and%20Surface%20Densification&body=Title%3A%20MVGaussian%3A%20High-Fidelity%20text-to-3D%20Content%20Generation%20with%20Multi-View%0A%20%20Guidance%20and%20Surface%20Densification%0AAuthor%3A%20Phu%20Pham%20and%20Aradhya%20N.%20Mathur%20and%20Ojaswa%20Sharma%20and%20Aniket%20Bera%0AAbstract%3A%20%20%20The%20field%20of%20text-to-3D%20content%20generation%20has%20made%20significant%20progress%20in%0Agenerating%20realistic%203D%20objects%2C%20with%20existing%20methodologies%20like%20Score%0ADistillation%20Sampling%20%28SDS%29%20offering%20promising%20guidance.%20However%2C%20these%20methods%0Aoften%20encounter%20the%20%22Janus%22%20problem-multi-face%20ambiguities%20due%20to%20imprecise%0Aguidance.%20Additionally%2C%20while%20recent%20advancements%20in%203D%20gaussian%20splitting%20have%0Ashown%20its%20efficacy%20in%20representing%203D%20volumes%2C%20optimization%20of%20this%0Arepresentation%20remains%20largely%20unexplored.%20This%20paper%20introduces%20a%20unified%0Aframework%20for%20text-to-3D%20content%20generation%20that%20addresses%20these%20critical%20gaps.%0AOur%20approach%20utilizes%20multi-view%20guidance%20to%20iteratively%20form%20the%20structure%20of%0Athe%203D%20model%2C%20progressively%20enhancing%20detail%20and%20accuracy.%20We%20also%20introduce%20a%0Anovel%20densification%20algorithm%20that%20aligns%20gaussians%20close%20to%20the%20surface%2C%0Aoptimizing%20the%20structural%20integrity%20and%20fidelity%20of%20the%20generated%20models.%0AExtensive%20experiments%20validate%20our%20approach%2C%20demonstrating%20that%20it%20produces%0Ahigh-quality%20visual%20outputs%20with%20minimal%20time%20cost.%20Notably%2C%20our%20method%0Aachieves%20high-quality%20results%20within%20half%20an%20hour%20of%20training%2C%20offering%20a%0Asubstantial%20efficiency%20gain%20over%20most%20existing%20methods%2C%20which%20require%20hours%20of%0Atraining%20time%20to%20achieve%20comparable%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVGaussian%253A%2520High-Fidelity%2520text-to-3D%2520Content%2520Generation%2520with%2520Multi-View%250A%2520%2520Guidance%2520and%2520Surface%2520Densification%26entry.906535625%3DPhu%2520Pham%2520and%2520Aradhya%2520N.%2520Mathur%2520and%2520Ojaswa%2520Sharma%2520and%2520Aniket%2520Bera%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520text-to-3D%2520content%2520generation%2520has%2520made%2520significant%2520progress%2520in%250Agenerating%2520realistic%25203D%2520objects%252C%2520with%2520existing%2520methodologies%2520like%2520Score%250ADistillation%2520Sampling%2520%2528SDS%2529%2520offering%2520promising%2520guidance.%2520However%252C%2520these%2520methods%250Aoften%2520encounter%2520the%2520%2522Janus%2522%2520problem-multi-face%2520ambiguities%2520due%2520to%2520imprecise%250Aguidance.%2520Additionally%252C%2520while%2520recent%2520advancements%2520in%25203D%2520gaussian%2520splitting%2520have%250Ashown%2520its%2520efficacy%2520in%2520representing%25203D%2520volumes%252C%2520optimization%2520of%2520this%250Arepresentation%2520remains%2520largely%2520unexplored.%2520This%2520paper%2520introduces%2520a%2520unified%250Aframework%2520for%2520text-to-3D%2520content%2520generation%2520that%2520addresses%2520these%2520critical%2520gaps.%250AOur%2520approach%2520utilizes%2520multi-view%2520guidance%2520to%2520iteratively%2520form%2520the%2520structure%2520of%250Athe%25203D%2520model%252C%2520progressively%2520enhancing%2520detail%2520and%2520accuracy.%2520We%2520also%2520introduce%2520a%250Anovel%2520densification%2520algorithm%2520that%2520aligns%2520gaussians%2520close%2520to%2520the%2520surface%252C%250Aoptimizing%2520the%2520structural%2520integrity%2520and%2520fidelity%2520of%2520the%2520generated%2520models.%250AExtensive%2520experiments%2520validate%2520our%2520approach%252C%2520demonstrating%2520that%2520it%2520produces%250Ahigh-quality%2520visual%2520outputs%2520with%2520minimal%2520time%2520cost.%2520Notably%252C%2520our%2520method%250Aachieves%2520high-quality%2520results%2520within%2520half%2520an%2520hour%2520of%2520training%252C%2520offering%2520a%250Asubstantial%2520efficiency%2520gain%2520over%2520most%2520existing%2520methods%252C%2520which%2520require%2520hours%2520of%250Atraining%2520time%2520to%2520achieve%2520comparable%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVGaussian%3A%20High-Fidelity%20text-to-3D%20Content%20Generation%20with%20Multi-View%0A%20%20Guidance%20and%20Surface%20Densification&entry.906535625=Phu%20Pham%20and%20Aradhya%20N.%20Mathur%20and%20Ojaswa%20Sharma%20and%20Aniket%20Bera&entry.1292438233=%20%20The%20field%20of%20text-to-3D%20content%20generation%20has%20made%20significant%20progress%20in%0Agenerating%20realistic%203D%20objects%2C%20with%20existing%20methodologies%20like%20Score%0ADistillation%20Sampling%20%28SDS%29%20offering%20promising%20guidance.%20However%2C%20these%20methods%0Aoften%20encounter%20the%20%22Janus%22%20problem-multi-face%20ambiguities%20due%20to%20imprecise%0Aguidance.%20Additionally%2C%20while%20recent%20advancements%20in%203D%20gaussian%20splitting%20have%0Ashown%20its%20efficacy%20in%20representing%203D%20volumes%2C%20optimization%20of%20this%0Arepresentation%20remains%20largely%20unexplored.%20This%20paper%20introduces%20a%20unified%0Aframework%20for%20text-to-3D%20content%20generation%20that%20addresses%20these%20critical%20gaps.%0AOur%20approach%20utilizes%20multi-view%20guidance%20to%20iteratively%20form%20the%20structure%20of%0Athe%203D%20model%2C%20progressively%20enhancing%20detail%20and%20accuracy.%20We%20also%20introduce%20a%0Anovel%20densification%20algorithm%20that%20aligns%20gaussians%20close%20to%20the%20surface%2C%0Aoptimizing%20the%20structural%20integrity%20and%20fidelity%20of%20the%20generated%20models.%0AExtensive%20experiments%20validate%20our%20approach%2C%20demonstrating%20that%20it%20produces%0Ahigh-quality%20visual%20outputs%20with%20minimal%20time%20cost.%20Notably%2C%20our%20method%0Aachieves%20high-quality%20results%20within%20half%20an%20hour%20of%20training%2C%20offering%20a%0Asubstantial%20efficiency%20gain%20over%20most%20existing%20methods%2C%20which%20require%20hours%20of%0Atraining%20time%20to%20achieve%20comparable%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06620v1&entry.124074799=Read"},
{"title": "GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface\n  Reconstruction", "author": "Junyi Chen and Weicai Ye and Yifan Wang and Danpeng Chen and Di Huang and Wanli Ouyang and Guofeng Zhang and Yu Qiao and Tong He", "abstract": "  3D Gaussian Splatting (3DGS) has shown promising performance in novel view\nsynthesis. Previous methods adapt it to obtaining surfaces of either individual\n3D objects or within limited scenes. In this paper, we make the first attempt\nto tackle the challenging task of large-scale scene surface reconstruction.\nThis task is particularly difficult due to the high GPU memory consumption,\ndifferent levels of details for geometric representation, and noticeable\ninconsistencies in appearance. To this end, we propose GigaGS, the first work\nfor high-quality surface reconstruction for large-scale scenes using 3DGS.\nGigaGS first applies a partitioning strategy based on the mutual visibility of\nspatial regions, which effectively grouping cameras for parallel processing. To\nenhance the quality of the surface, we also propose novel multi-view\nphotometric and geometric consistency constraints based on Level-of-Detail\nrepresentation. In doing so, our method can reconstruct detailed surface\nstructures. Comprehensive experiments are conducted on various datasets. The\nconsistent improvement demonstrates the superiority of GigaGS.\n", "link": "http://arxiv.org/abs/2409.06685v1", "date": "2024-09-10", "relevancy": 3.3515, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7546}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6825}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GigaGS%3A%20Scaling%20up%20Planar-Based%203D%20Gaussians%20for%20Large%20Scene%20Surface%0A%20%20Reconstruction&body=Title%3A%20GigaGS%3A%20Scaling%20up%20Planar-Based%203D%20Gaussians%20for%20Large%20Scene%20Surface%0A%20%20Reconstruction%0AAuthor%3A%20Junyi%20Chen%20and%20Weicai%20Ye%20and%20Yifan%20Wang%20and%20Danpeng%20Chen%20and%20Di%20Huang%20and%20Wanli%20Ouyang%20and%20Guofeng%20Zhang%20and%20Yu%20Qiao%20and%20Tong%20He%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20promising%20performance%20in%20novel%20view%0Asynthesis.%20Previous%20methods%20adapt%20it%20to%20obtaining%20surfaces%20of%20either%20individual%0A3D%20objects%20or%20within%20limited%20scenes.%20In%20this%20paper%2C%20we%20make%20the%20first%20attempt%0Ato%20tackle%20the%20challenging%20task%20of%20large-scale%20scene%20surface%20reconstruction.%0AThis%20task%20is%20particularly%20difficult%20due%20to%20the%20high%20GPU%20memory%20consumption%2C%0Adifferent%20levels%20of%20details%20for%20geometric%20representation%2C%20and%20noticeable%0Ainconsistencies%20in%20appearance.%20To%20this%20end%2C%20we%20propose%20GigaGS%2C%20the%20first%20work%0Afor%20high-quality%20surface%20reconstruction%20for%20large-scale%20scenes%20using%203DGS.%0AGigaGS%20first%20applies%20a%20partitioning%20strategy%20based%20on%20the%20mutual%20visibility%20of%0Aspatial%20regions%2C%20which%20effectively%20grouping%20cameras%20for%20parallel%20processing.%20To%0Aenhance%20the%20quality%20of%20the%20surface%2C%20we%20also%20propose%20novel%20multi-view%0Aphotometric%20and%20geometric%20consistency%20constraints%20based%20on%20Level-of-Detail%0Arepresentation.%20In%20doing%20so%2C%20our%20method%20can%20reconstruct%20detailed%20surface%0Astructures.%20Comprehensive%20experiments%20are%20conducted%20on%20various%20datasets.%20The%0Aconsistent%20improvement%20demonstrates%20the%20superiority%20of%20GigaGS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGigaGS%253A%2520Scaling%2520up%2520Planar-Based%25203D%2520Gaussians%2520for%2520Large%2520Scene%2520Surface%250A%2520%2520Reconstruction%26entry.906535625%3DJunyi%2520Chen%2520and%2520Weicai%2520Ye%2520and%2520Yifan%2520Wang%2520and%2520Danpeng%2520Chen%2520and%2520Di%2520Huang%2520and%2520Wanli%2520Ouyang%2520and%2520Guofeng%2520Zhang%2520and%2520Yu%2520Qiao%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520shown%2520promising%2520performance%2520in%2520novel%2520view%250Asynthesis.%2520Previous%2520methods%2520adapt%2520it%2520to%2520obtaining%2520surfaces%2520of%2520either%2520individual%250A3D%2520objects%2520or%2520within%2520limited%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520make%2520the%2520first%2520attempt%250Ato%2520tackle%2520the%2520challenging%2520task%2520of%2520large-scale%2520scene%2520surface%2520reconstruction.%250AThis%2520task%2520is%2520particularly%2520difficult%2520due%2520to%2520the%2520high%2520GPU%2520memory%2520consumption%252C%250Adifferent%2520levels%2520of%2520details%2520for%2520geometric%2520representation%252C%2520and%2520noticeable%250Ainconsistencies%2520in%2520appearance.%2520To%2520this%2520end%252C%2520we%2520propose%2520GigaGS%252C%2520the%2520first%2520work%250Afor%2520high-quality%2520surface%2520reconstruction%2520for%2520large-scale%2520scenes%2520using%25203DGS.%250AGigaGS%2520first%2520applies%2520a%2520partitioning%2520strategy%2520based%2520on%2520the%2520mutual%2520visibility%2520of%250Aspatial%2520regions%252C%2520which%2520effectively%2520grouping%2520cameras%2520for%2520parallel%2520processing.%2520To%250Aenhance%2520the%2520quality%2520of%2520the%2520surface%252C%2520we%2520also%2520propose%2520novel%2520multi-view%250Aphotometric%2520and%2520geometric%2520consistency%2520constraints%2520based%2520on%2520Level-of-Detail%250Arepresentation.%2520In%2520doing%2520so%252C%2520our%2520method%2520can%2520reconstruct%2520detailed%2520surface%250Astructures.%2520Comprehensive%2520experiments%2520are%2520conducted%2520on%2520various%2520datasets.%2520The%250Aconsistent%2520improvement%2520demonstrates%2520the%2520superiority%2520of%2520GigaGS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GigaGS%3A%20Scaling%20up%20Planar-Based%203D%20Gaussians%20for%20Large%20Scene%20Surface%0A%20%20Reconstruction&entry.906535625=Junyi%20Chen%20and%20Weicai%20Ye%20and%20Yifan%20Wang%20and%20Danpeng%20Chen%20and%20Di%20Huang%20and%20Wanli%20Ouyang%20and%20Guofeng%20Zhang%20and%20Yu%20Qiao%20and%20Tong%20He&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20shown%20promising%20performance%20in%20novel%20view%0Asynthesis.%20Previous%20methods%20adapt%20it%20to%20obtaining%20surfaces%20of%20either%20individual%0A3D%20objects%20or%20within%20limited%20scenes.%20In%20this%20paper%2C%20we%20make%20the%20first%20attempt%0Ato%20tackle%20the%20challenging%20task%20of%20large-scale%20scene%20surface%20reconstruction.%0AThis%20task%20is%20particularly%20difficult%20due%20to%20the%20high%20GPU%20memory%20consumption%2C%0Adifferent%20levels%20of%20details%20for%20geometric%20representation%2C%20and%20noticeable%0Ainconsistencies%20in%20appearance.%20To%20this%20end%2C%20we%20propose%20GigaGS%2C%20the%20first%20work%0Afor%20high-quality%20surface%20reconstruction%20for%20large-scale%20scenes%20using%203DGS.%0AGigaGS%20first%20applies%20a%20partitioning%20strategy%20based%20on%20the%20mutual%20visibility%20of%0Aspatial%20regions%2C%20which%20effectively%20grouping%20cameras%20for%20parallel%20processing.%20To%0Aenhance%20the%20quality%20of%20the%20surface%2C%20we%20also%20propose%20novel%20multi-view%0Aphotometric%20and%20geometric%20consistency%20constraints%20based%20on%20Level-of-Detail%0Arepresentation.%20In%20doing%20so%2C%20our%20method%20can%20reconstruct%20detailed%20surface%0Astructures.%20Comprehensive%20experiments%20are%20conducted%20on%20various%20datasets.%20The%0Aconsistent%20improvement%20demonstrates%20the%20superiority%20of%20GigaGS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06685v1&entry.124074799=Read"},
{"title": "Towards Localizing Structural Elements: Merging Geometrical Detection\n  with Semantic Verification in RGB-D Data", "author": "Ali Tourani and Saad Ejaz and Hriday Bavle and Jose Luis Sanchez-Lopez and Holger Voos", "abstract": "  RGB-D cameras supply rich and dense visual and spatial information for\nvarious robotics tasks such as scene understanding, map reconstruction, and\nlocalization. Integrating depth and visual information can aid robots in\nlocalization and element mapping, advancing applications like 3D scene graph\ngeneration and Visual Simultaneous Localization and Mapping (VSLAM). While\npoint cloud data containing such information is primarily used for enhanced\nscene understanding, exploiting their potential to capture and represent rich\nsemantic information has yet to be adequately targeted. This paper presents a\nreal-time pipeline for localizing building components, including wall and\nground surfaces, by integrating geometric calculations for pure 3D plane\ndetection followed by validating their semantic category using point cloud data\nfrom RGB-D cameras. It has a parallel multi-thread architecture to precisely\nestimate poses and equations of all the planes detected in the environment,\nfilters the ones forming the map structure using a panoptic segmentation\nvalidation, and keeps only the validated building components. Incorporating the\nproposed method into a VSLAM framework confirmed that constraining the map with\nthe detected environment-driven semantic elements can improve scene\nunderstanding and map reconstruction accuracy. It can also ensure\n(re-)association of these detected components into a unified 3D scene graph,\nbridging the gap between geometric accuracy and semantic understanding.\nAdditionally, the pipeline allows for the detection of potential higher-level\nstructural entities, such as rooms, by identifying the relationships between\nbuilding components based on their layout.\n", "link": "http://arxiv.org/abs/2409.06625v1", "date": "2024-09-10", "relevancy": 3.1604, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6438}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6337}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Localizing%20Structural%20Elements%3A%20Merging%20Geometrical%20Detection%0A%20%20with%20Semantic%20Verification%20in%20RGB-D%20Data&body=Title%3A%20Towards%20Localizing%20Structural%20Elements%3A%20Merging%20Geometrical%20Detection%0A%20%20with%20Semantic%20Verification%20in%20RGB-D%20Data%0AAuthor%3A%20Ali%20Tourani%20and%20Saad%20Ejaz%20and%20Hriday%20Bavle%20and%20Jose%20Luis%20Sanchez-Lopez%20and%20Holger%20Voos%0AAbstract%3A%20%20%20RGB-D%20cameras%20supply%20rich%20and%20dense%20visual%20and%20spatial%20information%20for%0Avarious%20robotics%20tasks%20such%20as%20scene%20understanding%2C%20map%20reconstruction%2C%20and%0Alocalization.%20Integrating%20depth%20and%20visual%20information%20can%20aid%20robots%20in%0Alocalization%20and%20element%20mapping%2C%20advancing%20applications%20like%203D%20scene%20graph%0Ageneration%20and%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28VSLAM%29.%20While%0Apoint%20cloud%20data%20containing%20such%20information%20is%20primarily%20used%20for%20enhanced%0Ascene%20understanding%2C%20exploiting%20their%20potential%20to%20capture%20and%20represent%20rich%0Asemantic%20information%20has%20yet%20to%20be%20adequately%20targeted.%20This%20paper%20presents%20a%0Areal-time%20pipeline%20for%20localizing%20building%20components%2C%20including%20wall%20and%0Aground%20surfaces%2C%20by%20integrating%20geometric%20calculations%20for%20pure%203D%20plane%0Adetection%20followed%20by%20validating%20their%20semantic%20category%20using%20point%20cloud%20data%0Afrom%20RGB-D%20cameras.%20It%20has%20a%20parallel%20multi-thread%20architecture%20to%20precisely%0Aestimate%20poses%20and%20equations%20of%20all%20the%20planes%20detected%20in%20the%20environment%2C%0Afilters%20the%20ones%20forming%20the%20map%20structure%20using%20a%20panoptic%20segmentation%0Avalidation%2C%20and%20keeps%20only%20the%20validated%20building%20components.%20Incorporating%20the%0Aproposed%20method%20into%20a%20VSLAM%20framework%20confirmed%20that%20constraining%20the%20map%20with%0Athe%20detected%20environment-driven%20semantic%20elements%20can%20improve%20scene%0Aunderstanding%20and%20map%20reconstruction%20accuracy.%20It%20can%20also%20ensure%0A%28re-%29association%20of%20these%20detected%20components%20into%20a%20unified%203D%20scene%20graph%2C%0Abridging%20the%20gap%20between%20geometric%20accuracy%20and%20semantic%20understanding.%0AAdditionally%2C%20the%20pipeline%20allows%20for%20the%20detection%20of%20potential%20higher-level%0Astructural%20entities%2C%20such%20as%20rooms%2C%20by%20identifying%20the%20relationships%20between%0Abuilding%20components%20based%20on%20their%20layout.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Localizing%2520Structural%2520Elements%253A%2520Merging%2520Geometrical%2520Detection%250A%2520%2520with%2520Semantic%2520Verification%2520in%2520RGB-D%2520Data%26entry.906535625%3DAli%2520Tourani%2520and%2520Saad%2520Ejaz%2520and%2520Hriday%2520Bavle%2520and%2520Jose%2520Luis%2520Sanchez-Lopez%2520and%2520Holger%2520Voos%26entry.1292438233%3D%2520%2520RGB-D%2520cameras%2520supply%2520rich%2520and%2520dense%2520visual%2520and%2520spatial%2520information%2520for%250Avarious%2520robotics%2520tasks%2520such%2520as%2520scene%2520understanding%252C%2520map%2520reconstruction%252C%2520and%250Alocalization.%2520Integrating%2520depth%2520and%2520visual%2520information%2520can%2520aid%2520robots%2520in%250Alocalization%2520and%2520element%2520mapping%252C%2520advancing%2520applications%2520like%25203D%2520scene%2520graph%250Ageneration%2520and%2520Visual%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528VSLAM%2529.%2520While%250Apoint%2520cloud%2520data%2520containing%2520such%2520information%2520is%2520primarily%2520used%2520for%2520enhanced%250Ascene%2520understanding%252C%2520exploiting%2520their%2520potential%2520to%2520capture%2520and%2520represent%2520rich%250Asemantic%2520information%2520has%2520yet%2520to%2520be%2520adequately%2520targeted.%2520This%2520paper%2520presents%2520a%250Areal-time%2520pipeline%2520for%2520localizing%2520building%2520components%252C%2520including%2520wall%2520and%250Aground%2520surfaces%252C%2520by%2520integrating%2520geometric%2520calculations%2520for%2520pure%25203D%2520plane%250Adetection%2520followed%2520by%2520validating%2520their%2520semantic%2520category%2520using%2520point%2520cloud%2520data%250Afrom%2520RGB-D%2520cameras.%2520It%2520has%2520a%2520parallel%2520multi-thread%2520architecture%2520to%2520precisely%250Aestimate%2520poses%2520and%2520equations%2520of%2520all%2520the%2520planes%2520detected%2520in%2520the%2520environment%252C%250Afilters%2520the%2520ones%2520forming%2520the%2520map%2520structure%2520using%2520a%2520panoptic%2520segmentation%250Avalidation%252C%2520and%2520keeps%2520only%2520the%2520validated%2520building%2520components.%2520Incorporating%2520the%250Aproposed%2520method%2520into%2520a%2520VSLAM%2520framework%2520confirmed%2520that%2520constraining%2520the%2520map%2520with%250Athe%2520detected%2520environment-driven%2520semantic%2520elements%2520can%2520improve%2520scene%250Aunderstanding%2520and%2520map%2520reconstruction%2520accuracy.%2520It%2520can%2520also%2520ensure%250A%2528re-%2529association%2520of%2520these%2520detected%2520components%2520into%2520a%2520unified%25203D%2520scene%2520graph%252C%250Abridging%2520the%2520gap%2520between%2520geometric%2520accuracy%2520and%2520semantic%2520understanding.%250AAdditionally%252C%2520the%2520pipeline%2520allows%2520for%2520the%2520detection%2520of%2520potential%2520higher-level%250Astructural%2520entities%252C%2520such%2520as%2520rooms%252C%2520by%2520identifying%2520the%2520relationships%2520between%250Abuilding%2520components%2520based%2520on%2520their%2520layout.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Localizing%20Structural%20Elements%3A%20Merging%20Geometrical%20Detection%0A%20%20with%20Semantic%20Verification%20in%20RGB-D%20Data&entry.906535625=Ali%20Tourani%20and%20Saad%20Ejaz%20and%20Hriday%20Bavle%20and%20Jose%20Luis%20Sanchez-Lopez%20and%20Holger%20Voos&entry.1292438233=%20%20RGB-D%20cameras%20supply%20rich%20and%20dense%20visual%20and%20spatial%20information%20for%0Avarious%20robotics%20tasks%20such%20as%20scene%20understanding%2C%20map%20reconstruction%2C%20and%0Alocalization.%20Integrating%20depth%20and%20visual%20information%20can%20aid%20robots%20in%0Alocalization%20and%20element%20mapping%2C%20advancing%20applications%20like%203D%20scene%20graph%0Ageneration%20and%20Visual%20Simultaneous%20Localization%20and%20Mapping%20%28VSLAM%29.%20While%0Apoint%20cloud%20data%20containing%20such%20information%20is%20primarily%20used%20for%20enhanced%0Ascene%20understanding%2C%20exploiting%20their%20potential%20to%20capture%20and%20represent%20rich%0Asemantic%20information%20has%20yet%20to%20be%20adequately%20targeted.%20This%20paper%20presents%20a%0Areal-time%20pipeline%20for%20localizing%20building%20components%2C%20including%20wall%20and%0Aground%20surfaces%2C%20by%20integrating%20geometric%20calculations%20for%20pure%203D%20plane%0Adetection%20followed%20by%20validating%20their%20semantic%20category%20using%20point%20cloud%20data%0Afrom%20RGB-D%20cameras.%20It%20has%20a%20parallel%20multi-thread%20architecture%20to%20precisely%0Aestimate%20poses%20and%20equations%20of%20all%20the%20planes%20detected%20in%20the%20environment%2C%0Afilters%20the%20ones%20forming%20the%20map%20structure%20using%20a%20panoptic%20segmentation%0Avalidation%2C%20and%20keeps%20only%20the%20validated%20building%20components.%20Incorporating%20the%0Aproposed%20method%20into%20a%20VSLAM%20framework%20confirmed%20that%20constraining%20the%20map%20with%0Athe%20detected%20environment-driven%20semantic%20elements%20can%20improve%20scene%0Aunderstanding%20and%20map%20reconstruction%20accuracy.%20It%20can%20also%20ensure%0A%28re-%29association%20of%20these%20detected%20components%20into%20a%20unified%203D%20scene%20graph%2C%0Abridging%20the%20gap%20between%20geometric%20accuracy%20and%20semantic%20understanding.%0AAdditionally%2C%20the%20pipeline%20allows%20for%20the%20detection%20of%20potential%20higher-level%0Astructural%20entities%2C%20such%20as%20rooms%2C%20by%20identifying%20the%20relationships%20between%0Abuilding%20components%20based%20on%20their%20layout.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06625v1&entry.124074799=Read"},
{"title": "Weakly-supervised Camera Localization by Ground-to-satellite Image\n  Registration", "author": "Yujiao Shi and Hongdong Li and Akhil Perincherry and Ankit Vora", "abstract": "  The ground-to-satellite image matching/retrieval was initially proposed for\ncity-scale ground camera localization. This work addresses the problem of\nimproving camera pose accuracy by ground-to-satellite image matching after a\ncoarse location and orientation have been obtained, either from the city-scale\nretrieval or from consumer-level GPS and compass sensors. Existing\nlearning-based methods for solving this task require accurate GPS labels of\nground images for network training. However, obtaining such accurate GPS labels\nis difficult, often requiring an expensive {\\color{black}Real Time Kinematics\n(RTK)} setup and suffering from signal occlusion, multi-path signal\ndisruptions, \\etc. To alleviate this issue, this paper proposes a weakly\nsupervised learning strategy for ground-to-satellite image registration when\nonly noisy pose labels for ground images are available for network training. It\nderives positive and negative satellite images for each ground image and\nleverages contrastive learning to learn feature representations for ground and\nsatellite images useful for translation estimation. We also propose a\nself-supervision strategy for cross-view image relative rotation estimation,\nwhich trains the network by creating pseudo query and reference image pairs.\nExperimental results show that our weakly supervised learning strategy achieves\nthe best performance on cross-area evaluation compared to recent\nstate-of-the-art methods that are reliant on accurate pose labels for\nsupervision.\n", "link": "http://arxiv.org/abs/2409.06471v1", "date": "2024-09-10", "relevancy": 3.1334, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6477}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.619}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly-supervised%20Camera%20Localization%20by%20Ground-to-satellite%20Image%0A%20%20Registration&body=Title%3A%20Weakly-supervised%20Camera%20Localization%20by%20Ground-to-satellite%20Image%0A%20%20Registration%0AAuthor%3A%20Yujiao%20Shi%20and%20Hongdong%20Li%20and%20Akhil%20Perincherry%20and%20Ankit%20Vora%0AAbstract%3A%20%20%20The%20ground-to-satellite%20image%20matching/retrieval%20was%20initially%20proposed%20for%0Acity-scale%20ground%20camera%20localization.%20This%20work%20addresses%20the%20problem%20of%0Aimproving%20camera%20pose%20accuracy%20by%20ground-to-satellite%20image%20matching%20after%20a%0Acoarse%20location%20and%20orientation%20have%20been%20obtained%2C%20either%20from%20the%20city-scale%0Aretrieval%20or%20from%20consumer-level%20GPS%20and%20compass%20sensors.%20Existing%0Alearning-based%20methods%20for%20solving%20this%20task%20require%20accurate%20GPS%20labels%20of%0Aground%20images%20for%20network%20training.%20However%2C%20obtaining%20such%20accurate%20GPS%20labels%0Ais%20difficult%2C%20often%20requiring%20an%20expensive%20%7B%5Ccolor%7Bblack%7DReal%20Time%20Kinematics%0A%28RTK%29%7D%20setup%20and%20suffering%20from%20signal%20occlusion%2C%20multi-path%20signal%0Adisruptions%2C%20%5Cetc.%20To%20alleviate%20this%20issue%2C%20this%20paper%20proposes%20a%20weakly%0Asupervised%20learning%20strategy%20for%20ground-to-satellite%20image%20registration%20when%0Aonly%20noisy%20pose%20labels%20for%20ground%20images%20are%20available%20for%20network%20training.%20It%0Aderives%20positive%20and%20negative%20satellite%20images%20for%20each%20ground%20image%20and%0Aleverages%20contrastive%20learning%20to%20learn%20feature%20representations%20for%20ground%20and%0Asatellite%20images%20useful%20for%20translation%20estimation.%20We%20also%20propose%20a%0Aself-supervision%20strategy%20for%20cross-view%20image%20relative%20rotation%20estimation%2C%0Awhich%20trains%20the%20network%20by%20creating%20pseudo%20query%20and%20reference%20image%20pairs.%0AExperimental%20results%20show%20that%20our%20weakly%20supervised%20learning%20strategy%20achieves%0Athe%20best%20performance%20on%20cross-area%20evaluation%20compared%20to%20recent%0Astate-of-the-art%20methods%20that%20are%20reliant%20on%20accurate%20pose%20labels%20for%0Asupervision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly-supervised%2520Camera%2520Localization%2520by%2520Ground-to-satellite%2520Image%250A%2520%2520Registration%26entry.906535625%3DYujiao%2520Shi%2520and%2520Hongdong%2520Li%2520and%2520Akhil%2520Perincherry%2520and%2520Ankit%2520Vora%26entry.1292438233%3D%2520%2520The%2520ground-to-satellite%2520image%2520matching/retrieval%2520was%2520initially%2520proposed%2520for%250Acity-scale%2520ground%2520camera%2520localization.%2520This%2520work%2520addresses%2520the%2520problem%2520of%250Aimproving%2520camera%2520pose%2520accuracy%2520by%2520ground-to-satellite%2520image%2520matching%2520after%2520a%250Acoarse%2520location%2520and%2520orientation%2520have%2520been%2520obtained%252C%2520either%2520from%2520the%2520city-scale%250Aretrieval%2520or%2520from%2520consumer-level%2520GPS%2520and%2520compass%2520sensors.%2520Existing%250Alearning-based%2520methods%2520for%2520solving%2520this%2520task%2520require%2520accurate%2520GPS%2520labels%2520of%250Aground%2520images%2520for%2520network%2520training.%2520However%252C%2520obtaining%2520such%2520accurate%2520GPS%2520labels%250Ais%2520difficult%252C%2520often%2520requiring%2520an%2520expensive%2520%257B%255Ccolor%257Bblack%257DReal%2520Time%2520Kinematics%250A%2528RTK%2529%257D%2520setup%2520and%2520suffering%2520from%2520signal%2520occlusion%252C%2520multi-path%2520signal%250Adisruptions%252C%2520%255Cetc.%2520To%2520alleviate%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520weakly%250Asupervised%2520learning%2520strategy%2520for%2520ground-to-satellite%2520image%2520registration%2520when%250Aonly%2520noisy%2520pose%2520labels%2520for%2520ground%2520images%2520are%2520available%2520for%2520network%2520training.%2520It%250Aderives%2520positive%2520and%2520negative%2520satellite%2520images%2520for%2520each%2520ground%2520image%2520and%250Aleverages%2520contrastive%2520learning%2520to%2520learn%2520feature%2520representations%2520for%2520ground%2520and%250Asatellite%2520images%2520useful%2520for%2520translation%2520estimation.%2520We%2520also%2520propose%2520a%250Aself-supervision%2520strategy%2520for%2520cross-view%2520image%2520relative%2520rotation%2520estimation%252C%250Awhich%2520trains%2520the%2520network%2520by%2520creating%2520pseudo%2520query%2520and%2520reference%2520image%2520pairs.%250AExperimental%2520results%2520show%2520that%2520our%2520weakly%2520supervised%2520learning%2520strategy%2520achieves%250Athe%2520best%2520performance%2520on%2520cross-area%2520evaluation%2520compared%2520to%2520recent%250Astate-of-the-art%2520methods%2520that%2520are%2520reliant%2520on%2520accurate%2520pose%2520labels%2520for%250Asupervision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly-supervised%20Camera%20Localization%20by%20Ground-to-satellite%20Image%0A%20%20Registration&entry.906535625=Yujiao%20Shi%20and%20Hongdong%20Li%20and%20Akhil%20Perincherry%20and%20Ankit%20Vora&entry.1292438233=%20%20The%20ground-to-satellite%20image%20matching/retrieval%20was%20initially%20proposed%20for%0Acity-scale%20ground%20camera%20localization.%20This%20work%20addresses%20the%20problem%20of%0Aimproving%20camera%20pose%20accuracy%20by%20ground-to-satellite%20image%20matching%20after%20a%0Acoarse%20location%20and%20orientation%20have%20been%20obtained%2C%20either%20from%20the%20city-scale%0Aretrieval%20or%20from%20consumer-level%20GPS%20and%20compass%20sensors.%20Existing%0Alearning-based%20methods%20for%20solving%20this%20task%20require%20accurate%20GPS%20labels%20of%0Aground%20images%20for%20network%20training.%20However%2C%20obtaining%20such%20accurate%20GPS%20labels%0Ais%20difficult%2C%20often%20requiring%20an%20expensive%20%7B%5Ccolor%7Bblack%7DReal%20Time%20Kinematics%0A%28RTK%29%7D%20setup%20and%20suffering%20from%20signal%20occlusion%2C%20multi-path%20signal%0Adisruptions%2C%20%5Cetc.%20To%20alleviate%20this%20issue%2C%20this%20paper%20proposes%20a%20weakly%0Asupervised%20learning%20strategy%20for%20ground-to-satellite%20image%20registration%20when%0Aonly%20noisy%20pose%20labels%20for%20ground%20images%20are%20available%20for%20network%20training.%20It%0Aderives%20positive%20and%20negative%20satellite%20images%20for%20each%20ground%20image%20and%0Aleverages%20contrastive%20learning%20to%20learn%20feature%20representations%20for%20ground%20and%0Asatellite%20images%20useful%20for%20translation%20estimation.%20We%20also%20propose%20a%0Aself-supervision%20strategy%20for%20cross-view%20image%20relative%20rotation%20estimation%2C%0Awhich%20trains%20the%20network%20by%20creating%20pseudo%20query%20and%20reference%20image%20pairs.%0AExperimental%20results%20show%20that%20our%20weakly%20supervised%20learning%20strategy%20achieves%0Athe%20best%20performance%20on%20cross-area%20evaluation%20compared%20to%20recent%0Astate-of-the-art%20methods%20that%20are%20reliant%20on%20accurate%20pose%20labels%20for%0Asupervision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06471v1&entry.124074799=Read"},
{"title": "WonderWorld: Interactive 3D Scene Generation from a Single Image", "author": "Hong-Xing Yu and Haoyi Duan and Charles Herrmann and William T. Freeman and Jiajun Wu", "abstract": "  We present WonderWorld, a novel framework for interactive 3D scene generation\nthat enables users to interactively specify scene contents and layout and see\nthe created scenes in low latency. The major challenge lies in achieving fast\ngeneration of 3D scenes. Existing scene generation approaches fall short of\nspeed as they often require (1) progressively generating many views and depth\nmaps, and (2) time-consuming optimization of the scene geometry\nrepresentations. We introduce the Fast Layered Gaussian Surfels (FLAGS) as our\nscene representation and an algorithm to generate it from a single view. Our\napproach does not need multiple views, and it leverages a geometry-based\ninitialization that significantly reduces optimization time. Another challenge\nis generating coherent geometry that allows all scenes to be connected. We\nintroduce the guided depth diffusion that allows partial conditioning of depth\nestimation. WonderWorld generates connected and diverse 3D scenes in less than\n10 seconds on a single A6000 GPU, enabling real-time user interaction and\nexploration. We demonstrate the potential of WonderWorld for user-driven\ncontent creation and exploration in virtual environments. We will release full\ncode and software for reproducibility. Project website:\nhttps://kovenyu.com/WonderWorld/.\n", "link": "http://arxiv.org/abs/2406.09394v3", "date": "2024-09-10", "relevancy": 3.1223, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.653}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.653}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image&body=Title%3A%20WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Hong-Xing%20Yu%20and%20Haoyi%20Duan%20and%20Charles%20Herrmann%20and%20William%20T.%20Freeman%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20We%20present%20WonderWorld%2C%20a%20novel%20framework%20for%20interactive%203D%20scene%20generation%0Athat%20enables%20users%20to%20interactively%20specify%20scene%20contents%20and%20layout%20and%20see%0Athe%20created%20scenes%20in%20low%20latency.%20The%20major%20challenge%20lies%20in%20achieving%20fast%0Ageneration%20of%203D%20scenes.%20Existing%20scene%20generation%20approaches%20fall%20short%20of%0Aspeed%20as%20they%20often%20require%20%281%29%20progressively%20generating%20many%20views%20and%20depth%0Amaps%2C%20and%20%282%29%20time-consuming%20optimization%20of%20the%20scene%20geometry%0Arepresentations.%20We%20introduce%20the%20Fast%20Layered%20Gaussian%20Surfels%20%28FLAGS%29%20as%20our%0Ascene%20representation%20and%20an%20algorithm%20to%20generate%20it%20from%20a%20single%20view.%20Our%0Aapproach%20does%20not%20need%20multiple%20views%2C%20and%20it%20leverages%20a%20geometry-based%0Ainitialization%20that%20significantly%20reduces%20optimization%20time.%20Another%20challenge%0Ais%20generating%20coherent%20geometry%20that%20allows%20all%20scenes%20to%20be%20connected.%20We%0Aintroduce%20the%20guided%20depth%20diffusion%20that%20allows%20partial%20conditioning%20of%20depth%0Aestimation.%20WonderWorld%20generates%20connected%20and%20diverse%203D%20scenes%20in%20less%20than%0A10%20seconds%20on%20a%20single%20A6000%20GPU%2C%20enabling%20real-time%20user%20interaction%20and%0Aexploration.%20We%20demonstrate%20the%20potential%20of%20WonderWorld%20for%20user-driven%0Acontent%20creation%20and%20exploration%20in%20virtual%20environments.%20We%20will%20release%20full%0Acode%20and%20software%20for%20reproducibility.%20Project%20website%3A%0Ahttps%3A//kovenyu.com/WonderWorld/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09394v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWonderWorld%253A%2520Interactive%25203D%2520Scene%2520Generation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DHong-Xing%2520Yu%2520and%2520Haoyi%2520Duan%2520and%2520Charles%2520Herrmann%2520and%2520William%2520T.%2520Freeman%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520We%2520present%2520WonderWorld%252C%2520a%2520novel%2520framework%2520for%2520interactive%25203D%2520scene%2520generation%250Athat%2520enables%2520users%2520to%2520interactively%2520specify%2520scene%2520contents%2520and%2520layout%2520and%2520see%250Athe%2520created%2520scenes%2520in%2520low%2520latency.%2520The%2520major%2520challenge%2520lies%2520in%2520achieving%2520fast%250Ageneration%2520of%25203D%2520scenes.%2520Existing%2520scene%2520generation%2520approaches%2520fall%2520short%2520of%250Aspeed%2520as%2520they%2520often%2520require%2520%25281%2529%2520progressively%2520generating%2520many%2520views%2520and%2520depth%250Amaps%252C%2520and%2520%25282%2529%2520time-consuming%2520optimization%2520of%2520the%2520scene%2520geometry%250Arepresentations.%2520We%2520introduce%2520the%2520Fast%2520Layered%2520Gaussian%2520Surfels%2520%2528FLAGS%2529%2520as%2520our%250Ascene%2520representation%2520and%2520an%2520algorithm%2520to%2520generate%2520it%2520from%2520a%2520single%2520view.%2520Our%250Aapproach%2520does%2520not%2520need%2520multiple%2520views%252C%2520and%2520it%2520leverages%2520a%2520geometry-based%250Ainitialization%2520that%2520significantly%2520reduces%2520optimization%2520time.%2520Another%2520challenge%250Ais%2520generating%2520coherent%2520geometry%2520that%2520allows%2520all%2520scenes%2520to%2520be%2520connected.%2520We%250Aintroduce%2520the%2520guided%2520depth%2520diffusion%2520that%2520allows%2520partial%2520conditioning%2520of%2520depth%250Aestimation.%2520WonderWorld%2520generates%2520connected%2520and%2520diverse%25203D%2520scenes%2520in%2520less%2520than%250A10%2520seconds%2520on%2520a%2520single%2520A6000%2520GPU%252C%2520enabling%2520real-time%2520user%2520interaction%2520and%250Aexploration.%2520We%2520demonstrate%2520the%2520potential%2520of%2520WonderWorld%2520for%2520user-driven%250Acontent%2520creation%2520and%2520exploration%2520in%2520virtual%2520environments.%2520We%2520will%2520release%2520full%250Acode%2520and%2520software%2520for%2520reproducibility.%2520Project%2520website%253A%250Ahttps%253A//kovenyu.com/WonderWorld/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09394v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WonderWorld%3A%20Interactive%203D%20Scene%20Generation%20from%20a%20Single%20Image&entry.906535625=Hong-Xing%20Yu%20and%20Haoyi%20Duan%20and%20Charles%20Herrmann%20and%20William%20T.%20Freeman%20and%20Jiajun%20Wu&entry.1292438233=%20%20We%20present%20WonderWorld%2C%20a%20novel%20framework%20for%20interactive%203D%20scene%20generation%0Athat%20enables%20users%20to%20interactively%20specify%20scene%20contents%20and%20layout%20and%20see%0Athe%20created%20scenes%20in%20low%20latency.%20The%20major%20challenge%20lies%20in%20achieving%20fast%0Ageneration%20of%203D%20scenes.%20Existing%20scene%20generation%20approaches%20fall%20short%20of%0Aspeed%20as%20they%20often%20require%20%281%29%20progressively%20generating%20many%20views%20and%20depth%0Amaps%2C%20and%20%282%29%20time-consuming%20optimization%20of%20the%20scene%20geometry%0Arepresentations.%20We%20introduce%20the%20Fast%20Layered%20Gaussian%20Surfels%20%28FLAGS%29%20as%20our%0Ascene%20representation%20and%20an%20algorithm%20to%20generate%20it%20from%20a%20single%20view.%20Our%0Aapproach%20does%20not%20need%20multiple%20views%2C%20and%20it%20leverages%20a%20geometry-based%0Ainitialization%20that%20significantly%20reduces%20optimization%20time.%20Another%20challenge%0Ais%20generating%20coherent%20geometry%20that%20allows%20all%20scenes%20to%20be%20connected.%20We%0Aintroduce%20the%20guided%20depth%20diffusion%20that%20allows%20partial%20conditioning%20of%20depth%0Aestimation.%20WonderWorld%20generates%20connected%20and%20diverse%203D%20scenes%20in%20less%20than%0A10%20seconds%20on%20a%20single%20A6000%20GPU%2C%20enabling%20real-time%20user%20interaction%20and%0Aexploration.%20We%20demonstrate%20the%20potential%20of%20WonderWorld%20for%20user-driven%0Acontent%20creation%20and%20exploration%20in%20virtual%20environments.%20We%20will%20release%20full%0Acode%20and%20software%20for%20reproducibility.%20Project%20website%3A%0Ahttps%3A//kovenyu.com/WonderWorld/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09394v3&entry.124074799=Read"},
{"title": "PoseEmbroider: Towards a 3D, Visual, Semantic-aware Human Pose\n  Representation", "author": "Ginger Delmas and Philippe Weinzaepfel and Francesc Moreno-Noguer and Gr\u00e9gory Rogez", "abstract": "  Aligning multiple modalities in a latent space, such as images and texts, has\nshown to produce powerful semantic visual representations, fueling tasks like\nimage captioning, text-to-image generation, or image grounding. In the context\nof human-centric vision, albeit CLIP-like representations encode most standard\nhuman poses relatively well (such as standing or sitting), they lack sufficient\nacuteness to discern detailed or uncommon ones. Actually, while 3D human poses\nhave been often associated with images (e.g. to perform pose estimation or\npose-conditioned image generation), or more recently with text (e.g. for\ntext-to-pose generation), they have seldom been paired with both. In this work,\nwe combine 3D poses, person's pictures and textual pose descriptions to produce\nan enhanced 3D-, visual- and semantic-aware human pose representation. We\nintroduce a new transformer-based model, trained in a retrieval fashion, which\ncan take as input any combination of the aforementioned modalities. When\ncomposing modalities, it outperforms a standard multi-modal alignment retrieval\nmodel, making it possible to sort out partial information (e.g. image with the\nlower body occluded). We showcase the potential of such an embroidered pose\nrepresentation for (1) SMPL regression from image with optional text cue; and\n(2) on the task of fine-grained instruction generation, which consists in\ngenerating a text that describes how to move from one 3D pose to another (as a\nfitness coach). Unlike prior works, our model can take any kind of input (image\nand/or pose) without retraining.\n", "link": "http://arxiv.org/abs/2409.06535v1", "date": "2024-09-10", "relevancy": 3.1065, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6581}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6031}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseEmbroider%3A%20Towards%20a%203D%2C%20Visual%2C%20Semantic-aware%20Human%20Pose%0A%20%20Representation&body=Title%3A%20PoseEmbroider%3A%20Towards%20a%203D%2C%20Visual%2C%20Semantic-aware%20Human%20Pose%0A%20%20Representation%0AAuthor%3A%20Ginger%20Delmas%20and%20Philippe%20Weinzaepfel%20and%20Francesc%20Moreno-Noguer%20and%20Gr%C3%A9gory%20Rogez%0AAbstract%3A%20%20%20Aligning%20multiple%20modalities%20in%20a%20latent%20space%2C%20such%20as%20images%20and%20texts%2C%20has%0Ashown%20to%20produce%20powerful%20semantic%20visual%20representations%2C%20fueling%20tasks%20like%0Aimage%20captioning%2C%20text-to-image%20generation%2C%20or%20image%20grounding.%20In%20the%20context%0Aof%20human-centric%20vision%2C%20albeit%20CLIP-like%20representations%20encode%20most%20standard%0Ahuman%20poses%20relatively%20well%20%28such%20as%20standing%20or%20sitting%29%2C%20they%20lack%20sufficient%0Aacuteness%20to%20discern%20detailed%20or%20uncommon%20ones.%20Actually%2C%20while%203D%20human%20poses%0Ahave%20been%20often%20associated%20with%20images%20%28e.g.%20to%20perform%20pose%20estimation%20or%0Apose-conditioned%20image%20generation%29%2C%20or%20more%20recently%20with%20text%20%28e.g.%20for%0Atext-to-pose%20generation%29%2C%20they%20have%20seldom%20been%20paired%20with%20both.%20In%20this%20work%2C%0Awe%20combine%203D%20poses%2C%20person%27s%20pictures%20and%20textual%20pose%20descriptions%20to%20produce%0Aan%20enhanced%203D-%2C%20visual-%20and%20semantic-aware%20human%20pose%20representation.%20We%0Aintroduce%20a%20new%20transformer-based%20model%2C%20trained%20in%20a%20retrieval%20fashion%2C%20which%0Acan%20take%20as%20input%20any%20combination%20of%20the%20aforementioned%20modalities.%20When%0Acomposing%20modalities%2C%20it%20outperforms%20a%20standard%20multi-modal%20alignment%20retrieval%0Amodel%2C%20making%20it%20possible%20to%20sort%20out%20partial%20information%20%28e.g.%20image%20with%20the%0Alower%20body%20occluded%29.%20We%20showcase%20the%20potential%20of%20such%20an%20embroidered%20pose%0Arepresentation%20for%20%281%29%20SMPL%20regression%20from%20image%20with%20optional%20text%20cue%3B%20and%0A%282%29%20on%20the%20task%20of%20fine-grained%20instruction%20generation%2C%20which%20consists%20in%0Agenerating%20a%20text%20that%20describes%20how%20to%20move%20from%20one%203D%20pose%20to%20another%20%28as%20a%0Afitness%20coach%29.%20Unlike%20prior%20works%2C%20our%20model%20can%20take%20any%20kind%20of%20input%20%28image%0Aand/or%20pose%29%20without%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseEmbroider%253A%2520Towards%2520a%25203D%252C%2520Visual%252C%2520Semantic-aware%2520Human%2520Pose%250A%2520%2520Representation%26entry.906535625%3DGinger%2520Delmas%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Francesc%2520Moreno-Noguer%2520and%2520Gr%25C3%25A9gory%2520Rogez%26entry.1292438233%3D%2520%2520Aligning%2520multiple%2520modalities%2520in%2520a%2520latent%2520space%252C%2520such%2520as%2520images%2520and%2520texts%252C%2520has%250Ashown%2520to%2520produce%2520powerful%2520semantic%2520visual%2520representations%252C%2520fueling%2520tasks%2520like%250Aimage%2520captioning%252C%2520text-to-image%2520generation%252C%2520or%2520image%2520grounding.%2520In%2520the%2520context%250Aof%2520human-centric%2520vision%252C%2520albeit%2520CLIP-like%2520representations%2520encode%2520most%2520standard%250Ahuman%2520poses%2520relatively%2520well%2520%2528such%2520as%2520standing%2520or%2520sitting%2529%252C%2520they%2520lack%2520sufficient%250Aacuteness%2520to%2520discern%2520detailed%2520or%2520uncommon%2520ones.%2520Actually%252C%2520while%25203D%2520human%2520poses%250Ahave%2520been%2520often%2520associated%2520with%2520images%2520%2528e.g.%2520to%2520perform%2520pose%2520estimation%2520or%250Apose-conditioned%2520image%2520generation%2529%252C%2520or%2520more%2520recently%2520with%2520text%2520%2528e.g.%2520for%250Atext-to-pose%2520generation%2529%252C%2520they%2520have%2520seldom%2520been%2520paired%2520with%2520both.%2520In%2520this%2520work%252C%250Awe%2520combine%25203D%2520poses%252C%2520person%2527s%2520pictures%2520and%2520textual%2520pose%2520descriptions%2520to%2520produce%250Aan%2520enhanced%25203D-%252C%2520visual-%2520and%2520semantic-aware%2520human%2520pose%2520representation.%2520We%250Aintroduce%2520a%2520new%2520transformer-based%2520model%252C%2520trained%2520in%2520a%2520retrieval%2520fashion%252C%2520which%250Acan%2520take%2520as%2520input%2520any%2520combination%2520of%2520the%2520aforementioned%2520modalities.%2520When%250Acomposing%2520modalities%252C%2520it%2520outperforms%2520a%2520standard%2520multi-modal%2520alignment%2520retrieval%250Amodel%252C%2520making%2520it%2520possible%2520to%2520sort%2520out%2520partial%2520information%2520%2528e.g.%2520image%2520with%2520the%250Alower%2520body%2520occluded%2529.%2520We%2520showcase%2520the%2520potential%2520of%2520such%2520an%2520embroidered%2520pose%250Arepresentation%2520for%2520%25281%2529%2520SMPL%2520regression%2520from%2520image%2520with%2520optional%2520text%2520cue%253B%2520and%250A%25282%2529%2520on%2520the%2520task%2520of%2520fine-grained%2520instruction%2520generation%252C%2520which%2520consists%2520in%250Agenerating%2520a%2520text%2520that%2520describes%2520how%2520to%2520move%2520from%2520one%25203D%2520pose%2520to%2520another%2520%2528as%2520a%250Afitness%2520coach%2529.%2520Unlike%2520prior%2520works%252C%2520our%2520model%2520can%2520take%2520any%2520kind%2520of%2520input%2520%2528image%250Aand/or%2520pose%2529%2520without%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseEmbroider%3A%20Towards%20a%203D%2C%20Visual%2C%20Semantic-aware%20Human%20Pose%0A%20%20Representation&entry.906535625=Ginger%20Delmas%20and%20Philippe%20Weinzaepfel%20and%20Francesc%20Moreno-Noguer%20and%20Gr%C3%A9gory%20Rogez&entry.1292438233=%20%20Aligning%20multiple%20modalities%20in%20a%20latent%20space%2C%20such%20as%20images%20and%20texts%2C%20has%0Ashown%20to%20produce%20powerful%20semantic%20visual%20representations%2C%20fueling%20tasks%20like%0Aimage%20captioning%2C%20text-to-image%20generation%2C%20or%20image%20grounding.%20In%20the%20context%0Aof%20human-centric%20vision%2C%20albeit%20CLIP-like%20representations%20encode%20most%20standard%0Ahuman%20poses%20relatively%20well%20%28such%20as%20standing%20or%20sitting%29%2C%20they%20lack%20sufficient%0Aacuteness%20to%20discern%20detailed%20or%20uncommon%20ones.%20Actually%2C%20while%203D%20human%20poses%0Ahave%20been%20often%20associated%20with%20images%20%28e.g.%20to%20perform%20pose%20estimation%20or%0Apose-conditioned%20image%20generation%29%2C%20or%20more%20recently%20with%20text%20%28e.g.%20for%0Atext-to-pose%20generation%29%2C%20they%20have%20seldom%20been%20paired%20with%20both.%20In%20this%20work%2C%0Awe%20combine%203D%20poses%2C%20person%27s%20pictures%20and%20textual%20pose%20descriptions%20to%20produce%0Aan%20enhanced%203D-%2C%20visual-%20and%20semantic-aware%20human%20pose%20representation.%20We%0Aintroduce%20a%20new%20transformer-based%20model%2C%20trained%20in%20a%20retrieval%20fashion%2C%20which%0Acan%20take%20as%20input%20any%20combination%20of%20the%20aforementioned%20modalities.%20When%0Acomposing%20modalities%2C%20it%20outperforms%20a%20standard%20multi-modal%20alignment%20retrieval%0Amodel%2C%20making%20it%20possible%20to%20sort%20out%20partial%20information%20%28e.g.%20image%20with%20the%0Alower%20body%20occluded%29.%20We%20showcase%20the%20potential%20of%20such%20an%20embroidered%20pose%0Arepresentation%20for%20%281%29%20SMPL%20regression%20from%20image%20with%20optional%20text%20cue%3B%20and%0A%282%29%20on%20the%20task%20of%20fine-grained%20instruction%20generation%2C%20which%20consists%20in%0Agenerating%20a%20text%20that%20describes%20how%20to%20move%20from%20one%203D%20pose%20to%20another%20%28as%20a%0Afitness%20coach%29.%20Unlike%20prior%20works%2C%20our%20model%20can%20take%20any%20kind%20of%20input%20%28image%0Aand/or%20pose%29%20without%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06535v1&entry.124074799=Read"},
{"title": "Space3D-Bench: Spatial 3D Question Answering Benchmark", "author": "Emilia Szymanska and Mihai Dusmanu and Jan-Willem Buurlage and Mahdi Rad and Marc Pollefeys", "abstract": "  Answering questions about the spatial properties of the environment poses\nchallenges for existing language and vision foundation models due to a lack of\nunderstanding of the 3D world notably in terms of relationships between\nobjects. To push the field forward, multiple 3D Q&A datasets were proposed\nwhich, overall, provide a variety of questions, but they individually focus on\nparticular aspects of 3D reasoning or are limited in terms of data modalities.\nTo address this, we present Space3D-Bench - a collection of 1000 general\nspatial questions and answers related to scenes of the Replica dataset which\noffers a variety of data modalities: point clouds, posed RGB-D images,\nnavigation meshes and 3D object detections. To ensure that the questions cover\na wide range of 3D objectives, we propose an indoor spatial questions taxonomy\ninspired by geographic information systems and use it to balance the dataset\naccordingly. Moreover, we provide an assessment system that grades natural\nlanguage responses based on predefined ground-truth answers by leveraging a\nVision Language Model's comprehension of both text and images to compare the\nresponses with ground-truth textual information or relevant visual data.\nFinally, we introduce a baseline called RAG3D-Chat integrating the world\nunderstanding of foundation models with rich context retrieval, achieving an\naccuracy of 67% on the proposed dataset.\n", "link": "http://arxiv.org/abs/2408.16662v2", "date": "2024-09-10", "relevancy": 3.0174, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6381}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Space3D-Bench%3A%20Spatial%203D%20Question%20Answering%20Benchmark&body=Title%3A%20Space3D-Bench%3A%20Spatial%203D%20Question%20Answering%20Benchmark%0AAuthor%3A%20Emilia%20Szymanska%20and%20Mihai%20Dusmanu%20and%20Jan-Willem%20Buurlage%20and%20Mahdi%20Rad%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20Answering%20questions%20about%20the%20spatial%20properties%20of%20the%20environment%20poses%0Achallenges%20for%20existing%20language%20and%20vision%20foundation%20models%20due%20to%20a%20lack%20of%0Aunderstanding%20of%20the%203D%20world%20notably%20in%20terms%20of%20relationships%20between%0Aobjects.%20To%20push%20the%20field%20forward%2C%20multiple%203D%20Q%26A%20datasets%20were%20proposed%0Awhich%2C%20overall%2C%20provide%20a%20variety%20of%20questions%2C%20but%20they%20individually%20focus%20on%0Aparticular%20aspects%20of%203D%20reasoning%20or%20are%20limited%20in%20terms%20of%20data%20modalities.%0ATo%20address%20this%2C%20we%20present%20Space3D-Bench%20-%20a%20collection%20of%201000%20general%0Aspatial%20questions%20and%20answers%20related%20to%20scenes%20of%20the%20Replica%20dataset%20which%0Aoffers%20a%20variety%20of%20data%20modalities%3A%20point%20clouds%2C%20posed%20RGB-D%20images%2C%0Anavigation%20meshes%20and%203D%20object%20detections.%20To%20ensure%20that%20the%20questions%20cover%0Aa%20wide%20range%20of%203D%20objectives%2C%20we%20propose%20an%20indoor%20spatial%20questions%20taxonomy%0Ainspired%20by%20geographic%20information%20systems%20and%20use%20it%20to%20balance%20the%20dataset%0Aaccordingly.%20Moreover%2C%20we%20provide%20an%20assessment%20system%20that%20grades%20natural%0Alanguage%20responses%20based%20on%20predefined%20ground-truth%20answers%20by%20leveraging%20a%0AVision%20Language%20Model%27s%20comprehension%20of%20both%20text%20and%20images%20to%20compare%20the%0Aresponses%20with%20ground-truth%20textual%20information%20or%20relevant%20visual%20data.%0AFinally%2C%20we%20introduce%20a%20baseline%20called%20RAG3D-Chat%20integrating%20the%20world%0Aunderstanding%20of%20foundation%20models%20with%20rich%20context%20retrieval%2C%20achieving%20an%0Aaccuracy%20of%2067%25%20on%20the%20proposed%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpace3D-Bench%253A%2520Spatial%25203D%2520Question%2520Answering%2520Benchmark%26entry.906535625%3DEmilia%2520Szymanska%2520and%2520Mihai%2520Dusmanu%2520and%2520Jan-Willem%2520Buurlage%2520and%2520Mahdi%2520Rad%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520Answering%2520questions%2520about%2520the%2520spatial%2520properties%2520of%2520the%2520environment%2520poses%250Achallenges%2520for%2520existing%2520language%2520and%2520vision%2520foundation%2520models%2520due%2520to%2520a%2520lack%2520of%250Aunderstanding%2520of%2520the%25203D%2520world%2520notably%2520in%2520terms%2520of%2520relationships%2520between%250Aobjects.%2520To%2520push%2520the%2520field%2520forward%252C%2520multiple%25203D%2520Q%2526A%2520datasets%2520were%2520proposed%250Awhich%252C%2520overall%252C%2520provide%2520a%2520variety%2520of%2520questions%252C%2520but%2520they%2520individually%2520focus%2520on%250Aparticular%2520aspects%2520of%25203D%2520reasoning%2520or%2520are%2520limited%2520in%2520terms%2520of%2520data%2520modalities.%250ATo%2520address%2520this%252C%2520we%2520present%2520Space3D-Bench%2520-%2520a%2520collection%2520of%25201000%2520general%250Aspatial%2520questions%2520and%2520answers%2520related%2520to%2520scenes%2520of%2520the%2520Replica%2520dataset%2520which%250Aoffers%2520a%2520variety%2520of%2520data%2520modalities%253A%2520point%2520clouds%252C%2520posed%2520RGB-D%2520images%252C%250Anavigation%2520meshes%2520and%25203D%2520object%2520detections.%2520To%2520ensure%2520that%2520the%2520questions%2520cover%250Aa%2520wide%2520range%2520of%25203D%2520objectives%252C%2520we%2520propose%2520an%2520indoor%2520spatial%2520questions%2520taxonomy%250Ainspired%2520by%2520geographic%2520information%2520systems%2520and%2520use%2520it%2520to%2520balance%2520the%2520dataset%250Aaccordingly.%2520Moreover%252C%2520we%2520provide%2520an%2520assessment%2520system%2520that%2520grades%2520natural%250Alanguage%2520responses%2520based%2520on%2520predefined%2520ground-truth%2520answers%2520by%2520leveraging%2520a%250AVision%2520Language%2520Model%2527s%2520comprehension%2520of%2520both%2520text%2520and%2520images%2520to%2520compare%2520the%250Aresponses%2520with%2520ground-truth%2520textual%2520information%2520or%2520relevant%2520visual%2520data.%250AFinally%252C%2520we%2520introduce%2520a%2520baseline%2520called%2520RAG3D-Chat%2520integrating%2520the%2520world%250Aunderstanding%2520of%2520foundation%2520models%2520with%2520rich%2520context%2520retrieval%252C%2520achieving%2520an%250Aaccuracy%2520of%252067%2525%2520on%2520the%2520proposed%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Space3D-Bench%3A%20Spatial%203D%20Question%20Answering%20Benchmark&entry.906535625=Emilia%20Szymanska%20and%20Mihai%20Dusmanu%20and%20Jan-Willem%20Buurlage%20and%20Mahdi%20Rad%20and%20Marc%20Pollefeys&entry.1292438233=%20%20Answering%20questions%20about%20the%20spatial%20properties%20of%20the%20environment%20poses%0Achallenges%20for%20existing%20language%20and%20vision%20foundation%20models%20due%20to%20a%20lack%20of%0Aunderstanding%20of%20the%203D%20world%20notably%20in%20terms%20of%20relationships%20between%0Aobjects.%20To%20push%20the%20field%20forward%2C%20multiple%203D%20Q%26A%20datasets%20were%20proposed%0Awhich%2C%20overall%2C%20provide%20a%20variety%20of%20questions%2C%20but%20they%20individually%20focus%20on%0Aparticular%20aspects%20of%203D%20reasoning%20or%20are%20limited%20in%20terms%20of%20data%20modalities.%0ATo%20address%20this%2C%20we%20present%20Space3D-Bench%20-%20a%20collection%20of%201000%20general%0Aspatial%20questions%20and%20answers%20related%20to%20scenes%20of%20the%20Replica%20dataset%20which%0Aoffers%20a%20variety%20of%20data%20modalities%3A%20point%20clouds%2C%20posed%20RGB-D%20images%2C%0Anavigation%20meshes%20and%203D%20object%20detections.%20To%20ensure%20that%20the%20questions%20cover%0Aa%20wide%20range%20of%203D%20objectives%2C%20we%20propose%20an%20indoor%20spatial%20questions%20taxonomy%0Ainspired%20by%20geographic%20information%20systems%20and%20use%20it%20to%20balance%20the%20dataset%0Aaccordingly.%20Moreover%2C%20we%20provide%20an%20assessment%20system%20that%20grades%20natural%0Alanguage%20responses%20based%20on%20predefined%20ground-truth%20answers%20by%20leveraging%20a%0AVision%20Language%20Model%27s%20comprehension%20of%20both%20text%20and%20images%20to%20compare%20the%0Aresponses%20with%20ground-truth%20textual%20information%20or%20relevant%20visual%20data.%0AFinally%2C%20we%20introduce%20a%20baseline%20called%20RAG3D-Chat%20integrating%20the%20world%0Aunderstanding%20of%20foundation%20models%20with%20rich%20context%20retrieval%2C%20achieving%20an%0Aaccuracy%20of%2067%25%20on%20the%20proposed%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16662v2&entry.124074799=Read"},
{"title": "Quantifying and Enabling the Interpretability of CLIP-like Models", "author": "Avinash Madasu and Yossi Gandelsman and Vasudev Lal and Phillip Howard", "abstract": "  CLIP is one of the most popular foundational models and is heavily used for\nmany vision-language tasks. However, little is known about the inner workings\nof CLIP. To bridge this gap we propose a study to quantify the interpretability\nin CLIP like models. We conduct this study on six different CLIP models from\nOpenAI and OpenCLIP which vary by size, type of pre-training data and patch\nsize. Our approach begins with using the TEXTSPAN algorithm and in-context\nlearning to break down individual attention heads into specific properties. We\nthen evaluate how easily these heads can be interpreted using new metrics which\nmeasure property consistency within heads and property disentanglement across\nheads. Our findings reveal that larger CLIP models are generally more\ninterpretable than their smaller counterparts. To further assist users in\nunderstanding the inner workings of CLIP models, we introduce CLIP-InterpreT, a\ntool designed for interpretability analysis. CLIP-InterpreT offers five types\nof analyses: property-based nearest neighbor search, per-head topic\nsegmentation, contrastive segmentation, per-head nearest neighbors of an image,\nand per-head nearest neighbors of text.\n", "link": "http://arxiv.org/abs/2409.06579v1", "date": "2024-09-10", "relevancy": 3.0059, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6111}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20and%20Enabling%20the%20Interpretability%20of%20CLIP-like%20Models&body=Title%3A%20Quantifying%20and%20Enabling%20the%20Interpretability%20of%20CLIP-like%20Models%0AAuthor%3A%20Avinash%20Madasu%20and%20Yossi%20Gandelsman%20and%20Vasudev%20Lal%20and%20Phillip%20Howard%0AAbstract%3A%20%20%20CLIP%20is%20one%20of%20the%20most%20popular%20foundational%20models%20and%20is%20heavily%20used%20for%0Amany%20vision-language%20tasks.%20However%2C%20little%20is%20known%20about%20the%20inner%20workings%0Aof%20CLIP.%20To%20bridge%20this%20gap%20we%20propose%20a%20study%20to%20quantify%20the%20interpretability%0Ain%20CLIP%20like%20models.%20We%20conduct%20this%20study%20on%20six%20different%20CLIP%20models%20from%0AOpenAI%20and%20OpenCLIP%20which%20vary%20by%20size%2C%20type%20of%20pre-training%20data%20and%20patch%0Asize.%20Our%20approach%20begins%20with%20using%20the%20TEXTSPAN%20algorithm%20and%20in-context%0Alearning%20to%20break%20down%20individual%20attention%20heads%20into%20specific%20properties.%20We%0Athen%20evaluate%20how%20easily%20these%20heads%20can%20be%20interpreted%20using%20new%20metrics%20which%0Ameasure%20property%20consistency%20within%20heads%20and%20property%20disentanglement%20across%0Aheads.%20Our%20findings%20reveal%20that%20larger%20CLIP%20models%20are%20generally%20more%0Ainterpretable%20than%20their%20smaller%20counterparts.%20To%20further%20assist%20users%20in%0Aunderstanding%20the%20inner%20workings%20of%20CLIP%20models%2C%20we%20introduce%20CLIP-InterpreT%2C%20a%0Atool%20designed%20for%20interpretability%20analysis.%20CLIP-InterpreT%20offers%20five%20types%0Aof%20analyses%3A%20property-based%20nearest%20neighbor%20search%2C%20per-head%20topic%0Asegmentation%2C%20contrastive%20segmentation%2C%20per-head%20nearest%20neighbors%20of%20an%20image%2C%0Aand%20per-head%20nearest%20neighbors%20of%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520and%2520Enabling%2520the%2520Interpretability%2520of%2520CLIP-like%2520Models%26entry.906535625%3DAvinash%2520Madasu%2520and%2520Yossi%2520Gandelsman%2520and%2520Vasudev%2520Lal%2520and%2520Phillip%2520Howard%26entry.1292438233%3D%2520%2520CLIP%2520is%2520one%2520of%2520the%2520most%2520popular%2520foundational%2520models%2520and%2520is%2520heavily%2520used%2520for%250Amany%2520vision-language%2520tasks.%2520However%252C%2520little%2520is%2520known%2520about%2520the%2520inner%2520workings%250Aof%2520CLIP.%2520To%2520bridge%2520this%2520gap%2520we%2520propose%2520a%2520study%2520to%2520quantify%2520the%2520interpretability%250Ain%2520CLIP%2520like%2520models.%2520We%2520conduct%2520this%2520study%2520on%2520six%2520different%2520CLIP%2520models%2520from%250AOpenAI%2520and%2520OpenCLIP%2520which%2520vary%2520by%2520size%252C%2520type%2520of%2520pre-training%2520data%2520and%2520patch%250Asize.%2520Our%2520approach%2520begins%2520with%2520using%2520the%2520TEXTSPAN%2520algorithm%2520and%2520in-context%250Alearning%2520to%2520break%2520down%2520individual%2520attention%2520heads%2520into%2520specific%2520properties.%2520We%250Athen%2520evaluate%2520how%2520easily%2520these%2520heads%2520can%2520be%2520interpreted%2520using%2520new%2520metrics%2520which%250Ameasure%2520property%2520consistency%2520within%2520heads%2520and%2520property%2520disentanglement%2520across%250Aheads.%2520Our%2520findings%2520reveal%2520that%2520larger%2520CLIP%2520models%2520are%2520generally%2520more%250Ainterpretable%2520than%2520their%2520smaller%2520counterparts.%2520To%2520further%2520assist%2520users%2520in%250Aunderstanding%2520the%2520inner%2520workings%2520of%2520CLIP%2520models%252C%2520we%2520introduce%2520CLIP-InterpreT%252C%2520a%250Atool%2520designed%2520for%2520interpretability%2520analysis.%2520CLIP-InterpreT%2520offers%2520five%2520types%250Aof%2520analyses%253A%2520property-based%2520nearest%2520neighbor%2520search%252C%2520per-head%2520topic%250Asegmentation%252C%2520contrastive%2520segmentation%252C%2520per-head%2520nearest%2520neighbors%2520of%2520an%2520image%252C%250Aand%2520per-head%2520nearest%2520neighbors%2520of%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20and%20Enabling%20the%20Interpretability%20of%20CLIP-like%20Models&entry.906535625=Avinash%20Madasu%20and%20Yossi%20Gandelsman%20and%20Vasudev%20Lal%20and%20Phillip%20Howard&entry.1292438233=%20%20CLIP%20is%20one%20of%20the%20most%20popular%20foundational%20models%20and%20is%20heavily%20used%20for%0Amany%20vision-language%20tasks.%20However%2C%20little%20is%20known%20about%20the%20inner%20workings%0Aof%20CLIP.%20To%20bridge%20this%20gap%20we%20propose%20a%20study%20to%20quantify%20the%20interpretability%0Ain%20CLIP%20like%20models.%20We%20conduct%20this%20study%20on%20six%20different%20CLIP%20models%20from%0AOpenAI%20and%20OpenCLIP%20which%20vary%20by%20size%2C%20type%20of%20pre-training%20data%20and%20patch%0Asize.%20Our%20approach%20begins%20with%20using%20the%20TEXTSPAN%20algorithm%20and%20in-context%0Alearning%20to%20break%20down%20individual%20attention%20heads%20into%20specific%20properties.%20We%0Athen%20evaluate%20how%20easily%20these%20heads%20can%20be%20interpreted%20using%20new%20metrics%20which%0Ameasure%20property%20consistency%20within%20heads%20and%20property%20disentanglement%20across%0Aheads.%20Our%20findings%20reveal%20that%20larger%20CLIP%20models%20are%20generally%20more%0Ainterpretable%20than%20their%20smaller%20counterparts.%20To%20further%20assist%20users%20in%0Aunderstanding%20the%20inner%20workings%20of%20CLIP%20models%2C%20we%20introduce%20CLIP-InterpreT%2C%20a%0Atool%20designed%20for%20interpretability%20analysis.%20CLIP-InterpreT%20offers%20five%20types%0Aof%20analyses%3A%20property-based%20nearest%20neighbor%20search%2C%20per-head%20topic%0Asegmentation%2C%20contrastive%20segmentation%2C%20per-head%20nearest%20neighbors%20of%20an%20image%2C%0Aand%20per-head%20nearest%20neighbors%20of%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06579v1&entry.124074799=Read"},
{"title": "EyeCLIP: A visual-language foundation model for multi-modal ophthalmic\n  image analysis", "author": "Danli Shi and Weiyi Zhang and Jiancheng Yang and Siyu Huang and Xiaolan Chen and Mayinuer Yusufu and Kai Jin and Shan Lin and Shunming Liu and Qing Zhang and Mingguang He", "abstract": "  Early detection of eye diseases like glaucoma, macular degeneration, and\ndiabetic retinopathy is crucial for preventing vision loss. While artificial\nintelligence (AI) foundation models hold significant promise for addressing\nthese challenges, existing ophthalmic foundation models primarily focus on a\nsingle modality, whereas diagnosing eye diseases requires multiple modalities.\nA critical yet often overlooked aspect is harnessing the multi-view information\nacross various modalities for the same patient. Additionally, due to the\nlong-tail nature of ophthalmic diseases, standard fully supervised or\nunsupervised learning approaches often struggle. Therefore, it is essential to\nintegrate clinical text to capture a broader spectrum of diseases. We propose\nEyeCLIP, a visual-language foundation model developed using over 2.77 million\nmulti-modal ophthalmology images with partial text data. To fully leverage the\nlarge multi-modal unlabeled and labeled data, we introduced a pretraining\nstrategy that combines self-supervised reconstructions, multi-modal image\ncontrastive learning, and image-text contrastive learning to learn a shared\nrepresentation of multiple modalities. Through evaluation using 14 benchmark\ndatasets, EyeCLIP can be transferred to a wide range of downstream tasks\ninvolving ocular and systemic diseases, achieving state-of-the-art performance\nin disease classification, visual question answering, and cross-modal\nretrieval. EyeCLIP represents a significant advancement over previous methods,\nespecially showcasing few-shot, even zero-shot capabilities in real-world\nlong-tail scenarios.\n", "link": "http://arxiv.org/abs/2409.06644v1", "date": "2024-09-10", "relevancy": 2.993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6206}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EyeCLIP%3A%20A%20visual-language%20foundation%20model%20for%20multi-modal%20ophthalmic%0A%20%20image%20analysis&body=Title%3A%20EyeCLIP%3A%20A%20visual-language%20foundation%20model%20for%20multi-modal%20ophthalmic%0A%20%20image%20analysis%0AAuthor%3A%20Danli%20Shi%20and%20Weiyi%20Zhang%20and%20Jiancheng%20Yang%20and%20Siyu%20Huang%20and%20Xiaolan%20Chen%20and%20Mayinuer%20Yusufu%20and%20Kai%20Jin%20and%20Shan%20Lin%20and%20Shunming%20Liu%20and%20Qing%20Zhang%20and%20Mingguang%20He%0AAbstract%3A%20%20%20Early%20detection%20of%20eye%20diseases%20like%20glaucoma%2C%20macular%20degeneration%2C%20and%0Adiabetic%20retinopathy%20is%20crucial%20for%20preventing%20vision%20loss.%20While%20artificial%0Aintelligence%20%28AI%29%20foundation%20models%20hold%20significant%20promise%20for%20addressing%0Athese%20challenges%2C%20existing%20ophthalmic%20foundation%20models%20primarily%20focus%20on%20a%0Asingle%20modality%2C%20whereas%20diagnosing%20eye%20diseases%20requires%20multiple%20modalities.%0AA%20critical%20yet%20often%20overlooked%20aspect%20is%20harnessing%20the%20multi-view%20information%0Aacross%20various%20modalities%20for%20the%20same%20patient.%20Additionally%2C%20due%20to%20the%0Along-tail%20nature%20of%20ophthalmic%20diseases%2C%20standard%20fully%20supervised%20or%0Aunsupervised%20learning%20approaches%20often%20struggle.%20Therefore%2C%20it%20is%20essential%20to%0Aintegrate%20clinical%20text%20to%20capture%20a%20broader%20spectrum%20of%20diseases.%20We%20propose%0AEyeCLIP%2C%20a%20visual-language%20foundation%20model%20developed%20using%20over%202.77%20million%0Amulti-modal%20ophthalmology%20images%20with%20partial%20text%20data.%20To%20fully%20leverage%20the%0Alarge%20multi-modal%20unlabeled%20and%20labeled%20data%2C%20we%20introduced%20a%20pretraining%0Astrategy%20that%20combines%20self-supervised%20reconstructions%2C%20multi-modal%20image%0Acontrastive%20learning%2C%20and%20image-text%20contrastive%20learning%20to%20learn%20a%20shared%0Arepresentation%20of%20multiple%20modalities.%20Through%20evaluation%20using%2014%20benchmark%0Adatasets%2C%20EyeCLIP%20can%20be%20transferred%20to%20a%20wide%20range%20of%20downstream%20tasks%0Ainvolving%20ocular%20and%20systemic%20diseases%2C%20achieving%20state-of-the-art%20performance%0Ain%20disease%20classification%2C%20visual%20question%20answering%2C%20and%20cross-modal%0Aretrieval.%20EyeCLIP%20represents%20a%20significant%20advancement%20over%20previous%20methods%2C%0Aespecially%20showcasing%20few-shot%2C%20even%20zero-shot%20capabilities%20in%20real-world%0Along-tail%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyeCLIP%253A%2520A%2520visual-language%2520foundation%2520model%2520for%2520multi-modal%2520ophthalmic%250A%2520%2520image%2520analysis%26entry.906535625%3DDanli%2520Shi%2520and%2520Weiyi%2520Zhang%2520and%2520Jiancheng%2520Yang%2520and%2520Siyu%2520Huang%2520and%2520Xiaolan%2520Chen%2520and%2520Mayinuer%2520Yusufu%2520and%2520Kai%2520Jin%2520and%2520Shan%2520Lin%2520and%2520Shunming%2520Liu%2520and%2520Qing%2520Zhang%2520and%2520Mingguang%2520He%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520eye%2520diseases%2520like%2520glaucoma%252C%2520macular%2520degeneration%252C%2520and%250Adiabetic%2520retinopathy%2520is%2520crucial%2520for%2520preventing%2520vision%2520loss.%2520While%2520artificial%250Aintelligence%2520%2528AI%2529%2520foundation%2520models%2520hold%2520significant%2520promise%2520for%2520addressing%250Athese%2520challenges%252C%2520existing%2520ophthalmic%2520foundation%2520models%2520primarily%2520focus%2520on%2520a%250Asingle%2520modality%252C%2520whereas%2520diagnosing%2520eye%2520diseases%2520requires%2520multiple%2520modalities.%250AA%2520critical%2520yet%2520often%2520overlooked%2520aspect%2520is%2520harnessing%2520the%2520multi-view%2520information%250Aacross%2520various%2520modalities%2520for%2520the%2520same%2520patient.%2520Additionally%252C%2520due%2520to%2520the%250Along-tail%2520nature%2520of%2520ophthalmic%2520diseases%252C%2520standard%2520fully%2520supervised%2520or%250Aunsupervised%2520learning%2520approaches%2520often%2520struggle.%2520Therefore%252C%2520it%2520is%2520essential%2520to%250Aintegrate%2520clinical%2520text%2520to%2520capture%2520a%2520broader%2520spectrum%2520of%2520diseases.%2520We%2520propose%250AEyeCLIP%252C%2520a%2520visual-language%2520foundation%2520model%2520developed%2520using%2520over%25202.77%2520million%250Amulti-modal%2520ophthalmology%2520images%2520with%2520partial%2520text%2520data.%2520To%2520fully%2520leverage%2520the%250Alarge%2520multi-modal%2520unlabeled%2520and%2520labeled%2520data%252C%2520we%2520introduced%2520a%2520pretraining%250Astrategy%2520that%2520combines%2520self-supervised%2520reconstructions%252C%2520multi-modal%2520image%250Acontrastive%2520learning%252C%2520and%2520image-text%2520contrastive%2520learning%2520to%2520learn%2520a%2520shared%250Arepresentation%2520of%2520multiple%2520modalities.%2520Through%2520evaluation%2520using%252014%2520benchmark%250Adatasets%252C%2520EyeCLIP%2520can%2520be%2520transferred%2520to%2520a%2520wide%2520range%2520of%2520downstream%2520tasks%250Ainvolving%2520ocular%2520and%2520systemic%2520diseases%252C%2520achieving%2520state-of-the-art%2520performance%250Ain%2520disease%2520classification%252C%2520visual%2520question%2520answering%252C%2520and%2520cross-modal%250Aretrieval.%2520EyeCLIP%2520represents%2520a%2520significant%2520advancement%2520over%2520previous%2520methods%252C%250Aespecially%2520showcasing%2520few-shot%252C%2520even%2520zero-shot%2520capabilities%2520in%2520real-world%250Along-tail%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EyeCLIP%3A%20A%20visual-language%20foundation%20model%20for%20multi-modal%20ophthalmic%0A%20%20image%20analysis&entry.906535625=Danli%20Shi%20and%20Weiyi%20Zhang%20and%20Jiancheng%20Yang%20and%20Siyu%20Huang%20and%20Xiaolan%20Chen%20and%20Mayinuer%20Yusufu%20and%20Kai%20Jin%20and%20Shan%20Lin%20and%20Shunming%20Liu%20and%20Qing%20Zhang%20and%20Mingguang%20He&entry.1292438233=%20%20Early%20detection%20of%20eye%20diseases%20like%20glaucoma%2C%20macular%20degeneration%2C%20and%0Adiabetic%20retinopathy%20is%20crucial%20for%20preventing%20vision%20loss.%20While%20artificial%0Aintelligence%20%28AI%29%20foundation%20models%20hold%20significant%20promise%20for%20addressing%0Athese%20challenges%2C%20existing%20ophthalmic%20foundation%20models%20primarily%20focus%20on%20a%0Asingle%20modality%2C%20whereas%20diagnosing%20eye%20diseases%20requires%20multiple%20modalities.%0AA%20critical%20yet%20often%20overlooked%20aspect%20is%20harnessing%20the%20multi-view%20information%0Aacross%20various%20modalities%20for%20the%20same%20patient.%20Additionally%2C%20due%20to%20the%0Along-tail%20nature%20of%20ophthalmic%20diseases%2C%20standard%20fully%20supervised%20or%0Aunsupervised%20learning%20approaches%20often%20struggle.%20Therefore%2C%20it%20is%20essential%20to%0Aintegrate%20clinical%20text%20to%20capture%20a%20broader%20spectrum%20of%20diseases.%20We%20propose%0AEyeCLIP%2C%20a%20visual-language%20foundation%20model%20developed%20using%20over%202.77%20million%0Amulti-modal%20ophthalmology%20images%20with%20partial%20text%20data.%20To%20fully%20leverage%20the%0Alarge%20multi-modal%20unlabeled%20and%20labeled%20data%2C%20we%20introduced%20a%20pretraining%0Astrategy%20that%20combines%20self-supervised%20reconstructions%2C%20multi-modal%20image%0Acontrastive%20learning%2C%20and%20image-text%20contrastive%20learning%20to%20learn%20a%20shared%0Arepresentation%20of%20multiple%20modalities.%20Through%20evaluation%20using%2014%20benchmark%0Adatasets%2C%20EyeCLIP%20can%20be%20transferred%20to%20a%20wide%20range%20of%20downstream%20tasks%0Ainvolving%20ocular%20and%20systemic%20diseases%2C%20achieving%20state-of-the-art%20performance%0Ain%20disease%20classification%2C%20visual%20question%20answering%2C%20and%20cross-modal%0Aretrieval.%20EyeCLIP%20represents%20a%20significant%20advancement%20over%20previous%20methods%2C%0Aespecially%20showcasing%20few-shot%2C%20even%20zero-shot%20capabilities%20in%20real-world%0Along-tail%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06644v1&entry.124074799=Read"},
{"title": "Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in\n  Zero-shot Anomaly Detection", "author": "Jiaqi Zhu and Shaofeng Cai and Fang Deng and Beng Chin Ooi and Junran Wu", "abstract": "  Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness\nin harnessing the language potential for zero-shot VAD, achieving significant\nPRO improvements of 12.1% on MVTec and 8.9% on VisA compared to\nstate-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2404.09654v2", "date": "2024-09-10", "relevancy": 2.9486, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Understand%20Visual%20Anomalies%3F%20Uncovering%20LLM%27s%20Capabilities%20in%0A%20%20Zero-shot%20Anomaly%20Detection&body=Title%3A%20Do%20LLMs%20Understand%20Visual%20Anomalies%3F%20Uncovering%20LLM%27s%20Capabilities%20in%0A%20%20Zero-shot%20Anomaly%20Detection%0AAuthor%3A%20Jiaqi%20Zhu%20and%20Shaofeng%20Cai%20and%20Fang%20Deng%20and%20Beng%20Chin%20Ooi%20and%20Junran%20Wu%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20are%20markedly%20proficient%20in%20deriving%0Avisual%20representations%20guided%20by%20natural%20language.%20Recent%20explorations%20have%0Autilized%20LVLMs%20to%20tackle%20zero-shot%20visual%20anomaly%20detection%20%28VAD%29%20challenges%20by%0Apairing%20images%20with%20textual%20descriptions%20indicative%20of%20normal%20and%20abnormal%0Aconditions%2C%20referred%20to%20as%20anomaly%20prompts.%20However%2C%20existing%20approaches%20depend%0Aon%20static%20anomaly%20prompts%20that%20are%20prone%20to%20cross-semantic%20ambiguity%2C%20and%0Aprioritize%20global%20image-level%20representations%20over%20crucial%20local%20pixel-level%0Aimage-to-text%20alignment%20that%20is%20necessary%20for%20accurate%20anomaly%20localization.%20In%0Athis%20paper%2C%20we%20present%20ALFA%2C%20a%20training-free%20approach%20designed%20to%20address%20these%0Achallenges%20via%20a%20unified%20model.%20We%20propose%20a%20run-time%20prompt%20adaptation%0Astrategy%2C%20which%20first%20generates%20informative%20anomaly%20prompts%20to%20leverage%20the%0Acapabilities%20of%20a%20large%20language%20model%20%28LLM%29.%20This%20strategy%20is%20enhanced%20by%20a%0Acontextual%20scoring%20mechanism%20for%20per-image%20anomaly%20prompt%20adaptation%20and%0Across-semantic%20ambiguity%20mitigation.%20We%20further%20introduce%20a%20novel%20fine-grained%0Aaligner%20to%20fuse%20local%20pixel-level%20semantics%20for%20precise%20anomaly%20localization%2C%0Aby%20projecting%20the%20image-text%20alignment%20from%20global%20to%20local%20semantic%20spaces.%0AExtensive%20evaluations%20on%20MVTec%20and%20VisA%20datasets%20confirm%20ALFA%27s%20effectiveness%0Ain%20harnessing%20the%20language%20potential%20for%20zero-shot%20VAD%2C%20achieving%20significant%0APRO%20improvements%20of%2012.1%25%20on%20MVTec%20and%208.9%25%20on%20VisA%20compared%20to%0Astate-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Understand%2520Visual%2520Anomalies%253F%2520Uncovering%2520LLM%2527s%2520Capabilities%2520in%250A%2520%2520Zero-shot%2520Anomaly%2520Detection%26entry.906535625%3DJiaqi%2520Zhu%2520and%2520Shaofeng%2520Cai%2520and%2520Fang%2520Deng%2520and%2520Beng%2520Chin%2520Ooi%2520and%2520Junran%2520Wu%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520markedly%2520proficient%2520in%2520deriving%250Avisual%2520representations%2520guided%2520by%2520natural%2520language.%2520Recent%2520explorations%2520have%250Autilized%2520LVLMs%2520to%2520tackle%2520zero-shot%2520visual%2520anomaly%2520detection%2520%2528VAD%2529%2520challenges%2520by%250Apairing%2520images%2520with%2520textual%2520descriptions%2520indicative%2520of%2520normal%2520and%2520abnormal%250Aconditions%252C%2520referred%2520to%2520as%2520anomaly%2520prompts.%2520However%252C%2520existing%2520approaches%2520depend%250Aon%2520static%2520anomaly%2520prompts%2520that%2520are%2520prone%2520to%2520cross-semantic%2520ambiguity%252C%2520and%250Aprioritize%2520global%2520image-level%2520representations%2520over%2520crucial%2520local%2520pixel-level%250Aimage-to-text%2520alignment%2520that%2520is%2520necessary%2520for%2520accurate%2520anomaly%2520localization.%2520In%250Athis%2520paper%252C%2520we%2520present%2520ALFA%252C%2520a%2520training-free%2520approach%2520designed%2520to%2520address%2520these%250Achallenges%2520via%2520a%2520unified%2520model.%2520We%2520propose%2520a%2520run-time%2520prompt%2520adaptation%250Astrategy%252C%2520which%2520first%2520generates%2520informative%2520anomaly%2520prompts%2520to%2520leverage%2520the%250Acapabilities%2520of%2520a%2520large%2520language%2520model%2520%2528LLM%2529.%2520This%2520strategy%2520is%2520enhanced%2520by%2520a%250Acontextual%2520scoring%2520mechanism%2520for%2520per-image%2520anomaly%2520prompt%2520adaptation%2520and%250Across-semantic%2520ambiguity%2520mitigation.%2520We%2520further%2520introduce%2520a%2520novel%2520fine-grained%250Aaligner%2520to%2520fuse%2520local%2520pixel-level%2520semantics%2520for%2520precise%2520anomaly%2520localization%252C%250Aby%2520projecting%2520the%2520image-text%2520alignment%2520from%2520global%2520to%2520local%2520semantic%2520spaces.%250AExtensive%2520evaluations%2520on%2520MVTec%2520and%2520VisA%2520datasets%2520confirm%2520ALFA%2527s%2520effectiveness%250Ain%2520harnessing%2520the%2520language%2520potential%2520for%2520zero-shot%2520VAD%252C%2520achieving%2520significant%250APRO%2520improvements%2520of%252012.1%2525%2520on%2520MVTec%2520and%25208.9%2525%2520on%2520VisA%2520compared%2520to%250Astate-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Understand%20Visual%20Anomalies%3F%20Uncovering%20LLM%27s%20Capabilities%20in%0A%20%20Zero-shot%20Anomaly%20Detection&entry.906535625=Jiaqi%20Zhu%20and%20Shaofeng%20Cai%20and%20Fang%20Deng%20and%20Beng%20Chin%20Ooi%20and%20Junran%20Wu&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20are%20markedly%20proficient%20in%20deriving%0Avisual%20representations%20guided%20by%20natural%20language.%20Recent%20explorations%20have%0Autilized%20LVLMs%20to%20tackle%20zero-shot%20visual%20anomaly%20detection%20%28VAD%29%20challenges%20by%0Apairing%20images%20with%20textual%20descriptions%20indicative%20of%20normal%20and%20abnormal%0Aconditions%2C%20referred%20to%20as%20anomaly%20prompts.%20However%2C%20existing%20approaches%20depend%0Aon%20static%20anomaly%20prompts%20that%20are%20prone%20to%20cross-semantic%20ambiguity%2C%20and%0Aprioritize%20global%20image-level%20representations%20over%20crucial%20local%20pixel-level%0Aimage-to-text%20alignment%20that%20is%20necessary%20for%20accurate%20anomaly%20localization.%20In%0Athis%20paper%2C%20we%20present%20ALFA%2C%20a%20training-free%20approach%20designed%20to%20address%20these%0Achallenges%20via%20a%20unified%20model.%20We%20propose%20a%20run-time%20prompt%20adaptation%0Astrategy%2C%20which%20first%20generates%20informative%20anomaly%20prompts%20to%20leverage%20the%0Acapabilities%20of%20a%20large%20language%20model%20%28LLM%29.%20This%20strategy%20is%20enhanced%20by%20a%0Acontextual%20scoring%20mechanism%20for%20per-image%20anomaly%20prompt%20adaptation%20and%0Across-semantic%20ambiguity%20mitigation.%20We%20further%20introduce%20a%20novel%20fine-grained%0Aaligner%20to%20fuse%20local%20pixel-level%20semantics%20for%20precise%20anomaly%20localization%2C%0Aby%20projecting%20the%20image-text%20alignment%20from%20global%20to%20local%20semantic%20spaces.%0AExtensive%20evaluations%20on%20MVTec%20and%20VisA%20datasets%20confirm%20ALFA%27s%20effectiveness%0Ain%20harnessing%20the%20language%20potential%20for%20zero-shot%20VAD%2C%20achieving%20significant%0APRO%20improvements%20of%2012.1%25%20on%20MVTec%20and%208.9%25%20on%20VisA%20compared%20to%0Astate-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09654v2&entry.124074799=Read"},
{"title": "Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity", "author": "Zhentao Huang and Minglun Gong", "abstract": "  In this paper, we introduce Textured-GS, an innovative method for rendering\nGaussian splatting that incorporates spatially defined color and opacity\nvariations using Spherical Harmonics (SH). This approach enables each Gaussian\nto exhibit a richer representation by accommodating varying colors and\nopacities across its surface, significantly enhancing rendering quality\ncompared to traditional methods. To demonstrate the merits of our approach, we\nhave adapted the Mini-Splatting architecture to integrate textured Gaussians\nwithout increasing the number of Gaussians. Our experiments across multiple\nreal-world datasets show that Textured-GS consistently outperforms both the\nbaseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The\nresults highlight the potential of Textured-GS to advance Gaussian-based\nrendering technologies, promising more efficient and high-quality scene\nreconstructions.\n", "link": "http://arxiv.org/abs/2407.09733v2", "date": "2024-09-10", "relevancy": 2.9396, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6439}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6059}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Textured-GS%3A%20Gaussian%20Splatting%20with%20Spatially%20Defined%20Color%20and%20Opacity&body=Title%3A%20Textured-GS%3A%20Gaussian%20Splatting%20with%20Spatially%20Defined%20Color%20and%20Opacity%0AAuthor%3A%20Zhentao%20Huang%20and%20Minglun%20Gong%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Textured-GS%2C%20an%20innovative%20method%20for%20rendering%0AGaussian%20splatting%20that%20incorporates%20spatially%20defined%20color%20and%20opacity%0Avariations%20using%20Spherical%20Harmonics%20%28SH%29.%20This%20approach%20enables%20each%20Gaussian%0Ato%20exhibit%20a%20richer%20representation%20by%20accommodating%20varying%20colors%20and%0Aopacities%20across%20its%20surface%2C%20significantly%20enhancing%20rendering%20quality%0Acompared%20to%20traditional%20methods.%20To%20demonstrate%20the%20merits%20of%20our%20approach%2C%20we%0Ahave%20adapted%20the%20Mini-Splatting%20architecture%20to%20integrate%20textured%20Gaussians%0Awithout%20increasing%20the%20number%20of%20Gaussians.%20Our%20experiments%20across%20multiple%0Areal-world%20datasets%20show%20that%20Textured-GS%20consistently%20outperforms%20both%20the%0Abaseline%20Mini-Splatting%20and%20standard%203DGS%20in%20terms%20of%20visual%20fidelity.%20The%0Aresults%20highlight%20the%20potential%20of%20Textured-GS%20to%20advance%20Gaussian-based%0Arendering%20technologies%2C%20promising%20more%20efficient%20and%20high-quality%20scene%0Areconstructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextured-GS%253A%2520Gaussian%2520Splatting%2520with%2520Spatially%2520Defined%2520Color%2520and%2520Opacity%26entry.906535625%3DZhentao%2520Huang%2520and%2520Minglun%2520Gong%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Textured-GS%252C%2520an%2520innovative%2520method%2520for%2520rendering%250AGaussian%2520splatting%2520that%2520incorporates%2520spatially%2520defined%2520color%2520and%2520opacity%250Avariations%2520using%2520Spherical%2520Harmonics%2520%2528SH%2529.%2520This%2520approach%2520enables%2520each%2520Gaussian%250Ato%2520exhibit%2520a%2520richer%2520representation%2520by%2520accommodating%2520varying%2520colors%2520and%250Aopacities%2520across%2520its%2520surface%252C%2520significantly%2520enhancing%2520rendering%2520quality%250Acompared%2520to%2520traditional%2520methods.%2520To%2520demonstrate%2520the%2520merits%2520of%2520our%2520approach%252C%2520we%250Ahave%2520adapted%2520the%2520Mini-Splatting%2520architecture%2520to%2520integrate%2520textured%2520Gaussians%250Awithout%2520increasing%2520the%2520number%2520of%2520Gaussians.%2520Our%2520experiments%2520across%2520multiple%250Areal-world%2520datasets%2520show%2520that%2520Textured-GS%2520consistently%2520outperforms%2520both%2520the%250Abaseline%2520Mini-Splatting%2520and%2520standard%25203DGS%2520in%2520terms%2520of%2520visual%2520fidelity.%2520The%250Aresults%2520highlight%2520the%2520potential%2520of%2520Textured-GS%2520to%2520advance%2520Gaussian-based%250Arendering%2520technologies%252C%2520promising%2520more%2520efficient%2520and%2520high-quality%2520scene%250Areconstructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textured-GS%3A%20Gaussian%20Splatting%20with%20Spatially%20Defined%20Color%20and%20Opacity&entry.906535625=Zhentao%20Huang%20and%20Minglun%20Gong&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Textured-GS%2C%20an%20innovative%20method%20for%20rendering%0AGaussian%20splatting%20that%20incorporates%20spatially%20defined%20color%20and%20opacity%0Avariations%20using%20Spherical%20Harmonics%20%28SH%29.%20This%20approach%20enables%20each%20Gaussian%0Ato%20exhibit%20a%20richer%20representation%20by%20accommodating%20varying%20colors%20and%0Aopacities%20across%20its%20surface%2C%20significantly%20enhancing%20rendering%20quality%0Acompared%20to%20traditional%20methods.%20To%20demonstrate%20the%20merits%20of%20our%20approach%2C%20we%0Ahave%20adapted%20the%20Mini-Splatting%20architecture%20to%20integrate%20textured%20Gaussians%0Awithout%20increasing%20the%20number%20of%20Gaussians.%20Our%20experiments%20across%20multiple%0Areal-world%20datasets%20show%20that%20Textured-GS%20consistently%20outperforms%20both%20the%0Abaseline%20Mini-Splatting%20and%20standard%203DGS%20in%20terms%20of%20visual%20fidelity.%20The%0Aresults%20highlight%20the%20potential%20of%20Textured-GS%20to%20advance%20Gaussian-based%0Arendering%20technologies%2C%20promising%20more%20efficient%20and%20high-quality%20scene%0Areconstructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09733v2&entry.124074799=Read"},
{"title": "LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation", "author": "Archana Swaminathan and Anubhav Gupta and Kamal Gupta and Shishira R. Maiya and Vatsal Agarwal and Abhinav Shrivastava", "abstract": "  Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of\nstatic scenes and objects in 3D, offering unprecedented quality. However,\nextending NeRFs to model dynamic objects or object articulations remains a\nchallenging problem. Previous works have tackled this issue by focusing on\npart-level reconstruction and motion estimation for objects, but they often\nrely on heuristics regarding the number of moving parts or object categories,\nwhich can limit their practical use. In this work, we introduce LEIA, a novel\napproach for representing dynamic 3D objects. Our method involves observing the\nobject at distinct time steps or \"states\" and conditioning a hypernetwork on\nthe current state, using this to parameterize our NeRF. This approach allows us\nto learn a view-invariant latent representation for each state. We further\ndemonstrate that by interpolating between these states, we can generate novel\narticulation configurations in 3D space that were previously unseen. Our\nexperimental results highlight the effectiveness of our method in articulating\nobjects in a manner that is independent of the viewing angle and joint\nconfiguration. Notably, our approach outperforms previous methods that rely on\nmotion information for articulation registration.\n", "link": "http://arxiv.org/abs/2409.06703v1", "date": "2024-09-10", "relevancy": 2.922, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEIA%3A%20Latent%20View-invariant%20Embeddings%20for%20Implicit%203D%20Articulation&body=Title%3A%20LEIA%3A%20Latent%20View-invariant%20Embeddings%20for%20Implicit%203D%20Articulation%0AAuthor%3A%20Archana%20Swaminathan%20and%20Anubhav%20Gupta%20and%20Kamal%20Gupta%20and%20Shishira%20R.%20Maiya%20and%20Vatsal%20Agarwal%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20revolutionized%20the%20reconstruction%20of%0Astatic%20scenes%20and%20objects%20in%203D%2C%20offering%20unprecedented%20quality.%20However%2C%0Aextending%20NeRFs%20to%20model%20dynamic%20objects%20or%20object%20articulations%20remains%20a%0Achallenging%20problem.%20Previous%20works%20have%20tackled%20this%20issue%20by%20focusing%20on%0Apart-level%20reconstruction%20and%20motion%20estimation%20for%20objects%2C%20but%20they%20often%0Arely%20on%20heuristics%20regarding%20the%20number%20of%20moving%20parts%20or%20object%20categories%2C%0Awhich%20can%20limit%20their%20practical%20use.%20In%20this%20work%2C%20we%20introduce%20LEIA%2C%20a%20novel%0Aapproach%20for%20representing%20dynamic%203D%20objects.%20Our%20method%20involves%20observing%20the%0Aobject%20at%20distinct%20time%20steps%20or%20%22states%22%20and%20conditioning%20a%20hypernetwork%20on%0Athe%20current%20state%2C%20using%20this%20to%20parameterize%20our%20NeRF.%20This%20approach%20allows%20us%0Ato%20learn%20a%20view-invariant%20latent%20representation%20for%20each%20state.%20We%20further%0Ademonstrate%20that%20by%20interpolating%20between%20these%20states%2C%20we%20can%20generate%20novel%0Aarticulation%20configurations%20in%203D%20space%20that%20were%20previously%20unseen.%20Our%0Aexperimental%20results%20highlight%20the%20effectiveness%20of%20our%20method%20in%20articulating%0Aobjects%20in%20a%20manner%20that%20is%20independent%20of%20the%20viewing%20angle%20and%20joint%0Aconfiguration.%20Notably%2C%20our%20approach%20outperforms%20previous%20methods%20that%20rely%20on%0Amotion%20information%20for%20articulation%20registration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEIA%253A%2520Latent%2520View-invariant%2520Embeddings%2520for%2520Implicit%25203D%2520Articulation%26entry.906535625%3DArchana%2520Swaminathan%2520and%2520Anubhav%2520Gupta%2520and%2520Kamal%2520Gupta%2520and%2520Shishira%2520R.%2520Maiya%2520and%2520Vatsal%2520Agarwal%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520revolutionized%2520the%2520reconstruction%2520of%250Astatic%2520scenes%2520and%2520objects%2520in%25203D%252C%2520offering%2520unprecedented%2520quality.%2520However%252C%250Aextending%2520NeRFs%2520to%2520model%2520dynamic%2520objects%2520or%2520object%2520articulations%2520remains%2520a%250Achallenging%2520problem.%2520Previous%2520works%2520have%2520tackled%2520this%2520issue%2520by%2520focusing%2520on%250Apart-level%2520reconstruction%2520and%2520motion%2520estimation%2520for%2520objects%252C%2520but%2520they%2520often%250Arely%2520on%2520heuristics%2520regarding%2520the%2520number%2520of%2520moving%2520parts%2520or%2520object%2520categories%252C%250Awhich%2520can%2520limit%2520their%2520practical%2520use.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LEIA%252C%2520a%2520novel%250Aapproach%2520for%2520representing%2520dynamic%25203D%2520objects.%2520Our%2520method%2520involves%2520observing%2520the%250Aobject%2520at%2520distinct%2520time%2520steps%2520or%2520%2522states%2522%2520and%2520conditioning%2520a%2520hypernetwork%2520on%250Athe%2520current%2520state%252C%2520using%2520this%2520to%2520parameterize%2520our%2520NeRF.%2520This%2520approach%2520allows%2520us%250Ato%2520learn%2520a%2520view-invariant%2520latent%2520representation%2520for%2520each%2520state.%2520We%2520further%250Ademonstrate%2520that%2520by%2520interpolating%2520between%2520these%2520states%252C%2520we%2520can%2520generate%2520novel%250Aarticulation%2520configurations%2520in%25203D%2520space%2520that%2520were%2520previously%2520unseen.%2520Our%250Aexperimental%2520results%2520highlight%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520articulating%250Aobjects%2520in%2520a%2520manner%2520that%2520is%2520independent%2520of%2520the%2520viewing%2520angle%2520and%2520joint%250Aconfiguration.%2520Notably%252C%2520our%2520approach%2520outperforms%2520previous%2520methods%2520that%2520rely%2520on%250Amotion%2520information%2520for%2520articulation%2520registration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEIA%3A%20Latent%20View-invariant%20Embeddings%20for%20Implicit%203D%20Articulation&entry.906535625=Archana%20Swaminathan%20and%20Anubhav%20Gupta%20and%20Kamal%20Gupta%20and%20Shishira%20R.%20Maiya%20and%20Vatsal%20Agarwal%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20revolutionized%20the%20reconstruction%20of%0Astatic%20scenes%20and%20objects%20in%203D%2C%20offering%20unprecedented%20quality.%20However%2C%0Aextending%20NeRFs%20to%20model%20dynamic%20objects%20or%20object%20articulations%20remains%20a%0Achallenging%20problem.%20Previous%20works%20have%20tackled%20this%20issue%20by%20focusing%20on%0Apart-level%20reconstruction%20and%20motion%20estimation%20for%20objects%2C%20but%20they%20often%0Arely%20on%20heuristics%20regarding%20the%20number%20of%20moving%20parts%20or%20object%20categories%2C%0Awhich%20can%20limit%20their%20practical%20use.%20In%20this%20work%2C%20we%20introduce%20LEIA%2C%20a%20novel%0Aapproach%20for%20representing%20dynamic%203D%20objects.%20Our%20method%20involves%20observing%20the%0Aobject%20at%20distinct%20time%20steps%20or%20%22states%22%20and%20conditioning%20a%20hypernetwork%20on%0Athe%20current%20state%2C%20using%20this%20to%20parameterize%20our%20NeRF.%20This%20approach%20allows%20us%0Ato%20learn%20a%20view-invariant%20latent%20representation%20for%20each%20state.%20We%20further%0Ademonstrate%20that%20by%20interpolating%20between%20these%20states%2C%20we%20can%20generate%20novel%0Aarticulation%20configurations%20in%203D%20space%20that%20were%20previously%20unseen.%20Our%0Aexperimental%20results%20highlight%20the%20effectiveness%20of%20our%20method%20in%20articulating%0Aobjects%20in%20a%20manner%20that%20is%20independent%20of%20the%20viewing%20angle%20and%20joint%0Aconfiguration.%20Notably%2C%20our%20approach%20outperforms%20previous%20methods%20that%20rely%20on%0Amotion%20information%20for%20articulation%20registration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06703v1&entry.124074799=Read"},
{"title": "Implicit Filtering for Learning Neural Signed Distance Functions from 3D\n  Point Clouds", "author": "Shengtao Li and Ge Gao and Yudong Liu and Ming Gu and Yu-Shen Liu", "abstract": "  Neural signed distance functions (SDFs) have shown powerful ability in\nfitting the shape geometry. However, inferring continuous signed distance\nfields from discrete unoriented point clouds still remains a challenge. The\nneural network typically fits the shape with a rough surface and omits\nfine-grained geometric details such as shape edges and corners. In this paper,\nwe propose a novel non-linear implicit filter to smooth the implicit field\nwhile preserving high-frequency geometry details. Our novelty lies in that we\ncan filter the surface (zero level set) by the neighbor input points with\ngradients of the signed distance field. By moving the input raw point clouds\nalong the gradient, our proposed implicit filtering can be extended to non-zero\nlevel sets to keep the promise consistency between different level sets, which\nconsequently results in a better regularization of the zero level set. We\nconduct comprehensive experiments in surface reconstruction from objects and\ncomplex scene point clouds, the numerical and visual comparisons demonstrate\nour improvements over the state-of-the-art methods under the widely used\nbenchmarks.\n", "link": "http://arxiv.org/abs/2407.13342v2", "date": "2024-09-10", "relevancy": 2.8659, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6283}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5621}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Filtering%20for%20Learning%20Neural%20Signed%20Distance%20Functions%20from%203D%0A%20%20Point%20Clouds&body=Title%3A%20Implicit%20Filtering%20for%20Learning%20Neural%20Signed%20Distance%20Functions%20from%203D%0A%20%20Point%20Clouds%0AAuthor%3A%20Shengtao%20Li%20and%20Ge%20Gao%20and%20Yudong%20Liu%20and%20Ming%20Gu%20and%20Yu-Shen%20Liu%0AAbstract%3A%20%20%20Neural%20signed%20distance%20functions%20%28SDFs%29%20have%20shown%20powerful%20ability%20in%0Afitting%20the%20shape%20geometry.%20However%2C%20inferring%20continuous%20signed%20distance%0Afields%20from%20discrete%20unoriented%20point%20clouds%20still%20remains%20a%20challenge.%20The%0Aneural%20network%20typically%20fits%20the%20shape%20with%20a%20rough%20surface%20and%20omits%0Afine-grained%20geometric%20details%20such%20as%20shape%20edges%20and%20corners.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20non-linear%20implicit%20filter%20to%20smooth%20the%20implicit%20field%0Awhile%20preserving%20high-frequency%20geometry%20details.%20Our%20novelty%20lies%20in%20that%20we%0Acan%20filter%20the%20surface%20%28zero%20level%20set%29%20by%20the%20neighbor%20input%20points%20with%0Agradients%20of%20the%20signed%20distance%20field.%20By%20moving%20the%20input%20raw%20point%20clouds%0Aalong%20the%20gradient%2C%20our%20proposed%20implicit%20filtering%20can%20be%20extended%20to%20non-zero%0Alevel%20sets%20to%20keep%20the%20promise%20consistency%20between%20different%20level%20sets%2C%20which%0Aconsequently%20results%20in%20a%20better%20regularization%20of%20the%20zero%20level%20set.%20We%0Aconduct%20comprehensive%20experiments%20in%20surface%20reconstruction%20from%20objects%20and%0Acomplex%20scene%20point%20clouds%2C%20the%20numerical%20and%20visual%20comparisons%20demonstrate%0Aour%20improvements%20over%20the%20state-of-the-art%20methods%20under%20the%20widely%20used%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Filtering%2520for%2520Learning%2520Neural%2520Signed%2520Distance%2520Functions%2520from%25203D%250A%2520%2520Point%2520Clouds%26entry.906535625%3DShengtao%2520Li%2520and%2520Ge%2520Gao%2520and%2520Yudong%2520Liu%2520and%2520Ming%2520Gu%2520and%2520Yu-Shen%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520signed%2520distance%2520functions%2520%2528SDFs%2529%2520have%2520shown%2520powerful%2520ability%2520in%250Afitting%2520the%2520shape%2520geometry.%2520However%252C%2520inferring%2520continuous%2520signed%2520distance%250Afields%2520from%2520discrete%2520unoriented%2520point%2520clouds%2520still%2520remains%2520a%2520challenge.%2520The%250Aneural%2520network%2520typically%2520fits%2520the%2520shape%2520with%2520a%2520rough%2520surface%2520and%2520omits%250Afine-grained%2520geometric%2520details%2520such%2520as%2520shape%2520edges%2520and%2520corners.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520novel%2520non-linear%2520implicit%2520filter%2520to%2520smooth%2520the%2520implicit%2520field%250Awhile%2520preserving%2520high-frequency%2520geometry%2520details.%2520Our%2520novelty%2520lies%2520in%2520that%2520we%250Acan%2520filter%2520the%2520surface%2520%2528zero%2520level%2520set%2529%2520by%2520the%2520neighbor%2520input%2520points%2520with%250Agradients%2520of%2520the%2520signed%2520distance%2520field.%2520By%2520moving%2520the%2520input%2520raw%2520point%2520clouds%250Aalong%2520the%2520gradient%252C%2520our%2520proposed%2520implicit%2520filtering%2520can%2520be%2520extended%2520to%2520non-zero%250Alevel%2520sets%2520to%2520keep%2520the%2520promise%2520consistency%2520between%2520different%2520level%2520sets%252C%2520which%250Aconsequently%2520results%2520in%2520a%2520better%2520regularization%2520of%2520the%2520zero%2520level%2520set.%2520We%250Aconduct%2520comprehensive%2520experiments%2520in%2520surface%2520reconstruction%2520from%2520objects%2520and%250Acomplex%2520scene%2520point%2520clouds%252C%2520the%2520numerical%2520and%2520visual%2520comparisons%2520demonstrate%250Aour%2520improvements%2520over%2520the%2520state-of-the-art%2520methods%2520under%2520the%2520widely%2520used%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Filtering%20for%20Learning%20Neural%20Signed%20Distance%20Functions%20from%203D%0A%20%20Point%20Clouds&entry.906535625=Shengtao%20Li%20and%20Ge%20Gao%20and%20Yudong%20Liu%20and%20Ming%20Gu%20and%20Yu-Shen%20Liu&entry.1292438233=%20%20Neural%20signed%20distance%20functions%20%28SDFs%29%20have%20shown%20powerful%20ability%20in%0Afitting%20the%20shape%20geometry.%20However%2C%20inferring%20continuous%20signed%20distance%0Afields%20from%20discrete%20unoriented%20point%20clouds%20still%20remains%20a%20challenge.%20The%0Aneural%20network%20typically%20fits%20the%20shape%20with%20a%20rough%20surface%20and%20omits%0Afine-grained%20geometric%20details%20such%20as%20shape%20edges%20and%20corners.%20In%20this%20paper%2C%0Awe%20propose%20a%20novel%20non-linear%20implicit%20filter%20to%20smooth%20the%20implicit%20field%0Awhile%20preserving%20high-frequency%20geometry%20details.%20Our%20novelty%20lies%20in%20that%20we%0Acan%20filter%20the%20surface%20%28zero%20level%20set%29%20by%20the%20neighbor%20input%20points%20with%0Agradients%20of%20the%20signed%20distance%20field.%20By%20moving%20the%20input%20raw%20point%20clouds%0Aalong%20the%20gradient%2C%20our%20proposed%20implicit%20filtering%20can%20be%20extended%20to%20non-zero%0Alevel%20sets%20to%20keep%20the%20promise%20consistency%20between%20different%20level%20sets%2C%20which%0Aconsequently%20results%20in%20a%20better%20regularization%20of%20the%20zero%20level%20set.%20We%0Aconduct%20comprehensive%20experiments%20in%20surface%20reconstruction%20from%20objects%20and%0Acomplex%20scene%20point%20clouds%2C%20the%20numerical%20and%20visual%20comparisons%20demonstrate%0Aour%20improvements%20over%20the%20state-of-the-art%20methods%20under%20the%20widely%20used%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13342v2&entry.124074799=Read"},
{"title": "PoseScript: Linking 3D Human Poses and Natural Language", "author": "Ginger Delmas and Philippe Weinzaepfel and Thomas Lucas and Francesc Moreno-Noguer and Gr\u00e9gory Rogez", "abstract": "  Natural language plays a critical role in many computer vision applications,\nsuch as image captioning, visual question answering, and cross-modal retrieval,\nto provide fine-grained semantic information. Unfortunately, while human pose\nis key to human understanding, current 3D human pose datasets lack detailed\nlanguage descriptions. To address this issue, we have introduced the PoseScript\ndataset. This dataset pairs more than six thousand 3D human poses from AMASS\nwith rich human-annotated descriptions of the body parts and their spatial\nrelationships. Additionally, to increase the size of the dataset to a scale\nthat is compatible with data-hungry learning algorithms, we have proposed an\nelaborate captioning process that generates automatic synthetic descriptions in\nnatural language from given 3D keypoints. This process extracts low-level pose\ninformation, known as \"posecodes\", using a set of simple but generic rules on\nthe 3D keypoints. These posecodes are then combined into higher level textual\ndescriptions using syntactic rules. With automatic annotations, the amount of\navailable data significantly scales up (100k), making it possible to\neffectively pretrain deep models for finetuning on human captions. To showcase\nthe potential of annotated poses, we present three multi-modal learning tasks\nthat utilize the PoseScript dataset. Firstly, we develop a pipeline that maps\n3D poses and textual descriptions into a joint embedding space, allowing for\ncross-modal retrieval of relevant poses from large-scale datasets. Secondly, we\nestablish a baseline for a text-conditioned model generating 3D poses. Thirdly,\nwe present a learned process for generating pose descriptions. These\napplications demonstrate the versatility and usefulness of annotated poses in\nvarious tasks and pave the way for future research in the field.\n", "link": "http://arxiv.org/abs/2210.11795v3", "date": "2024-09-10", "relevancy": 2.8513, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseScript%3A%20Linking%203D%20Human%20Poses%20and%20Natural%20Language&body=Title%3A%20PoseScript%3A%20Linking%203D%20Human%20Poses%20and%20Natural%20Language%0AAuthor%3A%20Ginger%20Delmas%20and%20Philippe%20Weinzaepfel%20and%20Thomas%20Lucas%20and%20Francesc%20Moreno-Noguer%20and%20Gr%C3%A9gory%20Rogez%0AAbstract%3A%20%20%20Natural%20language%20plays%20a%20critical%20role%20in%20many%20computer%20vision%20applications%2C%0Asuch%20as%20image%20captioning%2C%20visual%20question%20answering%2C%20and%20cross-modal%20retrieval%2C%0Ato%20provide%20fine-grained%20semantic%20information.%20Unfortunately%2C%20while%20human%20pose%0Ais%20key%20to%20human%20understanding%2C%20current%203D%20human%20pose%20datasets%20lack%20detailed%0Alanguage%20descriptions.%20To%20address%20this%20issue%2C%20we%20have%20introduced%20the%20PoseScript%0Adataset.%20This%20dataset%20pairs%20more%20than%20six%20thousand%203D%20human%20poses%20from%20AMASS%0Awith%20rich%20human-annotated%20descriptions%20of%20the%20body%20parts%20and%20their%20spatial%0Arelationships.%20Additionally%2C%20to%20increase%20the%20size%20of%20the%20dataset%20to%20a%20scale%0Athat%20is%20compatible%20with%20data-hungry%20learning%20algorithms%2C%20we%20have%20proposed%20an%0Aelaborate%20captioning%20process%20that%20generates%20automatic%20synthetic%20descriptions%20in%0Anatural%20language%20from%20given%203D%20keypoints.%20This%20process%20extracts%20low-level%20pose%0Ainformation%2C%20known%20as%20%22posecodes%22%2C%20using%20a%20set%20of%20simple%20but%20generic%20rules%20on%0Athe%203D%20keypoints.%20These%20posecodes%20are%20then%20combined%20into%20higher%20level%20textual%0Adescriptions%20using%20syntactic%20rules.%20With%20automatic%20annotations%2C%20the%20amount%20of%0Aavailable%20data%20significantly%20scales%20up%20%28100k%29%2C%20making%20it%20possible%20to%0Aeffectively%20pretrain%20deep%20models%20for%20finetuning%20on%20human%20captions.%20To%20showcase%0Athe%20potential%20of%20annotated%20poses%2C%20we%20present%20three%20multi-modal%20learning%20tasks%0Athat%20utilize%20the%20PoseScript%20dataset.%20Firstly%2C%20we%20develop%20a%20pipeline%20that%20maps%0A3D%20poses%20and%20textual%20descriptions%20into%20a%20joint%20embedding%20space%2C%20allowing%20for%0Across-modal%20retrieval%20of%20relevant%20poses%20from%20large-scale%20datasets.%20Secondly%2C%20we%0Aestablish%20a%20baseline%20for%20a%20text-conditioned%20model%20generating%203D%20poses.%20Thirdly%2C%0Awe%20present%20a%20learned%20process%20for%20generating%20pose%20descriptions.%20These%0Aapplications%20demonstrate%20the%20versatility%20and%20usefulness%20of%20annotated%20poses%20in%0Avarious%20tasks%20and%20pave%20the%20way%20for%20future%20research%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.11795v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseScript%253A%2520Linking%25203D%2520Human%2520Poses%2520and%2520Natural%2520Language%26entry.906535625%3DGinger%2520Delmas%2520and%2520Philippe%2520Weinzaepfel%2520and%2520Thomas%2520Lucas%2520and%2520Francesc%2520Moreno-Noguer%2520and%2520Gr%25C3%25A9gory%2520Rogez%26entry.1292438233%3D%2520%2520Natural%2520language%2520plays%2520a%2520critical%2520role%2520in%2520many%2520computer%2520vision%2520applications%252C%250Asuch%2520as%2520image%2520captioning%252C%2520visual%2520question%2520answering%252C%2520and%2520cross-modal%2520retrieval%252C%250Ato%2520provide%2520fine-grained%2520semantic%2520information.%2520Unfortunately%252C%2520while%2520human%2520pose%250Ais%2520key%2520to%2520human%2520understanding%252C%2520current%25203D%2520human%2520pose%2520datasets%2520lack%2520detailed%250Alanguage%2520descriptions.%2520To%2520address%2520this%2520issue%252C%2520we%2520have%2520introduced%2520the%2520PoseScript%250Adataset.%2520This%2520dataset%2520pairs%2520more%2520than%2520six%2520thousand%25203D%2520human%2520poses%2520from%2520AMASS%250Awith%2520rich%2520human-annotated%2520descriptions%2520of%2520the%2520body%2520parts%2520and%2520their%2520spatial%250Arelationships.%2520Additionally%252C%2520to%2520increase%2520the%2520size%2520of%2520the%2520dataset%2520to%2520a%2520scale%250Athat%2520is%2520compatible%2520with%2520data-hungry%2520learning%2520algorithms%252C%2520we%2520have%2520proposed%2520an%250Aelaborate%2520captioning%2520process%2520that%2520generates%2520automatic%2520synthetic%2520descriptions%2520in%250Anatural%2520language%2520from%2520given%25203D%2520keypoints.%2520This%2520process%2520extracts%2520low-level%2520pose%250Ainformation%252C%2520known%2520as%2520%2522posecodes%2522%252C%2520using%2520a%2520set%2520of%2520simple%2520but%2520generic%2520rules%2520on%250Athe%25203D%2520keypoints.%2520These%2520posecodes%2520are%2520then%2520combined%2520into%2520higher%2520level%2520textual%250Adescriptions%2520using%2520syntactic%2520rules.%2520With%2520automatic%2520annotations%252C%2520the%2520amount%2520of%250Aavailable%2520data%2520significantly%2520scales%2520up%2520%2528100k%2529%252C%2520making%2520it%2520possible%2520to%250Aeffectively%2520pretrain%2520deep%2520models%2520for%2520finetuning%2520on%2520human%2520captions.%2520To%2520showcase%250Athe%2520potential%2520of%2520annotated%2520poses%252C%2520we%2520present%2520three%2520multi-modal%2520learning%2520tasks%250Athat%2520utilize%2520the%2520PoseScript%2520dataset.%2520Firstly%252C%2520we%2520develop%2520a%2520pipeline%2520that%2520maps%250A3D%2520poses%2520and%2520textual%2520descriptions%2520into%2520a%2520joint%2520embedding%2520space%252C%2520allowing%2520for%250Across-modal%2520retrieval%2520of%2520relevant%2520poses%2520from%2520large-scale%2520datasets.%2520Secondly%252C%2520we%250Aestablish%2520a%2520baseline%2520for%2520a%2520text-conditioned%2520model%2520generating%25203D%2520poses.%2520Thirdly%252C%250Awe%2520present%2520a%2520learned%2520process%2520for%2520generating%2520pose%2520descriptions.%2520These%250Aapplications%2520demonstrate%2520the%2520versatility%2520and%2520usefulness%2520of%2520annotated%2520poses%2520in%250Avarious%2520tasks%2520and%2520pave%2520the%2520way%2520for%2520future%2520research%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.11795v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseScript%3A%20Linking%203D%20Human%20Poses%20and%20Natural%20Language&entry.906535625=Ginger%20Delmas%20and%20Philippe%20Weinzaepfel%20and%20Thomas%20Lucas%20and%20Francesc%20Moreno-Noguer%20and%20Gr%C3%A9gory%20Rogez&entry.1292438233=%20%20Natural%20language%20plays%20a%20critical%20role%20in%20many%20computer%20vision%20applications%2C%0Asuch%20as%20image%20captioning%2C%20visual%20question%20answering%2C%20and%20cross-modal%20retrieval%2C%0Ato%20provide%20fine-grained%20semantic%20information.%20Unfortunately%2C%20while%20human%20pose%0Ais%20key%20to%20human%20understanding%2C%20current%203D%20human%20pose%20datasets%20lack%20detailed%0Alanguage%20descriptions.%20To%20address%20this%20issue%2C%20we%20have%20introduced%20the%20PoseScript%0Adataset.%20This%20dataset%20pairs%20more%20than%20six%20thousand%203D%20human%20poses%20from%20AMASS%0Awith%20rich%20human-annotated%20descriptions%20of%20the%20body%20parts%20and%20their%20spatial%0Arelationships.%20Additionally%2C%20to%20increase%20the%20size%20of%20the%20dataset%20to%20a%20scale%0Athat%20is%20compatible%20with%20data-hungry%20learning%20algorithms%2C%20we%20have%20proposed%20an%0Aelaborate%20captioning%20process%20that%20generates%20automatic%20synthetic%20descriptions%20in%0Anatural%20language%20from%20given%203D%20keypoints.%20This%20process%20extracts%20low-level%20pose%0Ainformation%2C%20known%20as%20%22posecodes%22%2C%20using%20a%20set%20of%20simple%20but%20generic%20rules%20on%0Athe%203D%20keypoints.%20These%20posecodes%20are%20then%20combined%20into%20higher%20level%20textual%0Adescriptions%20using%20syntactic%20rules.%20With%20automatic%20annotations%2C%20the%20amount%20of%0Aavailable%20data%20significantly%20scales%20up%20%28100k%29%2C%20making%20it%20possible%20to%0Aeffectively%20pretrain%20deep%20models%20for%20finetuning%20on%20human%20captions.%20To%20showcase%0Athe%20potential%20of%20annotated%20poses%2C%20we%20present%20three%20multi-modal%20learning%20tasks%0Athat%20utilize%20the%20PoseScript%20dataset.%20Firstly%2C%20we%20develop%20a%20pipeline%20that%20maps%0A3D%20poses%20and%20textual%20descriptions%20into%20a%20joint%20embedding%20space%2C%20allowing%20for%0Across-modal%20retrieval%20of%20relevant%20poses%20from%20large-scale%20datasets.%20Secondly%2C%20we%0Aestablish%20a%20baseline%20for%20a%20text-conditioned%20model%20generating%203D%20poses.%20Thirdly%2C%0Awe%20present%20a%20learned%20process%20for%20generating%20pose%20descriptions.%20These%0Aapplications%20demonstrate%20the%20versatility%20and%20usefulness%20of%20annotated%20poses%20in%0Avarious%20tasks%20and%20pave%20the%20way%20for%20future%20research%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.11795v3&entry.124074799=Read"},
{"title": "GeoCalib: Learning Single-image Calibration with Geometric Optimization", "author": "Alexander Veicht and Paul-Edouard Sarlin and Philipp Lindenberger and Marc Pollefeys", "abstract": "  From a single image, visual cues can help deduce intrinsic and extrinsic\ncamera parameters like the focal length and the gravity direction. This\nsingle-image calibration can benefit various downstream applications like image\nediting and 3D mapping. Current approaches to this problem are based on either\nclassical geometry with lines and vanishing points or on deep neural networks\ntrained end-to-end. The learned approaches are more robust but struggle to\ngeneralize to new environments and are less accurate than their classical\ncounterparts. We hypothesize that they lack the constraints that 3D geometry\nprovides. In this work, we introduce GeoCalib, a deep neural network that\nleverages universal rules of 3D geometry through an optimization process.\nGeoCalib is trained end-to-end to estimate camera parameters and learns to find\nuseful visual cues from the data. Experiments on various benchmarks show that\nGeoCalib is more robust and more accurate than existing classical and learned\napproaches. Its internal optimization estimates uncertainties, which help flag\nfailure cases and benefit downstream applications like visual localization. The\ncode and trained models are publicly available at\nhttps://github.com/cvg/GeoCalib.\n", "link": "http://arxiv.org/abs/2409.06704v1", "date": "2024-09-10", "relevancy": 2.7958, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5973}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoCalib%3A%20Learning%20Single-image%20Calibration%20with%20Geometric%20Optimization&body=Title%3A%20GeoCalib%3A%20Learning%20Single-image%20Calibration%20with%20Geometric%20Optimization%0AAuthor%3A%20Alexander%20Veicht%20and%20Paul-Edouard%20Sarlin%20and%20Philipp%20Lindenberger%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20From%20a%20single%20image%2C%20visual%20cues%20can%20help%20deduce%20intrinsic%20and%20extrinsic%0Acamera%20parameters%20like%20the%20focal%20length%20and%20the%20gravity%20direction.%20This%0Asingle-image%20calibration%20can%20benefit%20various%20downstream%20applications%20like%20image%0Aediting%20and%203D%20mapping.%20Current%20approaches%20to%20this%20problem%20are%20based%20on%20either%0Aclassical%20geometry%20with%20lines%20and%20vanishing%20points%20or%20on%20deep%20neural%20networks%0Atrained%20end-to-end.%20The%20learned%20approaches%20are%20more%20robust%20but%20struggle%20to%0Ageneralize%20to%20new%20environments%20and%20are%20less%20accurate%20than%20their%20classical%0Acounterparts.%20We%20hypothesize%20that%20they%20lack%20the%20constraints%20that%203D%20geometry%0Aprovides.%20In%20this%20work%2C%20we%20introduce%20GeoCalib%2C%20a%20deep%20neural%20network%20that%0Aleverages%20universal%20rules%20of%203D%20geometry%20through%20an%20optimization%20process.%0AGeoCalib%20is%20trained%20end-to-end%20to%20estimate%20camera%20parameters%20and%20learns%20to%20find%0Auseful%20visual%20cues%20from%20the%20data.%20Experiments%20on%20various%20benchmarks%20show%20that%0AGeoCalib%20is%20more%20robust%20and%20more%20accurate%20than%20existing%20classical%20and%20learned%0Aapproaches.%20Its%20internal%20optimization%20estimates%20uncertainties%2C%20which%20help%20flag%0Afailure%20cases%20and%20benefit%20downstream%20applications%20like%20visual%20localization.%20The%0Acode%20and%20trained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/cvg/GeoCalib.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoCalib%253A%2520Learning%2520Single-image%2520Calibration%2520with%2520Geometric%2520Optimization%26entry.906535625%3DAlexander%2520Veicht%2520and%2520Paul-Edouard%2520Sarlin%2520and%2520Philipp%2520Lindenberger%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520From%2520a%2520single%2520image%252C%2520visual%2520cues%2520can%2520help%2520deduce%2520intrinsic%2520and%2520extrinsic%250Acamera%2520parameters%2520like%2520the%2520focal%2520length%2520and%2520the%2520gravity%2520direction.%2520This%250Asingle-image%2520calibration%2520can%2520benefit%2520various%2520downstream%2520applications%2520like%2520image%250Aediting%2520and%25203D%2520mapping.%2520Current%2520approaches%2520to%2520this%2520problem%2520are%2520based%2520on%2520either%250Aclassical%2520geometry%2520with%2520lines%2520and%2520vanishing%2520points%2520or%2520on%2520deep%2520neural%2520networks%250Atrained%2520end-to-end.%2520The%2520learned%2520approaches%2520are%2520more%2520robust%2520but%2520struggle%2520to%250Ageneralize%2520to%2520new%2520environments%2520and%2520are%2520less%2520accurate%2520than%2520their%2520classical%250Acounterparts.%2520We%2520hypothesize%2520that%2520they%2520lack%2520the%2520constraints%2520that%25203D%2520geometry%250Aprovides.%2520In%2520this%2520work%252C%2520we%2520introduce%2520GeoCalib%252C%2520a%2520deep%2520neural%2520network%2520that%250Aleverages%2520universal%2520rules%2520of%25203D%2520geometry%2520through%2520an%2520optimization%2520process.%250AGeoCalib%2520is%2520trained%2520end-to-end%2520to%2520estimate%2520camera%2520parameters%2520and%2520learns%2520to%2520find%250Auseful%2520visual%2520cues%2520from%2520the%2520data.%2520Experiments%2520on%2520various%2520benchmarks%2520show%2520that%250AGeoCalib%2520is%2520more%2520robust%2520and%2520more%2520accurate%2520than%2520existing%2520classical%2520and%2520learned%250Aapproaches.%2520Its%2520internal%2520optimization%2520estimates%2520uncertainties%252C%2520which%2520help%2520flag%250Afailure%2520cases%2520and%2520benefit%2520downstream%2520applications%2520like%2520visual%2520localization.%2520The%250Acode%2520and%2520trained%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/cvg/GeoCalib.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoCalib%3A%20Learning%20Single-image%20Calibration%20with%20Geometric%20Optimization&entry.906535625=Alexander%20Veicht%20and%20Paul-Edouard%20Sarlin%20and%20Philipp%20Lindenberger%20and%20Marc%20Pollefeys&entry.1292438233=%20%20From%20a%20single%20image%2C%20visual%20cues%20can%20help%20deduce%20intrinsic%20and%20extrinsic%0Acamera%20parameters%20like%20the%20focal%20length%20and%20the%20gravity%20direction.%20This%0Asingle-image%20calibration%20can%20benefit%20various%20downstream%20applications%20like%20image%0Aediting%20and%203D%20mapping.%20Current%20approaches%20to%20this%20problem%20are%20based%20on%20either%0Aclassical%20geometry%20with%20lines%20and%20vanishing%20points%20or%20on%20deep%20neural%20networks%0Atrained%20end-to-end.%20The%20learned%20approaches%20are%20more%20robust%20but%20struggle%20to%0Ageneralize%20to%20new%20environments%20and%20are%20less%20accurate%20than%20their%20classical%0Acounterparts.%20We%20hypothesize%20that%20they%20lack%20the%20constraints%20that%203D%20geometry%0Aprovides.%20In%20this%20work%2C%20we%20introduce%20GeoCalib%2C%20a%20deep%20neural%20network%20that%0Aleverages%20universal%20rules%20of%203D%20geometry%20through%20an%20optimization%20process.%0AGeoCalib%20is%20trained%20end-to-end%20to%20estimate%20camera%20parameters%20and%20learns%20to%20find%0Auseful%20visual%20cues%20from%20the%20data.%20Experiments%20on%20various%20benchmarks%20show%20that%0AGeoCalib%20is%20more%20robust%20and%20more%20accurate%20than%20existing%20classical%20and%20learned%0Aapproaches.%20Its%20internal%20optimization%20estimates%20uncertainties%2C%20which%20help%20flag%0Afailure%20cases%20and%20benefit%20downstream%20applications%20like%20visual%20localization.%20The%0Acode%20and%20trained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/cvg/GeoCalib.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06704v1&entry.124074799=Read"},
{"title": "Aligning Machine and Human Visual Representations across Abstraction\n  Levels", "author": "Lukas Muttenthaler and Klaus Greff and Frieda Born and Bernhard Spitzer and Simon Kornblith and Michael C. Mozer and Klaus-Robert M\u00fcller and Thomas Unterthiner and Andrew K. Lampinen", "abstract": "  Deep neural networks have achieved success across a wide range of\napplications, including as models of human behavior in vision tasks. However,\nneural network training and human learning differ in fundamental ways, and\nneural networks often fail to generalize as robustly as humans do, raising\nquestions regarding the similarity of their underlying representations. What is\nmissing for modern learning systems to exhibit more human-like behavior? We\nhighlight a key misalignment between vision models and humans: whereas human\nconceptual knowledge is hierarchically organized from fine- to coarse-scale\ndistinctions, model representations do not accurately capture all these levels\nof abstraction. To address this misalignment, we first train a teacher model to\nimitate human judgments, then transfer human-like structure from its\nrepresentations into pretrained state-of-the-art vision foundation models.\nThese human-aligned models more accurately approximate human behavior and\nuncertainty across a wide range of similarity tasks, including a new dataset of\nhuman judgments spanning multiple levels of semantic abstractions. They also\nperform better on a diverse set of machine learning tasks, increasing\ngeneralization and out-of-distribution robustness. Thus, infusing neural\nnetworks with additional human knowledge yields a best-of-both-worlds\nrepresentation that is both more consistent with human cognition and more\npractically useful, thus paving the way toward more robust, interpretable, and\nhuman-like artificial intelligence systems.\n", "link": "http://arxiv.org/abs/2409.06509v1", "date": "2024-09-10", "relevancy": 2.7846, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels&body=Title%3A%20Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels%0AAuthor%3A%20Lukas%20Muttenthaler%20and%20Klaus%20Greff%20and%20Frieda%20Born%20and%20Bernhard%20Spitzer%20and%20Simon%20Kornblith%20and%20Michael%20C.%20Mozer%20and%20Klaus-Robert%20M%C3%BCller%20and%20Thomas%20Unterthiner%20and%20Andrew%20K.%20Lampinen%0AAbstract%3A%20%20%20Deep%20neural%20networks%20have%20achieved%20success%20across%20a%20wide%20range%20of%0Aapplications%2C%20including%20as%20models%20of%20human%20behavior%20in%20vision%20tasks.%20However%2C%0Aneural%20network%20training%20and%20human%20learning%20differ%20in%20fundamental%20ways%2C%20and%0Aneural%20networks%20often%20fail%20to%20generalize%20as%20robustly%20as%20humans%20do%2C%20raising%0Aquestions%20regarding%20the%20similarity%20of%20their%20underlying%20representations.%20What%20is%0Amissing%20for%20modern%20learning%20systems%20to%20exhibit%20more%20human-like%20behavior%3F%20We%0Ahighlight%20a%20key%20misalignment%20between%20vision%20models%20and%20humans%3A%20whereas%20human%0Aconceptual%20knowledge%20is%20hierarchically%20organized%20from%20fine-%20to%20coarse-scale%0Adistinctions%2C%20model%20representations%20do%20not%20accurately%20capture%20all%20these%20levels%0Aof%20abstraction.%20To%20address%20this%20misalignment%2C%20we%20first%20train%20a%20teacher%20model%20to%0Aimitate%20human%20judgments%2C%20then%20transfer%20human-like%20structure%20from%20its%0Arepresentations%20into%20pretrained%20state-of-the-art%20vision%20foundation%20models.%0AThese%20human-aligned%20models%20more%20accurately%20approximate%20human%20behavior%20and%0Auncertainty%20across%20a%20wide%20range%20of%20similarity%20tasks%2C%20including%20a%20new%20dataset%20of%0Ahuman%20judgments%20spanning%20multiple%20levels%20of%20semantic%20abstractions.%20They%20also%0Aperform%20better%20on%20a%20diverse%20set%20of%20machine%20learning%20tasks%2C%20increasing%0Ageneralization%20and%20out-of-distribution%20robustness.%20Thus%2C%20infusing%20neural%0Anetworks%20with%20additional%20human%20knowledge%20yields%20a%20best-of-both-worlds%0Arepresentation%20that%20is%20both%20more%20consistent%20with%20human%20cognition%20and%20more%0Apractically%20useful%2C%20thus%20paving%20the%20way%20toward%20more%20robust%2C%20interpretable%2C%20and%0Ahuman-like%20artificial%20intelligence%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Machine%2520and%2520Human%2520Visual%2520Representations%2520across%2520Abstraction%250A%2520%2520Levels%26entry.906535625%3DLukas%2520Muttenthaler%2520and%2520Klaus%2520Greff%2520and%2520Frieda%2520Born%2520and%2520Bernhard%2520Spitzer%2520and%2520Simon%2520Kornblith%2520and%2520Michael%2520C.%2520Mozer%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Thomas%2520Unterthiner%2520and%2520Andrew%2520K.%2520Lampinen%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520have%2520achieved%2520success%2520across%2520a%2520wide%2520range%2520of%250Aapplications%252C%2520including%2520as%2520models%2520of%2520human%2520behavior%2520in%2520vision%2520tasks.%2520However%252C%250Aneural%2520network%2520training%2520and%2520human%2520learning%2520differ%2520in%2520fundamental%2520ways%252C%2520and%250Aneural%2520networks%2520often%2520fail%2520to%2520generalize%2520as%2520robustly%2520as%2520humans%2520do%252C%2520raising%250Aquestions%2520regarding%2520the%2520similarity%2520of%2520their%2520underlying%2520representations.%2520What%2520is%250Amissing%2520for%2520modern%2520learning%2520systems%2520to%2520exhibit%2520more%2520human-like%2520behavior%253F%2520We%250Ahighlight%2520a%2520key%2520misalignment%2520between%2520vision%2520models%2520and%2520humans%253A%2520whereas%2520human%250Aconceptual%2520knowledge%2520is%2520hierarchically%2520organized%2520from%2520fine-%2520to%2520coarse-scale%250Adistinctions%252C%2520model%2520representations%2520do%2520not%2520accurately%2520capture%2520all%2520these%2520levels%250Aof%2520abstraction.%2520To%2520address%2520this%2520misalignment%252C%2520we%2520first%2520train%2520a%2520teacher%2520model%2520to%250Aimitate%2520human%2520judgments%252C%2520then%2520transfer%2520human-like%2520structure%2520from%2520its%250Arepresentations%2520into%2520pretrained%2520state-of-the-art%2520vision%2520foundation%2520models.%250AThese%2520human-aligned%2520models%2520more%2520accurately%2520approximate%2520human%2520behavior%2520and%250Auncertainty%2520across%2520a%2520wide%2520range%2520of%2520similarity%2520tasks%252C%2520including%2520a%2520new%2520dataset%2520of%250Ahuman%2520judgments%2520spanning%2520multiple%2520levels%2520of%2520semantic%2520abstractions.%2520They%2520also%250Aperform%2520better%2520on%2520a%2520diverse%2520set%2520of%2520machine%2520learning%2520tasks%252C%2520increasing%250Ageneralization%2520and%2520out-of-distribution%2520robustness.%2520Thus%252C%2520infusing%2520neural%250Anetworks%2520with%2520additional%2520human%2520knowledge%2520yields%2520a%2520best-of-both-worlds%250Arepresentation%2520that%2520is%2520both%2520more%2520consistent%2520with%2520human%2520cognition%2520and%2520more%250Apractically%2520useful%252C%2520thus%2520paving%2520the%2520way%2520toward%2520more%2520robust%252C%2520interpretable%252C%2520and%250Ahuman-like%2520artificial%2520intelligence%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Machine%20and%20Human%20Visual%20Representations%20across%20Abstraction%0A%20%20Levels&entry.906535625=Lukas%20Muttenthaler%20and%20Klaus%20Greff%20and%20Frieda%20Born%20and%20Bernhard%20Spitzer%20and%20Simon%20Kornblith%20and%20Michael%20C.%20Mozer%20and%20Klaus-Robert%20M%C3%BCller%20and%20Thomas%20Unterthiner%20and%20Andrew%20K.%20Lampinen&entry.1292438233=%20%20Deep%20neural%20networks%20have%20achieved%20success%20across%20a%20wide%20range%20of%0Aapplications%2C%20including%20as%20models%20of%20human%20behavior%20in%20vision%20tasks.%20However%2C%0Aneural%20network%20training%20and%20human%20learning%20differ%20in%20fundamental%20ways%2C%20and%0Aneural%20networks%20often%20fail%20to%20generalize%20as%20robustly%20as%20humans%20do%2C%20raising%0Aquestions%20regarding%20the%20similarity%20of%20their%20underlying%20representations.%20What%20is%0Amissing%20for%20modern%20learning%20systems%20to%20exhibit%20more%20human-like%20behavior%3F%20We%0Ahighlight%20a%20key%20misalignment%20between%20vision%20models%20and%20humans%3A%20whereas%20human%0Aconceptual%20knowledge%20is%20hierarchically%20organized%20from%20fine-%20to%20coarse-scale%0Adistinctions%2C%20model%20representations%20do%20not%20accurately%20capture%20all%20these%20levels%0Aof%20abstraction.%20To%20address%20this%20misalignment%2C%20we%20first%20train%20a%20teacher%20model%20to%0Aimitate%20human%20judgments%2C%20then%20transfer%20human-like%20structure%20from%20its%0Arepresentations%20into%20pretrained%20state-of-the-art%20vision%20foundation%20models.%0AThese%20human-aligned%20models%20more%20accurately%20approximate%20human%20behavior%20and%0Auncertainty%20across%20a%20wide%20range%20of%20similarity%20tasks%2C%20including%20a%20new%20dataset%20of%0Ahuman%20judgments%20spanning%20multiple%20levels%20of%20semantic%20abstractions.%20They%20also%0Aperform%20better%20on%20a%20diverse%20set%20of%20machine%20learning%20tasks%2C%20increasing%0Ageneralization%20and%20out-of-distribution%20robustness.%20Thus%2C%20infusing%20neural%0Anetworks%20with%20additional%20human%20knowledge%20yields%20a%20best-of-both-worlds%0Arepresentation%20that%20is%20both%20more%20consistent%20with%20human%20cognition%20and%20more%0Apractically%20useful%2C%20thus%20paving%20the%20way%20toward%20more%20robust%2C%20interpretable%2C%20and%0Ahuman-like%20artificial%20intelligence%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06509v1&entry.124074799=Read"},
{"title": "World-Grounded Human Motion Recovery via Gravity-View Coordinates", "author": "Zehong Shen and Huaijin Pi and Yan Xia and Zhi Cen and Sida Peng and Zechen Hu and Hujun Bao and Ruizhen Hu and Xiaowei Zhou", "abstract": "  We present a novel method for recovering world-grounded human motion from\nmonocular video. The main challenge lies in the ambiguity of defining the world\ncoordinate system, which varies between sequences. Previous approaches attempt\nto alleviate this issue by predicting relative motion in an autoregressive\nmanner, but are prone to accumulating errors. Instead, we propose estimating\nhuman poses in a novel Gravity-View (GV) coordinate system, which is defined by\nthe world gravity and the camera view direction. The proposed GV system is\nnaturally gravity-aligned and uniquely defined for each video frame, largely\nreducing the ambiguity of learning image-pose mapping. The estimated poses can\nbe transformed back to the world coordinate system using camera rotations,\nforming a global motion sequence. Additionally, the per-frame estimation avoids\nerror accumulation in the autoregressive methods. Experiments on in-the-wild\nbenchmarks demonstrate that our method recovers more realistic motion in both\nthe camera space and world-grounded settings, outperforming state-of-the-art\nmethods in both accuracy and speed. The code is available at\nhttps://zju3dv.github.io/gvhmr/.\n", "link": "http://arxiv.org/abs/2409.06662v1", "date": "2024-09-10", "relevancy": 2.773, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5894}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5414}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20World-Grounded%20Human%20Motion%20Recovery%20via%20Gravity-View%20Coordinates&body=Title%3A%20World-Grounded%20Human%20Motion%20Recovery%20via%20Gravity-View%20Coordinates%0AAuthor%3A%20Zehong%20Shen%20and%20Huaijin%20Pi%20and%20Yan%20Xia%20and%20Zhi%20Cen%20and%20Sida%20Peng%20and%20Zechen%20Hu%20and%20Hujun%20Bao%20and%20Ruizhen%20Hu%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20recovering%20world-grounded%20human%20motion%20from%0Amonocular%20video.%20The%20main%20challenge%20lies%20in%20the%20ambiguity%20of%20defining%20the%20world%0Acoordinate%20system%2C%20which%20varies%20between%20sequences.%20Previous%20approaches%20attempt%0Ato%20alleviate%20this%20issue%20by%20predicting%20relative%20motion%20in%20an%20autoregressive%0Amanner%2C%20but%20are%20prone%20to%20accumulating%20errors.%20Instead%2C%20we%20propose%20estimating%0Ahuman%20poses%20in%20a%20novel%20Gravity-View%20%28GV%29%20coordinate%20system%2C%20which%20is%20defined%20by%0Athe%20world%20gravity%20and%20the%20camera%20view%20direction.%20The%20proposed%20GV%20system%20is%0Anaturally%20gravity-aligned%20and%20uniquely%20defined%20for%20each%20video%20frame%2C%20largely%0Areducing%20the%20ambiguity%20of%20learning%20image-pose%20mapping.%20The%20estimated%20poses%20can%0Abe%20transformed%20back%20to%20the%20world%20coordinate%20system%20using%20camera%20rotations%2C%0Aforming%20a%20global%20motion%20sequence.%20Additionally%2C%20the%20per-frame%20estimation%20avoids%0Aerror%20accumulation%20in%20the%20autoregressive%20methods.%20Experiments%20on%20in-the-wild%0Abenchmarks%20demonstrate%20that%20our%20method%20recovers%20more%20realistic%20motion%20in%20both%0Athe%20camera%20space%20and%20world-grounded%20settings%2C%20outperforming%20state-of-the-art%0Amethods%20in%20both%20accuracy%20and%20speed.%20The%20code%20is%20available%20at%0Ahttps%3A//zju3dv.github.io/gvhmr/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorld-Grounded%2520Human%2520Motion%2520Recovery%2520via%2520Gravity-View%2520Coordinates%26entry.906535625%3DZehong%2520Shen%2520and%2520Huaijin%2520Pi%2520and%2520Yan%2520Xia%2520and%2520Zhi%2520Cen%2520and%2520Sida%2520Peng%2520and%2520Zechen%2520Hu%2520and%2520Hujun%2520Bao%2520and%2520Ruizhen%2520Hu%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520method%2520for%2520recovering%2520world-grounded%2520human%2520motion%2520from%250Amonocular%2520video.%2520The%2520main%2520challenge%2520lies%2520in%2520the%2520ambiguity%2520of%2520defining%2520the%2520world%250Acoordinate%2520system%252C%2520which%2520varies%2520between%2520sequences.%2520Previous%2520approaches%2520attempt%250Ato%2520alleviate%2520this%2520issue%2520by%2520predicting%2520relative%2520motion%2520in%2520an%2520autoregressive%250Amanner%252C%2520but%2520are%2520prone%2520to%2520accumulating%2520errors.%2520Instead%252C%2520we%2520propose%2520estimating%250Ahuman%2520poses%2520in%2520a%2520novel%2520Gravity-View%2520%2528GV%2529%2520coordinate%2520system%252C%2520which%2520is%2520defined%2520by%250Athe%2520world%2520gravity%2520and%2520the%2520camera%2520view%2520direction.%2520The%2520proposed%2520GV%2520system%2520is%250Anaturally%2520gravity-aligned%2520and%2520uniquely%2520defined%2520for%2520each%2520video%2520frame%252C%2520largely%250Areducing%2520the%2520ambiguity%2520of%2520learning%2520image-pose%2520mapping.%2520The%2520estimated%2520poses%2520can%250Abe%2520transformed%2520back%2520to%2520the%2520world%2520coordinate%2520system%2520using%2520camera%2520rotations%252C%250Aforming%2520a%2520global%2520motion%2520sequence.%2520Additionally%252C%2520the%2520per-frame%2520estimation%2520avoids%250Aerror%2520accumulation%2520in%2520the%2520autoregressive%2520methods.%2520Experiments%2520on%2520in-the-wild%250Abenchmarks%2520demonstrate%2520that%2520our%2520method%2520recovers%2520more%2520realistic%2520motion%2520in%2520both%250Athe%2520camera%2520space%2520and%2520world-grounded%2520settings%252C%2520outperforming%2520state-of-the-art%250Amethods%2520in%2520both%2520accuracy%2520and%2520speed.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//zju3dv.github.io/gvhmr/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=World-Grounded%20Human%20Motion%20Recovery%20via%20Gravity-View%20Coordinates&entry.906535625=Zehong%20Shen%20and%20Huaijin%20Pi%20and%20Yan%20Xia%20and%20Zhi%20Cen%20and%20Sida%20Peng%20and%20Zechen%20Hu%20and%20Hujun%20Bao%20and%20Ruizhen%20Hu%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20recovering%20world-grounded%20human%20motion%20from%0Amonocular%20video.%20The%20main%20challenge%20lies%20in%20the%20ambiguity%20of%20defining%20the%20world%0Acoordinate%20system%2C%20which%20varies%20between%20sequences.%20Previous%20approaches%20attempt%0Ato%20alleviate%20this%20issue%20by%20predicting%20relative%20motion%20in%20an%20autoregressive%0Amanner%2C%20but%20are%20prone%20to%20accumulating%20errors.%20Instead%2C%20we%20propose%20estimating%0Ahuman%20poses%20in%20a%20novel%20Gravity-View%20%28GV%29%20coordinate%20system%2C%20which%20is%20defined%20by%0Athe%20world%20gravity%20and%20the%20camera%20view%20direction.%20The%20proposed%20GV%20system%20is%0Anaturally%20gravity-aligned%20and%20uniquely%20defined%20for%20each%20video%20frame%2C%20largely%0Areducing%20the%20ambiguity%20of%20learning%20image-pose%20mapping.%20The%20estimated%20poses%20can%0Abe%20transformed%20back%20to%20the%20world%20coordinate%20system%20using%20camera%20rotations%2C%0Aforming%20a%20global%20motion%20sequence.%20Additionally%2C%20the%20per-frame%20estimation%20avoids%0Aerror%20accumulation%20in%20the%20autoregressive%20methods.%20Experiments%20on%20in-the-wild%0Abenchmarks%20demonstrate%20that%20our%20method%20recovers%20more%20realistic%20motion%20in%20both%0Athe%20camera%20space%20and%20world-grounded%20settings%2C%20outperforming%20state-of-the-art%0Amethods%20in%20both%20accuracy%20and%20speed.%20The%20code%20is%20available%20at%0Ahttps%3A//zju3dv.github.io/gvhmr/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06662v1&entry.124074799=Read"},
{"title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "author": "Chaoyou Fu and Haojia Lin and Zuwei Long and Yunhang Shen and Meng Zhao and Yifan Zhang and Shaoqi Dong and Xiong Wang and Di Yin and Long Ma and Xiawu Zheng and Ran He and Rongrong Ji and Yunsheng Wu and Caifeng Shan and Xing Sun", "abstract": "  The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io.\n", "link": "http://arxiv.org/abs/2408.05211v2", "date": "2024-09-10", "relevancy": 2.7719, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM&body=Title%3A%20VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM%0AAuthor%3A%20Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Zuwei%20Long%20and%20Yunhang%20Shen%20and%20Meng%20Zhao%20and%20Yifan%20Zhang%20and%20Shaoqi%20Dong%20and%20Xiong%20Wang%20and%20Di%20Yin%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Ran%20He%20and%20Rongrong%20Ji%20and%20Yunsheng%20Wu%20and%20Caifeng%20Shan%20and%20Xing%20Sun%0AAbstract%3A%20%20%20The%20remarkable%20multimodal%20capabilities%20and%20interactive%20experience%20of%20GPT-4o%0Aunderscore%20their%20necessity%20in%20practical%20applications%2C%20yet%20open-source%20models%0Ararely%20excel%20in%20both%20areas.%20In%20this%20paper%2C%20we%20introduce%20VITA%2C%20the%20first-ever%0Aopen-source%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20adept%20at%20simultaneous%0Aprocessing%20and%20analysis%20of%20Video%2C%20Image%2C%20Text%2C%20and%20Audio%20modalities%2C%20and%0Ameanwhile%20has%20an%20advanced%20multimodal%20interactive%20experience.%20Starting%20from%0AMixtral%208x7B%20as%20a%20language%20foundation%2C%20we%20expand%20its%20Chinese%20vocabulary%0Afollowed%20by%20bilingual%20instruction%20tuning.%20We%20further%20endow%20the%20language%20model%0Awith%20visual%20and%20audio%20capabilities%20through%20two-stage%20multi-task%20learning%20of%0Amultimodal%20alignment%20and%20instruction%20tuning.%20VITA%20demonstrates%20robust%0Afoundational%20capabilities%20of%20multilingual%2C%20vision%2C%20and%20audio%20understanding%2C%20as%0Aevidenced%20by%20its%20strong%20performance%20across%20a%20range%20of%20both%20unimodal%20and%0Amultimodal%20benchmarks.%20Beyond%20foundational%20capabilities%2C%20we%20have%20made%0Aconsiderable%20progress%20in%20enhancing%20the%20natural%20multimodal%20human-computer%0Ainteraction%20experience.%20VITA%20is%20the%20first%20step%20for%20the%20open-source%20community%20to%0Aexplore%20the%20seamless%20integration%20of%20multimodal%20understanding%20and%20interaction.%0AWhile%20there%20is%20still%20lots%20of%20work%20to%20be%20done%20on%20VITA%20to%20get%20close%20to%0Aclose-source%20counterparts%2C%20we%20hope%20that%20its%20role%20as%20a%20pioneer%20can%20serve%20as%20a%0Acornerstone%20for%20subsequent%20research.%20Project%20Page%3A%20https%3A//vita-home.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVITA%253A%2520Towards%2520Open-Source%2520Interactive%2520Omni%2520Multimodal%2520LLM%26entry.906535625%3DChaoyou%2520Fu%2520and%2520Haojia%2520Lin%2520and%2520Zuwei%2520Long%2520and%2520Yunhang%2520Shen%2520and%2520Meng%2520Zhao%2520and%2520Yifan%2520Zhang%2520and%2520Shaoqi%2520Dong%2520and%2520Xiong%2520Wang%2520and%2520Di%2520Yin%2520and%2520Long%2520Ma%2520and%2520Xiawu%2520Zheng%2520and%2520Ran%2520He%2520and%2520Rongrong%2520Ji%2520and%2520Yunsheng%2520Wu%2520and%2520Caifeng%2520Shan%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520The%2520remarkable%2520multimodal%2520capabilities%2520and%2520interactive%2520experience%2520of%2520GPT-4o%250Aunderscore%2520their%2520necessity%2520in%2520practical%2520applications%252C%2520yet%2520open-source%2520models%250Ararely%2520excel%2520in%2520both%2520areas.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VITA%252C%2520the%2520first-ever%250Aopen-source%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520adept%2520at%2520simultaneous%250Aprocessing%2520and%2520analysis%2520of%2520Video%252C%2520Image%252C%2520Text%252C%2520and%2520Audio%2520modalities%252C%2520and%250Ameanwhile%2520has%2520an%2520advanced%2520multimodal%2520interactive%2520experience.%2520Starting%2520from%250AMixtral%25208x7B%2520as%2520a%2520language%2520foundation%252C%2520we%2520expand%2520its%2520Chinese%2520vocabulary%250Afollowed%2520by%2520bilingual%2520instruction%2520tuning.%2520We%2520further%2520endow%2520the%2520language%2520model%250Awith%2520visual%2520and%2520audio%2520capabilities%2520through%2520two-stage%2520multi-task%2520learning%2520of%250Amultimodal%2520alignment%2520and%2520instruction%2520tuning.%2520VITA%2520demonstrates%2520robust%250Afoundational%2520capabilities%2520of%2520multilingual%252C%2520vision%252C%2520and%2520audio%2520understanding%252C%2520as%250Aevidenced%2520by%2520its%2520strong%2520performance%2520across%2520a%2520range%2520of%2520both%2520unimodal%2520and%250Amultimodal%2520benchmarks.%2520Beyond%2520foundational%2520capabilities%252C%2520we%2520have%2520made%250Aconsiderable%2520progress%2520in%2520enhancing%2520the%2520natural%2520multimodal%2520human-computer%250Ainteraction%2520experience.%2520VITA%2520is%2520the%2520first%2520step%2520for%2520the%2520open-source%2520community%2520to%250Aexplore%2520the%2520seamless%2520integration%2520of%2520multimodal%2520understanding%2520and%2520interaction.%250AWhile%2520there%2520is%2520still%2520lots%2520of%2520work%2520to%2520be%2520done%2520on%2520VITA%2520to%2520get%2520close%2520to%250Aclose-source%2520counterparts%252C%2520we%2520hope%2520that%2520its%2520role%2520as%2520a%2520pioneer%2520can%2520serve%2520as%2520a%250Acornerstone%2520for%2520subsequent%2520research.%2520Project%2520Page%253A%2520https%253A//vita-home.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VITA%3A%20Towards%20Open-Source%20Interactive%20Omni%20Multimodal%20LLM&entry.906535625=Chaoyou%20Fu%20and%20Haojia%20Lin%20and%20Zuwei%20Long%20and%20Yunhang%20Shen%20and%20Meng%20Zhao%20and%20Yifan%20Zhang%20and%20Shaoqi%20Dong%20and%20Xiong%20Wang%20and%20Di%20Yin%20and%20Long%20Ma%20and%20Xiawu%20Zheng%20and%20Ran%20He%20and%20Rongrong%20Ji%20and%20Yunsheng%20Wu%20and%20Caifeng%20Shan%20and%20Xing%20Sun&entry.1292438233=%20%20The%20remarkable%20multimodal%20capabilities%20and%20interactive%20experience%20of%20GPT-4o%0Aunderscore%20their%20necessity%20in%20practical%20applications%2C%20yet%20open-source%20models%0Ararely%20excel%20in%20both%20areas.%20In%20this%20paper%2C%20we%20introduce%20VITA%2C%20the%20first-ever%0Aopen-source%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20adept%20at%20simultaneous%0Aprocessing%20and%20analysis%20of%20Video%2C%20Image%2C%20Text%2C%20and%20Audio%20modalities%2C%20and%0Ameanwhile%20has%20an%20advanced%20multimodal%20interactive%20experience.%20Starting%20from%0AMixtral%208x7B%20as%20a%20language%20foundation%2C%20we%20expand%20its%20Chinese%20vocabulary%0Afollowed%20by%20bilingual%20instruction%20tuning.%20We%20further%20endow%20the%20language%20model%0Awith%20visual%20and%20audio%20capabilities%20through%20two-stage%20multi-task%20learning%20of%0Amultimodal%20alignment%20and%20instruction%20tuning.%20VITA%20demonstrates%20robust%0Afoundational%20capabilities%20of%20multilingual%2C%20vision%2C%20and%20audio%20understanding%2C%20as%0Aevidenced%20by%20its%20strong%20performance%20across%20a%20range%20of%20both%20unimodal%20and%0Amultimodal%20benchmarks.%20Beyond%20foundational%20capabilities%2C%20we%20have%20made%0Aconsiderable%20progress%20in%20enhancing%20the%20natural%20multimodal%20human-computer%0Ainteraction%20experience.%20VITA%20is%20the%20first%20step%20for%20the%20open-source%20community%20to%0Aexplore%20the%20seamless%20integration%20of%20multimodal%20understanding%20and%20interaction.%0AWhile%20there%20is%20still%20lots%20of%20work%20to%20be%20done%20on%20VITA%20to%20get%20close%20to%0Aclose-source%20counterparts%2C%20we%20hope%20that%20its%20role%20as%20a%20pioneer%20can%20serve%20as%20a%0Acornerstone%20for%20subsequent%20research.%20Project%20Page%3A%20https%3A//vita-home.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05211v2&entry.124074799=Read"},
{"title": "Alignist: CAD-Informed Orientation Distribution Estimation by Fusing\n  Shape and Correspondences", "author": "Shishir Reddy Vutukur and Rasmus Laurvig Haugaard and Junwen Huang and Benjamin Busam and Tolga Birdal", "abstract": "  Object pose distribution estimation is crucial in robotics for better path\nplanning and handling of symmetric objects. Recent distribution estimation\napproaches employ contrastive learning-based approaches by maximizing the\nlikelihood of a single pose estimate in the absence of a CAD model. We propose\na pose distribution estimation method leveraging symmetry respecting\ncorrespondence distributions and shape information obtained using a CAD model.\nContrastive learning-based approaches require an exhaustive amount of training\nimages from different viewpoints to learn the distribution properly, which is\nnot possible in realistic scenarios. Instead, we propose a pipeline that can\nleverage correspondence distributions and shape information from the CAD model,\nwhich are later used to learn pose distributions. Besides, having access to\npose distribution based on correspondences before learning pose distributions\nconditioned on images, can help formulate the loss between distributions. The\nprior knowledge of distribution also helps the network to focus on getting\nsharper modes instead. With the CAD prior, our approach converges much faster\nand learns distribution better by focusing on learning sharper distribution\nnear all the valid modes, unlike contrastive approaches, which focus on a\nsingle mode at a time. We achieve benchmark results on SYMSOL-I and T-Less\ndatasets.\n", "link": "http://arxiv.org/abs/2409.06683v1", "date": "2024-09-10", "relevancy": 2.7676, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.557}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5555}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignist%3A%20CAD-Informed%20Orientation%20Distribution%20Estimation%20by%20Fusing%0A%20%20Shape%20and%20Correspondences&body=Title%3A%20Alignist%3A%20CAD-Informed%20Orientation%20Distribution%20Estimation%20by%20Fusing%0A%20%20Shape%20and%20Correspondences%0AAuthor%3A%20Shishir%20Reddy%20Vutukur%20and%20Rasmus%20Laurvig%20Haugaard%20and%20Junwen%20Huang%20and%20Benjamin%20Busam%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Object%20pose%20distribution%20estimation%20is%20crucial%20in%20robotics%20for%20better%20path%0Aplanning%20and%20handling%20of%20symmetric%20objects.%20Recent%20distribution%20estimation%0Aapproaches%20employ%20contrastive%20learning-based%20approaches%20by%20maximizing%20the%0Alikelihood%20of%20a%20single%20pose%20estimate%20in%20the%20absence%20of%20a%20CAD%20model.%20We%20propose%0Aa%20pose%20distribution%20estimation%20method%20leveraging%20symmetry%20respecting%0Acorrespondence%20distributions%20and%20shape%20information%20obtained%20using%20a%20CAD%20model.%0AContrastive%20learning-based%20approaches%20require%20an%20exhaustive%20amount%20of%20training%0Aimages%20from%20different%20viewpoints%20to%20learn%20the%20distribution%20properly%2C%20which%20is%0Anot%20possible%20in%20realistic%20scenarios.%20Instead%2C%20we%20propose%20a%20pipeline%20that%20can%0Aleverage%20correspondence%20distributions%20and%20shape%20information%20from%20the%20CAD%20model%2C%0Awhich%20are%20later%20used%20to%20learn%20pose%20distributions.%20Besides%2C%20having%20access%20to%0Apose%20distribution%20based%20on%20correspondences%20before%20learning%20pose%20distributions%0Aconditioned%20on%20images%2C%20can%20help%20formulate%20the%20loss%20between%20distributions.%20The%0Aprior%20knowledge%20of%20distribution%20also%20helps%20the%20network%20to%20focus%20on%20getting%0Asharper%20modes%20instead.%20With%20the%20CAD%20prior%2C%20our%20approach%20converges%20much%20faster%0Aand%20learns%20distribution%20better%20by%20focusing%20on%20learning%20sharper%20distribution%0Anear%20all%20the%20valid%20modes%2C%20unlike%20contrastive%20approaches%2C%20which%20focus%20on%20a%0Asingle%20mode%20at%20a%20time.%20We%20achieve%20benchmark%20results%20on%20SYMSOL-I%20and%20T-Less%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignist%253A%2520CAD-Informed%2520Orientation%2520Distribution%2520Estimation%2520by%2520Fusing%250A%2520%2520Shape%2520and%2520Correspondences%26entry.906535625%3DShishir%2520Reddy%2520Vutukur%2520and%2520Rasmus%2520Laurvig%2520Haugaard%2520and%2520Junwen%2520Huang%2520and%2520Benjamin%2520Busam%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Object%2520pose%2520distribution%2520estimation%2520is%2520crucial%2520in%2520robotics%2520for%2520better%2520path%250Aplanning%2520and%2520handling%2520of%2520symmetric%2520objects.%2520Recent%2520distribution%2520estimation%250Aapproaches%2520employ%2520contrastive%2520learning-based%2520approaches%2520by%2520maximizing%2520the%250Alikelihood%2520of%2520a%2520single%2520pose%2520estimate%2520in%2520the%2520absence%2520of%2520a%2520CAD%2520model.%2520We%2520propose%250Aa%2520pose%2520distribution%2520estimation%2520method%2520leveraging%2520symmetry%2520respecting%250Acorrespondence%2520distributions%2520and%2520shape%2520information%2520obtained%2520using%2520a%2520CAD%2520model.%250AContrastive%2520learning-based%2520approaches%2520require%2520an%2520exhaustive%2520amount%2520of%2520training%250Aimages%2520from%2520different%2520viewpoints%2520to%2520learn%2520the%2520distribution%2520properly%252C%2520which%2520is%250Anot%2520possible%2520in%2520realistic%2520scenarios.%2520Instead%252C%2520we%2520propose%2520a%2520pipeline%2520that%2520can%250Aleverage%2520correspondence%2520distributions%2520and%2520shape%2520information%2520from%2520the%2520CAD%2520model%252C%250Awhich%2520are%2520later%2520used%2520to%2520learn%2520pose%2520distributions.%2520Besides%252C%2520having%2520access%2520to%250Apose%2520distribution%2520based%2520on%2520correspondences%2520before%2520learning%2520pose%2520distributions%250Aconditioned%2520on%2520images%252C%2520can%2520help%2520formulate%2520the%2520loss%2520between%2520distributions.%2520The%250Aprior%2520knowledge%2520of%2520distribution%2520also%2520helps%2520the%2520network%2520to%2520focus%2520on%2520getting%250Asharper%2520modes%2520instead.%2520With%2520the%2520CAD%2520prior%252C%2520our%2520approach%2520converges%2520much%2520faster%250Aand%2520learns%2520distribution%2520better%2520by%2520focusing%2520on%2520learning%2520sharper%2520distribution%250Anear%2520all%2520the%2520valid%2520modes%252C%2520unlike%2520contrastive%2520approaches%252C%2520which%2520focus%2520on%2520a%250Asingle%2520mode%2520at%2520a%2520time.%2520We%2520achieve%2520benchmark%2520results%2520on%2520SYMSOL-I%2520and%2520T-Less%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignist%3A%20CAD-Informed%20Orientation%20Distribution%20Estimation%20by%20Fusing%0A%20%20Shape%20and%20Correspondences&entry.906535625=Shishir%20Reddy%20Vutukur%20and%20Rasmus%20Laurvig%20Haugaard%20and%20Junwen%20Huang%20and%20Benjamin%20Busam%20and%20Tolga%20Birdal&entry.1292438233=%20%20Object%20pose%20distribution%20estimation%20is%20crucial%20in%20robotics%20for%20better%20path%0Aplanning%20and%20handling%20of%20symmetric%20objects.%20Recent%20distribution%20estimation%0Aapproaches%20employ%20contrastive%20learning-based%20approaches%20by%20maximizing%20the%0Alikelihood%20of%20a%20single%20pose%20estimate%20in%20the%20absence%20of%20a%20CAD%20model.%20We%20propose%0Aa%20pose%20distribution%20estimation%20method%20leveraging%20symmetry%20respecting%0Acorrespondence%20distributions%20and%20shape%20information%20obtained%20using%20a%20CAD%20model.%0AContrastive%20learning-based%20approaches%20require%20an%20exhaustive%20amount%20of%20training%0Aimages%20from%20different%20viewpoints%20to%20learn%20the%20distribution%20properly%2C%20which%20is%0Anot%20possible%20in%20realistic%20scenarios.%20Instead%2C%20we%20propose%20a%20pipeline%20that%20can%0Aleverage%20correspondence%20distributions%20and%20shape%20information%20from%20the%20CAD%20model%2C%0Awhich%20are%20later%20used%20to%20learn%20pose%20distributions.%20Besides%2C%20having%20access%20to%0Apose%20distribution%20based%20on%20correspondences%20before%20learning%20pose%20distributions%0Aconditioned%20on%20images%2C%20can%20help%20formulate%20the%20loss%20between%20distributions.%20The%0Aprior%20knowledge%20of%20distribution%20also%20helps%20the%20network%20to%20focus%20on%20getting%0Asharper%20modes%20instead.%20With%20the%20CAD%20prior%2C%20our%20approach%20converges%20much%20faster%0Aand%20learns%20distribution%20better%20by%20focusing%20on%20learning%20sharper%20distribution%0Anear%20all%20the%20valid%20modes%2C%20unlike%20contrastive%20approaches%2C%20which%20focus%20on%20a%0Asingle%20mode%20at%20a%20time.%20We%20achieve%20benchmark%20results%20on%20SYMSOL-I%20and%20T-Less%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06683v1&entry.124074799=Read"},
{"title": "Image Similarity using An Ensemble of Context-Sensitive Models", "author": "Zukang Liao and Min Chen", "abstract": "  Image similarity has been extensively studied in computer vision. In recent\nyears, machine-learned models have shown their ability to encode more semantics\nthan traditional multivariate metrics. However, in labelling semantic\nsimilarity, assigning a numerical score to a pair of images is impractical,\nmaking the improvement and comparisons on the task difficult. In this work, we\npresent a more intuitive approach to build and compare image similarity models\nbased on labelled data in the form of A:R vs B:R, i.e., determining if an image\nA is closer to a reference image R than another image B. We address the\nchallenges of sparse sampling in the image space (R, A, B) and biases in the\nmodels trained with context-based data by using an ensemble model. Our testing\nresults show that the ensemble model constructed performs ~5% better than the\nbest individual context-sensitive models. They also performed better than the\nmodels that were directly fine-tuned using mixed imagery data as well as\nexisting deep embeddings, e.g., CLIP and DINO. This work demonstrates that\ncontext-based labelling and model training can be effective when an appropriate\nensemble approach is used to alleviate the limitation due to sparse sampling.\n", "link": "http://arxiv.org/abs/2401.07951v2", "date": "2024-09-10", "relevancy": 2.7534, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5641}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Similarity%20using%20An%20Ensemble%20of%20Context-Sensitive%20Models&body=Title%3A%20Image%20Similarity%20using%20An%20Ensemble%20of%20Context-Sensitive%20Models%0AAuthor%3A%20Zukang%20Liao%20and%20Min%20Chen%0AAbstract%3A%20%20%20Image%20similarity%20has%20been%20extensively%20studied%20in%20computer%20vision.%20In%20recent%0Ayears%2C%20machine-learned%20models%20have%20shown%20their%20ability%20to%20encode%20more%20semantics%0Athan%20traditional%20multivariate%20metrics.%20However%2C%20in%20labelling%20semantic%0Asimilarity%2C%20assigning%20a%20numerical%20score%20to%20a%20pair%20of%20images%20is%20impractical%2C%0Amaking%20the%20improvement%20and%20comparisons%20on%20the%20task%20difficult.%20In%20this%20work%2C%20we%0Apresent%20a%20more%20intuitive%20approach%20to%20build%20and%20compare%20image%20similarity%20models%0Abased%20on%20labelled%20data%20in%20the%20form%20of%20A%3AR%20vs%20B%3AR%2C%20i.e.%2C%20determining%20if%20an%20image%0AA%20is%20closer%20to%20a%20reference%20image%20R%20than%20another%20image%20B.%20We%20address%20the%0Achallenges%20of%20sparse%20sampling%20in%20the%20image%20space%20%28R%2C%20A%2C%20B%29%20and%20biases%20in%20the%0Amodels%20trained%20with%20context-based%20data%20by%20using%20an%20ensemble%20model.%20Our%20testing%0Aresults%20show%20that%20the%20ensemble%20model%20constructed%20performs%20~5%25%20better%20than%20the%0Abest%20individual%20context-sensitive%20models.%20They%20also%20performed%20better%20than%20the%0Amodels%20that%20were%20directly%20fine-tuned%20using%20mixed%20imagery%20data%20as%20well%20as%0Aexisting%20deep%20embeddings%2C%20e.g.%2C%20CLIP%20and%20DINO.%20This%20work%20demonstrates%20that%0Acontext-based%20labelling%20and%20model%20training%20can%20be%20effective%20when%20an%20appropriate%0Aensemble%20approach%20is%20used%20to%20alleviate%20the%20limitation%20due%20to%20sparse%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07951v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Similarity%2520using%2520An%2520Ensemble%2520of%2520Context-Sensitive%2520Models%26entry.906535625%3DZukang%2520Liao%2520and%2520Min%2520Chen%26entry.1292438233%3D%2520%2520Image%2520similarity%2520has%2520been%2520extensively%2520studied%2520in%2520computer%2520vision.%2520In%2520recent%250Ayears%252C%2520machine-learned%2520models%2520have%2520shown%2520their%2520ability%2520to%2520encode%2520more%2520semantics%250Athan%2520traditional%2520multivariate%2520metrics.%2520However%252C%2520in%2520labelling%2520semantic%250Asimilarity%252C%2520assigning%2520a%2520numerical%2520score%2520to%2520a%2520pair%2520of%2520images%2520is%2520impractical%252C%250Amaking%2520the%2520improvement%2520and%2520comparisons%2520on%2520the%2520task%2520difficult.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520more%2520intuitive%2520approach%2520to%2520build%2520and%2520compare%2520image%2520similarity%2520models%250Abased%2520on%2520labelled%2520data%2520in%2520the%2520form%2520of%2520A%253AR%2520vs%2520B%253AR%252C%2520i.e.%252C%2520determining%2520if%2520an%2520image%250AA%2520is%2520closer%2520to%2520a%2520reference%2520image%2520R%2520than%2520another%2520image%2520B.%2520We%2520address%2520the%250Achallenges%2520of%2520sparse%2520sampling%2520in%2520the%2520image%2520space%2520%2528R%252C%2520A%252C%2520B%2529%2520and%2520biases%2520in%2520the%250Amodels%2520trained%2520with%2520context-based%2520data%2520by%2520using%2520an%2520ensemble%2520model.%2520Our%2520testing%250Aresults%2520show%2520that%2520the%2520ensemble%2520model%2520constructed%2520performs%2520~5%2525%2520better%2520than%2520the%250Abest%2520individual%2520context-sensitive%2520models.%2520They%2520also%2520performed%2520better%2520than%2520the%250Amodels%2520that%2520were%2520directly%2520fine-tuned%2520using%2520mixed%2520imagery%2520data%2520as%2520well%2520as%250Aexisting%2520deep%2520embeddings%252C%2520e.g.%252C%2520CLIP%2520and%2520DINO.%2520This%2520work%2520demonstrates%2520that%250Acontext-based%2520labelling%2520and%2520model%2520training%2520can%2520be%2520effective%2520when%2520an%2520appropriate%250Aensemble%2520approach%2520is%2520used%2520to%2520alleviate%2520the%2520limitation%2520due%2520to%2520sparse%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07951v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Similarity%20using%20An%20Ensemble%20of%20Context-Sensitive%20Models&entry.906535625=Zukang%20Liao%20and%20Min%20Chen&entry.1292438233=%20%20Image%20similarity%20has%20been%20extensively%20studied%20in%20computer%20vision.%20In%20recent%0Ayears%2C%20machine-learned%20models%20have%20shown%20their%20ability%20to%20encode%20more%20semantics%0Athan%20traditional%20multivariate%20metrics.%20However%2C%20in%20labelling%20semantic%0Asimilarity%2C%20assigning%20a%20numerical%20score%20to%20a%20pair%20of%20images%20is%20impractical%2C%0Amaking%20the%20improvement%20and%20comparisons%20on%20the%20task%20difficult.%20In%20this%20work%2C%20we%0Apresent%20a%20more%20intuitive%20approach%20to%20build%20and%20compare%20image%20similarity%20models%0Abased%20on%20labelled%20data%20in%20the%20form%20of%20A%3AR%20vs%20B%3AR%2C%20i.e.%2C%20determining%20if%20an%20image%0AA%20is%20closer%20to%20a%20reference%20image%20R%20than%20another%20image%20B.%20We%20address%20the%0Achallenges%20of%20sparse%20sampling%20in%20the%20image%20space%20%28R%2C%20A%2C%20B%29%20and%20biases%20in%20the%0Amodels%20trained%20with%20context-based%20data%20by%20using%20an%20ensemble%20model.%20Our%20testing%0Aresults%20show%20that%20the%20ensemble%20model%20constructed%20performs%20~5%25%20better%20than%20the%0Abest%20individual%20context-sensitive%20models.%20They%20also%20performed%20better%20than%20the%0Amodels%20that%20were%20directly%20fine-tuned%20using%20mixed%20imagery%20data%20as%20well%20as%0Aexisting%20deep%20embeddings%2C%20e.g.%2C%20CLIP%20and%20DINO.%20This%20work%20demonstrates%20that%0Acontext-based%20labelling%20and%20model%20training%20can%20be%20effective%20when%20an%20appropriate%0Aensemble%20approach%20is%20used%20to%20alleviate%20the%20limitation%20due%20to%20sparse%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07951v2&entry.124074799=Read"},
{"title": "Learning Generative Interactive Environments By Trained Agent\n  Exploration", "author": "Naser Kazemi and Nedko Savov and Danda Paudel and Luc Van Gool", "abstract": "  World models are increasingly pivotal in interpreting and simulating the\nrules and actions of complex environments. Genie, a recent model, excels at\nlearning from visually diverse environments but relies on costly\nhuman-collected data. We observe that their alternative method of using random\nagents is too limited to explore the environment. We propose to improve the\nmodel by employing reinforcement learning based agents for data generation.\nThis approach produces diverse datasets that enhance the model's ability to\nadapt and perform well across various scenarios and realistic actions within\nthe environment. In this paper, we first release the model GenieRedux - an\nimplementation based on Genie. Additionally, we introduce GenieRedux-G, a\nvariant that uses the agent's readily available actions to factor out action\nprediction uncertainty during validation. Our evaluation, including a\nreplication of the Coinrun case study, shows that GenieRedux-G achieves\nsuperior visual fidelity and controllability using the trained agent\nexploration. The proposed approach is reproducable, scalable and adaptable to\nnew types of environments. Our codebase is available at\nhttps://github.com/insait-institute/GenieRedux .\n", "link": "http://arxiv.org/abs/2409.06445v1", "date": "2024-09-10", "relevancy": 2.7362, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.8151}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6089}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Generative%20Interactive%20Environments%20By%20Trained%20Agent%0A%20%20Exploration&body=Title%3A%20Learning%20Generative%20Interactive%20Environments%20By%20Trained%20Agent%0A%20%20Exploration%0AAuthor%3A%20Naser%20Kazemi%20and%20Nedko%20Savov%20and%20Danda%20Paudel%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20World%20models%20are%20increasingly%20pivotal%20in%20interpreting%20and%20simulating%20the%0Arules%20and%20actions%20of%20complex%20environments.%20Genie%2C%20a%20recent%20model%2C%20excels%20at%0Alearning%20from%20visually%20diverse%20environments%20but%20relies%20on%20costly%0Ahuman-collected%20data.%20We%20observe%20that%20their%20alternative%20method%20of%20using%20random%0Aagents%20is%20too%20limited%20to%20explore%20the%20environment.%20We%20propose%20to%20improve%20the%0Amodel%20by%20employing%20reinforcement%20learning%20based%20agents%20for%20data%20generation.%0AThis%20approach%20produces%20diverse%20datasets%20that%20enhance%20the%20model%27s%20ability%20to%0Aadapt%20and%20perform%20well%20across%20various%20scenarios%20and%20realistic%20actions%20within%0Athe%20environment.%20In%20this%20paper%2C%20we%20first%20release%20the%20model%20GenieRedux%20-%20an%0Aimplementation%20based%20on%20Genie.%20Additionally%2C%20we%20introduce%20GenieRedux-G%2C%20a%0Avariant%20that%20uses%20the%20agent%27s%20readily%20available%20actions%20to%20factor%20out%20action%0Aprediction%20uncertainty%20during%20validation.%20Our%20evaluation%2C%20including%20a%0Areplication%20of%20the%20Coinrun%20case%20study%2C%20shows%20that%20GenieRedux-G%20achieves%0Asuperior%20visual%20fidelity%20and%20controllability%20using%20the%20trained%20agent%0Aexploration.%20The%20proposed%20approach%20is%20reproducable%2C%20scalable%20and%20adaptable%20to%0Anew%20types%20of%20environments.%20Our%20codebase%20is%20available%20at%0Ahttps%3A//github.com/insait-institute/GenieRedux%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Generative%2520Interactive%2520Environments%2520By%2520Trained%2520Agent%250A%2520%2520Exploration%26entry.906535625%3DNaser%2520Kazemi%2520and%2520Nedko%2520Savov%2520and%2520Danda%2520Paudel%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520World%2520models%2520are%2520increasingly%2520pivotal%2520in%2520interpreting%2520and%2520simulating%2520the%250Arules%2520and%2520actions%2520of%2520complex%2520environments.%2520Genie%252C%2520a%2520recent%2520model%252C%2520excels%2520at%250Alearning%2520from%2520visually%2520diverse%2520environments%2520but%2520relies%2520on%2520costly%250Ahuman-collected%2520data.%2520We%2520observe%2520that%2520their%2520alternative%2520method%2520of%2520using%2520random%250Aagents%2520is%2520too%2520limited%2520to%2520explore%2520the%2520environment.%2520We%2520propose%2520to%2520improve%2520the%250Amodel%2520by%2520employing%2520reinforcement%2520learning%2520based%2520agents%2520for%2520data%2520generation.%250AThis%2520approach%2520produces%2520diverse%2520datasets%2520that%2520enhance%2520the%2520model%2527s%2520ability%2520to%250Aadapt%2520and%2520perform%2520well%2520across%2520various%2520scenarios%2520and%2520realistic%2520actions%2520within%250Athe%2520environment.%2520In%2520this%2520paper%252C%2520we%2520first%2520release%2520the%2520model%2520GenieRedux%2520-%2520an%250Aimplementation%2520based%2520on%2520Genie.%2520Additionally%252C%2520we%2520introduce%2520GenieRedux-G%252C%2520a%250Avariant%2520that%2520uses%2520the%2520agent%2527s%2520readily%2520available%2520actions%2520to%2520factor%2520out%2520action%250Aprediction%2520uncertainty%2520during%2520validation.%2520Our%2520evaluation%252C%2520including%2520a%250Areplication%2520of%2520the%2520Coinrun%2520case%2520study%252C%2520shows%2520that%2520GenieRedux-G%2520achieves%250Asuperior%2520visual%2520fidelity%2520and%2520controllability%2520using%2520the%2520trained%2520agent%250Aexploration.%2520The%2520proposed%2520approach%2520is%2520reproducable%252C%2520scalable%2520and%2520adaptable%2520to%250Anew%2520types%2520of%2520environments.%2520Our%2520codebase%2520is%2520available%2520at%250Ahttps%253A//github.com/insait-institute/GenieRedux%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Generative%20Interactive%20Environments%20By%20Trained%20Agent%0A%20%20Exploration&entry.906535625=Naser%20Kazemi%20and%20Nedko%20Savov%20and%20Danda%20Paudel%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20World%20models%20are%20increasingly%20pivotal%20in%20interpreting%20and%20simulating%20the%0Arules%20and%20actions%20of%20complex%20environments.%20Genie%2C%20a%20recent%20model%2C%20excels%20at%0Alearning%20from%20visually%20diverse%20environments%20but%20relies%20on%20costly%0Ahuman-collected%20data.%20We%20observe%20that%20their%20alternative%20method%20of%20using%20random%0Aagents%20is%20too%20limited%20to%20explore%20the%20environment.%20We%20propose%20to%20improve%20the%0Amodel%20by%20employing%20reinforcement%20learning%20based%20agents%20for%20data%20generation.%0AThis%20approach%20produces%20diverse%20datasets%20that%20enhance%20the%20model%27s%20ability%20to%0Aadapt%20and%20perform%20well%20across%20various%20scenarios%20and%20realistic%20actions%20within%0Athe%20environment.%20In%20this%20paper%2C%20we%20first%20release%20the%20model%20GenieRedux%20-%20an%0Aimplementation%20based%20on%20Genie.%20Additionally%2C%20we%20introduce%20GenieRedux-G%2C%20a%0Avariant%20that%20uses%20the%20agent%27s%20readily%20available%20actions%20to%20factor%20out%20action%0Aprediction%20uncertainty%20during%20validation.%20Our%20evaluation%2C%20including%20a%0Areplication%20of%20the%20Coinrun%20case%20study%2C%20shows%20that%20GenieRedux-G%20achieves%0Asuperior%20visual%20fidelity%20and%20controllability%20using%20the%20trained%20agent%0Aexploration.%20The%20proposed%20approach%20is%20reproducable%2C%20scalable%20and%20adaptable%20to%0Anew%20types%20of%20environments.%20Our%20codebase%20is%20available%20at%0Ahttps%3A//github.com/insait-institute/GenieRedux%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06445v1&entry.124074799=Read"},
{"title": "Neural Laplacian Operator for 3D Point Clouds", "author": "Bo Pang and Zhongtian Zheng and Yilong Li and Guoping Wang and Peng-Shuai Wang", "abstract": "  The discrete Laplacian operator holds a crucial role in 3D geometry\nprocessing, yet it is still challenging to define it on point clouds. Previous\nworks mainly focused on constructing a local triangulation around each point to\napproximate the underlying manifold for defining the Laplacian operator, which\nmay not be robust or accurate. In contrast, we simply use the K-nearest\nneighbors (KNN) graph constructed from the input point cloud and learn the\nLaplacian operator on the KNN graph with graph neural networks (GNNs). However,\nthe ground-truth Laplacian operator is defined on a manifold mesh with a\ndifferent connectivity from the KNN graph and thus cannot be directly used for\ntraining. To train the GNN, we propose a novel training scheme by imitating the\nbehavior of the ground-truth Laplacian operator on a set of probe functions so\nthat the learned Laplacian operator behaves similarly to the ground-truth\nLaplacian operator. We train our network on a subset of ShapeNet and evaluate\nit across a variety of point clouds. Compared with previous methods, our method\nreduces the error by an order of magnitude and excels in handling sparse point\nclouds with thin structures or sharp features. Our method also demonstrates a\nstrong generalization ability to unseen shapes. With our learned Laplacian\noperator, we further apply a series of Laplacian-based geometry processing\nalgorithms directly to point clouds and achieve accurate results, enabling many\nexciting possibilities for geometry processing on point clouds. The code and\ntrained models are available at https://github.com/IntelligentGeometry/NeLo.\n", "link": "http://arxiv.org/abs/2409.06506v1", "date": "2024-09-10", "relevancy": 2.7337, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.584}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5309}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Laplacian%20Operator%20for%203D%20Point%20Clouds&body=Title%3A%20Neural%20Laplacian%20Operator%20for%203D%20Point%20Clouds%0AAuthor%3A%20Bo%20Pang%20and%20Zhongtian%20Zheng%20and%20Yilong%20Li%20and%20Guoping%20Wang%20and%20Peng-Shuai%20Wang%0AAbstract%3A%20%20%20The%20discrete%20Laplacian%20operator%20holds%20a%20crucial%20role%20in%203D%20geometry%0Aprocessing%2C%20yet%20it%20is%20still%20challenging%20to%20define%20it%20on%20point%20clouds.%20Previous%0Aworks%20mainly%20focused%20on%20constructing%20a%20local%20triangulation%20around%20each%20point%20to%0Aapproximate%20the%20underlying%20manifold%20for%20defining%20the%20Laplacian%20operator%2C%20which%0Amay%20not%20be%20robust%20or%20accurate.%20In%20contrast%2C%20we%20simply%20use%20the%20K-nearest%0Aneighbors%20%28KNN%29%20graph%20constructed%20from%20the%20input%20point%20cloud%20and%20learn%20the%0ALaplacian%20operator%20on%20the%20KNN%20graph%20with%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%0Athe%20ground-truth%20Laplacian%20operator%20is%20defined%20on%20a%20manifold%20mesh%20with%20a%0Adifferent%20connectivity%20from%20the%20KNN%20graph%20and%20thus%20cannot%20be%20directly%20used%20for%0Atraining.%20To%20train%20the%20GNN%2C%20we%20propose%20a%20novel%20training%20scheme%20by%20imitating%20the%0Abehavior%20of%20the%20ground-truth%20Laplacian%20operator%20on%20a%20set%20of%20probe%20functions%20so%0Athat%20the%20learned%20Laplacian%20operator%20behaves%20similarly%20to%20the%20ground-truth%0ALaplacian%20operator.%20We%20train%20our%20network%20on%20a%20subset%20of%20ShapeNet%20and%20evaluate%0Ait%20across%20a%20variety%20of%20point%20clouds.%20Compared%20with%20previous%20methods%2C%20our%20method%0Areduces%20the%20error%20by%20an%20order%20of%20magnitude%20and%20excels%20in%20handling%20sparse%20point%0Aclouds%20with%20thin%20structures%20or%20sharp%20features.%20Our%20method%20also%20demonstrates%20a%0Astrong%20generalization%20ability%20to%20unseen%20shapes.%20With%20our%20learned%20Laplacian%0Aoperator%2C%20we%20further%20apply%20a%20series%20of%20Laplacian-based%20geometry%20processing%0Aalgorithms%20directly%20to%20point%20clouds%20and%20achieve%20accurate%20results%2C%20enabling%20many%0Aexciting%20possibilities%20for%20geometry%20processing%20on%20point%20clouds.%20The%20code%20and%0Atrained%20models%20are%20available%20at%20https%3A//github.com/IntelligentGeometry/NeLo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Laplacian%2520Operator%2520for%25203D%2520Point%2520Clouds%26entry.906535625%3DBo%2520Pang%2520and%2520Zhongtian%2520Zheng%2520and%2520Yilong%2520Li%2520and%2520Guoping%2520Wang%2520and%2520Peng-Shuai%2520Wang%26entry.1292438233%3D%2520%2520The%2520discrete%2520Laplacian%2520operator%2520holds%2520a%2520crucial%2520role%2520in%25203D%2520geometry%250Aprocessing%252C%2520yet%2520it%2520is%2520still%2520challenging%2520to%2520define%2520it%2520on%2520point%2520clouds.%2520Previous%250Aworks%2520mainly%2520focused%2520on%2520constructing%2520a%2520local%2520triangulation%2520around%2520each%2520point%2520to%250Aapproximate%2520the%2520underlying%2520manifold%2520for%2520defining%2520the%2520Laplacian%2520operator%252C%2520which%250Amay%2520not%2520be%2520robust%2520or%2520accurate.%2520In%2520contrast%252C%2520we%2520simply%2520use%2520the%2520K-nearest%250Aneighbors%2520%2528KNN%2529%2520graph%2520constructed%2520from%2520the%2520input%2520point%2520cloud%2520and%2520learn%2520the%250ALaplacian%2520operator%2520on%2520the%2520KNN%2520graph%2520with%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520However%252C%250Athe%2520ground-truth%2520Laplacian%2520operator%2520is%2520defined%2520on%2520a%2520manifold%2520mesh%2520with%2520a%250Adifferent%2520connectivity%2520from%2520the%2520KNN%2520graph%2520and%2520thus%2520cannot%2520be%2520directly%2520used%2520for%250Atraining.%2520To%2520train%2520the%2520GNN%252C%2520we%2520propose%2520a%2520novel%2520training%2520scheme%2520by%2520imitating%2520the%250Abehavior%2520of%2520the%2520ground-truth%2520Laplacian%2520operator%2520on%2520a%2520set%2520of%2520probe%2520functions%2520so%250Athat%2520the%2520learned%2520Laplacian%2520operator%2520behaves%2520similarly%2520to%2520the%2520ground-truth%250ALaplacian%2520operator.%2520We%2520train%2520our%2520network%2520on%2520a%2520subset%2520of%2520ShapeNet%2520and%2520evaluate%250Ait%2520across%2520a%2520variety%2520of%2520point%2520clouds.%2520Compared%2520with%2520previous%2520methods%252C%2520our%2520method%250Areduces%2520the%2520error%2520by%2520an%2520order%2520of%2520magnitude%2520and%2520excels%2520in%2520handling%2520sparse%2520point%250Aclouds%2520with%2520thin%2520structures%2520or%2520sharp%2520features.%2520Our%2520method%2520also%2520demonstrates%2520a%250Astrong%2520generalization%2520ability%2520to%2520unseen%2520shapes.%2520With%2520our%2520learned%2520Laplacian%250Aoperator%252C%2520we%2520further%2520apply%2520a%2520series%2520of%2520Laplacian-based%2520geometry%2520processing%250Aalgorithms%2520directly%2520to%2520point%2520clouds%2520and%2520achieve%2520accurate%2520results%252C%2520enabling%2520many%250Aexciting%2520possibilities%2520for%2520geometry%2520processing%2520on%2520point%2520clouds.%2520The%2520code%2520and%250Atrained%2520models%2520are%2520available%2520at%2520https%253A//github.com/IntelligentGeometry/NeLo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Laplacian%20Operator%20for%203D%20Point%20Clouds&entry.906535625=Bo%20Pang%20and%20Zhongtian%20Zheng%20and%20Yilong%20Li%20and%20Guoping%20Wang%20and%20Peng-Shuai%20Wang&entry.1292438233=%20%20The%20discrete%20Laplacian%20operator%20holds%20a%20crucial%20role%20in%203D%20geometry%0Aprocessing%2C%20yet%20it%20is%20still%20challenging%20to%20define%20it%20on%20point%20clouds.%20Previous%0Aworks%20mainly%20focused%20on%20constructing%20a%20local%20triangulation%20around%20each%20point%20to%0Aapproximate%20the%20underlying%20manifold%20for%20defining%20the%20Laplacian%20operator%2C%20which%0Amay%20not%20be%20robust%20or%20accurate.%20In%20contrast%2C%20we%20simply%20use%20the%20K-nearest%0Aneighbors%20%28KNN%29%20graph%20constructed%20from%20the%20input%20point%20cloud%20and%20learn%20the%0ALaplacian%20operator%20on%20the%20KNN%20graph%20with%20graph%20neural%20networks%20%28GNNs%29.%20However%2C%0Athe%20ground-truth%20Laplacian%20operator%20is%20defined%20on%20a%20manifold%20mesh%20with%20a%0Adifferent%20connectivity%20from%20the%20KNN%20graph%20and%20thus%20cannot%20be%20directly%20used%20for%0Atraining.%20To%20train%20the%20GNN%2C%20we%20propose%20a%20novel%20training%20scheme%20by%20imitating%20the%0Abehavior%20of%20the%20ground-truth%20Laplacian%20operator%20on%20a%20set%20of%20probe%20functions%20so%0Athat%20the%20learned%20Laplacian%20operator%20behaves%20similarly%20to%20the%20ground-truth%0ALaplacian%20operator.%20We%20train%20our%20network%20on%20a%20subset%20of%20ShapeNet%20and%20evaluate%0Ait%20across%20a%20variety%20of%20point%20clouds.%20Compared%20with%20previous%20methods%2C%20our%20method%0Areduces%20the%20error%20by%20an%20order%20of%20magnitude%20and%20excels%20in%20handling%20sparse%20point%0Aclouds%20with%20thin%20structures%20or%20sharp%20features.%20Our%20method%20also%20demonstrates%20a%0Astrong%20generalization%20ability%20to%20unseen%20shapes.%20With%20our%20learned%20Laplacian%0Aoperator%2C%20we%20further%20apply%20a%20series%20of%20Laplacian-based%20geometry%20processing%0Aalgorithms%20directly%20to%20point%20clouds%20and%20achieve%20accurate%20results%2C%20enabling%20many%0Aexciting%20possibilities%20for%20geometry%20processing%20on%20point%20clouds.%20The%20code%20and%0Atrained%20models%20are%20available%20at%20https%3A//github.com/IntelligentGeometry/NeLo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06506v1&entry.124074799=Read"},
{"title": "FC-Planner: A Skeleton-guided Planning Framework for Fast Aerial\n  Coverage of Complex 3D Scenes", "author": "Chen Feng and Haojia Li and Mingjie Zhang and Xinyi Chen and Boyu Zhou and Shaojie Shen", "abstract": "  3D coverage path planning for UAVs is a crucial problem in diverse practical\napplications. However, existing methods have shown unsatisfactory system\nsimplicity, computation efficiency, and path quality in large and complex\nscenes. To address these challenges, we propose FC-Planner, a skeleton-guided\nplanning framework that can achieve fast aerial coverage of complex 3D scenes\nwithout pre-processing. We decompose the scene into several simple subspaces by\na skeleton-based space decomposition (SSD). Additionally, the skeleton guides\nus to effortlessly determine free space. We utilize the skeleton to efficiently\ngenerate a minimal set of specialized and informative viewpoints for complete\ncoverage. Based on SSD, a hierarchical planner effectively divides the large\nplanning problem into independent sub-problems, enabling parallel planning for\neach subspace. The carefully designed global and local planning strategies are\nthen incorporated to guarantee both high quality and efficiency in path\ngeneration. We conduct extensive benchmark and real-world tests, where\nFC-Planner computes over 10 times faster compared to state-of-the-art methods\nwith shorter path and more complete coverage. The source code will be made\npublicly available to benefit the community. Project page:\nhttps://hkust-aerial-robotics.github.io/FC-Planner.\n", "link": "http://arxiv.org/abs/2309.13882v3", "date": "2024-09-10", "relevancy": 2.7266, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5512}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5512}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FC-Planner%3A%20A%20Skeleton-guided%20Planning%20Framework%20for%20Fast%20Aerial%0A%20%20Coverage%20of%20Complex%203D%20Scenes&body=Title%3A%20FC-Planner%3A%20A%20Skeleton-guided%20Planning%20Framework%20for%20Fast%20Aerial%0A%20%20Coverage%20of%20Complex%203D%20Scenes%0AAuthor%3A%20Chen%20Feng%20and%20Haojia%20Li%20and%20Mingjie%20Zhang%20and%20Xinyi%20Chen%20and%20Boyu%20Zhou%20and%20Shaojie%20Shen%0AAbstract%3A%20%20%203D%20coverage%20path%20planning%20for%20UAVs%20is%20a%20crucial%20problem%20in%20diverse%20practical%0Aapplications.%20However%2C%20existing%20methods%20have%20shown%20unsatisfactory%20system%0Asimplicity%2C%20computation%20efficiency%2C%20and%20path%20quality%20in%20large%20and%20complex%0Ascenes.%20To%20address%20these%20challenges%2C%20we%20propose%20FC-Planner%2C%20a%20skeleton-guided%0Aplanning%20framework%20that%20can%20achieve%20fast%20aerial%20coverage%20of%20complex%203D%20scenes%0Awithout%20pre-processing.%20We%20decompose%20the%20scene%20into%20several%20simple%20subspaces%20by%0Aa%20skeleton-based%20space%20decomposition%20%28SSD%29.%20Additionally%2C%20the%20skeleton%20guides%0Aus%20to%20effortlessly%20determine%20free%20space.%20We%20utilize%20the%20skeleton%20to%20efficiently%0Agenerate%20a%20minimal%20set%20of%20specialized%20and%20informative%20viewpoints%20for%20complete%0Acoverage.%20Based%20on%20SSD%2C%20a%20hierarchical%20planner%20effectively%20divides%20the%20large%0Aplanning%20problem%20into%20independent%20sub-problems%2C%20enabling%20parallel%20planning%20for%0Aeach%20subspace.%20The%20carefully%20designed%20global%20and%20local%20planning%20strategies%20are%0Athen%20incorporated%20to%20guarantee%20both%20high%20quality%20and%20efficiency%20in%20path%0Ageneration.%20We%20conduct%20extensive%20benchmark%20and%20real-world%20tests%2C%20where%0AFC-Planner%20computes%20over%2010%20times%20faster%20compared%20to%20state-of-the-art%20methods%0Awith%20shorter%20path%20and%20more%20complete%20coverage.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20to%20benefit%20the%20community.%20Project%20page%3A%0Ahttps%3A//hkust-aerial-robotics.github.io/FC-Planner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.13882v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFC-Planner%253A%2520A%2520Skeleton-guided%2520Planning%2520Framework%2520for%2520Fast%2520Aerial%250A%2520%2520Coverage%2520of%2520Complex%25203D%2520Scenes%26entry.906535625%3DChen%2520Feng%2520and%2520Haojia%2520Li%2520and%2520Mingjie%2520Zhang%2520and%2520Xinyi%2520Chen%2520and%2520Boyu%2520Zhou%2520and%2520Shaojie%2520Shen%26entry.1292438233%3D%2520%25203D%2520coverage%2520path%2520planning%2520for%2520UAVs%2520is%2520a%2520crucial%2520problem%2520in%2520diverse%2520practical%250Aapplications.%2520However%252C%2520existing%2520methods%2520have%2520shown%2520unsatisfactory%2520system%250Asimplicity%252C%2520computation%2520efficiency%252C%2520and%2520path%2520quality%2520in%2520large%2520and%2520complex%250Ascenes.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FC-Planner%252C%2520a%2520skeleton-guided%250Aplanning%2520framework%2520that%2520can%2520achieve%2520fast%2520aerial%2520coverage%2520of%2520complex%25203D%2520scenes%250Awithout%2520pre-processing.%2520We%2520decompose%2520the%2520scene%2520into%2520several%2520simple%2520subspaces%2520by%250Aa%2520skeleton-based%2520space%2520decomposition%2520%2528SSD%2529.%2520Additionally%252C%2520the%2520skeleton%2520guides%250Aus%2520to%2520effortlessly%2520determine%2520free%2520space.%2520We%2520utilize%2520the%2520skeleton%2520to%2520efficiently%250Agenerate%2520a%2520minimal%2520set%2520of%2520specialized%2520and%2520informative%2520viewpoints%2520for%2520complete%250Acoverage.%2520Based%2520on%2520SSD%252C%2520a%2520hierarchical%2520planner%2520effectively%2520divides%2520the%2520large%250Aplanning%2520problem%2520into%2520independent%2520sub-problems%252C%2520enabling%2520parallel%2520planning%2520for%250Aeach%2520subspace.%2520The%2520carefully%2520designed%2520global%2520and%2520local%2520planning%2520strategies%2520are%250Athen%2520incorporated%2520to%2520guarantee%2520both%2520high%2520quality%2520and%2520efficiency%2520in%2520path%250Ageneration.%2520We%2520conduct%2520extensive%2520benchmark%2520and%2520real-world%2520tests%252C%2520where%250AFC-Planner%2520computes%2520over%252010%2520times%2520faster%2520compared%2520to%2520state-of-the-art%2520methods%250Awith%2520shorter%2520path%2520and%2520more%2520complete%2520coverage.%2520The%2520source%2520code%2520will%2520be%2520made%250Apublicly%2520available%2520to%2520benefit%2520the%2520community.%2520Project%2520page%253A%250Ahttps%253A//hkust-aerial-robotics.github.io/FC-Planner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.13882v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FC-Planner%3A%20A%20Skeleton-guided%20Planning%20Framework%20for%20Fast%20Aerial%0A%20%20Coverage%20of%20Complex%203D%20Scenes&entry.906535625=Chen%20Feng%20and%20Haojia%20Li%20and%20Mingjie%20Zhang%20and%20Xinyi%20Chen%20and%20Boyu%20Zhou%20and%20Shaojie%20Shen&entry.1292438233=%20%203D%20coverage%20path%20planning%20for%20UAVs%20is%20a%20crucial%20problem%20in%20diverse%20practical%0Aapplications.%20However%2C%20existing%20methods%20have%20shown%20unsatisfactory%20system%0Asimplicity%2C%20computation%20efficiency%2C%20and%20path%20quality%20in%20large%20and%20complex%0Ascenes.%20To%20address%20these%20challenges%2C%20we%20propose%20FC-Planner%2C%20a%20skeleton-guided%0Aplanning%20framework%20that%20can%20achieve%20fast%20aerial%20coverage%20of%20complex%203D%20scenes%0Awithout%20pre-processing.%20We%20decompose%20the%20scene%20into%20several%20simple%20subspaces%20by%0Aa%20skeleton-based%20space%20decomposition%20%28SSD%29.%20Additionally%2C%20the%20skeleton%20guides%0Aus%20to%20effortlessly%20determine%20free%20space.%20We%20utilize%20the%20skeleton%20to%20efficiently%0Agenerate%20a%20minimal%20set%20of%20specialized%20and%20informative%20viewpoints%20for%20complete%0Acoverage.%20Based%20on%20SSD%2C%20a%20hierarchical%20planner%20effectively%20divides%20the%20large%0Aplanning%20problem%20into%20independent%20sub-problems%2C%20enabling%20parallel%20planning%20for%0Aeach%20subspace.%20The%20carefully%20designed%20global%20and%20local%20planning%20strategies%20are%0Athen%20incorporated%20to%20guarantee%20both%20high%20quality%20and%20efficiency%20in%20path%0Ageneration.%20We%20conduct%20extensive%20benchmark%20and%20real-world%20tests%2C%20where%0AFC-Planner%20computes%20over%2010%20times%20faster%20compared%20to%20state-of-the-art%20methods%0Awith%20shorter%20path%20and%20more%20complete%20coverage.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20to%20benefit%20the%20community.%20Project%20page%3A%0Ahttps%3A//hkust-aerial-robotics.github.io/FC-Planner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.13882v3&entry.124074799=Read"},
{"title": "Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous\n  Driving", "author": "Kairui Ding and Boyuan Chen and Yuchen Su and Huan-ang Gao and Bu Jin and Chonghao Sima and Wuqiang Zhang and Xiaohui Li and Paul Barsch and Hongyang Li and Hao Zhao", "abstract": "  End-to-end architectures in autonomous driving (AD) face a significant\nchallenge in interpretability, impeding human-AI trust. Human-friendly natural\nlanguage has been explored for tasks such as driving explanation and 3D\ncaptioning. However, previous works primarily focused on the paradigm of\ndeclarative interpretability, where the natural language interpretations are\nnot grounded in the intermediate outputs of AD systems, making the\ninterpretations only declarative. In contrast, aligned interpretability\nestablishes a connection between language and the intermediate outputs of AD\nsystems. Here we introduce Hint-AD, an integrated AD-language system that\ngenerates language aligned with the holistic perception-prediction-planning\noutputs of the AD model. By incorporating the intermediate outputs and a\nholistic token mixer sub-network for effective feature adaptation, Hint-AD\nachieves desirable accuracy, achieving state-of-the-art results in driving\nlanguage tasks including driving explanation, 3D dense captioning, and command\nprediction. To facilitate further study on driving explanation task on\nnuScenes, we also introduce a human-labeled dataset, Nu-X. Codes, dataset, and\nmodels will be publicly available.\n", "link": "http://arxiv.org/abs/2409.06702v1", "date": "2024-09-10", "relevancy": 2.7137, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hint-AD%3A%20Holistically%20Aligned%20Interpretability%20in%20End-to-End%20Autonomous%0A%20%20Driving&body=Title%3A%20Hint-AD%3A%20Holistically%20Aligned%20Interpretability%20in%20End-to-End%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Kairui%20Ding%20and%20Boyuan%20Chen%20and%20Yuchen%20Su%20and%20Huan-ang%20Gao%20and%20Bu%20Jin%20and%20Chonghao%20Sima%20and%20Wuqiang%20Zhang%20and%20Xiaohui%20Li%20and%20Paul%20Barsch%20and%20Hongyang%20Li%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20End-to-end%20architectures%20in%20autonomous%20driving%20%28AD%29%20face%20a%20significant%0Achallenge%20in%20interpretability%2C%20impeding%20human-AI%20trust.%20Human-friendly%20natural%0Alanguage%20has%20been%20explored%20for%20tasks%20such%20as%20driving%20explanation%20and%203D%0Acaptioning.%20However%2C%20previous%20works%20primarily%20focused%20on%20the%20paradigm%20of%0Adeclarative%20interpretability%2C%20where%20the%20natural%20language%20interpretations%20are%0Anot%20grounded%20in%20the%20intermediate%20outputs%20of%20AD%20systems%2C%20making%20the%0Ainterpretations%20only%20declarative.%20In%20contrast%2C%20aligned%20interpretability%0Aestablishes%20a%20connection%20between%20language%20and%20the%20intermediate%20outputs%20of%20AD%0Asystems.%20Here%20we%20introduce%20Hint-AD%2C%20an%20integrated%20AD-language%20system%20that%0Agenerates%20language%20aligned%20with%20the%20holistic%20perception-prediction-planning%0Aoutputs%20of%20the%20AD%20model.%20By%20incorporating%20the%20intermediate%20outputs%20and%20a%0Aholistic%20token%20mixer%20sub-network%20for%20effective%20feature%20adaptation%2C%20Hint-AD%0Aachieves%20desirable%20accuracy%2C%20achieving%20state-of-the-art%20results%20in%20driving%0Alanguage%20tasks%20including%20driving%20explanation%2C%203D%20dense%20captioning%2C%20and%20command%0Aprediction.%20To%20facilitate%20further%20study%20on%20driving%20explanation%20task%20on%0AnuScenes%2C%20we%20also%20introduce%20a%20human-labeled%20dataset%2C%20Nu-X.%20Codes%2C%20dataset%2C%20and%0Amodels%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHint-AD%253A%2520Holistically%2520Aligned%2520Interpretability%2520in%2520End-to-End%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DKairui%2520Ding%2520and%2520Boyuan%2520Chen%2520and%2520Yuchen%2520Su%2520and%2520Huan-ang%2520Gao%2520and%2520Bu%2520Jin%2520and%2520Chonghao%2520Sima%2520and%2520Wuqiang%2520Zhang%2520and%2520Xiaohui%2520Li%2520and%2520Paul%2520Barsch%2520and%2520Hongyang%2520Li%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520End-to-end%2520architectures%2520in%2520autonomous%2520driving%2520%2528AD%2529%2520face%2520a%2520significant%250Achallenge%2520in%2520interpretability%252C%2520impeding%2520human-AI%2520trust.%2520Human-friendly%2520natural%250Alanguage%2520has%2520been%2520explored%2520for%2520tasks%2520such%2520as%2520driving%2520explanation%2520and%25203D%250Acaptioning.%2520However%252C%2520previous%2520works%2520primarily%2520focused%2520on%2520the%2520paradigm%2520of%250Adeclarative%2520interpretability%252C%2520where%2520the%2520natural%2520language%2520interpretations%2520are%250Anot%2520grounded%2520in%2520the%2520intermediate%2520outputs%2520of%2520AD%2520systems%252C%2520making%2520the%250Ainterpretations%2520only%2520declarative.%2520In%2520contrast%252C%2520aligned%2520interpretability%250Aestablishes%2520a%2520connection%2520between%2520language%2520and%2520the%2520intermediate%2520outputs%2520of%2520AD%250Asystems.%2520Here%2520we%2520introduce%2520Hint-AD%252C%2520an%2520integrated%2520AD-language%2520system%2520that%250Agenerates%2520language%2520aligned%2520with%2520the%2520holistic%2520perception-prediction-planning%250Aoutputs%2520of%2520the%2520AD%2520model.%2520By%2520incorporating%2520the%2520intermediate%2520outputs%2520and%2520a%250Aholistic%2520token%2520mixer%2520sub-network%2520for%2520effective%2520feature%2520adaptation%252C%2520Hint-AD%250Aachieves%2520desirable%2520accuracy%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520driving%250Alanguage%2520tasks%2520including%2520driving%2520explanation%252C%25203D%2520dense%2520captioning%252C%2520and%2520command%250Aprediction.%2520To%2520facilitate%2520further%2520study%2520on%2520driving%2520explanation%2520task%2520on%250AnuScenes%252C%2520we%2520also%2520introduce%2520a%2520human-labeled%2520dataset%252C%2520Nu-X.%2520Codes%252C%2520dataset%252C%2520and%250Amodels%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hint-AD%3A%20Holistically%20Aligned%20Interpretability%20in%20End-to-End%20Autonomous%0A%20%20Driving&entry.906535625=Kairui%20Ding%20and%20Boyuan%20Chen%20and%20Yuchen%20Su%20and%20Huan-ang%20Gao%20and%20Bu%20Jin%20and%20Chonghao%20Sima%20and%20Wuqiang%20Zhang%20and%20Xiaohui%20Li%20and%20Paul%20Barsch%20and%20Hongyang%20Li%20and%20Hao%20Zhao&entry.1292438233=%20%20End-to-end%20architectures%20in%20autonomous%20driving%20%28AD%29%20face%20a%20significant%0Achallenge%20in%20interpretability%2C%20impeding%20human-AI%20trust.%20Human-friendly%20natural%0Alanguage%20has%20been%20explored%20for%20tasks%20such%20as%20driving%20explanation%20and%203D%0Acaptioning.%20However%2C%20previous%20works%20primarily%20focused%20on%20the%20paradigm%20of%0Adeclarative%20interpretability%2C%20where%20the%20natural%20language%20interpretations%20are%0Anot%20grounded%20in%20the%20intermediate%20outputs%20of%20AD%20systems%2C%20making%20the%0Ainterpretations%20only%20declarative.%20In%20contrast%2C%20aligned%20interpretability%0Aestablishes%20a%20connection%20between%20language%20and%20the%20intermediate%20outputs%20of%20AD%0Asystems.%20Here%20we%20introduce%20Hint-AD%2C%20an%20integrated%20AD-language%20system%20that%0Agenerates%20language%20aligned%20with%20the%20holistic%20perception-prediction-planning%0Aoutputs%20of%20the%20AD%20model.%20By%20incorporating%20the%20intermediate%20outputs%20and%20a%0Aholistic%20token%20mixer%20sub-network%20for%20effective%20feature%20adaptation%2C%20Hint-AD%0Aachieves%20desirable%20accuracy%2C%20achieving%20state-of-the-art%20results%20in%20driving%0Alanguage%20tasks%20including%20driving%20explanation%2C%203D%20dense%20captioning%2C%20and%20command%0Aprediction.%20To%20facilitate%20further%20study%20on%20driving%20explanation%20task%20on%0AnuScenes%2C%20we%20also%20introduce%20a%20human-labeled%20dataset%2C%20Nu-X.%20Codes%2C%20dataset%2C%20and%0Amodels%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06702v1&entry.124074799=Read"},
{"title": "Constructing an Interpretable Deep Denoiser by Unrolling Graph Laplacian\n  Regularizer", "author": "Seyed Alireza Hosseini and Tam Thuc Do and Gene Cheung and Yuichi Tanaka", "abstract": "  An image denoiser can be used for a wide range of restoration problems via\nthe Plug-and-Play (PnP) architecture. In this paper, we propose a general\nframework to build an interpretable graph-based deep denoiser (GDD) by\nunrolling a solution to a maximum a posteriori (MAP) problem equipped with a\ngraph Laplacian regularizer (GLR) as signal prior. Leveraging a recent theorem\nshowing that any (pseudo-)linear denoiser $\\boldsymbol \\Psi$, under mild\nconditions, can be mapped to a solution of a MAP denoising problem regularized\nusing GLR, we first initialize a graph Laplacian matrix $\\mathbf L$ via\ntruncated Taylor Series Expansion (TSE) of $\\boldsymbol \\Psi^{-1}$. Then, we\ncompute the MAP linear system solution by unrolling iterations of the conjugate\ngradient (CG) algorithm into a sequence of neural layers as a feed-forward\nnetwork -- one that is amenable to parameter tuning. The resulting GDD network\nis \"graph-interpretable\", low in parameter count, and easy to initialize thanks\nto $\\mathbf L$ derived from a known well-performing denoiser $\\boldsymbol\n\\Psi$. Experimental results show that GDD achieves competitive image denoising\nperformance compared to competitors, but employing far fewer parameters, and is\nmore robust to covariate shift.\n", "link": "http://arxiv.org/abs/2409.06676v1", "date": "2024-09-10", "relevancy": 2.7032, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5461}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.538}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constructing%20an%20Interpretable%20Deep%20Denoiser%20by%20Unrolling%20Graph%20Laplacian%0A%20%20Regularizer&body=Title%3A%20Constructing%20an%20Interpretable%20Deep%20Denoiser%20by%20Unrolling%20Graph%20Laplacian%0A%20%20Regularizer%0AAuthor%3A%20Seyed%20Alireza%20Hosseini%20and%20Tam%20Thuc%20Do%20and%20Gene%20Cheung%20and%20Yuichi%20Tanaka%0AAbstract%3A%20%20%20An%20image%20denoiser%20can%20be%20used%20for%20a%20wide%20range%20of%20restoration%20problems%20via%0Athe%20Plug-and-Play%20%28PnP%29%20architecture.%20In%20this%20paper%2C%20we%20propose%20a%20general%0Aframework%20to%20build%20an%20interpretable%20graph-based%20deep%20denoiser%20%28GDD%29%20by%0Aunrolling%20a%20solution%20to%20a%20maximum%20a%20posteriori%20%28MAP%29%20problem%20equipped%20with%20a%0Agraph%20Laplacian%20regularizer%20%28GLR%29%20as%20signal%20prior.%20Leveraging%20a%20recent%20theorem%0Ashowing%20that%20any%20%28pseudo-%29linear%20denoiser%20%24%5Cboldsymbol%20%5CPsi%24%2C%20under%20mild%0Aconditions%2C%20can%20be%20mapped%20to%20a%20solution%20of%20a%20MAP%20denoising%20problem%20regularized%0Ausing%20GLR%2C%20we%20first%20initialize%20a%20graph%20Laplacian%20matrix%20%24%5Cmathbf%20L%24%20via%0Atruncated%20Taylor%20Series%20Expansion%20%28TSE%29%20of%20%24%5Cboldsymbol%20%5CPsi%5E%7B-1%7D%24.%20Then%2C%20we%0Acompute%20the%20MAP%20linear%20system%20solution%20by%20unrolling%20iterations%20of%20the%20conjugate%0Agradient%20%28CG%29%20algorithm%20into%20a%20sequence%20of%20neural%20layers%20as%20a%20feed-forward%0Anetwork%20--%20one%20that%20is%20amenable%20to%20parameter%20tuning.%20The%20resulting%20GDD%20network%0Ais%20%22graph-interpretable%22%2C%20low%20in%20parameter%20count%2C%20and%20easy%20to%20initialize%20thanks%0Ato%20%24%5Cmathbf%20L%24%20derived%20from%20a%20known%20well-performing%20denoiser%20%24%5Cboldsymbol%0A%5CPsi%24.%20Experimental%20results%20show%20that%20GDD%20achieves%20competitive%20image%20denoising%0Aperformance%20compared%20to%20competitors%2C%20but%20employing%20far%20fewer%20parameters%2C%20and%20is%0Amore%20robust%20to%20covariate%20shift.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstructing%2520an%2520Interpretable%2520Deep%2520Denoiser%2520by%2520Unrolling%2520Graph%2520Laplacian%250A%2520%2520Regularizer%26entry.906535625%3DSeyed%2520Alireza%2520Hosseini%2520and%2520Tam%2520Thuc%2520Do%2520and%2520Gene%2520Cheung%2520and%2520Yuichi%2520Tanaka%26entry.1292438233%3D%2520%2520An%2520image%2520denoiser%2520can%2520be%2520used%2520for%2520a%2520wide%2520range%2520of%2520restoration%2520problems%2520via%250Athe%2520Plug-and-Play%2520%2528PnP%2529%2520architecture.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520general%250Aframework%2520to%2520build%2520an%2520interpretable%2520graph-based%2520deep%2520denoiser%2520%2528GDD%2529%2520by%250Aunrolling%2520a%2520solution%2520to%2520a%2520maximum%2520a%2520posteriori%2520%2528MAP%2529%2520problem%2520equipped%2520with%2520a%250Agraph%2520Laplacian%2520regularizer%2520%2528GLR%2529%2520as%2520signal%2520prior.%2520Leveraging%2520a%2520recent%2520theorem%250Ashowing%2520that%2520any%2520%2528pseudo-%2529linear%2520denoiser%2520%2524%255Cboldsymbol%2520%255CPsi%2524%252C%2520under%2520mild%250Aconditions%252C%2520can%2520be%2520mapped%2520to%2520a%2520solution%2520of%2520a%2520MAP%2520denoising%2520problem%2520regularized%250Ausing%2520GLR%252C%2520we%2520first%2520initialize%2520a%2520graph%2520Laplacian%2520matrix%2520%2524%255Cmathbf%2520L%2524%2520via%250Atruncated%2520Taylor%2520Series%2520Expansion%2520%2528TSE%2529%2520of%2520%2524%255Cboldsymbol%2520%255CPsi%255E%257B-1%257D%2524.%2520Then%252C%2520we%250Acompute%2520the%2520MAP%2520linear%2520system%2520solution%2520by%2520unrolling%2520iterations%2520of%2520the%2520conjugate%250Agradient%2520%2528CG%2529%2520algorithm%2520into%2520a%2520sequence%2520of%2520neural%2520layers%2520as%2520a%2520feed-forward%250Anetwork%2520--%2520one%2520that%2520is%2520amenable%2520to%2520parameter%2520tuning.%2520The%2520resulting%2520GDD%2520network%250Ais%2520%2522graph-interpretable%2522%252C%2520low%2520in%2520parameter%2520count%252C%2520and%2520easy%2520to%2520initialize%2520thanks%250Ato%2520%2524%255Cmathbf%2520L%2524%2520derived%2520from%2520a%2520known%2520well-performing%2520denoiser%2520%2524%255Cboldsymbol%250A%255CPsi%2524.%2520Experimental%2520results%2520show%2520that%2520GDD%2520achieves%2520competitive%2520image%2520denoising%250Aperformance%2520compared%2520to%2520competitors%252C%2520but%2520employing%2520far%2520fewer%2520parameters%252C%2520and%2520is%250Amore%2520robust%2520to%2520covariate%2520shift.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constructing%20an%20Interpretable%20Deep%20Denoiser%20by%20Unrolling%20Graph%20Laplacian%0A%20%20Regularizer&entry.906535625=Seyed%20Alireza%20Hosseini%20and%20Tam%20Thuc%20Do%20and%20Gene%20Cheung%20and%20Yuichi%20Tanaka&entry.1292438233=%20%20An%20image%20denoiser%20can%20be%20used%20for%20a%20wide%20range%20of%20restoration%20problems%20via%0Athe%20Plug-and-Play%20%28PnP%29%20architecture.%20In%20this%20paper%2C%20we%20propose%20a%20general%0Aframework%20to%20build%20an%20interpretable%20graph-based%20deep%20denoiser%20%28GDD%29%20by%0Aunrolling%20a%20solution%20to%20a%20maximum%20a%20posteriori%20%28MAP%29%20problem%20equipped%20with%20a%0Agraph%20Laplacian%20regularizer%20%28GLR%29%20as%20signal%20prior.%20Leveraging%20a%20recent%20theorem%0Ashowing%20that%20any%20%28pseudo-%29linear%20denoiser%20%24%5Cboldsymbol%20%5CPsi%24%2C%20under%20mild%0Aconditions%2C%20can%20be%20mapped%20to%20a%20solution%20of%20a%20MAP%20denoising%20problem%20regularized%0Ausing%20GLR%2C%20we%20first%20initialize%20a%20graph%20Laplacian%20matrix%20%24%5Cmathbf%20L%24%20via%0Atruncated%20Taylor%20Series%20Expansion%20%28TSE%29%20of%20%24%5Cboldsymbol%20%5CPsi%5E%7B-1%7D%24.%20Then%2C%20we%0Acompute%20the%20MAP%20linear%20system%20solution%20by%20unrolling%20iterations%20of%20the%20conjugate%0Agradient%20%28CG%29%20algorithm%20into%20a%20sequence%20of%20neural%20layers%20as%20a%20feed-forward%0Anetwork%20--%20one%20that%20is%20amenable%20to%20parameter%20tuning.%20The%20resulting%20GDD%20network%0Ais%20%22graph-interpretable%22%2C%20low%20in%20parameter%20count%2C%20and%20easy%20to%20initialize%20thanks%0Ato%20%24%5Cmathbf%20L%24%20derived%20from%20a%20known%20well-performing%20denoiser%20%24%5Cboldsymbol%0A%5CPsi%24.%20Experimental%20results%20show%20that%20GDD%20achieves%20competitive%20image%20denoising%0Aperformance%20compared%20to%20competitors%2C%20but%20employing%20far%20fewer%20parameters%2C%20and%20is%0Amore%20robust%20to%20covariate%20shift.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06676v1&entry.124074799=Read"},
{"title": "Seg-HGNN: Unsupervised and Light-Weight Image Segmentation with\n  Hyperbolic Graph Neural Networks", "author": "Debjyoti Mondal and Rahul Mishra and Chandan Pandey", "abstract": "  Image analysis in the euclidean space through linear hyperspaces is well\nstudied. However, in the quest for more effective image representations, we\nturn to hyperbolic manifolds. They provide a compelling alternative to capture\ncomplex hierarchical relationships in images with remarkably small\ndimensionality. To demonstrate hyperbolic embeddings' competence, we introduce\na light-weight hyperbolic graph neural network for image segmentation,\nencompassing patch-level features in a very small embedding size. Our solution,\nSeg-HGNN, surpasses the current best unsupervised method by 2.5\\%, 4\\% on\nVOC-07, VOC-12 for localization, and by 0.8\\%, 1.3\\% on CUB-200, ECSSD for\nsegmentation, respectively. With less than 7.5k trainable parameters, Seg-HGNN\ndelivers effective and fast ($\\approx 2$ images/second) results on very\nstandard GPUs like the GTX1650. This empirical evaluation presents compelling\nevidence of the efficacy and potential of hyperbolic representations for vision\ntasks.\n", "link": "http://arxiv.org/abs/2409.06589v1", "date": "2024-09-10", "relevancy": 2.6407, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5467}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5231}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seg-HGNN%3A%20Unsupervised%20and%20Light-Weight%20Image%20Segmentation%20with%0A%20%20Hyperbolic%20Graph%20Neural%20Networks&body=Title%3A%20Seg-HGNN%3A%20Unsupervised%20and%20Light-Weight%20Image%20Segmentation%20with%0A%20%20Hyperbolic%20Graph%20Neural%20Networks%0AAuthor%3A%20Debjyoti%20Mondal%20and%20Rahul%20Mishra%20and%20Chandan%20Pandey%0AAbstract%3A%20%20%20Image%20analysis%20in%20the%20euclidean%20space%20through%20linear%20hyperspaces%20is%20well%0Astudied.%20However%2C%20in%20the%20quest%20for%20more%20effective%20image%20representations%2C%20we%0Aturn%20to%20hyperbolic%20manifolds.%20They%20provide%20a%20compelling%20alternative%20to%20capture%0Acomplex%20hierarchical%20relationships%20in%20images%20with%20remarkably%20small%0Adimensionality.%20To%20demonstrate%20hyperbolic%20embeddings%27%20competence%2C%20we%20introduce%0Aa%20light-weight%20hyperbolic%20graph%20neural%20network%20for%20image%20segmentation%2C%0Aencompassing%20patch-level%20features%20in%20a%20very%20small%20embedding%20size.%20Our%20solution%2C%0ASeg-HGNN%2C%20surpasses%20the%20current%20best%20unsupervised%20method%20by%202.5%5C%25%2C%204%5C%25%20on%0AVOC-07%2C%20VOC-12%20for%20localization%2C%20and%20by%200.8%5C%25%2C%201.3%5C%25%20on%20CUB-200%2C%20ECSSD%20for%0Asegmentation%2C%20respectively.%20With%20less%20than%207.5k%20trainable%20parameters%2C%20Seg-HGNN%0Adelivers%20effective%20and%20fast%20%28%24%5Capprox%202%24%20images/second%29%20results%20on%20very%0Astandard%20GPUs%20like%20the%20GTX1650.%20This%20empirical%20evaluation%20presents%20compelling%0Aevidence%20of%20the%20efficacy%20and%20potential%20of%20hyperbolic%20representations%20for%20vision%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeg-HGNN%253A%2520Unsupervised%2520and%2520Light-Weight%2520Image%2520Segmentation%2520with%250A%2520%2520Hyperbolic%2520Graph%2520Neural%2520Networks%26entry.906535625%3DDebjyoti%2520Mondal%2520and%2520Rahul%2520Mishra%2520and%2520Chandan%2520Pandey%26entry.1292438233%3D%2520%2520Image%2520analysis%2520in%2520the%2520euclidean%2520space%2520through%2520linear%2520hyperspaces%2520is%2520well%250Astudied.%2520However%252C%2520in%2520the%2520quest%2520for%2520more%2520effective%2520image%2520representations%252C%2520we%250Aturn%2520to%2520hyperbolic%2520manifolds.%2520They%2520provide%2520a%2520compelling%2520alternative%2520to%2520capture%250Acomplex%2520hierarchical%2520relationships%2520in%2520images%2520with%2520remarkably%2520small%250Adimensionality.%2520To%2520demonstrate%2520hyperbolic%2520embeddings%2527%2520competence%252C%2520we%2520introduce%250Aa%2520light-weight%2520hyperbolic%2520graph%2520neural%2520network%2520for%2520image%2520segmentation%252C%250Aencompassing%2520patch-level%2520features%2520in%2520a%2520very%2520small%2520embedding%2520size.%2520Our%2520solution%252C%250ASeg-HGNN%252C%2520surpasses%2520the%2520current%2520best%2520unsupervised%2520method%2520by%25202.5%255C%2525%252C%25204%255C%2525%2520on%250AVOC-07%252C%2520VOC-12%2520for%2520localization%252C%2520and%2520by%25200.8%255C%2525%252C%25201.3%255C%2525%2520on%2520CUB-200%252C%2520ECSSD%2520for%250Asegmentation%252C%2520respectively.%2520With%2520less%2520than%25207.5k%2520trainable%2520parameters%252C%2520Seg-HGNN%250Adelivers%2520effective%2520and%2520fast%2520%2528%2524%255Capprox%25202%2524%2520images/second%2529%2520results%2520on%2520very%250Astandard%2520GPUs%2520like%2520the%2520GTX1650.%2520This%2520empirical%2520evaluation%2520presents%2520compelling%250Aevidence%2520of%2520the%2520efficacy%2520and%2520potential%2520of%2520hyperbolic%2520representations%2520for%2520vision%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seg-HGNN%3A%20Unsupervised%20and%20Light-Weight%20Image%20Segmentation%20with%0A%20%20Hyperbolic%20Graph%20Neural%20Networks&entry.906535625=Debjyoti%20Mondal%20and%20Rahul%20Mishra%20and%20Chandan%20Pandey&entry.1292438233=%20%20Image%20analysis%20in%20the%20euclidean%20space%20through%20linear%20hyperspaces%20is%20well%0Astudied.%20However%2C%20in%20the%20quest%20for%20more%20effective%20image%20representations%2C%20we%0Aturn%20to%20hyperbolic%20manifolds.%20They%20provide%20a%20compelling%20alternative%20to%20capture%0Acomplex%20hierarchical%20relationships%20in%20images%20with%20remarkably%20small%0Adimensionality.%20To%20demonstrate%20hyperbolic%20embeddings%27%20competence%2C%20we%20introduce%0Aa%20light-weight%20hyperbolic%20graph%20neural%20network%20for%20image%20segmentation%2C%0Aencompassing%20patch-level%20features%20in%20a%20very%20small%20embedding%20size.%20Our%20solution%2C%0ASeg-HGNN%2C%20surpasses%20the%20current%20best%20unsupervised%20method%20by%202.5%5C%25%2C%204%5C%25%20on%0AVOC-07%2C%20VOC-12%20for%20localization%2C%20and%20by%200.8%5C%25%2C%201.3%5C%25%20on%20CUB-200%2C%20ECSSD%20for%0Asegmentation%2C%20respectively.%20With%20less%20than%207.5k%20trainable%20parameters%2C%20Seg-HGNN%0Adelivers%20effective%20and%20fast%20%28%24%5Capprox%202%24%20images/second%29%20results%20on%20very%0Astandard%20GPUs%20like%20the%20GTX1650.%20This%20empirical%20evaluation%20presents%20compelling%0Aevidence%20of%20the%20efficacy%20and%20potential%20of%20hyperbolic%20representations%20for%20vision%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06589v1&entry.124074799=Read"},
{"title": "An Effective Context-Balanced Adaptation Approach for Long-Tailed Speech\n  Recognition", "author": "Yi-Cheng Wang and Li-Ting Pai and Bi-Cheng Yan and Hsin-Wei Wang and Chi-Han Lin and Berlin Chen", "abstract": "  End-to-end (E2E) automatic speech recognition (ASR) models have become\nstandard practice for various commercial applications. However, in real-world\nscenarios, the long-tailed nature of word distribution often leads E2E ASR\nmodels to perform well on common words but fall short in recognizing uncommon\nones. Recently, the notion of a contextual adapter (CA) was proposed to infuse\nexternal knowledge represented by a context word list into E2E ASR models.\nAlthough CA can improve recognition performance on rare words, two crucial data\nimbalance problems remain. First, when using low-frequency words as context\nwords during training, since these words rarely occur in the utterance, CA\nbecomes prone to overfit on attending to the <no-context> token due to\nhigher-frequency words not being present in the context list. Second, the\nlong-tailed distribution within the context list itself still causes the model\nto perform poorly on low-frequency context words. In light of this, we explore\nin-depth the impact of altering the context list to have words with different\nfrequency distributions on model performance, and meanwhile extend CA with a\nsimple yet effective context-balanced learning objective. A series of\nexperiments conducted on the AISHELL-1 benchmark dataset suggests that using\nall vocabulary words from the training corpus as the context list and pairing\nthem with our balanced objective yields the best performance, demonstrating a\nsignificant reduction in character error rate (CER) by up to 1.21% and a more\npronounced 9.44% reduction in the error rate of zero-shot words.\n", "link": "http://arxiv.org/abs/2409.06468v1", "date": "2024-09-10", "relevancy": 2.5976, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Effective%20Context-Balanced%20Adaptation%20Approach%20for%20Long-Tailed%20Speech%0A%20%20Recognition&body=Title%3A%20An%20Effective%20Context-Balanced%20Adaptation%20Approach%20for%20Long-Tailed%20Speech%0A%20%20Recognition%0AAuthor%3A%20Yi-Cheng%20Wang%20and%20Li-Ting%20Pai%20and%20Bi-Cheng%20Yan%20and%20Hsin-Wei%20Wang%20and%20Chi-Han%20Lin%20and%20Berlin%20Chen%0AAbstract%3A%20%20%20End-to-end%20%28E2E%29%20automatic%20speech%20recognition%20%28ASR%29%20models%20have%20become%0Astandard%20practice%20for%20various%20commercial%20applications.%20However%2C%20in%20real-world%0Ascenarios%2C%20the%20long-tailed%20nature%20of%20word%20distribution%20often%20leads%20E2E%20ASR%0Amodels%20to%20perform%20well%20on%20common%20words%20but%20fall%20short%20in%20recognizing%20uncommon%0Aones.%20Recently%2C%20the%20notion%20of%20a%20contextual%20adapter%20%28CA%29%20was%20proposed%20to%20infuse%0Aexternal%20knowledge%20represented%20by%20a%20context%20word%20list%20into%20E2E%20ASR%20models.%0AAlthough%20CA%20can%20improve%20recognition%20performance%20on%20rare%20words%2C%20two%20crucial%20data%0Aimbalance%20problems%20remain.%20First%2C%20when%20using%20low-frequency%20words%20as%20context%0Awords%20during%20training%2C%20since%20these%20words%20rarely%20occur%20in%20the%20utterance%2C%20CA%0Abecomes%20prone%20to%20overfit%20on%20attending%20to%20the%20%3Cno-context%3E%20token%20due%20to%0Ahigher-frequency%20words%20not%20being%20present%20in%20the%20context%20list.%20Second%2C%20the%0Along-tailed%20distribution%20within%20the%20context%20list%20itself%20still%20causes%20the%20model%0Ato%20perform%20poorly%20on%20low-frequency%20context%20words.%20In%20light%20of%20this%2C%20we%20explore%0Ain-depth%20the%20impact%20of%20altering%20the%20context%20list%20to%20have%20words%20with%20different%0Afrequency%20distributions%20on%20model%20performance%2C%20and%20meanwhile%20extend%20CA%20with%20a%0Asimple%20yet%20effective%20context-balanced%20learning%20objective.%20A%20series%20of%0Aexperiments%20conducted%20on%20the%20AISHELL-1%20benchmark%20dataset%20suggests%20that%20using%0Aall%20vocabulary%20words%20from%20the%20training%20corpus%20as%20the%20context%20list%20and%20pairing%0Athem%20with%20our%20balanced%20objective%20yields%20the%20best%20performance%2C%20demonstrating%20a%0Asignificant%20reduction%20in%20character%20error%20rate%20%28CER%29%20by%20up%20to%201.21%25%20and%20a%20more%0Apronounced%209.44%25%20reduction%20in%20the%20error%20rate%20of%20zero-shot%20words.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Effective%2520Context-Balanced%2520Adaptation%2520Approach%2520for%2520Long-Tailed%2520Speech%250A%2520%2520Recognition%26entry.906535625%3DYi-Cheng%2520Wang%2520and%2520Li-Ting%2520Pai%2520and%2520Bi-Cheng%2520Yan%2520and%2520Hsin-Wei%2520Wang%2520and%2520Chi-Han%2520Lin%2520and%2520Berlin%2520Chen%26entry.1292438233%3D%2520%2520End-to-end%2520%2528E2E%2529%2520automatic%2520speech%2520recognition%2520%2528ASR%2529%2520models%2520have%2520become%250Astandard%2520practice%2520for%2520various%2520commercial%2520applications.%2520However%252C%2520in%2520real-world%250Ascenarios%252C%2520the%2520long-tailed%2520nature%2520of%2520word%2520distribution%2520often%2520leads%2520E2E%2520ASR%250Amodels%2520to%2520perform%2520well%2520on%2520common%2520words%2520but%2520fall%2520short%2520in%2520recognizing%2520uncommon%250Aones.%2520Recently%252C%2520the%2520notion%2520of%2520a%2520contextual%2520adapter%2520%2528CA%2529%2520was%2520proposed%2520to%2520infuse%250Aexternal%2520knowledge%2520represented%2520by%2520a%2520context%2520word%2520list%2520into%2520E2E%2520ASR%2520models.%250AAlthough%2520CA%2520can%2520improve%2520recognition%2520performance%2520on%2520rare%2520words%252C%2520two%2520crucial%2520data%250Aimbalance%2520problems%2520remain.%2520First%252C%2520when%2520using%2520low-frequency%2520words%2520as%2520context%250Awords%2520during%2520training%252C%2520since%2520these%2520words%2520rarely%2520occur%2520in%2520the%2520utterance%252C%2520CA%250Abecomes%2520prone%2520to%2520overfit%2520on%2520attending%2520to%2520the%2520%253Cno-context%253E%2520token%2520due%2520to%250Ahigher-frequency%2520words%2520not%2520being%2520present%2520in%2520the%2520context%2520list.%2520Second%252C%2520the%250Along-tailed%2520distribution%2520within%2520the%2520context%2520list%2520itself%2520still%2520causes%2520the%2520model%250Ato%2520perform%2520poorly%2520on%2520low-frequency%2520context%2520words.%2520In%2520light%2520of%2520this%252C%2520we%2520explore%250Ain-depth%2520the%2520impact%2520of%2520altering%2520the%2520context%2520list%2520to%2520have%2520words%2520with%2520different%250Afrequency%2520distributions%2520on%2520model%2520performance%252C%2520and%2520meanwhile%2520extend%2520CA%2520with%2520a%250Asimple%2520yet%2520effective%2520context-balanced%2520learning%2520objective.%2520A%2520series%2520of%250Aexperiments%2520conducted%2520on%2520the%2520AISHELL-1%2520benchmark%2520dataset%2520suggests%2520that%2520using%250Aall%2520vocabulary%2520words%2520from%2520the%2520training%2520corpus%2520as%2520the%2520context%2520list%2520and%2520pairing%250Athem%2520with%2520our%2520balanced%2520objective%2520yields%2520the%2520best%2520performance%252C%2520demonstrating%2520a%250Asignificant%2520reduction%2520in%2520character%2520error%2520rate%2520%2528CER%2529%2520by%2520up%2520to%25201.21%2525%2520and%2520a%2520more%250Apronounced%25209.44%2525%2520reduction%2520in%2520the%2520error%2520rate%2520of%2520zero-shot%2520words.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Effective%20Context-Balanced%20Adaptation%20Approach%20for%20Long-Tailed%20Speech%0A%20%20Recognition&entry.906535625=Yi-Cheng%20Wang%20and%20Li-Ting%20Pai%20and%20Bi-Cheng%20Yan%20and%20Hsin-Wei%20Wang%20and%20Chi-Han%20Lin%20and%20Berlin%20Chen&entry.1292438233=%20%20End-to-end%20%28E2E%29%20automatic%20speech%20recognition%20%28ASR%29%20models%20have%20become%0Astandard%20practice%20for%20various%20commercial%20applications.%20However%2C%20in%20real-world%0Ascenarios%2C%20the%20long-tailed%20nature%20of%20word%20distribution%20often%20leads%20E2E%20ASR%0Amodels%20to%20perform%20well%20on%20common%20words%20but%20fall%20short%20in%20recognizing%20uncommon%0Aones.%20Recently%2C%20the%20notion%20of%20a%20contextual%20adapter%20%28CA%29%20was%20proposed%20to%20infuse%0Aexternal%20knowledge%20represented%20by%20a%20context%20word%20list%20into%20E2E%20ASR%20models.%0AAlthough%20CA%20can%20improve%20recognition%20performance%20on%20rare%20words%2C%20two%20crucial%20data%0Aimbalance%20problems%20remain.%20First%2C%20when%20using%20low-frequency%20words%20as%20context%0Awords%20during%20training%2C%20since%20these%20words%20rarely%20occur%20in%20the%20utterance%2C%20CA%0Abecomes%20prone%20to%20overfit%20on%20attending%20to%20the%20%3Cno-context%3E%20token%20due%20to%0Ahigher-frequency%20words%20not%20being%20present%20in%20the%20context%20list.%20Second%2C%20the%0Along-tailed%20distribution%20within%20the%20context%20list%20itself%20still%20causes%20the%20model%0Ato%20perform%20poorly%20on%20low-frequency%20context%20words.%20In%20light%20of%20this%2C%20we%20explore%0Ain-depth%20the%20impact%20of%20altering%20the%20context%20list%20to%20have%20words%20with%20different%0Afrequency%20distributions%20on%20model%20performance%2C%20and%20meanwhile%20extend%20CA%20with%20a%0Asimple%20yet%20effective%20context-balanced%20learning%20objective.%20A%20series%20of%0Aexperiments%20conducted%20on%20the%20AISHELL-1%20benchmark%20dataset%20suggests%20that%20using%0Aall%20vocabulary%20words%20from%20the%20training%20corpus%20as%20the%20context%20list%20and%20pairing%0Athem%20with%20our%20balanced%20objective%20yields%20the%20best%20performance%2C%20demonstrating%20a%0Asignificant%20reduction%20in%20character%20error%20rate%20%28CER%29%20by%20up%20to%201.21%25%20and%20a%20more%0Apronounced%209.44%25%20reduction%20in%20the%20error%20rate%20of%20zero-shot%20words.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06468v1&entry.124074799=Read"},
{"title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models", "author": "Qingkai Fang and Shoutao Guo and Yan Zhou and Zhengrui Ma and Shaolei Zhang and Yang Feng", "abstract": "  Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.\n", "link": "http://arxiv.org/abs/2409.06666v1", "date": "2024-09-10", "relevancy": 2.519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaMA-Omni%3A%20Seamless%20Speech%20Interaction%20with%20Large%20Language%20Models&body=Title%3A%20LLaMA-Omni%3A%20Seamless%20Speech%20Interaction%20with%20Large%20Language%20Models%0AAuthor%3A%20Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Zhengrui%20Ma%20and%20Shaolei%20Zhang%20and%20Yang%20Feng%0AAbstract%3A%20%20%20Models%20like%20GPT-4o%20enable%20real-time%20interaction%20with%20large%20language%20models%0A%28LLMs%29%20through%20speech%2C%20significantly%20enhancing%20user%20experience%20compared%20to%0Atraditional%20text-based%20interaction.%20However%2C%20there%20is%20still%20a%20lack%20of%0Aexploration%20on%20how%20to%20build%20speech%20interaction%20models%20based%20on%20open-source%0ALLMs.%20To%20address%20this%2C%20we%20propose%20LLaMA-Omni%2C%20a%20novel%20model%20architecture%0Adesigned%20for%20low-latency%20and%20high-quality%20speech%20interaction%20with%20LLMs.%0ALLaMA-Omni%20integrates%20a%20pretrained%20speech%20encoder%2C%20a%20speech%20adaptor%2C%20an%20LLM%2C%0Aand%20a%20streaming%20speech%20decoder.%20It%20eliminates%20the%20need%20for%20speech%0Atranscription%2C%20and%20can%20simultaneously%20generate%20text%20and%20speech%20responses%0Adirectly%20from%20speech%20instructions%20with%20extremely%20low%20latency.%20We%20build%20our%0Amodel%20based%20on%20the%20latest%20Llama-3.1-8B-Instruct%20model.%20To%20align%20the%20model%20with%0Aspeech%20interaction%20scenarios%2C%20we%20construct%20a%20dataset%20named%20InstructS2S-200K%2C%0Awhich%20includes%20200K%20speech%20instructions%20and%20corresponding%20speech%20responses.%0AExperimental%20results%20show%20that%20compared%20to%20previous%20speech-language%20models%2C%0ALLaMA-Omni%20provides%20better%20responses%20in%20both%20content%20and%20style%2C%20with%20a%20response%0Alatency%20as%20low%20as%20226ms.%20Additionally%2C%20training%20LLaMA-Omni%20takes%20less%20than%203%0Adays%20on%20just%204%20GPUs%2C%20paving%20the%20way%20for%20the%20efficient%20development%20of%0Aspeech-language%20models%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaMA-Omni%253A%2520Seamless%2520Speech%2520Interaction%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DQingkai%2520Fang%2520and%2520Shoutao%2520Guo%2520and%2520Yan%2520Zhou%2520and%2520Zhengrui%2520Ma%2520and%2520Shaolei%2520Zhang%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520Models%2520like%2520GPT-4o%2520enable%2520real-time%2520interaction%2520with%2520large%2520language%2520models%250A%2528LLMs%2529%2520through%2520speech%252C%2520significantly%2520enhancing%2520user%2520experience%2520compared%2520to%250Atraditional%2520text-based%2520interaction.%2520However%252C%2520there%2520is%2520still%2520a%2520lack%2520of%250Aexploration%2520on%2520how%2520to%2520build%2520speech%2520interaction%2520models%2520based%2520on%2520open-source%250ALLMs.%2520To%2520address%2520this%252C%2520we%2520propose%2520LLaMA-Omni%252C%2520a%2520novel%2520model%2520architecture%250Adesigned%2520for%2520low-latency%2520and%2520high-quality%2520speech%2520interaction%2520with%2520LLMs.%250ALLaMA-Omni%2520integrates%2520a%2520pretrained%2520speech%2520encoder%252C%2520a%2520speech%2520adaptor%252C%2520an%2520LLM%252C%250Aand%2520a%2520streaming%2520speech%2520decoder.%2520It%2520eliminates%2520the%2520need%2520for%2520speech%250Atranscription%252C%2520and%2520can%2520simultaneously%2520generate%2520text%2520and%2520speech%2520responses%250Adirectly%2520from%2520speech%2520instructions%2520with%2520extremely%2520low%2520latency.%2520We%2520build%2520our%250Amodel%2520based%2520on%2520the%2520latest%2520Llama-3.1-8B-Instruct%2520model.%2520To%2520align%2520the%2520model%2520with%250Aspeech%2520interaction%2520scenarios%252C%2520we%2520construct%2520a%2520dataset%2520named%2520InstructS2S-200K%252C%250Awhich%2520includes%2520200K%2520speech%2520instructions%2520and%2520corresponding%2520speech%2520responses.%250AExperimental%2520results%2520show%2520that%2520compared%2520to%2520previous%2520speech-language%2520models%252C%250ALLaMA-Omni%2520provides%2520better%2520responses%2520in%2520both%2520content%2520and%2520style%252C%2520with%2520a%2520response%250Alatency%2520as%2520low%2520as%2520226ms.%2520Additionally%252C%2520training%2520LLaMA-Omni%2520takes%2520less%2520than%25203%250Adays%2520on%2520just%25204%2520GPUs%252C%2520paving%2520the%2520way%2520for%2520the%2520efficient%2520development%2520of%250Aspeech-language%2520models%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaMA-Omni%3A%20Seamless%20Speech%20Interaction%20with%20Large%20Language%20Models&entry.906535625=Qingkai%20Fang%20and%20Shoutao%20Guo%20and%20Yan%20Zhou%20and%20Zhengrui%20Ma%20and%20Shaolei%20Zhang%20and%20Yang%20Feng&entry.1292438233=%20%20Models%20like%20GPT-4o%20enable%20real-time%20interaction%20with%20large%20language%20models%0A%28LLMs%29%20through%20speech%2C%20significantly%20enhancing%20user%20experience%20compared%20to%0Atraditional%20text-based%20interaction.%20However%2C%20there%20is%20still%20a%20lack%20of%0Aexploration%20on%20how%20to%20build%20speech%20interaction%20models%20based%20on%20open-source%0ALLMs.%20To%20address%20this%2C%20we%20propose%20LLaMA-Omni%2C%20a%20novel%20model%20architecture%0Adesigned%20for%20low-latency%20and%20high-quality%20speech%20interaction%20with%20LLMs.%0ALLaMA-Omni%20integrates%20a%20pretrained%20speech%20encoder%2C%20a%20speech%20adaptor%2C%20an%20LLM%2C%0Aand%20a%20streaming%20speech%20decoder.%20It%20eliminates%20the%20need%20for%20speech%0Atranscription%2C%20and%20can%20simultaneously%20generate%20text%20and%20speech%20responses%0Adirectly%20from%20speech%20instructions%20with%20extremely%20low%20latency.%20We%20build%20our%0Amodel%20based%20on%20the%20latest%20Llama-3.1-8B-Instruct%20model.%20To%20align%20the%20model%20with%0Aspeech%20interaction%20scenarios%2C%20we%20construct%20a%20dataset%20named%20InstructS2S-200K%2C%0Awhich%20includes%20200K%20speech%20instructions%20and%20corresponding%20speech%20responses.%0AExperimental%20results%20show%20that%20compared%20to%20previous%20speech-language%20models%2C%0ALLaMA-Omni%20provides%20better%20responses%20in%20both%20content%20and%20style%2C%20with%20a%20response%0Alatency%20as%20low%20as%20226ms.%20Additionally%2C%20training%20LLaMA-Omni%20takes%20less%20than%203%0Adays%20on%20just%204%20GPUs%2C%20paving%20the%20way%20for%20the%20efficient%20development%20of%0Aspeech-language%20models%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06666v1&entry.124074799=Read"},
{"title": "When to Extract ReID Features: A Selective Approach for Improved\n  Multiple Object Tracking", "author": "Emirhan Bayar and Cemal Aker", "abstract": "  Extracting and matching Re-Identification (ReID) features is used by many\nstate-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularly\neffective against frequent and long-term occlusions. While end-to-end object\ndetection and tracking have been the main focus of recent research, they have\nyet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus,\nfrom an application standpoint, methods with separate detection and embedding\nremain the best option for accuracy, modularity, and ease of implementation,\nthough they are impractical for edge devices due to the overhead involved. In\nthis paper, we investigate a selective approach to minimize the overhead of\nfeature extraction while preserving accuracy, modularity, and ease of\nimplementation. This approach can be integrated into various SOTA methods. We\ndemonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT.\nExperiments on MOT17, MOT20, and DanceTrack datasets show that our mechanism\nretains the advantages of feature extraction during occlusions while\nsignificantly reducing runtime. Additionally, it improves accuracy by\npreventing confusion in the feature-matching stage, particularly in cases of\ndeformation and appearance similarity, which are common in DanceTrack.\nhttps://github.com/emirhanbayar/Fast-StrongSORT,\nhttps://github.com/emirhanbayar/Fast-Deep-OC-SORT\n", "link": "http://arxiv.org/abs/2409.06617v1", "date": "2024-09-10", "relevancy": 2.5089, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5101}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4994}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20to%20Extract%20ReID%20Features%3A%20A%20Selective%20Approach%20for%20Improved%0A%20%20Multiple%20Object%20Tracking&body=Title%3A%20When%20to%20Extract%20ReID%20Features%3A%20A%20Selective%20Approach%20for%20Improved%0A%20%20Multiple%20Object%20Tracking%0AAuthor%3A%20Emirhan%20Bayar%20and%20Cemal%20Aker%0AAbstract%3A%20%20%20Extracting%20and%20matching%20Re-Identification%20%28ReID%29%20features%20is%20used%20by%20many%0Astate-of-the-art%20%28SOTA%29%20Multiple%20Object%20Tracking%20%28MOT%29%20methods%2C%20particularly%0Aeffective%20against%20frequent%20and%20long-term%20occlusions.%20While%20end-to-end%20object%0Adetection%20and%20tracking%20have%20been%20the%20main%20focus%20of%20recent%20research%2C%20they%20have%0Ayet%20to%20outperform%20traditional%20methods%20in%20benchmarks%20like%20MOT17%20and%20MOT20.%20Thus%2C%0Afrom%20an%20application%20standpoint%2C%20methods%20with%20separate%20detection%20and%20embedding%0Aremain%20the%20best%20option%20for%20accuracy%2C%20modularity%2C%20and%20ease%20of%20implementation%2C%0Athough%20they%20are%20impractical%20for%20edge%20devices%20due%20to%20the%20overhead%20involved.%20In%0Athis%20paper%2C%20we%20investigate%20a%20selective%20approach%20to%20minimize%20the%20overhead%20of%0Afeature%20extraction%20while%20preserving%20accuracy%2C%20modularity%2C%20and%20ease%20of%0Aimplementation.%20This%20approach%20can%20be%20integrated%20into%20various%20SOTA%20methods.%20We%0Ademonstrate%20its%20effectiveness%20by%20applying%20it%20to%20StrongSORT%20and%20Deep%20OC-SORT.%0AExperiments%20on%20MOT17%2C%20MOT20%2C%20and%20DanceTrack%20datasets%20show%20that%20our%20mechanism%0Aretains%20the%20advantages%20of%20feature%20extraction%20during%20occlusions%20while%0Asignificantly%20reducing%20runtime.%20Additionally%2C%20it%20improves%20accuracy%20by%0Apreventing%20confusion%20in%20the%20feature-matching%20stage%2C%20particularly%20in%20cases%20of%0Adeformation%20and%20appearance%20similarity%2C%20which%20are%20common%20in%20DanceTrack.%0Ahttps%3A//github.com/emirhanbayar/Fast-StrongSORT%2C%0Ahttps%3A//github.com/emirhanbayar/Fast-Deep-OC-SORT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520to%2520Extract%2520ReID%2520Features%253A%2520A%2520Selective%2520Approach%2520for%2520Improved%250A%2520%2520Multiple%2520Object%2520Tracking%26entry.906535625%3DEmirhan%2520Bayar%2520and%2520Cemal%2520Aker%26entry.1292438233%3D%2520%2520Extracting%2520and%2520matching%2520Re-Identification%2520%2528ReID%2529%2520features%2520is%2520used%2520by%2520many%250Astate-of-the-art%2520%2528SOTA%2529%2520Multiple%2520Object%2520Tracking%2520%2528MOT%2529%2520methods%252C%2520particularly%250Aeffective%2520against%2520frequent%2520and%2520long-term%2520occlusions.%2520While%2520end-to-end%2520object%250Adetection%2520and%2520tracking%2520have%2520been%2520the%2520main%2520focus%2520of%2520recent%2520research%252C%2520they%2520have%250Ayet%2520to%2520outperform%2520traditional%2520methods%2520in%2520benchmarks%2520like%2520MOT17%2520and%2520MOT20.%2520Thus%252C%250Afrom%2520an%2520application%2520standpoint%252C%2520methods%2520with%2520separate%2520detection%2520and%2520embedding%250Aremain%2520the%2520best%2520option%2520for%2520accuracy%252C%2520modularity%252C%2520and%2520ease%2520of%2520implementation%252C%250Athough%2520they%2520are%2520impractical%2520for%2520edge%2520devices%2520due%2520to%2520the%2520overhead%2520involved.%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520a%2520selective%2520approach%2520to%2520minimize%2520the%2520overhead%2520of%250Afeature%2520extraction%2520while%2520preserving%2520accuracy%252C%2520modularity%252C%2520and%2520ease%2520of%250Aimplementation.%2520This%2520approach%2520can%2520be%2520integrated%2520into%2520various%2520SOTA%2520methods.%2520We%250Ademonstrate%2520its%2520effectiveness%2520by%2520applying%2520it%2520to%2520StrongSORT%2520and%2520Deep%2520OC-SORT.%250AExperiments%2520on%2520MOT17%252C%2520MOT20%252C%2520and%2520DanceTrack%2520datasets%2520show%2520that%2520our%2520mechanism%250Aretains%2520the%2520advantages%2520of%2520feature%2520extraction%2520during%2520occlusions%2520while%250Asignificantly%2520reducing%2520runtime.%2520Additionally%252C%2520it%2520improves%2520accuracy%2520by%250Apreventing%2520confusion%2520in%2520the%2520feature-matching%2520stage%252C%2520particularly%2520in%2520cases%2520of%250Adeformation%2520and%2520appearance%2520similarity%252C%2520which%2520are%2520common%2520in%2520DanceTrack.%250Ahttps%253A//github.com/emirhanbayar/Fast-StrongSORT%252C%250Ahttps%253A//github.com/emirhanbayar/Fast-Deep-OC-SORT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20to%20Extract%20ReID%20Features%3A%20A%20Selective%20Approach%20for%20Improved%0A%20%20Multiple%20Object%20Tracking&entry.906535625=Emirhan%20Bayar%20and%20Cemal%20Aker&entry.1292438233=%20%20Extracting%20and%20matching%20Re-Identification%20%28ReID%29%20features%20is%20used%20by%20many%0Astate-of-the-art%20%28SOTA%29%20Multiple%20Object%20Tracking%20%28MOT%29%20methods%2C%20particularly%0Aeffective%20against%20frequent%20and%20long-term%20occlusions.%20While%20end-to-end%20object%0Adetection%20and%20tracking%20have%20been%20the%20main%20focus%20of%20recent%20research%2C%20they%20have%0Ayet%20to%20outperform%20traditional%20methods%20in%20benchmarks%20like%20MOT17%20and%20MOT20.%20Thus%2C%0Afrom%20an%20application%20standpoint%2C%20methods%20with%20separate%20detection%20and%20embedding%0Aremain%20the%20best%20option%20for%20accuracy%2C%20modularity%2C%20and%20ease%20of%20implementation%2C%0Athough%20they%20are%20impractical%20for%20edge%20devices%20due%20to%20the%20overhead%20involved.%20In%0Athis%20paper%2C%20we%20investigate%20a%20selective%20approach%20to%20minimize%20the%20overhead%20of%0Afeature%20extraction%20while%20preserving%20accuracy%2C%20modularity%2C%20and%20ease%20of%0Aimplementation.%20This%20approach%20can%20be%20integrated%20into%20various%20SOTA%20methods.%20We%0Ademonstrate%20its%20effectiveness%20by%20applying%20it%20to%20StrongSORT%20and%20Deep%20OC-SORT.%0AExperiments%20on%20MOT17%2C%20MOT20%2C%20and%20DanceTrack%20datasets%20show%20that%20our%20mechanism%0Aretains%20the%20advantages%20of%20feature%20extraction%20during%20occlusions%20while%0Asignificantly%20reducing%20runtime.%20Additionally%2C%20it%20improves%20accuracy%20by%0Apreventing%20confusion%20in%20the%20feature-matching%20stage%2C%20particularly%20in%20cases%20of%0Adeformation%20and%20appearance%20similarity%2C%20which%20are%20common%20in%20DanceTrack.%0Ahttps%3A//github.com/emirhanbayar/Fast-StrongSORT%2C%0Ahttps%3A//github.com/emirhanbayar/Fast-Deep-OC-SORT%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06617v1&entry.124074799=Read"},
{"title": "Length Desensitization in Directed Preference Optimization", "author": "Wei Liu and Yang Bai and Chengcheng Han and Rongxiang Weng and Jun Xu and Xuezhi Cao and Jingang Wang and Xunliang Cai", "abstract": "  Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40\\% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-real preferences.\n", "link": "http://arxiv.org/abs/2409.06411v1", "date": "2024-09-10", "relevancy": 2.4705, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Length%20Desensitization%20in%20Directed%20Preference%20Optimization&body=Title%3A%20Length%20Desensitization%20in%20Directed%20Preference%20Optimization%0AAuthor%3A%20Wei%20Liu%20and%20Yang%20Bai%20and%20Chengcheng%20Han%20and%20Rongxiang%20Weng%20and%20Jun%20Xu%20and%20Xuezhi%20Cao%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%0AAbstract%3A%20%20%20Direct%20Preference%20Optimization%20%28DPO%29%20is%20widely%20utilized%20in%20the%20Reinforcement%0ALearning%20from%20Human%20Feedback%20%28RLHF%29%20phase%20to%20align%20Large%20Language%20Models%20%28LLMs%29%0Awith%20human%20preferences%2C%20thereby%20enhancing%20both%20their%20harmlessness%20and%20efficacy.%0AHowever%2C%20it%20has%20been%20observed%20that%20DPO%20tends%20to%20over-optimize%20for%20verbosity%2C%0Awhich%20can%20detrimentally%20affect%20both%20performance%20and%20user%20experience.%20In%20this%0Apaper%2C%20we%20conduct%20an%20in-depth%20theoretical%20analysis%20of%20DPO%27s%20optimization%0Aobjective%20and%20reveal%20a%20strong%20correlation%20between%20its%20implicit%20reward%20and%20data%0Alength.%20This%20correlation%20misguides%20the%20optimization%20direction%2C%20resulting%20in%0Alength%20sensitivity%20during%20the%20DPO%20training%20and%20leading%20to%20verbosity.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20length-desensitization%20improvement%20method%20for%20DPO%2C%0Atermed%20LD-DPO.%20The%20proposed%20method%20aims%20to%20desensitize%20DPO%20to%20data%20length%20by%0Adecoupling%20explicit%20length%20preference%2C%20which%20is%20relatively%20insignificant%2C%20from%0Athe%20other%20implicit%20preferences%2C%20thereby%20enabling%20more%20effective%20learning%20of%20the%0Aintrinsic%20preferences.%20We%20utilized%20two%20settings%20%28Base%20and%20Instruct%29%20of%0ALlama2-13B%2C%20Llama3-8B%2C%20and%20Qwen2-7B%20for%20experimental%20validation%20on%20various%0Abenchmarks%20including%20MT-Bench%20and%20AlpacaEval%202.%20The%20experimental%20results%0Aindicate%20that%20LD-DPO%20consistently%20outperforms%20DPO%20and%20other%20baseline%20methods%2C%0Aachieving%20more%20concise%20responses%20with%20a%2010-40%5C%25%20reduction%20in%20length%20compared%20to%0ADPO.%20We%20conducted%20in-depth%20experimental%20analyses%20to%20demonstrate%20that%20LD-DPO%20can%0Aindeed%20achieve%20length%20desensitization%20and%20align%20the%20model%20more%20closely%20with%0Ahuman-real%20preferences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLength%2520Desensitization%2520in%2520Directed%2520Preference%2520Optimization%26entry.906535625%3DWei%2520Liu%2520and%2520Yang%2520Bai%2520and%2520Chengcheng%2520Han%2520and%2520Rongxiang%2520Weng%2520and%2520Jun%2520Xu%2520and%2520Xuezhi%2520Cao%2520and%2520Jingang%2520Wang%2520and%2520Xunliang%2520Cai%26entry.1292438233%3D%2520%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520is%2520widely%2520utilized%2520in%2520the%2520Reinforcement%250ALearning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520phase%2520to%2520align%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Awith%2520human%2520preferences%252C%2520thereby%2520enhancing%2520both%2520their%2520harmlessness%2520and%2520efficacy.%250AHowever%252C%2520it%2520has%2520been%2520observed%2520that%2520DPO%2520tends%2520to%2520over-optimize%2520for%2520verbosity%252C%250Awhich%2520can%2520detrimentally%2520affect%2520both%2520performance%2520and%2520user%2520experience.%2520In%2520this%250Apaper%252C%2520we%2520conduct%2520an%2520in-depth%2520theoretical%2520analysis%2520of%2520DPO%2527s%2520optimization%250Aobjective%2520and%2520reveal%2520a%2520strong%2520correlation%2520between%2520its%2520implicit%2520reward%2520and%2520data%250Alength.%2520This%2520correlation%2520misguides%2520the%2520optimization%2520direction%252C%2520resulting%2520in%250Alength%2520sensitivity%2520during%2520the%2520DPO%2520training%2520and%2520leading%2520to%2520verbosity.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520a%2520length-desensitization%2520improvement%2520method%2520for%2520DPO%252C%250Atermed%2520LD-DPO.%2520The%2520proposed%2520method%2520aims%2520to%2520desensitize%2520DPO%2520to%2520data%2520length%2520by%250Adecoupling%2520explicit%2520length%2520preference%252C%2520which%2520is%2520relatively%2520insignificant%252C%2520from%250Athe%2520other%2520implicit%2520preferences%252C%2520thereby%2520enabling%2520more%2520effective%2520learning%2520of%2520the%250Aintrinsic%2520preferences.%2520We%2520utilized%2520two%2520settings%2520%2528Base%2520and%2520Instruct%2529%2520of%250ALlama2-13B%252C%2520Llama3-8B%252C%2520and%2520Qwen2-7B%2520for%2520experimental%2520validation%2520on%2520various%250Abenchmarks%2520including%2520MT-Bench%2520and%2520AlpacaEval%25202.%2520The%2520experimental%2520results%250Aindicate%2520that%2520LD-DPO%2520consistently%2520outperforms%2520DPO%2520and%2520other%2520baseline%2520methods%252C%250Aachieving%2520more%2520concise%2520responses%2520with%2520a%252010-40%255C%2525%2520reduction%2520in%2520length%2520compared%2520to%250ADPO.%2520We%2520conducted%2520in-depth%2520experimental%2520analyses%2520to%2520demonstrate%2520that%2520LD-DPO%2520can%250Aindeed%2520achieve%2520length%2520desensitization%2520and%2520align%2520the%2520model%2520more%2520closely%2520with%250Ahuman-real%2520preferences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Length%20Desensitization%20in%20Directed%20Preference%20Optimization&entry.906535625=Wei%20Liu%20and%20Yang%20Bai%20and%20Chengcheng%20Han%20and%20Rongxiang%20Weng%20and%20Jun%20Xu%20and%20Xuezhi%20Cao%20and%20Jingang%20Wang%20and%20Xunliang%20Cai&entry.1292438233=%20%20Direct%20Preference%20Optimization%20%28DPO%29%20is%20widely%20utilized%20in%20the%20Reinforcement%0ALearning%20from%20Human%20Feedback%20%28RLHF%29%20phase%20to%20align%20Large%20Language%20Models%20%28LLMs%29%0Awith%20human%20preferences%2C%20thereby%20enhancing%20both%20their%20harmlessness%20and%20efficacy.%0AHowever%2C%20it%20has%20been%20observed%20that%20DPO%20tends%20to%20over-optimize%20for%20verbosity%2C%0Awhich%20can%20detrimentally%20affect%20both%20performance%20and%20user%20experience.%20In%20this%0Apaper%2C%20we%20conduct%20an%20in-depth%20theoretical%20analysis%20of%20DPO%27s%20optimization%0Aobjective%20and%20reveal%20a%20strong%20correlation%20between%20its%20implicit%20reward%20and%20data%0Alength.%20This%20correlation%20misguides%20the%20optimization%20direction%2C%20resulting%20in%0Alength%20sensitivity%20during%20the%20DPO%20training%20and%20leading%20to%20verbosity.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20length-desensitization%20improvement%20method%20for%20DPO%2C%0Atermed%20LD-DPO.%20The%20proposed%20method%20aims%20to%20desensitize%20DPO%20to%20data%20length%20by%0Adecoupling%20explicit%20length%20preference%2C%20which%20is%20relatively%20insignificant%2C%20from%0Athe%20other%20implicit%20preferences%2C%20thereby%20enabling%20more%20effective%20learning%20of%20the%0Aintrinsic%20preferences.%20We%20utilized%20two%20settings%20%28Base%20and%20Instruct%29%20of%0ALlama2-13B%2C%20Llama3-8B%2C%20and%20Qwen2-7B%20for%20experimental%20validation%20on%20various%0Abenchmarks%20including%20MT-Bench%20and%20AlpacaEval%202.%20The%20experimental%20results%0Aindicate%20that%20LD-DPO%20consistently%20outperforms%20DPO%20and%20other%20baseline%20methods%2C%0Aachieving%20more%20concise%20responses%20with%20a%2010-40%5C%25%20reduction%20in%20length%20compared%20to%0ADPO.%20We%20conducted%20in-depth%20experimental%20analyses%20to%20demonstrate%20that%20LD-DPO%20can%0Aindeed%20achieve%20length%20desensitization%20and%20align%20the%20model%20more%20closely%20with%0Ahuman-real%20preferences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06411v1&entry.124074799=Read"},
{"title": "Prompt2Fashion: An automatically generated fashion dataset", "author": "Georgia Argyro and Angeliki Dimitriou and Maria Lymperaiou and Giorgos Filandrianos and Giorgos Stamou", "abstract": "  Despite the rapid evolution and increasing efficacy of language and vision\ngenerative models, there remains a lack of comprehensive datasets that bridge\nthe gap between personalized fashion needs and AI-driven design, limiting the\npotential for truly inclusive and customized fashion solutions. In this work,\nwe leverage generative models to automatically construct a fashion image\ndataset tailored to various occasions, styles, and body types as instructed by\nusers. We use different Large Language Models (LLMs) and prompting strategies\nto offer personalized outfits of high aesthetic quality, detail, and relevance\nto both expert and non-expert users' requirements, as demonstrated by\nqualitative analysis. Up until now the evaluation of the generated outfits has\nbeen conducted by non-expert human subjects. Despite the provided fine-grained\ninsights on the quality and relevance of generation, we extend the discussion\non the importance of expert knowledge for the evaluation of artistic\nAI-generated datasets such as this one. Our dataset is publicly available on\nGitHub at https://github.com/georgiarg/Prompt2Fashion.\n", "link": "http://arxiv.org/abs/2409.06442v1", "date": "2024-09-10", "relevancy": 2.4672, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6798}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6681}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt2Fashion%3A%20An%20automatically%20generated%20fashion%20dataset&body=Title%3A%20Prompt2Fashion%3A%20An%20automatically%20generated%20fashion%20dataset%0AAuthor%3A%20Georgia%20Argyro%20and%20Angeliki%20Dimitriou%20and%20Maria%20Lymperaiou%20and%20Giorgos%20Filandrianos%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20Despite%20the%20rapid%20evolution%20and%20increasing%20efficacy%20of%20language%20and%20vision%0Agenerative%20models%2C%20there%20remains%20a%20lack%20of%20comprehensive%20datasets%20that%20bridge%0Athe%20gap%20between%20personalized%20fashion%20needs%20and%20AI-driven%20design%2C%20limiting%20the%0Apotential%20for%20truly%20inclusive%20and%20customized%20fashion%20solutions.%20In%20this%20work%2C%0Awe%20leverage%20generative%20models%20to%20automatically%20construct%20a%20fashion%20image%0Adataset%20tailored%20to%20various%20occasions%2C%20styles%2C%20and%20body%20types%20as%20instructed%20by%0Ausers.%20We%20use%20different%20Large%20Language%20Models%20%28LLMs%29%20and%20prompting%20strategies%0Ato%20offer%20personalized%20outfits%20of%20high%20aesthetic%20quality%2C%20detail%2C%20and%20relevance%0Ato%20both%20expert%20and%20non-expert%20users%27%20requirements%2C%20as%20demonstrated%20by%0Aqualitative%20analysis.%20Up%20until%20now%20the%20evaluation%20of%20the%20generated%20outfits%20has%0Abeen%20conducted%20by%20non-expert%20human%20subjects.%20Despite%20the%20provided%20fine-grained%0Ainsights%20on%20the%20quality%20and%20relevance%20of%20generation%2C%20we%20extend%20the%20discussion%0Aon%20the%20importance%20of%20expert%20knowledge%20for%20the%20evaluation%20of%20artistic%0AAI-generated%20datasets%20such%20as%20this%20one.%20Our%20dataset%20is%20publicly%20available%20on%0AGitHub%20at%20https%3A//github.com/georgiarg/Prompt2Fashion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt2Fashion%253A%2520An%2520automatically%2520generated%2520fashion%2520dataset%26entry.906535625%3DGeorgia%2520Argyro%2520and%2520Angeliki%2520Dimitriou%2520and%2520Maria%2520Lymperaiou%2520and%2520Giorgos%2520Filandrianos%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520Despite%2520the%2520rapid%2520evolution%2520and%2520increasing%2520efficacy%2520of%2520language%2520and%2520vision%250Agenerative%2520models%252C%2520there%2520remains%2520a%2520lack%2520of%2520comprehensive%2520datasets%2520that%2520bridge%250Athe%2520gap%2520between%2520personalized%2520fashion%2520needs%2520and%2520AI-driven%2520design%252C%2520limiting%2520the%250Apotential%2520for%2520truly%2520inclusive%2520and%2520customized%2520fashion%2520solutions.%2520In%2520this%2520work%252C%250Awe%2520leverage%2520generative%2520models%2520to%2520automatically%2520construct%2520a%2520fashion%2520image%250Adataset%2520tailored%2520to%2520various%2520occasions%252C%2520styles%252C%2520and%2520body%2520types%2520as%2520instructed%2520by%250Ausers.%2520We%2520use%2520different%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520prompting%2520strategies%250Ato%2520offer%2520personalized%2520outfits%2520of%2520high%2520aesthetic%2520quality%252C%2520detail%252C%2520and%2520relevance%250Ato%2520both%2520expert%2520and%2520non-expert%2520users%2527%2520requirements%252C%2520as%2520demonstrated%2520by%250Aqualitative%2520analysis.%2520Up%2520until%2520now%2520the%2520evaluation%2520of%2520the%2520generated%2520outfits%2520has%250Abeen%2520conducted%2520by%2520non-expert%2520human%2520subjects.%2520Despite%2520the%2520provided%2520fine-grained%250Ainsights%2520on%2520the%2520quality%2520and%2520relevance%2520of%2520generation%252C%2520we%2520extend%2520the%2520discussion%250Aon%2520the%2520importance%2520of%2520expert%2520knowledge%2520for%2520the%2520evaluation%2520of%2520artistic%250AAI-generated%2520datasets%2520such%2520as%2520this%2520one.%2520Our%2520dataset%2520is%2520publicly%2520available%2520on%250AGitHub%2520at%2520https%253A//github.com/georgiarg/Prompt2Fashion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt2Fashion%3A%20An%20automatically%20generated%20fashion%20dataset&entry.906535625=Georgia%20Argyro%20and%20Angeliki%20Dimitriou%20and%20Maria%20Lymperaiou%20and%20Giorgos%20Filandrianos%20and%20Giorgos%20Stamou&entry.1292438233=%20%20Despite%20the%20rapid%20evolution%20and%20increasing%20efficacy%20of%20language%20and%20vision%0Agenerative%20models%2C%20there%20remains%20a%20lack%20of%20comprehensive%20datasets%20that%20bridge%0Athe%20gap%20between%20personalized%20fashion%20needs%20and%20AI-driven%20design%2C%20limiting%20the%0Apotential%20for%20truly%20inclusive%20and%20customized%20fashion%20solutions.%20In%20this%20work%2C%0Awe%20leverage%20generative%20models%20to%20automatically%20construct%20a%20fashion%20image%0Adataset%20tailored%20to%20various%20occasions%2C%20styles%2C%20and%20body%20types%20as%20instructed%20by%0Ausers.%20We%20use%20different%20Large%20Language%20Models%20%28LLMs%29%20and%20prompting%20strategies%0Ato%20offer%20personalized%20outfits%20of%20high%20aesthetic%20quality%2C%20detail%2C%20and%20relevance%0Ato%20both%20expert%20and%20non-expert%20users%27%20requirements%2C%20as%20demonstrated%20by%0Aqualitative%20analysis.%20Up%20until%20now%20the%20evaluation%20of%20the%20generated%20outfits%20has%0Abeen%20conducted%20by%20non-expert%20human%20subjects.%20Despite%20the%20provided%20fine-grained%0Ainsights%20on%20the%20quality%20and%20relevance%20of%20generation%2C%20we%20extend%20the%20discussion%0Aon%20the%20importance%20of%20expert%20knowledge%20for%20the%20evaluation%20of%20artistic%0AAI-generated%20datasets%20such%20as%20this%20one.%20Our%20dataset%20is%20publicly%20available%20on%0AGitHub%20at%20https%3A//github.com/georgiarg/Prompt2Fashion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06442v1&entry.124074799=Read"},
{"title": "In Flight Boresight Rectification for Lightweight Airborne Pushbroom\n  Imaging Spectrometry", "author": "Julien Yuuki Burkhard and Jesse Ray Murray Lahaye and Laurent Valentin Jospin and Jan Skaloud", "abstract": "  Hyperspectral cameras have recently been miniaturized for operation on\nlightweight airborne platforms such as UAV or small aircraft. Unlike frame\ncameras (RGB or Multispectral), many hyperspectral sensors use a linear array\nor 'push-broom' scanning design. This design presents significant challenges\nfor image rectification and the calibration of the intrinsic and extrinsic\ncamera parameters. Typically, methods employed to address such tasks rely on a\nprecise GPS/INS estimate of the airborne platform trajectory and a detailed\nterrain model. However, inaccuracies in the trajectory or surface model\ninformation can introduce systematic errors and complicate geometric modeling\nwhich ultimately degrade the quality of the rectification. To overcome these\nchallenges, we propose a method for tie point extraction and camera calibration\nfor 'push-broom' hyperspectral sensors using only the raw spectral imagery and\nraw, possibly low quality, GPS/INS trajectory. We demonstrate that our approach\nallows for the automatic calibration of airborne systems with hyperspectral\ncameras, outperforms other state-of-the-art automatic rectification methods and\nreaches an accuracy on par with manual calibration methods.\n", "link": "http://arxiv.org/abs/2409.06520v1", "date": "2024-09-10", "relevancy": 2.4649, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.513}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5084}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In%20Flight%20Boresight%20Rectification%20for%20Lightweight%20Airborne%20Pushbroom%0A%20%20Imaging%20Spectrometry&body=Title%3A%20In%20Flight%20Boresight%20Rectification%20for%20Lightweight%20Airborne%20Pushbroom%0A%20%20Imaging%20Spectrometry%0AAuthor%3A%20Julien%20Yuuki%20Burkhard%20and%20Jesse%20Ray%20Murray%20Lahaye%20and%20Laurent%20Valentin%20Jospin%20and%20Jan%20Skaloud%0AAbstract%3A%20%20%20Hyperspectral%20cameras%20have%20recently%20been%20miniaturized%20for%20operation%20on%0Alightweight%20airborne%20platforms%20such%20as%20UAV%20or%20small%20aircraft.%20Unlike%20frame%0Acameras%20%28RGB%20or%20Multispectral%29%2C%20many%20hyperspectral%20sensors%20use%20a%20linear%20array%0Aor%20%27push-broom%27%20scanning%20design.%20This%20design%20presents%20significant%20challenges%0Afor%20image%20rectification%20and%20the%20calibration%20of%20the%20intrinsic%20and%20extrinsic%0Acamera%20parameters.%20Typically%2C%20methods%20employed%20to%20address%20such%20tasks%20rely%20on%20a%0Aprecise%20GPS/INS%20estimate%20of%20the%20airborne%20platform%20trajectory%20and%20a%20detailed%0Aterrain%20model.%20However%2C%20inaccuracies%20in%20the%20trajectory%20or%20surface%20model%0Ainformation%20can%20introduce%20systematic%20errors%20and%20complicate%20geometric%20modeling%0Awhich%20ultimately%20degrade%20the%20quality%20of%20the%20rectification.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20a%20method%20for%20tie%20point%20extraction%20and%20camera%20calibration%0Afor%20%27push-broom%27%20hyperspectral%20sensors%20using%20only%20the%20raw%20spectral%20imagery%20and%0Araw%2C%20possibly%20low%20quality%2C%20GPS/INS%20trajectory.%20We%20demonstrate%20that%20our%20approach%0Aallows%20for%20the%20automatic%20calibration%20of%20airborne%20systems%20with%20hyperspectral%0Acameras%2C%20outperforms%20other%20state-of-the-art%20automatic%20rectification%20methods%20and%0Areaches%20an%20accuracy%20on%20par%20with%20manual%20calibration%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn%2520Flight%2520Boresight%2520Rectification%2520for%2520Lightweight%2520Airborne%2520Pushbroom%250A%2520%2520Imaging%2520Spectrometry%26entry.906535625%3DJulien%2520Yuuki%2520Burkhard%2520and%2520Jesse%2520Ray%2520Murray%2520Lahaye%2520and%2520Laurent%2520Valentin%2520Jospin%2520and%2520Jan%2520Skaloud%26entry.1292438233%3D%2520%2520Hyperspectral%2520cameras%2520have%2520recently%2520been%2520miniaturized%2520for%2520operation%2520on%250Alightweight%2520airborne%2520platforms%2520such%2520as%2520UAV%2520or%2520small%2520aircraft.%2520Unlike%2520frame%250Acameras%2520%2528RGB%2520or%2520Multispectral%2529%252C%2520many%2520hyperspectral%2520sensors%2520use%2520a%2520linear%2520array%250Aor%2520%2527push-broom%2527%2520scanning%2520design.%2520This%2520design%2520presents%2520significant%2520challenges%250Afor%2520image%2520rectification%2520and%2520the%2520calibration%2520of%2520the%2520intrinsic%2520and%2520extrinsic%250Acamera%2520parameters.%2520Typically%252C%2520methods%2520employed%2520to%2520address%2520such%2520tasks%2520rely%2520on%2520a%250Aprecise%2520GPS/INS%2520estimate%2520of%2520the%2520airborne%2520platform%2520trajectory%2520and%2520a%2520detailed%250Aterrain%2520model.%2520However%252C%2520inaccuracies%2520in%2520the%2520trajectory%2520or%2520surface%2520model%250Ainformation%2520can%2520introduce%2520systematic%2520errors%2520and%2520complicate%2520geometric%2520modeling%250Awhich%2520ultimately%2520degrade%2520the%2520quality%2520of%2520the%2520rectification.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520method%2520for%2520tie%2520point%2520extraction%2520and%2520camera%2520calibration%250Afor%2520%2527push-broom%2527%2520hyperspectral%2520sensors%2520using%2520only%2520the%2520raw%2520spectral%2520imagery%2520and%250Araw%252C%2520possibly%2520low%2520quality%252C%2520GPS/INS%2520trajectory.%2520We%2520demonstrate%2520that%2520our%2520approach%250Aallows%2520for%2520the%2520automatic%2520calibration%2520of%2520airborne%2520systems%2520with%2520hyperspectral%250Acameras%252C%2520outperforms%2520other%2520state-of-the-art%2520automatic%2520rectification%2520methods%2520and%250Areaches%2520an%2520accuracy%2520on%2520par%2520with%2520manual%2520calibration%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In%20Flight%20Boresight%20Rectification%20for%20Lightweight%20Airborne%20Pushbroom%0A%20%20Imaging%20Spectrometry&entry.906535625=Julien%20Yuuki%20Burkhard%20and%20Jesse%20Ray%20Murray%20Lahaye%20and%20Laurent%20Valentin%20Jospin%20and%20Jan%20Skaloud&entry.1292438233=%20%20Hyperspectral%20cameras%20have%20recently%20been%20miniaturized%20for%20operation%20on%0Alightweight%20airborne%20platforms%20such%20as%20UAV%20or%20small%20aircraft.%20Unlike%20frame%0Acameras%20%28RGB%20or%20Multispectral%29%2C%20many%20hyperspectral%20sensors%20use%20a%20linear%20array%0Aor%20%27push-broom%27%20scanning%20design.%20This%20design%20presents%20significant%20challenges%0Afor%20image%20rectification%20and%20the%20calibration%20of%20the%20intrinsic%20and%20extrinsic%0Acamera%20parameters.%20Typically%2C%20methods%20employed%20to%20address%20such%20tasks%20rely%20on%20a%0Aprecise%20GPS/INS%20estimate%20of%20the%20airborne%20platform%20trajectory%20and%20a%20detailed%0Aterrain%20model.%20However%2C%20inaccuracies%20in%20the%20trajectory%20or%20surface%20model%0Ainformation%20can%20introduce%20systematic%20errors%20and%20complicate%20geometric%20modeling%0Awhich%20ultimately%20degrade%20the%20quality%20of%20the%20rectification.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20a%20method%20for%20tie%20point%20extraction%20and%20camera%20calibration%0Afor%20%27push-broom%27%20hyperspectral%20sensors%20using%20only%20the%20raw%20spectral%20imagery%20and%0Araw%2C%20possibly%20low%20quality%2C%20GPS/INS%20trajectory.%20We%20demonstrate%20that%20our%20approach%0Aallows%20for%20the%20automatic%20calibration%20of%20airborne%20systems%20with%20hyperspectral%0Acameras%2C%20outperforms%20other%20state-of-the-art%20automatic%20rectification%20methods%20and%0Areaches%20an%20accuracy%20on%20par%20with%20manual%20calibration%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06520v1&entry.124074799=Read"},
{"title": "EMCNet : Graph-Nets for Electron Micrographs Classification", "author": "Sakhinana Sagar Srinivas and Rajat Kumar Sarkar and Venkataramana Runkana", "abstract": "  Characterization of materials via electron micrographs is an important and\nchallenging task in several materials processing industries. Classification of\nelectron micrographs is complex due to the high intra-class dissimilarity, high\ninter-class similarity, and multi-spatial scales of patterns. However, existing\nmethods are ineffective in learning complex image patterns. We propose an\neffective end-to-end electron micrograph representation learning-based\nframework for nanomaterial identification to overcome the challenges. We\ndemonstrate that our framework outperforms the popular baselines on the\nopen-source datasets in nanomaterials-based identification tasks. The ablation\nstudies are reported in great detail to support the efficacy of our approach.\n", "link": "http://arxiv.org/abs/2409.03767v2", "date": "2024-09-10", "relevancy": 2.4561, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5101}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4839}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMCNet%20%3A%20Graph-Nets%20for%20Electron%20Micrographs%20Classification&body=Title%3A%20EMCNet%20%3A%20Graph-Nets%20for%20Electron%20Micrographs%20Classification%0AAuthor%3A%20Sakhinana%20Sagar%20Srinivas%20and%20Rajat%20Kumar%20Sarkar%20and%20Venkataramana%20Runkana%0AAbstract%3A%20%20%20Characterization%20of%20materials%20via%20electron%20micrographs%20is%20an%20important%20and%0Achallenging%20task%20in%20several%20materials%20processing%20industries.%20Classification%20of%0Aelectron%20micrographs%20is%20complex%20due%20to%20the%20high%20intra-class%20dissimilarity%2C%20high%0Ainter-class%20similarity%2C%20and%20multi-spatial%20scales%20of%20patterns.%20However%2C%20existing%0Amethods%20are%20ineffective%20in%20learning%20complex%20image%20patterns.%20We%20propose%20an%0Aeffective%20end-to-end%20electron%20micrograph%20representation%20learning-based%0Aframework%20for%20nanomaterial%20identification%20to%20overcome%20the%20challenges.%20We%0Ademonstrate%20that%20our%20framework%20outperforms%20the%20popular%20baselines%20on%20the%0Aopen-source%20datasets%20in%20nanomaterials-based%20identification%20tasks.%20The%20ablation%0Astudies%20are%20reported%20in%20great%20detail%20to%20support%20the%20efficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMCNet%2520%253A%2520Graph-Nets%2520for%2520Electron%2520Micrographs%2520Classification%26entry.906535625%3DSakhinana%2520Sagar%2520Srinivas%2520and%2520Rajat%2520Kumar%2520Sarkar%2520and%2520Venkataramana%2520Runkana%26entry.1292438233%3D%2520%2520Characterization%2520of%2520materials%2520via%2520electron%2520micrographs%2520is%2520an%2520important%2520and%250Achallenging%2520task%2520in%2520several%2520materials%2520processing%2520industries.%2520Classification%2520of%250Aelectron%2520micrographs%2520is%2520complex%2520due%2520to%2520the%2520high%2520intra-class%2520dissimilarity%252C%2520high%250Ainter-class%2520similarity%252C%2520and%2520multi-spatial%2520scales%2520of%2520patterns.%2520However%252C%2520existing%250Amethods%2520are%2520ineffective%2520in%2520learning%2520complex%2520image%2520patterns.%2520We%2520propose%2520an%250Aeffective%2520end-to-end%2520electron%2520micrograph%2520representation%2520learning-based%250Aframework%2520for%2520nanomaterial%2520identification%2520to%2520overcome%2520the%2520challenges.%2520We%250Ademonstrate%2520that%2520our%2520framework%2520outperforms%2520the%2520popular%2520baselines%2520on%2520the%250Aopen-source%2520datasets%2520in%2520nanomaterials-based%2520identification%2520tasks.%2520The%2520ablation%250Astudies%2520are%2520reported%2520in%2520great%2520detail%2520to%2520support%2520the%2520efficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMCNet%20%3A%20Graph-Nets%20for%20Electron%20Micrographs%20Classification&entry.906535625=Sakhinana%20Sagar%20Srinivas%20and%20Rajat%20Kumar%20Sarkar%20and%20Venkataramana%20Runkana&entry.1292438233=%20%20Characterization%20of%20materials%20via%20electron%20micrographs%20is%20an%20important%20and%0Achallenging%20task%20in%20several%20materials%20processing%20industries.%20Classification%20of%0Aelectron%20micrographs%20is%20complex%20due%20to%20the%20high%20intra-class%20dissimilarity%2C%20high%0Ainter-class%20similarity%2C%20and%20multi-spatial%20scales%20of%20patterns.%20However%2C%20existing%0Amethods%20are%20ineffective%20in%20learning%20complex%20image%20patterns.%20We%20propose%20an%0Aeffective%20end-to-end%20electron%20micrograph%20representation%20learning-based%0Aframework%20for%20nanomaterial%20identification%20to%20overcome%20the%20challenges.%20We%0Ademonstrate%20that%20our%20framework%20outperforms%20the%20popular%20baselines%20on%20the%0Aopen-source%20datasets%20in%20nanomaterials-based%20identification%20tasks.%20The%20ablation%0Astudies%20are%20reported%20in%20great%20detail%20to%20support%20the%20efficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03767v2&entry.124074799=Read"},
{"title": "Learn2Aggregate: Supervised Generation of Chv\u00e1tal-Gomory Cuts Using\n  Graph Neural Networks", "author": "Arnaud Deza and Elias B. Khalil and Zhenan Fan and Zirui Zhou and Yong Zhang", "abstract": "  We present $\\textit{Learn2Aggregate}$, a machine learning (ML) framework for\noptimizing the generation of Chv\\'atal-Gomory (CG) cuts in mixed integer linear\nprogramming (MILP). The framework trains a graph neural network to classify\nuseful constraints for aggregation in CG cut generation. The ML-driven CG\nseparator selectively focuses on a small set of impactful constraints,\nimproving runtimes without compromising the strength of the generated cuts. Key\nto our approach is the formulation of a constraint classification task which\nfavours sparse aggregation of constraints, consistent with empirical findings.\nThis, in conjunction with a careful constraint labeling scheme and a hybrid of\ndeep learning and feature engineering, results in enhanced CG cut generation\nacross five diverse MILP benchmarks. On the largest test sets, our method\ncloses roughly $\\textit{twice}$ as much of the integrality gap as the standard\nCG method while running 40$% faster. This performance improvement is due to our\nmethod eliminating 75% of the constraints prior to aggregation.\n", "link": "http://arxiv.org/abs/2409.06559v1", "date": "2024-09-10", "relevancy": 2.4542, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5157}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4986}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn2Aggregate%3A%20Supervised%20Generation%20of%20Chv%C3%A1tal-Gomory%20Cuts%20Using%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20Learn2Aggregate%3A%20Supervised%20Generation%20of%20Chv%C3%A1tal-Gomory%20Cuts%20Using%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20Arnaud%20Deza%20and%20Elias%20B.%20Khalil%20and%20Zhenan%20Fan%20and%20Zirui%20Zhou%20and%20Yong%20Zhang%0AAbstract%3A%20%20%20We%20present%20%24%5Ctextit%7BLearn2Aggregate%7D%24%2C%20a%20machine%20learning%20%28ML%29%20framework%20for%0Aoptimizing%20the%20generation%20of%20Chv%5C%27atal-Gomory%20%28CG%29%20cuts%20in%20mixed%20integer%20linear%0Aprogramming%20%28MILP%29.%20The%20framework%20trains%20a%20graph%20neural%20network%20to%20classify%0Auseful%20constraints%20for%20aggregation%20in%20CG%20cut%20generation.%20The%20ML-driven%20CG%0Aseparator%20selectively%20focuses%20on%20a%20small%20set%20of%20impactful%20constraints%2C%0Aimproving%20runtimes%20without%20compromising%20the%20strength%20of%20the%20generated%20cuts.%20Key%0Ato%20our%20approach%20is%20the%20formulation%20of%20a%20constraint%20classification%20task%20which%0Afavours%20sparse%20aggregation%20of%20constraints%2C%20consistent%20with%20empirical%20findings.%0AThis%2C%20in%20conjunction%20with%20a%20careful%20constraint%20labeling%20scheme%20and%20a%20hybrid%20of%0Adeep%20learning%20and%20feature%20engineering%2C%20results%20in%20enhanced%20CG%20cut%20generation%0Aacross%20five%20diverse%20MILP%20benchmarks.%20On%20the%20largest%20test%20sets%2C%20our%20method%0Acloses%20roughly%20%24%5Ctextit%7Btwice%7D%24%20as%20much%20of%20the%20integrality%20gap%20as%20the%20standard%0ACG%20method%20while%20running%2040%24%25%20faster.%20This%20performance%20improvement%20is%20due%20to%20our%0Amethod%20eliminating%2075%25%20of%20the%20constraints%20prior%20to%20aggregation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn2Aggregate%253A%2520Supervised%2520Generation%2520of%2520Chv%25C3%25A1tal-Gomory%2520Cuts%2520Using%250A%2520%2520Graph%2520Neural%2520Networks%26entry.906535625%3DArnaud%2520Deza%2520and%2520Elias%2520B.%2520Khalil%2520and%2520Zhenan%2520Fan%2520and%2520Zirui%2520Zhou%2520and%2520Yong%2520Zhang%26entry.1292438233%3D%2520%2520We%2520present%2520%2524%255Ctextit%257BLearn2Aggregate%257D%2524%252C%2520a%2520machine%2520learning%2520%2528ML%2529%2520framework%2520for%250Aoptimizing%2520the%2520generation%2520of%2520Chv%255C%2527atal-Gomory%2520%2528CG%2529%2520cuts%2520in%2520mixed%2520integer%2520linear%250Aprogramming%2520%2528MILP%2529.%2520The%2520framework%2520trains%2520a%2520graph%2520neural%2520network%2520to%2520classify%250Auseful%2520constraints%2520for%2520aggregation%2520in%2520CG%2520cut%2520generation.%2520The%2520ML-driven%2520CG%250Aseparator%2520selectively%2520focuses%2520on%2520a%2520small%2520set%2520of%2520impactful%2520constraints%252C%250Aimproving%2520runtimes%2520without%2520compromising%2520the%2520strength%2520of%2520the%2520generated%2520cuts.%2520Key%250Ato%2520our%2520approach%2520is%2520the%2520formulation%2520of%2520a%2520constraint%2520classification%2520task%2520which%250Afavours%2520sparse%2520aggregation%2520of%2520constraints%252C%2520consistent%2520with%2520empirical%2520findings.%250AThis%252C%2520in%2520conjunction%2520with%2520a%2520careful%2520constraint%2520labeling%2520scheme%2520and%2520a%2520hybrid%2520of%250Adeep%2520learning%2520and%2520feature%2520engineering%252C%2520results%2520in%2520enhanced%2520CG%2520cut%2520generation%250Aacross%2520five%2520diverse%2520MILP%2520benchmarks.%2520On%2520the%2520largest%2520test%2520sets%252C%2520our%2520method%250Acloses%2520roughly%2520%2524%255Ctextit%257Btwice%257D%2524%2520as%2520much%2520of%2520the%2520integrality%2520gap%2520as%2520the%2520standard%250ACG%2520method%2520while%2520running%252040%2524%2525%2520faster.%2520This%2520performance%2520improvement%2520is%2520due%2520to%2520our%250Amethod%2520eliminating%252075%2525%2520of%2520the%2520constraints%2520prior%2520to%2520aggregation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn2Aggregate%3A%20Supervised%20Generation%20of%20Chv%C3%A1tal-Gomory%20Cuts%20Using%0A%20%20Graph%20Neural%20Networks&entry.906535625=Arnaud%20Deza%20and%20Elias%20B.%20Khalil%20and%20Zhenan%20Fan%20and%20Zirui%20Zhou%20and%20Yong%20Zhang&entry.1292438233=%20%20We%20present%20%24%5Ctextit%7BLearn2Aggregate%7D%24%2C%20a%20machine%20learning%20%28ML%29%20framework%20for%0Aoptimizing%20the%20generation%20of%20Chv%5C%27atal-Gomory%20%28CG%29%20cuts%20in%20mixed%20integer%20linear%0Aprogramming%20%28MILP%29.%20The%20framework%20trains%20a%20graph%20neural%20network%20to%20classify%0Auseful%20constraints%20for%20aggregation%20in%20CG%20cut%20generation.%20The%20ML-driven%20CG%0Aseparator%20selectively%20focuses%20on%20a%20small%20set%20of%20impactful%20constraints%2C%0Aimproving%20runtimes%20without%20compromising%20the%20strength%20of%20the%20generated%20cuts.%20Key%0Ato%20our%20approach%20is%20the%20formulation%20of%20a%20constraint%20classification%20task%20which%0Afavours%20sparse%20aggregation%20of%20constraints%2C%20consistent%20with%20empirical%20findings.%0AThis%2C%20in%20conjunction%20with%20a%20careful%20constraint%20labeling%20scheme%20and%20a%20hybrid%20of%0Adeep%20learning%20and%20feature%20engineering%2C%20results%20in%20enhanced%20CG%20cut%20generation%0Aacross%20five%20diverse%20MILP%20benchmarks.%20On%20the%20largest%20test%20sets%2C%20our%20method%0Acloses%20roughly%20%24%5Ctextit%7Btwice%7D%24%20as%20much%20of%20the%20integrality%20gap%20as%20the%20standard%0ACG%20method%20while%20running%2040%24%25%20faster.%20This%20performance%20improvement%20is%20due%20to%20our%0Amethod%20eliminating%2075%25%20of%20the%20constraints%20prior%20to%20aggregation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06559v1&entry.124074799=Read"},
{"title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio", "author": "Ningyuan Xi and Yetao Wu and Kun Fan and Teng Chen and Qingqing Gu and Peng Yu and Jinxian Qu and Chenxi Liu and Zhonglin Jiang and Yong Chen and Luo Ji", "abstract": "  Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.\n", "link": "http://arxiv.org/abs/2409.06624v1", "date": "2024-09-10", "relevancy": 2.4163, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Practice%20of%20Post-Training%20on%20Llama-3%2070B%20with%20Optimal%20Selection%20of%0A%20%20Additional%20Language%20Mixture%20Ratio&body=Title%3A%20A%20Practice%20of%20Post-Training%20on%20Llama-3%2070B%20with%20Optimal%20Selection%20of%0A%20%20Additional%20Language%20Mixture%20Ratio%0AAuthor%3A%20Ningyuan%20Xi%20and%20Yetao%20Wu%20and%20Kun%20Fan%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Peng%20Yu%20and%20Jinxian%20Qu%20and%20Chenxi%20Liu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLM%29%20often%20needs%20to%20be%20Continual%20Pre-Trained%20%28CPT%29%20to%0Aobtain%20the%20unfamiliar%20language%20skill%20or%20adapt%20into%20new%20domains.%20The%20huge%0Atraining%20cost%20of%20CPT%20often%20asks%20for%20cautious%20choice%20of%20key%20hyper-parameters%0Asuch%20as%20the%20mixture%20ratio%20of%20extra%20language%20or%20domain%20corpus.%20However%2C%20there%20is%0Ano%20systematic%20study%20which%20bridge%20the%20gap%20between%20the%20optimal%20mixture%20ratio%20and%0Athe%20actual%20model%20performance%2C%20and%20the%20gap%20between%20experimental%20scaling%20law%20and%0Athe%20actual%20deployment%20in%20the%20full%20model%20size.%20In%20this%20paper%2C%20we%20perform%20CPT%20on%0ALlama-3%208B%20and%2070B%20to%20enhance%20its%20Chinese%20ability.%20We%20study%20the%20optimal%0Acorrelation%20between%20the%20Additional%20Language%20Mixture%20Ratio%20%28ALMR%29%20and%20the%0ALearning%20Rate%20%28LR%29%20on%20the%208B%20size%20which%20directly%20indicate%20the%20optimal%0Aexperimental%20set%20up.%20By%20thorough%20choice%20of%20hyper-parameter%2C%20and%20subsequent%0Afine-tuning%2C%20the%20model%20capability%20is%20improved%20not%20only%20on%20the%20Chinese-related%0Abenchmark%2C%20but%20also%20some%20specific%20domains%20including%20math%2C%20coding%20and%20emotional%0Aintelligence.%20We%20deploy%20the%20final%2070B%20version%20of%20LLM%20on%20an%20real-life%20chat%0Asystem%20which%20obtain%20satisfying%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Practice%2520of%2520Post-Training%2520on%2520Llama-3%252070B%2520with%2520Optimal%2520Selection%2520of%250A%2520%2520Additional%2520Language%2520Mixture%2520Ratio%26entry.906535625%3DNingyuan%2520Xi%2520and%2520Yetao%2520Wu%2520and%2520Kun%2520Fan%2520and%2520Teng%2520Chen%2520and%2520Qingqing%2520Gu%2520and%2520Peng%2520Yu%2520and%2520Jinxian%2520Qu%2520and%2520Chenxi%2520Liu%2520and%2520Zhonglin%2520Jiang%2520and%2520Yong%2520Chen%2520and%2520Luo%2520Ji%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520often%2520needs%2520to%2520be%2520Continual%2520Pre-Trained%2520%2528CPT%2529%2520to%250Aobtain%2520the%2520unfamiliar%2520language%2520skill%2520or%2520adapt%2520into%2520new%2520domains.%2520The%2520huge%250Atraining%2520cost%2520of%2520CPT%2520often%2520asks%2520for%2520cautious%2520choice%2520of%2520key%2520hyper-parameters%250Asuch%2520as%2520the%2520mixture%2520ratio%2520of%2520extra%2520language%2520or%2520domain%2520corpus.%2520However%252C%2520there%2520is%250Ano%2520systematic%2520study%2520which%2520bridge%2520the%2520gap%2520between%2520the%2520optimal%2520mixture%2520ratio%2520and%250Athe%2520actual%2520model%2520performance%252C%2520and%2520the%2520gap%2520between%2520experimental%2520scaling%2520law%2520and%250Athe%2520actual%2520deployment%2520in%2520the%2520full%2520model%2520size.%2520In%2520this%2520paper%252C%2520we%2520perform%2520CPT%2520on%250ALlama-3%25208B%2520and%252070B%2520to%2520enhance%2520its%2520Chinese%2520ability.%2520We%2520study%2520the%2520optimal%250Acorrelation%2520between%2520the%2520Additional%2520Language%2520Mixture%2520Ratio%2520%2528ALMR%2529%2520and%2520the%250ALearning%2520Rate%2520%2528LR%2529%2520on%2520the%25208B%2520size%2520which%2520directly%2520indicate%2520the%2520optimal%250Aexperimental%2520set%2520up.%2520By%2520thorough%2520choice%2520of%2520hyper-parameter%252C%2520and%2520subsequent%250Afine-tuning%252C%2520the%2520model%2520capability%2520is%2520improved%2520not%2520only%2520on%2520the%2520Chinese-related%250Abenchmark%252C%2520but%2520also%2520some%2520specific%2520domains%2520including%2520math%252C%2520coding%2520and%2520emotional%250Aintelligence.%2520We%2520deploy%2520the%2520final%252070B%2520version%2520of%2520LLM%2520on%2520an%2520real-life%2520chat%250Asystem%2520which%2520obtain%2520satisfying%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Practice%20of%20Post-Training%20on%20Llama-3%2070B%20with%20Optimal%20Selection%20of%0A%20%20Additional%20Language%20Mixture%20Ratio&entry.906535625=Ningyuan%20Xi%20and%20Yetao%20Wu%20and%20Kun%20Fan%20and%20Teng%20Chen%20and%20Qingqing%20Gu%20and%20Peng%20Yu%20and%20Jinxian%20Qu%20and%20Chenxi%20Liu%20and%20Zhonglin%20Jiang%20and%20Yong%20Chen%20and%20Luo%20Ji&entry.1292438233=%20%20Large%20Language%20Models%20%28LLM%29%20often%20needs%20to%20be%20Continual%20Pre-Trained%20%28CPT%29%20to%0Aobtain%20the%20unfamiliar%20language%20skill%20or%20adapt%20into%20new%20domains.%20The%20huge%0Atraining%20cost%20of%20CPT%20often%20asks%20for%20cautious%20choice%20of%20key%20hyper-parameters%0Asuch%20as%20the%20mixture%20ratio%20of%20extra%20language%20or%20domain%20corpus.%20However%2C%20there%20is%0Ano%20systematic%20study%20which%20bridge%20the%20gap%20between%20the%20optimal%20mixture%20ratio%20and%0Athe%20actual%20model%20performance%2C%20and%20the%20gap%20between%20experimental%20scaling%20law%20and%0Athe%20actual%20deployment%20in%20the%20full%20model%20size.%20In%20this%20paper%2C%20we%20perform%20CPT%20on%0ALlama-3%208B%20and%2070B%20to%20enhance%20its%20Chinese%20ability.%20We%20study%20the%20optimal%0Acorrelation%20between%20the%20Additional%20Language%20Mixture%20Ratio%20%28ALMR%29%20and%20the%0ALearning%20Rate%20%28LR%29%20on%20the%208B%20size%20which%20directly%20indicate%20the%20optimal%0Aexperimental%20set%20up.%20By%20thorough%20choice%20of%20hyper-parameter%2C%20and%20subsequent%0Afine-tuning%2C%20the%20model%20capability%20is%20improved%20not%20only%20on%20the%20Chinese-related%0Abenchmark%2C%20but%20also%20some%20specific%20domains%20including%20math%2C%20coding%20and%20emotional%0Aintelligence.%20We%20deploy%20the%20final%2070B%20version%20of%20LLM%20on%20an%20real-life%20chat%0Asystem%20which%20obtain%20satisfying%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06624v1&entry.124074799=Read"},
{"title": "Robust Single Rotation Averaging Revisited", "author": "Seong Hun Lee and Javier Civera", "abstract": "  In this work, we propose a novel method for robust single rotation averaging\nthat can efficiently handle an extremely large fraction of outliers. Our\napproach is to minimize the total truncated least unsquared deviations (TLUD)\ncost of geodesic distances. The proposed algorithm consists of three steps:\nFirst, we consider each input rotation as a potential initial solution and\nchoose the one that yields the least sum of truncated chordal deviations. Next,\nwe obtain the inlier set using the initial solution and compute its chordal\n$L_2$-mean. Finally, starting from this estimate, we iteratively compute the\ngeodesic $L_1$-mean of the inliers using the Weiszfeld algorithm on $SO(3)$. An\nextensive evaluation shows that our method is robust against up to 99% outliers\ngiven a sufficient number of accurate inliers, outperforming the current state\nof the art.\n", "link": "http://arxiv.org/abs/2309.05388v5", "date": "2024-09-10", "relevancy": 2.4012, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5018}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4838}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Single%20Rotation%20Averaging%20Revisited&body=Title%3A%20Robust%20Single%20Rotation%20Averaging%20Revisited%0AAuthor%3A%20Seong%20Hun%20Lee%20and%20Javier%20Civera%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20for%20robust%20single%20rotation%20averaging%0Athat%20can%20efficiently%20handle%20an%20extremely%20large%20fraction%20of%20outliers.%20Our%0Aapproach%20is%20to%20minimize%20the%20total%20truncated%20least%20unsquared%20deviations%20%28TLUD%29%0Acost%20of%20geodesic%20distances.%20The%20proposed%20algorithm%20consists%20of%20three%20steps%3A%0AFirst%2C%20we%20consider%20each%20input%20rotation%20as%20a%20potential%20initial%20solution%20and%0Achoose%20the%20one%20that%20yields%20the%20least%20sum%20of%20truncated%20chordal%20deviations.%20Next%2C%0Awe%20obtain%20the%20inlier%20set%20using%20the%20initial%20solution%20and%20compute%20its%20chordal%0A%24L_2%24-mean.%20Finally%2C%20starting%20from%20this%20estimate%2C%20we%20iteratively%20compute%20the%0Ageodesic%20%24L_1%24-mean%20of%20the%20inliers%20using%20the%20Weiszfeld%20algorithm%20on%20%24SO%283%29%24.%20An%0Aextensive%20evaluation%20shows%20that%20our%20method%20is%20robust%20against%20up%20to%2099%25%20outliers%0Agiven%20a%20sufficient%20number%20of%20accurate%20inliers%2C%20outperforming%20the%20current%20state%0Aof%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05388v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Single%2520Rotation%2520Averaging%2520Revisited%26entry.906535625%3DSeong%2520Hun%2520Lee%2520and%2520Javier%2520Civera%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520robust%2520single%2520rotation%2520averaging%250Athat%2520can%2520efficiently%2520handle%2520an%2520extremely%2520large%2520fraction%2520of%2520outliers.%2520Our%250Aapproach%2520is%2520to%2520minimize%2520the%2520total%2520truncated%2520least%2520unsquared%2520deviations%2520%2528TLUD%2529%250Acost%2520of%2520geodesic%2520distances.%2520The%2520proposed%2520algorithm%2520consists%2520of%2520three%2520steps%253A%250AFirst%252C%2520we%2520consider%2520each%2520input%2520rotation%2520as%2520a%2520potential%2520initial%2520solution%2520and%250Achoose%2520the%2520one%2520that%2520yields%2520the%2520least%2520sum%2520of%2520truncated%2520chordal%2520deviations.%2520Next%252C%250Awe%2520obtain%2520the%2520inlier%2520set%2520using%2520the%2520initial%2520solution%2520and%2520compute%2520its%2520chordal%250A%2524L_2%2524-mean.%2520Finally%252C%2520starting%2520from%2520this%2520estimate%252C%2520we%2520iteratively%2520compute%2520the%250Ageodesic%2520%2524L_1%2524-mean%2520of%2520the%2520inliers%2520using%2520the%2520Weiszfeld%2520algorithm%2520on%2520%2524SO%25283%2529%2524.%2520An%250Aextensive%2520evaluation%2520shows%2520that%2520our%2520method%2520is%2520robust%2520against%2520up%2520to%252099%2525%2520outliers%250Agiven%2520a%2520sufficient%2520number%2520of%2520accurate%2520inliers%252C%2520outperforming%2520the%2520current%2520state%250Aof%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.05388v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Single%20Rotation%20Averaging%20Revisited&entry.906535625=Seong%20Hun%20Lee%20and%20Javier%20Civera&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20for%20robust%20single%20rotation%20averaging%0Athat%20can%20efficiently%20handle%20an%20extremely%20large%20fraction%20of%20outliers.%20Our%0Aapproach%20is%20to%20minimize%20the%20total%20truncated%20least%20unsquared%20deviations%20%28TLUD%29%0Acost%20of%20geodesic%20distances.%20The%20proposed%20algorithm%20consists%20of%20three%20steps%3A%0AFirst%2C%20we%20consider%20each%20input%20rotation%20as%20a%20potential%20initial%20solution%20and%0Achoose%20the%20one%20that%20yields%20the%20least%20sum%20of%20truncated%20chordal%20deviations.%20Next%2C%0Awe%20obtain%20the%20inlier%20set%20using%20the%20initial%20solution%20and%20compute%20its%20chordal%0A%24L_2%24-mean.%20Finally%2C%20starting%20from%20this%20estimate%2C%20we%20iteratively%20compute%20the%0Ageodesic%20%24L_1%24-mean%20of%20the%20inliers%20using%20the%20Weiszfeld%20algorithm%20on%20%24SO%283%29%24.%20An%0Aextensive%20evaluation%20shows%20that%20our%20method%20is%20robust%20against%20up%20to%2099%25%20outliers%0Agiven%20a%20sufficient%20number%20of%20accurate%20inliers%2C%20outperforming%20the%20current%20state%0Aof%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05388v5&entry.124074799=Read"},
{"title": "A comprehensive study on Blood Cancer detection and classification using\n  Convolutional Neural Network", "author": "Md Taimur Ahad and Sajib Bin Mamun and Sumaya Mustofa and Bo Song and Yan Li", "abstract": "  Over the years in object detection several efficient Convolutional Neural\nNetworks (CNN) networks, such as DenseNet201, InceptionV3, ResNet152v2,\nSEresNet152, VGG19, Xception gained significant attention due to their\nperformance. Moreover, CNN paradigms have expanded to transfer learning and\nensemble models from original CNN architectures. Research studies suggest that\ntransfer learning and ensemble models are capable of increasing the accuracy of\ndeep learning (DL) models. However, very few studies have conducted\ncomprehensive experiments utilizing these techniques in detecting and\nlocalizing blood malignancies. Realizing the gap, this study conducted three\nexperiments; in the first experiment -- six original CNNs were used, in the\nsecond experiment -- transfer learning and, in the third experiment a novel\nensemble model DIX (DenseNet201, InceptionV3, and Xception) was developed to\ndetect and classify blood cancer. The statistical result suggests that DIX\noutperformed the original and transfer learning performance, providing an\naccuracy of 99.12%. However, this study also provides a negative result in the\ncase of transfer learning, as the transfer learning did not increase the\naccuracy of the original CNNs. Like many other cancers, blood cancer diseases\nrequire timely identification for effective treatment plans and increased\nsurvival possibilities. The high accuracy in detecting and categorization blood\ncancer detection using CNN suggests that the CNN model is promising in blood\ncancer disease detection. This research is significant in the fields of\nbiomedical engineering, computer-aided disease diagnosis, and ML-based disease\ndetection.\n", "link": "http://arxiv.org/abs/2409.06689v1", "date": "2024-09-10", "relevancy": 2.389, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5091}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4758}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20comprehensive%20study%20on%20Blood%20Cancer%20detection%20and%20classification%20using%0A%20%20Convolutional%20Neural%20Network&body=Title%3A%20A%20comprehensive%20study%20on%20Blood%20Cancer%20detection%20and%20classification%20using%0A%20%20Convolutional%20Neural%20Network%0AAuthor%3A%20Md%20Taimur%20Ahad%20and%20Sajib%20Bin%20Mamun%20and%20Sumaya%20Mustofa%20and%20Bo%20Song%20and%20Yan%20Li%0AAbstract%3A%20%20%20Over%20the%20years%20in%20object%20detection%20several%20efficient%20Convolutional%20Neural%0ANetworks%20%28CNN%29%20networks%2C%20such%20as%20DenseNet201%2C%20InceptionV3%2C%20ResNet152v2%2C%0ASEresNet152%2C%20VGG19%2C%20Xception%20gained%20significant%20attention%20due%20to%20their%0Aperformance.%20Moreover%2C%20CNN%20paradigms%20have%20expanded%20to%20transfer%20learning%20and%0Aensemble%20models%20from%20original%20CNN%20architectures.%20Research%20studies%20suggest%20that%0Atransfer%20learning%20and%20ensemble%20models%20are%20capable%20of%20increasing%20the%20accuracy%20of%0Adeep%20learning%20%28DL%29%20models.%20However%2C%20very%20few%20studies%20have%20conducted%0Acomprehensive%20experiments%20utilizing%20these%20techniques%20in%20detecting%20and%0Alocalizing%20blood%20malignancies.%20Realizing%20the%20gap%2C%20this%20study%20conducted%20three%0Aexperiments%3B%20in%20the%20first%20experiment%20--%20six%20original%20CNNs%20were%20used%2C%20in%20the%0Asecond%20experiment%20--%20transfer%20learning%20and%2C%20in%20the%20third%20experiment%20a%20novel%0Aensemble%20model%20DIX%20%28DenseNet201%2C%20InceptionV3%2C%20and%20Xception%29%20was%20developed%20to%0Adetect%20and%20classify%20blood%20cancer.%20The%20statistical%20result%20suggests%20that%20DIX%0Aoutperformed%20the%20original%20and%20transfer%20learning%20performance%2C%20providing%20an%0Aaccuracy%20of%2099.12%25.%20However%2C%20this%20study%20also%20provides%20a%20negative%20result%20in%20the%0Acase%20of%20transfer%20learning%2C%20as%20the%20transfer%20learning%20did%20not%20increase%20the%0Aaccuracy%20of%20the%20original%20CNNs.%20Like%20many%20other%20cancers%2C%20blood%20cancer%20diseases%0Arequire%20timely%20identification%20for%20effective%20treatment%20plans%20and%20increased%0Asurvival%20possibilities.%20The%20high%20accuracy%20in%20detecting%20and%20categorization%20blood%0Acancer%20detection%20using%20CNN%20suggests%20that%20the%20CNN%20model%20is%20promising%20in%20blood%0Acancer%20disease%20detection.%20This%20research%20is%20significant%20in%20the%20fields%20of%0Abiomedical%20engineering%2C%20computer-aided%20disease%20diagnosis%2C%20and%20ML-based%20disease%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520comprehensive%2520study%2520on%2520Blood%2520Cancer%2520detection%2520and%2520classification%2520using%250A%2520%2520Convolutional%2520Neural%2520Network%26entry.906535625%3DMd%2520Taimur%2520Ahad%2520and%2520Sajib%2520Bin%2520Mamun%2520and%2520Sumaya%2520Mustofa%2520and%2520Bo%2520Song%2520and%2520Yan%2520Li%26entry.1292438233%3D%2520%2520Over%2520the%2520years%2520in%2520object%2520detection%2520several%2520efficient%2520Convolutional%2520Neural%250ANetworks%2520%2528CNN%2529%2520networks%252C%2520such%2520as%2520DenseNet201%252C%2520InceptionV3%252C%2520ResNet152v2%252C%250ASEresNet152%252C%2520VGG19%252C%2520Xception%2520gained%2520significant%2520attention%2520due%2520to%2520their%250Aperformance.%2520Moreover%252C%2520CNN%2520paradigms%2520have%2520expanded%2520to%2520transfer%2520learning%2520and%250Aensemble%2520models%2520from%2520original%2520CNN%2520architectures.%2520Research%2520studies%2520suggest%2520that%250Atransfer%2520learning%2520and%2520ensemble%2520models%2520are%2520capable%2520of%2520increasing%2520the%2520accuracy%2520of%250Adeep%2520learning%2520%2528DL%2529%2520models.%2520However%252C%2520very%2520few%2520studies%2520have%2520conducted%250Acomprehensive%2520experiments%2520utilizing%2520these%2520techniques%2520in%2520detecting%2520and%250Alocalizing%2520blood%2520malignancies.%2520Realizing%2520the%2520gap%252C%2520this%2520study%2520conducted%2520three%250Aexperiments%253B%2520in%2520the%2520first%2520experiment%2520--%2520six%2520original%2520CNNs%2520were%2520used%252C%2520in%2520the%250Asecond%2520experiment%2520--%2520transfer%2520learning%2520and%252C%2520in%2520the%2520third%2520experiment%2520a%2520novel%250Aensemble%2520model%2520DIX%2520%2528DenseNet201%252C%2520InceptionV3%252C%2520and%2520Xception%2529%2520was%2520developed%2520to%250Adetect%2520and%2520classify%2520blood%2520cancer.%2520The%2520statistical%2520result%2520suggests%2520that%2520DIX%250Aoutperformed%2520the%2520original%2520and%2520transfer%2520learning%2520performance%252C%2520providing%2520an%250Aaccuracy%2520of%252099.12%2525.%2520However%252C%2520this%2520study%2520also%2520provides%2520a%2520negative%2520result%2520in%2520the%250Acase%2520of%2520transfer%2520learning%252C%2520as%2520the%2520transfer%2520learning%2520did%2520not%2520increase%2520the%250Aaccuracy%2520of%2520the%2520original%2520CNNs.%2520Like%2520many%2520other%2520cancers%252C%2520blood%2520cancer%2520diseases%250Arequire%2520timely%2520identification%2520for%2520effective%2520treatment%2520plans%2520and%2520increased%250Asurvival%2520possibilities.%2520The%2520high%2520accuracy%2520in%2520detecting%2520and%2520categorization%2520blood%250Acancer%2520detection%2520using%2520CNN%2520suggests%2520that%2520the%2520CNN%2520model%2520is%2520promising%2520in%2520blood%250Acancer%2520disease%2520detection.%2520This%2520research%2520is%2520significant%2520in%2520the%2520fields%2520of%250Abiomedical%2520engineering%252C%2520computer-aided%2520disease%2520diagnosis%252C%2520and%2520ML-based%2520disease%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20comprehensive%20study%20on%20Blood%20Cancer%20detection%20and%20classification%20using%0A%20%20Convolutional%20Neural%20Network&entry.906535625=Md%20Taimur%20Ahad%20and%20Sajib%20Bin%20Mamun%20and%20Sumaya%20Mustofa%20and%20Bo%20Song%20and%20Yan%20Li&entry.1292438233=%20%20Over%20the%20years%20in%20object%20detection%20several%20efficient%20Convolutional%20Neural%0ANetworks%20%28CNN%29%20networks%2C%20such%20as%20DenseNet201%2C%20InceptionV3%2C%20ResNet152v2%2C%0ASEresNet152%2C%20VGG19%2C%20Xception%20gained%20significant%20attention%20due%20to%20their%0Aperformance.%20Moreover%2C%20CNN%20paradigms%20have%20expanded%20to%20transfer%20learning%20and%0Aensemble%20models%20from%20original%20CNN%20architectures.%20Research%20studies%20suggest%20that%0Atransfer%20learning%20and%20ensemble%20models%20are%20capable%20of%20increasing%20the%20accuracy%20of%0Adeep%20learning%20%28DL%29%20models.%20However%2C%20very%20few%20studies%20have%20conducted%0Acomprehensive%20experiments%20utilizing%20these%20techniques%20in%20detecting%20and%0Alocalizing%20blood%20malignancies.%20Realizing%20the%20gap%2C%20this%20study%20conducted%20three%0Aexperiments%3B%20in%20the%20first%20experiment%20--%20six%20original%20CNNs%20were%20used%2C%20in%20the%0Asecond%20experiment%20--%20transfer%20learning%20and%2C%20in%20the%20third%20experiment%20a%20novel%0Aensemble%20model%20DIX%20%28DenseNet201%2C%20InceptionV3%2C%20and%20Xception%29%20was%20developed%20to%0Adetect%20and%20classify%20blood%20cancer.%20The%20statistical%20result%20suggests%20that%20DIX%0Aoutperformed%20the%20original%20and%20transfer%20learning%20performance%2C%20providing%20an%0Aaccuracy%20of%2099.12%25.%20However%2C%20this%20study%20also%20provides%20a%20negative%20result%20in%20the%0Acase%20of%20transfer%20learning%2C%20as%20the%20transfer%20learning%20did%20not%20increase%20the%0Aaccuracy%20of%20the%20original%20CNNs.%20Like%20many%20other%20cancers%2C%20blood%20cancer%20diseases%0Arequire%20timely%20identification%20for%20effective%20treatment%20plans%20and%20increased%0Asurvival%20possibilities.%20The%20high%20accuracy%20in%20detecting%20and%20categorization%20blood%0Acancer%20detection%20using%20CNN%20suggests%20that%20the%20CNN%20model%20is%20promising%20in%20blood%0Acancer%20disease%20detection.%20This%20research%20is%20significant%20in%20the%20fields%20of%0Abiomedical%20engineering%2C%20computer-aided%20disease%20diagnosis%2C%20and%20ML-based%20disease%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06689v1&entry.124074799=Read"},
{"title": "Generalization of Graph Neural Networks is Robust to Model Mismatch", "author": "Zhiyang Wang and Juan Cervino and Alejandro Ribeiro", "abstract": "  Graph neural networks (GNNs) have demonstrated their effectiveness in various\ntasks supported by their generalization capabilities. However, the current\nanalysis of GNN generalization relies on the assumption that training and\ntesting data are independent and identically distributed (i.i.d). This imposes\nlimitations on the cases where a model mismatch exists when generating testing\ndata. In this paper, we examine GNNs that operate on geometric graphs generated\nfrom manifold models, explicitly focusing on scenarios where there is a\nmismatch between manifold models generating training and testing data. Our\nanalysis reveals the robustness of the GNN generalization in the presence of\nsuch model mismatch. This indicates that GNNs trained on graphs generated from\na manifold can still generalize well to unseen nodes and graphs generated from\na mismatched manifold. We attribute this mismatch to both node feature\nperturbations and edge perturbations within the generated graph. Our findings\nindicate that the generalization gap decreases as the number of nodes grows in\nthe training graph while increasing with larger manifold dimension as well as\nlarger mismatch. Importantly, we observe a trade-off between the generalization\nof GNNs and the capability to discriminate high-frequency components when\nfacing a model mismatch. The most important practical consequence of this\nanalysis is to shed light on the filter design of generalizable GNNs robust to\nmodel mismatch. We verify our theoretical findings with experiments on multiple\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2408.13878v2", "date": "2024-09-10", "relevancy": 2.3861, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4833}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.479}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20of%20Graph%20Neural%20Networks%20is%20Robust%20to%20Model%20Mismatch&body=Title%3A%20Generalization%20of%20Graph%20Neural%20Networks%20is%20Robust%20to%20Model%20Mismatch%0AAuthor%3A%20Zhiyang%20Wang%20and%20Juan%20Cervino%20and%20Alejandro%20Ribeiro%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20their%20effectiveness%20in%20various%0Atasks%20supported%20by%20their%20generalization%20capabilities.%20However%2C%20the%20current%0Aanalysis%20of%20GNN%20generalization%20relies%20on%20the%20assumption%20that%20training%20and%0Atesting%20data%20are%20independent%20and%20identically%20distributed%20%28i.i.d%29.%20This%20imposes%0Alimitations%20on%20the%20cases%20where%20a%20model%20mismatch%20exists%20when%20generating%20testing%0Adata.%20In%20this%20paper%2C%20we%20examine%20GNNs%20that%20operate%20on%20geometric%20graphs%20generated%0Afrom%20manifold%20models%2C%20explicitly%20focusing%20on%20scenarios%20where%20there%20is%20a%0Amismatch%20between%20manifold%20models%20generating%20training%20and%20testing%20data.%20Our%0Aanalysis%20reveals%20the%20robustness%20of%20the%20GNN%20generalization%20in%20the%20presence%20of%0Asuch%20model%20mismatch.%20This%20indicates%20that%20GNNs%20trained%20on%20graphs%20generated%20from%0Aa%20manifold%20can%20still%20generalize%20well%20to%20unseen%20nodes%20and%20graphs%20generated%20from%0Aa%20mismatched%20manifold.%20We%20attribute%20this%20mismatch%20to%20both%20node%20feature%0Aperturbations%20and%20edge%20perturbations%20within%20the%20generated%20graph.%20Our%20findings%0Aindicate%20that%20the%20generalization%20gap%20decreases%20as%20the%20number%20of%20nodes%20grows%20in%0Athe%20training%20graph%20while%20increasing%20with%20larger%20manifold%20dimension%20as%20well%20as%0Alarger%20mismatch.%20Importantly%2C%20we%20observe%20a%20trade-off%20between%20the%20generalization%0Aof%20GNNs%20and%20the%20capability%20to%20discriminate%20high-frequency%20components%20when%0Afacing%20a%20model%20mismatch.%20The%20most%20important%20practical%20consequence%20of%20this%0Aanalysis%20is%20to%20shed%20light%20on%20the%20filter%20design%20of%20generalizable%20GNNs%20robust%20to%0Amodel%20mismatch.%20We%20verify%20our%20theoretical%20findings%20with%20experiments%20on%20multiple%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13878v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520of%2520Graph%2520Neural%2520Networks%2520is%2520Robust%2520to%2520Model%2520Mismatch%26entry.906535625%3DZhiyang%2520Wang%2520and%2520Juan%2520Cervino%2520and%2520Alejandro%2520Ribeiro%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520demonstrated%2520their%2520effectiveness%2520in%2520various%250Atasks%2520supported%2520by%2520their%2520generalization%2520capabilities.%2520However%252C%2520the%2520current%250Aanalysis%2520of%2520GNN%2520generalization%2520relies%2520on%2520the%2520assumption%2520that%2520training%2520and%250Atesting%2520data%2520are%2520independent%2520and%2520identically%2520distributed%2520%2528i.i.d%2529.%2520This%2520imposes%250Alimitations%2520on%2520the%2520cases%2520where%2520a%2520model%2520mismatch%2520exists%2520when%2520generating%2520testing%250Adata.%2520In%2520this%2520paper%252C%2520we%2520examine%2520GNNs%2520that%2520operate%2520on%2520geometric%2520graphs%2520generated%250Afrom%2520manifold%2520models%252C%2520explicitly%2520focusing%2520on%2520scenarios%2520where%2520there%2520is%2520a%250Amismatch%2520between%2520manifold%2520models%2520generating%2520training%2520and%2520testing%2520data.%2520Our%250Aanalysis%2520reveals%2520the%2520robustness%2520of%2520the%2520GNN%2520generalization%2520in%2520the%2520presence%2520of%250Asuch%2520model%2520mismatch.%2520This%2520indicates%2520that%2520GNNs%2520trained%2520on%2520graphs%2520generated%2520from%250Aa%2520manifold%2520can%2520still%2520generalize%2520well%2520to%2520unseen%2520nodes%2520and%2520graphs%2520generated%2520from%250Aa%2520mismatched%2520manifold.%2520We%2520attribute%2520this%2520mismatch%2520to%2520both%2520node%2520feature%250Aperturbations%2520and%2520edge%2520perturbations%2520within%2520the%2520generated%2520graph.%2520Our%2520findings%250Aindicate%2520that%2520the%2520generalization%2520gap%2520decreases%2520as%2520the%2520number%2520of%2520nodes%2520grows%2520in%250Athe%2520training%2520graph%2520while%2520increasing%2520with%2520larger%2520manifold%2520dimension%2520as%2520well%2520as%250Alarger%2520mismatch.%2520Importantly%252C%2520we%2520observe%2520a%2520trade-off%2520between%2520the%2520generalization%250Aof%2520GNNs%2520and%2520the%2520capability%2520to%2520discriminate%2520high-frequency%2520components%2520when%250Afacing%2520a%2520model%2520mismatch.%2520The%2520most%2520important%2520practical%2520consequence%2520of%2520this%250Aanalysis%2520is%2520to%2520shed%2520light%2520on%2520the%2520filter%2520design%2520of%2520generalizable%2520GNNs%2520robust%2520to%250Amodel%2520mismatch.%2520We%2520verify%2520our%2520theoretical%2520findings%2520with%2520experiments%2520on%2520multiple%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13878v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20of%20Graph%20Neural%20Networks%20is%20Robust%20to%20Model%20Mismatch&entry.906535625=Zhiyang%20Wang%20and%20Juan%20Cervino%20and%20Alejandro%20Ribeiro&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20demonstrated%20their%20effectiveness%20in%20various%0Atasks%20supported%20by%20their%20generalization%20capabilities.%20However%2C%20the%20current%0Aanalysis%20of%20GNN%20generalization%20relies%20on%20the%20assumption%20that%20training%20and%0Atesting%20data%20are%20independent%20and%20identically%20distributed%20%28i.i.d%29.%20This%20imposes%0Alimitations%20on%20the%20cases%20where%20a%20model%20mismatch%20exists%20when%20generating%20testing%0Adata.%20In%20this%20paper%2C%20we%20examine%20GNNs%20that%20operate%20on%20geometric%20graphs%20generated%0Afrom%20manifold%20models%2C%20explicitly%20focusing%20on%20scenarios%20where%20there%20is%20a%0Amismatch%20between%20manifold%20models%20generating%20training%20and%20testing%20data.%20Our%0Aanalysis%20reveals%20the%20robustness%20of%20the%20GNN%20generalization%20in%20the%20presence%20of%0Asuch%20model%20mismatch.%20This%20indicates%20that%20GNNs%20trained%20on%20graphs%20generated%20from%0Aa%20manifold%20can%20still%20generalize%20well%20to%20unseen%20nodes%20and%20graphs%20generated%20from%0Aa%20mismatched%20manifold.%20We%20attribute%20this%20mismatch%20to%20both%20node%20feature%0Aperturbations%20and%20edge%20perturbations%20within%20the%20generated%20graph.%20Our%20findings%0Aindicate%20that%20the%20generalization%20gap%20decreases%20as%20the%20number%20of%20nodes%20grows%20in%0Athe%20training%20graph%20while%20increasing%20with%20larger%20manifold%20dimension%20as%20well%20as%0Alarger%20mismatch.%20Importantly%2C%20we%20observe%20a%20trade-off%20between%20the%20generalization%0Aof%20GNNs%20and%20the%20capability%20to%20discriminate%20high-frequency%20components%20when%0Afacing%20a%20model%20mismatch.%20The%20most%20important%20practical%20consequence%20of%20this%0Aanalysis%20is%20to%20shed%20light%20on%20the%20filter%20design%20of%20generalizable%20GNNs%20robust%20to%0Amodel%20mismatch.%20We%20verify%20our%20theoretical%20findings%20with%20experiments%20on%20multiple%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13878v2&entry.124074799=Read"},
{"title": "LiDAR-based 4D Occupancy Completion and Forecasting", "author": "Xinhao Liu and Moonjun Gong and Qi Fang and Haoyu Xie and Yiming Li and Hang Zhao and Chen Feng", "abstract": "  Scene completion and forecasting are two popular perception problems in\nresearch for mobile agents like autonomous vehicles. Existing approaches treat\nthe two problems in isolation, resulting in a separate perception of the two\naspects. In this paper, we introduce a novel LiDAR perception task of Occupancy\nCompletion and Forecasting (OCF) in the context of autonomous driving to unify\nthese aspects into a cohesive framework. This task requires new algorithms to\naddress three challenges altogether: (1) sparse-to-dense reconstruction, (2)\npartial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable\nsupervision and evaluation, we curate a large-scale dataset termed OCFBench\nfrom public autonomous driving datasets. We analyze the performance of closely\nrelated existing baseline models and our own ones on our dataset. We envision\nthat this research will inspire and call for further investigation in this\nevolving and crucial area of 4D perception. Our code for data curation and\nbaseline implementation is available at https://github.com/ai4ce/Occ4cast.\n", "link": "http://arxiv.org/abs/2310.11239v2", "date": "2024-09-10", "relevancy": 2.3851, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-based%204D%20Occupancy%20Completion%20and%20Forecasting&body=Title%3A%20LiDAR-based%204D%20Occupancy%20Completion%20and%20Forecasting%0AAuthor%3A%20Xinhao%20Liu%20and%20Moonjun%20Gong%20and%20Qi%20Fang%20and%20Haoyu%20Xie%20and%20Yiming%20Li%20and%20Hang%20Zhao%20and%20Chen%20Feng%0AAbstract%3A%20%20%20Scene%20completion%20and%20forecasting%20are%20two%20popular%20perception%20problems%20in%0Aresearch%20for%20mobile%20agents%20like%20autonomous%20vehicles.%20Existing%20approaches%20treat%0Athe%20two%20problems%20in%20isolation%2C%20resulting%20in%20a%20separate%20perception%20of%20the%20two%0Aaspects.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20LiDAR%20perception%20task%20of%20Occupancy%0ACompletion%20and%20Forecasting%20%28OCF%29%20in%20the%20context%20of%20autonomous%20driving%20to%20unify%0Athese%20aspects%20into%20a%20cohesive%20framework.%20This%20task%20requires%20new%20algorithms%20to%0Aaddress%20three%20challenges%20altogether%3A%20%281%29%20sparse-to-dense%20reconstruction%2C%20%282%29%0Apartial-to-complete%20hallucination%2C%20and%20%283%29%203D-to-4D%20prediction.%20To%20enable%0Asupervision%20and%20evaluation%2C%20we%20curate%20a%20large-scale%20dataset%20termed%20OCFBench%0Afrom%20public%20autonomous%20driving%20datasets.%20We%20analyze%20the%20performance%20of%20closely%0Arelated%20existing%20baseline%20models%20and%20our%20own%20ones%20on%20our%20dataset.%20We%20envision%0Athat%20this%20research%20will%20inspire%20and%20call%20for%20further%20investigation%20in%20this%0Aevolving%20and%20crucial%20area%20of%204D%20perception.%20Our%20code%20for%20data%20curation%20and%0Abaseline%20implementation%20is%20available%20at%20https%3A//github.com/ai4ce/Occ4cast.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-based%25204D%2520Occupancy%2520Completion%2520and%2520Forecasting%26entry.906535625%3DXinhao%2520Liu%2520and%2520Moonjun%2520Gong%2520and%2520Qi%2520Fang%2520and%2520Haoyu%2520Xie%2520and%2520Yiming%2520Li%2520and%2520Hang%2520Zhao%2520and%2520Chen%2520Feng%26entry.1292438233%3D%2520%2520Scene%2520completion%2520and%2520forecasting%2520are%2520two%2520popular%2520perception%2520problems%2520in%250Aresearch%2520for%2520mobile%2520agents%2520like%2520autonomous%2520vehicles.%2520Existing%2520approaches%2520treat%250Athe%2520two%2520problems%2520in%2520isolation%252C%2520resulting%2520in%2520a%2520separate%2520perception%2520of%2520the%2520two%250Aaspects.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520LiDAR%2520perception%2520task%2520of%2520Occupancy%250ACompletion%2520and%2520Forecasting%2520%2528OCF%2529%2520in%2520the%2520context%2520of%2520autonomous%2520driving%2520to%2520unify%250Athese%2520aspects%2520into%2520a%2520cohesive%2520framework.%2520This%2520task%2520requires%2520new%2520algorithms%2520to%250Aaddress%2520three%2520challenges%2520altogether%253A%2520%25281%2529%2520sparse-to-dense%2520reconstruction%252C%2520%25282%2529%250Apartial-to-complete%2520hallucination%252C%2520and%2520%25283%2529%25203D-to-4D%2520prediction.%2520To%2520enable%250Asupervision%2520and%2520evaluation%252C%2520we%2520curate%2520a%2520large-scale%2520dataset%2520termed%2520OCFBench%250Afrom%2520public%2520autonomous%2520driving%2520datasets.%2520We%2520analyze%2520the%2520performance%2520of%2520closely%250Arelated%2520existing%2520baseline%2520models%2520and%2520our%2520own%2520ones%2520on%2520our%2520dataset.%2520We%2520envision%250Athat%2520this%2520research%2520will%2520inspire%2520and%2520call%2520for%2520further%2520investigation%2520in%2520this%250Aevolving%2520and%2520crucial%2520area%2520of%25204D%2520perception.%2520Our%2520code%2520for%2520data%2520curation%2520and%250Abaseline%2520implementation%2520is%2520available%2520at%2520https%253A//github.com/ai4ce/Occ4cast.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-based%204D%20Occupancy%20Completion%20and%20Forecasting&entry.906535625=Xinhao%20Liu%20and%20Moonjun%20Gong%20and%20Qi%20Fang%20and%20Haoyu%20Xie%20and%20Yiming%20Li%20and%20Hang%20Zhao%20and%20Chen%20Feng&entry.1292438233=%20%20Scene%20completion%20and%20forecasting%20are%20two%20popular%20perception%20problems%20in%0Aresearch%20for%20mobile%20agents%20like%20autonomous%20vehicles.%20Existing%20approaches%20treat%0Athe%20two%20problems%20in%20isolation%2C%20resulting%20in%20a%20separate%20perception%20of%20the%20two%0Aaspects.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20LiDAR%20perception%20task%20of%20Occupancy%0ACompletion%20and%20Forecasting%20%28OCF%29%20in%20the%20context%20of%20autonomous%20driving%20to%20unify%0Athese%20aspects%20into%20a%20cohesive%20framework.%20This%20task%20requires%20new%20algorithms%20to%0Aaddress%20three%20challenges%20altogether%3A%20%281%29%20sparse-to-dense%20reconstruction%2C%20%282%29%0Apartial-to-complete%20hallucination%2C%20and%20%283%29%203D-to-4D%20prediction.%20To%20enable%0Asupervision%20and%20evaluation%2C%20we%20curate%20a%20large-scale%20dataset%20termed%20OCFBench%0Afrom%20public%20autonomous%20driving%20datasets.%20We%20analyze%20the%20performance%20of%20closely%0Arelated%20existing%20baseline%20models%20and%20our%20own%20ones%20on%20our%20dataset.%20We%20envision%0Athat%20this%20research%20will%20inspire%20and%20call%20for%20further%20investigation%20in%20this%0Aevolving%20and%20crucial%20area%20of%204D%20perception.%20Our%20code%20for%20data%20curation%20and%0Abaseline%20implementation%20is%20available%20at%20https%3A//github.com/ai4ce/Occ4cast.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11239v2&entry.124074799=Read"},
{"title": "A Manifold Perspective on the Statistical Generalization of Graph Neural\n  Networks", "author": "Zhiyang Wang and Juan Cervino and Alejandro Ribeiro", "abstract": "  Convolutional neural networks have been successfully extended to operate on\ngraphs, giving rise to Graph Neural Networks (GNNs). GNNs combine information\nfrom adjacent nodes by successive applications of graph convolutions. GNNs have\nbeen implemented successfully in various learning tasks while the theoretical\nunderstanding of their generalization capability is still in progress. In this\npaper, we leverage manifold theory to analyze the statistical generalization\ngap of GNNs operating on graphs constructed on sampled points from manifolds.\nWe study the generalization gaps of GNNs on both node-level and graph-level\ntasks. We show that the generalization gaps decrease with the number of nodes\nin the training graphs, which guarantees the generalization of GNNs to unseen\npoints over manifolds. We validate our theoretical results in multiple\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2406.05225v3", "date": "2024-09-10", "relevancy": 2.3848, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4834}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4789}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Manifold%20Perspective%20on%20the%20Statistical%20Generalization%20of%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20A%20Manifold%20Perspective%20on%20the%20Statistical%20Generalization%20of%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Zhiyang%20Wang%20and%20Juan%20Cervino%20and%20Alejandro%20Ribeiro%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20have%20been%20successfully%20extended%20to%20operate%20on%0Agraphs%2C%20giving%20rise%20to%20Graph%20Neural%20Networks%20%28GNNs%29.%20GNNs%20combine%20information%0Afrom%20adjacent%20nodes%20by%20successive%20applications%20of%20graph%20convolutions.%20GNNs%20have%0Abeen%20implemented%20successfully%20in%20various%20learning%20tasks%20while%20the%20theoretical%0Aunderstanding%20of%20their%20generalization%20capability%20is%20still%20in%20progress.%20In%20this%0Apaper%2C%20we%20leverage%20manifold%20theory%20to%20analyze%20the%20statistical%20generalization%0Agap%20of%20GNNs%20operating%20on%20graphs%20constructed%20on%20sampled%20points%20from%20manifolds.%0AWe%20study%20the%20generalization%20gaps%20of%20GNNs%20on%20both%20node-level%20and%20graph-level%0Atasks.%20We%20show%20that%20the%20generalization%20gaps%20decrease%20with%20the%20number%20of%20nodes%0Ain%20the%20training%20graphs%2C%20which%20guarantees%20the%20generalization%20of%20GNNs%20to%20unseen%0Apoints%20over%20manifolds.%20We%20validate%20our%20theoretical%20results%20in%20multiple%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05225v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Manifold%2520Perspective%2520on%2520the%2520Statistical%2520Generalization%2520of%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DZhiyang%2520Wang%2520and%2520Juan%2520Cervino%2520and%2520Alejandro%2520Ribeiro%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520have%2520been%2520successfully%2520extended%2520to%2520operate%2520on%250Agraphs%252C%2520giving%2520rise%2520to%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529.%2520GNNs%2520combine%2520information%250Afrom%2520adjacent%2520nodes%2520by%2520successive%2520applications%2520of%2520graph%2520convolutions.%2520GNNs%2520have%250Abeen%2520implemented%2520successfully%2520in%2520various%2520learning%2520tasks%2520while%2520the%2520theoretical%250Aunderstanding%2520of%2520their%2520generalization%2520capability%2520is%2520still%2520in%2520progress.%2520In%2520this%250Apaper%252C%2520we%2520leverage%2520manifold%2520theory%2520to%2520analyze%2520the%2520statistical%2520generalization%250Agap%2520of%2520GNNs%2520operating%2520on%2520graphs%2520constructed%2520on%2520sampled%2520points%2520from%2520manifolds.%250AWe%2520study%2520the%2520generalization%2520gaps%2520of%2520GNNs%2520on%2520both%2520node-level%2520and%2520graph-level%250Atasks.%2520We%2520show%2520that%2520the%2520generalization%2520gaps%2520decrease%2520with%2520the%2520number%2520of%2520nodes%250Ain%2520the%2520training%2520graphs%252C%2520which%2520guarantees%2520the%2520generalization%2520of%2520GNNs%2520to%2520unseen%250Apoints%2520over%2520manifolds.%2520We%2520validate%2520our%2520theoretical%2520results%2520in%2520multiple%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05225v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Manifold%20Perspective%20on%20the%20Statistical%20Generalization%20of%20Graph%20Neural%0A%20%20Networks&entry.906535625=Zhiyang%20Wang%20and%20Juan%20Cervino%20and%20Alejandro%20Ribeiro&entry.1292438233=%20%20Convolutional%20neural%20networks%20have%20been%20successfully%20extended%20to%20operate%20on%0Agraphs%2C%20giving%20rise%20to%20Graph%20Neural%20Networks%20%28GNNs%29.%20GNNs%20combine%20information%0Afrom%20adjacent%20nodes%20by%20successive%20applications%20of%20graph%20convolutions.%20GNNs%20have%0Abeen%20implemented%20successfully%20in%20various%20learning%20tasks%20while%20the%20theoretical%0Aunderstanding%20of%20their%20generalization%20capability%20is%20still%20in%20progress.%20In%20this%0Apaper%2C%20we%20leverage%20manifold%20theory%20to%20analyze%20the%20statistical%20generalization%0Agap%20of%20GNNs%20operating%20on%20graphs%20constructed%20on%20sampled%20points%20from%20manifolds.%0AWe%20study%20the%20generalization%20gaps%20of%20GNNs%20on%20both%20node-level%20and%20graph-level%0Atasks.%20We%20show%20that%20the%20generalization%20gaps%20decrease%20with%20the%20number%20of%20nodes%0Ain%20the%20training%20graphs%2C%20which%20guarantees%20the%20generalization%20of%20GNNs%20to%20unseen%0Apoints%20over%20manifolds.%20We%20validate%20our%20theoretical%20results%20in%20multiple%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05225v3&entry.124074799=Read"},
{"title": "A study on Deep Convolutional Neural Networks, Transfer Learning and\n  Ensemble Model for Breast Cancer Detection", "author": "Md Taimur Ahad and Sumaya Mustofa and Faruk Ahmed and Yousuf Rayhan Emon and Aunirudra Dey Anu", "abstract": "  In deep learning, transfer learning and ensemble models have shown promise in\nimproving computer-aided disease diagnosis. However, applying the transfer\nlearning and ensemble model is still relatively limited. Moreover, the ensemble\nmodel's development is ad-hoc, overlooks redundant layers, and suffers from\nimbalanced datasets and inadequate augmentation. Lastly, significant Deep\nConvolutional Neural Networks (D-CNNs) have been introduced to detect and\nclassify breast cancer. Still, very few comparative studies were conducted to\ninvestigate the accuracy and efficiency of existing CNN architectures.\nRealising the gaps, this study compares the performance of D-CNN, which\nincludes the original CNN, transfer learning, and an ensemble model, in\ndetecting breast cancer. The comparison study of this paper consists of\ncomparison using six CNN-based deep learning architectures (SE-ResNet152,\nMobileNetV2, VGG19, ResNet18, InceptionV3, and DenseNet-121), a transfer\nlearning, and an ensemble model on breast cancer detection. Among the\ncomparison of these models, the ensemble model provides the highest detection\nand classification accuracy of 99.94% for breast cancer detection and\nclassification. However, this study also provides a negative result in the case\nof transfer learning, as the transfer learning did not increase the accuracy of\nthe original SE-ResNet152, MobileNetV2, VGG19, ResNet18, InceptionV3, and\nDenseNet-121 model. The high accuracy in detecting and categorising breast\ncancer detection using CNN suggests that the CNN model is promising in breast\ncancer disease detection. This research is significant in biomedical\nengineering, computer-aided disease diagnosis, and ML-based disease detection.\n", "link": "http://arxiv.org/abs/2409.06699v1", "date": "2024-09-10", "relevancy": 2.3832, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20study%20on%20Deep%20Convolutional%20Neural%20Networks%2C%20Transfer%20Learning%20and%0A%20%20Ensemble%20Model%20for%20Breast%20Cancer%20Detection&body=Title%3A%20A%20study%20on%20Deep%20Convolutional%20Neural%20Networks%2C%20Transfer%20Learning%20and%0A%20%20Ensemble%20Model%20for%20Breast%20Cancer%20Detection%0AAuthor%3A%20Md%20Taimur%20Ahad%20and%20Sumaya%20Mustofa%20and%20Faruk%20Ahmed%20and%20Yousuf%20Rayhan%20Emon%20and%20Aunirudra%20Dey%20Anu%0AAbstract%3A%20%20%20In%20deep%20learning%2C%20transfer%20learning%20and%20ensemble%20models%20have%20shown%20promise%20in%0Aimproving%20computer-aided%20disease%20diagnosis.%20However%2C%20applying%20the%20transfer%0Alearning%20and%20ensemble%20model%20is%20still%20relatively%20limited.%20Moreover%2C%20the%20ensemble%0Amodel%27s%20development%20is%20ad-hoc%2C%20overlooks%20redundant%20layers%2C%20and%20suffers%20from%0Aimbalanced%20datasets%20and%20inadequate%20augmentation.%20Lastly%2C%20significant%20Deep%0AConvolutional%20Neural%20Networks%20%28D-CNNs%29%20have%20been%20introduced%20to%20detect%20and%0Aclassify%20breast%20cancer.%20Still%2C%20very%20few%20comparative%20studies%20were%20conducted%20to%0Ainvestigate%20the%20accuracy%20and%20efficiency%20of%20existing%20CNN%20architectures.%0ARealising%20the%20gaps%2C%20this%20study%20compares%20the%20performance%20of%20D-CNN%2C%20which%0Aincludes%20the%20original%20CNN%2C%20transfer%20learning%2C%20and%20an%20ensemble%20model%2C%20in%0Adetecting%20breast%20cancer.%20The%20comparison%20study%20of%20this%20paper%20consists%20of%0Acomparison%20using%20six%20CNN-based%20deep%20learning%20architectures%20%28SE-ResNet152%2C%0AMobileNetV2%2C%20VGG19%2C%20ResNet18%2C%20InceptionV3%2C%20and%20DenseNet-121%29%2C%20a%20transfer%0Alearning%2C%20and%20an%20ensemble%20model%20on%20breast%20cancer%20detection.%20Among%20the%0Acomparison%20of%20these%20models%2C%20the%20ensemble%20model%20provides%20the%20highest%20detection%0Aand%20classification%20accuracy%20of%2099.94%25%20for%20breast%20cancer%20detection%20and%0Aclassification.%20However%2C%20this%20study%20also%20provides%20a%20negative%20result%20in%20the%20case%0Aof%20transfer%20learning%2C%20as%20the%20transfer%20learning%20did%20not%20increase%20the%20accuracy%20of%0Athe%20original%20SE-ResNet152%2C%20MobileNetV2%2C%20VGG19%2C%20ResNet18%2C%20InceptionV3%2C%20and%0ADenseNet-121%20model.%20The%20high%20accuracy%20in%20detecting%20and%20categorising%20breast%0Acancer%20detection%20using%20CNN%20suggests%20that%20the%20CNN%20model%20is%20promising%20in%20breast%0Acancer%20disease%20detection.%20This%20research%20is%20significant%20in%20biomedical%0Aengineering%2C%20computer-aided%20disease%20diagnosis%2C%20and%20ML-based%20disease%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520study%2520on%2520Deep%2520Convolutional%2520Neural%2520Networks%252C%2520Transfer%2520Learning%2520and%250A%2520%2520Ensemble%2520Model%2520for%2520Breast%2520Cancer%2520Detection%26entry.906535625%3DMd%2520Taimur%2520Ahad%2520and%2520Sumaya%2520Mustofa%2520and%2520Faruk%2520Ahmed%2520and%2520Yousuf%2520Rayhan%2520Emon%2520and%2520Aunirudra%2520Dey%2520Anu%26entry.1292438233%3D%2520%2520In%2520deep%2520learning%252C%2520transfer%2520learning%2520and%2520ensemble%2520models%2520have%2520shown%2520promise%2520in%250Aimproving%2520computer-aided%2520disease%2520diagnosis.%2520However%252C%2520applying%2520the%2520transfer%250Alearning%2520and%2520ensemble%2520model%2520is%2520still%2520relatively%2520limited.%2520Moreover%252C%2520the%2520ensemble%250Amodel%2527s%2520development%2520is%2520ad-hoc%252C%2520overlooks%2520redundant%2520layers%252C%2520and%2520suffers%2520from%250Aimbalanced%2520datasets%2520and%2520inadequate%2520augmentation.%2520Lastly%252C%2520significant%2520Deep%250AConvolutional%2520Neural%2520Networks%2520%2528D-CNNs%2529%2520have%2520been%2520introduced%2520to%2520detect%2520and%250Aclassify%2520breast%2520cancer.%2520Still%252C%2520very%2520few%2520comparative%2520studies%2520were%2520conducted%2520to%250Ainvestigate%2520the%2520accuracy%2520and%2520efficiency%2520of%2520existing%2520CNN%2520architectures.%250ARealising%2520the%2520gaps%252C%2520this%2520study%2520compares%2520the%2520performance%2520of%2520D-CNN%252C%2520which%250Aincludes%2520the%2520original%2520CNN%252C%2520transfer%2520learning%252C%2520and%2520an%2520ensemble%2520model%252C%2520in%250Adetecting%2520breast%2520cancer.%2520The%2520comparison%2520study%2520of%2520this%2520paper%2520consists%2520of%250Acomparison%2520using%2520six%2520CNN-based%2520deep%2520learning%2520architectures%2520%2528SE-ResNet152%252C%250AMobileNetV2%252C%2520VGG19%252C%2520ResNet18%252C%2520InceptionV3%252C%2520and%2520DenseNet-121%2529%252C%2520a%2520transfer%250Alearning%252C%2520and%2520an%2520ensemble%2520model%2520on%2520breast%2520cancer%2520detection.%2520Among%2520the%250Acomparison%2520of%2520these%2520models%252C%2520the%2520ensemble%2520model%2520provides%2520the%2520highest%2520detection%250Aand%2520classification%2520accuracy%2520of%252099.94%2525%2520for%2520breast%2520cancer%2520detection%2520and%250Aclassification.%2520However%252C%2520this%2520study%2520also%2520provides%2520a%2520negative%2520result%2520in%2520the%2520case%250Aof%2520transfer%2520learning%252C%2520as%2520the%2520transfer%2520learning%2520did%2520not%2520increase%2520the%2520accuracy%2520of%250Athe%2520original%2520SE-ResNet152%252C%2520MobileNetV2%252C%2520VGG19%252C%2520ResNet18%252C%2520InceptionV3%252C%2520and%250ADenseNet-121%2520model.%2520The%2520high%2520accuracy%2520in%2520detecting%2520and%2520categorising%2520breast%250Acancer%2520detection%2520using%2520CNN%2520suggests%2520that%2520the%2520CNN%2520model%2520is%2520promising%2520in%2520breast%250Acancer%2520disease%2520detection.%2520This%2520research%2520is%2520significant%2520in%2520biomedical%250Aengineering%252C%2520computer-aided%2520disease%2520diagnosis%252C%2520and%2520ML-based%2520disease%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20study%20on%20Deep%20Convolutional%20Neural%20Networks%2C%20Transfer%20Learning%20and%0A%20%20Ensemble%20Model%20for%20Breast%20Cancer%20Detection&entry.906535625=Md%20Taimur%20Ahad%20and%20Sumaya%20Mustofa%20and%20Faruk%20Ahmed%20and%20Yousuf%20Rayhan%20Emon%20and%20Aunirudra%20Dey%20Anu&entry.1292438233=%20%20In%20deep%20learning%2C%20transfer%20learning%20and%20ensemble%20models%20have%20shown%20promise%20in%0Aimproving%20computer-aided%20disease%20diagnosis.%20However%2C%20applying%20the%20transfer%0Alearning%20and%20ensemble%20model%20is%20still%20relatively%20limited.%20Moreover%2C%20the%20ensemble%0Amodel%27s%20development%20is%20ad-hoc%2C%20overlooks%20redundant%20layers%2C%20and%20suffers%20from%0Aimbalanced%20datasets%20and%20inadequate%20augmentation.%20Lastly%2C%20significant%20Deep%0AConvolutional%20Neural%20Networks%20%28D-CNNs%29%20have%20been%20introduced%20to%20detect%20and%0Aclassify%20breast%20cancer.%20Still%2C%20very%20few%20comparative%20studies%20were%20conducted%20to%0Ainvestigate%20the%20accuracy%20and%20efficiency%20of%20existing%20CNN%20architectures.%0ARealising%20the%20gaps%2C%20this%20study%20compares%20the%20performance%20of%20D-CNN%2C%20which%0Aincludes%20the%20original%20CNN%2C%20transfer%20learning%2C%20and%20an%20ensemble%20model%2C%20in%0Adetecting%20breast%20cancer.%20The%20comparison%20study%20of%20this%20paper%20consists%20of%0Acomparison%20using%20six%20CNN-based%20deep%20learning%20architectures%20%28SE-ResNet152%2C%0AMobileNetV2%2C%20VGG19%2C%20ResNet18%2C%20InceptionV3%2C%20and%20DenseNet-121%29%2C%20a%20transfer%0Alearning%2C%20and%20an%20ensemble%20model%20on%20breast%20cancer%20detection.%20Among%20the%0Acomparison%20of%20these%20models%2C%20the%20ensemble%20model%20provides%20the%20highest%20detection%0Aand%20classification%20accuracy%20of%2099.94%25%20for%20breast%20cancer%20detection%20and%0Aclassification.%20However%2C%20this%20study%20also%20provides%20a%20negative%20result%20in%20the%20case%0Aof%20transfer%20learning%2C%20as%20the%20transfer%20learning%20did%20not%20increase%20the%20accuracy%20of%0Athe%20original%20SE-ResNet152%2C%20MobileNetV2%2C%20VGG19%2C%20ResNet18%2C%20InceptionV3%2C%20and%0ADenseNet-121%20model.%20The%20high%20accuracy%20in%20detecting%20and%20categorising%20breast%0Acancer%20detection%20using%20CNN%20suggests%20that%20the%20CNN%20model%20is%20promising%20in%20breast%0Acancer%20disease%20detection.%20This%20research%20is%20significant%20in%20biomedical%0Aengineering%2C%20computer-aided%20disease%20diagnosis%2C%20and%20ML-based%20disease%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06699v1&entry.124074799=Read"},
{"title": "DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos\n  Enhanced Kaleidoscopic Images", "author": "Taslim Murad and Prakash Chourasia and Sarwan Ali and Murray Patterson", "abstract": "  Cancer is a complex disease characterized by uncontrolled cell growth. T cell\nreceptors (TCRs), crucial proteins in the immune system, play a key role in\nrecognizing antigens, including those associated with cancer. Recent\nadvancements in sequencing technologies have facilitated comprehensive\nprofiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity\nand enabling TCR-based immunotherapies. However, analyzing these intricate\nbiomolecules necessitates efficient representations that capture their\nstructural and functional information. T-cell protein sequences pose unique\nchallenges due to their relatively smaller lengths compared to other\nbiomolecules. An image-based representation approach becomes a preferred choice\nfor efficient embeddings, allowing for the preservation of essential details\nand enabling comprehensive analysis of T-cell protein sequences. In this paper,\nwe propose to generate images from the protein sequences using the idea of\nChaos Game Representation (CGR) using the Kaleidoscopic images approach. This\nDeep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced\nKaleidoscopic Images (called DANCE) provides a unique way to visualize protein\nsequences by recursively applying chaos game rules around a central seed point.\nwe perform the classification of the T cell receptors (TCRs) protein sequences\nin terms of their respective target cancer cells, as TCRs are known for their\nimmune response against cancer disease. The TCR sequences are converted into\nimages using the DANCE method. We employ deep-learning vision models to perform\nthe classification to obtain insights into the relationship between the visual\npatterns observed in the generated kaleidoscopic images and the underlying\nprotein properties. By combining CGR-based image generation with deep learning\nclassification, this study opens novel possibilities in the protein analysis\ndomain.\n", "link": "http://arxiv.org/abs/2409.06694v1", "date": "2024-09-10", "relevancy": 2.3595, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4724}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images&body=Title%3A%20DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images%0AAuthor%3A%20Taslim%20Murad%20and%20Prakash%20Chourasia%20and%20Sarwan%20Ali%20and%20Murray%20Patterson%0AAbstract%3A%20%20%20Cancer%20is%20a%20complex%20disease%20characterized%20by%20uncontrolled%20cell%20growth.%20T%20cell%0Areceptors%20%28TCRs%29%2C%20crucial%20proteins%20in%20the%20immune%20system%2C%20play%20a%20key%20role%20in%0Arecognizing%20antigens%2C%20including%20those%20associated%20with%20cancer.%20Recent%0Aadvancements%20in%20sequencing%20technologies%20have%20facilitated%20comprehensive%0Aprofiling%20of%20TCR%20repertoires%2C%20uncovering%20TCRs%20with%20potent%20anti-cancer%20activity%0Aand%20enabling%20TCR-based%20immunotherapies.%20However%2C%20analyzing%20these%20intricate%0Abiomolecules%20necessitates%20efficient%20representations%20that%20capture%20their%0Astructural%20and%20functional%20information.%20T-cell%20protein%20sequences%20pose%20unique%0Achallenges%20due%20to%20their%20relatively%20smaller%20lengths%20compared%20to%20other%0Abiomolecules.%20An%20image-based%20representation%20approach%20becomes%20a%20preferred%20choice%0Afor%20efficient%20embeddings%2C%20allowing%20for%20the%20preservation%20of%20essential%20details%0Aand%20enabling%20comprehensive%20analysis%20of%20T-cell%20protein%20sequences.%20In%20this%20paper%2C%0Awe%20propose%20to%20generate%20images%20from%20the%20protein%20sequences%20using%20the%20idea%20of%0AChaos%20Game%20Representation%20%28CGR%29%20using%20the%20Kaleidoscopic%20images%20approach.%20This%0ADeep%20Learning%20Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%20Enhanced%0AKaleidoscopic%20Images%20%28called%20DANCE%29%20provides%20a%20unique%20way%20to%20visualize%20protein%0Asequences%20by%20recursively%20applying%20chaos%20game%20rules%20around%20a%20central%20seed%20point.%0Awe%20perform%20the%20classification%20of%20the%20T%20cell%20receptors%20%28TCRs%29%20protein%20sequences%0Ain%20terms%20of%20their%20respective%20target%20cancer%20cells%2C%20as%20TCRs%20are%20known%20for%20their%0Aimmune%20response%20against%20cancer%20disease.%20The%20TCR%20sequences%20are%20converted%20into%0Aimages%20using%20the%20DANCE%20method.%20We%20employ%20deep-learning%20vision%20models%20to%20perform%0Athe%20classification%20to%20obtain%20insights%20into%20the%20relationship%20between%20the%20visual%0Apatterns%20observed%20in%20the%20generated%20kaleidoscopic%20images%20and%20the%20underlying%0Aprotein%20properties.%20By%20combining%20CGR-based%20image%20generation%20with%20deep%20learning%0Aclassification%2C%20this%20study%20opens%20novel%20possibilities%20in%20the%20protein%20analysis%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDANCE%253A%2520Deep%2520Learning-Assisted%2520Analysis%2520of%2520Protein%2520Sequences%2520Using%2520Chaos%250A%2520%2520Enhanced%2520Kaleidoscopic%2520Images%26entry.906535625%3DTaslim%2520Murad%2520and%2520Prakash%2520Chourasia%2520and%2520Sarwan%2520Ali%2520and%2520Murray%2520Patterson%26entry.1292438233%3D%2520%2520Cancer%2520is%2520a%2520complex%2520disease%2520characterized%2520by%2520uncontrolled%2520cell%2520growth.%2520T%2520cell%250Areceptors%2520%2528TCRs%2529%252C%2520crucial%2520proteins%2520in%2520the%2520immune%2520system%252C%2520play%2520a%2520key%2520role%2520in%250Arecognizing%2520antigens%252C%2520including%2520those%2520associated%2520with%2520cancer.%2520Recent%250Aadvancements%2520in%2520sequencing%2520technologies%2520have%2520facilitated%2520comprehensive%250Aprofiling%2520of%2520TCR%2520repertoires%252C%2520uncovering%2520TCRs%2520with%2520potent%2520anti-cancer%2520activity%250Aand%2520enabling%2520TCR-based%2520immunotherapies.%2520However%252C%2520analyzing%2520these%2520intricate%250Abiomolecules%2520necessitates%2520efficient%2520representations%2520that%2520capture%2520their%250Astructural%2520and%2520functional%2520information.%2520T-cell%2520protein%2520sequences%2520pose%2520unique%250Achallenges%2520due%2520to%2520their%2520relatively%2520smaller%2520lengths%2520compared%2520to%2520other%250Abiomolecules.%2520An%2520image-based%2520representation%2520approach%2520becomes%2520a%2520preferred%2520choice%250Afor%2520efficient%2520embeddings%252C%2520allowing%2520for%2520the%2520preservation%2520of%2520essential%2520details%250Aand%2520enabling%2520comprehensive%2520analysis%2520of%2520T-cell%2520protein%2520sequences.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520to%2520generate%2520images%2520from%2520the%2520protein%2520sequences%2520using%2520the%2520idea%2520of%250AChaos%2520Game%2520Representation%2520%2528CGR%2529%2520using%2520the%2520Kaleidoscopic%2520images%2520approach.%2520This%250ADeep%2520Learning%2520Assisted%2520Analysis%2520of%2520Protein%2520Sequences%2520Using%2520Chaos%2520Enhanced%250AKaleidoscopic%2520Images%2520%2528called%2520DANCE%2529%2520provides%2520a%2520unique%2520way%2520to%2520visualize%2520protein%250Asequences%2520by%2520recursively%2520applying%2520chaos%2520game%2520rules%2520around%2520a%2520central%2520seed%2520point.%250Awe%2520perform%2520the%2520classification%2520of%2520the%2520T%2520cell%2520receptors%2520%2528TCRs%2529%2520protein%2520sequences%250Ain%2520terms%2520of%2520their%2520respective%2520target%2520cancer%2520cells%252C%2520as%2520TCRs%2520are%2520known%2520for%2520their%250Aimmune%2520response%2520against%2520cancer%2520disease.%2520The%2520TCR%2520sequences%2520are%2520converted%2520into%250Aimages%2520using%2520the%2520DANCE%2520method.%2520We%2520employ%2520deep-learning%2520vision%2520models%2520to%2520perform%250Athe%2520classification%2520to%2520obtain%2520insights%2520into%2520the%2520relationship%2520between%2520the%2520visual%250Apatterns%2520observed%2520in%2520the%2520generated%2520kaleidoscopic%2520images%2520and%2520the%2520underlying%250Aprotein%2520properties.%2520By%2520combining%2520CGR-based%2520image%2520generation%2520with%2520deep%2520learning%250Aclassification%252C%2520this%2520study%2520opens%2520novel%2520possibilities%2520in%2520the%2520protein%2520analysis%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DANCE%3A%20Deep%20Learning-Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%0A%20%20Enhanced%20Kaleidoscopic%20Images&entry.906535625=Taslim%20Murad%20and%20Prakash%20Chourasia%20and%20Sarwan%20Ali%20and%20Murray%20Patterson&entry.1292438233=%20%20Cancer%20is%20a%20complex%20disease%20characterized%20by%20uncontrolled%20cell%20growth.%20T%20cell%0Areceptors%20%28TCRs%29%2C%20crucial%20proteins%20in%20the%20immune%20system%2C%20play%20a%20key%20role%20in%0Arecognizing%20antigens%2C%20including%20those%20associated%20with%20cancer.%20Recent%0Aadvancements%20in%20sequencing%20technologies%20have%20facilitated%20comprehensive%0Aprofiling%20of%20TCR%20repertoires%2C%20uncovering%20TCRs%20with%20potent%20anti-cancer%20activity%0Aand%20enabling%20TCR-based%20immunotherapies.%20However%2C%20analyzing%20these%20intricate%0Abiomolecules%20necessitates%20efficient%20representations%20that%20capture%20their%0Astructural%20and%20functional%20information.%20T-cell%20protein%20sequences%20pose%20unique%0Achallenges%20due%20to%20their%20relatively%20smaller%20lengths%20compared%20to%20other%0Abiomolecules.%20An%20image-based%20representation%20approach%20becomes%20a%20preferred%20choice%0Afor%20efficient%20embeddings%2C%20allowing%20for%20the%20preservation%20of%20essential%20details%0Aand%20enabling%20comprehensive%20analysis%20of%20T-cell%20protein%20sequences.%20In%20this%20paper%2C%0Awe%20propose%20to%20generate%20images%20from%20the%20protein%20sequences%20using%20the%20idea%20of%0AChaos%20Game%20Representation%20%28CGR%29%20using%20the%20Kaleidoscopic%20images%20approach.%20This%0ADeep%20Learning%20Assisted%20Analysis%20of%20Protein%20Sequences%20Using%20Chaos%20Enhanced%0AKaleidoscopic%20Images%20%28called%20DANCE%29%20provides%20a%20unique%20way%20to%20visualize%20protein%0Asequences%20by%20recursively%20applying%20chaos%20game%20rules%20around%20a%20central%20seed%20point.%0Awe%20perform%20the%20classification%20of%20the%20T%20cell%20receptors%20%28TCRs%29%20protein%20sequences%0Ain%20terms%20of%20their%20respective%20target%20cancer%20cells%2C%20as%20TCRs%20are%20known%20for%20their%0Aimmune%20response%20against%20cancer%20disease.%20The%20TCR%20sequences%20are%20converted%20into%0Aimages%20using%20the%20DANCE%20method.%20We%20employ%20deep-learning%20vision%20models%20to%20perform%0Athe%20classification%20to%20obtain%20insights%20into%20the%20relationship%20between%20the%20visual%0Apatterns%20observed%20in%20the%20generated%20kaleidoscopic%20images%20and%20the%20underlying%0Aprotein%20properties.%20By%20combining%20CGR-based%20image%20generation%20with%20deep%20learning%0Aclassification%2C%20this%20study%20opens%20novel%20possibilities%20in%20the%20protein%20analysis%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06694v1&entry.124074799=Read"},
{"title": "Efficient Visuo-Haptic Object Shape Completion for Robot Manipulation", "author": "Lukas Rustler and Jiri Matas and Matej Hoffmann", "abstract": "  For robot manipulation, a complete and accurate object shape is desirable.\nHere, we present a method that combines visual and haptic reconstruction in a\nclosed-loop pipeline. From an initial viewpoint, the object shape is\nreconstructed using an implicit surface deep neural network. The location with\nhighest uncertainty is selected for haptic exploration, the object is touched,\nthe new information from touch and a new point cloud from the camera are added,\nobject position is re-estimated and the cycle is repeated. We extend Rustler et\nal. (2022) by using a new theoretically grounded method to determine the points\nwith highest uncertainty, and we increase the yield of every haptic exploration\nby adding not only the contact points to the point cloud but also incorporating\nthe empty space established through the robot movement to the object.\nAdditionally, the solution is compact in that the jaws of a closed two-finger\ngripper are directly used for exploration. The object position is re-estimated\nafter every robot action and multiple objects can be present simultaneously on\nthe table. We achieve a steady improvement with every touch using three\ndifferent metrics and demonstrate the utility of the better shape\nreconstruction in grasping experiments on the real robot. On average, grasp\nsuccess rate increases from 63.3% to 70.4% after a single exploratory touch and\nto 82.7% after five touches. The collected data and code are publicly available\n(https://osf.io/j6rkd/, https://github.com/ctu-vras/vishac)\n", "link": "http://arxiv.org/abs/2303.04700v2", "date": "2024-09-10", "relevancy": 2.3508, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.608}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5868}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Visuo-Haptic%20Object%20Shape%20Completion%20for%20Robot%20Manipulation&body=Title%3A%20Efficient%20Visuo-Haptic%20Object%20Shape%20Completion%20for%20Robot%20Manipulation%0AAuthor%3A%20Lukas%20Rustler%20and%20Jiri%20Matas%20and%20Matej%20Hoffmann%0AAbstract%3A%20%20%20For%20robot%20manipulation%2C%20a%20complete%20and%20accurate%20object%20shape%20is%20desirable.%0AHere%2C%20we%20present%20a%20method%20that%20combines%20visual%20and%20haptic%20reconstruction%20in%20a%0Aclosed-loop%20pipeline.%20From%20an%20initial%20viewpoint%2C%20the%20object%20shape%20is%0Areconstructed%20using%20an%20implicit%20surface%20deep%20neural%20network.%20The%20location%20with%0Ahighest%20uncertainty%20is%20selected%20for%20haptic%20exploration%2C%20the%20object%20is%20touched%2C%0Athe%20new%20information%20from%20touch%20and%20a%20new%20point%20cloud%20from%20the%20camera%20are%20added%2C%0Aobject%20position%20is%20re-estimated%20and%20the%20cycle%20is%20repeated.%20We%20extend%20Rustler%20et%0Aal.%20%282022%29%20by%20using%20a%20new%20theoretically%20grounded%20method%20to%20determine%20the%20points%0Awith%20highest%20uncertainty%2C%20and%20we%20increase%20the%20yield%20of%20every%20haptic%20exploration%0Aby%20adding%20not%20only%20the%20contact%20points%20to%20the%20point%20cloud%20but%20also%20incorporating%0Athe%20empty%20space%20established%20through%20the%20robot%20movement%20to%20the%20object.%0AAdditionally%2C%20the%20solution%20is%20compact%20in%20that%20the%20jaws%20of%20a%20closed%20two-finger%0Agripper%20are%20directly%20used%20for%20exploration.%20The%20object%20position%20is%20re-estimated%0Aafter%20every%20robot%20action%20and%20multiple%20objects%20can%20be%20present%20simultaneously%20on%0Athe%20table.%20We%20achieve%20a%20steady%20improvement%20with%20every%20touch%20using%20three%0Adifferent%20metrics%20and%20demonstrate%20the%20utility%20of%20the%20better%20shape%0Areconstruction%20in%20grasping%20experiments%20on%20the%20real%20robot.%20On%20average%2C%20grasp%0Asuccess%20rate%20increases%20from%2063.3%25%20to%2070.4%25%20after%20a%20single%20exploratory%20touch%20and%0Ato%2082.7%25%20after%20five%20touches.%20The%20collected%20data%20and%20code%20are%20publicly%20available%0A%28https%3A//osf.io/j6rkd/%2C%20https%3A//github.com/ctu-vras/vishac%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.04700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Visuo-Haptic%2520Object%2520Shape%2520Completion%2520for%2520Robot%2520Manipulation%26entry.906535625%3DLukas%2520Rustler%2520and%2520Jiri%2520Matas%2520and%2520Matej%2520Hoffmann%26entry.1292438233%3D%2520%2520For%2520robot%2520manipulation%252C%2520a%2520complete%2520and%2520accurate%2520object%2520shape%2520is%2520desirable.%250AHere%252C%2520we%2520present%2520a%2520method%2520that%2520combines%2520visual%2520and%2520haptic%2520reconstruction%2520in%2520a%250Aclosed-loop%2520pipeline.%2520From%2520an%2520initial%2520viewpoint%252C%2520the%2520object%2520shape%2520is%250Areconstructed%2520using%2520an%2520implicit%2520surface%2520deep%2520neural%2520network.%2520The%2520location%2520with%250Ahighest%2520uncertainty%2520is%2520selected%2520for%2520haptic%2520exploration%252C%2520the%2520object%2520is%2520touched%252C%250Athe%2520new%2520information%2520from%2520touch%2520and%2520a%2520new%2520point%2520cloud%2520from%2520the%2520camera%2520are%2520added%252C%250Aobject%2520position%2520is%2520re-estimated%2520and%2520the%2520cycle%2520is%2520repeated.%2520We%2520extend%2520Rustler%2520et%250Aal.%2520%25282022%2529%2520by%2520using%2520a%2520new%2520theoretically%2520grounded%2520method%2520to%2520determine%2520the%2520points%250Awith%2520highest%2520uncertainty%252C%2520and%2520we%2520increase%2520the%2520yield%2520of%2520every%2520haptic%2520exploration%250Aby%2520adding%2520not%2520only%2520the%2520contact%2520points%2520to%2520the%2520point%2520cloud%2520but%2520also%2520incorporating%250Athe%2520empty%2520space%2520established%2520through%2520the%2520robot%2520movement%2520to%2520the%2520object.%250AAdditionally%252C%2520the%2520solution%2520is%2520compact%2520in%2520that%2520the%2520jaws%2520of%2520a%2520closed%2520two-finger%250Agripper%2520are%2520directly%2520used%2520for%2520exploration.%2520The%2520object%2520position%2520is%2520re-estimated%250Aafter%2520every%2520robot%2520action%2520and%2520multiple%2520objects%2520can%2520be%2520present%2520simultaneously%2520on%250Athe%2520table.%2520We%2520achieve%2520a%2520steady%2520improvement%2520with%2520every%2520touch%2520using%2520three%250Adifferent%2520metrics%2520and%2520demonstrate%2520the%2520utility%2520of%2520the%2520better%2520shape%250Areconstruction%2520in%2520grasping%2520experiments%2520on%2520the%2520real%2520robot.%2520On%2520average%252C%2520grasp%250Asuccess%2520rate%2520increases%2520from%252063.3%2525%2520to%252070.4%2525%2520after%2520a%2520single%2520exploratory%2520touch%2520and%250Ato%252082.7%2525%2520after%2520five%2520touches.%2520The%2520collected%2520data%2520and%2520code%2520are%2520publicly%2520available%250A%2528https%253A//osf.io/j6rkd/%252C%2520https%253A//github.com/ctu-vras/vishac%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.04700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Visuo-Haptic%20Object%20Shape%20Completion%20for%20Robot%20Manipulation&entry.906535625=Lukas%20Rustler%20and%20Jiri%20Matas%20and%20Matej%20Hoffmann&entry.1292438233=%20%20For%20robot%20manipulation%2C%20a%20complete%20and%20accurate%20object%20shape%20is%20desirable.%0AHere%2C%20we%20present%20a%20method%20that%20combines%20visual%20and%20haptic%20reconstruction%20in%20a%0Aclosed-loop%20pipeline.%20From%20an%20initial%20viewpoint%2C%20the%20object%20shape%20is%0Areconstructed%20using%20an%20implicit%20surface%20deep%20neural%20network.%20The%20location%20with%0Ahighest%20uncertainty%20is%20selected%20for%20haptic%20exploration%2C%20the%20object%20is%20touched%2C%0Athe%20new%20information%20from%20touch%20and%20a%20new%20point%20cloud%20from%20the%20camera%20are%20added%2C%0Aobject%20position%20is%20re-estimated%20and%20the%20cycle%20is%20repeated.%20We%20extend%20Rustler%20et%0Aal.%20%282022%29%20by%20using%20a%20new%20theoretically%20grounded%20method%20to%20determine%20the%20points%0Awith%20highest%20uncertainty%2C%20and%20we%20increase%20the%20yield%20of%20every%20haptic%20exploration%0Aby%20adding%20not%20only%20the%20contact%20points%20to%20the%20point%20cloud%20but%20also%20incorporating%0Athe%20empty%20space%20established%20through%20the%20robot%20movement%20to%20the%20object.%0AAdditionally%2C%20the%20solution%20is%20compact%20in%20that%20the%20jaws%20of%20a%20closed%20two-finger%0Agripper%20are%20directly%20used%20for%20exploration.%20The%20object%20position%20is%20re-estimated%0Aafter%20every%20robot%20action%20and%20multiple%20objects%20can%20be%20present%20simultaneously%20on%0Athe%20table.%20We%20achieve%20a%20steady%20improvement%20with%20every%20touch%20using%20three%0Adifferent%20metrics%20and%20demonstrate%20the%20utility%20of%20the%20better%20shape%0Areconstruction%20in%20grasping%20experiments%20on%20the%20real%20robot.%20On%20average%2C%20grasp%0Asuccess%20rate%20increases%20from%2063.3%25%20to%2070.4%25%20after%20a%20single%20exploratory%20touch%20and%0Ato%2082.7%25%20after%20five%20touches.%20The%20collected%20data%20and%20code%20are%20publicly%20available%0A%28https%3A//osf.io/j6rkd/%2C%20https%3A//github.com/ctu-vras/vishac%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.04700v2&entry.124074799=Read"},
{"title": "Sources of Uncertainty in 3D Scene Reconstruction", "author": "Marcus Klasson and Riccardo Mereu and Juho Kannala and Arno Solin", "abstract": "  The process of 3D scene reconstruction can be affected by numerous\nuncertainty sources in real-world scenes. While Neural Radiance Fields (NeRFs)\nand 3D Gaussian Splatting (GS) achieve high-fidelity rendering, they lack\nbuilt-in mechanisms to directly address or quantify uncertainties arising from\nthe presence of noise, occlusions, confounding outliers, and imprecise camera\npose inputs. In this paper, we introduce a taxonomy that categorizes different\nsources of uncertainty inherent in these methods. Moreover, we extend NeRF- and\nGS-based methods with uncertainty estimation techniques, including learning\nuncertainty outputs and ensembles, and perform an empirical study to assess\ntheir ability to capture the sensitivity of the reconstruction. Our study\nhighlights the need for addressing various uncertainty aspects when designing\nNeRF/GS-based methods for uncertainty-aware 3D reconstruction.\n", "link": "http://arxiv.org/abs/2409.06407v1", "date": "2024-09-10", "relevancy": 2.3484, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6015}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5878}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sources%20of%20Uncertainty%20in%203D%20Scene%20Reconstruction&body=Title%3A%20Sources%20of%20Uncertainty%20in%203D%20Scene%20Reconstruction%0AAuthor%3A%20Marcus%20Klasson%20and%20Riccardo%20Mereu%20and%20Juho%20Kannala%20and%20Arno%20Solin%0AAbstract%3A%20%20%20The%20process%20of%203D%20scene%20reconstruction%20can%20be%20affected%20by%20numerous%0Auncertainty%20sources%20in%20real-world%20scenes.%20While%20Neural%20Radiance%20Fields%20%28NeRFs%29%0Aand%203D%20Gaussian%20Splatting%20%28GS%29%20achieve%20high-fidelity%20rendering%2C%20they%20lack%0Abuilt-in%20mechanisms%20to%20directly%20address%20or%20quantify%20uncertainties%20arising%20from%0Athe%20presence%20of%20noise%2C%20occlusions%2C%20confounding%20outliers%2C%20and%20imprecise%20camera%0Apose%20inputs.%20In%20this%20paper%2C%20we%20introduce%20a%20taxonomy%20that%20categorizes%20different%0Asources%20of%20uncertainty%20inherent%20in%20these%20methods.%20Moreover%2C%20we%20extend%20NeRF-%20and%0AGS-based%20methods%20with%20uncertainty%20estimation%20techniques%2C%20including%20learning%0Auncertainty%20outputs%20and%20ensembles%2C%20and%20perform%20an%20empirical%20study%20to%20assess%0Atheir%20ability%20to%20capture%20the%20sensitivity%20of%20the%20reconstruction.%20Our%20study%0Ahighlights%20the%20need%20for%20addressing%20various%20uncertainty%20aspects%20when%20designing%0ANeRF/GS-based%20methods%20for%20uncertainty-aware%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSources%2520of%2520Uncertainty%2520in%25203D%2520Scene%2520Reconstruction%26entry.906535625%3DMarcus%2520Klasson%2520and%2520Riccardo%2520Mereu%2520and%2520Juho%2520Kannala%2520and%2520Arno%2520Solin%26entry.1292438233%3D%2520%2520The%2520process%2520of%25203D%2520scene%2520reconstruction%2520can%2520be%2520affected%2520by%2520numerous%250Auncertainty%2520sources%2520in%2520real-world%2520scenes.%2520While%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%250Aand%25203D%2520Gaussian%2520Splatting%2520%2528GS%2529%2520achieve%2520high-fidelity%2520rendering%252C%2520they%2520lack%250Abuilt-in%2520mechanisms%2520to%2520directly%2520address%2520or%2520quantify%2520uncertainties%2520arising%2520from%250Athe%2520presence%2520of%2520noise%252C%2520occlusions%252C%2520confounding%2520outliers%252C%2520and%2520imprecise%2520camera%250Apose%2520inputs.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520taxonomy%2520that%2520categorizes%2520different%250Asources%2520of%2520uncertainty%2520inherent%2520in%2520these%2520methods.%2520Moreover%252C%2520we%2520extend%2520NeRF-%2520and%250AGS-based%2520methods%2520with%2520uncertainty%2520estimation%2520techniques%252C%2520including%2520learning%250Auncertainty%2520outputs%2520and%2520ensembles%252C%2520and%2520perform%2520an%2520empirical%2520study%2520to%2520assess%250Atheir%2520ability%2520to%2520capture%2520the%2520sensitivity%2520of%2520the%2520reconstruction.%2520Our%2520study%250Ahighlights%2520the%2520need%2520for%2520addressing%2520various%2520uncertainty%2520aspects%2520when%2520designing%250ANeRF/GS-based%2520methods%2520for%2520uncertainty-aware%25203D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sources%20of%20Uncertainty%20in%203D%20Scene%20Reconstruction&entry.906535625=Marcus%20Klasson%20and%20Riccardo%20Mereu%20and%20Juho%20Kannala%20and%20Arno%20Solin&entry.1292438233=%20%20The%20process%20of%203D%20scene%20reconstruction%20can%20be%20affected%20by%20numerous%0Auncertainty%20sources%20in%20real-world%20scenes.%20While%20Neural%20Radiance%20Fields%20%28NeRFs%29%0Aand%203D%20Gaussian%20Splatting%20%28GS%29%20achieve%20high-fidelity%20rendering%2C%20they%20lack%0Abuilt-in%20mechanisms%20to%20directly%20address%20or%20quantify%20uncertainties%20arising%20from%0Athe%20presence%20of%20noise%2C%20occlusions%2C%20confounding%20outliers%2C%20and%20imprecise%20camera%0Apose%20inputs.%20In%20this%20paper%2C%20we%20introduce%20a%20taxonomy%20that%20categorizes%20different%0Asources%20of%20uncertainty%20inherent%20in%20these%20methods.%20Moreover%2C%20we%20extend%20NeRF-%20and%0AGS-based%20methods%20with%20uncertainty%20estimation%20techniques%2C%20including%20learning%0Auncertainty%20outputs%20and%20ensembles%2C%20and%20perform%20an%20empirical%20study%20to%20assess%0Atheir%20ability%20to%20capture%20the%20sensitivity%20of%20the%20reconstruction.%20Our%20study%0Ahighlights%20the%20need%20for%20addressing%20various%20uncertainty%20aspects%20when%20designing%0ANeRF/GS-based%20methods%20for%20uncertainty-aware%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06407v1&entry.124074799=Read"},
{"title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding\n  Benchmark", "author": "Xiang Yue and Tianyu Zheng and Yuansheng Ni and Yubo Wang and Kai Zhang and Shengbang Tong and Yuxuan Sun and Botao Yu and Ge Zhang and Huan Sun and Yu Su and Wenhu Chen and Graham Neubig", "abstract": "  This paper introduces MMMU-Pro, a robust version of the Massive\nMulti-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.\nMMMU-Pro rigorously assesses multimodal models' true understanding and\nreasoning capabilities through a three-step process based on MMMU: (1)\nfiltering out questions answerable by text-only models, (2) augmenting\ncandidate options, and (3) introducing a vision-only input setting where\nquestions are embedded within images. This setting challenges AI to truly \"see\"\nand \"read\" simultaneously, testing a fundamental human cognitive skill of\nseamlessly integrating visual and textual information. Results show that model\nperformance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%\nto 26.9% across models. We explore the impact of OCR prompts and Chain of\nThought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT\ngenerally improves performance. MMMU-Pro provides a more rigorous evaluation\ntool, closely mimicking real-world scenarios and offering valuable directions\nfor future research in multimodal AI.\n", "link": "http://arxiv.org/abs/2409.02813v2", "date": "2024-09-10", "relevancy": 2.3371, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMMU-Pro%3A%20A%20More%20Robust%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark&body=Title%3A%20MMMU-Pro%3A%20A%20More%20Robust%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark%0AAuthor%3A%20Xiang%20Yue%20and%20Tianyu%20Zheng%20and%20Yuansheng%20Ni%20and%20Yubo%20Wang%20and%20Kai%20Zhang%20and%20Shengbang%20Tong%20and%20Yuxuan%20Sun%20and%20Botao%20Yu%20and%20Ge%20Zhang%20and%20Huan%20Sun%20and%20Yu%20Su%20and%20Wenhu%20Chen%20and%20Graham%20Neubig%0AAbstract%3A%20%20%20This%20paper%20introduces%20MMMU-Pro%2C%20a%20robust%20version%20of%20the%20Massive%0AMulti-discipline%20Multimodal%20Understanding%20and%20Reasoning%20%28MMMU%29%20benchmark.%0AMMMU-Pro%20rigorously%20assesses%20multimodal%20models%27%20true%20understanding%20and%0Areasoning%20capabilities%20through%20a%20three-step%20process%20based%20on%20MMMU%3A%20%281%29%0Afiltering%20out%20questions%20answerable%20by%20text-only%20models%2C%20%282%29%20augmenting%0Acandidate%20options%2C%20and%20%283%29%20introducing%20a%20vision-only%20input%20setting%20where%0Aquestions%20are%20embedded%20within%20images.%20This%20setting%20challenges%20AI%20to%20truly%20%22see%22%0Aand%20%22read%22%20simultaneously%2C%20testing%20a%20fundamental%20human%20cognitive%20skill%20of%0Aseamlessly%20integrating%20visual%20and%20textual%20information.%20Results%20show%20that%20model%0Aperformance%20is%20substantially%20lower%20on%20MMMU-Pro%20than%20on%20MMMU%2C%20ranging%20from%2016.8%25%0Ato%2026.9%25%20across%20models.%20We%20explore%20the%20impact%20of%20OCR%20prompts%20and%20Chain%20of%0AThought%20%28CoT%29%20reasoning%2C%20finding%20that%20OCR%20prompts%20have%20minimal%20effect%20while%20CoT%0Agenerally%20improves%20performance.%20MMMU-Pro%20provides%20a%20more%20rigorous%20evaluation%0Atool%2C%20closely%20mimicking%20real-world%20scenarios%20and%20offering%20valuable%20directions%0Afor%20future%20research%20in%20multimodal%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMMU-Pro%253A%2520A%2520More%2520Robust%2520Multi-discipline%2520Multimodal%2520Understanding%250A%2520%2520Benchmark%26entry.906535625%3DXiang%2520Yue%2520and%2520Tianyu%2520Zheng%2520and%2520Yuansheng%2520Ni%2520and%2520Yubo%2520Wang%2520and%2520Kai%2520Zhang%2520and%2520Shengbang%2520Tong%2520and%2520Yuxuan%2520Sun%2520and%2520Botao%2520Yu%2520and%2520Ge%2520Zhang%2520and%2520Huan%2520Sun%2520and%2520Yu%2520Su%2520and%2520Wenhu%2520Chen%2520and%2520Graham%2520Neubig%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MMMU-Pro%252C%2520a%2520robust%2520version%2520of%2520the%2520Massive%250AMulti-discipline%2520Multimodal%2520Understanding%2520and%2520Reasoning%2520%2528MMMU%2529%2520benchmark.%250AMMMU-Pro%2520rigorously%2520assesses%2520multimodal%2520models%2527%2520true%2520understanding%2520and%250Areasoning%2520capabilities%2520through%2520a%2520three-step%2520process%2520based%2520on%2520MMMU%253A%2520%25281%2529%250Afiltering%2520out%2520questions%2520answerable%2520by%2520text-only%2520models%252C%2520%25282%2529%2520augmenting%250Acandidate%2520options%252C%2520and%2520%25283%2529%2520introducing%2520a%2520vision-only%2520input%2520setting%2520where%250Aquestions%2520are%2520embedded%2520within%2520images.%2520This%2520setting%2520challenges%2520AI%2520to%2520truly%2520%2522see%2522%250Aand%2520%2522read%2522%2520simultaneously%252C%2520testing%2520a%2520fundamental%2520human%2520cognitive%2520skill%2520of%250Aseamlessly%2520integrating%2520visual%2520and%2520textual%2520information.%2520Results%2520show%2520that%2520model%250Aperformance%2520is%2520substantially%2520lower%2520on%2520MMMU-Pro%2520than%2520on%2520MMMU%252C%2520ranging%2520from%252016.8%2525%250Ato%252026.9%2525%2520across%2520models.%2520We%2520explore%2520the%2520impact%2520of%2520OCR%2520prompts%2520and%2520Chain%2520of%250AThought%2520%2528CoT%2529%2520reasoning%252C%2520finding%2520that%2520OCR%2520prompts%2520have%2520minimal%2520effect%2520while%2520CoT%250Agenerally%2520improves%2520performance.%2520MMMU-Pro%2520provides%2520a%2520more%2520rigorous%2520evaluation%250Atool%252C%2520closely%2520mimicking%2520real-world%2520scenarios%2520and%2520offering%2520valuable%2520directions%250Afor%2520future%2520research%2520in%2520multimodal%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMMU-Pro%3A%20A%20More%20Robust%20Multi-discipline%20Multimodal%20Understanding%0A%20%20Benchmark&entry.906535625=Xiang%20Yue%20and%20Tianyu%20Zheng%20and%20Yuansheng%20Ni%20and%20Yubo%20Wang%20and%20Kai%20Zhang%20and%20Shengbang%20Tong%20and%20Yuxuan%20Sun%20and%20Botao%20Yu%20and%20Ge%20Zhang%20and%20Huan%20Sun%20and%20Yu%20Su%20and%20Wenhu%20Chen%20and%20Graham%20Neubig&entry.1292438233=%20%20This%20paper%20introduces%20MMMU-Pro%2C%20a%20robust%20version%20of%20the%20Massive%0AMulti-discipline%20Multimodal%20Understanding%20and%20Reasoning%20%28MMMU%29%20benchmark.%0AMMMU-Pro%20rigorously%20assesses%20multimodal%20models%27%20true%20understanding%20and%0Areasoning%20capabilities%20through%20a%20three-step%20process%20based%20on%20MMMU%3A%20%281%29%0Afiltering%20out%20questions%20answerable%20by%20text-only%20models%2C%20%282%29%20augmenting%0Acandidate%20options%2C%20and%20%283%29%20introducing%20a%20vision-only%20input%20setting%20where%0Aquestions%20are%20embedded%20within%20images.%20This%20setting%20challenges%20AI%20to%20truly%20%22see%22%0Aand%20%22read%22%20simultaneously%2C%20testing%20a%20fundamental%20human%20cognitive%20skill%20of%0Aseamlessly%20integrating%20visual%20and%20textual%20information.%20Results%20show%20that%20model%0Aperformance%20is%20substantially%20lower%20on%20MMMU-Pro%20than%20on%20MMMU%2C%20ranging%20from%2016.8%25%0Ato%2026.9%25%20across%20models.%20We%20explore%20the%20impact%20of%20OCR%20prompts%20and%20Chain%20of%0AThought%20%28CoT%29%20reasoning%2C%20finding%20that%20OCR%20prompts%20have%20minimal%20effect%20while%20CoT%0Agenerally%20improves%20performance.%20MMMU-Pro%20provides%20a%20more%20rigorous%20evaluation%0Atool%2C%20closely%20mimicking%20real-world%20scenarios%20and%20offering%20valuable%20directions%0Afor%20future%20research%20in%20multimodal%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02813v2&entry.124074799=Read"},
{"title": "Deep Visual Odometry with Events and Frames", "author": "Roberto Pellerito and Marco Cannici and Daniel Gehrig and Joris Belhadj and Olivier Dubois-Matra and Massimo Casasco and Davide Scaramuzza", "abstract": "  Visual Odometry (VO) is crucial for autonomous robotic navigation, especially\nin GPS-denied environments like planetary terrains. To improve robustness,\nrecent model-based VO systems have begun combining standard and event-based\ncameras. While event cameras excel in low-light and high-speed motion, standard\ncameras provide dense and easier-to-track features. However, the field of\nimage- and event-based VO still predominantly relies on model-based methods and\nis yet to fully integrate recent image-only advancements leveraging end-to-end\nlearning-based architectures. Seamlessly integrating the two modalities remains\nchallenging due to their different nature, one asynchronous, the other not,\nlimiting the potential for a more effective image- and event-based VO. We\nintroduce RAMP-VO, the first end-to-end learned image- and event-based VO\nsystem. It leverages novel Recurrent, Asynchronous, and Massively Parallel\n(RAMP) encoders capable of fusing asynchronous events with image data,\nproviding 8x faster inference and 33% more accurate predictions than existing\nsolutions. Despite being trained only in simulation, RAMP-VO outperforms\nprevious methods on the newly introduced Apollo and Malapert datasets, and on\nexisting benchmarks, where it improves image- and event-based methods by 58.8%\nand 30.6%, paving the way for robust and asynchronous VO in space.\n", "link": "http://arxiv.org/abs/2309.09947v3", "date": "2024-09-10", "relevancy": 2.3366, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6013}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5932}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Visual%20Odometry%20with%20Events%20and%20Frames&body=Title%3A%20Deep%20Visual%20Odometry%20with%20Events%20and%20Frames%0AAuthor%3A%20Roberto%20Pellerito%20and%20Marco%20Cannici%20and%20Daniel%20Gehrig%20and%20Joris%20Belhadj%20and%20Olivier%20Dubois-Matra%20and%20Massimo%20Casasco%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Visual%20Odometry%20%28VO%29%20is%20crucial%20for%20autonomous%20robotic%20navigation%2C%20especially%0Ain%20GPS-denied%20environments%20like%20planetary%20terrains.%20To%20improve%20robustness%2C%0Arecent%20model-based%20VO%20systems%20have%20begun%20combining%20standard%20and%20event-based%0Acameras.%20While%20event%20cameras%20excel%20in%20low-light%20and%20high-speed%20motion%2C%20standard%0Acameras%20provide%20dense%20and%20easier-to-track%20features.%20However%2C%20the%20field%20of%0Aimage-%20and%20event-based%20VO%20still%20predominantly%20relies%20on%20model-based%20methods%20and%0Ais%20yet%20to%20fully%20integrate%20recent%20image-only%20advancements%20leveraging%20end-to-end%0Alearning-based%20architectures.%20Seamlessly%20integrating%20the%20two%20modalities%20remains%0Achallenging%20due%20to%20their%20different%20nature%2C%20one%20asynchronous%2C%20the%20other%20not%2C%0Alimiting%20the%20potential%20for%20a%20more%20effective%20image-%20and%20event-based%20VO.%20We%0Aintroduce%20RAMP-VO%2C%20the%20first%20end-to-end%20learned%20image-%20and%20event-based%20VO%0Asystem.%20It%20leverages%20novel%20Recurrent%2C%20Asynchronous%2C%20and%20Massively%20Parallel%0A%28RAMP%29%20encoders%20capable%20of%20fusing%20asynchronous%20events%20with%20image%20data%2C%0Aproviding%208x%20faster%20inference%20and%2033%25%20more%20accurate%20predictions%20than%20existing%0Asolutions.%20Despite%20being%20trained%20only%20in%20simulation%2C%20RAMP-VO%20outperforms%0Aprevious%20methods%20on%20the%20newly%20introduced%20Apollo%20and%20Malapert%20datasets%2C%20and%20on%0Aexisting%20benchmarks%2C%20where%20it%20improves%20image-%20and%20event-based%20methods%20by%2058.8%25%0Aand%2030.6%25%2C%20paving%20the%20way%20for%20robust%20and%20asynchronous%20VO%20in%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09947v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Visual%2520Odometry%2520with%2520Events%2520and%2520Frames%26entry.906535625%3DRoberto%2520Pellerito%2520and%2520Marco%2520Cannici%2520and%2520Daniel%2520Gehrig%2520and%2520Joris%2520Belhadj%2520and%2520Olivier%2520Dubois-Matra%2520and%2520Massimo%2520Casasco%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Visual%2520Odometry%2520%2528VO%2529%2520is%2520crucial%2520for%2520autonomous%2520robotic%2520navigation%252C%2520especially%250Ain%2520GPS-denied%2520environments%2520like%2520planetary%2520terrains.%2520To%2520improve%2520robustness%252C%250Arecent%2520model-based%2520VO%2520systems%2520have%2520begun%2520combining%2520standard%2520and%2520event-based%250Acameras.%2520While%2520event%2520cameras%2520excel%2520in%2520low-light%2520and%2520high-speed%2520motion%252C%2520standard%250Acameras%2520provide%2520dense%2520and%2520easier-to-track%2520features.%2520However%252C%2520the%2520field%2520of%250Aimage-%2520and%2520event-based%2520VO%2520still%2520predominantly%2520relies%2520on%2520model-based%2520methods%2520and%250Ais%2520yet%2520to%2520fully%2520integrate%2520recent%2520image-only%2520advancements%2520leveraging%2520end-to-end%250Alearning-based%2520architectures.%2520Seamlessly%2520integrating%2520the%2520two%2520modalities%2520remains%250Achallenging%2520due%2520to%2520their%2520different%2520nature%252C%2520one%2520asynchronous%252C%2520the%2520other%2520not%252C%250Alimiting%2520the%2520potential%2520for%2520a%2520more%2520effective%2520image-%2520and%2520event-based%2520VO.%2520We%250Aintroduce%2520RAMP-VO%252C%2520the%2520first%2520end-to-end%2520learned%2520image-%2520and%2520event-based%2520VO%250Asystem.%2520It%2520leverages%2520novel%2520Recurrent%252C%2520Asynchronous%252C%2520and%2520Massively%2520Parallel%250A%2528RAMP%2529%2520encoders%2520capable%2520of%2520fusing%2520asynchronous%2520events%2520with%2520image%2520data%252C%250Aproviding%25208x%2520faster%2520inference%2520and%252033%2525%2520more%2520accurate%2520predictions%2520than%2520existing%250Asolutions.%2520Despite%2520being%2520trained%2520only%2520in%2520simulation%252C%2520RAMP-VO%2520outperforms%250Aprevious%2520methods%2520on%2520the%2520newly%2520introduced%2520Apollo%2520and%2520Malapert%2520datasets%252C%2520and%2520on%250Aexisting%2520benchmarks%252C%2520where%2520it%2520improves%2520image-%2520and%2520event-based%2520methods%2520by%252058.8%2525%250Aand%252030.6%2525%252C%2520paving%2520the%2520way%2520for%2520robust%2520and%2520asynchronous%2520VO%2520in%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09947v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Visual%20Odometry%20with%20Events%20and%20Frames&entry.906535625=Roberto%20Pellerito%20and%20Marco%20Cannici%20and%20Daniel%20Gehrig%20and%20Joris%20Belhadj%20and%20Olivier%20Dubois-Matra%20and%20Massimo%20Casasco%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Visual%20Odometry%20%28VO%29%20is%20crucial%20for%20autonomous%20robotic%20navigation%2C%20especially%0Ain%20GPS-denied%20environments%20like%20planetary%20terrains.%20To%20improve%20robustness%2C%0Arecent%20model-based%20VO%20systems%20have%20begun%20combining%20standard%20and%20event-based%0Acameras.%20While%20event%20cameras%20excel%20in%20low-light%20and%20high-speed%20motion%2C%20standard%0Acameras%20provide%20dense%20and%20easier-to-track%20features.%20However%2C%20the%20field%20of%0Aimage-%20and%20event-based%20VO%20still%20predominantly%20relies%20on%20model-based%20methods%20and%0Ais%20yet%20to%20fully%20integrate%20recent%20image-only%20advancements%20leveraging%20end-to-end%0Alearning-based%20architectures.%20Seamlessly%20integrating%20the%20two%20modalities%20remains%0Achallenging%20due%20to%20their%20different%20nature%2C%20one%20asynchronous%2C%20the%20other%20not%2C%0Alimiting%20the%20potential%20for%20a%20more%20effective%20image-%20and%20event-based%20VO.%20We%0Aintroduce%20RAMP-VO%2C%20the%20first%20end-to-end%20learned%20image-%20and%20event-based%20VO%0Asystem.%20It%20leverages%20novel%20Recurrent%2C%20Asynchronous%2C%20and%20Massively%20Parallel%0A%28RAMP%29%20encoders%20capable%20of%20fusing%20asynchronous%20events%20with%20image%20data%2C%0Aproviding%208x%20faster%20inference%20and%2033%25%20more%20accurate%20predictions%20than%20existing%0Asolutions.%20Despite%20being%20trained%20only%20in%20simulation%2C%20RAMP-VO%20outperforms%0Aprevious%20methods%20on%20the%20newly%20introduced%20Apollo%20and%20Malapert%20datasets%2C%20and%20on%0Aexisting%20benchmarks%2C%20where%20it%20improves%20image-%20and%20event-based%20methods%20by%2058.8%25%0Aand%2030.6%25%2C%20paving%20the%20way%20for%20robust%20and%20asynchronous%20VO%20in%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09947v3&entry.124074799=Read"},
{"title": "A Practical Gated Recurrent Transformer Network Incorporating Multiple\n  Fusions for Video Denoising", "author": "Kai Guo and Seungwon Choi and Jongseong Choi and Lae-Hoon Kim", "abstract": "  State-of-the-art (SOTA) video denoising methods employ multi-frame\nsimultaneous denoising mechanisms, resulting in significant delays (e.g., 16\nframes), making them impractical for real-time cameras. To overcome this\nlimitation, we propose a multi-fusion gated recurrent Transformer network\n(GRTN) that achieves SOTA denoising performance with only a single-frame delay.\nSpecifically, the spatial denoising module extracts features from the current\nframe, while the reset gate selects relevant information from the previous\nframe and fuses it with current frame features via the temporal denoising\nmodule. The update gate then further blends this result with the previous frame\nfeatures, and the reconstruction module integrates it with the current frame.\nTo robustly compute attention for noisy features, we propose a residual\nsimplified Swin Transformer with Euclidean distance (RSSTE) in the spatial and\ntemporal denoising modules. Comparative objective and subjective results show\nthat our GRTN achieves denoising performance comparable to SOTA multi-frame\ndelay networks, with only a single-frame delay.\n", "link": "http://arxiv.org/abs/2409.06603v1", "date": "2024-09-10", "relevancy": 2.336, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5902}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.583}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Practical%20Gated%20Recurrent%20Transformer%20Network%20Incorporating%20Multiple%0A%20%20Fusions%20for%20Video%20Denoising&body=Title%3A%20A%20Practical%20Gated%20Recurrent%20Transformer%20Network%20Incorporating%20Multiple%0A%20%20Fusions%20for%20Video%20Denoising%0AAuthor%3A%20Kai%20Guo%20and%20Seungwon%20Choi%20and%20Jongseong%20Choi%20and%20Lae-Hoon%20Kim%0AAbstract%3A%20%20%20State-of-the-art%20%28SOTA%29%20video%20denoising%20methods%20employ%20multi-frame%0Asimultaneous%20denoising%20mechanisms%2C%20resulting%20in%20significant%20delays%20%28e.g.%2C%2016%0Aframes%29%2C%20making%20them%20impractical%20for%20real-time%20cameras.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20a%20multi-fusion%20gated%20recurrent%20Transformer%20network%0A%28GRTN%29%20that%20achieves%20SOTA%20denoising%20performance%20with%20only%20a%20single-frame%20delay.%0ASpecifically%2C%20the%20spatial%20denoising%20module%20extracts%20features%20from%20the%20current%0Aframe%2C%20while%20the%20reset%20gate%20selects%20relevant%20information%20from%20the%20previous%0Aframe%20and%20fuses%20it%20with%20current%20frame%20features%20via%20the%20temporal%20denoising%0Amodule.%20The%20update%20gate%20then%20further%20blends%20this%20result%20with%20the%20previous%20frame%0Afeatures%2C%20and%20the%20reconstruction%20module%20integrates%20it%20with%20the%20current%20frame.%0ATo%20robustly%20compute%20attention%20for%20noisy%20features%2C%20we%20propose%20a%20residual%0Asimplified%20Swin%20Transformer%20with%20Euclidean%20distance%20%28RSSTE%29%20in%20the%20spatial%20and%0Atemporal%20denoising%20modules.%20Comparative%20objective%20and%20subjective%20results%20show%0Athat%20our%20GRTN%20achieves%20denoising%20performance%20comparable%20to%20SOTA%20multi-frame%0Adelay%20networks%2C%20with%20only%20a%20single-frame%20delay.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Practical%2520Gated%2520Recurrent%2520Transformer%2520Network%2520Incorporating%2520Multiple%250A%2520%2520Fusions%2520for%2520Video%2520Denoising%26entry.906535625%3DKai%2520Guo%2520and%2520Seungwon%2520Choi%2520and%2520Jongseong%2520Choi%2520and%2520Lae-Hoon%2520Kim%26entry.1292438233%3D%2520%2520State-of-the-art%2520%2528SOTA%2529%2520video%2520denoising%2520methods%2520employ%2520multi-frame%250Asimultaneous%2520denoising%2520mechanisms%252C%2520resulting%2520in%2520significant%2520delays%2520%2528e.g.%252C%252016%250Aframes%2529%252C%2520making%2520them%2520impractical%2520for%2520real-time%2520cameras.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520propose%2520a%2520multi-fusion%2520gated%2520recurrent%2520Transformer%2520network%250A%2528GRTN%2529%2520that%2520achieves%2520SOTA%2520denoising%2520performance%2520with%2520only%2520a%2520single-frame%2520delay.%250ASpecifically%252C%2520the%2520spatial%2520denoising%2520module%2520extracts%2520features%2520from%2520the%2520current%250Aframe%252C%2520while%2520the%2520reset%2520gate%2520selects%2520relevant%2520information%2520from%2520the%2520previous%250Aframe%2520and%2520fuses%2520it%2520with%2520current%2520frame%2520features%2520via%2520the%2520temporal%2520denoising%250Amodule.%2520The%2520update%2520gate%2520then%2520further%2520blends%2520this%2520result%2520with%2520the%2520previous%2520frame%250Afeatures%252C%2520and%2520the%2520reconstruction%2520module%2520integrates%2520it%2520with%2520the%2520current%2520frame.%250ATo%2520robustly%2520compute%2520attention%2520for%2520noisy%2520features%252C%2520we%2520propose%2520a%2520residual%250Asimplified%2520Swin%2520Transformer%2520with%2520Euclidean%2520distance%2520%2528RSSTE%2529%2520in%2520the%2520spatial%2520and%250Atemporal%2520denoising%2520modules.%2520Comparative%2520objective%2520and%2520subjective%2520results%2520show%250Athat%2520our%2520GRTN%2520achieves%2520denoising%2520performance%2520comparable%2520to%2520SOTA%2520multi-frame%250Adelay%2520networks%252C%2520with%2520only%2520a%2520single-frame%2520delay.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Practical%20Gated%20Recurrent%20Transformer%20Network%20Incorporating%20Multiple%0A%20%20Fusions%20for%20Video%20Denoising&entry.906535625=Kai%20Guo%20and%20Seungwon%20Choi%20and%20Jongseong%20Choi%20and%20Lae-Hoon%20Kim&entry.1292438233=%20%20State-of-the-art%20%28SOTA%29%20video%20denoising%20methods%20employ%20multi-frame%0Asimultaneous%20denoising%20mechanisms%2C%20resulting%20in%20significant%20delays%20%28e.g.%2C%2016%0Aframes%29%2C%20making%20them%20impractical%20for%20real-time%20cameras.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20a%20multi-fusion%20gated%20recurrent%20Transformer%20network%0A%28GRTN%29%20that%20achieves%20SOTA%20denoising%20performance%20with%20only%20a%20single-frame%20delay.%0ASpecifically%2C%20the%20spatial%20denoising%20module%20extracts%20features%20from%20the%20current%0Aframe%2C%20while%20the%20reset%20gate%20selects%20relevant%20information%20from%20the%20previous%0Aframe%20and%20fuses%20it%20with%20current%20frame%20features%20via%20the%20temporal%20denoising%0Amodule.%20The%20update%20gate%20then%20further%20blends%20this%20result%20with%20the%20previous%20frame%0Afeatures%2C%20and%20the%20reconstruction%20module%20integrates%20it%20with%20the%20current%20frame.%0ATo%20robustly%20compute%20attention%20for%20noisy%20features%2C%20we%20propose%20a%20residual%0Asimplified%20Swin%20Transformer%20with%20Euclidean%20distance%20%28RSSTE%29%20in%20the%20spatial%20and%0Atemporal%20denoising%20modules.%20Comparative%20objective%20and%20subjective%20results%20show%0Athat%20our%20GRTN%20achieves%20denoising%20performance%20comparable%20to%20SOTA%20multi-frame%0Adelay%20networks%2C%20with%20only%20a%20single-frame%20delay.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06603v1&entry.124074799=Read"},
{"title": "Semi-Supervised 3D Object Detection with Chanel Augmentation using\n  Transformation Equivariance", "author": "Minju Kang and Taehun Kong and Tae-Kyun Kim", "abstract": "  Accurate 3D object detection is crucial for autonomous vehicles and robots to\nnavigate and interact with the environment safely and effectively. Meanwhile,\nthe performance of 3D detector relies on the data size and annotation which is\nexpensive. Consequently, the demand of training with limited labeled data is\ngrowing. We explore a novel teacher-student framework employing channel\naugmentation for 3D semi-supervised object detection. The teacher-student SSL\ntypically adopts a weak augmentation and strong augmentation to teacher and\nstudent, respectively. In this work, we apply multiple channel augmentations to\nboth networks using the transformation equivariance detector (TED). The TED\nallows us to explore different combinations of augmentation on point clouds and\nefficiently aggregates multi-channel transformation equivariance features. In\nprinciple, by adopting fixed channel augmentations for the teacher network, the\nstudent can train stably on reliable pseudo-labels. Adopting strong channel\naugmentations can enrich the diversity of data, fostering robustness to\ntransformations and enhancing generalization performance of the student\nnetwork. We use SOTA hierarchical supervision as a baseline and adapt its\ndual-threshold to TED, which is called channel IoU consistency. We evaluate our\nmethod with KITTI dataset, and achieved a significant performance leap,\nsurpassing SOTA 3D semi-supervised object detection models.\n", "link": "http://arxiv.org/abs/2409.06583v1", "date": "2024-09-10", "relevancy": 2.3319, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5968}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5955}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%203D%20Object%20Detection%20with%20Chanel%20Augmentation%20using%0A%20%20Transformation%20Equivariance&body=Title%3A%20Semi-Supervised%203D%20Object%20Detection%20with%20Chanel%20Augmentation%20using%0A%20%20Transformation%20Equivariance%0AAuthor%3A%20Minju%20Kang%20and%20Taehun%20Kong%20and%20Tae-Kyun%20Kim%0AAbstract%3A%20%20%20Accurate%203D%20object%20detection%20is%20crucial%20for%20autonomous%20vehicles%20and%20robots%20to%0Anavigate%20and%20interact%20with%20the%20environment%20safely%20and%20effectively.%20Meanwhile%2C%0Athe%20performance%20of%203D%20detector%20relies%20on%20the%20data%20size%20and%20annotation%20which%20is%0Aexpensive.%20Consequently%2C%20the%20demand%20of%20training%20with%20limited%20labeled%20data%20is%0Agrowing.%20We%20explore%20a%20novel%20teacher-student%20framework%20employing%20channel%0Aaugmentation%20for%203D%20semi-supervised%20object%20detection.%20The%20teacher-student%20SSL%0Atypically%20adopts%20a%20weak%20augmentation%20and%20strong%20augmentation%20to%20teacher%20and%0Astudent%2C%20respectively.%20In%20this%20work%2C%20we%20apply%20multiple%20channel%20augmentations%20to%0Aboth%20networks%20using%20the%20transformation%20equivariance%20detector%20%28TED%29.%20The%20TED%0Aallows%20us%20to%20explore%20different%20combinations%20of%20augmentation%20on%20point%20clouds%20and%0Aefficiently%20aggregates%20multi-channel%20transformation%20equivariance%20features.%20In%0Aprinciple%2C%20by%20adopting%20fixed%20channel%20augmentations%20for%20the%20teacher%20network%2C%20the%0Astudent%20can%20train%20stably%20on%20reliable%20pseudo-labels.%20Adopting%20strong%20channel%0Aaugmentations%20can%20enrich%20the%20diversity%20of%20data%2C%20fostering%20robustness%20to%0Atransformations%20and%20enhancing%20generalization%20performance%20of%20the%20student%0Anetwork.%20We%20use%20SOTA%20hierarchical%20supervision%20as%20a%20baseline%20and%20adapt%20its%0Adual-threshold%20to%20TED%2C%20which%20is%20called%20channel%20IoU%20consistency.%20We%20evaluate%20our%0Amethod%20with%20KITTI%20dataset%2C%20and%20achieved%20a%20significant%20performance%20leap%2C%0Asurpassing%20SOTA%203D%20semi-supervised%20object%20detection%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%25203D%2520Object%2520Detection%2520with%2520Chanel%2520Augmentation%2520using%250A%2520%2520Transformation%2520Equivariance%26entry.906535625%3DMinju%2520Kang%2520and%2520Taehun%2520Kong%2520and%2520Tae-Kyun%2520Kim%26entry.1292438233%3D%2520%2520Accurate%25203D%2520object%2520detection%2520is%2520crucial%2520for%2520autonomous%2520vehicles%2520and%2520robots%2520to%250Anavigate%2520and%2520interact%2520with%2520the%2520environment%2520safely%2520and%2520effectively.%2520Meanwhile%252C%250Athe%2520performance%2520of%25203D%2520detector%2520relies%2520on%2520the%2520data%2520size%2520and%2520annotation%2520which%2520is%250Aexpensive.%2520Consequently%252C%2520the%2520demand%2520of%2520training%2520with%2520limited%2520labeled%2520data%2520is%250Agrowing.%2520We%2520explore%2520a%2520novel%2520teacher-student%2520framework%2520employing%2520channel%250Aaugmentation%2520for%25203D%2520semi-supervised%2520object%2520detection.%2520The%2520teacher-student%2520SSL%250Atypically%2520adopts%2520a%2520weak%2520augmentation%2520and%2520strong%2520augmentation%2520to%2520teacher%2520and%250Astudent%252C%2520respectively.%2520In%2520this%2520work%252C%2520we%2520apply%2520multiple%2520channel%2520augmentations%2520to%250Aboth%2520networks%2520using%2520the%2520transformation%2520equivariance%2520detector%2520%2528TED%2529.%2520The%2520TED%250Aallows%2520us%2520to%2520explore%2520different%2520combinations%2520of%2520augmentation%2520on%2520point%2520clouds%2520and%250Aefficiently%2520aggregates%2520multi-channel%2520transformation%2520equivariance%2520features.%2520In%250Aprinciple%252C%2520by%2520adopting%2520fixed%2520channel%2520augmentations%2520for%2520the%2520teacher%2520network%252C%2520the%250Astudent%2520can%2520train%2520stably%2520on%2520reliable%2520pseudo-labels.%2520Adopting%2520strong%2520channel%250Aaugmentations%2520can%2520enrich%2520the%2520diversity%2520of%2520data%252C%2520fostering%2520robustness%2520to%250Atransformations%2520and%2520enhancing%2520generalization%2520performance%2520of%2520the%2520student%250Anetwork.%2520We%2520use%2520SOTA%2520hierarchical%2520supervision%2520as%2520a%2520baseline%2520and%2520adapt%2520its%250Adual-threshold%2520to%2520TED%252C%2520which%2520is%2520called%2520channel%2520IoU%2520consistency.%2520We%2520evaluate%2520our%250Amethod%2520with%2520KITTI%2520dataset%252C%2520and%2520achieved%2520a%2520significant%2520performance%2520leap%252C%250Asurpassing%2520SOTA%25203D%2520semi-supervised%2520object%2520detection%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%203D%20Object%20Detection%20with%20Chanel%20Augmentation%20using%0A%20%20Transformation%20Equivariance&entry.906535625=Minju%20Kang%20and%20Taehun%20Kong%20and%20Tae-Kyun%20Kim&entry.1292438233=%20%20Accurate%203D%20object%20detection%20is%20crucial%20for%20autonomous%20vehicles%20and%20robots%20to%0Anavigate%20and%20interact%20with%20the%20environment%20safely%20and%20effectively.%20Meanwhile%2C%0Athe%20performance%20of%203D%20detector%20relies%20on%20the%20data%20size%20and%20annotation%20which%20is%0Aexpensive.%20Consequently%2C%20the%20demand%20of%20training%20with%20limited%20labeled%20data%20is%0Agrowing.%20We%20explore%20a%20novel%20teacher-student%20framework%20employing%20channel%0Aaugmentation%20for%203D%20semi-supervised%20object%20detection.%20The%20teacher-student%20SSL%0Atypically%20adopts%20a%20weak%20augmentation%20and%20strong%20augmentation%20to%20teacher%20and%0Astudent%2C%20respectively.%20In%20this%20work%2C%20we%20apply%20multiple%20channel%20augmentations%20to%0Aboth%20networks%20using%20the%20transformation%20equivariance%20detector%20%28TED%29.%20The%20TED%0Aallows%20us%20to%20explore%20different%20combinations%20of%20augmentation%20on%20point%20clouds%20and%0Aefficiently%20aggregates%20multi-channel%20transformation%20equivariance%20features.%20In%0Aprinciple%2C%20by%20adopting%20fixed%20channel%20augmentations%20for%20the%20teacher%20network%2C%20the%0Astudent%20can%20train%20stably%20on%20reliable%20pseudo-labels.%20Adopting%20strong%20channel%0Aaugmentations%20can%20enrich%20the%20diversity%20of%20data%2C%20fostering%20robustness%20to%0Atransformations%20and%20enhancing%20generalization%20performance%20of%20the%20student%0Anetwork.%20We%20use%20SOTA%20hierarchical%20supervision%20as%20a%20baseline%20and%20adapt%20its%0Adual-threshold%20to%20TED%2C%20which%20is%20called%20channel%20IoU%20consistency.%20We%20evaluate%20our%0Amethod%20with%20KITTI%20dataset%2C%20and%20achieved%20a%20significant%20performance%20leap%2C%0Asurpassing%20SOTA%203D%20semi-supervised%20object%20detection%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06583v1&entry.124074799=Read"},
{"title": "Data Collection-free Masked Video Modeling", "author": "Yuchi Ishikawa and Masayoshi Kondo and Yoshimitsu Aoki", "abstract": "  Pre-training video transformers generally requires a large amount of data,\npresenting significant challenges in terms of data collection costs and\nconcerns related to privacy, licensing, and inherent biases. Synthesizing data\nis one of the promising ways to solve these issues, yet pre-training solely on\nsynthetic data has its own challenges. In this paper, we introduce an effective\nself-supervised learning framework for videos that leverages readily available\nand less costly static images. Specifically, we define the Pseudo Motion\nGenerator (PMG) module that recursively applies image transformations to\ngenerate pseudo-motion videos from images. These pseudo-motion videos are then\nleveraged in masked video modeling. Our approach is applicable to synthetic\nimages as well, thus entirely freeing video pre-training from data collection\ncosts and other concerns in real data. Through experiments in action\nrecognition tasks, we demonstrate that this framework allows effective learning\nof spatio-temporal features through pseudo-motion videos, significantly\nimproving over existing methods which also use static images and partially\noutperforming those using both real and synthetic videos. These results uncover\nfragments of what video transformers learn through masked video modeling.\n", "link": "http://arxiv.org/abs/2409.06665v1", "date": "2024-09-10", "relevancy": 2.3183, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5953}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5928}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Collection-free%20Masked%20Video%20Modeling&body=Title%3A%20Data%20Collection-free%20Masked%20Video%20Modeling%0AAuthor%3A%20Yuchi%20Ishikawa%20and%20Masayoshi%20Kondo%20and%20Yoshimitsu%20Aoki%0AAbstract%3A%20%20%20Pre-training%20video%20transformers%20generally%20requires%20a%20large%20amount%20of%20data%2C%0Apresenting%20significant%20challenges%20in%20terms%20of%20data%20collection%20costs%20and%0Aconcerns%20related%20to%20privacy%2C%20licensing%2C%20and%20inherent%20biases.%20Synthesizing%20data%0Ais%20one%20of%20the%20promising%20ways%20to%20solve%20these%20issues%2C%20yet%20pre-training%20solely%20on%0Asynthetic%20data%20has%20its%20own%20challenges.%20In%20this%20paper%2C%20we%20introduce%20an%20effective%0Aself-supervised%20learning%20framework%20for%20videos%20that%20leverages%20readily%20available%0Aand%20less%20costly%20static%20images.%20Specifically%2C%20we%20define%20the%20Pseudo%20Motion%0AGenerator%20%28PMG%29%20module%20that%20recursively%20applies%20image%20transformations%20to%0Agenerate%20pseudo-motion%20videos%20from%20images.%20These%20pseudo-motion%20videos%20are%20then%0Aleveraged%20in%20masked%20video%20modeling.%20Our%20approach%20is%20applicable%20to%20synthetic%0Aimages%20as%20well%2C%20thus%20entirely%20freeing%20video%20pre-training%20from%20data%20collection%0Acosts%20and%20other%20concerns%20in%20real%20data.%20Through%20experiments%20in%20action%0Arecognition%20tasks%2C%20we%20demonstrate%20that%20this%20framework%20allows%20effective%20learning%0Aof%20spatio-temporal%20features%20through%20pseudo-motion%20videos%2C%20significantly%0Aimproving%20over%20existing%20methods%20which%20also%20use%20static%20images%20and%20partially%0Aoutperforming%20those%20using%20both%20real%20and%20synthetic%20videos.%20These%20results%20uncover%0Afragments%20of%20what%20video%20transformers%20learn%20through%20masked%20video%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Collection-free%2520Masked%2520Video%2520Modeling%26entry.906535625%3DYuchi%2520Ishikawa%2520and%2520Masayoshi%2520Kondo%2520and%2520Yoshimitsu%2520Aoki%26entry.1292438233%3D%2520%2520Pre-training%2520video%2520transformers%2520generally%2520requires%2520a%2520large%2520amount%2520of%2520data%252C%250Apresenting%2520significant%2520challenges%2520in%2520terms%2520of%2520data%2520collection%2520costs%2520and%250Aconcerns%2520related%2520to%2520privacy%252C%2520licensing%252C%2520and%2520inherent%2520biases.%2520Synthesizing%2520data%250Ais%2520one%2520of%2520the%2520promising%2520ways%2520to%2520solve%2520these%2520issues%252C%2520yet%2520pre-training%2520solely%2520on%250Asynthetic%2520data%2520has%2520its%2520own%2520challenges.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520effective%250Aself-supervised%2520learning%2520framework%2520for%2520videos%2520that%2520leverages%2520readily%2520available%250Aand%2520less%2520costly%2520static%2520images.%2520Specifically%252C%2520we%2520define%2520the%2520Pseudo%2520Motion%250AGenerator%2520%2528PMG%2529%2520module%2520that%2520recursively%2520applies%2520image%2520transformations%2520to%250Agenerate%2520pseudo-motion%2520videos%2520from%2520images.%2520These%2520pseudo-motion%2520videos%2520are%2520then%250Aleveraged%2520in%2520masked%2520video%2520modeling.%2520Our%2520approach%2520is%2520applicable%2520to%2520synthetic%250Aimages%2520as%2520well%252C%2520thus%2520entirely%2520freeing%2520video%2520pre-training%2520from%2520data%2520collection%250Acosts%2520and%2520other%2520concerns%2520in%2520real%2520data.%2520Through%2520experiments%2520in%2520action%250Arecognition%2520tasks%252C%2520we%2520demonstrate%2520that%2520this%2520framework%2520allows%2520effective%2520learning%250Aof%2520spatio-temporal%2520features%2520through%2520pseudo-motion%2520videos%252C%2520significantly%250Aimproving%2520over%2520existing%2520methods%2520which%2520also%2520use%2520static%2520images%2520and%2520partially%250Aoutperforming%2520those%2520using%2520both%2520real%2520and%2520synthetic%2520videos.%2520These%2520results%2520uncover%250Afragments%2520of%2520what%2520video%2520transformers%2520learn%2520through%2520masked%2520video%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Collection-free%20Masked%20Video%20Modeling&entry.906535625=Yuchi%20Ishikawa%20and%20Masayoshi%20Kondo%20and%20Yoshimitsu%20Aoki&entry.1292438233=%20%20Pre-training%20video%20transformers%20generally%20requires%20a%20large%20amount%20of%20data%2C%0Apresenting%20significant%20challenges%20in%20terms%20of%20data%20collection%20costs%20and%0Aconcerns%20related%20to%20privacy%2C%20licensing%2C%20and%20inherent%20biases.%20Synthesizing%20data%0Ais%20one%20of%20the%20promising%20ways%20to%20solve%20these%20issues%2C%20yet%20pre-training%20solely%20on%0Asynthetic%20data%20has%20its%20own%20challenges.%20In%20this%20paper%2C%20we%20introduce%20an%20effective%0Aself-supervised%20learning%20framework%20for%20videos%20that%20leverages%20readily%20available%0Aand%20less%20costly%20static%20images.%20Specifically%2C%20we%20define%20the%20Pseudo%20Motion%0AGenerator%20%28PMG%29%20module%20that%20recursively%20applies%20image%20transformations%20to%0Agenerate%20pseudo-motion%20videos%20from%20images.%20These%20pseudo-motion%20videos%20are%20then%0Aleveraged%20in%20masked%20video%20modeling.%20Our%20approach%20is%20applicable%20to%20synthetic%0Aimages%20as%20well%2C%20thus%20entirely%20freeing%20video%20pre-training%20from%20data%20collection%0Acosts%20and%20other%20concerns%20in%20real%20data.%20Through%20experiments%20in%20action%0Arecognition%20tasks%2C%20we%20demonstrate%20that%20this%20framework%20allows%20effective%20learning%0Aof%20spatio-temporal%20features%20through%20pseudo-motion%20videos%2C%20significantly%0Aimproving%20over%20existing%20methods%20which%20also%20use%20static%20images%20and%20partially%0Aoutperforming%20those%20using%20both%20real%20and%20synthetic%20videos.%20These%20results%20uncover%0Afragments%20of%20what%20video%20transformers%20learn%20through%20masked%20video%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06665v1&entry.124074799=Read"},
{"title": "TiCoSS: Tightening the Coupling between Semantic Segmentation and Stereo\n  Matching within A Joint Learning Framework", "author": "Guanfeng Tang and Zhiyuan Wu and Jiahang Li and Ping Zhong and Xieyuanli Chen and Huiming Lu and Rui Fan", "abstract": "  Semantic segmentation and stereo matching, respectively analogous to the\nventral and dorsal streams in our human brain, are two key components of\nautonomous driving perception systems. Addressing these two tasks with separate\nnetworks is no longer the mainstream direction in developing computer vision\nalgorithms, particularly with the recent advances in large vision models and\nembodied artificial intelligence. The trend is shifting towards combining them\nwithin a joint learning framework, especially emphasizing feature sharing\nbetween the two tasks. The major contributions of this study lie in\ncomprehensively tightening the coupling between semantic segmentation and\nstereo matching. Specifically, this study introduces three novelties: (1) a\ntightly coupled, gated feature fusion strategy, (2) a hierarchical deep\nsupervision strategy, and (3) a coupling tightening loss function. The combined\nuse of these technical contributions results in TiCoSS, a state-of-the-art\njoint learning framework that simultaneously tackles semantic segmentation and\nstereo matching. Through extensive experiments on the KITTI and vKITTI2\ndatasets, along with qualitative and quantitative analyses, we validate the\neffectiveness of our developed strategies and loss function, and demonstrate\nits superior performance compared to prior arts, with a notable increase in\nmIoU by over 9%. Our source code will be publicly available at\nmias.group/TiCoSS upon publication.\n", "link": "http://arxiv.org/abs/2407.18038v3", "date": "2024-09-10", "relevancy": 2.3132, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5817}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework&body=Title%3A%20TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework%0AAuthor%3A%20Guanfeng%20Tang%20and%20Zhiyuan%20Wu%20and%20Jiahang%20Li%20and%20Ping%20Zhong%20and%20Xieyuanli%20Chen%20and%20Huiming%20Lu%20and%20Rui%20Fan%0AAbstract%3A%20%20%20Semantic%20segmentation%20and%20stereo%20matching%2C%20respectively%20analogous%20to%20the%0Aventral%20and%20dorsal%20streams%20in%20our%20human%20brain%2C%20are%20two%20key%20components%20of%0Aautonomous%20driving%20perception%20systems.%20Addressing%20these%20two%20tasks%20with%20separate%0Anetworks%20is%20no%20longer%20the%20mainstream%20direction%20in%20developing%20computer%20vision%0Aalgorithms%2C%20particularly%20with%20the%20recent%20advances%20in%20large%20vision%20models%20and%0Aembodied%20artificial%20intelligence.%20The%20trend%20is%20shifting%20towards%20combining%20them%0Awithin%20a%20joint%20learning%20framework%2C%20especially%20emphasizing%20feature%20sharing%0Abetween%20the%20two%20tasks.%20The%20major%20contributions%20of%20this%20study%20lie%20in%0Acomprehensively%20tightening%20the%20coupling%20between%20semantic%20segmentation%20and%0Astereo%20matching.%20Specifically%2C%20this%20study%20introduces%20three%20novelties%3A%20%281%29%20a%0Atightly%20coupled%2C%20gated%20feature%20fusion%20strategy%2C%20%282%29%20a%20hierarchical%20deep%0Asupervision%20strategy%2C%20and%20%283%29%20a%20coupling%20tightening%20loss%20function.%20The%20combined%0Ause%20of%20these%20technical%20contributions%20results%20in%20TiCoSS%2C%20a%20state-of-the-art%0Ajoint%20learning%20framework%20that%20simultaneously%20tackles%20semantic%20segmentation%20and%0Astereo%20matching.%20Through%20extensive%20experiments%20on%20the%20KITTI%20and%20vKITTI2%0Adatasets%2C%20along%20with%20qualitative%20and%20quantitative%20analyses%2C%20we%20validate%20the%0Aeffectiveness%20of%20our%20developed%20strategies%20and%20loss%20function%2C%20and%20demonstrate%0Aits%20superior%20performance%20compared%20to%20prior%20arts%2C%20with%20a%20notable%20increase%20in%0AmIoU%20by%20over%209%25.%20Our%20source%20code%20will%20be%20publicly%20available%20at%0Amias.group/TiCoSS%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18038v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiCoSS%253A%2520Tightening%2520the%2520Coupling%2520between%2520Semantic%2520Segmentation%2520and%2520Stereo%250A%2520%2520Matching%2520within%2520A%2520Joint%2520Learning%2520Framework%26entry.906535625%3DGuanfeng%2520Tang%2520and%2520Zhiyuan%2520Wu%2520and%2520Jiahang%2520Li%2520and%2520Ping%2520Zhong%2520and%2520Xieyuanli%2520Chen%2520and%2520Huiming%2520Lu%2520and%2520Rui%2520Fan%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520and%2520stereo%2520matching%252C%2520respectively%2520analogous%2520to%2520the%250Aventral%2520and%2520dorsal%2520streams%2520in%2520our%2520human%2520brain%252C%2520are%2520two%2520key%2520components%2520of%250Aautonomous%2520driving%2520perception%2520systems.%2520Addressing%2520these%2520two%2520tasks%2520with%2520separate%250Anetworks%2520is%2520no%2520longer%2520the%2520mainstream%2520direction%2520in%2520developing%2520computer%2520vision%250Aalgorithms%252C%2520particularly%2520with%2520the%2520recent%2520advances%2520in%2520large%2520vision%2520models%2520and%250Aembodied%2520artificial%2520intelligence.%2520The%2520trend%2520is%2520shifting%2520towards%2520combining%2520them%250Awithin%2520a%2520joint%2520learning%2520framework%252C%2520especially%2520emphasizing%2520feature%2520sharing%250Abetween%2520the%2520two%2520tasks.%2520The%2520major%2520contributions%2520of%2520this%2520study%2520lie%2520in%250Acomprehensively%2520tightening%2520the%2520coupling%2520between%2520semantic%2520segmentation%2520and%250Astereo%2520matching.%2520Specifically%252C%2520this%2520study%2520introduces%2520three%2520novelties%253A%2520%25281%2529%2520a%250Atightly%2520coupled%252C%2520gated%2520feature%2520fusion%2520strategy%252C%2520%25282%2529%2520a%2520hierarchical%2520deep%250Asupervision%2520strategy%252C%2520and%2520%25283%2529%2520a%2520coupling%2520tightening%2520loss%2520function.%2520The%2520combined%250Ause%2520of%2520these%2520technical%2520contributions%2520results%2520in%2520TiCoSS%252C%2520a%2520state-of-the-art%250Ajoint%2520learning%2520framework%2520that%2520simultaneously%2520tackles%2520semantic%2520segmentation%2520and%250Astereo%2520matching.%2520Through%2520extensive%2520experiments%2520on%2520the%2520KITTI%2520and%2520vKITTI2%250Adatasets%252C%2520along%2520with%2520qualitative%2520and%2520quantitative%2520analyses%252C%2520we%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520developed%2520strategies%2520and%2520loss%2520function%252C%2520and%2520demonstrate%250Aits%2520superior%2520performance%2520compared%2520to%2520prior%2520arts%252C%2520with%2520a%2520notable%2520increase%2520in%250AmIoU%2520by%2520over%25209%2525.%2520Our%2520source%2520code%2520will%2520be%2520publicly%2520available%2520at%250Amias.group/TiCoSS%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18038v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework&entry.906535625=Guanfeng%20Tang%20and%20Zhiyuan%20Wu%20and%20Jiahang%20Li%20and%20Ping%20Zhong%20and%20Xieyuanli%20Chen%20and%20Huiming%20Lu%20and%20Rui%20Fan&entry.1292438233=%20%20Semantic%20segmentation%20and%20stereo%20matching%2C%20respectively%20analogous%20to%20the%0Aventral%20and%20dorsal%20streams%20in%20our%20human%20brain%2C%20are%20two%20key%20components%20of%0Aautonomous%20driving%20perception%20systems.%20Addressing%20these%20two%20tasks%20with%20separate%0Anetworks%20is%20no%20longer%20the%20mainstream%20direction%20in%20developing%20computer%20vision%0Aalgorithms%2C%20particularly%20with%20the%20recent%20advances%20in%20large%20vision%20models%20and%0Aembodied%20artificial%20intelligence.%20The%20trend%20is%20shifting%20towards%20combining%20them%0Awithin%20a%20joint%20learning%20framework%2C%20especially%20emphasizing%20feature%20sharing%0Abetween%20the%20two%20tasks.%20The%20major%20contributions%20of%20this%20study%20lie%20in%0Acomprehensively%20tightening%20the%20coupling%20between%20semantic%20segmentation%20and%0Astereo%20matching.%20Specifically%2C%20this%20study%20introduces%20three%20novelties%3A%20%281%29%20a%0Atightly%20coupled%2C%20gated%20feature%20fusion%20strategy%2C%20%282%29%20a%20hierarchical%20deep%0Asupervision%20strategy%2C%20and%20%283%29%20a%20coupling%20tightening%20loss%20function.%20The%20combined%0Ause%20of%20these%20technical%20contributions%20results%20in%20TiCoSS%2C%20a%20state-of-the-art%0Ajoint%20learning%20framework%20that%20simultaneously%20tackles%20semantic%20segmentation%20and%0Astereo%20matching.%20Through%20extensive%20experiments%20on%20the%20KITTI%20and%20vKITTI2%0Adatasets%2C%20along%20with%20qualitative%20and%20quantitative%20analyses%2C%20we%20validate%20the%0Aeffectiveness%20of%20our%20developed%20strategies%20and%20loss%20function%2C%20and%20demonstrate%0Aits%20superior%20performance%20compared%20to%20prior%20arts%2C%20with%20a%20notable%20increase%20in%0AmIoU%20by%20over%209%25.%20Our%20source%20code%20will%20be%20publicly%20available%20at%0Amias.group/TiCoSS%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18038v3&entry.124074799=Read"},
{"title": "A Cross-Font Image Retrieval Network for Recognizing Undeciphered Oracle\n  Bone Inscriptions", "author": "Zhicong Wu and Qifeng Su and Ke Gu and Xiaodong Shi", "abstract": "  Oracle Bone Inscription (OBI) is the earliest mature writing system known in\nChina to date, which represents a crucial stage in the development of\nhieroglyphs. Nevertheless, the substantial quantity of undeciphered OBI\ncharacters continues to pose a persistent challenge for scholars, while\nconventional methods of ancient script research are both time-consuming and\nlabor-intensive. In this paper, we propose a cross-font image retrieval network\n(CFIRN) to decipher OBI characters by establishing associations between OBI\ncharacters and other script forms, simulating the interpretive behavior of\npaleography scholars. Concretely, our network employs a siamese framework to\nextract deep features from character images of various fonts, fully exploring\nstructure clues with different resolution by designed multiscale feature\nintegration (MFI) module and multiscale refinement classifier (MRC). Extensive\nexperiments on three challenging cross-font image retrieval datasets\ndemonstrate that, given undeciphered OBI characters, our CFIRN can effectively\nachieve accurate matches with characters from other gallery fonts.\n", "link": "http://arxiv.org/abs/2409.06381v1", "date": "2024-09-10", "relevancy": 2.29, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Cross-Font%20Image%20Retrieval%20Network%20for%20Recognizing%20Undeciphered%20Oracle%0A%20%20Bone%20Inscriptions&body=Title%3A%20A%20Cross-Font%20Image%20Retrieval%20Network%20for%20Recognizing%20Undeciphered%20Oracle%0A%20%20Bone%20Inscriptions%0AAuthor%3A%20Zhicong%20Wu%20and%20Qifeng%20Su%20and%20Ke%20Gu%20and%20Xiaodong%20Shi%0AAbstract%3A%20%20%20Oracle%20Bone%20Inscription%20%28OBI%29%20is%20the%20earliest%20mature%20writing%20system%20known%20in%0AChina%20to%20date%2C%20which%20represents%20a%20crucial%20stage%20in%20the%20development%20of%0Ahieroglyphs.%20Nevertheless%2C%20the%20substantial%20quantity%20of%20undeciphered%20OBI%0Acharacters%20continues%20to%20pose%20a%20persistent%20challenge%20for%20scholars%2C%20while%0Aconventional%20methods%20of%20ancient%20script%20research%20are%20both%20time-consuming%20and%0Alabor-intensive.%20In%20this%20paper%2C%20we%20propose%20a%20cross-font%20image%20retrieval%20network%0A%28CFIRN%29%20to%20decipher%20OBI%20characters%20by%20establishing%20associations%20between%20OBI%0Acharacters%20and%20other%20script%20forms%2C%20simulating%20the%20interpretive%20behavior%20of%0Apaleography%20scholars.%20Concretely%2C%20our%20network%20employs%20a%20siamese%20framework%20to%0Aextract%20deep%20features%20from%20character%20images%20of%20various%20fonts%2C%20fully%20exploring%0Astructure%20clues%20with%20different%20resolution%20by%20designed%20multiscale%20feature%0Aintegration%20%28MFI%29%20module%20and%20multiscale%20refinement%20classifier%20%28MRC%29.%20Extensive%0Aexperiments%20on%20three%20challenging%20cross-font%20image%20retrieval%20datasets%0Ademonstrate%20that%2C%20given%20undeciphered%20OBI%20characters%2C%20our%20CFIRN%20can%20effectively%0Aachieve%20accurate%20matches%20with%20characters%20from%20other%20gallery%20fonts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Cross-Font%2520Image%2520Retrieval%2520Network%2520for%2520Recognizing%2520Undeciphered%2520Oracle%250A%2520%2520Bone%2520Inscriptions%26entry.906535625%3DZhicong%2520Wu%2520and%2520Qifeng%2520Su%2520and%2520Ke%2520Gu%2520and%2520Xiaodong%2520Shi%26entry.1292438233%3D%2520%2520Oracle%2520Bone%2520Inscription%2520%2528OBI%2529%2520is%2520the%2520earliest%2520mature%2520writing%2520system%2520known%2520in%250AChina%2520to%2520date%252C%2520which%2520represents%2520a%2520crucial%2520stage%2520in%2520the%2520development%2520of%250Ahieroglyphs.%2520Nevertheless%252C%2520the%2520substantial%2520quantity%2520of%2520undeciphered%2520OBI%250Acharacters%2520continues%2520to%2520pose%2520a%2520persistent%2520challenge%2520for%2520scholars%252C%2520while%250Aconventional%2520methods%2520of%2520ancient%2520script%2520research%2520are%2520both%2520time-consuming%2520and%250Alabor-intensive.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520cross-font%2520image%2520retrieval%2520network%250A%2528CFIRN%2529%2520to%2520decipher%2520OBI%2520characters%2520by%2520establishing%2520associations%2520between%2520OBI%250Acharacters%2520and%2520other%2520script%2520forms%252C%2520simulating%2520the%2520interpretive%2520behavior%2520of%250Apaleography%2520scholars.%2520Concretely%252C%2520our%2520network%2520employs%2520a%2520siamese%2520framework%2520to%250Aextract%2520deep%2520features%2520from%2520character%2520images%2520of%2520various%2520fonts%252C%2520fully%2520exploring%250Astructure%2520clues%2520with%2520different%2520resolution%2520by%2520designed%2520multiscale%2520feature%250Aintegration%2520%2528MFI%2529%2520module%2520and%2520multiscale%2520refinement%2520classifier%2520%2528MRC%2529.%2520Extensive%250Aexperiments%2520on%2520three%2520challenging%2520cross-font%2520image%2520retrieval%2520datasets%250Ademonstrate%2520that%252C%2520given%2520undeciphered%2520OBI%2520characters%252C%2520our%2520CFIRN%2520can%2520effectively%250Aachieve%2520accurate%2520matches%2520with%2520characters%2520from%2520other%2520gallery%2520fonts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cross-Font%20Image%20Retrieval%20Network%20for%20Recognizing%20Undeciphered%20Oracle%0A%20%20Bone%20Inscriptions&entry.906535625=Zhicong%20Wu%20and%20Qifeng%20Su%20and%20Ke%20Gu%20and%20Xiaodong%20Shi&entry.1292438233=%20%20Oracle%20Bone%20Inscription%20%28OBI%29%20is%20the%20earliest%20mature%20writing%20system%20known%20in%0AChina%20to%20date%2C%20which%20represents%20a%20crucial%20stage%20in%20the%20development%20of%0Ahieroglyphs.%20Nevertheless%2C%20the%20substantial%20quantity%20of%20undeciphered%20OBI%0Acharacters%20continues%20to%20pose%20a%20persistent%20challenge%20for%20scholars%2C%20while%0Aconventional%20methods%20of%20ancient%20script%20research%20are%20both%20time-consuming%20and%0Alabor-intensive.%20In%20this%20paper%2C%20we%20propose%20a%20cross-font%20image%20retrieval%20network%0A%28CFIRN%29%20to%20decipher%20OBI%20characters%20by%20establishing%20associations%20between%20OBI%0Acharacters%20and%20other%20script%20forms%2C%20simulating%20the%20interpretive%20behavior%20of%0Apaleography%20scholars.%20Concretely%2C%20our%20network%20employs%20a%20siamese%20framework%20to%0Aextract%20deep%20features%20from%20character%20images%20of%20various%20fonts%2C%20fully%20exploring%0Astructure%20clues%20with%20different%20resolution%20by%20designed%20multiscale%20feature%0Aintegration%20%28MFI%29%20module%20and%20multiscale%20refinement%20classifier%20%28MRC%29.%20Extensive%0Aexperiments%20on%20three%20challenging%20cross-font%20image%20retrieval%20datasets%0Ademonstrate%20that%2C%20given%20undeciphered%20OBI%20characters%2C%20our%20CFIRN%20can%20effectively%0Aachieve%20accurate%20matches%20with%20characters%20from%20other%20gallery%20fonts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06381v1&entry.124074799=Read"},
{"title": "A Likelihood Ratio-Based Approach to Segmenting Unknown Objects", "author": "Nazir Nayal and Youssef Shoeb and Fatma G\u00fcney", "abstract": "  Addressing the Out-of-Distribution (OoD) segmentation task is a prerequisite\nfor perception systems operating in an open-world environment. Large\nfoundational models are frequently used in downstream tasks, however, their\npotential for OoD remains mostly unexplored. We seek to leverage a large\nfoundational model to achieve robust representation. Outlier supervision is a\nwidely used strategy for improving OoD detection of the existing segmentation\nnetworks. However, current approaches for outlier supervision involve\nretraining parts of the original network, which is typically disruptive to the\nmodel's learned feature representation. Furthermore, retraining becomes\ninfeasible in the case of large foundational models. Our goal is to retrain for\noutlier segmentation without compromising the strong representation space of\nthe foundational model. To this end, we propose an adaptive, lightweight\nunknown estimation module (UEM) for outlier supervision that significantly\nenhances the OoD segmentation performance without affecting the learned feature\nrepresentation of the original network. UEM learns a distribution for outliers\nand a generic distribution for known classes. Using the learned distributions,\nwe propose a likelihood-ratio-based outlier scoring function that fuses the\nconfidence of UEM with that of the pixel-wise segmentation inlier network to\ndetect unknown objects. We also propose an objective to optimize this score\ndirectly. Our approach achieves a new state-of-the-art across multiple\ndatasets, outperforming the previous best method by 5.74% average precision\npoints while having a lower false-positive rate. Importantly, strong inlier\nperformance remains unaffected.\n", "link": "http://arxiv.org/abs/2409.06424v1", "date": "2024-09-10", "relevancy": 2.247, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6046}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Likelihood%20Ratio-Based%20Approach%20to%20Segmenting%20Unknown%20Objects&body=Title%3A%20A%20Likelihood%20Ratio-Based%20Approach%20to%20Segmenting%20Unknown%20Objects%0AAuthor%3A%20Nazir%20Nayal%20and%20Youssef%20Shoeb%20and%20Fatma%20G%C3%BCney%0AAbstract%3A%20%20%20Addressing%20the%20Out-of-Distribution%20%28OoD%29%20segmentation%20task%20is%20a%20prerequisite%0Afor%20perception%20systems%20operating%20in%20an%20open-world%20environment.%20Large%0Afoundational%20models%20are%20frequently%20used%20in%20downstream%20tasks%2C%20however%2C%20their%0Apotential%20for%20OoD%20remains%20mostly%20unexplored.%20We%20seek%20to%20leverage%20a%20large%0Afoundational%20model%20to%20achieve%20robust%20representation.%20Outlier%20supervision%20is%20a%0Awidely%20used%20strategy%20for%20improving%20OoD%20detection%20of%20the%20existing%20segmentation%0Anetworks.%20However%2C%20current%20approaches%20for%20outlier%20supervision%20involve%0Aretraining%20parts%20of%20the%20original%20network%2C%20which%20is%20typically%20disruptive%20to%20the%0Amodel%27s%20learned%20feature%20representation.%20Furthermore%2C%20retraining%20becomes%0Ainfeasible%20in%20the%20case%20of%20large%20foundational%20models.%20Our%20goal%20is%20to%20retrain%20for%0Aoutlier%20segmentation%20without%20compromising%20the%20strong%20representation%20space%20of%0Athe%20foundational%20model.%20To%20this%20end%2C%20we%20propose%20an%20adaptive%2C%20lightweight%0Aunknown%20estimation%20module%20%28UEM%29%20for%20outlier%20supervision%20that%20significantly%0Aenhances%20the%20OoD%20segmentation%20performance%20without%20affecting%20the%20learned%20feature%0Arepresentation%20of%20the%20original%20network.%20UEM%20learns%20a%20distribution%20for%20outliers%0Aand%20a%20generic%20distribution%20for%20known%20classes.%20Using%20the%20learned%20distributions%2C%0Awe%20propose%20a%20likelihood-ratio-based%20outlier%20scoring%20function%20that%20fuses%20the%0Aconfidence%20of%20UEM%20with%20that%20of%20the%20pixel-wise%20segmentation%20inlier%20network%20to%0Adetect%20unknown%20objects.%20We%20also%20propose%20an%20objective%20to%20optimize%20this%20score%0Adirectly.%20Our%20approach%20achieves%20a%20new%20state-of-the-art%20across%20multiple%0Adatasets%2C%20outperforming%20the%20previous%20best%20method%20by%205.74%25%20average%20precision%0Apoints%20while%20having%20a%20lower%20false-positive%20rate.%20Importantly%2C%20strong%20inlier%0Aperformance%20remains%20unaffected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Likelihood%2520Ratio-Based%2520Approach%2520to%2520Segmenting%2520Unknown%2520Objects%26entry.906535625%3DNazir%2520Nayal%2520and%2520Youssef%2520Shoeb%2520and%2520Fatma%2520G%25C3%25BCney%26entry.1292438233%3D%2520%2520Addressing%2520the%2520Out-of-Distribution%2520%2528OoD%2529%2520segmentation%2520task%2520is%2520a%2520prerequisite%250Afor%2520perception%2520systems%2520operating%2520in%2520an%2520open-world%2520environment.%2520Large%250Afoundational%2520models%2520are%2520frequently%2520used%2520in%2520downstream%2520tasks%252C%2520however%252C%2520their%250Apotential%2520for%2520OoD%2520remains%2520mostly%2520unexplored.%2520We%2520seek%2520to%2520leverage%2520a%2520large%250Afoundational%2520model%2520to%2520achieve%2520robust%2520representation.%2520Outlier%2520supervision%2520is%2520a%250Awidely%2520used%2520strategy%2520for%2520improving%2520OoD%2520detection%2520of%2520the%2520existing%2520segmentation%250Anetworks.%2520However%252C%2520current%2520approaches%2520for%2520outlier%2520supervision%2520involve%250Aretraining%2520parts%2520of%2520the%2520original%2520network%252C%2520which%2520is%2520typically%2520disruptive%2520to%2520the%250Amodel%2527s%2520learned%2520feature%2520representation.%2520Furthermore%252C%2520retraining%2520becomes%250Ainfeasible%2520in%2520the%2520case%2520of%2520large%2520foundational%2520models.%2520Our%2520goal%2520is%2520to%2520retrain%2520for%250Aoutlier%2520segmentation%2520without%2520compromising%2520the%2520strong%2520representation%2520space%2520of%250Athe%2520foundational%2520model.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520adaptive%252C%2520lightweight%250Aunknown%2520estimation%2520module%2520%2528UEM%2529%2520for%2520outlier%2520supervision%2520that%2520significantly%250Aenhances%2520the%2520OoD%2520segmentation%2520performance%2520without%2520affecting%2520the%2520learned%2520feature%250Arepresentation%2520of%2520the%2520original%2520network.%2520UEM%2520learns%2520a%2520distribution%2520for%2520outliers%250Aand%2520a%2520generic%2520distribution%2520for%2520known%2520classes.%2520Using%2520the%2520learned%2520distributions%252C%250Awe%2520propose%2520a%2520likelihood-ratio-based%2520outlier%2520scoring%2520function%2520that%2520fuses%2520the%250Aconfidence%2520of%2520UEM%2520with%2520that%2520of%2520the%2520pixel-wise%2520segmentation%2520inlier%2520network%2520to%250Adetect%2520unknown%2520objects.%2520We%2520also%2520propose%2520an%2520objective%2520to%2520optimize%2520this%2520score%250Adirectly.%2520Our%2520approach%2520achieves%2520a%2520new%2520state-of-the-art%2520across%2520multiple%250Adatasets%252C%2520outperforming%2520the%2520previous%2520best%2520method%2520by%25205.74%2525%2520average%2520precision%250Apoints%2520while%2520having%2520a%2520lower%2520false-positive%2520rate.%2520Importantly%252C%2520strong%2520inlier%250Aperformance%2520remains%2520unaffected.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Likelihood%20Ratio-Based%20Approach%20to%20Segmenting%20Unknown%20Objects&entry.906535625=Nazir%20Nayal%20and%20Youssef%20Shoeb%20and%20Fatma%20G%C3%BCney&entry.1292438233=%20%20Addressing%20the%20Out-of-Distribution%20%28OoD%29%20segmentation%20task%20is%20a%20prerequisite%0Afor%20perception%20systems%20operating%20in%20an%20open-world%20environment.%20Large%0Afoundational%20models%20are%20frequently%20used%20in%20downstream%20tasks%2C%20however%2C%20their%0Apotential%20for%20OoD%20remains%20mostly%20unexplored.%20We%20seek%20to%20leverage%20a%20large%0Afoundational%20model%20to%20achieve%20robust%20representation.%20Outlier%20supervision%20is%20a%0Awidely%20used%20strategy%20for%20improving%20OoD%20detection%20of%20the%20existing%20segmentation%0Anetworks.%20However%2C%20current%20approaches%20for%20outlier%20supervision%20involve%0Aretraining%20parts%20of%20the%20original%20network%2C%20which%20is%20typically%20disruptive%20to%20the%0Amodel%27s%20learned%20feature%20representation.%20Furthermore%2C%20retraining%20becomes%0Ainfeasible%20in%20the%20case%20of%20large%20foundational%20models.%20Our%20goal%20is%20to%20retrain%20for%0Aoutlier%20segmentation%20without%20compromising%20the%20strong%20representation%20space%20of%0Athe%20foundational%20model.%20To%20this%20end%2C%20we%20propose%20an%20adaptive%2C%20lightweight%0Aunknown%20estimation%20module%20%28UEM%29%20for%20outlier%20supervision%20that%20significantly%0Aenhances%20the%20OoD%20segmentation%20performance%20without%20affecting%20the%20learned%20feature%0Arepresentation%20of%20the%20original%20network.%20UEM%20learns%20a%20distribution%20for%20outliers%0Aand%20a%20generic%20distribution%20for%20known%20classes.%20Using%20the%20learned%20distributions%2C%0Awe%20propose%20a%20likelihood-ratio-based%20outlier%20scoring%20function%20that%20fuses%20the%0Aconfidence%20of%20UEM%20with%20that%20of%20the%20pixel-wise%20segmentation%20inlier%20network%20to%0Adetect%20unknown%20objects.%20We%20also%20propose%20an%20objective%20to%20optimize%20this%20score%0Adirectly.%20Our%20approach%20achieves%20a%20new%20state-of-the-art%20across%20multiple%0Adatasets%2C%20outperforming%20the%20previous%20best%20method%20by%205.74%25%20average%20precision%0Apoints%20while%20having%20a%20lower%20false-positive%20rate.%20Importantly%2C%20strong%20inlier%0Aperformance%20remains%20unaffected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06424v1&entry.124074799=Read"},
{"title": "Distilling Generative-Discriminative Representations for Very\n  Low-Resolution Face Recognition", "author": "Junzheng Zhang and Weijia Guo and Bochao Liu and Ruixin Shi and Yong Li and Shiming Ge", "abstract": "  Very low-resolution face recognition is challenging due to the serious loss\nof informative facial details in resolution degradation. In this paper, we\npropose a generative-discriminative representation distillation approach that\ncombines generative representation with cross-resolution aligned knowledge\ndistillation. This approach facilitates very low-resolution face recognition by\njointly distilling generative and discriminative models via two distillation\nmodules. Firstly, the generative representation distillation takes the encoder\nof a diffusion model pretrained for face super-resolution as the generative\nteacher to supervise the learning of the student backbone via feature\nregression, and then freezes the student backbone. After that, the\ndiscriminative representation distillation further considers a pretrained face\nrecognizer as the discriminative teacher to supervise the learning of the\nstudent head via cross-resolution relational contrastive distillation. In this\nway, the general backbone representation can be transformed into discriminative\nhead representation, leading to a robust and discriminative student model for\nvery low-resolution face recognition. Our approach improves the recovery of the\nmissing details in very low-resolution faces and achieves better knowledge\ntransfer. Extensive experiments on face datasets demonstrate that our approach\nenhances the recognition accuracy of very low-resolution faces, showcasing its\neffectiveness and adaptability.\n", "link": "http://arxiv.org/abs/2409.06371v1", "date": "2024-09-10", "relevancy": 2.2454, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5985}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5367}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Generative-Discriminative%20Representations%20for%20Very%0A%20%20Low-Resolution%20Face%20Recognition&body=Title%3A%20Distilling%20Generative-Discriminative%20Representations%20for%20Very%0A%20%20Low-Resolution%20Face%20Recognition%0AAuthor%3A%20Junzheng%20Zhang%20and%20Weijia%20Guo%20and%20Bochao%20Liu%20and%20Ruixin%20Shi%20and%20Yong%20Li%20and%20Shiming%20Ge%0AAbstract%3A%20%20%20Very%20low-resolution%20face%20recognition%20is%20challenging%20due%20to%20the%20serious%20loss%0Aof%20informative%20facial%20details%20in%20resolution%20degradation.%20In%20this%20paper%2C%20we%0Apropose%20a%20generative-discriminative%20representation%20distillation%20approach%20that%0Acombines%20generative%20representation%20with%20cross-resolution%20aligned%20knowledge%0Adistillation.%20This%20approach%20facilitates%20very%20low-resolution%20face%20recognition%20by%0Ajointly%20distilling%20generative%20and%20discriminative%20models%20via%20two%20distillation%0Amodules.%20Firstly%2C%20the%20generative%20representation%20distillation%20takes%20the%20encoder%0Aof%20a%20diffusion%20model%20pretrained%20for%20face%20super-resolution%20as%20the%20generative%0Ateacher%20to%20supervise%20the%20learning%20of%20the%20student%20backbone%20via%20feature%0Aregression%2C%20and%20then%20freezes%20the%20student%20backbone.%20After%20that%2C%20the%0Adiscriminative%20representation%20distillation%20further%20considers%20a%20pretrained%20face%0Arecognizer%20as%20the%20discriminative%20teacher%20to%20supervise%20the%20learning%20of%20the%0Astudent%20head%20via%20cross-resolution%20relational%20contrastive%20distillation.%20In%20this%0Away%2C%20the%20general%20backbone%20representation%20can%20be%20transformed%20into%20discriminative%0Ahead%20representation%2C%20leading%20to%20a%20robust%20and%20discriminative%20student%20model%20for%0Avery%20low-resolution%20face%20recognition.%20Our%20approach%20improves%20the%20recovery%20of%20the%0Amissing%20details%20in%20very%20low-resolution%20faces%20and%20achieves%20better%20knowledge%0Atransfer.%20Extensive%20experiments%20on%20face%20datasets%20demonstrate%20that%20our%20approach%0Aenhances%20the%20recognition%20accuracy%20of%20very%20low-resolution%20faces%2C%20showcasing%20its%0Aeffectiveness%20and%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Generative-Discriminative%2520Representations%2520for%2520Very%250A%2520%2520Low-Resolution%2520Face%2520Recognition%26entry.906535625%3DJunzheng%2520Zhang%2520and%2520Weijia%2520Guo%2520and%2520Bochao%2520Liu%2520and%2520Ruixin%2520Shi%2520and%2520Yong%2520Li%2520and%2520Shiming%2520Ge%26entry.1292438233%3D%2520%2520Very%2520low-resolution%2520face%2520recognition%2520is%2520challenging%2520due%2520to%2520the%2520serious%2520loss%250Aof%2520informative%2520facial%2520details%2520in%2520resolution%2520degradation.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520generative-discriminative%2520representation%2520distillation%2520approach%2520that%250Acombines%2520generative%2520representation%2520with%2520cross-resolution%2520aligned%2520knowledge%250Adistillation.%2520This%2520approach%2520facilitates%2520very%2520low-resolution%2520face%2520recognition%2520by%250Ajointly%2520distilling%2520generative%2520and%2520discriminative%2520models%2520via%2520two%2520distillation%250Amodules.%2520Firstly%252C%2520the%2520generative%2520representation%2520distillation%2520takes%2520the%2520encoder%250Aof%2520a%2520diffusion%2520model%2520pretrained%2520for%2520face%2520super-resolution%2520as%2520the%2520generative%250Ateacher%2520to%2520supervise%2520the%2520learning%2520of%2520the%2520student%2520backbone%2520via%2520feature%250Aregression%252C%2520and%2520then%2520freezes%2520the%2520student%2520backbone.%2520After%2520that%252C%2520the%250Adiscriminative%2520representation%2520distillation%2520further%2520considers%2520a%2520pretrained%2520face%250Arecognizer%2520as%2520the%2520discriminative%2520teacher%2520to%2520supervise%2520the%2520learning%2520of%2520the%250Astudent%2520head%2520via%2520cross-resolution%2520relational%2520contrastive%2520distillation.%2520In%2520this%250Away%252C%2520the%2520general%2520backbone%2520representation%2520can%2520be%2520transformed%2520into%2520discriminative%250Ahead%2520representation%252C%2520leading%2520to%2520a%2520robust%2520and%2520discriminative%2520student%2520model%2520for%250Avery%2520low-resolution%2520face%2520recognition.%2520Our%2520approach%2520improves%2520the%2520recovery%2520of%2520the%250Amissing%2520details%2520in%2520very%2520low-resolution%2520faces%2520and%2520achieves%2520better%2520knowledge%250Atransfer.%2520Extensive%2520experiments%2520on%2520face%2520datasets%2520demonstrate%2520that%2520our%2520approach%250Aenhances%2520the%2520recognition%2520accuracy%2520of%2520very%2520low-resolution%2520faces%252C%2520showcasing%2520its%250Aeffectiveness%2520and%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Generative-Discriminative%20Representations%20for%20Very%0A%20%20Low-Resolution%20Face%20Recognition&entry.906535625=Junzheng%20Zhang%20and%20Weijia%20Guo%20and%20Bochao%20Liu%20and%20Ruixin%20Shi%20and%20Yong%20Li%20and%20Shiming%20Ge&entry.1292438233=%20%20Very%20low-resolution%20face%20recognition%20is%20challenging%20due%20to%20the%20serious%20loss%0Aof%20informative%20facial%20details%20in%20resolution%20degradation.%20In%20this%20paper%2C%20we%0Apropose%20a%20generative-discriminative%20representation%20distillation%20approach%20that%0Acombines%20generative%20representation%20with%20cross-resolution%20aligned%20knowledge%0Adistillation.%20This%20approach%20facilitates%20very%20low-resolution%20face%20recognition%20by%0Ajointly%20distilling%20generative%20and%20discriminative%20models%20via%20two%20distillation%0Amodules.%20Firstly%2C%20the%20generative%20representation%20distillation%20takes%20the%20encoder%0Aof%20a%20diffusion%20model%20pretrained%20for%20face%20super-resolution%20as%20the%20generative%0Ateacher%20to%20supervise%20the%20learning%20of%20the%20student%20backbone%20via%20feature%0Aregression%2C%20and%20then%20freezes%20the%20student%20backbone.%20After%20that%2C%20the%0Adiscriminative%20representation%20distillation%20further%20considers%20a%20pretrained%20face%0Arecognizer%20as%20the%20discriminative%20teacher%20to%20supervise%20the%20learning%20of%20the%0Astudent%20head%20via%20cross-resolution%20relational%20contrastive%20distillation.%20In%20this%0Away%2C%20the%20general%20backbone%20representation%20can%20be%20transformed%20into%20discriminative%0Ahead%20representation%2C%20leading%20to%20a%20robust%20and%20discriminative%20student%20model%20for%0Avery%20low-resolution%20face%20recognition.%20Our%20approach%20improves%20the%20recovery%20of%20the%0Amissing%20details%20in%20very%20low-resolution%20faces%20and%20achieves%20better%20knowledge%0Atransfer.%20Extensive%20experiments%20on%20face%20datasets%20demonstrate%20that%20our%20approach%0Aenhances%20the%20recognition%20accuracy%20of%20very%20low-resolution%20faces%2C%20showcasing%20its%0Aeffectiveness%20and%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06371v1&entry.124074799=Read"},
{"title": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training\n  Data", "author": "Hossein Hajipour and Lea Sch\u00f6nherr and Thorsten Holz and Mario Fritz", "abstract": "  Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness.\n", "link": "http://arxiv.org/abs/2409.06446v1", "date": "2024-09-10", "relevancy": 2.2311, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4506}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4447}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HexaCoder%3A%20Secure%20Code%20Generation%20via%20Oracle-Guided%20Synthetic%20Training%0A%20%20Data&body=Title%3A%20HexaCoder%3A%20Secure%20Code%20Generation%20via%20Oracle-Guided%20Synthetic%20Training%0A%20%20Data%0AAuthor%3A%20Hossein%20Hajipour%20and%20Lea%20Sch%C3%B6nherr%20and%20Thorsten%20Holz%20and%20Mario%20Fritz%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20great%20potential%20for%20automatic%20code%0Ageneration%20and%20form%20the%20basis%20for%20various%20tools%20such%20as%20GitHub%20Copilot.%0AHowever%2C%20recent%20studies%20highlight%20that%20many%20LLM-generated%20code%20contains%20serious%0Asecurity%20vulnerabilities.%20While%20previous%20work%20tries%20to%20address%20this%20by%20training%0Amodels%20that%20generate%20secure%20code%2C%20these%20attempts%20remain%20constrained%20by%20limited%0Aaccess%20to%20training%20data%20and%20labor-intensive%20data%20preparation.%0A%20%20In%20this%20paper%2C%20we%20introduce%20HexaCoder%2C%20a%20novel%20approach%20to%20enhance%20the%0Aability%20of%20LLMs%20to%20generate%20secure%20codes%20by%20automatically%20synthesizing%20secure%0Acodes%2C%20which%20reduces%20the%20effort%20of%20finding%20suitable%20training%20data.%20HexaCoder%0Acomprises%20two%20key%20components%3A%20an%20oracle-guided%20data%20synthesis%20pipeline%20and%20a%0Atwo-step%20process%20for%20secure%20code%20generation.%20The%20data%20synthesis%20pipeline%0Agenerates%20pairs%20of%20vulnerable%20and%20fixed%20codes%20for%20specific%20Common%20Weakness%0AEnumeration%20%28CWE%29%20types%20by%20utilizing%20a%20state-of-the-art%20LLM%20for%20repairing%0Avulnerable%20code.%20A%20security%20oracle%20identifies%20vulnerabilities%2C%20and%20a%0Astate-of-the-art%20LLM%20repairs%20them%20by%20extending%20and/or%20editing%20the%20codes%2C%0Acreating%20data%20pairs%20for%20fine-tuning%20using%20the%20Low-Rank%20Adaptation%20%28LoRA%29%0Amethod.%20Each%20example%20of%20our%20fine-tuning%20dataset%20includes%20the%20necessary%0Asecurity-related%20libraries%20and%20code%20that%20form%20the%20basis%20of%20our%20novel%20two-step%0Ageneration%20approach.%20This%20allows%20the%20model%20to%20integrate%20security-relevant%0Alibraries%20before%20generating%20the%20main%20code%2C%20significantly%20reducing%20the%20number%20of%0Agenerated%20vulnerable%20codes%20by%20up%20to%2085%25%20compared%20to%20the%20baseline%20methods.%20We%0Aperform%20extensive%20evaluations%20on%20three%20different%20benchmarks%20for%20four%20LLMs%2C%0Ademonstrating%20that%20HexaCoder%20not%20only%20improves%20the%20security%20of%20the%20generated%0Acode%20but%20also%20maintains%20a%20high%20level%20of%20functional%20correctness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHexaCoder%253A%2520Secure%2520Code%2520Generation%2520via%2520Oracle-Guided%2520Synthetic%2520Training%250A%2520%2520Data%26entry.906535625%3DHossein%2520Hajipour%2520and%2520Lea%2520Sch%25C3%25B6nherr%2520and%2520Thorsten%2520Holz%2520and%2520Mario%2520Fritz%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520great%2520potential%2520for%2520automatic%2520code%250Ageneration%2520and%2520form%2520the%2520basis%2520for%2520various%2520tools%2520such%2520as%2520GitHub%2520Copilot.%250AHowever%252C%2520recent%2520studies%2520highlight%2520that%2520many%2520LLM-generated%2520code%2520contains%2520serious%250Asecurity%2520vulnerabilities.%2520While%2520previous%2520work%2520tries%2520to%2520address%2520this%2520by%2520training%250Amodels%2520that%2520generate%2520secure%2520code%252C%2520these%2520attempts%2520remain%2520constrained%2520by%2520limited%250Aaccess%2520to%2520training%2520data%2520and%2520labor-intensive%2520data%2520preparation.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520HexaCoder%252C%2520a%2520novel%2520approach%2520to%2520enhance%2520the%250Aability%2520of%2520LLMs%2520to%2520generate%2520secure%2520codes%2520by%2520automatically%2520synthesizing%2520secure%250Acodes%252C%2520which%2520reduces%2520the%2520effort%2520of%2520finding%2520suitable%2520training%2520data.%2520HexaCoder%250Acomprises%2520two%2520key%2520components%253A%2520an%2520oracle-guided%2520data%2520synthesis%2520pipeline%2520and%2520a%250Atwo-step%2520process%2520for%2520secure%2520code%2520generation.%2520The%2520data%2520synthesis%2520pipeline%250Agenerates%2520pairs%2520of%2520vulnerable%2520and%2520fixed%2520codes%2520for%2520specific%2520Common%2520Weakness%250AEnumeration%2520%2528CWE%2529%2520types%2520by%2520utilizing%2520a%2520state-of-the-art%2520LLM%2520for%2520repairing%250Avulnerable%2520code.%2520A%2520security%2520oracle%2520identifies%2520vulnerabilities%252C%2520and%2520a%250Astate-of-the-art%2520LLM%2520repairs%2520them%2520by%2520extending%2520and/or%2520editing%2520the%2520codes%252C%250Acreating%2520data%2520pairs%2520for%2520fine-tuning%2520using%2520the%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%250Amethod.%2520Each%2520example%2520of%2520our%2520fine-tuning%2520dataset%2520includes%2520the%2520necessary%250Asecurity-related%2520libraries%2520and%2520code%2520that%2520form%2520the%2520basis%2520of%2520our%2520novel%2520two-step%250Ageneration%2520approach.%2520This%2520allows%2520the%2520model%2520to%2520integrate%2520security-relevant%250Alibraries%2520before%2520generating%2520the%2520main%2520code%252C%2520significantly%2520reducing%2520the%2520number%2520of%250Agenerated%2520vulnerable%2520codes%2520by%2520up%2520to%252085%2525%2520compared%2520to%2520the%2520baseline%2520methods.%2520We%250Aperform%2520extensive%2520evaluations%2520on%2520three%2520different%2520benchmarks%2520for%2520four%2520LLMs%252C%250Ademonstrating%2520that%2520HexaCoder%2520not%2520only%2520improves%2520the%2520security%2520of%2520the%2520generated%250Acode%2520but%2520also%2520maintains%2520a%2520high%2520level%2520of%2520functional%2520correctness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HexaCoder%3A%20Secure%20Code%20Generation%20via%20Oracle-Guided%20Synthetic%20Training%0A%20%20Data&entry.906535625=Hossein%20Hajipour%20and%20Lea%20Sch%C3%B6nherr%20and%20Thorsten%20Holz%20and%20Mario%20Fritz&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20great%20potential%20for%20automatic%20code%0Ageneration%20and%20form%20the%20basis%20for%20various%20tools%20such%20as%20GitHub%20Copilot.%0AHowever%2C%20recent%20studies%20highlight%20that%20many%20LLM-generated%20code%20contains%20serious%0Asecurity%20vulnerabilities.%20While%20previous%20work%20tries%20to%20address%20this%20by%20training%0Amodels%20that%20generate%20secure%20code%2C%20these%20attempts%20remain%20constrained%20by%20limited%0Aaccess%20to%20training%20data%20and%20labor-intensive%20data%20preparation.%0A%20%20In%20this%20paper%2C%20we%20introduce%20HexaCoder%2C%20a%20novel%20approach%20to%20enhance%20the%0Aability%20of%20LLMs%20to%20generate%20secure%20codes%20by%20automatically%20synthesizing%20secure%0Acodes%2C%20which%20reduces%20the%20effort%20of%20finding%20suitable%20training%20data.%20HexaCoder%0Acomprises%20two%20key%20components%3A%20an%20oracle-guided%20data%20synthesis%20pipeline%20and%20a%0Atwo-step%20process%20for%20secure%20code%20generation.%20The%20data%20synthesis%20pipeline%0Agenerates%20pairs%20of%20vulnerable%20and%20fixed%20codes%20for%20specific%20Common%20Weakness%0AEnumeration%20%28CWE%29%20types%20by%20utilizing%20a%20state-of-the-art%20LLM%20for%20repairing%0Avulnerable%20code.%20A%20security%20oracle%20identifies%20vulnerabilities%2C%20and%20a%0Astate-of-the-art%20LLM%20repairs%20them%20by%20extending%20and/or%20editing%20the%20codes%2C%0Acreating%20data%20pairs%20for%20fine-tuning%20using%20the%20Low-Rank%20Adaptation%20%28LoRA%29%0Amethod.%20Each%20example%20of%20our%20fine-tuning%20dataset%20includes%20the%20necessary%0Asecurity-related%20libraries%20and%20code%20that%20form%20the%20basis%20of%20our%20novel%20two-step%0Ageneration%20approach.%20This%20allows%20the%20model%20to%20integrate%20security-relevant%0Alibraries%20before%20generating%20the%20main%20code%2C%20significantly%20reducing%20the%20number%20of%0Agenerated%20vulnerable%20codes%20by%20up%20to%2085%25%20compared%20to%20the%20baseline%20methods.%20We%0Aperform%20extensive%20evaluations%20on%20three%20different%20benchmarks%20for%20four%20LLMs%2C%0Ademonstrating%20that%20HexaCoder%20not%20only%20improves%20the%20security%20of%20the%20generated%0Acode%20but%20also%20maintains%20a%20high%20level%20of%20functional%20correctness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06446v1&entry.124074799=Read"},
{"title": "Mitigating Hallucination in Visual-Language Models via Re-Balancing\n  Contrastive Decoding", "author": "Xiaoyu Liang and Jiayuan Yu and Lianrui Mu and Jiedong Zhuang and Jiaqi Hu and Yuchen Yang and Jiangnan Ye and Lu Lu and Jian Chen and Haoji Hu", "abstract": "  Although Visual-Language Models (VLMs) have shown impressive capabilities in\ntasks like visual question answering and image captioning, they still struggle\nwith hallucinations. Analysis of attention distribution in these models shows\nthat VLMs tend to processing textual tokens rather than visual tokens. This\nimbalance of attention distribution causes VLMs to favor textual knowledge in\nthe case of multimodal knowledge conflicts, resulting in differences from the\nimage information. In this paper, we propose Re-Balancing Contrastive Decoding\n(RBD) method, which employs textual and visual branches to recalibrate\nattention distribution in VLMs. Specifically, the textual branch injects image\nnoise to stimulate the model's dependency on text, thereby reducing textual\nbias. Concurrently, the visual branch focuses on the selection of significant\ntokens, refining the attention mechanism to highlight the primary subject. This\ndual-branch strategy enables the RBD method to diminish textual bias while\nenhancing visual information. Experimental results demonstrate that our method,\nRBD, outperforms the existing methods by the CHAIR and POPE metrics, mitigate\nhallucinations without reducing the model's general capabilities.\n", "link": "http://arxiv.org/abs/2409.06485v1", "date": "2024-09-10", "relevancy": 2.2127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Hallucination%20in%20Visual-Language%20Models%20via%20Re-Balancing%0A%20%20Contrastive%20Decoding&body=Title%3A%20Mitigating%20Hallucination%20in%20Visual-Language%20Models%20via%20Re-Balancing%0A%20%20Contrastive%20Decoding%0AAuthor%3A%20Xiaoyu%20Liang%20and%20Jiayuan%20Yu%20and%20Lianrui%20Mu%20and%20Jiedong%20Zhuang%20and%20Jiaqi%20Hu%20and%20Yuchen%20Yang%20and%20Jiangnan%20Ye%20and%20Lu%20Lu%20and%20Jian%20Chen%20and%20Haoji%20Hu%0AAbstract%3A%20%20%20Although%20Visual-Language%20Models%20%28VLMs%29%20have%20shown%20impressive%20capabilities%20in%0Atasks%20like%20visual%20question%20answering%20and%20image%20captioning%2C%20they%20still%20struggle%0Awith%20hallucinations.%20Analysis%20of%20attention%20distribution%20in%20these%20models%20shows%0Athat%20VLMs%20tend%20to%20processing%20textual%20tokens%20rather%20than%20visual%20tokens.%20This%0Aimbalance%20of%20attention%20distribution%20causes%20VLMs%20to%20favor%20textual%20knowledge%20in%0Athe%20case%20of%20multimodal%20knowledge%20conflicts%2C%20resulting%20in%20differences%20from%20the%0Aimage%20information.%20In%20this%20paper%2C%20we%20propose%20Re-Balancing%20Contrastive%20Decoding%0A%28RBD%29%20method%2C%20which%20employs%20textual%20and%20visual%20branches%20to%20recalibrate%0Aattention%20distribution%20in%20VLMs.%20Specifically%2C%20the%20textual%20branch%20injects%20image%0Anoise%20to%20stimulate%20the%20model%27s%20dependency%20on%20text%2C%20thereby%20reducing%20textual%0Abias.%20Concurrently%2C%20the%20visual%20branch%20focuses%20on%20the%20selection%20of%20significant%0Atokens%2C%20refining%20the%20attention%20mechanism%20to%20highlight%20the%20primary%20subject.%20This%0Adual-branch%20strategy%20enables%20the%20RBD%20method%20to%20diminish%20textual%20bias%20while%0Aenhancing%20visual%20information.%20Experimental%20results%20demonstrate%20that%20our%20method%2C%0ARBD%2C%20outperforms%20the%20existing%20methods%20by%20the%20CHAIR%20and%20POPE%20metrics%2C%20mitigate%0Ahallucinations%20without%20reducing%20the%20model%27s%20general%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Hallucination%2520in%2520Visual-Language%2520Models%2520via%2520Re-Balancing%250A%2520%2520Contrastive%2520Decoding%26entry.906535625%3DXiaoyu%2520Liang%2520and%2520Jiayuan%2520Yu%2520and%2520Lianrui%2520Mu%2520and%2520Jiedong%2520Zhuang%2520and%2520Jiaqi%2520Hu%2520and%2520Yuchen%2520Yang%2520and%2520Jiangnan%2520Ye%2520and%2520Lu%2520Lu%2520and%2520Jian%2520Chen%2520and%2520Haoji%2520Hu%26entry.1292438233%3D%2520%2520Although%2520Visual-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520impressive%2520capabilities%2520in%250Atasks%2520like%2520visual%2520question%2520answering%2520and%2520image%2520captioning%252C%2520they%2520still%2520struggle%250Awith%2520hallucinations.%2520Analysis%2520of%2520attention%2520distribution%2520in%2520these%2520models%2520shows%250Athat%2520VLMs%2520tend%2520to%2520processing%2520textual%2520tokens%2520rather%2520than%2520visual%2520tokens.%2520This%250Aimbalance%2520of%2520attention%2520distribution%2520causes%2520VLMs%2520to%2520favor%2520textual%2520knowledge%2520in%250Athe%2520case%2520of%2520multimodal%2520knowledge%2520conflicts%252C%2520resulting%2520in%2520differences%2520from%2520the%250Aimage%2520information.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Re-Balancing%2520Contrastive%2520Decoding%250A%2528RBD%2529%2520method%252C%2520which%2520employs%2520textual%2520and%2520visual%2520branches%2520to%2520recalibrate%250Aattention%2520distribution%2520in%2520VLMs.%2520Specifically%252C%2520the%2520textual%2520branch%2520injects%2520image%250Anoise%2520to%2520stimulate%2520the%2520model%2527s%2520dependency%2520on%2520text%252C%2520thereby%2520reducing%2520textual%250Abias.%2520Concurrently%252C%2520the%2520visual%2520branch%2520focuses%2520on%2520the%2520selection%2520of%2520significant%250Atokens%252C%2520refining%2520the%2520attention%2520mechanism%2520to%2520highlight%2520the%2520primary%2520subject.%2520This%250Adual-branch%2520strategy%2520enables%2520the%2520RBD%2520method%2520to%2520diminish%2520textual%2520bias%2520while%250Aenhancing%2520visual%2520information.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%252C%250ARBD%252C%2520outperforms%2520the%2520existing%2520methods%2520by%2520the%2520CHAIR%2520and%2520POPE%2520metrics%252C%2520mitigate%250Ahallucinations%2520without%2520reducing%2520the%2520model%2527s%2520general%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Hallucination%20in%20Visual-Language%20Models%20via%20Re-Balancing%0A%20%20Contrastive%20Decoding&entry.906535625=Xiaoyu%20Liang%20and%20Jiayuan%20Yu%20and%20Lianrui%20Mu%20and%20Jiedong%20Zhuang%20and%20Jiaqi%20Hu%20and%20Yuchen%20Yang%20and%20Jiangnan%20Ye%20and%20Lu%20Lu%20and%20Jian%20Chen%20and%20Haoji%20Hu&entry.1292438233=%20%20Although%20Visual-Language%20Models%20%28VLMs%29%20have%20shown%20impressive%20capabilities%20in%0Atasks%20like%20visual%20question%20answering%20and%20image%20captioning%2C%20they%20still%20struggle%0Awith%20hallucinations.%20Analysis%20of%20attention%20distribution%20in%20these%20models%20shows%0Athat%20VLMs%20tend%20to%20processing%20textual%20tokens%20rather%20than%20visual%20tokens.%20This%0Aimbalance%20of%20attention%20distribution%20causes%20VLMs%20to%20favor%20textual%20knowledge%20in%0Athe%20case%20of%20multimodal%20knowledge%20conflicts%2C%20resulting%20in%20differences%20from%20the%0Aimage%20information.%20In%20this%20paper%2C%20we%20propose%20Re-Balancing%20Contrastive%20Decoding%0A%28RBD%29%20method%2C%20which%20employs%20textual%20and%20visual%20branches%20to%20recalibrate%0Aattention%20distribution%20in%20VLMs.%20Specifically%2C%20the%20textual%20branch%20injects%20image%0Anoise%20to%20stimulate%20the%20model%27s%20dependency%20on%20text%2C%20thereby%20reducing%20textual%0Abias.%20Concurrently%2C%20the%20visual%20branch%20focuses%20on%20the%20selection%20of%20significant%0Atokens%2C%20refining%20the%20attention%20mechanism%20to%20highlight%20the%20primary%20subject.%20This%0Adual-branch%20strategy%20enables%20the%20RBD%20method%20to%20diminish%20textual%20bias%20while%0Aenhancing%20visual%20information.%20Experimental%20results%20demonstrate%20that%20our%20method%2C%0ARBD%2C%20outperforms%20the%20existing%20methods%20by%20the%20CHAIR%20and%20POPE%20metrics%2C%20mitigate%0Ahallucinations%20without%20reducing%20the%20model%27s%20general%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06485v1&entry.124074799=Read"},
{"title": "TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction", "author": "Bahar Aydemir and Ludo Hoffstetter and Tong Zhang and Mathieu Salzmann and Sabine S\u00fcsstrunk", "abstract": "  Deep saliency prediction algorithms complement the object recognition\nfeatures, they typically rely on additional information, such as scene context,\nsemantic relationships, gaze direction, and object dissimilarity. However, none\nof these models consider the temporal nature of gaze shifts during image\nobservation. We introduce a novel saliency prediction model that learns to\noutput saliency maps in sequential time intervals by exploiting human temporal\nattention patterns. Our approach locally modulates the saliency predictions by\ncombining the learned temporal maps. Our experiments show that our method\noutperforms the state-of-the-art models, including a multi-duration saliency\nmodel, on the SALICON benchmark. Our code will be publicly available on GitHub.\n", "link": "http://arxiv.org/abs/2301.02315v2", "date": "2024-09-10", "relevancy": 2.1879, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5858}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5309}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempSAL%20--%20Uncovering%20Temporal%20Information%20for%20Deep%20Saliency%20Prediction&body=Title%3A%20TempSAL%20--%20Uncovering%20Temporal%20Information%20for%20Deep%20Saliency%20Prediction%0AAuthor%3A%20Bahar%20Aydemir%20and%20Ludo%20Hoffstetter%20and%20Tong%20Zhang%20and%20Mathieu%20Salzmann%20and%20Sabine%20S%C3%BCsstrunk%0AAbstract%3A%20%20%20Deep%20saliency%20prediction%20algorithms%20complement%20the%20object%20recognition%0Afeatures%2C%20they%20typically%20rely%20on%20additional%20information%2C%20such%20as%20scene%20context%2C%0Asemantic%20relationships%2C%20gaze%20direction%2C%20and%20object%20dissimilarity.%20However%2C%20none%0Aof%20these%20models%20consider%20the%20temporal%20nature%20of%20gaze%20shifts%20during%20image%0Aobservation.%20We%20introduce%20a%20novel%20saliency%20prediction%20model%20that%20learns%20to%0Aoutput%20saliency%20maps%20in%20sequential%20time%20intervals%20by%20exploiting%20human%20temporal%0Aattention%20patterns.%20Our%20approach%20locally%20modulates%20the%20saliency%20predictions%20by%0Acombining%20the%20learned%20temporal%20maps.%20Our%20experiments%20show%20that%20our%20method%0Aoutperforms%20the%20state-of-the-art%20models%2C%20including%20a%20multi-duration%20saliency%0Amodel%2C%20on%20the%20SALICON%20benchmark.%20Our%20code%20will%20be%20publicly%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.02315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempSAL%2520--%2520Uncovering%2520Temporal%2520Information%2520for%2520Deep%2520Saliency%2520Prediction%26entry.906535625%3DBahar%2520Aydemir%2520and%2520Ludo%2520Hoffstetter%2520and%2520Tong%2520Zhang%2520and%2520Mathieu%2520Salzmann%2520and%2520Sabine%2520S%25C3%25BCsstrunk%26entry.1292438233%3D%2520%2520Deep%2520saliency%2520prediction%2520algorithms%2520complement%2520the%2520object%2520recognition%250Afeatures%252C%2520they%2520typically%2520rely%2520on%2520additional%2520information%252C%2520such%2520as%2520scene%2520context%252C%250Asemantic%2520relationships%252C%2520gaze%2520direction%252C%2520and%2520object%2520dissimilarity.%2520However%252C%2520none%250Aof%2520these%2520models%2520consider%2520the%2520temporal%2520nature%2520of%2520gaze%2520shifts%2520during%2520image%250Aobservation.%2520We%2520introduce%2520a%2520novel%2520saliency%2520prediction%2520model%2520that%2520learns%2520to%250Aoutput%2520saliency%2520maps%2520in%2520sequential%2520time%2520intervals%2520by%2520exploiting%2520human%2520temporal%250Aattention%2520patterns.%2520Our%2520approach%2520locally%2520modulates%2520the%2520saliency%2520predictions%2520by%250Acombining%2520the%2520learned%2520temporal%2520maps.%2520Our%2520experiments%2520show%2520that%2520our%2520method%250Aoutperforms%2520the%2520state-of-the-art%2520models%252C%2520including%2520a%2520multi-duration%2520saliency%250Amodel%252C%2520on%2520the%2520SALICON%2520benchmark.%2520Our%2520code%2520will%2520be%2520publicly%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.02315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempSAL%20--%20Uncovering%20Temporal%20Information%20for%20Deep%20Saliency%20Prediction&entry.906535625=Bahar%20Aydemir%20and%20Ludo%20Hoffstetter%20and%20Tong%20Zhang%20and%20Mathieu%20Salzmann%20and%20Sabine%20S%C3%BCsstrunk&entry.1292438233=%20%20Deep%20saliency%20prediction%20algorithms%20complement%20the%20object%20recognition%0Afeatures%2C%20they%20typically%20rely%20on%20additional%20information%2C%20such%20as%20scene%20context%2C%0Asemantic%20relationships%2C%20gaze%20direction%2C%20and%20object%20dissimilarity.%20However%2C%20none%0Aof%20these%20models%20consider%20the%20temporal%20nature%20of%20gaze%20shifts%20during%20image%0Aobservation.%20We%20introduce%20a%20novel%20saliency%20prediction%20model%20that%20learns%20to%0Aoutput%20saliency%20maps%20in%20sequential%20time%20intervals%20by%20exploiting%20human%20temporal%0Aattention%20patterns.%20Our%20approach%20locally%20modulates%20the%20saliency%20predictions%20by%0Acombining%20the%20learned%20temporal%20maps.%20Our%20experiments%20show%20that%20our%20method%0Aoutperforms%20the%20state-of-the-art%20models%2C%20including%20a%20multi-duration%20saliency%0Amodel%2C%20on%20the%20SALICON%20benchmark.%20Our%20code%20will%20be%20publicly%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.02315v2&entry.124074799=Read"},
{"title": "CathFlow: Self-Supervised Segmentation of Catheters in Interventional\n  Ultrasound Using Optical Flow and Transformers", "author": "Alex Ranne and Liming Kuang and Yordanka Velikova and Nassir Navab and Ferdinando Rodriguez y Baena", "abstract": "  In minimally invasive endovascular procedures, contrast-enhanced angiography\nremains the most robust imaging technique. However, it is at the expense of the\npatient and clinician's health due to prolonged radiation exposure. As an\nalternative, interventional ultrasound has notable benefits such as being\nradiation-free, fast to deploy, and having a small footprint in the operating\nroom. Yet, ultrasound is hard to interpret, and highly prone to artifacts and\nnoise. Additionally, interventional radiologists must undergo extensive\ntraining before they become qualified to diagnose and treat patients\neffectively, leading to a shortage of staff, and a lack of open-source\ndatasets. In this work, we seek to address both problems by introducing a\nself-supervised deep learning architecture to segment catheters in longitudinal\nultrasound images, without demanding any labeled data. The network architecture\nbuilds upon AiAReSeg, a segmentation transformer built with the Attention in\nAttention mechanism, and is capable of learning feature changes across time and\nspace. To facilitate training, we used synthetic ultrasound data based on\nphysics-driven catheter insertion simulations, and translated the data into a\nunique CT-Ultrasound common domain, CACTUSS, to improve the segmentation\nperformance. We generated ground truth segmentation masks by computing the\noptical flow between adjacent frames using FlowNet2, and performed thresholding\nto obtain a binary map estimate. Finally, we validated our model on a test\ndataset, consisting of unseen synthetic data and images collected from silicon\naorta phantoms, thus demonstrating its potential for applications to clinical\ndata in the future.\n", "link": "http://arxiv.org/abs/2403.14465v2", "date": "2024-09-10", "relevancy": 2.1613, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5794}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.533}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CathFlow%3A%20Self-Supervised%20Segmentation%20of%20Catheters%20in%20Interventional%0A%20%20Ultrasound%20Using%20Optical%20Flow%20and%20Transformers&body=Title%3A%20CathFlow%3A%20Self-Supervised%20Segmentation%20of%20Catheters%20in%20Interventional%0A%20%20Ultrasound%20Using%20Optical%20Flow%20and%20Transformers%0AAuthor%3A%20Alex%20Ranne%20and%20Liming%20Kuang%20and%20Yordanka%20Velikova%20and%20Nassir%20Navab%20and%20Ferdinando%20Rodriguez%20y%20Baena%0AAbstract%3A%20%20%20In%20minimally%20invasive%20endovascular%20procedures%2C%20contrast-enhanced%20angiography%0Aremains%20the%20most%20robust%20imaging%20technique.%20However%2C%20it%20is%20at%20the%20expense%20of%20the%0Apatient%20and%20clinician%27s%20health%20due%20to%20prolonged%20radiation%20exposure.%20As%20an%0Aalternative%2C%20interventional%20ultrasound%20has%20notable%20benefits%20such%20as%20being%0Aradiation-free%2C%20fast%20to%20deploy%2C%20and%20having%20a%20small%20footprint%20in%20the%20operating%0Aroom.%20Yet%2C%20ultrasound%20is%20hard%20to%20interpret%2C%20and%20highly%20prone%20to%20artifacts%20and%0Anoise.%20Additionally%2C%20interventional%20radiologists%20must%20undergo%20extensive%0Atraining%20before%20they%20become%20qualified%20to%20diagnose%20and%20treat%20patients%0Aeffectively%2C%20leading%20to%20a%20shortage%20of%20staff%2C%20and%20a%20lack%20of%20open-source%0Adatasets.%20In%20this%20work%2C%20we%20seek%20to%20address%20both%20problems%20by%20introducing%20a%0Aself-supervised%20deep%20learning%20architecture%20to%20segment%20catheters%20in%20longitudinal%0Aultrasound%20images%2C%20without%20demanding%20any%20labeled%20data.%20The%20network%20architecture%0Abuilds%20upon%20AiAReSeg%2C%20a%20segmentation%20transformer%20built%20with%20the%20Attention%20in%0AAttention%20mechanism%2C%20and%20is%20capable%20of%20learning%20feature%20changes%20across%20time%20and%0Aspace.%20To%20facilitate%20training%2C%20we%20used%20synthetic%20ultrasound%20data%20based%20on%0Aphysics-driven%20catheter%20insertion%20simulations%2C%20and%20translated%20the%20data%20into%20a%0Aunique%20CT-Ultrasound%20common%20domain%2C%20CACTUSS%2C%20to%20improve%20the%20segmentation%0Aperformance.%20We%20generated%20ground%20truth%20segmentation%20masks%20by%20computing%20the%0Aoptical%20flow%20between%20adjacent%20frames%20using%20FlowNet2%2C%20and%20performed%20thresholding%0Ato%20obtain%20a%20binary%20map%20estimate.%20Finally%2C%20we%20validated%20our%20model%20on%20a%20test%0Adataset%2C%20consisting%20of%20unseen%20synthetic%20data%20and%20images%20collected%20from%20silicon%0Aaorta%20phantoms%2C%20thus%20demonstrating%20its%20potential%20for%20applications%20to%20clinical%0Adata%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCathFlow%253A%2520Self-Supervised%2520Segmentation%2520of%2520Catheters%2520in%2520Interventional%250A%2520%2520Ultrasound%2520Using%2520Optical%2520Flow%2520and%2520Transformers%26entry.906535625%3DAlex%2520Ranne%2520and%2520Liming%2520Kuang%2520and%2520Yordanka%2520Velikova%2520and%2520Nassir%2520Navab%2520and%2520Ferdinando%2520Rodriguez%2520y%2520Baena%26entry.1292438233%3D%2520%2520In%2520minimally%2520invasive%2520endovascular%2520procedures%252C%2520contrast-enhanced%2520angiography%250Aremains%2520the%2520most%2520robust%2520imaging%2520technique.%2520However%252C%2520it%2520is%2520at%2520the%2520expense%2520of%2520the%250Apatient%2520and%2520clinician%2527s%2520health%2520due%2520to%2520prolonged%2520radiation%2520exposure.%2520As%2520an%250Aalternative%252C%2520interventional%2520ultrasound%2520has%2520notable%2520benefits%2520such%2520as%2520being%250Aradiation-free%252C%2520fast%2520to%2520deploy%252C%2520and%2520having%2520a%2520small%2520footprint%2520in%2520the%2520operating%250Aroom.%2520Yet%252C%2520ultrasound%2520is%2520hard%2520to%2520interpret%252C%2520and%2520highly%2520prone%2520to%2520artifacts%2520and%250Anoise.%2520Additionally%252C%2520interventional%2520radiologists%2520must%2520undergo%2520extensive%250Atraining%2520before%2520they%2520become%2520qualified%2520to%2520diagnose%2520and%2520treat%2520patients%250Aeffectively%252C%2520leading%2520to%2520a%2520shortage%2520of%2520staff%252C%2520and%2520a%2520lack%2520of%2520open-source%250Adatasets.%2520In%2520this%2520work%252C%2520we%2520seek%2520to%2520address%2520both%2520problems%2520by%2520introducing%2520a%250Aself-supervised%2520deep%2520learning%2520architecture%2520to%2520segment%2520catheters%2520in%2520longitudinal%250Aultrasound%2520images%252C%2520without%2520demanding%2520any%2520labeled%2520data.%2520The%2520network%2520architecture%250Abuilds%2520upon%2520AiAReSeg%252C%2520a%2520segmentation%2520transformer%2520built%2520with%2520the%2520Attention%2520in%250AAttention%2520mechanism%252C%2520and%2520is%2520capable%2520of%2520learning%2520feature%2520changes%2520across%2520time%2520and%250Aspace.%2520To%2520facilitate%2520training%252C%2520we%2520used%2520synthetic%2520ultrasound%2520data%2520based%2520on%250Aphysics-driven%2520catheter%2520insertion%2520simulations%252C%2520and%2520translated%2520the%2520data%2520into%2520a%250Aunique%2520CT-Ultrasound%2520common%2520domain%252C%2520CACTUSS%252C%2520to%2520improve%2520the%2520segmentation%250Aperformance.%2520We%2520generated%2520ground%2520truth%2520segmentation%2520masks%2520by%2520computing%2520the%250Aoptical%2520flow%2520between%2520adjacent%2520frames%2520using%2520FlowNet2%252C%2520and%2520performed%2520thresholding%250Ato%2520obtain%2520a%2520binary%2520map%2520estimate.%2520Finally%252C%2520we%2520validated%2520our%2520model%2520on%2520a%2520test%250Adataset%252C%2520consisting%2520of%2520unseen%2520synthetic%2520data%2520and%2520images%2520collected%2520from%2520silicon%250Aaorta%2520phantoms%252C%2520thus%2520demonstrating%2520its%2520potential%2520for%2520applications%2520to%2520clinical%250Adata%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CathFlow%3A%20Self-Supervised%20Segmentation%20of%20Catheters%20in%20Interventional%0A%20%20Ultrasound%20Using%20Optical%20Flow%20and%20Transformers&entry.906535625=Alex%20Ranne%20and%20Liming%20Kuang%20and%20Yordanka%20Velikova%20and%20Nassir%20Navab%20and%20Ferdinando%20Rodriguez%20y%20Baena&entry.1292438233=%20%20In%20minimally%20invasive%20endovascular%20procedures%2C%20contrast-enhanced%20angiography%0Aremains%20the%20most%20robust%20imaging%20technique.%20However%2C%20it%20is%20at%20the%20expense%20of%20the%0Apatient%20and%20clinician%27s%20health%20due%20to%20prolonged%20radiation%20exposure.%20As%20an%0Aalternative%2C%20interventional%20ultrasound%20has%20notable%20benefits%20such%20as%20being%0Aradiation-free%2C%20fast%20to%20deploy%2C%20and%20having%20a%20small%20footprint%20in%20the%20operating%0Aroom.%20Yet%2C%20ultrasound%20is%20hard%20to%20interpret%2C%20and%20highly%20prone%20to%20artifacts%20and%0Anoise.%20Additionally%2C%20interventional%20radiologists%20must%20undergo%20extensive%0Atraining%20before%20they%20become%20qualified%20to%20diagnose%20and%20treat%20patients%0Aeffectively%2C%20leading%20to%20a%20shortage%20of%20staff%2C%20and%20a%20lack%20of%20open-source%0Adatasets.%20In%20this%20work%2C%20we%20seek%20to%20address%20both%20problems%20by%20introducing%20a%0Aself-supervised%20deep%20learning%20architecture%20to%20segment%20catheters%20in%20longitudinal%0Aultrasound%20images%2C%20without%20demanding%20any%20labeled%20data.%20The%20network%20architecture%0Abuilds%20upon%20AiAReSeg%2C%20a%20segmentation%20transformer%20built%20with%20the%20Attention%20in%0AAttention%20mechanism%2C%20and%20is%20capable%20of%20learning%20feature%20changes%20across%20time%20and%0Aspace.%20To%20facilitate%20training%2C%20we%20used%20synthetic%20ultrasound%20data%20based%20on%0Aphysics-driven%20catheter%20insertion%20simulations%2C%20and%20translated%20the%20data%20into%20a%0Aunique%20CT-Ultrasound%20common%20domain%2C%20CACTUSS%2C%20to%20improve%20the%20segmentation%0Aperformance.%20We%20generated%20ground%20truth%20segmentation%20masks%20by%20computing%20the%0Aoptical%20flow%20between%20adjacent%20frames%20using%20FlowNet2%2C%20and%20performed%20thresholding%0Ato%20obtain%20a%20binary%20map%20estimate.%20Finally%2C%20we%20validated%20our%20model%20on%20a%20test%0Adataset%2C%20consisting%20of%20unseen%20synthetic%20data%20and%20images%20collected%20from%20silicon%0Aaorta%20phantoms%2C%20thus%20demonstrating%20its%20potential%20for%20applications%20to%20clinical%0Adata%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14465v2&entry.124074799=Read"},
{"title": "SUMix: Mixup with Semantic and Uncertain Information", "author": "Huafeng Qin and Xin Jin and Hongyu Zhu and Hongchao Liao and Moun\u00eem A. El-Yacoubi and Xinbo Gao", "abstract": "  Mixup data augmentation approaches have been applied for various tasks of\ndeep learning to improve the generalization ability of deep neural networks.\nSome existing approaches CutMix, SaliencyMix, etc. randomly replace a patch in\none image with patches from another to generate the mixed image. Similarly, the\ncorresponding labels are linearly combined by a fixed ratio $\\lambda$ by l. The\nobjects in two images may be overlapped during the mixing process, so some\nsemantic information is corrupted in the mixed samples. In this case, the mixed\nimage does not match the mixed label information. Besides, such a label may\nmislead the deep learning model training, which results in poor performance. To\nsolve this problem, we proposed a novel approach named SUMix to learn the\nmixing ratio as well as the uncertainty for the mixed samples during the\ntraining process. First, we design a learnable similarity function to compute\nan accurate mix ratio. Second, an approach is investigated as a regularized\nterm to model the uncertainty of the mixed samples. We conduct experiments on\nfive image benchmarks, and extensive experimental results imply that our method\nis capable of improving the performance of classifiers with different\ncutting-based mixup approaches. The source code is available at\nhttps://github.com/JinXins/SUMix.\n", "link": "http://arxiv.org/abs/2407.07805v4", "date": "2024-09-10", "relevancy": 2.1498, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5767}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5444}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information&body=Title%3A%20SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information%0AAuthor%3A%20Huafeng%20Qin%20and%20Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Hongchao%20Liao%20and%20Moun%C3%AEm%20A.%20El-Yacoubi%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Mixup%20data%20augmentation%20approaches%20have%20been%20applied%20for%20various%20tasks%20of%0Adeep%20learning%20to%20improve%20the%20generalization%20ability%20of%20deep%20neural%20networks.%0ASome%20existing%20approaches%20CutMix%2C%20SaliencyMix%2C%20etc.%20randomly%20replace%20a%20patch%20in%0Aone%20image%20with%20patches%20from%20another%20to%20generate%20the%20mixed%20image.%20Similarly%2C%20the%0Acorresponding%20labels%20are%20linearly%20combined%20by%20a%20fixed%20ratio%20%24%5Clambda%24%20by%20l.%20The%0Aobjects%20in%20two%20images%20may%20be%20overlapped%20during%20the%20mixing%20process%2C%20so%20some%0Asemantic%20information%20is%20corrupted%20in%20the%20mixed%20samples.%20In%20this%20case%2C%20the%20mixed%0Aimage%20does%20not%20match%20the%20mixed%20label%20information.%20Besides%2C%20such%20a%20label%20may%0Amislead%20the%20deep%20learning%20model%20training%2C%20which%20results%20in%20poor%20performance.%20To%0Asolve%20this%20problem%2C%20we%20proposed%20a%20novel%20approach%20named%20SUMix%20to%20learn%20the%0Amixing%20ratio%20as%20well%20as%20the%20uncertainty%20for%20the%20mixed%20samples%20during%20the%0Atraining%20process.%20First%2C%20we%20design%20a%20learnable%20similarity%20function%20to%20compute%0Aan%20accurate%20mix%20ratio.%20Second%2C%20an%20approach%20is%20investigated%20as%20a%20regularized%0Aterm%20to%20model%20the%20uncertainty%20of%20the%20mixed%20samples.%20We%20conduct%20experiments%20on%0Afive%20image%20benchmarks%2C%20and%20extensive%20experimental%20results%20imply%20that%20our%20method%0Ais%20capable%20of%20improving%20the%20performance%20of%20classifiers%20with%20different%0Acutting-based%20mixup%20approaches.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JinXins/SUMix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07805v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUMix%253A%2520Mixup%2520with%2520Semantic%2520and%2520Uncertain%2520Information%26entry.906535625%3DHuafeng%2520Qin%2520and%2520Xin%2520Jin%2520and%2520Hongyu%2520Zhu%2520and%2520Hongchao%2520Liao%2520and%2520Moun%25C3%25AEm%2520A.%2520El-Yacoubi%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Mixup%2520data%2520augmentation%2520approaches%2520have%2520been%2520applied%2520for%2520various%2520tasks%2520of%250Adeep%2520learning%2520to%2520improve%2520the%2520generalization%2520ability%2520of%2520deep%2520neural%2520networks.%250ASome%2520existing%2520approaches%2520CutMix%252C%2520SaliencyMix%252C%2520etc.%2520randomly%2520replace%2520a%2520patch%2520in%250Aone%2520image%2520with%2520patches%2520from%2520another%2520to%2520generate%2520the%2520mixed%2520image.%2520Similarly%252C%2520the%250Acorresponding%2520labels%2520are%2520linearly%2520combined%2520by%2520a%2520fixed%2520ratio%2520%2524%255Clambda%2524%2520by%2520l.%2520The%250Aobjects%2520in%2520two%2520images%2520may%2520be%2520overlapped%2520during%2520the%2520mixing%2520process%252C%2520so%2520some%250Asemantic%2520information%2520is%2520corrupted%2520in%2520the%2520mixed%2520samples.%2520In%2520this%2520case%252C%2520the%2520mixed%250Aimage%2520does%2520not%2520match%2520the%2520mixed%2520label%2520information.%2520Besides%252C%2520such%2520a%2520label%2520may%250Amislead%2520the%2520deep%2520learning%2520model%2520training%252C%2520which%2520results%2520in%2520poor%2520performance.%2520To%250Asolve%2520this%2520problem%252C%2520we%2520proposed%2520a%2520novel%2520approach%2520named%2520SUMix%2520to%2520learn%2520the%250Amixing%2520ratio%2520as%2520well%2520as%2520the%2520uncertainty%2520for%2520the%2520mixed%2520samples%2520during%2520the%250Atraining%2520process.%2520First%252C%2520we%2520design%2520a%2520learnable%2520similarity%2520function%2520to%2520compute%250Aan%2520accurate%2520mix%2520ratio.%2520Second%252C%2520an%2520approach%2520is%2520investigated%2520as%2520a%2520regularized%250Aterm%2520to%2520model%2520the%2520uncertainty%2520of%2520the%2520mixed%2520samples.%2520We%2520conduct%2520experiments%2520on%250Afive%2520image%2520benchmarks%252C%2520and%2520extensive%2520experimental%2520results%2520imply%2520that%2520our%2520method%250Ais%2520capable%2520of%2520improving%2520the%2520performance%2520of%2520classifiers%2520with%2520different%250Acutting-based%2520mixup%2520approaches.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/JinXins/SUMix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07805v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUMix%3A%20Mixup%20with%20Semantic%20and%20Uncertain%20Information&entry.906535625=Huafeng%20Qin%20and%20Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Hongchao%20Liao%20and%20Moun%C3%AEm%20A.%20El-Yacoubi%20and%20Xinbo%20Gao&entry.1292438233=%20%20Mixup%20data%20augmentation%20approaches%20have%20been%20applied%20for%20various%20tasks%20of%0Adeep%20learning%20to%20improve%20the%20generalization%20ability%20of%20deep%20neural%20networks.%0ASome%20existing%20approaches%20CutMix%2C%20SaliencyMix%2C%20etc.%20randomly%20replace%20a%20patch%20in%0Aone%20image%20with%20patches%20from%20another%20to%20generate%20the%20mixed%20image.%20Similarly%2C%20the%0Acorresponding%20labels%20are%20linearly%20combined%20by%20a%20fixed%20ratio%20%24%5Clambda%24%20by%20l.%20The%0Aobjects%20in%20two%20images%20may%20be%20overlapped%20during%20the%20mixing%20process%2C%20so%20some%0Asemantic%20information%20is%20corrupted%20in%20the%20mixed%20samples.%20In%20this%20case%2C%20the%20mixed%0Aimage%20does%20not%20match%20the%20mixed%20label%20information.%20Besides%2C%20such%20a%20label%20may%0Amislead%20the%20deep%20learning%20model%20training%2C%20which%20results%20in%20poor%20performance.%20To%0Asolve%20this%20problem%2C%20we%20proposed%20a%20novel%20approach%20named%20SUMix%20to%20learn%20the%0Amixing%20ratio%20as%20well%20as%20the%20uncertainty%20for%20the%20mixed%20samples%20during%20the%0Atraining%20process.%20First%2C%20we%20design%20a%20learnable%20similarity%20function%20to%20compute%0Aan%20accurate%20mix%20ratio.%20Second%2C%20an%20approach%20is%20investigated%20as%20a%20regularized%0Aterm%20to%20model%20the%20uncertainty%20of%20the%20mixed%20samples.%20We%20conduct%20experiments%20on%0Afive%20image%20benchmarks%2C%20and%20extensive%20experimental%20results%20imply%20that%20our%20method%0Ais%20capable%20of%20improving%20the%20performance%20of%20classifiers%20with%20different%0Acutting-based%20mixup%20approaches.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/JinXins/SUMix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07805v4&entry.124074799=Read"},
{"title": "Leveraging LLMs, Graphs and Object Hierarchies for Task Planning in\n  Large-Scale Environments", "author": "Rodrigo P\u00e9rez-Dattari and Zhaoting Li and Robert Babu\u0161ka and Jens Kober and Cosimo Della Santina", "abstract": "  Planning methods struggle with computational intractability in solving\ntask-level problems in large-scale environments. This work explores leveraging\nthe commonsense knowledge encoded in LLMs to empower planning techniques to\ndeal with these complex scenarios. We achieve this by efficiently using LLMs to\nprune irrelevant components from the planning problem's state space,\nsubstantially simplifying its complexity. We demonstrate the efficacy of this\nsystem through extensive experiments within a household simulation environment,\nalongside real-world validation using a 7-DoF manipulator (video\nhttps://youtu.be/6ro2UOtOQS4).\n", "link": "http://arxiv.org/abs/2409.04775v2", "date": "2024-09-10", "relevancy": 2.1397, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20LLMs%2C%20Graphs%20and%20Object%20Hierarchies%20for%20Task%20Planning%20in%0A%20%20Large-Scale%20Environments&body=Title%3A%20Leveraging%20LLMs%2C%20Graphs%20and%20Object%20Hierarchies%20for%20Task%20Planning%20in%0A%20%20Large-Scale%20Environments%0AAuthor%3A%20Rodrigo%20P%C3%A9rez-Dattari%20and%20Zhaoting%20Li%20and%20Robert%20Babu%C5%A1ka%20and%20Jens%20Kober%20and%20Cosimo%20Della%20Santina%0AAbstract%3A%20%20%20Planning%20methods%20struggle%20with%20computational%20intractability%20in%20solving%0Atask-level%20problems%20in%20large-scale%20environments.%20This%20work%20explores%20leveraging%0Athe%20commonsense%20knowledge%20encoded%20in%20LLMs%20to%20empower%20planning%20techniques%20to%0Adeal%20with%20these%20complex%20scenarios.%20We%20achieve%20this%20by%20efficiently%20using%20LLMs%20to%0Aprune%20irrelevant%20components%20from%20the%20planning%20problem%27s%20state%20space%2C%0Asubstantially%20simplifying%20its%20complexity.%20We%20demonstrate%20the%20efficacy%20of%20this%0Asystem%20through%20extensive%20experiments%20within%20a%20household%20simulation%20environment%2C%0Aalongside%20real-world%20validation%20using%20a%207-DoF%20manipulator%20%28video%0Ahttps%3A//youtu.be/6ro2UOtOQS4%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04775v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520LLMs%252C%2520Graphs%2520and%2520Object%2520Hierarchies%2520for%2520Task%2520Planning%2520in%250A%2520%2520Large-Scale%2520Environments%26entry.906535625%3DRodrigo%2520P%25C3%25A9rez-Dattari%2520and%2520Zhaoting%2520Li%2520and%2520Robert%2520Babu%25C5%25A1ka%2520and%2520Jens%2520Kober%2520and%2520Cosimo%2520Della%2520Santina%26entry.1292438233%3D%2520%2520Planning%2520methods%2520struggle%2520with%2520computational%2520intractability%2520in%2520solving%250Atask-level%2520problems%2520in%2520large-scale%2520environments.%2520This%2520work%2520explores%2520leveraging%250Athe%2520commonsense%2520knowledge%2520encoded%2520in%2520LLMs%2520to%2520empower%2520planning%2520techniques%2520to%250Adeal%2520with%2520these%2520complex%2520scenarios.%2520We%2520achieve%2520this%2520by%2520efficiently%2520using%2520LLMs%2520to%250Aprune%2520irrelevant%2520components%2520from%2520the%2520planning%2520problem%2527s%2520state%2520space%252C%250Asubstantially%2520simplifying%2520its%2520complexity.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520this%250Asystem%2520through%2520extensive%2520experiments%2520within%2520a%2520household%2520simulation%2520environment%252C%250Aalongside%2520real-world%2520validation%2520using%2520a%25207-DoF%2520manipulator%2520%2528video%250Ahttps%253A//youtu.be/6ro2UOtOQS4%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04775v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20LLMs%2C%20Graphs%20and%20Object%20Hierarchies%20for%20Task%20Planning%20in%0A%20%20Large-Scale%20Environments&entry.906535625=Rodrigo%20P%C3%A9rez-Dattari%20and%20Zhaoting%20Li%20and%20Robert%20Babu%C5%A1ka%20and%20Jens%20Kober%20and%20Cosimo%20Della%20Santina&entry.1292438233=%20%20Planning%20methods%20struggle%20with%20computational%20intractability%20in%20solving%0Atask-level%20problems%20in%20large-scale%20environments.%20This%20work%20explores%20leveraging%0Athe%20commonsense%20knowledge%20encoded%20in%20LLMs%20to%20empower%20planning%20techniques%20to%0Adeal%20with%20these%20complex%20scenarios.%20We%20achieve%20this%20by%20efficiently%20using%20LLMs%20to%0Aprune%20irrelevant%20components%20from%20the%20planning%20problem%27s%20state%20space%2C%0Asubstantially%20simplifying%20its%20complexity.%20We%20demonstrate%20the%20efficacy%20of%20this%0Asystem%20through%20extensive%20experiments%20within%20a%20household%20simulation%20environment%2C%0Aalongside%20real-world%20validation%20using%20a%207-DoF%20manipulator%20%28video%0Ahttps%3A//youtu.be/6ro2UOtOQS4%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04775v2&entry.124074799=Read"},
{"title": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games", "author": "Juhwan Choi and YoungBin Kim", "abstract": "  Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs.\n", "link": "http://arxiv.org/abs/2409.06518v1", "date": "2024-09-10", "relevancy": 2.131, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Questioning%20Internal%20Knowledge%20Structure%20of%20Large%20Language%20Models%0A%20%20Through%20the%20Lens%20of%20the%20Olympic%20Games&body=Title%3A%20Questioning%20Internal%20Knowledge%20Structure%20of%20Large%20Language%20Models%0A%20%20Through%20the%20Lens%20of%20the%20Olympic%20Games%0AAuthor%3A%20Juhwan%20Choi%20and%20YoungBin%20Kim%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20a%20dominant%20approach%20in%20natural%0Alanguage%20processing%2C%20yet%20their%20internal%20knowledge%20structures%20remain%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20analyze%20the%20internal%20knowledge%20structures%20of%20LLMs%0Ausing%20historical%20medal%20tallies%20from%20the%20Olympic%20Games.%20We%20task%20the%20models%20with%0Aproviding%20the%20medal%20counts%20for%20each%20team%20and%20identifying%20which%20teams%20achieved%0Aspecific%20rankings.%20Our%20results%20reveal%20that%20while%20state-of-the-art%20LLMs%20perform%0Aremarkably%20well%20in%20reporting%20medal%20counts%20for%20individual%20teams%2C%20they%20struggle%0Asignificantly%20with%20questions%20about%20specific%20rankings.%20This%20suggests%20that%20the%0Ainternal%20knowledge%20structures%20of%20LLMs%20are%20fundamentally%20different%20from%20those%20of%0Ahumans%2C%20who%20can%20easily%20infer%20rankings%20from%20known%20medal%20counts.%20To%20support%0Afurther%20research%2C%20we%20publicly%20release%20our%20code%2C%20dataset%2C%20and%20model%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuestioning%2520Internal%2520Knowledge%2520Structure%2520of%2520Large%2520Language%2520Models%250A%2520%2520Through%2520the%2520Lens%2520of%2520the%2520Olympic%2520Games%26entry.906535625%3DJuhwan%2520Choi%2520and%2520YoungBin%2520Kim%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520become%2520a%2520dominant%2520approach%2520in%2520natural%250Alanguage%2520processing%252C%2520yet%2520their%2520internal%2520knowledge%2520structures%2520remain%2520largely%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520internal%2520knowledge%2520structures%2520of%2520LLMs%250Ausing%2520historical%2520medal%2520tallies%2520from%2520the%2520Olympic%2520Games.%2520We%2520task%2520the%2520models%2520with%250Aproviding%2520the%2520medal%2520counts%2520for%2520each%2520team%2520and%2520identifying%2520which%2520teams%2520achieved%250Aspecific%2520rankings.%2520Our%2520results%2520reveal%2520that%2520while%2520state-of-the-art%2520LLMs%2520perform%250Aremarkably%2520well%2520in%2520reporting%2520medal%2520counts%2520for%2520individual%2520teams%252C%2520they%2520struggle%250Asignificantly%2520with%2520questions%2520about%2520specific%2520rankings.%2520This%2520suggests%2520that%2520the%250Ainternal%2520knowledge%2520structures%2520of%2520LLMs%2520are%2520fundamentally%2520different%2520from%2520those%2520of%250Ahumans%252C%2520who%2520can%2520easily%2520infer%2520rankings%2520from%2520known%2520medal%2520counts.%2520To%2520support%250Afurther%2520research%252C%2520we%2520publicly%2520release%2520our%2520code%252C%2520dataset%252C%2520and%2520model%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Questioning%20Internal%20Knowledge%20Structure%20of%20Large%20Language%20Models%0A%20%20Through%20the%20Lens%20of%20the%20Olympic%20Games&entry.906535625=Juhwan%20Choi%20and%20YoungBin%20Kim&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20a%20dominant%20approach%20in%20natural%0Alanguage%20processing%2C%20yet%20their%20internal%20knowledge%20structures%20remain%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20analyze%20the%20internal%20knowledge%20structures%20of%20LLMs%0Ausing%20historical%20medal%20tallies%20from%20the%20Olympic%20Games.%20We%20task%20the%20models%20with%0Aproviding%20the%20medal%20counts%20for%20each%20team%20and%20identifying%20which%20teams%20achieved%0Aspecific%20rankings.%20Our%20results%20reveal%20that%20while%20state-of-the-art%20LLMs%20perform%0Aremarkably%20well%20in%20reporting%20medal%20counts%20for%20individual%20teams%2C%20they%20struggle%0Asignificantly%20with%20questions%20about%20specific%20rankings.%20This%20suggests%20that%20the%0Ainternal%20knowledge%20structures%20of%20LLMs%20are%20fundamentally%20different%20from%20those%20of%0Ahumans%2C%20who%20can%20easily%20infer%20rankings%20from%20known%20medal%20counts.%20To%20support%0Afurther%20research%2C%20we%20publicly%20release%20our%20code%2C%20dataset%2C%20and%20model%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06518v1&entry.124074799=Read"},
{"title": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games", "author": "Juhwan Choi and YoungBin Kim", "abstract": "  Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs.\n", "link": "http://arxiv.org/abs/2409.06518v1", "date": "2024-09-10", "relevancy": 2.131, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.543}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Questioning%20Internal%20Knowledge%20Structure%20of%20Large%20Language%20Models%0A%20%20Through%20the%20Lens%20of%20the%20Olympic%20Games&body=Title%3A%20Questioning%20Internal%20Knowledge%20Structure%20of%20Large%20Language%20Models%0A%20%20Through%20the%20Lens%20of%20the%20Olympic%20Games%0AAuthor%3A%20Juhwan%20Choi%20and%20YoungBin%20Kim%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20a%20dominant%20approach%20in%20natural%0Alanguage%20processing%2C%20yet%20their%20internal%20knowledge%20structures%20remain%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20analyze%20the%20internal%20knowledge%20structures%20of%20LLMs%0Ausing%20historical%20medal%20tallies%20from%20the%20Olympic%20Games.%20We%20task%20the%20models%20with%0Aproviding%20the%20medal%20counts%20for%20each%20team%20and%20identifying%20which%20teams%20achieved%0Aspecific%20rankings.%20Our%20results%20reveal%20that%20while%20state-of-the-art%20LLMs%20perform%0Aremarkably%20well%20in%20reporting%20medal%20counts%20for%20individual%20teams%2C%20they%20struggle%0Asignificantly%20with%20questions%20about%20specific%20rankings.%20This%20suggests%20that%20the%0Ainternal%20knowledge%20structures%20of%20LLMs%20are%20fundamentally%20different%20from%20those%20of%0Ahumans%2C%20who%20can%20easily%20infer%20rankings%20from%20known%20medal%20counts.%20To%20support%0Afurther%20research%2C%20we%20publicly%20release%20our%20code%2C%20dataset%2C%20and%20model%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuestioning%2520Internal%2520Knowledge%2520Structure%2520of%2520Large%2520Language%2520Models%250A%2520%2520Through%2520the%2520Lens%2520of%2520the%2520Olympic%2520Games%26entry.906535625%3DJuhwan%2520Choi%2520and%2520YoungBin%2520Kim%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520become%2520a%2520dominant%2520approach%2520in%2520natural%250Alanguage%2520processing%252C%2520yet%2520their%2520internal%2520knowledge%2520structures%2520remain%2520largely%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520internal%2520knowledge%2520structures%2520of%2520LLMs%250Ausing%2520historical%2520medal%2520tallies%2520from%2520the%2520Olympic%2520Games.%2520We%2520task%2520the%2520models%2520with%250Aproviding%2520the%2520medal%2520counts%2520for%2520each%2520team%2520and%2520identifying%2520which%2520teams%2520achieved%250Aspecific%2520rankings.%2520Our%2520results%2520reveal%2520that%2520while%2520state-of-the-art%2520LLMs%2520perform%250Aremarkably%2520well%2520in%2520reporting%2520medal%2520counts%2520for%2520individual%2520teams%252C%2520they%2520struggle%250Asignificantly%2520with%2520questions%2520about%2520specific%2520rankings.%2520This%2520suggests%2520that%2520the%250Ainternal%2520knowledge%2520structures%2520of%2520LLMs%2520are%2520fundamentally%2520different%2520from%2520those%2520of%250Ahumans%252C%2520who%2520can%2520easily%2520infer%2520rankings%2520from%2520known%2520medal%2520counts.%2520To%2520support%250Afurther%2520research%252C%2520we%2520publicly%2520release%2520our%2520code%252C%2520dataset%252C%2520and%2520model%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Questioning%20Internal%20Knowledge%20Structure%20of%20Large%20Language%20Models%0A%20%20Through%20the%20Lens%20of%20the%20Olympic%20Games&entry.906535625=Juhwan%20Choi%20and%20YoungBin%20Kim&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20become%20a%20dominant%20approach%20in%20natural%0Alanguage%20processing%2C%20yet%20their%20internal%20knowledge%20structures%20remain%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20analyze%20the%20internal%20knowledge%20structures%20of%20LLMs%0Ausing%20historical%20medal%20tallies%20from%20the%20Olympic%20Games.%20We%20task%20the%20models%20with%0Aproviding%20the%20medal%20counts%20for%20each%20team%20and%20identifying%20which%20teams%20achieved%0Aspecific%20rankings.%20Our%20results%20reveal%20that%20while%20state-of-the-art%20LLMs%20perform%0Aremarkably%20well%20in%20reporting%20medal%20counts%20for%20individual%20teams%2C%20they%20struggle%0Asignificantly%20with%20questions%20about%20specific%20rankings.%20This%20suggests%20that%20the%0Ainternal%20knowledge%20structures%20of%20LLMs%20are%20fundamentally%20different%20from%20those%20of%0Ahumans%2C%20who%20can%20easily%20infer%20rankings%20from%20known%20medal%20counts.%20To%20support%0Afurther%20research%2C%20we%20publicly%20release%20our%20code%2C%20dataset%2C%20and%20model%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06518v1&entry.124074799=Read"},
{"title": "Shedding More Light on Robust Classifiers under the lens of Energy-based\n  Models", "author": "Mujtaba Hussain Mirza and Maria Rosaria Briglia and Senad Beadini and Iacopo Masi", "abstract": "  By reinterpreting a robust discriminative classifier as Energy-based Model\n(EBM), we offer a new take on the dynamics of adversarial training (AT). Our\nanalysis of the energy landscape during AT reveals that untargeted attacks\ngenerate adversarial images much more in-distribution (lower energy) than the\noriginal data from the point of view of the model. Conversely, we observe the\nopposite for targeted attacks. On the ground of our thorough analysis, we\npresent new theoretical and practical results that show how interpreting AT\nenergy dynamics unlocks a better understanding: (1) AT dynamic is governed by\nthree phases and robust overfitting occurs in the third phase with a drastic\ndivergence between natural and adversarial energies (2) by rewriting the loss\nof TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization\n(TRADES) in terms of energies, we show that TRADES implicitly alleviates\noverfitting by means of aligning the natural energy with the adversarial one\n(3) we empirically show that all recent state-of-the-art robust classifiers are\nsmoothing the energy landscape and we reconcile a variety of studies about\nunderstanding AT and weighting the loss function under the umbrella of EBMs.\nMotivated by rigorous evidence, we propose Weighted Energy Adversarial Training\n(WEAT), a novel sample weighting scheme that yields robust accuracy matching\nthe state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going\nbeyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers\nvary in the intensity and quality of their generative capabilities, and offer a\nsimple method to push this capability, reaching a remarkable Inception Score\n(IS) and FID using a robust classifier without training for generative\nmodeling. The code to reproduce our results is available at\nhttp://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/ .\n", "link": "http://arxiv.org/abs/2407.06315v3", "date": "2024-09-10", "relevancy": 2.1234, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5551}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5181}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shedding%20More%20Light%20on%20Robust%20Classifiers%20under%20the%20lens%20of%20Energy-based%0A%20%20Models&body=Title%3A%20Shedding%20More%20Light%20on%20Robust%20Classifiers%20under%20the%20lens%20of%20Energy-based%0A%20%20Models%0AAuthor%3A%20Mujtaba%20Hussain%20Mirza%20and%20Maria%20Rosaria%20Briglia%20and%20Senad%20Beadini%20and%20Iacopo%20Masi%0AAbstract%3A%20%20%20By%20reinterpreting%20a%20robust%20discriminative%20classifier%20as%20Energy-based%20Model%0A%28EBM%29%2C%20we%20offer%20a%20new%20take%20on%20the%20dynamics%20of%20adversarial%20training%20%28AT%29.%20Our%0Aanalysis%20of%20the%20energy%20landscape%20during%20AT%20reveals%20that%20untargeted%20attacks%0Agenerate%20adversarial%20images%20much%20more%20in-distribution%20%28lower%20energy%29%20than%20the%0Aoriginal%20data%20from%20the%20point%20of%20view%20of%20the%20model.%20Conversely%2C%20we%20observe%20the%0Aopposite%20for%20targeted%20attacks.%20On%20the%20ground%20of%20our%20thorough%20analysis%2C%20we%0Apresent%20new%20theoretical%20and%20practical%20results%20that%20show%20how%20interpreting%20AT%0Aenergy%20dynamics%20unlocks%20a%20better%20understanding%3A%20%281%29%20AT%20dynamic%20is%20governed%20by%0Athree%20phases%20and%20robust%20overfitting%20occurs%20in%20the%20third%20phase%20with%20a%20drastic%0Adivergence%20between%20natural%20and%20adversarial%20energies%20%282%29%20by%20rewriting%20the%20loss%0Aof%20TRadeoff-inspired%20Adversarial%20DEfense%20via%20Surrogate-loss%20minimization%0A%28TRADES%29%20in%20terms%20of%20energies%2C%20we%20show%20that%20TRADES%20implicitly%20alleviates%0Aoverfitting%20by%20means%20of%20aligning%20the%20natural%20energy%20with%20the%20adversarial%20one%0A%283%29%20we%20empirically%20show%20that%20all%20recent%20state-of-the-art%20robust%20classifiers%20are%0Asmoothing%20the%20energy%20landscape%20and%20we%20reconcile%20a%20variety%20of%20studies%20about%0Aunderstanding%20AT%20and%20weighting%20the%20loss%20function%20under%20the%20umbrella%20of%20EBMs.%0AMotivated%20by%20rigorous%20evidence%2C%20we%20propose%20Weighted%20Energy%20Adversarial%20Training%0A%28WEAT%29%2C%20a%20novel%20sample%20weighting%20scheme%20that%20yields%20robust%20accuracy%20matching%0Athe%20state-of-the-art%20on%20multiple%20benchmarks%20such%20as%20CIFAR-10%20and%20SVHN%20and%20going%0Abeyond%20in%20CIFAR-100%20and%20Tiny-ImageNet.%20We%20further%20show%20that%20robust%20classifiers%0Avary%20in%20the%20intensity%20and%20quality%20of%20their%20generative%20capabilities%2C%20and%20offer%20a%0Asimple%20method%20to%20push%20this%20capability%2C%20reaching%20a%20remarkable%20Inception%20Score%0A%28IS%29%20and%20FID%20using%20a%20robust%20classifier%20without%20training%20for%20generative%0Amodeling.%20The%20code%20to%20reproduce%20our%20results%20is%20available%20at%0Ahttp%3A//github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06315v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShedding%2520More%2520Light%2520on%2520Robust%2520Classifiers%2520under%2520the%2520lens%2520of%2520Energy-based%250A%2520%2520Models%26entry.906535625%3DMujtaba%2520Hussain%2520Mirza%2520and%2520Maria%2520Rosaria%2520Briglia%2520and%2520Senad%2520Beadini%2520and%2520Iacopo%2520Masi%26entry.1292438233%3D%2520%2520By%2520reinterpreting%2520a%2520robust%2520discriminative%2520classifier%2520as%2520Energy-based%2520Model%250A%2528EBM%2529%252C%2520we%2520offer%2520a%2520new%2520take%2520on%2520the%2520dynamics%2520of%2520adversarial%2520training%2520%2528AT%2529.%2520Our%250Aanalysis%2520of%2520the%2520energy%2520landscape%2520during%2520AT%2520reveals%2520that%2520untargeted%2520attacks%250Agenerate%2520adversarial%2520images%2520much%2520more%2520in-distribution%2520%2528lower%2520energy%2529%2520than%2520the%250Aoriginal%2520data%2520from%2520the%2520point%2520of%2520view%2520of%2520the%2520model.%2520Conversely%252C%2520we%2520observe%2520the%250Aopposite%2520for%2520targeted%2520attacks.%2520On%2520the%2520ground%2520of%2520our%2520thorough%2520analysis%252C%2520we%250Apresent%2520new%2520theoretical%2520and%2520practical%2520results%2520that%2520show%2520how%2520interpreting%2520AT%250Aenergy%2520dynamics%2520unlocks%2520a%2520better%2520understanding%253A%2520%25281%2529%2520AT%2520dynamic%2520is%2520governed%2520by%250Athree%2520phases%2520and%2520robust%2520overfitting%2520occurs%2520in%2520the%2520third%2520phase%2520with%2520a%2520drastic%250Adivergence%2520between%2520natural%2520and%2520adversarial%2520energies%2520%25282%2529%2520by%2520rewriting%2520the%2520loss%250Aof%2520TRadeoff-inspired%2520Adversarial%2520DEfense%2520via%2520Surrogate-loss%2520minimization%250A%2528TRADES%2529%2520in%2520terms%2520of%2520energies%252C%2520we%2520show%2520that%2520TRADES%2520implicitly%2520alleviates%250Aoverfitting%2520by%2520means%2520of%2520aligning%2520the%2520natural%2520energy%2520with%2520the%2520adversarial%2520one%250A%25283%2529%2520we%2520empirically%2520show%2520that%2520all%2520recent%2520state-of-the-art%2520robust%2520classifiers%2520are%250Asmoothing%2520the%2520energy%2520landscape%2520and%2520we%2520reconcile%2520a%2520variety%2520of%2520studies%2520about%250Aunderstanding%2520AT%2520and%2520weighting%2520the%2520loss%2520function%2520under%2520the%2520umbrella%2520of%2520EBMs.%250AMotivated%2520by%2520rigorous%2520evidence%252C%2520we%2520propose%2520Weighted%2520Energy%2520Adversarial%2520Training%250A%2528WEAT%2529%252C%2520a%2520novel%2520sample%2520weighting%2520scheme%2520that%2520yields%2520robust%2520accuracy%2520matching%250Athe%2520state-of-the-art%2520on%2520multiple%2520benchmarks%2520such%2520as%2520CIFAR-10%2520and%2520SVHN%2520and%2520going%250Abeyond%2520in%2520CIFAR-100%2520and%2520Tiny-ImageNet.%2520We%2520further%2520show%2520that%2520robust%2520classifiers%250Avary%2520in%2520the%2520intensity%2520and%2520quality%2520of%2520their%2520generative%2520capabilities%252C%2520and%2520offer%2520a%250Asimple%2520method%2520to%2520push%2520this%2520capability%252C%2520reaching%2520a%2520remarkable%2520Inception%2520Score%250A%2528IS%2529%2520and%2520FID%2520using%2520a%2520robust%2520classifier%2520without%2520training%2520for%2520generative%250Amodeling.%2520The%2520code%2520to%2520reproduce%2520our%2520results%2520is%2520available%2520at%250Ahttp%253A//github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06315v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shedding%20More%20Light%20on%20Robust%20Classifiers%20under%20the%20lens%20of%20Energy-based%0A%20%20Models&entry.906535625=Mujtaba%20Hussain%20Mirza%20and%20Maria%20Rosaria%20Briglia%20and%20Senad%20Beadini%20and%20Iacopo%20Masi&entry.1292438233=%20%20By%20reinterpreting%20a%20robust%20discriminative%20classifier%20as%20Energy-based%20Model%0A%28EBM%29%2C%20we%20offer%20a%20new%20take%20on%20the%20dynamics%20of%20adversarial%20training%20%28AT%29.%20Our%0Aanalysis%20of%20the%20energy%20landscape%20during%20AT%20reveals%20that%20untargeted%20attacks%0Agenerate%20adversarial%20images%20much%20more%20in-distribution%20%28lower%20energy%29%20than%20the%0Aoriginal%20data%20from%20the%20point%20of%20view%20of%20the%20model.%20Conversely%2C%20we%20observe%20the%0Aopposite%20for%20targeted%20attacks.%20On%20the%20ground%20of%20our%20thorough%20analysis%2C%20we%0Apresent%20new%20theoretical%20and%20practical%20results%20that%20show%20how%20interpreting%20AT%0Aenergy%20dynamics%20unlocks%20a%20better%20understanding%3A%20%281%29%20AT%20dynamic%20is%20governed%20by%0Athree%20phases%20and%20robust%20overfitting%20occurs%20in%20the%20third%20phase%20with%20a%20drastic%0Adivergence%20between%20natural%20and%20adversarial%20energies%20%282%29%20by%20rewriting%20the%20loss%0Aof%20TRadeoff-inspired%20Adversarial%20DEfense%20via%20Surrogate-loss%20minimization%0A%28TRADES%29%20in%20terms%20of%20energies%2C%20we%20show%20that%20TRADES%20implicitly%20alleviates%0Aoverfitting%20by%20means%20of%20aligning%20the%20natural%20energy%20with%20the%20adversarial%20one%0A%283%29%20we%20empirically%20show%20that%20all%20recent%20state-of-the-art%20robust%20classifiers%20are%0Asmoothing%20the%20energy%20landscape%20and%20we%20reconcile%20a%20variety%20of%20studies%20about%0Aunderstanding%20AT%20and%20weighting%20the%20loss%20function%20under%20the%20umbrella%20of%20EBMs.%0AMotivated%20by%20rigorous%20evidence%2C%20we%20propose%20Weighted%20Energy%20Adversarial%20Training%0A%28WEAT%29%2C%20a%20novel%20sample%20weighting%20scheme%20that%20yields%20robust%20accuracy%20matching%0Athe%20state-of-the-art%20on%20multiple%20benchmarks%20such%20as%20CIFAR-10%20and%20SVHN%20and%20going%0Abeyond%20in%20CIFAR-100%20and%20Tiny-ImageNet.%20We%20further%20show%20that%20robust%20classifiers%0Avary%20in%20the%20intensity%20and%20quality%20of%20their%20generative%20capabilities%2C%20and%20offer%20a%0Asimple%20method%20to%20push%20this%20capability%2C%20reaching%20a%20remarkable%20Inception%20Score%0A%28IS%29%20and%20FID%20using%20a%20robust%20classifier%20without%20training%20for%20generative%0Amodeling.%20The%20code%20to%20reproduce%20our%20results%20is%20available%20at%0Ahttp%3A//github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06315v3&entry.124074799=Read"},
{"title": "A study on deep feature extraction to detect and classify Acute\n  Lymphoblastic Leukemia (ALL)", "author": "Sabit Ahamed Preanto and Md. Taimur Ahad and Yousuf Rayhan Emon and Sumaya Mustofa and Md Alamin", "abstract": "  Acute lymphoblastic leukaemia (ALL) is a blood malignancy that mainly affects\nadults and children. This study looks into the use of deep learning,\nspecifically Convolutional Neural Networks (CNNs), for the detection and\nclassification of ALL. Conventional techniques for ALL diagnosis, such bone\nmarrow biopsy, are costly and prone to mistakes made by hand. By utilising\nautomated technologies, the research seeks to improve diagnostic accuracy. The\nresearch uses a variety of pre-trained CNN models, such as InceptionV3,\nResNet101, VGG19, DenseNet121, MobileNetV2, and DenseNet121, to extract\ncharacteristics from pictures of blood smears. ANOVA, Recursive Feature\nElimination (RFE), Random Forest, Lasso, and Principal Component Analysis (PCA)\nare a few of the selection approaches used to find the most relevant features\nafter feature extraction. Following that, machine learning methods like Na\\\"ive\nBayes, Random Forest, Support Vector Machine (SVM), and K-Nearest Neighbours\n(KNN) are used to classify these features. With an 87% accuracy rate, the\nResNet101 model produced the best results, closely followed by DenseNet121 and\nVGG19. According to the study, CNN-based models have the potential to decrease\nthe need for medical specialists by increasing the speed and accuracy of ALL\ndiagnosis. To improve model performance, the study also recommends expanding\nand diversifying datasets and investigating more sophisticated designs such as\ntransformers. This study highlights how well automated deep learning systems do\nmedical diagnosis.\n", "link": "http://arxiv.org/abs/2409.06687v1", "date": "2024-09-10", "relevancy": 2.122, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4398}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20study%20on%20deep%20feature%20extraction%20to%20detect%20and%20classify%20Acute%0A%20%20Lymphoblastic%20Leukemia%20%28ALL%29&body=Title%3A%20A%20study%20on%20deep%20feature%20extraction%20to%20detect%20and%20classify%20Acute%0A%20%20Lymphoblastic%20Leukemia%20%28ALL%29%0AAuthor%3A%20Sabit%20Ahamed%20Preanto%20and%20Md.%20Taimur%20Ahad%20and%20Yousuf%20Rayhan%20Emon%20and%20Sumaya%20Mustofa%20and%20Md%20Alamin%0AAbstract%3A%20%20%20Acute%20lymphoblastic%20leukaemia%20%28ALL%29%20is%20a%20blood%20malignancy%20that%20mainly%20affects%0Aadults%20and%20children.%20This%20study%20looks%20into%20the%20use%20of%20deep%20learning%2C%0Aspecifically%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20for%20the%20detection%20and%0Aclassification%20of%20ALL.%20Conventional%20techniques%20for%20ALL%20diagnosis%2C%20such%20bone%0Amarrow%20biopsy%2C%20are%20costly%20and%20prone%20to%20mistakes%20made%20by%20hand.%20By%20utilising%0Aautomated%20technologies%2C%20the%20research%20seeks%20to%20improve%20diagnostic%20accuracy.%20The%0Aresearch%20uses%20a%20variety%20of%20pre-trained%20CNN%20models%2C%20such%20as%20InceptionV3%2C%0AResNet101%2C%20VGG19%2C%20DenseNet121%2C%20MobileNetV2%2C%20and%20DenseNet121%2C%20to%20extract%0Acharacteristics%20from%20pictures%20of%20blood%20smears.%20ANOVA%2C%20Recursive%20Feature%0AElimination%20%28RFE%29%2C%20Random%20Forest%2C%20Lasso%2C%20and%20Principal%20Component%20Analysis%20%28PCA%29%0Aare%20a%20few%20of%20the%20selection%20approaches%20used%20to%20find%20the%20most%20relevant%20features%0Aafter%20feature%20extraction.%20Following%20that%2C%20machine%20learning%20methods%20like%20Na%5C%22ive%0ABayes%2C%20Random%20Forest%2C%20Support%20Vector%20Machine%20%28SVM%29%2C%20and%20K-Nearest%20Neighbours%0A%28KNN%29%20are%20used%20to%20classify%20these%20features.%20With%20an%2087%25%20accuracy%20rate%2C%20the%0AResNet101%20model%20produced%20the%20best%20results%2C%20closely%20followed%20by%20DenseNet121%20and%0AVGG19.%20According%20to%20the%20study%2C%20CNN-based%20models%20have%20the%20potential%20to%20decrease%0Athe%20need%20for%20medical%20specialists%20by%20increasing%20the%20speed%20and%20accuracy%20of%20ALL%0Adiagnosis.%20To%20improve%20model%20performance%2C%20the%20study%20also%20recommends%20expanding%0Aand%20diversifying%20datasets%20and%20investigating%20more%20sophisticated%20designs%20such%20as%0Atransformers.%20This%20study%20highlights%20how%20well%20automated%20deep%20learning%20systems%20do%0Amedical%20diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520study%2520on%2520deep%2520feature%2520extraction%2520to%2520detect%2520and%2520classify%2520Acute%250A%2520%2520Lymphoblastic%2520Leukemia%2520%2528ALL%2529%26entry.906535625%3DSabit%2520Ahamed%2520Preanto%2520and%2520Md.%2520Taimur%2520Ahad%2520and%2520Yousuf%2520Rayhan%2520Emon%2520and%2520Sumaya%2520Mustofa%2520and%2520Md%2520Alamin%26entry.1292438233%3D%2520%2520Acute%2520lymphoblastic%2520leukaemia%2520%2528ALL%2529%2520is%2520a%2520blood%2520malignancy%2520that%2520mainly%2520affects%250Aadults%2520and%2520children.%2520This%2520study%2520looks%2520into%2520the%2520use%2520of%2520deep%2520learning%252C%250Aspecifically%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520for%2520the%2520detection%2520and%250Aclassification%2520of%2520ALL.%2520Conventional%2520techniques%2520for%2520ALL%2520diagnosis%252C%2520such%2520bone%250Amarrow%2520biopsy%252C%2520are%2520costly%2520and%2520prone%2520to%2520mistakes%2520made%2520by%2520hand.%2520By%2520utilising%250Aautomated%2520technologies%252C%2520the%2520research%2520seeks%2520to%2520improve%2520diagnostic%2520accuracy.%2520The%250Aresearch%2520uses%2520a%2520variety%2520of%2520pre-trained%2520CNN%2520models%252C%2520such%2520as%2520InceptionV3%252C%250AResNet101%252C%2520VGG19%252C%2520DenseNet121%252C%2520MobileNetV2%252C%2520and%2520DenseNet121%252C%2520to%2520extract%250Acharacteristics%2520from%2520pictures%2520of%2520blood%2520smears.%2520ANOVA%252C%2520Recursive%2520Feature%250AElimination%2520%2528RFE%2529%252C%2520Random%2520Forest%252C%2520Lasso%252C%2520and%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%250Aare%2520a%2520few%2520of%2520the%2520selection%2520approaches%2520used%2520to%2520find%2520the%2520most%2520relevant%2520features%250Aafter%2520feature%2520extraction.%2520Following%2520that%252C%2520machine%2520learning%2520methods%2520like%2520Na%255C%2522ive%250ABayes%252C%2520Random%2520Forest%252C%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%2520and%2520K-Nearest%2520Neighbours%250A%2528KNN%2529%2520are%2520used%2520to%2520classify%2520these%2520features.%2520With%2520an%252087%2525%2520accuracy%2520rate%252C%2520the%250AResNet101%2520model%2520produced%2520the%2520best%2520results%252C%2520closely%2520followed%2520by%2520DenseNet121%2520and%250AVGG19.%2520According%2520to%2520the%2520study%252C%2520CNN-based%2520models%2520have%2520the%2520potential%2520to%2520decrease%250Athe%2520need%2520for%2520medical%2520specialists%2520by%2520increasing%2520the%2520speed%2520and%2520accuracy%2520of%2520ALL%250Adiagnosis.%2520To%2520improve%2520model%2520performance%252C%2520the%2520study%2520also%2520recommends%2520expanding%250Aand%2520diversifying%2520datasets%2520and%2520investigating%2520more%2520sophisticated%2520designs%2520such%2520as%250Atransformers.%2520This%2520study%2520highlights%2520how%2520well%2520automated%2520deep%2520learning%2520systems%2520do%250Amedical%2520diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20study%20on%20deep%20feature%20extraction%20to%20detect%20and%20classify%20Acute%0A%20%20Lymphoblastic%20Leukemia%20%28ALL%29&entry.906535625=Sabit%20Ahamed%20Preanto%20and%20Md.%20Taimur%20Ahad%20and%20Yousuf%20Rayhan%20Emon%20and%20Sumaya%20Mustofa%20and%20Md%20Alamin&entry.1292438233=%20%20Acute%20lymphoblastic%20leukaemia%20%28ALL%29%20is%20a%20blood%20malignancy%20that%20mainly%20affects%0Aadults%20and%20children.%20This%20study%20looks%20into%20the%20use%20of%20deep%20learning%2C%0Aspecifically%20Convolutional%20Neural%20Networks%20%28CNNs%29%2C%20for%20the%20detection%20and%0Aclassification%20of%20ALL.%20Conventional%20techniques%20for%20ALL%20diagnosis%2C%20such%20bone%0Amarrow%20biopsy%2C%20are%20costly%20and%20prone%20to%20mistakes%20made%20by%20hand.%20By%20utilising%0Aautomated%20technologies%2C%20the%20research%20seeks%20to%20improve%20diagnostic%20accuracy.%20The%0Aresearch%20uses%20a%20variety%20of%20pre-trained%20CNN%20models%2C%20such%20as%20InceptionV3%2C%0AResNet101%2C%20VGG19%2C%20DenseNet121%2C%20MobileNetV2%2C%20and%20DenseNet121%2C%20to%20extract%0Acharacteristics%20from%20pictures%20of%20blood%20smears.%20ANOVA%2C%20Recursive%20Feature%0AElimination%20%28RFE%29%2C%20Random%20Forest%2C%20Lasso%2C%20and%20Principal%20Component%20Analysis%20%28PCA%29%0Aare%20a%20few%20of%20the%20selection%20approaches%20used%20to%20find%20the%20most%20relevant%20features%0Aafter%20feature%20extraction.%20Following%20that%2C%20machine%20learning%20methods%20like%20Na%5C%22ive%0ABayes%2C%20Random%20Forest%2C%20Support%20Vector%20Machine%20%28SVM%29%2C%20and%20K-Nearest%20Neighbours%0A%28KNN%29%20are%20used%20to%20classify%20these%20features.%20With%20an%2087%25%20accuracy%20rate%2C%20the%0AResNet101%20model%20produced%20the%20best%20results%2C%20closely%20followed%20by%20DenseNet121%20and%0AVGG19.%20According%20to%20the%20study%2C%20CNN-based%20models%20have%20the%20potential%20to%20decrease%0Athe%20need%20for%20medical%20specialists%20by%20increasing%20the%20speed%20and%20accuracy%20of%20ALL%0Adiagnosis.%20To%20improve%20model%20performance%2C%20the%20study%20also%20recommends%20expanding%0Aand%20diversifying%20datasets%20and%20investigating%20more%20sophisticated%20designs%20such%20as%0Atransformers.%20This%20study%20highlights%20how%20well%20automated%20deep%20learning%20systems%20do%0Amedical%20diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06687v1&entry.124074799=Read"},
{"title": "Not All Errors Are Made Equal: A Regret Metric for Detecting\n  System-level Trajectory Prediction Failures", "author": "Kensuke Nakamura and Ran Tian and Andrea Bajcsy", "abstract": "  Robot decision-making increasingly relies on data-driven human prediction\nmodels when operating around people. While these models are known to mispredict\nin out-of-distribution interactions, only a subset of prediction errors impact\ndownstream robot performance. We propose characterizing such \"system-level\"\nprediction failures via the mathematical notion of regret: high-regret\ninteractions are precisely those in which mispredictions degraded closed-loop\nrobot performance. We further introduce a probabilistic generalization of\nregret that calibrates failure detection across disparate deployment contexts\nand renders regret compatible with reward-based and reward-free (e.g.,\ngenerative) planners. In simulated autonomous driving interactions and social\nnavigation interactions deployed on hardware, we showcase that our system-level\nfailure metric can be used offline to automatically extract closed-loop\nhuman-robot interactions that state-of-the-art generative human predictors and\nrobot planners previously struggled with. We further find that the very\npresence of high-regret data during human predictor fine-tuning is highly\npredictive of robot re-deployment performance improvements. Fine-tuning with\nthe informative but significantly smaller high-regret data (23% of deployment\ndata) is competitive with fine-tuning on the full deployment dataset,\nindicating a promising avenue for efficiently mitigating system-level\nhuman-robot interaction failures. Project website:\nhttps://cmu-intentlab.github.io/not-all-errors/\n", "link": "http://arxiv.org/abs/2403.04745v3", "date": "2024-09-10", "relevancy": 2.105, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6053}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5254}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Errors%20Are%20Made%20Equal%3A%20A%20Regret%20Metric%20for%20Detecting%0A%20%20System-level%20Trajectory%20Prediction%20Failures&body=Title%3A%20Not%20All%20Errors%20Are%20Made%20Equal%3A%20A%20Regret%20Metric%20for%20Detecting%0A%20%20System-level%20Trajectory%20Prediction%20Failures%0AAuthor%3A%20Kensuke%20Nakamura%20and%20Ran%20Tian%20and%20Andrea%20Bajcsy%0AAbstract%3A%20%20%20Robot%20decision-making%20increasingly%20relies%20on%20data-driven%20human%20prediction%0Amodels%20when%20operating%20around%20people.%20While%20these%20models%20are%20known%20to%20mispredict%0Ain%20out-of-distribution%20interactions%2C%20only%20a%20subset%20of%20prediction%20errors%20impact%0Adownstream%20robot%20performance.%20We%20propose%20characterizing%20such%20%22system-level%22%0Aprediction%20failures%20via%20the%20mathematical%20notion%20of%20regret%3A%20high-regret%0Ainteractions%20are%20precisely%20those%20in%20which%20mispredictions%20degraded%20closed-loop%0Arobot%20performance.%20We%20further%20introduce%20a%20probabilistic%20generalization%20of%0Aregret%20that%20calibrates%20failure%20detection%20across%20disparate%20deployment%20contexts%0Aand%20renders%20regret%20compatible%20with%20reward-based%20and%20reward-free%20%28e.g.%2C%0Agenerative%29%20planners.%20In%20simulated%20autonomous%20driving%20interactions%20and%20social%0Anavigation%20interactions%20deployed%20on%20hardware%2C%20we%20showcase%20that%20our%20system-level%0Afailure%20metric%20can%20be%20used%20offline%20to%20automatically%20extract%20closed-loop%0Ahuman-robot%20interactions%20that%20state-of-the-art%20generative%20human%20predictors%20and%0Arobot%20planners%20previously%20struggled%20with.%20We%20further%20find%20that%20the%20very%0Apresence%20of%20high-regret%20data%20during%20human%20predictor%20fine-tuning%20is%20highly%0Apredictive%20of%20robot%20re-deployment%20performance%20improvements.%20Fine-tuning%20with%0Athe%20informative%20but%20significantly%20smaller%20high-regret%20data%20%2823%25%20of%20deployment%0Adata%29%20is%20competitive%20with%20fine-tuning%20on%20the%20full%20deployment%20dataset%2C%0Aindicating%20a%20promising%20avenue%20for%20efficiently%20mitigating%20system-level%0Ahuman-robot%20interaction%20failures.%20Project%20website%3A%0Ahttps%3A//cmu-intentlab.github.io/not-all-errors/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Errors%2520Are%2520Made%2520Equal%253A%2520A%2520Regret%2520Metric%2520for%2520Detecting%250A%2520%2520System-level%2520Trajectory%2520Prediction%2520Failures%26entry.906535625%3DKensuke%2520Nakamura%2520and%2520Ran%2520Tian%2520and%2520Andrea%2520Bajcsy%26entry.1292438233%3D%2520%2520Robot%2520decision-making%2520increasingly%2520relies%2520on%2520data-driven%2520human%2520prediction%250Amodels%2520when%2520operating%2520around%2520people.%2520While%2520these%2520models%2520are%2520known%2520to%2520mispredict%250Ain%2520out-of-distribution%2520interactions%252C%2520only%2520a%2520subset%2520of%2520prediction%2520errors%2520impact%250Adownstream%2520robot%2520performance.%2520We%2520propose%2520characterizing%2520such%2520%2522system-level%2522%250Aprediction%2520failures%2520via%2520the%2520mathematical%2520notion%2520of%2520regret%253A%2520high-regret%250Ainteractions%2520are%2520precisely%2520those%2520in%2520which%2520mispredictions%2520degraded%2520closed-loop%250Arobot%2520performance.%2520We%2520further%2520introduce%2520a%2520probabilistic%2520generalization%2520of%250Aregret%2520that%2520calibrates%2520failure%2520detection%2520across%2520disparate%2520deployment%2520contexts%250Aand%2520renders%2520regret%2520compatible%2520with%2520reward-based%2520and%2520reward-free%2520%2528e.g.%252C%250Agenerative%2529%2520planners.%2520In%2520simulated%2520autonomous%2520driving%2520interactions%2520and%2520social%250Anavigation%2520interactions%2520deployed%2520on%2520hardware%252C%2520we%2520showcase%2520that%2520our%2520system-level%250Afailure%2520metric%2520can%2520be%2520used%2520offline%2520to%2520automatically%2520extract%2520closed-loop%250Ahuman-robot%2520interactions%2520that%2520state-of-the-art%2520generative%2520human%2520predictors%2520and%250Arobot%2520planners%2520previously%2520struggled%2520with.%2520We%2520further%2520find%2520that%2520the%2520very%250Apresence%2520of%2520high-regret%2520data%2520during%2520human%2520predictor%2520fine-tuning%2520is%2520highly%250Apredictive%2520of%2520robot%2520re-deployment%2520performance%2520improvements.%2520Fine-tuning%2520with%250Athe%2520informative%2520but%2520significantly%2520smaller%2520high-regret%2520data%2520%252823%2525%2520of%2520deployment%250Adata%2529%2520is%2520competitive%2520with%2520fine-tuning%2520on%2520the%2520full%2520deployment%2520dataset%252C%250Aindicating%2520a%2520promising%2520avenue%2520for%2520efficiently%2520mitigating%2520system-level%250Ahuman-robot%2520interaction%2520failures.%2520Project%2520website%253A%250Ahttps%253A//cmu-intentlab.github.io/not-all-errors/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Errors%20Are%20Made%20Equal%3A%20A%20Regret%20Metric%20for%20Detecting%0A%20%20System-level%20Trajectory%20Prediction%20Failures&entry.906535625=Kensuke%20Nakamura%20and%20Ran%20Tian%20and%20Andrea%20Bajcsy&entry.1292438233=%20%20Robot%20decision-making%20increasingly%20relies%20on%20data-driven%20human%20prediction%0Amodels%20when%20operating%20around%20people.%20While%20these%20models%20are%20known%20to%20mispredict%0Ain%20out-of-distribution%20interactions%2C%20only%20a%20subset%20of%20prediction%20errors%20impact%0Adownstream%20robot%20performance.%20We%20propose%20characterizing%20such%20%22system-level%22%0Aprediction%20failures%20via%20the%20mathematical%20notion%20of%20regret%3A%20high-regret%0Ainteractions%20are%20precisely%20those%20in%20which%20mispredictions%20degraded%20closed-loop%0Arobot%20performance.%20We%20further%20introduce%20a%20probabilistic%20generalization%20of%0Aregret%20that%20calibrates%20failure%20detection%20across%20disparate%20deployment%20contexts%0Aand%20renders%20regret%20compatible%20with%20reward-based%20and%20reward-free%20%28e.g.%2C%0Agenerative%29%20planners.%20In%20simulated%20autonomous%20driving%20interactions%20and%20social%0Anavigation%20interactions%20deployed%20on%20hardware%2C%20we%20showcase%20that%20our%20system-level%0Afailure%20metric%20can%20be%20used%20offline%20to%20automatically%20extract%20closed-loop%0Ahuman-robot%20interactions%20that%20state-of-the-art%20generative%20human%20predictors%20and%0Arobot%20planners%20previously%20struggled%20with.%20We%20further%20find%20that%20the%20very%0Apresence%20of%20high-regret%20data%20during%20human%20predictor%20fine-tuning%20is%20highly%0Apredictive%20of%20robot%20re-deployment%20performance%20improvements.%20Fine-tuning%20with%0Athe%20informative%20but%20significantly%20smaller%20high-regret%20data%20%2823%25%20of%20deployment%0Adata%29%20is%20competitive%20with%20fine-tuning%20on%20the%20full%20deployment%20dataset%2C%0Aindicating%20a%20promising%20avenue%20for%20efficiently%20mitigating%20system-level%0Ahuman-robot%20interaction%20failures.%20Project%20website%3A%0Ahttps%3A//cmu-intentlab.github.io/not-all-errors/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04745v3&entry.124074799=Read"},
{"title": "CAMS: Convolution and Attention-Free Mamba-based Cardiac Image\n  Segmentation", "author": "Abbas Khan and Muhammad Asad and Martin Benning and Caroline Roney and Gregory Slabaugh", "abstract": "  Convolutional Neural Networks (CNNs) and Transformer-based self-attention\nmodels have become the standard for medical image segmentation. This paper\ndemonstrates that convolution and self-attention, while widely used, are not\nthe only effective methods for segmentation. Breaking with convention, we\npresent a Convolution and self-Attention-free Mamba-based semantic Segmentation\nNetwork named CAMS-Net. Specifically, we design Mamba-based Channel Aggregator\nand Spatial Aggregator, which are applied independently in each encoder-decoder\nstage. The Channel Aggregator extracts information across different channels,\nand the Spatial Aggregator learns features across different spatial locations.\nWe also propose a Linearly Interconnected Factorized Mamba (LIFM) block to\nreduce the computational complexity of a Mamba block and to enhance its\ndecision function by introducing a non-linearity between two factorized Mamba\nblocks. Our model outperforms the existing state-of-the-art CNN,\nself-attention, and Mamba-based methods on CMR and M&Ms-2 Cardiac segmentation\ndatasets, showing how this innovative, convolution, and self-attention-free\nmethod can inspire further research beyond CNN and Transformer paradigms,\nachieving linear complexity and reducing the number of parameters. Source code\nand pre-trained models will be publicly available upon acceptance.\n", "link": "http://arxiv.org/abs/2406.05786v2", "date": "2024-09-10", "relevancy": 2.0793, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5155}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMS%3A%20Convolution%20and%20Attention-Free%20Mamba-based%20Cardiac%20Image%0A%20%20Segmentation&body=Title%3A%20CAMS%3A%20Convolution%20and%20Attention-Free%20Mamba-based%20Cardiac%20Image%0A%20%20Segmentation%0AAuthor%3A%20Abbas%20Khan%20and%20Muhammad%20Asad%20and%20Martin%20Benning%20and%20Caroline%20Roney%20and%20Gregory%20Slabaugh%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformer-based%20self-attention%0Amodels%20have%20become%20the%20standard%20for%20medical%20image%20segmentation.%20This%20paper%0Ademonstrates%20that%20convolution%20and%20self-attention%2C%20while%20widely%20used%2C%20are%20not%0Athe%20only%20effective%20methods%20for%20segmentation.%20Breaking%20with%20convention%2C%20we%0Apresent%20a%20Convolution%20and%20self-Attention-free%20Mamba-based%20semantic%20Segmentation%0ANetwork%20named%20CAMS-Net.%20Specifically%2C%20we%20design%20Mamba-based%20Channel%20Aggregator%0Aand%20Spatial%20Aggregator%2C%20which%20are%20applied%20independently%20in%20each%20encoder-decoder%0Astage.%20The%20Channel%20Aggregator%20extracts%20information%20across%20different%20channels%2C%0Aand%20the%20Spatial%20Aggregator%20learns%20features%20across%20different%20spatial%20locations.%0AWe%20also%20propose%20a%20Linearly%20Interconnected%20Factorized%20Mamba%20%28LIFM%29%20block%20to%0Areduce%20the%20computational%20complexity%20of%20a%20Mamba%20block%20and%20to%20enhance%20its%0Adecision%20function%20by%20introducing%20a%20non-linearity%20between%20two%20factorized%20Mamba%0Ablocks.%20Our%20model%20outperforms%20the%20existing%20state-of-the-art%20CNN%2C%0Aself-attention%2C%20and%20Mamba-based%20methods%20on%20CMR%20and%20M%26Ms-2%20Cardiac%20segmentation%0Adatasets%2C%20showing%20how%20this%20innovative%2C%20convolution%2C%20and%20self-attention-free%0Amethod%20can%20inspire%20further%20research%20beyond%20CNN%20and%20Transformer%20paradigms%2C%0Aachieving%20linear%20complexity%20and%20reducing%20the%20number%20of%20parameters.%20Source%20code%0Aand%20pre-trained%20models%20will%20be%20publicly%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05786v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMS%253A%2520Convolution%2520and%2520Attention-Free%2520Mamba-based%2520Cardiac%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DAbbas%2520Khan%2520and%2520Muhammad%2520Asad%2520and%2520Martin%2520Benning%2520and%2520Caroline%2520Roney%2520and%2520Gregory%2520Slabaugh%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Transformer-based%2520self-attention%250Amodels%2520have%2520become%2520the%2520standard%2520for%2520medical%2520image%2520segmentation.%2520This%2520paper%250Ademonstrates%2520that%2520convolution%2520and%2520self-attention%252C%2520while%2520widely%2520used%252C%2520are%2520not%250Athe%2520only%2520effective%2520methods%2520for%2520segmentation.%2520Breaking%2520with%2520convention%252C%2520we%250Apresent%2520a%2520Convolution%2520and%2520self-Attention-free%2520Mamba-based%2520semantic%2520Segmentation%250ANetwork%2520named%2520CAMS-Net.%2520Specifically%252C%2520we%2520design%2520Mamba-based%2520Channel%2520Aggregator%250Aand%2520Spatial%2520Aggregator%252C%2520which%2520are%2520applied%2520independently%2520in%2520each%2520encoder-decoder%250Astage.%2520The%2520Channel%2520Aggregator%2520extracts%2520information%2520across%2520different%2520channels%252C%250Aand%2520the%2520Spatial%2520Aggregator%2520learns%2520features%2520across%2520different%2520spatial%2520locations.%250AWe%2520also%2520propose%2520a%2520Linearly%2520Interconnected%2520Factorized%2520Mamba%2520%2528LIFM%2529%2520block%2520to%250Areduce%2520the%2520computational%2520complexity%2520of%2520a%2520Mamba%2520block%2520and%2520to%2520enhance%2520its%250Adecision%2520function%2520by%2520introducing%2520a%2520non-linearity%2520between%2520two%2520factorized%2520Mamba%250Ablocks.%2520Our%2520model%2520outperforms%2520the%2520existing%2520state-of-the-art%2520CNN%252C%250Aself-attention%252C%2520and%2520Mamba-based%2520methods%2520on%2520CMR%2520and%2520M%2526Ms-2%2520Cardiac%2520segmentation%250Adatasets%252C%2520showing%2520how%2520this%2520innovative%252C%2520convolution%252C%2520and%2520self-attention-free%250Amethod%2520can%2520inspire%2520further%2520research%2520beyond%2520CNN%2520and%2520Transformer%2520paradigms%252C%250Aachieving%2520linear%2520complexity%2520and%2520reducing%2520the%2520number%2520of%2520parameters.%2520Source%2520code%250Aand%2520pre-trained%2520models%2520will%2520be%2520publicly%2520available%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05786v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMS%3A%20Convolution%20and%20Attention-Free%20Mamba-based%20Cardiac%20Image%0A%20%20Segmentation&entry.906535625=Abbas%20Khan%20and%20Muhammad%20Asad%20and%20Martin%20Benning%20and%20Caroline%20Roney%20and%20Gregory%20Slabaugh&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformer-based%20self-attention%0Amodels%20have%20become%20the%20standard%20for%20medical%20image%20segmentation.%20This%20paper%0Ademonstrates%20that%20convolution%20and%20self-attention%2C%20while%20widely%20used%2C%20are%20not%0Athe%20only%20effective%20methods%20for%20segmentation.%20Breaking%20with%20convention%2C%20we%0Apresent%20a%20Convolution%20and%20self-Attention-free%20Mamba-based%20semantic%20Segmentation%0ANetwork%20named%20CAMS-Net.%20Specifically%2C%20we%20design%20Mamba-based%20Channel%20Aggregator%0Aand%20Spatial%20Aggregator%2C%20which%20are%20applied%20independently%20in%20each%20encoder-decoder%0Astage.%20The%20Channel%20Aggregator%20extracts%20information%20across%20different%20channels%2C%0Aand%20the%20Spatial%20Aggregator%20learns%20features%20across%20different%20spatial%20locations.%0AWe%20also%20propose%20a%20Linearly%20Interconnected%20Factorized%20Mamba%20%28LIFM%29%20block%20to%0Areduce%20the%20computational%20complexity%20of%20a%20Mamba%20block%20and%20to%20enhance%20its%0Adecision%20function%20by%20introducing%20a%20non-linearity%20between%20two%20factorized%20Mamba%0Ablocks.%20Our%20model%20outperforms%20the%20existing%20state-of-the-art%20CNN%2C%0Aself-attention%2C%20and%20Mamba-based%20methods%20on%20CMR%20and%20M%26Ms-2%20Cardiac%20segmentation%0Adatasets%2C%20showing%20how%20this%20innovative%2C%20convolution%2C%20and%20self-attention-free%0Amethod%20can%20inspire%20further%20research%20beyond%20CNN%20and%20Transformer%20paradigms%2C%0Aachieving%20linear%20complexity%20and%20reducing%20the%20number%20of%20parameters.%20Source%20code%0Aand%20pre-trained%20models%20will%20be%20publicly%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05786v2&entry.124074799=Read"},
{"title": "Label-free Monitoring of Self-Supervised Learning Progress", "author": "Isaac Xu and Scott Lowe and Thomas Trappenberg", "abstract": "  Self-supervised learning (SSL) is an effective method for exploiting\nunlabelled data to learn a high-level embedding space that can be used for\nvarious downstream tasks. However, existing methods to monitor the quality of\nthe encoder -- either during training for one model or to compare several\ntrained models -- still rely on access to annotated data. When SSL\nmethodologies are applied to new data domains, a sufficiently large labelled\ndataset may not always be available. In this study, we propose several\nevaluation metrics which can be applied on the embeddings of unlabelled data\nand investigate their viability by comparing them to linear probe accuracy (a\ncommon metric which utilizes an annotated dataset). In particular, we apply\n$k$-means clustering and measure the clustering quality with the silhouette\nscore and clustering agreement. We also measure the entropy of the embedding\ndistribution. We find that while the clusters did correspond better to the\nground truth annotations as training of the network progressed, label-free\nclustering metrics correlated with the linear probe accuracy only when training\nwith SSL methods SimCLR and MoCo-v2, but not with SimSiam. Additionally,\nalthough entropy did not always have strong correlations with LP accuracy, this\nappears to be due to instability arising from early training, with the metric\nstabilizing and becoming more reliable at later stages of learning.\nFurthermore, while entropy generally decreases as learning progresses, this\ntrend reverses for SimSiam. More research is required to establish the cause\nfor this unexpected behaviour. Lastly, we find that while clustering based\napproaches are likely only viable for same-architecture comparisons, entropy\nmay be architecture-independent.\n", "link": "http://arxiv.org/abs/2409.06612v1", "date": "2024-09-10", "relevancy": 2.0571, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5386}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label-free%20Monitoring%20of%20Self-Supervised%20Learning%20Progress&body=Title%3A%20Label-free%20Monitoring%20of%20Self-Supervised%20Learning%20Progress%0AAuthor%3A%20Isaac%20Xu%20and%20Scott%20Lowe%20and%20Thomas%20Trappenberg%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20is%20an%20effective%20method%20for%20exploiting%0Aunlabelled%20data%20to%20learn%20a%20high-level%20embedding%20space%20that%20can%20be%20used%20for%0Avarious%20downstream%20tasks.%20However%2C%20existing%20methods%20to%20monitor%20the%20quality%20of%0Athe%20encoder%20--%20either%20during%20training%20for%20one%20model%20or%20to%20compare%20several%0Atrained%20models%20--%20still%20rely%20on%20access%20to%20annotated%20data.%20When%20SSL%0Amethodologies%20are%20applied%20to%20new%20data%20domains%2C%20a%20sufficiently%20large%20labelled%0Adataset%20may%20not%20always%20be%20available.%20In%20this%20study%2C%20we%20propose%20several%0Aevaluation%20metrics%20which%20can%20be%20applied%20on%20the%20embeddings%20of%20unlabelled%20data%0Aand%20investigate%20their%20viability%20by%20comparing%20them%20to%20linear%20probe%20accuracy%20%28a%0Acommon%20metric%20which%20utilizes%20an%20annotated%20dataset%29.%20In%20particular%2C%20we%20apply%0A%24k%24-means%20clustering%20and%20measure%20the%20clustering%20quality%20with%20the%20silhouette%0Ascore%20and%20clustering%20agreement.%20We%20also%20measure%20the%20entropy%20of%20the%20embedding%0Adistribution.%20We%20find%20that%20while%20the%20clusters%20did%20correspond%20better%20to%20the%0Aground%20truth%20annotations%20as%20training%20of%20the%20network%20progressed%2C%20label-free%0Aclustering%20metrics%20correlated%20with%20the%20linear%20probe%20accuracy%20only%20when%20training%0Awith%20SSL%20methods%20SimCLR%20and%20MoCo-v2%2C%20but%20not%20with%20SimSiam.%20Additionally%2C%0Aalthough%20entropy%20did%20not%20always%20have%20strong%20correlations%20with%20LP%20accuracy%2C%20this%0Aappears%20to%20be%20due%20to%20instability%20arising%20from%20early%20training%2C%20with%20the%20metric%0Astabilizing%20and%20becoming%20more%20reliable%20at%20later%20stages%20of%20learning.%0AFurthermore%2C%20while%20entropy%20generally%20decreases%20as%20learning%20progresses%2C%20this%0Atrend%20reverses%20for%20SimSiam.%20More%20research%20is%20required%20to%20establish%20the%20cause%0Afor%20this%20unexpected%20behaviour.%20Lastly%2C%20we%20find%20that%20while%20clustering%20based%0Aapproaches%20are%20likely%20only%20viable%20for%20same-architecture%20comparisons%2C%20entropy%0Amay%20be%20architecture-independent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel-free%2520Monitoring%2520of%2520Self-Supervised%2520Learning%2520Progress%26entry.906535625%3DIsaac%2520Xu%2520and%2520Scott%2520Lowe%2520and%2520Thomas%2520Trappenberg%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520is%2520an%2520effective%2520method%2520for%2520exploiting%250Aunlabelled%2520data%2520to%2520learn%2520a%2520high-level%2520embedding%2520space%2520that%2520can%2520be%2520used%2520for%250Avarious%2520downstream%2520tasks.%2520However%252C%2520existing%2520methods%2520to%2520monitor%2520the%2520quality%2520of%250Athe%2520encoder%2520--%2520either%2520during%2520training%2520for%2520one%2520model%2520or%2520to%2520compare%2520several%250Atrained%2520models%2520--%2520still%2520rely%2520on%2520access%2520to%2520annotated%2520data.%2520When%2520SSL%250Amethodologies%2520are%2520applied%2520to%2520new%2520data%2520domains%252C%2520a%2520sufficiently%2520large%2520labelled%250Adataset%2520may%2520not%2520always%2520be%2520available.%2520In%2520this%2520study%252C%2520we%2520propose%2520several%250Aevaluation%2520metrics%2520which%2520can%2520be%2520applied%2520on%2520the%2520embeddings%2520of%2520unlabelled%2520data%250Aand%2520investigate%2520their%2520viability%2520by%2520comparing%2520them%2520to%2520linear%2520probe%2520accuracy%2520%2528a%250Acommon%2520metric%2520which%2520utilizes%2520an%2520annotated%2520dataset%2529.%2520In%2520particular%252C%2520we%2520apply%250A%2524k%2524-means%2520clustering%2520and%2520measure%2520the%2520clustering%2520quality%2520with%2520the%2520silhouette%250Ascore%2520and%2520clustering%2520agreement.%2520We%2520also%2520measure%2520the%2520entropy%2520of%2520the%2520embedding%250Adistribution.%2520We%2520find%2520that%2520while%2520the%2520clusters%2520did%2520correspond%2520better%2520to%2520the%250Aground%2520truth%2520annotations%2520as%2520training%2520of%2520the%2520network%2520progressed%252C%2520label-free%250Aclustering%2520metrics%2520correlated%2520with%2520the%2520linear%2520probe%2520accuracy%2520only%2520when%2520training%250Awith%2520SSL%2520methods%2520SimCLR%2520and%2520MoCo-v2%252C%2520but%2520not%2520with%2520SimSiam.%2520Additionally%252C%250Aalthough%2520entropy%2520did%2520not%2520always%2520have%2520strong%2520correlations%2520with%2520LP%2520accuracy%252C%2520this%250Aappears%2520to%2520be%2520due%2520to%2520instability%2520arising%2520from%2520early%2520training%252C%2520with%2520the%2520metric%250Astabilizing%2520and%2520becoming%2520more%2520reliable%2520at%2520later%2520stages%2520of%2520learning.%250AFurthermore%252C%2520while%2520entropy%2520generally%2520decreases%2520as%2520learning%2520progresses%252C%2520this%250Atrend%2520reverses%2520for%2520SimSiam.%2520More%2520research%2520is%2520required%2520to%2520establish%2520the%2520cause%250Afor%2520this%2520unexpected%2520behaviour.%2520Lastly%252C%2520we%2520find%2520that%2520while%2520clustering%2520based%250Aapproaches%2520are%2520likely%2520only%2520viable%2520for%2520same-architecture%2520comparisons%252C%2520entropy%250Amay%2520be%2520architecture-independent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-free%20Monitoring%20of%20Self-Supervised%20Learning%20Progress&entry.906535625=Isaac%20Xu%20and%20Scott%20Lowe%20and%20Thomas%20Trappenberg&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20is%20an%20effective%20method%20for%20exploiting%0Aunlabelled%20data%20to%20learn%20a%20high-level%20embedding%20space%20that%20can%20be%20used%20for%0Avarious%20downstream%20tasks.%20However%2C%20existing%20methods%20to%20monitor%20the%20quality%20of%0Athe%20encoder%20--%20either%20during%20training%20for%20one%20model%20or%20to%20compare%20several%0Atrained%20models%20--%20still%20rely%20on%20access%20to%20annotated%20data.%20When%20SSL%0Amethodologies%20are%20applied%20to%20new%20data%20domains%2C%20a%20sufficiently%20large%20labelled%0Adataset%20may%20not%20always%20be%20available.%20In%20this%20study%2C%20we%20propose%20several%0Aevaluation%20metrics%20which%20can%20be%20applied%20on%20the%20embeddings%20of%20unlabelled%20data%0Aand%20investigate%20their%20viability%20by%20comparing%20them%20to%20linear%20probe%20accuracy%20%28a%0Acommon%20metric%20which%20utilizes%20an%20annotated%20dataset%29.%20In%20particular%2C%20we%20apply%0A%24k%24-means%20clustering%20and%20measure%20the%20clustering%20quality%20with%20the%20silhouette%0Ascore%20and%20clustering%20agreement.%20We%20also%20measure%20the%20entropy%20of%20the%20embedding%0Adistribution.%20We%20find%20that%20while%20the%20clusters%20did%20correspond%20better%20to%20the%0Aground%20truth%20annotations%20as%20training%20of%20the%20network%20progressed%2C%20label-free%0Aclustering%20metrics%20correlated%20with%20the%20linear%20probe%20accuracy%20only%20when%20training%0Awith%20SSL%20methods%20SimCLR%20and%20MoCo-v2%2C%20but%20not%20with%20SimSiam.%20Additionally%2C%0Aalthough%20entropy%20did%20not%20always%20have%20strong%20correlations%20with%20LP%20accuracy%2C%20this%0Aappears%20to%20be%20due%20to%20instability%20arising%20from%20early%20training%2C%20with%20the%20metric%0Astabilizing%20and%20becoming%20more%20reliable%20at%20later%20stages%20of%20learning.%0AFurthermore%2C%20while%20entropy%20generally%20decreases%20as%20learning%20progresses%2C%20this%0Atrend%20reverses%20for%20SimSiam.%20More%20research%20is%20required%20to%20establish%20the%20cause%0Afor%20this%20unexpected%20behaviour.%20Lastly%2C%20we%20find%20that%20while%20clustering%20based%0Aapproaches%20are%20likely%20only%20viable%20for%20same-architecture%20comparisons%2C%20entropy%0Amay%20be%20architecture-independent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06612v1&entry.124074799=Read"},
{"title": "Extending 6D Object Pose Estimators for Stereo Vision", "author": "Thomas P\u00f6llabauer and Jan Emrich and Volker Knauthe and Arjan Kuijper", "abstract": "  Estimating the 6D pose of objects accurately, quickly, and robustly remains a\ndifficult task. However, recent methods for directly regressing poses from RGB\nimages using dense features have achieved state-of-the-art results. Stereo\nvision, which provides an additional perspective on the object, can help reduce\npose ambiguity and occlusion. Moreover, stereo can directly infer the distance\nof an object, while mono-vision requires internalized knowledge of the object's\nsize. To extend the state-of-the-art in 6D object pose estimation to stereo, we\ncreated a BOP compatible stereo version of the YCB-V dataset. Our method\noutperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo\nvision and can easily be adopted for other dense feature-based algorithms.\n", "link": "http://arxiv.org/abs/2402.05610v2", "date": "2024-09-10", "relevancy": 2.0571, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5156}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extending%206D%20Object%20Pose%20Estimators%20for%20Stereo%20Vision&body=Title%3A%20Extending%206D%20Object%20Pose%20Estimators%20for%20Stereo%20Vision%0AAuthor%3A%20Thomas%20P%C3%B6llabauer%20and%20Jan%20Emrich%20and%20Volker%20Knauthe%20and%20Arjan%20Kuijper%0AAbstract%3A%20%20%20Estimating%20the%206D%20pose%20of%20objects%20accurately%2C%20quickly%2C%20and%20robustly%20remains%20a%0Adifficult%20task.%20However%2C%20recent%20methods%20for%20directly%20regressing%20poses%20from%20RGB%0Aimages%20using%20dense%20features%20have%20achieved%20state-of-the-art%20results.%20Stereo%0Avision%2C%20which%20provides%20an%20additional%20perspective%20on%20the%20object%2C%20can%20help%20reduce%0Apose%20ambiguity%20and%20occlusion.%20Moreover%2C%20stereo%20can%20directly%20infer%20the%20distance%0Aof%20an%20object%2C%20while%20mono-vision%20requires%20internalized%20knowledge%20of%20the%20object%27s%0Asize.%20To%20extend%20the%20state-of-the-art%20in%206D%20object%20pose%20estimation%20to%20stereo%2C%20we%0Acreated%20a%20BOP%20compatible%20stereo%20version%20of%20the%20YCB-V%20dataset.%20Our%20method%0Aoutperforms%20state-of-the-art%206D%20pose%20estimation%20algorithms%20by%20utilizing%20stereo%0Avision%20and%20can%20easily%20be%20adopted%20for%20other%20dense%20feature-based%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05610v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtending%25206D%2520Object%2520Pose%2520Estimators%2520for%2520Stereo%2520Vision%26entry.906535625%3DThomas%2520P%25C3%25B6llabauer%2520and%2520Jan%2520Emrich%2520and%2520Volker%2520Knauthe%2520and%2520Arjan%2520Kuijper%26entry.1292438233%3D%2520%2520Estimating%2520the%25206D%2520pose%2520of%2520objects%2520accurately%252C%2520quickly%252C%2520and%2520robustly%2520remains%2520a%250Adifficult%2520task.%2520However%252C%2520recent%2520methods%2520for%2520directly%2520regressing%2520poses%2520from%2520RGB%250Aimages%2520using%2520dense%2520features%2520have%2520achieved%2520state-of-the-art%2520results.%2520Stereo%250Avision%252C%2520which%2520provides%2520an%2520additional%2520perspective%2520on%2520the%2520object%252C%2520can%2520help%2520reduce%250Apose%2520ambiguity%2520and%2520occlusion.%2520Moreover%252C%2520stereo%2520can%2520directly%2520infer%2520the%2520distance%250Aof%2520an%2520object%252C%2520while%2520mono-vision%2520requires%2520internalized%2520knowledge%2520of%2520the%2520object%2527s%250Asize.%2520To%2520extend%2520the%2520state-of-the-art%2520in%25206D%2520object%2520pose%2520estimation%2520to%2520stereo%252C%2520we%250Acreated%2520a%2520BOP%2520compatible%2520stereo%2520version%2520of%2520the%2520YCB-V%2520dataset.%2520Our%2520method%250Aoutperforms%2520state-of-the-art%25206D%2520pose%2520estimation%2520algorithms%2520by%2520utilizing%2520stereo%250Avision%2520and%2520can%2520easily%2520be%2520adopted%2520for%2520other%2520dense%2520feature-based%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05610v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extending%206D%20Object%20Pose%20Estimators%20for%20Stereo%20Vision&entry.906535625=Thomas%20P%C3%B6llabauer%20and%20Jan%20Emrich%20and%20Volker%20Knauthe%20and%20Arjan%20Kuijper&entry.1292438233=%20%20Estimating%20the%206D%20pose%20of%20objects%20accurately%2C%20quickly%2C%20and%20robustly%20remains%20a%0Adifficult%20task.%20However%2C%20recent%20methods%20for%20directly%20regressing%20poses%20from%20RGB%0Aimages%20using%20dense%20features%20have%20achieved%20state-of-the-art%20results.%20Stereo%0Avision%2C%20which%20provides%20an%20additional%20perspective%20on%20the%20object%2C%20can%20help%20reduce%0Apose%20ambiguity%20and%20occlusion.%20Moreover%2C%20stereo%20can%20directly%20infer%20the%20distance%0Aof%20an%20object%2C%20while%20mono-vision%20requires%20internalized%20knowledge%20of%20the%20object%27s%0Asize.%20To%20extend%20the%20state-of-the-art%20in%206D%20object%20pose%20estimation%20to%20stereo%2C%20we%0Acreated%20a%20BOP%20compatible%20stereo%20version%20of%20the%20YCB-V%20dataset.%20Our%20method%0Aoutperforms%20state-of-the-art%206D%20pose%20estimation%20algorithms%20by%20utilizing%20stereo%0Avision%20and%20can%20easily%20be%20adopted%20for%20other%20dense%20feature-based%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05610v2&entry.124074799=Read"},
{"title": "Analysis of Unstructured High-Density Crowded Scenes for Crowd\n  Monitoring", "author": "Alexandre Matov", "abstract": "  We are interested in developing an automated system for detection of\norganized movements in human crowds. Computer vision algorithms can extract\ninformation from videos of crowded scenes and automatically detect and track\ngroups of individuals undergoing organized motion that represents an anomalous\nbehavior in the context of conflict aversion. Our system can detect organized\ncohorts against the background of randomly moving objects and we can estimate\nthe number of participants in an organized cohort, the speed and direction of\nmotion in real time, within three to four video frames, which is less than one\nsecond from the onset of motion captured on a CCTV. We have performed\npreliminary analysis in this context in biological cell data containing up to\nfour thousand objects per frame and will extend this numerically to a\nhundred-fold for public safety applications.\n  We envisage using the existing infrastructure of video cameras for acquiring\nimage datasets on-the-fly and deploying an easy-to-use data-driven software\nsystem for parsing of significant events by analyzing image sequences taken\ninside and outside of sports stadiums or other public venues. Other prospective\nusers are organizers of political rallies, civic and wildlife organizations,\nsecurity firms, and the military. We will optimize the performance of the\nsoftware by implementing a classification method able to distinguish between\nactivities posing a threat and those not posing a threat.\n", "link": "http://arxiv.org/abs/2408.11836v4", "date": "2024-09-10", "relevancy": 2.0411, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Unstructured%20High-Density%20Crowded%20Scenes%20for%20Crowd%0A%20%20Monitoring&body=Title%3A%20Analysis%20of%20Unstructured%20High-Density%20Crowded%20Scenes%20for%20Crowd%0A%20%20Monitoring%0AAuthor%3A%20Alexandre%20Matov%0AAbstract%3A%20%20%20We%20are%20interested%20in%20developing%20an%20automated%20system%20for%20detection%20of%0Aorganized%20movements%20in%20human%20crowds.%20Computer%20vision%20algorithms%20can%20extract%0Ainformation%20from%20videos%20of%20crowded%20scenes%20and%20automatically%20detect%20and%20track%0Agroups%20of%20individuals%20undergoing%20organized%20motion%20that%20represents%20an%20anomalous%0Abehavior%20in%20the%20context%20of%20conflict%20aversion.%20Our%20system%20can%20detect%20organized%0Acohorts%20against%20the%20background%20of%20randomly%20moving%20objects%20and%20we%20can%20estimate%0Athe%20number%20of%20participants%20in%20an%20organized%20cohort%2C%20the%20speed%20and%20direction%20of%0Amotion%20in%20real%20time%2C%20within%20three%20to%20four%20video%20frames%2C%20which%20is%20less%20than%20one%0Asecond%20from%20the%20onset%20of%20motion%20captured%20on%20a%20CCTV.%20We%20have%20performed%0Apreliminary%20analysis%20in%20this%20context%20in%20biological%20cell%20data%20containing%20up%20to%0Afour%20thousand%20objects%20per%20frame%20and%20will%20extend%20this%20numerically%20to%20a%0Ahundred-fold%20for%20public%20safety%20applications.%0A%20%20We%20envisage%20using%20the%20existing%20infrastructure%20of%20video%20cameras%20for%20acquiring%0Aimage%20datasets%20on-the-fly%20and%20deploying%20an%20easy-to-use%20data-driven%20software%0Asystem%20for%20parsing%20of%20significant%20events%20by%20analyzing%20image%20sequences%20taken%0Ainside%20and%20outside%20of%20sports%20stadiums%20or%20other%20public%20venues.%20Other%20prospective%0Ausers%20are%20organizers%20of%20political%20rallies%2C%20civic%20and%20wildlife%20organizations%2C%0Asecurity%20firms%2C%20and%20the%20military.%20We%20will%20optimize%20the%20performance%20of%20the%0Asoftware%20by%20implementing%20a%20classification%20method%20able%20to%20distinguish%20between%0Aactivities%20posing%20a%20threat%20and%20those%20not%20posing%20a%20threat.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11836v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Unstructured%2520High-Density%2520Crowded%2520Scenes%2520for%2520Crowd%250A%2520%2520Monitoring%26entry.906535625%3DAlexandre%2520Matov%26entry.1292438233%3D%2520%2520We%2520are%2520interested%2520in%2520developing%2520an%2520automated%2520system%2520for%2520detection%2520of%250Aorganized%2520movements%2520in%2520human%2520crowds.%2520Computer%2520vision%2520algorithms%2520can%2520extract%250Ainformation%2520from%2520videos%2520of%2520crowded%2520scenes%2520and%2520automatically%2520detect%2520and%2520track%250Agroups%2520of%2520individuals%2520undergoing%2520organized%2520motion%2520that%2520represents%2520an%2520anomalous%250Abehavior%2520in%2520the%2520context%2520of%2520conflict%2520aversion.%2520Our%2520system%2520can%2520detect%2520organized%250Acohorts%2520against%2520the%2520background%2520of%2520randomly%2520moving%2520objects%2520and%2520we%2520can%2520estimate%250Athe%2520number%2520of%2520participants%2520in%2520an%2520organized%2520cohort%252C%2520the%2520speed%2520and%2520direction%2520of%250Amotion%2520in%2520real%2520time%252C%2520within%2520three%2520to%2520four%2520video%2520frames%252C%2520which%2520is%2520less%2520than%2520one%250Asecond%2520from%2520the%2520onset%2520of%2520motion%2520captured%2520on%2520a%2520CCTV.%2520We%2520have%2520performed%250Apreliminary%2520analysis%2520in%2520this%2520context%2520in%2520biological%2520cell%2520data%2520containing%2520up%2520to%250Afour%2520thousand%2520objects%2520per%2520frame%2520and%2520will%2520extend%2520this%2520numerically%2520to%2520a%250Ahundred-fold%2520for%2520public%2520safety%2520applications.%250A%2520%2520We%2520envisage%2520using%2520the%2520existing%2520infrastructure%2520of%2520video%2520cameras%2520for%2520acquiring%250Aimage%2520datasets%2520on-the-fly%2520and%2520deploying%2520an%2520easy-to-use%2520data-driven%2520software%250Asystem%2520for%2520parsing%2520of%2520significant%2520events%2520by%2520analyzing%2520image%2520sequences%2520taken%250Ainside%2520and%2520outside%2520of%2520sports%2520stadiums%2520or%2520other%2520public%2520venues.%2520Other%2520prospective%250Ausers%2520are%2520organizers%2520of%2520political%2520rallies%252C%2520civic%2520and%2520wildlife%2520organizations%252C%250Asecurity%2520firms%252C%2520and%2520the%2520military.%2520We%2520will%2520optimize%2520the%2520performance%2520of%2520the%250Asoftware%2520by%2520implementing%2520a%2520classification%2520method%2520able%2520to%2520distinguish%2520between%250Aactivities%2520posing%2520a%2520threat%2520and%2520those%2520not%2520posing%2520a%2520threat.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11836v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Unstructured%20High-Density%20Crowded%20Scenes%20for%20Crowd%0A%20%20Monitoring&entry.906535625=Alexandre%20Matov&entry.1292438233=%20%20We%20are%20interested%20in%20developing%20an%20automated%20system%20for%20detection%20of%0Aorganized%20movements%20in%20human%20crowds.%20Computer%20vision%20algorithms%20can%20extract%0Ainformation%20from%20videos%20of%20crowded%20scenes%20and%20automatically%20detect%20and%20track%0Agroups%20of%20individuals%20undergoing%20organized%20motion%20that%20represents%20an%20anomalous%0Abehavior%20in%20the%20context%20of%20conflict%20aversion.%20Our%20system%20can%20detect%20organized%0Acohorts%20against%20the%20background%20of%20randomly%20moving%20objects%20and%20we%20can%20estimate%0Athe%20number%20of%20participants%20in%20an%20organized%20cohort%2C%20the%20speed%20and%20direction%20of%0Amotion%20in%20real%20time%2C%20within%20three%20to%20four%20video%20frames%2C%20which%20is%20less%20than%20one%0Asecond%20from%20the%20onset%20of%20motion%20captured%20on%20a%20CCTV.%20We%20have%20performed%0Apreliminary%20analysis%20in%20this%20context%20in%20biological%20cell%20data%20containing%20up%20to%0Afour%20thousand%20objects%20per%20frame%20and%20will%20extend%20this%20numerically%20to%20a%0Ahundred-fold%20for%20public%20safety%20applications.%0A%20%20We%20envisage%20using%20the%20existing%20infrastructure%20of%20video%20cameras%20for%20acquiring%0Aimage%20datasets%20on-the-fly%20and%20deploying%20an%20easy-to-use%20data-driven%20software%0Asystem%20for%20parsing%20of%20significant%20events%20by%20analyzing%20image%20sequences%20taken%0Ainside%20and%20outside%20of%20sports%20stadiums%20or%20other%20public%20venues.%20Other%20prospective%0Ausers%20are%20organizers%20of%20political%20rallies%2C%20civic%20and%20wildlife%20organizations%2C%0Asecurity%20firms%2C%20and%20the%20military.%20We%20will%20optimize%20the%20performance%20of%20the%0Asoftware%20by%20implementing%20a%20classification%20method%20able%20to%20distinguish%20between%0Aactivities%20posing%20a%20threat%20and%20those%20not%20posing%20a%20threat.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11836v4&entry.124074799=Read"},
{"title": "Dynamic Decoupling of Placid Terminal Attractor-based Gradient Descent\n  Algorithm", "author": "Jinwei Zhao and Marco Gori and Alessandro Betti and Stefano Melacci and Hongtao Zhang and Jiedong Liu and Xinhong Hei", "abstract": "  Gradient descent (GD) and stochastic gradient descent (SGD) have been widely\nused in a large number of application domains. Therefore, understanding the\ndynamics of GD and improving its convergence speed is still of great\nimportance. This paper carefully analyzes the dynamics of GD based on the\nterminal attractor at different stages of its gradient flow. On the basis of\nthe terminal sliding mode theory and the terminal attractor theory, four\nadaptive learning rates are designed. Their performances are investigated in\nlight of a detailed theoretical investigation, and the running times of the\nlearning procedures are evaluated and compared. The total times of their\nlearning processes are also studied in detail. To evaluate their effectiveness,\nvarious simulation results are investigated on a function approximation problem\nand an image classification problem.\n", "link": "http://arxiv.org/abs/2409.06542v1", "date": "2024-09-10", "relevancy": 2.027, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5091}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5062}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Decoupling%20of%20Placid%20Terminal%20Attractor-based%20Gradient%20Descent%0A%20%20Algorithm&body=Title%3A%20Dynamic%20Decoupling%20of%20Placid%20Terminal%20Attractor-based%20Gradient%20Descent%0A%20%20Algorithm%0AAuthor%3A%20Jinwei%20Zhao%20and%20Marco%20Gori%20and%20Alessandro%20Betti%20and%20Stefano%20Melacci%20and%20Hongtao%20Zhang%20and%20Jiedong%20Liu%20and%20Xinhong%20Hei%0AAbstract%3A%20%20%20Gradient%20descent%20%28GD%29%20and%20stochastic%20gradient%20descent%20%28SGD%29%20have%20been%20widely%0Aused%20in%20a%20large%20number%20of%20application%20domains.%20Therefore%2C%20understanding%20the%0Adynamics%20of%20GD%20and%20improving%20its%20convergence%20speed%20is%20still%20of%20great%0Aimportance.%20This%20paper%20carefully%20analyzes%20the%20dynamics%20of%20GD%20based%20on%20the%0Aterminal%20attractor%20at%20different%20stages%20of%20its%20gradient%20flow.%20On%20the%20basis%20of%0Athe%20terminal%20sliding%20mode%20theory%20and%20the%20terminal%20attractor%20theory%2C%20four%0Aadaptive%20learning%20rates%20are%20designed.%20Their%20performances%20are%20investigated%20in%0Alight%20of%20a%20detailed%20theoretical%20investigation%2C%20and%20the%20running%20times%20of%20the%0Alearning%20procedures%20are%20evaluated%20and%20compared.%20The%20total%20times%20of%20their%0Alearning%20processes%20are%20also%20studied%20in%20detail.%20To%20evaluate%20their%20effectiveness%2C%0Avarious%20simulation%20results%20are%20investigated%20on%20a%20function%20approximation%20problem%0Aand%20an%20image%20classification%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Decoupling%2520of%2520Placid%2520Terminal%2520Attractor-based%2520Gradient%2520Descent%250A%2520%2520Algorithm%26entry.906535625%3DJinwei%2520Zhao%2520and%2520Marco%2520Gori%2520and%2520Alessandro%2520Betti%2520and%2520Stefano%2520Melacci%2520and%2520Hongtao%2520Zhang%2520and%2520Jiedong%2520Liu%2520and%2520Xinhong%2520Hei%26entry.1292438233%3D%2520%2520Gradient%2520descent%2520%2528GD%2529%2520and%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520have%2520been%2520widely%250Aused%2520in%2520a%2520large%2520number%2520of%2520application%2520domains.%2520Therefore%252C%2520understanding%2520the%250Adynamics%2520of%2520GD%2520and%2520improving%2520its%2520convergence%2520speed%2520is%2520still%2520of%2520great%250Aimportance.%2520This%2520paper%2520carefully%2520analyzes%2520the%2520dynamics%2520of%2520GD%2520based%2520on%2520the%250Aterminal%2520attractor%2520at%2520different%2520stages%2520of%2520its%2520gradient%2520flow.%2520On%2520the%2520basis%2520of%250Athe%2520terminal%2520sliding%2520mode%2520theory%2520and%2520the%2520terminal%2520attractor%2520theory%252C%2520four%250Aadaptive%2520learning%2520rates%2520are%2520designed.%2520Their%2520performances%2520are%2520investigated%2520in%250Alight%2520of%2520a%2520detailed%2520theoretical%2520investigation%252C%2520and%2520the%2520running%2520times%2520of%2520the%250Alearning%2520procedures%2520are%2520evaluated%2520and%2520compared.%2520The%2520total%2520times%2520of%2520their%250Alearning%2520processes%2520are%2520also%2520studied%2520in%2520detail.%2520To%2520evaluate%2520their%2520effectiveness%252C%250Avarious%2520simulation%2520results%2520are%2520investigated%2520on%2520a%2520function%2520approximation%2520problem%250Aand%2520an%2520image%2520classification%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Decoupling%20of%20Placid%20Terminal%20Attractor-based%20Gradient%20Descent%0A%20%20Algorithm&entry.906535625=Jinwei%20Zhao%20and%20Marco%20Gori%20and%20Alessandro%20Betti%20and%20Stefano%20Melacci%20and%20Hongtao%20Zhang%20and%20Jiedong%20Liu%20and%20Xinhong%20Hei&entry.1292438233=%20%20Gradient%20descent%20%28GD%29%20and%20stochastic%20gradient%20descent%20%28SGD%29%20have%20been%20widely%0Aused%20in%20a%20large%20number%20of%20application%20domains.%20Therefore%2C%20understanding%20the%0Adynamics%20of%20GD%20and%20improving%20its%20convergence%20speed%20is%20still%20of%20great%0Aimportance.%20This%20paper%20carefully%20analyzes%20the%20dynamics%20of%20GD%20based%20on%20the%0Aterminal%20attractor%20at%20different%20stages%20of%20its%20gradient%20flow.%20On%20the%20basis%20of%0Athe%20terminal%20sliding%20mode%20theory%20and%20the%20terminal%20attractor%20theory%2C%20four%0Aadaptive%20learning%20rates%20are%20designed.%20Their%20performances%20are%20investigated%20in%0Alight%20of%20a%20detailed%20theoretical%20investigation%2C%20and%20the%20running%20times%20of%20the%0Alearning%20procedures%20are%20evaluated%20and%20compared.%20The%20total%20times%20of%20their%0Alearning%20processes%20are%20also%20studied%20in%20detail.%20To%20evaluate%20their%20effectiveness%2C%0Avarious%20simulation%20results%20are%20investigated%20on%20a%20function%20approximation%20problem%0Aand%20an%20image%20classification%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06542v1&entry.124074799=Read"},
{"title": "Multi-Margin Cosine Loss: Proposal and Application in Recommender\n  Systems", "author": "Makbule Gulcin Ozsoy", "abstract": "  Recommender systems guide users through vast amounts of information by\nsuggesting items based on their predicted preferences. Collaborative\nfiltering-based deep learning techniques have regained popularity due to their\nstraightforward nature, relying only on user-item interactions. Typically,\nthese systems consist of three main components: an interaction module, a loss\nfunction, and a negative sampling strategy. Initially, researchers focused on\nenhancing performance by developing complex interaction modules. However, there\nhas been a recent shift toward refining loss functions and negative sampling\nstrategies. This shift has led to an increased interest in contrastive\nlearning, which pulls similar pairs closer while pushing dissimilar ones apart.\nContrastive learning may bring challenges like high memory demands and\nunder-utilization of some negative samples. The proposed Multi-Margin Cosine\nLoss (MMCL) addresses these challenges by introducing multiple margins and\nvarying weights for negative samples. It efficiently utilizes not only the\nhardest negatives but also other non-trivial negatives, offers a simpler yet\neffective loss function that outperforms more complex methods, especially when\nresources are limited. Experiments on two well-known datasets demonstrated that\nMMCL achieved up to a 20\\% performance improvement compared to a baseline loss\nfunction when fewer number of negative samples are used.\n", "link": "http://arxiv.org/abs/2405.04614v3", "date": "2024-09-10", "relevancy": 2.0264, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5075}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5075}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Margin%20Cosine%20Loss%3A%20Proposal%20and%20Application%20in%20Recommender%0A%20%20Systems&body=Title%3A%20Multi-Margin%20Cosine%20Loss%3A%20Proposal%20and%20Application%20in%20Recommender%0A%20%20Systems%0AAuthor%3A%20Makbule%20Gulcin%20Ozsoy%0AAbstract%3A%20%20%20Recommender%20systems%20guide%20users%20through%20vast%20amounts%20of%20information%20by%0Asuggesting%20items%20based%20on%20their%20predicted%20preferences.%20Collaborative%0Afiltering-based%20deep%20learning%20techniques%20have%20regained%20popularity%20due%20to%20their%0Astraightforward%20nature%2C%20relying%20only%20on%20user-item%20interactions.%20Typically%2C%0Athese%20systems%20consist%20of%20three%20main%20components%3A%20an%20interaction%20module%2C%20a%20loss%0Afunction%2C%20and%20a%20negative%20sampling%20strategy.%20Initially%2C%20researchers%20focused%20on%0Aenhancing%20performance%20by%20developing%20complex%20interaction%20modules.%20However%2C%20there%0Ahas%20been%20a%20recent%20shift%20toward%20refining%20loss%20functions%20and%20negative%20sampling%0Astrategies.%20This%20shift%20has%20led%20to%20an%20increased%20interest%20in%20contrastive%0Alearning%2C%20which%20pulls%20similar%20pairs%20closer%20while%20pushing%20dissimilar%20ones%20apart.%0AContrastive%20learning%20may%20bring%20challenges%20like%20high%20memory%20demands%20and%0Aunder-utilization%20of%20some%20negative%20samples.%20The%20proposed%20Multi-Margin%20Cosine%0ALoss%20%28MMCL%29%20addresses%20these%20challenges%20by%20introducing%20multiple%20margins%20and%0Avarying%20weights%20for%20negative%20samples.%20It%20efficiently%20utilizes%20not%20only%20the%0Ahardest%20negatives%20but%20also%20other%20non-trivial%20negatives%2C%20offers%20a%20simpler%20yet%0Aeffective%20loss%20function%20that%20outperforms%20more%20complex%20methods%2C%20especially%20when%0Aresources%20are%20limited.%20Experiments%20on%20two%20well-known%20datasets%20demonstrated%20that%0AMMCL%20achieved%20up%20to%20a%2020%5C%25%20performance%20improvement%20compared%20to%20a%20baseline%20loss%0Afunction%20when%20fewer%20number%20of%20negative%20samples%20are%20used.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04614v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Margin%2520Cosine%2520Loss%253A%2520Proposal%2520and%2520Application%2520in%2520Recommender%250A%2520%2520Systems%26entry.906535625%3DMakbule%2520Gulcin%2520Ozsoy%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520guide%2520users%2520through%2520vast%2520amounts%2520of%2520information%2520by%250Asuggesting%2520items%2520based%2520on%2520their%2520predicted%2520preferences.%2520Collaborative%250Afiltering-based%2520deep%2520learning%2520techniques%2520have%2520regained%2520popularity%2520due%2520to%2520their%250Astraightforward%2520nature%252C%2520relying%2520only%2520on%2520user-item%2520interactions.%2520Typically%252C%250Athese%2520systems%2520consist%2520of%2520three%2520main%2520components%253A%2520an%2520interaction%2520module%252C%2520a%2520loss%250Afunction%252C%2520and%2520a%2520negative%2520sampling%2520strategy.%2520Initially%252C%2520researchers%2520focused%2520on%250Aenhancing%2520performance%2520by%2520developing%2520complex%2520interaction%2520modules.%2520However%252C%2520there%250Ahas%2520been%2520a%2520recent%2520shift%2520toward%2520refining%2520loss%2520functions%2520and%2520negative%2520sampling%250Astrategies.%2520This%2520shift%2520has%2520led%2520to%2520an%2520increased%2520interest%2520in%2520contrastive%250Alearning%252C%2520which%2520pulls%2520similar%2520pairs%2520closer%2520while%2520pushing%2520dissimilar%2520ones%2520apart.%250AContrastive%2520learning%2520may%2520bring%2520challenges%2520like%2520high%2520memory%2520demands%2520and%250Aunder-utilization%2520of%2520some%2520negative%2520samples.%2520The%2520proposed%2520Multi-Margin%2520Cosine%250ALoss%2520%2528MMCL%2529%2520addresses%2520these%2520challenges%2520by%2520introducing%2520multiple%2520margins%2520and%250Avarying%2520weights%2520for%2520negative%2520samples.%2520It%2520efficiently%2520utilizes%2520not%2520only%2520the%250Ahardest%2520negatives%2520but%2520also%2520other%2520non-trivial%2520negatives%252C%2520offers%2520a%2520simpler%2520yet%250Aeffective%2520loss%2520function%2520that%2520outperforms%2520more%2520complex%2520methods%252C%2520especially%2520when%250Aresources%2520are%2520limited.%2520Experiments%2520on%2520two%2520well-known%2520datasets%2520demonstrated%2520that%250AMMCL%2520achieved%2520up%2520to%2520a%252020%255C%2525%2520performance%2520improvement%2520compared%2520to%2520a%2520baseline%2520loss%250Afunction%2520when%2520fewer%2520number%2520of%2520negative%2520samples%2520are%2520used.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04614v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Margin%20Cosine%20Loss%3A%20Proposal%20and%20Application%20in%20Recommender%0A%20%20Systems&entry.906535625=Makbule%20Gulcin%20Ozsoy&entry.1292438233=%20%20Recommender%20systems%20guide%20users%20through%20vast%20amounts%20of%20information%20by%0Asuggesting%20items%20based%20on%20their%20predicted%20preferences.%20Collaborative%0Afiltering-based%20deep%20learning%20techniques%20have%20regained%20popularity%20due%20to%20their%0Astraightforward%20nature%2C%20relying%20only%20on%20user-item%20interactions.%20Typically%2C%0Athese%20systems%20consist%20of%20three%20main%20components%3A%20an%20interaction%20module%2C%20a%20loss%0Afunction%2C%20and%20a%20negative%20sampling%20strategy.%20Initially%2C%20researchers%20focused%20on%0Aenhancing%20performance%20by%20developing%20complex%20interaction%20modules.%20However%2C%20there%0Ahas%20been%20a%20recent%20shift%20toward%20refining%20loss%20functions%20and%20negative%20sampling%0Astrategies.%20This%20shift%20has%20led%20to%20an%20increased%20interest%20in%20contrastive%0Alearning%2C%20which%20pulls%20similar%20pairs%20closer%20while%20pushing%20dissimilar%20ones%20apart.%0AContrastive%20learning%20may%20bring%20challenges%20like%20high%20memory%20demands%20and%0Aunder-utilization%20of%20some%20negative%20samples.%20The%20proposed%20Multi-Margin%20Cosine%0ALoss%20%28MMCL%29%20addresses%20these%20challenges%20by%20introducing%20multiple%20margins%20and%0Avarying%20weights%20for%20negative%20samples.%20It%20efficiently%20utilizes%20not%20only%20the%0Ahardest%20negatives%20but%20also%20other%20non-trivial%20negatives%2C%20offers%20a%20simpler%20yet%0Aeffective%20loss%20function%20that%20outperforms%20more%20complex%20methods%2C%20especially%20when%0Aresources%20are%20limited.%20Experiments%20on%20two%20well-known%20datasets%20demonstrated%20that%0AMMCL%20achieved%20up%20to%20a%2020%5C%25%20performance%20improvement%20compared%20to%20a%20baseline%20loss%0Afunction%20when%20fewer%20number%20of%20negative%20samples%20are%20used.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04614v3&entry.124074799=Read"},
{"title": "Caterpillar: A Pure-MLP Architecture with Shifted-Pillars-Concatenation", "author": "Jin Sun and Xiaoshuang Shi and Zhiyuan Wang and Kaidi Xu and Heng Tao Shen and Xiaofeng Zhu", "abstract": "  Modeling in Computer Vision has evolved to MLPs. Vision MLPs naturally lack\nlocal modeling capability, to which the simplest treatment is combined with\nconvolutional layers. Convolution, famous for its sliding window scheme, also\nsuffers from this scheme of redundancy and lower parallel computation. In this\npaper, we seek to dispense with the windowing scheme and introduce a more\nelaborate and parallelizable method to exploit locality. To this end, we\npropose a new MLP module, namely Shifted-Pillars-Concatenation (SPC), that\nconsists of two steps of processes: (1) Pillars-Shift, which generates four\nneighboring maps by shifting the input image along four directions, and (2)\nPillars-Concatenation, which applies linear transformations and concatenation\non the maps to aggregate local features. SPC module offers superior local\nmodeling power and performance gains, making it a promising alternative to the\nconvolutional layer. Then, we build a pure-MLP architecture called Caterpillar\nby replacing the convolutional layer with the SPC module in a hybrid model of\nsMLPNet. Extensive experiments show Caterpillar's excellent performance on both\nsmall-scale and ImageNet-1k classification benchmarks, with remarkable\nscalability and transfer capability possessed as well. The code is available at\nhttps://github.com/sunjin19126/Caterpillar.\n", "link": "http://arxiv.org/abs/2305.17644v3", "date": "2024-09-10", "relevancy": 2.0028, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4922}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Caterpillar%3A%20A%20Pure-MLP%20Architecture%20with%20Shifted-Pillars-Concatenation&body=Title%3A%20Caterpillar%3A%20A%20Pure-MLP%20Architecture%20with%20Shifted-Pillars-Concatenation%0AAuthor%3A%20Jin%20Sun%20and%20Xiaoshuang%20Shi%20and%20Zhiyuan%20Wang%20and%20Kaidi%20Xu%20and%20Heng%20Tao%20Shen%20and%20Xiaofeng%20Zhu%0AAbstract%3A%20%20%20Modeling%20in%20Computer%20Vision%20has%20evolved%20to%20MLPs.%20Vision%20MLPs%20naturally%20lack%0Alocal%20modeling%20capability%2C%20to%20which%20the%20simplest%20treatment%20is%20combined%20with%0Aconvolutional%20layers.%20Convolution%2C%20famous%20for%20its%20sliding%20window%20scheme%2C%20also%0Asuffers%20from%20this%20scheme%20of%20redundancy%20and%20lower%20parallel%20computation.%20In%20this%0Apaper%2C%20we%20seek%20to%20dispense%20with%20the%20windowing%20scheme%20and%20introduce%20a%20more%0Aelaborate%20and%20parallelizable%20method%20to%20exploit%20locality.%20To%20this%20end%2C%20we%0Apropose%20a%20new%20MLP%20module%2C%20namely%20Shifted-Pillars-Concatenation%20%28SPC%29%2C%20that%0Aconsists%20of%20two%20steps%20of%20processes%3A%20%281%29%20Pillars-Shift%2C%20which%20generates%20four%0Aneighboring%20maps%20by%20shifting%20the%20input%20image%20along%20four%20directions%2C%20and%20%282%29%0APillars-Concatenation%2C%20which%20applies%20linear%20transformations%20and%20concatenation%0Aon%20the%20maps%20to%20aggregate%20local%20features.%20SPC%20module%20offers%20superior%20local%0Amodeling%20power%20and%20performance%20gains%2C%20making%20it%20a%20promising%20alternative%20to%20the%0Aconvolutional%20layer.%20Then%2C%20we%20build%20a%20pure-MLP%20architecture%20called%20Caterpillar%0Aby%20replacing%20the%20convolutional%20layer%20with%20the%20SPC%20module%20in%20a%20hybrid%20model%20of%0AsMLPNet.%20Extensive%20experiments%20show%20Caterpillar%27s%20excellent%20performance%20on%20both%0Asmall-scale%20and%20ImageNet-1k%20classification%20benchmarks%2C%20with%20remarkable%0Ascalability%20and%20transfer%20capability%20possessed%20as%20well.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/sunjin19126/Caterpillar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17644v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaterpillar%253A%2520A%2520Pure-MLP%2520Architecture%2520with%2520Shifted-Pillars-Concatenation%26entry.906535625%3DJin%2520Sun%2520and%2520Xiaoshuang%2520Shi%2520and%2520Zhiyuan%2520Wang%2520and%2520Kaidi%2520Xu%2520and%2520Heng%2520Tao%2520Shen%2520and%2520Xiaofeng%2520Zhu%26entry.1292438233%3D%2520%2520Modeling%2520in%2520Computer%2520Vision%2520has%2520evolved%2520to%2520MLPs.%2520Vision%2520MLPs%2520naturally%2520lack%250Alocal%2520modeling%2520capability%252C%2520to%2520which%2520the%2520simplest%2520treatment%2520is%2520combined%2520with%250Aconvolutional%2520layers.%2520Convolution%252C%2520famous%2520for%2520its%2520sliding%2520window%2520scheme%252C%2520also%250Asuffers%2520from%2520this%2520scheme%2520of%2520redundancy%2520and%2520lower%2520parallel%2520computation.%2520In%2520this%250Apaper%252C%2520we%2520seek%2520to%2520dispense%2520with%2520the%2520windowing%2520scheme%2520and%2520introduce%2520a%2520more%250Aelaborate%2520and%2520parallelizable%2520method%2520to%2520exploit%2520locality.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520new%2520MLP%2520module%252C%2520namely%2520Shifted-Pillars-Concatenation%2520%2528SPC%2529%252C%2520that%250Aconsists%2520of%2520two%2520steps%2520of%2520processes%253A%2520%25281%2529%2520Pillars-Shift%252C%2520which%2520generates%2520four%250Aneighboring%2520maps%2520by%2520shifting%2520the%2520input%2520image%2520along%2520four%2520directions%252C%2520and%2520%25282%2529%250APillars-Concatenation%252C%2520which%2520applies%2520linear%2520transformations%2520and%2520concatenation%250Aon%2520the%2520maps%2520to%2520aggregate%2520local%2520features.%2520SPC%2520module%2520offers%2520superior%2520local%250Amodeling%2520power%2520and%2520performance%2520gains%252C%2520making%2520it%2520a%2520promising%2520alternative%2520to%2520the%250Aconvolutional%2520layer.%2520Then%252C%2520we%2520build%2520a%2520pure-MLP%2520architecture%2520called%2520Caterpillar%250Aby%2520replacing%2520the%2520convolutional%2520layer%2520with%2520the%2520SPC%2520module%2520in%2520a%2520hybrid%2520model%2520of%250AsMLPNet.%2520Extensive%2520experiments%2520show%2520Caterpillar%2527s%2520excellent%2520performance%2520on%2520both%250Asmall-scale%2520and%2520ImageNet-1k%2520classification%2520benchmarks%252C%2520with%2520remarkable%250Ascalability%2520and%2520transfer%2520capability%2520possessed%2520as%2520well.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/sunjin19126/Caterpillar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.17644v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Caterpillar%3A%20A%20Pure-MLP%20Architecture%20with%20Shifted-Pillars-Concatenation&entry.906535625=Jin%20Sun%20and%20Xiaoshuang%20Shi%20and%20Zhiyuan%20Wang%20and%20Kaidi%20Xu%20and%20Heng%20Tao%20Shen%20and%20Xiaofeng%20Zhu&entry.1292438233=%20%20Modeling%20in%20Computer%20Vision%20has%20evolved%20to%20MLPs.%20Vision%20MLPs%20naturally%20lack%0Alocal%20modeling%20capability%2C%20to%20which%20the%20simplest%20treatment%20is%20combined%20with%0Aconvolutional%20layers.%20Convolution%2C%20famous%20for%20its%20sliding%20window%20scheme%2C%20also%0Asuffers%20from%20this%20scheme%20of%20redundancy%20and%20lower%20parallel%20computation.%20In%20this%0Apaper%2C%20we%20seek%20to%20dispense%20with%20the%20windowing%20scheme%20and%20introduce%20a%20more%0Aelaborate%20and%20parallelizable%20method%20to%20exploit%20locality.%20To%20this%20end%2C%20we%0Apropose%20a%20new%20MLP%20module%2C%20namely%20Shifted-Pillars-Concatenation%20%28SPC%29%2C%20that%0Aconsists%20of%20two%20steps%20of%20processes%3A%20%281%29%20Pillars-Shift%2C%20which%20generates%20four%0Aneighboring%20maps%20by%20shifting%20the%20input%20image%20along%20four%20directions%2C%20and%20%282%29%0APillars-Concatenation%2C%20which%20applies%20linear%20transformations%20and%20concatenation%0Aon%20the%20maps%20to%20aggregate%20local%20features.%20SPC%20module%20offers%20superior%20local%0Amodeling%20power%20and%20performance%20gains%2C%20making%20it%20a%20promising%20alternative%20to%20the%0Aconvolutional%20layer.%20Then%2C%20we%20build%20a%20pure-MLP%20architecture%20called%20Caterpillar%0Aby%20replacing%20the%20convolutional%20layer%20with%20the%20SPC%20module%20in%20a%20hybrid%20model%20of%0AsMLPNet.%20Extensive%20experiments%20show%20Caterpillar%27s%20excellent%20performance%20on%20both%0Asmall-scale%20and%20ImageNet-1k%20classification%20benchmarks%2C%20with%20remarkable%0Ascalability%20and%20transfer%20capability%20possessed%20as%20well.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/sunjin19126/Caterpillar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17644v3&entry.124074799=Read"},
{"title": "Valeo4Cast: A Modular Approach to End-to-End Forecasting", "author": "Yihong Xu and \u00c9loi Zablocki and Alexandre Boulch and Gilles Puy and Mickael Chen and Florent Bartoccioni and Nermin Samet and Oriane Sim\u00e9oni and Spyros Gidaris and Tuan-Hung Vu and Andrei Bursuc and Eduardo Valle and Renaud Marlet and Matthieu Cord", "abstract": "  Motion forecasting is crucial in autonomous driving systems to anticipate the\nfuture trajectories of surrounding agents such as pedestrians, vehicles, and\ntraffic signals. In end-to-end forecasting, the model must jointly detect and\ntrack from sensor data (cameras or LiDARs) the past trajectories of the\ndifferent elements of the scene and predict their future locations. We depart\nfrom the current trend of tackling this task via end-to-end training from\nperception to forecasting, and instead use a modular approach. We individually\nbuild and train detection, tracking and forecasting modules. We then only use\nconsecutive finetuning steps to integrate the modules better and alleviate\ncompounding errors. We conduct an in-depth study on the finetuning strategies\nand it reveals that our simple yet effective approach significantly improves\nperformance on the end-to-end forecasting benchmark. Consequently, our solution\nranks first in the Argoverse 2 End-to-end Forecasting Challenge, with 63.82\nmAPf. We surpass forecasting results by +17.1 points over last year's winner\nand by +13.3 points over this year's runner-up. This remarkable performance in\nforecasting can be explained by our modular paradigm, which integrates\nfinetuning strategies and significantly outperforms the end-to-end-trained\ncounterparts.\n", "link": "http://arxiv.org/abs/2406.08113v2", "date": "2024-09-10", "relevancy": 2.0021, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5286}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Valeo4Cast%3A%20A%20Modular%20Approach%20to%20End-to-End%20Forecasting&body=Title%3A%20Valeo4Cast%3A%20A%20Modular%20Approach%20to%20End-to-End%20Forecasting%0AAuthor%3A%20Yihong%20Xu%20and%20%C3%89loi%20Zablocki%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Mickael%20Chen%20and%20Florent%20Bartoccioni%20and%20Nermin%20Samet%20and%20Oriane%20Sim%C3%A9oni%20and%20Spyros%20Gidaris%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Eduardo%20Valle%20and%20Renaud%20Marlet%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Motion%20forecasting%20is%20crucial%20in%20autonomous%20driving%20systems%20to%20anticipate%20the%0Afuture%20trajectories%20of%20surrounding%20agents%20such%20as%20pedestrians%2C%20vehicles%2C%20and%0Atraffic%20signals.%20In%20end-to-end%20forecasting%2C%20the%20model%20must%20jointly%20detect%20and%0Atrack%20from%20sensor%20data%20%28cameras%20or%20LiDARs%29%20the%20past%20trajectories%20of%20the%0Adifferent%20elements%20of%20the%20scene%20and%20predict%20their%20future%20locations.%20We%20depart%0Afrom%20the%20current%20trend%20of%20tackling%20this%20task%20via%20end-to-end%20training%20from%0Aperception%20to%20forecasting%2C%20and%20instead%20use%20a%20modular%20approach.%20We%20individually%0Abuild%20and%20train%20detection%2C%20tracking%20and%20forecasting%20modules.%20We%20then%20only%20use%0Aconsecutive%20finetuning%20steps%20to%20integrate%20the%20modules%20better%20and%20alleviate%0Acompounding%20errors.%20We%20conduct%20an%20in-depth%20study%20on%20the%20finetuning%20strategies%0Aand%20it%20reveals%20that%20our%20simple%20yet%20effective%20approach%20significantly%20improves%0Aperformance%20on%20the%20end-to-end%20forecasting%20benchmark.%20Consequently%2C%20our%20solution%0Aranks%20first%20in%20the%20Argoverse%202%20End-to-end%20Forecasting%20Challenge%2C%20with%2063.82%0AmAPf.%20We%20surpass%20forecasting%20results%20by%20%2B17.1%20points%20over%20last%20year%27s%20winner%0Aand%20by%20%2B13.3%20points%20over%20this%20year%27s%20runner-up.%20This%20remarkable%20performance%20in%0Aforecasting%20can%20be%20explained%20by%20our%20modular%20paradigm%2C%20which%20integrates%0Afinetuning%20strategies%20and%20significantly%20outperforms%20the%20end-to-end-trained%0Acounterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValeo4Cast%253A%2520A%2520Modular%2520Approach%2520to%2520End-to-End%2520Forecasting%26entry.906535625%3DYihong%2520Xu%2520and%2520%25C3%2589loi%2520Zablocki%2520and%2520Alexandre%2520Boulch%2520and%2520Gilles%2520Puy%2520and%2520Mickael%2520Chen%2520and%2520Florent%2520Bartoccioni%2520and%2520Nermin%2520Samet%2520and%2520Oriane%2520Sim%25C3%25A9oni%2520and%2520Spyros%2520Gidaris%2520and%2520Tuan-Hung%2520Vu%2520and%2520Andrei%2520Bursuc%2520and%2520Eduardo%2520Valle%2520and%2520Renaud%2520Marlet%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Motion%2520forecasting%2520is%2520crucial%2520in%2520autonomous%2520driving%2520systems%2520to%2520anticipate%2520the%250Afuture%2520trajectories%2520of%2520surrounding%2520agents%2520such%2520as%2520pedestrians%252C%2520vehicles%252C%2520and%250Atraffic%2520signals.%2520In%2520end-to-end%2520forecasting%252C%2520the%2520model%2520must%2520jointly%2520detect%2520and%250Atrack%2520from%2520sensor%2520data%2520%2528cameras%2520or%2520LiDARs%2529%2520the%2520past%2520trajectories%2520of%2520the%250Adifferent%2520elements%2520of%2520the%2520scene%2520and%2520predict%2520their%2520future%2520locations.%2520We%2520depart%250Afrom%2520the%2520current%2520trend%2520of%2520tackling%2520this%2520task%2520via%2520end-to-end%2520training%2520from%250Aperception%2520to%2520forecasting%252C%2520and%2520instead%2520use%2520a%2520modular%2520approach.%2520We%2520individually%250Abuild%2520and%2520train%2520detection%252C%2520tracking%2520and%2520forecasting%2520modules.%2520We%2520then%2520only%2520use%250Aconsecutive%2520finetuning%2520steps%2520to%2520integrate%2520the%2520modules%2520better%2520and%2520alleviate%250Acompounding%2520errors.%2520We%2520conduct%2520an%2520in-depth%2520study%2520on%2520the%2520finetuning%2520strategies%250Aand%2520it%2520reveals%2520that%2520our%2520simple%2520yet%2520effective%2520approach%2520significantly%2520improves%250Aperformance%2520on%2520the%2520end-to-end%2520forecasting%2520benchmark.%2520Consequently%252C%2520our%2520solution%250Aranks%2520first%2520in%2520the%2520Argoverse%25202%2520End-to-end%2520Forecasting%2520Challenge%252C%2520with%252063.82%250AmAPf.%2520We%2520surpass%2520forecasting%2520results%2520by%2520%252B17.1%2520points%2520over%2520last%2520year%2527s%2520winner%250Aand%2520by%2520%252B13.3%2520points%2520over%2520this%2520year%2527s%2520runner-up.%2520This%2520remarkable%2520performance%2520in%250Aforecasting%2520can%2520be%2520explained%2520by%2520our%2520modular%2520paradigm%252C%2520which%2520integrates%250Afinetuning%2520strategies%2520and%2520significantly%2520outperforms%2520the%2520end-to-end-trained%250Acounterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Valeo4Cast%3A%20A%20Modular%20Approach%20to%20End-to-End%20Forecasting&entry.906535625=Yihong%20Xu%20and%20%C3%89loi%20Zablocki%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Mickael%20Chen%20and%20Florent%20Bartoccioni%20and%20Nermin%20Samet%20and%20Oriane%20Sim%C3%A9oni%20and%20Spyros%20Gidaris%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Eduardo%20Valle%20and%20Renaud%20Marlet%20and%20Matthieu%20Cord&entry.1292438233=%20%20Motion%20forecasting%20is%20crucial%20in%20autonomous%20driving%20systems%20to%20anticipate%20the%0Afuture%20trajectories%20of%20surrounding%20agents%20such%20as%20pedestrians%2C%20vehicles%2C%20and%0Atraffic%20signals.%20In%20end-to-end%20forecasting%2C%20the%20model%20must%20jointly%20detect%20and%0Atrack%20from%20sensor%20data%20%28cameras%20or%20LiDARs%29%20the%20past%20trajectories%20of%20the%0Adifferent%20elements%20of%20the%20scene%20and%20predict%20their%20future%20locations.%20We%20depart%0Afrom%20the%20current%20trend%20of%20tackling%20this%20task%20via%20end-to-end%20training%20from%0Aperception%20to%20forecasting%2C%20and%20instead%20use%20a%20modular%20approach.%20We%20individually%0Abuild%20and%20train%20detection%2C%20tracking%20and%20forecasting%20modules.%20We%20then%20only%20use%0Aconsecutive%20finetuning%20steps%20to%20integrate%20the%20modules%20better%20and%20alleviate%0Acompounding%20errors.%20We%20conduct%20an%20in-depth%20study%20on%20the%20finetuning%20strategies%0Aand%20it%20reveals%20that%20our%20simple%20yet%20effective%20approach%20significantly%20improves%0Aperformance%20on%20the%20end-to-end%20forecasting%20benchmark.%20Consequently%2C%20our%20solution%0Aranks%20first%20in%20the%20Argoverse%202%20End-to-end%20Forecasting%20Challenge%2C%20with%2063.82%0AmAPf.%20We%20surpass%20forecasting%20results%20by%20%2B17.1%20points%20over%20last%20year%27s%20winner%0Aand%20by%20%2B13.3%20points%20over%20this%20year%27s%20runner-up.%20This%20remarkable%20performance%20in%0Aforecasting%20can%20be%20explained%20by%20our%20modular%20paradigm%2C%20which%20integrates%0Afinetuning%20strategies%20and%20significantly%20outperforms%20the%20end-to-end-trained%0Acounterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08113v2&entry.124074799=Read"},
{"title": "Some Results on Neural Network Stability, Consistency, and Convergence:\n  Insights into Non-IID Data, High-Dimensional Settings, and Physics-Informed\n  Neural Networks", "author": "Ronald Katende and Henry Kasumba and Godwin Kakuba and John M. Mango", "abstract": "  This paper addresses critical challenges in machine learning, particularly\nthe stability, consistency, and convergence of neural networks under non-IID\ndata, distribution shifts, and high-dimensional settings. We provide new\ntheoretical results on uniform stability for neural networks with dynamic\nlearning rates in non-convex settings. Further, we establish consistency bounds\nfor federated learning models in non-Euclidean spaces, accounting for\ndistribution shifts and curvature effects. For Physics-Informed Neural Networks\n(PINNs), we derive stability, consistency, and convergence guarantees for\nsolving Partial Differential Equations (PDEs) in noisy environments. These\nresults fill significant gaps in understanding model behavior in complex,\nnon-ideal conditions, paving the way for more robust and reliable machine\nlearning applications.\n", "link": "http://arxiv.org/abs/2409.05030v2", "date": "2024-09-10", "relevancy": 1.996, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5059}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5041}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Some%20Results%20on%20Neural%20Network%20Stability%2C%20Consistency%2C%20and%20Convergence%3A%0A%20%20Insights%20into%20Non-IID%20Data%2C%20High-Dimensional%20Settings%2C%20and%20Physics-Informed%0A%20%20Neural%20Networks&body=Title%3A%20Some%20Results%20on%20Neural%20Network%20Stability%2C%20Consistency%2C%20and%20Convergence%3A%0A%20%20Insights%20into%20Non-IID%20Data%2C%20High-Dimensional%20Settings%2C%20and%20Physics-Informed%0A%20%20Neural%20Networks%0AAuthor%3A%20Ronald%20Katende%20and%20Henry%20Kasumba%20and%20Godwin%20Kakuba%20and%20John%20M.%20Mango%0AAbstract%3A%20%20%20This%20paper%20addresses%20critical%20challenges%20in%20machine%20learning%2C%20particularly%0Athe%20stability%2C%20consistency%2C%20and%20convergence%20of%20neural%20networks%20under%20non-IID%0Adata%2C%20distribution%20shifts%2C%20and%20high-dimensional%20settings.%20We%20provide%20new%0Atheoretical%20results%20on%20uniform%20stability%20for%20neural%20networks%20with%20dynamic%0Alearning%20rates%20in%20non-convex%20settings.%20Further%2C%20we%20establish%20consistency%20bounds%0Afor%20federated%20learning%20models%20in%20non-Euclidean%20spaces%2C%20accounting%20for%0Adistribution%20shifts%20and%20curvature%20effects.%20For%20Physics-Informed%20Neural%20Networks%0A%28PINNs%29%2C%20we%20derive%20stability%2C%20consistency%2C%20and%20convergence%20guarantees%20for%0Asolving%20Partial%20Differential%20Equations%20%28PDEs%29%20in%20noisy%20environments.%20These%0Aresults%20fill%20significant%20gaps%20in%20understanding%20model%20behavior%20in%20complex%2C%0Anon-ideal%20conditions%2C%20paving%20the%20way%20for%20more%20robust%20and%20reliable%20machine%0Alearning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05030v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSome%2520Results%2520on%2520Neural%2520Network%2520Stability%252C%2520Consistency%252C%2520and%2520Convergence%253A%250A%2520%2520Insights%2520into%2520Non-IID%2520Data%252C%2520High-Dimensional%2520Settings%252C%2520and%2520Physics-Informed%250A%2520%2520Neural%2520Networks%26entry.906535625%3DRonald%2520Katende%2520and%2520Henry%2520Kasumba%2520and%2520Godwin%2520Kakuba%2520and%2520John%2520M.%2520Mango%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520critical%2520challenges%2520in%2520machine%2520learning%252C%2520particularly%250Athe%2520stability%252C%2520consistency%252C%2520and%2520convergence%2520of%2520neural%2520networks%2520under%2520non-IID%250Adata%252C%2520distribution%2520shifts%252C%2520and%2520high-dimensional%2520settings.%2520We%2520provide%2520new%250Atheoretical%2520results%2520on%2520uniform%2520stability%2520for%2520neural%2520networks%2520with%2520dynamic%250Alearning%2520rates%2520in%2520non-convex%2520settings.%2520Further%252C%2520we%2520establish%2520consistency%2520bounds%250Afor%2520federated%2520learning%2520models%2520in%2520non-Euclidean%2520spaces%252C%2520accounting%2520for%250Adistribution%2520shifts%2520and%2520curvature%2520effects.%2520For%2520Physics-Informed%2520Neural%2520Networks%250A%2528PINNs%2529%252C%2520we%2520derive%2520stability%252C%2520consistency%252C%2520and%2520convergence%2520guarantees%2520for%250Asolving%2520Partial%2520Differential%2520Equations%2520%2528PDEs%2529%2520in%2520noisy%2520environments.%2520These%250Aresults%2520fill%2520significant%2520gaps%2520in%2520understanding%2520model%2520behavior%2520in%2520complex%252C%250Anon-ideal%2520conditions%252C%2520paving%2520the%2520way%2520for%2520more%2520robust%2520and%2520reliable%2520machine%250Alearning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05030v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Some%20Results%20on%20Neural%20Network%20Stability%2C%20Consistency%2C%20and%20Convergence%3A%0A%20%20Insights%20into%20Non-IID%20Data%2C%20High-Dimensional%20Settings%2C%20and%20Physics-Informed%0A%20%20Neural%20Networks&entry.906535625=Ronald%20Katende%20and%20Henry%20Kasumba%20and%20Godwin%20Kakuba%20and%20John%20M.%20Mango&entry.1292438233=%20%20This%20paper%20addresses%20critical%20challenges%20in%20machine%20learning%2C%20particularly%0Athe%20stability%2C%20consistency%2C%20and%20convergence%20of%20neural%20networks%20under%20non-IID%0Adata%2C%20distribution%20shifts%2C%20and%20high-dimensional%20settings.%20We%20provide%20new%0Atheoretical%20results%20on%20uniform%20stability%20for%20neural%20networks%20with%20dynamic%0Alearning%20rates%20in%20non-convex%20settings.%20Further%2C%20we%20establish%20consistency%20bounds%0Afor%20federated%20learning%20models%20in%20non-Euclidean%20spaces%2C%20accounting%20for%0Adistribution%20shifts%20and%20curvature%20effects.%20For%20Physics-Informed%20Neural%20Networks%0A%28PINNs%29%2C%20we%20derive%20stability%2C%20consistency%2C%20and%20convergence%20guarantees%20for%0Asolving%20Partial%20Differential%20Equations%20%28PDEs%29%20in%20noisy%20environments.%20These%0Aresults%20fill%20significant%20gaps%20in%20understanding%20model%20behavior%20in%20complex%2C%0Anon-ideal%20conditions%2C%20paving%20the%20way%20for%20more%20robust%20and%20reliable%20machine%0Alearning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05030v2&entry.124074799=Read"},
{"title": "Cooptimizing Safety and Performance with a Control-Constrained\n  Formulation", "author": "Hao Wang and Adityaya Dhande and Somil Bansal", "abstract": "  Autonomous systems have witnessed a rapid increase in their capabilities, but\nit remains a challenge for them to perform tasks both effectively and safely.\nThe fact that performance and safety can sometimes be competing objectives\nrenders the cooptimization between them difficult. One school of thought is to\ntreat this cooptimization as a constrained optimal control problem with a\nperformance-oriented objective function and safety as a constraint. However,\nsolving this constrained optimal control problem for general nonlinear systems\nremains challenging. In this work, we use the general framework of constrained\noptimal control, but given the safety state constraint, we convert it into an\nequivalent control constraint, resulting in a state and time-dependent\ncontrol-constrained optimal control problem. This equivalent optimal control\nproblem can readily be solved using the dynamic programming principle. We show\nthe corresponding value function is a viscosity solution of a certain\nHamilton-Jacobi-Bellman Partial Differential Equation (HJB-PDE). Furthermore,\nwe demonstrate the effectiveness of our method with a two-dimensional case\nstudy, and the experiment shows that the controller synthesized using our\nmethod consistently outperforms the baselines, both in safety and performance.\n", "link": "http://arxiv.org/abs/2409.06696v1", "date": "2024-09-10", "relevancy": 1.992, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5439}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cooptimizing%20Safety%20and%20Performance%20with%20a%20Control-Constrained%0A%20%20Formulation&body=Title%3A%20Cooptimizing%20Safety%20and%20Performance%20with%20a%20Control-Constrained%0A%20%20Formulation%0AAuthor%3A%20Hao%20Wang%20and%20Adityaya%20Dhande%20and%20Somil%20Bansal%0AAbstract%3A%20%20%20Autonomous%20systems%20have%20witnessed%20a%20rapid%20increase%20in%20their%20capabilities%2C%20but%0Ait%20remains%20a%20challenge%20for%20them%20to%20perform%20tasks%20both%20effectively%20and%20safely.%0AThe%20fact%20that%20performance%20and%20safety%20can%20sometimes%20be%20competing%20objectives%0Arenders%20the%20cooptimization%20between%20them%20difficult.%20One%20school%20of%20thought%20is%20to%0Atreat%20this%20cooptimization%20as%20a%20constrained%20optimal%20control%20problem%20with%20a%0Aperformance-oriented%20objective%20function%20and%20safety%20as%20a%20constraint.%20However%2C%0Asolving%20this%20constrained%20optimal%20control%20problem%20for%20general%20nonlinear%20systems%0Aremains%20challenging.%20In%20this%20work%2C%20we%20use%20the%20general%20framework%20of%20constrained%0Aoptimal%20control%2C%20but%20given%20the%20safety%20state%20constraint%2C%20we%20convert%20it%20into%20an%0Aequivalent%20control%20constraint%2C%20resulting%20in%20a%20state%20and%20time-dependent%0Acontrol-constrained%20optimal%20control%20problem.%20This%20equivalent%20optimal%20control%0Aproblem%20can%20readily%20be%20solved%20using%20the%20dynamic%20programming%20principle.%20We%20show%0Athe%20corresponding%20value%20function%20is%20a%20viscosity%20solution%20of%20a%20certain%0AHamilton-Jacobi-Bellman%20Partial%20Differential%20Equation%20%28HJB-PDE%29.%20Furthermore%2C%0Awe%20demonstrate%20the%20effectiveness%20of%20our%20method%20with%20a%20two-dimensional%20case%0Astudy%2C%20and%20the%20experiment%20shows%20that%20the%20controller%20synthesized%20using%20our%0Amethod%20consistently%20outperforms%20the%20baselines%2C%20both%20in%20safety%20and%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCooptimizing%2520Safety%2520and%2520Performance%2520with%2520a%2520Control-Constrained%250A%2520%2520Formulation%26entry.906535625%3DHao%2520Wang%2520and%2520Adityaya%2520Dhande%2520and%2520Somil%2520Bansal%26entry.1292438233%3D%2520%2520Autonomous%2520systems%2520have%2520witnessed%2520a%2520rapid%2520increase%2520in%2520their%2520capabilities%252C%2520but%250Ait%2520remains%2520a%2520challenge%2520for%2520them%2520to%2520perform%2520tasks%2520both%2520effectively%2520and%2520safely.%250AThe%2520fact%2520that%2520performance%2520and%2520safety%2520can%2520sometimes%2520be%2520competing%2520objectives%250Arenders%2520the%2520cooptimization%2520between%2520them%2520difficult.%2520One%2520school%2520of%2520thought%2520is%2520to%250Atreat%2520this%2520cooptimization%2520as%2520a%2520constrained%2520optimal%2520control%2520problem%2520with%2520a%250Aperformance-oriented%2520objective%2520function%2520and%2520safety%2520as%2520a%2520constraint.%2520However%252C%250Asolving%2520this%2520constrained%2520optimal%2520control%2520problem%2520for%2520general%2520nonlinear%2520systems%250Aremains%2520challenging.%2520In%2520this%2520work%252C%2520we%2520use%2520the%2520general%2520framework%2520of%2520constrained%250Aoptimal%2520control%252C%2520but%2520given%2520the%2520safety%2520state%2520constraint%252C%2520we%2520convert%2520it%2520into%2520an%250Aequivalent%2520control%2520constraint%252C%2520resulting%2520in%2520a%2520state%2520and%2520time-dependent%250Acontrol-constrained%2520optimal%2520control%2520problem.%2520This%2520equivalent%2520optimal%2520control%250Aproblem%2520can%2520readily%2520be%2520solved%2520using%2520the%2520dynamic%2520programming%2520principle.%2520We%2520show%250Athe%2520corresponding%2520value%2520function%2520is%2520a%2520viscosity%2520solution%2520of%2520a%2520certain%250AHamilton-Jacobi-Bellman%2520Partial%2520Differential%2520Equation%2520%2528HJB-PDE%2529.%2520Furthermore%252C%250Awe%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520with%2520a%2520two-dimensional%2520case%250Astudy%252C%2520and%2520the%2520experiment%2520shows%2520that%2520the%2520controller%2520synthesized%2520using%2520our%250Amethod%2520consistently%2520outperforms%2520the%2520baselines%252C%2520both%2520in%2520safety%2520and%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cooptimizing%20Safety%20and%20Performance%20with%20a%20Control-Constrained%0A%20%20Formulation&entry.906535625=Hao%20Wang%20and%20Adityaya%20Dhande%20and%20Somil%20Bansal&entry.1292438233=%20%20Autonomous%20systems%20have%20witnessed%20a%20rapid%20increase%20in%20their%20capabilities%2C%20but%0Ait%20remains%20a%20challenge%20for%20them%20to%20perform%20tasks%20both%20effectively%20and%20safely.%0AThe%20fact%20that%20performance%20and%20safety%20can%20sometimes%20be%20competing%20objectives%0Arenders%20the%20cooptimization%20between%20them%20difficult.%20One%20school%20of%20thought%20is%20to%0Atreat%20this%20cooptimization%20as%20a%20constrained%20optimal%20control%20problem%20with%20a%0Aperformance-oriented%20objective%20function%20and%20safety%20as%20a%20constraint.%20However%2C%0Asolving%20this%20constrained%20optimal%20control%20problem%20for%20general%20nonlinear%20systems%0Aremains%20challenging.%20In%20this%20work%2C%20we%20use%20the%20general%20framework%20of%20constrained%0Aoptimal%20control%2C%20but%20given%20the%20safety%20state%20constraint%2C%20we%20convert%20it%20into%20an%0Aequivalent%20control%20constraint%2C%20resulting%20in%20a%20state%20and%20time-dependent%0Acontrol-constrained%20optimal%20control%20problem.%20This%20equivalent%20optimal%20control%0Aproblem%20can%20readily%20be%20solved%20using%20the%20dynamic%20programming%20principle.%20We%20show%0Athe%20corresponding%20value%20function%20is%20a%20viscosity%20solution%20of%20a%20certain%0AHamilton-Jacobi-Bellman%20Partial%20Differential%20Equation%20%28HJB-PDE%29.%20Furthermore%2C%0Awe%20demonstrate%20the%20effectiveness%20of%20our%20method%20with%20a%20two-dimensional%20case%0Astudy%2C%20and%20the%20experiment%20shows%20that%20the%20controller%20synthesized%20using%20our%0Amethod%20consistently%20outperforms%20the%20baselines%2C%20both%20in%20safety%20and%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06696v1&entry.124074799=Read"},
{"title": "Unlocking the Use of Raw Multispectral Earth Observation Imagery for\n  Onboard Artificial Intelligence", "author": "Gabriele Meoni and Roberto Del Prete and Federico Serva and Alix De Beussche and Olivier Colin and Nicolas Long\u00e9p\u00e9", "abstract": "  Nowadays, there is growing interest in applying Artificial Intelligence (AI)\non board Earth Observation (EO) satellites for time-critical applications, such\nas natural disaster response. However, the unavailability of raw satellite data\ncurrently hinders research on lightweight pre-processing techniques and limits\nthe exploration of end-to-end pipelines, which could offer more efficient and\naccurate extraction of insights directly from the source data. To fill this\ngap, this work presents a novel methodology to automate the creation of\ndatasets for the detection of target events (e.g., warm thermal hotspots) or\nobjects (e.g., vessels) from Sentinel-2 raw data and other multispectral EO\npushbroom raw imagery. The presented approach first processes the raw data by\napplying a pipeline consisting of spatial band registration and georeferencing\nof the raw data pixels. Then, it detects the target events by leveraging\nevent-specific state-of-the-art algorithms on the Level-1C products, which are\nmosaicked and cropped on the georeferenced correspondent raw granule area. The\ndetected events are finally re-projected back onto the corresponding raw\nimages. We apply the proposed methodology to realize THRawS (Thermal Hotspots\nin Raw Sentinel-2 data), the first dataset of Sentinel-2 raw data containing\nwarm thermal hotspots. THRawS includes 1090 samples containing wildfires,\nvolcanic eruptions, and 33,335 event-free acquisitions to enable thermal\nhotspot detection and general classification applications. This dataset and\nassociated toolkits provide the community with both an immediately useful\nresource as well as a framework and methodology acting as a template for future\nadditions. With this work, we hope to pave the way for research on\nenergy-efficient pre-processing algorithms and AI-based end-to-end processing\nsystems on board EO satellites.\n", "link": "http://arxiv.org/abs/2305.11891v2", "date": "2024-09-10", "relevancy": 1.9788, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5029}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4956}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20the%20Use%20of%20Raw%20Multispectral%20Earth%20Observation%20Imagery%20for%0A%20%20Onboard%20Artificial%20Intelligence&body=Title%3A%20Unlocking%20the%20Use%20of%20Raw%20Multispectral%20Earth%20Observation%20Imagery%20for%0A%20%20Onboard%20Artificial%20Intelligence%0AAuthor%3A%20Gabriele%20Meoni%20and%20Roberto%20Del%20Prete%20and%20Federico%20Serva%20and%20Alix%20De%20Beussche%20and%20Olivier%20Colin%20and%20Nicolas%20Long%C3%A9p%C3%A9%0AAbstract%3A%20%20%20Nowadays%2C%20there%20is%20growing%20interest%20in%20applying%20Artificial%20Intelligence%20%28AI%29%0Aon%20board%20Earth%20Observation%20%28EO%29%20satellites%20for%20time-critical%20applications%2C%20such%0Aas%20natural%20disaster%20response.%20However%2C%20the%20unavailability%20of%20raw%20satellite%20data%0Acurrently%20hinders%20research%20on%20lightweight%20pre-processing%20techniques%20and%20limits%0Athe%20exploration%20of%20end-to-end%20pipelines%2C%20which%20could%20offer%20more%20efficient%20and%0Aaccurate%20extraction%20of%20insights%20directly%20from%20the%20source%20data.%20To%20fill%20this%0Agap%2C%20this%20work%20presents%20a%20novel%20methodology%20to%20automate%20the%20creation%20of%0Adatasets%20for%20the%20detection%20of%20target%20events%20%28e.g.%2C%20warm%20thermal%20hotspots%29%20or%0Aobjects%20%28e.g.%2C%20vessels%29%20from%20Sentinel-2%20raw%20data%20and%20other%20multispectral%20EO%0Apushbroom%20raw%20imagery.%20The%20presented%20approach%20first%20processes%20the%20raw%20data%20by%0Aapplying%20a%20pipeline%20consisting%20of%20spatial%20band%20registration%20and%20georeferencing%0Aof%20the%20raw%20data%20pixels.%20Then%2C%20it%20detects%20the%20target%20events%20by%20leveraging%0Aevent-specific%20state-of-the-art%20algorithms%20on%20the%20Level-1C%20products%2C%20which%20are%0Amosaicked%20and%20cropped%20on%20the%20georeferenced%20correspondent%20raw%20granule%20area.%20The%0Adetected%20events%20are%20finally%20re-projected%20back%20onto%20the%20corresponding%20raw%0Aimages.%20We%20apply%20the%20proposed%20methodology%20to%20realize%20THRawS%20%28Thermal%20Hotspots%0Ain%20Raw%20Sentinel-2%20data%29%2C%20the%20first%20dataset%20of%20Sentinel-2%20raw%20data%20containing%0Awarm%20thermal%20hotspots.%20THRawS%20includes%201090%20samples%20containing%20wildfires%2C%0Avolcanic%20eruptions%2C%20and%2033%2C335%20event-free%20acquisitions%20to%20enable%20thermal%0Ahotspot%20detection%20and%20general%20classification%20applications.%20This%20dataset%20and%0Aassociated%20toolkits%20provide%20the%20community%20with%20both%20an%20immediately%20useful%0Aresource%20as%20well%20as%20a%20framework%20and%20methodology%20acting%20as%20a%20template%20for%20future%0Aadditions.%20With%20this%20work%2C%20we%20hope%20to%20pave%20the%20way%20for%20research%20on%0Aenergy-efficient%20pre-processing%20algorithms%20and%20AI-based%20end-to-end%20processing%0Asystems%20on%20board%20EO%20satellites.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.11891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520the%2520Use%2520of%2520Raw%2520Multispectral%2520Earth%2520Observation%2520Imagery%2520for%250A%2520%2520Onboard%2520Artificial%2520Intelligence%26entry.906535625%3DGabriele%2520Meoni%2520and%2520Roberto%2520Del%2520Prete%2520and%2520Federico%2520Serva%2520and%2520Alix%2520De%2520Beussche%2520and%2520Olivier%2520Colin%2520and%2520Nicolas%2520Long%25C3%25A9p%25C3%25A9%26entry.1292438233%3D%2520%2520Nowadays%252C%2520there%2520is%2520growing%2520interest%2520in%2520applying%2520Artificial%2520Intelligence%2520%2528AI%2529%250Aon%2520board%2520Earth%2520Observation%2520%2528EO%2529%2520satellites%2520for%2520time-critical%2520applications%252C%2520such%250Aas%2520natural%2520disaster%2520response.%2520However%252C%2520the%2520unavailability%2520of%2520raw%2520satellite%2520data%250Acurrently%2520hinders%2520research%2520on%2520lightweight%2520pre-processing%2520techniques%2520and%2520limits%250Athe%2520exploration%2520of%2520end-to-end%2520pipelines%252C%2520which%2520could%2520offer%2520more%2520efficient%2520and%250Aaccurate%2520extraction%2520of%2520insights%2520directly%2520from%2520the%2520source%2520data.%2520To%2520fill%2520this%250Agap%252C%2520this%2520work%2520presents%2520a%2520novel%2520methodology%2520to%2520automate%2520the%2520creation%2520of%250Adatasets%2520for%2520the%2520detection%2520of%2520target%2520events%2520%2528e.g.%252C%2520warm%2520thermal%2520hotspots%2529%2520or%250Aobjects%2520%2528e.g.%252C%2520vessels%2529%2520from%2520Sentinel-2%2520raw%2520data%2520and%2520other%2520multispectral%2520EO%250Apushbroom%2520raw%2520imagery.%2520The%2520presented%2520approach%2520first%2520processes%2520the%2520raw%2520data%2520by%250Aapplying%2520a%2520pipeline%2520consisting%2520of%2520spatial%2520band%2520registration%2520and%2520georeferencing%250Aof%2520the%2520raw%2520data%2520pixels.%2520Then%252C%2520it%2520detects%2520the%2520target%2520events%2520by%2520leveraging%250Aevent-specific%2520state-of-the-art%2520algorithms%2520on%2520the%2520Level-1C%2520products%252C%2520which%2520are%250Amosaicked%2520and%2520cropped%2520on%2520the%2520georeferenced%2520correspondent%2520raw%2520granule%2520area.%2520The%250Adetected%2520events%2520are%2520finally%2520re-projected%2520back%2520onto%2520the%2520corresponding%2520raw%250Aimages.%2520We%2520apply%2520the%2520proposed%2520methodology%2520to%2520realize%2520THRawS%2520%2528Thermal%2520Hotspots%250Ain%2520Raw%2520Sentinel-2%2520data%2529%252C%2520the%2520first%2520dataset%2520of%2520Sentinel-2%2520raw%2520data%2520containing%250Awarm%2520thermal%2520hotspots.%2520THRawS%2520includes%25201090%2520samples%2520containing%2520wildfires%252C%250Avolcanic%2520eruptions%252C%2520and%252033%252C335%2520event-free%2520acquisitions%2520to%2520enable%2520thermal%250Ahotspot%2520detection%2520and%2520general%2520classification%2520applications.%2520This%2520dataset%2520and%250Aassociated%2520toolkits%2520provide%2520the%2520community%2520with%2520both%2520an%2520immediately%2520useful%250Aresource%2520as%2520well%2520as%2520a%2520framework%2520and%2520methodology%2520acting%2520as%2520a%2520template%2520for%2520future%250Aadditions.%2520With%2520this%2520work%252C%2520we%2520hope%2520to%2520pave%2520the%2520way%2520for%2520research%2520on%250Aenergy-efficient%2520pre-processing%2520algorithms%2520and%2520AI-based%2520end-to-end%2520processing%250Asystems%2520on%2520board%2520EO%2520satellites.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.11891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20the%20Use%20of%20Raw%20Multispectral%20Earth%20Observation%20Imagery%20for%0A%20%20Onboard%20Artificial%20Intelligence&entry.906535625=Gabriele%20Meoni%20and%20Roberto%20Del%20Prete%20and%20Federico%20Serva%20and%20Alix%20De%20Beussche%20and%20Olivier%20Colin%20and%20Nicolas%20Long%C3%A9p%C3%A9&entry.1292438233=%20%20Nowadays%2C%20there%20is%20growing%20interest%20in%20applying%20Artificial%20Intelligence%20%28AI%29%0Aon%20board%20Earth%20Observation%20%28EO%29%20satellites%20for%20time-critical%20applications%2C%20such%0Aas%20natural%20disaster%20response.%20However%2C%20the%20unavailability%20of%20raw%20satellite%20data%0Acurrently%20hinders%20research%20on%20lightweight%20pre-processing%20techniques%20and%20limits%0Athe%20exploration%20of%20end-to-end%20pipelines%2C%20which%20could%20offer%20more%20efficient%20and%0Aaccurate%20extraction%20of%20insights%20directly%20from%20the%20source%20data.%20To%20fill%20this%0Agap%2C%20this%20work%20presents%20a%20novel%20methodology%20to%20automate%20the%20creation%20of%0Adatasets%20for%20the%20detection%20of%20target%20events%20%28e.g.%2C%20warm%20thermal%20hotspots%29%20or%0Aobjects%20%28e.g.%2C%20vessels%29%20from%20Sentinel-2%20raw%20data%20and%20other%20multispectral%20EO%0Apushbroom%20raw%20imagery.%20The%20presented%20approach%20first%20processes%20the%20raw%20data%20by%0Aapplying%20a%20pipeline%20consisting%20of%20spatial%20band%20registration%20and%20georeferencing%0Aof%20the%20raw%20data%20pixels.%20Then%2C%20it%20detects%20the%20target%20events%20by%20leveraging%0Aevent-specific%20state-of-the-art%20algorithms%20on%20the%20Level-1C%20products%2C%20which%20are%0Amosaicked%20and%20cropped%20on%20the%20georeferenced%20correspondent%20raw%20granule%20area.%20The%0Adetected%20events%20are%20finally%20re-projected%20back%20onto%20the%20corresponding%20raw%0Aimages.%20We%20apply%20the%20proposed%20methodology%20to%20realize%20THRawS%20%28Thermal%20Hotspots%0Ain%20Raw%20Sentinel-2%20data%29%2C%20the%20first%20dataset%20of%20Sentinel-2%20raw%20data%20containing%0Awarm%20thermal%20hotspots.%20THRawS%20includes%201090%20samples%20containing%20wildfires%2C%0Avolcanic%20eruptions%2C%20and%2033%2C335%20event-free%20acquisitions%20to%20enable%20thermal%0Ahotspot%20detection%20and%20general%20classification%20applications.%20This%20dataset%20and%0Aassociated%20toolkits%20provide%20the%20community%20with%20both%20an%20immediately%20useful%0Aresource%20as%20well%20as%20a%20framework%20and%20methodology%20acting%20as%20a%20template%20for%20future%0Aadditions.%20With%20this%20work%2C%20we%20hope%20to%20pave%20the%20way%20for%20research%20on%0Aenergy-efficient%20pre-processing%20algorithms%20and%20AI-based%20end-to-end%20processing%0Asystems%20on%20board%20EO%20satellites.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.11891v2&entry.124074799=Read"},
{"title": "Deeper-PINNs: Element-wise Multiplication Based Physics-informed Neural\n  Networks", "author": "Feilong Jiang and Xiaonan Hou and Min Xia", "abstract": "  As a promising framework for resolving partial differential equations (PDEs),\nphysics-informed neural networks (PINNs) have received widespread attention\nfrom industrial and scientific fields. However, lack of expressive ability and\ninitialization pathology issues are found to prevent the application of PINNs\nin complex PDEs. In this work, we propose Deeper Physics-Informed Neural\nNetwork (Deeper-PINN) to resolve these issues. The element-wise multiplication\noperation is adopted to transform features into high-dimensional, non-linear\nspaces. Benefiting from element-wise multiplication operation, Deeper-PINNs can\nalleviate the initialization pathologies of PINNs and enhance the expressive\ncapability of PINNs. The proposed structure is verified on various benchmarks.\nThe results show that Deeper-PINNs can effectively resolve the initialization\npathology and exhibit strong expressive ability.\n", "link": "http://arxiv.org/abs/2406.04170v3", "date": "2024-09-10", "relevancy": 1.9757, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5017}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4936}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deeper-PINNs%3A%20Element-wise%20Multiplication%20Based%20Physics-informed%20Neural%0A%20%20Networks&body=Title%3A%20Deeper-PINNs%3A%20Element-wise%20Multiplication%20Based%20Physics-informed%20Neural%0A%20%20Networks%0AAuthor%3A%20Feilong%20Jiang%20and%20Xiaonan%20Hou%20and%20Min%20Xia%0AAbstract%3A%20%20%20As%20a%20promising%20framework%20for%20resolving%20partial%20differential%20equations%20%28PDEs%29%2C%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20have%20received%20widespread%20attention%0Afrom%20industrial%20and%20scientific%20fields.%20However%2C%20lack%20of%20expressive%20ability%20and%0Ainitialization%20pathology%20issues%20are%20found%20to%20prevent%20the%20application%20of%20PINNs%0Ain%20complex%20PDEs.%20In%20this%20work%2C%20we%20propose%20Deeper%20Physics-Informed%20Neural%0ANetwork%20%28Deeper-PINN%29%20to%20resolve%20these%20issues.%20The%20element-wise%20multiplication%0Aoperation%20is%20adopted%20to%20transform%20features%20into%20high-dimensional%2C%20non-linear%0Aspaces.%20Benefiting%20from%20element-wise%20multiplication%20operation%2C%20Deeper-PINNs%20can%0Aalleviate%20the%20initialization%20pathologies%20of%20PINNs%20and%20enhance%20the%20expressive%0Acapability%20of%20PINNs.%20The%20proposed%20structure%20is%20verified%20on%20various%20benchmarks.%0AThe%20results%20show%20that%20Deeper-PINNs%20can%20effectively%20resolve%20the%20initialization%0Apathology%20and%20exhibit%20strong%20expressive%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04170v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeeper-PINNs%253A%2520Element-wise%2520Multiplication%2520Based%2520Physics-informed%2520Neural%250A%2520%2520Networks%26entry.906535625%3DFeilong%2520Jiang%2520and%2520Xiaonan%2520Hou%2520and%2520Min%2520Xia%26entry.1292438233%3D%2520%2520As%2520a%2520promising%2520framework%2520for%2520resolving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%250Aphysics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520have%2520received%2520widespread%2520attention%250Afrom%2520industrial%2520and%2520scientific%2520fields.%2520However%252C%2520lack%2520of%2520expressive%2520ability%2520and%250Ainitialization%2520pathology%2520issues%2520are%2520found%2520to%2520prevent%2520the%2520application%2520of%2520PINNs%250Ain%2520complex%2520PDEs.%2520In%2520this%2520work%252C%2520we%2520propose%2520Deeper%2520Physics-Informed%2520Neural%250ANetwork%2520%2528Deeper-PINN%2529%2520to%2520resolve%2520these%2520issues.%2520The%2520element-wise%2520multiplication%250Aoperation%2520is%2520adopted%2520to%2520transform%2520features%2520into%2520high-dimensional%252C%2520non-linear%250Aspaces.%2520Benefiting%2520from%2520element-wise%2520multiplication%2520operation%252C%2520Deeper-PINNs%2520can%250Aalleviate%2520the%2520initialization%2520pathologies%2520of%2520PINNs%2520and%2520enhance%2520the%2520expressive%250Acapability%2520of%2520PINNs.%2520The%2520proposed%2520structure%2520is%2520verified%2520on%2520various%2520benchmarks.%250AThe%2520results%2520show%2520that%2520Deeper-PINNs%2520can%2520effectively%2520resolve%2520the%2520initialization%250Apathology%2520and%2520exhibit%2520strong%2520expressive%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04170v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deeper-PINNs%3A%20Element-wise%20Multiplication%20Based%20Physics-informed%20Neural%0A%20%20Networks&entry.906535625=Feilong%20Jiang%20and%20Xiaonan%20Hou%20and%20Min%20Xia&entry.1292438233=%20%20As%20a%20promising%20framework%20for%20resolving%20partial%20differential%20equations%20%28PDEs%29%2C%0Aphysics-informed%20neural%20networks%20%28PINNs%29%20have%20received%20widespread%20attention%0Afrom%20industrial%20and%20scientific%20fields.%20However%2C%20lack%20of%20expressive%20ability%20and%0Ainitialization%20pathology%20issues%20are%20found%20to%20prevent%20the%20application%20of%20PINNs%0Ain%20complex%20PDEs.%20In%20this%20work%2C%20we%20propose%20Deeper%20Physics-Informed%20Neural%0ANetwork%20%28Deeper-PINN%29%20to%20resolve%20these%20issues.%20The%20element-wise%20multiplication%0Aoperation%20is%20adopted%20to%20transform%20features%20into%20high-dimensional%2C%20non-linear%0Aspaces.%20Benefiting%20from%20element-wise%20multiplication%20operation%2C%20Deeper-PINNs%20can%0Aalleviate%20the%20initialization%20pathologies%20of%20PINNs%20and%20enhance%20the%20expressive%0Acapability%20of%20PINNs.%20The%20proposed%20structure%20is%20verified%20on%20various%20benchmarks.%0AThe%20results%20show%20that%20Deeper-PINNs%20can%20effectively%20resolve%20the%20initialization%0Apathology%20and%20exhibit%20strong%20expressive%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04170v3&entry.124074799=Read"},
{"title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for\n  Scholarly Knowledge Organization", "author": "Gollam Rabby and S\u00f6ren Auer and Jennifer D'Souza and Allard Oelen", "abstract": "  The increasing amount of published scholarly articles, exceeding 2.5 million\nyearly, raises the challenge for researchers in following scientific progress.\nIntegrating the contributions from scholarly articles into a novel type of\ncognitive knowledge graph (CKG) will be a crucial element for accessing and\norganizing scholarly knowledge, surpassing the insights provided by titles and\nabstracts. This research focuses on effectively conveying structured scholarly\nknowledge by utilizing large language models (LLMs) to categorize scholarly\narticles and describe their contributions in a structured and comparable\nmanner. While previous studies explored language models within specific\nresearch domains, the extensive domain-independent knowledge captured by LLMs\noffers a substantial opportunity for generating structured contribution\ndescriptions as CKGs. Additionally, LLMs offer customizable pathways through\nprompt engineering or fine-tuning, thus facilitating to leveraging of smaller\nLLMs known for their efficiency, cost-effectiveness, and environmental\nconsiderations. Our methodology involves harnessing LLM knowledge, and\ncomplementing it with domain expert-verified scholarly data sourced from a CKG.\nThis strategic fusion significantly enhances LLM performance, especially in\ntasks like scholarly article categorization and predicate recommendation. Our\nmethod involves fine-tuning LLMs with CKG knowledge and additionally injecting\nknowledge from a CKG with a novel prompting technique significantly increasing\nthe accuracy of scholarly knowledge extraction. We integrated our approach in\nthe Open Research Knowledge Graph (ORKG), thus enabling precise access to\norganized scholarly knowledge, crucially benefiting domain-independent\nscholarly knowledge exchange and dissemination among policymakers, industrial\npractitioners, and the general public.\n", "link": "http://arxiv.org/abs/2409.06433v1", "date": "2024-09-10", "relevancy": 1.9728, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20and%20Prompt%20Engineering%20with%20Cognitive%20Knowledge%20Graphs%20for%0A%20%20Scholarly%20Knowledge%20Organization&body=Title%3A%20Fine-tuning%20and%20Prompt%20Engineering%20with%20Cognitive%20Knowledge%20Graphs%20for%0A%20%20Scholarly%20Knowledge%20Organization%0AAuthor%3A%20Gollam%20Rabby%20and%20S%C3%B6ren%20Auer%20and%20Jennifer%20D%27Souza%20and%20Allard%20Oelen%0AAbstract%3A%20%20%20The%20increasing%20amount%20of%20published%20scholarly%20articles%2C%20exceeding%202.5%20million%0Ayearly%2C%20raises%20the%20challenge%20for%20researchers%20in%20following%20scientific%20progress.%0AIntegrating%20the%20contributions%20from%20scholarly%20articles%20into%20a%20novel%20type%20of%0Acognitive%20knowledge%20graph%20%28CKG%29%20will%20be%20a%20crucial%20element%20for%20accessing%20and%0Aorganizing%20scholarly%20knowledge%2C%20surpassing%20the%20insights%20provided%20by%20titles%20and%0Aabstracts.%20This%20research%20focuses%20on%20effectively%20conveying%20structured%20scholarly%0Aknowledge%20by%20utilizing%20large%20language%20models%20%28LLMs%29%20to%20categorize%20scholarly%0Aarticles%20and%20describe%20their%20contributions%20in%20a%20structured%20and%20comparable%0Amanner.%20While%20previous%20studies%20explored%20language%20models%20within%20specific%0Aresearch%20domains%2C%20the%20extensive%20domain-independent%20knowledge%20captured%20by%20LLMs%0Aoffers%20a%20substantial%20opportunity%20for%20generating%20structured%20contribution%0Adescriptions%20as%20CKGs.%20Additionally%2C%20LLMs%20offer%20customizable%20pathways%20through%0Aprompt%20engineering%20or%20fine-tuning%2C%20thus%20facilitating%20to%20leveraging%20of%20smaller%0ALLMs%20known%20for%20their%20efficiency%2C%20cost-effectiveness%2C%20and%20environmental%0Aconsiderations.%20Our%20methodology%20involves%20harnessing%20LLM%20knowledge%2C%20and%0Acomplementing%20it%20with%20domain%20expert-verified%20scholarly%20data%20sourced%20from%20a%20CKG.%0AThis%20strategic%20fusion%20significantly%20enhances%20LLM%20performance%2C%20especially%20in%0Atasks%20like%20scholarly%20article%20categorization%20and%20predicate%20recommendation.%20Our%0Amethod%20involves%20fine-tuning%20LLMs%20with%20CKG%20knowledge%20and%20additionally%20injecting%0Aknowledge%20from%20a%20CKG%20with%20a%20novel%20prompting%20technique%20significantly%20increasing%0Athe%20accuracy%20of%20scholarly%20knowledge%20extraction.%20We%20integrated%20our%20approach%20in%0Athe%20Open%20Research%20Knowledge%20Graph%20%28ORKG%29%2C%20thus%20enabling%20precise%20access%20to%0Aorganized%20scholarly%20knowledge%2C%20crucially%20benefiting%20domain-independent%0Ascholarly%20knowledge%20exchange%20and%20dissemination%20among%20policymakers%2C%20industrial%0Apractitioners%2C%20and%20the%20general%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tuning%2520and%2520Prompt%2520Engineering%2520with%2520Cognitive%2520Knowledge%2520Graphs%2520for%250A%2520%2520Scholarly%2520Knowledge%2520Organization%26entry.906535625%3DGollam%2520Rabby%2520and%2520S%25C3%25B6ren%2520Auer%2520and%2520Jennifer%2520D%2527Souza%2520and%2520Allard%2520Oelen%26entry.1292438233%3D%2520%2520The%2520increasing%2520amount%2520of%2520published%2520scholarly%2520articles%252C%2520exceeding%25202.5%2520million%250Ayearly%252C%2520raises%2520the%2520challenge%2520for%2520researchers%2520in%2520following%2520scientific%2520progress.%250AIntegrating%2520the%2520contributions%2520from%2520scholarly%2520articles%2520into%2520a%2520novel%2520type%2520of%250Acognitive%2520knowledge%2520graph%2520%2528CKG%2529%2520will%2520be%2520a%2520crucial%2520element%2520for%2520accessing%2520and%250Aorganizing%2520scholarly%2520knowledge%252C%2520surpassing%2520the%2520insights%2520provided%2520by%2520titles%2520and%250Aabstracts.%2520This%2520research%2520focuses%2520on%2520effectively%2520conveying%2520structured%2520scholarly%250Aknowledge%2520by%2520utilizing%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520categorize%2520scholarly%250Aarticles%2520and%2520describe%2520their%2520contributions%2520in%2520a%2520structured%2520and%2520comparable%250Amanner.%2520While%2520previous%2520studies%2520explored%2520language%2520models%2520within%2520specific%250Aresearch%2520domains%252C%2520the%2520extensive%2520domain-independent%2520knowledge%2520captured%2520by%2520LLMs%250Aoffers%2520a%2520substantial%2520opportunity%2520for%2520generating%2520structured%2520contribution%250Adescriptions%2520as%2520CKGs.%2520Additionally%252C%2520LLMs%2520offer%2520customizable%2520pathways%2520through%250Aprompt%2520engineering%2520or%2520fine-tuning%252C%2520thus%2520facilitating%2520to%2520leveraging%2520of%2520smaller%250ALLMs%2520known%2520for%2520their%2520efficiency%252C%2520cost-effectiveness%252C%2520and%2520environmental%250Aconsiderations.%2520Our%2520methodology%2520involves%2520harnessing%2520LLM%2520knowledge%252C%2520and%250Acomplementing%2520it%2520with%2520domain%2520expert-verified%2520scholarly%2520data%2520sourced%2520from%2520a%2520CKG.%250AThis%2520strategic%2520fusion%2520significantly%2520enhances%2520LLM%2520performance%252C%2520especially%2520in%250Atasks%2520like%2520scholarly%2520article%2520categorization%2520and%2520predicate%2520recommendation.%2520Our%250Amethod%2520involves%2520fine-tuning%2520LLMs%2520with%2520CKG%2520knowledge%2520and%2520additionally%2520injecting%250Aknowledge%2520from%2520a%2520CKG%2520with%2520a%2520novel%2520prompting%2520technique%2520significantly%2520increasing%250Athe%2520accuracy%2520of%2520scholarly%2520knowledge%2520extraction.%2520We%2520integrated%2520our%2520approach%2520in%250Athe%2520Open%2520Research%2520Knowledge%2520Graph%2520%2528ORKG%2529%252C%2520thus%2520enabling%2520precise%2520access%2520to%250Aorganized%2520scholarly%2520knowledge%252C%2520crucially%2520benefiting%2520domain-independent%250Ascholarly%2520knowledge%2520exchange%2520and%2520dissemination%2520among%2520policymakers%252C%2520industrial%250Apractitioners%252C%2520and%2520the%2520general%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20and%20Prompt%20Engineering%20with%20Cognitive%20Knowledge%20Graphs%20for%0A%20%20Scholarly%20Knowledge%20Organization&entry.906535625=Gollam%20Rabby%20and%20S%C3%B6ren%20Auer%20and%20Jennifer%20D%27Souza%20and%20Allard%20Oelen&entry.1292438233=%20%20The%20increasing%20amount%20of%20published%20scholarly%20articles%2C%20exceeding%202.5%20million%0Ayearly%2C%20raises%20the%20challenge%20for%20researchers%20in%20following%20scientific%20progress.%0AIntegrating%20the%20contributions%20from%20scholarly%20articles%20into%20a%20novel%20type%20of%0Acognitive%20knowledge%20graph%20%28CKG%29%20will%20be%20a%20crucial%20element%20for%20accessing%20and%0Aorganizing%20scholarly%20knowledge%2C%20surpassing%20the%20insights%20provided%20by%20titles%20and%0Aabstracts.%20This%20research%20focuses%20on%20effectively%20conveying%20structured%20scholarly%0Aknowledge%20by%20utilizing%20large%20language%20models%20%28LLMs%29%20to%20categorize%20scholarly%0Aarticles%20and%20describe%20their%20contributions%20in%20a%20structured%20and%20comparable%0Amanner.%20While%20previous%20studies%20explored%20language%20models%20within%20specific%0Aresearch%20domains%2C%20the%20extensive%20domain-independent%20knowledge%20captured%20by%20LLMs%0Aoffers%20a%20substantial%20opportunity%20for%20generating%20structured%20contribution%0Adescriptions%20as%20CKGs.%20Additionally%2C%20LLMs%20offer%20customizable%20pathways%20through%0Aprompt%20engineering%20or%20fine-tuning%2C%20thus%20facilitating%20to%20leveraging%20of%20smaller%0ALLMs%20known%20for%20their%20efficiency%2C%20cost-effectiveness%2C%20and%20environmental%0Aconsiderations.%20Our%20methodology%20involves%20harnessing%20LLM%20knowledge%2C%20and%0Acomplementing%20it%20with%20domain%20expert-verified%20scholarly%20data%20sourced%20from%20a%20CKG.%0AThis%20strategic%20fusion%20significantly%20enhances%20LLM%20performance%2C%20especially%20in%0Atasks%20like%20scholarly%20article%20categorization%20and%20predicate%20recommendation.%20Our%0Amethod%20involves%20fine-tuning%20LLMs%20with%20CKG%20knowledge%20and%20additionally%20injecting%0Aknowledge%20from%20a%20CKG%20with%20a%20novel%20prompting%20technique%20significantly%20increasing%0Athe%20accuracy%20of%20scholarly%20knowledge%20extraction.%20We%20integrated%20our%20approach%20in%0Athe%20Open%20Research%20Knowledge%20Graph%20%28ORKG%29%2C%20thus%20enabling%20precise%20access%20to%0Aorganized%20scholarly%20knowledge%2C%20crucially%20benefiting%20domain-independent%0Ascholarly%20knowledge%20exchange%20and%20dissemination%20among%20policymakers%2C%20industrial%0Apractitioners%2C%20and%20the%20general%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06433v1&entry.124074799=Read"},
{"title": "Optimal Neural Network Approximation for High-Dimensional Continuous\n  Functions", "author": "Ayan Maiti and Michelle Michelle and Haizhao Yang", "abstract": "  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural\nnetwork with width $36d(2d + 1)$ and depth $11$, which utilizes a special\nactivation function called the elementary universal activation function, to\nachieve the super approximation property for functions in $C([a,b]^d)$. That\nis, the constructed network only requires a fixed number of neurons to\napproximate a $d$-variate continuous function on a $d$-dimensional hypercube\nwith arbitrary accuracy. Their network uses $\\mathcal{O}(d^2)$ fixed neurons.\nOne natural question to address is whether we can reduce the number of these\nneurons in such a network. By leveraging a variant of the Kolmogorov\nSuperposition Theorem, our analysis shows that there is a neural network\ngenerated by the elementary universal activation function with only $366d +365$\nfixed, intrinsic (non-repeated) neurons that attains this super approximation\nproperty. Furthermore, we present a family of continuous functions that\nrequires at least width $d$, and therefore at least $d$ intrinsic neurons, to\nachieve arbitrary accuracy in its approximation. This shows that the\nrequirement of $\\mathcal{O}(d)$ intrinsic neurons is optimal in the sense that\nit grows linearly with the input dimension $d$, unlike some approximation\nmethods where parameters may grow exponentially with $d$.\n", "link": "http://arxiv.org/abs/2409.02363v2", "date": "2024-09-10", "relevancy": 1.9588, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4971}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Neural%20Network%20Approximation%20for%20High-Dimensional%20Continuous%0A%20%20Functions&body=Title%3A%20Optimal%20Neural%20Network%20Approximation%20for%20High-Dimensional%20Continuous%0A%20%20Functions%0AAuthor%3A%20Ayan%20Maiti%20and%20Michelle%20Michelle%20and%20Haizhao%20Yang%0AAbstract%3A%20%20%20Recently%2C%20the%20authors%20of%20Shen%20Yang%20Zhang%20%28JMLR%2C%202022%29%20developed%20a%20neural%0Anetwork%20with%20width%20%2436d%282d%20%2B%201%29%24%20and%20depth%20%2411%24%2C%20which%20utilizes%20a%20special%0Aactivation%20function%20called%20the%20elementary%20universal%20activation%20function%2C%20to%0Aachieve%20the%20super%20approximation%20property%20for%20functions%20in%20%24C%28%5Ba%2Cb%5D%5Ed%29%24.%20That%0Ais%2C%20the%20constructed%20network%20only%20requires%20a%20fixed%20number%20of%20neurons%20to%0Aapproximate%20a%20%24d%24-variate%20continuous%20function%20on%20a%20%24d%24-dimensional%20hypercube%0Awith%20arbitrary%20accuracy.%20Their%20network%20uses%20%24%5Cmathcal%7BO%7D%28d%5E2%29%24%20fixed%20neurons.%0AOne%20natural%20question%20to%20address%20is%20whether%20we%20can%20reduce%20the%20number%20of%20these%0Aneurons%20in%20such%20a%20network.%20By%20leveraging%20a%20variant%20of%20the%20Kolmogorov%0ASuperposition%20Theorem%2C%20our%20analysis%20shows%20that%20there%20is%20a%20neural%20network%0Agenerated%20by%20the%20elementary%20universal%20activation%20function%20with%20only%20%24366d%20%2B365%24%0Afixed%2C%20intrinsic%20%28non-repeated%29%20neurons%20that%20attains%20this%20super%20approximation%0Aproperty.%20Furthermore%2C%20we%20present%20a%20family%20of%20continuous%20functions%20that%0Arequires%20at%20least%20width%20%24d%24%2C%20and%20therefore%20at%20least%20%24d%24%20intrinsic%20neurons%2C%20to%0Aachieve%20arbitrary%20accuracy%20in%20its%20approximation.%20This%20shows%20that%20the%0Arequirement%20of%20%24%5Cmathcal%7BO%7D%28d%29%24%20intrinsic%20neurons%20is%20optimal%20in%20the%20sense%20that%0Ait%20grows%20linearly%20with%20the%20input%20dimension%20%24d%24%2C%20unlike%20some%20approximation%0Amethods%20where%20parameters%20may%20grow%20exponentially%20with%20%24d%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02363v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Neural%2520Network%2520Approximation%2520for%2520High-Dimensional%2520Continuous%250A%2520%2520Functions%26entry.906535625%3DAyan%2520Maiti%2520and%2520Michelle%2520Michelle%2520and%2520Haizhao%2520Yang%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520authors%2520of%2520Shen%2520Yang%2520Zhang%2520%2528JMLR%252C%25202022%2529%2520developed%2520a%2520neural%250Anetwork%2520with%2520width%2520%252436d%25282d%2520%252B%25201%2529%2524%2520and%2520depth%2520%252411%2524%252C%2520which%2520utilizes%2520a%2520special%250Aactivation%2520function%2520called%2520the%2520elementary%2520universal%2520activation%2520function%252C%2520to%250Aachieve%2520the%2520super%2520approximation%2520property%2520for%2520functions%2520in%2520%2524C%2528%255Ba%252Cb%255D%255Ed%2529%2524.%2520That%250Ais%252C%2520the%2520constructed%2520network%2520only%2520requires%2520a%2520fixed%2520number%2520of%2520neurons%2520to%250Aapproximate%2520a%2520%2524d%2524-variate%2520continuous%2520function%2520on%2520a%2520%2524d%2524-dimensional%2520hypercube%250Awith%2520arbitrary%2520accuracy.%2520Their%2520network%2520uses%2520%2524%255Cmathcal%257BO%257D%2528d%255E2%2529%2524%2520fixed%2520neurons.%250AOne%2520natural%2520question%2520to%2520address%2520is%2520whether%2520we%2520can%2520reduce%2520the%2520number%2520of%2520these%250Aneurons%2520in%2520such%2520a%2520network.%2520By%2520leveraging%2520a%2520variant%2520of%2520the%2520Kolmogorov%250ASuperposition%2520Theorem%252C%2520our%2520analysis%2520shows%2520that%2520there%2520is%2520a%2520neural%2520network%250Agenerated%2520by%2520the%2520elementary%2520universal%2520activation%2520function%2520with%2520only%2520%2524366d%2520%252B365%2524%250Afixed%252C%2520intrinsic%2520%2528non-repeated%2529%2520neurons%2520that%2520attains%2520this%2520super%2520approximation%250Aproperty.%2520Furthermore%252C%2520we%2520present%2520a%2520family%2520of%2520continuous%2520functions%2520that%250Arequires%2520at%2520least%2520width%2520%2524d%2524%252C%2520and%2520therefore%2520at%2520least%2520%2524d%2524%2520intrinsic%2520neurons%252C%2520to%250Aachieve%2520arbitrary%2520accuracy%2520in%2520its%2520approximation.%2520This%2520shows%2520that%2520the%250Arequirement%2520of%2520%2524%255Cmathcal%257BO%257D%2528d%2529%2524%2520intrinsic%2520neurons%2520is%2520optimal%2520in%2520the%2520sense%2520that%250Ait%2520grows%2520linearly%2520with%2520the%2520input%2520dimension%2520%2524d%2524%252C%2520unlike%2520some%2520approximation%250Amethods%2520where%2520parameters%2520may%2520grow%2520exponentially%2520with%2520%2524d%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02363v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Neural%20Network%20Approximation%20for%20High-Dimensional%20Continuous%0A%20%20Functions&entry.906535625=Ayan%20Maiti%20and%20Michelle%20Michelle%20and%20Haizhao%20Yang&entry.1292438233=%20%20Recently%2C%20the%20authors%20of%20Shen%20Yang%20Zhang%20%28JMLR%2C%202022%29%20developed%20a%20neural%0Anetwork%20with%20width%20%2436d%282d%20%2B%201%29%24%20and%20depth%20%2411%24%2C%20which%20utilizes%20a%20special%0Aactivation%20function%20called%20the%20elementary%20universal%20activation%20function%2C%20to%0Aachieve%20the%20super%20approximation%20property%20for%20functions%20in%20%24C%28%5Ba%2Cb%5D%5Ed%29%24.%20That%0Ais%2C%20the%20constructed%20network%20only%20requires%20a%20fixed%20number%20of%20neurons%20to%0Aapproximate%20a%20%24d%24-variate%20continuous%20function%20on%20a%20%24d%24-dimensional%20hypercube%0Awith%20arbitrary%20accuracy.%20Their%20network%20uses%20%24%5Cmathcal%7BO%7D%28d%5E2%29%24%20fixed%20neurons.%0AOne%20natural%20question%20to%20address%20is%20whether%20we%20can%20reduce%20the%20number%20of%20these%0Aneurons%20in%20such%20a%20network.%20By%20leveraging%20a%20variant%20of%20the%20Kolmogorov%0ASuperposition%20Theorem%2C%20our%20analysis%20shows%20that%20there%20is%20a%20neural%20network%0Agenerated%20by%20the%20elementary%20universal%20activation%20function%20with%20only%20%24366d%20%2B365%24%0Afixed%2C%20intrinsic%20%28non-repeated%29%20neurons%20that%20attains%20this%20super%20approximation%0Aproperty.%20Furthermore%2C%20we%20present%20a%20family%20of%20continuous%20functions%20that%0Arequires%20at%20least%20width%20%24d%24%2C%20and%20therefore%20at%20least%20%24d%24%20intrinsic%20neurons%2C%20to%0Aachieve%20arbitrary%20accuracy%20in%20its%20approximation.%20This%20shows%20that%20the%0Arequirement%20of%20%24%5Cmathcal%7BO%7D%28d%29%24%20intrinsic%20neurons%20is%20optimal%20in%20the%20sense%20that%0Ait%20grows%20linearly%20with%20the%20input%20dimension%20%24d%24%2C%20unlike%20some%20approximation%0Amethods%20where%20parameters%20may%20grow%20exponentially%20with%20%24d%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02363v2&entry.124074799=Read"},
{"title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders", "author": "Wenyu Zhang and Shuo Sun and Bin Wang and Xunlong Zou and Zhuohan Liu and Yingxu He and Geyu Lin and Nancy F. Chen and Ai Ti Aw", "abstract": "  The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.\n", "link": "http://arxiv.org/abs/2409.06635v1", "date": "2024-09-10", "relevancy": 1.9558, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoWE-Audio%3A%20Multitask%20AudioLLMs%20with%20Mixture%20of%20Weak%20Encoders&body=Title%3A%20MoWE-Audio%3A%20Multitask%20AudioLLMs%20with%20Mixture%20of%20Weak%20Encoders%0AAuthor%3A%20Wenyu%20Zhang%20and%20Shuo%20Sun%20and%20Bin%20Wang%20and%20Xunlong%20Zou%20and%20Zhuohan%20Liu%20and%20Yingxu%20He%20and%20Geyu%20Lin%20and%20Nancy%20F.%20Chen%20and%20Ai%20Ti%20Aw%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aenhanced%20natural%20language%20processing%20capabilities%2C%20facilitating%20the%20development%0Aof%20AudioLLMs%20that%20process%20and%20understand%20speech%20and%20audio%20inputs%20alongside%0Atext.%20Existing%20AudioLLMs%20typically%20combine%20a%20pre-trained%20audio%20encoder%20with%20a%0Apre-trained%20LLM%2C%20which%20are%20subsequently%20finetuned%20on%20specific%20audio%20tasks.%0AHowever%2C%20the%20pre-trained%20audio%20encoder%20has%20constrained%20capacity%20to%20capture%0Afeatures%20for%20new%20tasks%20and%20datasets.%20To%20address%20this%2C%20we%20propose%20to%20incorporate%0Amixtures%20of%20%60weak%27%20encoders%20%28MoWE%29%20into%20the%20AudioLLM%20framework.%20MoWE%0Asupplements%20a%20base%20encoder%20with%20a%20pool%20of%20relatively%20light%20weight%20encoders%2C%0Aselectively%20activated%20based%20on%20the%20audio%20input%20to%20enhance%20feature%20extraction%0Awithout%20significantly%20increasing%20model%20size.%20Our%20empirical%20results%20demonstrate%0Athat%20MoWE%20effectively%20improves%20multi-task%20performance%2C%20broadening%20the%0Aapplicability%20of%20AudioLLMs%20to%20more%20diverse%20audio%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoWE-Audio%253A%2520Multitask%2520AudioLLMs%2520with%2520Mixture%2520of%2520Weak%2520Encoders%26entry.906535625%3DWenyu%2520Zhang%2520and%2520Shuo%2520Sun%2520and%2520Bin%2520Wang%2520and%2520Xunlong%2520Zou%2520and%2520Zhuohan%2520Liu%2520and%2520Yingxu%2520He%2520and%2520Geyu%2520Lin%2520and%2520Nancy%2520F.%2520Chen%2520and%2520Ai%2520Ti%2520Aw%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%250Aenhanced%2520natural%2520language%2520processing%2520capabilities%252C%2520facilitating%2520the%2520development%250Aof%2520AudioLLMs%2520that%2520process%2520and%2520understand%2520speech%2520and%2520audio%2520inputs%2520alongside%250Atext.%2520Existing%2520AudioLLMs%2520typically%2520combine%2520a%2520pre-trained%2520audio%2520encoder%2520with%2520a%250Apre-trained%2520LLM%252C%2520which%2520are%2520subsequently%2520finetuned%2520on%2520specific%2520audio%2520tasks.%250AHowever%252C%2520the%2520pre-trained%2520audio%2520encoder%2520has%2520constrained%2520capacity%2520to%2520capture%250Afeatures%2520for%2520new%2520tasks%2520and%2520datasets.%2520To%2520address%2520this%252C%2520we%2520propose%2520to%2520incorporate%250Amixtures%2520of%2520%2560weak%2527%2520encoders%2520%2528MoWE%2529%2520into%2520the%2520AudioLLM%2520framework.%2520MoWE%250Asupplements%2520a%2520base%2520encoder%2520with%2520a%2520pool%2520of%2520relatively%2520light%2520weight%2520encoders%252C%250Aselectively%2520activated%2520based%2520on%2520the%2520audio%2520input%2520to%2520enhance%2520feature%2520extraction%250Awithout%2520significantly%2520increasing%2520model%2520size.%2520Our%2520empirical%2520results%2520demonstrate%250Athat%2520MoWE%2520effectively%2520improves%2520multi-task%2520performance%252C%2520broadening%2520the%250Aapplicability%2520of%2520AudioLLMs%2520to%2520more%2520diverse%2520audio%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoWE-Audio%3A%20Multitask%20AudioLLMs%20with%20Mixture%20of%20Weak%20Encoders&entry.906535625=Wenyu%20Zhang%20and%20Shuo%20Sun%20and%20Bin%20Wang%20and%20Xunlong%20Zou%20and%20Zhuohan%20Liu%20and%20Yingxu%20He%20and%20Geyu%20Lin%20and%20Nancy%20F.%20Chen%20and%20Ai%20Ti%20Aw&entry.1292438233=%20%20The%20rapid%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20significantly%0Aenhanced%20natural%20language%20processing%20capabilities%2C%20facilitating%20the%20development%0Aof%20AudioLLMs%20that%20process%20and%20understand%20speech%20and%20audio%20inputs%20alongside%0Atext.%20Existing%20AudioLLMs%20typically%20combine%20a%20pre-trained%20audio%20encoder%20with%20a%0Apre-trained%20LLM%2C%20which%20are%20subsequently%20finetuned%20on%20specific%20audio%20tasks.%0AHowever%2C%20the%20pre-trained%20audio%20encoder%20has%20constrained%20capacity%20to%20capture%0Afeatures%20for%20new%20tasks%20and%20datasets.%20To%20address%20this%2C%20we%20propose%20to%20incorporate%0Amixtures%20of%20%60weak%27%20encoders%20%28MoWE%29%20into%20the%20AudioLLM%20framework.%20MoWE%0Asupplements%20a%20base%20encoder%20with%20a%20pool%20of%20relatively%20light%20weight%20encoders%2C%0Aselectively%20activated%20based%20on%20the%20audio%20input%20to%20enhance%20feature%20extraction%0Awithout%20significantly%20increasing%20model%20size.%20Our%20empirical%20results%20demonstrate%0Athat%20MoWE%20effectively%20improves%20multi-task%20performance%2C%20broadening%20the%0Aapplicability%20of%20AudioLLMs%20to%20more%20diverse%20audio%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06635v1&entry.124074799=Read"},
{"title": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models", "author": "Yang Cao", "abstract": "  The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA.\n", "link": "http://arxiv.org/abs/2409.00055v2", "date": "2024-09-10", "relevancy": 1.9542, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4774}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SORSA%3A%20Singular%20Values%20and%20Orthonormal%20Regularized%20Singular%20Vectors%0A%20%20Adaptation%20of%20Large%20Language%20Models&body=Title%3A%20SORSA%3A%20Singular%20Values%20and%20Orthonormal%20Regularized%20Singular%20Vectors%0A%20%20Adaptation%20of%20Large%20Language%20Models%0AAuthor%3A%20Yang%20Cao%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20large%20language%20models%20%28LLMs%29%20comes%20with%20a%0Asignificant%20increase%20in%20their%20parameter%20size%2C%20presenting%20challenges%20for%0Aadaptation%20and%20fine-tuning.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20are%0Awidely%20used%20to%20adapt%20LLMs%20for%20downstream%20tasks%20efficiently.%20In%20this%20paper%2C%20we%0Apropose%20Singular%20Values%20and%20Orthonormal%20Regularized%20Singular%20Vectors%0AAdaptation%2C%20or%20SORSA%2C%20a%20novel%20PEFT%20method.%20We%20introduce%20a%20method%20to%20analyze%20the%0Avariation%20of%20the%20parameters%20by%20performing%20singular%20value%20decomposition%20%28SVD%29%0Aand%20discuss%20and%20analyze%20SORSA%27s%20superiority%20in%20minimizing%20the%20alteration%20in%20the%0ASVD%20aspect.%20Each%20SORSA%20adapter%20consists%20of%20two%20main%20parts%3A%20trainable%20principal%0Asingular%20weights%20%24W_p%20%3D%20U_p%20%5CSigma_p%20V%5E%5Ctop_p%24%2C%20and%20frozen%20residual%20weights%0A%24W_r%20%3D%20U_r%20%5CSigma_r%20V%5E%5Ctop_r%24.%20These%20parts%20are%20initialized%20by%20performing%20SVD%20on%0Apre-trained%20weights.%20Moreover%2C%20we%20implement%20and%20analyze%20an%20orthonormal%0Aregularizer%2C%20which%20could%20effectively%20transfer%20the%20scaling%20information%20into%0A%24%5CSigma_p%24%20and%20ultimately%20allows%20the%20training%20process%20to%20be%20more%20efficient.%0ASORSA%20adapters%20could%20be%20merged%20during%20inference%2C%20thus%20eliminating%20any%20inference%0Alatency.%20After%20all%2C%20SORSA%20shows%20a%20faster%20convergence%20than%20PiSSA%20and%20LoRA%20in%20our%0Aexperiments.%20On%20the%20MATH%20benchmark%2C%20Llama%202%207B%20adapted%20using%20SORSA%20achieved%0A10.36%25%20accuracy%2C%20outperforming%20LoRA%20%285.50%25%29%2C%20Full%20FT%20%287.22%25%29%2C%20and%20PiSSA%0A%287.44%25%29.%20On%20the%20GSM-8K%20benchmark%2C%20SORSA%20achieved%2056.03%25%20accuracy%2C%20surpassing%0ALoRA%20%2842.30%25%29%2C%20Full%20FT%20%2849.05%25%29%2C%20and%20PiSSA%20%2853.07%25%29.%20We%20conclude%20that%20SORSA%0Aoffers%20a%20new%20perspective%20on%20parameter-efficient%20fine-tuning%2C%20demonstrating%0Aremarkable%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Gunale0926/SORSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSORSA%253A%2520Singular%2520Values%2520and%2520Orthonormal%2520Regularized%2520Singular%2520Vectors%250A%2520%2520Adaptation%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DYang%2520Cao%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520comes%2520with%2520a%250Asignificant%2520increase%2520in%2520their%2520parameter%2520size%252C%2520presenting%2520challenges%2520for%250Aadaptation%2520and%2520fine-tuning.%2520Parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520methods%2520are%250Awidely%2520used%2520to%2520adapt%2520LLMs%2520for%2520downstream%2520tasks%2520efficiently.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Singular%2520Values%2520and%2520Orthonormal%2520Regularized%2520Singular%2520Vectors%250AAdaptation%252C%2520or%2520SORSA%252C%2520a%2520novel%2520PEFT%2520method.%2520We%2520introduce%2520a%2520method%2520to%2520analyze%2520the%250Avariation%2520of%2520the%2520parameters%2520by%2520performing%2520singular%2520value%2520decomposition%2520%2528SVD%2529%250Aand%2520discuss%2520and%2520analyze%2520SORSA%2527s%2520superiority%2520in%2520minimizing%2520the%2520alteration%2520in%2520the%250ASVD%2520aspect.%2520Each%2520SORSA%2520adapter%2520consists%2520of%2520two%2520main%2520parts%253A%2520trainable%2520principal%250Asingular%2520weights%2520%2524W_p%2520%253D%2520U_p%2520%255CSigma_p%2520V%255E%255Ctop_p%2524%252C%2520and%2520frozen%2520residual%2520weights%250A%2524W_r%2520%253D%2520U_r%2520%255CSigma_r%2520V%255E%255Ctop_r%2524.%2520These%2520parts%2520are%2520initialized%2520by%2520performing%2520SVD%2520on%250Apre-trained%2520weights.%2520Moreover%252C%2520we%2520implement%2520and%2520analyze%2520an%2520orthonormal%250Aregularizer%252C%2520which%2520could%2520effectively%2520transfer%2520the%2520scaling%2520information%2520into%250A%2524%255CSigma_p%2524%2520and%2520ultimately%2520allows%2520the%2520training%2520process%2520to%2520be%2520more%2520efficient.%250ASORSA%2520adapters%2520could%2520be%2520merged%2520during%2520inference%252C%2520thus%2520eliminating%2520any%2520inference%250Alatency.%2520After%2520all%252C%2520SORSA%2520shows%2520a%2520faster%2520convergence%2520than%2520PiSSA%2520and%2520LoRA%2520in%2520our%250Aexperiments.%2520On%2520the%2520MATH%2520benchmark%252C%2520Llama%25202%25207B%2520adapted%2520using%2520SORSA%2520achieved%250A10.36%2525%2520accuracy%252C%2520outperforming%2520LoRA%2520%25285.50%2525%2529%252C%2520Full%2520FT%2520%25287.22%2525%2529%252C%2520and%2520PiSSA%250A%25287.44%2525%2529.%2520On%2520the%2520GSM-8K%2520benchmark%252C%2520SORSA%2520achieved%252056.03%2525%2520accuracy%252C%2520surpassing%250ALoRA%2520%252842.30%2525%2529%252C%2520Full%2520FT%2520%252849.05%2525%2529%252C%2520and%2520PiSSA%2520%252853.07%2525%2529.%2520We%2520conclude%2520that%2520SORSA%250Aoffers%2520a%2520new%2520perspective%2520on%2520parameter-efficient%2520fine-tuning%252C%2520demonstrating%250Aremarkable%2520performance.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Gunale0926/SORSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SORSA%3A%20Singular%20Values%20and%20Orthonormal%20Regularized%20Singular%20Vectors%0A%20%20Adaptation%20of%20Large%20Language%20Models&entry.906535625=Yang%20Cao&entry.1292438233=%20%20The%20rapid%20advancement%20in%20large%20language%20models%20%28LLMs%29%20comes%20with%20a%0Asignificant%20increase%20in%20their%20parameter%20size%2C%20presenting%20challenges%20for%0Aadaptation%20and%20fine-tuning.%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20methods%20are%0Awidely%20used%20to%20adapt%20LLMs%20for%20downstream%20tasks%20efficiently.%20In%20this%20paper%2C%20we%0Apropose%20Singular%20Values%20and%20Orthonormal%20Regularized%20Singular%20Vectors%0AAdaptation%2C%20or%20SORSA%2C%20a%20novel%20PEFT%20method.%20We%20introduce%20a%20method%20to%20analyze%20the%0Avariation%20of%20the%20parameters%20by%20performing%20singular%20value%20decomposition%20%28SVD%29%0Aand%20discuss%20and%20analyze%20SORSA%27s%20superiority%20in%20minimizing%20the%20alteration%20in%20the%0ASVD%20aspect.%20Each%20SORSA%20adapter%20consists%20of%20two%20main%20parts%3A%20trainable%20principal%0Asingular%20weights%20%24W_p%20%3D%20U_p%20%5CSigma_p%20V%5E%5Ctop_p%24%2C%20and%20frozen%20residual%20weights%0A%24W_r%20%3D%20U_r%20%5CSigma_r%20V%5E%5Ctop_r%24.%20These%20parts%20are%20initialized%20by%20performing%20SVD%20on%0Apre-trained%20weights.%20Moreover%2C%20we%20implement%20and%20analyze%20an%20orthonormal%0Aregularizer%2C%20which%20could%20effectively%20transfer%20the%20scaling%20information%20into%0A%24%5CSigma_p%24%20and%20ultimately%20allows%20the%20training%20process%20to%20be%20more%20efficient.%0ASORSA%20adapters%20could%20be%20merged%20during%20inference%2C%20thus%20eliminating%20any%20inference%0Alatency.%20After%20all%2C%20SORSA%20shows%20a%20faster%20convergence%20than%20PiSSA%20and%20LoRA%20in%20our%0Aexperiments.%20On%20the%20MATH%20benchmark%2C%20Llama%202%207B%20adapted%20using%20SORSA%20achieved%0A10.36%25%20accuracy%2C%20outperforming%20LoRA%20%285.50%25%29%2C%20Full%20FT%20%287.22%25%29%2C%20and%20PiSSA%0A%287.44%25%29.%20On%20the%20GSM-8K%20benchmark%2C%20SORSA%20achieved%2056.03%25%20accuracy%2C%20surpassing%0ALoRA%20%2842.30%25%29%2C%20Full%20FT%20%2849.05%25%29%2C%20and%20PiSSA%20%2853.07%25%29.%20We%20conclude%20that%20SORSA%0Aoffers%20a%20new%20perspective%20on%20parameter-efficient%20fine-tuning%2C%20demonstrating%0Aremarkable%20performance.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Gunale0926/SORSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00055v2&entry.124074799=Read"},
{"title": "Interactive 3D Segmentation for Primary Gross Tumor Volume in\n  Oropharyngeal Cancer", "author": "Mikko Saukkoriipi and Jaakko Sahlsten and Joel Jaskari and Lotta Orasmaa and Jari Kangas and Nastaran Rasouli and Roope Raisamo and Jussi Hirvonen and Helena Mehtonen and Jorma J\u00e4rnstedt and Antti M\u00e4kitie and Mohamed Naser and Clifton Fuller and Benjamin Kann and Kimmo Kaski", "abstract": "  The main treatment modality for oropharyngeal cancer (OPC) is radiotherapy,\nwhere accurate segmentation of the primary gross tumor volume (GTVp) is\nessential. However, accurate GTVp segmentation is challenging due to\nsignificant interobserver variability and the time-consuming nature of manual\nannotation, while fully automated methods can occasionally fail. An interactive\ndeep learning (DL) model offers the advantage of automatic high-performance\nsegmentation with the flexibility for user correction when necessary. In this\nstudy, we examine interactive DL for GTVp segmentation in OPC. We implement\nstate-of-the-art algorithms and propose a novel two-stage Interactive Click\nRefinement (2S-ICR) framework. Using the 2021 HEad and neCK TumOR (HECKTOR)\ndataset for development and an external dataset from The University of Texas MD\nAnderson Cancer Center for evaluation, the 2S-ICR framework achieves a Dice\nsimilarity coefficient of 0.713 $\\pm$ 0.152 without user interaction and 0.824\n$\\pm$ 0.099 after five interactions, outperforming existing methods in both\ncases.\n", "link": "http://arxiv.org/abs/2409.06605v1", "date": "2024-09-10", "relevancy": 1.9404, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4913}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4896}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%203D%20Segmentation%20for%20Primary%20Gross%20Tumor%20Volume%20in%0A%20%20Oropharyngeal%20Cancer&body=Title%3A%20Interactive%203D%20Segmentation%20for%20Primary%20Gross%20Tumor%20Volume%20in%0A%20%20Oropharyngeal%20Cancer%0AAuthor%3A%20Mikko%20Saukkoriipi%20and%20Jaakko%20Sahlsten%20and%20Joel%20Jaskari%20and%20Lotta%20Orasmaa%20and%20Jari%20Kangas%20and%20Nastaran%20Rasouli%20and%20Roope%20Raisamo%20and%20Jussi%20Hirvonen%20and%20Helena%20Mehtonen%20and%20Jorma%20J%C3%A4rnstedt%20and%20Antti%20M%C3%A4kitie%20and%20Mohamed%20Naser%20and%20Clifton%20Fuller%20and%20Benjamin%20Kann%20and%20Kimmo%20Kaski%0AAbstract%3A%20%20%20The%20main%20treatment%20modality%20for%20oropharyngeal%20cancer%20%28OPC%29%20is%20radiotherapy%2C%0Awhere%20accurate%20segmentation%20of%20the%20primary%20gross%20tumor%20volume%20%28GTVp%29%20is%0Aessential.%20However%2C%20accurate%20GTVp%20segmentation%20is%20challenging%20due%20to%0Asignificant%20interobserver%20variability%20and%20the%20time-consuming%20nature%20of%20manual%0Aannotation%2C%20while%20fully%20automated%20methods%20can%20occasionally%20fail.%20An%20interactive%0Adeep%20learning%20%28DL%29%20model%20offers%20the%20advantage%20of%20automatic%20high-performance%0Asegmentation%20with%20the%20flexibility%20for%20user%20correction%20when%20necessary.%20In%20this%0Astudy%2C%20we%20examine%20interactive%20DL%20for%20GTVp%20segmentation%20in%20OPC.%20We%20implement%0Astate-of-the-art%20algorithms%20and%20propose%20a%20novel%20two-stage%20Interactive%20Click%0ARefinement%20%282S-ICR%29%20framework.%20Using%20the%202021%20HEad%20and%20neCK%20TumOR%20%28HECKTOR%29%0Adataset%20for%20development%20and%20an%20external%20dataset%20from%20The%20University%20of%20Texas%20MD%0AAnderson%20Cancer%20Center%20for%20evaluation%2C%20the%202S-ICR%20framework%20achieves%20a%20Dice%0Asimilarity%20coefficient%20of%200.713%20%24%5Cpm%24%200.152%20without%20user%20interaction%20and%200.824%0A%24%5Cpm%24%200.099%20after%20five%20interactions%2C%20outperforming%20existing%20methods%20in%20both%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%25203D%2520Segmentation%2520for%2520Primary%2520Gross%2520Tumor%2520Volume%2520in%250A%2520%2520Oropharyngeal%2520Cancer%26entry.906535625%3DMikko%2520Saukkoriipi%2520and%2520Jaakko%2520Sahlsten%2520and%2520Joel%2520Jaskari%2520and%2520Lotta%2520Orasmaa%2520and%2520Jari%2520Kangas%2520and%2520Nastaran%2520Rasouli%2520and%2520Roope%2520Raisamo%2520and%2520Jussi%2520Hirvonen%2520and%2520Helena%2520Mehtonen%2520and%2520Jorma%2520J%25C3%25A4rnstedt%2520and%2520Antti%2520M%25C3%25A4kitie%2520and%2520Mohamed%2520Naser%2520and%2520Clifton%2520Fuller%2520and%2520Benjamin%2520Kann%2520and%2520Kimmo%2520Kaski%26entry.1292438233%3D%2520%2520The%2520main%2520treatment%2520modality%2520for%2520oropharyngeal%2520cancer%2520%2528OPC%2529%2520is%2520radiotherapy%252C%250Awhere%2520accurate%2520segmentation%2520of%2520the%2520primary%2520gross%2520tumor%2520volume%2520%2528GTVp%2529%2520is%250Aessential.%2520However%252C%2520accurate%2520GTVp%2520segmentation%2520is%2520challenging%2520due%2520to%250Asignificant%2520interobserver%2520variability%2520and%2520the%2520time-consuming%2520nature%2520of%2520manual%250Aannotation%252C%2520while%2520fully%2520automated%2520methods%2520can%2520occasionally%2520fail.%2520An%2520interactive%250Adeep%2520learning%2520%2528DL%2529%2520model%2520offers%2520the%2520advantage%2520of%2520automatic%2520high-performance%250Asegmentation%2520with%2520the%2520flexibility%2520for%2520user%2520correction%2520when%2520necessary.%2520In%2520this%250Astudy%252C%2520we%2520examine%2520interactive%2520DL%2520for%2520GTVp%2520segmentation%2520in%2520OPC.%2520We%2520implement%250Astate-of-the-art%2520algorithms%2520and%2520propose%2520a%2520novel%2520two-stage%2520Interactive%2520Click%250ARefinement%2520%25282S-ICR%2529%2520framework.%2520Using%2520the%25202021%2520HEad%2520and%2520neCK%2520TumOR%2520%2528HECKTOR%2529%250Adataset%2520for%2520development%2520and%2520an%2520external%2520dataset%2520from%2520The%2520University%2520of%2520Texas%2520MD%250AAnderson%2520Cancer%2520Center%2520for%2520evaluation%252C%2520the%25202S-ICR%2520framework%2520achieves%2520a%2520Dice%250Asimilarity%2520coefficient%2520of%25200.713%2520%2524%255Cpm%2524%25200.152%2520without%2520user%2520interaction%2520and%25200.824%250A%2524%255Cpm%2524%25200.099%2520after%2520five%2520interactions%252C%2520outperforming%2520existing%2520methods%2520in%2520both%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%203D%20Segmentation%20for%20Primary%20Gross%20Tumor%20Volume%20in%0A%20%20Oropharyngeal%20Cancer&entry.906535625=Mikko%20Saukkoriipi%20and%20Jaakko%20Sahlsten%20and%20Joel%20Jaskari%20and%20Lotta%20Orasmaa%20and%20Jari%20Kangas%20and%20Nastaran%20Rasouli%20and%20Roope%20Raisamo%20and%20Jussi%20Hirvonen%20and%20Helena%20Mehtonen%20and%20Jorma%20J%C3%A4rnstedt%20and%20Antti%20M%C3%A4kitie%20and%20Mohamed%20Naser%20and%20Clifton%20Fuller%20and%20Benjamin%20Kann%20and%20Kimmo%20Kaski&entry.1292438233=%20%20The%20main%20treatment%20modality%20for%20oropharyngeal%20cancer%20%28OPC%29%20is%20radiotherapy%2C%0Awhere%20accurate%20segmentation%20of%20the%20primary%20gross%20tumor%20volume%20%28GTVp%29%20is%0Aessential.%20However%2C%20accurate%20GTVp%20segmentation%20is%20challenging%20due%20to%0Asignificant%20interobserver%20variability%20and%20the%20time-consuming%20nature%20of%20manual%0Aannotation%2C%20while%20fully%20automated%20methods%20can%20occasionally%20fail.%20An%20interactive%0Adeep%20learning%20%28DL%29%20model%20offers%20the%20advantage%20of%20automatic%20high-performance%0Asegmentation%20with%20the%20flexibility%20for%20user%20correction%20when%20necessary.%20In%20this%0Astudy%2C%20we%20examine%20interactive%20DL%20for%20GTVp%20segmentation%20in%20OPC.%20We%20implement%0Astate-of-the-art%20algorithms%20and%20propose%20a%20novel%20two-stage%20Interactive%20Click%0ARefinement%20%282S-ICR%29%20framework.%20Using%20the%202021%20HEad%20and%20neCK%20TumOR%20%28HECKTOR%29%0Adataset%20for%20development%20and%20an%20external%20dataset%20from%20The%20University%20of%20Texas%20MD%0AAnderson%20Cancer%20Center%20for%20evaluation%2C%20the%202S-ICR%20framework%20achieves%20a%20Dice%0Asimilarity%20coefficient%20of%200.713%20%24%5Cpm%24%200.152%20without%20user%20interaction%20and%200.824%0A%24%5Cpm%24%200.099%20after%20five%20interactions%2C%20outperforming%20existing%20methods%20in%20both%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06605v1&entry.124074799=Read"},
{"title": "Multi-scale Cycle Tracking in Dynamic Planar Graphs", "author": "Farhan Rasheed and Abrar Naseer and Emma Nilsson and Talha Bin Masood and Ingrid Hotz", "abstract": "  This paper presents a nested tracking framework for analyzing cycles in 2D\nforce networks within granular materials. These materials are composed of\ninteracting particles, whose interactions are described by a force network.\nUnderstanding the cycles within these networks at various scales and their\nevolution under external loads is crucial, as they significantly contribute to\nthe mechanical and kinematic properties of the system. Our approach involves\ncomputing a cycle hierarchy by partitioning the 2D domain into segments bounded\nby cycles in the force network. We can adapt concepts from nested tracking\ngraphs originally developed for merge trees by leveraging the duality between\nthis partitioning and the cycles. We demonstrate the effectiveness of our\nmethod on two force networks derived from experiments with photoelastic disks.\n", "link": "http://arxiv.org/abs/2409.06476v1", "date": "2024-09-10", "relevancy": 1.9327, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5094}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4662}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20Cycle%20Tracking%20in%20Dynamic%20Planar%20Graphs&body=Title%3A%20Multi-scale%20Cycle%20Tracking%20in%20Dynamic%20Planar%20Graphs%0AAuthor%3A%20Farhan%20Rasheed%20and%20Abrar%20Naseer%20and%20Emma%20Nilsson%20and%20Talha%20Bin%20Masood%20and%20Ingrid%20Hotz%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20nested%20tracking%20framework%20for%20analyzing%20cycles%20in%202D%0Aforce%20networks%20within%20granular%20materials.%20These%20materials%20are%20composed%20of%0Ainteracting%20particles%2C%20whose%20interactions%20are%20described%20by%20a%20force%20network.%0AUnderstanding%20the%20cycles%20within%20these%20networks%20at%20various%20scales%20and%20their%0Aevolution%20under%20external%20loads%20is%20crucial%2C%20as%20they%20significantly%20contribute%20to%0Athe%20mechanical%20and%20kinematic%20properties%20of%20the%20system.%20Our%20approach%20involves%0Acomputing%20a%20cycle%20hierarchy%20by%20partitioning%20the%202D%20domain%20into%20segments%20bounded%0Aby%20cycles%20in%20the%20force%20network.%20We%20can%20adapt%20concepts%20from%20nested%20tracking%0Agraphs%20originally%20developed%20for%20merge%20trees%20by%20leveraging%20the%20duality%20between%0Athis%20partitioning%20and%20the%20cycles.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20on%20two%20force%20networks%20derived%20from%20experiments%20with%20photoelastic%20disks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-scale%2520Cycle%2520Tracking%2520in%2520Dynamic%2520Planar%2520Graphs%26entry.906535625%3DFarhan%2520Rasheed%2520and%2520Abrar%2520Naseer%2520and%2520Emma%2520Nilsson%2520and%2520Talha%2520Bin%2520Masood%2520and%2520Ingrid%2520Hotz%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520nested%2520tracking%2520framework%2520for%2520analyzing%2520cycles%2520in%25202D%250Aforce%2520networks%2520within%2520granular%2520materials.%2520These%2520materials%2520are%2520composed%2520of%250Ainteracting%2520particles%252C%2520whose%2520interactions%2520are%2520described%2520by%2520a%2520force%2520network.%250AUnderstanding%2520the%2520cycles%2520within%2520these%2520networks%2520at%2520various%2520scales%2520and%2520their%250Aevolution%2520under%2520external%2520loads%2520is%2520crucial%252C%2520as%2520they%2520significantly%2520contribute%2520to%250Athe%2520mechanical%2520and%2520kinematic%2520properties%2520of%2520the%2520system.%2520Our%2520approach%2520involves%250Acomputing%2520a%2520cycle%2520hierarchy%2520by%2520partitioning%2520the%25202D%2520domain%2520into%2520segments%2520bounded%250Aby%2520cycles%2520in%2520the%2520force%2520network.%2520We%2520can%2520adapt%2520concepts%2520from%2520nested%2520tracking%250Agraphs%2520originally%2520developed%2520for%2520merge%2520trees%2520by%2520leveraging%2520the%2520duality%2520between%250Athis%2520partitioning%2520and%2520the%2520cycles.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%2520on%2520two%2520force%2520networks%2520derived%2520from%2520experiments%2520with%2520photoelastic%2520disks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20Cycle%20Tracking%20in%20Dynamic%20Planar%20Graphs&entry.906535625=Farhan%20Rasheed%20and%20Abrar%20Naseer%20and%20Emma%20Nilsson%20and%20Talha%20Bin%20Masood%20and%20Ingrid%20Hotz&entry.1292438233=%20%20This%20paper%20presents%20a%20nested%20tracking%20framework%20for%20analyzing%20cycles%20in%202D%0Aforce%20networks%20within%20granular%20materials.%20These%20materials%20are%20composed%20of%0Ainteracting%20particles%2C%20whose%20interactions%20are%20described%20by%20a%20force%20network.%0AUnderstanding%20the%20cycles%20within%20these%20networks%20at%20various%20scales%20and%20their%0Aevolution%20under%20external%20loads%20is%20crucial%2C%20as%20they%20significantly%20contribute%20to%0Athe%20mechanical%20and%20kinematic%20properties%20of%20the%20system.%20Our%20approach%20involves%0Acomputing%20a%20cycle%20hierarchy%20by%20partitioning%20the%202D%20domain%20into%20segments%20bounded%0Aby%20cycles%20in%20the%20force%20network.%20We%20can%20adapt%20concepts%20from%20nested%20tracking%0Agraphs%20originally%20developed%20for%20merge%20trees%20by%20leveraging%20the%20duality%20between%0Athis%20partitioning%20and%20the%20cycles.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20on%20two%20force%20networks%20derived%20from%20experiments%20with%20photoelastic%20disks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06476v1&entry.124074799=Read"},
{"title": "Qwen2 Technical Report", "author": "An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan", "abstract": "  This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.\n", "link": "http://arxiv.org/abs/2407.10671v4", "date": "2024-09-10", "relevancy": 1.9274, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qwen2%20Technical%20Report&body=Title%3A%20Qwen2%20Technical%20Report%0AAuthor%3A%20An%20Yang%20and%20Baosong%20Yang%20and%20Binyuan%20Hui%20and%20Bo%20Zheng%20and%20Bowen%20Yu%20and%20Chang%20Zhou%20and%20Chengpeng%20Li%20and%20Chengyuan%20Li%20and%20Dayiheng%20Liu%20and%20Fei%20Huang%20and%20Guanting%20Dong%20and%20Haoran%20Wei%20and%20Huan%20Lin%20and%20Jialong%20Tang%20and%20Jialin%20Wang%20and%20Jian%20Yang%20and%20Jianhong%20Tu%20and%20Jianwei%20Zhang%20and%20Jianxin%20Ma%20and%20Jianxin%20Yang%20and%20Jin%20Xu%20and%20Jingren%20Zhou%20and%20Jinze%20Bai%20and%20Jinzheng%20He%20and%20Junyang%20Lin%20and%20Kai%20Dang%20and%20Keming%20Lu%20and%20Keqin%20Chen%20and%20Kexin%20Yang%20and%20Mei%20Li%20and%20Mingfeng%20Xue%20and%20Na%20Ni%20and%20Pei%20Zhang%20and%20Peng%20Wang%20and%20Ru%20Peng%20and%20Rui%20Men%20and%20Ruize%20Gao%20and%20Runji%20Lin%20and%20Shijie%20Wang%20and%20Shuai%20Bai%20and%20Sinan%20Tan%20and%20Tianhang%20Zhu%20and%20Tianhao%20Li%20and%20Tianyu%20Liu%20and%20Wenbin%20Ge%20and%20Xiaodong%20Deng%20and%20Xiaohuan%20Zhou%20and%20Xingzhang%20Ren%20and%20Xinyu%20Zhang%20and%20Xipin%20Wei%20and%20Xuancheng%20Ren%20and%20Xuejing%20Liu%20and%20Yang%20Fan%20and%20Yang%20Yao%20and%20Yichang%20Zhang%20and%20Yu%20Wan%20and%20Yunfei%20Chu%20and%20Yuqiong%20Liu%20and%20Zeyu%20Cui%20and%20Zhenru%20Zhang%20and%20Zhifang%20Guo%20and%20Zhihao%20Fan%0AAbstract%3A%20%20%20This%20report%20introduces%20the%20Qwen2%20series%2C%20the%20latest%20addition%20to%20our%20large%0Alanguage%20models%20and%20large%20multimodal%20models.%20We%20release%20a%20comprehensive%20suite%0Aof%20foundational%20and%20instruction-tuned%20language%20models%2C%20encompassing%20a%20parameter%0Arange%20from%200.5%20to%2072%20billion%2C%20featuring%20dense%20models%20and%20a%20Mixture-of-Experts%0Amodel.%20Qwen2%20surpasses%20most%20prior%20open-weight%20models%2C%20including%20its%20predecessor%0AQwen1.5%2C%20and%20exhibits%20competitive%20performance%20relative%20to%20proprietary%20models%0Aacross%20diverse%20benchmarks%20on%20language%20understanding%2C%20generation%2C%20multilingual%0Aproficiency%2C%20coding%2C%20mathematics%2C%20and%20reasoning.%0A%20%20The%20flagship%20model%2C%20Qwen2-72B%2C%20showcases%20remarkable%20performance%3A%2084.2%20on%0AMMLU%2C%2037.9%20on%20GPQA%2C%2064.6%20on%20HumanEval%2C%2089.5%20on%20GSM8K%2C%20and%2082.4%20on%20BBH%20as%20a%20base%0Alanguage%20model.%20The%20instruction-tuned%20variant%2C%20Qwen2-72B-Instruct%2C%20attains%209.1%0Aon%20MT-Bench%2C%2048.1%20on%20Arena-Hard%2C%20and%2035.7%20on%20LiveCodeBench.%20Moreover%2C%20Qwen2%0Ademonstrates%20robust%20multilingual%20capabilities%2C%20proficient%20in%20approximately%2030%0Alanguages%2C%20spanning%20English%2C%20Chinese%2C%20Spanish%2C%20French%2C%20German%2C%20Arabic%2C%20Russian%2C%0AKorean%2C%20Japanese%2C%20Thai%2C%20Vietnamese%2C%20and%20more%2C%20underscoring%20its%20versatility%20and%0Aglobal%20reach.%0A%20%20To%20foster%20community%20innovation%20and%20accessibility%2C%20we%20have%20made%20the%20Qwen2%0Amodel%20weights%20openly%20available%20on%20Hugging%20Face%20and%20ModelScope%2C%20and%20the%0Asupplementary%20materials%20including%20example%20code%20on%20GitHub.%20These%20platforms%20also%0Ainclude%20resources%20for%20quantization%2C%20fine-tuning%2C%20and%20deployment%2C%20facilitating%20a%0Awide%20range%20of%20applications%20and%20research%20endeavors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10671v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQwen2%2520Technical%2520Report%26entry.906535625%3DAn%2520Yang%2520and%2520Baosong%2520Yang%2520and%2520Binyuan%2520Hui%2520and%2520Bo%2520Zheng%2520and%2520Bowen%2520Yu%2520and%2520Chang%2520Zhou%2520and%2520Chengpeng%2520Li%2520and%2520Chengyuan%2520Li%2520and%2520Dayiheng%2520Liu%2520and%2520Fei%2520Huang%2520and%2520Guanting%2520Dong%2520and%2520Haoran%2520Wei%2520and%2520Huan%2520Lin%2520and%2520Jialong%2520Tang%2520and%2520Jialin%2520Wang%2520and%2520Jian%2520Yang%2520and%2520Jianhong%2520Tu%2520and%2520Jianwei%2520Zhang%2520and%2520Jianxin%2520Ma%2520and%2520Jianxin%2520Yang%2520and%2520Jin%2520Xu%2520and%2520Jingren%2520Zhou%2520and%2520Jinze%2520Bai%2520and%2520Jinzheng%2520He%2520and%2520Junyang%2520Lin%2520and%2520Kai%2520Dang%2520and%2520Keming%2520Lu%2520and%2520Keqin%2520Chen%2520and%2520Kexin%2520Yang%2520and%2520Mei%2520Li%2520and%2520Mingfeng%2520Xue%2520and%2520Na%2520Ni%2520and%2520Pei%2520Zhang%2520and%2520Peng%2520Wang%2520and%2520Ru%2520Peng%2520and%2520Rui%2520Men%2520and%2520Ruize%2520Gao%2520and%2520Runji%2520Lin%2520and%2520Shijie%2520Wang%2520and%2520Shuai%2520Bai%2520and%2520Sinan%2520Tan%2520and%2520Tianhang%2520Zhu%2520and%2520Tianhao%2520Li%2520and%2520Tianyu%2520Liu%2520and%2520Wenbin%2520Ge%2520and%2520Xiaodong%2520Deng%2520and%2520Xiaohuan%2520Zhou%2520and%2520Xingzhang%2520Ren%2520and%2520Xinyu%2520Zhang%2520and%2520Xipin%2520Wei%2520and%2520Xuancheng%2520Ren%2520and%2520Xuejing%2520Liu%2520and%2520Yang%2520Fan%2520and%2520Yang%2520Yao%2520and%2520Yichang%2520Zhang%2520and%2520Yu%2520Wan%2520and%2520Yunfei%2520Chu%2520and%2520Yuqiong%2520Liu%2520and%2520Zeyu%2520Cui%2520and%2520Zhenru%2520Zhang%2520and%2520Zhifang%2520Guo%2520and%2520Zhihao%2520Fan%26entry.1292438233%3D%2520%2520This%2520report%2520introduces%2520the%2520Qwen2%2520series%252C%2520the%2520latest%2520addition%2520to%2520our%2520large%250Alanguage%2520models%2520and%2520large%2520multimodal%2520models.%2520We%2520release%2520a%2520comprehensive%2520suite%250Aof%2520foundational%2520and%2520instruction-tuned%2520language%2520models%252C%2520encompassing%2520a%2520parameter%250Arange%2520from%25200.5%2520to%252072%2520billion%252C%2520featuring%2520dense%2520models%2520and%2520a%2520Mixture-of-Experts%250Amodel.%2520Qwen2%2520surpasses%2520most%2520prior%2520open-weight%2520models%252C%2520including%2520its%2520predecessor%250AQwen1.5%252C%2520and%2520exhibits%2520competitive%2520performance%2520relative%2520to%2520proprietary%2520models%250Aacross%2520diverse%2520benchmarks%2520on%2520language%2520understanding%252C%2520generation%252C%2520multilingual%250Aproficiency%252C%2520coding%252C%2520mathematics%252C%2520and%2520reasoning.%250A%2520%2520The%2520flagship%2520model%252C%2520Qwen2-72B%252C%2520showcases%2520remarkable%2520performance%253A%252084.2%2520on%250AMMLU%252C%252037.9%2520on%2520GPQA%252C%252064.6%2520on%2520HumanEval%252C%252089.5%2520on%2520GSM8K%252C%2520and%252082.4%2520on%2520BBH%2520as%2520a%2520base%250Alanguage%2520model.%2520The%2520instruction-tuned%2520variant%252C%2520Qwen2-72B-Instruct%252C%2520attains%25209.1%250Aon%2520MT-Bench%252C%252048.1%2520on%2520Arena-Hard%252C%2520and%252035.7%2520on%2520LiveCodeBench.%2520Moreover%252C%2520Qwen2%250Ademonstrates%2520robust%2520multilingual%2520capabilities%252C%2520proficient%2520in%2520approximately%252030%250Alanguages%252C%2520spanning%2520English%252C%2520Chinese%252C%2520Spanish%252C%2520French%252C%2520German%252C%2520Arabic%252C%2520Russian%252C%250AKorean%252C%2520Japanese%252C%2520Thai%252C%2520Vietnamese%252C%2520and%2520more%252C%2520underscoring%2520its%2520versatility%2520and%250Aglobal%2520reach.%250A%2520%2520To%2520foster%2520community%2520innovation%2520and%2520accessibility%252C%2520we%2520have%2520made%2520the%2520Qwen2%250Amodel%2520weights%2520openly%2520available%2520on%2520Hugging%2520Face%2520and%2520ModelScope%252C%2520and%2520the%250Asupplementary%2520materials%2520including%2520example%2520code%2520on%2520GitHub.%2520These%2520platforms%2520also%250Ainclude%2520resources%2520for%2520quantization%252C%2520fine-tuning%252C%2520and%2520deployment%252C%2520facilitating%2520a%250Awide%2520range%2520of%2520applications%2520and%2520research%2520endeavors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10671v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qwen2%20Technical%20Report&entry.906535625=An%20Yang%20and%20Baosong%20Yang%20and%20Binyuan%20Hui%20and%20Bo%20Zheng%20and%20Bowen%20Yu%20and%20Chang%20Zhou%20and%20Chengpeng%20Li%20and%20Chengyuan%20Li%20and%20Dayiheng%20Liu%20and%20Fei%20Huang%20and%20Guanting%20Dong%20and%20Haoran%20Wei%20and%20Huan%20Lin%20and%20Jialong%20Tang%20and%20Jialin%20Wang%20and%20Jian%20Yang%20and%20Jianhong%20Tu%20and%20Jianwei%20Zhang%20and%20Jianxin%20Ma%20and%20Jianxin%20Yang%20and%20Jin%20Xu%20and%20Jingren%20Zhou%20and%20Jinze%20Bai%20and%20Jinzheng%20He%20and%20Junyang%20Lin%20and%20Kai%20Dang%20and%20Keming%20Lu%20and%20Keqin%20Chen%20and%20Kexin%20Yang%20and%20Mei%20Li%20and%20Mingfeng%20Xue%20and%20Na%20Ni%20and%20Pei%20Zhang%20and%20Peng%20Wang%20and%20Ru%20Peng%20and%20Rui%20Men%20and%20Ruize%20Gao%20and%20Runji%20Lin%20and%20Shijie%20Wang%20and%20Shuai%20Bai%20and%20Sinan%20Tan%20and%20Tianhang%20Zhu%20and%20Tianhao%20Li%20and%20Tianyu%20Liu%20and%20Wenbin%20Ge%20and%20Xiaodong%20Deng%20and%20Xiaohuan%20Zhou%20and%20Xingzhang%20Ren%20and%20Xinyu%20Zhang%20and%20Xipin%20Wei%20and%20Xuancheng%20Ren%20and%20Xuejing%20Liu%20and%20Yang%20Fan%20and%20Yang%20Yao%20and%20Yichang%20Zhang%20and%20Yu%20Wan%20and%20Yunfei%20Chu%20and%20Yuqiong%20Liu%20and%20Zeyu%20Cui%20and%20Zhenru%20Zhang%20and%20Zhifang%20Guo%20and%20Zhihao%20Fan&entry.1292438233=%20%20This%20report%20introduces%20the%20Qwen2%20series%2C%20the%20latest%20addition%20to%20our%20large%0Alanguage%20models%20and%20large%20multimodal%20models.%20We%20release%20a%20comprehensive%20suite%0Aof%20foundational%20and%20instruction-tuned%20language%20models%2C%20encompassing%20a%20parameter%0Arange%20from%200.5%20to%2072%20billion%2C%20featuring%20dense%20models%20and%20a%20Mixture-of-Experts%0Amodel.%20Qwen2%20surpasses%20most%20prior%20open-weight%20models%2C%20including%20its%20predecessor%0AQwen1.5%2C%20and%20exhibits%20competitive%20performance%20relative%20to%20proprietary%20models%0Aacross%20diverse%20benchmarks%20on%20language%20understanding%2C%20generation%2C%20multilingual%0Aproficiency%2C%20coding%2C%20mathematics%2C%20and%20reasoning.%0A%20%20The%20flagship%20model%2C%20Qwen2-72B%2C%20showcases%20remarkable%20performance%3A%2084.2%20on%0AMMLU%2C%2037.9%20on%20GPQA%2C%2064.6%20on%20HumanEval%2C%2089.5%20on%20GSM8K%2C%20and%2082.4%20on%20BBH%20as%20a%20base%0Alanguage%20model.%20The%20instruction-tuned%20variant%2C%20Qwen2-72B-Instruct%2C%20attains%209.1%0Aon%20MT-Bench%2C%2048.1%20on%20Arena-Hard%2C%20and%2035.7%20on%20LiveCodeBench.%20Moreover%2C%20Qwen2%0Ademonstrates%20robust%20multilingual%20capabilities%2C%20proficient%20in%20approximately%2030%0Alanguages%2C%20spanning%20English%2C%20Chinese%2C%20Spanish%2C%20French%2C%20German%2C%20Arabic%2C%20Russian%2C%0AKorean%2C%20Japanese%2C%20Thai%2C%20Vietnamese%2C%20and%20more%2C%20underscoring%20its%20versatility%20and%0Aglobal%20reach.%0A%20%20To%20foster%20community%20innovation%20and%20accessibility%2C%20we%20have%20made%20the%20Qwen2%0Amodel%20weights%20openly%20available%20on%20Hugging%20Face%20and%20ModelScope%2C%20and%20the%0Asupplementary%20materials%20including%20example%20code%20on%20GitHub.%20These%20platforms%20also%0Ainclude%20resources%20for%20quantization%2C%20fine-tuning%2C%20and%20deployment%2C%20facilitating%20a%0Awide%20range%20of%20applications%20and%20research%20endeavors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10671v4&entry.124074799=Read"},
{"title": "Symmetry Breaking in Neural Network Optimization: Insights from Input\n  Dimension Expansion", "author": "Jun-Jie Zhang and Nan Cheng and Fu-Peng Li and Xiu-Cheng Wang and Jian-Nan Chen and Long-Gang Pang and Deyu Meng", "abstract": "  Understanding the mechanisms behind neural network optimization is crucial\nfor improving network design and performance. While various optimization\ntechniques have been developed, a comprehensive understanding of the underlying\nprinciples that govern these techniques remains elusive. Specifically, the role\nof symmetry breaking, a fundamental concept in physics, has not been fully\nexplored in neural network optimization. This gap in knowledge limits our\nability to design networks that are both efficient and effective. Here, we\npropose the symmetry breaking hypothesis to elucidate the significance of\nsymmetry breaking in enhancing neural network optimization. We demonstrate that\na simple input expansion can significantly improve network performance across\nvarious tasks, and we show that this improvement can be attributed to the\nunderlying symmetry breaking mechanism. We further develop a metric to quantify\nthe degree of symmetry breaking in neural networks, providing a practical\napproach to evaluate and guide network design. Our findings confirm that\nsymmetry breaking is a fundamental principle that underpins various\noptimization techniques, including dropout, batch normalization, and\nequivariance. By quantifying the degree of symmetry breaking, our work offers a\npractical technique for performance enhancement and a metric to guide network\ndesign without the need for complete datasets and extensive training processes.\n", "link": "http://arxiv.org/abs/2409.06402v1", "date": "2024-09-10", "relevancy": 1.9119, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4845}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4744}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symmetry%20Breaking%20in%20Neural%20Network%20Optimization%3A%20Insights%20from%20Input%0A%20%20Dimension%20Expansion&body=Title%3A%20Symmetry%20Breaking%20in%20Neural%20Network%20Optimization%3A%20Insights%20from%20Input%0A%20%20Dimension%20Expansion%0AAuthor%3A%20Jun-Jie%20Zhang%20and%20Nan%20Cheng%20and%20Fu-Peng%20Li%20and%20Xiu-Cheng%20Wang%20and%20Jian-Nan%20Chen%20and%20Long-Gang%20Pang%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Understanding%20the%20mechanisms%20behind%20neural%20network%20optimization%20is%20crucial%0Afor%20improving%20network%20design%20and%20performance.%20While%20various%20optimization%0Atechniques%20have%20been%20developed%2C%20a%20comprehensive%20understanding%20of%20the%20underlying%0Aprinciples%20that%20govern%20these%20techniques%20remains%20elusive.%20Specifically%2C%20the%20role%0Aof%20symmetry%20breaking%2C%20a%20fundamental%20concept%20in%20physics%2C%20has%20not%20been%20fully%0Aexplored%20in%20neural%20network%20optimization.%20This%20gap%20in%20knowledge%20limits%20our%0Aability%20to%20design%20networks%20that%20are%20both%20efficient%20and%20effective.%20Here%2C%20we%0Apropose%20the%20symmetry%20breaking%20hypothesis%20to%20elucidate%20the%20significance%20of%0Asymmetry%20breaking%20in%20enhancing%20neural%20network%20optimization.%20We%20demonstrate%20that%0Aa%20simple%20input%20expansion%20can%20significantly%20improve%20network%20performance%20across%0Avarious%20tasks%2C%20and%20we%20show%20that%20this%20improvement%20can%20be%20attributed%20to%20the%0Aunderlying%20symmetry%20breaking%20mechanism.%20We%20further%20develop%20a%20metric%20to%20quantify%0Athe%20degree%20of%20symmetry%20breaking%20in%20neural%20networks%2C%20providing%20a%20practical%0Aapproach%20to%20evaluate%20and%20guide%20network%20design.%20Our%20findings%20confirm%20that%0Asymmetry%20breaking%20is%20a%20fundamental%20principle%20that%20underpins%20various%0Aoptimization%20techniques%2C%20including%20dropout%2C%20batch%20normalization%2C%20and%0Aequivariance.%20By%20quantifying%20the%20degree%20of%20symmetry%20breaking%2C%20our%20work%20offers%20a%0Apractical%20technique%20for%20performance%20enhancement%20and%20a%20metric%20to%20guide%20network%0Adesign%20without%20the%20need%20for%20complete%20datasets%20and%20extensive%20training%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymmetry%2520Breaking%2520in%2520Neural%2520Network%2520Optimization%253A%2520Insights%2520from%2520Input%250A%2520%2520Dimension%2520Expansion%26entry.906535625%3DJun-Jie%2520Zhang%2520and%2520Nan%2520Cheng%2520and%2520Fu-Peng%2520Li%2520and%2520Xiu-Cheng%2520Wang%2520and%2520Jian-Nan%2520Chen%2520and%2520Long-Gang%2520Pang%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Understanding%2520the%2520mechanisms%2520behind%2520neural%2520network%2520optimization%2520is%2520crucial%250Afor%2520improving%2520network%2520design%2520and%2520performance.%2520While%2520various%2520optimization%250Atechniques%2520have%2520been%2520developed%252C%2520a%2520comprehensive%2520understanding%2520of%2520the%2520underlying%250Aprinciples%2520that%2520govern%2520these%2520techniques%2520remains%2520elusive.%2520Specifically%252C%2520the%2520role%250Aof%2520symmetry%2520breaking%252C%2520a%2520fundamental%2520concept%2520in%2520physics%252C%2520has%2520not%2520been%2520fully%250Aexplored%2520in%2520neural%2520network%2520optimization.%2520This%2520gap%2520in%2520knowledge%2520limits%2520our%250Aability%2520to%2520design%2520networks%2520that%2520are%2520both%2520efficient%2520and%2520effective.%2520Here%252C%2520we%250Apropose%2520the%2520symmetry%2520breaking%2520hypothesis%2520to%2520elucidate%2520the%2520significance%2520of%250Asymmetry%2520breaking%2520in%2520enhancing%2520neural%2520network%2520optimization.%2520We%2520demonstrate%2520that%250Aa%2520simple%2520input%2520expansion%2520can%2520significantly%2520improve%2520network%2520performance%2520across%250Avarious%2520tasks%252C%2520and%2520we%2520show%2520that%2520this%2520improvement%2520can%2520be%2520attributed%2520to%2520the%250Aunderlying%2520symmetry%2520breaking%2520mechanism.%2520We%2520further%2520develop%2520a%2520metric%2520to%2520quantify%250Athe%2520degree%2520of%2520symmetry%2520breaking%2520in%2520neural%2520networks%252C%2520providing%2520a%2520practical%250Aapproach%2520to%2520evaluate%2520and%2520guide%2520network%2520design.%2520Our%2520findings%2520confirm%2520that%250Asymmetry%2520breaking%2520is%2520a%2520fundamental%2520principle%2520that%2520underpins%2520various%250Aoptimization%2520techniques%252C%2520including%2520dropout%252C%2520batch%2520normalization%252C%2520and%250Aequivariance.%2520By%2520quantifying%2520the%2520degree%2520of%2520symmetry%2520breaking%252C%2520our%2520work%2520offers%2520a%250Apractical%2520technique%2520for%2520performance%2520enhancement%2520and%2520a%2520metric%2520to%2520guide%2520network%250Adesign%2520without%2520the%2520need%2520for%2520complete%2520datasets%2520and%2520extensive%2520training%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetry%20Breaking%20in%20Neural%20Network%20Optimization%3A%20Insights%20from%20Input%0A%20%20Dimension%20Expansion&entry.906535625=Jun-Jie%20Zhang%20and%20Nan%20Cheng%20and%20Fu-Peng%20Li%20and%20Xiu-Cheng%20Wang%20and%20Jian-Nan%20Chen%20and%20Long-Gang%20Pang%20and%20Deyu%20Meng&entry.1292438233=%20%20Understanding%20the%20mechanisms%20behind%20neural%20network%20optimization%20is%20crucial%0Afor%20improving%20network%20design%20and%20performance.%20While%20various%20optimization%0Atechniques%20have%20been%20developed%2C%20a%20comprehensive%20understanding%20of%20the%20underlying%0Aprinciples%20that%20govern%20these%20techniques%20remains%20elusive.%20Specifically%2C%20the%20role%0Aof%20symmetry%20breaking%2C%20a%20fundamental%20concept%20in%20physics%2C%20has%20not%20been%20fully%0Aexplored%20in%20neural%20network%20optimization.%20This%20gap%20in%20knowledge%20limits%20our%0Aability%20to%20design%20networks%20that%20are%20both%20efficient%20and%20effective.%20Here%2C%20we%0Apropose%20the%20symmetry%20breaking%20hypothesis%20to%20elucidate%20the%20significance%20of%0Asymmetry%20breaking%20in%20enhancing%20neural%20network%20optimization.%20We%20demonstrate%20that%0Aa%20simple%20input%20expansion%20can%20significantly%20improve%20network%20performance%20across%0Avarious%20tasks%2C%20and%20we%20show%20that%20this%20improvement%20can%20be%20attributed%20to%20the%0Aunderlying%20symmetry%20breaking%20mechanism.%20We%20further%20develop%20a%20metric%20to%20quantify%0Athe%20degree%20of%20symmetry%20breaking%20in%20neural%20networks%2C%20providing%20a%20practical%0Aapproach%20to%20evaluate%20and%20guide%20network%20design.%20Our%20findings%20confirm%20that%0Asymmetry%20breaking%20is%20a%20fundamental%20principle%20that%20underpins%20various%0Aoptimization%20techniques%2C%20including%20dropout%2C%20batch%20normalization%2C%20and%0Aequivariance.%20By%20quantifying%20the%20degree%20of%20symmetry%20breaking%2C%20our%20work%20offers%20a%0Apractical%20technique%20for%20performance%20enhancement%20and%20a%20metric%20to%20guide%20network%0Adesign%20without%20the%20need%20for%20complete%20datasets%20and%20extensive%20training%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06402v1&entry.124074799=Read"},
{"title": "Dialogue You Can Trust: Human and AI Perspectives on Generated\n  Conversations", "author": "Ike Ebubechukwu and Johane Takeuchi and Antonello Ceravola and Frank Joublin", "abstract": "  As dialogue systems and chatbots increasingly integrate into everyday\ninteractions, the need for efficient and accurate evaluation methods becomes\nparamount. This study explores the comparative performance of human and AI\nassessments across a range of dialogue scenarios, focusing on seven key\nperformance indicators (KPIs): Coherence, Innovation, Concreteness, Goal\nContribution, Commonsense Contradiction, Incorrect Fact, and Redundancy.\nUtilizing the GPT-4o API, we generated a diverse dataset of conversations and\nconducted a two-part experimental analysis. In Experiment 1, we evaluated\nmulti-party conversations on Coherence, Innovation, Concreteness, and Goal\nContribution, revealing that GPT models align closely with human judgments.\nNotably, both human and AI evaluators exhibited a tendency towards binary\njudgment rather than linear scaling, highlighting a shared challenge in these\nassessments. Experiment 2 extended the work of Finch et al. (2023) by focusing\non dyadic dialogues and assessing Commonsense Contradiction, Incorrect Fact,\nand Redundancy. The results indicate that while GPT-4o demonstrates strong\nperformance in maintaining factual accuracy and commonsense reasoning, it still\nstruggles with reducing redundancy and self-contradiction. Our findings\nunderscore the potential of GPT models to closely replicate human evaluation in\ndialogue systems, while also pointing to areas for improvement. This research\noffers valuable insights for advancing the development and implementation of\nmore refined dialogue evaluation methodologies, contributing to the evolution\nof more effective and human-like AI communication tools.\n", "link": "http://arxiv.org/abs/2409.01808v2", "date": "2024-09-10", "relevancy": 1.9022, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4809}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dialogue%20You%20Can%20Trust%3A%20Human%20and%20AI%20Perspectives%20on%20Generated%0A%20%20Conversations&body=Title%3A%20Dialogue%20You%20Can%20Trust%3A%20Human%20and%20AI%20Perspectives%20on%20Generated%0A%20%20Conversations%0AAuthor%3A%20Ike%20Ebubechukwu%20and%20Johane%20Takeuchi%20and%20Antonello%20Ceravola%20and%20Frank%20Joublin%0AAbstract%3A%20%20%20As%20dialogue%20systems%20and%20chatbots%20increasingly%20integrate%20into%20everyday%0Ainteractions%2C%20the%20need%20for%20efficient%20and%20accurate%20evaluation%20methods%20becomes%0Aparamount.%20This%20study%20explores%20the%20comparative%20performance%20of%20human%20and%20AI%0Aassessments%20across%20a%20range%20of%20dialogue%20scenarios%2C%20focusing%20on%20seven%20key%0Aperformance%20indicators%20%28KPIs%29%3A%20Coherence%2C%20Innovation%2C%20Concreteness%2C%20Goal%0AContribution%2C%20Commonsense%20Contradiction%2C%20Incorrect%20Fact%2C%20and%20Redundancy.%0AUtilizing%20the%20GPT-4o%20API%2C%20we%20generated%20a%20diverse%20dataset%20of%20conversations%20and%0Aconducted%20a%20two-part%20experimental%20analysis.%20In%20Experiment%201%2C%20we%20evaluated%0Amulti-party%20conversations%20on%20Coherence%2C%20Innovation%2C%20Concreteness%2C%20and%20Goal%0AContribution%2C%20revealing%20that%20GPT%20models%20align%20closely%20with%20human%20judgments.%0ANotably%2C%20both%20human%20and%20AI%20evaluators%20exhibited%20a%20tendency%20towards%20binary%0Ajudgment%20rather%20than%20linear%20scaling%2C%20highlighting%20a%20shared%20challenge%20in%20these%0Aassessments.%20Experiment%202%20extended%20the%20work%20of%20Finch%20et%20al.%20%282023%29%20by%20focusing%0Aon%20dyadic%20dialogues%20and%20assessing%20Commonsense%20Contradiction%2C%20Incorrect%20Fact%2C%0Aand%20Redundancy.%20The%20results%20indicate%20that%20while%20GPT-4o%20demonstrates%20strong%0Aperformance%20in%20maintaining%20factual%20accuracy%20and%20commonsense%20reasoning%2C%20it%20still%0Astruggles%20with%20reducing%20redundancy%20and%20self-contradiction.%20Our%20findings%0Aunderscore%20the%20potential%20of%20GPT%20models%20to%20closely%20replicate%20human%20evaluation%20in%0Adialogue%20systems%2C%20while%20also%20pointing%20to%20areas%20for%20improvement.%20This%20research%0Aoffers%20valuable%20insights%20for%20advancing%20the%20development%20and%20implementation%20of%0Amore%20refined%20dialogue%20evaluation%20methodologies%2C%20contributing%20to%20the%20evolution%0Aof%20more%20effective%20and%20human-like%20AI%20communication%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01808v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDialogue%2520You%2520Can%2520Trust%253A%2520Human%2520and%2520AI%2520Perspectives%2520on%2520Generated%250A%2520%2520Conversations%26entry.906535625%3DIke%2520Ebubechukwu%2520and%2520Johane%2520Takeuchi%2520and%2520Antonello%2520Ceravola%2520and%2520Frank%2520Joublin%26entry.1292438233%3D%2520%2520As%2520dialogue%2520systems%2520and%2520chatbots%2520increasingly%2520integrate%2520into%2520everyday%250Ainteractions%252C%2520the%2520need%2520for%2520efficient%2520and%2520accurate%2520evaluation%2520methods%2520becomes%250Aparamount.%2520This%2520study%2520explores%2520the%2520comparative%2520performance%2520of%2520human%2520and%2520AI%250Aassessments%2520across%2520a%2520range%2520of%2520dialogue%2520scenarios%252C%2520focusing%2520on%2520seven%2520key%250Aperformance%2520indicators%2520%2528KPIs%2529%253A%2520Coherence%252C%2520Innovation%252C%2520Concreteness%252C%2520Goal%250AContribution%252C%2520Commonsense%2520Contradiction%252C%2520Incorrect%2520Fact%252C%2520and%2520Redundancy.%250AUtilizing%2520the%2520GPT-4o%2520API%252C%2520we%2520generated%2520a%2520diverse%2520dataset%2520of%2520conversations%2520and%250Aconducted%2520a%2520two-part%2520experimental%2520analysis.%2520In%2520Experiment%25201%252C%2520we%2520evaluated%250Amulti-party%2520conversations%2520on%2520Coherence%252C%2520Innovation%252C%2520Concreteness%252C%2520and%2520Goal%250AContribution%252C%2520revealing%2520that%2520GPT%2520models%2520align%2520closely%2520with%2520human%2520judgments.%250ANotably%252C%2520both%2520human%2520and%2520AI%2520evaluators%2520exhibited%2520a%2520tendency%2520towards%2520binary%250Ajudgment%2520rather%2520than%2520linear%2520scaling%252C%2520highlighting%2520a%2520shared%2520challenge%2520in%2520these%250Aassessments.%2520Experiment%25202%2520extended%2520the%2520work%2520of%2520Finch%2520et%2520al.%2520%25282023%2529%2520by%2520focusing%250Aon%2520dyadic%2520dialogues%2520and%2520assessing%2520Commonsense%2520Contradiction%252C%2520Incorrect%2520Fact%252C%250Aand%2520Redundancy.%2520The%2520results%2520indicate%2520that%2520while%2520GPT-4o%2520demonstrates%2520strong%250Aperformance%2520in%2520maintaining%2520factual%2520accuracy%2520and%2520commonsense%2520reasoning%252C%2520it%2520still%250Astruggles%2520with%2520reducing%2520redundancy%2520and%2520self-contradiction.%2520Our%2520findings%250Aunderscore%2520the%2520potential%2520of%2520GPT%2520models%2520to%2520closely%2520replicate%2520human%2520evaluation%2520in%250Adialogue%2520systems%252C%2520while%2520also%2520pointing%2520to%2520areas%2520for%2520improvement.%2520This%2520research%250Aoffers%2520valuable%2520insights%2520for%2520advancing%2520the%2520development%2520and%2520implementation%2520of%250Amore%2520refined%2520dialogue%2520evaluation%2520methodologies%252C%2520contributing%2520to%2520the%2520evolution%250Aof%2520more%2520effective%2520and%2520human-like%2520AI%2520communication%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01808v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dialogue%20You%20Can%20Trust%3A%20Human%20and%20AI%20Perspectives%20on%20Generated%0A%20%20Conversations&entry.906535625=Ike%20Ebubechukwu%20and%20Johane%20Takeuchi%20and%20Antonello%20Ceravola%20and%20Frank%20Joublin&entry.1292438233=%20%20As%20dialogue%20systems%20and%20chatbots%20increasingly%20integrate%20into%20everyday%0Ainteractions%2C%20the%20need%20for%20efficient%20and%20accurate%20evaluation%20methods%20becomes%0Aparamount.%20This%20study%20explores%20the%20comparative%20performance%20of%20human%20and%20AI%0Aassessments%20across%20a%20range%20of%20dialogue%20scenarios%2C%20focusing%20on%20seven%20key%0Aperformance%20indicators%20%28KPIs%29%3A%20Coherence%2C%20Innovation%2C%20Concreteness%2C%20Goal%0AContribution%2C%20Commonsense%20Contradiction%2C%20Incorrect%20Fact%2C%20and%20Redundancy.%0AUtilizing%20the%20GPT-4o%20API%2C%20we%20generated%20a%20diverse%20dataset%20of%20conversations%20and%0Aconducted%20a%20two-part%20experimental%20analysis.%20In%20Experiment%201%2C%20we%20evaluated%0Amulti-party%20conversations%20on%20Coherence%2C%20Innovation%2C%20Concreteness%2C%20and%20Goal%0AContribution%2C%20revealing%20that%20GPT%20models%20align%20closely%20with%20human%20judgments.%0ANotably%2C%20both%20human%20and%20AI%20evaluators%20exhibited%20a%20tendency%20towards%20binary%0Ajudgment%20rather%20than%20linear%20scaling%2C%20highlighting%20a%20shared%20challenge%20in%20these%0Aassessments.%20Experiment%202%20extended%20the%20work%20of%20Finch%20et%20al.%20%282023%29%20by%20focusing%0Aon%20dyadic%20dialogues%20and%20assessing%20Commonsense%20Contradiction%2C%20Incorrect%20Fact%2C%0Aand%20Redundancy.%20The%20results%20indicate%20that%20while%20GPT-4o%20demonstrates%20strong%0Aperformance%20in%20maintaining%20factual%20accuracy%20and%20commonsense%20reasoning%2C%20it%20still%0Astruggles%20with%20reducing%20redundancy%20and%20self-contradiction.%20Our%20findings%0Aunderscore%20the%20potential%20of%20GPT%20models%20to%20closely%20replicate%20human%20evaluation%20in%0Adialogue%20systems%2C%20while%20also%20pointing%20to%20areas%20for%20improvement.%20This%20research%0Aoffers%20valuable%20insights%20for%20advancing%20the%20development%20and%20implementation%20of%0Amore%20refined%20dialogue%20evaluation%20methodologies%2C%20contributing%20to%20the%20evolution%0Aof%20more%20effective%20and%20human-like%20AI%20communication%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01808v2&entry.124074799=Read"},
{"title": "Mathematical Modeling Of Four Finger Robotic Grippers", "author": "Sajjad Hussain and M. Suhaib", "abstract": "  Robotic grippers are the end effector in the robot system of handling any\ntask which used for performing various operations for the purpose of industrial\napplication and hazardous tasks.In this paper, we developed the mathematical\nmodel for multi fingers robotics grippers. we are concerned with Jamia'shand\nwhich is developed in Robotics Lab, Mechanical Engineering Deptt, Faculty of\nEngg & Technolgy, Jamia Millia Islamia, India. This is a tendon-driven gripper\neach finger having three DOF having a total of 11 DOF. The term tendon is\nwidely used to imply belts, cables, or similar types of applications. It is\nmade up of three fingers and a thumb. Every finger and thumb has one degree of\nfreedom. The power transmission mechanism is a rope and pulley system. Both\nhands have similar structures. Aluminum from the 5083 families was used to make\nthis product. The gripping force can be adjusted we have done the kinematics,\nforce, and dynamic analysis by developing a Mathematical model for the\nfour-finger robotics grippers and their thumb. we focused it control motions in\nX and Y Displacements with the angular positions movements and we make the\nforce analysis of the four fingers and thumb calculate the maximum weight,\nforce, and torque required to move it with mass. Draw the force -displacements\ngraph which shows the linear behavior up to 250 N and shows nonlinear behavior\nbeyond this. and required Dmin of wire is 0.86 mm for grasping the maximum 1 kg\nload also developed the dynamic model (using energy )approach lagrangian method\nto find it torque required to move the fingers.\n", "link": "http://arxiv.org/abs/2409.06419v1", "date": "2024-09-10", "relevancy": 1.8931, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5046}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4529}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mathematical%20Modeling%20Of%20Four%20Finger%20Robotic%20Grippers&body=Title%3A%20Mathematical%20Modeling%20Of%20Four%20Finger%20Robotic%20Grippers%0AAuthor%3A%20Sajjad%20Hussain%20and%20M.%20Suhaib%0AAbstract%3A%20%20%20Robotic%20grippers%20are%20the%20end%20effector%20in%20the%20robot%20system%20of%20handling%20any%0Atask%20which%20used%20for%20performing%20various%20operations%20for%20the%20purpose%20of%20industrial%0Aapplication%20and%20hazardous%20tasks.In%20this%20paper%2C%20we%20developed%20the%20mathematical%0Amodel%20for%20multi%20fingers%20robotics%20grippers.%20we%20are%20concerned%20with%20Jamia%27shand%0Awhich%20is%20developed%20in%20Robotics%20Lab%2C%20Mechanical%20Engineering%20Deptt%2C%20Faculty%20of%0AEngg%20%26%20Technolgy%2C%20Jamia%20Millia%20Islamia%2C%20India.%20This%20is%20a%20tendon-driven%20gripper%0Aeach%20finger%20having%20three%20DOF%20having%20a%20total%20of%2011%20DOF.%20The%20term%20tendon%20is%0Awidely%20used%20to%20imply%20belts%2C%20cables%2C%20or%20similar%20types%20of%20applications.%20It%20is%0Amade%20up%20of%20three%20fingers%20and%20a%20thumb.%20Every%20finger%20and%20thumb%20has%20one%20degree%20of%0Afreedom.%20The%20power%20transmission%20mechanism%20is%20a%20rope%20and%20pulley%20system.%20Both%0Ahands%20have%20similar%20structures.%20Aluminum%20from%20the%205083%20families%20was%20used%20to%20make%0Athis%20product.%20The%20gripping%20force%20can%20be%20adjusted%20we%20have%20done%20the%20kinematics%2C%0Aforce%2C%20and%20dynamic%20analysis%20by%20developing%20a%20Mathematical%20model%20for%20the%0Afour-finger%20robotics%20grippers%20and%20their%20thumb.%20we%20focused%20it%20control%20motions%20in%0AX%20and%20Y%20Displacements%20with%20the%20angular%20positions%20movements%20and%20we%20make%20the%0Aforce%20analysis%20of%20the%20four%20fingers%20and%20thumb%20calculate%20the%20maximum%20weight%2C%0Aforce%2C%20and%20torque%20required%20to%20move%20it%20with%20mass.%20Draw%20the%20force%20-displacements%0Agraph%20which%20shows%20the%20linear%20behavior%20up%20to%20250%20N%20and%20shows%20nonlinear%20behavior%0Abeyond%20this.%20and%20required%20Dmin%20of%20wire%20is%200.86%20mm%20for%20grasping%20the%20maximum%201%20kg%0Aload%20also%20developed%20the%20dynamic%20model%20%28using%20energy%20%29approach%20lagrangian%20method%0Ato%20find%20it%20torque%20required%20to%20move%20the%20fingers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathematical%2520Modeling%2520Of%2520Four%2520Finger%2520Robotic%2520Grippers%26entry.906535625%3DSajjad%2520Hussain%2520and%2520M.%2520Suhaib%26entry.1292438233%3D%2520%2520Robotic%2520grippers%2520are%2520the%2520end%2520effector%2520in%2520the%2520robot%2520system%2520of%2520handling%2520any%250Atask%2520which%2520used%2520for%2520performing%2520various%2520operations%2520for%2520the%2520purpose%2520of%2520industrial%250Aapplication%2520and%2520hazardous%2520tasks.In%2520this%2520paper%252C%2520we%2520developed%2520the%2520mathematical%250Amodel%2520for%2520multi%2520fingers%2520robotics%2520grippers.%2520we%2520are%2520concerned%2520with%2520Jamia%2527shand%250Awhich%2520is%2520developed%2520in%2520Robotics%2520Lab%252C%2520Mechanical%2520Engineering%2520Deptt%252C%2520Faculty%2520of%250AEngg%2520%2526%2520Technolgy%252C%2520Jamia%2520Millia%2520Islamia%252C%2520India.%2520This%2520is%2520a%2520tendon-driven%2520gripper%250Aeach%2520finger%2520having%2520three%2520DOF%2520having%2520a%2520total%2520of%252011%2520DOF.%2520The%2520term%2520tendon%2520is%250Awidely%2520used%2520to%2520imply%2520belts%252C%2520cables%252C%2520or%2520similar%2520types%2520of%2520applications.%2520It%2520is%250Amade%2520up%2520of%2520three%2520fingers%2520and%2520a%2520thumb.%2520Every%2520finger%2520and%2520thumb%2520has%2520one%2520degree%2520of%250Afreedom.%2520The%2520power%2520transmission%2520mechanism%2520is%2520a%2520rope%2520and%2520pulley%2520system.%2520Both%250Ahands%2520have%2520similar%2520structures.%2520Aluminum%2520from%2520the%25205083%2520families%2520was%2520used%2520to%2520make%250Athis%2520product.%2520The%2520gripping%2520force%2520can%2520be%2520adjusted%2520we%2520have%2520done%2520the%2520kinematics%252C%250Aforce%252C%2520and%2520dynamic%2520analysis%2520by%2520developing%2520a%2520Mathematical%2520model%2520for%2520the%250Afour-finger%2520robotics%2520grippers%2520and%2520their%2520thumb.%2520we%2520focused%2520it%2520control%2520motions%2520in%250AX%2520and%2520Y%2520Displacements%2520with%2520the%2520angular%2520positions%2520movements%2520and%2520we%2520make%2520the%250Aforce%2520analysis%2520of%2520the%2520four%2520fingers%2520and%2520thumb%2520calculate%2520the%2520maximum%2520weight%252C%250Aforce%252C%2520and%2520torque%2520required%2520to%2520move%2520it%2520with%2520mass.%2520Draw%2520the%2520force%2520-displacements%250Agraph%2520which%2520shows%2520the%2520linear%2520behavior%2520up%2520to%2520250%2520N%2520and%2520shows%2520nonlinear%2520behavior%250Abeyond%2520this.%2520and%2520required%2520Dmin%2520of%2520wire%2520is%25200.86%2520mm%2520for%2520grasping%2520the%2520maximum%25201%2520kg%250Aload%2520also%2520developed%2520the%2520dynamic%2520model%2520%2528using%2520energy%2520%2529approach%2520lagrangian%2520method%250Ato%2520find%2520it%2520torque%2520required%2520to%2520move%2520the%2520fingers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mathematical%20Modeling%20Of%20Four%20Finger%20Robotic%20Grippers&entry.906535625=Sajjad%20Hussain%20and%20M.%20Suhaib&entry.1292438233=%20%20Robotic%20grippers%20are%20the%20end%20effector%20in%20the%20robot%20system%20of%20handling%20any%0Atask%20which%20used%20for%20performing%20various%20operations%20for%20the%20purpose%20of%20industrial%0Aapplication%20and%20hazardous%20tasks.In%20this%20paper%2C%20we%20developed%20the%20mathematical%0Amodel%20for%20multi%20fingers%20robotics%20grippers.%20we%20are%20concerned%20with%20Jamia%27shand%0Awhich%20is%20developed%20in%20Robotics%20Lab%2C%20Mechanical%20Engineering%20Deptt%2C%20Faculty%20of%0AEngg%20%26%20Technolgy%2C%20Jamia%20Millia%20Islamia%2C%20India.%20This%20is%20a%20tendon-driven%20gripper%0Aeach%20finger%20having%20three%20DOF%20having%20a%20total%20of%2011%20DOF.%20The%20term%20tendon%20is%0Awidely%20used%20to%20imply%20belts%2C%20cables%2C%20or%20similar%20types%20of%20applications.%20It%20is%0Amade%20up%20of%20three%20fingers%20and%20a%20thumb.%20Every%20finger%20and%20thumb%20has%20one%20degree%20of%0Afreedom.%20The%20power%20transmission%20mechanism%20is%20a%20rope%20and%20pulley%20system.%20Both%0Ahands%20have%20similar%20structures.%20Aluminum%20from%20the%205083%20families%20was%20used%20to%20make%0Athis%20product.%20The%20gripping%20force%20can%20be%20adjusted%20we%20have%20done%20the%20kinematics%2C%0Aforce%2C%20and%20dynamic%20analysis%20by%20developing%20a%20Mathematical%20model%20for%20the%0Afour-finger%20robotics%20grippers%20and%20their%20thumb.%20we%20focused%20it%20control%20motions%20in%0AX%20and%20Y%20Displacements%20with%20the%20angular%20positions%20movements%20and%20we%20make%20the%0Aforce%20analysis%20of%20the%20four%20fingers%20and%20thumb%20calculate%20the%20maximum%20weight%2C%0Aforce%2C%20and%20torque%20required%20to%20move%20it%20with%20mass.%20Draw%20the%20force%20-displacements%0Agraph%20which%20shows%20the%20linear%20behavior%20up%20to%20250%20N%20and%20shows%20nonlinear%20behavior%0Abeyond%20this.%20and%20required%20Dmin%20of%20wire%20is%200.86%20mm%20for%20grasping%20the%20maximum%201%20kg%0Aload%20also%20developed%20the%20dynamic%20model%20%28using%20energy%20%29approach%20lagrangian%20method%0Ato%20find%20it%20torque%20required%20to%20move%20the%20fingers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06419v1&entry.124074799=Read"},
{"title": "ChatGPT's Potential in Cryptography Misuse Detection: A Comparative\n  Analysis with Static Analysis Tools", "author": "Ehsan Firouzi and Mohammad Ghafari and Mike Ebrahimi", "abstract": "  The correct adoption of cryptography APIs is challenging for mainstream\ndevelopers, often resulting in widespread API misuse. Meanwhile, cryptography\nmisuse detectors have demonstrated inconsistent performance and remain largely\ninaccessible to most developers. We investigated the extent to which ChatGPT\ncan detect cryptography misuses and compared its performance with that of the\nstate-of-the-art static analysis tools. Our investigation, mainly based on the\nCryptoAPI-Bench benchmark, demonstrated that ChatGPT is effective in\nidentifying cryptography API misuses, and with the use of prompt engineering,\nit can even outperform leading static cryptography misuse detectors.\n", "link": "http://arxiv.org/abs/2409.06561v1", "date": "2024-09-10", "relevancy": 1.8888, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3996}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3682}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatGPT%27s%20Potential%20in%20Cryptography%20Misuse%20Detection%3A%20A%20Comparative%0A%20%20Analysis%20with%20Static%20Analysis%20Tools&body=Title%3A%20ChatGPT%27s%20Potential%20in%20Cryptography%20Misuse%20Detection%3A%20A%20Comparative%0A%20%20Analysis%20with%20Static%20Analysis%20Tools%0AAuthor%3A%20Ehsan%20Firouzi%20and%20Mohammad%20Ghafari%20and%20Mike%20Ebrahimi%0AAbstract%3A%20%20%20The%20correct%20adoption%20of%20cryptography%20APIs%20is%20challenging%20for%20mainstream%0Adevelopers%2C%20often%20resulting%20in%20widespread%20API%20misuse.%20Meanwhile%2C%20cryptography%0Amisuse%20detectors%20have%20demonstrated%20inconsistent%20performance%20and%20remain%20largely%0Ainaccessible%20to%20most%20developers.%20We%20investigated%20the%20extent%20to%20which%20ChatGPT%0Acan%20detect%20cryptography%20misuses%20and%20compared%20its%20performance%20with%20that%20of%20the%0Astate-of-the-art%20static%20analysis%20tools.%20Our%20investigation%2C%20mainly%20based%20on%20the%0ACryptoAPI-Bench%20benchmark%2C%20demonstrated%20that%20ChatGPT%20is%20effective%20in%0Aidentifying%20cryptography%20API%20misuses%2C%20and%20with%20the%20use%20of%20prompt%20engineering%2C%0Ait%20can%20even%20outperform%20leading%20static%20cryptography%20misuse%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatGPT%2527s%2520Potential%2520in%2520Cryptography%2520Misuse%2520Detection%253A%2520A%2520Comparative%250A%2520%2520Analysis%2520with%2520Static%2520Analysis%2520Tools%26entry.906535625%3DEhsan%2520Firouzi%2520and%2520Mohammad%2520Ghafari%2520and%2520Mike%2520Ebrahimi%26entry.1292438233%3D%2520%2520The%2520correct%2520adoption%2520of%2520cryptography%2520APIs%2520is%2520challenging%2520for%2520mainstream%250Adevelopers%252C%2520often%2520resulting%2520in%2520widespread%2520API%2520misuse.%2520Meanwhile%252C%2520cryptography%250Amisuse%2520detectors%2520have%2520demonstrated%2520inconsistent%2520performance%2520and%2520remain%2520largely%250Ainaccessible%2520to%2520most%2520developers.%2520We%2520investigated%2520the%2520extent%2520to%2520which%2520ChatGPT%250Acan%2520detect%2520cryptography%2520misuses%2520and%2520compared%2520its%2520performance%2520with%2520that%2520of%2520the%250Astate-of-the-art%2520static%2520analysis%2520tools.%2520Our%2520investigation%252C%2520mainly%2520based%2520on%2520the%250ACryptoAPI-Bench%2520benchmark%252C%2520demonstrated%2520that%2520ChatGPT%2520is%2520effective%2520in%250Aidentifying%2520cryptography%2520API%2520misuses%252C%2520and%2520with%2520the%2520use%2520of%2520prompt%2520engineering%252C%250Ait%2520can%2520even%2520outperform%2520leading%2520static%2520cryptography%2520misuse%2520detectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatGPT%27s%20Potential%20in%20Cryptography%20Misuse%20Detection%3A%20A%20Comparative%0A%20%20Analysis%20with%20Static%20Analysis%20Tools&entry.906535625=Ehsan%20Firouzi%20and%20Mohammad%20Ghafari%20and%20Mike%20Ebrahimi&entry.1292438233=%20%20The%20correct%20adoption%20of%20cryptography%20APIs%20is%20challenging%20for%20mainstream%0Adevelopers%2C%20often%20resulting%20in%20widespread%20API%20misuse.%20Meanwhile%2C%20cryptography%0Amisuse%20detectors%20have%20demonstrated%20inconsistent%20performance%20and%20remain%20largely%0Ainaccessible%20to%20most%20developers.%20We%20investigated%20the%20extent%20to%20which%20ChatGPT%0Acan%20detect%20cryptography%20misuses%20and%20compared%20its%20performance%20with%20that%20of%20the%0Astate-of-the-art%20static%20analysis%20tools.%20Our%20investigation%2C%20mainly%20based%20on%20the%0ACryptoAPI-Bench%20benchmark%2C%20demonstrated%20that%20ChatGPT%20is%20effective%20in%0Aidentifying%20cryptography%20API%20misuses%2C%20and%20with%20the%20use%20of%20prompt%20engineering%2C%0Ait%20can%20even%20outperform%20leading%20static%20cryptography%20misuse%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06561v1&entry.124074799=Read"},
{"title": "DNN-Defender: A Victim-Focused In-DRAM Defense Mechanism for Taming\n  Adversarial Weight Attack on DNNs", "author": "Ranyang Zhou and Sabbir Ahmed and Adnan Siraj Rakin and Shaahin Angizi", "abstract": "  With deep learning deployed in many security-sensitive areas, machine\nlearning security is becoming progressively important. Recent studies\ndemonstrate attackers can exploit system-level techniques exploiting the\nRowHammer vulnerability of DRAM to deterministically and precisely flip bits in\nDeep Neural Networks (DNN) model weights to affect inference accuracy. The\nexisting defense mechanisms are software-based, such as weight reconstruction\nrequiring expensive training overhead or performance degradation. On the other\nhand, generic hardware-based victim-/aggressor-focused mechanisms impose\nexpensive hardware overheads and preserve the spatial connection between victim\nand aggressor rows. In this paper, we present the first DRAM-based\nvictim-focused defense mechanism tailored for quantized DNNs, named\nDNN-Defender that leverages the potential of in-DRAM swapping to withstand the\ntargeted bit-flip attacks with a priority protection mechanism. Our results\nindicate that DNN-Defender can deliver a high level of protection downgrading\nthe performance of targeted RowHammer attacks to a random attack level. In\naddition, the proposed defense has no accuracy drop on CIFAR-10 and ImageNet\ndatasets without requiring any software training or incurring hardware\noverhead.\n", "link": "http://arxiv.org/abs/2305.08034v2", "date": "2024-09-10", "relevancy": 1.8845, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4781}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4734}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNN-Defender%3A%20A%20Victim-Focused%20In-DRAM%20Defense%20Mechanism%20for%20Taming%0A%20%20Adversarial%20Weight%20Attack%20on%20DNNs&body=Title%3A%20DNN-Defender%3A%20A%20Victim-Focused%20In-DRAM%20Defense%20Mechanism%20for%20Taming%0A%20%20Adversarial%20Weight%20Attack%20on%20DNNs%0AAuthor%3A%20Ranyang%20Zhou%20and%20Sabbir%20Ahmed%20and%20Adnan%20Siraj%20Rakin%20and%20Shaahin%20Angizi%0AAbstract%3A%20%20%20With%20deep%20learning%20deployed%20in%20many%20security-sensitive%20areas%2C%20machine%0Alearning%20security%20is%20becoming%20progressively%20important.%20Recent%20studies%0Ademonstrate%20attackers%20can%20exploit%20system-level%20techniques%20exploiting%20the%0ARowHammer%20vulnerability%20of%20DRAM%20to%20deterministically%20and%20precisely%20flip%20bits%20in%0ADeep%20Neural%20Networks%20%28DNN%29%20model%20weights%20to%20affect%20inference%20accuracy.%20The%0Aexisting%20defense%20mechanisms%20are%20software-based%2C%20such%20as%20weight%20reconstruction%0Arequiring%20expensive%20training%20overhead%20or%20performance%20degradation.%20On%20the%20other%0Ahand%2C%20generic%20hardware-based%20victim-/aggressor-focused%20mechanisms%20impose%0Aexpensive%20hardware%20overheads%20and%20preserve%20the%20spatial%20connection%20between%20victim%0Aand%20aggressor%20rows.%20In%20this%20paper%2C%20we%20present%20the%20first%20DRAM-based%0Avictim-focused%20defense%20mechanism%20tailored%20for%20quantized%20DNNs%2C%20named%0ADNN-Defender%20that%20leverages%20the%20potential%20of%20in-DRAM%20swapping%20to%20withstand%20the%0Atargeted%20bit-flip%20attacks%20with%20a%20priority%20protection%20mechanism.%20Our%20results%0Aindicate%20that%20DNN-Defender%20can%20deliver%20a%20high%20level%20of%20protection%20downgrading%0Athe%20performance%20of%20targeted%20RowHammer%20attacks%20to%20a%20random%20attack%20level.%20In%0Aaddition%2C%20the%20proposed%20defense%20has%20no%20accuracy%20drop%20on%20CIFAR-10%20and%20ImageNet%0Adatasets%20without%20requiring%20any%20software%20training%20or%20incurring%20hardware%0Aoverhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.08034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNN-Defender%253A%2520A%2520Victim-Focused%2520In-DRAM%2520Defense%2520Mechanism%2520for%2520Taming%250A%2520%2520Adversarial%2520Weight%2520Attack%2520on%2520DNNs%26entry.906535625%3DRanyang%2520Zhou%2520and%2520Sabbir%2520Ahmed%2520and%2520Adnan%2520Siraj%2520Rakin%2520and%2520Shaahin%2520Angizi%26entry.1292438233%3D%2520%2520With%2520deep%2520learning%2520deployed%2520in%2520many%2520security-sensitive%2520areas%252C%2520machine%250Alearning%2520security%2520is%2520becoming%2520progressively%2520important.%2520Recent%2520studies%250Ademonstrate%2520attackers%2520can%2520exploit%2520system-level%2520techniques%2520exploiting%2520the%250ARowHammer%2520vulnerability%2520of%2520DRAM%2520to%2520deterministically%2520and%2520precisely%2520flip%2520bits%2520in%250ADeep%2520Neural%2520Networks%2520%2528DNN%2529%2520model%2520weights%2520to%2520affect%2520inference%2520accuracy.%2520The%250Aexisting%2520defense%2520mechanisms%2520are%2520software-based%252C%2520such%2520as%2520weight%2520reconstruction%250Arequiring%2520expensive%2520training%2520overhead%2520or%2520performance%2520degradation.%2520On%2520the%2520other%250Ahand%252C%2520generic%2520hardware-based%2520victim-/aggressor-focused%2520mechanisms%2520impose%250Aexpensive%2520hardware%2520overheads%2520and%2520preserve%2520the%2520spatial%2520connection%2520between%2520victim%250Aand%2520aggressor%2520rows.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520DRAM-based%250Avictim-focused%2520defense%2520mechanism%2520tailored%2520for%2520quantized%2520DNNs%252C%2520named%250ADNN-Defender%2520that%2520leverages%2520the%2520potential%2520of%2520in-DRAM%2520swapping%2520to%2520withstand%2520the%250Atargeted%2520bit-flip%2520attacks%2520with%2520a%2520priority%2520protection%2520mechanism.%2520Our%2520results%250Aindicate%2520that%2520DNN-Defender%2520can%2520deliver%2520a%2520high%2520level%2520of%2520protection%2520downgrading%250Athe%2520performance%2520of%2520targeted%2520RowHammer%2520attacks%2520to%2520a%2520random%2520attack%2520level.%2520In%250Aaddition%252C%2520the%2520proposed%2520defense%2520has%2520no%2520accuracy%2520drop%2520on%2520CIFAR-10%2520and%2520ImageNet%250Adatasets%2520without%2520requiring%2520any%2520software%2520training%2520or%2520incurring%2520hardware%250Aoverhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.08034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNN-Defender%3A%20A%20Victim-Focused%20In-DRAM%20Defense%20Mechanism%20for%20Taming%0A%20%20Adversarial%20Weight%20Attack%20on%20DNNs&entry.906535625=Ranyang%20Zhou%20and%20Sabbir%20Ahmed%20and%20Adnan%20Siraj%20Rakin%20and%20Shaahin%20Angizi&entry.1292438233=%20%20With%20deep%20learning%20deployed%20in%20many%20security-sensitive%20areas%2C%20machine%0Alearning%20security%20is%20becoming%20progressively%20important.%20Recent%20studies%0Ademonstrate%20attackers%20can%20exploit%20system-level%20techniques%20exploiting%20the%0ARowHammer%20vulnerability%20of%20DRAM%20to%20deterministically%20and%20precisely%20flip%20bits%20in%0ADeep%20Neural%20Networks%20%28DNN%29%20model%20weights%20to%20affect%20inference%20accuracy.%20The%0Aexisting%20defense%20mechanisms%20are%20software-based%2C%20such%20as%20weight%20reconstruction%0Arequiring%20expensive%20training%20overhead%20or%20performance%20degradation.%20On%20the%20other%0Ahand%2C%20generic%20hardware-based%20victim-/aggressor-focused%20mechanisms%20impose%0Aexpensive%20hardware%20overheads%20and%20preserve%20the%20spatial%20connection%20between%20victim%0Aand%20aggressor%20rows.%20In%20this%20paper%2C%20we%20present%20the%20first%20DRAM-based%0Avictim-focused%20defense%20mechanism%20tailored%20for%20quantized%20DNNs%2C%20named%0ADNN-Defender%20that%20leverages%20the%20potential%20of%20in-DRAM%20swapping%20to%20withstand%20the%0Atargeted%20bit-flip%20attacks%20with%20a%20priority%20protection%20mechanism.%20Our%20results%0Aindicate%20that%20DNN-Defender%20can%20deliver%20a%20high%20level%20of%20protection%20downgrading%0Athe%20performance%20of%20targeted%20RowHammer%20attacks%20to%20a%20random%20attack%20level.%20In%0Aaddition%2C%20the%20proposed%20defense%20has%20no%20accuracy%20drop%20on%20CIFAR-10%20and%20ImageNet%0Adatasets%20without%20requiring%20any%20software%20training%20or%20incurring%20hardware%0Aoverhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.08034v2&entry.124074799=Read"},
{"title": "Sortformer: Seamless Integration of Speaker Diarization and ASR by\n  Bridging Timestamps and Tokens", "author": "Taejin Park and Ivan Medennikov and Kunal Dhawan and Weiqing Wang and He Huang and Nithin Rao Koluguri and Krishna C. Puvvada and Jagadeesh Balam and Boris Ginsburg", "abstract": "  We propose Sortformer, a novel neural model for speaker diarization, trained\nwith unconventional objectives compared to existing end-to-end diarization\nmodels. The permutation problem in speaker diarization has long been regarded\nas a critical challenge. Most prior end-to-end diarization systems employ\npermutation invariant loss (PIL), which optimizes for the permutation that\nyields the lowest error. In contrast, we introduce Sort Loss, which enables a\ndiarization model to autonomously resolve permutation, with or without PIL. We\ndemonstrate that combining Sort Loss and PIL achieves performance competitive\nwith state-of-the-art end-to-end diarization models trained exclusively with\nPIL. Crucially, we present a streamlined multispeaker ASR architecture that\nleverages Sortformer as a speaker supervision model, embedding speaker label\nestimation within the ASR encoder state using a sinusoidal kernel function.\nThis approach resolves the speaker permutation problem through sorted\nobjectives, effectively bridging speaker-label timestamps and speaker tokens.\nIn our experiments, we show that the proposed multispeaker ASR architecture,\nenhanced with speaker supervision, improves performance via adapter techniques.\nCode and trained models will be made publicly available via the NVIDIA NeMo\nframework\n", "link": "http://arxiv.org/abs/2409.06656v1", "date": "2024-09-10", "relevancy": 1.8583, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4904}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4643}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sortformer%3A%20Seamless%20Integration%20of%20Speaker%20Diarization%20and%20ASR%20by%0A%20%20Bridging%20Timestamps%20and%20Tokens&body=Title%3A%20Sortformer%3A%20Seamless%20Integration%20of%20Speaker%20Diarization%20and%20ASR%20by%0A%20%20Bridging%20Timestamps%20and%20Tokens%0AAuthor%3A%20Taejin%20Park%20and%20Ivan%20Medennikov%20and%20Kunal%20Dhawan%20and%20Weiqing%20Wang%20and%20He%20Huang%20and%20Nithin%20Rao%20Koluguri%20and%20Krishna%20C.%20Puvvada%20and%20Jagadeesh%20Balam%20and%20Boris%20Ginsburg%0AAbstract%3A%20%20%20We%20propose%20Sortformer%2C%20a%20novel%20neural%20model%20for%20speaker%20diarization%2C%20trained%0Awith%20unconventional%20objectives%20compared%20to%20existing%20end-to-end%20diarization%0Amodels.%20The%20permutation%20problem%20in%20speaker%20diarization%20has%20long%20been%20regarded%0Aas%20a%20critical%20challenge.%20Most%20prior%20end-to-end%20diarization%20systems%20employ%0Apermutation%20invariant%20loss%20%28PIL%29%2C%20which%20optimizes%20for%20the%20permutation%20that%0Ayields%20the%20lowest%20error.%20In%20contrast%2C%20we%20introduce%20Sort%20Loss%2C%20which%20enables%20a%0Adiarization%20model%20to%20autonomously%20resolve%20permutation%2C%20with%20or%20without%20PIL.%20We%0Ademonstrate%20that%20combining%20Sort%20Loss%20and%20PIL%20achieves%20performance%20competitive%0Awith%20state-of-the-art%20end-to-end%20diarization%20models%20trained%20exclusively%20with%0APIL.%20Crucially%2C%20we%20present%20a%20streamlined%20multispeaker%20ASR%20architecture%20that%0Aleverages%20Sortformer%20as%20a%20speaker%20supervision%20model%2C%20embedding%20speaker%20label%0Aestimation%20within%20the%20ASR%20encoder%20state%20using%20a%20sinusoidal%20kernel%20function.%0AThis%20approach%20resolves%20the%20speaker%20permutation%20problem%20through%20sorted%0Aobjectives%2C%20effectively%20bridging%20speaker-label%20timestamps%20and%20speaker%20tokens.%0AIn%20our%20experiments%2C%20we%20show%20that%20the%20proposed%20multispeaker%20ASR%20architecture%2C%0Aenhanced%20with%20speaker%20supervision%2C%20improves%20performance%20via%20adapter%20techniques.%0ACode%20and%20trained%20models%20will%20be%20made%20publicly%20available%20via%20the%20NVIDIA%20NeMo%0Aframework%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSortformer%253A%2520Seamless%2520Integration%2520of%2520Speaker%2520Diarization%2520and%2520ASR%2520by%250A%2520%2520Bridging%2520Timestamps%2520and%2520Tokens%26entry.906535625%3DTaejin%2520Park%2520and%2520Ivan%2520Medennikov%2520and%2520Kunal%2520Dhawan%2520and%2520Weiqing%2520Wang%2520and%2520He%2520Huang%2520and%2520Nithin%2520Rao%2520Koluguri%2520and%2520Krishna%2520C.%2520Puvvada%2520and%2520Jagadeesh%2520Balam%2520and%2520Boris%2520Ginsburg%26entry.1292438233%3D%2520%2520We%2520propose%2520Sortformer%252C%2520a%2520novel%2520neural%2520model%2520for%2520speaker%2520diarization%252C%2520trained%250Awith%2520unconventional%2520objectives%2520compared%2520to%2520existing%2520end-to-end%2520diarization%250Amodels.%2520The%2520permutation%2520problem%2520in%2520speaker%2520diarization%2520has%2520long%2520been%2520regarded%250Aas%2520a%2520critical%2520challenge.%2520Most%2520prior%2520end-to-end%2520diarization%2520systems%2520employ%250Apermutation%2520invariant%2520loss%2520%2528PIL%2529%252C%2520which%2520optimizes%2520for%2520the%2520permutation%2520that%250Ayields%2520the%2520lowest%2520error.%2520In%2520contrast%252C%2520we%2520introduce%2520Sort%2520Loss%252C%2520which%2520enables%2520a%250Adiarization%2520model%2520to%2520autonomously%2520resolve%2520permutation%252C%2520with%2520or%2520without%2520PIL.%2520We%250Ademonstrate%2520that%2520combining%2520Sort%2520Loss%2520and%2520PIL%2520achieves%2520performance%2520competitive%250Awith%2520state-of-the-art%2520end-to-end%2520diarization%2520models%2520trained%2520exclusively%2520with%250APIL.%2520Crucially%252C%2520we%2520present%2520a%2520streamlined%2520multispeaker%2520ASR%2520architecture%2520that%250Aleverages%2520Sortformer%2520as%2520a%2520speaker%2520supervision%2520model%252C%2520embedding%2520speaker%2520label%250Aestimation%2520within%2520the%2520ASR%2520encoder%2520state%2520using%2520a%2520sinusoidal%2520kernel%2520function.%250AThis%2520approach%2520resolves%2520the%2520speaker%2520permutation%2520problem%2520through%2520sorted%250Aobjectives%252C%2520effectively%2520bridging%2520speaker-label%2520timestamps%2520and%2520speaker%2520tokens.%250AIn%2520our%2520experiments%252C%2520we%2520show%2520that%2520the%2520proposed%2520multispeaker%2520ASR%2520architecture%252C%250Aenhanced%2520with%2520speaker%2520supervision%252C%2520improves%2520performance%2520via%2520adapter%2520techniques.%250ACode%2520and%2520trained%2520models%2520will%2520be%2520made%2520publicly%2520available%2520via%2520the%2520NVIDIA%2520NeMo%250Aframework%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sortformer%3A%20Seamless%20Integration%20of%20Speaker%20Diarization%20and%20ASR%20by%0A%20%20Bridging%20Timestamps%20and%20Tokens&entry.906535625=Taejin%20Park%20and%20Ivan%20Medennikov%20and%20Kunal%20Dhawan%20and%20Weiqing%20Wang%20and%20He%20Huang%20and%20Nithin%20Rao%20Koluguri%20and%20Krishna%20C.%20Puvvada%20and%20Jagadeesh%20Balam%20and%20Boris%20Ginsburg&entry.1292438233=%20%20We%20propose%20Sortformer%2C%20a%20novel%20neural%20model%20for%20speaker%20diarization%2C%20trained%0Awith%20unconventional%20objectives%20compared%20to%20existing%20end-to-end%20diarization%0Amodels.%20The%20permutation%20problem%20in%20speaker%20diarization%20has%20long%20been%20regarded%0Aas%20a%20critical%20challenge.%20Most%20prior%20end-to-end%20diarization%20systems%20employ%0Apermutation%20invariant%20loss%20%28PIL%29%2C%20which%20optimizes%20for%20the%20permutation%20that%0Ayields%20the%20lowest%20error.%20In%20contrast%2C%20we%20introduce%20Sort%20Loss%2C%20which%20enables%20a%0Adiarization%20model%20to%20autonomously%20resolve%20permutation%2C%20with%20or%20without%20PIL.%20We%0Ademonstrate%20that%20combining%20Sort%20Loss%20and%20PIL%20achieves%20performance%20competitive%0Awith%20state-of-the-art%20end-to-end%20diarization%20models%20trained%20exclusively%20with%0APIL.%20Crucially%2C%20we%20present%20a%20streamlined%20multispeaker%20ASR%20architecture%20that%0Aleverages%20Sortformer%20as%20a%20speaker%20supervision%20model%2C%20embedding%20speaker%20label%0Aestimation%20within%20the%20ASR%20encoder%20state%20using%20a%20sinusoidal%20kernel%20function.%0AThis%20approach%20resolves%20the%20speaker%20permutation%20problem%20through%20sorted%0Aobjectives%2C%20effectively%20bridging%20speaker-label%20timestamps%20and%20speaker%20tokens.%0AIn%20our%20experiments%2C%20we%20show%20that%20the%20proposed%20multispeaker%20ASR%20architecture%2C%0Aenhanced%20with%20speaker%20supervision%2C%20improves%20performance%20via%20adapter%20techniques.%0ACode%20and%20trained%20models%20will%20be%20made%20publicly%20available%20via%20the%20NVIDIA%20NeMo%0Aframework%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06656v1&entry.124074799=Read"},
{"title": "What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and\n  Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence", "author": "Robert Kaufman and Aaron Broukhim and David Kirsh and Nadir Weibel", "abstract": "  Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems.\n", "link": "http://arxiv.org/abs/2409.05731v2", "date": "2024-09-10", "relevancy": 1.8483, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.448}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Did%20My%20Car%20Say%3F%20Impact%20of%20Autonomous%20Vehicle%20Explanation%20Errors%20and%0A%20%20Driving%20Context%20On%20Comfort%2C%20Reliance%2C%20Satisfaction%2C%20and%20Driving%20Confidence&body=Title%3A%20What%20Did%20My%20Car%20Say%3F%20Impact%20of%20Autonomous%20Vehicle%20Explanation%20Errors%20and%0A%20%20Driving%20Context%20On%20Comfort%2C%20Reliance%2C%20Satisfaction%2C%20and%20Driving%20Confidence%0AAuthor%3A%20Robert%20Kaufman%20and%20Aaron%20Broukhim%20and%20David%20Kirsh%20and%20Nadir%20Weibel%0AAbstract%3A%20%20%20Explanations%20for%20autonomous%20vehicle%20%28AV%29%20decisions%20may%20build%20trust%2C%20however%2C%0Aexplanations%20can%20contain%20errors.%20In%20a%20simulated%20driving%20study%20%28n%20%3D%20232%29%2C%20we%0Atested%20how%20AV%20explanation%20errors%2C%20driving%20context%20characteristics%20%28perceived%0Aharm%20and%20driving%20difficulty%29%2C%20and%20personal%20traits%20%28prior%20trust%20and%20expertise%29%0Aaffected%20a%20passenger%27s%20comfort%20in%20relying%20on%20an%20AV%2C%20preference%20for%20control%2C%0Aconfidence%20in%20the%20AV%27s%20ability%2C%20and%20explanation%20satisfaction.%20Errors%20negatively%0Aaffected%20all%20outcomes.%20Surprisingly%2C%20despite%20identical%20driving%2C%20explanation%0Aerrors%20reduced%20ratings%20of%20the%20AV%27s%20driving%20ability.%20Severity%20and%20potential%20harm%0Aamplified%20the%20negative%20impact%20of%20errors.%20Contextual%20harm%20and%20driving%20difficulty%0Adirectly%20impacted%20outcome%20ratings%20and%20influenced%20the%20relationship%20between%0Aerrors%20and%20outcomes.%20Prior%20trust%20and%20expertise%20were%20positively%20associated%20with%0Aoutcome%20ratings.%20Results%20emphasize%20the%20need%20for%20accurate%2C%20contextually%0Aadaptive%2C%20and%20personalized%20AV%20explanations%20to%20foster%20trust%2C%20reliance%2C%0Asatisfaction%2C%20and%20confidence.%20We%20conclude%20with%20design%2C%20research%2C%20and%20deployment%0Arecommendations%20for%20trustworthy%20AV%20explanation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.05731v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Did%2520My%2520Car%2520Say%253F%2520Impact%2520of%2520Autonomous%2520Vehicle%2520Explanation%2520Errors%2520and%250A%2520%2520Driving%2520Context%2520On%2520Comfort%252C%2520Reliance%252C%2520Satisfaction%252C%2520and%2520Driving%2520Confidence%26entry.906535625%3DRobert%2520Kaufman%2520and%2520Aaron%2520Broukhim%2520and%2520David%2520Kirsh%2520and%2520Nadir%2520Weibel%26entry.1292438233%3D%2520%2520Explanations%2520for%2520autonomous%2520vehicle%2520%2528AV%2529%2520decisions%2520may%2520build%2520trust%252C%2520however%252C%250Aexplanations%2520can%2520contain%2520errors.%2520In%2520a%2520simulated%2520driving%2520study%2520%2528n%2520%253D%2520232%2529%252C%2520we%250Atested%2520how%2520AV%2520explanation%2520errors%252C%2520driving%2520context%2520characteristics%2520%2528perceived%250Aharm%2520and%2520driving%2520difficulty%2529%252C%2520and%2520personal%2520traits%2520%2528prior%2520trust%2520and%2520expertise%2529%250Aaffected%2520a%2520passenger%2527s%2520comfort%2520in%2520relying%2520on%2520an%2520AV%252C%2520preference%2520for%2520control%252C%250Aconfidence%2520in%2520the%2520AV%2527s%2520ability%252C%2520and%2520explanation%2520satisfaction.%2520Errors%2520negatively%250Aaffected%2520all%2520outcomes.%2520Surprisingly%252C%2520despite%2520identical%2520driving%252C%2520explanation%250Aerrors%2520reduced%2520ratings%2520of%2520the%2520AV%2527s%2520driving%2520ability.%2520Severity%2520and%2520potential%2520harm%250Aamplified%2520the%2520negative%2520impact%2520of%2520errors.%2520Contextual%2520harm%2520and%2520driving%2520difficulty%250Adirectly%2520impacted%2520outcome%2520ratings%2520and%2520influenced%2520the%2520relationship%2520between%250Aerrors%2520and%2520outcomes.%2520Prior%2520trust%2520and%2520expertise%2520were%2520positively%2520associated%2520with%250Aoutcome%2520ratings.%2520Results%2520emphasize%2520the%2520need%2520for%2520accurate%252C%2520contextually%250Aadaptive%252C%2520and%2520personalized%2520AV%2520explanations%2520to%2520foster%2520trust%252C%2520reliance%252C%250Asatisfaction%252C%2520and%2520confidence.%2520We%2520conclude%2520with%2520design%252C%2520research%252C%2520and%2520deployment%250Arecommendations%2520for%2520trustworthy%2520AV%2520explanation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.05731v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Did%20My%20Car%20Say%3F%20Impact%20of%20Autonomous%20Vehicle%20Explanation%20Errors%20and%0A%20%20Driving%20Context%20On%20Comfort%2C%20Reliance%2C%20Satisfaction%2C%20and%20Driving%20Confidence&entry.906535625=Robert%20Kaufman%20and%20Aaron%20Broukhim%20and%20David%20Kirsh%20and%20Nadir%20Weibel&entry.1292438233=%20%20Explanations%20for%20autonomous%20vehicle%20%28AV%29%20decisions%20may%20build%20trust%2C%20however%2C%0Aexplanations%20can%20contain%20errors.%20In%20a%20simulated%20driving%20study%20%28n%20%3D%20232%29%2C%20we%0Atested%20how%20AV%20explanation%20errors%2C%20driving%20context%20characteristics%20%28perceived%0Aharm%20and%20driving%20difficulty%29%2C%20and%20personal%20traits%20%28prior%20trust%20and%20expertise%29%0Aaffected%20a%20passenger%27s%20comfort%20in%20relying%20on%20an%20AV%2C%20preference%20for%20control%2C%0Aconfidence%20in%20the%20AV%27s%20ability%2C%20and%20explanation%20satisfaction.%20Errors%20negatively%0Aaffected%20all%20outcomes.%20Surprisingly%2C%20despite%20identical%20driving%2C%20explanation%0Aerrors%20reduced%20ratings%20of%20the%20AV%27s%20driving%20ability.%20Severity%20and%20potential%20harm%0Aamplified%20the%20negative%20impact%20of%20errors.%20Contextual%20harm%20and%20driving%20difficulty%0Adirectly%20impacted%20outcome%20ratings%20and%20influenced%20the%20relationship%20between%0Aerrors%20and%20outcomes.%20Prior%20trust%20and%20expertise%20were%20positively%20associated%20with%0Aoutcome%20ratings.%20Results%20emphasize%20the%20need%20for%20accurate%2C%20contextually%0Aadaptive%2C%20and%20personalized%20AV%20explanations%20to%20foster%20trust%2C%20reliance%2C%0Asatisfaction%2C%20and%20confidence.%20We%20conclude%20with%20design%2C%20research%2C%20and%20deployment%0Arecommendations%20for%20trustworthy%20AV%20explanation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.05731v2&entry.124074799=Read"},
{"title": "Technical Report of Mobile Manipulator Robot for Industrial Environments", "author": "Erfan Amoozad Khalili and Kiarash Ghasemzadeh and Hossein Gohari and Mohammadreza Jafari and Matin Jamshidi and Mahdi Khaksar and AmirReza AkramiFard and Mana Hatamzadeh and Saba Sadeghi and Mohammad Hossein Moaiyeri", "abstract": "  This paper presents the development of the Auriga @Work robot, designed by\nthe Robotics and Intelligent Automation Lab at Shahid Beheshti University,\nDepartment of Electrical Engineering, for the RoboCup 2024 competition. The\nrobot is tailored for industrial applications, focusing on enhancing efficiency\nin repetitive or hazardous environments. It is equipped with a 4-wheel Mecanum\ndrive system for omnidirectional mobility and a 5-degree-of-freedom manipulator\narm with a custom 3D-printed gripper for object manipulation and navigation\ntasks. The robot's electronics are powered by custom-designed boards utilizing\nESP32 microcontrollers and an Nvidia Jetson Nano for real-time control and\ndecision-making. The key software stack integrates Hector SLAM for mapping, the\nA* algorithm for path planning, and YOLO for object detection, along with\nadvanced sensor fusion for improved navigation and collision avoidance.\n", "link": "http://arxiv.org/abs/2409.06693v1", "date": "2024-09-10", "relevancy": 1.6477, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5607}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5539}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Technical%20Report%20of%20Mobile%20Manipulator%20Robot%20for%20Industrial%20Environments&body=Title%3A%20Technical%20Report%20of%20Mobile%20Manipulator%20Robot%20for%20Industrial%20Environments%0AAuthor%3A%20Erfan%20Amoozad%20Khalili%20and%20Kiarash%20Ghasemzadeh%20and%20Hossein%20Gohari%20and%20Mohammadreza%20Jafari%20and%20Matin%20Jamshidi%20and%20Mahdi%20Khaksar%20and%20AmirReza%20AkramiFard%20and%20Mana%20Hatamzadeh%20and%20Saba%20Sadeghi%20and%20Mohammad%20Hossein%20Moaiyeri%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20development%20of%20the%20Auriga%20%40Work%20robot%2C%20designed%20by%0Athe%20Robotics%20and%20Intelligent%20Automation%20Lab%20at%20Shahid%20Beheshti%20University%2C%0ADepartment%20of%20Electrical%20Engineering%2C%20for%20the%20RoboCup%202024%20competition.%20The%0Arobot%20is%20tailored%20for%20industrial%20applications%2C%20focusing%20on%20enhancing%20efficiency%0Ain%20repetitive%20or%20hazardous%20environments.%20It%20is%20equipped%20with%20a%204-wheel%20Mecanum%0Adrive%20system%20for%20omnidirectional%20mobility%20and%20a%205-degree-of-freedom%20manipulator%0Aarm%20with%20a%20custom%203D-printed%20gripper%20for%20object%20manipulation%20and%20navigation%0Atasks.%20The%20robot%27s%20electronics%20are%20powered%20by%20custom-designed%20boards%20utilizing%0AESP32%20microcontrollers%20and%20an%20Nvidia%20Jetson%20Nano%20for%20real-time%20control%20and%0Adecision-making.%20The%20key%20software%20stack%20integrates%20Hector%20SLAM%20for%20mapping%2C%20the%0AA%2A%20algorithm%20for%20path%20planning%2C%20and%20YOLO%20for%20object%20detection%2C%20along%20with%0Aadvanced%20sensor%20fusion%20for%20improved%20navigation%20and%20collision%20avoidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTechnical%2520Report%2520of%2520Mobile%2520Manipulator%2520Robot%2520for%2520Industrial%2520Environments%26entry.906535625%3DErfan%2520Amoozad%2520Khalili%2520and%2520Kiarash%2520Ghasemzadeh%2520and%2520Hossein%2520Gohari%2520and%2520Mohammadreza%2520Jafari%2520and%2520Matin%2520Jamshidi%2520and%2520Mahdi%2520Khaksar%2520and%2520AmirReza%2520AkramiFard%2520and%2520Mana%2520Hatamzadeh%2520and%2520Saba%2520Sadeghi%2520and%2520Mohammad%2520Hossein%2520Moaiyeri%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520development%2520of%2520the%2520Auriga%2520%2540Work%2520robot%252C%2520designed%2520by%250Athe%2520Robotics%2520and%2520Intelligent%2520Automation%2520Lab%2520at%2520Shahid%2520Beheshti%2520University%252C%250ADepartment%2520of%2520Electrical%2520Engineering%252C%2520for%2520the%2520RoboCup%25202024%2520competition.%2520The%250Arobot%2520is%2520tailored%2520for%2520industrial%2520applications%252C%2520focusing%2520on%2520enhancing%2520efficiency%250Ain%2520repetitive%2520or%2520hazardous%2520environments.%2520It%2520is%2520equipped%2520with%2520a%25204-wheel%2520Mecanum%250Adrive%2520system%2520for%2520omnidirectional%2520mobility%2520and%2520a%25205-degree-of-freedom%2520manipulator%250Aarm%2520with%2520a%2520custom%25203D-printed%2520gripper%2520for%2520object%2520manipulation%2520and%2520navigation%250Atasks.%2520The%2520robot%2527s%2520electronics%2520are%2520powered%2520by%2520custom-designed%2520boards%2520utilizing%250AESP32%2520microcontrollers%2520and%2520an%2520Nvidia%2520Jetson%2520Nano%2520for%2520real-time%2520control%2520and%250Adecision-making.%2520The%2520key%2520software%2520stack%2520integrates%2520Hector%2520SLAM%2520for%2520mapping%252C%2520the%250AA%252A%2520algorithm%2520for%2520path%2520planning%252C%2520and%2520YOLO%2520for%2520object%2520detection%252C%2520along%2520with%250Aadvanced%2520sensor%2520fusion%2520for%2520improved%2520navigation%2520and%2520collision%2520avoidance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20Report%20of%20Mobile%20Manipulator%20Robot%20for%20Industrial%20Environments&entry.906535625=Erfan%20Amoozad%20Khalili%20and%20Kiarash%20Ghasemzadeh%20and%20Hossein%20Gohari%20and%20Mohammadreza%20Jafari%20and%20Matin%20Jamshidi%20and%20Mahdi%20Khaksar%20and%20AmirReza%20AkramiFard%20and%20Mana%20Hatamzadeh%20and%20Saba%20Sadeghi%20and%20Mohammad%20Hossein%20Moaiyeri&entry.1292438233=%20%20This%20paper%20presents%20the%20development%20of%20the%20Auriga%20%40Work%20robot%2C%20designed%20by%0Athe%20Robotics%20and%20Intelligent%20Automation%20Lab%20at%20Shahid%20Beheshti%20University%2C%0ADepartment%20of%20Electrical%20Engineering%2C%20for%20the%20RoboCup%202024%20competition.%20The%0Arobot%20is%20tailored%20for%20industrial%20applications%2C%20focusing%20on%20enhancing%20efficiency%0Ain%20repetitive%20or%20hazardous%20environments.%20It%20is%20equipped%20with%20a%204-wheel%20Mecanum%0Adrive%20system%20for%20omnidirectional%20mobility%20and%20a%205-degree-of-freedom%20manipulator%0Aarm%20with%20a%20custom%203D-printed%20gripper%20for%20object%20manipulation%20and%20navigation%0Atasks.%20The%20robot%27s%20electronics%20are%20powered%20by%20custom-designed%20boards%20utilizing%0AESP32%20microcontrollers%20and%20an%20Nvidia%20Jetson%20Nano%20for%20real-time%20control%20and%0Adecision-making.%20The%20key%20software%20stack%20integrates%20Hector%20SLAM%20for%20mapping%2C%20the%0AA%2A%20algorithm%20for%20path%20planning%2C%20and%20YOLO%20for%20object%20detection%2C%20along%20with%0Aadvanced%20sensor%20fusion%20for%20improved%20navigation%20and%20collision%20avoidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06693v1&entry.124074799=Read"},
{"title": "HybridFC: A Hybrid Fact-Checking Approach for Knowledge Graphs", "author": "Umair Qudus and Michael Roeder and Muhammad Saleem and Axel-Cyrille Ngonga Ngomo", "abstract": "  We consider fact-checking approaches that aim to predict the veracity of\nassertions in knowledge graphs. Five main categories of fact-checking\napproaches for knowledge graphs have been proposed in the recent literature, of\nwhich each is subject to partially overlapping limitations. In particular,\ncurrent text-based approaches are limited by manual feature engineering.\nPath-based and rule-based approaches are limited by their exclusive use of\nknowledge graphs as background knowledge, and embedding-based approaches suffer\nfrom low accuracy scores on current fact-checking tasks. We propose a hybrid\napproach -- dubbed HybridFC -- that exploits the diversity of existing\ncategories of fact-checking approaches within an ensemble learning setting to\nachieve a significantly better prediction performance. In particular, our\napproach outperforms the state of the art by 0.14 to 0.27 in terms of Area\nUnder the Receiver Operating Characteristic curve on the FactBench dataset. Our\ncode is open-source and can be found at https://github.com/dice-group/HybridFC.\n", "link": "http://arxiv.org/abs/2409.06692v1", "date": "2024-09-10", "relevancy": 1.3304, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4472}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.443}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridFC%3A%20A%20Hybrid%20Fact-Checking%20Approach%20for%20Knowledge%20Graphs&body=Title%3A%20HybridFC%3A%20A%20Hybrid%20Fact-Checking%20Approach%20for%20Knowledge%20Graphs%0AAuthor%3A%20Umair%20Qudus%20and%20Michael%20Roeder%20and%20Muhammad%20Saleem%20and%20Axel-Cyrille%20Ngonga%20Ngomo%0AAbstract%3A%20%20%20We%20consider%20fact-checking%20approaches%20that%20aim%20to%20predict%20the%20veracity%20of%0Aassertions%20in%20knowledge%20graphs.%20Five%20main%20categories%20of%20fact-checking%0Aapproaches%20for%20knowledge%20graphs%20have%20been%20proposed%20in%20the%20recent%20literature%2C%20of%0Awhich%20each%20is%20subject%20to%20partially%20overlapping%20limitations.%20In%20particular%2C%0Acurrent%20text-based%20approaches%20are%20limited%20by%20manual%20feature%20engineering.%0APath-based%20and%20rule-based%20approaches%20are%20limited%20by%20their%20exclusive%20use%20of%0Aknowledge%20graphs%20as%20background%20knowledge%2C%20and%20embedding-based%20approaches%20suffer%0Afrom%20low%20accuracy%20scores%20on%20current%20fact-checking%20tasks.%20We%20propose%20a%20hybrid%0Aapproach%20--%20dubbed%20HybridFC%20--%20that%20exploits%20the%20diversity%20of%20existing%0Acategories%20of%20fact-checking%20approaches%20within%20an%20ensemble%20learning%20setting%20to%0Aachieve%20a%20significantly%20better%20prediction%20performance.%20In%20particular%2C%20our%0Aapproach%20outperforms%20the%20state%20of%20the%20art%20by%200.14%20to%200.27%20in%20terms%20of%20Area%0AUnder%20the%20Receiver%20Operating%20Characteristic%20curve%20on%20the%20FactBench%20dataset.%20Our%0Acode%20is%20open-source%20and%20can%20be%20found%20at%20https%3A//github.com/dice-group/HybridFC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridFC%253A%2520A%2520Hybrid%2520Fact-Checking%2520Approach%2520for%2520Knowledge%2520Graphs%26entry.906535625%3DUmair%2520Qudus%2520and%2520Michael%2520Roeder%2520and%2520Muhammad%2520Saleem%2520and%2520Axel-Cyrille%2520Ngonga%2520Ngomo%26entry.1292438233%3D%2520%2520We%2520consider%2520fact-checking%2520approaches%2520that%2520aim%2520to%2520predict%2520the%2520veracity%2520of%250Aassertions%2520in%2520knowledge%2520graphs.%2520Five%2520main%2520categories%2520of%2520fact-checking%250Aapproaches%2520for%2520knowledge%2520graphs%2520have%2520been%2520proposed%2520in%2520the%2520recent%2520literature%252C%2520of%250Awhich%2520each%2520is%2520subject%2520to%2520partially%2520overlapping%2520limitations.%2520In%2520particular%252C%250Acurrent%2520text-based%2520approaches%2520are%2520limited%2520by%2520manual%2520feature%2520engineering.%250APath-based%2520and%2520rule-based%2520approaches%2520are%2520limited%2520by%2520their%2520exclusive%2520use%2520of%250Aknowledge%2520graphs%2520as%2520background%2520knowledge%252C%2520and%2520embedding-based%2520approaches%2520suffer%250Afrom%2520low%2520accuracy%2520scores%2520on%2520current%2520fact-checking%2520tasks.%2520We%2520propose%2520a%2520hybrid%250Aapproach%2520--%2520dubbed%2520HybridFC%2520--%2520that%2520exploits%2520the%2520diversity%2520of%2520existing%250Acategories%2520of%2520fact-checking%2520approaches%2520within%2520an%2520ensemble%2520learning%2520setting%2520to%250Aachieve%2520a%2520significantly%2520better%2520prediction%2520performance.%2520In%2520particular%252C%2520our%250Aapproach%2520outperforms%2520the%2520state%2520of%2520the%2520art%2520by%25200.14%2520to%25200.27%2520in%2520terms%2520of%2520Area%250AUnder%2520the%2520Receiver%2520Operating%2520Characteristic%2520curve%2520on%2520the%2520FactBench%2520dataset.%2520Our%250Acode%2520is%2520open-source%2520and%2520can%2520be%2520found%2520at%2520https%253A//github.com/dice-group/HybridFC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridFC%3A%20A%20Hybrid%20Fact-Checking%20Approach%20for%20Knowledge%20Graphs&entry.906535625=Umair%20Qudus%20and%20Michael%20Roeder%20and%20Muhammad%20Saleem%20and%20Axel-Cyrille%20Ngonga%20Ngomo&entry.1292438233=%20%20We%20consider%20fact-checking%20approaches%20that%20aim%20to%20predict%20the%20veracity%20of%0Aassertions%20in%20knowledge%20graphs.%20Five%20main%20categories%20of%20fact-checking%0Aapproaches%20for%20knowledge%20graphs%20have%20been%20proposed%20in%20the%20recent%20literature%2C%20of%0Awhich%20each%20is%20subject%20to%20partially%20overlapping%20limitations.%20In%20particular%2C%0Acurrent%20text-based%20approaches%20are%20limited%20by%20manual%20feature%20engineering.%0APath-based%20and%20rule-based%20approaches%20are%20limited%20by%20their%20exclusive%20use%20of%0Aknowledge%20graphs%20as%20background%20knowledge%2C%20and%20embedding-based%20approaches%20suffer%0Afrom%20low%20accuracy%20scores%20on%20current%20fact-checking%20tasks.%20We%20propose%20a%20hybrid%0Aapproach%20--%20dubbed%20HybridFC%20--%20that%20exploits%20the%20diversity%20of%20existing%0Acategories%20of%20fact-checking%20approaches%20within%20an%20ensemble%20learning%20setting%20to%0Aachieve%20a%20significantly%20better%20prediction%20performance.%20In%20particular%2C%20our%0Aapproach%20outperforms%20the%20state%20of%20the%20art%20by%200.14%20to%200.27%20in%20terms%20of%20Area%0AUnder%20the%20Receiver%20Operating%20Characteristic%20curve%20on%20the%20FactBench%20dataset.%20Our%0Acode%20is%20open-source%20and%20can%20be%20found%20at%20https%3A//github.com/dice-group/HybridFC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06692v1&entry.124074799=Read"},
{"title": "Extending Explainable Ensemble Trees (E2Tree) to regression contexts", "author": "Massimo Aria and Agostino Gnasso and Carmela Iorio and Marjolein Fokkema", "abstract": "  Ensemble methods such as random forests have transformed the landscape of\nsupervised learning, offering highly accurate prediction through the\naggregation of multiple weak learners. However, despite their effectiveness,\nthese methods often lack transparency, impeding users' comprehension of how RF\nmodels arrive at their predictions. Explainable ensemble trees (E2Tree) is a\nnovel methodology for explaining random forests, that provides a graphical\nrepresentation of the relationship between response variables and predictors. A\nstriking characteristic of E2Tree is that it not only accounts for the effects\nof predictor variables on the response but also accounts for associations\nbetween the predictor variables through the computation and use of\ndissimilarity measures. The E2Tree methodology was initially proposed for use\nin classification tasks. In this paper, we extend the methodology to encompass\nregression contexts. To demonstrate the explanatory power of the proposed\nalgorithm, we illustrate its use on real-world datasets.\n", "link": "http://arxiv.org/abs/2409.06439v1", "date": "2024-09-10", "relevancy": 1.7117, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4367}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4285}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extending%20Explainable%20Ensemble%20Trees%20%28E2Tree%29%20to%20regression%20contexts&body=Title%3A%20Extending%20Explainable%20Ensemble%20Trees%20%28E2Tree%29%20to%20regression%20contexts%0AAuthor%3A%20Massimo%20Aria%20and%20Agostino%20Gnasso%20and%20Carmela%20Iorio%20and%20Marjolein%20Fokkema%0AAbstract%3A%20%20%20Ensemble%20methods%20such%20as%20random%20forests%20have%20transformed%20the%20landscape%20of%0Asupervised%20learning%2C%20offering%20highly%20accurate%20prediction%20through%20the%0Aaggregation%20of%20multiple%20weak%20learners.%20However%2C%20despite%20their%20effectiveness%2C%0Athese%20methods%20often%20lack%20transparency%2C%20impeding%20users%27%20comprehension%20of%20how%20RF%0Amodels%20arrive%20at%20their%20predictions.%20Explainable%20ensemble%20trees%20%28E2Tree%29%20is%20a%0Anovel%20methodology%20for%20explaining%20random%20forests%2C%20that%20provides%20a%20graphical%0Arepresentation%20of%20the%20relationship%20between%20response%20variables%20and%20predictors.%20A%0Astriking%20characteristic%20of%20E2Tree%20is%20that%20it%20not%20only%20accounts%20for%20the%20effects%0Aof%20predictor%20variables%20on%20the%20response%20but%20also%20accounts%20for%20associations%0Abetween%20the%20predictor%20variables%20through%20the%20computation%20and%20use%20of%0Adissimilarity%20measures.%20The%20E2Tree%20methodology%20was%20initially%20proposed%20for%20use%0Ain%20classification%20tasks.%20In%20this%20paper%2C%20we%20extend%20the%20methodology%20to%20encompass%0Aregression%20contexts.%20To%20demonstrate%20the%20explanatory%20power%20of%20the%20proposed%0Aalgorithm%2C%20we%20illustrate%20its%20use%20on%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtending%2520Explainable%2520Ensemble%2520Trees%2520%2528E2Tree%2529%2520to%2520regression%2520contexts%26entry.906535625%3DMassimo%2520Aria%2520and%2520Agostino%2520Gnasso%2520and%2520Carmela%2520Iorio%2520and%2520Marjolein%2520Fokkema%26entry.1292438233%3D%2520%2520Ensemble%2520methods%2520such%2520as%2520random%2520forests%2520have%2520transformed%2520the%2520landscape%2520of%250Asupervised%2520learning%252C%2520offering%2520highly%2520accurate%2520prediction%2520through%2520the%250Aaggregation%2520of%2520multiple%2520weak%2520learners.%2520However%252C%2520despite%2520their%2520effectiveness%252C%250Athese%2520methods%2520often%2520lack%2520transparency%252C%2520impeding%2520users%2527%2520comprehension%2520of%2520how%2520RF%250Amodels%2520arrive%2520at%2520their%2520predictions.%2520Explainable%2520ensemble%2520trees%2520%2528E2Tree%2529%2520is%2520a%250Anovel%2520methodology%2520for%2520explaining%2520random%2520forests%252C%2520that%2520provides%2520a%2520graphical%250Arepresentation%2520of%2520the%2520relationship%2520between%2520response%2520variables%2520and%2520predictors.%2520A%250Astriking%2520characteristic%2520of%2520E2Tree%2520is%2520that%2520it%2520not%2520only%2520accounts%2520for%2520the%2520effects%250Aof%2520predictor%2520variables%2520on%2520the%2520response%2520but%2520also%2520accounts%2520for%2520associations%250Abetween%2520the%2520predictor%2520variables%2520through%2520the%2520computation%2520and%2520use%2520of%250Adissimilarity%2520measures.%2520The%2520E2Tree%2520methodology%2520was%2520initially%2520proposed%2520for%2520use%250Ain%2520classification%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520methodology%2520to%2520encompass%250Aregression%2520contexts.%2520To%2520demonstrate%2520the%2520explanatory%2520power%2520of%2520the%2520proposed%250Aalgorithm%252C%2520we%2520illustrate%2520its%2520use%2520on%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extending%20Explainable%20Ensemble%20Trees%20%28E2Tree%29%20to%20regression%20contexts&entry.906535625=Massimo%20Aria%20and%20Agostino%20Gnasso%20and%20Carmela%20Iorio%20and%20Marjolein%20Fokkema&entry.1292438233=%20%20Ensemble%20methods%20such%20as%20random%20forests%20have%20transformed%20the%20landscape%20of%0Asupervised%20learning%2C%20offering%20highly%20accurate%20prediction%20through%20the%0Aaggregation%20of%20multiple%20weak%20learners.%20However%2C%20despite%20their%20effectiveness%2C%0Athese%20methods%20often%20lack%20transparency%2C%20impeding%20users%27%20comprehension%20of%20how%20RF%0Amodels%20arrive%20at%20their%20predictions.%20Explainable%20ensemble%20trees%20%28E2Tree%29%20is%20a%0Anovel%20methodology%20for%20explaining%20random%20forests%2C%20that%20provides%20a%20graphical%0Arepresentation%20of%20the%20relationship%20between%20response%20variables%20and%20predictors.%20A%0Astriking%20characteristic%20of%20E2Tree%20is%20that%20it%20not%20only%20accounts%20for%20the%20effects%0Aof%20predictor%20variables%20on%20the%20response%20but%20also%20accounts%20for%20associations%0Abetween%20the%20predictor%20variables%20through%20the%20computation%20and%20use%20of%0Adissimilarity%20measures.%20The%20E2Tree%20methodology%20was%20initially%20proposed%20for%20use%0Ain%20classification%20tasks.%20In%20this%20paper%2C%20we%20extend%20the%20methodology%20to%20encompass%0Aregression%20contexts.%20To%20demonstrate%20the%20explanatory%20power%20of%20the%20proposed%0Aalgorithm%2C%20we%20illustrate%20its%20use%20on%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06439v1&entry.124074799=Read"},
{"title": "MENSA: A Multi-Event Network for Survival Analysis under Informative\n  Censoring", "author": "Christian Marius Lillelund and Ali Hossein Gharari Foomani and Weijie Sun and Shi-ang Qi and Russell Greiner", "abstract": "  Given an instance, a multi-event survival model predicts the time until that\ninstance experiences each of several different events. These events are not\nmutually exclusive and there are often statistical dependencies between them.\nThere are relatively few multi-event survival results, most focusing on\nproducing a simple risk score, rather than the time-to-event itself. To\novercome these issues, we introduce MENSA, a novel, deep learning approach for\nmulti-event survival analysis that can jointly learn representations of the\ninput covariates and the dependence structure between events. As a practical\nmotivation for multi-event survival analysis, we consider the problem of\npredicting the time until a patient with amyotrophic lateral sclerosis (ALS)\nloses various physical functions, i.e., the ability to speak, swallow, write,\nor walk. When estimating when a patient is no longer able to swallow, our\napproach achieves an L1-Margin loss of 278.8 days, compared to 355.2 days when\nmodeling each event separately. In addition, we also evaluate our approach in\nsingle-event and competing risk scenarios by modeling the censoring and event\ndistributions as equal contributing factors in the optimization process, and\nshow that our approach performs well across multiple benchmark datasets. The\nsource code is available at: https://github.com/thecml/mensa\n", "link": "http://arxiv.org/abs/2409.06525v1", "date": "2024-09-10", "relevancy": 1.3909, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4754}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MENSA%3A%20A%20Multi-Event%20Network%20for%20Survival%20Analysis%20under%20Informative%0A%20%20Censoring&body=Title%3A%20MENSA%3A%20A%20Multi-Event%20Network%20for%20Survival%20Analysis%20under%20Informative%0A%20%20Censoring%0AAuthor%3A%20Christian%20Marius%20Lillelund%20and%20Ali%20Hossein%20Gharari%20Foomani%20and%20Weijie%20Sun%20and%20Shi-ang%20Qi%20and%20Russell%20Greiner%0AAbstract%3A%20%20%20Given%20an%20instance%2C%20a%20multi-event%20survival%20model%20predicts%20the%20time%20until%20that%0Ainstance%20experiences%20each%20of%20several%20different%20events.%20These%20events%20are%20not%0Amutually%20exclusive%20and%20there%20are%20often%20statistical%20dependencies%20between%20them.%0AThere%20are%20relatively%20few%20multi-event%20survival%20results%2C%20most%20focusing%20on%0Aproducing%20a%20simple%20risk%20score%2C%20rather%20than%20the%20time-to-event%20itself.%20To%0Aovercome%20these%20issues%2C%20we%20introduce%20MENSA%2C%20a%20novel%2C%20deep%20learning%20approach%20for%0Amulti-event%20survival%20analysis%20that%20can%20jointly%20learn%20representations%20of%20the%0Ainput%20covariates%20and%20the%20dependence%20structure%20between%20events.%20As%20a%20practical%0Amotivation%20for%20multi-event%20survival%20analysis%2C%20we%20consider%20the%20problem%20of%0Apredicting%20the%20time%20until%20a%20patient%20with%20amyotrophic%20lateral%20sclerosis%20%28ALS%29%0Aloses%20various%20physical%20functions%2C%20i.e.%2C%20the%20ability%20to%20speak%2C%20swallow%2C%20write%2C%0Aor%20walk.%20When%20estimating%20when%20a%20patient%20is%20no%20longer%20able%20to%20swallow%2C%20our%0Aapproach%20achieves%20an%20L1-Margin%20loss%20of%20278.8%20days%2C%20compared%20to%20355.2%20days%20when%0Amodeling%20each%20event%20separately.%20In%20addition%2C%20we%20also%20evaluate%20our%20approach%20in%0Asingle-event%20and%20competing%20risk%20scenarios%20by%20modeling%20the%20censoring%20and%20event%0Adistributions%20as%20equal%20contributing%20factors%20in%20the%20optimization%20process%2C%20and%0Ashow%20that%20our%20approach%20performs%20well%20across%20multiple%20benchmark%20datasets.%20The%0Asource%20code%20is%20available%20at%3A%20https%3A//github.com/thecml/mensa%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMENSA%253A%2520A%2520Multi-Event%2520Network%2520for%2520Survival%2520Analysis%2520under%2520Informative%250A%2520%2520Censoring%26entry.906535625%3DChristian%2520Marius%2520Lillelund%2520and%2520Ali%2520Hossein%2520Gharari%2520Foomani%2520and%2520Weijie%2520Sun%2520and%2520Shi-ang%2520Qi%2520and%2520Russell%2520Greiner%26entry.1292438233%3D%2520%2520Given%2520an%2520instance%252C%2520a%2520multi-event%2520survival%2520model%2520predicts%2520the%2520time%2520until%2520that%250Ainstance%2520experiences%2520each%2520of%2520several%2520different%2520events.%2520These%2520events%2520are%2520not%250Amutually%2520exclusive%2520and%2520there%2520are%2520often%2520statistical%2520dependencies%2520between%2520them.%250AThere%2520are%2520relatively%2520few%2520multi-event%2520survival%2520results%252C%2520most%2520focusing%2520on%250Aproducing%2520a%2520simple%2520risk%2520score%252C%2520rather%2520than%2520the%2520time-to-event%2520itself.%2520To%250Aovercome%2520these%2520issues%252C%2520we%2520introduce%2520MENSA%252C%2520a%2520novel%252C%2520deep%2520learning%2520approach%2520for%250Amulti-event%2520survival%2520analysis%2520that%2520can%2520jointly%2520learn%2520representations%2520of%2520the%250Ainput%2520covariates%2520and%2520the%2520dependence%2520structure%2520between%2520events.%2520As%2520a%2520practical%250Amotivation%2520for%2520multi-event%2520survival%2520analysis%252C%2520we%2520consider%2520the%2520problem%2520of%250Apredicting%2520the%2520time%2520until%2520a%2520patient%2520with%2520amyotrophic%2520lateral%2520sclerosis%2520%2528ALS%2529%250Aloses%2520various%2520physical%2520functions%252C%2520i.e.%252C%2520the%2520ability%2520to%2520speak%252C%2520swallow%252C%2520write%252C%250Aor%2520walk.%2520When%2520estimating%2520when%2520a%2520patient%2520is%2520no%2520longer%2520able%2520to%2520swallow%252C%2520our%250Aapproach%2520achieves%2520an%2520L1-Margin%2520loss%2520of%2520278.8%2520days%252C%2520compared%2520to%2520355.2%2520days%2520when%250Amodeling%2520each%2520event%2520separately.%2520In%2520addition%252C%2520we%2520also%2520evaluate%2520our%2520approach%2520in%250Asingle-event%2520and%2520competing%2520risk%2520scenarios%2520by%2520modeling%2520the%2520censoring%2520and%2520event%250Adistributions%2520as%2520equal%2520contributing%2520factors%2520in%2520the%2520optimization%2520process%252C%2520and%250Ashow%2520that%2520our%2520approach%2520performs%2520well%2520across%2520multiple%2520benchmark%2520datasets.%2520The%250Asource%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/thecml/mensa%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MENSA%3A%20A%20Multi-Event%20Network%20for%20Survival%20Analysis%20under%20Informative%0A%20%20Censoring&entry.906535625=Christian%20Marius%20Lillelund%20and%20Ali%20Hossein%20Gharari%20Foomani%20and%20Weijie%20Sun%20and%20Shi-ang%20Qi%20and%20Russell%20Greiner&entry.1292438233=%20%20Given%20an%20instance%2C%20a%20multi-event%20survival%20model%20predicts%20the%20time%20until%20that%0Ainstance%20experiences%20each%20of%20several%20different%20events.%20These%20events%20are%20not%0Amutually%20exclusive%20and%20there%20are%20often%20statistical%20dependencies%20between%20them.%0AThere%20are%20relatively%20few%20multi-event%20survival%20results%2C%20most%20focusing%20on%0Aproducing%20a%20simple%20risk%20score%2C%20rather%20than%20the%20time-to-event%20itself.%20To%0Aovercome%20these%20issues%2C%20we%20introduce%20MENSA%2C%20a%20novel%2C%20deep%20learning%20approach%20for%0Amulti-event%20survival%20analysis%20that%20can%20jointly%20learn%20representations%20of%20the%0Ainput%20covariates%20and%20the%20dependence%20structure%20between%20events.%20As%20a%20practical%0Amotivation%20for%20multi-event%20survival%20analysis%2C%20we%20consider%20the%20problem%20of%0Apredicting%20the%20time%20until%20a%20patient%20with%20amyotrophic%20lateral%20sclerosis%20%28ALS%29%0Aloses%20various%20physical%20functions%2C%20i.e.%2C%20the%20ability%20to%20speak%2C%20swallow%2C%20write%2C%0Aor%20walk.%20When%20estimating%20when%20a%20patient%20is%20no%20longer%20able%20to%20swallow%2C%20our%0Aapproach%20achieves%20an%20L1-Margin%20loss%20of%20278.8%20days%2C%20compared%20to%20355.2%20days%20when%0Amodeling%20each%20event%20separately.%20In%20addition%2C%20we%20also%20evaluate%20our%20approach%20in%0Asingle-event%20and%20competing%20risk%20scenarios%20by%20modeling%20the%20censoring%20and%20event%0Adistributions%20as%20equal%20contributing%20factors%20in%20the%20optimization%20process%2C%20and%0Ashow%20that%20our%20approach%20performs%20well%20across%20multiple%20benchmark%20datasets.%20The%0Asource%20code%20is%20available%20at%3A%20https%3A//github.com/thecml/mensa%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06525v1&entry.124074799=Read"},
{"title": "GeMuCo: Generalized Multisensory Correlational Model for Body Schema\n  Learning", "author": "Kento Kawaharazuka and Kei Okada and Masayuki Inaba", "abstract": "  Humans can autonomously learn the relationship between sensation and motion\nin their own bodies, estimate and control their own body states, and move while\ncontinuously adapting to the current environment. On the other hand, current\nrobots control their bodies by learning the network structure described by\nhumans from their experiences, making certain assumptions on the relationship\nbetween sensors and actuators. In addition, the network model does not adapt to\nchanges in the robot's body, the tools that are grasped, or the environment,\nand there is no unified theory, not only for control but also for state\nestimation, anomaly detection, simulation, and so on. In this study, we propose\na Generalized Multisensory Correlational Model (GeMuCo), in which the robot\nitself acquires a body schema describing the correlation between sensors and\nactuators from its own experience, including model structures such as network\ninput/output. The robot adapts to the current environment by updating this body\nschema model online, estimates and controls its body state, and even performs\nanomaly detection and simulation. We demonstrate the effectiveness of this\nmethod by applying it to tool-use considering changes in grasping state for an\naxis-driven robot, to joint-muscle mapping learning for a musculoskeletal\nrobot, and to full-body tool manipulation for a low-rigidity plastic-made\nhumanoid.\n", "link": "http://arxiv.org/abs/2409.06427v1", "date": "2024-09-10", "relevancy": 1.7214, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6215}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6118}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeMuCo%3A%20Generalized%20Multisensory%20Correlational%20Model%20for%20Body%20Schema%0A%20%20Learning&body=Title%3A%20GeMuCo%3A%20Generalized%20Multisensory%20Correlational%20Model%20for%20Body%20Schema%0A%20%20Learning%0AAuthor%3A%20Kento%20Kawaharazuka%20and%20Kei%20Okada%20and%20Masayuki%20Inaba%0AAbstract%3A%20%20%20Humans%20can%20autonomously%20learn%20the%20relationship%20between%20sensation%20and%20motion%0Ain%20their%20own%20bodies%2C%20estimate%20and%20control%20their%20own%20body%20states%2C%20and%20move%20while%0Acontinuously%20adapting%20to%20the%20current%20environment.%20On%20the%20other%20hand%2C%20current%0Arobots%20control%20their%20bodies%20by%20learning%20the%20network%20structure%20described%20by%0Ahumans%20from%20their%20experiences%2C%20making%20certain%20assumptions%20on%20the%20relationship%0Abetween%20sensors%20and%20actuators.%20In%20addition%2C%20the%20network%20model%20does%20not%20adapt%20to%0Achanges%20in%20the%20robot%27s%20body%2C%20the%20tools%20that%20are%20grasped%2C%20or%20the%20environment%2C%0Aand%20there%20is%20no%20unified%20theory%2C%20not%20only%20for%20control%20but%20also%20for%20state%0Aestimation%2C%20anomaly%20detection%2C%20simulation%2C%20and%20so%20on.%20In%20this%20study%2C%20we%20propose%0Aa%20Generalized%20Multisensory%20Correlational%20Model%20%28GeMuCo%29%2C%20in%20which%20the%20robot%0Aitself%20acquires%20a%20body%20schema%20describing%20the%20correlation%20between%20sensors%20and%0Aactuators%20from%20its%20own%20experience%2C%20including%20model%20structures%20such%20as%20network%0Ainput/output.%20The%20robot%20adapts%20to%20the%20current%20environment%20by%20updating%20this%20body%0Aschema%20model%20online%2C%20estimates%20and%20controls%20its%20body%20state%2C%20and%20even%20performs%0Aanomaly%20detection%20and%20simulation.%20We%20demonstrate%20the%20effectiveness%20of%20this%0Amethod%20by%20applying%20it%20to%20tool-use%20considering%20changes%20in%20grasping%20state%20for%20an%0Aaxis-driven%20robot%2C%20to%20joint-muscle%20mapping%20learning%20for%20a%20musculoskeletal%0Arobot%2C%20and%20to%20full-body%20tool%20manipulation%20for%20a%20low-rigidity%20plastic-made%0Ahumanoid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeMuCo%253A%2520Generalized%2520Multisensory%2520Correlational%2520Model%2520for%2520Body%2520Schema%250A%2520%2520Learning%26entry.906535625%3DKento%2520Kawaharazuka%2520and%2520Kei%2520Okada%2520and%2520Masayuki%2520Inaba%26entry.1292438233%3D%2520%2520Humans%2520can%2520autonomously%2520learn%2520the%2520relationship%2520between%2520sensation%2520and%2520motion%250Ain%2520their%2520own%2520bodies%252C%2520estimate%2520and%2520control%2520their%2520own%2520body%2520states%252C%2520and%2520move%2520while%250Acontinuously%2520adapting%2520to%2520the%2520current%2520environment.%2520On%2520the%2520other%2520hand%252C%2520current%250Arobots%2520control%2520their%2520bodies%2520by%2520learning%2520the%2520network%2520structure%2520described%2520by%250Ahumans%2520from%2520their%2520experiences%252C%2520making%2520certain%2520assumptions%2520on%2520the%2520relationship%250Abetween%2520sensors%2520and%2520actuators.%2520In%2520addition%252C%2520the%2520network%2520model%2520does%2520not%2520adapt%2520to%250Achanges%2520in%2520the%2520robot%2527s%2520body%252C%2520the%2520tools%2520that%2520are%2520grasped%252C%2520or%2520the%2520environment%252C%250Aand%2520there%2520is%2520no%2520unified%2520theory%252C%2520not%2520only%2520for%2520control%2520but%2520also%2520for%2520state%250Aestimation%252C%2520anomaly%2520detection%252C%2520simulation%252C%2520and%2520so%2520on.%2520In%2520this%2520study%252C%2520we%2520propose%250Aa%2520Generalized%2520Multisensory%2520Correlational%2520Model%2520%2528GeMuCo%2529%252C%2520in%2520which%2520the%2520robot%250Aitself%2520acquires%2520a%2520body%2520schema%2520describing%2520the%2520correlation%2520between%2520sensors%2520and%250Aactuators%2520from%2520its%2520own%2520experience%252C%2520including%2520model%2520structures%2520such%2520as%2520network%250Ainput/output.%2520The%2520robot%2520adapts%2520to%2520the%2520current%2520environment%2520by%2520updating%2520this%2520body%250Aschema%2520model%2520online%252C%2520estimates%2520and%2520controls%2520its%2520body%2520state%252C%2520and%2520even%2520performs%250Aanomaly%2520detection%2520and%2520simulation.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%250Amethod%2520by%2520applying%2520it%2520to%2520tool-use%2520considering%2520changes%2520in%2520grasping%2520state%2520for%2520an%250Aaxis-driven%2520robot%252C%2520to%2520joint-muscle%2520mapping%2520learning%2520for%2520a%2520musculoskeletal%250Arobot%252C%2520and%2520to%2520full-body%2520tool%2520manipulation%2520for%2520a%2520low-rigidity%2520plastic-made%250Ahumanoid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeMuCo%3A%20Generalized%20Multisensory%20Correlational%20Model%20for%20Body%20Schema%0A%20%20Learning&entry.906535625=Kento%20Kawaharazuka%20and%20Kei%20Okada%20and%20Masayuki%20Inaba&entry.1292438233=%20%20Humans%20can%20autonomously%20learn%20the%20relationship%20between%20sensation%20and%20motion%0Ain%20their%20own%20bodies%2C%20estimate%20and%20control%20their%20own%20body%20states%2C%20and%20move%20while%0Acontinuously%20adapting%20to%20the%20current%20environment.%20On%20the%20other%20hand%2C%20current%0Arobots%20control%20their%20bodies%20by%20learning%20the%20network%20structure%20described%20by%0Ahumans%20from%20their%20experiences%2C%20making%20certain%20assumptions%20on%20the%20relationship%0Abetween%20sensors%20and%20actuators.%20In%20addition%2C%20the%20network%20model%20does%20not%20adapt%20to%0Achanges%20in%20the%20robot%27s%20body%2C%20the%20tools%20that%20are%20grasped%2C%20or%20the%20environment%2C%0Aand%20there%20is%20no%20unified%20theory%2C%20not%20only%20for%20control%20but%20also%20for%20state%0Aestimation%2C%20anomaly%20detection%2C%20simulation%2C%20and%20so%20on.%20In%20this%20study%2C%20we%20propose%0Aa%20Generalized%20Multisensory%20Correlational%20Model%20%28GeMuCo%29%2C%20in%20which%20the%20robot%0Aitself%20acquires%20a%20body%20schema%20describing%20the%20correlation%20between%20sensors%20and%0Aactuators%20from%20its%20own%20experience%2C%20including%20model%20structures%20such%20as%20network%0Ainput/output.%20The%20robot%20adapts%20to%20the%20current%20environment%20by%20updating%20this%20body%0Aschema%20model%20online%2C%20estimates%20and%20controls%20its%20body%20state%2C%20and%20even%20performs%0Aanomaly%20detection%20and%20simulation.%20We%20demonstrate%20the%20effectiveness%20of%20this%0Amethod%20by%20applying%20it%20to%20tool-use%20considering%20changes%20in%20grasping%20state%20for%20an%0Aaxis-driven%20robot%2C%20to%20joint-muscle%20mapping%20learning%20for%20a%20musculoskeletal%0Arobot%2C%20and%20to%20full-body%20tool%20manipulation%20for%20a%20low-rigidity%20plastic-made%0Ahumanoid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06427v1&entry.124074799=Read"},
{"title": "A Primer on Variational Inference for Physics-Informed Deep Generative\n  Modelling", "author": "Alex Glyn-Davies and Arnaud Vadeboncoeur and O. Deniz Akyildiz and Ieva Kazlauskaite and Mark Girolami", "abstract": "  Variational inference (VI) is a computationally efficient and scalable\nmethodology for approximate Bayesian inference. It strikes a balance between\naccuracy of uncertainty quantification and practical tractability. It excels at\ngenerative modelling and inversion tasks due to its built-in Bayesian\nregularisation and flexibility, essential qualities for physics related\nproblems. Deriving the central learning objective for VI must often be tailored\nto new learning tasks where the nature of the problems dictates the conditional\ndependence between variables of interest, such as arising in physics problems.\nIn this paper, we provide an accessible and thorough technical introduction to\nVI for forward and inverse problems, guiding the reader through standard\nderivations of the VI framework and how it can best be realized through deep\nlearning. We then review and unify recent literature exemplifying the creative\nflexibility allowed by VI. This paper is designed for a general scientific\naudience looking to solve physics-based problems with an emphasis on\nuncertainty quantification.\n", "link": "http://arxiv.org/abs/2409.06560v1", "date": "2024-09-10", "relevancy": 1.819, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5282}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.442}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Primer%20on%20Variational%20Inference%20for%20Physics-Informed%20Deep%20Generative%0A%20%20Modelling&body=Title%3A%20A%20Primer%20on%20Variational%20Inference%20for%20Physics-Informed%20Deep%20Generative%0A%20%20Modelling%0AAuthor%3A%20Alex%20Glyn-Davies%20and%20Arnaud%20Vadeboncoeur%20and%20O.%20Deniz%20Akyildiz%20and%20Ieva%20Kazlauskaite%20and%20Mark%20Girolami%0AAbstract%3A%20%20%20Variational%20inference%20%28VI%29%20is%20a%20computationally%20efficient%20and%20scalable%0Amethodology%20for%20approximate%20Bayesian%20inference.%20It%20strikes%20a%20balance%20between%0Aaccuracy%20of%20uncertainty%20quantification%20and%20practical%20tractability.%20It%20excels%20at%0Agenerative%20modelling%20and%20inversion%20tasks%20due%20to%20its%20built-in%20Bayesian%0Aregularisation%20and%20flexibility%2C%20essential%20qualities%20for%20physics%20related%0Aproblems.%20Deriving%20the%20central%20learning%20objective%20for%20VI%20must%20often%20be%20tailored%0Ato%20new%20learning%20tasks%20where%20the%20nature%20of%20the%20problems%20dictates%20the%20conditional%0Adependence%20between%20variables%20of%20interest%2C%20such%20as%20arising%20in%20physics%20problems.%0AIn%20this%20paper%2C%20we%20provide%20an%20accessible%20and%20thorough%20technical%20introduction%20to%0AVI%20for%20forward%20and%20inverse%20problems%2C%20guiding%20the%20reader%20through%20standard%0Aderivations%20of%20the%20VI%20framework%20and%20how%20it%20can%20best%20be%20realized%20through%20deep%0Alearning.%20We%20then%20review%20and%20unify%20recent%20literature%20exemplifying%20the%20creative%0Aflexibility%20allowed%20by%20VI.%20This%20paper%20is%20designed%20for%20a%20general%20scientific%0Aaudience%20looking%20to%20solve%20physics-based%20problems%20with%20an%20emphasis%20on%0Auncertainty%20quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Primer%2520on%2520Variational%2520Inference%2520for%2520Physics-Informed%2520Deep%2520Generative%250A%2520%2520Modelling%26entry.906535625%3DAlex%2520Glyn-Davies%2520and%2520Arnaud%2520Vadeboncoeur%2520and%2520O.%2520Deniz%2520Akyildiz%2520and%2520Ieva%2520Kazlauskaite%2520and%2520Mark%2520Girolami%26entry.1292438233%3D%2520%2520Variational%2520inference%2520%2528VI%2529%2520is%2520a%2520computationally%2520efficient%2520and%2520scalable%250Amethodology%2520for%2520approximate%2520Bayesian%2520inference.%2520It%2520strikes%2520a%2520balance%2520between%250Aaccuracy%2520of%2520uncertainty%2520quantification%2520and%2520practical%2520tractability.%2520It%2520excels%2520at%250Agenerative%2520modelling%2520and%2520inversion%2520tasks%2520due%2520to%2520its%2520built-in%2520Bayesian%250Aregularisation%2520and%2520flexibility%252C%2520essential%2520qualities%2520for%2520physics%2520related%250Aproblems.%2520Deriving%2520the%2520central%2520learning%2520objective%2520for%2520VI%2520must%2520often%2520be%2520tailored%250Ato%2520new%2520learning%2520tasks%2520where%2520the%2520nature%2520of%2520the%2520problems%2520dictates%2520the%2520conditional%250Adependence%2520between%2520variables%2520of%2520interest%252C%2520such%2520as%2520arising%2520in%2520physics%2520problems.%250AIn%2520this%2520paper%252C%2520we%2520provide%2520an%2520accessible%2520and%2520thorough%2520technical%2520introduction%2520to%250AVI%2520for%2520forward%2520and%2520inverse%2520problems%252C%2520guiding%2520the%2520reader%2520through%2520standard%250Aderivations%2520of%2520the%2520VI%2520framework%2520and%2520how%2520it%2520can%2520best%2520be%2520realized%2520through%2520deep%250Alearning.%2520We%2520then%2520review%2520and%2520unify%2520recent%2520literature%2520exemplifying%2520the%2520creative%250Aflexibility%2520allowed%2520by%2520VI.%2520This%2520paper%2520is%2520designed%2520for%2520a%2520general%2520scientific%250Aaudience%2520looking%2520to%2520solve%2520physics-based%2520problems%2520with%2520an%2520emphasis%2520on%250Auncertainty%2520quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Primer%20on%20Variational%20Inference%20for%20Physics-Informed%20Deep%20Generative%0A%20%20Modelling&entry.906535625=Alex%20Glyn-Davies%20and%20Arnaud%20Vadeboncoeur%20and%20O.%20Deniz%20Akyildiz%20and%20Ieva%20Kazlauskaite%20and%20Mark%20Girolami&entry.1292438233=%20%20Variational%20inference%20%28VI%29%20is%20a%20computationally%20efficient%20and%20scalable%0Amethodology%20for%20approximate%20Bayesian%20inference.%20It%20strikes%20a%20balance%20between%0Aaccuracy%20of%20uncertainty%20quantification%20and%20practical%20tractability.%20It%20excels%20at%0Agenerative%20modelling%20and%20inversion%20tasks%20due%20to%20its%20built-in%20Bayesian%0Aregularisation%20and%20flexibility%2C%20essential%20qualities%20for%20physics%20related%0Aproblems.%20Deriving%20the%20central%20learning%20objective%20for%20VI%20must%20often%20be%20tailored%0Ato%20new%20learning%20tasks%20where%20the%20nature%20of%20the%20problems%20dictates%20the%20conditional%0Adependence%20between%20variables%20of%20interest%2C%20such%20as%20arising%20in%20physics%20problems.%0AIn%20this%20paper%2C%20we%20provide%20an%20accessible%20and%20thorough%20technical%20introduction%20to%0AVI%20for%20forward%20and%20inverse%20problems%2C%20guiding%20the%20reader%20through%20standard%0Aderivations%20of%20the%20VI%20framework%20and%20how%20it%20can%20best%20be%20realized%20through%20deep%0Alearning.%20We%20then%20review%20and%20unify%20recent%20literature%20exemplifying%20the%20creative%0Aflexibility%20allowed%20by%20VI.%20This%20paper%20is%20designed%20for%20a%20general%20scientific%0Aaudience%20looking%20to%20solve%20physics-based%20problems%20with%20an%20emphasis%20on%0Auncertainty%20quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06560v1&entry.124074799=Read"},
{"title": "Unrevealed Threats: A Comprehensive Study of the Adversarial Robustness\n  of Underwater Image Enhancement Models", "author": "Siyu Zhai and Zhibo He and Xiaofeng Cong and Junming Hou and Jie Gui and Jian Wei You and Xin Gong and James Tin-Yau Kwok and Yuan Yan Tang", "abstract": "  Learning-based methods for underwater image enhancement (UWIE) have undergone\nextensive exploration. However, learning-based models are usually vulnerable to\nadversarial examples so as the UWIE models. To the best of our knowledge, there\nis no comprehensive study on the adversarial robustness of UWIE models, which\nindicates that UWIE models are potentially under the threat of adversarial\nattacks. In this paper, we propose a general adversarial attack protocol. We\nmake a first attempt to conduct adversarial attacks on five well-designed UWIE\nmodels on three common underwater image benchmark datasets. Considering the\nscattering and absorption of light in the underwater environment, there exists\na strong correlation between color correction and underwater image enhancement.\nOn the basis of that, we also design two effective UWIE-oriented adversarial\nattack methods Pixel Attack and Color Shift Attack targeting different color\nspaces. The results show that five models exhibit varying degrees of\nvulnerability to adversarial attacks and well-designed small perturbations on\ndegraded images are capable of preventing UWIE models from generating enhanced\nresults. Further, we conduct adversarial training on these models and\nsuccessfully mitigated the effectiveness of adversarial attacks. In summary, we\nreveal the adversarial vulnerability of UWIE models and propose a new\nevaluation dimension of UWIE models.\n", "link": "http://arxiv.org/abs/2409.06420v1", "date": "2024-09-10", "relevancy": 1.5462, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5319}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5134}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unrevealed%20Threats%3A%20A%20Comprehensive%20Study%20of%20the%20Adversarial%20Robustness%0A%20%20of%20Underwater%20Image%20Enhancement%20Models&body=Title%3A%20Unrevealed%20Threats%3A%20A%20Comprehensive%20Study%20of%20the%20Adversarial%20Robustness%0A%20%20of%20Underwater%20Image%20Enhancement%20Models%0AAuthor%3A%20Siyu%20Zhai%20and%20Zhibo%20He%20and%20Xiaofeng%20Cong%20and%20Junming%20Hou%20and%20Jie%20Gui%20and%20Jian%20Wei%20You%20and%20Xin%20Gong%20and%20James%20Tin-Yau%20Kwok%20and%20Yuan%20Yan%20Tang%0AAbstract%3A%20%20%20Learning-based%20methods%20for%20underwater%20image%20enhancement%20%28UWIE%29%20have%20undergone%0Aextensive%20exploration.%20However%2C%20learning-based%20models%20are%20usually%20vulnerable%20to%0Aadversarial%20examples%20so%20as%20the%20UWIE%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20there%0Ais%20no%20comprehensive%20study%20on%20the%20adversarial%20robustness%20of%20UWIE%20models%2C%20which%0Aindicates%20that%20UWIE%20models%20are%20potentially%20under%20the%20threat%20of%20adversarial%0Aattacks.%20In%20this%20paper%2C%20we%20propose%20a%20general%20adversarial%20attack%20protocol.%20We%0Amake%20a%20first%20attempt%20to%20conduct%20adversarial%20attacks%20on%20five%20well-designed%20UWIE%0Amodels%20on%20three%20common%20underwater%20image%20benchmark%20datasets.%20Considering%20the%0Ascattering%20and%20absorption%20of%20light%20in%20the%20underwater%20environment%2C%20there%20exists%0Aa%20strong%20correlation%20between%20color%20correction%20and%20underwater%20image%20enhancement.%0AOn%20the%20basis%20of%20that%2C%20we%20also%20design%20two%20effective%20UWIE-oriented%20adversarial%0Aattack%20methods%20Pixel%20Attack%20and%20Color%20Shift%20Attack%20targeting%20different%20color%0Aspaces.%20The%20results%20show%20that%20five%20models%20exhibit%20varying%20degrees%20of%0Avulnerability%20to%20adversarial%20attacks%20and%20well-designed%20small%20perturbations%20on%0Adegraded%20images%20are%20capable%20of%20preventing%20UWIE%20models%20from%20generating%20enhanced%0Aresults.%20Further%2C%20we%20conduct%20adversarial%20training%20on%20these%20models%20and%0Asuccessfully%20mitigated%20the%20effectiveness%20of%20adversarial%20attacks.%20In%20summary%2C%20we%0Areveal%20the%20adversarial%20vulnerability%20of%20UWIE%20models%20and%20propose%20a%20new%0Aevaluation%20dimension%20of%20UWIE%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnrevealed%2520Threats%253A%2520A%2520Comprehensive%2520Study%2520of%2520the%2520Adversarial%2520Robustness%250A%2520%2520of%2520Underwater%2520Image%2520Enhancement%2520Models%26entry.906535625%3DSiyu%2520Zhai%2520and%2520Zhibo%2520He%2520and%2520Xiaofeng%2520Cong%2520and%2520Junming%2520Hou%2520and%2520Jie%2520Gui%2520and%2520Jian%2520Wei%2520You%2520and%2520Xin%2520Gong%2520and%2520James%2520Tin-Yau%2520Kwok%2520and%2520Yuan%2520Yan%2520Tang%26entry.1292438233%3D%2520%2520Learning-based%2520methods%2520for%2520underwater%2520image%2520enhancement%2520%2528UWIE%2529%2520have%2520undergone%250Aextensive%2520exploration.%2520However%252C%2520learning-based%2520models%2520are%2520usually%2520vulnerable%2520to%250Aadversarial%2520examples%2520so%2520as%2520the%2520UWIE%2520models.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520there%250Ais%2520no%2520comprehensive%2520study%2520on%2520the%2520adversarial%2520robustness%2520of%2520UWIE%2520models%252C%2520which%250Aindicates%2520that%2520UWIE%2520models%2520are%2520potentially%2520under%2520the%2520threat%2520of%2520adversarial%250Aattacks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520general%2520adversarial%2520attack%2520protocol.%2520We%250Amake%2520a%2520first%2520attempt%2520to%2520conduct%2520adversarial%2520attacks%2520on%2520five%2520well-designed%2520UWIE%250Amodels%2520on%2520three%2520common%2520underwater%2520image%2520benchmark%2520datasets.%2520Considering%2520the%250Ascattering%2520and%2520absorption%2520of%2520light%2520in%2520the%2520underwater%2520environment%252C%2520there%2520exists%250Aa%2520strong%2520correlation%2520between%2520color%2520correction%2520and%2520underwater%2520image%2520enhancement.%250AOn%2520the%2520basis%2520of%2520that%252C%2520we%2520also%2520design%2520two%2520effective%2520UWIE-oriented%2520adversarial%250Aattack%2520methods%2520Pixel%2520Attack%2520and%2520Color%2520Shift%2520Attack%2520targeting%2520different%2520color%250Aspaces.%2520The%2520results%2520show%2520that%2520five%2520models%2520exhibit%2520varying%2520degrees%2520of%250Avulnerability%2520to%2520adversarial%2520attacks%2520and%2520well-designed%2520small%2520perturbations%2520on%250Adegraded%2520images%2520are%2520capable%2520of%2520preventing%2520UWIE%2520models%2520from%2520generating%2520enhanced%250Aresults.%2520Further%252C%2520we%2520conduct%2520adversarial%2520training%2520on%2520these%2520models%2520and%250Asuccessfully%2520mitigated%2520the%2520effectiveness%2520of%2520adversarial%2520attacks.%2520In%2520summary%252C%2520we%250Areveal%2520the%2520adversarial%2520vulnerability%2520of%2520UWIE%2520models%2520and%2520propose%2520a%2520new%250Aevaluation%2520dimension%2520of%2520UWIE%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unrevealed%20Threats%3A%20A%20Comprehensive%20Study%20of%20the%20Adversarial%20Robustness%0A%20%20of%20Underwater%20Image%20Enhancement%20Models&entry.906535625=Siyu%20Zhai%20and%20Zhibo%20He%20and%20Xiaofeng%20Cong%20and%20Junming%20Hou%20and%20Jie%20Gui%20and%20Jian%20Wei%20You%20and%20Xin%20Gong%20and%20James%20Tin-Yau%20Kwok%20and%20Yuan%20Yan%20Tang&entry.1292438233=%20%20Learning-based%20methods%20for%20underwater%20image%20enhancement%20%28UWIE%29%20have%20undergone%0Aextensive%20exploration.%20However%2C%20learning-based%20models%20are%20usually%20vulnerable%20to%0Aadversarial%20examples%20so%20as%20the%20UWIE%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20there%0Ais%20no%20comprehensive%20study%20on%20the%20adversarial%20robustness%20of%20UWIE%20models%2C%20which%0Aindicates%20that%20UWIE%20models%20are%20potentially%20under%20the%20threat%20of%20adversarial%0Aattacks.%20In%20this%20paper%2C%20we%20propose%20a%20general%20adversarial%20attack%20protocol.%20We%0Amake%20a%20first%20attempt%20to%20conduct%20adversarial%20attacks%20on%20five%20well-designed%20UWIE%0Amodels%20on%20three%20common%20underwater%20image%20benchmark%20datasets.%20Considering%20the%0Ascattering%20and%20absorption%20of%20light%20in%20the%20underwater%20environment%2C%20there%20exists%0Aa%20strong%20correlation%20between%20color%20correction%20and%20underwater%20image%20enhancement.%0AOn%20the%20basis%20of%20that%2C%20we%20also%20design%20two%20effective%20UWIE-oriented%20adversarial%0Aattack%20methods%20Pixel%20Attack%20and%20Color%20Shift%20Attack%20targeting%20different%20color%0Aspaces.%20The%20results%20show%20that%20five%20models%20exhibit%20varying%20degrees%20of%0Avulnerability%20to%20adversarial%20attacks%20and%20well-designed%20small%20perturbations%20on%0Adegraded%20images%20are%20capable%20of%20preventing%20UWIE%20models%20from%20generating%20enhanced%0Aresults.%20Further%2C%20we%20conduct%20adversarial%20training%20on%20these%20models%20and%0Asuccessfully%20mitigated%20the%20effectiveness%20of%20adversarial%20attacks.%20In%20summary%2C%20we%0Areveal%20the%20adversarial%20vulnerability%20of%20UWIE%20models%20and%20propose%20a%20new%0Aevaluation%20dimension%20of%20UWIE%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06420v1&entry.124074799=Read"},
{"title": "AMNS: Attention-Weighted Selective Mask and Noise Label Suppression for\n  Text-to-Image Person Retrieval", "author": "Runqing Zhang and Xue Zhou", "abstract": "  Text-to-image person retrieval aims to retrieve images of person given\ntextual descriptions, and most methods implicitly assume that the training\nimage-text pairs are correctly aligned, but in practice, under-correlated and\nfalse-correlated problems arise for image-text pairs due to poor image quality\nand mislabeling. Meanwhile, the random masking augmentation strategy may\nincorrectly discard semantic content resulting in the problem of generating\nnoisy pairings between image lexical elements and text descriptions. To solve\nthese two problems, we propose a new noise label suppression method and\nalleviate the problem generated by random mask through an attention-weighted\nselective mask strategy. In the proposed noise label suppression method, the\neffect of noise labels is suppressed by preventing the model from being\noverconfident by considering the inverse KL scatter loss, which is combined\nwith the weight adjustment focus loss to further improve the model's\nrecognition ability on difficult samples. On the other hand, Attention-Weighted\nSelective Mask processes the raw image through the EMA version of the image\nencoder, retaining some of the tokens with strong semantic associations with\nthe corresponding text descriptions in order to extract better features.\nNumerous experiments validate the effectiveness of our approach in terms of\ndealing with noisy problems. The code will be available soon at\nhttps://github.com/RunQing715/AMNS.git.\n", "link": "http://arxiv.org/abs/2409.06385v1", "date": "2024-09-10", "relevancy": 1.5716, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5246}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5235}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMNS%3A%20Attention-Weighted%20Selective%20Mask%20and%20Noise%20Label%20Suppression%20for%0A%20%20Text-to-Image%20Person%20Retrieval&body=Title%3A%20AMNS%3A%20Attention-Weighted%20Selective%20Mask%20and%20Noise%20Label%20Suppression%20for%0A%20%20Text-to-Image%20Person%20Retrieval%0AAuthor%3A%20Runqing%20Zhang%20and%20Xue%20Zhou%0AAbstract%3A%20%20%20Text-to-image%20person%20retrieval%20aims%20to%20retrieve%20images%20of%20person%20given%0Atextual%20descriptions%2C%20and%20most%20methods%20implicitly%20assume%20that%20the%20training%0Aimage-text%20pairs%20are%20correctly%20aligned%2C%20but%20in%20practice%2C%20under-correlated%20and%0Afalse-correlated%20problems%20arise%20for%20image-text%20pairs%20due%20to%20poor%20image%20quality%0Aand%20mislabeling.%20Meanwhile%2C%20the%20random%20masking%20augmentation%20strategy%20may%0Aincorrectly%20discard%20semantic%20content%20resulting%20in%20the%20problem%20of%20generating%0Anoisy%20pairings%20between%20image%20lexical%20elements%20and%20text%20descriptions.%20To%20solve%0Athese%20two%20problems%2C%20we%20propose%20a%20new%20noise%20label%20suppression%20method%20and%0Aalleviate%20the%20problem%20generated%20by%20random%20mask%20through%20an%20attention-weighted%0Aselective%20mask%20strategy.%20In%20the%20proposed%20noise%20label%20suppression%20method%2C%20the%0Aeffect%20of%20noise%20labels%20is%20suppressed%20by%20preventing%20the%20model%20from%20being%0Aoverconfident%20by%20considering%20the%20inverse%20KL%20scatter%20loss%2C%20which%20is%20combined%0Awith%20the%20weight%20adjustment%20focus%20loss%20to%20further%20improve%20the%20model%27s%0Arecognition%20ability%20on%20difficult%20samples.%20On%20the%20other%20hand%2C%20Attention-Weighted%0ASelective%20Mask%20processes%20the%20raw%20image%20through%20the%20EMA%20version%20of%20the%20image%0Aencoder%2C%20retaining%20some%20of%20the%20tokens%20with%20strong%20semantic%20associations%20with%0Athe%20corresponding%20text%20descriptions%20in%20order%20to%20extract%20better%20features.%0ANumerous%20experiments%20validate%20the%20effectiveness%20of%20our%20approach%20in%20terms%20of%0Adealing%20with%20noisy%20problems.%20The%20code%20will%20be%20available%20soon%20at%0Ahttps%3A//github.com/RunQing715/AMNS.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMNS%253A%2520Attention-Weighted%2520Selective%2520Mask%2520and%2520Noise%2520Label%2520Suppression%2520for%250A%2520%2520Text-to-Image%2520Person%2520Retrieval%26entry.906535625%3DRunqing%2520Zhang%2520and%2520Xue%2520Zhou%26entry.1292438233%3D%2520%2520Text-to-image%2520person%2520retrieval%2520aims%2520to%2520retrieve%2520images%2520of%2520person%2520given%250Atextual%2520descriptions%252C%2520and%2520most%2520methods%2520implicitly%2520assume%2520that%2520the%2520training%250Aimage-text%2520pairs%2520are%2520correctly%2520aligned%252C%2520but%2520in%2520practice%252C%2520under-correlated%2520and%250Afalse-correlated%2520problems%2520arise%2520for%2520image-text%2520pairs%2520due%2520to%2520poor%2520image%2520quality%250Aand%2520mislabeling.%2520Meanwhile%252C%2520the%2520random%2520masking%2520augmentation%2520strategy%2520may%250Aincorrectly%2520discard%2520semantic%2520content%2520resulting%2520in%2520the%2520problem%2520of%2520generating%250Anoisy%2520pairings%2520between%2520image%2520lexical%2520elements%2520and%2520text%2520descriptions.%2520To%2520solve%250Athese%2520two%2520problems%252C%2520we%2520propose%2520a%2520new%2520noise%2520label%2520suppression%2520method%2520and%250Aalleviate%2520the%2520problem%2520generated%2520by%2520random%2520mask%2520through%2520an%2520attention-weighted%250Aselective%2520mask%2520strategy.%2520In%2520the%2520proposed%2520noise%2520label%2520suppression%2520method%252C%2520the%250Aeffect%2520of%2520noise%2520labels%2520is%2520suppressed%2520by%2520preventing%2520the%2520model%2520from%2520being%250Aoverconfident%2520by%2520considering%2520the%2520inverse%2520KL%2520scatter%2520loss%252C%2520which%2520is%2520combined%250Awith%2520the%2520weight%2520adjustment%2520focus%2520loss%2520to%2520further%2520improve%2520the%2520model%2527s%250Arecognition%2520ability%2520on%2520difficult%2520samples.%2520On%2520the%2520other%2520hand%252C%2520Attention-Weighted%250ASelective%2520Mask%2520processes%2520the%2520raw%2520image%2520through%2520the%2520EMA%2520version%2520of%2520the%2520image%250Aencoder%252C%2520retaining%2520some%2520of%2520the%2520tokens%2520with%2520strong%2520semantic%2520associations%2520with%250Athe%2520corresponding%2520text%2520descriptions%2520in%2520order%2520to%2520extract%2520better%2520features.%250ANumerous%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520terms%2520of%250Adealing%2520with%2520noisy%2520problems.%2520The%2520code%2520will%2520be%2520available%2520soon%2520at%250Ahttps%253A//github.com/RunQing715/AMNS.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMNS%3A%20Attention-Weighted%20Selective%20Mask%20and%20Noise%20Label%20Suppression%20for%0A%20%20Text-to-Image%20Person%20Retrieval&entry.906535625=Runqing%20Zhang%20and%20Xue%20Zhou&entry.1292438233=%20%20Text-to-image%20person%20retrieval%20aims%20to%20retrieve%20images%20of%20person%20given%0Atextual%20descriptions%2C%20and%20most%20methods%20implicitly%20assume%20that%20the%20training%0Aimage-text%20pairs%20are%20correctly%20aligned%2C%20but%20in%20practice%2C%20under-correlated%20and%0Afalse-correlated%20problems%20arise%20for%20image-text%20pairs%20due%20to%20poor%20image%20quality%0Aand%20mislabeling.%20Meanwhile%2C%20the%20random%20masking%20augmentation%20strategy%20may%0Aincorrectly%20discard%20semantic%20content%20resulting%20in%20the%20problem%20of%20generating%0Anoisy%20pairings%20between%20image%20lexical%20elements%20and%20text%20descriptions.%20To%20solve%0Athese%20two%20problems%2C%20we%20propose%20a%20new%20noise%20label%20suppression%20method%20and%0Aalleviate%20the%20problem%20generated%20by%20random%20mask%20through%20an%20attention-weighted%0Aselective%20mask%20strategy.%20In%20the%20proposed%20noise%20label%20suppression%20method%2C%20the%0Aeffect%20of%20noise%20labels%20is%20suppressed%20by%20preventing%20the%20model%20from%20being%0Aoverconfident%20by%20considering%20the%20inverse%20KL%20scatter%20loss%2C%20which%20is%20combined%0Awith%20the%20weight%20adjustment%20focus%20loss%20to%20further%20improve%20the%20model%27s%0Arecognition%20ability%20on%20difficult%20samples.%20On%20the%20other%20hand%2C%20Attention-Weighted%0ASelective%20Mask%20processes%20the%20raw%20image%20through%20the%20EMA%20version%20of%20the%20image%0Aencoder%2C%20retaining%20some%20of%20the%20tokens%20with%20strong%20semantic%20associations%20with%0Athe%20corresponding%20text%20descriptions%20in%20order%20to%20extract%20better%20features.%0ANumerous%20experiments%20validate%20the%20effectiveness%20of%20our%20approach%20in%20terms%20of%0Adealing%20with%20noisy%20problems.%20The%20code%20will%20be%20available%20soon%20at%0Ahttps%3A//github.com/RunQing715/AMNS.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06385v1&entry.124074799=Read"},
{"title": "Asymptotically Optimal Lazy Lifelong Sampling-based Algorithm for\n  Efficient Motion Planning in Dynamic Environments", "author": "Lu Huang and Xingjian Jing", "abstract": "  The paper introduces an asymptotically optimal lifelong sampling-based path\nplanning algorithm that combines the merits of lifelong planning algorithms and\nlazy search algorithms for rapid replanning in dynamic environments where edge\nevaluation is expensive. By evaluating only sub-path candidates for the optimal\nsolution, the algorithm saves considerable evaluation time and thereby reduces\nthe overall planning cost. It employs a novel informed rewiring cascade to\nefficiently repair the search tree when the underlying search graph changes.\nSimulation results demonstrate that the algorithm outperforms various\nstate-of-the-art sampling-based planners in addressing both static and dynamic\nmotion planning problems.\n", "link": "http://arxiv.org/abs/2409.06521v1", "date": "2024-09-10", "relevancy": 1.4864, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5005}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4958}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymptotically%20Optimal%20Lazy%20Lifelong%20Sampling-based%20Algorithm%20for%0A%20%20Efficient%20Motion%20Planning%20in%20Dynamic%20Environments&body=Title%3A%20Asymptotically%20Optimal%20Lazy%20Lifelong%20Sampling-based%20Algorithm%20for%0A%20%20Efficient%20Motion%20Planning%20in%20Dynamic%20Environments%0AAuthor%3A%20Lu%20Huang%20and%20Xingjian%20Jing%0AAbstract%3A%20%20%20The%20paper%20introduces%20an%20asymptotically%20optimal%20lifelong%20sampling-based%20path%0Aplanning%20algorithm%20that%20combines%20the%20merits%20of%20lifelong%20planning%20algorithms%20and%0Alazy%20search%20algorithms%20for%20rapid%20replanning%20in%20dynamic%20environments%20where%20edge%0Aevaluation%20is%20expensive.%20By%20evaluating%20only%20sub-path%20candidates%20for%20the%20optimal%0Asolution%2C%20the%20algorithm%20saves%20considerable%20evaluation%20time%20and%20thereby%20reduces%0Athe%20overall%20planning%20cost.%20It%20employs%20a%20novel%20informed%20rewiring%20cascade%20to%0Aefficiently%20repair%20the%20search%20tree%20when%20the%20underlying%20search%20graph%20changes.%0ASimulation%20results%20demonstrate%20that%20the%20algorithm%20outperforms%20various%0Astate-of-the-art%20sampling-based%20planners%20in%20addressing%20both%20static%20and%20dynamic%0Amotion%20planning%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymptotically%2520Optimal%2520Lazy%2520Lifelong%2520Sampling-based%2520Algorithm%2520for%250A%2520%2520Efficient%2520Motion%2520Planning%2520in%2520Dynamic%2520Environments%26entry.906535625%3DLu%2520Huang%2520and%2520Xingjian%2520Jing%26entry.1292438233%3D%2520%2520The%2520paper%2520introduces%2520an%2520asymptotically%2520optimal%2520lifelong%2520sampling-based%2520path%250Aplanning%2520algorithm%2520that%2520combines%2520the%2520merits%2520of%2520lifelong%2520planning%2520algorithms%2520and%250Alazy%2520search%2520algorithms%2520for%2520rapid%2520replanning%2520in%2520dynamic%2520environments%2520where%2520edge%250Aevaluation%2520is%2520expensive.%2520By%2520evaluating%2520only%2520sub-path%2520candidates%2520for%2520the%2520optimal%250Asolution%252C%2520the%2520algorithm%2520saves%2520considerable%2520evaluation%2520time%2520and%2520thereby%2520reduces%250Athe%2520overall%2520planning%2520cost.%2520It%2520employs%2520a%2520novel%2520informed%2520rewiring%2520cascade%2520to%250Aefficiently%2520repair%2520the%2520search%2520tree%2520when%2520the%2520underlying%2520search%2520graph%2520changes.%250ASimulation%2520results%2520demonstrate%2520that%2520the%2520algorithm%2520outperforms%2520various%250Astate-of-the-art%2520sampling-based%2520planners%2520in%2520addressing%2520both%2520static%2520and%2520dynamic%250Amotion%2520planning%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymptotically%20Optimal%20Lazy%20Lifelong%20Sampling-based%20Algorithm%20for%0A%20%20Efficient%20Motion%20Planning%20in%20Dynamic%20Environments&entry.906535625=Lu%20Huang%20and%20Xingjian%20Jing&entry.1292438233=%20%20The%20paper%20introduces%20an%20asymptotically%20optimal%20lifelong%20sampling-based%20path%0Aplanning%20algorithm%20that%20combines%20the%20merits%20of%20lifelong%20planning%20algorithms%20and%0Alazy%20search%20algorithms%20for%20rapid%20replanning%20in%20dynamic%20environments%20where%20edge%0Aevaluation%20is%20expensive.%20By%20evaluating%20only%20sub-path%20candidates%20for%20the%20optimal%0Asolution%2C%20the%20algorithm%20saves%20considerable%20evaluation%20time%20and%20thereby%20reduces%0Athe%20overall%20planning%20cost.%20It%20employs%20a%20novel%20informed%20rewiring%20cascade%20to%0Aefficiently%20repair%20the%20search%20tree%20when%20the%20underlying%20search%20graph%20changes.%0ASimulation%20results%20demonstrate%20that%20the%20algorithm%20outperforms%20various%0Astate-of-the-art%20sampling-based%20planners%20in%20addressing%20both%20static%20and%20dynamic%0Amotion%20planning%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06521v1&entry.124074799=Read"},
{"title": "Exploiting Uncertainty for Querying Inconsistent Description Logics\n  Knowledge Bases", "author": "Riccardo Zese and Evelina Lamma and Fabrizio Riguzzi", "abstract": "  The necessity to manage inconsistency in Description Logics Knowledge Bases\n(KBs) has come to the fore with the increasing importance gained by the\nSemantic Web, where information comes from different sources that constantly\nchange their content and may contain contradictory descriptions when considered\neither alone or together. Classical reasoning algorithms do not handle\ninconsistent KBs, forcing the debugging of the KB in order to remove the\ninconsistency. In this paper, we exploit an existing probabilistic semantics\ncalled DISPONTE to overcome this problem and allow queries also in case of\ninconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE\nand empirically tested the validity of our proposal. Moreover, we formally\ncompare the presented approach to that of the repair semantics, one of the most\nestablished semantics when considering DL reasoning tasks.\n", "link": "http://arxiv.org/abs/2306.09138v3", "date": "2024-09-10", "relevancy": 1.429, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4843}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4783}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Uncertainty%20for%20Querying%20Inconsistent%20Description%20Logics%0A%20%20Knowledge%20Bases&body=Title%3A%20Exploiting%20Uncertainty%20for%20Querying%20Inconsistent%20Description%20Logics%0A%20%20Knowledge%20Bases%0AAuthor%3A%20Riccardo%20Zese%20and%20Evelina%20Lamma%20and%20Fabrizio%20Riguzzi%0AAbstract%3A%20%20%20The%20necessity%20to%20manage%20inconsistency%20in%20Description%20Logics%20Knowledge%20Bases%0A%28KBs%29%20has%20come%20to%20the%20fore%20with%20the%20increasing%20importance%20gained%20by%20the%0ASemantic%20Web%2C%20where%20information%20comes%20from%20different%20sources%20that%20constantly%0Achange%20their%20content%20and%20may%20contain%20contradictory%20descriptions%20when%20considered%0Aeither%20alone%20or%20together.%20Classical%20reasoning%20algorithms%20do%20not%20handle%0Ainconsistent%20KBs%2C%20forcing%20the%20debugging%20of%20the%20KB%20in%20order%20to%20remove%20the%0Ainconsistency.%20In%20this%20paper%2C%20we%20exploit%20an%20existing%20probabilistic%20semantics%0Acalled%20DISPONTE%20to%20overcome%20this%20problem%20and%20allow%20queries%20also%20in%20case%20of%0Ainconsistent%20KBs.%20We%20implemented%20our%20approach%20in%20the%20reasoners%20TRILL%20and%20BUNDLE%0Aand%20empirically%20tested%20the%20validity%20of%20our%20proposal.%20Moreover%2C%20we%20formally%0Acompare%20the%20presented%20approach%20to%20that%20of%20the%20repair%20semantics%2C%20one%20of%20the%20most%0Aestablished%20semantics%20when%20considering%20DL%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09138v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Uncertainty%2520for%2520Querying%2520Inconsistent%2520Description%2520Logics%250A%2520%2520Knowledge%2520Bases%26entry.906535625%3DRiccardo%2520Zese%2520and%2520Evelina%2520Lamma%2520and%2520Fabrizio%2520Riguzzi%26entry.1292438233%3D%2520%2520The%2520necessity%2520to%2520manage%2520inconsistency%2520in%2520Description%2520Logics%2520Knowledge%2520Bases%250A%2528KBs%2529%2520has%2520come%2520to%2520the%2520fore%2520with%2520the%2520increasing%2520importance%2520gained%2520by%2520the%250ASemantic%2520Web%252C%2520where%2520information%2520comes%2520from%2520different%2520sources%2520that%2520constantly%250Achange%2520their%2520content%2520and%2520may%2520contain%2520contradictory%2520descriptions%2520when%2520considered%250Aeither%2520alone%2520or%2520together.%2520Classical%2520reasoning%2520algorithms%2520do%2520not%2520handle%250Ainconsistent%2520KBs%252C%2520forcing%2520the%2520debugging%2520of%2520the%2520KB%2520in%2520order%2520to%2520remove%2520the%250Ainconsistency.%2520In%2520this%2520paper%252C%2520we%2520exploit%2520an%2520existing%2520probabilistic%2520semantics%250Acalled%2520DISPONTE%2520to%2520overcome%2520this%2520problem%2520and%2520allow%2520queries%2520also%2520in%2520case%2520of%250Ainconsistent%2520KBs.%2520We%2520implemented%2520our%2520approach%2520in%2520the%2520reasoners%2520TRILL%2520and%2520BUNDLE%250Aand%2520empirically%2520tested%2520the%2520validity%2520of%2520our%2520proposal.%2520Moreover%252C%2520we%2520formally%250Acompare%2520the%2520presented%2520approach%2520to%2520that%2520of%2520the%2520repair%2520semantics%252C%2520one%2520of%2520the%2520most%250Aestablished%2520semantics%2520when%2520considering%2520DL%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09138v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Uncertainty%20for%20Querying%20Inconsistent%20Description%20Logics%0A%20%20Knowledge%20Bases&entry.906535625=Riccardo%20Zese%20and%20Evelina%20Lamma%20and%20Fabrizio%20Riguzzi&entry.1292438233=%20%20The%20necessity%20to%20manage%20inconsistency%20in%20Description%20Logics%20Knowledge%20Bases%0A%28KBs%29%20has%20come%20to%20the%20fore%20with%20the%20increasing%20importance%20gained%20by%20the%0ASemantic%20Web%2C%20where%20information%20comes%20from%20different%20sources%20that%20constantly%0Achange%20their%20content%20and%20may%20contain%20contradictory%20descriptions%20when%20considered%0Aeither%20alone%20or%20together.%20Classical%20reasoning%20algorithms%20do%20not%20handle%0Ainconsistent%20KBs%2C%20forcing%20the%20debugging%20of%20the%20KB%20in%20order%20to%20remove%20the%0Ainconsistency.%20In%20this%20paper%2C%20we%20exploit%20an%20existing%20probabilistic%20semantics%0Acalled%20DISPONTE%20to%20overcome%20this%20problem%20and%20allow%20queries%20also%20in%20case%20of%0Ainconsistent%20KBs.%20We%20implemented%20our%20approach%20in%20the%20reasoners%20TRILL%20and%20BUNDLE%0Aand%20empirically%20tested%20the%20validity%20of%20our%20proposal.%20Moreover%2C%20we%20formally%0Acompare%20the%20presented%20approach%20to%20that%20of%20the%20repair%20semantics%2C%20one%20of%20the%20most%0Aestablished%20semantics%20when%20considering%20DL%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09138v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


