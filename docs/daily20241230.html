<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241225.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction", "author": "Kai Xu and Tze Ho Elden Tse and Jizong Peng and Angela Yao", "abstract": "  We propose a novel framework for scene decomposition and static background\nreconstruction from everyday videos. By integrating the trained motion masks\nand modeling the static scene as Gaussian splats with dynamics-aware\noptimization, our method achieves more accurate background reconstruction\nresults than previous works. Our proposed method is termed DAS3R, an\nabbreviation for Dynamics-Aware Gaussian Splatting for Static Scene\nReconstruction. Compared to existing methods, DAS3R is more robust in complex\nmotion scenarios, capable of handling videos where dynamic objects occupy a\nsignificant portion of the scene, and does not require camera pose inputs or\npoint cloud data from SLAM-based methods. We compared DAS3R against recent\ndistractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates\nenhanced performance and robustness with a margin of more than 2 dB in PSNR.\nThe project's webpage can be accessed via \\url{https://kai422.github.io/DAS3R/}\n", "link": "http://arxiv.org/abs/2412.19584v1", "date": "2024-12-27", "relevancy": 3.3124, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6862}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6638}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAS3R%3A%20Dynamics-Aware%20Gaussian%20Splatting%20for%20Static%20Scene%20Reconstruction&body=Title%3A%20DAS3R%3A%20Dynamics-Aware%20Gaussian%20Splatting%20for%20Static%20Scene%20Reconstruction%0AAuthor%3A%20Kai%20Xu%20and%20Tze%20Ho%20Elden%20Tse%20and%20Jizong%20Peng%20and%20Angela%20Yao%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20framework%20for%20scene%20decomposition%20and%20static%20background%0Areconstruction%20from%20everyday%20videos.%20By%20integrating%20the%20trained%20motion%20masks%0Aand%20modeling%20the%20static%20scene%20as%20Gaussian%20splats%20with%20dynamics-aware%0Aoptimization%2C%20our%20method%20achieves%20more%20accurate%20background%20reconstruction%0Aresults%20than%20previous%20works.%20Our%20proposed%20method%20is%20termed%20DAS3R%2C%20an%0Aabbreviation%20for%20Dynamics-Aware%20Gaussian%20Splatting%20for%20Static%20Scene%0AReconstruction.%20Compared%20to%20existing%20methods%2C%20DAS3R%20is%20more%20robust%20in%20complex%0Amotion%20scenarios%2C%20capable%20of%20handling%20videos%20where%20dynamic%20objects%20occupy%20a%0Asignificant%20portion%20of%20the%20scene%2C%20and%20does%20not%20require%20camera%20pose%20inputs%20or%0Apoint%20cloud%20data%20from%20SLAM-based%20methods.%20We%20compared%20DAS3R%20against%20recent%0Adistractor-free%20approaches%20on%20the%20DAVIS%20and%20Sintel%20datasets%3B%20DAS3R%20demonstrates%0Aenhanced%20performance%20and%20robustness%20with%20a%20margin%20of%20more%20than%202%20dB%20in%20PSNR.%0AThe%20project%27s%20webpage%20can%20be%20accessed%20via%20%5Curl%7Bhttps%3A//kai422.github.io/DAS3R/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAS3R%253A%2520Dynamics-Aware%2520Gaussian%2520Splatting%2520for%2520Static%2520Scene%2520Reconstruction%26entry.906535625%3DKai%2520Xu%2520and%2520Tze%2520Ho%2520Elden%2520Tse%2520and%2520Jizong%2520Peng%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520framework%2520for%2520scene%2520decomposition%2520and%2520static%2520background%250Areconstruction%2520from%2520everyday%2520videos.%2520By%2520integrating%2520the%2520trained%2520motion%2520masks%250Aand%2520modeling%2520the%2520static%2520scene%2520as%2520Gaussian%2520splats%2520with%2520dynamics-aware%250Aoptimization%252C%2520our%2520method%2520achieves%2520more%2520accurate%2520background%2520reconstruction%250Aresults%2520than%2520previous%2520works.%2520Our%2520proposed%2520method%2520is%2520termed%2520DAS3R%252C%2520an%250Aabbreviation%2520for%2520Dynamics-Aware%2520Gaussian%2520Splatting%2520for%2520Static%2520Scene%250AReconstruction.%2520Compared%2520to%2520existing%2520methods%252C%2520DAS3R%2520is%2520more%2520robust%2520in%2520complex%250Amotion%2520scenarios%252C%2520capable%2520of%2520handling%2520videos%2520where%2520dynamic%2520objects%2520occupy%2520a%250Asignificant%2520portion%2520of%2520the%2520scene%252C%2520and%2520does%2520not%2520require%2520camera%2520pose%2520inputs%2520or%250Apoint%2520cloud%2520data%2520from%2520SLAM-based%2520methods.%2520We%2520compared%2520DAS3R%2520against%2520recent%250Adistractor-free%2520approaches%2520on%2520the%2520DAVIS%2520and%2520Sintel%2520datasets%253B%2520DAS3R%2520demonstrates%250Aenhanced%2520performance%2520and%2520robustness%2520with%2520a%2520margin%2520of%2520more%2520than%25202%2520dB%2520in%2520PSNR.%250AThe%2520project%2527s%2520webpage%2520can%2520be%2520accessed%2520via%2520%255Curl%257Bhttps%253A//kai422.github.io/DAS3R/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAS3R%3A%20Dynamics-Aware%20Gaussian%20Splatting%20for%20Static%20Scene%20Reconstruction&entry.906535625=Kai%20Xu%20and%20Tze%20Ho%20Elden%20Tse%20and%20Jizong%20Peng%20and%20Angela%20Yao&entry.1292438233=%20%20We%20propose%20a%20novel%20framework%20for%20scene%20decomposition%20and%20static%20background%0Areconstruction%20from%20everyday%20videos.%20By%20integrating%20the%20trained%20motion%20masks%0Aand%20modeling%20the%20static%20scene%20as%20Gaussian%20splats%20with%20dynamics-aware%0Aoptimization%2C%20our%20method%20achieves%20more%20accurate%20background%20reconstruction%0Aresults%20than%20previous%20works.%20Our%20proposed%20method%20is%20termed%20DAS3R%2C%20an%0Aabbreviation%20for%20Dynamics-Aware%20Gaussian%20Splatting%20for%20Static%20Scene%0AReconstruction.%20Compared%20to%20existing%20methods%2C%20DAS3R%20is%20more%20robust%20in%20complex%0Amotion%20scenarios%2C%20capable%20of%20handling%20videos%20where%20dynamic%20objects%20occupy%20a%0Asignificant%20portion%20of%20the%20scene%2C%20and%20does%20not%20require%20camera%20pose%20inputs%20or%0Apoint%20cloud%20data%20from%20SLAM-based%20methods.%20We%20compared%20DAS3R%20against%20recent%0Adistractor-free%20approaches%20on%20the%20DAVIS%20and%20Sintel%20datasets%3B%20DAS3R%20demonstrates%0Aenhanced%20performance%20and%20robustness%20with%20a%20margin%20of%20more%20than%202%20dB%20in%20PSNR.%0AThe%20project%27s%20webpage%20can%20be%20accessed%20via%20%5Curl%7Bhttps%3A//kai422.github.io/DAS3R/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19584v1&entry.124074799=Read"},
{"title": "Optimizing Local-Global Dependencies for Accurate 3D Human Pose\n  Estimation", "author": "Guangsheng Xu and Guoyi Zhang and Lejia Ye and Shuwei Gan and Xiaohu Zhang and Xia Yang", "abstract": "  Transformer-based methods have recently achieved significant success in 3D\nhuman pose estimation, owing to their strong ability to model long-range\ndependencies. However, relying solely on the global attention mechanism is\ninsufficient for capturing the fine-grained local details, which are crucial\nfor accurate pose estimation. To address this, we propose SSR-STF, a\ndual-stream model that effectively integrates local features with global\ndependencies to enhance 3D human pose estimation. Specifically, we introduce\nSSRFormer, a simple yet effective module that employs the skeleton selective\nrefine attention (SSRA) mechanism to capture fine-grained local dependencies in\nhuman pose sequences, complementing the global dependencies modeled by the\nTransformer. By adaptively fusing these two feature streams, SSR-STF can better\nlearn the underlying structure of human poses, overcoming the limitations of\ntraditional methods in local feature extraction. Extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves\nstate-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm\nrespectively, outperforming existing methods in both accuracy and\ngeneralization. Furthermore, the motion representations learned by our model\nprove effective in downstream tasks such as human mesh recovery. Codes are\navailable at https://github.com/poker-xu/SSR-STF.\n", "link": "http://arxiv.org/abs/2412.19676v1", "date": "2024-12-27", "relevancy": 3.0959, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6595}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6179}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Local-Global%20Dependencies%20for%20Accurate%203D%20Human%20Pose%0A%20%20Estimation&body=Title%3A%20Optimizing%20Local-Global%20Dependencies%20for%20Accurate%203D%20Human%20Pose%0A%20%20Estimation%0AAuthor%3A%20Guangsheng%20Xu%20and%20Guoyi%20Zhang%20and%20Lejia%20Ye%20and%20Shuwei%20Gan%20and%20Xiaohu%20Zhang%20and%20Xia%20Yang%0AAbstract%3A%20%20%20Transformer-based%20methods%20have%20recently%20achieved%20significant%20success%20in%203D%0Ahuman%20pose%20estimation%2C%20owing%20to%20their%20strong%20ability%20to%20model%20long-range%0Adependencies.%20However%2C%20relying%20solely%20on%20the%20global%20attention%20mechanism%20is%0Ainsufficient%20for%20capturing%20the%20fine-grained%20local%20details%2C%20which%20are%20crucial%0Afor%20accurate%20pose%20estimation.%20To%20address%20this%2C%20we%20propose%20SSR-STF%2C%20a%0Adual-stream%20model%20that%20effectively%20integrates%20local%20features%20with%20global%0Adependencies%20to%20enhance%203D%20human%20pose%20estimation.%20Specifically%2C%20we%20introduce%0ASSRFormer%2C%20a%20simple%20yet%20effective%20module%20that%20employs%20the%20skeleton%20selective%0Arefine%20attention%20%28SSRA%29%20mechanism%20to%20capture%20fine-grained%20local%20dependencies%20in%0Ahuman%20pose%20sequences%2C%20complementing%20the%20global%20dependencies%20modeled%20by%20the%0ATransformer.%20By%20adaptively%20fusing%20these%20two%20feature%20streams%2C%20SSR-STF%20can%20better%0Alearn%20the%20underlying%20structure%20of%20human%20poses%2C%20overcoming%20the%20limitations%20of%0Atraditional%20methods%20in%20local%20feature%20extraction.%20Extensive%20experiments%20on%20the%0AHuman3.6M%20and%20MPI-INF-3DHP%20datasets%20demonstrate%20that%20SSR-STF%20achieves%0Astate-of-the-art%20performance%2C%20with%20P1%20errors%20of%2037.4%20mm%20and%2013.2%20mm%0Arespectively%2C%20outperforming%20existing%20methods%20in%20both%20accuracy%20and%0Ageneralization.%20Furthermore%2C%20the%20motion%20representations%20learned%20by%20our%20model%0Aprove%20effective%20in%20downstream%20tasks%20such%20as%20human%20mesh%20recovery.%20Codes%20are%0Aavailable%20at%20https%3A//github.com/poker-xu/SSR-STF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Local-Global%2520Dependencies%2520for%2520Accurate%25203D%2520Human%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DGuangsheng%2520Xu%2520and%2520Guoyi%2520Zhang%2520and%2520Lejia%2520Ye%2520and%2520Shuwei%2520Gan%2520and%2520Xiaohu%2520Zhang%2520and%2520Xia%2520Yang%26entry.1292438233%3D%2520%2520Transformer-based%2520methods%2520have%2520recently%2520achieved%2520significant%2520success%2520in%25203D%250Ahuman%2520pose%2520estimation%252C%2520owing%2520to%2520their%2520strong%2520ability%2520to%2520model%2520long-range%250Adependencies.%2520However%252C%2520relying%2520solely%2520on%2520the%2520global%2520attention%2520mechanism%2520is%250Ainsufficient%2520for%2520capturing%2520the%2520fine-grained%2520local%2520details%252C%2520which%2520are%2520crucial%250Afor%2520accurate%2520pose%2520estimation.%2520To%2520address%2520this%252C%2520we%2520propose%2520SSR-STF%252C%2520a%250Adual-stream%2520model%2520that%2520effectively%2520integrates%2520local%2520features%2520with%2520global%250Adependencies%2520to%2520enhance%25203D%2520human%2520pose%2520estimation.%2520Specifically%252C%2520we%2520introduce%250ASSRFormer%252C%2520a%2520simple%2520yet%2520effective%2520module%2520that%2520employs%2520the%2520skeleton%2520selective%250Arefine%2520attention%2520%2528SSRA%2529%2520mechanism%2520to%2520capture%2520fine-grained%2520local%2520dependencies%2520in%250Ahuman%2520pose%2520sequences%252C%2520complementing%2520the%2520global%2520dependencies%2520modeled%2520by%2520the%250ATransformer.%2520By%2520adaptively%2520fusing%2520these%2520two%2520feature%2520streams%252C%2520SSR-STF%2520can%2520better%250Alearn%2520the%2520underlying%2520structure%2520of%2520human%2520poses%252C%2520overcoming%2520the%2520limitations%2520of%250Atraditional%2520methods%2520in%2520local%2520feature%2520extraction.%2520Extensive%2520experiments%2520on%2520the%250AHuman3.6M%2520and%2520MPI-INF-3DHP%2520datasets%2520demonstrate%2520that%2520SSR-STF%2520achieves%250Astate-of-the-art%2520performance%252C%2520with%2520P1%2520errors%2520of%252037.4%2520mm%2520and%252013.2%2520mm%250Arespectively%252C%2520outperforming%2520existing%2520methods%2520in%2520both%2520accuracy%2520and%250Ageneralization.%2520Furthermore%252C%2520the%2520motion%2520representations%2520learned%2520by%2520our%2520model%250Aprove%2520effective%2520in%2520downstream%2520tasks%2520such%2520as%2520human%2520mesh%2520recovery.%2520Codes%2520are%250Aavailable%2520at%2520https%253A//github.com/poker-xu/SSR-STF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Local-Global%20Dependencies%20for%20Accurate%203D%20Human%20Pose%0A%20%20Estimation&entry.906535625=Guangsheng%20Xu%20and%20Guoyi%20Zhang%20and%20Lejia%20Ye%20and%20Shuwei%20Gan%20and%20Xiaohu%20Zhang%20and%20Xia%20Yang&entry.1292438233=%20%20Transformer-based%20methods%20have%20recently%20achieved%20significant%20success%20in%203D%0Ahuman%20pose%20estimation%2C%20owing%20to%20their%20strong%20ability%20to%20model%20long-range%0Adependencies.%20However%2C%20relying%20solely%20on%20the%20global%20attention%20mechanism%20is%0Ainsufficient%20for%20capturing%20the%20fine-grained%20local%20details%2C%20which%20are%20crucial%0Afor%20accurate%20pose%20estimation.%20To%20address%20this%2C%20we%20propose%20SSR-STF%2C%20a%0Adual-stream%20model%20that%20effectively%20integrates%20local%20features%20with%20global%0Adependencies%20to%20enhance%203D%20human%20pose%20estimation.%20Specifically%2C%20we%20introduce%0ASSRFormer%2C%20a%20simple%20yet%20effective%20module%20that%20employs%20the%20skeleton%20selective%0Arefine%20attention%20%28SSRA%29%20mechanism%20to%20capture%20fine-grained%20local%20dependencies%20in%0Ahuman%20pose%20sequences%2C%20complementing%20the%20global%20dependencies%20modeled%20by%20the%0ATransformer.%20By%20adaptively%20fusing%20these%20two%20feature%20streams%2C%20SSR-STF%20can%20better%0Alearn%20the%20underlying%20structure%20of%20human%20poses%2C%20overcoming%20the%20limitations%20of%0Atraditional%20methods%20in%20local%20feature%20extraction.%20Extensive%20experiments%20on%20the%0AHuman3.6M%20and%20MPI-INF-3DHP%20datasets%20demonstrate%20that%20SSR-STF%20achieves%0Astate-of-the-art%20performance%2C%20with%20P1%20errors%20of%2037.4%20mm%20and%2013.2%20mm%0Arespectively%2C%20outperforming%20existing%20methods%20in%20both%20accuracy%20and%0Ageneralization.%20Furthermore%2C%20the%20motion%20representations%20learned%20by%20our%20model%0Aprove%20effective%20in%20downstream%20tasks%20such%20as%20human%20mesh%20recovery.%20Codes%20are%0Aavailable%20at%20https%3A//github.com/poker-xu/SSR-STF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19676v1&entry.124074799=Read"},
{"title": "Toward Modality Gap: Vision Prototype Learning for Weakly-supervised\n  Semantic Segmentation with CLIP", "author": "Zhongxing Xu and Feilong Tang and Zhe Chen and Yingxue Su and Zhiyi Zhao and Ge Zhang and Jionglong Su and Zongyuan Ge", "abstract": "  The application of Contrastive Language-Image Pre-training (CLIP) in Weakly\nSupervised Semantic Segmentation (WSSS) research powerful cross-modal semantic\nunderstanding capabilities. Existing methods attempt to optimize input text\nprompts for improved alignment of images and text, by finely adjusting text\nprototypes to facilitate semantic matching. Nevertheless, given the modality\ngap between text and vision spaces, the text prototypes employed by these\nmethods have not effectively established a close correspondence with\npixel-level vision features. In this work, our theoretical analysis indicates\nthat the inherent modality gap results in misalignment of text and region\nfeatures, and that this gap cannot be sufficiently reduced by minimizing\ncontrast loss in CLIP. To mitigate the impact of the modality gap, we propose a\nVision Prototype Learning (VPL) framework, by introducing more representative\nvision prototypes. The core of this framework is to learn class-specific vision\nprototypes in vision space with the help of text prototypes, for capturing\nhigh-quality localization maps. Moreover, we propose a regional semantic\ncontrast module that contrasts regions embedding with corresponding prototypes,\nleading to more comprehensive and robust feature learning. Experimental results\nshow that our proposed framework achieves state-of-the-art performance on two\nbenchmark datasets.\n", "link": "http://arxiv.org/abs/2412.19650v1", "date": "2024-12-27", "relevancy": 3.063, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Modality%20Gap%3A%20Vision%20Prototype%20Learning%20for%20Weakly-supervised%0A%20%20Semantic%20Segmentation%20with%20CLIP&body=Title%3A%20Toward%20Modality%20Gap%3A%20Vision%20Prototype%20Learning%20for%20Weakly-supervised%0A%20%20Semantic%20Segmentation%20with%20CLIP%0AAuthor%3A%20Zhongxing%20Xu%20and%20Feilong%20Tang%20and%20Zhe%20Chen%20and%20Yingxue%20Su%20and%20Zhiyi%20Zhao%20and%20Ge%20Zhang%20and%20Jionglong%20Su%20and%20Zongyuan%20Ge%0AAbstract%3A%20%20%20The%20application%20of%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20in%20Weakly%0ASupervised%20Semantic%20Segmentation%20%28WSSS%29%20research%20powerful%20cross-modal%20semantic%0Aunderstanding%20capabilities.%20Existing%20methods%20attempt%20to%20optimize%20input%20text%0Aprompts%20for%20improved%20alignment%20of%20images%20and%20text%2C%20by%20finely%20adjusting%20text%0Aprototypes%20to%20facilitate%20semantic%20matching.%20Nevertheless%2C%20given%20the%20modality%0Agap%20between%20text%20and%20vision%20spaces%2C%20the%20text%20prototypes%20employed%20by%20these%0Amethods%20have%20not%20effectively%20established%20a%20close%20correspondence%20with%0Apixel-level%20vision%20features.%20In%20this%20work%2C%20our%20theoretical%20analysis%20indicates%0Athat%20the%20inherent%20modality%20gap%20results%20in%20misalignment%20of%20text%20and%20region%0Afeatures%2C%20and%20that%20this%20gap%20cannot%20be%20sufficiently%20reduced%20by%20minimizing%0Acontrast%20loss%20in%20CLIP.%20To%20mitigate%20the%20impact%20of%20the%20modality%20gap%2C%20we%20propose%20a%0AVision%20Prototype%20Learning%20%28VPL%29%20framework%2C%20by%20introducing%20more%20representative%0Avision%20prototypes.%20The%20core%20of%20this%20framework%20is%20to%20learn%20class-specific%20vision%0Aprototypes%20in%20vision%20space%20with%20the%20help%20of%20text%20prototypes%2C%20for%20capturing%0Ahigh-quality%20localization%20maps.%20Moreover%2C%20we%20propose%20a%20regional%20semantic%0Acontrast%20module%20that%20contrasts%20regions%20embedding%20with%20corresponding%20prototypes%2C%0Aleading%20to%20more%20comprehensive%20and%20robust%20feature%20learning.%20Experimental%20results%0Ashow%20that%20our%20proposed%20framework%20achieves%20state-of-the-art%20performance%20on%20two%0Abenchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Modality%2520Gap%253A%2520Vision%2520Prototype%2520Learning%2520for%2520Weakly-supervised%250A%2520%2520Semantic%2520Segmentation%2520with%2520CLIP%26entry.906535625%3DZhongxing%2520Xu%2520and%2520Feilong%2520Tang%2520and%2520Zhe%2520Chen%2520and%2520Yingxue%2520Su%2520and%2520Zhiyi%2520Zhao%2520and%2520Ge%2520Zhang%2520and%2520Jionglong%2520Su%2520and%2520Zongyuan%2520Ge%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520in%2520Weakly%250ASupervised%2520Semantic%2520Segmentation%2520%2528WSSS%2529%2520research%2520powerful%2520cross-modal%2520semantic%250Aunderstanding%2520capabilities.%2520Existing%2520methods%2520attempt%2520to%2520optimize%2520input%2520text%250Aprompts%2520for%2520improved%2520alignment%2520of%2520images%2520and%2520text%252C%2520by%2520finely%2520adjusting%2520text%250Aprototypes%2520to%2520facilitate%2520semantic%2520matching.%2520Nevertheless%252C%2520given%2520the%2520modality%250Agap%2520between%2520text%2520and%2520vision%2520spaces%252C%2520the%2520text%2520prototypes%2520employed%2520by%2520these%250Amethods%2520have%2520not%2520effectively%2520established%2520a%2520close%2520correspondence%2520with%250Apixel-level%2520vision%2520features.%2520In%2520this%2520work%252C%2520our%2520theoretical%2520analysis%2520indicates%250Athat%2520the%2520inherent%2520modality%2520gap%2520results%2520in%2520misalignment%2520of%2520text%2520and%2520region%250Afeatures%252C%2520and%2520that%2520this%2520gap%2520cannot%2520be%2520sufficiently%2520reduced%2520by%2520minimizing%250Acontrast%2520loss%2520in%2520CLIP.%2520To%2520mitigate%2520the%2520impact%2520of%2520the%2520modality%2520gap%252C%2520we%2520propose%2520a%250AVision%2520Prototype%2520Learning%2520%2528VPL%2529%2520framework%252C%2520by%2520introducing%2520more%2520representative%250Avision%2520prototypes.%2520The%2520core%2520of%2520this%2520framework%2520is%2520to%2520learn%2520class-specific%2520vision%250Aprototypes%2520in%2520vision%2520space%2520with%2520the%2520help%2520of%2520text%2520prototypes%252C%2520for%2520capturing%250Ahigh-quality%2520localization%2520maps.%2520Moreover%252C%2520we%2520propose%2520a%2520regional%2520semantic%250Acontrast%2520module%2520that%2520contrasts%2520regions%2520embedding%2520with%2520corresponding%2520prototypes%252C%250Aleading%2520to%2520more%2520comprehensive%2520and%2520robust%2520feature%2520learning.%2520Experimental%2520results%250Ashow%2520that%2520our%2520proposed%2520framework%2520achieves%2520state-of-the-art%2520performance%2520on%2520two%250Abenchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Modality%20Gap%3A%20Vision%20Prototype%20Learning%20for%20Weakly-supervised%0A%20%20Semantic%20Segmentation%20with%20CLIP&entry.906535625=Zhongxing%20Xu%20and%20Feilong%20Tang%20and%20Zhe%20Chen%20and%20Yingxue%20Su%20and%20Zhiyi%20Zhao%20and%20Ge%20Zhang%20and%20Jionglong%20Su%20and%20Zongyuan%20Ge&entry.1292438233=%20%20The%20application%20of%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20in%20Weakly%0ASupervised%20Semantic%20Segmentation%20%28WSSS%29%20research%20powerful%20cross-modal%20semantic%0Aunderstanding%20capabilities.%20Existing%20methods%20attempt%20to%20optimize%20input%20text%0Aprompts%20for%20improved%20alignment%20of%20images%20and%20text%2C%20by%20finely%20adjusting%20text%0Aprototypes%20to%20facilitate%20semantic%20matching.%20Nevertheless%2C%20given%20the%20modality%0Agap%20between%20text%20and%20vision%20spaces%2C%20the%20text%20prototypes%20employed%20by%20these%0Amethods%20have%20not%20effectively%20established%20a%20close%20correspondence%20with%0Apixel-level%20vision%20features.%20In%20this%20work%2C%20our%20theoretical%20analysis%20indicates%0Athat%20the%20inherent%20modality%20gap%20results%20in%20misalignment%20of%20text%20and%20region%0Afeatures%2C%20and%20that%20this%20gap%20cannot%20be%20sufficiently%20reduced%20by%20minimizing%0Acontrast%20loss%20in%20CLIP.%20To%20mitigate%20the%20impact%20of%20the%20modality%20gap%2C%20we%20propose%20a%0AVision%20Prototype%20Learning%20%28VPL%29%20framework%2C%20by%20introducing%20more%20representative%0Avision%20prototypes.%20The%20core%20of%20this%20framework%20is%20to%20learn%20class-specific%20vision%0Aprototypes%20in%20vision%20space%20with%20the%20help%20of%20text%20prototypes%2C%20for%20capturing%0Ahigh-quality%20localization%20maps.%20Moreover%2C%20we%20propose%20a%20regional%20semantic%0Acontrast%20module%20that%20contrasts%20regions%20embedding%20with%20corresponding%20prototypes%2C%0Aleading%20to%20more%20comprehensive%20and%20robust%20feature%20learning.%20Experimental%20results%0Ashow%20that%20our%20proposed%20framework%20achieves%20state-of-the-art%20performance%20on%20two%0Abenchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19650v1&entry.124074799=Read"},
{"title": "MVTamperBench: Evaluating Robustness of Vision-Language Models", "author": "Amit Agarwal and Srikant Panda and Angeline Charles and Bhargava Kumar and Hitesh Patel and Priyanranjan Pattnayak and Taki Hasan Rafi and Tejaswini Kumar and Dong-Kyu Chae", "abstract": "  Recent advancements in Vision-Language Models (VLMs) have enabled significant\nprogress in complex video understanding tasks. However, their robustness to\nreal-world manipulations remains underexplored, limiting their reliability in\ncritical applications. To address this gap, we introduce MVTamperBench, a\ncomprehensive benchmark designed to evaluate VLM's resilience to video\ntampering effects, including rotation, dropping, masking, substitution, and\nrepetition. By systematically assessing state-of-the-art models, MVTamperBench\nreveals substantial variability in robustness, with models like InternVL2-8B\nachieving high performance, while others, such as Llama-VILA1.5-8B, exhibit\nsevere vulnerabilities. To foster broader adoption and reproducibility,\nMVTamperBench is integrated into VLMEvalKit, a modular evaluation toolkit,\nenabling streamlined testing and facilitating advancements in model robustness.\nOur benchmark represents a critical step towards developing tamper-resilient\nVLMs, ensuring their dependability in real-world scenarios.\n  Project Page: https://amitbcp.github.io/MVTamperBench/\n", "link": "http://arxiv.org/abs/2412.19794v1", "date": "2024-12-27", "relevancy": 2.7633, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models&body=Title%3A%20MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models%0AAuthor%3A%20Amit%20Agarwal%20and%20Srikant%20Panda%20and%20Angeline%20Charles%20and%20Bhargava%20Kumar%20and%20Hitesh%20Patel%20and%20Priyanranjan%20Pattnayak%20and%20Taki%20Hasan%20Rafi%20and%20Tejaswini%20Kumar%20and%20Dong-Kyu%20Chae%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20enabled%20significant%0Aprogress%20in%20complex%20video%20understanding%20tasks.%20However%2C%20their%20robustness%20to%0Areal-world%20manipulations%20remains%20underexplored%2C%20limiting%20their%20reliability%20in%0Acritical%20applications.%20To%20address%20this%20gap%2C%20we%20introduce%20MVTamperBench%2C%20a%0Acomprehensive%20benchmark%20designed%20to%20evaluate%20VLM%27s%20resilience%20to%20video%0Atampering%20effects%2C%20including%20rotation%2C%20dropping%2C%20masking%2C%20substitution%2C%20and%0Arepetition.%20By%20systematically%20assessing%20state-of-the-art%20models%2C%20MVTamperBench%0Areveals%20substantial%20variability%20in%20robustness%2C%20with%20models%20like%20InternVL2-8B%0Aachieving%20high%20performance%2C%20while%20others%2C%20such%20as%20Llama-VILA1.5-8B%2C%20exhibit%0Asevere%20vulnerabilities.%20To%20foster%20broader%20adoption%20and%20reproducibility%2C%0AMVTamperBench%20is%20integrated%20into%20VLMEvalKit%2C%20a%20modular%20evaluation%20toolkit%2C%0Aenabling%20streamlined%20testing%20and%20facilitating%20advancements%20in%20model%20robustness.%0AOur%20benchmark%20represents%20a%20critical%20step%20towards%20developing%20tamper-resilient%0AVLMs%2C%20ensuring%20their%20dependability%20in%20real-world%20scenarios.%0A%20%20Project%20Page%3A%20https%3A//amitbcp.github.io/MVTamperBench/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVTamperBench%253A%2520Evaluating%2520Robustness%2520of%2520Vision-Language%2520Models%26entry.906535625%3DAmit%2520Agarwal%2520and%2520Srikant%2520Panda%2520and%2520Angeline%2520Charles%2520and%2520Bhargava%2520Kumar%2520and%2520Hitesh%2520Patel%2520and%2520Priyanranjan%2520Pattnayak%2520and%2520Taki%2520Hasan%2520Rafi%2520and%2520Tejaswini%2520Kumar%2520and%2520Dong-Kyu%2520Chae%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520enabled%2520significant%250Aprogress%2520in%2520complex%2520video%2520understanding%2520tasks.%2520However%252C%2520their%2520robustness%2520to%250Areal-world%2520manipulations%2520remains%2520underexplored%252C%2520limiting%2520their%2520reliability%2520in%250Acritical%2520applications.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520MVTamperBench%252C%2520a%250Acomprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520VLM%2527s%2520resilience%2520to%2520video%250Atampering%2520effects%252C%2520including%2520rotation%252C%2520dropping%252C%2520masking%252C%2520substitution%252C%2520and%250Arepetition.%2520By%2520systematically%2520assessing%2520state-of-the-art%2520models%252C%2520MVTamperBench%250Areveals%2520substantial%2520variability%2520in%2520robustness%252C%2520with%2520models%2520like%2520InternVL2-8B%250Aachieving%2520high%2520performance%252C%2520while%2520others%252C%2520such%2520as%2520Llama-VILA1.5-8B%252C%2520exhibit%250Asevere%2520vulnerabilities.%2520To%2520foster%2520broader%2520adoption%2520and%2520reproducibility%252C%250AMVTamperBench%2520is%2520integrated%2520into%2520VLMEvalKit%252C%2520a%2520modular%2520evaluation%2520toolkit%252C%250Aenabling%2520streamlined%2520testing%2520and%2520facilitating%2520advancements%2520in%2520model%2520robustness.%250AOur%2520benchmark%2520represents%2520a%2520critical%2520step%2520towards%2520developing%2520tamper-resilient%250AVLMs%252C%2520ensuring%2520their%2520dependability%2520in%2520real-world%2520scenarios.%250A%2520%2520Project%2520Page%253A%2520https%253A//amitbcp.github.io/MVTamperBench/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVTamperBench%3A%20Evaluating%20Robustness%20of%20Vision-Language%20Models&entry.906535625=Amit%20Agarwal%20and%20Srikant%20Panda%20and%20Angeline%20Charles%20and%20Bhargava%20Kumar%20and%20Hitesh%20Patel%20and%20Priyanranjan%20Pattnayak%20and%20Taki%20Hasan%20Rafi%20and%20Tejaswini%20Kumar%20and%20Dong-Kyu%20Chae&entry.1292438233=%20%20Recent%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20enabled%20significant%0Aprogress%20in%20complex%20video%20understanding%20tasks.%20However%2C%20their%20robustness%20to%0Areal-world%20manipulations%20remains%20underexplored%2C%20limiting%20their%20reliability%20in%0Acritical%20applications.%20To%20address%20this%20gap%2C%20we%20introduce%20MVTamperBench%2C%20a%0Acomprehensive%20benchmark%20designed%20to%20evaluate%20VLM%27s%20resilience%20to%20video%0Atampering%20effects%2C%20including%20rotation%2C%20dropping%2C%20masking%2C%20substitution%2C%20and%0Arepetition.%20By%20systematically%20assessing%20state-of-the-art%20models%2C%20MVTamperBench%0Areveals%20substantial%20variability%20in%20robustness%2C%20with%20models%20like%20InternVL2-8B%0Aachieving%20high%20performance%2C%20while%20others%2C%20such%20as%20Llama-VILA1.5-8B%2C%20exhibit%0Asevere%20vulnerabilities.%20To%20foster%20broader%20adoption%20and%20reproducibility%2C%0AMVTamperBench%20is%20integrated%20into%20VLMEvalKit%2C%20a%20modular%20evaluation%20toolkit%2C%0Aenabling%20streamlined%20testing%20and%20facilitating%20advancements%20in%20model%20robustness.%0AOur%20benchmark%20represents%20a%20critical%20step%20towards%20developing%20tamper-resilient%0AVLMs%2C%20ensuring%20their%20dependability%20in%20real-world%20scenarios.%0A%20%20Project%20Page%3A%20https%3A//amitbcp.github.io/MVTamperBench/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19794v1&entry.124074799=Read"},
{"title": "DGNN-YOLO: Interpretable Dynamic Graph Neural Networks with YOLO11 for\n  Small Object Detection and Tracking in Traffic Surveillance", "author": "Shahriar Soudeep and M. F. Mridha and Md Abrar Jahin and Nilanjan Dey", "abstract": "  Accurate detection and tracking of small objects, such as pedestrians,\ncyclists, and motorbikes, is critical for traffic surveillance systems, which\nare crucial for improving road safety and decision-making in intelligent\ntransportation systems. However, traditional methods face challenges such as\nocclusion, low resolution, and dynamic traffic conditions, necessitating\ninnovative approaches to address these limitations. This paper introduces\nDGNN-YOLO, a novel framework integrating dynamic graph neural networks (DGNN)\nwith YOLO11 to enhance small-object detection and tracking in traffic\nsurveillance systems. The framework leverages YOLO11's advanced spatial feature\nextraction capabilities for precise object detection and incorporates a DGNN to\nmodel spatial-temporal relationships for robust real-time tracking dynamically.\nBy constructing and updating graph structures, DGNN-YOLO effectively represents\nobjects as nodes and their interactions as edges, thereby ensuring adaptive and\naccurate tracking in complex and dynamic environments. Additionally, Grad-CAM,\nGrad-CAM++, and Eigen-CAM visualization techniques were applied to DGNN-YOLO to\nprovide model-agnostic interpretability and deeper insights into the model's\ndecision-making process, enhancing its transparency and trustworthiness.\nExtensive experiments demonstrated that DGNN-YOLO consistently outperformed\nstate-of-the-art methods in detecting and tracking small objects under diverse\ntraffic conditions, achieving the highest precision (0.8382), recall (0.6875),\nand mAP@0.5:0.95 (0.6476), showing its robustness and scalability, particularly\nin challenging scenarios involving small and occluded objects. This study\nprovides a scalable, real-time traffic surveillance and analysis solution,\nsignificantly contributing to intelligent transportation systems.\n", "link": "http://arxiv.org/abs/2411.17251v4", "date": "2024-12-27", "relevancy": 2.7539, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5618}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5565}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGNN-YOLO%3A%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%0A%20%20Small%20Object%20Detection%20and%20Tracking%20in%20Traffic%20Surveillance&body=Title%3A%20DGNN-YOLO%3A%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%0A%20%20Small%20Object%20Detection%20and%20Tracking%20in%20Traffic%20Surveillance%0AAuthor%3A%20Shahriar%20Soudeep%20and%20M.%20F.%20Mridha%20and%20Md%20Abrar%20Jahin%20and%20Nilanjan%20Dey%0AAbstract%3A%20%20%20Accurate%20detection%20and%20tracking%20of%20small%20objects%2C%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%2C%20is%20critical%20for%20traffic%20surveillance%20systems%2C%20which%0Aare%20crucial%20for%20improving%20road%20safety%20and%20decision-making%20in%20intelligent%0Atransportation%20systems.%20However%2C%20traditional%20methods%20face%20challenges%20such%20as%0Aocclusion%2C%20low%20resolution%2C%20and%20dynamic%20traffic%20conditions%2C%20necessitating%0Ainnovative%20approaches%20to%20address%20these%20limitations.%20This%20paper%20introduces%0ADGNN-YOLO%2C%20a%20novel%20framework%20integrating%20dynamic%20graph%20neural%20networks%20%28DGNN%29%0Awith%20YOLO11%20to%20enhance%20small-object%20detection%20and%20tracking%20in%20traffic%0Asurveillance%20systems.%20The%20framework%20leverages%20YOLO11%27s%20advanced%20spatial%20feature%0Aextraction%20capabilities%20for%20precise%20object%20detection%20and%20incorporates%20a%20DGNN%20to%0Amodel%20spatial-temporal%20relationships%20for%20robust%20real-time%20tracking%20dynamically.%0ABy%20constructing%20and%20updating%20graph%20structures%2C%20DGNN-YOLO%20effectively%20represents%0Aobjects%20as%20nodes%20and%20their%20interactions%20as%20edges%2C%20thereby%20ensuring%20adaptive%20and%0Aaccurate%20tracking%20in%20complex%20and%20dynamic%20environments.%20Additionally%2C%20Grad-CAM%2C%0AGrad-CAM%2B%2B%2C%20and%20Eigen-CAM%20visualization%20techniques%20were%20applied%20to%20DGNN-YOLO%20to%0Aprovide%20model-agnostic%20interpretability%20and%20deeper%20insights%20into%20the%20model%27s%0Adecision-making%20process%2C%20enhancing%20its%20transparency%20and%20trustworthiness.%0AExtensive%20experiments%20demonstrated%20that%20DGNN-YOLO%20consistently%20outperformed%0Astate-of-the-art%20methods%20in%20detecting%20and%20tracking%20small%20objects%20under%20diverse%0Atraffic%20conditions%2C%20achieving%20the%20highest%20precision%20%280.8382%29%2C%20recall%20%280.6875%29%2C%0Aand%20mAP%400.5%3A0.95%20%280.6476%29%2C%20showing%20its%20robustness%20and%20scalability%2C%20particularly%0Ain%20challenging%20scenarios%20involving%20small%20and%20occluded%20objects.%20This%20study%0Aprovides%20a%20scalable%2C%20real-time%20traffic%20surveillance%20and%20analysis%20solution%2C%0Asignificantly%20contributing%20to%20intelligent%20transportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17251v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGNN-YOLO%253A%2520Interpretable%2520Dynamic%2520Graph%2520Neural%2520Networks%2520with%2520YOLO11%2520for%250A%2520%2520Small%2520Object%2520Detection%2520and%2520Tracking%2520in%2520Traffic%2520Surveillance%26entry.906535625%3DShahriar%2520Soudeep%2520and%2520M.%2520F.%2520Mridha%2520and%2520Md%2520Abrar%2520Jahin%2520and%2520Nilanjan%2520Dey%26entry.1292438233%3D%2520%2520Accurate%2520detection%2520and%2520tracking%2520of%2520small%2520objects%252C%2520such%2520as%2520pedestrians%252C%250Acyclists%252C%2520and%2520motorbikes%252C%2520is%2520critical%2520for%2520traffic%2520surveillance%2520systems%252C%2520which%250Aare%2520crucial%2520for%2520improving%2520road%2520safety%2520and%2520decision-making%2520in%2520intelligent%250Atransportation%2520systems.%2520However%252C%2520traditional%2520methods%2520face%2520challenges%2520such%2520as%250Aocclusion%252C%2520low%2520resolution%252C%2520and%2520dynamic%2520traffic%2520conditions%252C%2520necessitating%250Ainnovative%2520approaches%2520to%2520address%2520these%2520limitations.%2520This%2520paper%2520introduces%250ADGNN-YOLO%252C%2520a%2520novel%2520framework%2520integrating%2520dynamic%2520graph%2520neural%2520networks%2520%2528DGNN%2529%250Awith%2520YOLO11%2520to%2520enhance%2520small-object%2520detection%2520and%2520tracking%2520in%2520traffic%250Asurveillance%2520systems.%2520The%2520framework%2520leverages%2520YOLO11%2527s%2520advanced%2520spatial%2520feature%250Aextraction%2520capabilities%2520for%2520precise%2520object%2520detection%2520and%2520incorporates%2520a%2520DGNN%2520to%250Amodel%2520spatial-temporal%2520relationships%2520for%2520robust%2520real-time%2520tracking%2520dynamically.%250ABy%2520constructing%2520and%2520updating%2520graph%2520structures%252C%2520DGNN-YOLO%2520effectively%2520represents%250Aobjects%2520as%2520nodes%2520and%2520their%2520interactions%2520as%2520edges%252C%2520thereby%2520ensuring%2520adaptive%2520and%250Aaccurate%2520tracking%2520in%2520complex%2520and%2520dynamic%2520environments.%2520Additionally%252C%2520Grad-CAM%252C%250AGrad-CAM%252B%252B%252C%2520and%2520Eigen-CAM%2520visualization%2520techniques%2520were%2520applied%2520to%2520DGNN-YOLO%2520to%250Aprovide%2520model-agnostic%2520interpretability%2520and%2520deeper%2520insights%2520into%2520the%2520model%2527s%250Adecision-making%2520process%252C%2520enhancing%2520its%2520transparency%2520and%2520trustworthiness.%250AExtensive%2520experiments%2520demonstrated%2520that%2520DGNN-YOLO%2520consistently%2520outperformed%250Astate-of-the-art%2520methods%2520in%2520detecting%2520and%2520tracking%2520small%2520objects%2520under%2520diverse%250Atraffic%2520conditions%252C%2520achieving%2520the%2520highest%2520precision%2520%25280.8382%2529%252C%2520recall%2520%25280.6875%2529%252C%250Aand%2520mAP%25400.5%253A0.95%2520%25280.6476%2529%252C%2520showing%2520its%2520robustness%2520and%2520scalability%252C%2520particularly%250Ain%2520challenging%2520scenarios%2520involving%2520small%2520and%2520occluded%2520objects.%2520This%2520study%250Aprovides%2520a%2520scalable%252C%2520real-time%2520traffic%2520surveillance%2520and%2520analysis%2520solution%252C%250Asignificantly%2520contributing%2520to%2520intelligent%2520transportation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17251v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGNN-YOLO%3A%20Interpretable%20Dynamic%20Graph%20Neural%20Networks%20with%20YOLO11%20for%0A%20%20Small%20Object%20Detection%20and%20Tracking%20in%20Traffic%20Surveillance&entry.906535625=Shahriar%20Soudeep%20and%20M.%20F.%20Mridha%20and%20Md%20Abrar%20Jahin%20and%20Nilanjan%20Dey&entry.1292438233=%20%20Accurate%20detection%20and%20tracking%20of%20small%20objects%2C%20such%20as%20pedestrians%2C%0Acyclists%2C%20and%20motorbikes%2C%20is%20critical%20for%20traffic%20surveillance%20systems%2C%20which%0Aare%20crucial%20for%20improving%20road%20safety%20and%20decision-making%20in%20intelligent%0Atransportation%20systems.%20However%2C%20traditional%20methods%20face%20challenges%20such%20as%0Aocclusion%2C%20low%20resolution%2C%20and%20dynamic%20traffic%20conditions%2C%20necessitating%0Ainnovative%20approaches%20to%20address%20these%20limitations.%20This%20paper%20introduces%0ADGNN-YOLO%2C%20a%20novel%20framework%20integrating%20dynamic%20graph%20neural%20networks%20%28DGNN%29%0Awith%20YOLO11%20to%20enhance%20small-object%20detection%20and%20tracking%20in%20traffic%0Asurveillance%20systems.%20The%20framework%20leverages%20YOLO11%27s%20advanced%20spatial%20feature%0Aextraction%20capabilities%20for%20precise%20object%20detection%20and%20incorporates%20a%20DGNN%20to%0Amodel%20spatial-temporal%20relationships%20for%20robust%20real-time%20tracking%20dynamically.%0ABy%20constructing%20and%20updating%20graph%20structures%2C%20DGNN-YOLO%20effectively%20represents%0Aobjects%20as%20nodes%20and%20their%20interactions%20as%20edges%2C%20thereby%20ensuring%20adaptive%20and%0Aaccurate%20tracking%20in%20complex%20and%20dynamic%20environments.%20Additionally%2C%20Grad-CAM%2C%0AGrad-CAM%2B%2B%2C%20and%20Eigen-CAM%20visualization%20techniques%20were%20applied%20to%20DGNN-YOLO%20to%0Aprovide%20model-agnostic%20interpretability%20and%20deeper%20insights%20into%20the%20model%27s%0Adecision-making%20process%2C%20enhancing%20its%20transparency%20and%20trustworthiness.%0AExtensive%20experiments%20demonstrated%20that%20DGNN-YOLO%20consistently%20outperformed%0Astate-of-the-art%20methods%20in%20detecting%20and%20tracking%20small%20objects%20under%20diverse%0Atraffic%20conditions%2C%20achieving%20the%20highest%20precision%20%280.8382%29%2C%20recall%20%280.6875%29%2C%0Aand%20mAP%400.5%3A0.95%20%280.6476%29%2C%20showing%20its%20robustness%20and%20scalability%2C%20particularly%0Ain%20challenging%20scenarios%20involving%20small%20and%20occluded%20objects.%20This%20study%0Aprovides%20a%20scalable%2C%20real-time%20traffic%20surveillance%20and%20analysis%20solution%2C%0Asignificantly%20contributing%20to%20intelligent%20transportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17251v4&entry.124074799=Read"},
{"title": "Generative Pretrained Embedding and Hierarchical Irregular Time Series\n  Representation for Daily Living Activity Recognition", "author": "Damien Bouchabou and Sao Mai Nguyen", "abstract": "  Within the evolving landscape of smart homes, the precise recognition of\ndaily living activities using ambient sensor data stands paramount. This paper\nnot only aims to bolster existing algorithms by evaluating two distinct\npretrained embeddings suited for ambient sensor activations but also introduces\na novel hierarchical architecture. We delve into an architecture anchored on\nTransformer Decoder-based pre-trained embeddings, reminiscent of the GPT\ndesign, and contrast it with the previously established state-of-the-art (SOTA)\nELMo embeddings for ambient sensors. Our proposed hierarchical structure\nleverages the strengths of each pre-trained embedding, enabling the discernment\nof activity dependencies and sequence order, thereby enhancing classification\nprecision. To further refine recognition, we incorporate into our proposed\narchitecture an hour-of-the-day embedding. Empirical evaluations underscore the\npreeminence of the Transformer Decoder embedding in classification endeavors.\nAdditionally, our innovative hierarchical design significantly bolsters the\nefficacy of both pre-trained embeddings, notably in capturing inter-activity\nnuances. The integration of temporal aspects subtly but distinctively augments\nclassification, especially for time-sensitive activities. In conclusion, our\nGPT-inspired hierarchical approach, infused with temporal insights, outshines\nthe SOTA ELMo benchmark.\n", "link": "http://arxiv.org/abs/2412.19732v1", "date": "2024-12-27", "relevancy": 2.7377, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6027}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5202}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Pretrained%20Embedding%20and%20Hierarchical%20Irregular%20Time%20Series%0A%20%20Representation%20for%20Daily%20Living%20Activity%20Recognition&body=Title%3A%20Generative%20Pretrained%20Embedding%20and%20Hierarchical%20Irregular%20Time%20Series%0A%20%20Representation%20for%20Daily%20Living%20Activity%20Recognition%0AAuthor%3A%20Damien%20Bouchabou%20and%20Sao%20Mai%20Nguyen%0AAbstract%3A%20%20%20Within%20the%20evolving%20landscape%20of%20smart%20homes%2C%20the%20precise%20recognition%20of%0Adaily%20living%20activities%20using%20ambient%20sensor%20data%20stands%20paramount.%20This%20paper%0Anot%20only%20aims%20to%20bolster%20existing%20algorithms%20by%20evaluating%20two%20distinct%0Apretrained%20embeddings%20suited%20for%20ambient%20sensor%20activations%20but%20also%20introduces%0Aa%20novel%20hierarchical%20architecture.%20We%20delve%20into%20an%20architecture%20anchored%20on%0ATransformer%20Decoder-based%20pre-trained%20embeddings%2C%20reminiscent%20of%20the%20GPT%0Adesign%2C%20and%20contrast%20it%20with%20the%20previously%20established%20state-of-the-art%20%28SOTA%29%0AELMo%20embeddings%20for%20ambient%20sensors.%20Our%20proposed%20hierarchical%20structure%0Aleverages%20the%20strengths%20of%20each%20pre-trained%20embedding%2C%20enabling%20the%20discernment%0Aof%20activity%20dependencies%20and%20sequence%20order%2C%20thereby%20enhancing%20classification%0Aprecision.%20To%20further%20refine%20recognition%2C%20we%20incorporate%20into%20our%20proposed%0Aarchitecture%20an%20hour-of-the-day%20embedding.%20Empirical%20evaluations%20underscore%20the%0Apreeminence%20of%20the%20Transformer%20Decoder%20embedding%20in%20classification%20endeavors.%0AAdditionally%2C%20our%20innovative%20hierarchical%20design%20significantly%20bolsters%20the%0Aefficacy%20of%20both%20pre-trained%20embeddings%2C%20notably%20in%20capturing%20inter-activity%0Anuances.%20The%20integration%20of%20temporal%20aspects%20subtly%20but%20distinctively%20augments%0Aclassification%2C%20especially%20for%20time-sensitive%20activities.%20In%20conclusion%2C%20our%0AGPT-inspired%20hierarchical%20approach%2C%20infused%20with%20temporal%20insights%2C%20outshines%0Athe%20SOTA%20ELMo%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Pretrained%2520Embedding%2520and%2520Hierarchical%2520Irregular%2520Time%2520Series%250A%2520%2520Representation%2520for%2520Daily%2520Living%2520Activity%2520Recognition%26entry.906535625%3DDamien%2520Bouchabou%2520and%2520Sao%2520Mai%2520Nguyen%26entry.1292438233%3D%2520%2520Within%2520the%2520evolving%2520landscape%2520of%2520smart%2520homes%252C%2520the%2520precise%2520recognition%2520of%250Adaily%2520living%2520activities%2520using%2520ambient%2520sensor%2520data%2520stands%2520paramount.%2520This%2520paper%250Anot%2520only%2520aims%2520to%2520bolster%2520existing%2520algorithms%2520by%2520evaluating%2520two%2520distinct%250Apretrained%2520embeddings%2520suited%2520for%2520ambient%2520sensor%2520activations%2520but%2520also%2520introduces%250Aa%2520novel%2520hierarchical%2520architecture.%2520We%2520delve%2520into%2520an%2520architecture%2520anchored%2520on%250ATransformer%2520Decoder-based%2520pre-trained%2520embeddings%252C%2520reminiscent%2520of%2520the%2520GPT%250Adesign%252C%2520and%2520contrast%2520it%2520with%2520the%2520previously%2520established%2520state-of-the-art%2520%2528SOTA%2529%250AELMo%2520embeddings%2520for%2520ambient%2520sensors.%2520Our%2520proposed%2520hierarchical%2520structure%250Aleverages%2520the%2520strengths%2520of%2520each%2520pre-trained%2520embedding%252C%2520enabling%2520the%2520discernment%250Aof%2520activity%2520dependencies%2520and%2520sequence%2520order%252C%2520thereby%2520enhancing%2520classification%250Aprecision.%2520To%2520further%2520refine%2520recognition%252C%2520we%2520incorporate%2520into%2520our%2520proposed%250Aarchitecture%2520an%2520hour-of-the-day%2520embedding.%2520Empirical%2520evaluations%2520underscore%2520the%250Apreeminence%2520of%2520the%2520Transformer%2520Decoder%2520embedding%2520in%2520classification%2520endeavors.%250AAdditionally%252C%2520our%2520innovative%2520hierarchical%2520design%2520significantly%2520bolsters%2520the%250Aefficacy%2520of%2520both%2520pre-trained%2520embeddings%252C%2520notably%2520in%2520capturing%2520inter-activity%250Anuances.%2520The%2520integration%2520of%2520temporal%2520aspects%2520subtly%2520but%2520distinctively%2520augments%250Aclassification%252C%2520especially%2520for%2520time-sensitive%2520activities.%2520In%2520conclusion%252C%2520our%250AGPT-inspired%2520hierarchical%2520approach%252C%2520infused%2520with%2520temporal%2520insights%252C%2520outshines%250Athe%2520SOTA%2520ELMo%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Pretrained%20Embedding%20and%20Hierarchical%20Irregular%20Time%20Series%0A%20%20Representation%20for%20Daily%20Living%20Activity%20Recognition&entry.906535625=Damien%20Bouchabou%20and%20Sao%20Mai%20Nguyen&entry.1292438233=%20%20Within%20the%20evolving%20landscape%20of%20smart%20homes%2C%20the%20precise%20recognition%20of%0Adaily%20living%20activities%20using%20ambient%20sensor%20data%20stands%20paramount.%20This%20paper%0Anot%20only%20aims%20to%20bolster%20existing%20algorithms%20by%20evaluating%20two%20distinct%0Apretrained%20embeddings%20suited%20for%20ambient%20sensor%20activations%20but%20also%20introduces%0Aa%20novel%20hierarchical%20architecture.%20We%20delve%20into%20an%20architecture%20anchored%20on%0ATransformer%20Decoder-based%20pre-trained%20embeddings%2C%20reminiscent%20of%20the%20GPT%0Adesign%2C%20and%20contrast%20it%20with%20the%20previously%20established%20state-of-the-art%20%28SOTA%29%0AELMo%20embeddings%20for%20ambient%20sensors.%20Our%20proposed%20hierarchical%20structure%0Aleverages%20the%20strengths%20of%20each%20pre-trained%20embedding%2C%20enabling%20the%20discernment%0Aof%20activity%20dependencies%20and%20sequence%20order%2C%20thereby%20enhancing%20classification%0Aprecision.%20To%20further%20refine%20recognition%2C%20we%20incorporate%20into%20our%20proposed%0Aarchitecture%20an%20hour-of-the-day%20embedding.%20Empirical%20evaluations%20underscore%20the%0Apreeminence%20of%20the%20Transformer%20Decoder%20embedding%20in%20classification%20endeavors.%0AAdditionally%2C%20our%20innovative%20hierarchical%20design%20significantly%20bolsters%20the%0Aefficacy%20of%20both%20pre-trained%20embeddings%2C%20notably%20in%20capturing%20inter-activity%0Anuances.%20The%20integration%20of%20temporal%20aspects%20subtly%20but%20distinctively%20augments%0Aclassification%2C%20especially%20for%20time-sensitive%20activities.%20In%20conclusion%2C%20our%0AGPT-inspired%20hierarchical%20approach%2C%20infused%20with%20temporal%20insights%2C%20outshines%0Athe%20SOTA%20ELMo%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19732v1&entry.124074799=Read"},
{"title": "VideoMaker: Zero-shot Customized Video Generation with the Inherent\n  Force of Video Diffusion Models", "author": "Tao Wu and Yong Zhang and Xiaodong Cun and Zhongang Qi and Junfu Pu and Huanzhang Dou and Guangcong Zheng and Ying Shan and Xi Li", "abstract": "  Zero-shot customized video generation has gained significant attention due to\nits substantial application potential. Existing methods rely on additional\nmodels to extract and inject reference subject features, assuming that the\nVideo Diffusion Model (VDM) alone is insufficient for zero-shot customized\nvideo generation. However, these methods often struggle to maintain consistent\nsubject appearance due to suboptimal feature extraction and injection\ntechniques. In this paper, we reveal that VDM inherently possesses the force to\nextract and inject subject features. Departing from previous heuristic\napproaches, we introduce a novel framework that leverages VDM's inherent force\nto enable high-quality zero-shot customized video generation. Specifically, for\nfeature extraction, we directly input reference images into VDM and use its\nintrinsic feature extraction process, which not only provides fine-grained\nfeatures but also significantly aligns with VDM's pre-trained knowledge. For\nfeature injection, we devise an innovative bidirectional interaction between\nsubject features and generated content through spatial self-attention within\nVDM, ensuring that VDM has better subject fidelity while maintaining the\ndiversity of the generated video.Experiments on both customized human and\nobject video generation validate the effectiveness of our framework.\n", "link": "http://arxiv.org/abs/2412.19645v1", "date": "2024-12-27", "relevancy": 2.7309, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.748}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7155}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoMaker%3A%20Zero-shot%20Customized%20Video%20Generation%20with%20the%20Inherent%0A%20%20Force%20of%20Video%20Diffusion%20Models&body=Title%3A%20VideoMaker%3A%20Zero-shot%20Customized%20Video%20Generation%20with%20the%20Inherent%0A%20%20Force%20of%20Video%20Diffusion%20Models%0AAuthor%3A%20Tao%20Wu%20and%20Yong%20Zhang%20and%20Xiaodong%20Cun%20and%20Zhongang%20Qi%20and%20Junfu%20Pu%20and%20Huanzhang%20Dou%20and%20Guangcong%20Zheng%20and%20Ying%20Shan%20and%20Xi%20Li%0AAbstract%3A%20%20%20Zero-shot%20customized%20video%20generation%20has%20gained%20significant%20attention%20due%20to%0Aits%20substantial%20application%20potential.%20Existing%20methods%20rely%20on%20additional%0Amodels%20to%20extract%20and%20inject%20reference%20subject%20features%2C%20assuming%20that%20the%0AVideo%20Diffusion%20Model%20%28VDM%29%20alone%20is%20insufficient%20for%20zero-shot%20customized%0Avideo%20generation.%20However%2C%20these%20methods%20often%20struggle%20to%20maintain%20consistent%0Asubject%20appearance%20due%20to%20suboptimal%20feature%20extraction%20and%20injection%0Atechniques.%20In%20this%20paper%2C%20we%20reveal%20that%20VDM%20inherently%20possesses%20the%20force%20to%0Aextract%20and%20inject%20subject%20features.%20Departing%20from%20previous%20heuristic%0Aapproaches%2C%20we%20introduce%20a%20novel%20framework%20that%20leverages%20VDM%27s%20inherent%20force%0Ato%20enable%20high-quality%20zero-shot%20customized%20video%20generation.%20Specifically%2C%20for%0Afeature%20extraction%2C%20we%20directly%20input%20reference%20images%20into%20VDM%20and%20use%20its%0Aintrinsic%20feature%20extraction%20process%2C%20which%20not%20only%20provides%20fine-grained%0Afeatures%20but%20also%20significantly%20aligns%20with%20VDM%27s%20pre-trained%20knowledge.%20For%0Afeature%20injection%2C%20we%20devise%20an%20innovative%20bidirectional%20interaction%20between%0Asubject%20features%20and%20generated%20content%20through%20spatial%20self-attention%20within%0AVDM%2C%20ensuring%20that%20VDM%20has%20better%20subject%20fidelity%20while%20maintaining%20the%0Adiversity%20of%20the%20generated%20video.Experiments%20on%20both%20customized%20human%20and%0Aobject%20video%20generation%20validate%20the%20effectiveness%20of%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoMaker%253A%2520Zero-shot%2520Customized%2520Video%2520Generation%2520with%2520the%2520Inherent%250A%2520%2520Force%2520of%2520Video%2520Diffusion%2520Models%26entry.906535625%3DTao%2520Wu%2520and%2520Yong%2520Zhang%2520and%2520Xiaodong%2520Cun%2520and%2520Zhongang%2520Qi%2520and%2520Junfu%2520Pu%2520and%2520Huanzhang%2520Dou%2520and%2520Guangcong%2520Zheng%2520and%2520Ying%2520Shan%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520Zero-shot%2520customized%2520video%2520generation%2520has%2520gained%2520significant%2520attention%2520due%2520to%250Aits%2520substantial%2520application%2520potential.%2520Existing%2520methods%2520rely%2520on%2520additional%250Amodels%2520to%2520extract%2520and%2520inject%2520reference%2520subject%2520features%252C%2520assuming%2520that%2520the%250AVideo%2520Diffusion%2520Model%2520%2528VDM%2529%2520alone%2520is%2520insufficient%2520for%2520zero-shot%2520customized%250Avideo%2520generation.%2520However%252C%2520these%2520methods%2520often%2520struggle%2520to%2520maintain%2520consistent%250Asubject%2520appearance%2520due%2520to%2520suboptimal%2520feature%2520extraction%2520and%2520injection%250Atechniques.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520that%2520VDM%2520inherently%2520possesses%2520the%2520force%2520to%250Aextract%2520and%2520inject%2520subject%2520features.%2520Departing%2520from%2520previous%2520heuristic%250Aapproaches%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%2520leverages%2520VDM%2527s%2520inherent%2520force%250Ato%2520enable%2520high-quality%2520zero-shot%2520customized%2520video%2520generation.%2520Specifically%252C%2520for%250Afeature%2520extraction%252C%2520we%2520directly%2520input%2520reference%2520images%2520into%2520VDM%2520and%2520use%2520its%250Aintrinsic%2520feature%2520extraction%2520process%252C%2520which%2520not%2520only%2520provides%2520fine-grained%250Afeatures%2520but%2520also%2520significantly%2520aligns%2520with%2520VDM%2527s%2520pre-trained%2520knowledge.%2520For%250Afeature%2520injection%252C%2520we%2520devise%2520an%2520innovative%2520bidirectional%2520interaction%2520between%250Asubject%2520features%2520and%2520generated%2520content%2520through%2520spatial%2520self-attention%2520within%250AVDM%252C%2520ensuring%2520that%2520VDM%2520has%2520better%2520subject%2520fidelity%2520while%2520maintaining%2520the%250Adiversity%2520of%2520the%2520generated%2520video.Experiments%2520on%2520both%2520customized%2520human%2520and%250Aobject%2520video%2520generation%2520validate%2520the%2520effectiveness%2520of%2520our%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoMaker%3A%20Zero-shot%20Customized%20Video%20Generation%20with%20the%20Inherent%0A%20%20Force%20of%20Video%20Diffusion%20Models&entry.906535625=Tao%20Wu%20and%20Yong%20Zhang%20and%20Xiaodong%20Cun%20and%20Zhongang%20Qi%20and%20Junfu%20Pu%20and%20Huanzhang%20Dou%20and%20Guangcong%20Zheng%20and%20Ying%20Shan%20and%20Xi%20Li&entry.1292438233=%20%20Zero-shot%20customized%20video%20generation%20has%20gained%20significant%20attention%20due%20to%0Aits%20substantial%20application%20potential.%20Existing%20methods%20rely%20on%20additional%0Amodels%20to%20extract%20and%20inject%20reference%20subject%20features%2C%20assuming%20that%20the%0AVideo%20Diffusion%20Model%20%28VDM%29%20alone%20is%20insufficient%20for%20zero-shot%20customized%0Avideo%20generation.%20However%2C%20these%20methods%20often%20struggle%20to%20maintain%20consistent%0Asubject%20appearance%20due%20to%20suboptimal%20feature%20extraction%20and%20injection%0Atechniques.%20In%20this%20paper%2C%20we%20reveal%20that%20VDM%20inherently%20possesses%20the%20force%20to%0Aextract%20and%20inject%20subject%20features.%20Departing%20from%20previous%20heuristic%0Aapproaches%2C%20we%20introduce%20a%20novel%20framework%20that%20leverages%20VDM%27s%20inherent%20force%0Ato%20enable%20high-quality%20zero-shot%20customized%20video%20generation.%20Specifically%2C%20for%0Afeature%20extraction%2C%20we%20directly%20input%20reference%20images%20into%20VDM%20and%20use%20its%0Aintrinsic%20feature%20extraction%20process%2C%20which%20not%20only%20provides%20fine-grained%0Afeatures%20but%20also%20significantly%20aligns%20with%20VDM%27s%20pre-trained%20knowledge.%20For%0Afeature%20injection%2C%20we%20devise%20an%20innovative%20bidirectional%20interaction%20between%0Asubject%20features%20and%20generated%20content%20through%20spatial%20self-attention%20within%0AVDM%2C%20ensuring%20that%20VDM%20has%20better%20subject%20fidelity%20while%20maintaining%20the%0Adiversity%20of%20the%20generated%20video.Experiments%20on%20both%20customized%20human%20and%0Aobject%20video%20generation%20validate%20the%20effectiveness%20of%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19645v1&entry.124074799=Read"},
{"title": "CustomCrafter: Customized Video Generation with Preserving Motion and\n  Concept Composition Abilities", "author": "Tao Wu and Yong Zhang and Xintao Wang and Xianpan Zhou and Guangcong Zheng and Zhongang Qi and Ying Shan and Xi Li", "abstract": "  Customized video generation aims to generate high-quality videos guided by\ntext prompts and subject's reference images. However, since it is only trained\non static images, the fine-tuning process of subject learning disrupts\nabilities of video diffusion models (VDMs) to combine concepts and generate\nmotions. To restore these abilities, some methods use additional video similar\nto the prompt to fine-tune or guide the model. This requires frequent changes\nof guiding videos and even re-tuning of the model when generating different\nmotions, which is very inconvenient for users. In this paper, we propose\nCustomCrafter, a novel framework that preserves the model's motion generation\nand conceptual combination abilities without additional video and fine-tuning\nto recovery. For preserving conceptual combination ability, we design a\nplug-and-play module to update few parameters in VDMs, enhancing the model's\nability to capture the appearance details and the ability of concept\ncombinations for new subjects. For motion generation, we observed that VDMs\ntend to restore the motion of video in the early stage of denoising, while\nfocusing on the recovery of subject details in the later stage. Therefore, we\npropose Dynamic Weighted Video Sampling Strategy. Using the pluggability of our\nsubject learning modules, we reduce the impact of this module on motion\ngeneration in the early stage of denoising, preserving the ability to generate\nmotion of VDMs. In the later stage of denoising, we restore this module to\nrepair the appearance details of the specified subject, thereby ensuring the\nfidelity of the subject's appearance. Experimental results show that our method\nhas a significant improvement compared to previous methods. Code is available\nat https://github.com/WuTao-CS/CustomCrafter\n", "link": "http://arxiv.org/abs/2408.13239v2", "date": "2024-12-27", "relevancy": 2.7219, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7392}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7182}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CustomCrafter%3A%20Customized%20Video%20Generation%20with%20Preserving%20Motion%20and%0A%20%20Concept%20Composition%20Abilities&body=Title%3A%20CustomCrafter%3A%20Customized%20Video%20Generation%20with%20Preserving%20Motion%20and%0A%20%20Concept%20Composition%20Abilities%0AAuthor%3A%20Tao%20Wu%20and%20Yong%20Zhang%20and%20Xintao%20Wang%20and%20Xianpan%20Zhou%20and%20Guangcong%20Zheng%20and%20Zhongang%20Qi%20and%20Ying%20Shan%20and%20Xi%20Li%0AAbstract%3A%20%20%20Customized%20video%20generation%20aims%20to%20generate%20high-quality%20videos%20guided%20by%0Atext%20prompts%20and%20subject%27s%20reference%20images.%20However%2C%20since%20it%20is%20only%20trained%0Aon%20static%20images%2C%20the%20fine-tuning%20process%20of%20subject%20learning%20disrupts%0Aabilities%20of%20video%20diffusion%20models%20%28VDMs%29%20to%20combine%20concepts%20and%20generate%0Amotions.%20To%20restore%20these%20abilities%2C%20some%20methods%20use%20additional%20video%20similar%0Ato%20the%20prompt%20to%20fine-tune%20or%20guide%20the%20model.%20This%20requires%20frequent%20changes%0Aof%20guiding%20videos%20and%20even%20re-tuning%20of%20the%20model%20when%20generating%20different%0Amotions%2C%20which%20is%20very%20inconvenient%20for%20users.%20In%20this%20paper%2C%20we%20propose%0ACustomCrafter%2C%20a%20novel%20framework%20that%20preserves%20the%20model%27s%20motion%20generation%0Aand%20conceptual%20combination%20abilities%20without%20additional%20video%20and%20fine-tuning%0Ato%20recovery.%20For%20preserving%20conceptual%20combination%20ability%2C%20we%20design%20a%0Aplug-and-play%20module%20to%20update%20few%20parameters%20in%20VDMs%2C%20enhancing%20the%20model%27s%0Aability%20to%20capture%20the%20appearance%20details%20and%20the%20ability%20of%20concept%0Acombinations%20for%20new%20subjects.%20For%20motion%20generation%2C%20we%20observed%20that%20VDMs%0Atend%20to%20restore%20the%20motion%20of%20video%20in%20the%20early%20stage%20of%20denoising%2C%20while%0Afocusing%20on%20the%20recovery%20of%20subject%20details%20in%20the%20later%20stage.%20Therefore%2C%20we%0Apropose%20Dynamic%20Weighted%20Video%20Sampling%20Strategy.%20Using%20the%20pluggability%20of%20our%0Asubject%20learning%20modules%2C%20we%20reduce%20the%20impact%20of%20this%20module%20on%20motion%0Ageneration%20in%20the%20early%20stage%20of%20denoising%2C%20preserving%20the%20ability%20to%20generate%0Amotion%20of%20VDMs.%20In%20the%20later%20stage%20of%20denoising%2C%20we%20restore%20this%20module%20to%0Arepair%20the%20appearance%20details%20of%20the%20specified%20subject%2C%20thereby%20ensuring%20the%0Afidelity%20of%20the%20subject%27s%20appearance.%20Experimental%20results%20show%20that%20our%20method%0Ahas%20a%20significant%20improvement%20compared%20to%20previous%20methods.%20Code%20is%20available%0Aat%20https%3A//github.com/WuTao-CS/CustomCrafter%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomCrafter%253A%2520Customized%2520Video%2520Generation%2520with%2520Preserving%2520Motion%2520and%250A%2520%2520Concept%2520Composition%2520Abilities%26entry.906535625%3DTao%2520Wu%2520and%2520Yong%2520Zhang%2520and%2520Xintao%2520Wang%2520and%2520Xianpan%2520Zhou%2520and%2520Guangcong%2520Zheng%2520and%2520Zhongang%2520Qi%2520and%2520Ying%2520Shan%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520Customized%2520video%2520generation%2520aims%2520to%2520generate%2520high-quality%2520videos%2520guided%2520by%250Atext%2520prompts%2520and%2520subject%2527s%2520reference%2520images.%2520However%252C%2520since%2520it%2520is%2520only%2520trained%250Aon%2520static%2520images%252C%2520the%2520fine-tuning%2520process%2520of%2520subject%2520learning%2520disrupts%250Aabilities%2520of%2520video%2520diffusion%2520models%2520%2528VDMs%2529%2520to%2520combine%2520concepts%2520and%2520generate%250Amotions.%2520To%2520restore%2520these%2520abilities%252C%2520some%2520methods%2520use%2520additional%2520video%2520similar%250Ato%2520the%2520prompt%2520to%2520fine-tune%2520or%2520guide%2520the%2520model.%2520This%2520requires%2520frequent%2520changes%250Aof%2520guiding%2520videos%2520and%2520even%2520re-tuning%2520of%2520the%2520model%2520when%2520generating%2520different%250Amotions%252C%2520which%2520is%2520very%2520inconvenient%2520for%2520users.%2520In%2520this%2520paper%252C%2520we%2520propose%250ACustomCrafter%252C%2520a%2520novel%2520framework%2520that%2520preserves%2520the%2520model%2527s%2520motion%2520generation%250Aand%2520conceptual%2520combination%2520abilities%2520without%2520additional%2520video%2520and%2520fine-tuning%250Ato%2520recovery.%2520For%2520preserving%2520conceptual%2520combination%2520ability%252C%2520we%2520design%2520a%250Aplug-and-play%2520module%2520to%2520update%2520few%2520parameters%2520in%2520VDMs%252C%2520enhancing%2520the%2520model%2527s%250Aability%2520to%2520capture%2520the%2520appearance%2520details%2520and%2520the%2520ability%2520of%2520concept%250Acombinations%2520for%2520new%2520subjects.%2520For%2520motion%2520generation%252C%2520we%2520observed%2520that%2520VDMs%250Atend%2520to%2520restore%2520the%2520motion%2520of%2520video%2520in%2520the%2520early%2520stage%2520of%2520denoising%252C%2520while%250Afocusing%2520on%2520the%2520recovery%2520of%2520subject%2520details%2520in%2520the%2520later%2520stage.%2520Therefore%252C%2520we%250Apropose%2520Dynamic%2520Weighted%2520Video%2520Sampling%2520Strategy.%2520Using%2520the%2520pluggability%2520of%2520our%250Asubject%2520learning%2520modules%252C%2520we%2520reduce%2520the%2520impact%2520of%2520this%2520module%2520on%2520motion%250Ageneration%2520in%2520the%2520early%2520stage%2520of%2520denoising%252C%2520preserving%2520the%2520ability%2520to%2520generate%250Amotion%2520of%2520VDMs.%2520In%2520the%2520later%2520stage%2520of%2520denoising%252C%2520we%2520restore%2520this%2520module%2520to%250Arepair%2520the%2520appearance%2520details%2520of%2520the%2520specified%2520subject%252C%2520thereby%2520ensuring%2520the%250Afidelity%2520of%2520the%2520subject%2527s%2520appearance.%2520Experimental%2520results%2520show%2520that%2520our%2520method%250Ahas%2520a%2520significant%2520improvement%2520compared%2520to%2520previous%2520methods.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/WuTao-CS/CustomCrafter%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CustomCrafter%3A%20Customized%20Video%20Generation%20with%20Preserving%20Motion%20and%0A%20%20Concept%20Composition%20Abilities&entry.906535625=Tao%20Wu%20and%20Yong%20Zhang%20and%20Xintao%20Wang%20and%20Xianpan%20Zhou%20and%20Guangcong%20Zheng%20and%20Zhongang%20Qi%20and%20Ying%20Shan%20and%20Xi%20Li&entry.1292438233=%20%20Customized%20video%20generation%20aims%20to%20generate%20high-quality%20videos%20guided%20by%0Atext%20prompts%20and%20subject%27s%20reference%20images.%20However%2C%20since%20it%20is%20only%20trained%0Aon%20static%20images%2C%20the%20fine-tuning%20process%20of%20subject%20learning%20disrupts%0Aabilities%20of%20video%20diffusion%20models%20%28VDMs%29%20to%20combine%20concepts%20and%20generate%0Amotions.%20To%20restore%20these%20abilities%2C%20some%20methods%20use%20additional%20video%20similar%0Ato%20the%20prompt%20to%20fine-tune%20or%20guide%20the%20model.%20This%20requires%20frequent%20changes%0Aof%20guiding%20videos%20and%20even%20re-tuning%20of%20the%20model%20when%20generating%20different%0Amotions%2C%20which%20is%20very%20inconvenient%20for%20users.%20In%20this%20paper%2C%20we%20propose%0ACustomCrafter%2C%20a%20novel%20framework%20that%20preserves%20the%20model%27s%20motion%20generation%0Aand%20conceptual%20combination%20abilities%20without%20additional%20video%20and%20fine-tuning%0Ato%20recovery.%20For%20preserving%20conceptual%20combination%20ability%2C%20we%20design%20a%0Aplug-and-play%20module%20to%20update%20few%20parameters%20in%20VDMs%2C%20enhancing%20the%20model%27s%0Aability%20to%20capture%20the%20appearance%20details%20and%20the%20ability%20of%20concept%0Acombinations%20for%20new%20subjects.%20For%20motion%20generation%2C%20we%20observed%20that%20VDMs%0Atend%20to%20restore%20the%20motion%20of%20video%20in%20the%20early%20stage%20of%20denoising%2C%20while%0Afocusing%20on%20the%20recovery%20of%20subject%20details%20in%20the%20later%20stage.%20Therefore%2C%20we%0Apropose%20Dynamic%20Weighted%20Video%20Sampling%20Strategy.%20Using%20the%20pluggability%20of%20our%0Asubject%20learning%20modules%2C%20we%20reduce%20the%20impact%20of%20this%20module%20on%20motion%0Ageneration%20in%20the%20early%20stage%20of%20denoising%2C%20preserving%20the%20ability%20to%20generate%0Amotion%20of%20VDMs.%20In%20the%20later%20stage%20of%20denoising%2C%20we%20restore%20this%20module%20to%0Arepair%20the%20appearance%20details%20of%20the%20specified%20subject%2C%20thereby%20ensuring%20the%0Afidelity%20of%20the%20subject%27s%20appearance.%20Experimental%20results%20show%20that%20our%20method%0Ahas%20a%20significant%20improvement%20compared%20to%20previous%20methods.%20Code%20is%20available%0Aat%20https%3A//github.com/WuTao-CS/CustomCrafter%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13239v2&entry.124074799=Read"},
{"title": "Baichuan-Omni Technical Report", "author": "Yadong Li and Haoze Sun and Mingan Lin and Tianpeng Li and Guosheng Dong and Tao Zhang and Bowen Ding and Wei Song and Zhenglin Cheng and Yuqi Huo and Song Chen and Xu Li and Da Pan and Shusen Zhang and Xin Wu and Zheng Liang and Jun Liu and Tao Zhang and Keer Lu and Yaqi Zhao and Yanjun Shen and Fan Yang and Kaicheng Yu and Tao Lin and Jianhua Xu and Zenan Zhou and Weipeng Chen", "abstract": "  The salient multimodal capabilities and interactive experience of GPT-4o\nhighlight its critical role in practical applications, yet it lacks a\nhigh-performing open-source counterpart. In this paper, we introduce\nBaichuan-omni, the first open-source 7B Multimodal Large Language Model (MLLM)\nadept at concurrently processing and analyzing modalities of image, video,\naudio, and text, while delivering an advanced multimodal interactive experience\nand strong performance. We propose an effective multimodal training schema\nstarting with 7B model and proceeding through two stages of multimodal\nalignment and multitask fine-tuning across audio, image, video, and text modal.\nThis approach equips the language model with the ability to handle visual and\naudio data effectively. Demonstrating strong performance across various\nomni-modal and multimodal benchmarks, we aim for this contribution to serve as\na competitive baseline for the open-source community in advancing multimodal\nunderstanding and real-time interaction.\n", "link": "http://arxiv.org/abs/2410.08565v4", "date": "2024-12-27", "relevancy": 2.7054, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Baichuan-Omni%20Technical%20Report&body=Title%3A%20Baichuan-Omni%20Technical%20Report%0AAuthor%3A%20Yadong%20Li%20and%20Haoze%20Sun%20and%20Mingan%20Lin%20and%20Tianpeng%20Li%20and%20Guosheng%20Dong%20and%20Tao%20Zhang%20and%20Bowen%20Ding%20and%20Wei%20Song%20and%20Zhenglin%20Cheng%20and%20Yuqi%20Huo%20and%20Song%20Chen%20and%20Xu%20Li%20and%20Da%20Pan%20and%20Shusen%20Zhang%20and%20Xin%20Wu%20and%20Zheng%20Liang%20and%20Jun%20Liu%20and%20Tao%20Zhang%20and%20Keer%20Lu%20and%20Yaqi%20Zhao%20and%20Yanjun%20Shen%20and%20Fan%20Yang%20and%20Kaicheng%20Yu%20and%20Tao%20Lin%20and%20Jianhua%20Xu%20and%20Zenan%20Zhou%20and%20Weipeng%20Chen%0AAbstract%3A%20%20%20The%20salient%20multimodal%20capabilities%20and%20interactive%20experience%20of%20GPT-4o%0Ahighlight%20its%20critical%20role%20in%20practical%20applications%2C%20yet%20it%20lacks%20a%0Ahigh-performing%20open-source%20counterpart.%20In%20this%20paper%2C%20we%20introduce%0ABaichuan-omni%2C%20the%20first%20open-source%207B%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%0Aadept%20at%20concurrently%20processing%20and%20analyzing%20modalities%20of%20image%2C%20video%2C%0Aaudio%2C%20and%20text%2C%20while%20delivering%20an%20advanced%20multimodal%20interactive%20experience%0Aand%20strong%20performance.%20We%20propose%20an%20effective%20multimodal%20training%20schema%0Astarting%20with%207B%20model%20and%20proceeding%20through%20two%20stages%20of%20multimodal%0Aalignment%20and%20multitask%20fine-tuning%20across%20audio%2C%20image%2C%20video%2C%20and%20text%20modal.%0AThis%20approach%20equips%20the%20language%20model%20with%20the%20ability%20to%20handle%20visual%20and%0Aaudio%20data%20effectively.%20Demonstrating%20strong%20performance%20across%20various%0Aomni-modal%20and%20multimodal%20benchmarks%2C%20we%20aim%20for%20this%20contribution%20to%20serve%20as%0Aa%20competitive%20baseline%20for%20the%20open-source%20community%20in%20advancing%20multimodal%0Aunderstanding%20and%20real-time%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08565v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBaichuan-Omni%2520Technical%2520Report%26entry.906535625%3DYadong%2520Li%2520and%2520Haoze%2520Sun%2520and%2520Mingan%2520Lin%2520and%2520Tianpeng%2520Li%2520and%2520Guosheng%2520Dong%2520and%2520Tao%2520Zhang%2520and%2520Bowen%2520Ding%2520and%2520Wei%2520Song%2520and%2520Zhenglin%2520Cheng%2520and%2520Yuqi%2520Huo%2520and%2520Song%2520Chen%2520and%2520Xu%2520Li%2520and%2520Da%2520Pan%2520and%2520Shusen%2520Zhang%2520and%2520Xin%2520Wu%2520and%2520Zheng%2520Liang%2520and%2520Jun%2520Liu%2520and%2520Tao%2520Zhang%2520and%2520Keer%2520Lu%2520and%2520Yaqi%2520Zhao%2520and%2520Yanjun%2520Shen%2520and%2520Fan%2520Yang%2520and%2520Kaicheng%2520Yu%2520and%2520Tao%2520Lin%2520and%2520Jianhua%2520Xu%2520and%2520Zenan%2520Zhou%2520and%2520Weipeng%2520Chen%26entry.1292438233%3D%2520%2520The%2520salient%2520multimodal%2520capabilities%2520and%2520interactive%2520experience%2520of%2520GPT-4o%250Ahighlight%2520its%2520critical%2520role%2520in%2520practical%2520applications%252C%2520yet%2520it%2520lacks%2520a%250Ahigh-performing%2520open-source%2520counterpart.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ABaichuan-omni%252C%2520the%2520first%2520open-source%25207B%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%250Aadept%2520at%2520concurrently%2520processing%2520and%2520analyzing%2520modalities%2520of%2520image%252C%2520video%252C%250Aaudio%252C%2520and%2520text%252C%2520while%2520delivering%2520an%2520advanced%2520multimodal%2520interactive%2520experience%250Aand%2520strong%2520performance.%2520We%2520propose%2520an%2520effective%2520multimodal%2520training%2520schema%250Astarting%2520with%25207B%2520model%2520and%2520proceeding%2520through%2520two%2520stages%2520of%2520multimodal%250Aalignment%2520and%2520multitask%2520fine-tuning%2520across%2520audio%252C%2520image%252C%2520video%252C%2520and%2520text%2520modal.%250AThis%2520approach%2520equips%2520the%2520language%2520model%2520with%2520the%2520ability%2520to%2520handle%2520visual%2520and%250Aaudio%2520data%2520effectively.%2520Demonstrating%2520strong%2520performance%2520across%2520various%250Aomni-modal%2520and%2520multimodal%2520benchmarks%252C%2520we%2520aim%2520for%2520this%2520contribution%2520to%2520serve%2520as%250Aa%2520competitive%2520baseline%2520for%2520the%2520open-source%2520community%2520in%2520advancing%2520multimodal%250Aunderstanding%2520and%2520real-time%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08565v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Baichuan-Omni%20Technical%20Report&entry.906535625=Yadong%20Li%20and%20Haoze%20Sun%20and%20Mingan%20Lin%20and%20Tianpeng%20Li%20and%20Guosheng%20Dong%20and%20Tao%20Zhang%20and%20Bowen%20Ding%20and%20Wei%20Song%20and%20Zhenglin%20Cheng%20and%20Yuqi%20Huo%20and%20Song%20Chen%20and%20Xu%20Li%20and%20Da%20Pan%20and%20Shusen%20Zhang%20and%20Xin%20Wu%20and%20Zheng%20Liang%20and%20Jun%20Liu%20and%20Tao%20Zhang%20and%20Keer%20Lu%20and%20Yaqi%20Zhao%20and%20Yanjun%20Shen%20and%20Fan%20Yang%20and%20Kaicheng%20Yu%20and%20Tao%20Lin%20and%20Jianhua%20Xu%20and%20Zenan%20Zhou%20and%20Weipeng%20Chen&entry.1292438233=%20%20The%20salient%20multimodal%20capabilities%20and%20interactive%20experience%20of%20GPT-4o%0Ahighlight%20its%20critical%20role%20in%20practical%20applications%2C%20yet%20it%20lacks%20a%0Ahigh-performing%20open-source%20counterpart.%20In%20this%20paper%2C%20we%20introduce%0ABaichuan-omni%2C%20the%20first%20open-source%207B%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%0Aadept%20at%20concurrently%20processing%20and%20analyzing%20modalities%20of%20image%2C%20video%2C%0Aaudio%2C%20and%20text%2C%20while%20delivering%20an%20advanced%20multimodal%20interactive%20experience%0Aand%20strong%20performance.%20We%20propose%20an%20effective%20multimodal%20training%20schema%0Astarting%20with%207B%20model%20and%20proceeding%20through%20two%20stages%20of%20multimodal%0Aalignment%20and%20multitask%20fine-tuning%20across%20audio%2C%20image%2C%20video%2C%20and%20text%20modal.%0AThis%20approach%20equips%20the%20language%20model%20with%20the%20ability%20to%20handle%20visual%20and%0Aaudio%20data%20effectively.%20Demonstrating%20strong%20performance%20across%20various%0Aomni-modal%20and%20multimodal%20benchmarks%2C%20we%20aim%20for%20this%20contribution%20to%20serve%20as%0Aa%20competitive%20baseline%20for%20the%20open-source%20community%20in%20advancing%20multimodal%0Aunderstanding%20and%20real-time%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08565v4&entry.124074799=Read"},
{"title": "Enhancing Fine-grained Image Classification through Attentive Batch\n  Training", "author": "Duy M. Le and Bao Q. Bui and Anh Tran and Cong Tran and Cuong Pham", "abstract": "  Fine-grained image classification, which is a challenging task in computer\nvision, requires precise differentiation among visually similar object\ncategories. In this paper, we propose 1) a novel module called Residual\nRelationship Attention (RRA) that leverages the relationships between images\nwithin each training batch to effectively integrate visual feature vectors of\nbatch images and 2) a novel technique called Relationship Position Encoding\n(RPE), which encodes the positions of relationships between original images in\na batch and effectively preserves the relationship information between images\nwithin the batch. Additionally, we design a novel framework, namely\nRelationship Batch Integration (RBI), which utilizes RRA in conjunction with\nRPE, allowing the discernment of vital visual features that may remain elusive\nwhen examining a singular image representative of a particular class. Through\nextensive experiments, our proposed method demonstrates significant\nimprovements in the accuracy of different fine-grained classifiers, with an\naverage increase of $(+2.78\\%)$ and $(+3.83\\%)$ on the CUB200-2011 and Stanford\nDog datasets, respectively, while achieving a state-of-the-art results\n$(95.79\\%)$ on the Stanford Dog dataset. Despite not achieving the same level\nof improvement as in fine-grained image classification, our method still\ndemonstrates its prowess in leveraging general image classification by\nattaining a state-of-the-art result of $(93.71\\%)$ on the Tiny-Imagenet\ndataset. Furthermore, our method serves as a plug-in refinement module and can\nbe easily integrated into different networks.\n", "link": "http://arxiv.org/abs/2412.19606v1", "date": "2024-12-27", "relevancy": 2.6979, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5532}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5487}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Fine-grained%20Image%20Classification%20through%20Attentive%20Batch%0A%20%20Training&body=Title%3A%20Enhancing%20Fine-grained%20Image%20Classification%20through%20Attentive%20Batch%0A%20%20Training%0AAuthor%3A%20Duy%20M.%20Le%20and%20Bao%20Q.%20Bui%20and%20Anh%20Tran%20and%20Cong%20Tran%20and%20Cuong%20Pham%0AAbstract%3A%20%20%20Fine-grained%20image%20classification%2C%20which%20is%20a%20challenging%20task%20in%20computer%0Avision%2C%20requires%20precise%20differentiation%20among%20visually%20similar%20object%0Acategories.%20In%20this%20paper%2C%20we%20propose%201%29%20a%20novel%20module%20called%20Residual%0ARelationship%20Attention%20%28RRA%29%20that%20leverages%20the%20relationships%20between%20images%0Awithin%20each%20training%20batch%20to%20effectively%20integrate%20visual%20feature%20vectors%20of%0Abatch%20images%20and%202%29%20a%20novel%20technique%20called%20Relationship%20Position%20Encoding%0A%28RPE%29%2C%20which%20encodes%20the%20positions%20of%20relationships%20between%20original%20images%20in%0Aa%20batch%20and%20effectively%20preserves%20the%20relationship%20information%20between%20images%0Awithin%20the%20batch.%20Additionally%2C%20we%20design%20a%20novel%20framework%2C%20namely%0ARelationship%20Batch%20Integration%20%28RBI%29%2C%20which%20utilizes%20RRA%20in%20conjunction%20with%0ARPE%2C%20allowing%20the%20discernment%20of%20vital%20visual%20features%20that%20may%20remain%20elusive%0Awhen%20examining%20a%20singular%20image%20representative%20of%20a%20particular%20class.%20Through%0Aextensive%20experiments%2C%20our%20proposed%20method%20demonstrates%20significant%0Aimprovements%20in%20the%20accuracy%20of%20different%20fine-grained%20classifiers%2C%20with%20an%0Aaverage%20increase%20of%20%24%28%2B2.78%5C%25%29%24%20and%20%24%28%2B3.83%5C%25%29%24%20on%20the%20CUB200-2011%20and%20Stanford%0ADog%20datasets%2C%20respectively%2C%20while%20achieving%20a%20state-of-the-art%20results%0A%24%2895.79%5C%25%29%24%20on%20the%20Stanford%20Dog%20dataset.%20Despite%20not%20achieving%20the%20same%20level%0Aof%20improvement%20as%20in%20fine-grained%20image%20classification%2C%20our%20method%20still%0Ademonstrates%20its%20prowess%20in%20leveraging%20general%20image%20classification%20by%0Aattaining%20a%20state-of-the-art%20result%20of%20%24%2893.71%5C%25%29%24%20on%20the%20Tiny-Imagenet%0Adataset.%20Furthermore%2C%20our%20method%20serves%20as%20a%20plug-in%20refinement%20module%20and%20can%0Abe%20easily%20integrated%20into%20different%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Fine-grained%2520Image%2520Classification%2520through%2520Attentive%2520Batch%250A%2520%2520Training%26entry.906535625%3DDuy%2520M.%2520Le%2520and%2520Bao%2520Q.%2520Bui%2520and%2520Anh%2520Tran%2520and%2520Cong%2520Tran%2520and%2520Cuong%2520Pham%26entry.1292438233%3D%2520%2520Fine-grained%2520image%2520classification%252C%2520which%2520is%2520a%2520challenging%2520task%2520in%2520computer%250Avision%252C%2520requires%2520precise%2520differentiation%2520among%2520visually%2520similar%2520object%250Acategories.%2520In%2520this%2520paper%252C%2520we%2520propose%25201%2529%2520a%2520novel%2520module%2520called%2520Residual%250ARelationship%2520Attention%2520%2528RRA%2529%2520that%2520leverages%2520the%2520relationships%2520between%2520images%250Awithin%2520each%2520training%2520batch%2520to%2520effectively%2520integrate%2520visual%2520feature%2520vectors%2520of%250Abatch%2520images%2520and%25202%2529%2520a%2520novel%2520technique%2520called%2520Relationship%2520Position%2520Encoding%250A%2528RPE%2529%252C%2520which%2520encodes%2520the%2520positions%2520of%2520relationships%2520between%2520original%2520images%2520in%250Aa%2520batch%2520and%2520effectively%2520preserves%2520the%2520relationship%2520information%2520between%2520images%250Awithin%2520the%2520batch.%2520Additionally%252C%2520we%2520design%2520a%2520novel%2520framework%252C%2520namely%250ARelationship%2520Batch%2520Integration%2520%2528RBI%2529%252C%2520which%2520utilizes%2520RRA%2520in%2520conjunction%2520with%250ARPE%252C%2520allowing%2520the%2520discernment%2520of%2520vital%2520visual%2520features%2520that%2520may%2520remain%2520elusive%250Awhen%2520examining%2520a%2520singular%2520image%2520representative%2520of%2520a%2520particular%2520class.%2520Through%250Aextensive%2520experiments%252C%2520our%2520proposed%2520method%2520demonstrates%2520significant%250Aimprovements%2520in%2520the%2520accuracy%2520of%2520different%2520fine-grained%2520classifiers%252C%2520with%2520an%250Aaverage%2520increase%2520of%2520%2524%2528%252B2.78%255C%2525%2529%2524%2520and%2520%2524%2528%252B3.83%255C%2525%2529%2524%2520on%2520the%2520CUB200-2011%2520and%2520Stanford%250ADog%2520datasets%252C%2520respectively%252C%2520while%2520achieving%2520a%2520state-of-the-art%2520results%250A%2524%252895.79%255C%2525%2529%2524%2520on%2520the%2520Stanford%2520Dog%2520dataset.%2520Despite%2520not%2520achieving%2520the%2520same%2520level%250Aof%2520improvement%2520as%2520in%2520fine-grained%2520image%2520classification%252C%2520our%2520method%2520still%250Ademonstrates%2520its%2520prowess%2520in%2520leveraging%2520general%2520image%2520classification%2520by%250Aattaining%2520a%2520state-of-the-art%2520result%2520of%2520%2524%252893.71%255C%2525%2529%2524%2520on%2520the%2520Tiny-Imagenet%250Adataset.%2520Furthermore%252C%2520our%2520method%2520serves%2520as%2520a%2520plug-in%2520refinement%2520module%2520and%2520can%250Abe%2520easily%2520integrated%2520into%2520different%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Fine-grained%20Image%20Classification%20through%20Attentive%20Batch%0A%20%20Training&entry.906535625=Duy%20M.%20Le%20and%20Bao%20Q.%20Bui%20and%20Anh%20Tran%20and%20Cong%20Tran%20and%20Cuong%20Pham&entry.1292438233=%20%20Fine-grained%20image%20classification%2C%20which%20is%20a%20challenging%20task%20in%20computer%0Avision%2C%20requires%20precise%20differentiation%20among%20visually%20similar%20object%0Acategories.%20In%20this%20paper%2C%20we%20propose%201%29%20a%20novel%20module%20called%20Residual%0ARelationship%20Attention%20%28RRA%29%20that%20leverages%20the%20relationships%20between%20images%0Awithin%20each%20training%20batch%20to%20effectively%20integrate%20visual%20feature%20vectors%20of%0Abatch%20images%20and%202%29%20a%20novel%20technique%20called%20Relationship%20Position%20Encoding%0A%28RPE%29%2C%20which%20encodes%20the%20positions%20of%20relationships%20between%20original%20images%20in%0Aa%20batch%20and%20effectively%20preserves%20the%20relationship%20information%20between%20images%0Awithin%20the%20batch.%20Additionally%2C%20we%20design%20a%20novel%20framework%2C%20namely%0ARelationship%20Batch%20Integration%20%28RBI%29%2C%20which%20utilizes%20RRA%20in%20conjunction%20with%0ARPE%2C%20allowing%20the%20discernment%20of%20vital%20visual%20features%20that%20may%20remain%20elusive%0Awhen%20examining%20a%20singular%20image%20representative%20of%20a%20particular%20class.%20Through%0Aextensive%20experiments%2C%20our%20proposed%20method%20demonstrates%20significant%0Aimprovements%20in%20the%20accuracy%20of%20different%20fine-grained%20classifiers%2C%20with%20an%0Aaverage%20increase%20of%20%24%28%2B2.78%5C%25%29%24%20and%20%24%28%2B3.83%5C%25%29%24%20on%20the%20CUB200-2011%20and%20Stanford%0ADog%20datasets%2C%20respectively%2C%20while%20achieving%20a%20state-of-the-art%20results%0A%24%2895.79%5C%25%29%24%20on%20the%20Stanford%20Dog%20dataset.%20Despite%20not%20achieving%20the%20same%20level%0Aof%20improvement%20as%20in%20fine-grained%20image%20classification%2C%20our%20method%20still%0Ademonstrates%20its%20prowess%20in%20leveraging%20general%20image%20classification%20by%0Aattaining%20a%20state-of-the-art%20result%20of%20%24%2893.71%5C%25%29%24%20on%20the%20Tiny-Imagenet%0Adataset.%20Furthermore%2C%20our%20method%20serves%20as%20a%20plug-in%20refinement%20module%20and%20can%0Abe%20easily%20integrated%20into%20different%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19606v1&entry.124074799=Read"},
{"title": "Keypoint Aware Masked Image Modelling", "author": "Madhava Krishna and A V Subramanyam", "abstract": "  SimMIM is a widely used method for pretraining vision transformers using\nmasked image modeling. However, despite its success in fine-tuning performance,\nit has been shown to perform sub-optimally when used for linear probing. We\npropose an efficient patch-wise weighting derived from keypoint features which\ncaptures the local information and provides better context during SimMIM's\nreconstruction phase. Our method, KAMIM, improves the top-1 linear probing\naccuracy from 16.12% to 33.97%, and finetuning accuracy from 76.78% to 77.3%\nwhen tested on the ImageNet-1K dataset with a ViT-B when trained for the same\nnumber of epochs. We conduct extensive testing on different datasets, keypoint\nextractors, and model architectures and observe that patch-wise weighting\naugments linear probing performance for larger pretraining datasets. We also\nanalyze the learned representations of a ViT-B trained using KAMIM and observe\nthat they behave similar to contrastive learning with regard to its behavior,\nwith longer attention distances and homogenous self-attention across layers.\nOur code is publicly available at https://github.com/madhava20217/KAMIM.\n", "link": "http://arxiv.org/abs/2407.13873v2", "date": "2024-12-27", "relevancy": 2.6895, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5826}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keypoint%20Aware%20Masked%20Image%20Modelling&body=Title%3A%20Keypoint%20Aware%20Masked%20Image%20Modelling%0AAuthor%3A%20Madhava%20Krishna%20and%20A%20V%20Subramanyam%0AAbstract%3A%20%20%20SimMIM%20is%20a%20widely%20used%20method%20for%20pretraining%20vision%20transformers%20using%0Amasked%20image%20modeling.%20However%2C%20despite%20its%20success%20in%20fine-tuning%20performance%2C%0Ait%20has%20been%20shown%20to%20perform%20sub-optimally%20when%20used%20for%20linear%20probing.%20We%0Apropose%20an%20efficient%20patch-wise%20weighting%20derived%20from%20keypoint%20features%20which%0Acaptures%20the%20local%20information%20and%20provides%20better%20context%20during%20SimMIM%27s%0Areconstruction%20phase.%20Our%20method%2C%20KAMIM%2C%20improves%20the%20top-1%20linear%20probing%0Aaccuracy%20from%2016.12%25%20to%2033.97%25%2C%20and%20finetuning%20accuracy%20from%2076.78%25%20to%2077.3%25%0Awhen%20tested%20on%20the%20ImageNet-1K%20dataset%20with%20a%20ViT-B%20when%20trained%20for%20the%20same%0Anumber%20of%20epochs.%20We%20conduct%20extensive%20testing%20on%20different%20datasets%2C%20keypoint%0Aextractors%2C%20and%20model%20architectures%20and%20observe%20that%20patch-wise%20weighting%0Aaugments%20linear%20probing%20performance%20for%20larger%20pretraining%20datasets.%20We%20also%0Aanalyze%20the%20learned%20representations%20of%20a%20ViT-B%20trained%20using%20KAMIM%20and%20observe%0Athat%20they%20behave%20similar%20to%20contrastive%20learning%20with%20regard%20to%20its%20behavior%2C%0Awith%20longer%20attention%20distances%20and%20homogenous%20self-attention%20across%20layers.%0AOur%20code%20is%20publicly%20available%20at%20https%3A//github.com/madhava20217/KAMIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeypoint%2520Aware%2520Masked%2520Image%2520Modelling%26entry.906535625%3DMadhava%2520Krishna%2520and%2520A%2520V%2520Subramanyam%26entry.1292438233%3D%2520%2520SimMIM%2520is%2520a%2520widely%2520used%2520method%2520for%2520pretraining%2520vision%2520transformers%2520using%250Amasked%2520image%2520modeling.%2520However%252C%2520despite%2520its%2520success%2520in%2520fine-tuning%2520performance%252C%250Ait%2520has%2520been%2520shown%2520to%2520perform%2520sub-optimally%2520when%2520used%2520for%2520linear%2520probing.%2520We%250Apropose%2520an%2520efficient%2520patch-wise%2520weighting%2520derived%2520from%2520keypoint%2520features%2520which%250Acaptures%2520the%2520local%2520information%2520and%2520provides%2520better%2520context%2520during%2520SimMIM%2527s%250Areconstruction%2520phase.%2520Our%2520method%252C%2520KAMIM%252C%2520improves%2520the%2520top-1%2520linear%2520probing%250Aaccuracy%2520from%252016.12%2525%2520to%252033.97%2525%252C%2520and%2520finetuning%2520accuracy%2520from%252076.78%2525%2520to%252077.3%2525%250Awhen%2520tested%2520on%2520the%2520ImageNet-1K%2520dataset%2520with%2520a%2520ViT-B%2520when%2520trained%2520for%2520the%2520same%250Anumber%2520of%2520epochs.%2520We%2520conduct%2520extensive%2520testing%2520on%2520different%2520datasets%252C%2520keypoint%250Aextractors%252C%2520and%2520model%2520architectures%2520and%2520observe%2520that%2520patch-wise%2520weighting%250Aaugments%2520linear%2520probing%2520performance%2520for%2520larger%2520pretraining%2520datasets.%2520We%2520also%250Aanalyze%2520the%2520learned%2520representations%2520of%2520a%2520ViT-B%2520trained%2520using%2520KAMIM%2520and%2520observe%250Athat%2520they%2520behave%2520similar%2520to%2520contrastive%2520learning%2520with%2520regard%2520to%2520its%2520behavior%252C%250Awith%2520longer%2520attention%2520distances%2520and%2520homogenous%2520self-attention%2520across%2520layers.%250AOur%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/madhava20217/KAMIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keypoint%20Aware%20Masked%20Image%20Modelling&entry.906535625=Madhava%20Krishna%20and%20A%20V%20Subramanyam&entry.1292438233=%20%20SimMIM%20is%20a%20widely%20used%20method%20for%20pretraining%20vision%20transformers%20using%0Amasked%20image%20modeling.%20However%2C%20despite%20its%20success%20in%20fine-tuning%20performance%2C%0Ait%20has%20been%20shown%20to%20perform%20sub-optimally%20when%20used%20for%20linear%20probing.%20We%0Apropose%20an%20efficient%20patch-wise%20weighting%20derived%20from%20keypoint%20features%20which%0Acaptures%20the%20local%20information%20and%20provides%20better%20context%20during%20SimMIM%27s%0Areconstruction%20phase.%20Our%20method%2C%20KAMIM%2C%20improves%20the%20top-1%20linear%20probing%0Aaccuracy%20from%2016.12%25%20to%2033.97%25%2C%20and%20finetuning%20accuracy%20from%2076.78%25%20to%2077.3%25%0Awhen%20tested%20on%20the%20ImageNet-1K%20dataset%20with%20a%20ViT-B%20when%20trained%20for%20the%20same%0Anumber%20of%20epochs.%20We%20conduct%20extensive%20testing%20on%20different%20datasets%2C%20keypoint%0Aextractors%2C%20and%20model%20architectures%20and%20observe%20that%20patch-wise%20weighting%0Aaugments%20linear%20probing%20performance%20for%20larger%20pretraining%20datasets.%20We%20also%0Aanalyze%20the%20learned%20representations%20of%20a%20ViT-B%20trained%20using%20KAMIM%20and%20observe%0Athat%20they%20behave%20similar%20to%20contrastive%20learning%20with%20regard%20to%20its%20behavior%2C%0Awith%20longer%20attention%20distances%20and%20homogenous%20self-attention%20across%20layers.%0AOur%20code%20is%20publicly%20available%20at%20https%3A//github.com/madhava20217/KAMIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13873v2&entry.124074799=Read"},
{"title": "Reinforced Label Denoising for Weakly-Supervised Audio-Visual Video\n  Parsing", "author": "Yongbiao Gao and Xiangcheng Sun and Guohua Lv and Deng Yu and Sijiu Niu", "abstract": "  Audio-visual video parsing (AVVP) aims to recognize audio and visual event\nlabels with precise temporal boundaries, which is quite challenging since audio\nor visual modality might include only one event label with only the overall\nvideo labels available. Existing label denoising models often treat the\ndenoising process as a separate preprocessing step, leading to a disconnect\nbetween label denoising and AVVP tasks. To bridge this gap, we present a novel\njoint reinforcement learning-based label denoising approach (RLLD). This\napproach enables simultaneous training of both label denoising and video\nparsing models through a joint optimization strategy. We introduce a novel\nAVVP-validation and soft inter-reward feedback mechanism that directly guides\nthe learning of label denoising policy. Extensive experiments on AVVP tasks\ndemonstrate the superior performance of our proposed method compared to label\ndenoising techniques. Furthermore, by incorporating our label denoising method\ninto other AVVP models, we find that it can further enhance parsing results.\n", "link": "http://arxiv.org/abs/2412.19563v1", "date": "2024-12-27", "relevancy": 2.6792, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5379}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforced%20Label%20Denoising%20for%20Weakly-Supervised%20Audio-Visual%20Video%0A%20%20Parsing&body=Title%3A%20Reinforced%20Label%20Denoising%20for%20Weakly-Supervised%20Audio-Visual%20Video%0A%20%20Parsing%0AAuthor%3A%20Yongbiao%20Gao%20and%20Xiangcheng%20Sun%20and%20Guohua%20Lv%20and%20Deng%20Yu%20and%20Sijiu%20Niu%0AAbstract%3A%20%20%20Audio-visual%20video%20parsing%20%28AVVP%29%20aims%20to%20recognize%20audio%20and%20visual%20event%0Alabels%20with%20precise%20temporal%20boundaries%2C%20which%20is%20quite%20challenging%20since%20audio%0Aor%20visual%20modality%20might%20include%20only%20one%20event%20label%20with%20only%20the%20overall%0Avideo%20labels%20available.%20Existing%20label%20denoising%20models%20often%20treat%20the%0Adenoising%20process%20as%20a%20separate%20preprocessing%20step%2C%20leading%20to%20a%20disconnect%0Abetween%20label%20denoising%20and%20AVVP%20tasks.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20novel%0Ajoint%20reinforcement%20learning-based%20label%20denoising%20approach%20%28RLLD%29.%20This%0Aapproach%20enables%20simultaneous%20training%20of%20both%20label%20denoising%20and%20video%0Aparsing%20models%20through%20a%20joint%20optimization%20strategy.%20We%20introduce%20a%20novel%0AAVVP-validation%20and%20soft%20inter-reward%20feedback%20mechanism%20that%20directly%20guides%0Athe%20learning%20of%20label%20denoising%20policy.%20Extensive%20experiments%20on%20AVVP%20tasks%0Ademonstrate%20the%20superior%20performance%20of%20our%20proposed%20method%20compared%20to%20label%0Adenoising%20techniques.%20Furthermore%2C%20by%20incorporating%20our%20label%20denoising%20method%0Ainto%20other%20AVVP%20models%2C%20we%20find%20that%20it%20can%20further%20enhance%20parsing%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforced%2520Label%2520Denoising%2520for%2520Weakly-Supervised%2520Audio-Visual%2520Video%250A%2520%2520Parsing%26entry.906535625%3DYongbiao%2520Gao%2520and%2520Xiangcheng%2520Sun%2520and%2520Guohua%2520Lv%2520and%2520Deng%2520Yu%2520and%2520Sijiu%2520Niu%26entry.1292438233%3D%2520%2520Audio-visual%2520video%2520parsing%2520%2528AVVP%2529%2520aims%2520to%2520recognize%2520audio%2520and%2520visual%2520event%250Alabels%2520with%2520precise%2520temporal%2520boundaries%252C%2520which%2520is%2520quite%2520challenging%2520since%2520audio%250Aor%2520visual%2520modality%2520might%2520include%2520only%2520one%2520event%2520label%2520with%2520only%2520the%2520overall%250Avideo%2520labels%2520available.%2520Existing%2520label%2520denoising%2520models%2520often%2520treat%2520the%250Adenoising%2520process%2520as%2520a%2520separate%2520preprocessing%2520step%252C%2520leading%2520to%2520a%2520disconnect%250Abetween%2520label%2520denoising%2520and%2520AVVP%2520tasks.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520a%2520novel%250Ajoint%2520reinforcement%2520learning-based%2520label%2520denoising%2520approach%2520%2528RLLD%2529.%2520This%250Aapproach%2520enables%2520simultaneous%2520training%2520of%2520both%2520label%2520denoising%2520and%2520video%250Aparsing%2520models%2520through%2520a%2520joint%2520optimization%2520strategy.%2520We%2520introduce%2520a%2520novel%250AAVVP-validation%2520and%2520soft%2520inter-reward%2520feedback%2520mechanism%2520that%2520directly%2520guides%250Athe%2520learning%2520of%2520label%2520denoising%2520policy.%2520Extensive%2520experiments%2520on%2520AVVP%2520tasks%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520proposed%2520method%2520compared%2520to%2520label%250Adenoising%2520techniques.%2520Furthermore%252C%2520by%2520incorporating%2520our%2520label%2520denoising%2520method%250Ainto%2520other%2520AVVP%2520models%252C%2520we%2520find%2520that%2520it%2520can%2520further%2520enhance%2520parsing%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforced%20Label%20Denoising%20for%20Weakly-Supervised%20Audio-Visual%20Video%0A%20%20Parsing&entry.906535625=Yongbiao%20Gao%20and%20Xiangcheng%20Sun%20and%20Guohua%20Lv%20and%20Deng%20Yu%20and%20Sijiu%20Niu&entry.1292438233=%20%20Audio-visual%20video%20parsing%20%28AVVP%29%20aims%20to%20recognize%20audio%20and%20visual%20event%0Alabels%20with%20precise%20temporal%20boundaries%2C%20which%20is%20quite%20challenging%20since%20audio%0Aor%20visual%20modality%20might%20include%20only%20one%20event%20label%20with%20only%20the%20overall%0Avideo%20labels%20available.%20Existing%20label%20denoising%20models%20often%20treat%20the%0Adenoising%20process%20as%20a%20separate%20preprocessing%20step%2C%20leading%20to%20a%20disconnect%0Abetween%20label%20denoising%20and%20AVVP%20tasks.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20novel%0Ajoint%20reinforcement%20learning-based%20label%20denoising%20approach%20%28RLLD%29.%20This%0Aapproach%20enables%20simultaneous%20training%20of%20both%20label%20denoising%20and%20video%0Aparsing%20models%20through%20a%20joint%20optimization%20strategy.%20We%20introduce%20a%20novel%0AAVVP-validation%20and%20soft%20inter-reward%20feedback%20mechanism%20that%20directly%20guides%0Athe%20learning%20of%20label%20denoising%20policy.%20Extensive%20experiments%20on%20AVVP%20tasks%0Ademonstrate%20the%20superior%20performance%20of%20our%20proposed%20method%20compared%20to%20label%0Adenoising%20techniques.%20Furthermore%2C%20by%20incorporating%20our%20label%20denoising%20method%0Ainto%20other%20AVVP%20models%2C%20we%20find%20that%20it%20can%20further%20enhance%20parsing%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19563v1&entry.124074799=Read"},
{"title": "Sharpening Neural Implicit Functions with Frequency Consolidation Priors", "author": "Chao Chen and Yu-Shen Liu and Zhizhong Han", "abstract": "  Signed Distance Functions (SDFs) are vital implicit representations to\nrepresent high fidelity 3D surfaces. Current methods mainly leverage a neural\nnetwork to learn an SDF from various supervisions including signed distances,\n3D point clouds, or multi-view images. However, due to various reasons\nincluding the bias of neural network on low frequency content, 3D unaware\nsampling, sparsity in point clouds, or low resolutions of images, neural\nimplicit representations still struggle to represent geometries with high\nfrequency components like sharp structures, especially for the ones learned\nfrom images or point clouds. To overcome this challenge, we introduce a method\nto sharpen a low frequency SDF observation by recovering its high frequency\ncomponents, pursuing a sharper and more complete surface. Our key idea is to\nlearn a mapping from a low frequency observation to a full frequency coverage\nin a data-driven manner, leading to a prior knowledge of shape consolidation in\nthe frequency domain, dubbed frequency consolidation priors. To better\ngeneralize a learned prior to unseen shapes, we introduce to represent\nfrequency components as embeddings and disentangle the embedding of the low\nfrequency component from the embedding of the full frequency component. This\ndisentanglement allows the prior to generalize on an unseen low frequency\nobservation by simply recovering its full frequency embedding through a\ntest-time self-reconstruction. Our evaluations under widely used benchmarks or\nreal scenes show that our method can recover high frequency component and\nproduce more accurate surfaces than the latest methods. The code, data, and\npre-trained models are available at \\url{https://github.com/chenchao15/FCP}.\n", "link": "http://arxiv.org/abs/2412.19720v1", "date": "2024-12-27", "relevancy": 2.6722, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5603}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharpening%20Neural%20Implicit%20Functions%20with%20Frequency%20Consolidation%20Priors&body=Title%3A%20Sharpening%20Neural%20Implicit%20Functions%20with%20Frequency%20Consolidation%20Priors%0AAuthor%3A%20Chao%20Chen%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han%0AAbstract%3A%20%20%20Signed%20Distance%20Functions%20%28SDFs%29%20are%20vital%20implicit%20representations%20to%0Arepresent%20high%20fidelity%203D%20surfaces.%20Current%20methods%20mainly%20leverage%20a%20neural%0Anetwork%20to%20learn%20an%20SDF%20from%20various%20supervisions%20including%20signed%20distances%2C%0A3D%20point%20clouds%2C%20or%20multi-view%20images.%20However%2C%20due%20to%20various%20reasons%0Aincluding%20the%20bias%20of%20neural%20network%20on%20low%20frequency%20content%2C%203D%20unaware%0Asampling%2C%20sparsity%20in%20point%20clouds%2C%20or%20low%20resolutions%20of%20images%2C%20neural%0Aimplicit%20representations%20still%20struggle%20to%20represent%20geometries%20with%20high%0Afrequency%20components%20like%20sharp%20structures%2C%20especially%20for%20the%20ones%20learned%0Afrom%20images%20or%20point%20clouds.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20a%20method%0Ato%20sharpen%20a%20low%20frequency%20SDF%20observation%20by%20recovering%20its%20high%20frequency%0Acomponents%2C%20pursuing%20a%20sharper%20and%20more%20complete%20surface.%20Our%20key%20idea%20is%20to%0Alearn%20a%20mapping%20from%20a%20low%20frequency%20observation%20to%20a%20full%20frequency%20coverage%0Ain%20a%20data-driven%20manner%2C%20leading%20to%20a%20prior%20knowledge%20of%20shape%20consolidation%20in%0Athe%20frequency%20domain%2C%20dubbed%20frequency%20consolidation%20priors.%20To%20better%0Ageneralize%20a%20learned%20prior%20to%20unseen%20shapes%2C%20we%20introduce%20to%20represent%0Afrequency%20components%20as%20embeddings%20and%20disentangle%20the%20embedding%20of%20the%20low%0Afrequency%20component%20from%20the%20embedding%20of%20the%20full%20frequency%20component.%20This%0Adisentanglement%20allows%20the%20prior%20to%20generalize%20on%20an%20unseen%20low%20frequency%0Aobservation%20by%20simply%20recovering%20its%20full%20frequency%20embedding%20through%20a%0Atest-time%20self-reconstruction.%20Our%20evaluations%20under%20widely%20used%20benchmarks%20or%0Areal%20scenes%20show%20that%20our%20method%20can%20recover%20high%20frequency%20component%20and%0Aproduce%20more%20accurate%20surfaces%20than%20the%20latest%20methods.%20The%20code%2C%20data%2C%20and%0Apre-trained%20models%20are%20available%20at%20%5Curl%7Bhttps%3A//github.com/chenchao15/FCP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharpening%2520Neural%2520Implicit%2520Functions%2520with%2520Frequency%2520Consolidation%2520Priors%26entry.906535625%3DChao%2520Chen%2520and%2520Yu-Shen%2520Liu%2520and%2520Zhizhong%2520Han%26entry.1292438233%3D%2520%2520Signed%2520Distance%2520Functions%2520%2528SDFs%2529%2520are%2520vital%2520implicit%2520representations%2520to%250Arepresent%2520high%2520fidelity%25203D%2520surfaces.%2520Current%2520methods%2520mainly%2520leverage%2520a%2520neural%250Anetwork%2520to%2520learn%2520an%2520SDF%2520from%2520various%2520supervisions%2520including%2520signed%2520distances%252C%250A3D%2520point%2520clouds%252C%2520or%2520multi-view%2520images.%2520However%252C%2520due%2520to%2520various%2520reasons%250Aincluding%2520the%2520bias%2520of%2520neural%2520network%2520on%2520low%2520frequency%2520content%252C%25203D%2520unaware%250Asampling%252C%2520sparsity%2520in%2520point%2520clouds%252C%2520or%2520low%2520resolutions%2520of%2520images%252C%2520neural%250Aimplicit%2520representations%2520still%2520struggle%2520to%2520represent%2520geometries%2520with%2520high%250Afrequency%2520components%2520like%2520sharp%2520structures%252C%2520especially%2520for%2520the%2520ones%2520learned%250Afrom%2520images%2520or%2520point%2520clouds.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520method%250Ato%2520sharpen%2520a%2520low%2520frequency%2520SDF%2520observation%2520by%2520recovering%2520its%2520high%2520frequency%250Acomponents%252C%2520pursuing%2520a%2520sharper%2520and%2520more%2520complete%2520surface.%2520Our%2520key%2520idea%2520is%2520to%250Alearn%2520a%2520mapping%2520from%2520a%2520low%2520frequency%2520observation%2520to%2520a%2520full%2520frequency%2520coverage%250Ain%2520a%2520data-driven%2520manner%252C%2520leading%2520to%2520a%2520prior%2520knowledge%2520of%2520shape%2520consolidation%2520in%250Athe%2520frequency%2520domain%252C%2520dubbed%2520frequency%2520consolidation%2520priors.%2520To%2520better%250Ageneralize%2520a%2520learned%2520prior%2520to%2520unseen%2520shapes%252C%2520we%2520introduce%2520to%2520represent%250Afrequency%2520components%2520as%2520embeddings%2520and%2520disentangle%2520the%2520embedding%2520of%2520the%2520low%250Afrequency%2520component%2520from%2520the%2520embedding%2520of%2520the%2520full%2520frequency%2520component.%2520This%250Adisentanglement%2520allows%2520the%2520prior%2520to%2520generalize%2520on%2520an%2520unseen%2520low%2520frequency%250Aobservation%2520by%2520simply%2520recovering%2520its%2520full%2520frequency%2520embedding%2520through%2520a%250Atest-time%2520self-reconstruction.%2520Our%2520evaluations%2520under%2520widely%2520used%2520benchmarks%2520or%250Areal%2520scenes%2520show%2520that%2520our%2520method%2520can%2520recover%2520high%2520frequency%2520component%2520and%250Aproduce%2520more%2520accurate%2520surfaces%2520than%2520the%2520latest%2520methods.%2520The%2520code%252C%2520data%252C%2520and%250Apre-trained%2520models%2520are%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/chenchao15/FCP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharpening%20Neural%20Implicit%20Functions%20with%20Frequency%20Consolidation%20Priors&entry.906535625=Chao%20Chen%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han&entry.1292438233=%20%20Signed%20Distance%20Functions%20%28SDFs%29%20are%20vital%20implicit%20representations%20to%0Arepresent%20high%20fidelity%203D%20surfaces.%20Current%20methods%20mainly%20leverage%20a%20neural%0Anetwork%20to%20learn%20an%20SDF%20from%20various%20supervisions%20including%20signed%20distances%2C%0A3D%20point%20clouds%2C%20or%20multi-view%20images.%20However%2C%20due%20to%20various%20reasons%0Aincluding%20the%20bias%20of%20neural%20network%20on%20low%20frequency%20content%2C%203D%20unaware%0Asampling%2C%20sparsity%20in%20point%20clouds%2C%20or%20low%20resolutions%20of%20images%2C%20neural%0Aimplicit%20representations%20still%20struggle%20to%20represent%20geometries%20with%20high%0Afrequency%20components%20like%20sharp%20structures%2C%20especially%20for%20the%20ones%20learned%0Afrom%20images%20or%20point%20clouds.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20a%20method%0Ato%20sharpen%20a%20low%20frequency%20SDF%20observation%20by%20recovering%20its%20high%20frequency%0Acomponents%2C%20pursuing%20a%20sharper%20and%20more%20complete%20surface.%20Our%20key%20idea%20is%20to%0Alearn%20a%20mapping%20from%20a%20low%20frequency%20observation%20to%20a%20full%20frequency%20coverage%0Ain%20a%20data-driven%20manner%2C%20leading%20to%20a%20prior%20knowledge%20of%20shape%20consolidation%20in%0Athe%20frequency%20domain%2C%20dubbed%20frequency%20consolidation%20priors.%20To%20better%0Ageneralize%20a%20learned%20prior%20to%20unseen%20shapes%2C%20we%20introduce%20to%20represent%0Afrequency%20components%20as%20embeddings%20and%20disentangle%20the%20embedding%20of%20the%20low%0Afrequency%20component%20from%20the%20embedding%20of%20the%20full%20frequency%20component.%20This%0Adisentanglement%20allows%20the%20prior%20to%20generalize%20on%20an%20unseen%20low%20frequency%0Aobservation%20by%20simply%20recovering%20its%20full%20frequency%20embedding%20through%20a%0Atest-time%20self-reconstruction.%20Our%20evaluations%20under%20widely%20used%20benchmarks%20or%0Areal%20scenes%20show%20that%20our%20method%20can%20recover%20high%20frequency%20component%20and%0Aproduce%20more%20accurate%20surfaces%20than%20the%20latest%20methods.%20The%20code%2C%20data%2C%20and%0Apre-trained%20models%20are%20available%20at%20%5Curl%7Bhttps%3A//github.com/chenchao15/FCP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19720v1&entry.124074799=Read"},
{"title": "ReNeg: Learning Negative Embedding with Reward Guidance", "author": "Xiaomin Li and Yixuan Liu and Takashi Isobe and Xu Jia and Qinpeng Cui and Dong Zhou and Dong Li and You He and Huchuan Lu and Zhongdao Wang and Emad Barsoum", "abstract": "  In text-to-image (T2I) generation applications, negative embeddings have\nproven to be a simple yet effective approach for enhancing generation quality.\nTypically, these negative embeddings are derived from user-defined negative\nprompts, which, while being functional, are not necessarily optimal. In this\npaper, we introduce ReNeg, an end-to-end method designed to learn improved\nNegative embeddings guided by a Reward model. We employ a reward feedback\nlearning framework and integrate classifier-free guidance (CFG) into the\ntraining process, which was previously utilized only during inference, thus\nenabling the effective learning of negative embeddings. We also propose two\nstrategies for learning both global and per-sample negative embeddings.\nExtensive experiments show that the learned negative embedding significantly\noutperforms null-text and handcrafted counterparts, achieving substantial\nimprovements in human preference alignment. Additionally, the negative\nembedding learned within the same text embedding space exhibits strong\ngeneralization capabilities. For example, using the same CLIP text encoder, the\nnegative embedding learned on SD1.5 can be seamlessly transferred to\ntext-to-image or even text-to-video models such as ControlNet, ZeroScope, and\nVideoCrafter2, resulting in consistent performance improvements across the\nboard.\n", "link": "http://arxiv.org/abs/2412.19637v1", "date": "2024-12-27", "relevancy": 2.6696, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5377}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5372}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReNeg%3A%20Learning%20Negative%20Embedding%20with%20Reward%20Guidance&body=Title%3A%20ReNeg%3A%20Learning%20Negative%20Embedding%20with%20Reward%20Guidance%0AAuthor%3A%20Xiaomin%20Li%20and%20Yixuan%20Liu%20and%20Takashi%20Isobe%20and%20Xu%20Jia%20and%20Qinpeng%20Cui%20and%20Dong%20Zhou%20and%20Dong%20Li%20and%20You%20He%20and%20Huchuan%20Lu%20and%20Zhongdao%20Wang%20and%20Emad%20Barsoum%0AAbstract%3A%20%20%20In%20text-to-image%20%28T2I%29%20generation%20applications%2C%20negative%20embeddings%20have%0Aproven%20to%20be%20a%20simple%20yet%20effective%20approach%20for%20enhancing%20generation%20quality.%0ATypically%2C%20these%20negative%20embeddings%20are%20derived%20from%20user-defined%20negative%0Aprompts%2C%20which%2C%20while%20being%20functional%2C%20are%20not%20necessarily%20optimal.%20In%20this%0Apaper%2C%20we%20introduce%20ReNeg%2C%20an%20end-to-end%20method%20designed%20to%20learn%20improved%0ANegative%20embeddings%20guided%20by%20a%20Reward%20model.%20We%20employ%20a%20reward%20feedback%0Alearning%20framework%20and%20integrate%20classifier-free%20guidance%20%28CFG%29%20into%20the%0Atraining%20process%2C%20which%20was%20previously%20utilized%20only%20during%20inference%2C%20thus%0Aenabling%20the%20effective%20learning%20of%20negative%20embeddings.%20We%20also%20propose%20two%0Astrategies%20for%20learning%20both%20global%20and%20per-sample%20negative%20embeddings.%0AExtensive%20experiments%20show%20that%20the%20learned%20negative%20embedding%20significantly%0Aoutperforms%20null-text%20and%20handcrafted%20counterparts%2C%20achieving%20substantial%0Aimprovements%20in%20human%20preference%20alignment.%20Additionally%2C%20the%20negative%0Aembedding%20learned%20within%20the%20same%20text%20embedding%20space%20exhibits%20strong%0Ageneralization%20capabilities.%20For%20example%2C%20using%20the%20same%20CLIP%20text%20encoder%2C%20the%0Anegative%20embedding%20learned%20on%20SD1.5%20can%20be%20seamlessly%20transferred%20to%0Atext-to-image%20or%20even%20text-to-video%20models%20such%20as%20ControlNet%2C%20ZeroScope%2C%20and%0AVideoCrafter2%2C%20resulting%20in%20consistent%20performance%20improvements%20across%20the%0Aboard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReNeg%253A%2520Learning%2520Negative%2520Embedding%2520with%2520Reward%2520Guidance%26entry.906535625%3DXiaomin%2520Li%2520and%2520Yixuan%2520Liu%2520and%2520Takashi%2520Isobe%2520and%2520Xu%2520Jia%2520and%2520Qinpeng%2520Cui%2520and%2520Dong%2520Zhou%2520and%2520Dong%2520Li%2520and%2520You%2520He%2520and%2520Huchuan%2520Lu%2520and%2520Zhongdao%2520Wang%2520and%2520Emad%2520Barsoum%26entry.1292438233%3D%2520%2520In%2520text-to-image%2520%2528T2I%2529%2520generation%2520applications%252C%2520negative%2520embeddings%2520have%250Aproven%2520to%2520be%2520a%2520simple%2520yet%2520effective%2520approach%2520for%2520enhancing%2520generation%2520quality.%250ATypically%252C%2520these%2520negative%2520embeddings%2520are%2520derived%2520from%2520user-defined%2520negative%250Aprompts%252C%2520which%252C%2520while%2520being%2520functional%252C%2520are%2520not%2520necessarily%2520optimal.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520ReNeg%252C%2520an%2520end-to-end%2520method%2520designed%2520to%2520learn%2520improved%250ANegative%2520embeddings%2520guided%2520by%2520a%2520Reward%2520model.%2520We%2520employ%2520a%2520reward%2520feedback%250Alearning%2520framework%2520and%2520integrate%2520classifier-free%2520guidance%2520%2528CFG%2529%2520into%2520the%250Atraining%2520process%252C%2520which%2520was%2520previously%2520utilized%2520only%2520during%2520inference%252C%2520thus%250Aenabling%2520the%2520effective%2520learning%2520of%2520negative%2520embeddings.%2520We%2520also%2520propose%2520two%250Astrategies%2520for%2520learning%2520both%2520global%2520and%2520per-sample%2520negative%2520embeddings.%250AExtensive%2520experiments%2520show%2520that%2520the%2520learned%2520negative%2520embedding%2520significantly%250Aoutperforms%2520null-text%2520and%2520handcrafted%2520counterparts%252C%2520achieving%2520substantial%250Aimprovements%2520in%2520human%2520preference%2520alignment.%2520Additionally%252C%2520the%2520negative%250Aembedding%2520learned%2520within%2520the%2520same%2520text%2520embedding%2520space%2520exhibits%2520strong%250Ageneralization%2520capabilities.%2520For%2520example%252C%2520using%2520the%2520same%2520CLIP%2520text%2520encoder%252C%2520the%250Anegative%2520embedding%2520learned%2520on%2520SD1.5%2520can%2520be%2520seamlessly%2520transferred%2520to%250Atext-to-image%2520or%2520even%2520text-to-video%2520models%2520such%2520as%2520ControlNet%252C%2520ZeroScope%252C%2520and%250AVideoCrafter2%252C%2520resulting%2520in%2520consistent%2520performance%2520improvements%2520across%2520the%250Aboard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReNeg%3A%20Learning%20Negative%20Embedding%20with%20Reward%20Guidance&entry.906535625=Xiaomin%20Li%20and%20Yixuan%20Liu%20and%20Takashi%20Isobe%20and%20Xu%20Jia%20and%20Qinpeng%20Cui%20and%20Dong%20Zhou%20and%20Dong%20Li%20and%20You%20He%20and%20Huchuan%20Lu%20and%20Zhongdao%20Wang%20and%20Emad%20Barsoum&entry.1292438233=%20%20In%20text-to-image%20%28T2I%29%20generation%20applications%2C%20negative%20embeddings%20have%0Aproven%20to%20be%20a%20simple%20yet%20effective%20approach%20for%20enhancing%20generation%20quality.%0ATypically%2C%20these%20negative%20embeddings%20are%20derived%20from%20user-defined%20negative%0Aprompts%2C%20which%2C%20while%20being%20functional%2C%20are%20not%20necessarily%20optimal.%20In%20this%0Apaper%2C%20we%20introduce%20ReNeg%2C%20an%20end-to-end%20method%20designed%20to%20learn%20improved%0ANegative%20embeddings%20guided%20by%20a%20Reward%20model.%20We%20employ%20a%20reward%20feedback%0Alearning%20framework%20and%20integrate%20classifier-free%20guidance%20%28CFG%29%20into%20the%0Atraining%20process%2C%20which%20was%20previously%20utilized%20only%20during%20inference%2C%20thus%0Aenabling%20the%20effective%20learning%20of%20negative%20embeddings.%20We%20also%20propose%20two%0Astrategies%20for%20learning%20both%20global%20and%20per-sample%20negative%20embeddings.%0AExtensive%20experiments%20show%20that%20the%20learned%20negative%20embedding%20significantly%0Aoutperforms%20null-text%20and%20handcrafted%20counterparts%2C%20achieving%20substantial%0Aimprovements%20in%20human%20preference%20alignment.%20Additionally%2C%20the%20negative%0Aembedding%20learned%20within%20the%20same%20text%20embedding%20space%20exhibits%20strong%0Ageneralization%20capabilities.%20For%20example%2C%20using%20the%20same%20CLIP%20text%20encoder%2C%20the%0Anegative%20embedding%20learned%20on%20SD1.5%20can%20be%20seamlessly%20transferred%20to%0Atext-to-image%20or%20even%20text-to-video%20models%20such%20as%20ControlNet%2C%20ZeroScope%2C%20and%0AVideoCrafter2%2C%20resulting%20in%20consistent%20performance%20improvements%20across%20the%0Aboard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19637v1&entry.124074799=Read"},
{"title": "Free-viewpoint Human Animation with Pose-correlated Reference Selection", "author": "Fa-Ting Hong and Zhan Xu and Haiyang Liu and Qinjie Lin and Luchuan Song and Zhixin Shu and Yang Zhou and Duygu Ceylan and Dan Xu", "abstract": "  Diffusion-based human animation aims to animate a human character based on a\nsource human image as well as driving signals such as a sequence of poses.\nLeveraging the generative capacity of diffusion model, existing approaches are\nable to generate high-fidelity poses, but struggle with significant viewpoint\nchanges, especially in zoom-in/zoom-out scenarios where camera-character\ndistance varies. This limits the applications such as cinematic shot type plan\nor camera control. We propose a pose-correlated reference selection diffusion\nnetwork, supporting substantial viewpoint variations in human animation. Our\nkey idea is to enable the network to utilize multiple reference images as\ninput, since significant viewpoint changes often lead to missing appearance\ndetails on the human body. To eliminate the computational cost, we first\nintroduce a novel pose correlation module to compute similarities between\nnon-aligned target and source poses, and then propose an adaptive reference\nselection strategy, utilizing the attention map to identify key regions for\nanimation generation. To train our model, we curated a large dataset from\npublic TED talks featuring varied shots of the same character, helping the\nmodel learn synthesis for different perspectives. Our experimental results show\nthat with the same number of reference images, our model performs favorably\ncompared to the current SOTA methods under large viewpoint change. We further\nshow that the adaptive reference selection is able to choose the most relevant\nreference regions to generate humans under free viewpoints.\n", "link": "http://arxiv.org/abs/2412.17290v2", "date": "2024-12-27", "relevancy": 2.6687, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6965}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6656}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-viewpoint%20Human%20Animation%20with%20Pose-correlated%20Reference%20Selection&body=Title%3A%20Free-viewpoint%20Human%20Animation%20with%20Pose-correlated%20Reference%20Selection%0AAuthor%3A%20Fa-Ting%20Hong%20and%20Zhan%20Xu%20and%20Haiyang%20Liu%20and%20Qinjie%20Lin%20and%20Luchuan%20Song%20and%20Zhixin%20Shu%20and%20Yang%20Zhou%20and%20Duygu%20Ceylan%20and%20Dan%20Xu%0AAbstract%3A%20%20%20Diffusion-based%20human%20animation%20aims%20to%20animate%20a%20human%20character%20based%20on%20a%0Asource%20human%20image%20as%20well%20as%20driving%20signals%20such%20as%20a%20sequence%20of%20poses.%0ALeveraging%20the%20generative%20capacity%20of%20diffusion%20model%2C%20existing%20approaches%20are%0Aable%20to%20generate%20high-fidelity%20poses%2C%20but%20struggle%20with%20significant%20viewpoint%0Achanges%2C%20especially%20in%20zoom-in/zoom-out%20scenarios%20where%20camera-character%0Adistance%20varies.%20This%20limits%20the%20applications%20such%20as%20cinematic%20shot%20type%20plan%0Aor%20camera%20control.%20We%20propose%20a%20pose-correlated%20reference%20selection%20diffusion%0Anetwork%2C%20supporting%20substantial%20viewpoint%20variations%20in%20human%20animation.%20Our%0Akey%20idea%20is%20to%20enable%20the%20network%20to%20utilize%20multiple%20reference%20images%20as%0Ainput%2C%20since%20significant%20viewpoint%20changes%20often%20lead%20to%20missing%20appearance%0Adetails%20on%20the%20human%20body.%20To%20eliminate%20the%20computational%20cost%2C%20we%20first%0Aintroduce%20a%20novel%20pose%20correlation%20module%20to%20compute%20similarities%20between%0Anon-aligned%20target%20and%20source%20poses%2C%20and%20then%20propose%20an%20adaptive%20reference%0Aselection%20strategy%2C%20utilizing%20the%20attention%20map%20to%20identify%20key%20regions%20for%0Aanimation%20generation.%20To%20train%20our%20model%2C%20we%20curated%20a%20large%20dataset%20from%0Apublic%20TED%20talks%20featuring%20varied%20shots%20of%20the%20same%20character%2C%20helping%20the%0Amodel%20learn%20synthesis%20for%20different%20perspectives.%20Our%20experimental%20results%20show%0Athat%20with%20the%20same%20number%20of%20reference%20images%2C%20our%20model%20performs%20favorably%0Acompared%20to%20the%20current%20SOTA%20methods%20under%20large%20viewpoint%20change.%20We%20further%0Ashow%20that%20the%20adaptive%20reference%20selection%20is%20able%20to%20choose%20the%20most%20relevant%0Areference%20regions%20to%20generate%20humans%20under%20free%20viewpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17290v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-viewpoint%2520Human%2520Animation%2520with%2520Pose-correlated%2520Reference%2520Selection%26entry.906535625%3DFa-Ting%2520Hong%2520and%2520Zhan%2520Xu%2520and%2520Haiyang%2520Liu%2520and%2520Qinjie%2520Lin%2520and%2520Luchuan%2520Song%2520and%2520Zhixin%2520Shu%2520and%2520Yang%2520Zhou%2520and%2520Duygu%2520Ceylan%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520Diffusion-based%2520human%2520animation%2520aims%2520to%2520animate%2520a%2520human%2520character%2520based%2520on%2520a%250Asource%2520human%2520image%2520as%2520well%2520as%2520driving%2520signals%2520such%2520as%2520a%2520sequence%2520of%2520poses.%250ALeveraging%2520the%2520generative%2520capacity%2520of%2520diffusion%2520model%252C%2520existing%2520approaches%2520are%250Aable%2520to%2520generate%2520high-fidelity%2520poses%252C%2520but%2520struggle%2520with%2520significant%2520viewpoint%250Achanges%252C%2520especially%2520in%2520zoom-in/zoom-out%2520scenarios%2520where%2520camera-character%250Adistance%2520varies.%2520This%2520limits%2520the%2520applications%2520such%2520as%2520cinematic%2520shot%2520type%2520plan%250Aor%2520camera%2520control.%2520We%2520propose%2520a%2520pose-correlated%2520reference%2520selection%2520diffusion%250Anetwork%252C%2520supporting%2520substantial%2520viewpoint%2520variations%2520in%2520human%2520animation.%2520Our%250Akey%2520idea%2520is%2520to%2520enable%2520the%2520network%2520to%2520utilize%2520multiple%2520reference%2520images%2520as%250Ainput%252C%2520since%2520significant%2520viewpoint%2520changes%2520often%2520lead%2520to%2520missing%2520appearance%250Adetails%2520on%2520the%2520human%2520body.%2520To%2520eliminate%2520the%2520computational%2520cost%252C%2520we%2520first%250Aintroduce%2520a%2520novel%2520pose%2520correlation%2520module%2520to%2520compute%2520similarities%2520between%250Anon-aligned%2520target%2520and%2520source%2520poses%252C%2520and%2520then%2520propose%2520an%2520adaptive%2520reference%250Aselection%2520strategy%252C%2520utilizing%2520the%2520attention%2520map%2520to%2520identify%2520key%2520regions%2520for%250Aanimation%2520generation.%2520To%2520train%2520our%2520model%252C%2520we%2520curated%2520a%2520large%2520dataset%2520from%250Apublic%2520TED%2520talks%2520featuring%2520varied%2520shots%2520of%2520the%2520same%2520character%252C%2520helping%2520the%250Amodel%2520learn%2520synthesis%2520for%2520different%2520perspectives.%2520Our%2520experimental%2520results%2520show%250Athat%2520with%2520the%2520same%2520number%2520of%2520reference%2520images%252C%2520our%2520model%2520performs%2520favorably%250Acompared%2520to%2520the%2520current%2520SOTA%2520methods%2520under%2520large%2520viewpoint%2520change.%2520We%2520further%250Ashow%2520that%2520the%2520adaptive%2520reference%2520selection%2520is%2520able%2520to%2520choose%2520the%2520most%2520relevant%250Areference%2520regions%2520to%2520generate%2520humans%2520under%2520free%2520viewpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17290v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-viewpoint%20Human%20Animation%20with%20Pose-correlated%20Reference%20Selection&entry.906535625=Fa-Ting%20Hong%20and%20Zhan%20Xu%20and%20Haiyang%20Liu%20and%20Qinjie%20Lin%20and%20Luchuan%20Song%20and%20Zhixin%20Shu%20and%20Yang%20Zhou%20and%20Duygu%20Ceylan%20and%20Dan%20Xu&entry.1292438233=%20%20Diffusion-based%20human%20animation%20aims%20to%20animate%20a%20human%20character%20based%20on%20a%0Asource%20human%20image%20as%20well%20as%20driving%20signals%20such%20as%20a%20sequence%20of%20poses.%0ALeveraging%20the%20generative%20capacity%20of%20diffusion%20model%2C%20existing%20approaches%20are%0Aable%20to%20generate%20high-fidelity%20poses%2C%20but%20struggle%20with%20significant%20viewpoint%0Achanges%2C%20especially%20in%20zoom-in/zoom-out%20scenarios%20where%20camera-character%0Adistance%20varies.%20This%20limits%20the%20applications%20such%20as%20cinematic%20shot%20type%20plan%0Aor%20camera%20control.%20We%20propose%20a%20pose-correlated%20reference%20selection%20diffusion%0Anetwork%2C%20supporting%20substantial%20viewpoint%20variations%20in%20human%20animation.%20Our%0Akey%20idea%20is%20to%20enable%20the%20network%20to%20utilize%20multiple%20reference%20images%20as%0Ainput%2C%20since%20significant%20viewpoint%20changes%20often%20lead%20to%20missing%20appearance%0Adetails%20on%20the%20human%20body.%20To%20eliminate%20the%20computational%20cost%2C%20we%20first%0Aintroduce%20a%20novel%20pose%20correlation%20module%20to%20compute%20similarities%20between%0Anon-aligned%20target%20and%20source%20poses%2C%20and%20then%20propose%20an%20adaptive%20reference%0Aselection%20strategy%2C%20utilizing%20the%20attention%20map%20to%20identify%20key%20regions%20for%0Aanimation%20generation.%20To%20train%20our%20model%2C%20we%20curated%20a%20large%20dataset%20from%0Apublic%20TED%20talks%20featuring%20varied%20shots%20of%20the%20same%20character%2C%20helping%20the%0Amodel%20learn%20synthesis%20for%20different%20perspectives.%20Our%20experimental%20results%20show%0Athat%20with%20the%20same%20number%20of%20reference%20images%2C%20our%20model%20performs%20favorably%0Acompared%20to%20the%20current%20SOTA%20methods%20under%20large%20viewpoint%20change.%20We%20further%0Ashow%20that%20the%20adaptive%20reference%20selection%20is%20able%20to%20choose%20the%20most%20relevant%0Areference%20regions%20to%20generate%20humans%20under%20free%20viewpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17290v2&entry.124074799=Read"},
{"title": "Enhancing Adversarial Robustness of Deep Neural Networks Through\n  Supervised Contrastive Learning", "author": "Longwei Wang and Navid Nayyem and Abdullah Rakin", "abstract": "  Adversarial attacks exploit the vulnerabilities of convolutional neural\nnetworks by introducing imperceptible perturbations that lead to\nmisclassifications, exposing weaknesses in feature representations and decision\nboundaries. This paper presents a novel framework combining supervised\ncontrastive learning and margin-based contrastive loss to enhance adversarial\nrobustness. Supervised contrastive learning improves the structure of the\nfeature space by clustering embeddings of samples within the same class and\nseparating those from different classes. Margin-based contrastive loss,\ninspired by support vector machines, enforces explicit constraints to create\nrobust decision boundaries with well-defined margins. Experiments on the\nCIFAR-100 dataset with a ResNet-18 backbone demonstrate robustness performance\nimprovements in adversarial accuracy under Fast Gradient Sign Method attacks.\n", "link": "http://arxiv.org/abs/2412.19747v1", "date": "2024-12-27", "relevancy": 2.6625, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5467}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5265}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Adversarial%20Robustness%20of%20Deep%20Neural%20Networks%20Through%0A%20%20Supervised%20Contrastive%20Learning&body=Title%3A%20Enhancing%20Adversarial%20Robustness%20of%20Deep%20Neural%20Networks%20Through%0A%20%20Supervised%20Contrastive%20Learning%0AAuthor%3A%20Longwei%20Wang%20and%20Navid%20Nayyem%20and%20Abdullah%20Rakin%0AAbstract%3A%20%20%20Adversarial%20attacks%20exploit%20the%20vulnerabilities%20of%20convolutional%20neural%0Anetworks%20by%20introducing%20imperceptible%20perturbations%20that%20lead%20to%0Amisclassifications%2C%20exposing%20weaknesses%20in%20feature%20representations%20and%20decision%0Aboundaries.%20This%20paper%20presents%20a%20novel%20framework%20combining%20supervised%0Acontrastive%20learning%20and%20margin-based%20contrastive%20loss%20to%20enhance%20adversarial%0Arobustness.%20Supervised%20contrastive%20learning%20improves%20the%20structure%20of%20the%0Afeature%20space%20by%20clustering%20embeddings%20of%20samples%20within%20the%20same%20class%20and%0Aseparating%20those%20from%20different%20classes.%20Margin-based%20contrastive%20loss%2C%0Ainspired%20by%20support%20vector%20machines%2C%20enforces%20explicit%20constraints%20to%20create%0Arobust%20decision%20boundaries%20with%20well-defined%20margins.%20Experiments%20on%20the%0ACIFAR-100%20dataset%20with%20a%20ResNet-18%20backbone%20demonstrate%20robustness%20performance%0Aimprovements%20in%20adversarial%20accuracy%20under%20Fast%20Gradient%20Sign%20Method%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Adversarial%2520Robustness%2520of%2520Deep%2520Neural%2520Networks%2520Through%250A%2520%2520Supervised%2520Contrastive%2520Learning%26entry.906535625%3DLongwei%2520Wang%2520and%2520Navid%2520Nayyem%2520and%2520Abdullah%2520Rakin%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520exploit%2520the%2520vulnerabilities%2520of%2520convolutional%2520neural%250Anetworks%2520by%2520introducing%2520imperceptible%2520perturbations%2520that%2520lead%2520to%250Amisclassifications%252C%2520exposing%2520weaknesses%2520in%2520feature%2520representations%2520and%2520decision%250Aboundaries.%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520combining%2520supervised%250Acontrastive%2520learning%2520and%2520margin-based%2520contrastive%2520loss%2520to%2520enhance%2520adversarial%250Arobustness.%2520Supervised%2520contrastive%2520learning%2520improves%2520the%2520structure%2520of%2520the%250Afeature%2520space%2520by%2520clustering%2520embeddings%2520of%2520samples%2520within%2520the%2520same%2520class%2520and%250Aseparating%2520those%2520from%2520different%2520classes.%2520Margin-based%2520contrastive%2520loss%252C%250Ainspired%2520by%2520support%2520vector%2520machines%252C%2520enforces%2520explicit%2520constraints%2520to%2520create%250Arobust%2520decision%2520boundaries%2520with%2520well-defined%2520margins.%2520Experiments%2520on%2520the%250ACIFAR-100%2520dataset%2520with%2520a%2520ResNet-18%2520backbone%2520demonstrate%2520robustness%2520performance%250Aimprovements%2520in%2520adversarial%2520accuracy%2520under%2520Fast%2520Gradient%2520Sign%2520Method%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Adversarial%20Robustness%20of%20Deep%20Neural%20Networks%20Through%0A%20%20Supervised%20Contrastive%20Learning&entry.906535625=Longwei%20Wang%20and%20Navid%20Nayyem%20and%20Abdullah%20Rakin&entry.1292438233=%20%20Adversarial%20attacks%20exploit%20the%20vulnerabilities%20of%20convolutional%20neural%0Anetworks%20by%20introducing%20imperceptible%20perturbations%20that%20lead%20to%0Amisclassifications%2C%20exposing%20weaknesses%20in%20feature%20representations%20and%20decision%0Aboundaries.%20This%20paper%20presents%20a%20novel%20framework%20combining%20supervised%0Acontrastive%20learning%20and%20margin-based%20contrastive%20loss%20to%20enhance%20adversarial%0Arobustness.%20Supervised%20contrastive%20learning%20improves%20the%20structure%20of%20the%0Afeature%20space%20by%20clustering%20embeddings%20of%20samples%20within%20the%20same%20class%20and%0Aseparating%20those%20from%20different%20classes.%20Margin-based%20contrastive%20loss%2C%0Ainspired%20by%20support%20vector%20machines%2C%20enforces%20explicit%20constraints%20to%20create%0Arobust%20decision%20boundaries%20with%20well-defined%20margins.%20Experiments%20on%20the%0ACIFAR-100%20dataset%20with%20a%20ResNet-18%20backbone%20demonstrate%20robustness%20performance%0Aimprovements%20in%20adversarial%20accuracy%20under%20Fast%20Gradient%20Sign%20Method%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19747v1&entry.124074799=Read"},
{"title": "Structural Similarity in Deep Features: Image Quality Assessment Robust\n  to Geometrically Disparate Reference", "author": "Keke Zhang and Weiling Chen and Tiesong Zhao and Zhou Wang", "abstract": "  Image Quality Assessment (IQA) with references plays an important role in\noptimizing and evaluating computer vision tasks. Traditional methods assume\nthat all pixels of the reference and test images are fully aligned. Such\nAligned-Reference IQA (AR-IQA) approaches fail to address many real-world\nproblems with various geometric deformations between the two images. Although\nsignificant effort has been made to attack Geometrically-Disparate-Reference\nIQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for\nexample, by dedicated designs for image super-resolution and retargeting, or by\nassuming the geometric distortions to be small that can be countered by\ntranslation-robust filters or by explicit image registrations. Here we rethink\nthis problem and propose a unified, non-training-based Deep Structural\nSimilarity (DeepSSIM) approach to address the above problems in a single\nframework, which assesses structural similarity of deep features in a simple\nbut efficient way and uses an attention calibration strategy to alleviate\nattention deviation. The proposed method, without application-specific design,\nachieves state-of-the-art performance on AR-IQA datasets and meanwhile shows\nstrong robustness to various GDR-IQA test cases. Interestingly, our test also\nshows the effectiveness of DeepSSIM as an optimization tool for training image\nsuper-resolution, enhancement and restoration, implying an even wider\ngeneralizability. \\footnote{Source code will be made public after the review is\ncompleted.\n", "link": "http://arxiv.org/abs/2412.19553v1", "date": "2024-12-27", "relevancy": 2.6329, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5355}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5237}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Similarity%20in%20Deep%20Features%3A%20Image%20Quality%20Assessment%20Robust%0A%20%20to%20Geometrically%20Disparate%20Reference&body=Title%3A%20Structural%20Similarity%20in%20Deep%20Features%3A%20Image%20Quality%20Assessment%20Robust%0A%20%20to%20Geometrically%20Disparate%20Reference%0AAuthor%3A%20Keke%20Zhang%20and%20Weiling%20Chen%20and%20Tiesong%20Zhao%20and%20Zhou%20Wang%0AAbstract%3A%20%20%20Image%20Quality%20Assessment%20%28IQA%29%20with%20references%20plays%20an%20important%20role%20in%0Aoptimizing%20and%20evaluating%20computer%20vision%20tasks.%20Traditional%20methods%20assume%0Athat%20all%20pixels%20of%20the%20reference%20and%20test%20images%20are%20fully%20aligned.%20Such%0AAligned-Reference%20IQA%20%28AR-IQA%29%20approaches%20fail%20to%20address%20many%20real-world%0Aproblems%20with%20various%20geometric%20deformations%20between%20the%20two%20images.%20Although%0Asignificant%20effort%20has%20been%20made%20to%20attack%20Geometrically-Disparate-Reference%0AIQA%20%28GDR-IQA%29%20problem%2C%20it%20has%20been%20addressed%20in%20a%20task-dependent%20fashion%2C%20for%0Aexample%2C%20by%20dedicated%20designs%20for%20image%20super-resolution%20and%20retargeting%2C%20or%20by%0Aassuming%20the%20geometric%20distortions%20to%20be%20small%20that%20can%20be%20countered%20by%0Atranslation-robust%20filters%20or%20by%20explicit%20image%20registrations.%20Here%20we%20rethink%0Athis%20problem%20and%20propose%20a%20unified%2C%20non-training-based%20Deep%20Structural%0ASimilarity%20%28DeepSSIM%29%20approach%20to%20address%20the%20above%20problems%20in%20a%20single%0Aframework%2C%20which%20assesses%20structural%20similarity%20of%20deep%20features%20in%20a%20simple%0Abut%20efficient%20way%20and%20uses%20an%20attention%20calibration%20strategy%20to%20alleviate%0Aattention%20deviation.%20The%20proposed%20method%2C%20without%20application-specific%20design%2C%0Aachieves%20state-of-the-art%20performance%20on%20AR-IQA%20datasets%20and%20meanwhile%20shows%0Astrong%20robustness%20to%20various%20GDR-IQA%20test%20cases.%20Interestingly%2C%20our%20test%20also%0Ashows%20the%20effectiveness%20of%20DeepSSIM%20as%20an%20optimization%20tool%20for%20training%20image%0Asuper-resolution%2C%20enhancement%20and%20restoration%2C%20implying%20an%20even%20wider%0Ageneralizability.%20%5Cfootnote%7BSource%20code%20will%20be%20made%20public%20after%20the%20review%20is%0Acompleted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Similarity%2520in%2520Deep%2520Features%253A%2520Image%2520Quality%2520Assessment%2520Robust%250A%2520%2520to%2520Geometrically%2520Disparate%2520Reference%26entry.906535625%3DKeke%2520Zhang%2520and%2520Weiling%2520Chen%2520and%2520Tiesong%2520Zhao%2520and%2520Zhou%2520Wang%26entry.1292438233%3D%2520%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520with%2520references%2520plays%2520an%2520important%2520role%2520in%250Aoptimizing%2520and%2520evaluating%2520computer%2520vision%2520tasks.%2520Traditional%2520methods%2520assume%250Athat%2520all%2520pixels%2520of%2520the%2520reference%2520and%2520test%2520images%2520are%2520fully%2520aligned.%2520Such%250AAligned-Reference%2520IQA%2520%2528AR-IQA%2529%2520approaches%2520fail%2520to%2520address%2520many%2520real-world%250Aproblems%2520with%2520various%2520geometric%2520deformations%2520between%2520the%2520two%2520images.%2520Although%250Asignificant%2520effort%2520has%2520been%2520made%2520to%2520attack%2520Geometrically-Disparate-Reference%250AIQA%2520%2528GDR-IQA%2529%2520problem%252C%2520it%2520has%2520been%2520addressed%2520in%2520a%2520task-dependent%2520fashion%252C%2520for%250Aexample%252C%2520by%2520dedicated%2520designs%2520for%2520image%2520super-resolution%2520and%2520retargeting%252C%2520or%2520by%250Aassuming%2520the%2520geometric%2520distortions%2520to%2520be%2520small%2520that%2520can%2520be%2520countered%2520by%250Atranslation-robust%2520filters%2520or%2520by%2520explicit%2520image%2520registrations.%2520Here%2520we%2520rethink%250Athis%2520problem%2520and%2520propose%2520a%2520unified%252C%2520non-training-based%2520Deep%2520Structural%250ASimilarity%2520%2528DeepSSIM%2529%2520approach%2520to%2520address%2520the%2520above%2520problems%2520in%2520a%2520single%250Aframework%252C%2520which%2520assesses%2520structural%2520similarity%2520of%2520deep%2520features%2520in%2520a%2520simple%250Abut%2520efficient%2520way%2520and%2520uses%2520an%2520attention%2520calibration%2520strategy%2520to%2520alleviate%250Aattention%2520deviation.%2520The%2520proposed%2520method%252C%2520without%2520application-specific%2520design%252C%250Aachieves%2520state-of-the-art%2520performance%2520on%2520AR-IQA%2520datasets%2520and%2520meanwhile%2520shows%250Astrong%2520robustness%2520to%2520various%2520GDR-IQA%2520test%2520cases.%2520Interestingly%252C%2520our%2520test%2520also%250Ashows%2520the%2520effectiveness%2520of%2520DeepSSIM%2520as%2520an%2520optimization%2520tool%2520for%2520training%2520image%250Asuper-resolution%252C%2520enhancement%2520and%2520restoration%252C%2520implying%2520an%2520even%2520wider%250Ageneralizability.%2520%255Cfootnote%257BSource%2520code%2520will%2520be%2520made%2520public%2520after%2520the%2520review%2520is%250Acompleted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Similarity%20in%20Deep%20Features%3A%20Image%20Quality%20Assessment%20Robust%0A%20%20to%20Geometrically%20Disparate%20Reference&entry.906535625=Keke%20Zhang%20and%20Weiling%20Chen%20and%20Tiesong%20Zhao%20and%20Zhou%20Wang&entry.1292438233=%20%20Image%20Quality%20Assessment%20%28IQA%29%20with%20references%20plays%20an%20important%20role%20in%0Aoptimizing%20and%20evaluating%20computer%20vision%20tasks.%20Traditional%20methods%20assume%0Athat%20all%20pixels%20of%20the%20reference%20and%20test%20images%20are%20fully%20aligned.%20Such%0AAligned-Reference%20IQA%20%28AR-IQA%29%20approaches%20fail%20to%20address%20many%20real-world%0Aproblems%20with%20various%20geometric%20deformations%20between%20the%20two%20images.%20Although%0Asignificant%20effort%20has%20been%20made%20to%20attack%20Geometrically-Disparate-Reference%0AIQA%20%28GDR-IQA%29%20problem%2C%20it%20has%20been%20addressed%20in%20a%20task-dependent%20fashion%2C%20for%0Aexample%2C%20by%20dedicated%20designs%20for%20image%20super-resolution%20and%20retargeting%2C%20or%20by%0Aassuming%20the%20geometric%20distortions%20to%20be%20small%20that%20can%20be%20countered%20by%0Atranslation-robust%20filters%20or%20by%20explicit%20image%20registrations.%20Here%20we%20rethink%0Athis%20problem%20and%20propose%20a%20unified%2C%20non-training-based%20Deep%20Structural%0ASimilarity%20%28DeepSSIM%29%20approach%20to%20address%20the%20above%20problems%20in%20a%20single%0Aframework%2C%20which%20assesses%20structural%20similarity%20of%20deep%20features%20in%20a%20simple%0Abut%20efficient%20way%20and%20uses%20an%20attention%20calibration%20strategy%20to%20alleviate%0Aattention%20deviation.%20The%20proposed%20method%2C%20without%20application-specific%20design%2C%0Aachieves%20state-of-the-art%20performance%20on%20AR-IQA%20datasets%20and%20meanwhile%20shows%0Astrong%20robustness%20to%20various%20GDR-IQA%20test%20cases.%20Interestingly%2C%20our%20test%20also%0Ashows%20the%20effectiveness%20of%20DeepSSIM%20as%20an%20optimization%20tool%20for%20training%20image%0Asuper-resolution%2C%20enhancement%20and%20restoration%2C%20implying%20an%20even%20wider%0Ageneralizability.%20%5Cfootnote%7BSource%20code%20will%20be%20made%20public%20after%20the%20review%20is%0Acompleted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19553v1&entry.124074799=Read"},
{"title": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation", "author": "Ziyang Chen and Yiwen Ye and Yongsheng Pan and Yong Xia", "abstract": "  Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels are available at https://github.com/Chen-Ziyang/GraTa.\n", "link": "http://arxiv.org/abs/2408.07343v4", "date": "2024-12-27", "relevancy": 2.6178, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5254}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Alignment%20Improves%20Test-Time%20Adaptation%20for%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20Gradient%20Alignment%20Improves%20Test-Time%20Adaptation%20for%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Ziyang%20Chen%20and%20Yiwen%20Ye%20and%20Yongsheng%20Pan%20and%20Yong%20Xia%0AAbstract%3A%20%20%20Although%20recent%20years%20have%20witnessed%20significant%20advancements%20in%20medical%0Aimage%20segmentation%2C%20the%20pervasive%20issue%20of%20domain%20shift%20among%20medical%20images%0Afrom%20diverse%20centres%20hinders%20the%20effective%20deployment%20of%20pre-trained%20models.%0AMany%20Test-time%20Adaptation%20%28TTA%29%20methods%20have%20been%20proposed%20to%20address%20this%0Aissue%20by%20fine-tuning%20pre-trained%20models%20with%20test%20data%20during%20inference.%20These%0Amethods%2C%20however%2C%20often%20suffer%20from%20less-satisfactory%20optimization%20due%20to%0Asuboptimal%20optimization%20direction%20%28dictated%20by%20the%20gradient%29%20and%20fixed%0Astep-size%20%28predicated%20on%20the%20learning%20rate%29.%20In%20this%20paper%2C%20we%20propose%20the%0AGradient%20alignment-based%20Test-time%20adaptation%20%28GraTa%29%20method%20to%20improve%20both%0Athe%20gradient%20direction%20and%20learning%20rate%20in%20the%20optimization%20procedure.%20Unlike%0Aconventional%20TTA%20methods%2C%20which%20primarily%20optimize%20the%20pseudo%20gradient%20derived%0Afrom%20a%20self-supervised%20objective%2C%20our%20method%20incorporates%20an%20auxiliary%20gradient%0Awith%20the%20pseudo%20one%20to%20facilitate%20gradient%20alignment.%20Such%20gradient%20alignment%0Aenables%20the%20model%20to%20excavate%20the%20similarities%20between%20different%20gradients%20and%0Acorrect%20the%20gradient%20direction%20to%20approximate%20the%20empirical%20gradient%20related%20to%0Athe%20current%20segmentation%20task.%20Additionally%2C%20we%20design%20a%20dynamic%20learning%20rate%0Abased%20on%20the%20cosine%20similarity%20between%20the%20pseudo%20and%20auxiliary%20gradients%2C%0Athereby%20empowering%20the%20adaptive%20fine-tuning%20of%20pre-trained%20models%20on%20diverse%0Atest%20data.%20Extensive%20experiments%20establish%20the%20effectiveness%20of%20the%20proposed%0Agradient%20alignment%20and%20dynamic%20learning%20rate%20and%20substantiate%20the%20superiority%0Aof%20our%20GraTa%20method%20over%20other%20state-of-the-art%20TTA%20methods%20on%20a%20benchmark%0Amedical%20image%20segmentation%20task.%20The%20code%20and%20weights%20of%20pre-trained%20source%0Amodels%20are%20available%20at%20https%3A//github.com/Chen-Ziyang/GraTa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07343v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Alignment%2520Improves%2520Test-Time%2520Adaptation%2520for%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DZiyang%2520Chen%2520and%2520Yiwen%2520Ye%2520and%2520Yongsheng%2520Pan%2520and%2520Yong%2520Xia%26entry.1292438233%3D%2520%2520Although%2520recent%2520years%2520have%2520witnessed%2520significant%2520advancements%2520in%2520medical%250Aimage%2520segmentation%252C%2520the%2520pervasive%2520issue%2520of%2520domain%2520shift%2520among%2520medical%2520images%250Afrom%2520diverse%2520centres%2520hinders%2520the%2520effective%2520deployment%2520of%2520pre-trained%2520models.%250AMany%2520Test-time%2520Adaptation%2520%2528TTA%2529%2520methods%2520have%2520been%2520proposed%2520to%2520address%2520this%250Aissue%2520by%2520fine-tuning%2520pre-trained%2520models%2520with%2520test%2520data%2520during%2520inference.%2520These%250Amethods%252C%2520however%252C%2520often%2520suffer%2520from%2520less-satisfactory%2520optimization%2520due%2520to%250Asuboptimal%2520optimization%2520direction%2520%2528dictated%2520by%2520the%2520gradient%2529%2520and%2520fixed%250Astep-size%2520%2528predicated%2520on%2520the%2520learning%2520rate%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%250AGradient%2520alignment-based%2520Test-time%2520adaptation%2520%2528GraTa%2529%2520method%2520to%2520improve%2520both%250Athe%2520gradient%2520direction%2520and%2520learning%2520rate%2520in%2520the%2520optimization%2520procedure.%2520Unlike%250Aconventional%2520TTA%2520methods%252C%2520which%2520primarily%2520optimize%2520the%2520pseudo%2520gradient%2520derived%250Afrom%2520a%2520self-supervised%2520objective%252C%2520our%2520method%2520incorporates%2520an%2520auxiliary%2520gradient%250Awith%2520the%2520pseudo%2520one%2520to%2520facilitate%2520gradient%2520alignment.%2520Such%2520gradient%2520alignment%250Aenables%2520the%2520model%2520to%2520excavate%2520the%2520similarities%2520between%2520different%2520gradients%2520and%250Acorrect%2520the%2520gradient%2520direction%2520to%2520approximate%2520the%2520empirical%2520gradient%2520related%2520to%250Athe%2520current%2520segmentation%2520task.%2520Additionally%252C%2520we%2520design%2520a%2520dynamic%2520learning%2520rate%250Abased%2520on%2520the%2520cosine%2520similarity%2520between%2520the%2520pseudo%2520and%2520auxiliary%2520gradients%252C%250Athereby%2520empowering%2520the%2520adaptive%2520fine-tuning%2520of%2520pre-trained%2520models%2520on%2520diverse%250Atest%2520data.%2520Extensive%2520experiments%2520establish%2520the%2520effectiveness%2520of%2520the%2520proposed%250Agradient%2520alignment%2520and%2520dynamic%2520learning%2520rate%2520and%2520substantiate%2520the%2520superiority%250Aof%2520our%2520GraTa%2520method%2520over%2520other%2520state-of-the-art%2520TTA%2520methods%2520on%2520a%2520benchmark%250Amedical%2520image%2520segmentation%2520task.%2520The%2520code%2520and%2520weights%2520of%2520pre-trained%2520source%250Amodels%2520are%2520available%2520at%2520https%253A//github.com/Chen-Ziyang/GraTa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07343v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Alignment%20Improves%20Test-Time%20Adaptation%20for%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Ziyang%20Chen%20and%20Yiwen%20Ye%20and%20Yongsheng%20Pan%20and%20Yong%20Xia&entry.1292438233=%20%20Although%20recent%20years%20have%20witnessed%20significant%20advancements%20in%20medical%0Aimage%20segmentation%2C%20the%20pervasive%20issue%20of%20domain%20shift%20among%20medical%20images%0Afrom%20diverse%20centres%20hinders%20the%20effective%20deployment%20of%20pre-trained%20models.%0AMany%20Test-time%20Adaptation%20%28TTA%29%20methods%20have%20been%20proposed%20to%20address%20this%0Aissue%20by%20fine-tuning%20pre-trained%20models%20with%20test%20data%20during%20inference.%20These%0Amethods%2C%20however%2C%20often%20suffer%20from%20less-satisfactory%20optimization%20due%20to%0Asuboptimal%20optimization%20direction%20%28dictated%20by%20the%20gradient%29%20and%20fixed%0Astep-size%20%28predicated%20on%20the%20learning%20rate%29.%20In%20this%20paper%2C%20we%20propose%20the%0AGradient%20alignment-based%20Test-time%20adaptation%20%28GraTa%29%20method%20to%20improve%20both%0Athe%20gradient%20direction%20and%20learning%20rate%20in%20the%20optimization%20procedure.%20Unlike%0Aconventional%20TTA%20methods%2C%20which%20primarily%20optimize%20the%20pseudo%20gradient%20derived%0Afrom%20a%20self-supervised%20objective%2C%20our%20method%20incorporates%20an%20auxiliary%20gradient%0Awith%20the%20pseudo%20one%20to%20facilitate%20gradient%20alignment.%20Such%20gradient%20alignment%0Aenables%20the%20model%20to%20excavate%20the%20similarities%20between%20different%20gradients%20and%0Acorrect%20the%20gradient%20direction%20to%20approximate%20the%20empirical%20gradient%20related%20to%0Athe%20current%20segmentation%20task.%20Additionally%2C%20we%20design%20a%20dynamic%20learning%20rate%0Abased%20on%20the%20cosine%20similarity%20between%20the%20pseudo%20and%20auxiliary%20gradients%2C%0Athereby%20empowering%20the%20adaptive%20fine-tuning%20of%20pre-trained%20models%20on%20diverse%0Atest%20data.%20Extensive%20experiments%20establish%20the%20effectiveness%20of%20the%20proposed%0Agradient%20alignment%20and%20dynamic%20learning%20rate%20and%20substantiate%20the%20superiority%0Aof%20our%20GraTa%20method%20over%20other%20state-of-the-art%20TTA%20methods%20on%20a%20benchmark%0Amedical%20image%20segmentation%20task.%20The%20code%20and%20weights%20of%20pre-trained%20source%0Amodels%20are%20available%20at%20https%3A//github.com/Chen-Ziyang/GraTa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07343v4&entry.124074799=Read"},
{"title": "Generative Video Propagation", "author": "Shaoteng Liu and Tianyu Wang and Jui-Hsien Wang and Qing Liu and Zhifei Zhang and Joon-Young Lee and Yijun Li and Bei Yu and Zhe Lin and Soo Ye Kim and Jiaya Jia", "abstract": "  Large-scale video generation models have the inherent ability to\nrealistically model natural scenes. In this paper, we demonstrate that through\na careful design of a generative video propagation framework, various video\ntasks can be addressed in a unified way by leveraging the generative power of\nsuch models. Specifically, our framework, GenProp, encodes the original video\nwith a selective content encoder and propagates the changes made to the first\nframe using an image-to-video generation model. We propose a data generation\nscheme to cover multiple video tasks based on instance-level video segmentation\ndatasets. Our model is trained by incorporating a mask prediction decoder head\nand optimizing a region-aware loss to aid the encoder to preserve the original\ncontent while the generation model propagates the modified region. This novel\ndesign opens up new possibilities: In editing scenarios, GenProp allows\nsubstantial changes to an object's shape; for insertion, the inserted objects\ncan exhibit independent motion; for removal, GenProp effectively removes\neffects like shadows and reflections from the whole video; for tracking,\nGenProp is capable of tracking objects and their associated effects together.\nExperiment results demonstrate the leading performance of our model in various\nvideo tasks, and we further provide in-depth analyses of the proposed\nframework.\n", "link": "http://arxiv.org/abs/2412.19761v1", "date": "2024-12-27", "relevancy": 2.6142, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6625}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6547}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Video%20Propagation&body=Title%3A%20Generative%20Video%20Propagation%0AAuthor%3A%20Shaoteng%20Liu%20and%20Tianyu%20Wang%20and%20Jui-Hsien%20Wang%20and%20Qing%20Liu%20and%20Zhifei%20Zhang%20and%20Joon-Young%20Lee%20and%20Yijun%20Li%20and%20Bei%20Yu%20and%20Zhe%20Lin%20and%20Soo%20Ye%20Kim%20and%20Jiaya%20Jia%0AAbstract%3A%20%20%20Large-scale%20video%20generation%20models%20have%20the%20inherent%20ability%20to%0Arealistically%20model%20natural%20scenes.%20In%20this%20paper%2C%20we%20demonstrate%20that%20through%0Aa%20careful%20design%20of%20a%20generative%20video%20propagation%20framework%2C%20various%20video%0Atasks%20can%20be%20addressed%20in%20a%20unified%20way%20by%20leveraging%20the%20generative%20power%20of%0Asuch%20models.%20Specifically%2C%20our%20framework%2C%20GenProp%2C%20encodes%20the%20original%20video%0Awith%20a%20selective%20content%20encoder%20and%20propagates%20the%20changes%20made%20to%20the%20first%0Aframe%20using%20an%20image-to-video%20generation%20model.%20We%20propose%20a%20data%20generation%0Ascheme%20to%20cover%20multiple%20video%20tasks%20based%20on%20instance-level%20video%20segmentation%0Adatasets.%20Our%20model%20is%20trained%20by%20incorporating%20a%20mask%20prediction%20decoder%20head%0Aand%20optimizing%20a%20region-aware%20loss%20to%20aid%20the%20encoder%20to%20preserve%20the%20original%0Acontent%20while%20the%20generation%20model%20propagates%20the%20modified%20region.%20This%20novel%0Adesign%20opens%20up%20new%20possibilities%3A%20In%20editing%20scenarios%2C%20GenProp%20allows%0Asubstantial%20changes%20to%20an%20object%27s%20shape%3B%20for%20insertion%2C%20the%20inserted%20objects%0Acan%20exhibit%20independent%20motion%3B%20for%20removal%2C%20GenProp%20effectively%20removes%0Aeffects%20like%20shadows%20and%20reflections%20from%20the%20whole%20video%3B%20for%20tracking%2C%0AGenProp%20is%20capable%20of%20tracking%20objects%20and%20their%20associated%20effects%20together.%0AExperiment%20results%20demonstrate%20the%20leading%20performance%20of%20our%20model%20in%20various%0Avideo%20tasks%2C%20and%20we%20further%20provide%20in-depth%20analyses%20of%20the%20proposed%0Aframework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Video%2520Propagation%26entry.906535625%3DShaoteng%2520Liu%2520and%2520Tianyu%2520Wang%2520and%2520Jui-Hsien%2520Wang%2520and%2520Qing%2520Liu%2520and%2520Zhifei%2520Zhang%2520and%2520Joon-Young%2520Lee%2520and%2520Yijun%2520Li%2520and%2520Bei%2520Yu%2520and%2520Zhe%2520Lin%2520and%2520Soo%2520Ye%2520Kim%2520and%2520Jiaya%2520Jia%26entry.1292438233%3D%2520%2520Large-scale%2520video%2520generation%2520models%2520have%2520the%2520inherent%2520ability%2520to%250Arealistically%2520model%2520natural%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520through%250Aa%2520careful%2520design%2520of%2520a%2520generative%2520video%2520propagation%2520framework%252C%2520various%2520video%250Atasks%2520can%2520be%2520addressed%2520in%2520a%2520unified%2520way%2520by%2520leveraging%2520the%2520generative%2520power%2520of%250Asuch%2520models.%2520Specifically%252C%2520our%2520framework%252C%2520GenProp%252C%2520encodes%2520the%2520original%2520video%250Awith%2520a%2520selective%2520content%2520encoder%2520and%2520propagates%2520the%2520changes%2520made%2520to%2520the%2520first%250Aframe%2520using%2520an%2520image-to-video%2520generation%2520model.%2520We%2520propose%2520a%2520data%2520generation%250Ascheme%2520to%2520cover%2520multiple%2520video%2520tasks%2520based%2520on%2520instance-level%2520video%2520segmentation%250Adatasets.%2520Our%2520model%2520is%2520trained%2520by%2520incorporating%2520a%2520mask%2520prediction%2520decoder%2520head%250Aand%2520optimizing%2520a%2520region-aware%2520loss%2520to%2520aid%2520the%2520encoder%2520to%2520preserve%2520the%2520original%250Acontent%2520while%2520the%2520generation%2520model%2520propagates%2520the%2520modified%2520region.%2520This%2520novel%250Adesign%2520opens%2520up%2520new%2520possibilities%253A%2520In%2520editing%2520scenarios%252C%2520GenProp%2520allows%250Asubstantial%2520changes%2520to%2520an%2520object%2527s%2520shape%253B%2520for%2520insertion%252C%2520the%2520inserted%2520objects%250Acan%2520exhibit%2520independent%2520motion%253B%2520for%2520removal%252C%2520GenProp%2520effectively%2520removes%250Aeffects%2520like%2520shadows%2520and%2520reflections%2520from%2520the%2520whole%2520video%253B%2520for%2520tracking%252C%250AGenProp%2520is%2520capable%2520of%2520tracking%2520objects%2520and%2520their%2520associated%2520effects%2520together.%250AExperiment%2520results%2520demonstrate%2520the%2520leading%2520performance%2520of%2520our%2520model%2520in%2520various%250Avideo%2520tasks%252C%2520and%2520we%2520further%2520provide%2520in-depth%2520analyses%2520of%2520the%2520proposed%250Aframework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Video%20Propagation&entry.906535625=Shaoteng%20Liu%20and%20Tianyu%20Wang%20and%20Jui-Hsien%20Wang%20and%20Qing%20Liu%20and%20Zhifei%20Zhang%20and%20Joon-Young%20Lee%20and%20Yijun%20Li%20and%20Bei%20Yu%20and%20Zhe%20Lin%20and%20Soo%20Ye%20Kim%20and%20Jiaya%20Jia&entry.1292438233=%20%20Large-scale%20video%20generation%20models%20have%20the%20inherent%20ability%20to%0Arealistically%20model%20natural%20scenes.%20In%20this%20paper%2C%20we%20demonstrate%20that%20through%0Aa%20careful%20design%20of%20a%20generative%20video%20propagation%20framework%2C%20various%20video%0Atasks%20can%20be%20addressed%20in%20a%20unified%20way%20by%20leveraging%20the%20generative%20power%20of%0Asuch%20models.%20Specifically%2C%20our%20framework%2C%20GenProp%2C%20encodes%20the%20original%20video%0Awith%20a%20selective%20content%20encoder%20and%20propagates%20the%20changes%20made%20to%20the%20first%0Aframe%20using%20an%20image-to-video%20generation%20model.%20We%20propose%20a%20data%20generation%0Ascheme%20to%20cover%20multiple%20video%20tasks%20based%20on%20instance-level%20video%20segmentation%0Adatasets.%20Our%20model%20is%20trained%20by%20incorporating%20a%20mask%20prediction%20decoder%20head%0Aand%20optimizing%20a%20region-aware%20loss%20to%20aid%20the%20encoder%20to%20preserve%20the%20original%0Acontent%20while%20the%20generation%20model%20propagates%20the%20modified%20region.%20This%20novel%0Adesign%20opens%20up%20new%20possibilities%3A%20In%20editing%20scenarios%2C%20GenProp%20allows%0Asubstantial%20changes%20to%20an%20object%27s%20shape%3B%20for%20insertion%2C%20the%20inserted%20objects%0Acan%20exhibit%20independent%20motion%3B%20for%20removal%2C%20GenProp%20effectively%20removes%0Aeffects%20like%20shadows%20and%20reflections%20from%20the%20whole%20video%3B%20for%20tracking%2C%0AGenProp%20is%20capable%20of%20tracking%20objects%20and%20their%20associated%20effects%20together.%0AExperiment%20results%20demonstrate%20the%20leading%20performance%20of%20our%20model%20in%20various%0Avideo%20tasks%2C%20and%20we%20further%20provide%20in-depth%20analyses%20of%20the%20proposed%0Aframework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19761v1&entry.124074799=Read"},
{"title": "ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient\n  Liver Segmentation", "author": "Bhavesh Gyanchandani and Aditya Oza and Abhinav Roy", "abstract": "  The growing need for accurate and efficient 3D identification of tumors,\nparticularly in liver segmentation, has spurred considerable research into deep\nlearning models. While many existing architectures offer strong performance,\nthey often face challenges such as overfitting and excessive computational\ncosts. An adjustable and flexible architecture that strikes a balance between\ntime efficiency and model complexity remains an unmet requirement. In this\npaper, we introduce proKAN, a progressive stacking methodology for\nKolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike\ntraditional architectures, proKAN dynamically adjusts its complexity by\nprogressively adding KAN blocks during training, based on overfitting behavior.\nThis approach allows the network to stop growing when overfitting is detected,\npreventing unnecessary computational overhead while maintaining high accuracy.\nAdditionally, proKAN utilizes KAN's learnable activation functions modeled\nthrough B-splines, which provide enhanced flexibility in learning complex\nrelationships in 3D medical data. Our proposed architecture achieves\nstate-of-the-art performance in liver segmentation tasks, outperforming\nstandard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The\ndynamic nature of proKAN ensures efficient training times and high accuracy\nwithout the risk of overfitting. Furthermore, proKAN provides better\ninterpretability by allowing insight into the decision-making process through\nits learnable coefficients. The experimental results demonstrate a significant\nimprovement in accuracy, Dice score, and time efficiency, making proKAN a\ncompelling solution for 3D medical image segmentation tasks.\n", "link": "http://arxiv.org/abs/2412.19713v1", "date": "2024-12-27", "relevancy": 2.5977, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5235}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProKAN%3A%20Progressive%20Stacking%20of%20Kolmogorov-Arnold%20Networks%20for%20Efficient%0A%20%20Liver%20Segmentation&body=Title%3A%20ProKAN%3A%20Progressive%20Stacking%20of%20Kolmogorov-Arnold%20Networks%20for%20Efficient%0A%20%20Liver%20Segmentation%0AAuthor%3A%20Bhavesh%20Gyanchandani%20and%20Aditya%20Oza%20and%20Abhinav%20Roy%0AAbstract%3A%20%20%20The%20growing%20need%20for%20accurate%20and%20efficient%203D%20identification%20of%20tumors%2C%0Aparticularly%20in%20liver%20segmentation%2C%20has%20spurred%20considerable%20research%20into%20deep%0Alearning%20models.%20While%20many%20existing%20architectures%20offer%20strong%20performance%2C%0Athey%20often%20face%20challenges%20such%20as%20overfitting%20and%20excessive%20computational%0Acosts.%20An%20adjustable%20and%20flexible%20architecture%20that%20strikes%20a%20balance%20between%0Atime%20efficiency%20and%20model%20complexity%20remains%20an%20unmet%20requirement.%20In%20this%0Apaper%2C%20we%20introduce%20proKAN%2C%20a%20progressive%20stacking%20methodology%20for%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20designed%20to%20address%20these%20challenges.%20Unlike%0Atraditional%20architectures%2C%20proKAN%20dynamically%20adjusts%20its%20complexity%20by%0Aprogressively%20adding%20KAN%20blocks%20during%20training%2C%20based%20on%20overfitting%20behavior.%0AThis%20approach%20allows%20the%20network%20to%20stop%20growing%20when%20overfitting%20is%20detected%2C%0Apreventing%20unnecessary%20computational%20overhead%20while%20maintaining%20high%20accuracy.%0AAdditionally%2C%20proKAN%20utilizes%20KAN%27s%20learnable%20activation%20functions%20modeled%0Athrough%20B-splines%2C%20which%20provide%20enhanced%20flexibility%20in%20learning%20complex%0Arelationships%20in%203D%20medical%20data.%20Our%20proposed%20architecture%20achieves%0Astate-of-the-art%20performance%20in%20liver%20segmentation%20tasks%2C%20outperforming%0Astandard%20Multi-Layer%20Perceptrons%20%28MLPs%29%20and%20fixed%20KAN%20architectures.%20The%0Adynamic%20nature%20of%20proKAN%20ensures%20efficient%20training%20times%20and%20high%20accuracy%0Awithout%20the%20risk%20of%20overfitting.%20Furthermore%2C%20proKAN%20provides%20better%0Ainterpretability%20by%20allowing%20insight%20into%20the%20decision-making%20process%20through%0Aits%20learnable%20coefficients.%20The%20experimental%20results%20demonstrate%20a%20significant%0Aimprovement%20in%20accuracy%2C%20Dice%20score%2C%20and%20time%20efficiency%2C%20making%20proKAN%20a%0Acompelling%20solution%20for%203D%20medical%20image%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProKAN%253A%2520Progressive%2520Stacking%2520of%2520Kolmogorov-Arnold%2520Networks%2520for%2520Efficient%250A%2520%2520Liver%2520Segmentation%26entry.906535625%3DBhavesh%2520Gyanchandani%2520and%2520Aditya%2520Oza%2520and%2520Abhinav%2520Roy%26entry.1292438233%3D%2520%2520The%2520growing%2520need%2520for%2520accurate%2520and%2520efficient%25203D%2520identification%2520of%2520tumors%252C%250Aparticularly%2520in%2520liver%2520segmentation%252C%2520has%2520spurred%2520considerable%2520research%2520into%2520deep%250Alearning%2520models.%2520While%2520many%2520existing%2520architectures%2520offer%2520strong%2520performance%252C%250Athey%2520often%2520face%2520challenges%2520such%2520as%2520overfitting%2520and%2520excessive%2520computational%250Acosts.%2520An%2520adjustable%2520and%2520flexible%2520architecture%2520that%2520strikes%2520a%2520balance%2520between%250Atime%2520efficiency%2520and%2520model%2520complexity%2520remains%2520an%2520unmet%2520requirement.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520proKAN%252C%2520a%2520progressive%2520stacking%2520methodology%2520for%250AKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520designed%2520to%2520address%2520these%2520challenges.%2520Unlike%250Atraditional%2520architectures%252C%2520proKAN%2520dynamically%2520adjusts%2520its%2520complexity%2520by%250Aprogressively%2520adding%2520KAN%2520blocks%2520during%2520training%252C%2520based%2520on%2520overfitting%2520behavior.%250AThis%2520approach%2520allows%2520the%2520network%2520to%2520stop%2520growing%2520when%2520overfitting%2520is%2520detected%252C%250Apreventing%2520unnecessary%2520computational%2520overhead%2520while%2520maintaining%2520high%2520accuracy.%250AAdditionally%252C%2520proKAN%2520utilizes%2520KAN%2527s%2520learnable%2520activation%2520functions%2520modeled%250Athrough%2520B-splines%252C%2520which%2520provide%2520enhanced%2520flexibility%2520in%2520learning%2520complex%250Arelationships%2520in%25203D%2520medical%2520data.%2520Our%2520proposed%2520architecture%2520achieves%250Astate-of-the-art%2520performance%2520in%2520liver%2520segmentation%2520tasks%252C%2520outperforming%250Astandard%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%2520and%2520fixed%2520KAN%2520architectures.%2520The%250Adynamic%2520nature%2520of%2520proKAN%2520ensures%2520efficient%2520training%2520times%2520and%2520high%2520accuracy%250Awithout%2520the%2520risk%2520of%2520overfitting.%2520Furthermore%252C%2520proKAN%2520provides%2520better%250Ainterpretability%2520by%2520allowing%2520insight%2520into%2520the%2520decision-making%2520process%2520through%250Aits%2520learnable%2520coefficients.%2520The%2520experimental%2520results%2520demonstrate%2520a%2520significant%250Aimprovement%2520in%2520accuracy%252C%2520Dice%2520score%252C%2520and%2520time%2520efficiency%252C%2520making%2520proKAN%2520a%250Acompelling%2520solution%2520for%25203D%2520medical%2520image%2520segmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProKAN%3A%20Progressive%20Stacking%20of%20Kolmogorov-Arnold%20Networks%20for%20Efficient%0A%20%20Liver%20Segmentation&entry.906535625=Bhavesh%20Gyanchandani%20and%20Aditya%20Oza%20and%20Abhinav%20Roy&entry.1292438233=%20%20The%20growing%20need%20for%20accurate%20and%20efficient%203D%20identification%20of%20tumors%2C%0Aparticularly%20in%20liver%20segmentation%2C%20has%20spurred%20considerable%20research%20into%20deep%0Alearning%20models.%20While%20many%20existing%20architectures%20offer%20strong%20performance%2C%0Athey%20often%20face%20challenges%20such%20as%20overfitting%20and%20excessive%20computational%0Acosts.%20An%20adjustable%20and%20flexible%20architecture%20that%20strikes%20a%20balance%20between%0Atime%20efficiency%20and%20model%20complexity%20remains%20an%20unmet%20requirement.%20In%20this%0Apaper%2C%20we%20introduce%20proKAN%2C%20a%20progressive%20stacking%20methodology%20for%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20designed%20to%20address%20these%20challenges.%20Unlike%0Atraditional%20architectures%2C%20proKAN%20dynamically%20adjusts%20its%20complexity%20by%0Aprogressively%20adding%20KAN%20blocks%20during%20training%2C%20based%20on%20overfitting%20behavior.%0AThis%20approach%20allows%20the%20network%20to%20stop%20growing%20when%20overfitting%20is%20detected%2C%0Apreventing%20unnecessary%20computational%20overhead%20while%20maintaining%20high%20accuracy.%0AAdditionally%2C%20proKAN%20utilizes%20KAN%27s%20learnable%20activation%20functions%20modeled%0Athrough%20B-splines%2C%20which%20provide%20enhanced%20flexibility%20in%20learning%20complex%0Arelationships%20in%203D%20medical%20data.%20Our%20proposed%20architecture%20achieves%0Astate-of-the-art%20performance%20in%20liver%20segmentation%20tasks%2C%20outperforming%0Astandard%20Multi-Layer%20Perceptrons%20%28MLPs%29%20and%20fixed%20KAN%20architectures.%20The%0Adynamic%20nature%20of%20proKAN%20ensures%20efficient%20training%20times%20and%20high%20accuracy%0Awithout%20the%20risk%20of%20overfitting.%20Furthermore%2C%20proKAN%20provides%20better%0Ainterpretability%20by%20allowing%20insight%20into%20the%20decision-making%20process%20through%0Aits%20learnable%20coefficients.%20The%20experimental%20results%20demonstrate%20a%20significant%0Aimprovement%20in%20accuracy%2C%20Dice%20score%2C%20and%20time%20efficiency%2C%20making%20proKAN%20a%0Acompelling%20solution%20for%203D%20medical%20image%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19713v1&entry.124074799=Read"},
{"title": "Enhancing Cognitive Diagnosis by Modeling Learner Cognitive Structure\n  State", "author": "Zhifu Chen and Hengnian Gu and Jin Peng Zhou and Dongdai Zhou", "abstract": "  Cognitive diagnosis represents a fundamental research area within intelligent\neducation, with the objective of measuring the cognitive status of individuals.\nTheoretically, an individual's cognitive state is essentially equivalent to\ntheir cognitive structure state. Cognitive structure state comprises two key\ncomponents: knowledge state (KS) and knowledge structure state (KUS). The\nknowledge state reflects the learner's mastery of individual concepts, a widely\nstudied focus within cognitive diagnosis. In contrast, the knowledge structure\nstate-representing the learner's understanding of the relationships between\nconcepts-remains inadequately modeled. A learner's cognitive structure is\nessential for promoting meaningful learning and shaping academic performance.\nAlthough various methods have been proposed, most focus on assessing KS and\nfail to assess KUS. To bridge this gap, we propose an innovative and effective\nframework-CSCD (Cognitive Structure State-based Cognitive Diagnosis)-which\nintroduces a novel framework to modeling learners' cognitive structures in\ndiagnostic assessments, thereby offering new insights into cognitive structure\nmodeling. Specifically, we employ an edge-feature-based graph attention network\nto represent the learner's cognitive structure state, effectively integrating\nKS and KUS. Extensive experiments conducted on real datasets demonstrate the\nsuperior performance of this framework in terms of diagnostic accuracy and\ninterpretability.\n", "link": "http://arxiv.org/abs/2412.19759v1", "date": "2024-12-27", "relevancy": 2.576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Cognitive%20Diagnosis%20by%20Modeling%20Learner%20Cognitive%20Structure%0A%20%20State&body=Title%3A%20Enhancing%20Cognitive%20Diagnosis%20by%20Modeling%20Learner%20Cognitive%20Structure%0A%20%20State%0AAuthor%3A%20Zhifu%20Chen%20and%20Hengnian%20Gu%20and%20Jin%20Peng%20Zhou%20and%20Dongdai%20Zhou%0AAbstract%3A%20%20%20Cognitive%20diagnosis%20represents%20a%20fundamental%20research%20area%20within%20intelligent%0Aeducation%2C%20with%20the%20objective%20of%20measuring%20the%20cognitive%20status%20of%20individuals.%0ATheoretically%2C%20an%20individual%27s%20cognitive%20state%20is%20essentially%20equivalent%20to%0Atheir%20cognitive%20structure%20state.%20Cognitive%20structure%20state%20comprises%20two%20key%0Acomponents%3A%20knowledge%20state%20%28KS%29%20and%20knowledge%20structure%20state%20%28KUS%29.%20The%0Aknowledge%20state%20reflects%20the%20learner%27s%20mastery%20of%20individual%20concepts%2C%20a%20widely%0Astudied%20focus%20within%20cognitive%20diagnosis.%20In%20contrast%2C%20the%20knowledge%20structure%0Astate-representing%20the%20learner%27s%20understanding%20of%20the%20relationships%20between%0Aconcepts-remains%20inadequately%20modeled.%20A%20learner%27s%20cognitive%20structure%20is%0Aessential%20for%20promoting%20meaningful%20learning%20and%20shaping%20academic%20performance.%0AAlthough%20various%20methods%20have%20been%20proposed%2C%20most%20focus%20on%20assessing%20KS%20and%0Afail%20to%20assess%20KUS.%20To%20bridge%20this%20gap%2C%20we%20propose%20an%20innovative%20and%20effective%0Aframework-CSCD%20%28Cognitive%20Structure%20State-based%20Cognitive%20Diagnosis%29-which%0Aintroduces%20a%20novel%20framework%20to%20modeling%20learners%27%20cognitive%20structures%20in%0Adiagnostic%20assessments%2C%20thereby%20offering%20new%20insights%20into%20cognitive%20structure%0Amodeling.%20Specifically%2C%20we%20employ%20an%20edge-feature-based%20graph%20attention%20network%0Ato%20represent%20the%20learner%27s%20cognitive%20structure%20state%2C%20effectively%20integrating%0AKS%20and%20KUS.%20Extensive%20experiments%20conducted%20on%20real%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20this%20framework%20in%20terms%20of%20diagnostic%20accuracy%20and%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Cognitive%2520Diagnosis%2520by%2520Modeling%2520Learner%2520Cognitive%2520Structure%250A%2520%2520State%26entry.906535625%3DZhifu%2520Chen%2520and%2520Hengnian%2520Gu%2520and%2520Jin%2520Peng%2520Zhou%2520and%2520Dongdai%2520Zhou%26entry.1292438233%3D%2520%2520Cognitive%2520diagnosis%2520represents%2520a%2520fundamental%2520research%2520area%2520within%2520intelligent%250Aeducation%252C%2520with%2520the%2520objective%2520of%2520measuring%2520the%2520cognitive%2520status%2520of%2520individuals.%250ATheoretically%252C%2520an%2520individual%2527s%2520cognitive%2520state%2520is%2520essentially%2520equivalent%2520to%250Atheir%2520cognitive%2520structure%2520state.%2520Cognitive%2520structure%2520state%2520comprises%2520two%2520key%250Acomponents%253A%2520knowledge%2520state%2520%2528KS%2529%2520and%2520knowledge%2520structure%2520state%2520%2528KUS%2529.%2520The%250Aknowledge%2520state%2520reflects%2520the%2520learner%2527s%2520mastery%2520of%2520individual%2520concepts%252C%2520a%2520widely%250Astudied%2520focus%2520within%2520cognitive%2520diagnosis.%2520In%2520contrast%252C%2520the%2520knowledge%2520structure%250Astate-representing%2520the%2520learner%2527s%2520understanding%2520of%2520the%2520relationships%2520between%250Aconcepts-remains%2520inadequately%2520modeled.%2520A%2520learner%2527s%2520cognitive%2520structure%2520is%250Aessential%2520for%2520promoting%2520meaningful%2520learning%2520and%2520shaping%2520academic%2520performance.%250AAlthough%2520various%2520methods%2520have%2520been%2520proposed%252C%2520most%2520focus%2520on%2520assessing%2520KS%2520and%250Afail%2520to%2520assess%2520KUS.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520an%2520innovative%2520and%2520effective%250Aframework-CSCD%2520%2528Cognitive%2520Structure%2520State-based%2520Cognitive%2520Diagnosis%2529-which%250Aintroduces%2520a%2520novel%2520framework%2520to%2520modeling%2520learners%2527%2520cognitive%2520structures%2520in%250Adiagnostic%2520assessments%252C%2520thereby%2520offering%2520new%2520insights%2520into%2520cognitive%2520structure%250Amodeling.%2520Specifically%252C%2520we%2520employ%2520an%2520edge-feature-based%2520graph%2520attention%2520network%250Ato%2520represent%2520the%2520learner%2527s%2520cognitive%2520structure%2520state%252C%2520effectively%2520integrating%250AKS%2520and%2520KUS.%2520Extensive%2520experiments%2520conducted%2520on%2520real%2520datasets%2520demonstrate%2520the%250Asuperior%2520performance%2520of%2520this%2520framework%2520in%2520terms%2520of%2520diagnostic%2520accuracy%2520and%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Cognitive%20Diagnosis%20by%20Modeling%20Learner%20Cognitive%20Structure%0A%20%20State&entry.906535625=Zhifu%20Chen%20and%20Hengnian%20Gu%20and%20Jin%20Peng%20Zhou%20and%20Dongdai%20Zhou&entry.1292438233=%20%20Cognitive%20diagnosis%20represents%20a%20fundamental%20research%20area%20within%20intelligent%0Aeducation%2C%20with%20the%20objective%20of%20measuring%20the%20cognitive%20status%20of%20individuals.%0ATheoretically%2C%20an%20individual%27s%20cognitive%20state%20is%20essentially%20equivalent%20to%0Atheir%20cognitive%20structure%20state.%20Cognitive%20structure%20state%20comprises%20two%20key%0Acomponents%3A%20knowledge%20state%20%28KS%29%20and%20knowledge%20structure%20state%20%28KUS%29.%20The%0Aknowledge%20state%20reflects%20the%20learner%27s%20mastery%20of%20individual%20concepts%2C%20a%20widely%0Astudied%20focus%20within%20cognitive%20diagnosis.%20In%20contrast%2C%20the%20knowledge%20structure%0Astate-representing%20the%20learner%27s%20understanding%20of%20the%20relationships%20between%0Aconcepts-remains%20inadequately%20modeled.%20A%20learner%27s%20cognitive%20structure%20is%0Aessential%20for%20promoting%20meaningful%20learning%20and%20shaping%20academic%20performance.%0AAlthough%20various%20methods%20have%20been%20proposed%2C%20most%20focus%20on%20assessing%20KS%20and%0Afail%20to%20assess%20KUS.%20To%20bridge%20this%20gap%2C%20we%20propose%20an%20innovative%20and%20effective%0Aframework-CSCD%20%28Cognitive%20Structure%20State-based%20Cognitive%20Diagnosis%29-which%0Aintroduces%20a%20novel%20framework%20to%20modeling%20learners%27%20cognitive%20structures%20in%0Adiagnostic%20assessments%2C%20thereby%20offering%20new%20insights%20into%20cognitive%20structure%0Amodeling.%20Specifically%2C%20we%20employ%20an%20edge-feature-based%20graph%20attention%20network%0Ato%20represent%20the%20learner%27s%20cognitive%20structure%20state%2C%20effectively%20integrating%0AKS%20and%20KUS.%20Extensive%20experiments%20conducted%20on%20real%20datasets%20demonstrate%20the%0Asuperior%20performance%20of%20this%20framework%20in%20terms%20of%20diagnostic%20accuracy%20and%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19759v1&entry.124074799=Read"},
{"title": "Improved image display by identifying the RGB family color space", "author": "Elvis Togban and Djemel Ziou", "abstract": "  To display an image, the color space in which the image is encoded is assumed\nto be known. Unfortunately, this assumption is rarely realistic. In this paper,\nwe propose to identify the color space of a given color image using pixel\nembedding and the Gaussian process. Five color spaces are supported, namely\nAdobe RGB, Apple RGB, ColorMatch RGB, ProPhoto RGB and sRGB. The results\nobtained show that this problem deserves more efforts.\n", "link": "http://arxiv.org/abs/2412.19775v1", "date": "2024-12-27", "relevancy": 2.5175, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5422}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5113}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20image%20display%20by%20identifying%20the%20RGB%20family%20color%20space&body=Title%3A%20Improved%20image%20display%20by%20identifying%20the%20RGB%20family%20color%20space%0AAuthor%3A%20Elvis%20Togban%20and%20Djemel%20Ziou%0AAbstract%3A%20%20%20To%20display%20an%20image%2C%20the%20color%20space%20in%20which%20the%20image%20is%20encoded%20is%20assumed%0Ato%20be%20known.%20Unfortunately%2C%20this%20assumption%20is%20rarely%20realistic.%20In%20this%20paper%2C%0Awe%20propose%20to%20identify%20the%20color%20space%20of%20a%20given%20color%20image%20using%20pixel%0Aembedding%20and%20the%20Gaussian%20process.%20Five%20color%20spaces%20are%20supported%2C%20namely%0AAdobe%20RGB%2C%20Apple%20RGB%2C%20ColorMatch%20RGB%2C%20ProPhoto%20RGB%20and%20sRGB.%20The%20results%0Aobtained%20show%20that%20this%20problem%20deserves%20more%20efforts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520image%2520display%2520by%2520identifying%2520the%2520RGB%2520family%2520color%2520space%26entry.906535625%3DElvis%2520Togban%2520and%2520Djemel%2520Ziou%26entry.1292438233%3D%2520%2520To%2520display%2520an%2520image%252C%2520the%2520color%2520space%2520in%2520which%2520the%2520image%2520is%2520encoded%2520is%2520assumed%250Ato%2520be%2520known.%2520Unfortunately%252C%2520this%2520assumption%2520is%2520rarely%2520realistic.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520to%2520identify%2520the%2520color%2520space%2520of%2520a%2520given%2520color%2520image%2520using%2520pixel%250Aembedding%2520and%2520the%2520Gaussian%2520process.%2520Five%2520color%2520spaces%2520are%2520supported%252C%2520namely%250AAdobe%2520RGB%252C%2520Apple%2520RGB%252C%2520ColorMatch%2520RGB%252C%2520ProPhoto%2520RGB%2520and%2520sRGB.%2520The%2520results%250Aobtained%2520show%2520that%2520this%2520problem%2520deserves%2520more%2520efforts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20image%20display%20by%20identifying%20the%20RGB%20family%20color%20space&entry.906535625=Elvis%20Togban%20and%20Djemel%20Ziou&entry.1292438233=%20%20To%20display%20an%20image%2C%20the%20color%20space%20in%20which%20the%20image%20is%20encoded%20is%20assumed%0Ato%20be%20known.%20Unfortunately%2C%20this%20assumption%20is%20rarely%20realistic.%20In%20this%20paper%2C%0Awe%20propose%20to%20identify%20the%20color%20space%20of%20a%20given%20color%20image%20using%20pixel%0Aembedding%20and%20the%20Gaussian%20process.%20Five%20color%20spaces%20are%20supported%2C%20namely%0AAdobe%20RGB%2C%20Apple%20RGB%2C%20ColorMatch%20RGB%2C%20ProPhoto%20RGB%20and%20sRGB.%20The%20results%0Aobtained%20show%20that%20this%20problem%20deserves%20more%20efforts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19775v1&entry.124074799=Read"},
{"title": "Sustainable Diffusion-based Incentive Mechanism for Generative AI-driven\n  Digital Twins in Industrial Cyber-Physical Systems", "author": "Jinbo Wen and Jiawen Kang and Dusit Niyato and Yang Zhang and Shiwen Mao", "abstract": "  Industrial Cyber-Physical Systems (ICPSs) are an integral component of modern\nmanufacturing and industries. By digitizing data throughout product life\ncycles, Digital Twins (DTs) in ICPSs enable a shift from current industrial\ninfrastructures to intelligent and adaptive infrastructures. Thanks to data\nprocess capability, Generative Artificial Intelligence (GenAI) can drive the\nconstruction and update of DTs to improve predictive accuracy and prepare for\ndiverse smart manufacturing. However, mechanisms that leverage Industrial\nInternet of Things (IIoT) devices to share sensing data for DT construction are\nsusceptible to adverse selection problems. In this paper, we first develop a\nGenAI-driven DT architecture in ICPSs. To address the adverse selection problem\ncaused by information asymmetry, we propose a contract theory model and develop\na sustainable diffusion-based soft actor-critic algorithm to identify the\noptimal feasible contract. Specifically, we leverage dynamic structured pruning\ntechniques to reduce parameter numbers of actor networks, allowing\nsustainability and efficient implementation of the proposed algorithm.\nNumerical results demonstrate the effectiveness of the proposed scheme and the\nalgorithm, enabling efficient DT construction and updates to monitor and manage\nICPSs.\n", "link": "http://arxiv.org/abs/2408.01173v2", "date": "2024-12-27", "relevancy": 2.5101, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5067}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5003}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sustainable%20Diffusion-based%20Incentive%20Mechanism%20for%20Generative%20AI-driven%0A%20%20Digital%20Twins%20in%20Industrial%20Cyber-Physical%20Systems&body=Title%3A%20Sustainable%20Diffusion-based%20Incentive%20Mechanism%20for%20Generative%20AI-driven%0A%20%20Digital%20Twins%20in%20Industrial%20Cyber-Physical%20Systems%0AAuthor%3A%20Jinbo%20Wen%20and%20Jiawen%20Kang%20and%20Dusit%20Niyato%20and%20Yang%20Zhang%20and%20Shiwen%20Mao%0AAbstract%3A%20%20%20Industrial%20Cyber-Physical%20Systems%20%28ICPSs%29%20are%20an%20integral%20component%20of%20modern%0Amanufacturing%20and%20industries.%20By%20digitizing%20data%20throughout%20product%20life%0Acycles%2C%20Digital%20Twins%20%28DTs%29%20in%20ICPSs%20enable%20a%20shift%20from%20current%20industrial%0Ainfrastructures%20to%20intelligent%20and%20adaptive%20infrastructures.%20Thanks%20to%20data%0Aprocess%20capability%2C%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20can%20drive%20the%0Aconstruction%20and%20update%20of%20DTs%20to%20improve%20predictive%20accuracy%20and%20prepare%20for%0Adiverse%20smart%20manufacturing.%20However%2C%20mechanisms%20that%20leverage%20Industrial%0AInternet%20of%20Things%20%28IIoT%29%20devices%20to%20share%20sensing%20data%20for%20DT%20construction%20are%0Asusceptible%20to%20adverse%20selection%20problems.%20In%20this%20paper%2C%20we%20first%20develop%20a%0AGenAI-driven%20DT%20architecture%20in%20ICPSs.%20To%20address%20the%20adverse%20selection%20problem%0Acaused%20by%20information%20asymmetry%2C%20we%20propose%20a%20contract%20theory%20model%20and%20develop%0Aa%20sustainable%20diffusion-based%20soft%20actor-critic%20algorithm%20to%20identify%20the%0Aoptimal%20feasible%20contract.%20Specifically%2C%20we%20leverage%20dynamic%20structured%20pruning%0Atechniques%20to%20reduce%20parameter%20numbers%20of%20actor%20networks%2C%20allowing%0Asustainability%20and%20efficient%20implementation%20of%20the%20proposed%20algorithm.%0ANumerical%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20scheme%20and%20the%0Aalgorithm%2C%20enabling%20efficient%20DT%20construction%20and%20updates%20to%20monitor%20and%20manage%0AICPSs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01173v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSustainable%2520Diffusion-based%2520Incentive%2520Mechanism%2520for%2520Generative%2520AI-driven%250A%2520%2520Digital%2520Twins%2520in%2520Industrial%2520Cyber-Physical%2520Systems%26entry.906535625%3DJinbo%2520Wen%2520and%2520Jiawen%2520Kang%2520and%2520Dusit%2520Niyato%2520and%2520Yang%2520Zhang%2520and%2520Shiwen%2520Mao%26entry.1292438233%3D%2520%2520Industrial%2520Cyber-Physical%2520Systems%2520%2528ICPSs%2529%2520are%2520an%2520integral%2520component%2520of%2520modern%250Amanufacturing%2520and%2520industries.%2520By%2520digitizing%2520data%2520throughout%2520product%2520life%250Acycles%252C%2520Digital%2520Twins%2520%2528DTs%2529%2520in%2520ICPSs%2520enable%2520a%2520shift%2520from%2520current%2520industrial%250Ainfrastructures%2520to%2520intelligent%2520and%2520adaptive%2520infrastructures.%2520Thanks%2520to%2520data%250Aprocess%2520capability%252C%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%2529%2520can%2520drive%2520the%250Aconstruction%2520and%2520update%2520of%2520DTs%2520to%2520improve%2520predictive%2520accuracy%2520and%2520prepare%2520for%250Adiverse%2520smart%2520manufacturing.%2520However%252C%2520mechanisms%2520that%2520leverage%2520Industrial%250AInternet%2520of%2520Things%2520%2528IIoT%2529%2520devices%2520to%2520share%2520sensing%2520data%2520for%2520DT%2520construction%2520are%250Asusceptible%2520to%2520adverse%2520selection%2520problems.%2520In%2520this%2520paper%252C%2520we%2520first%2520develop%2520a%250AGenAI-driven%2520DT%2520architecture%2520in%2520ICPSs.%2520To%2520address%2520the%2520adverse%2520selection%2520problem%250Acaused%2520by%2520information%2520asymmetry%252C%2520we%2520propose%2520a%2520contract%2520theory%2520model%2520and%2520develop%250Aa%2520sustainable%2520diffusion-based%2520soft%2520actor-critic%2520algorithm%2520to%2520identify%2520the%250Aoptimal%2520feasible%2520contract.%2520Specifically%252C%2520we%2520leverage%2520dynamic%2520structured%2520pruning%250Atechniques%2520to%2520reduce%2520parameter%2520numbers%2520of%2520actor%2520networks%252C%2520allowing%250Asustainability%2520and%2520efficient%2520implementation%2520of%2520the%2520proposed%2520algorithm.%250ANumerical%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520scheme%2520and%2520the%250Aalgorithm%252C%2520enabling%2520efficient%2520DT%2520construction%2520and%2520updates%2520to%2520monitor%2520and%2520manage%250AICPSs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01173v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sustainable%20Diffusion-based%20Incentive%20Mechanism%20for%20Generative%20AI-driven%0A%20%20Digital%20Twins%20in%20Industrial%20Cyber-Physical%20Systems&entry.906535625=Jinbo%20Wen%20and%20Jiawen%20Kang%20and%20Dusit%20Niyato%20and%20Yang%20Zhang%20and%20Shiwen%20Mao&entry.1292438233=%20%20Industrial%20Cyber-Physical%20Systems%20%28ICPSs%29%20are%20an%20integral%20component%20of%20modern%0Amanufacturing%20and%20industries.%20By%20digitizing%20data%20throughout%20product%20life%0Acycles%2C%20Digital%20Twins%20%28DTs%29%20in%20ICPSs%20enable%20a%20shift%20from%20current%20industrial%0Ainfrastructures%20to%20intelligent%20and%20adaptive%20infrastructures.%20Thanks%20to%20data%0Aprocess%20capability%2C%20Generative%20Artificial%20Intelligence%20%28GenAI%29%20can%20drive%20the%0Aconstruction%20and%20update%20of%20DTs%20to%20improve%20predictive%20accuracy%20and%20prepare%20for%0Adiverse%20smart%20manufacturing.%20However%2C%20mechanisms%20that%20leverage%20Industrial%0AInternet%20of%20Things%20%28IIoT%29%20devices%20to%20share%20sensing%20data%20for%20DT%20construction%20are%0Asusceptible%20to%20adverse%20selection%20problems.%20In%20this%20paper%2C%20we%20first%20develop%20a%0AGenAI-driven%20DT%20architecture%20in%20ICPSs.%20To%20address%20the%20adverse%20selection%20problem%0Acaused%20by%20information%20asymmetry%2C%20we%20propose%20a%20contract%20theory%20model%20and%20develop%0Aa%20sustainable%20diffusion-based%20soft%20actor-critic%20algorithm%20to%20identify%20the%0Aoptimal%20feasible%20contract.%20Specifically%2C%20we%20leverage%20dynamic%20structured%20pruning%0Atechniques%20to%20reduce%20parameter%20numbers%20of%20actor%20networks%2C%20allowing%0Asustainability%20and%20efficient%20implementation%20of%20the%20proposed%20algorithm.%0ANumerical%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20scheme%20and%20the%0Aalgorithm%2C%20enabling%20efficient%20DT%20construction%20and%20updates%20to%20monitor%20and%20manage%0AICPSs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01173v2&entry.124074799=Read"},
{"title": "ViDTA: Enhanced Drug-Target Affinity Prediction via Virtual Graph Nodes\n  and Attention-based Feature Fusion", "author": "Minghui Li and Zikang Guo and Yang Wu and Peijin Guo and Yao Shi and Shengshan Hu and Wei Wan and Shengqing Hu", "abstract": "  Drug-target interaction is fundamental in understanding how drugs affect\nbiological systems, and accurately predicting drug-target affinity (DTA) is\nvital for drug discovery. Recently, deep learning methods have emerged as a\nsignificant approach for estimating the binding strength between drugs and\ntarget proteins. However, existing methods simply utilize the drug's local\ninformation from molecular topology rather than global information.\nAdditionally, the features of drugs and proteins are usually fused with a\nsimple concatenation operation, limiting their effectiveness. To address these\nchallenges, we proposed ViDTA, an enhanced DTA prediction framework. We\nintroduce virtual nodes into the Graph Neural Network (GNN)-based drug feature\nextraction network, which acts as a global memory to exchange messages more\nefficiently. By incorporating virtual graph nodes, we seamlessly integrate\nlocal and global features of drug molecular structures, expanding the GNN's\nreceptive field. Additionally, we propose an attention-based linear feature\nfusion network for better capturing the interaction information between drugs\nand proteins. Experimental results evaluated on various benchmarks including\nDavis, Metz, and KIBA demonstrate that our proposed ViDTA outperforms the\nstate-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2412.19589v1", "date": "2024-12-27", "relevancy": 2.4906, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5136}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5037}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViDTA%3A%20Enhanced%20Drug-Target%20Affinity%20Prediction%20via%20Virtual%20Graph%20Nodes%0A%20%20and%20Attention-based%20Feature%20Fusion&body=Title%3A%20ViDTA%3A%20Enhanced%20Drug-Target%20Affinity%20Prediction%20via%20Virtual%20Graph%20Nodes%0A%20%20and%20Attention-based%20Feature%20Fusion%0AAuthor%3A%20Minghui%20Li%20and%20Zikang%20Guo%20and%20Yang%20Wu%20and%20Peijin%20Guo%20and%20Yao%20Shi%20and%20Shengshan%20Hu%20and%20Wei%20Wan%20and%20Shengqing%20Hu%0AAbstract%3A%20%20%20Drug-target%20interaction%20is%20fundamental%20in%20understanding%20how%20drugs%20affect%0Abiological%20systems%2C%20and%20accurately%20predicting%20drug-target%20affinity%20%28DTA%29%20is%0Avital%20for%20drug%20discovery.%20Recently%2C%20deep%20learning%20methods%20have%20emerged%20as%20a%0Asignificant%20approach%20for%20estimating%20the%20binding%20strength%20between%20drugs%20and%0Atarget%20proteins.%20However%2C%20existing%20methods%20simply%20utilize%20the%20drug%27s%20local%0Ainformation%20from%20molecular%20topology%20rather%20than%20global%20information.%0AAdditionally%2C%20the%20features%20of%20drugs%20and%20proteins%20are%20usually%20fused%20with%20a%0Asimple%20concatenation%20operation%2C%20limiting%20their%20effectiveness.%20To%20address%20these%0Achallenges%2C%20we%20proposed%20ViDTA%2C%20an%20enhanced%20DTA%20prediction%20framework.%20We%0Aintroduce%20virtual%20nodes%20into%20the%20Graph%20Neural%20Network%20%28GNN%29-based%20drug%20feature%0Aextraction%20network%2C%20which%20acts%20as%20a%20global%20memory%20to%20exchange%20messages%20more%0Aefficiently.%20By%20incorporating%20virtual%20graph%20nodes%2C%20we%20seamlessly%20integrate%0Alocal%20and%20global%20features%20of%20drug%20molecular%20structures%2C%20expanding%20the%20GNN%27s%0Areceptive%20field.%20Additionally%2C%20we%20propose%20an%20attention-based%20linear%20feature%0Afusion%20network%20for%20better%20capturing%20the%20interaction%20information%20between%20drugs%0Aand%20proteins.%20Experimental%20results%20evaluated%20on%20various%20benchmarks%20including%0ADavis%2C%20Metz%2C%20and%20KIBA%20demonstrate%20that%20our%20proposed%20ViDTA%20outperforms%20the%0Astate-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViDTA%253A%2520Enhanced%2520Drug-Target%2520Affinity%2520Prediction%2520via%2520Virtual%2520Graph%2520Nodes%250A%2520%2520and%2520Attention-based%2520Feature%2520Fusion%26entry.906535625%3DMinghui%2520Li%2520and%2520Zikang%2520Guo%2520and%2520Yang%2520Wu%2520and%2520Peijin%2520Guo%2520and%2520Yao%2520Shi%2520and%2520Shengshan%2520Hu%2520and%2520Wei%2520Wan%2520and%2520Shengqing%2520Hu%26entry.1292438233%3D%2520%2520Drug-target%2520interaction%2520is%2520fundamental%2520in%2520understanding%2520how%2520drugs%2520affect%250Abiological%2520systems%252C%2520and%2520accurately%2520predicting%2520drug-target%2520affinity%2520%2528DTA%2529%2520is%250Avital%2520for%2520drug%2520discovery.%2520Recently%252C%2520deep%2520learning%2520methods%2520have%2520emerged%2520as%2520a%250Asignificant%2520approach%2520for%2520estimating%2520the%2520binding%2520strength%2520between%2520drugs%2520and%250Atarget%2520proteins.%2520However%252C%2520existing%2520methods%2520simply%2520utilize%2520the%2520drug%2527s%2520local%250Ainformation%2520from%2520molecular%2520topology%2520rather%2520than%2520global%2520information.%250AAdditionally%252C%2520the%2520features%2520of%2520drugs%2520and%2520proteins%2520are%2520usually%2520fused%2520with%2520a%250Asimple%2520concatenation%2520operation%252C%2520limiting%2520their%2520effectiveness.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520proposed%2520ViDTA%252C%2520an%2520enhanced%2520DTA%2520prediction%2520framework.%2520We%250Aintroduce%2520virtual%2520nodes%2520into%2520the%2520Graph%2520Neural%2520Network%2520%2528GNN%2529-based%2520drug%2520feature%250Aextraction%2520network%252C%2520which%2520acts%2520as%2520a%2520global%2520memory%2520to%2520exchange%2520messages%2520more%250Aefficiently.%2520By%2520incorporating%2520virtual%2520graph%2520nodes%252C%2520we%2520seamlessly%2520integrate%250Alocal%2520and%2520global%2520features%2520of%2520drug%2520molecular%2520structures%252C%2520expanding%2520the%2520GNN%2527s%250Areceptive%2520field.%2520Additionally%252C%2520we%2520propose%2520an%2520attention-based%2520linear%2520feature%250Afusion%2520network%2520for%2520better%2520capturing%2520the%2520interaction%2520information%2520between%2520drugs%250Aand%2520proteins.%2520Experimental%2520results%2520evaluated%2520on%2520various%2520benchmarks%2520including%250ADavis%252C%2520Metz%252C%2520and%2520KIBA%2520demonstrate%2520that%2520our%2520proposed%2520ViDTA%2520outperforms%2520the%250Astate-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViDTA%3A%20Enhanced%20Drug-Target%20Affinity%20Prediction%20via%20Virtual%20Graph%20Nodes%0A%20%20and%20Attention-based%20Feature%20Fusion&entry.906535625=Minghui%20Li%20and%20Zikang%20Guo%20and%20Yang%20Wu%20and%20Peijin%20Guo%20and%20Yao%20Shi%20and%20Shengshan%20Hu%20and%20Wei%20Wan%20and%20Shengqing%20Hu&entry.1292438233=%20%20Drug-target%20interaction%20is%20fundamental%20in%20understanding%20how%20drugs%20affect%0Abiological%20systems%2C%20and%20accurately%20predicting%20drug-target%20affinity%20%28DTA%29%20is%0Avital%20for%20drug%20discovery.%20Recently%2C%20deep%20learning%20methods%20have%20emerged%20as%20a%0Asignificant%20approach%20for%20estimating%20the%20binding%20strength%20between%20drugs%20and%0Atarget%20proteins.%20However%2C%20existing%20methods%20simply%20utilize%20the%20drug%27s%20local%0Ainformation%20from%20molecular%20topology%20rather%20than%20global%20information.%0AAdditionally%2C%20the%20features%20of%20drugs%20and%20proteins%20are%20usually%20fused%20with%20a%0Asimple%20concatenation%20operation%2C%20limiting%20their%20effectiveness.%20To%20address%20these%0Achallenges%2C%20we%20proposed%20ViDTA%2C%20an%20enhanced%20DTA%20prediction%20framework.%20We%0Aintroduce%20virtual%20nodes%20into%20the%20Graph%20Neural%20Network%20%28GNN%29-based%20drug%20feature%0Aextraction%20network%2C%20which%20acts%20as%20a%20global%20memory%20to%20exchange%20messages%20more%0Aefficiently.%20By%20incorporating%20virtual%20graph%20nodes%2C%20we%20seamlessly%20integrate%0Alocal%20and%20global%20features%20of%20drug%20molecular%20structures%2C%20expanding%20the%20GNN%27s%0Areceptive%20field.%20Additionally%2C%20we%20propose%20an%20attention-based%20linear%20feature%0Afusion%20network%20for%20better%20capturing%20the%20interaction%20information%20between%20drugs%0Aand%20proteins.%20Experimental%20results%20evaluated%20on%20various%20benchmarks%20including%0ADavis%2C%20Metz%2C%20and%20KIBA%20demonstrate%20that%20our%20proposed%20ViDTA%20outperforms%20the%0Astate-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19589v1&entry.124074799=Read"},
{"title": "Text2Insight: Transform natural language text into insights seamlessly\n  using multi-model architecture", "author": "Pradeep Sain", "abstract": "  The growing demand for dynamic, user-centric data analysis and visualization\nis evident across domains like healthcare, finance, and research. Traditional\nvisualization tools often fail to meet individual user needs due to their\nstatic and predefined nature. To address this gap, Text2Insight is introduced\nas an innovative solution that delivers customized data analysis and\nvisualizations based on user-defined natural language requirements. Leveraging\na multi-model architecture, Text2Insight transforms user inputs into actionable\ninsights and dynamic visualizations.\n  The methodology begins with analyzing the input dataset to extract structural\ndetails such as columns and values. A pre-trained Llama3 model converts the\nuser's natural language query into an SQL query, which is further refined using\na Named Entity Recognition (NER) model for accuracy. A chart predictor\ndetermines the most suitable visualization type, while the Llama3 model\ngenerates insights based on the SQL query's results. The output is a\nuser-friendly and visually informative chart. To enhance analysis capabilities,\nthe system integrates a question-answering model and a predictive model using\nthe BERT framework. These models provide insights into historical data and\npredict future trends.\n  Performance evaluation of Text2Insight demonstrates its effectiveness,\nachieving high accuracy (99%), precision (100%), recall (99%), and F1-score\n(99%), with a BLEU score of 0.5. The question-answering model attained an\naccuracy of 89% and the predictive model achieved 70% accuracy. These results\nvalidate Text2Insight as a robust and viable solution for transforming natural\nlanguage text into dynamic, user-specific data analysis and visualizations.\n", "link": "http://arxiv.org/abs/2412.19718v1", "date": "2024-12-27", "relevancy": 2.4798, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Insight%3A%20Transform%20natural%20language%20text%20into%20insights%20seamlessly%0A%20%20using%20multi-model%20architecture&body=Title%3A%20Text2Insight%3A%20Transform%20natural%20language%20text%20into%20insights%20seamlessly%0A%20%20using%20multi-model%20architecture%0AAuthor%3A%20Pradeep%20Sain%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20dynamic%2C%20user-centric%20data%20analysis%20and%20visualization%0Ais%20evident%20across%20domains%20like%20healthcare%2C%20finance%2C%20and%20research.%20Traditional%0Avisualization%20tools%20often%20fail%20to%20meet%20individual%20user%20needs%20due%20to%20their%0Astatic%20and%20predefined%20nature.%20To%20address%20this%20gap%2C%20Text2Insight%20is%20introduced%0Aas%20an%20innovative%20solution%20that%20delivers%20customized%20data%20analysis%20and%0Avisualizations%20based%20on%20user-defined%20natural%20language%20requirements.%20Leveraging%0Aa%20multi-model%20architecture%2C%20Text2Insight%20transforms%20user%20inputs%20into%20actionable%0Ainsights%20and%20dynamic%20visualizations.%0A%20%20The%20methodology%20begins%20with%20analyzing%20the%20input%20dataset%20to%20extract%20structural%0Adetails%20such%20as%20columns%20and%20values.%20A%20pre-trained%20Llama3%20model%20converts%20the%0Auser%27s%20natural%20language%20query%20into%20an%20SQL%20query%2C%20which%20is%20further%20refined%20using%0Aa%20Named%20Entity%20Recognition%20%28NER%29%20model%20for%20accuracy.%20A%20chart%20predictor%0Adetermines%20the%20most%20suitable%20visualization%20type%2C%20while%20the%20Llama3%20model%0Agenerates%20insights%20based%20on%20the%20SQL%20query%27s%20results.%20The%20output%20is%20a%0Auser-friendly%20and%20visually%20informative%20chart.%20To%20enhance%20analysis%20capabilities%2C%0Athe%20system%20integrates%20a%20question-answering%20model%20and%20a%20predictive%20model%20using%0Athe%20BERT%20framework.%20These%20models%20provide%20insights%20into%20historical%20data%20and%0Apredict%20future%20trends.%0A%20%20Performance%20evaluation%20of%20Text2Insight%20demonstrates%20its%20effectiveness%2C%0Aachieving%20high%20accuracy%20%2899%25%29%2C%20precision%20%28100%25%29%2C%20recall%20%2899%25%29%2C%20and%20F1-score%0A%2899%25%29%2C%20with%20a%20BLEU%20score%20of%200.5.%20The%20question-answering%20model%20attained%20an%0Aaccuracy%20of%2089%25%20and%20the%20predictive%20model%20achieved%2070%25%20accuracy.%20These%20results%0Avalidate%20Text2Insight%20as%20a%20robust%20and%20viable%20solution%20for%20transforming%20natural%0Alanguage%20text%20into%20dynamic%2C%20user-specific%20data%20analysis%20and%20visualizations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Insight%253A%2520Transform%2520natural%2520language%2520text%2520into%2520insights%2520seamlessly%250A%2520%2520using%2520multi-model%2520architecture%26entry.906535625%3DPradeep%2520Sain%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520dynamic%252C%2520user-centric%2520data%2520analysis%2520and%2520visualization%250Ais%2520evident%2520across%2520domains%2520like%2520healthcare%252C%2520finance%252C%2520and%2520research.%2520Traditional%250Avisualization%2520tools%2520often%2520fail%2520to%2520meet%2520individual%2520user%2520needs%2520due%2520to%2520their%250Astatic%2520and%2520predefined%2520nature.%2520To%2520address%2520this%2520gap%252C%2520Text2Insight%2520is%2520introduced%250Aas%2520an%2520innovative%2520solution%2520that%2520delivers%2520customized%2520data%2520analysis%2520and%250Avisualizations%2520based%2520on%2520user-defined%2520natural%2520language%2520requirements.%2520Leveraging%250Aa%2520multi-model%2520architecture%252C%2520Text2Insight%2520transforms%2520user%2520inputs%2520into%2520actionable%250Ainsights%2520and%2520dynamic%2520visualizations.%250A%2520%2520The%2520methodology%2520begins%2520with%2520analyzing%2520the%2520input%2520dataset%2520to%2520extract%2520structural%250Adetails%2520such%2520as%2520columns%2520and%2520values.%2520A%2520pre-trained%2520Llama3%2520model%2520converts%2520the%250Auser%2527s%2520natural%2520language%2520query%2520into%2520an%2520SQL%2520query%252C%2520which%2520is%2520further%2520refined%2520using%250Aa%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%2520model%2520for%2520accuracy.%2520A%2520chart%2520predictor%250Adetermines%2520the%2520most%2520suitable%2520visualization%2520type%252C%2520while%2520the%2520Llama3%2520model%250Agenerates%2520insights%2520based%2520on%2520the%2520SQL%2520query%2527s%2520results.%2520The%2520output%2520is%2520a%250Auser-friendly%2520and%2520visually%2520informative%2520chart.%2520To%2520enhance%2520analysis%2520capabilities%252C%250Athe%2520system%2520integrates%2520a%2520question-answering%2520model%2520and%2520a%2520predictive%2520model%2520using%250Athe%2520BERT%2520framework.%2520These%2520models%2520provide%2520insights%2520into%2520historical%2520data%2520and%250Apredict%2520future%2520trends.%250A%2520%2520Performance%2520evaluation%2520of%2520Text2Insight%2520demonstrates%2520its%2520effectiveness%252C%250Aachieving%2520high%2520accuracy%2520%252899%2525%2529%252C%2520precision%2520%2528100%2525%2529%252C%2520recall%2520%252899%2525%2529%252C%2520and%2520F1-score%250A%252899%2525%2529%252C%2520with%2520a%2520BLEU%2520score%2520of%25200.5.%2520The%2520question-answering%2520model%2520attained%2520an%250Aaccuracy%2520of%252089%2525%2520and%2520the%2520predictive%2520model%2520achieved%252070%2525%2520accuracy.%2520These%2520results%250Avalidate%2520Text2Insight%2520as%2520a%2520robust%2520and%2520viable%2520solution%2520for%2520transforming%2520natural%250Alanguage%2520text%2520into%2520dynamic%252C%2520user-specific%2520data%2520analysis%2520and%2520visualizations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Insight%3A%20Transform%20natural%20language%20text%20into%20insights%20seamlessly%0A%20%20using%20multi-model%20architecture&entry.906535625=Pradeep%20Sain&entry.1292438233=%20%20The%20growing%20demand%20for%20dynamic%2C%20user-centric%20data%20analysis%20and%20visualization%0Ais%20evident%20across%20domains%20like%20healthcare%2C%20finance%2C%20and%20research.%20Traditional%0Avisualization%20tools%20often%20fail%20to%20meet%20individual%20user%20needs%20due%20to%20their%0Astatic%20and%20predefined%20nature.%20To%20address%20this%20gap%2C%20Text2Insight%20is%20introduced%0Aas%20an%20innovative%20solution%20that%20delivers%20customized%20data%20analysis%20and%0Avisualizations%20based%20on%20user-defined%20natural%20language%20requirements.%20Leveraging%0Aa%20multi-model%20architecture%2C%20Text2Insight%20transforms%20user%20inputs%20into%20actionable%0Ainsights%20and%20dynamic%20visualizations.%0A%20%20The%20methodology%20begins%20with%20analyzing%20the%20input%20dataset%20to%20extract%20structural%0Adetails%20such%20as%20columns%20and%20values.%20A%20pre-trained%20Llama3%20model%20converts%20the%0Auser%27s%20natural%20language%20query%20into%20an%20SQL%20query%2C%20which%20is%20further%20refined%20using%0Aa%20Named%20Entity%20Recognition%20%28NER%29%20model%20for%20accuracy.%20A%20chart%20predictor%0Adetermines%20the%20most%20suitable%20visualization%20type%2C%20while%20the%20Llama3%20model%0Agenerates%20insights%20based%20on%20the%20SQL%20query%27s%20results.%20The%20output%20is%20a%0Auser-friendly%20and%20visually%20informative%20chart.%20To%20enhance%20analysis%20capabilities%2C%0Athe%20system%20integrates%20a%20question-answering%20model%20and%20a%20predictive%20model%20using%0Athe%20BERT%20framework.%20These%20models%20provide%20insights%20into%20historical%20data%20and%0Apredict%20future%20trends.%0A%20%20Performance%20evaluation%20of%20Text2Insight%20demonstrates%20its%20effectiveness%2C%0Aachieving%20high%20accuracy%20%2899%25%29%2C%20precision%20%28100%25%29%2C%20recall%20%2899%25%29%2C%20and%20F1-score%0A%2899%25%29%2C%20with%20a%20BLEU%20score%20of%200.5.%20The%20question-answering%20model%20attained%20an%0Aaccuracy%20of%2089%25%20and%20the%20predictive%20model%20achieved%2070%25%20accuracy.%20These%20results%0Avalidate%20Text2Insight%20as%20a%20robust%20and%20viable%20solution%20for%20transforming%20natural%0Alanguage%20text%20into%20dynamic%2C%20user-specific%20data%20analysis%20and%20visualizations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19718v1&entry.124074799=Read"},
{"title": "Are Sparse Neural Networks Better Hard Sample Learners?", "author": "Qiao Xiao and Boqian Wu and Lu Yin and Christopher Neil Gadzinski and Tianjin Huang and Mykola Pechenizkiy and Decebal Constantin Mocanu", "abstract": "  While deep learning has demonstrated impressive progress, it remains a\ndaunting challenge to learn from hard samples as these samples are usually\nnoisy and intricate. These hard samples play a crucial role in the optimal\nperformance of deep neural networks. Most research on Sparse Neural Networks\n(SNNs) has focused on standard training data, leaving gaps in understanding\ntheir effectiveness on complex and challenging data. This paper's extensive\ninvestigation across scenarios reveals that most SNNs trained on challenging\nsamples can often match or surpass dense models in accuracy at certain sparsity\nlevels, especially with limited data. We observe that layer-wise density ratios\ntend to play an important role in SNN performance, particularly for methods\nthat train from scratch without pre-trained initialization. These insights\nenhance our understanding of SNNs' behavior and potential for efficient\nlearning approaches in data-centric AI. Our code is publicly available at:\n\\url{https://github.com/QiaoXiao7282/hard_sample_learners}.\n", "link": "http://arxiv.org/abs/2409.09196v2", "date": "2024-12-27", "relevancy": 2.4546, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5355}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4715}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Sparse%20Neural%20Networks%20Better%20Hard%20Sample%20Learners%3F&body=Title%3A%20Are%20Sparse%20Neural%20Networks%20Better%20Hard%20Sample%20Learners%3F%0AAuthor%3A%20Qiao%20Xiao%20and%20Boqian%20Wu%20and%20Lu%20Yin%20and%20Christopher%20Neil%20Gadzinski%20and%20Tianjin%20Huang%20and%20Mykola%20Pechenizkiy%20and%20Decebal%20Constantin%20Mocanu%0AAbstract%3A%20%20%20While%20deep%20learning%20has%20demonstrated%20impressive%20progress%2C%20it%20remains%20a%0Adaunting%20challenge%20to%20learn%20from%20hard%20samples%20as%20these%20samples%20are%20usually%0Anoisy%20and%20intricate.%20These%20hard%20samples%20play%20a%20crucial%20role%20in%20the%20optimal%0Aperformance%20of%20deep%20neural%20networks.%20Most%20research%20on%20Sparse%20Neural%20Networks%0A%28SNNs%29%20has%20focused%20on%20standard%20training%20data%2C%20leaving%20gaps%20in%20understanding%0Atheir%20effectiveness%20on%20complex%20and%20challenging%20data.%20This%20paper%27s%20extensive%0Ainvestigation%20across%20scenarios%20reveals%20that%20most%20SNNs%20trained%20on%20challenging%0Asamples%20can%20often%20match%20or%20surpass%20dense%20models%20in%20accuracy%20at%20certain%20sparsity%0Alevels%2C%20especially%20with%20limited%20data.%20We%20observe%20that%20layer-wise%20density%20ratios%0Atend%20to%20play%20an%20important%20role%20in%20SNN%20performance%2C%20particularly%20for%20methods%0Athat%20train%20from%20scratch%20without%20pre-trained%20initialization.%20These%20insights%0Aenhance%20our%20understanding%20of%20SNNs%27%20behavior%20and%20potential%20for%20efficient%0Alearning%20approaches%20in%20data-centric%20AI.%20Our%20code%20is%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/QiaoXiao7282/hard_sample_learners%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Sparse%2520Neural%2520Networks%2520Better%2520Hard%2520Sample%2520Learners%253F%26entry.906535625%3DQiao%2520Xiao%2520and%2520Boqian%2520Wu%2520and%2520Lu%2520Yin%2520and%2520Christopher%2520Neil%2520Gadzinski%2520and%2520Tianjin%2520Huang%2520and%2520Mykola%2520Pechenizkiy%2520and%2520Decebal%2520Constantin%2520Mocanu%26entry.1292438233%3D%2520%2520While%2520deep%2520learning%2520has%2520demonstrated%2520impressive%2520progress%252C%2520it%2520remains%2520a%250Adaunting%2520challenge%2520to%2520learn%2520from%2520hard%2520samples%2520as%2520these%2520samples%2520are%2520usually%250Anoisy%2520and%2520intricate.%2520These%2520hard%2520samples%2520play%2520a%2520crucial%2520role%2520in%2520the%2520optimal%250Aperformance%2520of%2520deep%2520neural%2520networks.%2520Most%2520research%2520on%2520Sparse%2520Neural%2520Networks%250A%2528SNNs%2529%2520has%2520focused%2520on%2520standard%2520training%2520data%252C%2520leaving%2520gaps%2520in%2520understanding%250Atheir%2520effectiveness%2520on%2520complex%2520and%2520challenging%2520data.%2520This%2520paper%2527s%2520extensive%250Ainvestigation%2520across%2520scenarios%2520reveals%2520that%2520most%2520SNNs%2520trained%2520on%2520challenging%250Asamples%2520can%2520often%2520match%2520or%2520surpass%2520dense%2520models%2520in%2520accuracy%2520at%2520certain%2520sparsity%250Alevels%252C%2520especially%2520with%2520limited%2520data.%2520We%2520observe%2520that%2520layer-wise%2520density%2520ratios%250Atend%2520to%2520play%2520an%2520important%2520role%2520in%2520SNN%2520performance%252C%2520particularly%2520for%2520methods%250Athat%2520train%2520from%2520scratch%2520without%2520pre-trained%2520initialization.%2520These%2520insights%250Aenhance%2520our%2520understanding%2520of%2520SNNs%2527%2520behavior%2520and%2520potential%2520for%2520efficient%250Alearning%2520approaches%2520in%2520data-centric%2520AI.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/QiaoXiao7282/hard_sample_learners%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Sparse%20Neural%20Networks%20Better%20Hard%20Sample%20Learners%3F&entry.906535625=Qiao%20Xiao%20and%20Boqian%20Wu%20and%20Lu%20Yin%20and%20Christopher%20Neil%20Gadzinski%20and%20Tianjin%20Huang%20and%20Mykola%20Pechenizkiy%20and%20Decebal%20Constantin%20Mocanu&entry.1292438233=%20%20While%20deep%20learning%20has%20demonstrated%20impressive%20progress%2C%20it%20remains%20a%0Adaunting%20challenge%20to%20learn%20from%20hard%20samples%20as%20these%20samples%20are%20usually%0Anoisy%20and%20intricate.%20These%20hard%20samples%20play%20a%20crucial%20role%20in%20the%20optimal%0Aperformance%20of%20deep%20neural%20networks.%20Most%20research%20on%20Sparse%20Neural%20Networks%0A%28SNNs%29%20has%20focused%20on%20standard%20training%20data%2C%20leaving%20gaps%20in%20understanding%0Atheir%20effectiveness%20on%20complex%20and%20challenging%20data.%20This%20paper%27s%20extensive%0Ainvestigation%20across%20scenarios%20reveals%20that%20most%20SNNs%20trained%20on%20challenging%0Asamples%20can%20often%20match%20or%20surpass%20dense%20models%20in%20accuracy%20at%20certain%20sparsity%0Alevels%2C%20especially%20with%20limited%20data.%20We%20observe%20that%20layer-wise%20density%20ratios%0Atend%20to%20play%20an%20important%20role%20in%20SNN%20performance%2C%20particularly%20for%20methods%0Athat%20train%20from%20scratch%20without%20pre-trained%20initialization.%20These%20insights%0Aenhance%20our%20understanding%20of%20SNNs%27%20behavior%20and%20potential%20for%20efficient%0Alearning%20approaches%20in%20data-centric%20AI.%20Our%20code%20is%20publicly%20available%20at%3A%0A%5Curl%7Bhttps%3A//github.com/QiaoXiao7282/hard_sample_learners%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09196v2&entry.124074799=Read"},
{"title": "A flexible framework for accurate LiDAR odometry, map manipulation, and\n  localization", "author": "Jos\u00e9 Luis Blanco-Claraco", "abstract": "  LiDAR-based SLAM is a core technology for autonomous vehicles and robots. One\nkey contribution of this work to 3D LiDAR SLAM and localization is a fierce\ndefense of view-based maps (pose graphs with time-stamped sensor readings) as\nthe fundamental representation of maps. As will be shown, they allow for the\ngreatest flexibility, enabling the posterior generation of arbitrary metric\nmaps optimized for particular tasks, e.g. obstacle avoidance, real-time\nlocalization. Moreover, this work introduces a new framework in which mapping\npipelines can be defined without coding, defining the connections of a network\nof reusable blocks much like deep-learning networks are designed by connecting\nlayers of standardized elements. We also introduce tightly-coupled estimation\nof linear and angular velocity vectors within the Iterative Closest Point\n(ICP)-like optimizer, leading to superior robustness against aggressive motion\nprofiles without the need for an IMU. Extensive experimental validation reveals\nthat the proposal compares well to, or improves, former state-of-the-art (SOTA)\nLiDAR odometry systems, while also successfully mapping some hard sequences\nwhere others diverge. A proposed self-adaptive configuration has been used,\nwithout parameter changes, for all 3D LiDAR datasets with sensors between 16\nand 128 rings, and has been extensively tested on 83 sequences over more than\n250~km of automotive, hand-held, airborne, and quadruped LiDAR datasets, both\nindoors and outdoors. The system flexibility is demonstrated with additional\nconfigurations for 2D LiDARs and for building 3D NDT-like maps. The framework\nis open-sourced online: https://github.com/MOLAorg/mola\n", "link": "http://arxiv.org/abs/2407.20465v2", "date": "2024-12-27", "relevancy": 2.4437, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.654}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5891}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20flexible%20framework%20for%20accurate%20LiDAR%20odometry%2C%20map%20manipulation%2C%20and%0A%20%20localization&body=Title%3A%20A%20flexible%20framework%20for%20accurate%20LiDAR%20odometry%2C%20map%20manipulation%2C%20and%0A%20%20localization%0AAuthor%3A%20Jos%C3%A9%20Luis%20Blanco-Claraco%0AAbstract%3A%20%20%20LiDAR-based%20SLAM%20is%20a%20core%20technology%20for%20autonomous%20vehicles%20and%20robots.%20One%0Akey%20contribution%20of%20this%20work%20to%203D%20LiDAR%20SLAM%20and%20localization%20is%20a%20fierce%0Adefense%20of%20view-based%20maps%20%28pose%20graphs%20with%20time-stamped%20sensor%20readings%29%20as%0Athe%20fundamental%20representation%20of%20maps.%20As%20will%20be%20shown%2C%20they%20allow%20for%20the%0Agreatest%20flexibility%2C%20enabling%20the%20posterior%20generation%20of%20arbitrary%20metric%0Amaps%20optimized%20for%20particular%20tasks%2C%20e.g.%20obstacle%20avoidance%2C%20real-time%0Alocalization.%20Moreover%2C%20this%20work%20introduces%20a%20new%20framework%20in%20which%20mapping%0Apipelines%20can%20be%20defined%20without%20coding%2C%20defining%20the%20connections%20of%20a%20network%0Aof%20reusable%20blocks%20much%20like%20deep-learning%20networks%20are%20designed%20by%20connecting%0Alayers%20of%20standardized%20elements.%20We%20also%20introduce%20tightly-coupled%20estimation%0Aof%20linear%20and%20angular%20velocity%20vectors%20within%20the%20Iterative%20Closest%20Point%0A%28ICP%29-like%20optimizer%2C%20leading%20to%20superior%20robustness%20against%20aggressive%20motion%0Aprofiles%20without%20the%20need%20for%20an%20IMU.%20Extensive%20experimental%20validation%20reveals%0Athat%20the%20proposal%20compares%20well%20to%2C%20or%20improves%2C%20former%20state-of-the-art%20%28SOTA%29%0ALiDAR%20odometry%20systems%2C%20while%20also%20successfully%20mapping%20some%20hard%20sequences%0Awhere%20others%20diverge.%20A%20proposed%20self-adaptive%20configuration%20has%20been%20used%2C%0Awithout%20parameter%20changes%2C%20for%20all%203D%20LiDAR%20datasets%20with%20sensors%20between%2016%0Aand%20128%20rings%2C%20and%20has%20been%20extensively%20tested%20on%2083%20sequences%20over%20more%20than%0A250~km%20of%20automotive%2C%20hand-held%2C%20airborne%2C%20and%20quadruped%20LiDAR%20datasets%2C%20both%0Aindoors%20and%20outdoors.%20The%20system%20flexibility%20is%20demonstrated%20with%20additional%0Aconfigurations%20for%202D%20LiDARs%20and%20for%20building%203D%20NDT-like%20maps.%20The%20framework%0Ais%20open-sourced%20online%3A%20https%3A//github.com/MOLAorg/mola%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520flexible%2520framework%2520for%2520accurate%2520LiDAR%2520odometry%252C%2520map%2520manipulation%252C%2520and%250A%2520%2520localization%26entry.906535625%3DJos%25C3%25A9%2520Luis%2520Blanco-Claraco%26entry.1292438233%3D%2520%2520LiDAR-based%2520SLAM%2520is%2520a%2520core%2520technology%2520for%2520autonomous%2520vehicles%2520and%2520robots.%2520One%250Akey%2520contribution%2520of%2520this%2520work%2520to%25203D%2520LiDAR%2520SLAM%2520and%2520localization%2520is%2520a%2520fierce%250Adefense%2520of%2520view-based%2520maps%2520%2528pose%2520graphs%2520with%2520time-stamped%2520sensor%2520readings%2529%2520as%250Athe%2520fundamental%2520representation%2520of%2520maps.%2520As%2520will%2520be%2520shown%252C%2520they%2520allow%2520for%2520the%250Agreatest%2520flexibility%252C%2520enabling%2520the%2520posterior%2520generation%2520of%2520arbitrary%2520metric%250Amaps%2520optimized%2520for%2520particular%2520tasks%252C%2520e.g.%2520obstacle%2520avoidance%252C%2520real-time%250Alocalization.%2520Moreover%252C%2520this%2520work%2520introduces%2520a%2520new%2520framework%2520in%2520which%2520mapping%250Apipelines%2520can%2520be%2520defined%2520without%2520coding%252C%2520defining%2520the%2520connections%2520of%2520a%2520network%250Aof%2520reusable%2520blocks%2520much%2520like%2520deep-learning%2520networks%2520are%2520designed%2520by%2520connecting%250Alayers%2520of%2520standardized%2520elements.%2520We%2520also%2520introduce%2520tightly-coupled%2520estimation%250Aof%2520linear%2520and%2520angular%2520velocity%2520vectors%2520within%2520the%2520Iterative%2520Closest%2520Point%250A%2528ICP%2529-like%2520optimizer%252C%2520leading%2520to%2520superior%2520robustness%2520against%2520aggressive%2520motion%250Aprofiles%2520without%2520the%2520need%2520for%2520an%2520IMU.%2520Extensive%2520experimental%2520validation%2520reveals%250Athat%2520the%2520proposal%2520compares%2520well%2520to%252C%2520or%2520improves%252C%2520former%2520state-of-the-art%2520%2528SOTA%2529%250ALiDAR%2520odometry%2520systems%252C%2520while%2520also%2520successfully%2520mapping%2520some%2520hard%2520sequences%250Awhere%2520others%2520diverge.%2520A%2520proposed%2520self-adaptive%2520configuration%2520has%2520been%2520used%252C%250Awithout%2520parameter%2520changes%252C%2520for%2520all%25203D%2520LiDAR%2520datasets%2520with%2520sensors%2520between%252016%250Aand%2520128%2520rings%252C%2520and%2520has%2520been%2520extensively%2520tested%2520on%252083%2520sequences%2520over%2520more%2520than%250A250~km%2520of%2520automotive%252C%2520hand-held%252C%2520airborne%252C%2520and%2520quadruped%2520LiDAR%2520datasets%252C%2520both%250Aindoors%2520and%2520outdoors.%2520The%2520system%2520flexibility%2520is%2520demonstrated%2520with%2520additional%250Aconfigurations%2520for%25202D%2520LiDARs%2520and%2520for%2520building%25203D%2520NDT-like%2520maps.%2520The%2520framework%250Ais%2520open-sourced%2520online%253A%2520https%253A//github.com/MOLAorg/mola%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20flexible%20framework%20for%20accurate%20LiDAR%20odometry%2C%20map%20manipulation%2C%20and%0A%20%20localization&entry.906535625=Jos%C3%A9%20Luis%20Blanco-Claraco&entry.1292438233=%20%20LiDAR-based%20SLAM%20is%20a%20core%20technology%20for%20autonomous%20vehicles%20and%20robots.%20One%0Akey%20contribution%20of%20this%20work%20to%203D%20LiDAR%20SLAM%20and%20localization%20is%20a%20fierce%0Adefense%20of%20view-based%20maps%20%28pose%20graphs%20with%20time-stamped%20sensor%20readings%29%20as%0Athe%20fundamental%20representation%20of%20maps.%20As%20will%20be%20shown%2C%20they%20allow%20for%20the%0Agreatest%20flexibility%2C%20enabling%20the%20posterior%20generation%20of%20arbitrary%20metric%0Amaps%20optimized%20for%20particular%20tasks%2C%20e.g.%20obstacle%20avoidance%2C%20real-time%0Alocalization.%20Moreover%2C%20this%20work%20introduces%20a%20new%20framework%20in%20which%20mapping%0Apipelines%20can%20be%20defined%20without%20coding%2C%20defining%20the%20connections%20of%20a%20network%0Aof%20reusable%20blocks%20much%20like%20deep-learning%20networks%20are%20designed%20by%20connecting%0Alayers%20of%20standardized%20elements.%20We%20also%20introduce%20tightly-coupled%20estimation%0Aof%20linear%20and%20angular%20velocity%20vectors%20within%20the%20Iterative%20Closest%20Point%0A%28ICP%29-like%20optimizer%2C%20leading%20to%20superior%20robustness%20against%20aggressive%20motion%0Aprofiles%20without%20the%20need%20for%20an%20IMU.%20Extensive%20experimental%20validation%20reveals%0Athat%20the%20proposal%20compares%20well%20to%2C%20or%20improves%2C%20former%20state-of-the-art%20%28SOTA%29%0ALiDAR%20odometry%20systems%2C%20while%20also%20successfully%20mapping%20some%20hard%20sequences%0Awhere%20others%20diverge.%20A%20proposed%20self-adaptive%20configuration%20has%20been%20used%2C%0Awithout%20parameter%20changes%2C%20for%20all%203D%20LiDAR%20datasets%20with%20sensors%20between%2016%0Aand%20128%20rings%2C%20and%20has%20been%20extensively%20tested%20on%2083%20sequences%20over%20more%20than%0A250~km%20of%20automotive%2C%20hand-held%2C%20airborne%2C%20and%20quadruped%20LiDAR%20datasets%2C%20both%0Aindoors%20and%20outdoors.%20The%20system%20flexibility%20is%20demonstrated%20with%20additional%0Aconfigurations%20for%202D%20LiDARs%20and%20for%20building%203D%20NDT-like%20maps.%20The%20framework%0Ais%20open-sourced%20online%3A%20https%3A//github.com/MOLAorg/mola%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20465v2&entry.124074799=Read"},
{"title": "Non-separable Spatio-temporal Graph Kernels via SPDEs", "author": "Alexander Nikitin and ST John and Arno Solin and Samuel Kaski", "abstract": "  Gaussian processes (GPs) provide a principled and direct approach for\ninference and learning on graphs. However, the lack of justified graph kernels\nfor spatio-temporal modelling has held back their use in graph problems. We\nleverage an explicit link between stochastic partial differential equations\n(SPDEs) and GPs on graphs, introduce a framework for deriving graph kernels via\nSPDEs, and derive non-separable spatio-temporal graph kernels that capture\ninteraction across space and time. We formulate the graph kernels for the\nstochastic heat equation and wave equation. We show that by providing novel\ntools for spatio-temporal GP modelling on graphs, we outperform pre-existing\ngraph kernels in real-world applications that feature diffusion, oscillation,\nand other complicated interactions.\n", "link": "http://arxiv.org/abs/2111.08524v3", "date": "2024-12-27", "relevancy": 2.3767, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4822}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4775}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-separable%20Spatio-temporal%20Graph%20Kernels%20via%20SPDEs&body=Title%3A%20Non-separable%20Spatio-temporal%20Graph%20Kernels%20via%20SPDEs%0AAuthor%3A%20Alexander%20Nikitin%20and%20ST%20John%20and%20Arno%20Solin%20and%20Samuel%20Kaski%0AAbstract%3A%20%20%20Gaussian%20processes%20%28GPs%29%20provide%20a%20principled%20and%20direct%20approach%20for%0Ainference%20and%20learning%20on%20graphs.%20However%2C%20the%20lack%20of%20justified%20graph%20kernels%0Afor%20spatio-temporal%20modelling%20has%20held%20back%20their%20use%20in%20graph%20problems.%20We%0Aleverage%20an%20explicit%20link%20between%20stochastic%20partial%20differential%20equations%0A%28SPDEs%29%20and%20GPs%20on%20graphs%2C%20introduce%20a%20framework%20for%20deriving%20graph%20kernels%20via%0ASPDEs%2C%20and%20derive%20non-separable%20spatio-temporal%20graph%20kernels%20that%20capture%0Ainteraction%20across%20space%20and%20time.%20We%20formulate%20the%20graph%20kernels%20for%20the%0Astochastic%20heat%20equation%20and%20wave%20equation.%20We%20show%20that%20by%20providing%20novel%0Atools%20for%20spatio-temporal%20GP%20modelling%20on%20graphs%2C%20we%20outperform%20pre-existing%0Agraph%20kernels%20in%20real-world%20applications%20that%20feature%20diffusion%2C%20oscillation%2C%0Aand%20other%20complicated%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2111.08524v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-separable%2520Spatio-temporal%2520Graph%2520Kernels%2520via%2520SPDEs%26entry.906535625%3DAlexander%2520Nikitin%2520and%2520ST%2520John%2520and%2520Arno%2520Solin%2520and%2520Samuel%2520Kaski%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520%2528GPs%2529%2520provide%2520a%2520principled%2520and%2520direct%2520approach%2520for%250Ainference%2520and%2520learning%2520on%2520graphs.%2520However%252C%2520the%2520lack%2520of%2520justified%2520graph%2520kernels%250Afor%2520spatio-temporal%2520modelling%2520has%2520held%2520back%2520their%2520use%2520in%2520graph%2520problems.%2520We%250Aleverage%2520an%2520explicit%2520link%2520between%2520stochastic%2520partial%2520differential%2520equations%250A%2528SPDEs%2529%2520and%2520GPs%2520on%2520graphs%252C%2520introduce%2520a%2520framework%2520for%2520deriving%2520graph%2520kernels%2520via%250ASPDEs%252C%2520and%2520derive%2520non-separable%2520spatio-temporal%2520graph%2520kernels%2520that%2520capture%250Ainteraction%2520across%2520space%2520and%2520time.%2520We%2520formulate%2520the%2520graph%2520kernels%2520for%2520the%250Astochastic%2520heat%2520equation%2520and%2520wave%2520equation.%2520We%2520show%2520that%2520by%2520providing%2520novel%250Atools%2520for%2520spatio-temporal%2520GP%2520modelling%2520on%2520graphs%252C%2520we%2520outperform%2520pre-existing%250Agraph%2520kernels%2520in%2520real-world%2520applications%2520that%2520feature%2520diffusion%252C%2520oscillation%252C%250Aand%2520other%2520complicated%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2111.08524v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-separable%20Spatio-temporal%20Graph%20Kernels%20via%20SPDEs&entry.906535625=Alexander%20Nikitin%20and%20ST%20John%20and%20Arno%20Solin%20and%20Samuel%20Kaski&entry.1292438233=%20%20Gaussian%20processes%20%28GPs%29%20provide%20a%20principled%20and%20direct%20approach%20for%0Ainference%20and%20learning%20on%20graphs.%20However%2C%20the%20lack%20of%20justified%20graph%20kernels%0Afor%20spatio-temporal%20modelling%20has%20held%20back%20their%20use%20in%20graph%20problems.%20We%0Aleverage%20an%20explicit%20link%20between%20stochastic%20partial%20differential%20equations%0A%28SPDEs%29%20and%20GPs%20on%20graphs%2C%20introduce%20a%20framework%20for%20deriving%20graph%20kernels%20via%0ASPDEs%2C%20and%20derive%20non-separable%20spatio-temporal%20graph%20kernels%20that%20capture%0Ainteraction%20across%20space%20and%20time.%20We%20formulate%20the%20graph%20kernels%20for%20the%0Astochastic%20heat%20equation%20and%20wave%20equation.%20We%20show%20that%20by%20providing%20novel%0Atools%20for%20spatio-temporal%20GP%20modelling%20on%20graphs%2C%20we%20outperform%20pre-existing%0Agraph%20kernels%20in%20real-world%20applications%20that%20feature%20diffusion%2C%20oscillation%2C%0Aand%20other%20complicated%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.08524v3&entry.124074799=Read"},
{"title": "xFLIE: Leveraging Actionable Hierarchical Scene Representations for\n  Autonomous Semantic-Aware Inspection Missions", "author": "Vignesh Kottayam Viswanathan and Mario A. V. Saucedo and Sumeet Gajanan Satpute and Christoforos Kanellakis and George Nikolakopoulos", "abstract": "  This article presents xFLIE, a fully integrated 3D hierarchical scene graph\nbased autonomous inspection architecture. Specifically, we present a\ntightly-coupled solution of incremental 3D Layered Semantic Graphs (LSG)\nconstruction and real-time exploitation by a multi-modal autonomy, First-Look\nbased Inspection and Exploration (FLIE) planner, to address the task of\ninspection of apriori unknown semantic targets of interest in unknown\nenvironments. This work aims to address the challenge of maintaining, in\naddition to or as an alternative to volumetric models, an intuitive scene\nrepresentation during large-scale inspection missions. Through its\ncontributions, the proposed architecture aims to provide a high-level\nmulti-tiered abstract environment representation whilst simultaneously\nmaintaining a tractable foundation for rapid and informed decision-making\ncapable of enhancing inspection planning through scene understanding, what\nshould it inspect ?, and reasoning, why should it inspect ?. The proposed LSG\nframework is designed to leverage the concept of nesting lower local graphs, at\nmultiple layers of abstraction, with the abstract concepts grounded on the\nfunctionality of the integrated FLIE planner. Through intuitive scene\nrepresentation, the proposed architecture offers an easily digestible\nenvironment model for human operators which helps to improve situational\nawareness and their understanding of the operating environment. We highlight\nthe use-case benefits of hierarchical and semantic path-planning capability\nover LSG to address queries, by the integrated planner as well as the human\noperator. The validity of the proposed architecture is evaluated in large-scale\nsimulated outdoor urban scenarios as well as being deployed onboard a Boston\nDynamics Spot quadruped robot for extensive outdoor field experiments.\n", "link": "http://arxiv.org/abs/2412.19571v1", "date": "2024-12-27", "relevancy": 2.3719, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xFLIE%3A%20Leveraging%20Actionable%20Hierarchical%20Scene%20Representations%20for%0A%20%20Autonomous%20Semantic-Aware%20Inspection%20Missions&body=Title%3A%20xFLIE%3A%20Leveraging%20Actionable%20Hierarchical%20Scene%20Representations%20for%0A%20%20Autonomous%20Semantic-Aware%20Inspection%20Missions%0AAuthor%3A%20Vignesh%20Kottayam%20Viswanathan%20and%20Mario%20A.%20V.%20Saucedo%20and%20Sumeet%20Gajanan%20Satpute%20and%20Christoforos%20Kanellakis%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20This%20article%20presents%20xFLIE%2C%20a%20fully%20integrated%203D%20hierarchical%20scene%20graph%0Abased%20autonomous%20inspection%20architecture.%20Specifically%2C%20we%20present%20a%0Atightly-coupled%20solution%20of%20incremental%203D%20Layered%20Semantic%20Graphs%20%28LSG%29%0Aconstruction%20and%20real-time%20exploitation%20by%20a%20multi-modal%20autonomy%2C%20First-Look%0Abased%20Inspection%20and%20Exploration%20%28FLIE%29%20planner%2C%20to%20address%20the%20task%20of%0Ainspection%20of%20apriori%20unknown%20semantic%20targets%20of%20interest%20in%20unknown%0Aenvironments.%20This%20work%20aims%20to%20address%20the%20challenge%20of%20maintaining%2C%20in%0Aaddition%20to%20or%20as%20an%20alternative%20to%20volumetric%20models%2C%20an%20intuitive%20scene%0Arepresentation%20during%20large-scale%20inspection%20missions.%20Through%20its%0Acontributions%2C%20the%20proposed%20architecture%20aims%20to%20provide%20a%20high-level%0Amulti-tiered%20abstract%20environment%20representation%20whilst%20simultaneously%0Amaintaining%20a%20tractable%20foundation%20for%20rapid%20and%20informed%20decision-making%0Acapable%20of%20enhancing%20inspection%20planning%20through%20scene%20understanding%2C%20what%0Ashould%20it%20inspect%20%3F%2C%20and%20reasoning%2C%20why%20should%20it%20inspect%20%3F.%20The%20proposed%20LSG%0Aframework%20is%20designed%20to%20leverage%20the%20concept%20of%20nesting%20lower%20local%20graphs%2C%20at%0Amultiple%20layers%20of%20abstraction%2C%20with%20the%20abstract%20concepts%20grounded%20on%20the%0Afunctionality%20of%20the%20integrated%20FLIE%20planner.%20Through%20intuitive%20scene%0Arepresentation%2C%20the%20proposed%20architecture%20offers%20an%20easily%20digestible%0Aenvironment%20model%20for%20human%20operators%20which%20helps%20to%20improve%20situational%0Aawareness%20and%20their%20understanding%20of%20the%20operating%20environment.%20We%20highlight%0Athe%20use-case%20benefits%20of%20hierarchical%20and%20semantic%20path-planning%20capability%0Aover%20LSG%20to%20address%20queries%2C%20by%20the%20integrated%20planner%20as%20well%20as%20the%20human%0Aoperator.%20The%20validity%20of%20the%20proposed%20architecture%20is%20evaluated%20in%20large-scale%0Asimulated%20outdoor%20urban%20scenarios%20as%20well%20as%20being%20deployed%20onboard%20a%20Boston%0ADynamics%20Spot%20quadruped%20robot%20for%20extensive%20outdoor%20field%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxFLIE%253A%2520Leveraging%2520Actionable%2520Hierarchical%2520Scene%2520Representations%2520for%250A%2520%2520Autonomous%2520Semantic-Aware%2520Inspection%2520Missions%26entry.906535625%3DVignesh%2520Kottayam%2520Viswanathan%2520and%2520Mario%2520A.%2520V.%2520Saucedo%2520and%2520Sumeet%2520Gajanan%2520Satpute%2520and%2520Christoforos%2520Kanellakis%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520xFLIE%252C%2520a%2520fully%2520integrated%25203D%2520hierarchical%2520scene%2520graph%250Abased%2520autonomous%2520inspection%2520architecture.%2520Specifically%252C%2520we%2520present%2520a%250Atightly-coupled%2520solution%2520of%2520incremental%25203D%2520Layered%2520Semantic%2520Graphs%2520%2528LSG%2529%250Aconstruction%2520and%2520real-time%2520exploitation%2520by%2520a%2520multi-modal%2520autonomy%252C%2520First-Look%250Abased%2520Inspection%2520and%2520Exploration%2520%2528FLIE%2529%2520planner%252C%2520to%2520address%2520the%2520task%2520of%250Ainspection%2520of%2520apriori%2520unknown%2520semantic%2520targets%2520of%2520interest%2520in%2520unknown%250Aenvironments.%2520This%2520work%2520aims%2520to%2520address%2520the%2520challenge%2520of%2520maintaining%252C%2520in%250Aaddition%2520to%2520or%2520as%2520an%2520alternative%2520to%2520volumetric%2520models%252C%2520an%2520intuitive%2520scene%250Arepresentation%2520during%2520large-scale%2520inspection%2520missions.%2520Through%2520its%250Acontributions%252C%2520the%2520proposed%2520architecture%2520aims%2520to%2520provide%2520a%2520high-level%250Amulti-tiered%2520abstract%2520environment%2520representation%2520whilst%2520simultaneously%250Amaintaining%2520a%2520tractable%2520foundation%2520for%2520rapid%2520and%2520informed%2520decision-making%250Acapable%2520of%2520enhancing%2520inspection%2520planning%2520through%2520scene%2520understanding%252C%2520what%250Ashould%2520it%2520inspect%2520%253F%252C%2520and%2520reasoning%252C%2520why%2520should%2520it%2520inspect%2520%253F.%2520The%2520proposed%2520LSG%250Aframework%2520is%2520designed%2520to%2520leverage%2520the%2520concept%2520of%2520nesting%2520lower%2520local%2520graphs%252C%2520at%250Amultiple%2520layers%2520of%2520abstraction%252C%2520with%2520the%2520abstract%2520concepts%2520grounded%2520on%2520the%250Afunctionality%2520of%2520the%2520integrated%2520FLIE%2520planner.%2520Through%2520intuitive%2520scene%250Arepresentation%252C%2520the%2520proposed%2520architecture%2520offers%2520an%2520easily%2520digestible%250Aenvironment%2520model%2520for%2520human%2520operators%2520which%2520helps%2520to%2520improve%2520situational%250Aawareness%2520and%2520their%2520understanding%2520of%2520the%2520operating%2520environment.%2520We%2520highlight%250Athe%2520use-case%2520benefits%2520of%2520hierarchical%2520and%2520semantic%2520path-planning%2520capability%250Aover%2520LSG%2520to%2520address%2520queries%252C%2520by%2520the%2520integrated%2520planner%2520as%2520well%2520as%2520the%2520human%250Aoperator.%2520The%2520validity%2520of%2520the%2520proposed%2520architecture%2520is%2520evaluated%2520in%2520large-scale%250Asimulated%2520outdoor%2520urban%2520scenarios%2520as%2520well%2520as%2520being%2520deployed%2520onboard%2520a%2520Boston%250ADynamics%2520Spot%2520quadruped%2520robot%2520for%2520extensive%2520outdoor%2520field%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xFLIE%3A%20Leveraging%20Actionable%20Hierarchical%20Scene%20Representations%20for%0A%20%20Autonomous%20Semantic-Aware%20Inspection%20Missions&entry.906535625=Vignesh%20Kottayam%20Viswanathan%20and%20Mario%20A.%20V.%20Saucedo%20and%20Sumeet%20Gajanan%20Satpute%20and%20Christoforos%20Kanellakis%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20This%20article%20presents%20xFLIE%2C%20a%20fully%20integrated%203D%20hierarchical%20scene%20graph%0Abased%20autonomous%20inspection%20architecture.%20Specifically%2C%20we%20present%20a%0Atightly-coupled%20solution%20of%20incremental%203D%20Layered%20Semantic%20Graphs%20%28LSG%29%0Aconstruction%20and%20real-time%20exploitation%20by%20a%20multi-modal%20autonomy%2C%20First-Look%0Abased%20Inspection%20and%20Exploration%20%28FLIE%29%20planner%2C%20to%20address%20the%20task%20of%0Ainspection%20of%20apriori%20unknown%20semantic%20targets%20of%20interest%20in%20unknown%0Aenvironments.%20This%20work%20aims%20to%20address%20the%20challenge%20of%20maintaining%2C%20in%0Aaddition%20to%20or%20as%20an%20alternative%20to%20volumetric%20models%2C%20an%20intuitive%20scene%0Arepresentation%20during%20large-scale%20inspection%20missions.%20Through%20its%0Acontributions%2C%20the%20proposed%20architecture%20aims%20to%20provide%20a%20high-level%0Amulti-tiered%20abstract%20environment%20representation%20whilst%20simultaneously%0Amaintaining%20a%20tractable%20foundation%20for%20rapid%20and%20informed%20decision-making%0Acapable%20of%20enhancing%20inspection%20planning%20through%20scene%20understanding%2C%20what%0Ashould%20it%20inspect%20%3F%2C%20and%20reasoning%2C%20why%20should%20it%20inspect%20%3F.%20The%20proposed%20LSG%0Aframework%20is%20designed%20to%20leverage%20the%20concept%20of%20nesting%20lower%20local%20graphs%2C%20at%0Amultiple%20layers%20of%20abstraction%2C%20with%20the%20abstract%20concepts%20grounded%20on%20the%0Afunctionality%20of%20the%20integrated%20FLIE%20planner.%20Through%20intuitive%20scene%0Arepresentation%2C%20the%20proposed%20architecture%20offers%20an%20easily%20digestible%0Aenvironment%20model%20for%20human%20operators%20which%20helps%20to%20improve%20situational%0Aawareness%20and%20their%20understanding%20of%20the%20operating%20environment.%20We%20highlight%0Athe%20use-case%20benefits%20of%20hierarchical%20and%20semantic%20path-planning%20capability%0Aover%20LSG%20to%20address%20queries%2C%20by%20the%20integrated%20planner%20as%20well%20as%20the%20human%0Aoperator.%20The%20validity%20of%20the%20proposed%20architecture%20is%20evaluated%20in%20large-scale%0Asimulated%20outdoor%20urban%20scenarios%20as%20well%20as%20being%20deployed%20onboard%20a%20Boston%0ADynamics%20Spot%20quadruped%20robot%20for%20extensive%20outdoor%20field%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19571v1&entry.124074799=Read"},
{"title": "GenDFIR: Advancing Cyber Incident Timeline Analysis Through Retrieval\n  Augmented Generation and Large Language Models", "author": "Fatma Yasmine Loumachi and Mohamed Chahine Ghanem and Mohamed Amine Ferrag", "abstract": "  Cyber timeline analysis, or forensic timeline analysis, is crucial in Digital\nForensics and Incident Response (DFIR). It examines artefacts and events\nparticularly timestamps and metadata to detect anomalies, establish\ncorrelations, and reconstruct incident timelines. Traditional methods rely on\nstructured artefacts, such as logs and filesystem metadata, using specialised\ntools for evidence identification and feature extraction. This paper introduces\nGenDFIR, a framework leveraging large language models (LLMs), specifically\nLlama 3.1 8B in zero shot mode, integrated with a Retrieval-Augmented\nGeneration (RAG) agent. Incident data is preprocessed into a structured\nknowledge base, enabling the RAG agent to retrieve relevant events based on\nuser prompts. The LLM interprets this context, offering semantic enrichment.\nTested on synthetic data in a controlled environment, results demonstrate\nGenDFIR's reliability and robustness, showcasing LLMs potential to automate\ntimeline analysis and advance threat detection.\n", "link": "http://arxiv.org/abs/2409.02572v4", "date": "2024-12-27", "relevancy": 2.3617, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenDFIR%3A%20Advancing%20Cyber%20Incident%20Timeline%20Analysis%20Through%20Retrieval%0A%20%20Augmented%20Generation%20and%20Large%20Language%20Models&body=Title%3A%20GenDFIR%3A%20Advancing%20Cyber%20Incident%20Timeline%20Analysis%20Through%20Retrieval%0A%20%20Augmented%20Generation%20and%20Large%20Language%20Models%0AAuthor%3A%20Fatma%20Yasmine%20Loumachi%20and%20Mohamed%20Chahine%20Ghanem%20and%20Mohamed%20Amine%20Ferrag%0AAbstract%3A%20%20%20Cyber%20timeline%20analysis%2C%20or%20forensic%20timeline%20analysis%2C%20is%20crucial%20in%20Digital%0AForensics%20and%20Incident%20Response%20%28DFIR%29.%20It%20examines%20artefacts%20and%20events%0Aparticularly%20timestamps%20and%20metadata%20to%20detect%20anomalies%2C%20establish%0Acorrelations%2C%20and%20reconstruct%20incident%20timelines.%20Traditional%20methods%20rely%20on%0Astructured%20artefacts%2C%20such%20as%20logs%20and%20filesystem%20metadata%2C%20using%20specialised%0Atools%20for%20evidence%20identification%20and%20feature%20extraction.%20This%20paper%20introduces%0AGenDFIR%2C%20a%20framework%20leveraging%20large%20language%20models%20%28LLMs%29%2C%20specifically%0ALlama%203.1%208B%20in%20zero%20shot%20mode%2C%20integrated%20with%20a%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20agent.%20Incident%20data%20is%20preprocessed%20into%20a%20structured%0Aknowledge%20base%2C%20enabling%20the%20RAG%20agent%20to%20retrieve%20relevant%20events%20based%20on%0Auser%20prompts.%20The%20LLM%20interprets%20this%20context%2C%20offering%20semantic%20enrichment.%0ATested%20on%20synthetic%20data%20in%20a%20controlled%20environment%2C%20results%20demonstrate%0AGenDFIR%27s%20reliability%20and%20robustness%2C%20showcasing%20LLMs%20potential%20to%20automate%0Atimeline%20analysis%20and%20advance%20threat%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02572v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenDFIR%253A%2520Advancing%2520Cyber%2520Incident%2520Timeline%2520Analysis%2520Through%2520Retrieval%250A%2520%2520Augmented%2520Generation%2520and%2520Large%2520Language%2520Models%26entry.906535625%3DFatma%2520Yasmine%2520Loumachi%2520and%2520Mohamed%2520Chahine%2520Ghanem%2520and%2520Mohamed%2520Amine%2520Ferrag%26entry.1292438233%3D%2520%2520Cyber%2520timeline%2520analysis%252C%2520or%2520forensic%2520timeline%2520analysis%252C%2520is%2520crucial%2520in%2520Digital%250AForensics%2520and%2520Incident%2520Response%2520%2528DFIR%2529.%2520It%2520examines%2520artefacts%2520and%2520events%250Aparticularly%2520timestamps%2520and%2520metadata%2520to%2520detect%2520anomalies%252C%2520establish%250Acorrelations%252C%2520and%2520reconstruct%2520incident%2520timelines.%2520Traditional%2520methods%2520rely%2520on%250Astructured%2520artefacts%252C%2520such%2520as%2520logs%2520and%2520filesystem%2520metadata%252C%2520using%2520specialised%250Atools%2520for%2520evidence%2520identification%2520and%2520feature%2520extraction.%2520This%2520paper%2520introduces%250AGenDFIR%252C%2520a%2520framework%2520leveraging%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520specifically%250ALlama%25203.1%25208B%2520in%2520zero%2520shot%2520mode%252C%2520integrated%2520with%2520a%2520Retrieval-Augmented%250AGeneration%2520%2528RAG%2529%2520agent.%2520Incident%2520data%2520is%2520preprocessed%2520into%2520a%2520structured%250Aknowledge%2520base%252C%2520enabling%2520the%2520RAG%2520agent%2520to%2520retrieve%2520relevant%2520events%2520based%2520on%250Auser%2520prompts.%2520The%2520LLM%2520interprets%2520this%2520context%252C%2520offering%2520semantic%2520enrichment.%250ATested%2520on%2520synthetic%2520data%2520in%2520a%2520controlled%2520environment%252C%2520results%2520demonstrate%250AGenDFIR%2527s%2520reliability%2520and%2520robustness%252C%2520showcasing%2520LLMs%2520potential%2520to%2520automate%250Atimeline%2520analysis%2520and%2520advance%2520threat%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02572v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenDFIR%3A%20Advancing%20Cyber%20Incident%20Timeline%20Analysis%20Through%20Retrieval%0A%20%20Augmented%20Generation%20and%20Large%20Language%20Models&entry.906535625=Fatma%20Yasmine%20Loumachi%20and%20Mohamed%20Chahine%20Ghanem%20and%20Mohamed%20Amine%20Ferrag&entry.1292438233=%20%20Cyber%20timeline%20analysis%2C%20or%20forensic%20timeline%20analysis%2C%20is%20crucial%20in%20Digital%0AForensics%20and%20Incident%20Response%20%28DFIR%29.%20It%20examines%20artefacts%20and%20events%0Aparticularly%20timestamps%20and%20metadata%20to%20detect%20anomalies%2C%20establish%0Acorrelations%2C%20and%20reconstruct%20incident%20timelines.%20Traditional%20methods%20rely%20on%0Astructured%20artefacts%2C%20such%20as%20logs%20and%20filesystem%20metadata%2C%20using%20specialised%0Atools%20for%20evidence%20identification%20and%20feature%20extraction.%20This%20paper%20introduces%0AGenDFIR%2C%20a%20framework%20leveraging%20large%20language%20models%20%28LLMs%29%2C%20specifically%0ALlama%203.1%208B%20in%20zero%20shot%20mode%2C%20integrated%20with%20a%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20agent.%20Incident%20data%20is%20preprocessed%20into%20a%20structured%0Aknowledge%20base%2C%20enabling%20the%20RAG%20agent%20to%20retrieve%20relevant%20events%20based%20on%0Auser%20prompts.%20The%20LLM%20interprets%20this%20context%2C%20offering%20semantic%20enrichment.%0ATested%20on%20synthetic%20data%20in%20a%20controlled%20environment%2C%20results%20demonstrate%0AGenDFIR%27s%20reliability%20and%20robustness%2C%20showcasing%20LLMs%20potential%20to%20automate%0Atimeline%20analysis%20and%20advance%20threat%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02572v4&entry.124074799=Read"},
{"title": "FlexiTex: Enhancing Texture Generation with Visual Guidance", "author": "DaDong Jiang and Xianghui Yang and Zibo Zhao and Sheng Zhang and Jiaao Yu and Zeqiang Lai and Shaoxiong Yang and Chunchao Guo and Xiaobo Zhou and Zhihui Ke", "abstract": "  Recent texture generation methods achieve impressive results due to the\npowerful generative prior they leverage from large-scale text-to-image\ndiffusion models. However, abstract textual prompts are limited in providing\nglobal textural or shape information, which results in the texture generation\nmethods producing blurry or inconsistent patterns. To tackle this, we present\nFlexiTex, embedding rich information via visual guidance to generate a\nhigh-quality texture. The core of FlexiTex is the Visual Guidance Enhancement\nmodule, which incorporates more specific information from visual guidance to\nreduce ambiguity in the text prompt and preserve high-frequency details. To\nfurther enhance the visual guidance, we introduce a Direction-Aware Adaptation\nmodule that automatically designs direction prompts based on different camera\nposes, avoiding the Janus problem and maintaining semantically global\nconsistency. Benefiting from the visual guidance, FlexiTex produces\nquantitatively and qualitatively sound results, demonstrating its potential to\nadvance texture generation for real-world applications.\n", "link": "http://arxiv.org/abs/2409.12431v4", "date": "2024-12-27", "relevancy": 2.347, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6141}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5676}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexiTex%3A%20Enhancing%20Texture%20Generation%20with%20Visual%20Guidance&body=Title%3A%20FlexiTex%3A%20Enhancing%20Texture%20Generation%20with%20Visual%20Guidance%0AAuthor%3A%20DaDong%20Jiang%20and%20Xianghui%20Yang%20and%20Zibo%20Zhao%20and%20Sheng%20Zhang%20and%20Jiaao%20Yu%20and%20Zeqiang%20Lai%20and%20Shaoxiong%20Yang%20and%20Chunchao%20Guo%20and%20Xiaobo%20Zhou%20and%20Zhihui%20Ke%0AAbstract%3A%20%20%20Recent%20texture%20generation%20methods%20achieve%20impressive%20results%20due%20to%20the%0Apowerful%20generative%20prior%20they%20leverage%20from%20large-scale%20text-to-image%0Adiffusion%20models.%20However%2C%20abstract%20textual%20prompts%20are%20limited%20in%20providing%0Aglobal%20textural%20or%20shape%20information%2C%20which%20results%20in%20the%20texture%20generation%0Amethods%20producing%20blurry%20or%20inconsistent%20patterns.%20To%20tackle%20this%2C%20we%20present%0AFlexiTex%2C%20embedding%20rich%20information%20via%20visual%20guidance%20to%20generate%20a%0Ahigh-quality%20texture.%20The%20core%20of%20FlexiTex%20is%20the%20Visual%20Guidance%20Enhancement%0Amodule%2C%20which%20incorporates%20more%20specific%20information%20from%20visual%20guidance%20to%0Areduce%20ambiguity%20in%20the%20text%20prompt%20and%20preserve%20high-frequency%20details.%20To%0Afurther%20enhance%20the%20visual%20guidance%2C%20we%20introduce%20a%20Direction-Aware%20Adaptation%0Amodule%20that%20automatically%20designs%20direction%20prompts%20based%20on%20different%20camera%0Aposes%2C%20avoiding%20the%20Janus%20problem%20and%20maintaining%20semantically%20global%0Aconsistency.%20Benefiting%20from%20the%20visual%20guidance%2C%20FlexiTex%20produces%0Aquantitatively%20and%20qualitatively%20sound%20results%2C%20demonstrating%20its%20potential%20to%0Aadvance%20texture%20generation%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.12431v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexiTex%253A%2520Enhancing%2520Texture%2520Generation%2520with%2520Visual%2520Guidance%26entry.906535625%3DDaDong%2520Jiang%2520and%2520Xianghui%2520Yang%2520and%2520Zibo%2520Zhao%2520and%2520Sheng%2520Zhang%2520and%2520Jiaao%2520Yu%2520and%2520Zeqiang%2520Lai%2520and%2520Shaoxiong%2520Yang%2520and%2520Chunchao%2520Guo%2520and%2520Xiaobo%2520Zhou%2520and%2520Zhihui%2520Ke%26entry.1292438233%3D%2520%2520Recent%2520texture%2520generation%2520methods%2520achieve%2520impressive%2520results%2520due%2520to%2520the%250Apowerful%2520generative%2520prior%2520they%2520leverage%2520from%2520large-scale%2520text-to-image%250Adiffusion%2520models.%2520However%252C%2520abstract%2520textual%2520prompts%2520are%2520limited%2520in%2520providing%250Aglobal%2520textural%2520or%2520shape%2520information%252C%2520which%2520results%2520in%2520the%2520texture%2520generation%250Amethods%2520producing%2520blurry%2520or%2520inconsistent%2520patterns.%2520To%2520tackle%2520this%252C%2520we%2520present%250AFlexiTex%252C%2520embedding%2520rich%2520information%2520via%2520visual%2520guidance%2520to%2520generate%2520a%250Ahigh-quality%2520texture.%2520The%2520core%2520of%2520FlexiTex%2520is%2520the%2520Visual%2520Guidance%2520Enhancement%250Amodule%252C%2520which%2520incorporates%2520more%2520specific%2520information%2520from%2520visual%2520guidance%2520to%250Areduce%2520ambiguity%2520in%2520the%2520text%2520prompt%2520and%2520preserve%2520high-frequency%2520details.%2520To%250Afurther%2520enhance%2520the%2520visual%2520guidance%252C%2520we%2520introduce%2520a%2520Direction-Aware%2520Adaptation%250Amodule%2520that%2520automatically%2520designs%2520direction%2520prompts%2520based%2520on%2520different%2520camera%250Aposes%252C%2520avoiding%2520the%2520Janus%2520problem%2520and%2520maintaining%2520semantically%2520global%250Aconsistency.%2520Benefiting%2520from%2520the%2520visual%2520guidance%252C%2520FlexiTex%2520produces%250Aquantitatively%2520and%2520qualitatively%2520sound%2520results%252C%2520demonstrating%2520its%2520potential%2520to%250Aadvance%2520texture%2520generation%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.12431v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiTex%3A%20Enhancing%20Texture%20Generation%20with%20Visual%20Guidance&entry.906535625=DaDong%20Jiang%20and%20Xianghui%20Yang%20and%20Zibo%20Zhao%20and%20Sheng%20Zhang%20and%20Jiaao%20Yu%20and%20Zeqiang%20Lai%20and%20Shaoxiong%20Yang%20and%20Chunchao%20Guo%20and%20Xiaobo%20Zhou%20and%20Zhihui%20Ke&entry.1292438233=%20%20Recent%20texture%20generation%20methods%20achieve%20impressive%20results%20due%20to%20the%0Apowerful%20generative%20prior%20they%20leverage%20from%20large-scale%20text-to-image%0Adiffusion%20models.%20However%2C%20abstract%20textual%20prompts%20are%20limited%20in%20providing%0Aglobal%20textural%20or%20shape%20information%2C%20which%20results%20in%20the%20texture%20generation%0Amethods%20producing%20blurry%20or%20inconsistent%20patterns.%20To%20tackle%20this%2C%20we%20present%0AFlexiTex%2C%20embedding%20rich%20information%20via%20visual%20guidance%20to%20generate%20a%0Ahigh-quality%20texture.%20The%20core%20of%20FlexiTex%20is%20the%20Visual%20Guidance%20Enhancement%0Amodule%2C%20which%20incorporates%20more%20specific%20information%20from%20visual%20guidance%20to%0Areduce%20ambiguity%20in%20the%20text%20prompt%20and%20preserve%20high-frequency%20details.%20To%0Afurther%20enhance%20the%20visual%20guidance%2C%20we%20introduce%20a%20Direction-Aware%20Adaptation%0Amodule%20that%20automatically%20designs%20direction%20prompts%20based%20on%20different%20camera%0Aposes%2C%20avoiding%20the%20Janus%20problem%20and%20maintaining%20semantically%20global%0Aconsistency.%20Benefiting%20from%20the%20visual%20guidance%2C%20FlexiTex%20produces%0Aquantitatively%20and%20qualitatively%20sound%20results%2C%20demonstrating%20its%20potential%20to%0Aadvance%20texture%20generation%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.12431v4&entry.124074799=Read"},
{"title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse\n  Task Synthesis", "author": "Qiushi Sun and Kanzhi Cheng and Zichen Ding and Chuanyang Jin and Yian Wang and Fangzhi Xu and Zhenyu Wu and Chengyou Jia and Liheng Chen and Zhoumianze Liu and Ben Kao and Guohao Li and Junxian He and Yu Qiao and Zhiyong Wu", "abstract": "  Graphical User Interface (GUI) agents powered by Vision-Language Models\n(VLMs) have demonstrated human-like computer control capability. Despite their\nutility in advancing digital automation, a critical bottleneck persists:\ncollecting high-quality trajectory data for training. Common practices for\ncollecting such data rely on human supervision or synthetic data generation\nthrough executing pre-defined tasks, which are either resource-intensive or\nunable to guarantee data quality. Moreover, these methods suffer from limited\ndata diversity and significant gaps between synthetic data and real-world\nenvironments. To address these challenges, we propose OS-Genesis, a novel GUI\ndata synthesis pipeline that reverses the conventional trajectory collection\nprocess. Instead of relying on pre-defined tasks, OS-Genesis enables agents\nfirst to perceive environments and perform step-wise interactions, then\nretrospectively derive high-quality tasks to enable trajectory-level\nexploration. A trajectory reward model is then employed to ensure the quality\nof the generated trajectories. We demonstrate that training GUI agents with\nOS-Genesis significantly improves their performance on highly challenging\nonline benchmarks. In-depth analysis further validates OS-Genesis's efficiency\nand its superior data quality and diversity compared to existing synthesis\nmethods. Our codes, data, and checkpoints are available at\n\\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.\n", "link": "http://arxiv.org/abs/2412.19723v1", "date": "2024-12-27", "relevancy": 2.3222, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6025}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5933}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OS-Genesis%3A%20Automating%20GUI%20Agent%20Trajectory%20Construction%20via%20Reverse%0A%20%20Task%20Synthesis&body=Title%3A%20OS-Genesis%3A%20Automating%20GUI%20Agent%20Trajectory%20Construction%20via%20Reverse%0A%20%20Task%20Synthesis%0AAuthor%3A%20Qiushi%20Sun%20and%20Kanzhi%20Cheng%20and%20Zichen%20Ding%20and%20Chuanyang%20Jin%20and%20Yian%20Wang%20and%20Fangzhi%20Xu%20and%20Zhenyu%20Wu%20and%20Chengyou%20Jia%20and%20Liheng%20Chen%20and%20Zhoumianze%20Liu%20and%20Ben%20Kao%20and%20Guohao%20Li%20and%20Junxian%20He%20and%20Yu%20Qiao%20and%20Zhiyong%20Wu%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20agents%20powered%20by%20Vision-Language%20Models%0A%28VLMs%29%20have%20demonstrated%20human-like%20computer%20control%20capability.%20Despite%20their%0Autility%20in%20advancing%20digital%20automation%2C%20a%20critical%20bottleneck%20persists%3A%0Acollecting%20high-quality%20trajectory%20data%20for%20training.%20Common%20practices%20for%0Acollecting%20such%20data%20rely%20on%20human%20supervision%20or%20synthetic%20data%20generation%0Athrough%20executing%20pre-defined%20tasks%2C%20which%20are%20either%20resource-intensive%20or%0Aunable%20to%20guarantee%20data%20quality.%20Moreover%2C%20these%20methods%20suffer%20from%20limited%0Adata%20diversity%20and%20significant%20gaps%20between%20synthetic%20data%20and%20real-world%0Aenvironments.%20To%20address%20these%20challenges%2C%20we%20propose%20OS-Genesis%2C%20a%20novel%20GUI%0Adata%20synthesis%20pipeline%20that%20reverses%20the%20conventional%20trajectory%20collection%0Aprocess.%20Instead%20of%20relying%20on%20pre-defined%20tasks%2C%20OS-Genesis%20enables%20agents%0Afirst%20to%20perceive%20environments%20and%20perform%20step-wise%20interactions%2C%20then%0Aretrospectively%20derive%20high-quality%20tasks%20to%20enable%20trajectory-level%0Aexploration.%20A%20trajectory%20reward%20model%20is%20then%20employed%20to%20ensure%20the%20quality%0Aof%20the%20generated%20trajectories.%20We%20demonstrate%20that%20training%20GUI%20agents%20with%0AOS-Genesis%20significantly%20improves%20their%20performance%20on%20highly%20challenging%0Aonline%20benchmarks.%20In-depth%20analysis%20further%20validates%20OS-Genesis%27s%20efficiency%0Aand%20its%20superior%20data%20quality%20and%20diversity%20compared%20to%20existing%20synthesis%0Amethods.%20Our%20codes%2C%20data%2C%20and%20checkpoints%20are%20available%20at%0A%5Chref%7Bhttps%3A//qiushisun.github.io/OS-Genesis-Home/%7D%7BOS-Genesis%20Homepage%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOS-Genesis%253A%2520Automating%2520GUI%2520Agent%2520Trajectory%2520Construction%2520via%2520Reverse%250A%2520%2520Task%2520Synthesis%26entry.906535625%3DQiushi%2520Sun%2520and%2520Kanzhi%2520Cheng%2520and%2520Zichen%2520Ding%2520and%2520Chuanyang%2520Jin%2520and%2520Yian%2520Wang%2520and%2520Fangzhi%2520Xu%2520and%2520Zhenyu%2520Wu%2520and%2520Chengyou%2520Jia%2520and%2520Liheng%2520Chen%2520and%2520Zhoumianze%2520Liu%2520and%2520Ben%2520Kao%2520and%2520Guohao%2520Li%2520and%2520Junxian%2520He%2520and%2520Yu%2520Qiao%2520and%2520Zhiyong%2520Wu%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520powered%2520by%2520Vision-Language%2520Models%250A%2528VLMs%2529%2520have%2520demonstrated%2520human-like%2520computer%2520control%2520capability.%2520Despite%2520their%250Autility%2520in%2520advancing%2520digital%2520automation%252C%2520a%2520critical%2520bottleneck%2520persists%253A%250Acollecting%2520high-quality%2520trajectory%2520data%2520for%2520training.%2520Common%2520practices%2520for%250Acollecting%2520such%2520data%2520rely%2520on%2520human%2520supervision%2520or%2520synthetic%2520data%2520generation%250Athrough%2520executing%2520pre-defined%2520tasks%252C%2520which%2520are%2520either%2520resource-intensive%2520or%250Aunable%2520to%2520guarantee%2520data%2520quality.%2520Moreover%252C%2520these%2520methods%2520suffer%2520from%2520limited%250Adata%2520diversity%2520and%2520significant%2520gaps%2520between%2520synthetic%2520data%2520and%2520real-world%250Aenvironments.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520OS-Genesis%252C%2520a%2520novel%2520GUI%250Adata%2520synthesis%2520pipeline%2520that%2520reverses%2520the%2520conventional%2520trajectory%2520collection%250Aprocess.%2520Instead%2520of%2520relying%2520on%2520pre-defined%2520tasks%252C%2520OS-Genesis%2520enables%2520agents%250Afirst%2520to%2520perceive%2520environments%2520and%2520perform%2520step-wise%2520interactions%252C%2520then%250Aretrospectively%2520derive%2520high-quality%2520tasks%2520to%2520enable%2520trajectory-level%250Aexploration.%2520A%2520trajectory%2520reward%2520model%2520is%2520then%2520employed%2520to%2520ensure%2520the%2520quality%250Aof%2520the%2520generated%2520trajectories.%2520We%2520demonstrate%2520that%2520training%2520GUI%2520agents%2520with%250AOS-Genesis%2520significantly%2520improves%2520their%2520performance%2520on%2520highly%2520challenging%250Aonline%2520benchmarks.%2520In-depth%2520analysis%2520further%2520validates%2520OS-Genesis%2527s%2520efficiency%250Aand%2520its%2520superior%2520data%2520quality%2520and%2520diversity%2520compared%2520to%2520existing%2520synthesis%250Amethods.%2520Our%2520codes%252C%2520data%252C%2520and%2520checkpoints%2520are%2520available%2520at%250A%255Chref%257Bhttps%253A//qiushisun.github.io/OS-Genesis-Home/%257D%257BOS-Genesis%2520Homepage%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OS-Genesis%3A%20Automating%20GUI%20Agent%20Trajectory%20Construction%20via%20Reverse%0A%20%20Task%20Synthesis&entry.906535625=Qiushi%20Sun%20and%20Kanzhi%20Cheng%20and%20Zichen%20Ding%20and%20Chuanyang%20Jin%20and%20Yian%20Wang%20and%20Fangzhi%20Xu%20and%20Zhenyu%20Wu%20and%20Chengyou%20Jia%20and%20Liheng%20Chen%20and%20Zhoumianze%20Liu%20and%20Ben%20Kao%20and%20Guohao%20Li%20and%20Junxian%20He%20and%20Yu%20Qiao%20and%20Zhiyong%20Wu&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20agents%20powered%20by%20Vision-Language%20Models%0A%28VLMs%29%20have%20demonstrated%20human-like%20computer%20control%20capability.%20Despite%20their%0Autility%20in%20advancing%20digital%20automation%2C%20a%20critical%20bottleneck%20persists%3A%0Acollecting%20high-quality%20trajectory%20data%20for%20training.%20Common%20practices%20for%0Acollecting%20such%20data%20rely%20on%20human%20supervision%20or%20synthetic%20data%20generation%0Athrough%20executing%20pre-defined%20tasks%2C%20which%20are%20either%20resource-intensive%20or%0Aunable%20to%20guarantee%20data%20quality.%20Moreover%2C%20these%20methods%20suffer%20from%20limited%0Adata%20diversity%20and%20significant%20gaps%20between%20synthetic%20data%20and%20real-world%0Aenvironments.%20To%20address%20these%20challenges%2C%20we%20propose%20OS-Genesis%2C%20a%20novel%20GUI%0Adata%20synthesis%20pipeline%20that%20reverses%20the%20conventional%20trajectory%20collection%0Aprocess.%20Instead%20of%20relying%20on%20pre-defined%20tasks%2C%20OS-Genesis%20enables%20agents%0Afirst%20to%20perceive%20environments%20and%20perform%20step-wise%20interactions%2C%20then%0Aretrospectively%20derive%20high-quality%20tasks%20to%20enable%20trajectory-level%0Aexploration.%20A%20trajectory%20reward%20model%20is%20then%20employed%20to%20ensure%20the%20quality%0Aof%20the%20generated%20trajectories.%20We%20demonstrate%20that%20training%20GUI%20agents%20with%0AOS-Genesis%20significantly%20improves%20their%20performance%20on%20highly%20challenging%0Aonline%20benchmarks.%20In-depth%20analysis%20further%20validates%20OS-Genesis%27s%20efficiency%0Aand%20its%20superior%20data%20quality%20and%20diversity%20compared%20to%20existing%20synthesis%0Amethods.%20Our%20codes%2C%20data%2C%20and%20checkpoints%20are%20available%20at%0A%5Chref%7Bhttps%3A//qiushisun.github.io/OS-Genesis-Home/%7D%7BOS-Genesis%20Homepage%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19723v1&entry.124074799=Read"},
{"title": "Global Prediction of COVID-19 Variant Emergence Using Dynamics-Informed\n  Graph Neural Networks", "author": "Majd Al Aawar and Srikar Mutnuri and Mansooreh Montazerin and Ajitesh Srivastava", "abstract": "  During the COVID-19 pandemic, a major driver of new surges has been the\nemergence of new variants. When a new variant emerges in one or more countries,\nother nations monitor its spread in preparation for its potential arrival. The\nimpact of the new variant and the timings of epidemic peaks in a country highly\ndepend on when the variant arrives. The current methods for predicting the\nspread of new variants rely on statistical modeling, however, these methods\nwork only when the new variant has already arrived in the region of interest\nand has a significant prevalence. Can we predict when a variant existing\nelsewhere will arrive in a given region? To address this question, we propose a\nvariant-dynamics-informed Graph Neural Network (GNN) approach. First, we derive\nthe dynamics of variant prevalence across pairs of regions (countries) that\napply to a large class of epidemic models. The dynamics motivate the\nintroduction of certain features in the GNN. We demonstrate that our proposed\ndynamics-informed GNN outperforms all the baselines, including the currently\npervasive framework of Physics-Informed Neural Networks (PINNs). To advance\nresearch in this area, we introduce a benchmarking tool to assess a\nuser-defined model's prediction performance across 87 countries and 36\nvariants.\n", "link": "http://arxiv.org/abs/2401.03390v3", "date": "2024-12-27", "relevancy": 2.3084, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5185}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4343}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Prediction%20of%20COVID-19%20Variant%20Emergence%20Using%20Dynamics-Informed%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20Global%20Prediction%20of%20COVID-19%20Variant%20Emergence%20Using%20Dynamics-Informed%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20Majd%20Al%20Aawar%20and%20Srikar%20Mutnuri%20and%20Mansooreh%20Montazerin%20and%20Ajitesh%20Srivastava%0AAbstract%3A%20%20%20During%20the%20COVID-19%20pandemic%2C%20a%20major%20driver%20of%20new%20surges%20has%20been%20the%0Aemergence%20of%20new%20variants.%20When%20a%20new%20variant%20emerges%20in%20one%20or%20more%20countries%2C%0Aother%20nations%20monitor%20its%20spread%20in%20preparation%20for%20its%20potential%20arrival.%20The%0Aimpact%20of%20the%20new%20variant%20and%20the%20timings%20of%20epidemic%20peaks%20in%20a%20country%20highly%0Adepend%20on%20when%20the%20variant%20arrives.%20The%20current%20methods%20for%20predicting%20the%0Aspread%20of%20new%20variants%20rely%20on%20statistical%20modeling%2C%20however%2C%20these%20methods%0Awork%20only%20when%20the%20new%20variant%20has%20already%20arrived%20in%20the%20region%20of%20interest%0Aand%20has%20a%20significant%20prevalence.%20Can%20we%20predict%20when%20a%20variant%20existing%0Aelsewhere%20will%20arrive%20in%20a%20given%20region%3F%20To%20address%20this%20question%2C%20we%20propose%20a%0Avariant-dynamics-informed%20Graph%20Neural%20Network%20%28GNN%29%20approach.%20First%2C%20we%20derive%0Athe%20dynamics%20of%20variant%20prevalence%20across%20pairs%20of%20regions%20%28countries%29%20that%0Aapply%20to%20a%20large%20class%20of%20epidemic%20models.%20The%20dynamics%20motivate%20the%0Aintroduction%20of%20certain%20features%20in%20the%20GNN.%20We%20demonstrate%20that%20our%20proposed%0Adynamics-informed%20GNN%20outperforms%20all%20the%20baselines%2C%20including%20the%20currently%0Apervasive%20framework%20of%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20To%20advance%0Aresearch%20in%20this%20area%2C%20we%20introduce%20a%20benchmarking%20tool%20to%20assess%20a%0Auser-defined%20model%27s%20prediction%20performance%20across%2087%20countries%20and%2036%0Avariants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03390v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Prediction%2520of%2520COVID-19%2520Variant%2520Emergence%2520Using%2520Dynamics-Informed%250A%2520%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMajd%2520Al%2520Aawar%2520and%2520Srikar%2520Mutnuri%2520and%2520Mansooreh%2520Montazerin%2520and%2520Ajitesh%2520Srivastava%26entry.1292438233%3D%2520%2520During%2520the%2520COVID-19%2520pandemic%252C%2520a%2520major%2520driver%2520of%2520new%2520surges%2520has%2520been%2520the%250Aemergence%2520of%2520new%2520variants.%2520When%2520a%2520new%2520variant%2520emerges%2520in%2520one%2520or%2520more%2520countries%252C%250Aother%2520nations%2520monitor%2520its%2520spread%2520in%2520preparation%2520for%2520its%2520potential%2520arrival.%2520The%250Aimpact%2520of%2520the%2520new%2520variant%2520and%2520the%2520timings%2520of%2520epidemic%2520peaks%2520in%2520a%2520country%2520highly%250Adepend%2520on%2520when%2520the%2520variant%2520arrives.%2520The%2520current%2520methods%2520for%2520predicting%2520the%250Aspread%2520of%2520new%2520variants%2520rely%2520on%2520statistical%2520modeling%252C%2520however%252C%2520these%2520methods%250Awork%2520only%2520when%2520the%2520new%2520variant%2520has%2520already%2520arrived%2520in%2520the%2520region%2520of%2520interest%250Aand%2520has%2520a%2520significant%2520prevalence.%2520Can%2520we%2520predict%2520when%2520a%2520variant%2520existing%250Aelsewhere%2520will%2520arrive%2520in%2520a%2520given%2520region%253F%2520To%2520address%2520this%2520question%252C%2520we%2520propose%2520a%250Avariant-dynamics-informed%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520approach.%2520First%252C%2520we%2520derive%250Athe%2520dynamics%2520of%2520variant%2520prevalence%2520across%2520pairs%2520of%2520regions%2520%2528countries%2529%2520that%250Aapply%2520to%2520a%2520large%2520class%2520of%2520epidemic%2520models.%2520The%2520dynamics%2520motivate%2520the%250Aintroduction%2520of%2520certain%2520features%2520in%2520the%2520GNN.%2520We%2520demonstrate%2520that%2520our%2520proposed%250Adynamics-informed%2520GNN%2520outperforms%2520all%2520the%2520baselines%252C%2520including%2520the%2520currently%250Apervasive%2520framework%2520of%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529.%2520To%2520advance%250Aresearch%2520in%2520this%2520area%252C%2520we%2520introduce%2520a%2520benchmarking%2520tool%2520to%2520assess%2520a%250Auser-defined%2520model%2527s%2520prediction%2520performance%2520across%252087%2520countries%2520and%252036%250Avariants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.03390v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Prediction%20of%20COVID-19%20Variant%20Emergence%20Using%20Dynamics-Informed%0A%20%20Graph%20Neural%20Networks&entry.906535625=Majd%20Al%20Aawar%20and%20Srikar%20Mutnuri%20and%20Mansooreh%20Montazerin%20and%20Ajitesh%20Srivastava&entry.1292438233=%20%20During%20the%20COVID-19%20pandemic%2C%20a%20major%20driver%20of%20new%20surges%20has%20been%20the%0Aemergence%20of%20new%20variants.%20When%20a%20new%20variant%20emerges%20in%20one%20or%20more%20countries%2C%0Aother%20nations%20monitor%20its%20spread%20in%20preparation%20for%20its%20potential%20arrival.%20The%0Aimpact%20of%20the%20new%20variant%20and%20the%20timings%20of%20epidemic%20peaks%20in%20a%20country%20highly%0Adepend%20on%20when%20the%20variant%20arrives.%20The%20current%20methods%20for%20predicting%20the%0Aspread%20of%20new%20variants%20rely%20on%20statistical%20modeling%2C%20however%2C%20these%20methods%0Awork%20only%20when%20the%20new%20variant%20has%20already%20arrived%20in%20the%20region%20of%20interest%0Aand%20has%20a%20significant%20prevalence.%20Can%20we%20predict%20when%20a%20variant%20existing%0Aelsewhere%20will%20arrive%20in%20a%20given%20region%3F%20To%20address%20this%20question%2C%20we%20propose%20a%0Avariant-dynamics-informed%20Graph%20Neural%20Network%20%28GNN%29%20approach.%20First%2C%20we%20derive%0Athe%20dynamics%20of%20variant%20prevalence%20across%20pairs%20of%20regions%20%28countries%29%20that%0Aapply%20to%20a%20large%20class%20of%20epidemic%20models.%20The%20dynamics%20motivate%20the%0Aintroduction%20of%20certain%20features%20in%20the%20GNN.%20We%20demonstrate%20that%20our%20proposed%0Adynamics-informed%20GNN%20outperforms%20all%20the%20baselines%2C%20including%20the%20currently%0Apervasive%20framework%20of%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20To%20advance%0Aresearch%20in%20this%20area%2C%20we%20introduce%20a%20benchmarking%20tool%20to%20assess%20a%0Auser-defined%20model%27s%20prediction%20performance%20across%2087%20countries%20and%2036%0Avariants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03390v3&entry.124074799=Read"},
{"title": "EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs", "author": "Daniil A. Berdyshev and Artem M. Grachev and Sergei L. Shishkin and Bogdan L. Kozyrskiy", "abstract": "  Meta-learning, i.e., \"learning to learn\", is a promising approach to enable\nefficient BCI classifier training with limited amounts of data. It can\neffectively use collections of in some way similar classification tasks, with\nrapid adaptation to new tasks where only minimal data are available. However,\napplying meta-learning to existing classifiers and BCI tasks requires\nsignificant effort. To address this issue, we propose EEG-Reptile, an automated\nlibrary that leverages meta-learning to improve classification accuracy of\nneural networks in BCIs and other EEG-based applications. It utilizes the\nReptile meta-learning algorithm to adapt neural network classifiers of EEG data\nto the inter-subject domain, allowing for more efficient fine-tuning for a new\nsubject on a small amount of data. The proposed library incorporates an\nautomated hyperparameter tuning module, a data management pipeline, and an\nimplementation of the Reptile meta-learning algorithm. EEG-Reptile automation\nlevel allows using it without deep understanding of meta-learning. We\ndemonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV\n2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,\nEEG-Inception). Our library achieved improvement in both zero-shot and few-shot\nlearning scenarios compared to traditional transfer learning approaches.\n", "link": "http://arxiv.org/abs/2412.19725v1", "date": "2024-12-27", "relevancy": 2.3032, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4712}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4563}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-Reptile%3A%20An%20Automatized%20Reptile-Based%20Meta-Learning%20Library%20for%20BCIs&body=Title%3A%20EEG-Reptile%3A%20An%20Automatized%20Reptile-Based%20Meta-Learning%20Library%20for%20BCIs%0AAuthor%3A%20Daniil%20A.%20Berdyshev%20and%20Artem%20M.%20Grachev%20and%20Sergei%20L.%20Shishkin%20and%20Bogdan%20L.%20Kozyrskiy%0AAbstract%3A%20%20%20Meta-learning%2C%20i.e.%2C%20%22learning%20to%20learn%22%2C%20is%20a%20promising%20approach%20to%20enable%0Aefficient%20BCI%20classifier%20training%20with%20limited%20amounts%20of%20data.%20It%20can%0Aeffectively%20use%20collections%20of%20in%20some%20way%20similar%20classification%20tasks%2C%20with%0Arapid%20adaptation%20to%20new%20tasks%20where%20only%20minimal%20data%20are%20available.%20However%2C%0Aapplying%20meta-learning%20to%20existing%20classifiers%20and%20BCI%20tasks%20requires%0Asignificant%20effort.%20To%20address%20this%20issue%2C%20we%20propose%20EEG-Reptile%2C%20an%20automated%0Alibrary%20that%20leverages%20meta-learning%20to%20improve%20classification%20accuracy%20of%0Aneural%20networks%20in%20BCIs%20and%20other%20EEG-based%20applications.%20It%20utilizes%20the%0AReptile%20meta-learning%20algorithm%20to%20adapt%20neural%20network%20classifiers%20of%20EEG%20data%0Ato%20the%20inter-subject%20domain%2C%20allowing%20for%20more%20efficient%20fine-tuning%20for%20a%20new%0Asubject%20on%20a%20small%20amount%20of%20data.%20The%20proposed%20library%20incorporates%20an%0Aautomated%20hyperparameter%20tuning%20module%2C%20a%20data%20management%20pipeline%2C%20and%20an%0Aimplementation%20of%20the%20Reptile%20meta-learning%20algorithm.%20EEG-Reptile%20automation%0Alevel%20allows%20using%20it%20without%20deep%20understanding%20of%20meta-learning.%20We%0Ademonstrate%20the%20effectiveness%20of%20EEG-Reptile%20on%20two%20benchmark%20datasets%20%28BCI%20IV%0A2a%2C%20Lee2019%20MI%29%20and%20three%20neural%20network%20architectures%20%28EEGNet%2C%20FBCNet%2C%0AEEG-Inception%29.%20Our%20library%20achieved%20improvement%20in%20both%20zero-shot%20and%20few-shot%0Alearning%20scenarios%20compared%20to%20traditional%20transfer%20learning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19725v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-Reptile%253A%2520An%2520Automatized%2520Reptile-Based%2520Meta-Learning%2520Library%2520for%2520BCIs%26entry.906535625%3DDaniil%2520A.%2520Berdyshev%2520and%2520Artem%2520M.%2520Grachev%2520and%2520Sergei%2520L.%2520Shishkin%2520and%2520Bogdan%2520L.%2520Kozyrskiy%26entry.1292438233%3D%2520%2520Meta-learning%252C%2520i.e.%252C%2520%2522learning%2520to%2520learn%2522%252C%2520is%2520a%2520promising%2520approach%2520to%2520enable%250Aefficient%2520BCI%2520classifier%2520training%2520with%2520limited%2520amounts%2520of%2520data.%2520It%2520can%250Aeffectively%2520use%2520collections%2520of%2520in%2520some%2520way%2520similar%2520classification%2520tasks%252C%2520with%250Arapid%2520adaptation%2520to%2520new%2520tasks%2520where%2520only%2520minimal%2520data%2520are%2520available.%2520However%252C%250Aapplying%2520meta-learning%2520to%2520existing%2520classifiers%2520and%2520BCI%2520tasks%2520requires%250Asignificant%2520effort.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520EEG-Reptile%252C%2520an%2520automated%250Alibrary%2520that%2520leverages%2520meta-learning%2520to%2520improve%2520classification%2520accuracy%2520of%250Aneural%2520networks%2520in%2520BCIs%2520and%2520other%2520EEG-based%2520applications.%2520It%2520utilizes%2520the%250AReptile%2520meta-learning%2520algorithm%2520to%2520adapt%2520neural%2520network%2520classifiers%2520of%2520EEG%2520data%250Ato%2520the%2520inter-subject%2520domain%252C%2520allowing%2520for%2520more%2520efficient%2520fine-tuning%2520for%2520a%2520new%250Asubject%2520on%2520a%2520small%2520amount%2520of%2520data.%2520The%2520proposed%2520library%2520incorporates%2520an%250Aautomated%2520hyperparameter%2520tuning%2520module%252C%2520a%2520data%2520management%2520pipeline%252C%2520and%2520an%250Aimplementation%2520of%2520the%2520Reptile%2520meta-learning%2520algorithm.%2520EEG-Reptile%2520automation%250Alevel%2520allows%2520using%2520it%2520without%2520deep%2520understanding%2520of%2520meta-learning.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520EEG-Reptile%2520on%2520two%2520benchmark%2520datasets%2520%2528BCI%2520IV%250A2a%252C%2520Lee2019%2520MI%2529%2520and%2520three%2520neural%2520network%2520architectures%2520%2528EEGNet%252C%2520FBCNet%252C%250AEEG-Inception%2529.%2520Our%2520library%2520achieved%2520improvement%2520in%2520both%2520zero-shot%2520and%2520few-shot%250Alearning%2520scenarios%2520compared%2520to%2520traditional%2520transfer%2520learning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19725v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-Reptile%3A%20An%20Automatized%20Reptile-Based%20Meta-Learning%20Library%20for%20BCIs&entry.906535625=Daniil%20A.%20Berdyshev%20and%20Artem%20M.%20Grachev%20and%20Sergei%20L.%20Shishkin%20and%20Bogdan%20L.%20Kozyrskiy&entry.1292438233=%20%20Meta-learning%2C%20i.e.%2C%20%22learning%20to%20learn%22%2C%20is%20a%20promising%20approach%20to%20enable%0Aefficient%20BCI%20classifier%20training%20with%20limited%20amounts%20of%20data.%20It%20can%0Aeffectively%20use%20collections%20of%20in%20some%20way%20similar%20classification%20tasks%2C%20with%0Arapid%20adaptation%20to%20new%20tasks%20where%20only%20minimal%20data%20are%20available.%20However%2C%0Aapplying%20meta-learning%20to%20existing%20classifiers%20and%20BCI%20tasks%20requires%0Asignificant%20effort.%20To%20address%20this%20issue%2C%20we%20propose%20EEG-Reptile%2C%20an%20automated%0Alibrary%20that%20leverages%20meta-learning%20to%20improve%20classification%20accuracy%20of%0Aneural%20networks%20in%20BCIs%20and%20other%20EEG-based%20applications.%20It%20utilizes%20the%0AReptile%20meta-learning%20algorithm%20to%20adapt%20neural%20network%20classifiers%20of%20EEG%20data%0Ato%20the%20inter-subject%20domain%2C%20allowing%20for%20more%20efficient%20fine-tuning%20for%20a%20new%0Asubject%20on%20a%20small%20amount%20of%20data.%20The%20proposed%20library%20incorporates%20an%0Aautomated%20hyperparameter%20tuning%20module%2C%20a%20data%20management%20pipeline%2C%20and%20an%0Aimplementation%20of%20the%20Reptile%20meta-learning%20algorithm.%20EEG-Reptile%20automation%0Alevel%20allows%20using%20it%20without%20deep%20understanding%20of%20meta-learning.%20We%0Ademonstrate%20the%20effectiveness%20of%20EEG-Reptile%20on%20two%20benchmark%20datasets%20%28BCI%20IV%0A2a%2C%20Lee2019%20MI%29%20and%20three%20neural%20network%20architectures%20%28EEGNet%2C%20FBCNet%2C%0AEEG-Inception%29.%20Our%20library%20achieved%20improvement%20in%20both%20zero-shot%20and%20few-shot%0Alearning%20scenarios%20compared%20to%20traditional%20transfer%20learning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19725v1&entry.124074799=Read"},
{"title": "CAD-GPT: Synthesising CAD Construction Sequence with Spatial\n  Reasoning-Enhanced Multimodal LLMs", "author": "Siyu Wang and Cailian Chen and Xinyi Le and Qimin Xu and Lei Xu and Yanzhou Zhang and Jie Yang", "abstract": "  Computer-aided design (CAD) significantly enhances the efficiency, accuracy,\nand innovation of design processes by enabling precise 2D and 3D modeling,\nextensive analysis, and optimization. Existing methods for creating CAD models\nrely on latent vectors or point clouds, which are difficult to obtain and\ncostly to store. Recent advances in Multimodal Large Language Models (MLLMs)\nhave inspired researchers to use natural language instructions and images for\nCAD model construction. However, these models still struggle with inferring\naccurate 3D spatial location and orientation, leading to inaccuracies in\ndetermining the spatial 3D starting points and extrusion directions for\nconstructing geometries. This work introduces CAD-GPT, a CAD synthesis method\nwith spatial reasoning-enhanced MLLM that takes either a single image or a\ntextual description as input. To achieve precise spatial inference, our\napproach introduces a 3D Modeling Spatial Mechanism. This method maps 3D\nspatial positions and 3D sketch plane rotation angles into a 1D linguistic\nfeature space using a specialized spatial unfolding mechanism, while\ndiscretizing 2D sketch coordinates into an appropriate planar space to enable\nprecise determination of spatial starting position, sketch orientation, and 2D\nsketch coordinate translations. Extensive experiments demonstrate that CAD-GPT\nconsistently outperforms existing state-of-the-art methods in CAD model\nsynthesis, both quantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2412.19663v1", "date": "2024-12-27", "relevancy": 2.2199, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5654}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-GPT%3A%20Synthesising%20CAD%20Construction%20Sequence%20with%20Spatial%0A%20%20Reasoning-Enhanced%20Multimodal%20LLMs&body=Title%3A%20CAD-GPT%3A%20Synthesising%20CAD%20Construction%20Sequence%20with%20Spatial%0A%20%20Reasoning-Enhanced%20Multimodal%20LLMs%0AAuthor%3A%20Siyu%20Wang%20and%20Cailian%20Chen%20and%20Xinyi%20Le%20and%20Qimin%20Xu%20and%20Lei%20Xu%20and%20Yanzhou%20Zhang%20and%20Jie%20Yang%0AAbstract%3A%20%20%20Computer-aided%20design%20%28CAD%29%20significantly%20enhances%20the%20efficiency%2C%20accuracy%2C%0Aand%20innovation%20of%20design%20processes%20by%20enabling%20precise%202D%20and%203D%20modeling%2C%0Aextensive%20analysis%2C%20and%20optimization.%20Existing%20methods%20for%20creating%20CAD%20models%0Arely%20on%20latent%20vectors%20or%20point%20clouds%2C%20which%20are%20difficult%20to%20obtain%20and%0Acostly%20to%20store.%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Ahave%20inspired%20researchers%20to%20use%20natural%20language%20instructions%20and%20images%20for%0ACAD%20model%20construction.%20However%2C%20these%20models%20still%20struggle%20with%20inferring%0Aaccurate%203D%20spatial%20location%20and%20orientation%2C%20leading%20to%20inaccuracies%20in%0Adetermining%20the%20spatial%203D%20starting%20points%20and%20extrusion%20directions%20for%0Aconstructing%20geometries.%20This%20work%20introduces%20CAD-GPT%2C%20a%20CAD%20synthesis%20method%0Awith%20spatial%20reasoning-enhanced%20MLLM%20that%20takes%20either%20a%20single%20image%20or%20a%0Atextual%20description%20as%20input.%20To%20achieve%20precise%20spatial%20inference%2C%20our%0Aapproach%20introduces%20a%203D%20Modeling%20Spatial%20Mechanism.%20This%20method%20maps%203D%0Aspatial%20positions%20and%203D%20sketch%20plane%20rotation%20angles%20into%20a%201D%20linguistic%0Afeature%20space%20using%20a%20specialized%20spatial%20unfolding%20mechanism%2C%20while%0Adiscretizing%202D%20sketch%20coordinates%20into%20an%20appropriate%20planar%20space%20to%20enable%0Aprecise%20determination%20of%20spatial%20starting%20position%2C%20sketch%20orientation%2C%20and%202D%0Asketch%20coordinate%20translations.%20Extensive%20experiments%20demonstrate%20that%20CAD-GPT%0Aconsistently%20outperforms%20existing%20state-of-the-art%20methods%20in%20CAD%20model%0Asynthesis%2C%20both%20quantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-GPT%253A%2520Synthesising%2520CAD%2520Construction%2520Sequence%2520with%2520Spatial%250A%2520%2520Reasoning-Enhanced%2520Multimodal%2520LLMs%26entry.906535625%3DSiyu%2520Wang%2520and%2520Cailian%2520Chen%2520and%2520Xinyi%2520Le%2520and%2520Qimin%2520Xu%2520and%2520Lei%2520Xu%2520and%2520Yanzhou%2520Zhang%2520and%2520Jie%2520Yang%26entry.1292438233%3D%2520%2520Computer-aided%2520design%2520%2528CAD%2529%2520significantly%2520enhances%2520the%2520efficiency%252C%2520accuracy%252C%250Aand%2520innovation%2520of%2520design%2520processes%2520by%2520enabling%2520precise%25202D%2520and%25203D%2520modeling%252C%250Aextensive%2520analysis%252C%2520and%2520optimization.%2520Existing%2520methods%2520for%2520creating%2520CAD%2520models%250Arely%2520on%2520latent%2520vectors%2520or%2520point%2520clouds%252C%2520which%2520are%2520difficult%2520to%2520obtain%2520and%250Acostly%2520to%2520store.%2520Recent%2520advances%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%250Ahave%2520inspired%2520researchers%2520to%2520use%2520natural%2520language%2520instructions%2520and%2520images%2520for%250ACAD%2520model%2520construction.%2520However%252C%2520these%2520models%2520still%2520struggle%2520with%2520inferring%250Aaccurate%25203D%2520spatial%2520location%2520and%2520orientation%252C%2520leading%2520to%2520inaccuracies%2520in%250Adetermining%2520the%2520spatial%25203D%2520starting%2520points%2520and%2520extrusion%2520directions%2520for%250Aconstructing%2520geometries.%2520This%2520work%2520introduces%2520CAD-GPT%252C%2520a%2520CAD%2520synthesis%2520method%250Awith%2520spatial%2520reasoning-enhanced%2520MLLM%2520that%2520takes%2520either%2520a%2520single%2520image%2520or%2520a%250Atextual%2520description%2520as%2520input.%2520To%2520achieve%2520precise%2520spatial%2520inference%252C%2520our%250Aapproach%2520introduces%2520a%25203D%2520Modeling%2520Spatial%2520Mechanism.%2520This%2520method%2520maps%25203D%250Aspatial%2520positions%2520and%25203D%2520sketch%2520plane%2520rotation%2520angles%2520into%2520a%25201D%2520linguistic%250Afeature%2520space%2520using%2520a%2520specialized%2520spatial%2520unfolding%2520mechanism%252C%2520while%250Adiscretizing%25202D%2520sketch%2520coordinates%2520into%2520an%2520appropriate%2520planar%2520space%2520to%2520enable%250Aprecise%2520determination%2520of%2520spatial%2520starting%2520position%252C%2520sketch%2520orientation%252C%2520and%25202D%250Asketch%2520coordinate%2520translations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CAD-GPT%250Aconsistently%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520CAD%2520model%250Asynthesis%252C%2520both%2520quantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-GPT%3A%20Synthesising%20CAD%20Construction%20Sequence%20with%20Spatial%0A%20%20Reasoning-Enhanced%20Multimodal%20LLMs&entry.906535625=Siyu%20Wang%20and%20Cailian%20Chen%20and%20Xinyi%20Le%20and%20Qimin%20Xu%20and%20Lei%20Xu%20and%20Yanzhou%20Zhang%20and%20Jie%20Yang&entry.1292438233=%20%20Computer-aided%20design%20%28CAD%29%20significantly%20enhances%20the%20efficiency%2C%20accuracy%2C%0Aand%20innovation%20of%20design%20processes%20by%20enabling%20precise%202D%20and%203D%20modeling%2C%0Aextensive%20analysis%2C%20and%20optimization.%20Existing%20methods%20for%20creating%20CAD%20models%0Arely%20on%20latent%20vectors%20or%20point%20clouds%2C%20which%20are%20difficult%20to%20obtain%20and%0Acostly%20to%20store.%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Ahave%20inspired%20researchers%20to%20use%20natural%20language%20instructions%20and%20images%20for%0ACAD%20model%20construction.%20However%2C%20these%20models%20still%20struggle%20with%20inferring%0Aaccurate%203D%20spatial%20location%20and%20orientation%2C%20leading%20to%20inaccuracies%20in%0Adetermining%20the%20spatial%203D%20starting%20points%20and%20extrusion%20directions%20for%0Aconstructing%20geometries.%20This%20work%20introduces%20CAD-GPT%2C%20a%20CAD%20synthesis%20method%0Awith%20spatial%20reasoning-enhanced%20MLLM%20that%20takes%20either%20a%20single%20image%20or%20a%0Atextual%20description%20as%20input.%20To%20achieve%20precise%20spatial%20inference%2C%20our%0Aapproach%20introduces%20a%203D%20Modeling%20Spatial%20Mechanism.%20This%20method%20maps%203D%0Aspatial%20positions%20and%203D%20sketch%20plane%20rotation%20angles%20into%20a%201D%20linguistic%0Afeature%20space%20using%20a%20specialized%20spatial%20unfolding%20mechanism%2C%20while%0Adiscretizing%202D%20sketch%20coordinates%20into%20an%20appropriate%20planar%20space%20to%20enable%0Aprecise%20determination%20of%20spatial%20starting%20position%2C%20sketch%20orientation%2C%20and%202D%0Asketch%20coordinate%20translations.%20Extensive%20experiments%20demonstrate%20that%20CAD-GPT%0Aconsistently%20outperforms%20existing%20state-of-the-art%20methods%20in%20CAD%20model%0Asynthesis%2C%20both%20quantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19663v1&entry.124074799=Read"},
{"title": "A Large-scale Interpretable Multi-modality Benchmark for Facial Image\n  Forgery Localization", "author": "Jingchun Lian and Lingyu Liu and Yaxiong Wang and Yujiao Wu and Li Zhu and Zhedong Zheng", "abstract": "  Image forgery localization, which centers on identifying tampered pixels\nwithin an image, has seen significant advancements. Traditional approaches\noften model this challenge as a variant of image segmentation, treating the\nbinary segmentation of forged areas as the end product. We argue that the basic\nbinary forgery mask is inadequate for explaining model predictions. It doesn't\nclarify why the model pinpoints certain areas and treats all forged pixels the\nsame, making it hard to spot the most fake-looking parts. In this study, we\nmitigate the aforementioned limitations by generating salient region-focused\ninterpretation for the forgery images. To support this, we craft a Multi-Modal\nTramper Tracing (MMTT) dataset, comprising facial images manipulated using\ndeepfake techniques and paired with manual, interpretable textual annotations.\nTo harvest high-quality annotation, annotators are instructed to meticulously\nobserve the manipulated images and articulate the typical characteristics of\nthe forgery regions. Subsequently, we collect a dataset of 128,303 image-text\npairs. Leveraging the MMTT dataset, we develop ForgeryTalker, an architecture\ndesigned for concurrent forgery localization and interpretation. ForgeryTalker\nfirst trains a forgery prompter network to identify the pivotal clues within\nthe explanatory text. Subsequently, the region prompter is incorporated into\nmultimodal large language model for finetuning to achieve the dual goals of\nlocalization and interpretation. Extensive experiments conducted on the MMTT\ndataset verify the superior performance of our proposed model. The dataset,\ncode as well as pretrained checkpoints will be made publicly available to\nfacilitate further research and ensure the reproducibility of our results.\n", "link": "http://arxiv.org/abs/2412.19685v1", "date": "2024-12-27", "relevancy": 2.2147, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.556}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-scale%20Interpretable%20Multi-modality%20Benchmark%20for%20Facial%20Image%0A%20%20Forgery%20Localization&body=Title%3A%20A%20Large-scale%20Interpretable%20Multi-modality%20Benchmark%20for%20Facial%20Image%0A%20%20Forgery%20Localization%0AAuthor%3A%20Jingchun%20Lian%20and%20Lingyu%20Liu%20and%20Yaxiong%20Wang%20and%20Yujiao%20Wu%20and%20Li%20Zhu%20and%20Zhedong%20Zheng%0AAbstract%3A%20%20%20Image%20forgery%20localization%2C%20which%20centers%20on%20identifying%20tampered%20pixels%0Awithin%20an%20image%2C%20has%20seen%20significant%20advancements.%20Traditional%20approaches%0Aoften%20model%20this%20challenge%20as%20a%20variant%20of%20image%20segmentation%2C%20treating%20the%0Abinary%20segmentation%20of%20forged%20areas%20as%20the%20end%20product.%20We%20argue%20that%20the%20basic%0Abinary%20forgery%20mask%20is%20inadequate%20for%20explaining%20model%20predictions.%20It%20doesn%27t%0Aclarify%20why%20the%20model%20pinpoints%20certain%20areas%20and%20treats%20all%20forged%20pixels%20the%0Asame%2C%20making%20it%20hard%20to%20spot%20the%20most%20fake-looking%20parts.%20In%20this%20study%2C%20we%0Amitigate%20the%20aforementioned%20limitations%20by%20generating%20salient%20region-focused%0Ainterpretation%20for%20the%20forgery%20images.%20To%20support%20this%2C%20we%20craft%20a%20Multi-Modal%0ATramper%20Tracing%20%28MMTT%29%20dataset%2C%20comprising%20facial%20images%20manipulated%20using%0Adeepfake%20techniques%20and%20paired%20with%20manual%2C%20interpretable%20textual%20annotations.%0ATo%20harvest%20high-quality%20annotation%2C%20annotators%20are%20instructed%20to%20meticulously%0Aobserve%20the%20manipulated%20images%20and%20articulate%20the%20typical%20characteristics%20of%0Athe%20forgery%20regions.%20Subsequently%2C%20we%20collect%20a%20dataset%20of%20128%2C303%20image-text%0Apairs.%20Leveraging%20the%20MMTT%20dataset%2C%20we%20develop%20ForgeryTalker%2C%20an%20architecture%0Adesigned%20for%20concurrent%20forgery%20localization%20and%20interpretation.%20ForgeryTalker%0Afirst%20trains%20a%20forgery%20prompter%20network%20to%20identify%20the%20pivotal%20clues%20within%0Athe%20explanatory%20text.%20Subsequently%2C%20the%20region%20prompter%20is%20incorporated%20into%0Amultimodal%20large%20language%20model%20for%20finetuning%20to%20achieve%20the%20dual%20goals%20of%0Alocalization%20and%20interpretation.%20Extensive%20experiments%20conducted%20on%20the%20MMTT%0Adataset%20verify%20the%20superior%20performance%20of%20our%20proposed%20model.%20The%20dataset%2C%0Acode%20as%20well%20as%20pretrained%20checkpoints%20will%20be%20made%20publicly%20available%20to%0Afacilitate%20further%20research%20and%20ensure%20the%20reproducibility%20of%20our%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-scale%2520Interpretable%2520Multi-modality%2520Benchmark%2520for%2520Facial%2520Image%250A%2520%2520Forgery%2520Localization%26entry.906535625%3DJingchun%2520Lian%2520and%2520Lingyu%2520Liu%2520and%2520Yaxiong%2520Wang%2520and%2520Yujiao%2520Wu%2520and%2520Li%2520Zhu%2520and%2520Zhedong%2520Zheng%26entry.1292438233%3D%2520%2520Image%2520forgery%2520localization%252C%2520which%2520centers%2520on%2520identifying%2520tampered%2520pixels%250Awithin%2520an%2520image%252C%2520has%2520seen%2520significant%2520advancements.%2520Traditional%2520approaches%250Aoften%2520model%2520this%2520challenge%2520as%2520a%2520variant%2520of%2520image%2520segmentation%252C%2520treating%2520the%250Abinary%2520segmentation%2520of%2520forged%2520areas%2520as%2520the%2520end%2520product.%2520We%2520argue%2520that%2520the%2520basic%250Abinary%2520forgery%2520mask%2520is%2520inadequate%2520for%2520explaining%2520model%2520predictions.%2520It%2520doesn%2527t%250Aclarify%2520why%2520the%2520model%2520pinpoints%2520certain%2520areas%2520and%2520treats%2520all%2520forged%2520pixels%2520the%250Asame%252C%2520making%2520it%2520hard%2520to%2520spot%2520the%2520most%2520fake-looking%2520parts.%2520In%2520this%2520study%252C%2520we%250Amitigate%2520the%2520aforementioned%2520limitations%2520by%2520generating%2520salient%2520region-focused%250Ainterpretation%2520for%2520the%2520forgery%2520images.%2520To%2520support%2520this%252C%2520we%2520craft%2520a%2520Multi-Modal%250ATramper%2520Tracing%2520%2528MMTT%2529%2520dataset%252C%2520comprising%2520facial%2520images%2520manipulated%2520using%250Adeepfake%2520techniques%2520and%2520paired%2520with%2520manual%252C%2520interpretable%2520textual%2520annotations.%250ATo%2520harvest%2520high-quality%2520annotation%252C%2520annotators%2520are%2520instructed%2520to%2520meticulously%250Aobserve%2520the%2520manipulated%2520images%2520and%2520articulate%2520the%2520typical%2520characteristics%2520of%250Athe%2520forgery%2520regions.%2520Subsequently%252C%2520we%2520collect%2520a%2520dataset%2520of%2520128%252C303%2520image-text%250Apairs.%2520Leveraging%2520the%2520MMTT%2520dataset%252C%2520we%2520develop%2520ForgeryTalker%252C%2520an%2520architecture%250Adesigned%2520for%2520concurrent%2520forgery%2520localization%2520and%2520interpretation.%2520ForgeryTalker%250Afirst%2520trains%2520a%2520forgery%2520prompter%2520network%2520to%2520identify%2520the%2520pivotal%2520clues%2520within%250Athe%2520explanatory%2520text.%2520Subsequently%252C%2520the%2520region%2520prompter%2520is%2520incorporated%2520into%250Amultimodal%2520large%2520language%2520model%2520for%2520finetuning%2520to%2520achieve%2520the%2520dual%2520goals%2520of%250Alocalization%2520and%2520interpretation.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520MMTT%250Adataset%2520verify%2520the%2520superior%2520performance%2520of%2520our%2520proposed%2520model.%2520The%2520dataset%252C%250Acode%2520as%2520well%2520as%2520pretrained%2520checkpoints%2520will%2520be%2520made%2520publicly%2520available%2520to%250Afacilitate%2520further%2520research%2520and%2520ensure%2520the%2520reproducibility%2520of%2520our%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-scale%20Interpretable%20Multi-modality%20Benchmark%20for%20Facial%20Image%0A%20%20Forgery%20Localization&entry.906535625=Jingchun%20Lian%20and%20Lingyu%20Liu%20and%20Yaxiong%20Wang%20and%20Yujiao%20Wu%20and%20Li%20Zhu%20and%20Zhedong%20Zheng&entry.1292438233=%20%20Image%20forgery%20localization%2C%20which%20centers%20on%20identifying%20tampered%20pixels%0Awithin%20an%20image%2C%20has%20seen%20significant%20advancements.%20Traditional%20approaches%0Aoften%20model%20this%20challenge%20as%20a%20variant%20of%20image%20segmentation%2C%20treating%20the%0Abinary%20segmentation%20of%20forged%20areas%20as%20the%20end%20product.%20We%20argue%20that%20the%20basic%0Abinary%20forgery%20mask%20is%20inadequate%20for%20explaining%20model%20predictions.%20It%20doesn%27t%0Aclarify%20why%20the%20model%20pinpoints%20certain%20areas%20and%20treats%20all%20forged%20pixels%20the%0Asame%2C%20making%20it%20hard%20to%20spot%20the%20most%20fake-looking%20parts.%20In%20this%20study%2C%20we%0Amitigate%20the%20aforementioned%20limitations%20by%20generating%20salient%20region-focused%0Ainterpretation%20for%20the%20forgery%20images.%20To%20support%20this%2C%20we%20craft%20a%20Multi-Modal%0ATramper%20Tracing%20%28MMTT%29%20dataset%2C%20comprising%20facial%20images%20manipulated%20using%0Adeepfake%20techniques%20and%20paired%20with%20manual%2C%20interpretable%20textual%20annotations.%0ATo%20harvest%20high-quality%20annotation%2C%20annotators%20are%20instructed%20to%20meticulously%0Aobserve%20the%20manipulated%20images%20and%20articulate%20the%20typical%20characteristics%20of%0Athe%20forgery%20regions.%20Subsequently%2C%20we%20collect%20a%20dataset%20of%20128%2C303%20image-text%0Apairs.%20Leveraging%20the%20MMTT%20dataset%2C%20we%20develop%20ForgeryTalker%2C%20an%20architecture%0Adesigned%20for%20concurrent%20forgery%20localization%20and%20interpretation.%20ForgeryTalker%0Afirst%20trains%20a%20forgery%20prompter%20network%20to%20identify%20the%20pivotal%20clues%20within%0Athe%20explanatory%20text.%20Subsequently%2C%20the%20region%20prompter%20is%20incorporated%20into%0Amultimodal%20large%20language%20model%20for%20finetuning%20to%20achieve%20the%20dual%20goals%20of%0Alocalization%20and%20interpretation.%20Extensive%20experiments%20conducted%20on%20the%20MMTT%0Adataset%20verify%20the%20superior%20performance%20of%20our%20proposed%20model.%20The%20dataset%2C%0Acode%20as%20well%20as%20pretrained%20checkpoints%20will%20be%20made%20publicly%20available%20to%0Afacilitate%20further%20research%20and%20ensure%20the%20reproducibility%20of%20our%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19685v1&entry.124074799=Read"},
{"title": "Enhancing Vision-Language Tracking by Effectively Converting Textual\n  Cues into Visual Cues", "author": "X. Feng and D. Zhang and S. Hu and X. Li and M. Wu and J. Zhang and X. Chen and K. Huang", "abstract": "  Vision-Language Tracking (VLT) aims to localize a target in video sequences\nusing a visual template and language description. While textual cues enhance\ntracking potential, current datasets typically contain much more image data\nthan text, limiting the ability of VLT methods to align the two modalities\neffectively. To address this imbalance, we propose a novel plug-and-play method\nnamed CTVLT that leverages the strong text-image alignment capabilities of\nfoundation grounding models. CTVLT converts textual cues into interpretable\nvisual heatmaps, which are easier for trackers to process. Specifically, we\ndesign a textual cue mapping module that transforms textual cues into target\ndistribution heatmaps, visually representing the location described by the\ntext. Additionally, the heatmap guidance module fuses these heatmaps with the\nsearch image to guide tracking more effectively. Extensive experiments on\nmainstream benchmarks demonstrate the effectiveness of our approach, achieving\nstate-of-the-art performance and validating the utility of our method for\nenhanced VLT.\n", "link": "http://arxiv.org/abs/2412.19648v1", "date": "2024-12-27", "relevancy": 2.208, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5554}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vision-Language%20Tracking%20by%20Effectively%20Converting%20Textual%0A%20%20Cues%20into%20Visual%20Cues&body=Title%3A%20Enhancing%20Vision-Language%20Tracking%20by%20Effectively%20Converting%20Textual%0A%20%20Cues%20into%20Visual%20Cues%0AAuthor%3A%20X.%20Feng%20and%20D.%20Zhang%20and%20S.%20Hu%20and%20X.%20Li%20and%20M.%20Wu%20and%20J.%20Zhang%20and%20X.%20Chen%20and%20K.%20Huang%0AAbstract%3A%20%20%20Vision-Language%20Tracking%20%28VLT%29%20aims%20to%20localize%20a%20target%20in%20video%20sequences%0Ausing%20a%20visual%20template%20and%20language%20description.%20While%20textual%20cues%20enhance%0Atracking%20potential%2C%20current%20datasets%20typically%20contain%20much%20more%20image%20data%0Athan%20text%2C%20limiting%20the%20ability%20of%20VLT%20methods%20to%20align%20the%20two%20modalities%0Aeffectively.%20To%20address%20this%20imbalance%2C%20we%20propose%20a%20novel%20plug-and-play%20method%0Anamed%20CTVLT%20that%20leverages%20the%20strong%20text-image%20alignment%20capabilities%20of%0Afoundation%20grounding%20models.%20CTVLT%20converts%20textual%20cues%20into%20interpretable%0Avisual%20heatmaps%2C%20which%20are%20easier%20for%20trackers%20to%20process.%20Specifically%2C%20we%0Adesign%20a%20textual%20cue%20mapping%20module%20that%20transforms%20textual%20cues%20into%20target%0Adistribution%20heatmaps%2C%20visually%20representing%20the%20location%20described%20by%20the%0Atext.%20Additionally%2C%20the%20heatmap%20guidance%20module%20fuses%20these%20heatmaps%20with%20the%0Asearch%20image%20to%20guide%20tracking%20more%20effectively.%20Extensive%20experiments%20on%0Amainstream%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20achieving%0Astate-of-the-art%20performance%20and%20validating%20the%20utility%20of%20our%20method%20for%0Aenhanced%20VLT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vision-Language%2520Tracking%2520by%2520Effectively%2520Converting%2520Textual%250A%2520%2520Cues%2520into%2520Visual%2520Cues%26entry.906535625%3DX.%2520Feng%2520and%2520D.%2520Zhang%2520and%2520S.%2520Hu%2520and%2520X.%2520Li%2520and%2520M.%2520Wu%2520and%2520J.%2520Zhang%2520and%2520X.%2520Chen%2520and%2520K.%2520Huang%26entry.1292438233%3D%2520%2520Vision-Language%2520Tracking%2520%2528VLT%2529%2520aims%2520to%2520localize%2520a%2520target%2520in%2520video%2520sequences%250Ausing%2520a%2520visual%2520template%2520and%2520language%2520description.%2520While%2520textual%2520cues%2520enhance%250Atracking%2520potential%252C%2520current%2520datasets%2520typically%2520contain%2520much%2520more%2520image%2520data%250Athan%2520text%252C%2520limiting%2520the%2520ability%2520of%2520VLT%2520methods%2520to%2520align%2520the%2520two%2520modalities%250Aeffectively.%2520To%2520address%2520this%2520imbalance%252C%2520we%2520propose%2520a%2520novel%2520plug-and-play%2520method%250Anamed%2520CTVLT%2520that%2520leverages%2520the%2520strong%2520text-image%2520alignment%2520capabilities%2520of%250Afoundation%2520grounding%2520models.%2520CTVLT%2520converts%2520textual%2520cues%2520into%2520interpretable%250Avisual%2520heatmaps%252C%2520which%2520are%2520easier%2520for%2520trackers%2520to%2520process.%2520Specifically%252C%2520we%250Adesign%2520a%2520textual%2520cue%2520mapping%2520module%2520that%2520transforms%2520textual%2520cues%2520into%2520target%250Adistribution%2520heatmaps%252C%2520visually%2520representing%2520the%2520location%2520described%2520by%2520the%250Atext.%2520Additionally%252C%2520the%2520heatmap%2520guidance%2520module%2520fuses%2520these%2520heatmaps%2520with%2520the%250Asearch%2520image%2520to%2520guide%2520tracking%2520more%2520effectively.%2520Extensive%2520experiments%2520on%250Amainstream%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520achieving%250Astate-of-the-art%2520performance%2520and%2520validating%2520the%2520utility%2520of%2520our%2520method%2520for%250Aenhanced%2520VLT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vision-Language%20Tracking%20by%20Effectively%20Converting%20Textual%0A%20%20Cues%20into%20Visual%20Cues&entry.906535625=X.%20Feng%20and%20D.%20Zhang%20and%20S.%20Hu%20and%20X.%20Li%20and%20M.%20Wu%20and%20J.%20Zhang%20and%20X.%20Chen%20and%20K.%20Huang&entry.1292438233=%20%20Vision-Language%20Tracking%20%28VLT%29%20aims%20to%20localize%20a%20target%20in%20video%20sequences%0Ausing%20a%20visual%20template%20and%20language%20description.%20While%20textual%20cues%20enhance%0Atracking%20potential%2C%20current%20datasets%20typically%20contain%20much%20more%20image%20data%0Athan%20text%2C%20limiting%20the%20ability%20of%20VLT%20methods%20to%20align%20the%20two%20modalities%0Aeffectively.%20To%20address%20this%20imbalance%2C%20we%20propose%20a%20novel%20plug-and-play%20method%0Anamed%20CTVLT%20that%20leverages%20the%20strong%20text-image%20alignment%20capabilities%20of%0Afoundation%20grounding%20models.%20CTVLT%20converts%20textual%20cues%20into%20interpretable%0Avisual%20heatmaps%2C%20which%20are%20easier%20for%20trackers%20to%20process.%20Specifically%2C%20we%0Adesign%20a%20textual%20cue%20mapping%20module%20that%20transforms%20textual%20cues%20into%20target%0Adistribution%20heatmaps%2C%20visually%20representing%20the%20location%20described%20by%20the%0Atext.%20Additionally%2C%20the%20heatmap%20guidance%20module%20fuses%20these%20heatmaps%20with%20the%0Asearch%20image%20to%20guide%20tracking%20more%20effectively.%20Extensive%20experiments%20on%0Amainstream%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20achieving%0Astate-of-the-art%20performance%20and%20validating%20the%20utility%20of%20our%20method%20for%0Aenhanced%20VLT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19648v1&entry.124074799=Read"},
{"title": "Generation through the lens of learning theory", "author": "Jiaxun Li and Vinod Raman and Ambuj Tewari", "abstract": "  We study generation through the lens of statistical learning theory. First,\nwe abstract and formalize the results of Gold [1967], Angluin [1979], Angluin\n[1980] and Kleinberg and Mullainathan [2024] in terms of a binary hypothesis\nclass defined over an abstract example space. Then, we extend the notion of\n\"generation\" from Kleinberg and Mullainathan [2024] to two new settings, we\ncall \"uniform\" and \"non-uniform\" generation, and provide a characterization of\nwhich hypothesis classes are uniformly and non-uniformly generatable. As is\nstandard in learning theory, our characterizations are in terms of the\nfiniteness of a new combinatorial dimension termed the Closure dimension. By\ndoing so, we are able to compare generatability with predictability (captured\nvia PAC and online learnability) and show that these two properties of\nhypothesis classes are incompatible -- there are classes that are generatable\nbut not predictable and vice versa. Finally, we extend our results to capture\nprompted generation and give a complete characterization of which classes are\nprompt generatable, generalizing some of the work by Kleinberg and Mullainathan\n[2024].\n", "link": "http://arxiv.org/abs/2410.13714v5", "date": "2024-12-27", "relevancy": 2.1626, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5774}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5245}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20through%20the%20lens%20of%20learning%20theory&body=Title%3A%20Generation%20through%20the%20lens%20of%20learning%20theory%0AAuthor%3A%20Jiaxun%20Li%20and%20Vinod%20Raman%20and%20Ambuj%20Tewari%0AAbstract%3A%20%20%20We%20study%20generation%20through%20the%20lens%20of%20statistical%20learning%20theory.%20First%2C%0Awe%20abstract%20and%20formalize%20the%20results%20of%20Gold%20%5B1967%5D%2C%20Angluin%20%5B1979%5D%2C%20Angluin%0A%5B1980%5D%20and%20Kleinberg%20and%20Mullainathan%20%5B2024%5D%20in%20terms%20of%20a%20binary%20hypothesis%0Aclass%20defined%20over%20an%20abstract%20example%20space.%20Then%2C%20we%20extend%20the%20notion%20of%0A%22generation%22%20from%20Kleinberg%20and%20Mullainathan%20%5B2024%5D%20to%20two%20new%20settings%2C%20we%0Acall%20%22uniform%22%20and%20%22non-uniform%22%20generation%2C%20and%20provide%20a%20characterization%20of%0Awhich%20hypothesis%20classes%20are%20uniformly%20and%20non-uniformly%20generatable.%20As%20is%0Astandard%20in%20learning%20theory%2C%20our%20characterizations%20are%20in%20terms%20of%20the%0Afiniteness%20of%20a%20new%20combinatorial%20dimension%20termed%20the%20Closure%20dimension.%20By%0Adoing%20so%2C%20we%20are%20able%20to%20compare%20generatability%20with%20predictability%20%28captured%0Avia%20PAC%20and%20online%20learnability%29%20and%20show%20that%20these%20two%20properties%20of%0Ahypothesis%20classes%20are%20incompatible%20--%20there%20are%20classes%20that%20are%20generatable%0Abut%20not%20predictable%20and%20vice%20versa.%20Finally%2C%20we%20extend%20our%20results%20to%20capture%0Aprompted%20generation%20and%20give%20a%20complete%20characterization%20of%20which%20classes%20are%0Aprompt%20generatable%2C%20generalizing%20some%20of%20the%20work%20by%20Kleinberg%20and%20Mullainathan%0A%5B2024%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13714v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520through%2520the%2520lens%2520of%2520learning%2520theory%26entry.906535625%3DJiaxun%2520Li%2520and%2520Vinod%2520Raman%2520and%2520Ambuj%2520Tewari%26entry.1292438233%3D%2520%2520We%2520study%2520generation%2520through%2520the%2520lens%2520of%2520statistical%2520learning%2520theory.%2520First%252C%250Awe%2520abstract%2520and%2520formalize%2520the%2520results%2520of%2520Gold%2520%255B1967%255D%252C%2520Angluin%2520%255B1979%255D%252C%2520Angluin%250A%255B1980%255D%2520and%2520Kleinberg%2520and%2520Mullainathan%2520%255B2024%255D%2520in%2520terms%2520of%2520a%2520binary%2520hypothesis%250Aclass%2520defined%2520over%2520an%2520abstract%2520example%2520space.%2520Then%252C%2520we%2520extend%2520the%2520notion%2520of%250A%2522generation%2522%2520from%2520Kleinberg%2520and%2520Mullainathan%2520%255B2024%255D%2520to%2520two%2520new%2520settings%252C%2520we%250Acall%2520%2522uniform%2522%2520and%2520%2522non-uniform%2522%2520generation%252C%2520and%2520provide%2520a%2520characterization%2520of%250Awhich%2520hypothesis%2520classes%2520are%2520uniformly%2520and%2520non-uniformly%2520generatable.%2520As%2520is%250Astandard%2520in%2520learning%2520theory%252C%2520our%2520characterizations%2520are%2520in%2520terms%2520of%2520the%250Afiniteness%2520of%2520a%2520new%2520combinatorial%2520dimension%2520termed%2520the%2520Closure%2520dimension.%2520By%250Adoing%2520so%252C%2520we%2520are%2520able%2520to%2520compare%2520generatability%2520with%2520predictability%2520%2528captured%250Avia%2520PAC%2520and%2520online%2520learnability%2529%2520and%2520show%2520that%2520these%2520two%2520properties%2520of%250Ahypothesis%2520classes%2520are%2520incompatible%2520--%2520there%2520are%2520classes%2520that%2520are%2520generatable%250Abut%2520not%2520predictable%2520and%2520vice%2520versa.%2520Finally%252C%2520we%2520extend%2520our%2520results%2520to%2520capture%250Aprompted%2520generation%2520and%2520give%2520a%2520complete%2520characterization%2520of%2520which%2520classes%2520are%250Aprompt%2520generatable%252C%2520generalizing%2520some%2520of%2520the%2520work%2520by%2520Kleinberg%2520and%2520Mullainathan%250A%255B2024%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13714v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20through%20the%20lens%20of%20learning%20theory&entry.906535625=Jiaxun%20Li%20and%20Vinod%20Raman%20and%20Ambuj%20Tewari&entry.1292438233=%20%20We%20study%20generation%20through%20the%20lens%20of%20statistical%20learning%20theory.%20First%2C%0Awe%20abstract%20and%20formalize%20the%20results%20of%20Gold%20%5B1967%5D%2C%20Angluin%20%5B1979%5D%2C%20Angluin%0A%5B1980%5D%20and%20Kleinberg%20and%20Mullainathan%20%5B2024%5D%20in%20terms%20of%20a%20binary%20hypothesis%0Aclass%20defined%20over%20an%20abstract%20example%20space.%20Then%2C%20we%20extend%20the%20notion%20of%0A%22generation%22%20from%20Kleinberg%20and%20Mullainathan%20%5B2024%5D%20to%20two%20new%20settings%2C%20we%0Acall%20%22uniform%22%20and%20%22non-uniform%22%20generation%2C%20and%20provide%20a%20characterization%20of%0Awhich%20hypothesis%20classes%20are%20uniformly%20and%20non-uniformly%20generatable.%20As%20is%0Astandard%20in%20learning%20theory%2C%20our%20characterizations%20are%20in%20terms%20of%20the%0Afiniteness%20of%20a%20new%20combinatorial%20dimension%20termed%20the%20Closure%20dimension.%20By%0Adoing%20so%2C%20we%20are%20able%20to%20compare%20generatability%20with%20predictability%20%28captured%0Avia%20PAC%20and%20online%20learnability%29%20and%20show%20that%20these%20two%20properties%20of%0Ahypothesis%20classes%20are%20incompatible%20--%20there%20are%20classes%20that%20are%20generatable%0Abut%20not%20predictable%20and%20vice%20versa.%20Finally%2C%20we%20extend%20our%20results%20to%20capture%0Aprompted%20generation%20and%20give%20a%20complete%20characterization%20of%20which%20classes%20are%0Aprompt%20generatable%2C%20generalizing%20some%20of%20the%20work%20by%20Kleinberg%20and%20Mullainathan%0A%5B2024%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13714v5&entry.124074799=Read"},
{"title": "RecConv: Efficient Recursive Convolutions for Multi-Frequency\n  Representations", "author": "Mingshu Zhao and Yi Luo and Yong Ouyang", "abstract": "  Recent advances in vision transformers (ViTs) have demonstrated the advantage\nof global modeling capabilities, prompting widespread integration of\nlarge-kernel convolutions for enlarging the effective receptive field (ERF).\nHowever, the quadratic scaling of parameter count and computational complexity\n(FLOPs) with respect to kernel size poses significant efficiency and\noptimization challenges. This paper introduces RecConv, a recursive\ndecomposition strategy that efficiently constructs multi-frequency\nrepresentations using small-kernel convolutions. RecConv establishes a linear\nrelationship between parameter growth and decomposing levels which determines\nthe effective kernel size $k\\times 2^\\ell$ for a base kernel $k$ and $\\ell$\nlevels of decomposition, while maintaining constant FLOPs regardless of the ERF\nexpansion. Specifically, RecConv achieves a parameter expansion of only\n$\\ell+2$ times and a maximum FLOPs increase of $5/3$ times, compared to the\nexponential growth ($4^\\ell$) of standard and depthwise convolutions.\nRecNeXt-M3 outperforms RepViT-M1.1 by 1.9 $AP^{box}$ on COCO with similar\nFLOPs. This innovation provides a promising avenue towards designing efficient\nand compact networks across various modalities. Codes and models can be found\nat \\url{https://github.com/suous/RecNeXt}.\n", "link": "http://arxiv.org/abs/2412.19628v1", "date": "2024-12-27", "relevancy": 2.1615, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.573}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5362}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RecConv%3A%20Efficient%20Recursive%20Convolutions%20for%20Multi-Frequency%0A%20%20Representations&body=Title%3A%20RecConv%3A%20Efficient%20Recursive%20Convolutions%20for%20Multi-Frequency%0A%20%20Representations%0AAuthor%3A%20Mingshu%20Zhao%20and%20Yi%20Luo%20and%20Yong%20Ouyang%0AAbstract%3A%20%20%20Recent%20advances%20in%20vision%20transformers%20%28ViTs%29%20have%20demonstrated%20the%20advantage%0Aof%20global%20modeling%20capabilities%2C%20prompting%20widespread%20integration%20of%0Alarge-kernel%20convolutions%20for%20enlarging%20the%20effective%20receptive%20field%20%28ERF%29.%0AHowever%2C%20the%20quadratic%20scaling%20of%20parameter%20count%20and%20computational%20complexity%0A%28FLOPs%29%20with%20respect%20to%20kernel%20size%20poses%20significant%20efficiency%20and%0Aoptimization%20challenges.%20This%20paper%20introduces%20RecConv%2C%20a%20recursive%0Adecomposition%20strategy%20that%20efficiently%20constructs%20multi-frequency%0Arepresentations%20using%20small-kernel%20convolutions.%20RecConv%20establishes%20a%20linear%0Arelationship%20between%20parameter%20growth%20and%20decomposing%20levels%20which%20determines%0Athe%20effective%20kernel%20size%20%24k%5Ctimes%202%5E%5Cell%24%20for%20a%20base%20kernel%20%24k%24%20and%20%24%5Cell%24%0Alevels%20of%20decomposition%2C%20while%20maintaining%20constant%20FLOPs%20regardless%20of%20the%20ERF%0Aexpansion.%20Specifically%2C%20RecConv%20achieves%20a%20parameter%20expansion%20of%20only%0A%24%5Cell%2B2%24%20times%20and%20a%20maximum%20FLOPs%20increase%20of%20%245/3%24%20times%2C%20compared%20to%20the%0Aexponential%20growth%20%28%244%5E%5Cell%24%29%20of%20standard%20and%20depthwise%20convolutions.%0ARecNeXt-M3%20outperforms%20RepViT-M1.1%20by%201.9%20%24AP%5E%7Bbox%7D%24%20on%20COCO%20with%20similar%0AFLOPs.%20This%20innovation%20provides%20a%20promising%20avenue%20towards%20designing%20efficient%0Aand%20compact%20networks%20across%20various%20modalities.%20Codes%20and%20models%20can%20be%20found%0Aat%20%5Curl%7Bhttps%3A//github.com/suous/RecNeXt%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecConv%253A%2520Efficient%2520Recursive%2520Convolutions%2520for%2520Multi-Frequency%250A%2520%2520Representations%26entry.906535625%3DMingshu%2520Zhao%2520and%2520Yi%2520Luo%2520and%2520Yong%2520Ouyang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520vision%2520transformers%2520%2528ViTs%2529%2520have%2520demonstrated%2520the%2520advantage%250Aof%2520global%2520modeling%2520capabilities%252C%2520prompting%2520widespread%2520integration%2520of%250Alarge-kernel%2520convolutions%2520for%2520enlarging%2520the%2520effective%2520receptive%2520field%2520%2528ERF%2529.%250AHowever%252C%2520the%2520quadratic%2520scaling%2520of%2520parameter%2520count%2520and%2520computational%2520complexity%250A%2528FLOPs%2529%2520with%2520respect%2520to%2520kernel%2520size%2520poses%2520significant%2520efficiency%2520and%250Aoptimization%2520challenges.%2520This%2520paper%2520introduces%2520RecConv%252C%2520a%2520recursive%250Adecomposition%2520strategy%2520that%2520efficiently%2520constructs%2520multi-frequency%250Arepresentations%2520using%2520small-kernel%2520convolutions.%2520RecConv%2520establishes%2520a%2520linear%250Arelationship%2520between%2520parameter%2520growth%2520and%2520decomposing%2520levels%2520which%2520determines%250Athe%2520effective%2520kernel%2520size%2520%2524k%255Ctimes%25202%255E%255Cell%2524%2520for%2520a%2520base%2520kernel%2520%2524k%2524%2520and%2520%2524%255Cell%2524%250Alevels%2520of%2520decomposition%252C%2520while%2520maintaining%2520constant%2520FLOPs%2520regardless%2520of%2520the%2520ERF%250Aexpansion.%2520Specifically%252C%2520RecConv%2520achieves%2520a%2520parameter%2520expansion%2520of%2520only%250A%2524%255Cell%252B2%2524%2520times%2520and%2520a%2520maximum%2520FLOPs%2520increase%2520of%2520%25245/3%2524%2520times%252C%2520compared%2520to%2520the%250Aexponential%2520growth%2520%2528%25244%255E%255Cell%2524%2529%2520of%2520standard%2520and%2520depthwise%2520convolutions.%250ARecNeXt-M3%2520outperforms%2520RepViT-M1.1%2520by%25201.9%2520%2524AP%255E%257Bbox%257D%2524%2520on%2520COCO%2520with%2520similar%250AFLOPs.%2520This%2520innovation%2520provides%2520a%2520promising%2520avenue%2520towards%2520designing%2520efficient%250Aand%2520compact%2520networks%2520across%2520various%2520modalities.%2520Codes%2520and%2520models%2520can%2520be%2520found%250Aat%2520%255Curl%257Bhttps%253A//github.com/suous/RecNeXt%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RecConv%3A%20Efficient%20Recursive%20Convolutions%20for%20Multi-Frequency%0A%20%20Representations&entry.906535625=Mingshu%20Zhao%20and%20Yi%20Luo%20and%20Yong%20Ouyang&entry.1292438233=%20%20Recent%20advances%20in%20vision%20transformers%20%28ViTs%29%20have%20demonstrated%20the%20advantage%0Aof%20global%20modeling%20capabilities%2C%20prompting%20widespread%20integration%20of%0Alarge-kernel%20convolutions%20for%20enlarging%20the%20effective%20receptive%20field%20%28ERF%29.%0AHowever%2C%20the%20quadratic%20scaling%20of%20parameter%20count%20and%20computational%20complexity%0A%28FLOPs%29%20with%20respect%20to%20kernel%20size%20poses%20significant%20efficiency%20and%0Aoptimization%20challenges.%20This%20paper%20introduces%20RecConv%2C%20a%20recursive%0Adecomposition%20strategy%20that%20efficiently%20constructs%20multi-frequency%0Arepresentations%20using%20small-kernel%20convolutions.%20RecConv%20establishes%20a%20linear%0Arelationship%20between%20parameter%20growth%20and%20decomposing%20levels%20which%20determines%0Athe%20effective%20kernel%20size%20%24k%5Ctimes%202%5E%5Cell%24%20for%20a%20base%20kernel%20%24k%24%20and%20%24%5Cell%24%0Alevels%20of%20decomposition%2C%20while%20maintaining%20constant%20FLOPs%20regardless%20of%20the%20ERF%0Aexpansion.%20Specifically%2C%20RecConv%20achieves%20a%20parameter%20expansion%20of%20only%0A%24%5Cell%2B2%24%20times%20and%20a%20maximum%20FLOPs%20increase%20of%20%245/3%24%20times%2C%20compared%20to%20the%0Aexponential%20growth%20%28%244%5E%5Cell%24%29%20of%20standard%20and%20depthwise%20convolutions.%0ARecNeXt-M3%20outperforms%20RepViT-M1.1%20by%201.9%20%24AP%5E%7Bbox%7D%24%20on%20COCO%20with%20similar%0AFLOPs.%20This%20innovation%20provides%20a%20promising%20avenue%20towards%20designing%20efficient%0Aand%20compact%20networks%20across%20various%20modalities.%20Codes%20and%20models%20can%20be%20found%0Aat%20%5Curl%7Bhttps%3A//github.com/suous/RecNeXt%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19628v1&entry.124074799=Read"},
{"title": "A Review on the Integration of Artificial Intelligence and Medical\n  Imaging in IVF Ovarian Stimulation", "author": "Jana Zakall and Birgit Pohn and Antonia Graf and Daniel Kovatchki and Arezoo Borji and Ragib Shahriar Islam and Hossam Haick and Heinz Strohmer and Sepideh Hatamikia", "abstract": "  Artificial intelligence (AI) has emerged as a powerful tool to enhance\ndecision-making and optimize treatment protocols in in vitro fertilization\n(IVF). In particular, AI shows significant promise in supporting\ndecision-making during the ovarian stimulation phase of the IVF process. This\nreview evaluates studies focused on the applications of AI combined with\nmedical imaging in ovarian stimulation, examining methodologies, outcomes, and\ncurrent limitations. Our analysis of 13 studies on this topic reveals that,\nreveal that while AI algorithms demonstrated notable potential in predicting\noptimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the\nmedical imaging data utilized predominantly came from two-dimensional (2D)\nultrasound which mainly involved basic quantifications, such as follicle size\nand number, with limited use of direct feature extraction or advanced image\nanalysis techniques. This points to an underexplored opportunity where advanced\nimage analysis approaches, such as deep learning, and more diverse imaging\nmodalities, like three-dimensional (3D) ultrasound, could unlock deeper\ninsights. Additionally, the lack of explainable AI (XAI) in most studies raises\nconcerns about the transparency and traceability of AI-driven decisions - key\nfactors for clinical adoption and trust. Furthermore, many studies relied on\nsingle-center designs and small datasets, which limit the generalizability of\ntheir findings. This review highlights the need for integrating advanced\nimaging analysis techniques with explainable AI methodologies, as well as the\nimportance of leveraging multicenter collaborations and larger datasets.\nAddressing these gaps has the potential to enhance ovarian stimulation\nmanagement, paving the way for efficient, personalized, and data-driven\ntreatment pathways that improve IVF outcomes.\n", "link": "http://arxiv.org/abs/2412.19688v1", "date": "2024-12-27", "relevancy": 2.1404, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20on%20the%20Integration%20of%20Artificial%20Intelligence%20and%20Medical%0A%20%20Imaging%20in%20IVF%20Ovarian%20Stimulation&body=Title%3A%20A%20Review%20on%20the%20Integration%20of%20Artificial%20Intelligence%20and%20Medical%0A%20%20Imaging%20in%20IVF%20Ovarian%20Stimulation%0AAuthor%3A%20Jana%20Zakall%20and%20Birgit%20Pohn%20and%20Antonia%20Graf%20and%20Daniel%20Kovatchki%20and%20Arezoo%20Borji%20and%20Ragib%20Shahriar%20Islam%20and%20Hossam%20Haick%20and%20Heinz%20Strohmer%20and%20Sepideh%20Hatamikia%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20has%20emerged%20as%20a%20powerful%20tool%20to%20enhance%0Adecision-making%20and%20optimize%20treatment%20protocols%20in%20in%20vitro%20fertilization%0A%28IVF%29.%20In%20particular%2C%20AI%20shows%20significant%20promise%20in%20supporting%0Adecision-making%20during%20the%20ovarian%20stimulation%20phase%20of%20the%20IVF%20process.%20This%0Areview%20evaluates%20studies%20focused%20on%20the%20applications%20of%20AI%20combined%20with%0Amedical%20imaging%20in%20ovarian%20stimulation%2C%20examining%20methodologies%2C%20outcomes%2C%20and%0Acurrent%20limitations.%20Our%20analysis%20of%2013%20studies%20on%20this%20topic%20reveals%20that%2C%0Areveal%20that%20while%20AI%20algorithms%20demonstrated%20notable%20potential%20in%20predicting%0Aoptimal%20hormonal%20dosages%2C%20trigger%20timing%2C%20and%20oocyte%20retrieval%20outcomes%2C%20the%0Amedical%20imaging%20data%20utilized%20predominantly%20came%20from%20two-dimensional%20%282D%29%0Aultrasound%20which%20mainly%20involved%20basic%20quantifications%2C%20such%20as%20follicle%20size%0Aand%20number%2C%20with%20limited%20use%20of%20direct%20feature%20extraction%20or%20advanced%20image%0Aanalysis%20techniques.%20This%20points%20to%20an%20underexplored%20opportunity%20where%20advanced%0Aimage%20analysis%20approaches%2C%20such%20as%20deep%20learning%2C%20and%20more%20diverse%20imaging%0Amodalities%2C%20like%20three-dimensional%20%283D%29%20ultrasound%2C%20could%20unlock%20deeper%0Ainsights.%20Additionally%2C%20the%20lack%20of%20explainable%20AI%20%28XAI%29%20in%20most%20studies%20raises%0Aconcerns%20about%20the%20transparency%20and%20traceability%20of%20AI-driven%20decisions%20-%20key%0Afactors%20for%20clinical%20adoption%20and%20trust.%20Furthermore%2C%20many%20studies%20relied%20on%0Asingle-center%20designs%20and%20small%20datasets%2C%20which%20limit%20the%20generalizability%20of%0Atheir%20findings.%20This%20review%20highlights%20the%20need%20for%20integrating%20advanced%0Aimaging%20analysis%20techniques%20with%20explainable%20AI%20methodologies%2C%20as%20well%20as%20the%0Aimportance%20of%20leveraging%20multicenter%20collaborations%20and%20larger%20datasets.%0AAddressing%20these%20gaps%20has%20the%20potential%20to%20enhance%20ovarian%20stimulation%0Amanagement%2C%20paving%20the%20way%20for%20efficient%2C%20personalized%2C%20and%20data-driven%0Atreatment%20pathways%20that%20improve%20IVF%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520on%2520the%2520Integration%2520of%2520Artificial%2520Intelligence%2520and%2520Medical%250A%2520%2520Imaging%2520in%2520IVF%2520Ovarian%2520Stimulation%26entry.906535625%3DJana%2520Zakall%2520and%2520Birgit%2520Pohn%2520and%2520Antonia%2520Graf%2520and%2520Daniel%2520Kovatchki%2520and%2520Arezoo%2520Borji%2520and%2520Ragib%2520Shahriar%2520Islam%2520and%2520Hossam%2520Haick%2520and%2520Heinz%2520Strohmer%2520and%2520Sepideh%2520Hatamikia%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520to%2520enhance%250Adecision-making%2520and%2520optimize%2520treatment%2520protocols%2520in%2520in%2520vitro%2520fertilization%250A%2528IVF%2529.%2520In%2520particular%252C%2520AI%2520shows%2520significant%2520promise%2520in%2520supporting%250Adecision-making%2520during%2520the%2520ovarian%2520stimulation%2520phase%2520of%2520the%2520IVF%2520process.%2520This%250Areview%2520evaluates%2520studies%2520focused%2520on%2520the%2520applications%2520of%2520AI%2520combined%2520with%250Amedical%2520imaging%2520in%2520ovarian%2520stimulation%252C%2520examining%2520methodologies%252C%2520outcomes%252C%2520and%250Acurrent%2520limitations.%2520Our%2520analysis%2520of%252013%2520studies%2520on%2520this%2520topic%2520reveals%2520that%252C%250Areveal%2520that%2520while%2520AI%2520algorithms%2520demonstrated%2520notable%2520potential%2520in%2520predicting%250Aoptimal%2520hormonal%2520dosages%252C%2520trigger%2520timing%252C%2520and%2520oocyte%2520retrieval%2520outcomes%252C%2520the%250Amedical%2520imaging%2520data%2520utilized%2520predominantly%2520came%2520from%2520two-dimensional%2520%25282D%2529%250Aultrasound%2520which%2520mainly%2520involved%2520basic%2520quantifications%252C%2520such%2520as%2520follicle%2520size%250Aand%2520number%252C%2520with%2520limited%2520use%2520of%2520direct%2520feature%2520extraction%2520or%2520advanced%2520image%250Aanalysis%2520techniques.%2520This%2520points%2520to%2520an%2520underexplored%2520opportunity%2520where%2520advanced%250Aimage%2520analysis%2520approaches%252C%2520such%2520as%2520deep%2520learning%252C%2520and%2520more%2520diverse%2520imaging%250Amodalities%252C%2520like%2520three-dimensional%2520%25283D%2529%2520ultrasound%252C%2520could%2520unlock%2520deeper%250Ainsights.%2520Additionally%252C%2520the%2520lack%2520of%2520explainable%2520AI%2520%2528XAI%2529%2520in%2520most%2520studies%2520raises%250Aconcerns%2520about%2520the%2520transparency%2520and%2520traceability%2520of%2520AI-driven%2520decisions%2520-%2520key%250Afactors%2520for%2520clinical%2520adoption%2520and%2520trust.%2520Furthermore%252C%2520many%2520studies%2520relied%2520on%250Asingle-center%2520designs%2520and%2520small%2520datasets%252C%2520which%2520limit%2520the%2520generalizability%2520of%250Atheir%2520findings.%2520This%2520review%2520highlights%2520the%2520need%2520for%2520integrating%2520advanced%250Aimaging%2520analysis%2520techniques%2520with%2520explainable%2520AI%2520methodologies%252C%2520as%2520well%2520as%2520the%250Aimportance%2520of%2520leveraging%2520multicenter%2520collaborations%2520and%2520larger%2520datasets.%250AAddressing%2520these%2520gaps%2520has%2520the%2520potential%2520to%2520enhance%2520ovarian%2520stimulation%250Amanagement%252C%2520paving%2520the%2520way%2520for%2520efficient%252C%2520personalized%252C%2520and%2520data-driven%250Atreatment%2520pathways%2520that%2520improve%2520IVF%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20on%20the%20Integration%20of%20Artificial%20Intelligence%20and%20Medical%0A%20%20Imaging%20in%20IVF%20Ovarian%20Stimulation&entry.906535625=Jana%20Zakall%20and%20Birgit%20Pohn%20and%20Antonia%20Graf%20and%20Daniel%20Kovatchki%20and%20Arezoo%20Borji%20and%20Ragib%20Shahriar%20Islam%20and%20Hossam%20Haick%20and%20Heinz%20Strohmer%20and%20Sepideh%20Hatamikia&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20has%20emerged%20as%20a%20powerful%20tool%20to%20enhance%0Adecision-making%20and%20optimize%20treatment%20protocols%20in%20in%20vitro%20fertilization%0A%28IVF%29.%20In%20particular%2C%20AI%20shows%20significant%20promise%20in%20supporting%0Adecision-making%20during%20the%20ovarian%20stimulation%20phase%20of%20the%20IVF%20process.%20This%0Areview%20evaluates%20studies%20focused%20on%20the%20applications%20of%20AI%20combined%20with%0Amedical%20imaging%20in%20ovarian%20stimulation%2C%20examining%20methodologies%2C%20outcomes%2C%20and%0Acurrent%20limitations.%20Our%20analysis%20of%2013%20studies%20on%20this%20topic%20reveals%20that%2C%0Areveal%20that%20while%20AI%20algorithms%20demonstrated%20notable%20potential%20in%20predicting%0Aoptimal%20hormonal%20dosages%2C%20trigger%20timing%2C%20and%20oocyte%20retrieval%20outcomes%2C%20the%0Amedical%20imaging%20data%20utilized%20predominantly%20came%20from%20two-dimensional%20%282D%29%0Aultrasound%20which%20mainly%20involved%20basic%20quantifications%2C%20such%20as%20follicle%20size%0Aand%20number%2C%20with%20limited%20use%20of%20direct%20feature%20extraction%20or%20advanced%20image%0Aanalysis%20techniques.%20This%20points%20to%20an%20underexplored%20opportunity%20where%20advanced%0Aimage%20analysis%20approaches%2C%20such%20as%20deep%20learning%2C%20and%20more%20diverse%20imaging%0Amodalities%2C%20like%20three-dimensional%20%283D%29%20ultrasound%2C%20could%20unlock%20deeper%0Ainsights.%20Additionally%2C%20the%20lack%20of%20explainable%20AI%20%28XAI%29%20in%20most%20studies%20raises%0Aconcerns%20about%20the%20transparency%20and%20traceability%20of%20AI-driven%20decisions%20-%20key%0Afactors%20for%20clinical%20adoption%20and%20trust.%20Furthermore%2C%20many%20studies%20relied%20on%0Asingle-center%20designs%20and%20small%20datasets%2C%20which%20limit%20the%20generalizability%20of%0Atheir%20findings.%20This%20review%20highlights%20the%20need%20for%20integrating%20advanced%0Aimaging%20analysis%20techniques%20with%20explainable%20AI%20methodologies%2C%20as%20well%20as%20the%0Aimportance%20of%20leveraging%20multicenter%20collaborations%20and%20larger%20datasets.%0AAddressing%20these%20gaps%20has%20the%20potential%20to%20enhance%20ovarian%20stimulation%0Amanagement%2C%20paving%20the%20way%20for%20efficient%2C%20personalized%2C%20and%20data-driven%0Atreatment%20pathways%20that%20improve%20IVF%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19688v1&entry.124074799=Read"},
{"title": "Intertwining CP and NLP: The Generation of Unreasonably Constrained\n  Sentences", "author": "Alexandre Bonlarron and Jean-Charles R\u00e9gin", "abstract": "  Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional NLP approaches prioritize generating\nmeaningful and coherent output. Also, the current state-of-the-art methods\noften lack the expressiveness and constraint satisfaction capabilities to\nhandle such tasks effectively. Recently, an approach for generating constrained\nsentences in CP has been proposed in (Bonlarron et al, 2023). This ad-hoc model\nto solve the sentences generation problem under MNREAD rules proved\nneithertheless to be computationaly and structuraly unsuitable to deal with\nother more constrained problems. In this paper, a novel more generic approach\nis introduced to tackle many of these previously untractable problems, and\nillustrated here with the quite untractable sentences generation problem\nfollowing RADNER rules.\n  More precisely, this paper presents the CPTextGen Framework. This framework\nconsiders a constrained text generation problem as a discrete combinatorial\noptimization problem. It is solved by a constraint programming method that\ncombines linguistic properties (e.g., n-grams or language level) with other\nmore classical constraints (e.g., the number of characters, syllables).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using an LLM.\n  The effectiveness of this approach is demonstrated by tackling a new, more\ntediously constrained text generation problem: the iconic RADNER sentences\nproblem. This problem aims to generate sentences respecting a set of quite\nstrict rules defined by their use in vision and clinical research. Thanks to\nour CP-based approach, many new strongly constrained sentences have been\nsuccessfully generated. This highlights our approach's potential to handle\nunreasonably constrained text generation scenarios.\n", "link": "http://arxiv.org/abs/2406.15473v2", "date": "2024-12-27", "relevancy": 2.1252, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5561}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5292}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intertwining%20CP%20and%20NLP%3A%20The%20Generation%20of%20Unreasonably%20Constrained%0A%20%20Sentences&body=Title%3A%20Intertwining%20CP%20and%20NLP%3A%20The%20Generation%20of%20Unreasonably%20Constrained%0A%20%20Sentences%0AAuthor%3A%20Alexandre%20Bonlarron%20and%20Jean-Charles%20R%C3%A9gin%0AAbstract%3A%20%20%20Constrained%20text%20generation%20remains%20a%20challenging%20task%2C%20particularly%20when%0Adealing%20with%20hard%20constraints.%20Traditional%20NLP%20approaches%20prioritize%20generating%0Ameaningful%20and%20coherent%20output.%20Also%2C%20the%20current%20state-of-the-art%20methods%0Aoften%20lack%20the%20expressiveness%20and%20constraint%20satisfaction%20capabilities%20to%0Ahandle%20such%20tasks%20effectively.%20Recently%2C%20an%20approach%20for%20generating%20constrained%0Asentences%20in%20CP%20has%20been%20proposed%20in%20%28Bonlarron%20et%20al%2C%202023%29.%20This%20ad-hoc%20model%0Ato%20solve%20the%20sentences%20generation%20problem%20under%20MNREAD%20rules%20proved%0Aneithertheless%20to%20be%20computationaly%20and%20structuraly%20unsuitable%20to%20deal%20with%0Aother%20more%20constrained%20problems.%20In%20this%20paper%2C%20a%20novel%20more%20generic%20approach%0Ais%20introduced%20to%20tackle%20many%20of%20these%20previously%20untractable%20problems%2C%20and%0Aillustrated%20here%20with%20the%20quite%20untractable%20sentences%20generation%20problem%0Afollowing%20RADNER%20rules.%0A%20%20More%20precisely%2C%20this%20paper%20presents%20the%20CPTextGen%20Framework.%20This%20framework%0Aconsiders%20a%20constrained%20text%20generation%20problem%20as%20a%20discrete%20combinatorial%0Aoptimization%20problem.%20It%20is%20solved%20by%20a%20constraint%20programming%20method%20that%0Acombines%20linguistic%20properties%20%28e.g.%2C%20n-grams%20or%20language%20level%29%20with%20other%0Amore%20classical%20constraints%20%28e.g.%2C%20the%20number%20of%20characters%2C%20syllables%29.%0AEventually%2C%20a%20curation%20phase%20allows%20for%20selecting%20the%20best-generated%20sentences%0Aaccording%20to%20perplexity%20using%20an%20LLM.%0A%20%20The%20effectiveness%20of%20this%20approach%20is%20demonstrated%20by%20tackling%20a%20new%2C%20more%0Atediously%20constrained%20text%20generation%20problem%3A%20the%20iconic%20RADNER%20sentences%0Aproblem.%20This%20problem%20aims%20to%20generate%20sentences%20respecting%20a%20set%20of%20quite%0Astrict%20rules%20defined%20by%20their%20use%20in%20vision%20and%20clinical%20research.%20Thanks%20to%0Aour%20CP-based%20approach%2C%20many%20new%20strongly%20constrained%20sentences%20have%20been%0Asuccessfully%20generated.%20This%20highlights%20our%20approach%27s%20potential%20to%20handle%0Aunreasonably%20constrained%20text%20generation%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntertwining%2520CP%2520and%2520NLP%253A%2520The%2520Generation%2520of%2520Unreasonably%2520Constrained%250A%2520%2520Sentences%26entry.906535625%3DAlexandre%2520Bonlarron%2520and%2520Jean-Charles%2520R%25C3%25A9gin%26entry.1292438233%3D%2520%2520Constrained%2520text%2520generation%2520remains%2520a%2520challenging%2520task%252C%2520particularly%2520when%250Adealing%2520with%2520hard%2520constraints.%2520Traditional%2520NLP%2520approaches%2520prioritize%2520generating%250Ameaningful%2520and%2520coherent%2520output.%2520Also%252C%2520the%2520current%2520state-of-the-art%2520methods%250Aoften%2520lack%2520the%2520expressiveness%2520and%2520constraint%2520satisfaction%2520capabilities%2520to%250Ahandle%2520such%2520tasks%2520effectively.%2520Recently%252C%2520an%2520approach%2520for%2520generating%2520constrained%250Asentences%2520in%2520CP%2520has%2520been%2520proposed%2520in%2520%2528Bonlarron%2520et%2520al%252C%25202023%2529.%2520This%2520ad-hoc%2520model%250Ato%2520solve%2520the%2520sentences%2520generation%2520problem%2520under%2520MNREAD%2520rules%2520proved%250Aneithertheless%2520to%2520be%2520computationaly%2520and%2520structuraly%2520unsuitable%2520to%2520deal%2520with%250Aother%2520more%2520constrained%2520problems.%2520In%2520this%2520paper%252C%2520a%2520novel%2520more%2520generic%2520approach%250Ais%2520introduced%2520to%2520tackle%2520many%2520of%2520these%2520previously%2520untractable%2520problems%252C%2520and%250Aillustrated%2520here%2520with%2520the%2520quite%2520untractable%2520sentences%2520generation%2520problem%250Afollowing%2520RADNER%2520rules.%250A%2520%2520More%2520precisely%252C%2520this%2520paper%2520presents%2520the%2520CPTextGen%2520Framework.%2520This%2520framework%250Aconsiders%2520a%2520constrained%2520text%2520generation%2520problem%2520as%2520a%2520discrete%2520combinatorial%250Aoptimization%2520problem.%2520It%2520is%2520solved%2520by%2520a%2520constraint%2520programming%2520method%2520that%250Acombines%2520linguistic%2520properties%2520%2528e.g.%252C%2520n-grams%2520or%2520language%2520level%2529%2520with%2520other%250Amore%2520classical%2520constraints%2520%2528e.g.%252C%2520the%2520number%2520of%2520characters%252C%2520syllables%2529.%250AEventually%252C%2520a%2520curation%2520phase%2520allows%2520for%2520selecting%2520the%2520best-generated%2520sentences%250Aaccording%2520to%2520perplexity%2520using%2520an%2520LLM.%250A%2520%2520The%2520effectiveness%2520of%2520this%2520approach%2520is%2520demonstrated%2520by%2520tackling%2520a%2520new%252C%2520more%250Atediously%2520constrained%2520text%2520generation%2520problem%253A%2520the%2520iconic%2520RADNER%2520sentences%250Aproblem.%2520This%2520problem%2520aims%2520to%2520generate%2520sentences%2520respecting%2520a%2520set%2520of%2520quite%250Astrict%2520rules%2520defined%2520by%2520their%2520use%2520in%2520vision%2520and%2520clinical%2520research.%2520Thanks%2520to%250Aour%2520CP-based%2520approach%252C%2520many%2520new%2520strongly%2520constrained%2520sentences%2520have%2520been%250Asuccessfully%2520generated.%2520This%2520highlights%2520our%2520approach%2527s%2520potential%2520to%2520handle%250Aunreasonably%2520constrained%2520text%2520generation%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intertwining%20CP%20and%20NLP%3A%20The%20Generation%20of%20Unreasonably%20Constrained%0A%20%20Sentences&entry.906535625=Alexandre%20Bonlarron%20and%20Jean-Charles%20R%C3%A9gin&entry.1292438233=%20%20Constrained%20text%20generation%20remains%20a%20challenging%20task%2C%20particularly%20when%0Adealing%20with%20hard%20constraints.%20Traditional%20NLP%20approaches%20prioritize%20generating%0Ameaningful%20and%20coherent%20output.%20Also%2C%20the%20current%20state-of-the-art%20methods%0Aoften%20lack%20the%20expressiveness%20and%20constraint%20satisfaction%20capabilities%20to%0Ahandle%20such%20tasks%20effectively.%20Recently%2C%20an%20approach%20for%20generating%20constrained%0Asentences%20in%20CP%20has%20been%20proposed%20in%20%28Bonlarron%20et%20al%2C%202023%29.%20This%20ad-hoc%20model%0Ato%20solve%20the%20sentences%20generation%20problem%20under%20MNREAD%20rules%20proved%0Aneithertheless%20to%20be%20computationaly%20and%20structuraly%20unsuitable%20to%20deal%20with%0Aother%20more%20constrained%20problems.%20In%20this%20paper%2C%20a%20novel%20more%20generic%20approach%0Ais%20introduced%20to%20tackle%20many%20of%20these%20previously%20untractable%20problems%2C%20and%0Aillustrated%20here%20with%20the%20quite%20untractable%20sentences%20generation%20problem%0Afollowing%20RADNER%20rules.%0A%20%20More%20precisely%2C%20this%20paper%20presents%20the%20CPTextGen%20Framework.%20This%20framework%0Aconsiders%20a%20constrained%20text%20generation%20problem%20as%20a%20discrete%20combinatorial%0Aoptimization%20problem.%20It%20is%20solved%20by%20a%20constraint%20programming%20method%20that%0Acombines%20linguistic%20properties%20%28e.g.%2C%20n-grams%20or%20language%20level%29%20with%20other%0Amore%20classical%20constraints%20%28e.g.%2C%20the%20number%20of%20characters%2C%20syllables%29.%0AEventually%2C%20a%20curation%20phase%20allows%20for%20selecting%20the%20best-generated%20sentences%0Aaccording%20to%20perplexity%20using%20an%20LLM.%0A%20%20The%20effectiveness%20of%20this%20approach%20is%20demonstrated%20by%20tackling%20a%20new%2C%20more%0Atediously%20constrained%20text%20generation%20problem%3A%20the%20iconic%20RADNER%20sentences%0Aproblem.%20This%20problem%20aims%20to%20generate%20sentences%20respecting%20a%20set%20of%20quite%0Astrict%20rules%20defined%20by%20their%20use%20in%20vision%20and%20clinical%20research.%20Thanks%20to%0Aour%20CP-based%20approach%2C%20many%20new%20strongly%20constrained%20sentences%20have%20been%0Asuccessfully%20generated.%20This%20highlights%20our%20approach%27s%20potential%20to%20handle%0Aunreasonably%20constrained%20text%20generation%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15473v2&entry.124074799=Read"},
{"title": "Can Large Language Models Adapt to Other Agents In-Context?", "author": "Matthew Riemer and Zahra Ashktorab and Djallel Bouneffouf and Payel Das and Miao Liu and Justin D. Weisz and Murray Campbell", "abstract": "  As the research community aims to build better AI assistants that are more\ndynamic and personalized to the diversity of humans that they interact with,\nthere is increased interest in evaluating the theory of mind capabilities of\nlarge language models (LLMs). Indeed, several recent studies suggest that LLM\ntheory of mind capabilities are quite impressive, approximating human-level\nperformance. Our paper aims to rebuke this narrative and argues instead that\npast studies were not directly measuring agent performance, potentially leading\nto findings that are illusory in nature as a result. We draw a strong\ndistinction between what we call literal theory of mind i.e. measuring the\nagent's ability to predict the behavior of others and functional theory of mind\ni.e. adapting to agents in-context based on a rational response to predictions\nof their behavior. We find that top performing open source LLMs may display\nstrong capabilities in literal theory of mind, depending on how they are\nprompted, but seem to struggle with functional theory of mind -- even when\npartner policies are exceedingly simple. Our work serves to highlight the\ndouble sided nature of inductive bias in LLMs when adapting to new situations.\nWhile this bias can lead to strong performance over limited horizons, it often\nhinders convergence to optimal long-term behavior.\n", "link": "http://arxiv.org/abs/2412.19726v1", "date": "2024-12-27", "relevancy": 2.1015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Adapt%20to%20Other%20Agents%20In-Context%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Adapt%20to%20Other%20Agents%20In-Context%3F%0AAuthor%3A%20Matthew%20Riemer%20and%20Zahra%20Ashktorab%20and%20Djallel%20Bouneffouf%20and%20Payel%20Das%20and%20Miao%20Liu%20and%20Justin%20D.%20Weisz%20and%20Murray%20Campbell%0AAbstract%3A%20%20%20As%20the%20research%20community%20aims%20to%20build%20better%20AI%20assistants%20that%20are%20more%0Adynamic%20and%20personalized%20to%20the%20diversity%20of%20humans%20that%20they%20interact%20with%2C%0Athere%20is%20increased%20interest%20in%20evaluating%20the%20theory%20of%20mind%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29.%20Indeed%2C%20several%20recent%20studies%20suggest%20that%20LLM%0Atheory%20of%20mind%20capabilities%20are%20quite%20impressive%2C%20approximating%20human-level%0Aperformance.%20Our%20paper%20aims%20to%20rebuke%20this%20narrative%20and%20argues%20instead%20that%0Apast%20studies%20were%20not%20directly%20measuring%20agent%20performance%2C%20potentially%20leading%0Ato%20findings%20that%20are%20illusory%20in%20nature%20as%20a%20result.%20We%20draw%20a%20strong%0Adistinction%20between%20what%20we%20call%20literal%20theory%20of%20mind%20i.e.%20measuring%20the%0Aagent%27s%20ability%20to%20predict%20the%20behavior%20of%20others%20and%20functional%20theory%20of%20mind%0Ai.e.%20adapting%20to%20agents%20in-context%20based%20on%20a%20rational%20response%20to%20predictions%0Aof%20their%20behavior.%20We%20find%20that%20top%20performing%20open%20source%20LLMs%20may%20display%0Astrong%20capabilities%20in%20literal%20theory%20of%20mind%2C%20depending%20on%20how%20they%20are%0Aprompted%2C%20but%20seem%20to%20struggle%20with%20functional%20theory%20of%20mind%20--%20even%20when%0Apartner%20policies%20are%20exceedingly%20simple.%20Our%20work%20serves%20to%20highlight%20the%0Adouble%20sided%20nature%20of%20inductive%20bias%20in%20LLMs%20when%20adapting%20to%20new%20situations.%0AWhile%20this%20bias%20can%20lead%20to%20strong%20performance%20over%20limited%20horizons%2C%20it%20often%0Ahinders%20convergence%20to%20optimal%20long-term%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Adapt%2520to%2520Other%2520Agents%2520In-Context%253F%26entry.906535625%3DMatthew%2520Riemer%2520and%2520Zahra%2520Ashktorab%2520and%2520Djallel%2520Bouneffouf%2520and%2520Payel%2520Das%2520and%2520Miao%2520Liu%2520and%2520Justin%2520D.%2520Weisz%2520and%2520Murray%2520Campbell%26entry.1292438233%3D%2520%2520As%2520the%2520research%2520community%2520aims%2520to%2520build%2520better%2520AI%2520assistants%2520that%2520are%2520more%250Adynamic%2520and%2520personalized%2520to%2520the%2520diversity%2520of%2520humans%2520that%2520they%2520interact%2520with%252C%250Athere%2520is%2520increased%2520interest%2520in%2520evaluating%2520the%2520theory%2520of%2520mind%2520capabilities%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520Indeed%252C%2520several%2520recent%2520studies%2520suggest%2520that%2520LLM%250Atheory%2520of%2520mind%2520capabilities%2520are%2520quite%2520impressive%252C%2520approximating%2520human-level%250Aperformance.%2520Our%2520paper%2520aims%2520to%2520rebuke%2520this%2520narrative%2520and%2520argues%2520instead%2520that%250Apast%2520studies%2520were%2520not%2520directly%2520measuring%2520agent%2520performance%252C%2520potentially%2520leading%250Ato%2520findings%2520that%2520are%2520illusory%2520in%2520nature%2520as%2520a%2520result.%2520We%2520draw%2520a%2520strong%250Adistinction%2520between%2520what%2520we%2520call%2520literal%2520theory%2520of%2520mind%2520i.e.%2520measuring%2520the%250Aagent%2527s%2520ability%2520to%2520predict%2520the%2520behavior%2520of%2520others%2520and%2520functional%2520theory%2520of%2520mind%250Ai.e.%2520adapting%2520to%2520agents%2520in-context%2520based%2520on%2520a%2520rational%2520response%2520to%2520predictions%250Aof%2520their%2520behavior.%2520We%2520find%2520that%2520top%2520performing%2520open%2520source%2520LLMs%2520may%2520display%250Astrong%2520capabilities%2520in%2520literal%2520theory%2520of%2520mind%252C%2520depending%2520on%2520how%2520they%2520are%250Aprompted%252C%2520but%2520seem%2520to%2520struggle%2520with%2520functional%2520theory%2520of%2520mind%2520--%2520even%2520when%250Apartner%2520policies%2520are%2520exceedingly%2520simple.%2520Our%2520work%2520serves%2520to%2520highlight%2520the%250Adouble%2520sided%2520nature%2520of%2520inductive%2520bias%2520in%2520LLMs%2520when%2520adapting%2520to%2520new%2520situations.%250AWhile%2520this%2520bias%2520can%2520lead%2520to%2520strong%2520performance%2520over%2520limited%2520horizons%252C%2520it%2520often%250Ahinders%2520convergence%2520to%2520optimal%2520long-term%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Adapt%20to%20Other%20Agents%20In-Context%3F&entry.906535625=Matthew%20Riemer%20and%20Zahra%20Ashktorab%20and%20Djallel%20Bouneffouf%20and%20Payel%20Das%20and%20Miao%20Liu%20and%20Justin%20D.%20Weisz%20and%20Murray%20Campbell&entry.1292438233=%20%20As%20the%20research%20community%20aims%20to%20build%20better%20AI%20assistants%20that%20are%20more%0Adynamic%20and%20personalized%20to%20the%20diversity%20of%20humans%20that%20they%20interact%20with%2C%0Athere%20is%20increased%20interest%20in%20evaluating%20the%20theory%20of%20mind%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29.%20Indeed%2C%20several%20recent%20studies%20suggest%20that%20LLM%0Atheory%20of%20mind%20capabilities%20are%20quite%20impressive%2C%20approximating%20human-level%0Aperformance.%20Our%20paper%20aims%20to%20rebuke%20this%20narrative%20and%20argues%20instead%20that%0Apast%20studies%20were%20not%20directly%20measuring%20agent%20performance%2C%20potentially%20leading%0Ato%20findings%20that%20are%20illusory%20in%20nature%20as%20a%20result.%20We%20draw%20a%20strong%0Adistinction%20between%20what%20we%20call%20literal%20theory%20of%20mind%20i.e.%20measuring%20the%0Aagent%27s%20ability%20to%20predict%20the%20behavior%20of%20others%20and%20functional%20theory%20of%20mind%0Ai.e.%20adapting%20to%20agents%20in-context%20based%20on%20a%20rational%20response%20to%20predictions%0Aof%20their%20behavior.%20We%20find%20that%20top%20performing%20open%20source%20LLMs%20may%20display%0Astrong%20capabilities%20in%20literal%20theory%20of%20mind%2C%20depending%20on%20how%20they%20are%0Aprompted%2C%20but%20seem%20to%20struggle%20with%20functional%20theory%20of%20mind%20--%20even%20when%0Apartner%20policies%20are%20exceedingly%20simple.%20Our%20work%20serves%20to%20highlight%20the%0Adouble%20sided%20nature%20of%20inductive%20bias%20in%20LLMs%20when%20adapting%20to%20new%20situations.%0AWhile%20this%20bias%20can%20lead%20to%20strong%20performance%20over%20limited%20horizons%2C%20it%20often%0Ahinders%20convergence%20to%20optimal%20long-term%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19726v1&entry.124074799=Read"},
{"title": "DLScanner: A parameter space scanner package assisted by deep learning\n  methods", "author": "A. Hammad and Raymundo Ramos", "abstract": "  In this paper, we introduce a scanner package enhanced by deep learning (DL)\ntechniques. The proposed package addresses two significant challenges\nassociated with previously developed DL-based methods: slow convergence in\nhigh-dimensional scans and the limited generalization of the DL network when\nmapping random points to the target space. To tackle the first issue, we\nutilize a similarity learning network that maps sampled points into a\nrepresentation space. In this space, in-target points are grouped together\nwhile out-target points are effectively pushed apart. This approach enhances\nthe scan convergence by refining the representation of sampled points. The\nsecond challenge is mitigated by integrating a dynamic sampling strategy.\nSpecifically, we employ a VEGAS mapping to adaptively suggest new points for\nthe DL network while also improving the mapping when more points are collected.\nOur proposed framework demonstrates substantial gains in both performance and\nefficiency compared to other scanning methods.\n", "link": "http://arxiv.org/abs/2412.19675v1", "date": "2024-12-27", "relevancy": 2.0803, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5345}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5105}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DLScanner%3A%20A%20parameter%20space%20scanner%20package%20assisted%20by%20deep%20learning%0A%20%20methods&body=Title%3A%20DLScanner%3A%20A%20parameter%20space%20scanner%20package%20assisted%20by%20deep%20learning%0A%20%20methods%0AAuthor%3A%20A.%20Hammad%20and%20Raymundo%20Ramos%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20scanner%20package%20enhanced%20by%20deep%20learning%20%28DL%29%0Atechniques.%20The%20proposed%20package%20addresses%20two%20significant%20challenges%0Aassociated%20with%20previously%20developed%20DL-based%20methods%3A%20slow%20convergence%20in%0Ahigh-dimensional%20scans%20and%20the%20limited%20generalization%20of%20the%20DL%20network%20when%0Amapping%20random%20points%20to%20the%20target%20space.%20To%20tackle%20the%20first%20issue%2C%20we%0Autilize%20a%20similarity%20learning%20network%20that%20maps%20sampled%20points%20into%20a%0Arepresentation%20space.%20In%20this%20space%2C%20in-target%20points%20are%20grouped%20together%0Awhile%20out-target%20points%20are%20effectively%20pushed%20apart.%20This%20approach%20enhances%0Athe%20scan%20convergence%20by%20refining%20the%20representation%20of%20sampled%20points.%20The%0Asecond%20challenge%20is%20mitigated%20by%20integrating%20a%20dynamic%20sampling%20strategy.%0ASpecifically%2C%20we%20employ%20a%20VEGAS%20mapping%20to%20adaptively%20suggest%20new%20points%20for%0Athe%20DL%20network%20while%20also%20improving%20the%20mapping%20when%20more%20points%20are%20collected.%0AOur%20proposed%20framework%20demonstrates%20substantial%20gains%20in%20both%20performance%20and%0Aefficiency%20compared%20to%20other%20scanning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDLScanner%253A%2520A%2520parameter%2520space%2520scanner%2520package%2520assisted%2520by%2520deep%2520learning%250A%2520%2520methods%26entry.906535625%3DA.%2520Hammad%2520and%2520Raymundo%2520Ramos%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520scanner%2520package%2520enhanced%2520by%2520deep%2520learning%2520%2528DL%2529%250Atechniques.%2520The%2520proposed%2520package%2520addresses%2520two%2520significant%2520challenges%250Aassociated%2520with%2520previously%2520developed%2520DL-based%2520methods%253A%2520slow%2520convergence%2520in%250Ahigh-dimensional%2520scans%2520and%2520the%2520limited%2520generalization%2520of%2520the%2520DL%2520network%2520when%250Amapping%2520random%2520points%2520to%2520the%2520target%2520space.%2520To%2520tackle%2520the%2520first%2520issue%252C%2520we%250Autilize%2520a%2520similarity%2520learning%2520network%2520that%2520maps%2520sampled%2520points%2520into%2520a%250Arepresentation%2520space.%2520In%2520this%2520space%252C%2520in-target%2520points%2520are%2520grouped%2520together%250Awhile%2520out-target%2520points%2520are%2520effectively%2520pushed%2520apart.%2520This%2520approach%2520enhances%250Athe%2520scan%2520convergence%2520by%2520refining%2520the%2520representation%2520of%2520sampled%2520points.%2520The%250Asecond%2520challenge%2520is%2520mitigated%2520by%2520integrating%2520a%2520dynamic%2520sampling%2520strategy.%250ASpecifically%252C%2520we%2520employ%2520a%2520VEGAS%2520mapping%2520to%2520adaptively%2520suggest%2520new%2520points%2520for%250Athe%2520DL%2520network%2520while%2520also%2520improving%2520the%2520mapping%2520when%2520more%2520points%2520are%2520collected.%250AOur%2520proposed%2520framework%2520demonstrates%2520substantial%2520gains%2520in%2520both%2520performance%2520and%250Aefficiency%2520compared%2520to%2520other%2520scanning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DLScanner%3A%20A%20parameter%20space%20scanner%20package%20assisted%20by%20deep%20learning%0A%20%20methods&entry.906535625=A.%20Hammad%20and%20Raymundo%20Ramos&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20scanner%20package%20enhanced%20by%20deep%20learning%20%28DL%29%0Atechniques.%20The%20proposed%20package%20addresses%20two%20significant%20challenges%0Aassociated%20with%20previously%20developed%20DL-based%20methods%3A%20slow%20convergence%20in%0Ahigh-dimensional%20scans%20and%20the%20limited%20generalization%20of%20the%20DL%20network%20when%0Amapping%20random%20points%20to%20the%20target%20space.%20To%20tackle%20the%20first%20issue%2C%20we%0Autilize%20a%20similarity%20learning%20network%20that%20maps%20sampled%20points%20into%20a%0Arepresentation%20space.%20In%20this%20space%2C%20in-target%20points%20are%20grouped%20together%0Awhile%20out-target%20points%20are%20effectively%20pushed%20apart.%20This%20approach%20enhances%0Athe%20scan%20convergence%20by%20refining%20the%20representation%20of%20sampled%20points.%20The%0Asecond%20challenge%20is%20mitigated%20by%20integrating%20a%20dynamic%20sampling%20strategy.%0ASpecifically%2C%20we%20employ%20a%20VEGAS%20mapping%20to%20adaptively%20suggest%20new%20points%20for%0Athe%20DL%20network%20while%20also%20improving%20the%20mapping%20when%20more%20points%20are%20collected.%0AOur%20proposed%20framework%20demonstrates%20substantial%20gains%20in%20both%20performance%20and%0Aefficiency%20compared%20to%20other%20scanning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19675v1&entry.124074799=Read"},
{"title": "Goal-oriented Communications based on Recursive Early Exit Neural\n  Networks", "author": "Jary Pomponi and Mattia Merluzzi and Alessio Devoto and Mateus Pontes Mota and Paolo Di Lorenzo and Simone Scardapane", "abstract": "  This paper presents a novel framework for goal-oriented semantic\ncommunications leveraging recursive early exit models. The proposed approach is\nbuilt on two key components. First, we introduce an innovative early exit\nstrategy that dynamically partitions computations, enabling samples to be\noffloaded to a server based on layer-wise recursive prediction dynamics that\ndetect samples for which the confidence is not increasing fast enough over\nlayers. Second, we develop a Reinforcement Learning-based online optimization\nframework that jointly determines early exit points, computation splitting, and\noffloading strategies, while accounting for wireless conditions, inference\naccuracy, and resource costs. Numerical evaluations in an edge inference\nscenario demonstrate the method's adaptability and effectiveness in striking an\nexcellent trade-off between performance, latency, and resource efficiency.\n", "link": "http://arxiv.org/abs/2412.19587v1", "date": "2024-12-27", "relevancy": 2.0782, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5408}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.505}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goal-oriented%20Communications%20based%20on%20Recursive%20Early%20Exit%20Neural%0A%20%20Networks&body=Title%3A%20Goal-oriented%20Communications%20based%20on%20Recursive%20Early%20Exit%20Neural%0A%20%20Networks%0AAuthor%3A%20Jary%20Pomponi%20and%20Mattia%20Merluzzi%20and%20Alessio%20Devoto%20and%20Mateus%20Pontes%20Mota%20and%20Paolo%20Di%20Lorenzo%20and%20Simone%20Scardapane%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20for%20goal-oriented%20semantic%0Acommunications%20leveraging%20recursive%20early%20exit%20models.%20The%20proposed%20approach%20is%0Abuilt%20on%20two%20key%20components.%20First%2C%20we%20introduce%20an%20innovative%20early%20exit%0Astrategy%20that%20dynamically%20partitions%20computations%2C%20enabling%20samples%20to%20be%0Aoffloaded%20to%20a%20server%20based%20on%20layer-wise%20recursive%20prediction%20dynamics%20that%0Adetect%20samples%20for%20which%20the%20confidence%20is%20not%20increasing%20fast%20enough%20over%0Alayers.%20Second%2C%20we%20develop%20a%20Reinforcement%20Learning-based%20online%20optimization%0Aframework%20that%20jointly%20determines%20early%20exit%20points%2C%20computation%20splitting%2C%20and%0Aoffloading%20strategies%2C%20while%20accounting%20for%20wireless%20conditions%2C%20inference%0Aaccuracy%2C%20and%20resource%20costs.%20Numerical%20evaluations%20in%20an%20edge%20inference%0Ascenario%20demonstrate%20the%20method%27s%20adaptability%20and%20effectiveness%20in%20striking%20an%0Aexcellent%20trade-off%20between%20performance%2C%20latency%2C%20and%20resource%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoal-oriented%2520Communications%2520based%2520on%2520Recursive%2520Early%2520Exit%2520Neural%250A%2520%2520Networks%26entry.906535625%3DJary%2520Pomponi%2520and%2520Mattia%2520Merluzzi%2520and%2520Alessio%2520Devoto%2520and%2520Mateus%2520Pontes%2520Mota%2520and%2520Paolo%2520Di%2520Lorenzo%2520and%2520Simone%2520Scardapane%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520for%2520goal-oriented%2520semantic%250Acommunications%2520leveraging%2520recursive%2520early%2520exit%2520models.%2520The%2520proposed%2520approach%2520is%250Abuilt%2520on%2520two%2520key%2520components.%2520First%252C%2520we%2520introduce%2520an%2520innovative%2520early%2520exit%250Astrategy%2520that%2520dynamically%2520partitions%2520computations%252C%2520enabling%2520samples%2520to%2520be%250Aoffloaded%2520to%2520a%2520server%2520based%2520on%2520layer-wise%2520recursive%2520prediction%2520dynamics%2520that%250Adetect%2520samples%2520for%2520which%2520the%2520confidence%2520is%2520not%2520increasing%2520fast%2520enough%2520over%250Alayers.%2520Second%252C%2520we%2520develop%2520a%2520Reinforcement%2520Learning-based%2520online%2520optimization%250Aframework%2520that%2520jointly%2520determines%2520early%2520exit%2520points%252C%2520computation%2520splitting%252C%2520and%250Aoffloading%2520strategies%252C%2520while%2520accounting%2520for%2520wireless%2520conditions%252C%2520inference%250Aaccuracy%252C%2520and%2520resource%2520costs.%2520Numerical%2520evaluations%2520in%2520an%2520edge%2520inference%250Ascenario%2520demonstrate%2520the%2520method%2527s%2520adaptability%2520and%2520effectiveness%2520in%2520striking%2520an%250Aexcellent%2520trade-off%2520between%2520performance%252C%2520latency%252C%2520and%2520resource%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal-oriented%20Communications%20based%20on%20Recursive%20Early%20Exit%20Neural%0A%20%20Networks&entry.906535625=Jary%20Pomponi%20and%20Mattia%20Merluzzi%20and%20Alessio%20Devoto%20and%20Mateus%20Pontes%20Mota%20and%20Paolo%20Di%20Lorenzo%20and%20Simone%20Scardapane&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20for%20goal-oriented%20semantic%0Acommunications%20leveraging%20recursive%20early%20exit%20models.%20The%20proposed%20approach%20is%0Abuilt%20on%20two%20key%20components.%20First%2C%20we%20introduce%20an%20innovative%20early%20exit%0Astrategy%20that%20dynamically%20partitions%20computations%2C%20enabling%20samples%20to%20be%0Aoffloaded%20to%20a%20server%20based%20on%20layer-wise%20recursive%20prediction%20dynamics%20that%0Adetect%20samples%20for%20which%20the%20confidence%20is%20not%20increasing%20fast%20enough%20over%0Alayers.%20Second%2C%20we%20develop%20a%20Reinforcement%20Learning-based%20online%20optimization%0Aframework%20that%20jointly%20determines%20early%20exit%20points%2C%20computation%20splitting%2C%20and%0Aoffloading%20strategies%2C%20while%20accounting%20for%20wireless%20conditions%2C%20inference%0Aaccuracy%2C%20and%20resource%20costs.%20Numerical%20evaluations%20in%20an%20edge%20inference%0Ascenario%20demonstrate%20the%20method%27s%20adaptability%20and%20effectiveness%20in%20striking%20an%0Aexcellent%20trade-off%20between%20performance%2C%20latency%2C%20and%20resource%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19587v1&entry.124074799=Read"},
{"title": "Gradient Weight-normalized Low-rank Projection for Efficient LLM\n  Training", "author": "Jia-Hong Huang and Yixian Shen and Hongyi Zhu and Stevan Rudinac and Evangelos Kanoulas", "abstract": "  Large Language Models (LLMs) have shown remarkable performance across various\ntasks, but the escalating demands on computational resources pose significant\nchallenges, particularly in the extensive utilization of full fine-tuning for\ndownstream tasks. To address this, parameter-efficient fine-tuning (PEFT)\nmethods have been developed, but they often underperform compared to full\nfine-tuning and struggle with memory efficiency. In this work, we introduce\nGradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach\nthat enhances both parameter and memory efficiency while maintaining comparable\nperformance to full fine-tuning. GradNormLoRP normalizes the weight matrix to\nimprove gradient conditioning, facilitating better convergence during\noptimization. Additionally, it applies low-rank approximations to the weight\nand gradient matrices, significantly reducing memory usage during training.\nExtensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer\nmemory usage by up to 89.5% and enables the pre-training of large LLMs, such as\nLLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional\ninference costs. Moreover, GradNormLoRP outperforms existing low-rank methods\nin fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all\nGLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65,\nsurpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a\npromising alternative for efficient LLM pre-training and fine-tuning. Source\ncode and Appendix:\nhttps://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training\n", "link": "http://arxiv.org/abs/2412.19616v1", "date": "2024-12-27", "relevancy": 2.0417, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.514}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5128}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Weight-normalized%20Low-rank%20Projection%20for%20Efficient%20LLM%0A%20%20Training&body=Title%3A%20Gradient%20Weight-normalized%20Low-rank%20Projection%20for%20Efficient%20LLM%0A%20%20Training%0AAuthor%3A%20Jia-Hong%20Huang%20and%20Yixian%20Shen%20and%20Hongyi%20Zhu%20and%20Stevan%20Rudinac%20and%20Evangelos%20Kanoulas%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20across%20various%0Atasks%2C%20but%20the%20escalating%20demands%20on%20computational%20resources%20pose%20significant%0Achallenges%2C%20particularly%20in%20the%20extensive%20utilization%20of%20full%20fine-tuning%20for%0Adownstream%20tasks.%20To%20address%20this%2C%20parameter-efficient%20fine-tuning%20%28PEFT%29%0Amethods%20have%20been%20developed%2C%20but%20they%20often%20underperform%20compared%20to%20full%0Afine-tuning%20and%20struggle%20with%20memory%20efficiency.%20In%20this%20work%2C%20we%20introduce%0AGradient%20Weight-Normalized%20Low-Rank%20Projection%20%28GradNormLoRP%29%2C%20a%20novel%20approach%0Athat%20enhances%20both%20parameter%20and%20memory%20efficiency%20while%20maintaining%20comparable%0Aperformance%20to%20full%20fine-tuning.%20GradNormLoRP%20normalizes%20the%20weight%20matrix%20to%0Aimprove%20gradient%20conditioning%2C%20facilitating%20better%20convergence%20during%0Aoptimization.%20Additionally%2C%20it%20applies%20low-rank%20approximations%20to%20the%20weight%0Aand%20gradient%20matrices%2C%20significantly%20reducing%20memory%20usage%20during%20training.%0AExtensive%20experiments%20demonstrate%20that%20our%208-bit%20GradNormLoRP%20reduces%20optimizer%0Amemory%20usage%20by%20up%20to%2089.5%25%20and%20enables%20the%20pre-training%20of%20large%20LLMs%2C%20such%20as%0ALLaMA%207B%2C%20on%20consumer-level%20GPUs%20like%20the%20NVIDIA%20RTX%204090%2C%20without%20additional%0Ainference%20costs.%20Moreover%2C%20GradNormLoRP%20outperforms%20existing%20low-rank%20methods%0Ain%20fine-tuning%20tasks.%20For%20instance%2C%20when%20fine-tuning%20the%20RoBERTa%20model%20on%20all%0AGLUE%20tasks%20with%20a%20rank%20of%208%2C%20GradNormLoRP%20achieves%20an%20average%20score%20of%2080.65%2C%0Asurpassing%20LoRA%27s%20score%20of%2079.23.%20These%20results%20underscore%20GradNormLoRP%20as%20a%0Apromising%20alternative%20for%20efficient%20LLM%20pre-training%20and%20fine-tuning.%20Source%0Acode%20and%20Appendix%3A%0Ahttps%3A//github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Weight-normalized%2520Low-rank%2520Projection%2520for%2520Efficient%2520LLM%250A%2520%2520Training%26entry.906535625%3DJia-Hong%2520Huang%2520and%2520Yixian%2520Shen%2520and%2520Hongyi%2520Zhu%2520and%2520Stevan%2520Rudinac%2520and%2520Evangelos%2520Kanoulas%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520performance%2520across%2520various%250Atasks%252C%2520but%2520the%2520escalating%2520demands%2520on%2520computational%2520resources%2520pose%2520significant%250Achallenges%252C%2520particularly%2520in%2520the%2520extensive%2520utilization%2520of%2520full%2520fine-tuning%2520for%250Adownstream%2520tasks.%2520To%2520address%2520this%252C%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%250Amethods%2520have%2520been%2520developed%252C%2520but%2520they%2520often%2520underperform%2520compared%2520to%2520full%250Afine-tuning%2520and%2520struggle%2520with%2520memory%2520efficiency.%2520In%2520this%2520work%252C%2520we%2520introduce%250AGradient%2520Weight-Normalized%2520Low-Rank%2520Projection%2520%2528GradNormLoRP%2529%252C%2520a%2520novel%2520approach%250Athat%2520enhances%2520both%2520parameter%2520and%2520memory%2520efficiency%2520while%2520maintaining%2520comparable%250Aperformance%2520to%2520full%2520fine-tuning.%2520GradNormLoRP%2520normalizes%2520the%2520weight%2520matrix%2520to%250Aimprove%2520gradient%2520conditioning%252C%2520facilitating%2520better%2520convergence%2520during%250Aoptimization.%2520Additionally%252C%2520it%2520applies%2520low-rank%2520approximations%2520to%2520the%2520weight%250Aand%2520gradient%2520matrices%252C%2520significantly%2520reducing%2520memory%2520usage%2520during%2520training.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%25208-bit%2520GradNormLoRP%2520reduces%2520optimizer%250Amemory%2520usage%2520by%2520up%2520to%252089.5%2525%2520and%2520enables%2520the%2520pre-training%2520of%2520large%2520LLMs%252C%2520such%2520as%250ALLaMA%25207B%252C%2520on%2520consumer-level%2520GPUs%2520like%2520the%2520NVIDIA%2520RTX%25204090%252C%2520without%2520additional%250Ainference%2520costs.%2520Moreover%252C%2520GradNormLoRP%2520outperforms%2520existing%2520low-rank%2520methods%250Ain%2520fine-tuning%2520tasks.%2520For%2520instance%252C%2520when%2520fine-tuning%2520the%2520RoBERTa%2520model%2520on%2520all%250AGLUE%2520tasks%2520with%2520a%2520rank%2520of%25208%252C%2520GradNormLoRP%2520achieves%2520an%2520average%2520score%2520of%252080.65%252C%250Asurpassing%2520LoRA%2527s%2520score%2520of%252079.23.%2520These%2520results%2520underscore%2520GradNormLoRP%2520as%2520a%250Apromising%2520alternative%2520for%2520efficient%2520LLM%2520pre-training%2520and%2520fine-tuning.%2520Source%250Acode%2520and%2520Appendix%253A%250Ahttps%253A//github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Weight-normalized%20Low-rank%20Projection%20for%20Efficient%20LLM%0A%20%20Training&entry.906535625=Jia-Hong%20Huang%20and%20Yixian%20Shen%20and%20Hongyi%20Zhu%20and%20Stevan%20Rudinac%20and%20Evangelos%20Kanoulas&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20performance%20across%20various%0Atasks%2C%20but%20the%20escalating%20demands%20on%20computational%20resources%20pose%20significant%0Achallenges%2C%20particularly%20in%20the%20extensive%20utilization%20of%20full%20fine-tuning%20for%0Adownstream%20tasks.%20To%20address%20this%2C%20parameter-efficient%20fine-tuning%20%28PEFT%29%0Amethods%20have%20been%20developed%2C%20but%20they%20often%20underperform%20compared%20to%20full%0Afine-tuning%20and%20struggle%20with%20memory%20efficiency.%20In%20this%20work%2C%20we%20introduce%0AGradient%20Weight-Normalized%20Low-Rank%20Projection%20%28GradNormLoRP%29%2C%20a%20novel%20approach%0Athat%20enhances%20both%20parameter%20and%20memory%20efficiency%20while%20maintaining%20comparable%0Aperformance%20to%20full%20fine-tuning.%20GradNormLoRP%20normalizes%20the%20weight%20matrix%20to%0Aimprove%20gradient%20conditioning%2C%20facilitating%20better%20convergence%20during%0Aoptimization.%20Additionally%2C%20it%20applies%20low-rank%20approximations%20to%20the%20weight%0Aand%20gradient%20matrices%2C%20significantly%20reducing%20memory%20usage%20during%20training.%0AExtensive%20experiments%20demonstrate%20that%20our%208-bit%20GradNormLoRP%20reduces%20optimizer%0Amemory%20usage%20by%20up%20to%2089.5%25%20and%20enables%20the%20pre-training%20of%20large%20LLMs%2C%20such%20as%0ALLaMA%207B%2C%20on%20consumer-level%20GPUs%20like%20the%20NVIDIA%20RTX%204090%2C%20without%20additional%0Ainference%20costs.%20Moreover%2C%20GradNormLoRP%20outperforms%20existing%20low-rank%20methods%0Ain%20fine-tuning%20tasks.%20For%20instance%2C%20when%20fine-tuning%20the%20RoBERTa%20model%20on%20all%0AGLUE%20tasks%20with%20a%20rank%20of%208%2C%20GradNormLoRP%20achieves%20an%20average%20score%20of%2080.65%2C%0Asurpassing%20LoRA%27s%20score%20of%2079.23.%20These%20results%20underscore%20GradNormLoRP%20as%20a%0Apromising%20alternative%20for%20efficient%20LLM%20pre-training%20and%20fine-tuning.%20Source%0Acode%20and%20Appendix%3A%0Ahttps%3A//github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19616v1&entry.124074799=Read"},
{"title": "Xmodel-2 Technical Report", "author": "Wang Qun and Liu Yang and Lin Qingquan and Qu Zhijiu and Jiang Ling", "abstract": "  Xmodel-2 is a 1.2-billion-parameter large language model designed\nspecifically for reasoning tasks. Its architecture enables different model\nscales to share a unified set of hyperparameters, allowing for extensive\nexperimentation on smaller models and seamless transfer of optimal\nconfigurations to larger models. To maximize training efficiency and stability,\nXmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on\n1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art\nperformance in complex reasoning and agent-based tasks, while maintaining low\ntraining costs. These results highlight the potential of efficient model design\nand training strategies in advancing reasoning capabilities. Model checkpoints\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/Xmodel-2\n", "link": "http://arxiv.org/abs/2412.19638v1", "date": "2024-12-27", "relevancy": 2.0314, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Xmodel-2%20Technical%20Report&body=Title%3A%20Xmodel-2%20Technical%20Report%0AAuthor%3A%20Wang%20Qun%20and%20Liu%20Yang%20and%20Lin%20Qingquan%20and%20Qu%20Zhijiu%20and%20Jiang%20Ling%0AAbstract%3A%20%20%20Xmodel-2%20is%20a%201.2-billion-parameter%20large%20language%20model%20designed%0Aspecifically%20for%20reasoning%20tasks.%20Its%20architecture%20enables%20different%20model%0Ascales%20to%20share%20a%20unified%20set%20of%20hyperparameters%2C%20allowing%20for%20extensive%0Aexperimentation%20on%20smaller%20models%20and%20seamless%20transfer%20of%20optimal%0Aconfigurations%20to%20larger%20models.%20To%20maximize%20training%20efficiency%20and%20stability%2C%0AXmodel-2%20employs%20the%20WSD%20learning%20rate%20scheduler%20from%20MiniCPM.%20Pretrained%20on%0A1.5%20trillion%20tokens%20from%20diverse%20sources%2C%20Xmodel-2%20achieves%20state-of-the-art%0Aperformance%20in%20complex%20reasoning%20and%20agent-based%20tasks%2C%20while%20maintaining%20low%0Atraining%20costs.%20These%20results%20highlight%20the%20potential%20of%20efficient%20model%20design%0Aand%20training%20strategies%20in%20advancing%20reasoning%20capabilities.%20Model%20checkpoints%0Aand%20code%20are%20publicly%20available%20on%20GitHub%20at%0Ahttps%3A//github.com/XiaoduoAILab/Xmodel-2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXmodel-2%2520Technical%2520Report%26entry.906535625%3DWang%2520Qun%2520and%2520Liu%2520Yang%2520and%2520Lin%2520Qingquan%2520and%2520Qu%2520Zhijiu%2520and%2520Jiang%2520Ling%26entry.1292438233%3D%2520%2520Xmodel-2%2520is%2520a%25201.2-billion-parameter%2520large%2520language%2520model%2520designed%250Aspecifically%2520for%2520reasoning%2520tasks.%2520Its%2520architecture%2520enables%2520different%2520model%250Ascales%2520to%2520share%2520a%2520unified%2520set%2520of%2520hyperparameters%252C%2520allowing%2520for%2520extensive%250Aexperimentation%2520on%2520smaller%2520models%2520and%2520seamless%2520transfer%2520of%2520optimal%250Aconfigurations%2520to%2520larger%2520models.%2520To%2520maximize%2520training%2520efficiency%2520and%2520stability%252C%250AXmodel-2%2520employs%2520the%2520WSD%2520learning%2520rate%2520scheduler%2520from%2520MiniCPM.%2520Pretrained%2520on%250A1.5%2520trillion%2520tokens%2520from%2520diverse%2520sources%252C%2520Xmodel-2%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520complex%2520reasoning%2520and%2520agent-based%2520tasks%252C%2520while%2520maintaining%2520low%250Atraining%2520costs.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520efficient%2520model%2520design%250Aand%2520training%2520strategies%2520in%2520advancing%2520reasoning%2520capabilities.%2520Model%2520checkpoints%250Aand%2520code%2520are%2520publicly%2520available%2520on%2520GitHub%2520at%250Ahttps%253A//github.com/XiaoduoAILab/Xmodel-2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Xmodel-2%20Technical%20Report&entry.906535625=Wang%20Qun%20and%20Liu%20Yang%20and%20Lin%20Qingquan%20and%20Qu%20Zhijiu%20and%20Jiang%20Ling&entry.1292438233=%20%20Xmodel-2%20is%20a%201.2-billion-parameter%20large%20language%20model%20designed%0Aspecifically%20for%20reasoning%20tasks.%20Its%20architecture%20enables%20different%20model%0Ascales%20to%20share%20a%20unified%20set%20of%20hyperparameters%2C%20allowing%20for%20extensive%0Aexperimentation%20on%20smaller%20models%20and%20seamless%20transfer%20of%20optimal%0Aconfigurations%20to%20larger%20models.%20To%20maximize%20training%20efficiency%20and%20stability%2C%0AXmodel-2%20employs%20the%20WSD%20learning%20rate%20scheduler%20from%20MiniCPM.%20Pretrained%20on%0A1.5%20trillion%20tokens%20from%20diverse%20sources%2C%20Xmodel-2%20achieves%20state-of-the-art%0Aperformance%20in%20complex%20reasoning%20and%20agent-based%20tasks%2C%20while%20maintaining%20low%0Atraining%20costs.%20These%20results%20highlight%20the%20potential%20of%20efficient%20model%20design%0Aand%20training%20strategies%20in%20advancing%20reasoning%20capabilities.%20Model%20checkpoints%0Aand%20code%20are%20publicly%20available%20on%20GitHub%20at%0Ahttps%3A//github.com/XiaoduoAILab/Xmodel-2%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19638v1&entry.124074799=Read"},
{"title": "Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free,\n  Adaptive, Universal Prompt Optimization Framework", "author": "Jiang Liu and Bolin Li and Haoyuan Li and Tianwei Lin and Wenqiao Zhang and Tao Zhong and Zhelun Yu and Jinghao Wei and Hao Cheng and Hao Jiang and Zheqi Lv and Juncheng Li and Siliang Tang and Yueting Zhuang", "abstract": "  Efficient multimodal large language models (EMLLMs), in contrast to\nmultimodal large language models (MLLMs), reduce model size and computational\ncosts and are often deployed on resource-constrained devices. However, due to\ndata privacy concerns, existing open-source EMLLMs rarely have access to\nprivate domain-specific data during the pre-training process, making them\ndifficult to directly apply in device-specific domains, such as certain\nbusiness scenarios. To address this weakness, this paper focuses on the\nefficient adaptation of EMLLMs to private domains, specifically in two areas:\n1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.\nSpecifically, we propose a tun\\textbf{\\underline{I}}ng-free,\na\\textbf{\\underline{D}}aptiv\\textbf{\\underline{E}},\nunivers\\textbf{\\underline{AL}} \\textbf{\\underline{Prompt}} Optimization\nFramework, abbreviated as \\textit{\\textbf{\\ourmethod{}}} which consists of two\nstages: 1) Predefined Prompt, based on the reinforcement searching strategy,\ngenerate a prompt optimization strategy tree to acquire optimization priors; 2)\nPrompt Reflection initializes the prompt based on optimization priors, followed\nby self-reflection to further search and refine the prompt. By doing so,\n\\ourmethod{} elegantly generates the ``ideal prompts'' for processing private\ndomain-specific data. Note that our method requires no parameter fine-tuning\nand only a small amount of data to quickly adapt to the data distribution of\nprivate data. Extensive experiments across multiple tasks demonstrate that our\nproposed \\ourmethod{} significantly improves both efficiency and performance\ncompared to baselines.\n", "link": "http://arxiv.org/abs/2412.19684v1", "date": "2024-12-27", "relevancy": 2.022, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5061}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Private%20Domain%20Understanding%20of%20Efficient%20MLLMs%3A%20A%20Tuning-free%2C%0A%20%20Adaptive%2C%20Universal%20Prompt%20Optimization%20Framework&body=Title%3A%20Boosting%20Private%20Domain%20Understanding%20of%20Efficient%20MLLMs%3A%20A%20Tuning-free%2C%0A%20%20Adaptive%2C%20Universal%20Prompt%20Optimization%20Framework%0AAuthor%3A%20Jiang%20Liu%20and%20Bolin%20Li%20and%20Haoyuan%20Li%20and%20Tianwei%20Lin%20and%20Wenqiao%20Zhang%20and%20Tao%20Zhong%20and%20Zhelun%20Yu%20and%20Jinghao%20Wei%20and%20Hao%20Cheng%20and%20Hao%20Jiang%20and%20Zheqi%20Lv%20and%20Juncheng%20Li%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Efficient%20multimodal%20large%20language%20models%20%28EMLLMs%29%2C%20in%20contrast%20to%0Amultimodal%20large%20language%20models%20%28MLLMs%29%2C%20reduce%20model%20size%20and%20computational%0Acosts%20and%20are%20often%20deployed%20on%20resource-constrained%20devices.%20However%2C%20due%20to%0Adata%20privacy%20concerns%2C%20existing%20open-source%20EMLLMs%20rarely%20have%20access%20to%0Aprivate%20domain-specific%20data%20during%20the%20pre-training%20process%2C%20making%20them%0Adifficult%20to%20directly%20apply%20in%20device-specific%20domains%2C%20such%20as%20certain%0Abusiness%20scenarios.%20To%20address%20this%20weakness%2C%20this%20paper%20focuses%20on%20the%0Aefficient%20adaptation%20of%20EMLLMs%20to%20private%20domains%2C%20specifically%20in%20two%20areas%3A%0A1%29%20how%20to%20reduce%20data%20requirements%2C%20and%202%29%20how%20to%20avoid%20parameter%20fine-tuning.%0ASpecifically%2C%20we%20propose%20a%20tun%5Ctextbf%7B%5Cunderline%7BI%7D%7Dng-free%2C%0Aa%5Ctextbf%7B%5Cunderline%7BD%7D%7Daptiv%5Ctextbf%7B%5Cunderline%7BE%7D%7D%2C%0Aunivers%5Ctextbf%7B%5Cunderline%7BAL%7D%7D%20%5Ctextbf%7B%5Cunderline%7BPrompt%7D%7D%20Optimization%0AFramework%2C%20abbreviated%20as%20%5Ctextit%7B%5Ctextbf%7B%5Courmethod%7B%7D%7D%7D%20which%20consists%20of%20two%0Astages%3A%201%29%20Predefined%20Prompt%2C%20based%20on%20the%20reinforcement%20searching%20strategy%2C%0Agenerate%20a%20prompt%20optimization%20strategy%20tree%20to%20acquire%20optimization%20priors%3B%202%29%0APrompt%20Reflection%20initializes%20the%20prompt%20based%20on%20optimization%20priors%2C%20followed%0Aby%20self-reflection%20to%20further%20search%20and%20refine%20the%20prompt.%20By%20doing%20so%2C%0A%5Courmethod%7B%7D%20elegantly%20generates%20the%20%60%60ideal%20prompts%27%27%20for%20processing%20private%0Adomain-specific%20data.%20Note%20that%20our%20method%20requires%20no%20parameter%20fine-tuning%0Aand%20only%20a%20small%20amount%20of%20data%20to%20quickly%20adapt%20to%20the%20data%20distribution%20of%0Aprivate%20data.%20Extensive%20experiments%20across%20multiple%20tasks%20demonstrate%20that%20our%0Aproposed%20%5Courmethod%7B%7D%20significantly%20improves%20both%20efficiency%20and%20performance%0Acompared%20to%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Private%2520Domain%2520Understanding%2520of%2520Efficient%2520MLLMs%253A%2520A%2520Tuning-free%252C%250A%2520%2520Adaptive%252C%2520Universal%2520Prompt%2520Optimization%2520Framework%26entry.906535625%3DJiang%2520Liu%2520and%2520Bolin%2520Li%2520and%2520Haoyuan%2520Li%2520and%2520Tianwei%2520Lin%2520and%2520Wenqiao%2520Zhang%2520and%2520Tao%2520Zhong%2520and%2520Zhelun%2520Yu%2520and%2520Jinghao%2520Wei%2520and%2520Hao%2520Cheng%2520and%2520Hao%2520Jiang%2520and%2520Zheqi%2520Lv%2520and%2520Juncheng%2520Li%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Efficient%2520multimodal%2520large%2520language%2520models%2520%2528EMLLMs%2529%252C%2520in%2520contrast%2520to%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520reduce%2520model%2520size%2520and%2520computational%250Acosts%2520and%2520are%2520often%2520deployed%2520on%2520resource-constrained%2520devices.%2520However%252C%2520due%2520to%250Adata%2520privacy%2520concerns%252C%2520existing%2520open-source%2520EMLLMs%2520rarely%2520have%2520access%2520to%250Aprivate%2520domain-specific%2520data%2520during%2520the%2520pre-training%2520process%252C%2520making%2520them%250Adifficult%2520to%2520directly%2520apply%2520in%2520device-specific%2520domains%252C%2520such%2520as%2520certain%250Abusiness%2520scenarios.%2520To%2520address%2520this%2520weakness%252C%2520this%2520paper%2520focuses%2520on%2520the%250Aefficient%2520adaptation%2520of%2520EMLLMs%2520to%2520private%2520domains%252C%2520specifically%2520in%2520two%2520areas%253A%250A1%2529%2520how%2520to%2520reduce%2520data%2520requirements%252C%2520and%25202%2529%2520how%2520to%2520avoid%2520parameter%2520fine-tuning.%250ASpecifically%252C%2520we%2520propose%2520a%2520tun%255Ctextbf%257B%255Cunderline%257BI%257D%257Dng-free%252C%250Aa%255Ctextbf%257B%255Cunderline%257BD%257D%257Daptiv%255Ctextbf%257B%255Cunderline%257BE%257D%257D%252C%250Aunivers%255Ctextbf%257B%255Cunderline%257BAL%257D%257D%2520%255Ctextbf%257B%255Cunderline%257BPrompt%257D%257D%2520Optimization%250AFramework%252C%2520abbreviated%2520as%2520%255Ctextit%257B%255Ctextbf%257B%255Courmethod%257B%257D%257D%257D%2520which%2520consists%2520of%2520two%250Astages%253A%25201%2529%2520Predefined%2520Prompt%252C%2520based%2520on%2520the%2520reinforcement%2520searching%2520strategy%252C%250Agenerate%2520a%2520prompt%2520optimization%2520strategy%2520tree%2520to%2520acquire%2520optimization%2520priors%253B%25202%2529%250APrompt%2520Reflection%2520initializes%2520the%2520prompt%2520based%2520on%2520optimization%2520priors%252C%2520followed%250Aby%2520self-reflection%2520to%2520further%2520search%2520and%2520refine%2520the%2520prompt.%2520By%2520doing%2520so%252C%250A%255Courmethod%257B%257D%2520elegantly%2520generates%2520the%2520%2560%2560ideal%2520prompts%2527%2527%2520for%2520processing%2520private%250Adomain-specific%2520data.%2520Note%2520that%2520our%2520method%2520requires%2520no%2520parameter%2520fine-tuning%250Aand%2520only%2520a%2520small%2520amount%2520of%2520data%2520to%2520quickly%2520adapt%2520to%2520the%2520data%2520distribution%2520of%250Aprivate%2520data.%2520Extensive%2520experiments%2520across%2520multiple%2520tasks%2520demonstrate%2520that%2520our%250Aproposed%2520%255Courmethod%257B%257D%2520significantly%2520improves%2520both%2520efficiency%2520and%2520performance%250Acompared%2520to%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Private%20Domain%20Understanding%20of%20Efficient%20MLLMs%3A%20A%20Tuning-free%2C%0A%20%20Adaptive%2C%20Universal%20Prompt%20Optimization%20Framework&entry.906535625=Jiang%20Liu%20and%20Bolin%20Li%20and%20Haoyuan%20Li%20and%20Tianwei%20Lin%20and%20Wenqiao%20Zhang%20and%20Tao%20Zhong%20and%20Zhelun%20Yu%20and%20Jinghao%20Wei%20and%20Hao%20Cheng%20and%20Hao%20Jiang%20and%20Zheqi%20Lv%20and%20Juncheng%20Li%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Efficient%20multimodal%20large%20language%20models%20%28EMLLMs%29%2C%20in%20contrast%20to%0Amultimodal%20large%20language%20models%20%28MLLMs%29%2C%20reduce%20model%20size%20and%20computational%0Acosts%20and%20are%20often%20deployed%20on%20resource-constrained%20devices.%20However%2C%20due%20to%0Adata%20privacy%20concerns%2C%20existing%20open-source%20EMLLMs%20rarely%20have%20access%20to%0Aprivate%20domain-specific%20data%20during%20the%20pre-training%20process%2C%20making%20them%0Adifficult%20to%20directly%20apply%20in%20device-specific%20domains%2C%20such%20as%20certain%0Abusiness%20scenarios.%20To%20address%20this%20weakness%2C%20this%20paper%20focuses%20on%20the%0Aefficient%20adaptation%20of%20EMLLMs%20to%20private%20domains%2C%20specifically%20in%20two%20areas%3A%0A1%29%20how%20to%20reduce%20data%20requirements%2C%20and%202%29%20how%20to%20avoid%20parameter%20fine-tuning.%0ASpecifically%2C%20we%20propose%20a%20tun%5Ctextbf%7B%5Cunderline%7BI%7D%7Dng-free%2C%0Aa%5Ctextbf%7B%5Cunderline%7BD%7D%7Daptiv%5Ctextbf%7B%5Cunderline%7BE%7D%7D%2C%0Aunivers%5Ctextbf%7B%5Cunderline%7BAL%7D%7D%20%5Ctextbf%7B%5Cunderline%7BPrompt%7D%7D%20Optimization%0AFramework%2C%20abbreviated%20as%20%5Ctextit%7B%5Ctextbf%7B%5Courmethod%7B%7D%7D%7D%20which%20consists%20of%20two%0Astages%3A%201%29%20Predefined%20Prompt%2C%20based%20on%20the%20reinforcement%20searching%20strategy%2C%0Agenerate%20a%20prompt%20optimization%20strategy%20tree%20to%20acquire%20optimization%20priors%3B%202%29%0APrompt%20Reflection%20initializes%20the%20prompt%20based%20on%20optimization%20priors%2C%20followed%0Aby%20self-reflection%20to%20further%20search%20and%20refine%20the%20prompt.%20By%20doing%20so%2C%0A%5Courmethod%7B%7D%20elegantly%20generates%20the%20%60%60ideal%20prompts%27%27%20for%20processing%20private%0Adomain-specific%20data.%20Note%20that%20our%20method%20requires%20no%20parameter%20fine-tuning%0Aand%20only%20a%20small%20amount%20of%20data%20to%20quickly%20adapt%20to%20the%20data%20distribution%20of%0Aprivate%20data.%20Extensive%20experiments%20across%20multiple%20tasks%20demonstrate%20that%20our%0Aproposed%20%5Courmethod%7B%7D%20significantly%20improves%20both%20efficiency%20and%20performance%0Acompared%20to%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19684v1&entry.124074799=Read"},
{"title": "A data driven approach to classify descriptors based on their efficiency\n  in translating noisy trajectories into physically-relevant information", "author": "Simone Martino and Domiziano Doria and Chiara Lionello and Matteo Becchi and Giovanni M. Pavan", "abstract": "  Reconstructing the physical complexity of many-body dynamical systems can be\nchallenging. Starting from the trajectories of their constitutive units (raw\ndata), typical approaches require selecting appropriate descriptors to convert\nthem into time-series, which are then analyzed to extract interpretable\ninformation. However, identifying the most effective descriptor is often\nnon-trivial. Here, we report a data-driven approach to compare the efficiency\nof various descriptors in extracting information from noisy trajectories and\ntranslating it into physically relevant insights. As a prototypical system with\nnon-trivial internal complexity, we analyze molecular dynamics trajectories of\nan atomistic system where ice and water coexist in equilibrium near the\nsolid/liquid transition temperature. We compare general and specific\ndescriptors often used in aqueous systems: number of neighbors, molecular\nvelocities, Smooth Overlap of Atomic Positions (SOAP), Local Environments and\nNeighbors Shuffling (LENS), Orientational Tetrahedral Order, and distance from\nthe fifth neighbor ($d_5$). Using Onion Clustering -- an efficient unsupervised\nmethod for single-point time-series analysis -- we assess the maximum\nextractable information for each descriptor and rank them via a\nhigh-dimensional metric. Our results show that advanced descriptors like SOAP\nand LENS outperform classical ones due to higher signal-to-noise ratios.\nNonetheless, even simple descriptors can rival or exceed advanced ones after\nlocal signal denoising. For example, $d_5$, initially among the weakest,\nbecomes the most effective at resolving the system's non-local dynamical\ncomplexity after denoising. This work highlights the critical role of noise in\ninformation extraction from molecular trajectories and offers a data-driven\napproach to identify optimal descriptors for systems with characteristic\ninternal complexity.\n", "link": "http://arxiv.org/abs/2411.12570v3", "date": "2024-12-27", "relevancy": 2.0113, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20data%20driven%20approach%20to%20classify%20descriptors%20based%20on%20their%20efficiency%0A%20%20in%20translating%20noisy%20trajectories%20into%20physically-relevant%20information&body=Title%3A%20A%20data%20driven%20approach%20to%20classify%20descriptors%20based%20on%20their%20efficiency%0A%20%20in%20translating%20noisy%20trajectories%20into%20physically-relevant%20information%0AAuthor%3A%20Simone%20Martino%20and%20Domiziano%20Doria%20and%20Chiara%20Lionello%20and%20Matteo%20Becchi%20and%20Giovanni%20M.%20Pavan%0AAbstract%3A%20%20%20Reconstructing%20the%20physical%20complexity%20of%20many-body%20dynamical%20systems%20can%20be%0Achallenging.%20Starting%20from%20the%20trajectories%20of%20their%20constitutive%20units%20%28raw%0Adata%29%2C%20typical%20approaches%20require%20selecting%20appropriate%20descriptors%20to%20convert%0Athem%20into%20time-series%2C%20which%20are%20then%20analyzed%20to%20extract%20interpretable%0Ainformation.%20However%2C%20identifying%20the%20most%20effective%20descriptor%20is%20often%0Anon-trivial.%20Here%2C%20we%20report%20a%20data-driven%20approach%20to%20compare%20the%20efficiency%0Aof%20various%20descriptors%20in%20extracting%20information%20from%20noisy%20trajectories%20and%0Atranslating%20it%20into%20physically%20relevant%20insights.%20As%20a%20prototypical%20system%20with%0Anon-trivial%20internal%20complexity%2C%20we%20analyze%20molecular%20dynamics%20trajectories%20of%0Aan%20atomistic%20system%20where%20ice%20and%20water%20coexist%20in%20equilibrium%20near%20the%0Asolid/liquid%20transition%20temperature.%20We%20compare%20general%20and%20specific%0Adescriptors%20often%20used%20in%20aqueous%20systems%3A%20number%20of%20neighbors%2C%20molecular%0Avelocities%2C%20Smooth%20Overlap%20of%20Atomic%20Positions%20%28SOAP%29%2C%20Local%20Environments%20and%0ANeighbors%20Shuffling%20%28LENS%29%2C%20Orientational%20Tetrahedral%20Order%2C%20and%20distance%20from%0Athe%20fifth%20neighbor%20%28%24d_5%24%29.%20Using%20Onion%20Clustering%20--%20an%20efficient%20unsupervised%0Amethod%20for%20single-point%20time-series%20analysis%20--%20we%20assess%20the%20maximum%0Aextractable%20information%20for%20each%20descriptor%20and%20rank%20them%20via%20a%0Ahigh-dimensional%20metric.%20Our%20results%20show%20that%20advanced%20descriptors%20like%20SOAP%0Aand%20LENS%20outperform%20classical%20ones%20due%20to%20higher%20signal-to-noise%20ratios.%0ANonetheless%2C%20even%20simple%20descriptors%20can%20rival%20or%20exceed%20advanced%20ones%20after%0Alocal%20signal%20denoising.%20For%20example%2C%20%24d_5%24%2C%20initially%20among%20the%20weakest%2C%0Abecomes%20the%20most%20effective%20at%20resolving%20the%20system%27s%20non-local%20dynamical%0Acomplexity%20after%20denoising.%20This%20work%20highlights%20the%20critical%20role%20of%20noise%20in%0Ainformation%20extraction%20from%20molecular%20trajectories%20and%20offers%20a%20data-driven%0Aapproach%20to%20identify%20optimal%20descriptors%20for%20systems%20with%20characteristic%0Ainternal%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12570v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520data%2520driven%2520approach%2520to%2520classify%2520descriptors%2520based%2520on%2520their%2520efficiency%250A%2520%2520in%2520translating%2520noisy%2520trajectories%2520into%2520physically-relevant%2520information%26entry.906535625%3DSimone%2520Martino%2520and%2520Domiziano%2520Doria%2520and%2520Chiara%2520Lionello%2520and%2520Matteo%2520Becchi%2520and%2520Giovanni%2520M.%2520Pavan%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520physical%2520complexity%2520of%2520many-body%2520dynamical%2520systems%2520can%2520be%250Achallenging.%2520Starting%2520from%2520the%2520trajectories%2520of%2520their%2520constitutive%2520units%2520%2528raw%250Adata%2529%252C%2520typical%2520approaches%2520require%2520selecting%2520appropriate%2520descriptors%2520to%2520convert%250Athem%2520into%2520time-series%252C%2520which%2520are%2520then%2520analyzed%2520to%2520extract%2520interpretable%250Ainformation.%2520However%252C%2520identifying%2520the%2520most%2520effective%2520descriptor%2520is%2520often%250Anon-trivial.%2520Here%252C%2520we%2520report%2520a%2520data-driven%2520approach%2520to%2520compare%2520the%2520efficiency%250Aof%2520various%2520descriptors%2520in%2520extracting%2520information%2520from%2520noisy%2520trajectories%2520and%250Atranslating%2520it%2520into%2520physically%2520relevant%2520insights.%2520As%2520a%2520prototypical%2520system%2520with%250Anon-trivial%2520internal%2520complexity%252C%2520we%2520analyze%2520molecular%2520dynamics%2520trajectories%2520of%250Aan%2520atomistic%2520system%2520where%2520ice%2520and%2520water%2520coexist%2520in%2520equilibrium%2520near%2520the%250Asolid/liquid%2520transition%2520temperature.%2520We%2520compare%2520general%2520and%2520specific%250Adescriptors%2520often%2520used%2520in%2520aqueous%2520systems%253A%2520number%2520of%2520neighbors%252C%2520molecular%250Avelocities%252C%2520Smooth%2520Overlap%2520of%2520Atomic%2520Positions%2520%2528SOAP%2529%252C%2520Local%2520Environments%2520and%250ANeighbors%2520Shuffling%2520%2528LENS%2529%252C%2520Orientational%2520Tetrahedral%2520Order%252C%2520and%2520distance%2520from%250Athe%2520fifth%2520neighbor%2520%2528%2524d_5%2524%2529.%2520Using%2520Onion%2520Clustering%2520--%2520an%2520efficient%2520unsupervised%250Amethod%2520for%2520single-point%2520time-series%2520analysis%2520--%2520we%2520assess%2520the%2520maximum%250Aextractable%2520information%2520for%2520each%2520descriptor%2520and%2520rank%2520them%2520via%2520a%250Ahigh-dimensional%2520metric.%2520Our%2520results%2520show%2520that%2520advanced%2520descriptors%2520like%2520SOAP%250Aand%2520LENS%2520outperform%2520classical%2520ones%2520due%2520to%2520higher%2520signal-to-noise%2520ratios.%250ANonetheless%252C%2520even%2520simple%2520descriptors%2520can%2520rival%2520or%2520exceed%2520advanced%2520ones%2520after%250Alocal%2520signal%2520denoising.%2520For%2520example%252C%2520%2524d_5%2524%252C%2520initially%2520among%2520the%2520weakest%252C%250Abecomes%2520the%2520most%2520effective%2520at%2520resolving%2520the%2520system%2527s%2520non-local%2520dynamical%250Acomplexity%2520after%2520denoising.%2520This%2520work%2520highlights%2520the%2520critical%2520role%2520of%2520noise%2520in%250Ainformation%2520extraction%2520from%2520molecular%2520trajectories%2520and%2520offers%2520a%2520data-driven%250Aapproach%2520to%2520identify%2520optimal%2520descriptors%2520for%2520systems%2520with%2520characteristic%250Ainternal%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12570v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20data%20driven%20approach%20to%20classify%20descriptors%20based%20on%20their%20efficiency%0A%20%20in%20translating%20noisy%20trajectories%20into%20physically-relevant%20information&entry.906535625=Simone%20Martino%20and%20Domiziano%20Doria%20and%20Chiara%20Lionello%20and%20Matteo%20Becchi%20and%20Giovanni%20M.%20Pavan&entry.1292438233=%20%20Reconstructing%20the%20physical%20complexity%20of%20many-body%20dynamical%20systems%20can%20be%0Achallenging.%20Starting%20from%20the%20trajectories%20of%20their%20constitutive%20units%20%28raw%0Adata%29%2C%20typical%20approaches%20require%20selecting%20appropriate%20descriptors%20to%20convert%0Athem%20into%20time-series%2C%20which%20are%20then%20analyzed%20to%20extract%20interpretable%0Ainformation.%20However%2C%20identifying%20the%20most%20effective%20descriptor%20is%20often%0Anon-trivial.%20Here%2C%20we%20report%20a%20data-driven%20approach%20to%20compare%20the%20efficiency%0Aof%20various%20descriptors%20in%20extracting%20information%20from%20noisy%20trajectories%20and%0Atranslating%20it%20into%20physically%20relevant%20insights.%20As%20a%20prototypical%20system%20with%0Anon-trivial%20internal%20complexity%2C%20we%20analyze%20molecular%20dynamics%20trajectories%20of%0Aan%20atomistic%20system%20where%20ice%20and%20water%20coexist%20in%20equilibrium%20near%20the%0Asolid/liquid%20transition%20temperature.%20We%20compare%20general%20and%20specific%0Adescriptors%20often%20used%20in%20aqueous%20systems%3A%20number%20of%20neighbors%2C%20molecular%0Avelocities%2C%20Smooth%20Overlap%20of%20Atomic%20Positions%20%28SOAP%29%2C%20Local%20Environments%20and%0ANeighbors%20Shuffling%20%28LENS%29%2C%20Orientational%20Tetrahedral%20Order%2C%20and%20distance%20from%0Athe%20fifth%20neighbor%20%28%24d_5%24%29.%20Using%20Onion%20Clustering%20--%20an%20efficient%20unsupervised%0Amethod%20for%20single-point%20time-series%20analysis%20--%20we%20assess%20the%20maximum%0Aextractable%20information%20for%20each%20descriptor%20and%20rank%20them%20via%20a%0Ahigh-dimensional%20metric.%20Our%20results%20show%20that%20advanced%20descriptors%20like%20SOAP%0Aand%20LENS%20outperform%20classical%20ones%20due%20to%20higher%20signal-to-noise%20ratios.%0ANonetheless%2C%20even%20simple%20descriptors%20can%20rival%20or%20exceed%20advanced%20ones%20after%0Alocal%20signal%20denoising.%20For%20example%2C%20%24d_5%24%2C%20initially%20among%20the%20weakest%2C%0Abecomes%20the%20most%20effective%20at%20resolving%20the%20system%27s%20non-local%20dynamical%0Acomplexity%20after%20denoising.%20This%20work%20highlights%20the%20critical%20role%20of%20noise%20in%0Ainformation%20extraction%20from%20molecular%20trajectories%20and%20offers%20a%20data-driven%0Aapproach%20to%20identify%20optimal%20descriptors%20for%20systems%20with%20characteristic%0Ainternal%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12570v3&entry.124074799=Read"},
{"title": "Chimera: A Block-Based Neural Architecture Search Framework for\n  Event-Based Object Detection", "author": "Diego A. Silva and Ahmed Elsheikh and Kamilya Smagulova and Mohammed E. Fouda and Ahmed M. Eltawil", "abstract": "  Event-based cameras are sensors that simulate the human eye, offering\nadvantages such as high-speed robustness and low power consumption. Established\nDeep Learning techniques have shown effectiveness in processing event data.\nChimera is a Block-Based Neural Architecture Search (NAS) framework\nspecifically designed for Event-Based Object Detection, aiming to create a\nsystematic approach for adapting RGB-domain processing methods to the event\ndomain. The Chimera design space is constructed from various macroblocks,\nincluding Attention blocks, Convolutions, State Space Models, and\nMLP-mixer-based architectures, which provide a valuable trade-off between local\nand global processing capabilities, as well as varying levels of complexity.\nThe results on the PErson Detection in Robotics (PEDRo) dataset demonstrated\nperformance levels comparable to leading state-of-the-art models, alongside an\naverage parameter reduction of 1.6 times.\n", "link": "http://arxiv.org/abs/2412.19646v1", "date": "2024-12-27", "relevancy": 2.0112, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.503}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chimera%3A%20A%20Block-Based%20Neural%20Architecture%20Search%20Framework%20for%0A%20%20Event-Based%20Object%20Detection&body=Title%3A%20Chimera%3A%20A%20Block-Based%20Neural%20Architecture%20Search%20Framework%20for%0A%20%20Event-Based%20Object%20Detection%0AAuthor%3A%20Diego%20A.%20Silva%20and%20Ahmed%20Elsheikh%20and%20Kamilya%20Smagulova%20and%20Mohammed%20E.%20Fouda%20and%20Ahmed%20M.%20Eltawil%0AAbstract%3A%20%20%20Event-based%20cameras%20are%20sensors%20that%20simulate%20the%20human%20eye%2C%20offering%0Aadvantages%20such%20as%20high-speed%20robustness%20and%20low%20power%20consumption.%20Established%0ADeep%20Learning%20techniques%20have%20shown%20effectiveness%20in%20processing%20event%20data.%0AChimera%20is%20a%20Block-Based%20Neural%20Architecture%20Search%20%28NAS%29%20framework%0Aspecifically%20designed%20for%20Event-Based%20Object%20Detection%2C%20aiming%20to%20create%20a%0Asystematic%20approach%20for%20adapting%20RGB-domain%20processing%20methods%20to%20the%20event%0Adomain.%20The%20Chimera%20design%20space%20is%20constructed%20from%20various%20macroblocks%2C%0Aincluding%20Attention%20blocks%2C%20Convolutions%2C%20State%20Space%20Models%2C%20and%0AMLP-mixer-based%20architectures%2C%20which%20provide%20a%20valuable%20trade-off%20between%20local%0Aand%20global%20processing%20capabilities%2C%20as%20well%20as%20varying%20levels%20of%20complexity.%0AThe%20results%20on%20the%20PErson%20Detection%20in%20Robotics%20%28PEDRo%29%20dataset%20demonstrated%0Aperformance%20levels%20comparable%20to%20leading%20state-of-the-art%20models%2C%20alongside%20an%0Aaverage%20parameter%20reduction%20of%201.6%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChimera%253A%2520A%2520Block-Based%2520Neural%2520Architecture%2520Search%2520Framework%2520for%250A%2520%2520Event-Based%2520Object%2520Detection%26entry.906535625%3DDiego%2520A.%2520Silva%2520and%2520Ahmed%2520Elsheikh%2520and%2520Kamilya%2520Smagulova%2520and%2520Mohammed%2520E.%2520Fouda%2520and%2520Ahmed%2520M.%2520Eltawil%26entry.1292438233%3D%2520%2520Event-based%2520cameras%2520are%2520sensors%2520that%2520simulate%2520the%2520human%2520eye%252C%2520offering%250Aadvantages%2520such%2520as%2520high-speed%2520robustness%2520and%2520low%2520power%2520consumption.%2520Established%250ADeep%2520Learning%2520techniques%2520have%2520shown%2520effectiveness%2520in%2520processing%2520event%2520data.%250AChimera%2520is%2520a%2520Block-Based%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%2520framework%250Aspecifically%2520designed%2520for%2520Event-Based%2520Object%2520Detection%252C%2520aiming%2520to%2520create%2520a%250Asystematic%2520approach%2520for%2520adapting%2520RGB-domain%2520processing%2520methods%2520to%2520the%2520event%250Adomain.%2520The%2520Chimera%2520design%2520space%2520is%2520constructed%2520from%2520various%2520macroblocks%252C%250Aincluding%2520Attention%2520blocks%252C%2520Convolutions%252C%2520State%2520Space%2520Models%252C%2520and%250AMLP-mixer-based%2520architectures%252C%2520which%2520provide%2520a%2520valuable%2520trade-off%2520between%2520local%250Aand%2520global%2520processing%2520capabilities%252C%2520as%2520well%2520as%2520varying%2520levels%2520of%2520complexity.%250AThe%2520results%2520on%2520the%2520PErson%2520Detection%2520in%2520Robotics%2520%2528PEDRo%2529%2520dataset%2520demonstrated%250Aperformance%2520levels%2520comparable%2520to%2520leading%2520state-of-the-art%2520models%252C%2520alongside%2520an%250Aaverage%2520parameter%2520reduction%2520of%25201.6%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chimera%3A%20A%20Block-Based%20Neural%20Architecture%20Search%20Framework%20for%0A%20%20Event-Based%20Object%20Detection&entry.906535625=Diego%20A.%20Silva%20and%20Ahmed%20Elsheikh%20and%20Kamilya%20Smagulova%20and%20Mohammed%20E.%20Fouda%20and%20Ahmed%20M.%20Eltawil&entry.1292438233=%20%20Event-based%20cameras%20are%20sensors%20that%20simulate%20the%20human%20eye%2C%20offering%0Aadvantages%20such%20as%20high-speed%20robustness%20and%20low%20power%20consumption.%20Established%0ADeep%20Learning%20techniques%20have%20shown%20effectiveness%20in%20processing%20event%20data.%0AChimera%20is%20a%20Block-Based%20Neural%20Architecture%20Search%20%28NAS%29%20framework%0Aspecifically%20designed%20for%20Event-Based%20Object%20Detection%2C%20aiming%20to%20create%20a%0Asystematic%20approach%20for%20adapting%20RGB-domain%20processing%20methods%20to%20the%20event%0Adomain.%20The%20Chimera%20design%20space%20is%20constructed%20from%20various%20macroblocks%2C%0Aincluding%20Attention%20blocks%2C%20Convolutions%2C%20State%20Space%20Models%2C%20and%0AMLP-mixer-based%20architectures%2C%20which%20provide%20a%20valuable%20trade-off%20between%20local%0Aand%20global%20processing%20capabilities%2C%20as%20well%20as%20varying%20levels%20of%20complexity.%0AThe%20results%20on%20the%20PErson%20Detection%20in%20Robotics%20%28PEDRo%29%20dataset%20demonstrated%0Aperformance%20levels%20comparable%20to%20leading%20state-of-the-art%20models%2C%20alongside%20an%0Aaverage%20parameter%20reduction%20of%201.6%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19646v1&entry.124074799=Read"},
{"title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and\n  Selective Sparsification", "author": "Junhui He and Shangyu Wu and Weidong Wen and Chun Jason Xue and Qingan Li", "abstract": "  Deploying large language models (LLMs) on edge devices presents significant\nchallenges due to the substantial computational overhead and memory\nrequirements. Activation sparsification can mitigate these resource challenges\nby reducing the number of activated neurons during inference. Existing methods\ntypically employ thresholding-based sparsification based on the statistics of\nactivation tensors. However, they do not model the impact of activation\nsparsification on performance, resulting in suboptimal performance degradation.\nTo address the limitations, this paper reformulates the activation\nsparsification problem to explicitly capture the relationship between\nactivation sparsity and model performance. Then, this paper proposes CHESS, a\ngeneral activation sparsification approach via CHannel-wise thrEsholding and\nSelective Sparsification. First, channel-wise thresholding assigns a unique\nthreshold to each activation channel in the feed-forward network (FFN) layers.\nThen, selective sparsification involves applying thresholding-based activation\nsparsification to specific layers within the attention modules. Finally, we\ndetail the implementation of sparse kernels to accelerate LLM inference.\nExperimental results demonstrate that the proposed CHESS achieves lower\nperformance degradation over eight downstream tasks while activating fewer\nparameters than existing methods, thus speeding up the LLM inference by up to\n1.27x.\n", "link": "http://arxiv.org/abs/2409.01366v2", "date": "2024-12-27", "relevancy": 1.9963, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5125}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHESS%3A%20Optimizing%20LLM%20Inference%20via%20Channel-Wise%20Thresholding%20and%0A%20%20Selective%20Sparsification&body=Title%3A%20CHESS%3A%20Optimizing%20LLM%20Inference%20via%20Channel-Wise%20Thresholding%20and%0A%20%20Selective%20Sparsification%0AAuthor%3A%20Junhui%20He%20and%20Shangyu%20Wu%20and%20Weidong%20Wen%20and%20Chun%20Jason%20Xue%20and%20Qingan%20Li%0AAbstract%3A%20%20%20Deploying%20large%20language%20models%20%28LLMs%29%20on%20edge%20devices%20presents%20significant%0Achallenges%20due%20to%20the%20substantial%20computational%20overhead%20and%20memory%0Arequirements.%20Activation%20sparsification%20can%20mitigate%20these%20resource%20challenges%0Aby%20reducing%20the%20number%20of%20activated%20neurons%20during%20inference.%20Existing%20methods%0Atypically%20employ%20thresholding-based%20sparsification%20based%20on%20the%20statistics%20of%0Aactivation%20tensors.%20However%2C%20they%20do%20not%20model%20the%20impact%20of%20activation%0Asparsification%20on%20performance%2C%20resulting%20in%20suboptimal%20performance%20degradation.%0ATo%20address%20the%20limitations%2C%20this%20paper%20reformulates%20the%20activation%0Asparsification%20problem%20to%20explicitly%20capture%20the%20relationship%20between%0Aactivation%20sparsity%20and%20model%20performance.%20Then%2C%20this%20paper%20proposes%20CHESS%2C%20a%0Ageneral%20activation%20sparsification%20approach%20via%20CHannel-wise%20thrEsholding%20and%0ASelective%20Sparsification.%20First%2C%20channel-wise%20thresholding%20assigns%20a%20unique%0Athreshold%20to%20each%20activation%20channel%20in%20the%20feed-forward%20network%20%28FFN%29%20layers.%0AThen%2C%20selective%20sparsification%20involves%20applying%20thresholding-based%20activation%0Asparsification%20to%20specific%20layers%20within%20the%20attention%20modules.%20Finally%2C%20we%0Adetail%20the%20implementation%20of%20sparse%20kernels%20to%20accelerate%20LLM%20inference.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20CHESS%20achieves%20lower%0Aperformance%20degradation%20over%20eight%20downstream%20tasks%20while%20activating%20fewer%0Aparameters%20than%20existing%20methods%2C%20thus%20speeding%20up%20the%20LLM%20inference%20by%20up%20to%0A1.27x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01366v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHESS%253A%2520Optimizing%2520LLM%2520Inference%2520via%2520Channel-Wise%2520Thresholding%2520and%250A%2520%2520Selective%2520Sparsification%26entry.906535625%3DJunhui%2520He%2520and%2520Shangyu%2520Wu%2520and%2520Weidong%2520Wen%2520and%2520Chun%2520Jason%2520Xue%2520and%2520Qingan%2520Li%26entry.1292438233%3D%2520%2520Deploying%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520edge%2520devices%2520presents%2520significant%250Achallenges%2520due%2520to%2520the%2520substantial%2520computational%2520overhead%2520and%2520memory%250Arequirements.%2520Activation%2520sparsification%2520can%2520mitigate%2520these%2520resource%2520challenges%250Aby%2520reducing%2520the%2520number%2520of%2520activated%2520neurons%2520during%2520inference.%2520Existing%2520methods%250Atypically%2520employ%2520thresholding-based%2520sparsification%2520based%2520on%2520the%2520statistics%2520of%250Aactivation%2520tensors.%2520However%252C%2520they%2520do%2520not%2520model%2520the%2520impact%2520of%2520activation%250Asparsification%2520on%2520performance%252C%2520resulting%2520in%2520suboptimal%2520performance%2520degradation.%250ATo%2520address%2520the%2520limitations%252C%2520this%2520paper%2520reformulates%2520the%2520activation%250Asparsification%2520problem%2520to%2520explicitly%2520capture%2520the%2520relationship%2520between%250Aactivation%2520sparsity%2520and%2520model%2520performance.%2520Then%252C%2520this%2520paper%2520proposes%2520CHESS%252C%2520a%250Ageneral%2520activation%2520sparsification%2520approach%2520via%2520CHannel-wise%2520thrEsholding%2520and%250ASelective%2520Sparsification.%2520First%252C%2520channel-wise%2520thresholding%2520assigns%2520a%2520unique%250Athreshold%2520to%2520each%2520activation%2520channel%2520in%2520the%2520feed-forward%2520network%2520%2528FFN%2529%2520layers.%250AThen%252C%2520selective%2520sparsification%2520involves%2520applying%2520thresholding-based%2520activation%250Asparsification%2520to%2520specific%2520layers%2520within%2520the%2520attention%2520modules.%2520Finally%252C%2520we%250Adetail%2520the%2520implementation%2520of%2520sparse%2520kernels%2520to%2520accelerate%2520LLM%2520inference.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520CHESS%2520achieves%2520lower%250Aperformance%2520degradation%2520over%2520eight%2520downstream%2520tasks%2520while%2520activating%2520fewer%250Aparameters%2520than%2520existing%2520methods%252C%2520thus%2520speeding%2520up%2520the%2520LLM%2520inference%2520by%2520up%2520to%250A1.27x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01366v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHESS%3A%20Optimizing%20LLM%20Inference%20via%20Channel-Wise%20Thresholding%20and%0A%20%20Selective%20Sparsification&entry.906535625=Junhui%20He%20and%20Shangyu%20Wu%20and%20Weidong%20Wen%20and%20Chun%20Jason%20Xue%20and%20Qingan%20Li&entry.1292438233=%20%20Deploying%20large%20language%20models%20%28LLMs%29%20on%20edge%20devices%20presents%20significant%0Achallenges%20due%20to%20the%20substantial%20computational%20overhead%20and%20memory%0Arequirements.%20Activation%20sparsification%20can%20mitigate%20these%20resource%20challenges%0Aby%20reducing%20the%20number%20of%20activated%20neurons%20during%20inference.%20Existing%20methods%0Atypically%20employ%20thresholding-based%20sparsification%20based%20on%20the%20statistics%20of%0Aactivation%20tensors.%20However%2C%20they%20do%20not%20model%20the%20impact%20of%20activation%0Asparsification%20on%20performance%2C%20resulting%20in%20suboptimal%20performance%20degradation.%0ATo%20address%20the%20limitations%2C%20this%20paper%20reformulates%20the%20activation%0Asparsification%20problem%20to%20explicitly%20capture%20the%20relationship%20between%0Aactivation%20sparsity%20and%20model%20performance.%20Then%2C%20this%20paper%20proposes%20CHESS%2C%20a%0Ageneral%20activation%20sparsification%20approach%20via%20CHannel-wise%20thrEsholding%20and%0ASelective%20Sparsification.%20First%2C%20channel-wise%20thresholding%20assigns%20a%20unique%0Athreshold%20to%20each%20activation%20channel%20in%20the%20feed-forward%20network%20%28FFN%29%20layers.%0AThen%2C%20selective%20sparsification%20involves%20applying%20thresholding-based%20activation%0Asparsification%20to%20specific%20layers%20within%20the%20attention%20modules.%20Finally%2C%20we%0Adetail%20the%20implementation%20of%20sparse%20kernels%20to%20accelerate%20LLM%20inference.%0AExperimental%20results%20demonstrate%20that%20the%20proposed%20CHESS%20achieves%20lower%0Aperformance%20degradation%20over%20eight%20downstream%20tasks%20while%20activating%20fewer%0Aparameters%20than%20existing%20methods%2C%20thus%20speeding%20up%20the%20LLM%20inference%20by%20up%20to%0A1.27x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01366v2&entry.124074799=Read"},
{"title": "Asymmetrical Reciprocity-based Federated Learning for Resolving\n  Disparities in Medical Diagnosis", "author": "Jiaqi Wang and Ziyi Yin and Quanzeng You and Lingjuan Lyu and Fenglong Ma", "abstract": "  Geographic health disparities pose a pressing global challenge, particularly\nin underserved regions of low- and middle-income nations. Addressing this issue\nrequires a collaborative approach to enhance healthcare quality, leveraging\nsupport from medically more developed areas. Federated learning emerges as a\npromising tool for this purpose. However, the scarcity of medical data and\nlimited computation resources in underserved regions make collaborative\ntraining of powerful machine learning models challenging. Furthermore, there\nexists an asymmetrical reciprocity between underserved and developed regions.\nTo overcome these challenges, we propose a novel cross-silo federated learning\nframework, named FedHelp, aimed at alleviating geographic health disparities\nand fortifying the diagnostic capabilities of underserved regions.\nSpecifically, FedHelp leverages foundational model knowledge via one-time API\naccess to guide the learning process of underserved small clients, addressing\nthe challenge of insufficient data. Additionally, we introduce a novel\nasymmetric dual knowledge distillation module to manage the issue of asymmetric\nreciprocity, facilitating the exchange of necessary knowledge between developed\nlarge clients and underserved small clients. We validate the effectiveness and\nutility of FedHelp through extensive experiments on both medical image\nclassification and segmentation tasks. The experimental results demonstrate\nsignificant performance improvement compared to state-of-the-art baselines,\nparticularly benefiting clients in underserved regions.\n", "link": "http://arxiv.org/abs/2412.19654v1", "date": "2024-12-27", "relevancy": 1.967, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5161}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4962}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymmetrical%20Reciprocity-based%20Federated%20Learning%20for%20Resolving%0A%20%20Disparities%20in%20Medical%20Diagnosis&body=Title%3A%20Asymmetrical%20Reciprocity-based%20Federated%20Learning%20for%20Resolving%0A%20%20Disparities%20in%20Medical%20Diagnosis%0AAuthor%3A%20Jiaqi%20Wang%20and%20Ziyi%20Yin%20and%20Quanzeng%20You%20and%20Lingjuan%20Lyu%20and%20Fenglong%20Ma%0AAbstract%3A%20%20%20Geographic%20health%20disparities%20pose%20a%20pressing%20global%20challenge%2C%20particularly%0Ain%20underserved%20regions%20of%20low-%20and%20middle-income%20nations.%20Addressing%20this%20issue%0Arequires%20a%20collaborative%20approach%20to%20enhance%20healthcare%20quality%2C%20leveraging%0Asupport%20from%20medically%20more%20developed%20areas.%20Federated%20learning%20emerges%20as%20a%0Apromising%20tool%20for%20this%20purpose.%20However%2C%20the%20scarcity%20of%20medical%20data%20and%0Alimited%20computation%20resources%20in%20underserved%20regions%20make%20collaborative%0Atraining%20of%20powerful%20machine%20learning%20models%20challenging.%20Furthermore%2C%20there%0Aexists%20an%20asymmetrical%20reciprocity%20between%20underserved%20and%20developed%20regions.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%20cross-silo%20federated%20learning%0Aframework%2C%20named%20FedHelp%2C%20aimed%20at%20alleviating%20geographic%20health%20disparities%0Aand%20fortifying%20the%20diagnostic%20capabilities%20of%20underserved%20regions.%0ASpecifically%2C%20FedHelp%20leverages%20foundational%20model%20knowledge%20via%20one-time%20API%0Aaccess%20to%20guide%20the%20learning%20process%20of%20underserved%20small%20clients%2C%20addressing%0Athe%20challenge%20of%20insufficient%20data.%20Additionally%2C%20we%20introduce%20a%20novel%0Aasymmetric%20dual%20knowledge%20distillation%20module%20to%20manage%20the%20issue%20of%20asymmetric%0Areciprocity%2C%20facilitating%20the%20exchange%20of%20necessary%20knowledge%20between%20developed%0Alarge%20clients%20and%20underserved%20small%20clients.%20We%20validate%20the%20effectiveness%20and%0Autility%20of%20FedHelp%20through%20extensive%20experiments%20on%20both%20medical%20image%0Aclassification%20and%20segmentation%20tasks.%20The%20experimental%20results%20demonstrate%0Asignificant%20performance%20improvement%20compared%20to%20state-of-the-art%20baselines%2C%0Aparticularly%20benefiting%20clients%20in%20underserved%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymmetrical%2520Reciprocity-based%2520Federated%2520Learning%2520for%2520Resolving%250A%2520%2520Disparities%2520in%2520Medical%2520Diagnosis%26entry.906535625%3DJiaqi%2520Wang%2520and%2520Ziyi%2520Yin%2520and%2520Quanzeng%2520You%2520and%2520Lingjuan%2520Lyu%2520and%2520Fenglong%2520Ma%26entry.1292438233%3D%2520%2520Geographic%2520health%2520disparities%2520pose%2520a%2520pressing%2520global%2520challenge%252C%2520particularly%250Ain%2520underserved%2520regions%2520of%2520low-%2520and%2520middle-income%2520nations.%2520Addressing%2520this%2520issue%250Arequires%2520a%2520collaborative%2520approach%2520to%2520enhance%2520healthcare%2520quality%252C%2520leveraging%250Asupport%2520from%2520medically%2520more%2520developed%2520areas.%2520Federated%2520learning%2520emerges%2520as%2520a%250Apromising%2520tool%2520for%2520this%2520purpose.%2520However%252C%2520the%2520scarcity%2520of%2520medical%2520data%2520and%250Alimited%2520computation%2520resources%2520in%2520underserved%2520regions%2520make%2520collaborative%250Atraining%2520of%2520powerful%2520machine%2520learning%2520models%2520challenging.%2520Furthermore%252C%2520there%250Aexists%2520an%2520asymmetrical%2520reciprocity%2520between%2520underserved%2520and%2520developed%2520regions.%250ATo%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520cross-silo%2520federated%2520learning%250Aframework%252C%2520named%2520FedHelp%252C%2520aimed%2520at%2520alleviating%2520geographic%2520health%2520disparities%250Aand%2520fortifying%2520the%2520diagnostic%2520capabilities%2520of%2520underserved%2520regions.%250ASpecifically%252C%2520FedHelp%2520leverages%2520foundational%2520model%2520knowledge%2520via%2520one-time%2520API%250Aaccess%2520to%2520guide%2520the%2520learning%2520process%2520of%2520underserved%2520small%2520clients%252C%2520addressing%250Athe%2520challenge%2520of%2520insufficient%2520data.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%250Aasymmetric%2520dual%2520knowledge%2520distillation%2520module%2520to%2520manage%2520the%2520issue%2520of%2520asymmetric%250Areciprocity%252C%2520facilitating%2520the%2520exchange%2520of%2520necessary%2520knowledge%2520between%2520developed%250Alarge%2520clients%2520and%2520underserved%2520small%2520clients.%2520We%2520validate%2520the%2520effectiveness%2520and%250Autility%2520of%2520FedHelp%2520through%2520extensive%2520experiments%2520on%2520both%2520medical%2520image%250Aclassification%2520and%2520segmentation%2520tasks.%2520The%2520experimental%2520results%2520demonstrate%250Asignificant%2520performance%2520improvement%2520compared%2520to%2520state-of-the-art%2520baselines%252C%250Aparticularly%2520benefiting%2520clients%2520in%2520underserved%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymmetrical%20Reciprocity-based%20Federated%20Learning%20for%20Resolving%0A%20%20Disparities%20in%20Medical%20Diagnosis&entry.906535625=Jiaqi%20Wang%20and%20Ziyi%20Yin%20and%20Quanzeng%20You%20and%20Lingjuan%20Lyu%20and%20Fenglong%20Ma&entry.1292438233=%20%20Geographic%20health%20disparities%20pose%20a%20pressing%20global%20challenge%2C%20particularly%0Ain%20underserved%20regions%20of%20low-%20and%20middle-income%20nations.%20Addressing%20this%20issue%0Arequires%20a%20collaborative%20approach%20to%20enhance%20healthcare%20quality%2C%20leveraging%0Asupport%20from%20medically%20more%20developed%20areas.%20Federated%20learning%20emerges%20as%20a%0Apromising%20tool%20for%20this%20purpose.%20However%2C%20the%20scarcity%20of%20medical%20data%20and%0Alimited%20computation%20resources%20in%20underserved%20regions%20make%20collaborative%0Atraining%20of%20powerful%20machine%20learning%20models%20challenging.%20Furthermore%2C%20there%0Aexists%20an%20asymmetrical%20reciprocity%20between%20underserved%20and%20developed%20regions.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20a%20novel%20cross-silo%20federated%20learning%0Aframework%2C%20named%20FedHelp%2C%20aimed%20at%20alleviating%20geographic%20health%20disparities%0Aand%20fortifying%20the%20diagnostic%20capabilities%20of%20underserved%20regions.%0ASpecifically%2C%20FedHelp%20leverages%20foundational%20model%20knowledge%20via%20one-time%20API%0Aaccess%20to%20guide%20the%20learning%20process%20of%20underserved%20small%20clients%2C%20addressing%0Athe%20challenge%20of%20insufficient%20data.%20Additionally%2C%20we%20introduce%20a%20novel%0Aasymmetric%20dual%20knowledge%20distillation%20module%20to%20manage%20the%20issue%20of%20asymmetric%0Areciprocity%2C%20facilitating%20the%20exchange%20of%20necessary%20knowledge%20between%20developed%0Alarge%20clients%20and%20underserved%20small%20clients.%20We%20validate%20the%20effectiveness%20and%0Autility%20of%20FedHelp%20through%20extensive%20experiments%20on%20both%20medical%20image%0Aclassification%20and%20segmentation%20tasks.%20The%20experimental%20results%20demonstrate%0Asignificant%20performance%20improvement%20compared%20to%20state-of-the-art%20baselines%2C%0Aparticularly%20benefiting%20clients%20in%20underserved%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19654v1&entry.124074799=Read"},
{"title": "A Hybrid Technique for Plant Disease Identification and Localisation in\n  Real-time", "author": "Mahendra Kumar Gohil and Anirudha Bhattacharjee and Rwik Rana and Kishan Lal and Samir Kumar Biswas and Nachiketa Tiwari and Bishakh Bhattacharya", "abstract": "  Over the past decade, several image-processing methods and algorithms have\nbeen proposed for identifying plant diseases based on visual data. DNN (Deep\nNeural Networks) have recently become popular for this task. Both traditional\nimage processing and DNN-based methods encounter significant performance issues\nin real-time detection owing to computational limitations and a broad spectrum\nof plant disease features. This article proposes a novel technique for\nidentifying and localising plant disease based on the Quad-Tree decomposition\nof an image and feature learning simultaneously. The proposed algorithm\nsignificantly improves accuracy and faster convergence in high-resolution\nimages with relatively low computational load. Hence it is ideal for deploying\nthe algorithm in a standalone processor in a remotely operated image\nacquisition and disease detection system, ideally mounted on drones and robots\nworking on large agricultural fields. The technique proposed in this article is\nhybrid as it exploits the advantages of traditional image processing methods\nand DNN-based models at different scales, resulting in faster inference. The F1\nscore is approximately 0.80 for four disease classes corresponding to potato\nand tomato crops.\n", "link": "http://arxiv.org/abs/2412.19682v1", "date": "2024-12-27", "relevancy": 1.965, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5403}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4909}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Technique%20for%20Plant%20Disease%20Identification%20and%20Localisation%20in%0A%20%20Real-time&body=Title%3A%20A%20Hybrid%20Technique%20for%20Plant%20Disease%20Identification%20and%20Localisation%20in%0A%20%20Real-time%0AAuthor%3A%20Mahendra%20Kumar%20Gohil%20and%20Anirudha%20Bhattacharjee%20and%20Rwik%20Rana%20and%20Kishan%20Lal%20and%20Samir%20Kumar%20Biswas%20and%20Nachiketa%20Tiwari%20and%20Bishakh%20Bhattacharya%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20several%20image-processing%20methods%20and%20algorithms%20have%0Abeen%20proposed%20for%20identifying%20plant%20diseases%20based%20on%20visual%20data.%20DNN%20%28Deep%0ANeural%20Networks%29%20have%20recently%20become%20popular%20for%20this%20task.%20Both%20traditional%0Aimage%20processing%20and%20DNN-based%20methods%20encounter%20significant%20performance%20issues%0Ain%20real-time%20detection%20owing%20to%20computational%20limitations%20and%20a%20broad%20spectrum%0Aof%20plant%20disease%20features.%20This%20article%20proposes%20a%20novel%20technique%20for%0Aidentifying%20and%20localising%20plant%20disease%20based%20on%20the%20Quad-Tree%20decomposition%0Aof%20an%20image%20and%20feature%20learning%20simultaneously.%20The%20proposed%20algorithm%0Asignificantly%20improves%20accuracy%20and%20faster%20convergence%20in%20high-resolution%0Aimages%20with%20relatively%20low%20computational%20load.%20Hence%20it%20is%20ideal%20for%20deploying%0Athe%20algorithm%20in%20a%20standalone%20processor%20in%20a%20remotely%20operated%20image%0Aacquisition%20and%20disease%20detection%20system%2C%20ideally%20mounted%20on%20drones%20and%20robots%0Aworking%20on%20large%20agricultural%20fields.%20The%20technique%20proposed%20in%20this%20article%20is%0Ahybrid%20as%20it%20exploits%20the%20advantages%20of%20traditional%20image%20processing%20methods%0Aand%20DNN-based%20models%20at%20different%20scales%2C%20resulting%20in%20faster%20inference.%20The%20F1%0Ascore%20is%20approximately%200.80%20for%20four%20disease%20classes%20corresponding%20to%20potato%0Aand%20tomato%20crops.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Technique%2520for%2520Plant%2520Disease%2520Identification%2520and%2520Localisation%2520in%250A%2520%2520Real-time%26entry.906535625%3DMahendra%2520Kumar%2520Gohil%2520and%2520Anirudha%2520Bhattacharjee%2520and%2520Rwik%2520Rana%2520and%2520Kishan%2520Lal%2520and%2520Samir%2520Kumar%2520Biswas%2520and%2520Nachiketa%2520Tiwari%2520and%2520Bishakh%2520Bhattacharya%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520several%2520image-processing%2520methods%2520and%2520algorithms%2520have%250Abeen%2520proposed%2520for%2520identifying%2520plant%2520diseases%2520based%2520on%2520visual%2520data.%2520DNN%2520%2528Deep%250ANeural%2520Networks%2529%2520have%2520recently%2520become%2520popular%2520for%2520this%2520task.%2520Both%2520traditional%250Aimage%2520processing%2520and%2520DNN-based%2520methods%2520encounter%2520significant%2520performance%2520issues%250Ain%2520real-time%2520detection%2520owing%2520to%2520computational%2520limitations%2520and%2520a%2520broad%2520spectrum%250Aof%2520plant%2520disease%2520features.%2520This%2520article%2520proposes%2520a%2520novel%2520technique%2520for%250Aidentifying%2520and%2520localising%2520plant%2520disease%2520based%2520on%2520the%2520Quad-Tree%2520decomposition%250Aof%2520an%2520image%2520and%2520feature%2520learning%2520simultaneously.%2520The%2520proposed%2520algorithm%250Asignificantly%2520improves%2520accuracy%2520and%2520faster%2520convergence%2520in%2520high-resolution%250Aimages%2520with%2520relatively%2520low%2520computational%2520load.%2520Hence%2520it%2520is%2520ideal%2520for%2520deploying%250Athe%2520algorithm%2520in%2520a%2520standalone%2520processor%2520in%2520a%2520remotely%2520operated%2520image%250Aacquisition%2520and%2520disease%2520detection%2520system%252C%2520ideally%2520mounted%2520on%2520drones%2520and%2520robots%250Aworking%2520on%2520large%2520agricultural%2520fields.%2520The%2520technique%2520proposed%2520in%2520this%2520article%2520is%250Ahybrid%2520as%2520it%2520exploits%2520the%2520advantages%2520of%2520traditional%2520image%2520processing%2520methods%250Aand%2520DNN-based%2520models%2520at%2520different%2520scales%252C%2520resulting%2520in%2520faster%2520inference.%2520The%2520F1%250Ascore%2520is%2520approximately%25200.80%2520for%2520four%2520disease%2520classes%2520corresponding%2520to%2520potato%250Aand%2520tomato%2520crops.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Technique%20for%20Plant%20Disease%20Identification%20and%20Localisation%20in%0A%20%20Real-time&entry.906535625=Mahendra%20Kumar%20Gohil%20and%20Anirudha%20Bhattacharjee%20and%20Rwik%20Rana%20and%20Kishan%20Lal%20and%20Samir%20Kumar%20Biswas%20and%20Nachiketa%20Tiwari%20and%20Bishakh%20Bhattacharya&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20several%20image-processing%20methods%20and%20algorithms%20have%0Abeen%20proposed%20for%20identifying%20plant%20diseases%20based%20on%20visual%20data.%20DNN%20%28Deep%0ANeural%20Networks%29%20have%20recently%20become%20popular%20for%20this%20task.%20Both%20traditional%0Aimage%20processing%20and%20DNN-based%20methods%20encounter%20significant%20performance%20issues%0Ain%20real-time%20detection%20owing%20to%20computational%20limitations%20and%20a%20broad%20spectrum%0Aof%20plant%20disease%20features.%20This%20article%20proposes%20a%20novel%20technique%20for%0Aidentifying%20and%20localising%20plant%20disease%20based%20on%20the%20Quad-Tree%20decomposition%0Aof%20an%20image%20and%20feature%20learning%20simultaneously.%20The%20proposed%20algorithm%0Asignificantly%20improves%20accuracy%20and%20faster%20convergence%20in%20high-resolution%0Aimages%20with%20relatively%20low%20computational%20load.%20Hence%20it%20is%20ideal%20for%20deploying%0Athe%20algorithm%20in%20a%20standalone%20processor%20in%20a%20remotely%20operated%20image%0Aacquisition%20and%20disease%20detection%20system%2C%20ideally%20mounted%20on%20drones%20and%20robots%0Aworking%20on%20large%20agricultural%20fields.%20The%20technique%20proposed%20in%20this%20article%20is%0Ahybrid%20as%20it%20exploits%20the%20advantages%20of%20traditional%20image%20processing%20methods%0Aand%20DNN-based%20models%20at%20different%20scales%2C%20resulting%20in%20faster%20inference.%20The%20F1%0Ascore%20is%20approximately%200.80%20for%20four%20disease%20classes%20corresponding%20to%20potato%0Aand%20tomato%20crops.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19682v1&entry.124074799=Read"},
{"title": "Impact of Sunglasses on One-to-Many Facial Identification Accuracy", "author": "Sicong Tian and Haiyu Wu and Michael C. King and Kevin W. Bowyer", "abstract": "  One-to-many facial identification is documented to achieve high accuracy in\nthe case where both the probe and the gallery are \"mugshot quality\" images.\nHowever, an increasing number of documented instances of wrongful arrest\nfollowing one-to-many facial identification have raised questions about its\naccuracy. Probe images used in one-to-many facial identification are often\ncropped from frames of surveillance video and deviate from \"mugshot quality\" in\nvarious ways. This paper systematically explores how the accuracy of\none-to-many facial identification is degraded by the person in the probe image\nchoosing to wear dark sunglasses. We show that sunglasses degrade accuracy for\nmugshot-quality images by an amount similar to strong blur or noticeably lower\nresolution. Further, we demonstrate that the combination of sunglasses with\nblur or lower resolution results in even more pronounced loss in accuracy.\nThese results have important implications for developing objective criteria to\nqualify a probe image for the level of accuracy to be expected if it used for\none-to-many identification. To ameliorate the accuracy degradation caused by\ndark sunglasses, we show that it is possible to recover about 38% of the lost\naccuracy by synthetically adding sunglasses to all the gallery images, without\nmodel re-training. We also show that the frequency of wearing-sunglasses images\nis very low in existing training sets, and that increasing the representation\nof wearing-sunglasses images can greatly reduce the error rate. The image set\nassembled for this research is available at https://cvrl.nd.edu/projects/data/\nto support replication and further research.\n", "link": "http://arxiv.org/abs/2412.05721v2", "date": "2024-12-27", "relevancy": 1.9458, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4907}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4874}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20Sunglasses%20on%20One-to-Many%20Facial%20Identification%20Accuracy&body=Title%3A%20Impact%20of%20Sunglasses%20on%20One-to-Many%20Facial%20Identification%20Accuracy%0AAuthor%3A%20Sicong%20Tian%20and%20Haiyu%20Wu%20and%20Michael%20C.%20King%20and%20Kevin%20W.%20Bowyer%0AAbstract%3A%20%20%20One-to-many%20facial%20identification%20is%20documented%20to%20achieve%20high%20accuracy%20in%0Athe%20case%20where%20both%20the%20probe%20and%20the%20gallery%20are%20%22mugshot%20quality%22%20images.%0AHowever%2C%20an%20increasing%20number%20of%20documented%20instances%20of%20wrongful%20arrest%0Afollowing%20one-to-many%20facial%20identification%20have%20raised%20questions%20about%20its%0Aaccuracy.%20Probe%20images%20used%20in%20one-to-many%20facial%20identification%20are%20often%0Acropped%20from%20frames%20of%20surveillance%20video%20and%20deviate%20from%20%22mugshot%20quality%22%20in%0Avarious%20ways.%20This%20paper%20systematically%20explores%20how%20the%20accuracy%20of%0Aone-to-many%20facial%20identification%20is%20degraded%20by%20the%20person%20in%20the%20probe%20image%0Achoosing%20to%20wear%20dark%20sunglasses.%20We%20show%20that%20sunglasses%20degrade%20accuracy%20for%0Amugshot-quality%20images%20by%20an%20amount%20similar%20to%20strong%20blur%20or%20noticeably%20lower%0Aresolution.%20Further%2C%20we%20demonstrate%20that%20the%20combination%20of%20sunglasses%20with%0Ablur%20or%20lower%20resolution%20results%20in%20even%20more%20pronounced%20loss%20in%20accuracy.%0AThese%20results%20have%20important%20implications%20for%20developing%20objective%20criteria%20to%0Aqualify%20a%20probe%20image%20for%20the%20level%20of%20accuracy%20to%20be%20expected%20if%20it%20used%20for%0Aone-to-many%20identification.%20To%20ameliorate%20the%20accuracy%20degradation%20caused%20by%0Adark%20sunglasses%2C%20we%20show%20that%20it%20is%20possible%20to%20recover%20about%2038%25%20of%20the%20lost%0Aaccuracy%20by%20synthetically%20adding%20sunglasses%20to%20all%20the%20gallery%20images%2C%20without%0Amodel%20re-training.%20We%20also%20show%20that%20the%20frequency%20of%20wearing-sunglasses%20images%0Ais%20very%20low%20in%20existing%20training%20sets%2C%20and%20that%20increasing%20the%20representation%0Aof%20wearing-sunglasses%20images%20can%20greatly%20reduce%20the%20error%20rate.%20The%20image%20set%0Aassembled%20for%20this%20research%20is%20available%20at%20https%3A//cvrl.nd.edu/projects/data/%0Ato%20support%20replication%20and%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05721v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520Sunglasses%2520on%2520One-to-Many%2520Facial%2520Identification%2520Accuracy%26entry.906535625%3DSicong%2520Tian%2520and%2520Haiyu%2520Wu%2520and%2520Michael%2520C.%2520King%2520and%2520Kevin%2520W.%2520Bowyer%26entry.1292438233%3D%2520%2520One-to-many%2520facial%2520identification%2520is%2520documented%2520to%2520achieve%2520high%2520accuracy%2520in%250Athe%2520case%2520where%2520both%2520the%2520probe%2520and%2520the%2520gallery%2520are%2520%2522mugshot%2520quality%2522%2520images.%250AHowever%252C%2520an%2520increasing%2520number%2520of%2520documented%2520instances%2520of%2520wrongful%2520arrest%250Afollowing%2520one-to-many%2520facial%2520identification%2520have%2520raised%2520questions%2520about%2520its%250Aaccuracy.%2520Probe%2520images%2520used%2520in%2520one-to-many%2520facial%2520identification%2520are%2520often%250Acropped%2520from%2520frames%2520of%2520surveillance%2520video%2520and%2520deviate%2520from%2520%2522mugshot%2520quality%2522%2520in%250Avarious%2520ways.%2520This%2520paper%2520systematically%2520explores%2520how%2520the%2520accuracy%2520of%250Aone-to-many%2520facial%2520identification%2520is%2520degraded%2520by%2520the%2520person%2520in%2520the%2520probe%2520image%250Achoosing%2520to%2520wear%2520dark%2520sunglasses.%2520We%2520show%2520that%2520sunglasses%2520degrade%2520accuracy%2520for%250Amugshot-quality%2520images%2520by%2520an%2520amount%2520similar%2520to%2520strong%2520blur%2520or%2520noticeably%2520lower%250Aresolution.%2520Further%252C%2520we%2520demonstrate%2520that%2520the%2520combination%2520of%2520sunglasses%2520with%250Ablur%2520or%2520lower%2520resolution%2520results%2520in%2520even%2520more%2520pronounced%2520loss%2520in%2520accuracy.%250AThese%2520results%2520have%2520important%2520implications%2520for%2520developing%2520objective%2520criteria%2520to%250Aqualify%2520a%2520probe%2520image%2520for%2520the%2520level%2520of%2520accuracy%2520to%2520be%2520expected%2520if%2520it%2520used%2520for%250Aone-to-many%2520identification.%2520To%2520ameliorate%2520the%2520accuracy%2520degradation%2520caused%2520by%250Adark%2520sunglasses%252C%2520we%2520show%2520that%2520it%2520is%2520possible%2520to%2520recover%2520about%252038%2525%2520of%2520the%2520lost%250Aaccuracy%2520by%2520synthetically%2520adding%2520sunglasses%2520to%2520all%2520the%2520gallery%2520images%252C%2520without%250Amodel%2520re-training.%2520We%2520also%2520show%2520that%2520the%2520frequency%2520of%2520wearing-sunglasses%2520images%250Ais%2520very%2520low%2520in%2520existing%2520training%2520sets%252C%2520and%2520that%2520increasing%2520the%2520representation%250Aof%2520wearing-sunglasses%2520images%2520can%2520greatly%2520reduce%2520the%2520error%2520rate.%2520The%2520image%2520set%250Aassembled%2520for%2520this%2520research%2520is%2520available%2520at%2520https%253A//cvrl.nd.edu/projects/data/%250Ato%2520support%2520replication%2520and%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05721v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20Sunglasses%20on%20One-to-Many%20Facial%20Identification%20Accuracy&entry.906535625=Sicong%20Tian%20and%20Haiyu%20Wu%20and%20Michael%20C.%20King%20and%20Kevin%20W.%20Bowyer&entry.1292438233=%20%20One-to-many%20facial%20identification%20is%20documented%20to%20achieve%20high%20accuracy%20in%0Athe%20case%20where%20both%20the%20probe%20and%20the%20gallery%20are%20%22mugshot%20quality%22%20images.%0AHowever%2C%20an%20increasing%20number%20of%20documented%20instances%20of%20wrongful%20arrest%0Afollowing%20one-to-many%20facial%20identification%20have%20raised%20questions%20about%20its%0Aaccuracy.%20Probe%20images%20used%20in%20one-to-many%20facial%20identification%20are%20often%0Acropped%20from%20frames%20of%20surveillance%20video%20and%20deviate%20from%20%22mugshot%20quality%22%20in%0Avarious%20ways.%20This%20paper%20systematically%20explores%20how%20the%20accuracy%20of%0Aone-to-many%20facial%20identification%20is%20degraded%20by%20the%20person%20in%20the%20probe%20image%0Achoosing%20to%20wear%20dark%20sunglasses.%20We%20show%20that%20sunglasses%20degrade%20accuracy%20for%0Amugshot-quality%20images%20by%20an%20amount%20similar%20to%20strong%20blur%20or%20noticeably%20lower%0Aresolution.%20Further%2C%20we%20demonstrate%20that%20the%20combination%20of%20sunglasses%20with%0Ablur%20or%20lower%20resolution%20results%20in%20even%20more%20pronounced%20loss%20in%20accuracy.%0AThese%20results%20have%20important%20implications%20for%20developing%20objective%20criteria%20to%0Aqualify%20a%20probe%20image%20for%20the%20level%20of%20accuracy%20to%20be%20expected%20if%20it%20used%20for%0Aone-to-many%20identification.%20To%20ameliorate%20the%20accuracy%20degradation%20caused%20by%0Adark%20sunglasses%2C%20we%20show%20that%20it%20is%20possible%20to%20recover%20about%2038%25%20of%20the%20lost%0Aaccuracy%20by%20synthetically%20adding%20sunglasses%20to%20all%20the%20gallery%20images%2C%20without%0Amodel%20re-training.%20We%20also%20show%20that%20the%20frequency%20of%20wearing-sunglasses%20images%0Ais%20very%20low%20in%20existing%20training%20sets%2C%20and%20that%20increasing%20the%20representation%0Aof%20wearing-sunglasses%20images%20can%20greatly%20reduce%20the%20error%20rate.%20The%20image%20set%0Aassembled%20for%20this%20research%20is%20available%20at%20https%3A//cvrl.nd.edu/projects/data/%0Ato%20support%20replication%20and%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05721v2&entry.124074799=Read"},
{"title": "Agent-OM: Leveraging LLM Agents for Ontology Matching", "author": "Zhangcheng Qiang and Weiqing Wang and Kerry Taylor", "abstract": "  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n", "link": "http://arxiv.org/abs/2312.00326v6", "date": "2024-12-27", "relevancy": 1.9401, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent-OM%3A%20Leveraging%20LLM%20Agents%20for%20Ontology%20Matching&body=Title%3A%20Agent-OM%3A%20Leveraging%20LLM%20Agents%20for%20Ontology%20Matching%0AAuthor%3A%20Zhangcheng%20Qiang%20and%20Weiqing%20Wang%20and%20Kerry%20Taylor%0AAbstract%3A%20%20%20Ontology%20matching%20%28OM%29%20enables%20semantic%20interoperability%20between%20different%0Aontologies%20and%20resolves%20their%20conceptual%20heterogeneity%20by%20aligning%20related%0Aentities.%20OM%20systems%20currently%20have%20two%20prevailing%20design%20paradigms%3A%0Aconventional%20knowledge-based%20expert%20systems%20and%20newer%20machine%20learning-based%0Apredictive%20systems.%20While%20large%20language%20models%20%28LLMs%29%20and%20LLM%20agents%20have%0Arevolutionised%20data%20engineering%20and%20have%20been%20applied%20creatively%20in%20many%0Adomains%2C%20their%20potential%20for%20OM%20remains%20underexplored.%20This%20study%20introduces%20a%0Anovel%20agent-powered%20LLM-based%20design%20paradigm%20for%20OM%20systems.%20With%0Aconsideration%20of%20several%20specific%20challenges%20in%20leveraging%20LLM%20agents%20for%20OM%2C%0Awe%20propose%20a%20generic%20framework%2C%20namely%20Agent-OM%20%28Agent%20for%20Ontology%20Matching%29%2C%0Aconsisting%20of%20two%20Siamese%20agents%20for%20retrieval%20and%20matching%2C%20with%20a%20set%20of%20OM%0Atools.%20Our%20framework%20is%20implemented%20in%20a%20proof-of-concept%20system.%20Evaluations%0Aof%20three%20Ontology%20Alignment%20Evaluation%20Initiative%20%28OAEI%29%20tracks%20over%0Astate-of-the-art%20OM%20systems%20show%20that%20our%20system%20can%20achieve%20results%20very%20close%0Ato%20the%20long-standing%20best%20performance%20on%20simple%20OM%20tasks%20and%20can%20significantly%0Aimprove%20the%20performance%20on%20complex%20and%20few-shot%20OM%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00326v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent-OM%253A%2520Leveraging%2520LLM%2520Agents%2520for%2520Ontology%2520Matching%26entry.906535625%3DZhangcheng%2520Qiang%2520and%2520Weiqing%2520Wang%2520and%2520Kerry%2520Taylor%26entry.1292438233%3D%2520%2520Ontology%2520matching%2520%2528OM%2529%2520enables%2520semantic%2520interoperability%2520between%2520different%250Aontologies%2520and%2520resolves%2520their%2520conceptual%2520heterogeneity%2520by%2520aligning%2520related%250Aentities.%2520OM%2520systems%2520currently%2520have%2520two%2520prevailing%2520design%2520paradigms%253A%250Aconventional%2520knowledge-based%2520expert%2520systems%2520and%2520newer%2520machine%2520learning-based%250Apredictive%2520systems.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520LLM%2520agents%2520have%250Arevolutionised%2520data%2520engineering%2520and%2520have%2520been%2520applied%2520creatively%2520in%2520many%250Adomains%252C%2520their%2520potential%2520for%2520OM%2520remains%2520underexplored.%2520This%2520study%2520introduces%2520a%250Anovel%2520agent-powered%2520LLM-based%2520design%2520paradigm%2520for%2520OM%2520systems.%2520With%250Aconsideration%2520of%2520several%2520specific%2520challenges%2520in%2520leveraging%2520LLM%2520agents%2520for%2520OM%252C%250Awe%2520propose%2520a%2520generic%2520framework%252C%2520namely%2520Agent-OM%2520%2528Agent%2520for%2520Ontology%2520Matching%2529%252C%250Aconsisting%2520of%2520two%2520Siamese%2520agents%2520for%2520retrieval%2520and%2520matching%252C%2520with%2520a%2520set%2520of%2520OM%250Atools.%2520Our%2520framework%2520is%2520implemented%2520in%2520a%2520proof-of-concept%2520system.%2520Evaluations%250Aof%2520three%2520Ontology%2520Alignment%2520Evaluation%2520Initiative%2520%2528OAEI%2529%2520tracks%2520over%250Astate-of-the-art%2520OM%2520systems%2520show%2520that%2520our%2520system%2520can%2520achieve%2520results%2520very%2520close%250Ato%2520the%2520long-standing%2520best%2520performance%2520on%2520simple%2520OM%2520tasks%2520and%2520can%2520significantly%250Aimprove%2520the%2520performance%2520on%2520complex%2520and%2520few-shot%2520OM%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00326v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent-OM%3A%20Leveraging%20LLM%20Agents%20for%20Ontology%20Matching&entry.906535625=Zhangcheng%20Qiang%20and%20Weiqing%20Wang%20and%20Kerry%20Taylor&entry.1292438233=%20%20Ontology%20matching%20%28OM%29%20enables%20semantic%20interoperability%20between%20different%0Aontologies%20and%20resolves%20their%20conceptual%20heterogeneity%20by%20aligning%20related%0Aentities.%20OM%20systems%20currently%20have%20two%20prevailing%20design%20paradigms%3A%0Aconventional%20knowledge-based%20expert%20systems%20and%20newer%20machine%20learning-based%0Apredictive%20systems.%20While%20large%20language%20models%20%28LLMs%29%20and%20LLM%20agents%20have%0Arevolutionised%20data%20engineering%20and%20have%20been%20applied%20creatively%20in%20many%0Adomains%2C%20their%20potential%20for%20OM%20remains%20underexplored.%20This%20study%20introduces%20a%0Anovel%20agent-powered%20LLM-based%20design%20paradigm%20for%20OM%20systems.%20With%0Aconsideration%20of%20several%20specific%20challenges%20in%20leveraging%20LLM%20agents%20for%20OM%2C%0Awe%20propose%20a%20generic%20framework%2C%20namely%20Agent-OM%20%28Agent%20for%20Ontology%20Matching%29%2C%0Aconsisting%20of%20two%20Siamese%20agents%20for%20retrieval%20and%20matching%2C%20with%20a%20set%20of%20OM%0Atools.%20Our%20framework%20is%20implemented%20in%20a%20proof-of-concept%20system.%20Evaluations%0Aof%20three%20Ontology%20Alignment%20Evaluation%20Initiative%20%28OAEI%29%20tracks%20over%0Astate-of-the-art%20OM%20systems%20show%20that%20our%20system%20can%20achieve%20results%20very%20close%0Ato%20the%20long-standing%20best%20performance%20on%20simple%20OM%20tasks%20and%20can%20significantly%0Aimprove%20the%20performance%20on%20complex%20and%20few-shot%20OM%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00326v6&entry.124074799=Read"},
{"title": "InfAlign: Inference-aware language model alignment", "author": "Ananth Balashankar and Ziteng Sun and Jonathan Berant and Jacob Eisenstein and Michael Collins and Adrian Hutter and Jong Lee and Chirag Nagpal and Flavien Prost and Aradhana Sinha and and Ananda Theertha Suresh and Ahmad Beirami", "abstract": "  Language model alignment has become a critical step in training modern\ngenerative language models. The goal of alignment is to finetune a reference\nmodel such that the win rate of a sample from the aligned model over a sample\nfrom the reference model is high, subject to a KL divergence constraint. Today,\nwe are increasingly using inference-time algorithms (e.g., Best-of-N,\ncontrolled decoding, tree search) to decode from language models rather than\nstandard sampling. However, the alignment objective does not capture such\ninference-time decoding procedures. We show that the existing alignment\nframework is sub-optimal in view of such inference-time methods. We then modify\nthe alignment objective and propose a framework for inference-aware alignment\n(IAPO). We prove that for any inference-time decoding algorithm, the optimal\nsolution that optimizes the inference-time win rate of the aligned policy\nagainst the reference policy is the solution to the typical RLHF problem with a\ntransformation of the reward. This motivates us to provide the KL-regularized\ncalibrate-and-transform RL (CTRL) algorithm to solve this problem, which\ninvolves a reward calibration step and a KL-regularized reward maximization\nstep with a transformation of the calibrated reward. We particularize our study\nto two important inference-time strategies: best-of-N sampling and best-of-N\njailbreaking, where N responses are sampled from the model and the one with the\nhighest or lowest reward is selected. We propose specific transformations for\nthese strategies and demonstrate that our framework offers significant\nimprovements over existing state-of-the-art methods for language model\nalignment. Empirically, we outperform baselines that are designed without\ntaking inference-time decoding into consideration by 8-12% and 4-9% on\ninference-time win rates over the Anthropic helpfulness and harmlessness dialog\nbenchmark datasets.\n", "link": "http://arxiv.org/abs/2412.19792v1", "date": "2024-12-27", "relevancy": 1.9388, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4839}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfAlign%3A%20Inference-aware%20language%20model%20alignment&body=Title%3A%20InfAlign%3A%20Inference-aware%20language%20model%20alignment%0AAuthor%3A%20Ananth%20Balashankar%20and%20Ziteng%20Sun%20and%20Jonathan%20Berant%20and%20Jacob%20Eisenstein%20and%20Michael%20Collins%20and%20Adrian%20Hutter%20and%20Jong%20Lee%20and%20Chirag%20Nagpal%20and%20Flavien%20Prost%20and%20Aradhana%20Sinha%20and%20and%20Ananda%20Theertha%20Suresh%20and%20Ahmad%20Beirami%0AAbstract%3A%20%20%20Language%20model%20alignment%20has%20become%20a%20critical%20step%20in%20training%20modern%0Agenerative%20language%20models.%20The%20goal%20of%20alignment%20is%20to%20finetune%20a%20reference%0Amodel%20such%20that%20the%20win%20rate%20of%20a%20sample%20from%20the%20aligned%20model%20over%20a%20sample%0Afrom%20the%20reference%20model%20is%20high%2C%20subject%20to%20a%20KL%20divergence%20constraint.%20Today%2C%0Awe%20are%20increasingly%20using%20inference-time%20algorithms%20%28e.g.%2C%20Best-of-N%2C%0Acontrolled%20decoding%2C%20tree%20search%29%20to%20decode%20from%20language%20models%20rather%20than%0Astandard%20sampling.%20However%2C%20the%20alignment%20objective%20does%20not%20capture%20such%0Ainference-time%20decoding%20procedures.%20We%20show%20that%20the%20existing%20alignment%0Aframework%20is%20sub-optimal%20in%20view%20of%20such%20inference-time%20methods.%20We%20then%20modify%0Athe%20alignment%20objective%20and%20propose%20a%20framework%20for%20inference-aware%20alignment%0A%28IAPO%29.%20We%20prove%20that%20for%20any%20inference-time%20decoding%20algorithm%2C%20the%20optimal%0Asolution%20that%20optimizes%20the%20inference-time%20win%20rate%20of%20the%20aligned%20policy%0Aagainst%20the%20reference%20policy%20is%20the%20solution%20to%20the%20typical%20RLHF%20problem%20with%20a%0Atransformation%20of%20the%20reward.%20This%20motivates%20us%20to%20provide%20the%20KL-regularized%0Acalibrate-and-transform%20RL%20%28CTRL%29%20algorithm%20to%20solve%20this%20problem%2C%20which%0Ainvolves%20a%20reward%20calibration%20step%20and%20a%20KL-regularized%20reward%20maximization%0Astep%20with%20a%20transformation%20of%20the%20calibrated%20reward.%20We%20particularize%20our%20study%0Ato%20two%20important%20inference-time%20strategies%3A%20best-of-N%20sampling%20and%20best-of-N%0Ajailbreaking%2C%20where%20N%20responses%20are%20sampled%20from%20the%20model%20and%20the%20one%20with%20the%0Ahighest%20or%20lowest%20reward%20is%20selected.%20We%20propose%20specific%20transformations%20for%0Athese%20strategies%20and%20demonstrate%20that%20our%20framework%20offers%20significant%0Aimprovements%20over%20existing%20state-of-the-art%20methods%20for%20language%20model%0Aalignment.%20Empirically%2C%20we%20outperform%20baselines%20that%20are%20designed%20without%0Ataking%20inference-time%20decoding%20into%20consideration%20by%208-12%25%20and%204-9%25%20on%0Ainference-time%20win%20rates%20over%20the%20Anthropic%20helpfulness%20and%20harmlessness%20dialog%0Abenchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfAlign%253A%2520Inference-aware%2520language%2520model%2520alignment%26entry.906535625%3DAnanth%2520Balashankar%2520and%2520Ziteng%2520Sun%2520and%2520Jonathan%2520Berant%2520and%2520Jacob%2520Eisenstein%2520and%2520Michael%2520Collins%2520and%2520Adrian%2520Hutter%2520and%2520Jong%2520Lee%2520and%2520Chirag%2520Nagpal%2520and%2520Flavien%2520Prost%2520and%2520Aradhana%2520Sinha%2520and%2520and%2520Ananda%2520Theertha%2520Suresh%2520and%2520Ahmad%2520Beirami%26entry.1292438233%3D%2520%2520Language%2520model%2520alignment%2520has%2520become%2520a%2520critical%2520step%2520in%2520training%2520modern%250Agenerative%2520language%2520models.%2520The%2520goal%2520of%2520alignment%2520is%2520to%2520finetune%2520a%2520reference%250Amodel%2520such%2520that%2520the%2520win%2520rate%2520of%2520a%2520sample%2520from%2520the%2520aligned%2520model%2520over%2520a%2520sample%250Afrom%2520the%2520reference%2520model%2520is%2520high%252C%2520subject%2520to%2520a%2520KL%2520divergence%2520constraint.%2520Today%252C%250Awe%2520are%2520increasingly%2520using%2520inference-time%2520algorithms%2520%2528e.g.%252C%2520Best-of-N%252C%250Acontrolled%2520decoding%252C%2520tree%2520search%2529%2520to%2520decode%2520from%2520language%2520models%2520rather%2520than%250Astandard%2520sampling.%2520However%252C%2520the%2520alignment%2520objective%2520does%2520not%2520capture%2520such%250Ainference-time%2520decoding%2520procedures.%2520We%2520show%2520that%2520the%2520existing%2520alignment%250Aframework%2520is%2520sub-optimal%2520in%2520view%2520of%2520such%2520inference-time%2520methods.%2520We%2520then%2520modify%250Athe%2520alignment%2520objective%2520and%2520propose%2520a%2520framework%2520for%2520inference-aware%2520alignment%250A%2528IAPO%2529.%2520We%2520prove%2520that%2520for%2520any%2520inference-time%2520decoding%2520algorithm%252C%2520the%2520optimal%250Asolution%2520that%2520optimizes%2520the%2520inference-time%2520win%2520rate%2520of%2520the%2520aligned%2520policy%250Aagainst%2520the%2520reference%2520policy%2520is%2520the%2520solution%2520to%2520the%2520typical%2520RLHF%2520problem%2520with%2520a%250Atransformation%2520of%2520the%2520reward.%2520This%2520motivates%2520us%2520to%2520provide%2520the%2520KL-regularized%250Acalibrate-and-transform%2520RL%2520%2528CTRL%2529%2520algorithm%2520to%2520solve%2520this%2520problem%252C%2520which%250Ainvolves%2520a%2520reward%2520calibration%2520step%2520and%2520a%2520KL-regularized%2520reward%2520maximization%250Astep%2520with%2520a%2520transformation%2520of%2520the%2520calibrated%2520reward.%2520We%2520particularize%2520our%2520study%250Ato%2520two%2520important%2520inference-time%2520strategies%253A%2520best-of-N%2520sampling%2520and%2520best-of-N%250Ajailbreaking%252C%2520where%2520N%2520responses%2520are%2520sampled%2520from%2520the%2520model%2520and%2520the%2520one%2520with%2520the%250Ahighest%2520or%2520lowest%2520reward%2520is%2520selected.%2520We%2520propose%2520specific%2520transformations%2520for%250Athese%2520strategies%2520and%2520demonstrate%2520that%2520our%2520framework%2520offers%2520significant%250Aimprovements%2520over%2520existing%2520state-of-the-art%2520methods%2520for%2520language%2520model%250Aalignment.%2520Empirically%252C%2520we%2520outperform%2520baselines%2520that%2520are%2520designed%2520without%250Ataking%2520inference-time%2520decoding%2520into%2520consideration%2520by%25208-12%2525%2520and%25204-9%2525%2520on%250Ainference-time%2520win%2520rates%2520over%2520the%2520Anthropic%2520helpfulness%2520and%2520harmlessness%2520dialog%250Abenchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfAlign%3A%20Inference-aware%20language%20model%20alignment&entry.906535625=Ananth%20Balashankar%20and%20Ziteng%20Sun%20and%20Jonathan%20Berant%20and%20Jacob%20Eisenstein%20and%20Michael%20Collins%20and%20Adrian%20Hutter%20and%20Jong%20Lee%20and%20Chirag%20Nagpal%20and%20Flavien%20Prost%20and%20Aradhana%20Sinha%20and%20and%20Ananda%20Theertha%20Suresh%20and%20Ahmad%20Beirami&entry.1292438233=%20%20Language%20model%20alignment%20has%20become%20a%20critical%20step%20in%20training%20modern%0Agenerative%20language%20models.%20The%20goal%20of%20alignment%20is%20to%20finetune%20a%20reference%0Amodel%20such%20that%20the%20win%20rate%20of%20a%20sample%20from%20the%20aligned%20model%20over%20a%20sample%0Afrom%20the%20reference%20model%20is%20high%2C%20subject%20to%20a%20KL%20divergence%20constraint.%20Today%2C%0Awe%20are%20increasingly%20using%20inference-time%20algorithms%20%28e.g.%2C%20Best-of-N%2C%0Acontrolled%20decoding%2C%20tree%20search%29%20to%20decode%20from%20language%20models%20rather%20than%0Astandard%20sampling.%20However%2C%20the%20alignment%20objective%20does%20not%20capture%20such%0Ainference-time%20decoding%20procedures.%20We%20show%20that%20the%20existing%20alignment%0Aframework%20is%20sub-optimal%20in%20view%20of%20such%20inference-time%20methods.%20We%20then%20modify%0Athe%20alignment%20objective%20and%20propose%20a%20framework%20for%20inference-aware%20alignment%0A%28IAPO%29.%20We%20prove%20that%20for%20any%20inference-time%20decoding%20algorithm%2C%20the%20optimal%0Asolution%20that%20optimizes%20the%20inference-time%20win%20rate%20of%20the%20aligned%20policy%0Aagainst%20the%20reference%20policy%20is%20the%20solution%20to%20the%20typical%20RLHF%20problem%20with%20a%0Atransformation%20of%20the%20reward.%20This%20motivates%20us%20to%20provide%20the%20KL-regularized%0Acalibrate-and-transform%20RL%20%28CTRL%29%20algorithm%20to%20solve%20this%20problem%2C%20which%0Ainvolves%20a%20reward%20calibration%20step%20and%20a%20KL-regularized%20reward%20maximization%0Astep%20with%20a%20transformation%20of%20the%20calibrated%20reward.%20We%20particularize%20our%20study%0Ato%20two%20important%20inference-time%20strategies%3A%20best-of-N%20sampling%20and%20best-of-N%0Ajailbreaking%2C%20where%20N%20responses%20are%20sampled%20from%20the%20model%20and%20the%20one%20with%20the%0Ahighest%20or%20lowest%20reward%20is%20selected.%20We%20propose%20specific%20transformations%20for%0Athese%20strategies%20and%20demonstrate%20that%20our%20framework%20offers%20significant%0Aimprovements%20over%20existing%20state-of-the-art%20methods%20for%20language%20model%0Aalignment.%20Empirically%2C%20we%20outperform%20baselines%20that%20are%20designed%20without%0Ataking%20inference-time%20decoding%20into%20consideration%20by%208-12%25%20and%204-9%25%20on%0Ainference-time%20win%20rates%20over%20the%20Anthropic%20helpfulness%20and%20harmlessness%20dialog%0Abenchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19792v1&entry.124074799=Read"},
{"title": "MERT: Acoustic Music Understanding Model with Large-Scale\n  Self-supervised Training", "author": "Yizhi Li and Ruibin Yuan and Ge Zhang and Yinghao Ma and Xingran Chen and Hanzhi Yin and Chenghao Xiao and Chenghua Lin and Anton Ragni and Emmanouil Benetos and Norbert Gyenge and Roger Dannenberg and Ruibo Liu and Wenhu Chen and Gus Xia and Yemin Shi and Wenhao Huang and Zili Wang and Yike Guo and Jie Fu", "abstract": "  Self-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision,\ntext, and speech. Although SSL has been proven effective in speech and audio,\nits application to music audio has yet to be thoroughly explored. This is\npartially due to the distinctive challenges associated with modelling musical\nknowledge, particularly tonal and pitched characteristics of music. To address\nthis research gap, we propose an acoustic Music undERstanding model with\nlarge-scale self-supervised Training (MERT), which incorporates teacher models\nto provide pseudo labels in the masked language modelling (MLM) style acoustic\npre-training. In our exploration, we identified an effective combination of\nteacher models, which outperforms conventional speech and audio approaches in\nterms of performance. This combination includes an acoustic teacher based on\nResidual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical\nteacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide\nrange of settings to overcome the instability in acoustic language model\npre-training, which allows our designed paradigm to scale from 95M to 330M\nparameters. Experimental results indicate that our model can generalise and\nperform well on 14 music understanding tasks and attain state-of-the-art (SOTA)\noverall scores.\n", "link": "http://arxiv.org/abs/2306.00107v5", "date": "2024-12-27", "relevancy": 1.9371, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5068}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MERT%3A%20Acoustic%20Music%20Understanding%20Model%20with%20Large-Scale%0A%20%20Self-supervised%20Training&body=Title%3A%20MERT%3A%20Acoustic%20Music%20Understanding%20Model%20with%20Large-Scale%0A%20%20Self-supervised%20Training%0AAuthor%3A%20Yizhi%20Li%20and%20Ruibin%20Yuan%20and%20Ge%20Zhang%20and%20Yinghao%20Ma%20and%20Xingran%20Chen%20and%20Hanzhi%20Yin%20and%20Chenghao%20Xiao%20and%20Chenghua%20Lin%20and%20Anton%20Ragni%20and%20Emmanouil%20Benetos%20and%20Norbert%20Gyenge%20and%20Roger%20Dannenberg%20and%20Ruibo%20Liu%20and%20Wenhu%20Chen%20and%20Gus%20Xia%20and%20Yemin%20Shi%20and%20Wenhao%20Huang%20and%20Zili%20Wang%20and%20Yike%20Guo%20and%20Jie%20Fu%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20emerged%20as%20a%20promising%20paradigm%0Afor%20training%20generalisable%20models%20on%20large-scale%20data%20in%20the%20fields%20of%20vision%2C%0Atext%2C%20and%20speech.%20Although%20SSL%20has%20been%20proven%20effective%20in%20speech%20and%20audio%2C%0Aits%20application%20to%20music%20audio%20has%20yet%20to%20be%20thoroughly%20explored.%20This%20is%0Apartially%20due%20to%20the%20distinctive%20challenges%20associated%20with%20modelling%20musical%0Aknowledge%2C%20particularly%20tonal%20and%20pitched%20characteristics%20of%20music.%20To%20address%0Athis%20research%20gap%2C%20we%20propose%20an%20acoustic%20Music%20undERstanding%20model%20with%0Alarge-scale%20self-supervised%20Training%20%28MERT%29%2C%20which%20incorporates%20teacher%20models%0Ato%20provide%20pseudo%20labels%20in%20the%20masked%20language%20modelling%20%28MLM%29%20style%20acoustic%0Apre-training.%20In%20our%20exploration%2C%20we%20identified%20an%20effective%20combination%20of%0Ateacher%20models%2C%20which%20outperforms%20conventional%20speech%20and%20audio%20approaches%20in%0Aterms%20of%20performance.%20This%20combination%20includes%20an%20acoustic%20teacher%20based%20on%0AResidual%20Vector%20Quantisation%20-%20Variational%20AutoEncoder%20%28RVQ-VAE%29%20and%20a%20musical%0Ateacher%20based%20on%20the%20Constant-Q%20Transform%20%28CQT%29.%20Furthermore%2C%20we%20explore%20a%20wide%0Arange%20of%20settings%20to%20overcome%20the%20instability%20in%20acoustic%20language%20model%0Apre-training%2C%20which%20allows%20our%20designed%20paradigm%20to%20scale%20from%2095M%20to%20330M%0Aparameters.%20Experimental%20results%20indicate%20that%20our%20model%20can%20generalise%20and%0Aperform%20well%20on%2014%20music%20understanding%20tasks%20and%20attain%20state-of-the-art%20%28SOTA%29%0Aoverall%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00107v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMERT%253A%2520Acoustic%2520Music%2520Understanding%2520Model%2520with%2520Large-Scale%250A%2520%2520Self-supervised%2520Training%26entry.906535625%3DYizhi%2520Li%2520and%2520Ruibin%2520Yuan%2520and%2520Ge%2520Zhang%2520and%2520Yinghao%2520Ma%2520and%2520Xingran%2520Chen%2520and%2520Hanzhi%2520Yin%2520and%2520Chenghao%2520Xiao%2520and%2520Chenghua%2520Lin%2520and%2520Anton%2520Ragni%2520and%2520Emmanouil%2520Benetos%2520and%2520Norbert%2520Gyenge%2520and%2520Roger%2520Dannenberg%2520and%2520Ruibo%2520Liu%2520and%2520Wenhu%2520Chen%2520and%2520Gus%2520Xia%2520and%2520Yemin%2520Shi%2520and%2520Wenhao%2520Huang%2520and%2520Zili%2520Wang%2520and%2520Yike%2520Guo%2520and%2520Jie%2520Fu%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520paradigm%250Afor%2520training%2520generalisable%2520models%2520on%2520large-scale%2520data%2520in%2520the%2520fields%2520of%2520vision%252C%250Atext%252C%2520and%2520speech.%2520Although%2520SSL%2520has%2520been%2520proven%2520effective%2520in%2520speech%2520and%2520audio%252C%250Aits%2520application%2520to%2520music%2520audio%2520has%2520yet%2520to%2520be%2520thoroughly%2520explored.%2520This%2520is%250Apartially%2520due%2520to%2520the%2520distinctive%2520challenges%2520associated%2520with%2520modelling%2520musical%250Aknowledge%252C%2520particularly%2520tonal%2520and%2520pitched%2520characteristics%2520of%2520music.%2520To%2520address%250Athis%2520research%2520gap%252C%2520we%2520propose%2520an%2520acoustic%2520Music%2520undERstanding%2520model%2520with%250Alarge-scale%2520self-supervised%2520Training%2520%2528MERT%2529%252C%2520which%2520incorporates%2520teacher%2520models%250Ato%2520provide%2520pseudo%2520labels%2520in%2520the%2520masked%2520language%2520modelling%2520%2528MLM%2529%2520style%2520acoustic%250Apre-training.%2520In%2520our%2520exploration%252C%2520we%2520identified%2520an%2520effective%2520combination%2520of%250Ateacher%2520models%252C%2520which%2520outperforms%2520conventional%2520speech%2520and%2520audio%2520approaches%2520in%250Aterms%2520of%2520performance.%2520This%2520combination%2520includes%2520an%2520acoustic%2520teacher%2520based%2520on%250AResidual%2520Vector%2520Quantisation%2520-%2520Variational%2520AutoEncoder%2520%2528RVQ-VAE%2529%2520and%2520a%2520musical%250Ateacher%2520based%2520on%2520the%2520Constant-Q%2520Transform%2520%2528CQT%2529.%2520Furthermore%252C%2520we%2520explore%2520a%2520wide%250Arange%2520of%2520settings%2520to%2520overcome%2520the%2520instability%2520in%2520acoustic%2520language%2520model%250Apre-training%252C%2520which%2520allows%2520our%2520designed%2520paradigm%2520to%2520scale%2520from%252095M%2520to%2520330M%250Aparameters.%2520Experimental%2520results%2520indicate%2520that%2520our%2520model%2520can%2520generalise%2520and%250Aperform%2520well%2520on%252014%2520music%2520understanding%2520tasks%2520and%2520attain%2520state-of-the-art%2520%2528SOTA%2529%250Aoverall%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00107v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MERT%3A%20Acoustic%20Music%20Understanding%20Model%20with%20Large-Scale%0A%20%20Self-supervised%20Training&entry.906535625=Yizhi%20Li%20and%20Ruibin%20Yuan%20and%20Ge%20Zhang%20and%20Yinghao%20Ma%20and%20Xingran%20Chen%20and%20Hanzhi%20Yin%20and%20Chenghao%20Xiao%20and%20Chenghua%20Lin%20and%20Anton%20Ragni%20and%20Emmanouil%20Benetos%20and%20Norbert%20Gyenge%20and%20Roger%20Dannenberg%20and%20Ruibo%20Liu%20and%20Wenhu%20Chen%20and%20Gus%20Xia%20and%20Yemin%20Shi%20and%20Wenhao%20Huang%20and%20Zili%20Wang%20and%20Yike%20Guo%20and%20Jie%20Fu&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20emerged%20as%20a%20promising%20paradigm%0Afor%20training%20generalisable%20models%20on%20large-scale%20data%20in%20the%20fields%20of%20vision%2C%0Atext%2C%20and%20speech.%20Although%20SSL%20has%20been%20proven%20effective%20in%20speech%20and%20audio%2C%0Aits%20application%20to%20music%20audio%20has%20yet%20to%20be%20thoroughly%20explored.%20This%20is%0Apartially%20due%20to%20the%20distinctive%20challenges%20associated%20with%20modelling%20musical%0Aknowledge%2C%20particularly%20tonal%20and%20pitched%20characteristics%20of%20music.%20To%20address%0Athis%20research%20gap%2C%20we%20propose%20an%20acoustic%20Music%20undERstanding%20model%20with%0Alarge-scale%20self-supervised%20Training%20%28MERT%29%2C%20which%20incorporates%20teacher%20models%0Ato%20provide%20pseudo%20labels%20in%20the%20masked%20language%20modelling%20%28MLM%29%20style%20acoustic%0Apre-training.%20In%20our%20exploration%2C%20we%20identified%20an%20effective%20combination%20of%0Ateacher%20models%2C%20which%20outperforms%20conventional%20speech%20and%20audio%20approaches%20in%0Aterms%20of%20performance.%20This%20combination%20includes%20an%20acoustic%20teacher%20based%20on%0AResidual%20Vector%20Quantisation%20-%20Variational%20AutoEncoder%20%28RVQ-VAE%29%20and%20a%20musical%0Ateacher%20based%20on%20the%20Constant-Q%20Transform%20%28CQT%29.%20Furthermore%2C%20we%20explore%20a%20wide%0Arange%20of%20settings%20to%20overcome%20the%20instability%20in%20acoustic%20language%20model%0Apre-training%2C%20which%20allows%20our%20designed%20paradigm%20to%20scale%20from%2095M%20to%20330M%0Aparameters.%20Experimental%20results%20indicate%20that%20our%20model%20can%20generalise%20and%0Aperform%20well%20on%2014%20music%20understanding%20tasks%20and%20attain%20state-of-the-art%20%28SOTA%29%0Aoverall%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00107v5&entry.124074799=Read"},
{"title": "\"Did my figure do justice to the answer?\" : Towards Multimodal Short\n  Answer Grading with Feedback (MMSAF)", "author": "Pritam Sil and Bhaskaran Raman and Pushpak Bhattacharyya", "abstract": "  Personalized feedback plays a vital role in a student's learning process.\nWhile existing systems are adept at providing feedback over MCQ-based\nevaluation, this work focuses more on subjective and open-ended questions,\nwhich is similar to the problem of Automatic Short Answer Grading (ASAG) with\nfeedback. Additionally, we introduce the Multimodal Short Answer grading with\nFeedback (MMSAF) problem over the traditional ASAG feedback problem to address\nthe scenario where the student answer and reference answer might contain\nimages. Moreover, we introduce the MMSAF dataset with 2197 data points along\nwith an automated framework for generating such data sets. Our evaluations on\nexisting LLMs over this dataset achieved an overall accuracy of 55\\% on Level\nof Correctness labels, 75\\% on Image Relevance labels and a score of 4.27 out\nof 5 in correctness level of LLM generated feedback as rated by experts. As per\nexperts, Pixtral achieved a rating of above 4 out of all metrics, indicating\nthat it is more aligned to human judgement, and that it is the best solution\nfor assisting students.\n", "link": "http://arxiv.org/abs/2412.19755v1", "date": "2024-12-27", "relevancy": 1.9356, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4976}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4923}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Did%20my%20figure%20do%20justice%20to%20the%20answer%3F%22%20%3A%20Towards%20Multimodal%20Short%0A%20%20Answer%20Grading%20with%20Feedback%20%28MMSAF%29&body=Title%3A%20%22Did%20my%20figure%20do%20justice%20to%20the%20answer%3F%22%20%3A%20Towards%20Multimodal%20Short%0A%20%20Answer%20Grading%20with%20Feedback%20%28MMSAF%29%0AAuthor%3A%20Pritam%20Sil%20and%20Bhaskaran%20Raman%20and%20Pushpak%20Bhattacharyya%0AAbstract%3A%20%20%20Personalized%20feedback%20plays%20a%20vital%20role%20in%20a%20student%27s%20learning%20process.%0AWhile%20existing%20systems%20are%20adept%20at%20providing%20feedback%20over%20MCQ-based%0Aevaluation%2C%20this%20work%20focuses%20more%20on%20subjective%20and%20open-ended%20questions%2C%0Awhich%20is%20similar%20to%20the%20problem%20of%20Automatic%20Short%20Answer%20Grading%20%28ASAG%29%20with%0Afeedback.%20Additionally%2C%20we%20introduce%20the%20Multimodal%20Short%20Answer%20grading%20with%0AFeedback%20%28MMSAF%29%20problem%20over%20the%20traditional%20ASAG%20feedback%20problem%20to%20address%0Athe%20scenario%20where%20the%20student%20answer%20and%20reference%20answer%20might%20contain%0Aimages.%20Moreover%2C%20we%20introduce%20the%20MMSAF%20dataset%20with%202197%20data%20points%20along%0Awith%20an%20automated%20framework%20for%20generating%20such%20data%20sets.%20Our%20evaluations%20on%0Aexisting%20LLMs%20over%20this%20dataset%20achieved%20an%20overall%20accuracy%20of%2055%5C%25%20on%20Level%0Aof%20Correctness%20labels%2C%2075%5C%25%20on%20Image%20Relevance%20labels%20and%20a%20score%20of%204.27%20out%0Aof%205%20in%20correctness%20level%20of%20LLM%20generated%20feedback%20as%20rated%20by%20experts.%20As%20per%0Aexperts%2C%20Pixtral%20achieved%20a%20rating%20of%20above%204%20out%20of%20all%20metrics%2C%20indicating%0Athat%20it%20is%20more%20aligned%20to%20human%20judgement%2C%20and%20that%20it%20is%20the%20best%20solution%0Afor%20assisting%20students.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Did%2520my%2520figure%2520do%2520justice%2520to%2520the%2520answer%253F%2522%2520%253A%2520Towards%2520Multimodal%2520Short%250A%2520%2520Answer%2520Grading%2520with%2520Feedback%2520%2528MMSAF%2529%26entry.906535625%3DPritam%2520Sil%2520and%2520Bhaskaran%2520Raman%2520and%2520Pushpak%2520Bhattacharyya%26entry.1292438233%3D%2520%2520Personalized%2520feedback%2520plays%2520a%2520vital%2520role%2520in%2520a%2520student%2527s%2520learning%2520process.%250AWhile%2520existing%2520systems%2520are%2520adept%2520at%2520providing%2520feedback%2520over%2520MCQ-based%250Aevaluation%252C%2520this%2520work%2520focuses%2520more%2520on%2520subjective%2520and%2520open-ended%2520questions%252C%250Awhich%2520is%2520similar%2520to%2520the%2520problem%2520of%2520Automatic%2520Short%2520Answer%2520Grading%2520%2528ASAG%2529%2520with%250Afeedback.%2520Additionally%252C%2520we%2520introduce%2520the%2520Multimodal%2520Short%2520Answer%2520grading%2520with%250AFeedback%2520%2528MMSAF%2529%2520problem%2520over%2520the%2520traditional%2520ASAG%2520feedback%2520problem%2520to%2520address%250Athe%2520scenario%2520where%2520the%2520student%2520answer%2520and%2520reference%2520answer%2520might%2520contain%250Aimages.%2520Moreover%252C%2520we%2520introduce%2520the%2520MMSAF%2520dataset%2520with%25202197%2520data%2520points%2520along%250Awith%2520an%2520automated%2520framework%2520for%2520generating%2520such%2520data%2520sets.%2520Our%2520evaluations%2520on%250Aexisting%2520LLMs%2520over%2520this%2520dataset%2520achieved%2520an%2520overall%2520accuracy%2520of%252055%255C%2525%2520on%2520Level%250Aof%2520Correctness%2520labels%252C%252075%255C%2525%2520on%2520Image%2520Relevance%2520labels%2520and%2520a%2520score%2520of%25204.27%2520out%250Aof%25205%2520in%2520correctness%2520level%2520of%2520LLM%2520generated%2520feedback%2520as%2520rated%2520by%2520experts.%2520As%2520per%250Aexperts%252C%2520Pixtral%2520achieved%2520a%2520rating%2520of%2520above%25204%2520out%2520of%2520all%2520metrics%252C%2520indicating%250Athat%2520it%2520is%2520more%2520aligned%2520to%2520human%2520judgement%252C%2520and%2520that%2520it%2520is%2520the%2520best%2520solution%250Afor%2520assisting%2520students.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Did%20my%20figure%20do%20justice%20to%20the%20answer%3F%22%20%3A%20Towards%20Multimodal%20Short%0A%20%20Answer%20Grading%20with%20Feedback%20%28MMSAF%29&entry.906535625=Pritam%20Sil%20and%20Bhaskaran%20Raman%20and%20Pushpak%20Bhattacharyya&entry.1292438233=%20%20Personalized%20feedback%20plays%20a%20vital%20role%20in%20a%20student%27s%20learning%20process.%0AWhile%20existing%20systems%20are%20adept%20at%20providing%20feedback%20over%20MCQ-based%0Aevaluation%2C%20this%20work%20focuses%20more%20on%20subjective%20and%20open-ended%20questions%2C%0Awhich%20is%20similar%20to%20the%20problem%20of%20Automatic%20Short%20Answer%20Grading%20%28ASAG%29%20with%0Afeedback.%20Additionally%2C%20we%20introduce%20the%20Multimodal%20Short%20Answer%20grading%20with%0AFeedback%20%28MMSAF%29%20problem%20over%20the%20traditional%20ASAG%20feedback%20problem%20to%20address%0Athe%20scenario%20where%20the%20student%20answer%20and%20reference%20answer%20might%20contain%0Aimages.%20Moreover%2C%20we%20introduce%20the%20MMSAF%20dataset%20with%202197%20data%20points%20along%0Awith%20an%20automated%20framework%20for%20generating%20such%20data%20sets.%20Our%20evaluations%20on%0Aexisting%20LLMs%20over%20this%20dataset%20achieved%20an%20overall%20accuracy%20of%2055%5C%25%20on%20Level%0Aof%20Correctness%20labels%2C%2075%5C%25%20on%20Image%20Relevance%20labels%20and%20a%20score%20of%204.27%20out%0Aof%205%20in%20correctness%20level%20of%20LLM%20generated%20feedback%20as%20rated%20by%20experts.%20As%20per%0Aexperts%2C%20Pixtral%20achieved%20a%20rating%20of%20above%204%20out%20of%20all%20metrics%2C%20indicating%0Athat%20it%20is%20more%20aligned%20to%20human%20judgement%2C%20and%20that%20it%20is%20the%20best%20solution%0Afor%20assisting%20students.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19755v1&entry.124074799=Read"},
{"title": "Tensor Network Estimation of Distribution Algorithms", "author": "John Gardiner and Javier Lopez-Piqueres", "abstract": "  Tensor networks are a tool first employed in the context of many-body quantum\nphysics that now have a wide range of uses across the computational sciences,\nfrom numerical methods to machine learning. Methods integrating tensor networks\ninto evolutionary optimization algorithms have appeared in the recent\nliterature. In essence, these methods can be understood as replacing the\ntraditional crossover operation of a genetic algorithm with a tensor\nnetwork-based generative model. We investigate these methods from the point of\nview that they are Estimation of Distribution Algorithms (EDAs). We find that\noptimization performance of these methods is not related to the power of the\ngenerative model in a straightforward way. Generative models that are better\n(in the sense that they better model the distribution from which their training\ndata is drawn) do not necessarily result in better performance of the\noptimization algorithm they form a part of. This raises the question of how\nbest to incorporate powerful generative models into optimization routines. In\nlight of this we find that adding an explicit mutation operator to the output\nof the generative model often improves optimization performance.\n", "link": "http://arxiv.org/abs/2412.19780v1", "date": "2024-12-27", "relevancy": 1.9299, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5158}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4781}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensor%20Network%20Estimation%20of%20Distribution%20Algorithms&body=Title%3A%20Tensor%20Network%20Estimation%20of%20Distribution%20Algorithms%0AAuthor%3A%20John%20Gardiner%20and%20Javier%20Lopez-Piqueres%0AAbstract%3A%20%20%20Tensor%20networks%20are%20a%20tool%20first%20employed%20in%20the%20context%20of%20many-body%20quantum%0Aphysics%20that%20now%20have%20a%20wide%20range%20of%20uses%20across%20the%20computational%20sciences%2C%0Afrom%20numerical%20methods%20to%20machine%20learning.%20Methods%20integrating%20tensor%20networks%0Ainto%20evolutionary%20optimization%20algorithms%20have%20appeared%20in%20the%20recent%0Aliterature.%20In%20essence%2C%20these%20methods%20can%20be%20understood%20as%20replacing%20the%0Atraditional%20crossover%20operation%20of%20a%20genetic%20algorithm%20with%20a%20tensor%0Anetwork-based%20generative%20model.%20We%20investigate%20these%20methods%20from%20the%20point%20of%0Aview%20that%20they%20are%20Estimation%20of%20Distribution%20Algorithms%20%28EDAs%29.%20We%20find%20that%0Aoptimization%20performance%20of%20these%20methods%20is%20not%20related%20to%20the%20power%20of%20the%0Agenerative%20model%20in%20a%20straightforward%20way.%20Generative%20models%20that%20are%20better%0A%28in%20the%20sense%20that%20they%20better%20model%20the%20distribution%20from%20which%20their%20training%0Adata%20is%20drawn%29%20do%20not%20necessarily%20result%20in%20better%20performance%20of%20the%0Aoptimization%20algorithm%20they%20form%20a%20part%20of.%20This%20raises%20the%20question%20of%20how%0Abest%20to%20incorporate%20powerful%20generative%20models%20into%20optimization%20routines.%20In%0Alight%20of%20this%20we%20find%20that%20adding%20an%20explicit%20mutation%20operator%20to%20the%20output%0Aof%20the%20generative%20model%20often%20improves%20optimization%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensor%2520Network%2520Estimation%2520of%2520Distribution%2520Algorithms%26entry.906535625%3DJohn%2520Gardiner%2520and%2520Javier%2520Lopez-Piqueres%26entry.1292438233%3D%2520%2520Tensor%2520networks%2520are%2520a%2520tool%2520first%2520employed%2520in%2520the%2520context%2520of%2520many-body%2520quantum%250Aphysics%2520that%2520now%2520have%2520a%2520wide%2520range%2520of%2520uses%2520across%2520the%2520computational%2520sciences%252C%250Afrom%2520numerical%2520methods%2520to%2520machine%2520learning.%2520Methods%2520integrating%2520tensor%2520networks%250Ainto%2520evolutionary%2520optimization%2520algorithms%2520have%2520appeared%2520in%2520the%2520recent%250Aliterature.%2520In%2520essence%252C%2520these%2520methods%2520can%2520be%2520understood%2520as%2520replacing%2520the%250Atraditional%2520crossover%2520operation%2520of%2520a%2520genetic%2520algorithm%2520with%2520a%2520tensor%250Anetwork-based%2520generative%2520model.%2520We%2520investigate%2520these%2520methods%2520from%2520the%2520point%2520of%250Aview%2520that%2520they%2520are%2520Estimation%2520of%2520Distribution%2520Algorithms%2520%2528EDAs%2529.%2520We%2520find%2520that%250Aoptimization%2520performance%2520of%2520these%2520methods%2520is%2520not%2520related%2520to%2520the%2520power%2520of%2520the%250Agenerative%2520model%2520in%2520a%2520straightforward%2520way.%2520Generative%2520models%2520that%2520are%2520better%250A%2528in%2520the%2520sense%2520that%2520they%2520better%2520model%2520the%2520distribution%2520from%2520which%2520their%2520training%250Adata%2520is%2520drawn%2529%2520do%2520not%2520necessarily%2520result%2520in%2520better%2520performance%2520of%2520the%250Aoptimization%2520algorithm%2520they%2520form%2520a%2520part%2520of.%2520This%2520raises%2520the%2520question%2520of%2520how%250Abest%2520to%2520incorporate%2520powerful%2520generative%2520models%2520into%2520optimization%2520routines.%2520In%250Alight%2520of%2520this%2520we%2520find%2520that%2520adding%2520an%2520explicit%2520mutation%2520operator%2520to%2520the%2520output%250Aof%2520the%2520generative%2520model%2520often%2520improves%2520optimization%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensor%20Network%20Estimation%20of%20Distribution%20Algorithms&entry.906535625=John%20Gardiner%20and%20Javier%20Lopez-Piqueres&entry.1292438233=%20%20Tensor%20networks%20are%20a%20tool%20first%20employed%20in%20the%20context%20of%20many-body%20quantum%0Aphysics%20that%20now%20have%20a%20wide%20range%20of%20uses%20across%20the%20computational%20sciences%2C%0Afrom%20numerical%20methods%20to%20machine%20learning.%20Methods%20integrating%20tensor%20networks%0Ainto%20evolutionary%20optimization%20algorithms%20have%20appeared%20in%20the%20recent%0Aliterature.%20In%20essence%2C%20these%20methods%20can%20be%20understood%20as%20replacing%20the%0Atraditional%20crossover%20operation%20of%20a%20genetic%20algorithm%20with%20a%20tensor%0Anetwork-based%20generative%20model.%20We%20investigate%20these%20methods%20from%20the%20point%20of%0Aview%20that%20they%20are%20Estimation%20of%20Distribution%20Algorithms%20%28EDAs%29.%20We%20find%20that%0Aoptimization%20performance%20of%20these%20methods%20is%20not%20related%20to%20the%20power%20of%20the%0Agenerative%20model%20in%20a%20straightforward%20way.%20Generative%20models%20that%20are%20better%0A%28in%20the%20sense%20that%20they%20better%20model%20the%20distribution%20from%20which%20their%20training%0Adata%20is%20drawn%29%20do%20not%20necessarily%20result%20in%20better%20performance%20of%20the%0Aoptimization%20algorithm%20they%20form%20a%20part%20of.%20This%20raises%20the%20question%20of%20how%0Abest%20to%20incorporate%20powerful%20generative%20models%20into%20optimization%20routines.%20In%0Alight%20of%20this%20we%20find%20that%20adding%20an%20explicit%20mutation%20operator%20to%20the%20output%0Aof%20the%20generative%20model%20often%20improves%20optimization%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19780v1&entry.124074799=Read"},
{"title": "Adaptive Context-Aware Multi-Path Transmission Control for VR/AR\n  Content: A Deep Reinforcement Learning Approach", "author": "Shakil Ahmed and Saifur Rahman Sabuj and Ashfaq Khokhar", "abstract": "  This paper introduces the Adaptive Context-Aware Multi-Path Transmission\nControl Protocol (ACMPTCP), an efficient approach designed to optimize the\nperformance of Multi-Path Transmission Control Protocol (MPTCP) for\ndata-intensive applications such as augmented and virtual reality (AR/VR)\nstreaming. ACMPTCP addresses the limitations of conventional MPTCP by\nleveraging deep reinforcement learning (DRL) for agile end-to-end path\nmanagement and optimal bandwidth allocation, facilitating path realignment\nacross diverse network environments.\n", "link": "http://arxiv.org/abs/2412.19737v1", "date": "2024-12-27", "relevancy": 1.8994, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4826}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Context-Aware%20Multi-Path%20Transmission%20Control%20for%20VR/AR%0A%20%20Content%3A%20A%20Deep%20Reinforcement%20Learning%20Approach&body=Title%3A%20Adaptive%20Context-Aware%20Multi-Path%20Transmission%20Control%20for%20VR/AR%0A%20%20Content%3A%20A%20Deep%20Reinforcement%20Learning%20Approach%0AAuthor%3A%20Shakil%20Ahmed%20and%20Saifur%20Rahman%20Sabuj%20and%20Ashfaq%20Khokhar%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20Adaptive%20Context-Aware%20Multi-Path%20Transmission%0AControl%20Protocol%20%28ACMPTCP%29%2C%20an%20efficient%20approach%20designed%20to%20optimize%20the%0Aperformance%20of%20Multi-Path%20Transmission%20Control%20Protocol%20%28MPTCP%29%20for%0Adata-intensive%20applications%20such%20as%20augmented%20and%20virtual%20reality%20%28AR/VR%29%0Astreaming.%20ACMPTCP%20addresses%20the%20limitations%20of%20conventional%20MPTCP%20by%0Aleveraging%20deep%20reinforcement%20learning%20%28DRL%29%20for%20agile%20end-to-end%20path%0Amanagement%20and%20optimal%20bandwidth%20allocation%2C%20facilitating%20path%20realignment%0Aacross%20diverse%20network%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Context-Aware%2520Multi-Path%2520Transmission%2520Control%2520for%2520VR/AR%250A%2520%2520Content%253A%2520A%2520Deep%2520Reinforcement%2520Learning%2520Approach%26entry.906535625%3DShakil%2520Ahmed%2520and%2520Saifur%2520Rahman%2520Sabuj%2520and%2520Ashfaq%2520Khokhar%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520Adaptive%2520Context-Aware%2520Multi-Path%2520Transmission%250AControl%2520Protocol%2520%2528ACMPTCP%2529%252C%2520an%2520efficient%2520approach%2520designed%2520to%2520optimize%2520the%250Aperformance%2520of%2520Multi-Path%2520Transmission%2520Control%2520Protocol%2520%2528MPTCP%2529%2520for%250Adata-intensive%2520applications%2520such%2520as%2520augmented%2520and%2520virtual%2520reality%2520%2528AR/VR%2529%250Astreaming.%2520ACMPTCP%2520addresses%2520the%2520limitations%2520of%2520conventional%2520MPTCP%2520by%250Aleveraging%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520for%2520agile%2520end-to-end%2520path%250Amanagement%2520and%2520optimal%2520bandwidth%2520allocation%252C%2520facilitating%2520path%2520realignment%250Aacross%2520diverse%2520network%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Context-Aware%20Multi-Path%20Transmission%20Control%20for%20VR/AR%0A%20%20Content%3A%20A%20Deep%20Reinforcement%20Learning%20Approach&entry.906535625=Shakil%20Ahmed%20and%20Saifur%20Rahman%20Sabuj%20and%20Ashfaq%20Khokhar&entry.1292438233=%20%20This%20paper%20introduces%20the%20Adaptive%20Context-Aware%20Multi-Path%20Transmission%0AControl%20Protocol%20%28ACMPTCP%29%2C%20an%20efficient%20approach%20designed%20to%20optimize%20the%0Aperformance%20of%20Multi-Path%20Transmission%20Control%20Protocol%20%28MPTCP%29%20for%0Adata-intensive%20applications%20such%20as%20augmented%20and%20virtual%20reality%20%28AR/VR%29%0Astreaming.%20ACMPTCP%20addresses%20the%20limitations%20of%20conventional%20MPTCP%20by%0Aleveraging%20deep%20reinforcement%20learning%20%28DRL%29%20for%20agile%20end-to-end%20path%0Amanagement%20and%20optimal%20bandwidth%20allocation%2C%20facilitating%20path%20realignment%0Aacross%20diverse%20network%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19737v1&entry.124074799=Read"},
{"title": "Hyperspectral Pansharpening: Critical Review, Tools and Future\n  Perspectives", "author": "Matteo Ciotola and Giuseppe Guarino and Gemine Vivone and Giovanni Poggi and Jocelyn Chanussot and Antonio Plaza and Giuseppe Scarpa", "abstract": "  Hyperspectral pansharpening consists of fusing a high-resolution panchromatic\nband and a low-resolution hyperspectral image to obtain a new image with high\nresolution in both the spatial and spectral domains. These remote sensing\nproducts are valuable for a wide range of applications, driving ever growing\nresearch efforts. Nonetheless, results still do not meet application demands.\nIn part, this comes from the technical complexity of the task: compared to\nmultispectral pansharpening, many more bands are involved, in a spectral range\nonly partially covered by the panchromatic component and with overwhelming\nnoise. However, another major limiting factor is the absence of a comprehensive\nframework for the rapid development and accurate evaluation of new methods.\nThis paper attempts to address this issue.\n  We started by designing a dataset large and diverse enough to allow reliable\ntraining (for data-driven methods) and testing of new methods. Then, we\nselected a set of state-of-the-art methods, following different approaches,\ncharacterized by promising performance, and reimplemented them in a single\nPyTorch framework. Finally, we carried out a critical comparative analysis of\nall methods, using the most accredited quality indicators. The analysis\nhighlights the main limitations of current solutions in terms of\nspectral/spatial quality and computational efficiency, and suggests promising\nresearch directions.\n  To ensure full reproducibility of the results and support future research,\nthe framework (including codes, evaluation procedures and links to the dataset)\nis shared on https://github.com/matciotola/hyperspectral_pansharpening_toolbox,\nas a single Python-based reference benchmark toolbox.\n", "link": "http://arxiv.org/abs/2407.01355v2", "date": "2024-12-27", "relevancy": 1.8935, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5022}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4603}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20Pansharpening%3A%20Critical%20Review%2C%20Tools%20and%20Future%0A%20%20Perspectives&body=Title%3A%20Hyperspectral%20Pansharpening%3A%20Critical%20Review%2C%20Tools%20and%20Future%0A%20%20Perspectives%0AAuthor%3A%20Matteo%20Ciotola%20and%20Giuseppe%20Guarino%20and%20Gemine%20Vivone%20and%20Giovanni%20Poggi%20and%20Jocelyn%20Chanussot%20and%20Antonio%20Plaza%20and%20Giuseppe%20Scarpa%0AAbstract%3A%20%20%20Hyperspectral%20pansharpening%20consists%20of%20fusing%20a%20high-resolution%20panchromatic%0Aband%20and%20a%20low-resolution%20hyperspectral%20image%20to%20obtain%20a%20new%20image%20with%20high%0Aresolution%20in%20both%20the%20spatial%20and%20spectral%20domains.%20These%20remote%20sensing%0Aproducts%20are%20valuable%20for%20a%20wide%20range%20of%20applications%2C%20driving%20ever%20growing%0Aresearch%20efforts.%20Nonetheless%2C%20results%20still%20do%20not%20meet%20application%20demands.%0AIn%20part%2C%20this%20comes%20from%20the%20technical%20complexity%20of%20the%20task%3A%20compared%20to%0Amultispectral%20pansharpening%2C%20many%20more%20bands%20are%20involved%2C%20in%20a%20spectral%20range%0Aonly%20partially%20covered%20by%20the%20panchromatic%20component%20and%20with%20overwhelming%0Anoise.%20However%2C%20another%20major%20limiting%20factor%20is%20the%20absence%20of%20a%20comprehensive%0Aframework%20for%20the%20rapid%20development%20and%20accurate%20evaluation%20of%20new%20methods.%0AThis%20paper%20attempts%20to%20address%20this%20issue.%0A%20%20We%20started%20by%20designing%20a%20dataset%20large%20and%20diverse%20enough%20to%20allow%20reliable%0Atraining%20%28for%20data-driven%20methods%29%20and%20testing%20of%20new%20methods.%20Then%2C%20we%0Aselected%20a%20set%20of%20state-of-the-art%20methods%2C%20following%20different%20approaches%2C%0Acharacterized%20by%20promising%20performance%2C%20and%20reimplemented%20them%20in%20a%20single%0APyTorch%20framework.%20Finally%2C%20we%20carried%20out%20a%20critical%20comparative%20analysis%20of%0Aall%20methods%2C%20using%20the%20most%20accredited%20quality%20indicators.%20The%20analysis%0Ahighlights%20the%20main%20limitations%20of%20current%20solutions%20in%20terms%20of%0Aspectral/spatial%20quality%20and%20computational%20efficiency%2C%20and%20suggests%20promising%0Aresearch%20directions.%0A%20%20To%20ensure%20full%20reproducibility%20of%20the%20results%20and%20support%20future%20research%2C%0Athe%20framework%20%28including%20codes%2C%20evaluation%20procedures%20and%20links%20to%20the%20dataset%29%0Ais%20shared%20on%20https%3A//github.com/matciotola/hyperspectral_pansharpening_toolbox%2C%0Aas%20a%20single%20Python-based%20reference%20benchmark%20toolbox.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520Pansharpening%253A%2520Critical%2520Review%252C%2520Tools%2520and%2520Future%250A%2520%2520Perspectives%26entry.906535625%3DMatteo%2520Ciotola%2520and%2520Giuseppe%2520Guarino%2520and%2520Gemine%2520Vivone%2520and%2520Giovanni%2520Poggi%2520and%2520Jocelyn%2520Chanussot%2520and%2520Antonio%2520Plaza%2520and%2520Giuseppe%2520Scarpa%26entry.1292438233%3D%2520%2520Hyperspectral%2520pansharpening%2520consists%2520of%2520fusing%2520a%2520high-resolution%2520panchromatic%250Aband%2520and%2520a%2520low-resolution%2520hyperspectral%2520image%2520to%2520obtain%2520a%2520new%2520image%2520with%2520high%250Aresolution%2520in%2520both%2520the%2520spatial%2520and%2520spectral%2520domains.%2520These%2520remote%2520sensing%250Aproducts%2520are%2520valuable%2520for%2520a%2520wide%2520range%2520of%2520applications%252C%2520driving%2520ever%2520growing%250Aresearch%2520efforts.%2520Nonetheless%252C%2520results%2520still%2520do%2520not%2520meet%2520application%2520demands.%250AIn%2520part%252C%2520this%2520comes%2520from%2520the%2520technical%2520complexity%2520of%2520the%2520task%253A%2520compared%2520to%250Amultispectral%2520pansharpening%252C%2520many%2520more%2520bands%2520are%2520involved%252C%2520in%2520a%2520spectral%2520range%250Aonly%2520partially%2520covered%2520by%2520the%2520panchromatic%2520component%2520and%2520with%2520overwhelming%250Anoise.%2520However%252C%2520another%2520major%2520limiting%2520factor%2520is%2520the%2520absence%2520of%2520a%2520comprehensive%250Aframework%2520for%2520the%2520rapid%2520development%2520and%2520accurate%2520evaluation%2520of%2520new%2520methods.%250AThis%2520paper%2520attempts%2520to%2520address%2520this%2520issue.%250A%2520%2520We%2520started%2520by%2520designing%2520a%2520dataset%2520large%2520and%2520diverse%2520enough%2520to%2520allow%2520reliable%250Atraining%2520%2528for%2520data-driven%2520methods%2529%2520and%2520testing%2520of%2520new%2520methods.%2520Then%252C%2520we%250Aselected%2520a%2520set%2520of%2520state-of-the-art%2520methods%252C%2520following%2520different%2520approaches%252C%250Acharacterized%2520by%2520promising%2520performance%252C%2520and%2520reimplemented%2520them%2520in%2520a%2520single%250APyTorch%2520framework.%2520Finally%252C%2520we%2520carried%2520out%2520a%2520critical%2520comparative%2520analysis%2520of%250Aall%2520methods%252C%2520using%2520the%2520most%2520accredited%2520quality%2520indicators.%2520The%2520analysis%250Ahighlights%2520the%2520main%2520limitations%2520of%2520current%2520solutions%2520in%2520terms%2520of%250Aspectral/spatial%2520quality%2520and%2520computational%2520efficiency%252C%2520and%2520suggests%2520promising%250Aresearch%2520directions.%250A%2520%2520To%2520ensure%2520full%2520reproducibility%2520of%2520the%2520results%2520and%2520support%2520future%2520research%252C%250Athe%2520framework%2520%2528including%2520codes%252C%2520evaluation%2520procedures%2520and%2520links%2520to%2520the%2520dataset%2529%250Ais%2520shared%2520on%2520https%253A//github.com/matciotola/hyperspectral_pansharpening_toolbox%252C%250Aas%2520a%2520single%2520Python-based%2520reference%2520benchmark%2520toolbox.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20Pansharpening%3A%20Critical%20Review%2C%20Tools%20and%20Future%0A%20%20Perspectives&entry.906535625=Matteo%20Ciotola%20and%20Giuseppe%20Guarino%20and%20Gemine%20Vivone%20and%20Giovanni%20Poggi%20and%20Jocelyn%20Chanussot%20and%20Antonio%20Plaza%20and%20Giuseppe%20Scarpa&entry.1292438233=%20%20Hyperspectral%20pansharpening%20consists%20of%20fusing%20a%20high-resolution%20panchromatic%0Aband%20and%20a%20low-resolution%20hyperspectral%20image%20to%20obtain%20a%20new%20image%20with%20high%0Aresolution%20in%20both%20the%20spatial%20and%20spectral%20domains.%20These%20remote%20sensing%0Aproducts%20are%20valuable%20for%20a%20wide%20range%20of%20applications%2C%20driving%20ever%20growing%0Aresearch%20efforts.%20Nonetheless%2C%20results%20still%20do%20not%20meet%20application%20demands.%0AIn%20part%2C%20this%20comes%20from%20the%20technical%20complexity%20of%20the%20task%3A%20compared%20to%0Amultispectral%20pansharpening%2C%20many%20more%20bands%20are%20involved%2C%20in%20a%20spectral%20range%0Aonly%20partially%20covered%20by%20the%20panchromatic%20component%20and%20with%20overwhelming%0Anoise.%20However%2C%20another%20major%20limiting%20factor%20is%20the%20absence%20of%20a%20comprehensive%0Aframework%20for%20the%20rapid%20development%20and%20accurate%20evaluation%20of%20new%20methods.%0AThis%20paper%20attempts%20to%20address%20this%20issue.%0A%20%20We%20started%20by%20designing%20a%20dataset%20large%20and%20diverse%20enough%20to%20allow%20reliable%0Atraining%20%28for%20data-driven%20methods%29%20and%20testing%20of%20new%20methods.%20Then%2C%20we%0Aselected%20a%20set%20of%20state-of-the-art%20methods%2C%20following%20different%20approaches%2C%0Acharacterized%20by%20promising%20performance%2C%20and%20reimplemented%20them%20in%20a%20single%0APyTorch%20framework.%20Finally%2C%20we%20carried%20out%20a%20critical%20comparative%20analysis%20of%0Aall%20methods%2C%20using%20the%20most%20accredited%20quality%20indicators.%20The%20analysis%0Ahighlights%20the%20main%20limitations%20of%20current%20solutions%20in%20terms%20of%0Aspectral/spatial%20quality%20and%20computational%20efficiency%2C%20and%20suggests%20promising%0Aresearch%20directions.%0A%20%20To%20ensure%20full%20reproducibility%20of%20the%20results%20and%20support%20future%20research%2C%0Athe%20framework%20%28including%20codes%2C%20evaluation%20procedures%20and%20links%20to%20the%20dataset%29%0Ais%20shared%20on%20https%3A//github.com/matciotola/hyperspectral_pansharpening_toolbox%2C%0Aas%20a%20single%20Python-based%20reference%20benchmark%20toolbox.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01355v2&entry.124074799=Read"},
{"title": "Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via\n  Multi-Turn Dialogue and Dual-Agent Integration", "author": "Le Chen and Bin Lei and Dunzhi Zhou and Pei-Hung Lin and Chunhua Liao and Caiwen Ding and Ali Jannesari", "abstract": "  Migrating Fortran code to C++ is a common task for many scientific computing\nteams, driven by the need to leverage modern programming paradigms, enhance\ncross-platform compatibility, and improve maintainability. Automating this\ntranslation process using large language models (LLMs) has shown promise, but\nthe lack of high-quality, specialized datasets has hindered their\neffectiveness. In this paper, we address this challenge by introducing a novel\nmulti-turn dialogue dataset, Fortran2CPP, specifically designed for\nFortran-to-C++ code migration. Our dataset, significantly larger than existing\nalternatives, is generated using a unique LLM-driven, dual-agent pipeline\nincorporating iterative compilation, execution, and code repair to ensure high\nquality and functional correctness. To demonstrate the effectiveness of our\ndataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated\ntheir performance on two independent benchmarks. Fine-tuning on our dataset led\nto remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU\nscore and a 92\\% improvement in compilation success rate. This highlights the\ndataset's ability to enhance both the syntactic accuracy and compilability of\nthe translated C++ code. Our dataset and model have been open-sourced and are\navailable on our public GitHub\nrepository\\footnote{\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}.\n", "link": "http://arxiv.org/abs/2412.19770v1", "date": "2024-12-27", "relevancy": 1.8781, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4776}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4739}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fortran2CPP%3A%20Automating%20Fortran-to-C%2B%2B%20Migration%20using%20LLMs%20via%0A%20%20Multi-Turn%20Dialogue%20and%20Dual-Agent%20Integration&body=Title%3A%20Fortran2CPP%3A%20Automating%20Fortran-to-C%2B%2B%20Migration%20using%20LLMs%20via%0A%20%20Multi-Turn%20Dialogue%20and%20Dual-Agent%20Integration%0AAuthor%3A%20Le%20Chen%20and%20Bin%20Lei%20and%20Dunzhi%20Zhou%20and%20Pei-Hung%20Lin%20and%20Chunhua%20Liao%20and%20Caiwen%20Ding%20and%20Ali%20Jannesari%0AAbstract%3A%20%20%20Migrating%20Fortran%20code%20to%20C%2B%2B%20is%20a%20common%20task%20for%20many%20scientific%20computing%0Ateams%2C%20driven%20by%20the%20need%20to%20leverage%20modern%20programming%20paradigms%2C%20enhance%0Across-platform%20compatibility%2C%20and%20improve%20maintainability.%20Automating%20this%0Atranslation%20process%20using%20large%20language%20models%20%28LLMs%29%20has%20shown%20promise%2C%20but%0Athe%20lack%20of%20high-quality%2C%20specialized%20datasets%20has%20hindered%20their%0Aeffectiveness.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20by%20introducing%20a%20novel%0Amulti-turn%20dialogue%20dataset%2C%20Fortran2CPP%2C%20specifically%20designed%20for%0AFortran-to-C%2B%2B%20code%20migration.%20Our%20dataset%2C%20significantly%20larger%20than%20existing%0Aalternatives%2C%20is%20generated%20using%20a%20unique%20LLM-driven%2C%20dual-agent%20pipeline%0Aincorporating%20iterative%20compilation%2C%20execution%2C%20and%20code%20repair%20to%20ensure%20high%0Aquality%20and%20functional%20correctness.%20To%20demonstrate%20the%20effectiveness%20of%20our%0Adataset%2C%20we%20fine-tuned%20several%20open-weight%20LLMs%20on%20Fortran2CPP%20and%20evaluated%0Atheir%20performance%20on%20two%20independent%20benchmarks.%20Fine-tuning%20on%20our%20dataset%20led%0Ato%20remarkable%20gains%2C%20with%20models%20achieving%20up%20to%20a%203.31x%20increase%20in%20CodeBLEU%0Ascore%20and%20a%2092%5C%25%20improvement%20in%20compilation%20success%20rate.%20This%20highlights%20the%0Adataset%27s%20ability%20to%20enhance%20both%20the%20syntactic%20accuracy%20and%20compilability%20of%0Athe%20translated%20C%2B%2B%20code.%20Our%20dataset%20and%20model%20have%20been%20open-sourced%20and%20are%0Aavailable%20on%20our%20public%20GitHub%0Arepository%5Cfootnote%7B%5Curl%7Bhttps%3A//github.com/HPC-Fortran2CPP/Fortran2Cpp%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFortran2CPP%253A%2520Automating%2520Fortran-to-C%252B%252B%2520Migration%2520using%2520LLMs%2520via%250A%2520%2520Multi-Turn%2520Dialogue%2520and%2520Dual-Agent%2520Integration%26entry.906535625%3DLe%2520Chen%2520and%2520Bin%2520Lei%2520and%2520Dunzhi%2520Zhou%2520and%2520Pei-Hung%2520Lin%2520and%2520Chunhua%2520Liao%2520and%2520Caiwen%2520Ding%2520and%2520Ali%2520Jannesari%26entry.1292438233%3D%2520%2520Migrating%2520Fortran%2520code%2520to%2520C%252B%252B%2520is%2520a%2520common%2520task%2520for%2520many%2520scientific%2520computing%250Ateams%252C%2520driven%2520by%2520the%2520need%2520to%2520leverage%2520modern%2520programming%2520paradigms%252C%2520enhance%250Across-platform%2520compatibility%252C%2520and%2520improve%2520maintainability.%2520Automating%2520this%250Atranslation%2520process%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520shown%2520promise%252C%2520but%250Athe%2520lack%2520of%2520high-quality%252C%2520specialized%2520datasets%2520has%2520hindered%2520their%250Aeffectiveness.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520challenge%2520by%2520introducing%2520a%2520novel%250Amulti-turn%2520dialogue%2520dataset%252C%2520Fortran2CPP%252C%2520specifically%2520designed%2520for%250AFortran-to-C%252B%252B%2520code%2520migration.%2520Our%2520dataset%252C%2520significantly%2520larger%2520than%2520existing%250Aalternatives%252C%2520is%2520generated%2520using%2520a%2520unique%2520LLM-driven%252C%2520dual-agent%2520pipeline%250Aincorporating%2520iterative%2520compilation%252C%2520execution%252C%2520and%2520code%2520repair%2520to%2520ensure%2520high%250Aquality%2520and%2520functional%2520correctness.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Adataset%252C%2520we%2520fine-tuned%2520several%2520open-weight%2520LLMs%2520on%2520Fortran2CPP%2520and%2520evaluated%250Atheir%2520performance%2520on%2520two%2520independent%2520benchmarks.%2520Fine-tuning%2520on%2520our%2520dataset%2520led%250Ato%2520remarkable%2520gains%252C%2520with%2520models%2520achieving%2520up%2520to%2520a%25203.31x%2520increase%2520in%2520CodeBLEU%250Ascore%2520and%2520a%252092%255C%2525%2520improvement%2520in%2520compilation%2520success%2520rate.%2520This%2520highlights%2520the%250Adataset%2527s%2520ability%2520to%2520enhance%2520both%2520the%2520syntactic%2520accuracy%2520and%2520compilability%2520of%250Athe%2520translated%2520C%252B%252B%2520code.%2520Our%2520dataset%2520and%2520model%2520have%2520been%2520open-sourced%2520and%2520are%250Aavailable%2520on%2520our%2520public%2520GitHub%250Arepository%255Cfootnote%257B%255Curl%257Bhttps%253A//github.com/HPC-Fortran2CPP/Fortran2Cpp%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fortran2CPP%3A%20Automating%20Fortran-to-C%2B%2B%20Migration%20using%20LLMs%20via%0A%20%20Multi-Turn%20Dialogue%20and%20Dual-Agent%20Integration&entry.906535625=Le%20Chen%20and%20Bin%20Lei%20and%20Dunzhi%20Zhou%20and%20Pei-Hung%20Lin%20and%20Chunhua%20Liao%20and%20Caiwen%20Ding%20and%20Ali%20Jannesari&entry.1292438233=%20%20Migrating%20Fortran%20code%20to%20C%2B%2B%20is%20a%20common%20task%20for%20many%20scientific%20computing%0Ateams%2C%20driven%20by%20the%20need%20to%20leverage%20modern%20programming%20paradigms%2C%20enhance%0Across-platform%20compatibility%2C%20and%20improve%20maintainability.%20Automating%20this%0Atranslation%20process%20using%20large%20language%20models%20%28LLMs%29%20has%20shown%20promise%2C%20but%0Athe%20lack%20of%20high-quality%2C%20specialized%20datasets%20has%20hindered%20their%0Aeffectiveness.%20In%20this%20paper%2C%20we%20address%20this%20challenge%20by%20introducing%20a%20novel%0Amulti-turn%20dialogue%20dataset%2C%20Fortran2CPP%2C%20specifically%20designed%20for%0AFortran-to-C%2B%2B%20code%20migration.%20Our%20dataset%2C%20significantly%20larger%20than%20existing%0Aalternatives%2C%20is%20generated%20using%20a%20unique%20LLM-driven%2C%20dual-agent%20pipeline%0Aincorporating%20iterative%20compilation%2C%20execution%2C%20and%20code%20repair%20to%20ensure%20high%0Aquality%20and%20functional%20correctness.%20To%20demonstrate%20the%20effectiveness%20of%20our%0Adataset%2C%20we%20fine-tuned%20several%20open-weight%20LLMs%20on%20Fortran2CPP%20and%20evaluated%0Atheir%20performance%20on%20two%20independent%20benchmarks.%20Fine-tuning%20on%20our%20dataset%20led%0Ato%20remarkable%20gains%2C%20with%20models%20achieving%20up%20to%20a%203.31x%20increase%20in%20CodeBLEU%0Ascore%20and%20a%2092%5C%25%20improvement%20in%20compilation%20success%20rate.%20This%20highlights%20the%0Adataset%27s%20ability%20to%20enhance%20both%20the%20syntactic%20accuracy%20and%20compilability%20of%0Athe%20translated%20C%2B%2B%20code.%20Our%20dataset%20and%20model%20have%20been%20open-sourced%20and%20are%0Aavailable%20on%20our%20public%20GitHub%0Arepository%5Cfootnote%7B%5Curl%7Bhttps%3A//github.com/HPC-Fortran2CPP/Fortran2Cpp%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19770v1&entry.124074799=Read"},
{"title": "Markov Process-Based Graph Convolutional Networks for Entity\n  Classification in Knowledge Graphs", "author": "Johannes M\u00e4kelburg and Yiwen Peng and Mehwish Alam and Tobias Weller and Maribel Acosta", "abstract": "  Despite the vast amount of information encoded in Knowledge Graphs (KGs),\ninformation about the class affiliation of entities remains often incomplete.\nGraph Convolutional Networks (GCNs) have been shown to be effective predictors\nof complete information about the class affiliation of entities in KGs.\nHowever, these models do not learn the class affiliation of entities in KGs\nincorporating the complexity of the task, which negatively affects the models\nprediction capabilities. To address this problem, we introduce a Markov\nprocess-based architecture into well-known GCN architectures. This end-to-end\nnetwork learns the prediction of class affiliation of entities in KGs within a\nMarkov process. The number of computational steps is learned during training\nusing a geometric distribution. At the same time, the loss function combines\ninsights from the field of evidential learning. The experiments show a\nperformance improvement over existing models in several studied architectures\nand datasets. Based on the chosen hyperparameters for the geometric\ndistribution, the expected number of computation steps can be adjusted to\nimprove efficiency and accuracy during training.\n", "link": "http://arxiv.org/abs/2412.17438v2", "date": "2024-12-27", "relevancy": 1.8751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4919}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4674}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Markov%20Process-Based%20Graph%20Convolutional%20Networks%20for%20Entity%0A%20%20Classification%20in%20Knowledge%20Graphs&body=Title%3A%20Markov%20Process-Based%20Graph%20Convolutional%20Networks%20for%20Entity%0A%20%20Classification%20in%20Knowledge%20Graphs%0AAuthor%3A%20Johannes%20M%C3%A4kelburg%20and%20Yiwen%20Peng%20and%20Mehwish%20Alam%20and%20Tobias%20Weller%20and%20Maribel%20Acosta%0AAbstract%3A%20%20%20Despite%20the%20vast%20amount%20of%20information%20encoded%20in%20Knowledge%20Graphs%20%28KGs%29%2C%0Ainformation%20about%20the%20class%20affiliation%20of%20entities%20remains%20often%20incomplete.%0AGraph%20Convolutional%20Networks%20%28GCNs%29%20have%20been%20shown%20to%20be%20effective%20predictors%0Aof%20complete%20information%20about%20the%20class%20affiliation%20of%20entities%20in%20KGs.%0AHowever%2C%20these%20models%20do%20not%20learn%20the%20class%20affiliation%20of%20entities%20in%20KGs%0Aincorporating%20the%20complexity%20of%20the%20task%2C%20which%20negatively%20affects%20the%20models%0Aprediction%20capabilities.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20Markov%0Aprocess-based%20architecture%20into%20well-known%20GCN%20architectures.%20This%20end-to-end%0Anetwork%20learns%20the%20prediction%20of%20class%20affiliation%20of%20entities%20in%20KGs%20within%20a%0AMarkov%20process.%20The%20number%20of%20computational%20steps%20is%20learned%20during%20training%0Ausing%20a%20geometric%20distribution.%20At%20the%20same%20time%2C%20the%20loss%20function%20combines%0Ainsights%20from%20the%20field%20of%20evidential%20learning.%20The%20experiments%20show%20a%0Aperformance%20improvement%20over%20existing%20models%20in%20several%20studied%20architectures%0Aand%20datasets.%20Based%20on%20the%20chosen%20hyperparameters%20for%20the%20geometric%0Adistribution%2C%20the%20expected%20number%20of%20computation%20steps%20can%20be%20adjusted%20to%0Aimprove%20efficiency%20and%20accuracy%20during%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMarkov%2520Process-Based%2520Graph%2520Convolutional%2520Networks%2520for%2520Entity%250A%2520%2520Classification%2520in%2520Knowledge%2520Graphs%26entry.906535625%3DJohannes%2520M%25C3%25A4kelburg%2520and%2520Yiwen%2520Peng%2520and%2520Mehwish%2520Alam%2520and%2520Tobias%2520Weller%2520and%2520Maribel%2520Acosta%26entry.1292438233%3D%2520%2520Despite%2520the%2520vast%2520amount%2520of%2520information%2520encoded%2520in%2520Knowledge%2520Graphs%2520%2528KGs%2529%252C%250Ainformation%2520about%2520the%2520class%2520affiliation%2520of%2520entities%2520remains%2520often%2520incomplete.%250AGraph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520have%2520been%2520shown%2520to%2520be%2520effective%2520predictors%250Aof%2520complete%2520information%2520about%2520the%2520class%2520affiliation%2520of%2520entities%2520in%2520KGs.%250AHowever%252C%2520these%2520models%2520do%2520not%2520learn%2520the%2520class%2520affiliation%2520of%2520entities%2520in%2520KGs%250Aincorporating%2520the%2520complexity%2520of%2520the%2520task%252C%2520which%2520negatively%2520affects%2520the%2520models%250Aprediction%2520capabilities.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%2520Markov%250Aprocess-based%2520architecture%2520into%2520well-known%2520GCN%2520architectures.%2520This%2520end-to-end%250Anetwork%2520learns%2520the%2520prediction%2520of%2520class%2520affiliation%2520of%2520entities%2520in%2520KGs%2520within%2520a%250AMarkov%2520process.%2520The%2520number%2520of%2520computational%2520steps%2520is%2520learned%2520during%2520training%250Ausing%2520a%2520geometric%2520distribution.%2520At%2520the%2520same%2520time%252C%2520the%2520loss%2520function%2520combines%250Ainsights%2520from%2520the%2520field%2520of%2520evidential%2520learning.%2520The%2520experiments%2520show%2520a%250Aperformance%2520improvement%2520over%2520existing%2520models%2520in%2520several%2520studied%2520architectures%250Aand%2520datasets.%2520Based%2520on%2520the%2520chosen%2520hyperparameters%2520for%2520the%2520geometric%250Adistribution%252C%2520the%2520expected%2520number%2520of%2520computation%2520steps%2520can%2520be%2520adjusted%2520to%250Aimprove%2520efficiency%2520and%2520accuracy%2520during%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Markov%20Process-Based%20Graph%20Convolutional%20Networks%20for%20Entity%0A%20%20Classification%20in%20Knowledge%20Graphs&entry.906535625=Johannes%20M%C3%A4kelburg%20and%20Yiwen%20Peng%20and%20Mehwish%20Alam%20and%20Tobias%20Weller%20and%20Maribel%20Acosta&entry.1292438233=%20%20Despite%20the%20vast%20amount%20of%20information%20encoded%20in%20Knowledge%20Graphs%20%28KGs%29%2C%0Ainformation%20about%20the%20class%20affiliation%20of%20entities%20remains%20often%20incomplete.%0AGraph%20Convolutional%20Networks%20%28GCNs%29%20have%20been%20shown%20to%20be%20effective%20predictors%0Aof%20complete%20information%20about%20the%20class%20affiliation%20of%20entities%20in%20KGs.%0AHowever%2C%20these%20models%20do%20not%20learn%20the%20class%20affiliation%20of%20entities%20in%20KGs%0Aincorporating%20the%20complexity%20of%20the%20task%2C%20which%20negatively%20affects%20the%20models%0Aprediction%20capabilities.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20Markov%0Aprocess-based%20architecture%20into%20well-known%20GCN%20architectures.%20This%20end-to-end%0Anetwork%20learns%20the%20prediction%20of%20class%20affiliation%20of%20entities%20in%20KGs%20within%20a%0AMarkov%20process.%20The%20number%20of%20computational%20steps%20is%20learned%20during%20training%0Ausing%20a%20geometric%20distribution.%20At%20the%20same%20time%2C%20the%20loss%20function%20combines%0Ainsights%20from%20the%20field%20of%20evidential%20learning.%20The%20experiments%20show%20a%0Aperformance%20improvement%20over%20existing%20models%20in%20several%20studied%20architectures%0Aand%20datasets.%20Based%20on%20the%20chosen%20hyperparameters%20for%20the%20geometric%0Adistribution%2C%20the%20expected%20number%20of%20computation%20steps%20can%20be%20adjusted%20to%0Aimprove%20efficiency%20and%20accuracy%20during%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17438v2&entry.124074799=Read"},
{"title": "Deep Linear Hawkes Processes", "author": "Yuxin Chang and Alex Boyd and Cao Xiao and Taha Kass-Hout and Parminder Bhatia and Padhraic Smyth and Andrew Warrington", "abstract": "  Marked temporal point processes (MTPPs) are used to model sequences of\ndifferent types of events with irregular arrival times, with broad applications\nranging from healthcare and social networks to finance. We address shortcomings\nin existing point process models by drawing connections between modern deep\nstate-space models (SSMs) and linear Hawkes processes (LHPs), culminating in an\nMTPP that we call the deep linear Hawkes process (DLHP). The DLHP modifies the\nlinear differential equations in deep SSMs to be stochastic jump differential\nequations, akin to LHPs. After discretizing, the resulting recurrence can be\nimplemented efficiently using a parallel scan. This brings parallelism and\nlinear scaling to MTPP models. This contrasts with attention-based MTPPs, which\nscale quadratically, and RNN-based MTPPs, which do not parallelize across the\nsequence length. We show empirically that DLHPs match or outperform existing\nmodels across a broad range of metrics on eight real-world datasets. Our\nproposed DLHP model is the first instance of the unique architectural\ncapabilities of SSMs being leveraged to construct a new class of MTPP models.\n", "link": "http://arxiv.org/abs/2412.19634v1", "date": "2024-12-27", "relevancy": 1.875, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5106}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4649}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Linear%20Hawkes%20Processes&body=Title%3A%20Deep%20Linear%20Hawkes%20Processes%0AAuthor%3A%20Yuxin%20Chang%20and%20Alex%20Boyd%20and%20Cao%20Xiao%20and%20Taha%20Kass-Hout%20and%20Parminder%20Bhatia%20and%20Padhraic%20Smyth%20and%20Andrew%20Warrington%0AAbstract%3A%20%20%20Marked%20temporal%20point%20processes%20%28MTPPs%29%20are%20used%20to%20model%20sequences%20of%0Adifferent%20types%20of%20events%20with%20irregular%20arrival%20times%2C%20with%20broad%20applications%0Aranging%20from%20healthcare%20and%20social%20networks%20to%20finance.%20We%20address%20shortcomings%0Ain%20existing%20point%20process%20models%20by%20drawing%20connections%20between%20modern%20deep%0Astate-space%20models%20%28SSMs%29%20and%20linear%20Hawkes%20processes%20%28LHPs%29%2C%20culminating%20in%20an%0AMTPP%20that%20we%20call%20the%20deep%20linear%20Hawkes%20process%20%28DLHP%29.%20The%20DLHP%20modifies%20the%0Alinear%20differential%20equations%20in%20deep%20SSMs%20to%20be%20stochastic%20jump%20differential%0Aequations%2C%20akin%20to%20LHPs.%20After%20discretizing%2C%20the%20resulting%20recurrence%20can%20be%0Aimplemented%20efficiently%20using%20a%20parallel%20scan.%20This%20brings%20parallelism%20and%0Alinear%20scaling%20to%20MTPP%20models.%20This%20contrasts%20with%20attention-based%20MTPPs%2C%20which%0Ascale%20quadratically%2C%20and%20RNN-based%20MTPPs%2C%20which%20do%20not%20parallelize%20across%20the%0Asequence%20length.%20We%20show%20empirically%20that%20DLHPs%20match%20or%20outperform%20existing%0Amodels%20across%20a%20broad%20range%20of%20metrics%20on%20eight%20real-world%20datasets.%20Our%0Aproposed%20DLHP%20model%20is%20the%20first%20instance%20of%20the%20unique%20architectural%0Acapabilities%20of%20SSMs%20being%20leveraged%20to%20construct%20a%20new%20class%20of%20MTPP%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Linear%2520Hawkes%2520Processes%26entry.906535625%3DYuxin%2520Chang%2520and%2520Alex%2520Boyd%2520and%2520Cao%2520Xiao%2520and%2520Taha%2520Kass-Hout%2520and%2520Parminder%2520Bhatia%2520and%2520Padhraic%2520Smyth%2520and%2520Andrew%2520Warrington%26entry.1292438233%3D%2520%2520Marked%2520temporal%2520point%2520processes%2520%2528MTPPs%2529%2520are%2520used%2520to%2520model%2520sequences%2520of%250Adifferent%2520types%2520of%2520events%2520with%2520irregular%2520arrival%2520times%252C%2520with%2520broad%2520applications%250Aranging%2520from%2520healthcare%2520and%2520social%2520networks%2520to%2520finance.%2520We%2520address%2520shortcomings%250Ain%2520existing%2520point%2520process%2520models%2520by%2520drawing%2520connections%2520between%2520modern%2520deep%250Astate-space%2520models%2520%2528SSMs%2529%2520and%2520linear%2520Hawkes%2520processes%2520%2528LHPs%2529%252C%2520culminating%2520in%2520an%250AMTPP%2520that%2520we%2520call%2520the%2520deep%2520linear%2520Hawkes%2520process%2520%2528DLHP%2529.%2520The%2520DLHP%2520modifies%2520the%250Alinear%2520differential%2520equations%2520in%2520deep%2520SSMs%2520to%2520be%2520stochastic%2520jump%2520differential%250Aequations%252C%2520akin%2520to%2520LHPs.%2520After%2520discretizing%252C%2520the%2520resulting%2520recurrence%2520can%2520be%250Aimplemented%2520efficiently%2520using%2520a%2520parallel%2520scan.%2520This%2520brings%2520parallelism%2520and%250Alinear%2520scaling%2520to%2520MTPP%2520models.%2520This%2520contrasts%2520with%2520attention-based%2520MTPPs%252C%2520which%250Ascale%2520quadratically%252C%2520and%2520RNN-based%2520MTPPs%252C%2520which%2520do%2520not%2520parallelize%2520across%2520the%250Asequence%2520length.%2520We%2520show%2520empirically%2520that%2520DLHPs%2520match%2520or%2520outperform%2520existing%250Amodels%2520across%2520a%2520broad%2520range%2520of%2520metrics%2520on%2520eight%2520real-world%2520datasets.%2520Our%250Aproposed%2520DLHP%2520model%2520is%2520the%2520first%2520instance%2520of%2520the%2520unique%2520architectural%250Acapabilities%2520of%2520SSMs%2520being%2520leveraged%2520to%2520construct%2520a%2520new%2520class%2520of%2520MTPP%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Linear%20Hawkes%20Processes&entry.906535625=Yuxin%20Chang%20and%20Alex%20Boyd%20and%20Cao%20Xiao%20and%20Taha%20Kass-Hout%20and%20Parminder%20Bhatia%20and%20Padhraic%20Smyth%20and%20Andrew%20Warrington&entry.1292438233=%20%20Marked%20temporal%20point%20processes%20%28MTPPs%29%20are%20used%20to%20model%20sequences%20of%0Adifferent%20types%20of%20events%20with%20irregular%20arrival%20times%2C%20with%20broad%20applications%0Aranging%20from%20healthcare%20and%20social%20networks%20to%20finance.%20We%20address%20shortcomings%0Ain%20existing%20point%20process%20models%20by%20drawing%20connections%20between%20modern%20deep%0Astate-space%20models%20%28SSMs%29%20and%20linear%20Hawkes%20processes%20%28LHPs%29%2C%20culminating%20in%20an%0AMTPP%20that%20we%20call%20the%20deep%20linear%20Hawkes%20process%20%28DLHP%29.%20The%20DLHP%20modifies%20the%0Alinear%20differential%20equations%20in%20deep%20SSMs%20to%20be%20stochastic%20jump%20differential%0Aequations%2C%20akin%20to%20LHPs.%20After%20discretizing%2C%20the%20resulting%20recurrence%20can%20be%0Aimplemented%20efficiently%20using%20a%20parallel%20scan.%20This%20brings%20parallelism%20and%0Alinear%20scaling%20to%20MTPP%20models.%20This%20contrasts%20with%20attention-based%20MTPPs%2C%20which%0Ascale%20quadratically%2C%20and%20RNN-based%20MTPPs%2C%20which%20do%20not%20parallelize%20across%20the%0Asequence%20length.%20We%20show%20empirically%20that%20DLHPs%20match%20or%20outperform%20existing%0Amodels%20across%20a%20broad%20range%20of%20metrics%20on%20eight%20real-world%20datasets.%20Our%0Aproposed%20DLHP%20model%20is%20the%20first%20instance%20of%20the%20unique%20architectural%0Acapabilities%20of%20SSMs%20being%20leveraged%20to%20construct%20a%20new%20class%20of%20MTPP%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19634v1&entry.124074799=Read"},
{"title": "Learning to Forget: Bayesian Time Series Forecasting using Recurrent\n  Sparse Spectrum Signature Gaussian Processes", "author": "Csaba T\u00f3th and Masaki Adachi and Michael A. Osborne and Harald Oberhauser", "abstract": "  The signature kernel is a kernel between time series of arbitrary length and\ncomes with strong theoretical guarantees from stochastic analysis. It has found\napplications in machine learning such as covariance functions for Gaussian\nprocesses. A strength of the underlying signature features is that they provide\na structured global description of a time series. However, this property can\nquickly become a curse when local information is essential and forgetting is\nrequired; so far this has only been addressed with ad-hoc methods such as\nslicing the time series into subsegments. To overcome this, we propose a\nprincipled, data-driven approach by introducing a novel forgetting mechanism\nfor signatures. This allows the model to dynamically adapt its context length\nto focus on more recent information. To achieve this, we revisit the recently\nintroduced Random Fourier Signature Features, and develop Random Fourier\nDecayed Signature Features (RFDSF) with Gaussian processes (GPs). This results\nin a Bayesian time series forecasting algorithm with variational inference,\nthat offers a scalable probabilistic algorithm that processes and transforms a\ntime series into a joint predictive distribution over time steps in one pass\nusing recurrence. For example, processing a sequence of length $10^4$ steps in\n$\\approx 10^{-2}$ seconds and in $< 1\\text{GB}$ of GPU memory. We demonstrate\nthat it outperforms other GP-based alternatives and competes with\nstate-of-the-art probabilistic time series forecasting algorithms.\n", "link": "http://arxiv.org/abs/2412.19727v1", "date": "2024-12-27", "relevancy": 1.8728, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4739}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4645}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Forget%3A%20Bayesian%20Time%20Series%20Forecasting%20using%20Recurrent%0A%20%20Sparse%20Spectrum%20Signature%20Gaussian%20Processes&body=Title%3A%20Learning%20to%20Forget%3A%20Bayesian%20Time%20Series%20Forecasting%20using%20Recurrent%0A%20%20Sparse%20Spectrum%20Signature%20Gaussian%20Processes%0AAuthor%3A%20Csaba%20T%C3%B3th%20and%20Masaki%20Adachi%20and%20Michael%20A.%20Osborne%20and%20Harald%20Oberhauser%0AAbstract%3A%20%20%20The%20signature%20kernel%20is%20a%20kernel%20between%20time%20series%20of%20arbitrary%20length%20and%0Acomes%20with%20strong%20theoretical%20guarantees%20from%20stochastic%20analysis.%20It%20has%20found%0Aapplications%20in%20machine%20learning%20such%20as%20covariance%20functions%20for%20Gaussian%0Aprocesses.%20A%20strength%20of%20the%20underlying%20signature%20features%20is%20that%20they%20provide%0Aa%20structured%20global%20description%20of%20a%20time%20series.%20However%2C%20this%20property%20can%0Aquickly%20become%20a%20curse%20when%20local%20information%20is%20essential%20and%20forgetting%20is%0Arequired%3B%20so%20far%20this%20has%20only%20been%20addressed%20with%20ad-hoc%20methods%20such%20as%0Aslicing%20the%20time%20series%20into%20subsegments.%20To%20overcome%20this%2C%20we%20propose%20a%0Aprincipled%2C%20data-driven%20approach%20by%20introducing%20a%20novel%20forgetting%20mechanism%0Afor%20signatures.%20This%20allows%20the%20model%20to%20dynamically%20adapt%20its%20context%20length%0Ato%20focus%20on%20more%20recent%20information.%20To%20achieve%20this%2C%20we%20revisit%20the%20recently%0Aintroduced%20Random%20Fourier%20Signature%20Features%2C%20and%20develop%20Random%20Fourier%0ADecayed%20Signature%20Features%20%28RFDSF%29%20with%20Gaussian%20processes%20%28GPs%29.%20This%20results%0Ain%20a%20Bayesian%20time%20series%20forecasting%20algorithm%20with%20variational%20inference%2C%0Athat%20offers%20a%20scalable%20probabilistic%20algorithm%20that%20processes%20and%20transforms%20a%0Atime%20series%20into%20a%20joint%20predictive%20distribution%20over%20time%20steps%20in%20one%20pass%0Ausing%20recurrence.%20For%20example%2C%20processing%20a%20sequence%20of%20length%20%2410%5E4%24%20steps%20in%0A%24%5Capprox%2010%5E%7B-2%7D%24%20seconds%20and%20in%20%24%3C%201%5Ctext%7BGB%7D%24%20of%20GPU%20memory.%20We%20demonstrate%0Athat%20it%20outperforms%20other%20GP-based%20alternatives%20and%20competes%20with%0Astate-of-the-art%20probabilistic%20time%20series%20forecasting%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Forget%253A%2520Bayesian%2520Time%2520Series%2520Forecasting%2520using%2520Recurrent%250A%2520%2520Sparse%2520Spectrum%2520Signature%2520Gaussian%2520Processes%26entry.906535625%3DCsaba%2520T%25C3%25B3th%2520and%2520Masaki%2520Adachi%2520and%2520Michael%2520A.%2520Osborne%2520and%2520Harald%2520Oberhauser%26entry.1292438233%3D%2520%2520The%2520signature%2520kernel%2520is%2520a%2520kernel%2520between%2520time%2520series%2520of%2520arbitrary%2520length%2520and%250Acomes%2520with%2520strong%2520theoretical%2520guarantees%2520from%2520stochastic%2520analysis.%2520It%2520has%2520found%250Aapplications%2520in%2520machine%2520learning%2520such%2520as%2520covariance%2520functions%2520for%2520Gaussian%250Aprocesses.%2520A%2520strength%2520of%2520the%2520underlying%2520signature%2520features%2520is%2520that%2520they%2520provide%250Aa%2520structured%2520global%2520description%2520of%2520a%2520time%2520series.%2520However%252C%2520this%2520property%2520can%250Aquickly%2520become%2520a%2520curse%2520when%2520local%2520information%2520is%2520essential%2520and%2520forgetting%2520is%250Arequired%253B%2520so%2520far%2520this%2520has%2520only%2520been%2520addressed%2520with%2520ad-hoc%2520methods%2520such%2520as%250Aslicing%2520the%2520time%2520series%2520into%2520subsegments.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520a%250Aprincipled%252C%2520data-driven%2520approach%2520by%2520introducing%2520a%2520novel%2520forgetting%2520mechanism%250Afor%2520signatures.%2520This%2520allows%2520the%2520model%2520to%2520dynamically%2520adapt%2520its%2520context%2520length%250Ato%2520focus%2520on%2520more%2520recent%2520information.%2520To%2520achieve%2520this%252C%2520we%2520revisit%2520the%2520recently%250Aintroduced%2520Random%2520Fourier%2520Signature%2520Features%252C%2520and%2520develop%2520Random%2520Fourier%250ADecayed%2520Signature%2520Features%2520%2528RFDSF%2529%2520with%2520Gaussian%2520processes%2520%2528GPs%2529.%2520This%2520results%250Ain%2520a%2520Bayesian%2520time%2520series%2520forecasting%2520algorithm%2520with%2520variational%2520inference%252C%250Athat%2520offers%2520a%2520scalable%2520probabilistic%2520algorithm%2520that%2520processes%2520and%2520transforms%2520a%250Atime%2520series%2520into%2520a%2520joint%2520predictive%2520distribution%2520over%2520time%2520steps%2520in%2520one%2520pass%250Ausing%2520recurrence.%2520For%2520example%252C%2520processing%2520a%2520sequence%2520of%2520length%2520%252410%255E4%2524%2520steps%2520in%250A%2524%255Capprox%252010%255E%257B-2%257D%2524%2520seconds%2520and%2520in%2520%2524%253C%25201%255Ctext%257BGB%257D%2524%2520of%2520GPU%2520memory.%2520We%2520demonstrate%250Athat%2520it%2520outperforms%2520other%2520GP-based%2520alternatives%2520and%2520competes%2520with%250Astate-of-the-art%2520probabilistic%2520time%2520series%2520forecasting%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Forget%3A%20Bayesian%20Time%20Series%20Forecasting%20using%20Recurrent%0A%20%20Sparse%20Spectrum%20Signature%20Gaussian%20Processes&entry.906535625=Csaba%20T%C3%B3th%20and%20Masaki%20Adachi%20and%20Michael%20A.%20Osborne%20and%20Harald%20Oberhauser&entry.1292438233=%20%20The%20signature%20kernel%20is%20a%20kernel%20between%20time%20series%20of%20arbitrary%20length%20and%0Acomes%20with%20strong%20theoretical%20guarantees%20from%20stochastic%20analysis.%20It%20has%20found%0Aapplications%20in%20machine%20learning%20such%20as%20covariance%20functions%20for%20Gaussian%0Aprocesses.%20A%20strength%20of%20the%20underlying%20signature%20features%20is%20that%20they%20provide%0Aa%20structured%20global%20description%20of%20a%20time%20series.%20However%2C%20this%20property%20can%0Aquickly%20become%20a%20curse%20when%20local%20information%20is%20essential%20and%20forgetting%20is%0Arequired%3B%20so%20far%20this%20has%20only%20been%20addressed%20with%20ad-hoc%20methods%20such%20as%0Aslicing%20the%20time%20series%20into%20subsegments.%20To%20overcome%20this%2C%20we%20propose%20a%0Aprincipled%2C%20data-driven%20approach%20by%20introducing%20a%20novel%20forgetting%20mechanism%0Afor%20signatures.%20This%20allows%20the%20model%20to%20dynamically%20adapt%20its%20context%20length%0Ato%20focus%20on%20more%20recent%20information.%20To%20achieve%20this%2C%20we%20revisit%20the%20recently%0Aintroduced%20Random%20Fourier%20Signature%20Features%2C%20and%20develop%20Random%20Fourier%0ADecayed%20Signature%20Features%20%28RFDSF%29%20with%20Gaussian%20processes%20%28GPs%29.%20This%20results%0Ain%20a%20Bayesian%20time%20series%20forecasting%20algorithm%20with%20variational%20inference%2C%0Athat%20offers%20a%20scalable%20probabilistic%20algorithm%20that%20processes%20and%20transforms%20a%0Atime%20series%20into%20a%20joint%20predictive%20distribution%20over%20time%20steps%20in%20one%20pass%0Ausing%20recurrence.%20For%20example%2C%20processing%20a%20sequence%20of%20length%20%2410%5E4%24%20steps%20in%0A%24%5Capprox%2010%5E%7B-2%7D%24%20seconds%20and%20in%20%24%3C%201%5Ctext%7BGB%7D%24%20of%20GPU%20memory.%20We%20demonstrate%0Athat%20it%20outperforms%20other%20GP-based%20alternatives%20and%20competes%20with%0Astate-of-the-art%20probabilistic%20time%20series%20forecasting%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19727v1&entry.124074799=Read"},
{"title": "LASER: A new method for locally adaptive nonparametric regression", "author": "Sabyasachi Chatterjee and Subhajit Goswami and Soumendu Sundar Mukherjee", "abstract": "  In this article, we introduce \\textsf{LASER} (Locally Adaptive Smoothing\nEstimator for Regression), a computationally efficient locally adaptive\nnonparametric regression method that performs variable bandwidth local\npolynomial regression. We prove that it adapts (near-)optimally to the local\nH\\\"{o}lder exponent of the underlying regression function\n\\texttt{simultaneously} at all points in its domain. Furthermore, we show that\nthere is a single ideal choice of a global tuning parameter under which the\nabove mentioned local adaptivity holds. Despite the vast literature on\nnonparametric regression, instances of practicable methods with provable\nguarantees of such a strong notion of local adaptivity are rare. The proposed\nmethod achieves excellent performance across a broad range of numerical\nexperiments in comparison to popular alternative locally adaptive methods.\n", "link": "http://arxiv.org/abs/2412.19802v1", "date": "2024-12-27", "relevancy": 1.8529, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4757}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4566}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LASER%3A%20A%20new%20method%20for%20locally%20adaptive%20nonparametric%20regression&body=Title%3A%20LASER%3A%20A%20new%20method%20for%20locally%20adaptive%20nonparametric%20regression%0AAuthor%3A%20Sabyasachi%20Chatterjee%20and%20Subhajit%20Goswami%20and%20Soumendu%20Sundar%20Mukherjee%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20introduce%20%5Ctextsf%7BLASER%7D%20%28Locally%20Adaptive%20Smoothing%0AEstimator%20for%20Regression%29%2C%20a%20computationally%20efficient%20locally%20adaptive%0Anonparametric%20regression%20method%20that%20performs%20variable%20bandwidth%20local%0Apolynomial%20regression.%20We%20prove%20that%20it%20adapts%20%28near-%29optimally%20to%20the%20local%0AH%5C%22%7Bo%7Dlder%20exponent%20of%20the%20underlying%20regression%20function%0A%5Ctexttt%7Bsimultaneously%7D%20at%20all%20points%20in%20its%20domain.%20Furthermore%2C%20we%20show%20that%0Athere%20is%20a%20single%20ideal%20choice%20of%20a%20global%20tuning%20parameter%20under%20which%20the%0Aabove%20mentioned%20local%20adaptivity%20holds.%20Despite%20the%20vast%20literature%20on%0Anonparametric%20regression%2C%20instances%20of%20practicable%20methods%20with%20provable%0Aguarantees%20of%20such%20a%20strong%20notion%20of%20local%20adaptivity%20are%20rare.%20The%20proposed%0Amethod%20achieves%20excellent%20performance%20across%20a%20broad%20range%20of%20numerical%0Aexperiments%20in%20comparison%20to%20popular%20alternative%20locally%20adaptive%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLASER%253A%2520A%2520new%2520method%2520for%2520locally%2520adaptive%2520nonparametric%2520regression%26entry.906535625%3DSabyasachi%2520Chatterjee%2520and%2520Subhajit%2520Goswami%2520and%2520Soumendu%2520Sundar%2520Mukherjee%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520introduce%2520%255Ctextsf%257BLASER%257D%2520%2528Locally%2520Adaptive%2520Smoothing%250AEstimator%2520for%2520Regression%2529%252C%2520a%2520computationally%2520efficient%2520locally%2520adaptive%250Anonparametric%2520regression%2520method%2520that%2520performs%2520variable%2520bandwidth%2520local%250Apolynomial%2520regression.%2520We%2520prove%2520that%2520it%2520adapts%2520%2528near-%2529optimally%2520to%2520the%2520local%250AH%255C%2522%257Bo%257Dlder%2520exponent%2520of%2520the%2520underlying%2520regression%2520function%250A%255Ctexttt%257Bsimultaneously%257D%2520at%2520all%2520points%2520in%2520its%2520domain.%2520Furthermore%252C%2520we%2520show%2520that%250Athere%2520is%2520a%2520single%2520ideal%2520choice%2520of%2520a%2520global%2520tuning%2520parameter%2520under%2520which%2520the%250Aabove%2520mentioned%2520local%2520adaptivity%2520holds.%2520Despite%2520the%2520vast%2520literature%2520on%250Anonparametric%2520regression%252C%2520instances%2520of%2520practicable%2520methods%2520with%2520provable%250Aguarantees%2520of%2520such%2520a%2520strong%2520notion%2520of%2520local%2520adaptivity%2520are%2520rare.%2520The%2520proposed%250Amethod%2520achieves%2520excellent%2520performance%2520across%2520a%2520broad%2520range%2520of%2520numerical%250Aexperiments%2520in%2520comparison%2520to%2520popular%2520alternative%2520locally%2520adaptive%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LASER%3A%20A%20new%20method%20for%20locally%20adaptive%20nonparametric%20regression&entry.906535625=Sabyasachi%20Chatterjee%20and%20Subhajit%20Goswami%20and%20Soumendu%20Sundar%20Mukherjee&entry.1292438233=%20%20In%20this%20article%2C%20we%20introduce%20%5Ctextsf%7BLASER%7D%20%28Locally%20Adaptive%20Smoothing%0AEstimator%20for%20Regression%29%2C%20a%20computationally%20efficient%20locally%20adaptive%0Anonparametric%20regression%20method%20that%20performs%20variable%20bandwidth%20local%0Apolynomial%20regression.%20We%20prove%20that%20it%20adapts%20%28near-%29optimally%20to%20the%20local%0AH%5C%22%7Bo%7Dlder%20exponent%20of%20the%20underlying%20regression%20function%0A%5Ctexttt%7Bsimultaneously%7D%20at%20all%20points%20in%20its%20domain.%20Furthermore%2C%20we%20show%20that%0Athere%20is%20a%20single%20ideal%20choice%20of%20a%20global%20tuning%20parameter%20under%20which%20the%0Aabove%20mentioned%20local%20adaptivity%20holds.%20Despite%20the%20vast%20literature%20on%0Anonparametric%20regression%2C%20instances%20of%20practicable%20methods%20with%20provable%0Aguarantees%20of%20such%20a%20strong%20notion%20of%20local%20adaptivity%20are%20rare.%20The%20proposed%0Amethod%20achieves%20excellent%20performance%20across%20a%20broad%20range%20of%20numerical%0Aexperiments%20in%20comparison%20to%20popular%20alternative%20locally%20adaptive%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19802v1&entry.124074799=Read"},
{"title": "DAG-aware Transformer for Causal Effect Estimation", "author": "Manqing Liu and David R. Bellamy and Andrew L. Beam", "abstract": "  Causal inference is a critical task across fields such as healthcare,\neconomics, and the social sciences. While recent advances in machine learning,\nespecially those based on the deep-learning architectures, have shown potential\nin estimating causal effects, existing approaches often fall short in handling\ncomplex causal structures and lack adaptability across various causal\nscenarios. In this paper, we present a novel transformer-based method for\ncausal inference that overcomes these challenges. The core innovation of our\nmodel lies in its integration of causal Directed Acyclic Graphs (DAGs) directly\ninto the attention mechanism, enabling it to accurately model the underlying\ncausal structure. This allows for flexible estimation of both average treatment\neffects (ATE) and conditional average treatment effects (CATE). Extensive\nexperiments on both synthetic and real-world datasets demonstrate that our\napproach surpasses existing methods in estimating causal effects across a wide\nrange of scenarios. The flexibility and robustness of our model make it a\nvaluable tool for researchers and practitioners tackling complex causal\ninference problems.\n", "link": "http://arxiv.org/abs/2410.10044v2", "date": "2024-12-27", "relevancy": 1.8065, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4896}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAG-aware%20Transformer%20for%20Causal%20Effect%20Estimation&body=Title%3A%20DAG-aware%20Transformer%20for%20Causal%20Effect%20Estimation%0AAuthor%3A%20Manqing%20Liu%20and%20David%20R.%20Bellamy%20and%20Andrew%20L.%20Beam%0AAbstract%3A%20%20%20Causal%20inference%20is%20a%20critical%20task%20across%20fields%20such%20as%20healthcare%2C%0Aeconomics%2C%20and%20the%20social%20sciences.%20While%20recent%20advances%20in%20machine%20learning%2C%0Aespecially%20those%20based%20on%20the%20deep-learning%20architectures%2C%20have%20shown%20potential%0Ain%20estimating%20causal%20effects%2C%20existing%20approaches%20often%20fall%20short%20in%20handling%0Acomplex%20causal%20structures%20and%20lack%20adaptability%20across%20various%20causal%0Ascenarios.%20In%20this%20paper%2C%20we%20present%20a%20novel%20transformer-based%20method%20for%0Acausal%20inference%20that%20overcomes%20these%20challenges.%20The%20core%20innovation%20of%20our%0Amodel%20lies%20in%20its%20integration%20of%20causal%20Directed%20Acyclic%20Graphs%20%28DAGs%29%20directly%0Ainto%20the%20attention%20mechanism%2C%20enabling%20it%20to%20accurately%20model%20the%20underlying%0Acausal%20structure.%20This%20allows%20for%20flexible%20estimation%20of%20both%20average%20treatment%0Aeffects%20%28ATE%29%20and%20conditional%20average%20treatment%20effects%20%28CATE%29.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%0Aapproach%20surpasses%20existing%20methods%20in%20estimating%20causal%20effects%20across%20a%20wide%0Arange%20of%20scenarios.%20The%20flexibility%20and%20robustness%20of%20our%20model%20make%20it%20a%0Avaluable%20tool%20for%20researchers%20and%20practitioners%20tackling%20complex%20causal%0Ainference%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10044v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAG-aware%2520Transformer%2520for%2520Causal%2520Effect%2520Estimation%26entry.906535625%3DManqing%2520Liu%2520and%2520David%2520R.%2520Bellamy%2520and%2520Andrew%2520L.%2520Beam%26entry.1292438233%3D%2520%2520Causal%2520inference%2520is%2520a%2520critical%2520task%2520across%2520fields%2520such%2520as%2520healthcare%252C%250Aeconomics%252C%2520and%2520the%2520social%2520sciences.%2520While%2520recent%2520advances%2520in%2520machine%2520learning%252C%250Aespecially%2520those%2520based%2520on%2520the%2520deep-learning%2520architectures%252C%2520have%2520shown%2520potential%250Ain%2520estimating%2520causal%2520effects%252C%2520existing%2520approaches%2520often%2520fall%2520short%2520in%2520handling%250Acomplex%2520causal%2520structures%2520and%2520lack%2520adaptability%2520across%2520various%2520causal%250Ascenarios.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520transformer-based%2520method%2520for%250Acausal%2520inference%2520that%2520overcomes%2520these%2520challenges.%2520The%2520core%2520innovation%2520of%2520our%250Amodel%2520lies%2520in%2520its%2520integration%2520of%2520causal%2520Directed%2520Acyclic%2520Graphs%2520%2528DAGs%2529%2520directly%250Ainto%2520the%2520attention%2520mechanism%252C%2520enabling%2520it%2520to%2520accurately%2520model%2520the%2520underlying%250Acausal%2520structure.%2520This%2520allows%2520for%2520flexible%2520estimation%2520of%2520both%2520average%2520treatment%250Aeffects%2520%2528ATE%2529%2520and%2520conditional%2520average%2520treatment%2520effects%2520%2528CATE%2529.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%250Aapproach%2520surpasses%2520existing%2520methods%2520in%2520estimating%2520causal%2520effects%2520across%2520a%2520wide%250Arange%2520of%2520scenarios.%2520The%2520flexibility%2520and%2520robustness%2520of%2520our%2520model%2520make%2520it%2520a%250Avaluable%2520tool%2520for%2520researchers%2520and%2520practitioners%2520tackling%2520complex%2520causal%250Ainference%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10044v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAG-aware%20Transformer%20for%20Causal%20Effect%20Estimation&entry.906535625=Manqing%20Liu%20and%20David%20R.%20Bellamy%20and%20Andrew%20L.%20Beam&entry.1292438233=%20%20Causal%20inference%20is%20a%20critical%20task%20across%20fields%20such%20as%20healthcare%2C%0Aeconomics%2C%20and%20the%20social%20sciences.%20While%20recent%20advances%20in%20machine%20learning%2C%0Aespecially%20those%20based%20on%20the%20deep-learning%20architectures%2C%20have%20shown%20potential%0Ain%20estimating%20causal%20effects%2C%20existing%20approaches%20often%20fall%20short%20in%20handling%0Acomplex%20causal%20structures%20and%20lack%20adaptability%20across%20various%20causal%0Ascenarios.%20In%20this%20paper%2C%20we%20present%20a%20novel%20transformer-based%20method%20for%0Acausal%20inference%20that%20overcomes%20these%20challenges.%20The%20core%20innovation%20of%20our%0Amodel%20lies%20in%20its%20integration%20of%20causal%20Directed%20Acyclic%20Graphs%20%28DAGs%29%20directly%0Ainto%20the%20attention%20mechanism%2C%20enabling%20it%20to%20accurately%20model%20the%20underlying%0Acausal%20structure.%20This%20allows%20for%20flexible%20estimation%20of%20both%20average%20treatment%0Aeffects%20%28ATE%29%20and%20conditional%20average%20treatment%20effects%20%28CATE%29.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%0Aapproach%20surpasses%20existing%20methods%20in%20estimating%20causal%20effects%20across%20a%20wide%0Arange%20of%20scenarios.%20The%20flexibility%20and%20robustness%20of%20our%20model%20make%20it%20a%0Avaluable%20tool%20for%20researchers%20and%20practitioners%20tackling%20complex%20causal%0Ainference%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10044v2&entry.124074799=Read"},
{"title": "Convergence analysis of wide shallow neural operators within the\n  framework of Neural Tangent Kernel", "author": "Xianliang Xu and Ye Li and Zhongyi Huang", "abstract": "  Neural operators are aiming at approximating operators mapping between Banach\nspaces of functions, achieving much success in the field of scientific\ncomputing. Compared to certain deep learning-based solvers, such as\nPhysics-Informed Neural Networks (PINNs), Deep Ritz Method (DRM), neural\noperators can solve a class of Partial Differential Equations (PDEs). Although\nmuch work has been done to analyze the approximation and generalization error\nof neural operators, there is still a lack of analysis on their training error.\nIn this work, we conduct the convergence analysis of gradient descent for the\nwide shallow neural operators within the framework of Neural Tangent Kernel\n(NTK). The core idea lies on the fact that over-parameterization and random\ninitialization together ensure that each weight vector remains near its\ninitialization throughout all iterations, yielding the linear convergence of\ngradient descent. In this work, we demonstrate that under the setting of\nover-parametrization, gradient descent can find the global minimum regardless\nof whether it is in continuous time or discrete time. Finally, we briefly\ndiscuss the case of physics-informed shallow neural operators.\n", "link": "http://arxiv.org/abs/2412.05545v2", "date": "2024-12-27", "relevancy": 1.7988, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4488}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20analysis%20of%20wide%20shallow%20neural%20operators%20within%20the%0A%20%20framework%20of%20Neural%20Tangent%20Kernel&body=Title%3A%20Convergence%20analysis%20of%20wide%20shallow%20neural%20operators%20within%20the%0A%20%20framework%20of%20Neural%20Tangent%20Kernel%0AAuthor%3A%20Xianliang%20Xu%20and%20Ye%20Li%20and%20Zhongyi%20Huang%0AAbstract%3A%20%20%20Neural%20operators%20are%20aiming%20at%20approximating%20operators%20mapping%20between%20Banach%0Aspaces%20of%20functions%2C%20achieving%20much%20success%20in%20the%20field%20of%20scientific%0Acomputing.%20Compared%20to%20certain%20deep%20learning-based%20solvers%2C%20such%20as%0APhysics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20Deep%20Ritz%20Method%20%28DRM%29%2C%20neural%0Aoperators%20can%20solve%20a%20class%20of%20Partial%20Differential%20Equations%20%28PDEs%29.%20Although%0Amuch%20work%20has%20been%20done%20to%20analyze%20the%20approximation%20and%20generalization%20error%0Aof%20neural%20operators%2C%20there%20is%20still%20a%20lack%20of%20analysis%20on%20their%20training%20error.%0AIn%20this%20work%2C%20we%20conduct%20the%20convergence%20analysis%20of%20gradient%20descent%20for%20the%0Awide%20shallow%20neural%20operators%20within%20the%20framework%20of%20Neural%20Tangent%20Kernel%0A%28NTK%29.%20The%20core%20idea%20lies%20on%20the%20fact%20that%20over-parameterization%20and%20random%0Ainitialization%20together%20ensure%20that%20each%20weight%20vector%20remains%20near%20its%0Ainitialization%20throughout%20all%20iterations%2C%20yielding%20the%20linear%20convergence%20of%0Agradient%20descent.%20In%20this%20work%2C%20we%20demonstrate%20that%20under%20the%20setting%20of%0Aover-parametrization%2C%20gradient%20descent%20can%20find%20the%20global%20minimum%20regardless%0Aof%20whether%20it%20is%20in%20continuous%20time%20or%20discrete%20time.%20Finally%2C%20we%20briefly%0Adiscuss%20the%20case%20of%20physics-informed%20shallow%20neural%20operators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520analysis%2520of%2520wide%2520shallow%2520neural%2520operators%2520within%2520the%250A%2520%2520framework%2520of%2520Neural%2520Tangent%2520Kernel%26entry.906535625%3DXianliang%2520Xu%2520and%2520Ye%2520Li%2520and%2520Zhongyi%2520Huang%26entry.1292438233%3D%2520%2520Neural%2520operators%2520are%2520aiming%2520at%2520approximating%2520operators%2520mapping%2520between%2520Banach%250Aspaces%2520of%2520functions%252C%2520achieving%2520much%2520success%2520in%2520the%2520field%2520of%2520scientific%250Acomputing.%2520Compared%2520to%2520certain%2520deep%2520learning-based%2520solvers%252C%2520such%2520as%250APhysics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%252C%2520Deep%2520Ritz%2520Method%2520%2528DRM%2529%252C%2520neural%250Aoperators%2520can%2520solve%2520a%2520class%2520of%2520Partial%2520Differential%2520Equations%2520%2528PDEs%2529.%2520Although%250Amuch%2520work%2520has%2520been%2520done%2520to%2520analyze%2520the%2520approximation%2520and%2520generalization%2520error%250Aof%2520neural%2520operators%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520analysis%2520on%2520their%2520training%2520error.%250AIn%2520this%2520work%252C%2520we%2520conduct%2520the%2520convergence%2520analysis%2520of%2520gradient%2520descent%2520for%2520the%250Awide%2520shallow%2520neural%2520operators%2520within%2520the%2520framework%2520of%2520Neural%2520Tangent%2520Kernel%250A%2528NTK%2529.%2520The%2520core%2520idea%2520lies%2520on%2520the%2520fact%2520that%2520over-parameterization%2520and%2520random%250Ainitialization%2520together%2520ensure%2520that%2520each%2520weight%2520vector%2520remains%2520near%2520its%250Ainitialization%2520throughout%2520all%2520iterations%252C%2520yielding%2520the%2520linear%2520convergence%2520of%250Agradient%2520descent.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520under%2520the%2520setting%2520of%250Aover-parametrization%252C%2520gradient%2520descent%2520can%2520find%2520the%2520global%2520minimum%2520regardless%250Aof%2520whether%2520it%2520is%2520in%2520continuous%2520time%2520or%2520discrete%2520time.%2520Finally%252C%2520we%2520briefly%250Adiscuss%2520the%2520case%2520of%2520physics-informed%2520shallow%2520neural%2520operators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20analysis%20of%20wide%20shallow%20neural%20operators%20within%20the%0A%20%20framework%20of%20Neural%20Tangent%20Kernel&entry.906535625=Xianliang%20Xu%20and%20Ye%20Li%20and%20Zhongyi%20Huang&entry.1292438233=%20%20Neural%20operators%20are%20aiming%20at%20approximating%20operators%20mapping%20between%20Banach%0Aspaces%20of%20functions%2C%20achieving%20much%20success%20in%20the%20field%20of%20scientific%0Acomputing.%20Compared%20to%20certain%20deep%20learning-based%20solvers%2C%20such%20as%0APhysics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20Deep%20Ritz%20Method%20%28DRM%29%2C%20neural%0Aoperators%20can%20solve%20a%20class%20of%20Partial%20Differential%20Equations%20%28PDEs%29.%20Although%0Amuch%20work%20has%20been%20done%20to%20analyze%20the%20approximation%20and%20generalization%20error%0Aof%20neural%20operators%2C%20there%20is%20still%20a%20lack%20of%20analysis%20on%20their%20training%20error.%0AIn%20this%20work%2C%20we%20conduct%20the%20convergence%20analysis%20of%20gradient%20descent%20for%20the%0Awide%20shallow%20neural%20operators%20within%20the%20framework%20of%20Neural%20Tangent%20Kernel%0A%28NTK%29.%20The%20core%20idea%20lies%20on%20the%20fact%20that%20over-parameterization%20and%20random%0Ainitialization%20together%20ensure%20that%20each%20weight%20vector%20remains%20near%20its%0Ainitialization%20throughout%20all%20iterations%2C%20yielding%20the%20linear%20convergence%20of%0Agradient%20descent.%20In%20this%20work%2C%20we%20demonstrate%20that%20under%20the%20setting%20of%0Aover-parametrization%2C%20gradient%20descent%20can%20find%20the%20global%20minimum%20regardless%0Aof%20whether%20it%20is%20in%20continuous%20time%20or%20discrete%20time.%20Finally%2C%20we%20briefly%0Adiscuss%20the%20case%20of%20physics-informed%20shallow%20neural%20operators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05545v2&entry.124074799=Read"},
{"title": "An Actionable Hierarchical Scene Representation Enhancing Autonomous\n  Inspection Missions in Unknown Environments", "author": "Vignesh Kottayam Viswanathan and Mario Alberto Valdes Saucedo and Sumeet Gajanan Satpute and Christoforos Kanellakis and George Nikolakopoulos", "abstract": "  In this article, we present the Layered Semantic Graphs (LSG), a novel\nactionable hierarchical scene graph, fully integrated with a multi-modal\nmission planner, the FLIE: A First-Look based Inspection and Exploration\nplanner. The novelty of this work stems from aiming to address the task of\nmaintaining an intuitive and multi-resolution scene representation, while\nsimultaneously offering a tractable foundation for planning and scene\nunderstanding during an ongoing inspection mission of apriori unknown\ntargets-of-interest in an unknown environment. The proposed LSG scheme is\ncomposed of locally nested hierarchical graphs, at multiple layers of\nabstraction, with the abstract concepts grounded on the functionality of the\nintegrated FLIE planner. Furthermore, LSG encapsulates real-time semantic\nsegmentation models that offer extraction and localization of desired semantic\nelements within the hierarchical representation. This extends the capability of\nthe inspection planner, which can then leverage LSG to make an informed\ndecision to inspect a particular semantic of interest. We also emphasize the\nhierarchical and semantic path-planning capabilities of LSG, which can extend\ninspection missions by improving situational awareness for human operators in\nan unknown environment. The validity of the proposed scheme is proven through\nextensive evaluations of the proposed architecture in simulations, as well as\nexperimental field deployments on a Boston Dynamics Spot quadruped robot in\nurban outdoor environment settings.\n", "link": "http://arxiv.org/abs/2412.19582v1", "date": "2024-12-27", "relevancy": 1.7818, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.682}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Actionable%20Hierarchical%20Scene%20Representation%20Enhancing%20Autonomous%0A%20%20Inspection%20Missions%20in%20Unknown%20Environments&body=Title%3A%20An%20Actionable%20Hierarchical%20Scene%20Representation%20Enhancing%20Autonomous%0A%20%20Inspection%20Missions%20in%20Unknown%20Environments%0AAuthor%3A%20Vignesh%20Kottayam%20Viswanathan%20and%20Mario%20Alberto%20Valdes%20Saucedo%20and%20Sumeet%20Gajanan%20Satpute%20and%20Christoforos%20Kanellakis%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20present%20the%20Layered%20Semantic%20Graphs%20%28LSG%29%2C%20a%20novel%0Aactionable%20hierarchical%20scene%20graph%2C%20fully%20integrated%20with%20a%20multi-modal%0Amission%20planner%2C%20the%20FLIE%3A%20A%20First-Look%20based%20Inspection%20and%20Exploration%0Aplanner.%20The%20novelty%20of%20this%20work%20stems%20from%20aiming%20to%20address%20the%20task%20of%0Amaintaining%20an%20intuitive%20and%20multi-resolution%20scene%20representation%2C%20while%0Asimultaneously%20offering%20a%20tractable%20foundation%20for%20planning%20and%20scene%0Aunderstanding%20during%20an%20ongoing%20inspection%20mission%20of%20apriori%20unknown%0Atargets-of-interest%20in%20an%20unknown%20environment.%20The%20proposed%20LSG%20scheme%20is%0Acomposed%20of%20locally%20nested%20hierarchical%20graphs%2C%20at%20multiple%20layers%20of%0Aabstraction%2C%20with%20the%20abstract%20concepts%20grounded%20on%20the%20functionality%20of%20the%0Aintegrated%20FLIE%20planner.%20Furthermore%2C%20LSG%20encapsulates%20real-time%20semantic%0Asegmentation%20models%20that%20offer%20extraction%20and%20localization%20of%20desired%20semantic%0Aelements%20within%20the%20hierarchical%20representation.%20This%20extends%20the%20capability%20of%0Athe%20inspection%20planner%2C%20which%20can%20then%20leverage%20LSG%20to%20make%20an%20informed%0Adecision%20to%20inspect%20a%20particular%20semantic%20of%20interest.%20We%20also%20emphasize%20the%0Ahierarchical%20and%20semantic%20path-planning%20capabilities%20of%20LSG%2C%20which%20can%20extend%0Ainspection%20missions%20by%20improving%20situational%20awareness%20for%20human%20operators%20in%0Aan%20unknown%20environment.%20The%20validity%20of%20the%20proposed%20scheme%20is%20proven%20through%0Aextensive%20evaluations%20of%20the%20proposed%20architecture%20in%20simulations%2C%20as%20well%20as%0Aexperimental%20field%20deployments%20on%20a%20Boston%20Dynamics%20Spot%20quadruped%20robot%20in%0Aurban%20outdoor%20environment%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19582v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Actionable%2520Hierarchical%2520Scene%2520Representation%2520Enhancing%2520Autonomous%250A%2520%2520Inspection%2520Missions%2520in%2520Unknown%2520Environments%26entry.906535625%3DVignesh%2520Kottayam%2520Viswanathan%2520and%2520Mario%2520Alberto%2520Valdes%2520Saucedo%2520and%2520Sumeet%2520Gajanan%2520Satpute%2520and%2520Christoforos%2520Kanellakis%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520present%2520the%2520Layered%2520Semantic%2520Graphs%2520%2528LSG%2529%252C%2520a%2520novel%250Aactionable%2520hierarchical%2520scene%2520graph%252C%2520fully%2520integrated%2520with%2520a%2520multi-modal%250Amission%2520planner%252C%2520the%2520FLIE%253A%2520A%2520First-Look%2520based%2520Inspection%2520and%2520Exploration%250Aplanner.%2520The%2520novelty%2520of%2520this%2520work%2520stems%2520from%2520aiming%2520to%2520address%2520the%2520task%2520of%250Amaintaining%2520an%2520intuitive%2520and%2520multi-resolution%2520scene%2520representation%252C%2520while%250Asimultaneously%2520offering%2520a%2520tractable%2520foundation%2520for%2520planning%2520and%2520scene%250Aunderstanding%2520during%2520an%2520ongoing%2520inspection%2520mission%2520of%2520apriori%2520unknown%250Atargets-of-interest%2520in%2520an%2520unknown%2520environment.%2520The%2520proposed%2520LSG%2520scheme%2520is%250Acomposed%2520of%2520locally%2520nested%2520hierarchical%2520graphs%252C%2520at%2520multiple%2520layers%2520of%250Aabstraction%252C%2520with%2520the%2520abstract%2520concepts%2520grounded%2520on%2520the%2520functionality%2520of%2520the%250Aintegrated%2520FLIE%2520planner.%2520Furthermore%252C%2520LSG%2520encapsulates%2520real-time%2520semantic%250Asegmentation%2520models%2520that%2520offer%2520extraction%2520and%2520localization%2520of%2520desired%2520semantic%250Aelements%2520within%2520the%2520hierarchical%2520representation.%2520This%2520extends%2520the%2520capability%2520of%250Athe%2520inspection%2520planner%252C%2520which%2520can%2520then%2520leverage%2520LSG%2520to%2520make%2520an%2520informed%250Adecision%2520to%2520inspect%2520a%2520particular%2520semantic%2520of%2520interest.%2520We%2520also%2520emphasize%2520the%250Ahierarchical%2520and%2520semantic%2520path-planning%2520capabilities%2520of%2520LSG%252C%2520which%2520can%2520extend%250Ainspection%2520missions%2520by%2520improving%2520situational%2520awareness%2520for%2520human%2520operators%2520in%250Aan%2520unknown%2520environment.%2520The%2520validity%2520of%2520the%2520proposed%2520scheme%2520is%2520proven%2520through%250Aextensive%2520evaluations%2520of%2520the%2520proposed%2520architecture%2520in%2520simulations%252C%2520as%2520well%2520as%250Aexperimental%2520field%2520deployments%2520on%2520a%2520Boston%2520Dynamics%2520Spot%2520quadruped%2520robot%2520in%250Aurban%2520outdoor%2520environment%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19582v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Actionable%20Hierarchical%20Scene%20Representation%20Enhancing%20Autonomous%0A%20%20Inspection%20Missions%20in%20Unknown%20Environments&entry.906535625=Vignesh%20Kottayam%20Viswanathan%20and%20Mario%20Alberto%20Valdes%20Saucedo%20and%20Sumeet%20Gajanan%20Satpute%20and%20Christoforos%20Kanellakis%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20In%20this%20article%2C%20we%20present%20the%20Layered%20Semantic%20Graphs%20%28LSG%29%2C%20a%20novel%0Aactionable%20hierarchical%20scene%20graph%2C%20fully%20integrated%20with%20a%20multi-modal%0Amission%20planner%2C%20the%20FLIE%3A%20A%20First-Look%20based%20Inspection%20and%20Exploration%0Aplanner.%20The%20novelty%20of%20this%20work%20stems%20from%20aiming%20to%20address%20the%20task%20of%0Amaintaining%20an%20intuitive%20and%20multi-resolution%20scene%20representation%2C%20while%0Asimultaneously%20offering%20a%20tractable%20foundation%20for%20planning%20and%20scene%0Aunderstanding%20during%20an%20ongoing%20inspection%20mission%20of%20apriori%20unknown%0Atargets-of-interest%20in%20an%20unknown%20environment.%20The%20proposed%20LSG%20scheme%20is%0Acomposed%20of%20locally%20nested%20hierarchical%20graphs%2C%20at%20multiple%20layers%20of%0Aabstraction%2C%20with%20the%20abstract%20concepts%20grounded%20on%20the%20functionality%20of%20the%0Aintegrated%20FLIE%20planner.%20Furthermore%2C%20LSG%20encapsulates%20real-time%20semantic%0Asegmentation%20models%20that%20offer%20extraction%20and%20localization%20of%20desired%20semantic%0Aelements%20within%20the%20hierarchical%20representation.%20This%20extends%20the%20capability%20of%0Athe%20inspection%20planner%2C%20which%20can%20then%20leverage%20LSG%20to%20make%20an%20informed%0Adecision%20to%20inspect%20a%20particular%20semantic%20of%20interest.%20We%20also%20emphasize%20the%0Ahierarchical%20and%20semantic%20path-planning%20capabilities%20of%20LSG%2C%20which%20can%20extend%0Ainspection%20missions%20by%20improving%20situational%20awareness%20for%20human%20operators%20in%0Aan%20unknown%20environment.%20The%20validity%20of%20the%20proposed%20scheme%20is%20proven%20through%0Aextensive%20evaluations%20of%20the%20proposed%20architecture%20in%20simulations%2C%20as%20well%20as%0Aexperimental%20field%20deployments%20on%20a%20Boston%20Dynamics%20Spot%20quadruped%20robot%20in%0Aurban%20outdoor%20environment%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19582v1&entry.124074799=Read"},
{"title": "Machine Learning for Sentiment Analysis of Imported Food in Trinidad and\n  Tobago", "author": "Cassandra Daniels and Koffka Khan", "abstract": "  This research investigates the performance of various machine learning\nalgorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter\ndata related to imported food items in Trinidad and Tobago. The study addresses\nthree primary research questions: the comparative accuracy and efficiency of\nthe algorithms, the optimal configurations for each model, and the potential\napplications of the optimized models in a live system for monitoring public\nsentiment and its impact on the import bill. The dataset comprises tweets from\n2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess\nthe impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten\nexperiments were conducted to evaluate the models under various configurations.\nResults indicated that VADER outperformed the other models in both multi-class\nand binary sentiment classifications. The study highlights significant changes\nin sentiment trends pre- and post-COVID-19, with implications for import\npolicies.\n", "link": "http://arxiv.org/abs/2412.19781v1", "date": "2024-12-27", "relevancy": 1.7816, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4516}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4412}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20for%20Sentiment%20Analysis%20of%20Imported%20Food%20in%20Trinidad%20and%0A%20%20Tobago&body=Title%3A%20Machine%20Learning%20for%20Sentiment%20Analysis%20of%20Imported%20Food%20in%20Trinidad%20and%0A%20%20Tobago%0AAuthor%3A%20Cassandra%20Daniels%20and%20Koffka%20Khan%0AAbstract%3A%20%20%20This%20research%20investigates%20the%20performance%20of%20various%20machine%20learning%0Aalgorithms%20%28CNN%2C%20LSTM%2C%20VADER%2C%20and%20RoBERTa%29%20for%20sentiment%20analysis%20of%20Twitter%0Adata%20related%20to%20imported%20food%20items%20in%20Trinidad%20and%20Tobago.%20The%20study%20addresses%0Athree%20primary%20research%20questions%3A%20the%20comparative%20accuracy%20and%20efficiency%20of%0Athe%20algorithms%2C%20the%20optimal%20configurations%20for%20each%20model%2C%20and%20the%20potential%0Aapplications%20of%20the%20optimized%20models%20in%20a%20live%20system%20for%20monitoring%20public%0Asentiment%20and%20its%20impact%20on%20the%20import%20bill.%20The%20dataset%20comprises%20tweets%20from%0A2018%20to%202024%2C%20divided%20into%20imbalanced%2C%20balanced%2C%20and%20temporal%20subsets%20to%20assess%0Athe%20impact%20of%20data%20balancing%20and%20the%20COVID-19%20pandemic%20on%20sentiment%20trends.%20Ten%0Aexperiments%20were%20conducted%20to%20evaluate%20the%20models%20under%20various%20configurations.%0AResults%20indicated%20that%20VADER%20outperformed%20the%20other%20models%20in%20both%20multi-class%0Aand%20binary%20sentiment%20classifications.%20The%20study%20highlights%20significant%20changes%0Ain%20sentiment%20trends%20pre-%20and%20post-COVID-19%2C%20with%20implications%20for%20import%0Apolicies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520for%2520Sentiment%2520Analysis%2520of%2520Imported%2520Food%2520in%2520Trinidad%2520and%250A%2520%2520Tobago%26entry.906535625%3DCassandra%2520Daniels%2520and%2520Koffka%2520Khan%26entry.1292438233%3D%2520%2520This%2520research%2520investigates%2520the%2520performance%2520of%2520various%2520machine%2520learning%250Aalgorithms%2520%2528CNN%252C%2520LSTM%252C%2520VADER%252C%2520and%2520RoBERTa%2529%2520for%2520sentiment%2520analysis%2520of%2520Twitter%250Adata%2520related%2520to%2520imported%2520food%2520items%2520in%2520Trinidad%2520and%2520Tobago.%2520The%2520study%2520addresses%250Athree%2520primary%2520research%2520questions%253A%2520the%2520comparative%2520accuracy%2520and%2520efficiency%2520of%250Athe%2520algorithms%252C%2520the%2520optimal%2520configurations%2520for%2520each%2520model%252C%2520and%2520the%2520potential%250Aapplications%2520of%2520the%2520optimized%2520models%2520in%2520a%2520live%2520system%2520for%2520monitoring%2520public%250Asentiment%2520and%2520its%2520impact%2520on%2520the%2520import%2520bill.%2520The%2520dataset%2520comprises%2520tweets%2520from%250A2018%2520to%25202024%252C%2520divided%2520into%2520imbalanced%252C%2520balanced%252C%2520and%2520temporal%2520subsets%2520to%2520assess%250Athe%2520impact%2520of%2520data%2520balancing%2520and%2520the%2520COVID-19%2520pandemic%2520on%2520sentiment%2520trends.%2520Ten%250Aexperiments%2520were%2520conducted%2520to%2520evaluate%2520the%2520models%2520under%2520various%2520configurations.%250AResults%2520indicated%2520that%2520VADER%2520outperformed%2520the%2520other%2520models%2520in%2520both%2520multi-class%250Aand%2520binary%2520sentiment%2520classifications.%2520The%2520study%2520highlights%2520significant%2520changes%250Ain%2520sentiment%2520trends%2520pre-%2520and%2520post-COVID-19%252C%2520with%2520implications%2520for%2520import%250Apolicies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20for%20Sentiment%20Analysis%20of%20Imported%20Food%20in%20Trinidad%20and%0A%20%20Tobago&entry.906535625=Cassandra%20Daniels%20and%20Koffka%20Khan&entry.1292438233=%20%20This%20research%20investigates%20the%20performance%20of%20various%20machine%20learning%0Aalgorithms%20%28CNN%2C%20LSTM%2C%20VADER%2C%20and%20RoBERTa%29%20for%20sentiment%20analysis%20of%20Twitter%0Adata%20related%20to%20imported%20food%20items%20in%20Trinidad%20and%20Tobago.%20The%20study%20addresses%0Athree%20primary%20research%20questions%3A%20the%20comparative%20accuracy%20and%20efficiency%20of%0Athe%20algorithms%2C%20the%20optimal%20configurations%20for%20each%20model%2C%20and%20the%20potential%0Aapplications%20of%20the%20optimized%20models%20in%20a%20live%20system%20for%20monitoring%20public%0Asentiment%20and%20its%20impact%20on%20the%20import%20bill.%20The%20dataset%20comprises%20tweets%20from%0A2018%20to%202024%2C%20divided%20into%20imbalanced%2C%20balanced%2C%20and%20temporal%20subsets%20to%20assess%0Athe%20impact%20of%20data%20balancing%20and%20the%20COVID-19%20pandemic%20on%20sentiment%20trends.%20Ten%0Aexperiments%20were%20conducted%20to%20evaluate%20the%20models%20under%20various%20configurations.%0AResults%20indicated%20that%20VADER%20outperformed%20the%20other%20models%20in%20both%20multi-class%0Aand%20binary%20sentiment%20classifications.%20The%20study%20highlights%20significant%20changes%0Ain%20sentiment%20trends%20pre-%20and%20post-COVID-19%2C%20with%20implications%20for%20import%0Apolicies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19781v1&entry.124074799=Read"},
{"title": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios", "author": "Zhi Chen and Lingxiao Jiang", "abstract": "  In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development.\n", "link": "http://arxiv.org/abs/2410.12468v2", "date": "2024-12-27", "relevancy": 1.7286, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4356}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4337}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Software%20Development%20Agents%3A%20Patch%20Patterns%2C%20Code%20Quality%2C%0A%20%20and%20Issue%20Complexity%20in%20Real-World%20GitHub%20Scenarios&body=Title%3A%20Evaluating%20Software%20Development%20Agents%3A%20Patch%20Patterns%2C%20Code%20Quality%2C%0A%20%20and%20Issue%20Complexity%20in%20Real-World%20GitHub%20Scenarios%0AAuthor%3A%20Zhi%20Chen%20and%20Lingxiao%20Jiang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20AI-based%20software%20engineering%20has%20progressed%20from%0Apre-trained%20models%20to%20advanced%20agentic%20workflows%2C%20with%20Software%20Development%0AAgents%20representing%20the%20next%20major%20leap.%20These%20agents%2C%20capable%20of%20reasoning%2C%0Aplanning%2C%20and%20interacting%20with%20external%20environments%2C%20offer%20promising%20solutions%0Ato%20complex%20software%20engineering%20tasks.%20However%2C%20while%20much%20research%20has%0Aevaluated%20code%20generated%20by%20large%20language%20models%20%28LLMs%29%2C%20comprehensive%20studies%0Aon%20agent-generated%20patches%2C%20particularly%20in%20real-world%20settings%2C%20are%20lacking.%0AThis%20study%20addresses%20that%20gap%20by%20evaluating%204%2C892%20patches%20from%2010%20top-ranked%0Aagents%20on%20500%20real-world%20GitHub%20issues%20from%20SWE-Bench%20Verified%2C%20focusing%20on%0Atheir%20impact%20on%20code%20quality.%20Our%20analysis%20shows%20no%20single%20agent%20dominated%2C%0Awith%20170%20issues%20unresolved%2C%20indicating%20room%20for%20improvement.%20Even%20for%20patches%0Athat%20passed%20unit%20tests%20and%20resolved%20issues%2C%20agents%20made%20different%20file%20and%0Afunction%20modifications%20compared%20to%20the%20gold%20patches%20from%20repository%20developers%2C%0Arevealing%20limitations%20in%20the%20benchmark%27s%20test%20case%20coverage.%20Most%20agents%0Amaintained%20code%20reliability%20and%20security%2C%20avoiding%20new%20bugs%20or%20vulnerabilities%3B%0Awhile%20some%20agents%20increased%20code%20complexity%2C%20many%20reduced%20code%20duplication%20and%0Aminimized%20code%20smells.%20Finally%2C%20agents%20performed%20better%20on%20simpler%20codebases%2C%0Asuggesting%20that%20breaking%20complex%20tasks%20into%20smaller%20sub-tasks%20could%20improve%0Aeffectiveness.%20This%20study%20provides%20the%20first%20comprehensive%20evaluation%20of%0Aagent-generated%20patches%20on%20real-world%20GitHub%20issues%2C%20offering%20insights%20to%0Aadvance%20AI-driven%20software%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Software%2520Development%2520Agents%253A%2520Patch%2520Patterns%252C%2520Code%2520Quality%252C%250A%2520%2520and%2520Issue%2520Complexity%2520in%2520Real-World%2520GitHub%2520Scenarios%26entry.906535625%3DZhi%2520Chen%2520and%2520Lingxiao%2520Jiang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520AI-based%2520software%2520engineering%2520has%2520progressed%2520from%250Apre-trained%2520models%2520to%2520advanced%2520agentic%2520workflows%252C%2520with%2520Software%2520Development%250AAgents%2520representing%2520the%2520next%2520major%2520leap.%2520These%2520agents%252C%2520capable%2520of%2520reasoning%252C%250Aplanning%252C%2520and%2520interacting%2520with%2520external%2520environments%252C%2520offer%2520promising%2520solutions%250Ato%2520complex%2520software%2520engineering%2520tasks.%2520However%252C%2520while%2520much%2520research%2520has%250Aevaluated%2520code%2520generated%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520comprehensive%2520studies%250Aon%2520agent-generated%2520patches%252C%2520particularly%2520in%2520real-world%2520settings%252C%2520are%2520lacking.%250AThis%2520study%2520addresses%2520that%2520gap%2520by%2520evaluating%25204%252C892%2520patches%2520from%252010%2520top-ranked%250Aagents%2520on%2520500%2520real-world%2520GitHub%2520issues%2520from%2520SWE-Bench%2520Verified%252C%2520focusing%2520on%250Atheir%2520impact%2520on%2520code%2520quality.%2520Our%2520analysis%2520shows%2520no%2520single%2520agent%2520dominated%252C%250Awith%2520170%2520issues%2520unresolved%252C%2520indicating%2520room%2520for%2520improvement.%2520Even%2520for%2520patches%250Athat%2520passed%2520unit%2520tests%2520and%2520resolved%2520issues%252C%2520agents%2520made%2520different%2520file%2520and%250Afunction%2520modifications%2520compared%2520to%2520the%2520gold%2520patches%2520from%2520repository%2520developers%252C%250Arevealing%2520limitations%2520in%2520the%2520benchmark%2527s%2520test%2520case%2520coverage.%2520Most%2520agents%250Amaintained%2520code%2520reliability%2520and%2520security%252C%2520avoiding%2520new%2520bugs%2520or%2520vulnerabilities%253B%250Awhile%2520some%2520agents%2520increased%2520code%2520complexity%252C%2520many%2520reduced%2520code%2520duplication%2520and%250Aminimized%2520code%2520smells.%2520Finally%252C%2520agents%2520performed%2520better%2520on%2520simpler%2520codebases%252C%250Asuggesting%2520that%2520breaking%2520complex%2520tasks%2520into%2520smaller%2520sub-tasks%2520could%2520improve%250Aeffectiveness.%2520This%2520study%2520provides%2520the%2520first%2520comprehensive%2520evaluation%2520of%250Aagent-generated%2520patches%2520on%2520real-world%2520GitHub%2520issues%252C%2520offering%2520insights%2520to%250Aadvance%2520AI-driven%2520software%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Software%20Development%20Agents%3A%20Patch%20Patterns%2C%20Code%20Quality%2C%0A%20%20and%20Issue%20Complexity%20in%20Real-World%20GitHub%20Scenarios&entry.906535625=Zhi%20Chen%20and%20Lingxiao%20Jiang&entry.1292438233=%20%20In%20recent%20years%2C%20AI-based%20software%20engineering%20has%20progressed%20from%0Apre-trained%20models%20to%20advanced%20agentic%20workflows%2C%20with%20Software%20Development%0AAgents%20representing%20the%20next%20major%20leap.%20These%20agents%2C%20capable%20of%20reasoning%2C%0Aplanning%2C%20and%20interacting%20with%20external%20environments%2C%20offer%20promising%20solutions%0Ato%20complex%20software%20engineering%20tasks.%20However%2C%20while%20much%20research%20has%0Aevaluated%20code%20generated%20by%20large%20language%20models%20%28LLMs%29%2C%20comprehensive%20studies%0Aon%20agent-generated%20patches%2C%20particularly%20in%20real-world%20settings%2C%20are%20lacking.%0AThis%20study%20addresses%20that%20gap%20by%20evaluating%204%2C892%20patches%20from%2010%20top-ranked%0Aagents%20on%20500%20real-world%20GitHub%20issues%20from%20SWE-Bench%20Verified%2C%20focusing%20on%0Atheir%20impact%20on%20code%20quality.%20Our%20analysis%20shows%20no%20single%20agent%20dominated%2C%0Awith%20170%20issues%20unresolved%2C%20indicating%20room%20for%20improvement.%20Even%20for%20patches%0Athat%20passed%20unit%20tests%20and%20resolved%20issues%2C%20agents%20made%20different%20file%20and%0Afunction%20modifications%20compared%20to%20the%20gold%20patches%20from%20repository%20developers%2C%0Arevealing%20limitations%20in%20the%20benchmark%27s%20test%20case%20coverage.%20Most%20agents%0Amaintained%20code%20reliability%20and%20security%2C%20avoiding%20new%20bugs%20or%20vulnerabilities%3B%0Awhile%20some%20agents%20increased%20code%20complexity%2C%20many%20reduced%20code%20duplication%20and%0Aminimized%20code%20smells.%20Finally%2C%20agents%20performed%20better%20on%20simpler%20codebases%2C%0Asuggesting%20that%20breaking%20complex%20tasks%20into%20smaller%20sub-tasks%20could%20improve%0Aeffectiveness.%20This%20study%20provides%20the%20first%20comprehensive%20evaluation%20of%0Aagent-generated%20patches%20on%20real-world%20GitHub%20issues%2C%20offering%20insights%20to%0Aadvance%20AI-driven%20software%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12468v2&entry.124074799=Read"},
{"title": "SocRATES: Towards Automated Scenario-based Testing of Social Navigation\n  Algorithms", "author": "Shashank Rao Marpally and Pranav Goyal and Harold Soh", "abstract": "  Current social navigation methods and benchmarks primarily focus on proxemics\nand task efficiency. While these factors are important, qualitative aspects\nsuch as perceptions of a robot's social competence are equally crucial for\nsuccessful adoption and integration into human environments. We propose a more\ncomprehensive evaluation of social navigation through scenario-based testing,\nwhere specific human-robot interaction scenarios can reveal key robot\nbehaviors. However, creating such scenarios is often labor-intensive and\ncomplex. In this work, we address this challenge by introducing a pipeline that\nautomates the generation of context-, and location-appropriate social\nnavigation scenarios, ready for simulation. Our pipeline transforms simple\nscenario metadata into detailed textual scenarios, infers pedestrian and robot\ntrajectories, and simulates pedestrian behaviors, which enables more controlled\nevaluation. We leverage the social reasoning and code-generation capabilities\nof Large Language Models (LLMs) to streamline scenario generation and\ntranslation. Our experiments show that our pipeline produces realistic\nscenarios and significantly improves scenario translation over naive LLM\nprompting. Additionally, we present initial feedback from a usability study\nwith social navigation experts and a case-study demonstrating a scenario-based\nevaluation of three navigation algorithms.\n", "link": "http://arxiv.org/abs/2412.19595v1", "date": "2024-12-27", "relevancy": 1.7269, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6267}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.563}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SocRATES%3A%20Towards%20Automated%20Scenario-based%20Testing%20of%20Social%20Navigation%0A%20%20Algorithms&body=Title%3A%20SocRATES%3A%20Towards%20Automated%20Scenario-based%20Testing%20of%20Social%20Navigation%0A%20%20Algorithms%0AAuthor%3A%20Shashank%20Rao%20Marpally%20and%20Pranav%20Goyal%20and%20Harold%20Soh%0AAbstract%3A%20%20%20Current%20social%20navigation%20methods%20and%20benchmarks%20primarily%20focus%20on%20proxemics%0Aand%20task%20efficiency.%20While%20these%20factors%20are%20important%2C%20qualitative%20aspects%0Asuch%20as%20perceptions%20of%20a%20robot%27s%20social%20competence%20are%20equally%20crucial%20for%0Asuccessful%20adoption%20and%20integration%20into%20human%20environments.%20We%20propose%20a%20more%0Acomprehensive%20evaluation%20of%20social%20navigation%20through%20scenario-based%20testing%2C%0Awhere%20specific%20human-robot%20interaction%20scenarios%20can%20reveal%20key%20robot%0Abehaviors.%20However%2C%20creating%20such%20scenarios%20is%20often%20labor-intensive%20and%0Acomplex.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%20introducing%20a%20pipeline%20that%0Aautomates%20the%20generation%20of%20context-%2C%20and%20location-appropriate%20social%0Anavigation%20scenarios%2C%20ready%20for%20simulation.%20Our%20pipeline%20transforms%20simple%0Ascenario%20metadata%20into%20detailed%20textual%20scenarios%2C%20infers%20pedestrian%20and%20robot%0Atrajectories%2C%20and%20simulates%20pedestrian%20behaviors%2C%20which%20enables%20more%20controlled%0Aevaluation.%20We%20leverage%20the%20social%20reasoning%20and%20code-generation%20capabilities%0Aof%20Large%20Language%20Models%20%28LLMs%29%20to%20streamline%20scenario%20generation%20and%0Atranslation.%20Our%20experiments%20show%20that%20our%20pipeline%20produces%20realistic%0Ascenarios%20and%20significantly%20improves%20scenario%20translation%20over%20naive%20LLM%0Aprompting.%20Additionally%2C%20we%20present%20initial%20feedback%20from%20a%20usability%20study%0Awith%20social%20navigation%20experts%20and%20a%20case-study%20demonstrating%20a%20scenario-based%0Aevaluation%20of%20three%20navigation%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocRATES%253A%2520Towards%2520Automated%2520Scenario-based%2520Testing%2520of%2520Social%2520Navigation%250A%2520%2520Algorithms%26entry.906535625%3DShashank%2520Rao%2520Marpally%2520and%2520Pranav%2520Goyal%2520and%2520Harold%2520Soh%26entry.1292438233%3D%2520%2520Current%2520social%2520navigation%2520methods%2520and%2520benchmarks%2520primarily%2520focus%2520on%2520proxemics%250Aand%2520task%2520efficiency.%2520While%2520these%2520factors%2520are%2520important%252C%2520qualitative%2520aspects%250Asuch%2520as%2520perceptions%2520of%2520a%2520robot%2527s%2520social%2520competence%2520are%2520equally%2520crucial%2520for%250Asuccessful%2520adoption%2520and%2520integration%2520into%2520human%2520environments.%2520We%2520propose%2520a%2520more%250Acomprehensive%2520evaluation%2520of%2520social%2520navigation%2520through%2520scenario-based%2520testing%252C%250Awhere%2520specific%2520human-robot%2520interaction%2520scenarios%2520can%2520reveal%2520key%2520robot%250Abehaviors.%2520However%252C%2520creating%2520such%2520scenarios%2520is%2520often%2520labor-intensive%2520and%250Acomplex.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520challenge%2520by%2520introducing%2520a%2520pipeline%2520that%250Aautomates%2520the%2520generation%2520of%2520context-%252C%2520and%2520location-appropriate%2520social%250Anavigation%2520scenarios%252C%2520ready%2520for%2520simulation.%2520Our%2520pipeline%2520transforms%2520simple%250Ascenario%2520metadata%2520into%2520detailed%2520textual%2520scenarios%252C%2520infers%2520pedestrian%2520and%2520robot%250Atrajectories%252C%2520and%2520simulates%2520pedestrian%2520behaviors%252C%2520which%2520enables%2520more%2520controlled%250Aevaluation.%2520We%2520leverage%2520the%2520social%2520reasoning%2520and%2520code-generation%2520capabilities%250Aof%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520streamline%2520scenario%2520generation%2520and%250Atranslation.%2520Our%2520experiments%2520show%2520that%2520our%2520pipeline%2520produces%2520realistic%250Ascenarios%2520and%2520significantly%2520improves%2520scenario%2520translation%2520over%2520naive%2520LLM%250Aprompting.%2520Additionally%252C%2520we%2520present%2520initial%2520feedback%2520from%2520a%2520usability%2520study%250Awith%2520social%2520navigation%2520experts%2520and%2520a%2520case-study%2520demonstrating%2520a%2520scenario-based%250Aevaluation%2520of%2520three%2520navigation%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SocRATES%3A%20Towards%20Automated%20Scenario-based%20Testing%20of%20Social%20Navigation%0A%20%20Algorithms&entry.906535625=Shashank%20Rao%20Marpally%20and%20Pranav%20Goyal%20and%20Harold%20Soh&entry.1292438233=%20%20Current%20social%20navigation%20methods%20and%20benchmarks%20primarily%20focus%20on%20proxemics%0Aand%20task%20efficiency.%20While%20these%20factors%20are%20important%2C%20qualitative%20aspects%0Asuch%20as%20perceptions%20of%20a%20robot%27s%20social%20competence%20are%20equally%20crucial%20for%0Asuccessful%20adoption%20and%20integration%20into%20human%20environments.%20We%20propose%20a%20more%0Acomprehensive%20evaluation%20of%20social%20navigation%20through%20scenario-based%20testing%2C%0Awhere%20specific%20human-robot%20interaction%20scenarios%20can%20reveal%20key%20robot%0Abehaviors.%20However%2C%20creating%20such%20scenarios%20is%20often%20labor-intensive%20and%0Acomplex.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%20introducing%20a%20pipeline%20that%0Aautomates%20the%20generation%20of%20context-%2C%20and%20location-appropriate%20social%0Anavigation%20scenarios%2C%20ready%20for%20simulation.%20Our%20pipeline%20transforms%20simple%0Ascenario%20metadata%20into%20detailed%20textual%20scenarios%2C%20infers%20pedestrian%20and%20robot%0Atrajectories%2C%20and%20simulates%20pedestrian%20behaviors%2C%20which%20enables%20more%20controlled%0Aevaluation.%20We%20leverage%20the%20social%20reasoning%20and%20code-generation%20capabilities%0Aof%20Large%20Language%20Models%20%28LLMs%29%20to%20streamline%20scenario%20generation%20and%0Atranslation.%20Our%20experiments%20show%20that%20our%20pipeline%20produces%20realistic%0Ascenarios%20and%20significantly%20improves%20scenario%20translation%20over%20naive%20LLM%0Aprompting.%20Additionally%2C%20we%20present%20initial%20feedback%20from%20a%20usability%20study%0Awith%20social%20navigation%20experts%20and%20a%20case-study%20demonstrating%20a%20scenario-based%0Aevaluation%20of%20three%20navigation%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19595v1&entry.124074799=Read"},
{"title": "Combining Machine Learning with Recurrence Analysis for resonance\n  detection", "author": "Ond\u0159ej Zelenka and Ond\u0159ej Kop\u00e1\u010dek and Georgios Lukes-Gerakopoulos", "abstract": "  The width of a resonance in a nearly integrable system, i.e. in a\nnon-integrable system where chaotic motion is still not prominent, can tell us\nhow a perturbation parameter is driving the system away from integrability.\nAlthough the tool that we are presenting here can be used is quite generic and\ncan be used in a variety of systems, our particular interest lies in binary\ncompact object systems known as extreme mass ratio inspirals (EMRIs). In an\nEMRI a lighter compact object, like a black hole or a neutron star, inspirals\ninto a supermassive black hole due to gravitational radiation reaction. During\nthis inspiral the lighter object crosses resonances, which are still not very\nwell modeled. Measuring the width of resonances in EMRI models allows us to\nestimate the importance of each perturbation parameter able to drive the system\naway from resonances and decide whether its impact should be included in EMRI\nwaveform modeling or not. To tackle this issue in our study we show first that\nrecurrence quantifiers of orbits carry imprints of resonant behavior,\nregardless of the system's dimensionality. As a next step, we apply a long\nshort-term memory machine learning architecture to automate the resonance\ndetection procedure. Our analysis is developed on a simple standard map and\ngradually we extend it to more complicated systems until finally we employ it\nin a generic deformed Kerr spacetime known in the literature as the\nJohannsen-Psaltis spacetime.\n", "link": "http://arxiv.org/abs/2412.19683v1", "date": "2024-12-27", "relevancy": 1.7249, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4424}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4344}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Machine%20Learning%20with%20Recurrence%20Analysis%20for%20resonance%0A%20%20detection&body=Title%3A%20Combining%20Machine%20Learning%20with%20Recurrence%20Analysis%20for%20resonance%0A%20%20detection%0AAuthor%3A%20Ond%C5%99ej%20Zelenka%20and%20Ond%C5%99ej%20Kop%C3%A1%C4%8Dek%20and%20Georgios%20Lukes-Gerakopoulos%0AAbstract%3A%20%20%20The%20width%20of%20a%20resonance%20in%20a%20nearly%20integrable%20system%2C%20i.e.%20in%20a%0Anon-integrable%20system%20where%20chaotic%20motion%20is%20still%20not%20prominent%2C%20can%20tell%20us%0Ahow%20a%20perturbation%20parameter%20is%20driving%20the%20system%20away%20from%20integrability.%0AAlthough%20the%20tool%20that%20we%20are%20presenting%20here%20can%20be%20used%20is%20quite%20generic%20and%0Acan%20be%20used%20in%20a%20variety%20of%20systems%2C%20our%20particular%20interest%20lies%20in%20binary%0Acompact%20object%20systems%20known%20as%20extreme%20mass%20ratio%20inspirals%20%28EMRIs%29.%20In%20an%0AEMRI%20a%20lighter%20compact%20object%2C%20like%20a%20black%20hole%20or%20a%20neutron%20star%2C%20inspirals%0Ainto%20a%20supermassive%20black%20hole%20due%20to%20gravitational%20radiation%20reaction.%20During%0Athis%20inspiral%20the%20lighter%20object%20crosses%20resonances%2C%20which%20are%20still%20not%20very%0Awell%20modeled.%20Measuring%20the%20width%20of%20resonances%20in%20EMRI%20models%20allows%20us%20to%0Aestimate%20the%20importance%20of%20each%20perturbation%20parameter%20able%20to%20drive%20the%20system%0Aaway%20from%20resonances%20and%20decide%20whether%20its%20impact%20should%20be%20included%20in%20EMRI%0Awaveform%20modeling%20or%20not.%20To%20tackle%20this%20issue%20in%20our%20study%20we%20show%20first%20that%0Arecurrence%20quantifiers%20of%20orbits%20carry%20imprints%20of%20resonant%20behavior%2C%0Aregardless%20of%20the%20system%27s%20dimensionality.%20As%20a%20next%20step%2C%20we%20apply%20a%20long%0Ashort-term%20memory%20machine%20learning%20architecture%20to%20automate%20the%20resonance%0Adetection%20procedure.%20Our%20analysis%20is%20developed%20on%20a%20simple%20standard%20map%20and%0Agradually%20we%20extend%20it%20to%20more%20complicated%20systems%20until%20finally%20we%20employ%20it%0Ain%20a%20generic%20deformed%20Kerr%20spacetime%20known%20in%20the%20literature%20as%20the%0AJohannsen-Psaltis%20spacetime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Machine%2520Learning%2520with%2520Recurrence%2520Analysis%2520for%2520resonance%250A%2520%2520detection%26entry.906535625%3DOnd%25C5%2599ej%2520Zelenka%2520and%2520Ond%25C5%2599ej%2520Kop%25C3%25A1%25C4%258Dek%2520and%2520Georgios%2520Lukes-Gerakopoulos%26entry.1292438233%3D%2520%2520The%2520width%2520of%2520a%2520resonance%2520in%2520a%2520nearly%2520integrable%2520system%252C%2520i.e.%2520in%2520a%250Anon-integrable%2520system%2520where%2520chaotic%2520motion%2520is%2520still%2520not%2520prominent%252C%2520can%2520tell%2520us%250Ahow%2520a%2520perturbation%2520parameter%2520is%2520driving%2520the%2520system%2520away%2520from%2520integrability.%250AAlthough%2520the%2520tool%2520that%2520we%2520are%2520presenting%2520here%2520can%2520be%2520used%2520is%2520quite%2520generic%2520and%250Acan%2520be%2520used%2520in%2520a%2520variety%2520of%2520systems%252C%2520our%2520particular%2520interest%2520lies%2520in%2520binary%250Acompact%2520object%2520systems%2520known%2520as%2520extreme%2520mass%2520ratio%2520inspirals%2520%2528EMRIs%2529.%2520In%2520an%250AEMRI%2520a%2520lighter%2520compact%2520object%252C%2520like%2520a%2520black%2520hole%2520or%2520a%2520neutron%2520star%252C%2520inspirals%250Ainto%2520a%2520supermassive%2520black%2520hole%2520due%2520to%2520gravitational%2520radiation%2520reaction.%2520During%250Athis%2520inspiral%2520the%2520lighter%2520object%2520crosses%2520resonances%252C%2520which%2520are%2520still%2520not%2520very%250Awell%2520modeled.%2520Measuring%2520the%2520width%2520of%2520resonances%2520in%2520EMRI%2520models%2520allows%2520us%2520to%250Aestimate%2520the%2520importance%2520of%2520each%2520perturbation%2520parameter%2520able%2520to%2520drive%2520the%2520system%250Aaway%2520from%2520resonances%2520and%2520decide%2520whether%2520its%2520impact%2520should%2520be%2520included%2520in%2520EMRI%250Awaveform%2520modeling%2520or%2520not.%2520To%2520tackle%2520this%2520issue%2520in%2520our%2520study%2520we%2520show%2520first%2520that%250Arecurrence%2520quantifiers%2520of%2520orbits%2520carry%2520imprints%2520of%2520resonant%2520behavior%252C%250Aregardless%2520of%2520the%2520system%2527s%2520dimensionality.%2520As%2520a%2520next%2520step%252C%2520we%2520apply%2520a%2520long%250Ashort-term%2520memory%2520machine%2520learning%2520architecture%2520to%2520automate%2520the%2520resonance%250Adetection%2520procedure.%2520Our%2520analysis%2520is%2520developed%2520on%2520a%2520simple%2520standard%2520map%2520and%250Agradually%2520we%2520extend%2520it%2520to%2520more%2520complicated%2520systems%2520until%2520finally%2520we%2520employ%2520it%250Ain%2520a%2520generic%2520deformed%2520Kerr%2520spacetime%2520known%2520in%2520the%2520literature%2520as%2520the%250AJohannsen-Psaltis%2520spacetime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Machine%20Learning%20with%20Recurrence%20Analysis%20for%20resonance%0A%20%20detection&entry.906535625=Ond%C5%99ej%20Zelenka%20and%20Ond%C5%99ej%20Kop%C3%A1%C4%8Dek%20and%20Georgios%20Lukes-Gerakopoulos&entry.1292438233=%20%20The%20width%20of%20a%20resonance%20in%20a%20nearly%20integrable%20system%2C%20i.e.%20in%20a%0Anon-integrable%20system%20where%20chaotic%20motion%20is%20still%20not%20prominent%2C%20can%20tell%20us%0Ahow%20a%20perturbation%20parameter%20is%20driving%20the%20system%20away%20from%20integrability.%0AAlthough%20the%20tool%20that%20we%20are%20presenting%20here%20can%20be%20used%20is%20quite%20generic%20and%0Acan%20be%20used%20in%20a%20variety%20of%20systems%2C%20our%20particular%20interest%20lies%20in%20binary%0Acompact%20object%20systems%20known%20as%20extreme%20mass%20ratio%20inspirals%20%28EMRIs%29.%20In%20an%0AEMRI%20a%20lighter%20compact%20object%2C%20like%20a%20black%20hole%20or%20a%20neutron%20star%2C%20inspirals%0Ainto%20a%20supermassive%20black%20hole%20due%20to%20gravitational%20radiation%20reaction.%20During%0Athis%20inspiral%20the%20lighter%20object%20crosses%20resonances%2C%20which%20are%20still%20not%20very%0Awell%20modeled.%20Measuring%20the%20width%20of%20resonances%20in%20EMRI%20models%20allows%20us%20to%0Aestimate%20the%20importance%20of%20each%20perturbation%20parameter%20able%20to%20drive%20the%20system%0Aaway%20from%20resonances%20and%20decide%20whether%20its%20impact%20should%20be%20included%20in%20EMRI%0Awaveform%20modeling%20or%20not.%20To%20tackle%20this%20issue%20in%20our%20study%20we%20show%20first%20that%0Arecurrence%20quantifiers%20of%20orbits%20carry%20imprints%20of%20resonant%20behavior%2C%0Aregardless%20of%20the%20system%27s%20dimensionality.%20As%20a%20next%20step%2C%20we%20apply%20a%20long%0Ashort-term%20memory%20machine%20learning%20architecture%20to%20automate%20the%20resonance%0Adetection%20procedure.%20Our%20analysis%20is%20developed%20on%20a%20simple%20standard%20map%20and%0Agradually%20we%20extend%20it%20to%20more%20complicated%20systems%20until%20finally%20we%20employ%20it%0Ain%20a%20generic%20deformed%20Kerr%20spacetime%20known%20in%20the%20literature%20as%20the%0AJohannsen-Psaltis%20spacetime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19683v1&entry.124074799=Read"},
{"title": "From Elements to Design: A Layered Approach for Automatic Graphic Design\n  Composition", "author": "Jiawei Lin and Shizhao Sun and Danqing Huang and Ting Liu and Ji Li and Jiang Bian", "abstract": "  In this work, we investigate automatic design composition from multimodal\ngraphic elements. Although recent studies have developed various generative\nmodels for graphic design, they usually face the following limitations: they\nonly focus on certain subtasks and are far from achieving the design\ncomposition task; they do not consider the hierarchical information of graphic\ndesigns during the generation process. To tackle these issues, we introduce the\nlayered design principle into Large Multimodal Models (LMMs) and propose a\nnovel approach, called LaDeCo, to accomplish this challenging task.\nSpecifically, LaDeCo first performs layer planning for a given element set,\ndividing the input elements into different semantic layers according to their\ncontents. Based on the planning results, it subsequently predicts element\nattributes that control the design composition in a layer-wise manner, and\nincludes the rendered image of previously generated layers into the context.\nWith this insightful design, LaDeCo decomposes the difficult task into smaller\nmanageable steps, making the generation process smoother and clearer. The\nexperimental results demonstrate the effectiveness of LaDeCo in design\ncomposition. Furthermore, we show that LaDeCo enables some interesting\napplications in graphic design, such as resolution adjustment, element filling,\ndesign variation, etc. In addition, it even outperforms the specialized models\nin some design subtasks without any task-specific training.\n", "link": "http://arxiv.org/abs/2412.19712v1", "date": "2024-12-27", "relevancy": 1.6884, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5642}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5632}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Elements%20to%20Design%3A%20A%20Layered%20Approach%20for%20Automatic%20Graphic%20Design%0A%20%20Composition&body=Title%3A%20From%20Elements%20to%20Design%3A%20A%20Layered%20Approach%20for%20Automatic%20Graphic%20Design%0A%20%20Composition%0AAuthor%3A%20Jiawei%20Lin%20and%20Shizhao%20Sun%20and%20Danqing%20Huang%20and%20Ting%20Liu%20and%20Ji%20Li%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20investigate%20automatic%20design%20composition%20from%20multimodal%0Agraphic%20elements.%20Although%20recent%20studies%20have%20developed%20various%20generative%0Amodels%20for%20graphic%20design%2C%20they%20usually%20face%20the%20following%20limitations%3A%20they%0Aonly%20focus%20on%20certain%20subtasks%20and%20are%20far%20from%20achieving%20the%20design%0Acomposition%20task%3B%20they%20do%20not%20consider%20the%20hierarchical%20information%20of%20graphic%0Adesigns%20during%20the%20generation%20process.%20To%20tackle%20these%20issues%2C%20we%20introduce%20the%0Alayered%20design%20principle%20into%20Large%20Multimodal%20Models%20%28LMMs%29%20and%20propose%20a%0Anovel%20approach%2C%20called%20LaDeCo%2C%20to%20accomplish%20this%20challenging%20task.%0ASpecifically%2C%20LaDeCo%20first%20performs%20layer%20planning%20for%20a%20given%20element%20set%2C%0Adividing%20the%20input%20elements%20into%20different%20semantic%20layers%20according%20to%20their%0Acontents.%20Based%20on%20the%20planning%20results%2C%20it%20subsequently%20predicts%20element%0Aattributes%20that%20control%20the%20design%20composition%20in%20a%20layer-wise%20manner%2C%20and%0Aincludes%20the%20rendered%20image%20of%20previously%20generated%20layers%20into%20the%20context.%0AWith%20this%20insightful%20design%2C%20LaDeCo%20decomposes%20the%20difficult%20task%20into%20smaller%0Amanageable%20steps%2C%20making%20the%20generation%20process%20smoother%20and%20clearer.%20The%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20LaDeCo%20in%20design%0Acomposition.%20Furthermore%2C%20we%20show%20that%20LaDeCo%20enables%20some%20interesting%0Aapplications%20in%20graphic%20design%2C%20such%20as%20resolution%20adjustment%2C%20element%20filling%2C%0Adesign%20variation%2C%20etc.%20In%20addition%2C%20it%20even%20outperforms%20the%20specialized%20models%0Ain%20some%20design%20subtasks%20without%20any%20task-specific%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Elements%2520to%2520Design%253A%2520A%2520Layered%2520Approach%2520for%2520Automatic%2520Graphic%2520Design%250A%2520%2520Composition%26entry.906535625%3DJiawei%2520Lin%2520and%2520Shizhao%2520Sun%2520and%2520Danqing%2520Huang%2520and%2520Ting%2520Liu%2520and%2520Ji%2520Li%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520investigate%2520automatic%2520design%2520composition%2520from%2520multimodal%250Agraphic%2520elements.%2520Although%2520recent%2520studies%2520have%2520developed%2520various%2520generative%250Amodels%2520for%2520graphic%2520design%252C%2520they%2520usually%2520face%2520the%2520following%2520limitations%253A%2520they%250Aonly%2520focus%2520on%2520certain%2520subtasks%2520and%2520are%2520far%2520from%2520achieving%2520the%2520design%250Acomposition%2520task%253B%2520they%2520do%2520not%2520consider%2520the%2520hierarchical%2520information%2520of%2520graphic%250Adesigns%2520during%2520the%2520generation%2520process.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520introduce%2520the%250Alayered%2520design%2520principle%2520into%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520and%2520propose%2520a%250Anovel%2520approach%252C%2520called%2520LaDeCo%252C%2520to%2520accomplish%2520this%2520challenging%2520task.%250ASpecifically%252C%2520LaDeCo%2520first%2520performs%2520layer%2520planning%2520for%2520a%2520given%2520element%2520set%252C%250Adividing%2520the%2520input%2520elements%2520into%2520different%2520semantic%2520layers%2520according%2520to%2520their%250Acontents.%2520Based%2520on%2520the%2520planning%2520results%252C%2520it%2520subsequently%2520predicts%2520element%250Aattributes%2520that%2520control%2520the%2520design%2520composition%2520in%2520a%2520layer-wise%2520manner%252C%2520and%250Aincludes%2520the%2520rendered%2520image%2520of%2520previously%2520generated%2520layers%2520into%2520the%2520context.%250AWith%2520this%2520insightful%2520design%252C%2520LaDeCo%2520decomposes%2520the%2520difficult%2520task%2520into%2520smaller%250Amanageable%2520steps%252C%2520making%2520the%2520generation%2520process%2520smoother%2520and%2520clearer.%2520The%250Aexperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520LaDeCo%2520in%2520design%250Acomposition.%2520Furthermore%252C%2520we%2520show%2520that%2520LaDeCo%2520enables%2520some%2520interesting%250Aapplications%2520in%2520graphic%2520design%252C%2520such%2520as%2520resolution%2520adjustment%252C%2520element%2520filling%252C%250Adesign%2520variation%252C%2520etc.%2520In%2520addition%252C%2520it%2520even%2520outperforms%2520the%2520specialized%2520models%250Ain%2520some%2520design%2520subtasks%2520without%2520any%2520task-specific%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Elements%20to%20Design%3A%20A%20Layered%20Approach%20for%20Automatic%20Graphic%20Design%0A%20%20Composition&entry.906535625=Jiawei%20Lin%20and%20Shizhao%20Sun%20and%20Danqing%20Huang%20and%20Ting%20Liu%20and%20Ji%20Li%20and%20Jiang%20Bian&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20automatic%20design%20composition%20from%20multimodal%0Agraphic%20elements.%20Although%20recent%20studies%20have%20developed%20various%20generative%0Amodels%20for%20graphic%20design%2C%20they%20usually%20face%20the%20following%20limitations%3A%20they%0Aonly%20focus%20on%20certain%20subtasks%20and%20are%20far%20from%20achieving%20the%20design%0Acomposition%20task%3B%20they%20do%20not%20consider%20the%20hierarchical%20information%20of%20graphic%0Adesigns%20during%20the%20generation%20process.%20To%20tackle%20these%20issues%2C%20we%20introduce%20the%0Alayered%20design%20principle%20into%20Large%20Multimodal%20Models%20%28LMMs%29%20and%20propose%20a%0Anovel%20approach%2C%20called%20LaDeCo%2C%20to%20accomplish%20this%20challenging%20task.%0ASpecifically%2C%20LaDeCo%20first%20performs%20layer%20planning%20for%20a%20given%20element%20set%2C%0Adividing%20the%20input%20elements%20into%20different%20semantic%20layers%20according%20to%20their%0Acontents.%20Based%20on%20the%20planning%20results%2C%20it%20subsequently%20predicts%20element%0Aattributes%20that%20control%20the%20design%20composition%20in%20a%20layer-wise%20manner%2C%20and%0Aincludes%20the%20rendered%20image%20of%20previously%20generated%20layers%20into%20the%20context.%0AWith%20this%20insightful%20design%2C%20LaDeCo%20decomposes%20the%20difficult%20task%20into%20smaller%0Amanageable%20steps%2C%20making%20the%20generation%20process%20smoother%20and%20clearer.%20The%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20LaDeCo%20in%20design%0Acomposition.%20Furthermore%2C%20we%20show%20that%20LaDeCo%20enables%20some%20interesting%0Aapplications%20in%20graphic%20design%2C%20such%20as%20resolution%20adjustment%2C%20element%20filling%2C%0Adesign%20variation%2C%20etc.%20In%20addition%2C%20it%20even%20outperforms%20the%20specialized%20models%0Ain%20some%20design%20subtasks%20without%20any%20task-specific%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19712v1&entry.124074799=Read"},
{"title": "Symbolic Approximations to Ricci-flat Metrics Via Extrinsic Symmetries\n  of Calabi-Yau Hypersurfaces", "author": "Viktor Mirjani\u0107 and Challenger Mishra", "abstract": "  Ever since Yau's non-constructive existence proof of Ricci-flat metrics on\nCalabi-Yau manifolds, finding their explicit construction remains a major\nobstacle to development of both string theory and algebraic geometry. Recent\ncomputational approaches employ machine learning to create novel neural\nrepresentations for approximating these metrics, offering high accuracy but\nlimited interpretability. In this paper, we analyse machine learning\napproximations to flat metrics of Fermat Calabi-Yau n-folds and some of their\none-parameter deformations in three dimensions in order to discover their new\nproperties. We formalise cases in which the flat metric has more symmetries\nthan the underlying manifold, and prove that these symmetries imply that the\nflat metric admits a surprisingly compact representation for certain choices of\ncomplex structure moduli. We show that such symmetries uniquely determine the\nflat metric on certain loci, for which we present an analytic form. We also\nincorporate our theoretical results into neural networks to achieve\nstate-of-the-art reductions in Ricci curvature for multiple Calabi-Yau\nmanifolds. We conclude by distilling the ML models to obtain for the first time\nclosed form expressions for Kahler metrics with near-zero scalar curvature.\n", "link": "http://arxiv.org/abs/2412.19778v1", "date": "2024-12-27", "relevancy": 1.6592, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4171}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4146}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbolic%20Approximations%20to%20Ricci-flat%20Metrics%20Via%20Extrinsic%20Symmetries%0A%20%20of%20Calabi-Yau%20Hypersurfaces&body=Title%3A%20Symbolic%20Approximations%20to%20Ricci-flat%20Metrics%20Via%20Extrinsic%20Symmetries%0A%20%20of%20Calabi-Yau%20Hypersurfaces%0AAuthor%3A%20Viktor%20Mirjani%C4%87%20and%20Challenger%20Mishra%0AAbstract%3A%20%20%20Ever%20since%20Yau%27s%20non-constructive%20existence%20proof%20of%20Ricci-flat%20metrics%20on%0ACalabi-Yau%20manifolds%2C%20finding%20their%20explicit%20construction%20remains%20a%20major%0Aobstacle%20to%20development%20of%20both%20string%20theory%20and%20algebraic%20geometry.%20Recent%0Acomputational%20approaches%20employ%20machine%20learning%20to%20create%20novel%20neural%0Arepresentations%20for%20approximating%20these%20metrics%2C%20offering%20high%20accuracy%20but%0Alimited%20interpretability.%20In%20this%20paper%2C%20we%20analyse%20machine%20learning%0Aapproximations%20to%20flat%20metrics%20of%20Fermat%20Calabi-Yau%20n-folds%20and%20some%20of%20their%0Aone-parameter%20deformations%20in%20three%20dimensions%20in%20order%20to%20discover%20their%20new%0Aproperties.%20We%20formalise%20cases%20in%20which%20the%20flat%20metric%20has%20more%20symmetries%0Athan%20the%20underlying%20manifold%2C%20and%20prove%20that%20these%20symmetries%20imply%20that%20the%0Aflat%20metric%20admits%20a%20surprisingly%20compact%20representation%20for%20certain%20choices%20of%0Acomplex%20structure%20moduli.%20We%20show%20that%20such%20symmetries%20uniquely%20determine%20the%0Aflat%20metric%20on%20certain%20loci%2C%20for%20which%20we%20present%20an%20analytic%20form.%20We%20also%0Aincorporate%20our%20theoretical%20results%20into%20neural%20networks%20to%20achieve%0Astate-of-the-art%20reductions%20in%20Ricci%20curvature%20for%20multiple%20Calabi-Yau%0Amanifolds.%20We%20conclude%20by%20distilling%20the%20ML%20models%20to%20obtain%20for%20the%20first%20time%0Aclosed%20form%20expressions%20for%20Kahler%20metrics%20with%20near-zero%20scalar%20curvature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbolic%2520Approximations%2520to%2520Ricci-flat%2520Metrics%2520Via%2520Extrinsic%2520Symmetries%250A%2520%2520of%2520Calabi-Yau%2520Hypersurfaces%26entry.906535625%3DViktor%2520Mirjani%25C4%2587%2520and%2520Challenger%2520Mishra%26entry.1292438233%3D%2520%2520Ever%2520since%2520Yau%2527s%2520non-constructive%2520existence%2520proof%2520of%2520Ricci-flat%2520metrics%2520on%250ACalabi-Yau%2520manifolds%252C%2520finding%2520their%2520explicit%2520construction%2520remains%2520a%2520major%250Aobstacle%2520to%2520development%2520of%2520both%2520string%2520theory%2520and%2520algebraic%2520geometry.%2520Recent%250Acomputational%2520approaches%2520employ%2520machine%2520learning%2520to%2520create%2520novel%2520neural%250Arepresentations%2520for%2520approximating%2520these%2520metrics%252C%2520offering%2520high%2520accuracy%2520but%250Alimited%2520interpretability.%2520In%2520this%2520paper%252C%2520we%2520analyse%2520machine%2520learning%250Aapproximations%2520to%2520flat%2520metrics%2520of%2520Fermat%2520Calabi-Yau%2520n-folds%2520and%2520some%2520of%2520their%250Aone-parameter%2520deformations%2520in%2520three%2520dimensions%2520in%2520order%2520to%2520discover%2520their%2520new%250Aproperties.%2520We%2520formalise%2520cases%2520in%2520which%2520the%2520flat%2520metric%2520has%2520more%2520symmetries%250Athan%2520the%2520underlying%2520manifold%252C%2520and%2520prove%2520that%2520these%2520symmetries%2520imply%2520that%2520the%250Aflat%2520metric%2520admits%2520a%2520surprisingly%2520compact%2520representation%2520for%2520certain%2520choices%2520of%250Acomplex%2520structure%2520moduli.%2520We%2520show%2520that%2520such%2520symmetries%2520uniquely%2520determine%2520the%250Aflat%2520metric%2520on%2520certain%2520loci%252C%2520for%2520which%2520we%2520present%2520an%2520analytic%2520form.%2520We%2520also%250Aincorporate%2520our%2520theoretical%2520results%2520into%2520neural%2520networks%2520to%2520achieve%250Astate-of-the-art%2520reductions%2520in%2520Ricci%2520curvature%2520for%2520multiple%2520Calabi-Yau%250Amanifolds.%2520We%2520conclude%2520by%2520distilling%2520the%2520ML%2520models%2520to%2520obtain%2520for%2520the%2520first%2520time%250Aclosed%2520form%2520expressions%2520for%2520Kahler%2520metrics%2520with%2520near-zero%2520scalar%2520curvature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20Approximations%20to%20Ricci-flat%20Metrics%20Via%20Extrinsic%20Symmetries%0A%20%20of%20Calabi-Yau%20Hypersurfaces&entry.906535625=Viktor%20Mirjani%C4%87%20and%20Challenger%20Mishra&entry.1292438233=%20%20Ever%20since%20Yau%27s%20non-constructive%20existence%20proof%20of%20Ricci-flat%20metrics%20on%0ACalabi-Yau%20manifolds%2C%20finding%20their%20explicit%20construction%20remains%20a%20major%0Aobstacle%20to%20development%20of%20both%20string%20theory%20and%20algebraic%20geometry.%20Recent%0Acomputational%20approaches%20employ%20machine%20learning%20to%20create%20novel%20neural%0Arepresentations%20for%20approximating%20these%20metrics%2C%20offering%20high%20accuracy%20but%0Alimited%20interpretability.%20In%20this%20paper%2C%20we%20analyse%20machine%20learning%0Aapproximations%20to%20flat%20metrics%20of%20Fermat%20Calabi-Yau%20n-folds%20and%20some%20of%20their%0Aone-parameter%20deformations%20in%20three%20dimensions%20in%20order%20to%20discover%20their%20new%0Aproperties.%20We%20formalise%20cases%20in%20which%20the%20flat%20metric%20has%20more%20symmetries%0Athan%20the%20underlying%20manifold%2C%20and%20prove%20that%20these%20symmetries%20imply%20that%20the%0Aflat%20metric%20admits%20a%20surprisingly%20compact%20representation%20for%20certain%20choices%20of%0Acomplex%20structure%20moduli.%20We%20show%20that%20such%20symmetries%20uniquely%20determine%20the%0Aflat%20metric%20on%20certain%20loci%2C%20for%20which%20we%20present%20an%20analytic%20form.%20We%20also%0Aincorporate%20our%20theoretical%20results%20into%20neural%20networks%20to%20achieve%0Astate-of-the-art%20reductions%20in%20Ricci%20curvature%20for%20multiple%20Calabi-Yau%0Amanifolds.%20We%20conclude%20by%20distilling%20the%20ML%20models%20to%20obtain%20for%20the%20first%20time%0Aclosed%20form%20expressions%20for%20Kahler%20metrics%20with%20near-zero%20scalar%20curvature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19778v1&entry.124074799=Read"},
{"title": "Geometric Freeze-Tag Problem", "author": "Sharareh Alipour and Kajal Baghestani and Mahdis Mirzaei and Soroush Sahraei", "abstract": "  We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02),\nwhere the objective is to activate a group of n robots, starting from a single\ninitially active robot. Robots are positioned in $\\mathbb{R}^d$, and once\nactivated, they move at a constant speed to wake up others. The goal is to\nminimize the time required to activate the last robot, known as the makespan.\nWe establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms\nin $\\mathbb{R}^2$ and $\\mathbb{R}^3$. Specifically, we improve the previous\nupper bound for $(\\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24)\nto $5.064r$. For $(\\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$,\nwhich translates to $22.52r$ for $(\\mathbb{R}^3, l_2)$. Here, $r$ denotes the\nmaximum distance of any robot from the initially active robot under the given\nnorm. To our knowledge, these are the first makespan bounds for FTP in\n$\\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots\nis not necessarily achieved when robots are equally distributed along the\nboundary in $(\\mathbb{R}^2, l_2)$. We further investigate FTP in\n$(\\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a\nboundary, providing insights into practical scenarios.\n", "link": "http://arxiv.org/abs/2412.19706v1", "date": "2024-12-27", "relevancy": 1.6469, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4407}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4212}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Freeze-Tag%20Problem&body=Title%3A%20Geometric%20Freeze-Tag%20Problem%0AAuthor%3A%20Sharareh%20Alipour%20and%20Kajal%20Baghestani%20and%20Mahdis%20Mirzaei%20and%20Soroush%20Sahraei%0AAbstract%3A%20%20%20We%20study%20the%20Freeze-Tag%20Problem%20%28FTP%29%2C%20introduced%20by%20Arkin%20et%20al.%20%28SODA%2702%29%2C%0Awhere%20the%20objective%20is%20to%20activate%20a%20group%20of%20n%20robots%2C%20starting%20from%20a%20single%0Ainitially%20active%20robot.%20Robots%20are%20positioned%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20and%20once%0Aactivated%2C%20they%20move%20at%20a%20constant%20speed%20to%20wake%20up%20others.%20The%20goal%20is%20to%0Aminimize%20the%20time%20required%20to%20activate%20the%20last%20robot%2C%20known%20as%20the%20makespan.%0AWe%20establish%20new%20upper%20bounds%20for%20the%20makespan%20under%20the%20%24l_1%24%20and%20%24l_2%24%20norms%0Ain%20%24%5Cmathbb%7BR%7D%5E2%24%20and%20%24%5Cmathbb%7BR%7D%5E3%24.%20Specifically%2C%20we%20improve%20the%20previous%0Aupper%20bound%20for%20%24%28%5Cmathbb%7BR%7D%5E2%2C%20l_2%29%24%20from%20%247.07r%24%20%28Bonichon%20et%20al.%2C%20DISC%2724%29%0Ato%20%245.064r%24.%20For%20%24%28%5Cmathbb%7BR%7D%5E3%2C%20l_1%29%24%2C%20we%20derive%20a%20makespan%20bound%20of%20%2413r%24%2C%0Awhich%20translates%20to%20%2422.52r%24%20for%20%24%28%5Cmathbb%7BR%7D%5E3%2C%20l_2%29%24.%20Here%2C%20%24r%24%20denotes%20the%0Amaximum%20distance%20of%20any%20robot%20from%20the%20initially%20active%20robot%20under%20the%20given%0Anorm.%20To%20our%20knowledge%2C%20these%20are%20the%20first%20makespan%20bounds%20for%20FTP%20in%0A%24%5Cmathbb%7BR%7D%5E3%24.%20Additionally%2C%20we%20show%20that%20the%20maximum%20makespan%20for%20%24n%24%20robots%0Ais%20not%20necessarily%20achieved%20when%20robots%20are%20equally%20distributed%20along%20the%0Aboundary%20in%20%24%28%5Cmathbb%7BR%7D%5E2%2C%20l_2%29%24.%20We%20further%20investigate%20FTP%20in%0A%24%28%5Cmathbb%7BR%7D%5E3%2C%20l_2%29%24%20for%20specific%20configurations%20where%20robots%20lie%20on%20a%0Aboundary%2C%20providing%20insights%20into%20practical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Freeze-Tag%2520Problem%26entry.906535625%3DSharareh%2520Alipour%2520and%2520Kajal%2520Baghestani%2520and%2520Mahdis%2520Mirzaei%2520and%2520Soroush%2520Sahraei%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520Freeze-Tag%2520Problem%2520%2528FTP%2529%252C%2520introduced%2520by%2520Arkin%2520et%2520al.%2520%2528SODA%252702%2529%252C%250Awhere%2520the%2520objective%2520is%2520to%2520activate%2520a%2520group%2520of%2520n%2520robots%252C%2520starting%2520from%2520a%2520single%250Ainitially%2520active%2520robot.%2520Robots%2520are%2520positioned%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520and%2520once%250Aactivated%252C%2520they%2520move%2520at%2520a%2520constant%2520speed%2520to%2520wake%2520up%2520others.%2520The%2520goal%2520is%2520to%250Aminimize%2520the%2520time%2520required%2520to%2520activate%2520the%2520last%2520robot%252C%2520known%2520as%2520the%2520makespan.%250AWe%2520establish%2520new%2520upper%2520bounds%2520for%2520the%2520makespan%2520under%2520the%2520%2524l_1%2524%2520and%2520%2524l_2%2524%2520norms%250Ain%2520%2524%255Cmathbb%257BR%257D%255E2%2524%2520and%2520%2524%255Cmathbb%257BR%257D%255E3%2524.%2520Specifically%252C%2520we%2520improve%2520the%2520previous%250Aupper%2520bound%2520for%2520%2524%2528%255Cmathbb%257BR%257D%255E2%252C%2520l_2%2529%2524%2520from%2520%25247.07r%2524%2520%2528Bonichon%2520et%2520al.%252C%2520DISC%252724%2529%250Ato%2520%25245.064r%2524.%2520For%2520%2524%2528%255Cmathbb%257BR%257D%255E3%252C%2520l_1%2529%2524%252C%2520we%2520derive%2520a%2520makespan%2520bound%2520of%2520%252413r%2524%252C%250Awhich%2520translates%2520to%2520%252422.52r%2524%2520for%2520%2524%2528%255Cmathbb%257BR%257D%255E3%252C%2520l_2%2529%2524.%2520Here%252C%2520%2524r%2524%2520denotes%2520the%250Amaximum%2520distance%2520of%2520any%2520robot%2520from%2520the%2520initially%2520active%2520robot%2520under%2520the%2520given%250Anorm.%2520To%2520our%2520knowledge%252C%2520these%2520are%2520the%2520first%2520makespan%2520bounds%2520for%2520FTP%2520in%250A%2524%255Cmathbb%257BR%257D%255E3%2524.%2520Additionally%252C%2520we%2520show%2520that%2520the%2520maximum%2520makespan%2520for%2520%2524n%2524%2520robots%250Ais%2520not%2520necessarily%2520achieved%2520when%2520robots%2520are%2520equally%2520distributed%2520along%2520the%250Aboundary%2520in%2520%2524%2528%255Cmathbb%257BR%257D%255E2%252C%2520l_2%2529%2524.%2520We%2520further%2520investigate%2520FTP%2520in%250A%2524%2528%255Cmathbb%257BR%257D%255E3%252C%2520l_2%2529%2524%2520for%2520specific%2520configurations%2520where%2520robots%2520lie%2520on%2520a%250Aboundary%252C%2520providing%2520insights%2520into%2520practical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Freeze-Tag%20Problem&entry.906535625=Sharareh%20Alipour%20and%20Kajal%20Baghestani%20and%20Mahdis%20Mirzaei%20and%20Soroush%20Sahraei&entry.1292438233=%20%20We%20study%20the%20Freeze-Tag%20Problem%20%28FTP%29%2C%20introduced%20by%20Arkin%20et%20al.%20%28SODA%2702%29%2C%0Awhere%20the%20objective%20is%20to%20activate%20a%20group%20of%20n%20robots%2C%20starting%20from%20a%20single%0Ainitially%20active%20robot.%20Robots%20are%20positioned%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20and%20once%0Aactivated%2C%20they%20move%20at%20a%20constant%20speed%20to%20wake%20up%20others.%20The%20goal%20is%20to%0Aminimize%20the%20time%20required%20to%20activate%20the%20last%20robot%2C%20known%20as%20the%20makespan.%0AWe%20establish%20new%20upper%20bounds%20for%20the%20makespan%20under%20the%20%24l_1%24%20and%20%24l_2%24%20norms%0Ain%20%24%5Cmathbb%7BR%7D%5E2%24%20and%20%24%5Cmathbb%7BR%7D%5E3%24.%20Specifically%2C%20we%20improve%20the%20previous%0Aupper%20bound%20for%20%24%28%5Cmathbb%7BR%7D%5E2%2C%20l_2%29%24%20from%20%247.07r%24%20%28Bonichon%20et%20al.%2C%20DISC%2724%29%0Ato%20%245.064r%24.%20For%20%24%28%5Cmathbb%7BR%7D%5E3%2C%20l_1%29%24%2C%20we%20derive%20a%20makespan%20bound%20of%20%2413r%24%2C%0Awhich%20translates%20to%20%2422.52r%24%20for%20%24%28%5Cmathbb%7BR%7D%5E3%2C%20l_2%29%24.%20Here%2C%20%24r%24%20denotes%20the%0Amaximum%20distance%20of%20any%20robot%20from%20the%20initially%20active%20robot%20under%20the%20given%0Anorm.%20To%20our%20knowledge%2C%20these%20are%20the%20first%20makespan%20bounds%20for%20FTP%20in%0A%24%5Cmathbb%7BR%7D%5E3%24.%20Additionally%2C%20we%20show%20that%20the%20maximum%20makespan%20for%20%24n%24%20robots%0Ais%20not%20necessarily%20achieved%20when%20robots%20are%20equally%20distributed%20along%20the%0Aboundary%20in%20%24%28%5Cmathbb%7BR%7D%5E2%2C%20l_2%29%24.%20We%20further%20investigate%20FTP%20in%0A%24%28%5Cmathbb%7BR%7D%5E3%2C%20l_2%29%24%20for%20specific%20configurations%20where%20robots%20lie%20on%20a%0Aboundary%2C%20providing%20insights%20into%20practical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19706v1&entry.124074799=Read"},
{"title": "Toward Scalable Multirobot Control: Fast Policy Learning in Distributed\n  MPC", "author": "Xinglong Zhang and Wei Pan and Cong Li and Xin Xu and Xiangke Wang and Ronghua Zhang and Dewen Hu", "abstract": "  Distributed model predictive control (DMPC) is promising in achieving optimal\ncooperative control in multirobot systems (MRS). However, real-time DMPC\nimplementation relies on numerical optimization tools to periodically calculate\nlocal control sequences online. This process is computationally demanding and\nlacks scalability for large-scale, nonlinear MRS. This article proposes a novel\ndistributed learning-based predictive control (DLPC) framework for scalable\nmultirobot control. Unlike conventional DMPC methods that calculate open-loop\ncontrol sequences, our approach centers around a computationally fast and\nefficient distributed policy learning algorithm that generates explicit\nclosed-loop DMPC policies for MRS without using numerical solvers. The policy\nlearning is executed incrementally and forward in time in each prediction\ninterval through an online distributed actor-critic implementation. The control\npolicies are successively updated in a receding-horizon manner, enabling fast\nand efficient policy learning with the closed-loop stability guarantee. The\nlearned control policies could be deployed online to MRS with varying robot\nscales, enhancing scalability and transferability for large-scale MRS.\nFurthermore, we extend our methodology to address the multirobot safe learning\nchallenge through a force field-inspired policy learning approach. We validate\nour approach's effectiveness, scalability, and efficiency through extensive\nexperiments on cooperative tasks of large-scale wheeled robots and multirotor\ndrones. Our results demonstrate the rapid learning and deployment of DMPC\npolicies for MRS with scales up to 10,000 units.\n", "link": "http://arxiv.org/abs/2412.19669v1", "date": "2024-12-27", "relevancy": 1.6396, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5662}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5494}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Scalable%20Multirobot%20Control%3A%20Fast%20Policy%20Learning%20in%20Distributed%0A%20%20MPC&body=Title%3A%20Toward%20Scalable%20Multirobot%20Control%3A%20Fast%20Policy%20Learning%20in%20Distributed%0A%20%20MPC%0AAuthor%3A%20Xinglong%20Zhang%20and%20Wei%20Pan%20and%20Cong%20Li%20and%20Xin%20Xu%20and%20Xiangke%20Wang%20and%20Ronghua%20Zhang%20and%20Dewen%20Hu%0AAbstract%3A%20%20%20Distributed%20model%20predictive%20control%20%28DMPC%29%20is%20promising%20in%20achieving%20optimal%0Acooperative%20control%20in%20multirobot%20systems%20%28MRS%29.%20However%2C%20real-time%20DMPC%0Aimplementation%20relies%20on%20numerical%20optimization%20tools%20to%20periodically%20calculate%0Alocal%20control%20sequences%20online.%20This%20process%20is%20computationally%20demanding%20and%0Alacks%20scalability%20for%20large-scale%2C%20nonlinear%20MRS.%20This%20article%20proposes%20a%20novel%0Adistributed%20learning-based%20predictive%20control%20%28DLPC%29%20framework%20for%20scalable%0Amultirobot%20control.%20Unlike%20conventional%20DMPC%20methods%20that%20calculate%20open-loop%0Acontrol%20sequences%2C%20our%20approach%20centers%20around%20a%20computationally%20fast%20and%0Aefficient%20distributed%20policy%20learning%20algorithm%20that%20generates%20explicit%0Aclosed-loop%20DMPC%20policies%20for%20MRS%20without%20using%20numerical%20solvers.%20The%20policy%0Alearning%20is%20executed%20incrementally%20and%20forward%20in%20time%20in%20each%20prediction%0Ainterval%20through%20an%20online%20distributed%20actor-critic%20implementation.%20The%20control%0Apolicies%20are%20successively%20updated%20in%20a%20receding-horizon%20manner%2C%20enabling%20fast%0Aand%20efficient%20policy%20learning%20with%20the%20closed-loop%20stability%20guarantee.%20The%0Alearned%20control%20policies%20could%20be%20deployed%20online%20to%20MRS%20with%20varying%20robot%0Ascales%2C%20enhancing%20scalability%20and%20transferability%20for%20large-scale%20MRS.%0AFurthermore%2C%20we%20extend%20our%20methodology%20to%20address%20the%20multirobot%20safe%20learning%0Achallenge%20through%20a%20force%20field-inspired%20policy%20learning%20approach.%20We%20validate%0Aour%20approach%27s%20effectiveness%2C%20scalability%2C%20and%20efficiency%20through%20extensive%0Aexperiments%20on%20cooperative%20tasks%20of%20large-scale%20wheeled%20robots%20and%20multirotor%0Adrones.%20Our%20results%20demonstrate%20the%20rapid%20learning%20and%20deployment%20of%20DMPC%0Apolicies%20for%20MRS%20with%20scales%20up%20to%2010%2C000%20units.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Scalable%2520Multirobot%2520Control%253A%2520Fast%2520Policy%2520Learning%2520in%2520Distributed%250A%2520%2520MPC%26entry.906535625%3DXinglong%2520Zhang%2520and%2520Wei%2520Pan%2520and%2520Cong%2520Li%2520and%2520Xin%2520Xu%2520and%2520Xiangke%2520Wang%2520and%2520Ronghua%2520Zhang%2520and%2520Dewen%2520Hu%26entry.1292438233%3D%2520%2520Distributed%2520model%2520predictive%2520control%2520%2528DMPC%2529%2520is%2520promising%2520in%2520achieving%2520optimal%250Acooperative%2520control%2520in%2520multirobot%2520systems%2520%2528MRS%2529.%2520However%252C%2520real-time%2520DMPC%250Aimplementation%2520relies%2520on%2520numerical%2520optimization%2520tools%2520to%2520periodically%2520calculate%250Alocal%2520control%2520sequences%2520online.%2520This%2520process%2520is%2520computationally%2520demanding%2520and%250Alacks%2520scalability%2520for%2520large-scale%252C%2520nonlinear%2520MRS.%2520This%2520article%2520proposes%2520a%2520novel%250Adistributed%2520learning-based%2520predictive%2520control%2520%2528DLPC%2529%2520framework%2520for%2520scalable%250Amultirobot%2520control.%2520Unlike%2520conventional%2520DMPC%2520methods%2520that%2520calculate%2520open-loop%250Acontrol%2520sequences%252C%2520our%2520approach%2520centers%2520around%2520a%2520computationally%2520fast%2520and%250Aefficient%2520distributed%2520policy%2520learning%2520algorithm%2520that%2520generates%2520explicit%250Aclosed-loop%2520DMPC%2520policies%2520for%2520MRS%2520without%2520using%2520numerical%2520solvers.%2520The%2520policy%250Alearning%2520is%2520executed%2520incrementally%2520and%2520forward%2520in%2520time%2520in%2520each%2520prediction%250Ainterval%2520through%2520an%2520online%2520distributed%2520actor-critic%2520implementation.%2520The%2520control%250Apolicies%2520are%2520successively%2520updated%2520in%2520a%2520receding-horizon%2520manner%252C%2520enabling%2520fast%250Aand%2520efficient%2520policy%2520learning%2520with%2520the%2520closed-loop%2520stability%2520guarantee.%2520The%250Alearned%2520control%2520policies%2520could%2520be%2520deployed%2520online%2520to%2520MRS%2520with%2520varying%2520robot%250Ascales%252C%2520enhancing%2520scalability%2520and%2520transferability%2520for%2520large-scale%2520MRS.%250AFurthermore%252C%2520we%2520extend%2520our%2520methodology%2520to%2520address%2520the%2520multirobot%2520safe%2520learning%250Achallenge%2520through%2520a%2520force%2520field-inspired%2520policy%2520learning%2520approach.%2520We%2520validate%250Aour%2520approach%2527s%2520effectiveness%252C%2520scalability%252C%2520and%2520efficiency%2520through%2520extensive%250Aexperiments%2520on%2520cooperative%2520tasks%2520of%2520large-scale%2520wheeled%2520robots%2520and%2520multirotor%250Adrones.%2520Our%2520results%2520demonstrate%2520the%2520rapid%2520learning%2520and%2520deployment%2520of%2520DMPC%250Apolicies%2520for%2520MRS%2520with%2520scales%2520up%2520to%252010%252C000%2520units.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Scalable%20Multirobot%20Control%3A%20Fast%20Policy%20Learning%20in%20Distributed%0A%20%20MPC&entry.906535625=Xinglong%20Zhang%20and%20Wei%20Pan%20and%20Cong%20Li%20and%20Xin%20Xu%20and%20Xiangke%20Wang%20and%20Ronghua%20Zhang%20and%20Dewen%20Hu&entry.1292438233=%20%20Distributed%20model%20predictive%20control%20%28DMPC%29%20is%20promising%20in%20achieving%20optimal%0Acooperative%20control%20in%20multirobot%20systems%20%28MRS%29.%20However%2C%20real-time%20DMPC%0Aimplementation%20relies%20on%20numerical%20optimization%20tools%20to%20periodically%20calculate%0Alocal%20control%20sequences%20online.%20This%20process%20is%20computationally%20demanding%20and%0Alacks%20scalability%20for%20large-scale%2C%20nonlinear%20MRS.%20This%20article%20proposes%20a%20novel%0Adistributed%20learning-based%20predictive%20control%20%28DLPC%29%20framework%20for%20scalable%0Amultirobot%20control.%20Unlike%20conventional%20DMPC%20methods%20that%20calculate%20open-loop%0Acontrol%20sequences%2C%20our%20approach%20centers%20around%20a%20computationally%20fast%20and%0Aefficient%20distributed%20policy%20learning%20algorithm%20that%20generates%20explicit%0Aclosed-loop%20DMPC%20policies%20for%20MRS%20without%20using%20numerical%20solvers.%20The%20policy%0Alearning%20is%20executed%20incrementally%20and%20forward%20in%20time%20in%20each%20prediction%0Ainterval%20through%20an%20online%20distributed%20actor-critic%20implementation.%20The%20control%0Apolicies%20are%20successively%20updated%20in%20a%20receding-horizon%20manner%2C%20enabling%20fast%0Aand%20efficient%20policy%20learning%20with%20the%20closed-loop%20stability%20guarantee.%20The%0Alearned%20control%20policies%20could%20be%20deployed%20online%20to%20MRS%20with%20varying%20robot%0Ascales%2C%20enhancing%20scalability%20and%20transferability%20for%20large-scale%20MRS.%0AFurthermore%2C%20we%20extend%20our%20methodology%20to%20address%20the%20multirobot%20safe%20learning%0Achallenge%20through%20a%20force%20field-inspired%20policy%20learning%20approach.%20We%20validate%0Aour%20approach%27s%20effectiveness%2C%20scalability%2C%20and%20efficiency%20through%20extensive%0Aexperiments%20on%20cooperative%20tasks%20of%20large-scale%20wheeled%20robots%20and%20multirotor%0Adrones.%20Our%20results%20demonstrate%20the%20rapid%20learning%20and%20deployment%20of%20DMPC%0Apolicies%20for%20MRS%20with%20scales%20up%20to%2010%2C000%20units.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19669v1&entry.124074799=Read"},
{"title": "From Ceilings to Walls: Universal Dynamic Perching of Small Aerial\n  Robots on Surfaces with Variable Orientations", "author": "Bryan Habas and Aaron Brown and Donghyeon Lee and Mitchell Goldman and Bo Cheng", "abstract": "  This work demonstrates universal dynamic perching capabilities for quadrotors\nof various sizes and on surfaces with different orientations. By employing a\nnon-dimensionalization framework and deep reinforcement learning, we\nsystematically assessed how robot size and surface orientation affect landing\ncapabilities. We hypothesized that maintaining geometric proportions across\ndifferent robot scales ensures consistent perching behavior, which was\nvalidated in both simulation and experimental tests. Additionally, we\ninvestigated the effects of joint stiffness and damping in the landing gear on\nperching behaviors and performance. While joint stiffness had minimal impact,\njoint damping ratios influenced landing success under vertical approaching\nconditions. The study also identified a critical velocity threshold necessary\nfor successful perching, determined by the robot's maneuverability and leg\ngeometry. Overall, this research advances robotic perching capabilities,\noffering insights into the role of mechanical design and scaling effects, and\nlays the groundwork for future drone autonomy and operational efficiency in\nunstructured environments.\n", "link": "http://arxiv.org/abs/2412.19765v1", "date": "2024-12-27", "relevancy": 1.6115, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5668}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5316}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Ceilings%20to%20Walls%3A%20Universal%20Dynamic%20Perching%20of%20Small%20Aerial%0A%20%20Robots%20on%20Surfaces%20with%20Variable%20Orientations&body=Title%3A%20From%20Ceilings%20to%20Walls%3A%20Universal%20Dynamic%20Perching%20of%20Small%20Aerial%0A%20%20Robots%20on%20Surfaces%20with%20Variable%20Orientations%0AAuthor%3A%20Bryan%20Habas%20and%20Aaron%20Brown%20and%20Donghyeon%20Lee%20and%20Mitchell%20Goldman%20and%20Bo%20Cheng%0AAbstract%3A%20%20%20This%20work%20demonstrates%20universal%20dynamic%20perching%20capabilities%20for%20quadrotors%0Aof%20various%20sizes%20and%20on%20surfaces%20with%20different%20orientations.%20By%20employing%20a%0Anon-dimensionalization%20framework%20and%20deep%20reinforcement%20learning%2C%20we%0Asystematically%20assessed%20how%20robot%20size%20and%20surface%20orientation%20affect%20landing%0Acapabilities.%20We%20hypothesized%20that%20maintaining%20geometric%20proportions%20across%0Adifferent%20robot%20scales%20ensures%20consistent%20perching%20behavior%2C%20which%20was%0Avalidated%20in%20both%20simulation%20and%20experimental%20tests.%20Additionally%2C%20we%0Ainvestigated%20the%20effects%20of%20joint%20stiffness%20and%20damping%20in%20the%20landing%20gear%20on%0Aperching%20behaviors%20and%20performance.%20While%20joint%20stiffness%20had%20minimal%20impact%2C%0Ajoint%20damping%20ratios%20influenced%20landing%20success%20under%20vertical%20approaching%0Aconditions.%20The%20study%20also%20identified%20a%20critical%20velocity%20threshold%20necessary%0Afor%20successful%20perching%2C%20determined%20by%20the%20robot%27s%20maneuverability%20and%20leg%0Ageometry.%20Overall%2C%20this%20research%20advances%20robotic%20perching%20capabilities%2C%0Aoffering%20insights%20into%20the%20role%20of%20mechanical%20design%20and%20scaling%20effects%2C%20and%0Alays%20the%20groundwork%20for%20future%20drone%20autonomy%20and%20operational%20efficiency%20in%0Aunstructured%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Ceilings%2520to%2520Walls%253A%2520Universal%2520Dynamic%2520Perching%2520of%2520Small%2520Aerial%250A%2520%2520Robots%2520on%2520Surfaces%2520with%2520Variable%2520Orientations%26entry.906535625%3DBryan%2520Habas%2520and%2520Aaron%2520Brown%2520and%2520Donghyeon%2520Lee%2520and%2520Mitchell%2520Goldman%2520and%2520Bo%2520Cheng%26entry.1292438233%3D%2520%2520This%2520work%2520demonstrates%2520universal%2520dynamic%2520perching%2520capabilities%2520for%2520quadrotors%250Aof%2520various%2520sizes%2520and%2520on%2520surfaces%2520with%2520different%2520orientations.%2520By%2520employing%2520a%250Anon-dimensionalization%2520framework%2520and%2520deep%2520reinforcement%2520learning%252C%2520we%250Asystematically%2520assessed%2520how%2520robot%2520size%2520and%2520surface%2520orientation%2520affect%2520landing%250Acapabilities.%2520We%2520hypothesized%2520that%2520maintaining%2520geometric%2520proportions%2520across%250Adifferent%2520robot%2520scales%2520ensures%2520consistent%2520perching%2520behavior%252C%2520which%2520was%250Avalidated%2520in%2520both%2520simulation%2520and%2520experimental%2520tests.%2520Additionally%252C%2520we%250Ainvestigated%2520the%2520effects%2520of%2520joint%2520stiffness%2520and%2520damping%2520in%2520the%2520landing%2520gear%2520on%250Aperching%2520behaviors%2520and%2520performance.%2520While%2520joint%2520stiffness%2520had%2520minimal%2520impact%252C%250Ajoint%2520damping%2520ratios%2520influenced%2520landing%2520success%2520under%2520vertical%2520approaching%250Aconditions.%2520The%2520study%2520also%2520identified%2520a%2520critical%2520velocity%2520threshold%2520necessary%250Afor%2520successful%2520perching%252C%2520determined%2520by%2520the%2520robot%2527s%2520maneuverability%2520and%2520leg%250Ageometry.%2520Overall%252C%2520this%2520research%2520advances%2520robotic%2520perching%2520capabilities%252C%250Aoffering%2520insights%2520into%2520the%2520role%2520of%2520mechanical%2520design%2520and%2520scaling%2520effects%252C%2520and%250Alays%2520the%2520groundwork%2520for%2520future%2520drone%2520autonomy%2520and%2520operational%2520efficiency%2520in%250Aunstructured%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Ceilings%20to%20Walls%3A%20Universal%20Dynamic%20Perching%20of%20Small%20Aerial%0A%20%20Robots%20on%20Surfaces%20with%20Variable%20Orientations&entry.906535625=Bryan%20Habas%20and%20Aaron%20Brown%20and%20Donghyeon%20Lee%20and%20Mitchell%20Goldman%20and%20Bo%20Cheng&entry.1292438233=%20%20This%20work%20demonstrates%20universal%20dynamic%20perching%20capabilities%20for%20quadrotors%0Aof%20various%20sizes%20and%20on%20surfaces%20with%20different%20orientations.%20By%20employing%20a%0Anon-dimensionalization%20framework%20and%20deep%20reinforcement%20learning%2C%20we%0Asystematically%20assessed%20how%20robot%20size%20and%20surface%20orientation%20affect%20landing%0Acapabilities.%20We%20hypothesized%20that%20maintaining%20geometric%20proportions%20across%0Adifferent%20robot%20scales%20ensures%20consistent%20perching%20behavior%2C%20which%20was%0Avalidated%20in%20both%20simulation%20and%20experimental%20tests.%20Additionally%2C%20we%0Ainvestigated%20the%20effects%20of%20joint%20stiffness%20and%20damping%20in%20the%20landing%20gear%20on%0Aperching%20behaviors%20and%20performance.%20While%20joint%20stiffness%20had%20minimal%20impact%2C%0Ajoint%20damping%20ratios%20influenced%20landing%20success%20under%20vertical%20approaching%0Aconditions.%20The%20study%20also%20identified%20a%20critical%20velocity%20threshold%20necessary%0Afor%20successful%20perching%2C%20determined%20by%20the%20robot%27s%20maneuverability%20and%20leg%0Ageometry.%20Overall%2C%20this%20research%20advances%20robotic%20perching%20capabilities%2C%0Aoffering%20insights%20into%20the%20role%20of%20mechanical%20design%20and%20scaling%20effects%2C%20and%0Alays%20the%20groundwork%20for%20future%20drone%20autonomy%20and%20operational%20efficiency%20in%0Aunstructured%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19765v1&entry.124074799=Read"},
{"title": "Graph-attention-based Casual Discovery with Trust Region-navigated\n  Clipping Policy Optimization", "author": "Shixuan Liu and Yanghe Feng and Keyu Wu and Guangquan Cheng and Jincai Huang and Zhong Liu", "abstract": "  In many domains of empirical sciences, discovering the causal structure\nwithin variables remains an indispensable task. Recently, to tackle with\nunoriented edges or latent assumptions violation suffered by conventional\nmethods, researchers formulated a reinforcement learning (RL) procedure for\ncausal discovery, and equipped REINFORCE algorithm to search for the\nbest-rewarded directed acyclic graph. The two keys to the overall performance\nof the procedure are the robustness of RL methods and the efficient encoding of\nvariables. However, on the one hand, REINFORCE is prone to local convergence\nand unstable performance during training. Neither trust region policy\noptimization, being computationally-expensive, nor proximal policy optimization\n(PPO), suffering from aggregate constraint deviation, is decent alternative for\ncombinatory optimization problems with considerable individual subactions. We\npropose a trust region-navigated clipping policy optimization method for causal\ndiscovery that guarantees both better search efficiency and steadiness in\npolicy optimization, in comparison with REINFORCE, PPO and our prioritized\nsampling-guided REINFORCE implementation. On the other hand, to boost the\nefficient encoding of variables, we propose a refined graph attention encoder\ncalled SDGAT that can grasp more feature information without priori\nneighbourhood information. With these improvements, the proposed method\noutperforms former RL method in both synthetic and benchmark datasets in terms\nof output results and optimization robustness.\n", "link": "http://arxiv.org/abs/2412.19578v1", "date": "2024-12-27", "relevancy": 1.56, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5536}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5135}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-attention-based%20Casual%20Discovery%20with%20Trust%20Region-navigated%0A%20%20Clipping%20Policy%20Optimization&body=Title%3A%20Graph-attention-based%20Casual%20Discovery%20with%20Trust%20Region-navigated%0A%20%20Clipping%20Policy%20Optimization%0AAuthor%3A%20Shixuan%20Liu%20and%20Yanghe%20Feng%20and%20Keyu%20Wu%20and%20Guangquan%20Cheng%20and%20Jincai%20Huang%20and%20Zhong%20Liu%0AAbstract%3A%20%20%20In%20many%20domains%20of%20empirical%20sciences%2C%20discovering%20the%20causal%20structure%0Awithin%20variables%20remains%20an%20indispensable%20task.%20Recently%2C%20to%20tackle%20with%0Aunoriented%20edges%20or%20latent%20assumptions%20violation%20suffered%20by%20conventional%0Amethods%2C%20researchers%20formulated%20a%20reinforcement%20learning%20%28RL%29%20procedure%20for%0Acausal%20discovery%2C%20and%20equipped%20REINFORCE%20algorithm%20to%20search%20for%20the%0Abest-rewarded%20directed%20acyclic%20graph.%20The%20two%20keys%20to%20the%20overall%20performance%0Aof%20the%20procedure%20are%20the%20robustness%20of%20RL%20methods%20and%20the%20efficient%20encoding%20of%0Avariables.%20However%2C%20on%20the%20one%20hand%2C%20REINFORCE%20is%20prone%20to%20local%20convergence%0Aand%20unstable%20performance%20during%20training.%20Neither%20trust%20region%20policy%0Aoptimization%2C%20being%20computationally-expensive%2C%20nor%20proximal%20policy%20optimization%0A%28PPO%29%2C%20suffering%20from%20aggregate%20constraint%20deviation%2C%20is%20decent%20alternative%20for%0Acombinatory%20optimization%20problems%20with%20considerable%20individual%20subactions.%20We%0Apropose%20a%20trust%20region-navigated%20clipping%20policy%20optimization%20method%20for%20causal%0Adiscovery%20that%20guarantees%20both%20better%20search%20efficiency%20and%20steadiness%20in%0Apolicy%20optimization%2C%20in%20comparison%20with%20REINFORCE%2C%20PPO%20and%20our%20prioritized%0Asampling-guided%20REINFORCE%20implementation.%20On%20the%20other%20hand%2C%20to%20boost%20the%0Aefficient%20encoding%20of%20variables%2C%20we%20propose%20a%20refined%20graph%20attention%20encoder%0Acalled%20SDGAT%20that%20can%20grasp%20more%20feature%20information%20without%20priori%0Aneighbourhood%20information.%20With%20these%20improvements%2C%20the%20proposed%20method%0Aoutperforms%20former%20RL%20method%20in%20both%20synthetic%20and%20benchmark%20datasets%20in%20terms%0Aof%20output%20results%20and%20optimization%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-attention-based%2520Casual%2520Discovery%2520with%2520Trust%2520Region-navigated%250A%2520%2520Clipping%2520Policy%2520Optimization%26entry.906535625%3DShixuan%2520Liu%2520and%2520Yanghe%2520Feng%2520and%2520Keyu%2520Wu%2520and%2520Guangquan%2520Cheng%2520and%2520Jincai%2520Huang%2520and%2520Zhong%2520Liu%26entry.1292438233%3D%2520%2520In%2520many%2520domains%2520of%2520empirical%2520sciences%252C%2520discovering%2520the%2520causal%2520structure%250Awithin%2520variables%2520remains%2520an%2520indispensable%2520task.%2520Recently%252C%2520to%2520tackle%2520with%250Aunoriented%2520edges%2520or%2520latent%2520assumptions%2520violation%2520suffered%2520by%2520conventional%250Amethods%252C%2520researchers%2520formulated%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520procedure%2520for%250Acausal%2520discovery%252C%2520and%2520equipped%2520REINFORCE%2520algorithm%2520to%2520search%2520for%2520the%250Abest-rewarded%2520directed%2520acyclic%2520graph.%2520The%2520two%2520keys%2520to%2520the%2520overall%2520performance%250Aof%2520the%2520procedure%2520are%2520the%2520robustness%2520of%2520RL%2520methods%2520and%2520the%2520efficient%2520encoding%2520of%250Avariables.%2520However%252C%2520on%2520the%2520one%2520hand%252C%2520REINFORCE%2520is%2520prone%2520to%2520local%2520convergence%250Aand%2520unstable%2520performance%2520during%2520training.%2520Neither%2520trust%2520region%2520policy%250Aoptimization%252C%2520being%2520computationally-expensive%252C%2520nor%2520proximal%2520policy%2520optimization%250A%2528PPO%2529%252C%2520suffering%2520from%2520aggregate%2520constraint%2520deviation%252C%2520is%2520decent%2520alternative%2520for%250Acombinatory%2520optimization%2520problems%2520with%2520considerable%2520individual%2520subactions.%2520We%250Apropose%2520a%2520trust%2520region-navigated%2520clipping%2520policy%2520optimization%2520method%2520for%2520causal%250Adiscovery%2520that%2520guarantees%2520both%2520better%2520search%2520efficiency%2520and%2520steadiness%2520in%250Apolicy%2520optimization%252C%2520in%2520comparison%2520with%2520REINFORCE%252C%2520PPO%2520and%2520our%2520prioritized%250Asampling-guided%2520REINFORCE%2520implementation.%2520On%2520the%2520other%2520hand%252C%2520to%2520boost%2520the%250Aefficient%2520encoding%2520of%2520variables%252C%2520we%2520propose%2520a%2520refined%2520graph%2520attention%2520encoder%250Acalled%2520SDGAT%2520that%2520can%2520grasp%2520more%2520feature%2520information%2520without%2520priori%250Aneighbourhood%2520information.%2520With%2520these%2520improvements%252C%2520the%2520proposed%2520method%250Aoutperforms%2520former%2520RL%2520method%2520in%2520both%2520synthetic%2520and%2520benchmark%2520datasets%2520in%2520terms%250Aof%2520output%2520results%2520and%2520optimization%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-attention-based%20Casual%20Discovery%20with%20Trust%20Region-navigated%0A%20%20Clipping%20Policy%20Optimization&entry.906535625=Shixuan%20Liu%20and%20Yanghe%20Feng%20and%20Keyu%20Wu%20and%20Guangquan%20Cheng%20and%20Jincai%20Huang%20and%20Zhong%20Liu&entry.1292438233=%20%20In%20many%20domains%20of%20empirical%20sciences%2C%20discovering%20the%20causal%20structure%0Awithin%20variables%20remains%20an%20indispensable%20task.%20Recently%2C%20to%20tackle%20with%0Aunoriented%20edges%20or%20latent%20assumptions%20violation%20suffered%20by%20conventional%0Amethods%2C%20researchers%20formulated%20a%20reinforcement%20learning%20%28RL%29%20procedure%20for%0Acausal%20discovery%2C%20and%20equipped%20REINFORCE%20algorithm%20to%20search%20for%20the%0Abest-rewarded%20directed%20acyclic%20graph.%20The%20two%20keys%20to%20the%20overall%20performance%0Aof%20the%20procedure%20are%20the%20robustness%20of%20RL%20methods%20and%20the%20efficient%20encoding%20of%0Avariables.%20However%2C%20on%20the%20one%20hand%2C%20REINFORCE%20is%20prone%20to%20local%20convergence%0Aand%20unstable%20performance%20during%20training.%20Neither%20trust%20region%20policy%0Aoptimization%2C%20being%20computationally-expensive%2C%20nor%20proximal%20policy%20optimization%0A%28PPO%29%2C%20suffering%20from%20aggregate%20constraint%20deviation%2C%20is%20decent%20alternative%20for%0Acombinatory%20optimization%20problems%20with%20considerable%20individual%20subactions.%20We%0Apropose%20a%20trust%20region-navigated%20clipping%20policy%20optimization%20method%20for%20causal%0Adiscovery%20that%20guarantees%20both%20better%20search%20efficiency%20and%20steadiness%20in%0Apolicy%20optimization%2C%20in%20comparison%20with%20REINFORCE%2C%20PPO%20and%20our%20prioritized%0Asampling-guided%20REINFORCE%20implementation.%20On%20the%20other%20hand%2C%20to%20boost%20the%0Aefficient%20encoding%20of%20variables%2C%20we%20propose%20a%20refined%20graph%20attention%20encoder%0Acalled%20SDGAT%20that%20can%20grasp%20more%20feature%20information%20without%20priori%0Aneighbourhood%20information.%20With%20these%20improvements%2C%20the%20proposed%20method%0Aoutperforms%20former%20RL%20method%20in%20both%20synthetic%20and%20benchmark%20datasets%20in%20terms%0Aof%20output%20results%20and%20optimization%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19578v1&entry.124074799=Read"},
{"title": "Safe Interval Randomized Path Planing For Manipulators", "author": "Nuraddin Kerimov and Aleksandr Onegin and Konstantin Yakovlev", "abstract": "  Planning safe paths in 3D workspace for high DoF robotic systems, such as\nmanipulators, is a challenging problem, especially when the environment is\npopulated with the dynamic obstacles that need to be avoided. In this case the\ntime dimension should be taken into account that further increases the\ncomplexity of planning. To mitigate this issue we suggest to combine\nsafe-interval path planning (a prominent technique in heuristic search) with\nthe randomized planning, specifically, with the bidirectional rapidly-exploring\nrandom trees (RRT-Connect) - a fast and efficient algorithm for\nhigh-dimensional planning. Leveraging a dedicated technique of fast computation\nof the safe intervals we end up with an efficient planner dubbed SI-RRT. We\ncompare it with the state of the art and show that SI-RRT consistently\noutperforms the competitors both in runtime and solution cost.\n  Our implementation of SI-RRT is publicly available at\nhttps://github.com/PathPlanning/ManipulationPlanning-SI-RRT\n", "link": "http://arxiv.org/abs/2412.19567v1", "date": "2024-12-27", "relevancy": 1.5529, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5804}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5283}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Interval%20Randomized%20Path%20Planing%20For%20Manipulators&body=Title%3A%20Safe%20Interval%20Randomized%20Path%20Planing%20For%20Manipulators%0AAuthor%3A%20Nuraddin%20Kerimov%20and%20Aleksandr%20Onegin%20and%20Konstantin%20Yakovlev%0AAbstract%3A%20%20%20Planning%20safe%20paths%20in%203D%20workspace%20for%20high%20DoF%20robotic%20systems%2C%20such%20as%0Amanipulators%2C%20is%20a%20challenging%20problem%2C%20especially%20when%20the%20environment%20is%0Apopulated%20with%20the%20dynamic%20obstacles%20that%20need%20to%20be%20avoided.%20In%20this%20case%20the%0Atime%20dimension%20should%20be%20taken%20into%20account%20that%20further%20increases%20the%0Acomplexity%20of%20planning.%20To%20mitigate%20this%20issue%20we%20suggest%20to%20combine%0Asafe-interval%20path%20planning%20%28a%20prominent%20technique%20in%20heuristic%20search%29%20with%0Athe%20randomized%20planning%2C%20specifically%2C%20with%20the%20bidirectional%20rapidly-exploring%0Arandom%20trees%20%28RRT-Connect%29%20-%20a%20fast%20and%20efficient%20algorithm%20for%0Ahigh-dimensional%20planning.%20Leveraging%20a%20dedicated%20technique%20of%20fast%20computation%0Aof%20the%20safe%20intervals%20we%20end%20up%20with%20an%20efficient%20planner%20dubbed%20SI-RRT.%20We%0Acompare%20it%20with%20the%20state%20of%20the%20art%20and%20show%20that%20SI-RRT%20consistently%0Aoutperforms%20the%20competitors%20both%20in%20runtime%20and%20solution%20cost.%0A%20%20Our%20implementation%20of%20SI-RRT%20is%20publicly%20available%20at%0Ahttps%3A//github.com/PathPlanning/ManipulationPlanning-SI-RRT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Interval%2520Randomized%2520Path%2520Planing%2520For%2520Manipulators%26entry.906535625%3DNuraddin%2520Kerimov%2520and%2520Aleksandr%2520Onegin%2520and%2520Konstantin%2520Yakovlev%26entry.1292438233%3D%2520%2520Planning%2520safe%2520paths%2520in%25203D%2520workspace%2520for%2520high%2520DoF%2520robotic%2520systems%252C%2520such%2520as%250Amanipulators%252C%2520is%2520a%2520challenging%2520problem%252C%2520especially%2520when%2520the%2520environment%2520is%250Apopulated%2520with%2520the%2520dynamic%2520obstacles%2520that%2520need%2520to%2520be%2520avoided.%2520In%2520this%2520case%2520the%250Atime%2520dimension%2520should%2520be%2520taken%2520into%2520account%2520that%2520further%2520increases%2520the%250Acomplexity%2520of%2520planning.%2520To%2520mitigate%2520this%2520issue%2520we%2520suggest%2520to%2520combine%250Asafe-interval%2520path%2520planning%2520%2528a%2520prominent%2520technique%2520in%2520heuristic%2520search%2529%2520with%250Athe%2520randomized%2520planning%252C%2520specifically%252C%2520with%2520the%2520bidirectional%2520rapidly-exploring%250Arandom%2520trees%2520%2528RRT-Connect%2529%2520-%2520a%2520fast%2520and%2520efficient%2520algorithm%2520for%250Ahigh-dimensional%2520planning.%2520Leveraging%2520a%2520dedicated%2520technique%2520of%2520fast%2520computation%250Aof%2520the%2520safe%2520intervals%2520we%2520end%2520up%2520with%2520an%2520efficient%2520planner%2520dubbed%2520SI-RRT.%2520We%250Acompare%2520it%2520with%2520the%2520state%2520of%2520the%2520art%2520and%2520show%2520that%2520SI-RRT%2520consistently%250Aoutperforms%2520the%2520competitors%2520both%2520in%2520runtime%2520and%2520solution%2520cost.%250A%2520%2520Our%2520implementation%2520of%2520SI-RRT%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/PathPlanning/ManipulationPlanning-SI-RRT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Interval%20Randomized%20Path%20Planing%20For%20Manipulators&entry.906535625=Nuraddin%20Kerimov%20and%20Aleksandr%20Onegin%20and%20Konstantin%20Yakovlev&entry.1292438233=%20%20Planning%20safe%20paths%20in%203D%20workspace%20for%20high%20DoF%20robotic%20systems%2C%20such%20as%0Amanipulators%2C%20is%20a%20challenging%20problem%2C%20especially%20when%20the%20environment%20is%0Apopulated%20with%20the%20dynamic%20obstacles%20that%20need%20to%20be%20avoided.%20In%20this%20case%20the%0Atime%20dimension%20should%20be%20taken%20into%20account%20that%20further%20increases%20the%0Acomplexity%20of%20planning.%20To%20mitigate%20this%20issue%20we%20suggest%20to%20combine%0Asafe-interval%20path%20planning%20%28a%20prominent%20technique%20in%20heuristic%20search%29%20with%0Athe%20randomized%20planning%2C%20specifically%2C%20with%20the%20bidirectional%20rapidly-exploring%0Arandom%20trees%20%28RRT-Connect%29%20-%20a%20fast%20and%20efficient%20algorithm%20for%0Ahigh-dimensional%20planning.%20Leveraging%20a%20dedicated%20technique%20of%20fast%20computation%0Aof%20the%20safe%20intervals%20we%20end%20up%20with%20an%20efficient%20planner%20dubbed%20SI-RRT.%20We%0Acompare%20it%20with%20the%20state%20of%20the%20art%20and%20show%20that%20SI-RRT%20consistently%0Aoutperforms%20the%20competitors%20both%20in%20runtime%20and%20solution%20cost.%0A%20%20Our%20implementation%20of%20SI-RRT%20is%20publicly%20available%20at%0Ahttps%3A//github.com/PathPlanning/ManipulationPlanning-SI-RRT%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19567v1&entry.124074799=Read"},
{"title": "Reasoning over Uncertain Text by Generative Large Language Models", "author": "Aliakbar Nafar and Kristen Brent Venable and Parisa Kordjamshidi", "abstract": "  This paper considers the challenges Large Language Models (LLMs) face when\nreasoning over text that includes information involving uncertainty explicitly\nquantified via probability values. This type of reasoning is relevant to a\nvariety of contexts ranging from everyday conversations to medical\ndecision-making. Despite improvements in the mathematical reasoning\ncapabilities of LLMs, they still exhibit significant difficulties when it comes\nto probabilistic reasoning. To deal with this problem, we introduce the\nBayesian Linguistic Inference Dataset (BLInD), a new dataset specifically\ndesigned to test the probabilistic reasoning capabilities of LLMs. We use BLInD\nto find out the limitations of LLMs for tasks involving probabilistic\nreasoning. In addition, we present several prompting strategies that map the\nproblem to different formal representations, including Python code,\nprobabilistic algorithms, and probabilistic logical programming. We conclude by\nproviding an evaluation of our methods on BLInD and an adaptation of a causal\nreasoning question-answering dataset. Our empirical results highlight the\neffectiveness of our proposed strategies for multiple LLMs.\n", "link": "http://arxiv.org/abs/2402.09614v3", "date": "2024-12-27", "relevancy": 1.5373, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5395}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5142}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20over%20Uncertain%20Text%20by%20Generative%20Large%20Language%20Models&body=Title%3A%20Reasoning%20over%20Uncertain%20Text%20by%20Generative%20Large%20Language%20Models%0AAuthor%3A%20Aliakbar%20Nafar%20and%20Kristen%20Brent%20Venable%20and%20Parisa%20Kordjamshidi%0AAbstract%3A%20%20%20This%20paper%20considers%20the%20challenges%20Large%20Language%20Models%20%28LLMs%29%20face%20when%0Areasoning%20over%20text%20that%20includes%20information%20involving%20uncertainty%20explicitly%0Aquantified%20via%20probability%20values.%20This%20type%20of%20reasoning%20is%20relevant%20to%20a%0Avariety%20of%20contexts%20ranging%20from%20everyday%20conversations%20to%20medical%0Adecision-making.%20Despite%20improvements%20in%20the%20mathematical%20reasoning%0Acapabilities%20of%20LLMs%2C%20they%20still%20exhibit%20significant%20difficulties%20when%20it%20comes%0Ato%20probabilistic%20reasoning.%20To%20deal%20with%20this%20problem%2C%20we%20introduce%20the%0ABayesian%20Linguistic%20Inference%20Dataset%20%28BLInD%29%2C%20a%20new%20dataset%20specifically%0Adesigned%20to%20test%20the%20probabilistic%20reasoning%20capabilities%20of%20LLMs.%20We%20use%20BLInD%0Ato%20find%20out%20the%20limitations%20of%20LLMs%20for%20tasks%20involving%20probabilistic%0Areasoning.%20In%20addition%2C%20we%20present%20several%20prompting%20strategies%20that%20map%20the%0Aproblem%20to%20different%20formal%20representations%2C%20including%20Python%20code%2C%0Aprobabilistic%20algorithms%2C%20and%20probabilistic%20logical%20programming.%20We%20conclude%20by%0Aproviding%20an%20evaluation%20of%20our%20methods%20on%20BLInD%20and%20an%20adaptation%20of%20a%20causal%0Areasoning%20question-answering%20dataset.%20Our%20empirical%20results%20highlight%20the%0Aeffectiveness%20of%20our%20proposed%20strategies%20for%20multiple%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09614v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520over%2520Uncertain%2520Text%2520by%2520Generative%2520Large%2520Language%2520Models%26entry.906535625%3DAliakbar%2520Nafar%2520and%2520Kristen%2520Brent%2520Venable%2520and%2520Parisa%2520Kordjamshidi%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520the%2520challenges%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520when%250Areasoning%2520over%2520text%2520that%2520includes%2520information%2520involving%2520uncertainty%2520explicitly%250Aquantified%2520via%2520probability%2520values.%2520This%2520type%2520of%2520reasoning%2520is%2520relevant%2520to%2520a%250Avariety%2520of%2520contexts%2520ranging%2520from%2520everyday%2520conversations%2520to%2520medical%250Adecision-making.%2520Despite%2520improvements%2520in%2520the%2520mathematical%2520reasoning%250Acapabilities%2520of%2520LLMs%252C%2520they%2520still%2520exhibit%2520significant%2520difficulties%2520when%2520it%2520comes%250Ato%2520probabilistic%2520reasoning.%2520To%2520deal%2520with%2520this%2520problem%252C%2520we%2520introduce%2520the%250ABayesian%2520Linguistic%2520Inference%2520Dataset%2520%2528BLInD%2529%252C%2520a%2520new%2520dataset%2520specifically%250Adesigned%2520to%2520test%2520the%2520probabilistic%2520reasoning%2520capabilities%2520of%2520LLMs.%2520We%2520use%2520BLInD%250Ato%2520find%2520out%2520the%2520limitations%2520of%2520LLMs%2520for%2520tasks%2520involving%2520probabilistic%250Areasoning.%2520In%2520addition%252C%2520we%2520present%2520several%2520prompting%2520strategies%2520that%2520map%2520the%250Aproblem%2520to%2520different%2520formal%2520representations%252C%2520including%2520Python%2520code%252C%250Aprobabilistic%2520algorithms%252C%2520and%2520probabilistic%2520logical%2520programming.%2520We%2520conclude%2520by%250Aproviding%2520an%2520evaluation%2520of%2520our%2520methods%2520on%2520BLInD%2520and%2520an%2520adaptation%2520of%2520a%2520causal%250Areasoning%2520question-answering%2520dataset.%2520Our%2520empirical%2520results%2520highlight%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520strategies%2520for%2520multiple%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09614v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20over%20Uncertain%20Text%20by%20Generative%20Large%20Language%20Models&entry.906535625=Aliakbar%20Nafar%20and%20Kristen%20Brent%20Venable%20and%20Parisa%20Kordjamshidi&entry.1292438233=%20%20This%20paper%20considers%20the%20challenges%20Large%20Language%20Models%20%28LLMs%29%20face%20when%0Areasoning%20over%20text%20that%20includes%20information%20involving%20uncertainty%20explicitly%0Aquantified%20via%20probability%20values.%20This%20type%20of%20reasoning%20is%20relevant%20to%20a%0Avariety%20of%20contexts%20ranging%20from%20everyday%20conversations%20to%20medical%0Adecision-making.%20Despite%20improvements%20in%20the%20mathematical%20reasoning%0Acapabilities%20of%20LLMs%2C%20they%20still%20exhibit%20significant%20difficulties%20when%20it%20comes%0Ato%20probabilistic%20reasoning.%20To%20deal%20with%20this%20problem%2C%20we%20introduce%20the%0ABayesian%20Linguistic%20Inference%20Dataset%20%28BLInD%29%2C%20a%20new%20dataset%20specifically%0Adesigned%20to%20test%20the%20probabilistic%20reasoning%20capabilities%20of%20LLMs.%20We%20use%20BLInD%0Ato%20find%20out%20the%20limitations%20of%20LLMs%20for%20tasks%20involving%20probabilistic%0Areasoning.%20In%20addition%2C%20we%20present%20several%20prompting%20strategies%20that%20map%20the%0Aproblem%20to%20different%20formal%20representations%2C%20including%20Python%20code%2C%0Aprobabilistic%20algorithms%2C%20and%20probabilistic%20logical%20programming.%20We%20conclude%20by%0Aproviding%20an%20evaluation%20of%20our%20methods%20on%20BLInD%20and%20an%20adaptation%20of%20a%20causal%0Areasoning%20question-answering%20dataset.%20Our%20empirical%20results%20highlight%20the%0Aeffectiveness%20of%20our%20proposed%20strategies%20for%20multiple%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09614v3&entry.124074799=Read"},
{"title": "AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land\n  Simulator", "author": "William Wang Yang and Karthikeya Kona and Yashveer Jain and Abhinav Bhamidipati and Tomer Atzili and Xiaomin Lin and Yantian Zha", "abstract": "  Current simulators lack the ability to accurately model integrated\nenvironments that encompass sea, air, and land. To address this gap, we\nintroduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator\n(SEALS), a comprehensive and photorealistic simulator designed for AAMs to\noperate and learn in these diverse environments. The development of AAM-SEALS\ntackles several significant challenges, including the creation of integrated\ncontrollers for flying, swimming, and manipulation, and the high-fidelity\nsimulation of aerial dynamics and hydrodynamics leveraging particle physics.\nOur evaluation demonstrates smooth operation and photorealistic transitions\nacross air, water, and their interfaces. We quantitatively validate the\nfidelity of particle-based hydrodynamics by comparing position-tracking errors\nacross real-world and simulated systems. AAM-SEALS promises to benefit a broad\nrange of robotics communities, including robot learning, aerial robotics,\nunderwater robotics, mobile manipulation, and robotic simulators. We will\nopen-source our code and data to foster the advancement of research in these\nfields. Please access our project website at: https:\n//aam-seals.github.io/aam-seals-v1/\n", "link": "http://arxiv.org/abs/2412.19744v1", "date": "2024-12-27", "relevancy": 1.5233, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5351}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5298}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AAM-SEALS%3A%20Developing%20Aerial-Aquatic%20Manipulators%20in%20SEa%2C%20Air%2C%20and%20Land%0A%20%20Simulator&body=Title%3A%20AAM-SEALS%3A%20Developing%20Aerial-Aquatic%20Manipulators%20in%20SEa%2C%20Air%2C%20and%20Land%0A%20%20Simulator%0AAuthor%3A%20William%20Wang%20Yang%20and%20Karthikeya%20Kona%20and%20Yashveer%20Jain%20and%20Abhinav%20Bhamidipati%20and%20Tomer%20Atzili%20and%20Xiaomin%20Lin%20and%20Yantian%20Zha%0AAbstract%3A%20%20%20Current%20simulators%20lack%20the%20ability%20to%20accurately%20model%20integrated%0Aenvironments%20that%20encompass%20sea%2C%20air%2C%20and%20land.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20Aerial-Aquatic%20Manipulators%20%28AAMs%29%20in%20SEa%2C%20Air%2C%20and%20Land%20Simulator%0A%28SEALS%29%2C%20a%20comprehensive%20and%20photorealistic%20simulator%20designed%20for%20AAMs%20to%0Aoperate%20and%20learn%20in%20these%20diverse%20environments.%20The%20development%20of%20AAM-SEALS%0Atackles%20several%20significant%20challenges%2C%20including%20the%20creation%20of%20integrated%0Acontrollers%20for%20flying%2C%20swimming%2C%20and%20manipulation%2C%20and%20the%20high-fidelity%0Asimulation%20of%20aerial%20dynamics%20and%20hydrodynamics%20leveraging%20particle%20physics.%0AOur%20evaluation%20demonstrates%20smooth%20operation%20and%20photorealistic%20transitions%0Aacross%20air%2C%20water%2C%20and%20their%20interfaces.%20We%20quantitatively%20validate%20the%0Afidelity%20of%20particle-based%20hydrodynamics%20by%20comparing%20position-tracking%20errors%0Aacross%20real-world%20and%20simulated%20systems.%20AAM-SEALS%20promises%20to%20benefit%20a%20broad%0Arange%20of%20robotics%20communities%2C%20including%20robot%20learning%2C%20aerial%20robotics%2C%0Aunderwater%20robotics%2C%20mobile%20manipulation%2C%20and%20robotic%20simulators.%20We%20will%0Aopen-source%20our%20code%20and%20data%20to%20foster%20the%20advancement%20of%20research%20in%20these%0Afields.%20Please%20access%20our%20project%20website%20at%3A%20https%3A%0A//aam-seals.github.io/aam-seals-v1/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAAM-SEALS%253A%2520Developing%2520Aerial-Aquatic%2520Manipulators%2520in%2520SEa%252C%2520Air%252C%2520and%2520Land%250A%2520%2520Simulator%26entry.906535625%3DWilliam%2520Wang%2520Yang%2520and%2520Karthikeya%2520Kona%2520and%2520Yashveer%2520Jain%2520and%2520Abhinav%2520Bhamidipati%2520and%2520Tomer%2520Atzili%2520and%2520Xiaomin%2520Lin%2520and%2520Yantian%2520Zha%26entry.1292438233%3D%2520%2520Current%2520simulators%2520lack%2520the%2520ability%2520to%2520accurately%2520model%2520integrated%250Aenvironments%2520that%2520encompass%2520sea%252C%2520air%252C%2520and%2520land.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520Aerial-Aquatic%2520Manipulators%2520%2528AAMs%2529%2520in%2520SEa%252C%2520Air%252C%2520and%2520Land%2520Simulator%250A%2528SEALS%2529%252C%2520a%2520comprehensive%2520and%2520photorealistic%2520simulator%2520designed%2520for%2520AAMs%2520to%250Aoperate%2520and%2520learn%2520in%2520these%2520diverse%2520environments.%2520The%2520development%2520of%2520AAM-SEALS%250Atackles%2520several%2520significant%2520challenges%252C%2520including%2520the%2520creation%2520of%2520integrated%250Acontrollers%2520for%2520flying%252C%2520swimming%252C%2520and%2520manipulation%252C%2520and%2520the%2520high-fidelity%250Asimulation%2520of%2520aerial%2520dynamics%2520and%2520hydrodynamics%2520leveraging%2520particle%2520physics.%250AOur%2520evaluation%2520demonstrates%2520smooth%2520operation%2520and%2520photorealistic%2520transitions%250Aacross%2520air%252C%2520water%252C%2520and%2520their%2520interfaces.%2520We%2520quantitatively%2520validate%2520the%250Afidelity%2520of%2520particle-based%2520hydrodynamics%2520by%2520comparing%2520position-tracking%2520errors%250Aacross%2520real-world%2520and%2520simulated%2520systems.%2520AAM-SEALS%2520promises%2520to%2520benefit%2520a%2520broad%250Arange%2520of%2520robotics%2520communities%252C%2520including%2520robot%2520learning%252C%2520aerial%2520robotics%252C%250Aunderwater%2520robotics%252C%2520mobile%2520manipulation%252C%2520and%2520robotic%2520simulators.%2520We%2520will%250Aopen-source%2520our%2520code%2520and%2520data%2520to%2520foster%2520the%2520advancement%2520of%2520research%2520in%2520these%250Afields.%2520Please%2520access%2520our%2520project%2520website%2520at%253A%2520https%253A%250A//aam-seals.github.io/aam-seals-v1/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AAM-SEALS%3A%20Developing%20Aerial-Aquatic%20Manipulators%20in%20SEa%2C%20Air%2C%20and%20Land%0A%20%20Simulator&entry.906535625=William%20Wang%20Yang%20and%20Karthikeya%20Kona%20and%20Yashveer%20Jain%20and%20Abhinav%20Bhamidipati%20and%20Tomer%20Atzili%20and%20Xiaomin%20Lin%20and%20Yantian%20Zha&entry.1292438233=%20%20Current%20simulators%20lack%20the%20ability%20to%20accurately%20model%20integrated%0Aenvironments%20that%20encompass%20sea%2C%20air%2C%20and%20land.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20Aerial-Aquatic%20Manipulators%20%28AAMs%29%20in%20SEa%2C%20Air%2C%20and%20Land%20Simulator%0A%28SEALS%29%2C%20a%20comprehensive%20and%20photorealistic%20simulator%20designed%20for%20AAMs%20to%0Aoperate%20and%20learn%20in%20these%20diverse%20environments.%20The%20development%20of%20AAM-SEALS%0Atackles%20several%20significant%20challenges%2C%20including%20the%20creation%20of%20integrated%0Acontrollers%20for%20flying%2C%20swimming%2C%20and%20manipulation%2C%20and%20the%20high-fidelity%0Asimulation%20of%20aerial%20dynamics%20and%20hydrodynamics%20leveraging%20particle%20physics.%0AOur%20evaluation%20demonstrates%20smooth%20operation%20and%20photorealistic%20transitions%0Aacross%20air%2C%20water%2C%20and%20their%20interfaces.%20We%20quantitatively%20validate%20the%0Afidelity%20of%20particle-based%20hydrodynamics%20by%20comparing%20position-tracking%20errors%0Aacross%20real-world%20and%20simulated%20systems.%20AAM-SEALS%20promises%20to%20benefit%20a%20broad%0Arange%20of%20robotics%20communities%2C%20including%20robot%20learning%2C%20aerial%20robotics%2C%0Aunderwater%20robotics%2C%20mobile%20manipulation%2C%20and%20robotic%20simulators.%20We%20will%0Aopen-source%20our%20code%20and%20data%20to%20foster%20the%20advancement%20of%20research%20in%20these%0Afields.%20Please%20access%20our%20project%20website%20at%3A%20https%3A%0A//aam-seals.github.io/aam-seals-v1/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19744v1&entry.124074799=Read"},
{"title": "KACQ-DCNN: Uncertainty-Aware Interpretable Kolmogorov-Arnold\n  Classical-Quantum Dual-Channel Neural Network for Heart Disease Detection", "author": "Md Abrar Jahin and Md. Akmol Masud and M. F. Mridha and Zeyar Aung and Nilanjan Dey", "abstract": "  Heart failure is a leading cause of global mortality, necessitating improved\ndiagnostic strategies. Classical machine learning models struggle with\nchallenges such as high-dimensional data, class imbalances, poor feature\nrepresentations, and lack of interpretability. While quantum machine learning\nholds promise, current hybrid models have not fully exploited quantum\nadvantages. In this paper, we propose the Kolmogorov-Arnold Classical-Quantum\nDual-Channel Neural Network (KACQ-DCNN), a novel hybrid architecture that\nreplaces traditional multilayer perceptrons with Kolmogorov-Arnold Networks\n(KANs), enabling learnable univariate activation functions. Our KACQ-DCNN\n4-qubit, 1-layer model outperforms 37 benchmark models, including 16 classical\nand 12 quantum neural networks, achieving an accuracy of 92.03%, with\nmacro-average precision, recall, and F1 scores of 92.00%. It also achieved a\nROC-AUC of 94.77%, surpassing other models by significant margins, as validated\nby paired t-tests with a significance threshold of 0.0056 (after Bonferroni\ncorrection). Ablation studies highlight the synergistic effect of\nclassical-quantum integration, improving performance by about 2% over MLP\nvariants. Additionally, LIME and SHAP explainability techniques enhance feature\ninterpretability, while conformal prediction provides robust uncertainty\nquantification. Our results demonstrate that KACQ-DCNN improves cardiovascular\ndiagnostics by combining high accuracy with interpretability and uncertainty\nquantification.\n", "link": "http://arxiv.org/abs/2410.07446v3", "date": "2024-12-27", "relevancy": 1.519, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5048}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KACQ-DCNN%3A%20Uncertainty-Aware%20Interpretable%20Kolmogorov-Arnold%0A%20%20Classical-Quantum%20Dual-Channel%20Neural%20Network%20for%20Heart%20Disease%20Detection&body=Title%3A%20KACQ-DCNN%3A%20Uncertainty-Aware%20Interpretable%20Kolmogorov-Arnold%0A%20%20Classical-Quantum%20Dual-Channel%20Neural%20Network%20for%20Heart%20Disease%20Detection%0AAuthor%3A%20Md%20Abrar%20Jahin%20and%20Md.%20Akmol%20Masud%20and%20M.%20F.%20Mridha%20and%20Zeyar%20Aung%20and%20Nilanjan%20Dey%0AAbstract%3A%20%20%20Heart%20failure%20is%20a%20leading%20cause%20of%20global%20mortality%2C%20necessitating%20improved%0Adiagnostic%20strategies.%20Classical%20machine%20learning%20models%20struggle%20with%0Achallenges%20such%20as%20high-dimensional%20data%2C%20class%20imbalances%2C%20poor%20feature%0Arepresentations%2C%20and%20lack%20of%20interpretability.%20While%20quantum%20machine%20learning%0Aholds%20promise%2C%20current%20hybrid%20models%20have%20not%20fully%20exploited%20quantum%0Aadvantages.%20In%20this%20paper%2C%20we%20propose%20the%20Kolmogorov-Arnold%20Classical-Quantum%0ADual-Channel%20Neural%20Network%20%28KACQ-DCNN%29%2C%20a%20novel%20hybrid%20architecture%20that%0Areplaces%20traditional%20multilayer%20perceptrons%20with%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%2C%20enabling%20learnable%20univariate%20activation%20functions.%20Our%20KACQ-DCNN%0A4-qubit%2C%201-layer%20model%20outperforms%2037%20benchmark%20models%2C%20including%2016%20classical%0Aand%2012%20quantum%20neural%20networks%2C%20achieving%20an%20accuracy%20of%2092.03%25%2C%20with%0Amacro-average%20precision%2C%20recall%2C%20and%20F1%20scores%20of%2092.00%25.%20It%20also%20achieved%20a%0AROC-AUC%20of%2094.77%25%2C%20surpassing%20other%20models%20by%20significant%20margins%2C%20as%20validated%0Aby%20paired%20t-tests%20with%20a%20significance%20threshold%20of%200.0056%20%28after%20Bonferroni%0Acorrection%29.%20Ablation%20studies%20highlight%20the%20synergistic%20effect%20of%0Aclassical-quantum%20integration%2C%20improving%20performance%20by%20about%202%25%20over%20MLP%0Avariants.%20Additionally%2C%20LIME%20and%20SHAP%20explainability%20techniques%20enhance%20feature%0Ainterpretability%2C%20while%20conformal%20prediction%20provides%20robust%20uncertainty%0Aquantification.%20Our%20results%20demonstrate%20that%20KACQ-DCNN%20improves%20cardiovascular%0Adiagnostics%20by%20combining%20high%20accuracy%20with%20interpretability%20and%20uncertainty%0Aquantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07446v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKACQ-DCNN%253A%2520Uncertainty-Aware%2520Interpretable%2520Kolmogorov-Arnold%250A%2520%2520Classical-Quantum%2520Dual-Channel%2520Neural%2520Network%2520for%2520Heart%2520Disease%2520Detection%26entry.906535625%3DMd%2520Abrar%2520Jahin%2520and%2520Md.%2520Akmol%2520Masud%2520and%2520M.%2520F.%2520Mridha%2520and%2520Zeyar%2520Aung%2520and%2520Nilanjan%2520Dey%26entry.1292438233%3D%2520%2520Heart%2520failure%2520is%2520a%2520leading%2520cause%2520of%2520global%2520mortality%252C%2520necessitating%2520improved%250Adiagnostic%2520strategies.%2520Classical%2520machine%2520learning%2520models%2520struggle%2520with%250Achallenges%2520such%2520as%2520high-dimensional%2520data%252C%2520class%2520imbalances%252C%2520poor%2520feature%250Arepresentations%252C%2520and%2520lack%2520of%2520interpretability.%2520While%2520quantum%2520machine%2520learning%250Aholds%2520promise%252C%2520current%2520hybrid%2520models%2520have%2520not%2520fully%2520exploited%2520quantum%250Aadvantages.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Kolmogorov-Arnold%2520Classical-Quantum%250ADual-Channel%2520Neural%2520Network%2520%2528KACQ-DCNN%2529%252C%2520a%2520novel%2520hybrid%2520architecture%2520that%250Areplaces%2520traditional%2520multilayer%2520perceptrons%2520with%2520Kolmogorov-Arnold%2520Networks%250A%2528KANs%2529%252C%2520enabling%2520learnable%2520univariate%2520activation%2520functions.%2520Our%2520KACQ-DCNN%250A4-qubit%252C%25201-layer%2520model%2520outperforms%252037%2520benchmark%2520models%252C%2520including%252016%2520classical%250Aand%252012%2520quantum%2520neural%2520networks%252C%2520achieving%2520an%2520accuracy%2520of%252092.03%2525%252C%2520with%250Amacro-average%2520precision%252C%2520recall%252C%2520and%2520F1%2520scores%2520of%252092.00%2525.%2520It%2520also%2520achieved%2520a%250AROC-AUC%2520of%252094.77%2525%252C%2520surpassing%2520other%2520models%2520by%2520significant%2520margins%252C%2520as%2520validated%250Aby%2520paired%2520t-tests%2520with%2520a%2520significance%2520threshold%2520of%25200.0056%2520%2528after%2520Bonferroni%250Acorrection%2529.%2520Ablation%2520studies%2520highlight%2520the%2520synergistic%2520effect%2520of%250Aclassical-quantum%2520integration%252C%2520improving%2520performance%2520by%2520about%25202%2525%2520over%2520MLP%250Avariants.%2520Additionally%252C%2520LIME%2520and%2520SHAP%2520explainability%2520techniques%2520enhance%2520feature%250Ainterpretability%252C%2520while%2520conformal%2520prediction%2520provides%2520robust%2520uncertainty%250Aquantification.%2520Our%2520results%2520demonstrate%2520that%2520KACQ-DCNN%2520improves%2520cardiovascular%250Adiagnostics%2520by%2520combining%2520high%2520accuracy%2520with%2520interpretability%2520and%2520uncertainty%250Aquantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07446v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KACQ-DCNN%3A%20Uncertainty-Aware%20Interpretable%20Kolmogorov-Arnold%0A%20%20Classical-Quantum%20Dual-Channel%20Neural%20Network%20for%20Heart%20Disease%20Detection&entry.906535625=Md%20Abrar%20Jahin%20and%20Md.%20Akmol%20Masud%20and%20M.%20F.%20Mridha%20and%20Zeyar%20Aung%20and%20Nilanjan%20Dey&entry.1292438233=%20%20Heart%20failure%20is%20a%20leading%20cause%20of%20global%20mortality%2C%20necessitating%20improved%0Adiagnostic%20strategies.%20Classical%20machine%20learning%20models%20struggle%20with%0Achallenges%20such%20as%20high-dimensional%20data%2C%20class%20imbalances%2C%20poor%20feature%0Arepresentations%2C%20and%20lack%20of%20interpretability.%20While%20quantum%20machine%20learning%0Aholds%20promise%2C%20current%20hybrid%20models%20have%20not%20fully%20exploited%20quantum%0Aadvantages.%20In%20this%20paper%2C%20we%20propose%20the%20Kolmogorov-Arnold%20Classical-Quantum%0ADual-Channel%20Neural%20Network%20%28KACQ-DCNN%29%2C%20a%20novel%20hybrid%20architecture%20that%0Areplaces%20traditional%20multilayer%20perceptrons%20with%20Kolmogorov-Arnold%20Networks%0A%28KANs%29%2C%20enabling%20learnable%20univariate%20activation%20functions.%20Our%20KACQ-DCNN%0A4-qubit%2C%201-layer%20model%20outperforms%2037%20benchmark%20models%2C%20including%2016%20classical%0Aand%2012%20quantum%20neural%20networks%2C%20achieving%20an%20accuracy%20of%2092.03%25%2C%20with%0Amacro-average%20precision%2C%20recall%2C%20and%20F1%20scores%20of%2092.00%25.%20It%20also%20achieved%20a%0AROC-AUC%20of%2094.77%25%2C%20surpassing%20other%20models%20by%20significant%20margins%2C%20as%20validated%0Aby%20paired%20t-tests%20with%20a%20significance%20threshold%20of%200.0056%20%28after%20Bonferroni%0Acorrection%29.%20Ablation%20studies%20highlight%20the%20synergistic%20effect%20of%0Aclassical-quantum%20integration%2C%20improving%20performance%20by%20about%202%25%20over%20MLP%0Avariants.%20Additionally%2C%20LIME%20and%20SHAP%20explainability%20techniques%20enhance%20feature%0Ainterpretability%2C%20while%20conformal%20prediction%20provides%20robust%20uncertainty%0Aquantification.%20Our%20results%20demonstrate%20that%20KACQ-DCNN%20improves%20cardiovascular%0Adiagnostics%20by%20combining%20high%20accuracy%20with%20interpretability%20and%20uncertainty%0Aquantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07446v3&entry.124074799=Read"},
{"title": "Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied\n  Instruction Following", "author": "Yuxiao Yang and Shenao Zhang and Zhihan Liu and Huaxiu Yao and Zhaoran Wang", "abstract": "  This work focuses on building a task planner for Embodied Instruction\nFollowing (EIF) using Large Language Models (LLMs). Previous works typically\ntrain a planner to imitate expert trajectories, treating this as a supervised\ntask. While these methods achieve competitive performance, they often lack\nsufficient robustness. When a suboptimal action is taken, the planner may\nencounter an out-of-distribution state, which can lead to task failure. In\ncontrast, we frame the task as a Partially Observable Markov Decision Process\n(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,\nwe propose a closed-loop planner with an adaptation module and a novel\nhindsight method, aiming to use as much information as possible to assist the\nplanner. Our experiments on the ALFRED dataset indicate that our planner\nachieves competitive performance under a few-shot assumption. For the first\ntime, our few-shot agent's performance approaches and even surpasses that of\nthe full-shot supervised agent.\n", "link": "http://arxiv.org/abs/2412.19562v1", "date": "2024-12-27", "relevancy": 1.5114, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5121}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5074}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hindsight%20Planner%3A%20A%20Closed-Loop%20Few-Shot%20Planner%20for%20Embodied%0A%20%20Instruction%20Following&body=Title%3A%20Hindsight%20Planner%3A%20A%20Closed-Loop%20Few-Shot%20Planner%20for%20Embodied%0A%20%20Instruction%20Following%0AAuthor%3A%20Yuxiao%20Yang%20and%20Shenao%20Zhang%20and%20Zhihan%20Liu%20and%20Huaxiu%20Yao%20and%20Zhaoran%20Wang%0AAbstract%3A%20%20%20This%20work%20focuses%20on%20building%20a%20task%20planner%20for%20Embodied%20Instruction%0AFollowing%20%28EIF%29%20using%20Large%20Language%20Models%20%28LLMs%29.%20Previous%20works%20typically%0Atrain%20a%20planner%20to%20imitate%20expert%20trajectories%2C%20treating%20this%20as%20a%20supervised%0Atask.%20While%20these%20methods%20achieve%20competitive%20performance%2C%20they%20often%20lack%0Asufficient%20robustness.%20When%20a%20suboptimal%20action%20is%20taken%2C%20the%20planner%20may%0Aencounter%20an%20out-of-distribution%20state%2C%20which%20can%20lead%20to%20task%20failure.%20In%0Acontrast%2C%20we%20frame%20the%20task%20as%20a%20Partially%20Observable%20Markov%20Decision%20Process%0A%28POMDP%29%20and%20aim%20to%20develop%20a%20robust%20planner%20under%20a%20few-shot%20assumption.%20Thus%2C%0Awe%20propose%20a%20closed-loop%20planner%20with%20an%20adaptation%20module%20and%20a%20novel%0Ahindsight%20method%2C%20aiming%20to%20use%20as%20much%20information%20as%20possible%20to%20assist%20the%0Aplanner.%20Our%20experiments%20on%20the%20ALFRED%20dataset%20indicate%20that%20our%20planner%0Aachieves%20competitive%20performance%20under%20a%20few-shot%20assumption.%20For%20the%20first%0Atime%2C%20our%20few-shot%20agent%27s%20performance%20approaches%20and%20even%20surpasses%20that%20of%0Athe%20full-shot%20supervised%20agent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHindsight%2520Planner%253A%2520A%2520Closed-Loop%2520Few-Shot%2520Planner%2520for%2520Embodied%250A%2520%2520Instruction%2520Following%26entry.906535625%3DYuxiao%2520Yang%2520and%2520Shenao%2520Zhang%2520and%2520Zhihan%2520Liu%2520and%2520Huaxiu%2520Yao%2520and%2520Zhaoran%2520Wang%26entry.1292438233%3D%2520%2520This%2520work%2520focuses%2520on%2520building%2520a%2520task%2520planner%2520for%2520Embodied%2520Instruction%250AFollowing%2520%2528EIF%2529%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Previous%2520works%2520typically%250Atrain%2520a%2520planner%2520to%2520imitate%2520expert%2520trajectories%252C%2520treating%2520this%2520as%2520a%2520supervised%250Atask.%2520While%2520these%2520methods%2520achieve%2520competitive%2520performance%252C%2520they%2520often%2520lack%250Asufficient%2520robustness.%2520When%2520a%2520suboptimal%2520action%2520is%2520taken%252C%2520the%2520planner%2520may%250Aencounter%2520an%2520out-of-distribution%2520state%252C%2520which%2520can%2520lead%2520to%2520task%2520failure.%2520In%250Acontrast%252C%2520we%2520frame%2520the%2520task%2520as%2520a%2520Partially%2520Observable%2520Markov%2520Decision%2520Process%250A%2528POMDP%2529%2520and%2520aim%2520to%2520develop%2520a%2520robust%2520planner%2520under%2520a%2520few-shot%2520assumption.%2520Thus%252C%250Awe%2520propose%2520a%2520closed-loop%2520planner%2520with%2520an%2520adaptation%2520module%2520and%2520a%2520novel%250Ahindsight%2520method%252C%2520aiming%2520to%2520use%2520as%2520much%2520information%2520as%2520possible%2520to%2520assist%2520the%250Aplanner.%2520Our%2520experiments%2520on%2520the%2520ALFRED%2520dataset%2520indicate%2520that%2520our%2520planner%250Aachieves%2520competitive%2520performance%2520under%2520a%2520few-shot%2520assumption.%2520For%2520the%2520first%250Atime%252C%2520our%2520few-shot%2520agent%2527s%2520performance%2520approaches%2520and%2520even%2520surpasses%2520that%2520of%250Athe%2520full-shot%2520supervised%2520agent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hindsight%20Planner%3A%20A%20Closed-Loop%20Few-Shot%20Planner%20for%20Embodied%0A%20%20Instruction%20Following&entry.906535625=Yuxiao%20Yang%20and%20Shenao%20Zhang%20and%20Zhihan%20Liu%20and%20Huaxiu%20Yao%20and%20Zhaoran%20Wang&entry.1292438233=%20%20This%20work%20focuses%20on%20building%20a%20task%20planner%20for%20Embodied%20Instruction%0AFollowing%20%28EIF%29%20using%20Large%20Language%20Models%20%28LLMs%29.%20Previous%20works%20typically%0Atrain%20a%20planner%20to%20imitate%20expert%20trajectories%2C%20treating%20this%20as%20a%20supervised%0Atask.%20While%20these%20methods%20achieve%20competitive%20performance%2C%20they%20often%20lack%0Asufficient%20robustness.%20When%20a%20suboptimal%20action%20is%20taken%2C%20the%20planner%20may%0Aencounter%20an%20out-of-distribution%20state%2C%20which%20can%20lead%20to%20task%20failure.%20In%0Acontrast%2C%20we%20frame%20the%20task%20as%20a%20Partially%20Observable%20Markov%20Decision%20Process%0A%28POMDP%29%20and%20aim%20to%20develop%20a%20robust%20planner%20under%20a%20few-shot%20assumption.%20Thus%2C%0Awe%20propose%20a%20closed-loop%20planner%20with%20an%20adaptation%20module%20and%20a%20novel%0Ahindsight%20method%2C%20aiming%20to%20use%20as%20much%20information%20as%20possible%20to%20assist%20the%0Aplanner.%20Our%20experiments%20on%20the%20ALFRED%20dataset%20indicate%20that%20our%20planner%0Aachieves%20competitive%20performance%20under%20a%20few-shot%20assumption.%20For%20the%20first%0Atime%2C%20our%20few-shot%20agent%27s%20performance%20approaches%20and%20even%20surpasses%20that%20of%0Athe%20full-shot%20supervised%20agent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19562v1&entry.124074799=Read"},
{"title": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake\n  News Detection", "author": "Yihao Wang and Lizhi Chen and Zhong Qian and Peifeng Li", "abstract": "  News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. We also propose a new baseline model called\nOFNVD, which captures key information from multimodal features through a GLU\nattention mechanism and performs feature enhancement and modal aggregation via\na cross-modal Transformer. Benchmarking the dataset and baselines demonstrates\nthe effectiveness of our model in multimodal news detection.\n", "link": "http://arxiv.org/abs/2407.19493v3", "date": "2024-12-27", "relevancy": 1.5065, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5179}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5148}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Official-NV%3A%20An%20LLM-Generated%20News%20Video%20Dataset%20for%20Multimodal%20Fake%0A%20%20News%20Detection&body=Title%3A%20Official-NV%3A%20An%20LLM-Generated%20News%20Video%20Dataset%20for%20Multimodal%20Fake%0A%20%20News%20Detection%0AAuthor%3A%20Yihao%20Wang%20and%20Lizhi%20Chen%20and%20Zhong%20Qian%20and%20Peifeng%20Li%0AAbstract%3A%20%20%20News%20media%2C%20especially%20video%20news%20media%2C%20have%20penetrated%20into%20every%20aspect%20of%0Adaily%20life%2C%20which%20also%20brings%20the%20risk%20of%20fake%20news.%20Therefore%2C%20multimodal%20fake%0Anews%20detection%20has%20recently%20garnered%20increased%20attention.%20However%2C%20the%20existing%0Adatasets%20are%20comprised%20of%20user-uploaded%20videos%20and%20contain%20an%20excess%20amounts%20of%0Asuperfluous%20data%2C%20which%20introduces%20noise%20into%20the%20model%20training%20process.%20To%0Aaddress%20this%20issue%2C%20we%20construct%20a%20dataset%20named%20Official-NV%2C%20comprising%0Aofficially%20published%20news%20videos.%20The%20crawl%20officially%20published%20videos%20are%0Aaugmented%20through%20the%20use%20of%20LLMs-based%20generation%20and%20manual%20verification%2C%0Athereby%20expanding%20the%20dataset.%20We%20also%20propose%20a%20new%20baseline%20model%20called%0AOFNVD%2C%20which%20captures%20key%20information%20from%20multimodal%20features%20through%20a%20GLU%0Aattention%20mechanism%20and%20performs%20feature%20enhancement%20and%20modal%20aggregation%20via%0Aa%20cross-modal%20Transformer.%20Benchmarking%20the%20dataset%20and%20baselines%20demonstrates%0Athe%20effectiveness%20of%20our%20model%20in%20multimodal%20news%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19493v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOfficial-NV%253A%2520An%2520LLM-Generated%2520News%2520Video%2520Dataset%2520for%2520Multimodal%2520Fake%250A%2520%2520News%2520Detection%26entry.906535625%3DYihao%2520Wang%2520and%2520Lizhi%2520Chen%2520and%2520Zhong%2520Qian%2520and%2520Peifeng%2520Li%26entry.1292438233%3D%2520%2520News%2520media%252C%2520especially%2520video%2520news%2520media%252C%2520have%2520penetrated%2520into%2520every%2520aspect%2520of%250Adaily%2520life%252C%2520which%2520also%2520brings%2520the%2520risk%2520of%2520fake%2520news.%2520Therefore%252C%2520multimodal%2520fake%250Anews%2520detection%2520has%2520recently%2520garnered%2520increased%2520attention.%2520However%252C%2520the%2520existing%250Adatasets%2520are%2520comprised%2520of%2520user-uploaded%2520videos%2520and%2520contain%2520an%2520excess%2520amounts%2520of%250Asuperfluous%2520data%252C%2520which%2520introduces%2520noise%2520into%2520the%2520model%2520training%2520process.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520construct%2520a%2520dataset%2520named%2520Official-NV%252C%2520comprising%250Aofficially%2520published%2520news%2520videos.%2520The%2520crawl%2520officially%2520published%2520videos%2520are%250Aaugmented%2520through%2520the%2520use%2520of%2520LLMs-based%2520generation%2520and%2520manual%2520verification%252C%250Athereby%2520expanding%2520the%2520dataset.%2520We%2520also%2520propose%2520a%2520new%2520baseline%2520model%2520called%250AOFNVD%252C%2520which%2520captures%2520key%2520information%2520from%2520multimodal%2520features%2520through%2520a%2520GLU%250Aattention%2520mechanism%2520and%2520performs%2520feature%2520enhancement%2520and%2520modal%2520aggregation%2520via%250Aa%2520cross-modal%2520Transformer.%2520Benchmarking%2520the%2520dataset%2520and%2520baselines%2520demonstrates%250Athe%2520effectiveness%2520of%2520our%2520model%2520in%2520multimodal%2520news%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19493v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Official-NV%3A%20An%20LLM-Generated%20News%20Video%20Dataset%20for%20Multimodal%20Fake%0A%20%20News%20Detection&entry.906535625=Yihao%20Wang%20and%20Lizhi%20Chen%20and%20Zhong%20Qian%20and%20Peifeng%20Li&entry.1292438233=%20%20News%20media%2C%20especially%20video%20news%20media%2C%20have%20penetrated%20into%20every%20aspect%20of%0Adaily%20life%2C%20which%20also%20brings%20the%20risk%20of%20fake%20news.%20Therefore%2C%20multimodal%20fake%0Anews%20detection%20has%20recently%20garnered%20increased%20attention.%20However%2C%20the%20existing%0Adatasets%20are%20comprised%20of%20user-uploaded%20videos%20and%20contain%20an%20excess%20amounts%20of%0Asuperfluous%20data%2C%20which%20introduces%20noise%20into%20the%20model%20training%20process.%20To%0Aaddress%20this%20issue%2C%20we%20construct%20a%20dataset%20named%20Official-NV%2C%20comprising%0Aofficially%20published%20news%20videos.%20The%20crawl%20officially%20published%20videos%20are%0Aaugmented%20through%20the%20use%20of%20LLMs-based%20generation%20and%20manual%20verification%2C%0Athereby%20expanding%20the%20dataset.%20We%20also%20propose%20a%20new%20baseline%20model%20called%0AOFNVD%2C%20which%20captures%20key%20information%20from%20multimodal%20features%20through%20a%20GLU%0Aattention%20mechanism%20and%20performs%20feature%20enhancement%20and%20modal%20aggregation%20via%0Aa%20cross-modal%20Transformer.%20Benchmarking%20the%20dataset%20and%20baselines%20demonstrates%0Athe%20effectiveness%20of%20our%20model%20in%20multimodal%20news%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19493v3&entry.124074799=Read"},
{"title": "Ultralight Signal Classification Model for Automatic Modulation\n  Recognition", "author": "Alessandro Daniele Genuardi Oquendo and Agust\u00edn Mat\u00edas Galante Cervi\u00f1o and Nilotpal Sinha and Luc Andrea and Sam Mugel and Rom\u00e1n Or\u00fas", "abstract": "  The growing complexity of radar signals demands responsive and accurate\ndetection systems that can operate efficiently on resource-constrained edge\ndevices. Existing models, while effective, often rely on substantial\ncomputational resources and large datasets, making them impractical for edge\ndeployment. In this work, we propose an ultralight hybrid neural network\noptimized for edge applications, delivering robust performance across\nunfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less\nthan 100 samples per class, and significantly reducing computational overhead.\n", "link": "http://arxiv.org/abs/2412.19585v1", "date": "2024-12-27", "relevancy": 1.4995, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5251}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5026}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultralight%20Signal%20Classification%20Model%20for%20Automatic%20Modulation%0A%20%20Recognition&body=Title%3A%20Ultralight%20Signal%20Classification%20Model%20for%20Automatic%20Modulation%0A%20%20Recognition%0AAuthor%3A%20Alessandro%20Daniele%20Genuardi%20Oquendo%20and%20Agust%C3%ADn%20Mat%C3%ADas%20Galante%20Cervi%C3%B1o%20and%20Nilotpal%20Sinha%20and%20Luc%20Andrea%20and%20Sam%20Mugel%20and%20Rom%C3%A1n%20Or%C3%BAs%0AAbstract%3A%20%20%20The%20growing%20complexity%20of%20radar%20signals%20demands%20responsive%20and%20accurate%0Adetection%20systems%20that%20can%20operate%20efficiently%20on%20resource-constrained%20edge%0Adevices.%20Existing%20models%2C%20while%20effective%2C%20often%20rely%20on%20substantial%0Acomputational%20resources%20and%20large%20datasets%2C%20making%20them%20impractical%20for%20edge%0Adeployment.%20In%20this%20work%2C%20we%20propose%20an%20ultralight%20hybrid%20neural%20network%0Aoptimized%20for%20edge%20applications%2C%20delivering%20robust%20performance%20across%0Aunfavorable%20signal-to-noise%20ratios%20%28mean%20accuracy%20of%2096.3%25%20at%200%20dB%29%20using%20less%0Athan%20100%20samples%20per%20class%2C%20and%20significantly%20reducing%20computational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltralight%2520Signal%2520Classification%2520Model%2520for%2520Automatic%2520Modulation%250A%2520%2520Recognition%26entry.906535625%3DAlessandro%2520Daniele%2520Genuardi%2520Oquendo%2520and%2520Agust%25C3%25ADn%2520Mat%25C3%25ADas%2520Galante%2520Cervi%25C3%25B1o%2520and%2520Nilotpal%2520Sinha%2520and%2520Luc%2520Andrea%2520and%2520Sam%2520Mugel%2520and%2520Rom%25C3%25A1n%2520Or%25C3%25BAs%26entry.1292438233%3D%2520%2520The%2520growing%2520complexity%2520of%2520radar%2520signals%2520demands%2520responsive%2520and%2520accurate%250Adetection%2520systems%2520that%2520can%2520operate%2520efficiently%2520on%2520resource-constrained%2520edge%250Adevices.%2520Existing%2520models%252C%2520while%2520effective%252C%2520often%2520rely%2520on%2520substantial%250Acomputational%2520resources%2520and%2520large%2520datasets%252C%2520making%2520them%2520impractical%2520for%2520edge%250Adeployment.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520ultralight%2520hybrid%2520neural%2520network%250Aoptimized%2520for%2520edge%2520applications%252C%2520delivering%2520robust%2520performance%2520across%250Aunfavorable%2520signal-to-noise%2520ratios%2520%2528mean%2520accuracy%2520of%252096.3%2525%2520at%25200%2520dB%2529%2520using%2520less%250Athan%2520100%2520samples%2520per%2520class%252C%2520and%2520significantly%2520reducing%2520computational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultralight%20Signal%20Classification%20Model%20for%20Automatic%20Modulation%0A%20%20Recognition&entry.906535625=Alessandro%20Daniele%20Genuardi%20Oquendo%20and%20Agust%C3%ADn%20Mat%C3%ADas%20Galante%20Cervi%C3%B1o%20and%20Nilotpal%20Sinha%20and%20Luc%20Andrea%20and%20Sam%20Mugel%20and%20Rom%C3%A1n%20Or%C3%BAs&entry.1292438233=%20%20The%20growing%20complexity%20of%20radar%20signals%20demands%20responsive%20and%20accurate%0Adetection%20systems%20that%20can%20operate%20efficiently%20on%20resource-constrained%20edge%0Adevices.%20Existing%20models%2C%20while%20effective%2C%20often%20rely%20on%20substantial%0Acomputational%20resources%20and%20large%20datasets%2C%20making%20them%20impractical%20for%20edge%0Adeployment.%20In%20this%20work%2C%20we%20propose%20an%20ultralight%20hybrid%20neural%20network%0Aoptimized%20for%20edge%20applications%2C%20delivering%20robust%20performance%20across%0Aunfavorable%20signal-to-noise%20ratios%20%28mean%20accuracy%20of%2096.3%25%20at%200%20dB%29%20using%20less%0Athan%20100%20samples%20per%20class%2C%20and%20significantly%20reducing%20computational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19585v1&entry.124074799=Read"},
{"title": "IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With\n  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring\n  Distribution-Aware Data Reshaping", "author": "Adrian Kneip and Martin Lefebvre and Pol Maistriaux and David Bol", "abstract": "  Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing\ncompromise between computing efficiency and accuracy to process sub-8b\nconvolutional neural networks (CNNs) at the edge. Yet, they commonly make use\nof a fixed dot-product (DP) voltage swing, which leads to a loss in effective\nADC bits due to data-dependent clipping or truncation effects that waste\nprecious conversion energy and computing accuracy. To overcome this, we present\nIMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It\nintroduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based\non an input-serial, weight-parallel accumulation that avoids power-hungry DACs.\nAn adaptive swing is achieved by combining a channel-wise DP array split with a\nlinear in-ADC implementation of analog batch-normalization (ABN), obtaining a\ndistribution-aware data reshaping. Critical design constraints are relaxed by\nincluding the post-silicon equivalent noise within a CIM-aware CNN training\nframework. Measurement results showcase an 8b system-level energy efficiency of\n40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.\nMoreover, the peak energy and area efficiencies of the 187kB/mm2 macro\nrespectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the\n8-to-1b computing precision. These results exceed previous charge-based designs\nby 3-to-5x while being the first work to provide linear in-memory rescaling.\n", "link": "http://arxiv.org/abs/2412.19750v1", "date": "2024-12-27", "relevancy": 1.4793, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5289}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4953}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMAGINE%3A%20An%208-to-1b%2022nm%20FD-SOI%20Compute-In-Memory%20CNN%20Accelerator%20With%0A%20%20an%20End-to-End%20Analog%20Charge-Based%200.15-8POPS/W%20Macro%20Featuring%0A%20%20Distribution-Aware%20Data%20Reshaping&body=Title%3A%20IMAGINE%3A%20An%208-to-1b%2022nm%20FD-SOI%20Compute-In-Memory%20CNN%20Accelerator%20With%0A%20%20an%20End-to-End%20Analog%20Charge-Based%200.15-8POPS/W%20Macro%20Featuring%0A%20%20Distribution-Aware%20Data%20Reshaping%0AAuthor%3A%20Adrian%20Kneip%20and%20Martin%20Lefebvre%20and%20Pol%20Maistriaux%20and%20David%20Bol%0AAbstract%3A%20%20%20Charge-domain%20compute-in-memory%20%28CIM%29%20SRAMs%20have%20recently%20become%20an%20enticing%0Acompromise%20between%20computing%20efficiency%20and%20accuracy%20to%20process%20sub-8b%0Aconvolutional%20neural%20networks%20%28CNNs%29%20at%20the%20edge.%20Yet%2C%20they%20commonly%20make%20use%0Aof%20a%20fixed%20dot-product%20%28DP%29%20voltage%20swing%2C%20which%20leads%20to%20a%20loss%20in%20effective%0AADC%20bits%20due%20to%20data-dependent%20clipping%20or%20truncation%20effects%20that%20waste%0Aprecious%20conversion%20energy%20and%20computing%20accuracy.%20To%20overcome%20this%2C%20we%20present%0AIMAGINE%2C%20a%20workload-adaptive%201-to-8b%20CIM-CNN%20accelerator%20in%2022nm%20FD-SOI.%20It%0Aintroduces%20a%201152x256%20end-to-end%20charge-based%20macro%20with%20a%20multi-bit%20DP%20based%0Aon%20an%20input-serial%2C%20weight-parallel%20accumulation%20that%20avoids%20power-hungry%20DACs.%0AAn%20adaptive%20swing%20is%20achieved%20by%20combining%20a%20channel-wise%20DP%20array%20split%20with%20a%0Alinear%20in-ADC%20implementation%20of%20analog%20batch-normalization%20%28ABN%29%2C%20obtaining%20a%0Adistribution-aware%20data%20reshaping.%20Critical%20design%20constraints%20are%20relaxed%20by%0Aincluding%20the%20post-silicon%20equivalent%20noise%20within%20a%20CIM-aware%20CNN%20training%0Aframework.%20Measurement%20results%20showcase%20an%208b%20system-level%20energy%20efficiency%20of%0A40TOPS/W%20at%200.3/0.6V%2C%20with%20competitive%20accuracies%20on%20MNIST%20and%20CIFAR-10.%0AMoreover%2C%20the%20peak%20energy%20and%20area%20efficiencies%20of%20the%20187kB/mm2%20macro%0Arespectively%20reach%20up%20to%200.15-8POPS/W%20and%202.6-154TOPS/mm2%2C%20scaling%20with%20the%0A8-to-1b%20computing%20precision.%20These%20results%20exceed%20previous%20charge-based%20designs%0Aby%203-to-5x%20while%20being%20the%20first%20work%20to%20provide%20linear%20in-memory%20rescaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMAGINE%253A%2520An%25208-to-1b%252022nm%2520FD-SOI%2520Compute-In-Memory%2520CNN%2520Accelerator%2520With%250A%2520%2520an%2520End-to-End%2520Analog%2520Charge-Based%25200.15-8POPS/W%2520Macro%2520Featuring%250A%2520%2520Distribution-Aware%2520Data%2520Reshaping%26entry.906535625%3DAdrian%2520Kneip%2520and%2520Martin%2520Lefebvre%2520and%2520Pol%2520Maistriaux%2520and%2520David%2520Bol%26entry.1292438233%3D%2520%2520Charge-domain%2520compute-in-memory%2520%2528CIM%2529%2520SRAMs%2520have%2520recently%2520become%2520an%2520enticing%250Acompromise%2520between%2520computing%2520efficiency%2520and%2520accuracy%2520to%2520process%2520sub-8b%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520at%2520the%2520edge.%2520Yet%252C%2520they%2520commonly%2520make%2520use%250Aof%2520a%2520fixed%2520dot-product%2520%2528DP%2529%2520voltage%2520swing%252C%2520which%2520leads%2520to%2520a%2520loss%2520in%2520effective%250AADC%2520bits%2520due%2520to%2520data-dependent%2520clipping%2520or%2520truncation%2520effects%2520that%2520waste%250Aprecious%2520conversion%2520energy%2520and%2520computing%2520accuracy.%2520To%2520overcome%2520this%252C%2520we%2520present%250AIMAGINE%252C%2520a%2520workload-adaptive%25201-to-8b%2520CIM-CNN%2520accelerator%2520in%252022nm%2520FD-SOI.%2520It%250Aintroduces%2520a%25201152x256%2520end-to-end%2520charge-based%2520macro%2520with%2520a%2520multi-bit%2520DP%2520based%250Aon%2520an%2520input-serial%252C%2520weight-parallel%2520accumulation%2520that%2520avoids%2520power-hungry%2520DACs.%250AAn%2520adaptive%2520swing%2520is%2520achieved%2520by%2520combining%2520a%2520channel-wise%2520DP%2520array%2520split%2520with%2520a%250Alinear%2520in-ADC%2520implementation%2520of%2520analog%2520batch-normalization%2520%2528ABN%2529%252C%2520obtaining%2520a%250Adistribution-aware%2520data%2520reshaping.%2520Critical%2520design%2520constraints%2520are%2520relaxed%2520by%250Aincluding%2520the%2520post-silicon%2520equivalent%2520noise%2520within%2520a%2520CIM-aware%2520CNN%2520training%250Aframework.%2520Measurement%2520results%2520showcase%2520an%25208b%2520system-level%2520energy%2520efficiency%2520of%250A40TOPS/W%2520at%25200.3/0.6V%252C%2520with%2520competitive%2520accuracies%2520on%2520MNIST%2520and%2520CIFAR-10.%250AMoreover%252C%2520the%2520peak%2520energy%2520and%2520area%2520efficiencies%2520of%2520the%2520187kB/mm2%2520macro%250Arespectively%2520reach%2520up%2520to%25200.15-8POPS/W%2520and%25202.6-154TOPS/mm2%252C%2520scaling%2520with%2520the%250A8-to-1b%2520computing%2520precision.%2520These%2520results%2520exceed%2520previous%2520charge-based%2520designs%250Aby%25203-to-5x%2520while%2520being%2520the%2520first%2520work%2520to%2520provide%2520linear%2520in-memory%2520rescaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMAGINE%3A%20An%208-to-1b%2022nm%20FD-SOI%20Compute-In-Memory%20CNN%20Accelerator%20With%0A%20%20an%20End-to-End%20Analog%20Charge-Based%200.15-8POPS/W%20Macro%20Featuring%0A%20%20Distribution-Aware%20Data%20Reshaping&entry.906535625=Adrian%20Kneip%20and%20Martin%20Lefebvre%20and%20Pol%20Maistriaux%20and%20David%20Bol&entry.1292438233=%20%20Charge-domain%20compute-in-memory%20%28CIM%29%20SRAMs%20have%20recently%20become%20an%20enticing%0Acompromise%20between%20computing%20efficiency%20and%20accuracy%20to%20process%20sub-8b%0Aconvolutional%20neural%20networks%20%28CNNs%29%20at%20the%20edge.%20Yet%2C%20they%20commonly%20make%20use%0Aof%20a%20fixed%20dot-product%20%28DP%29%20voltage%20swing%2C%20which%20leads%20to%20a%20loss%20in%20effective%0AADC%20bits%20due%20to%20data-dependent%20clipping%20or%20truncation%20effects%20that%20waste%0Aprecious%20conversion%20energy%20and%20computing%20accuracy.%20To%20overcome%20this%2C%20we%20present%0AIMAGINE%2C%20a%20workload-adaptive%201-to-8b%20CIM-CNN%20accelerator%20in%2022nm%20FD-SOI.%20It%0Aintroduces%20a%201152x256%20end-to-end%20charge-based%20macro%20with%20a%20multi-bit%20DP%20based%0Aon%20an%20input-serial%2C%20weight-parallel%20accumulation%20that%20avoids%20power-hungry%20DACs.%0AAn%20adaptive%20swing%20is%20achieved%20by%20combining%20a%20channel-wise%20DP%20array%20split%20with%20a%0Alinear%20in-ADC%20implementation%20of%20analog%20batch-normalization%20%28ABN%29%2C%20obtaining%20a%0Adistribution-aware%20data%20reshaping.%20Critical%20design%20constraints%20are%20relaxed%20by%0Aincluding%20the%20post-silicon%20equivalent%20noise%20within%20a%20CIM-aware%20CNN%20training%0Aframework.%20Measurement%20results%20showcase%20an%208b%20system-level%20energy%20efficiency%20of%0A40TOPS/W%20at%200.3/0.6V%2C%20with%20competitive%20accuracies%20on%20MNIST%20and%20CIFAR-10.%0AMoreover%2C%20the%20peak%20energy%20and%20area%20efficiencies%20of%20the%20187kB/mm2%20macro%0Arespectively%20reach%20up%20to%200.15-8POPS/W%20and%202.6-154TOPS/mm2%2C%20scaling%20with%20the%0A8-to-1b%20computing%20precision.%20These%20results%20exceed%20previous%20charge-based%20designs%0Aby%203-to-5x%20while%20being%20the%20first%20work%20to%20provide%20linear%20in-memory%20rescaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19750v1&entry.124074799=Read"},
{"title": "RL-MUL 2.0: Multiplier Design Optimization with Parallel Deep\n  Reinforcement Learning and Space Reduction", "author": "Dongsheng Zuo and Jiadong Zhu and Yikang Ouyang and Yuzhe Ma", "abstract": "  Multiplication is a fundamental operation in many applications, and\nmultipliers are widely adopted in various circuits. However, optimizing\nmultipliers is challenging due to the extensive design space. In this paper, we\npropose a multiplier design optimization framework based on reinforcement\nlearning. We utilize matrix and tensor representations for the compressor tree\nof a multiplier, enabling seamless integration of convolutional neural networks\nas the agent network. The agent optimizes the multiplier structure using a\nPareto-driven reward customized to balance area and delay. Furthermore, we\nenhance the original framework with parallel reinforcement learning and design\nspace pruning techniques and extend its capability to optimize fused\nmultiply-accumulate (MAC) designs. Experiments conducted on different bit\nwidths of multipliers demonstrate that multipliers produced by our approach\noutperform all baseline designs in terms of area, power, and delay. The\nperformance gain is further validated by comparing the area, power, and delay\nof processing element arrays using multipliers from our approach and baseline\napproaches.\n", "link": "http://arxiv.org/abs/2404.00639v2", "date": "2024-12-27", "relevancy": 1.4709, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4975}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-MUL%202.0%3A%20Multiplier%20Design%20Optimization%20with%20Parallel%20Deep%0A%20%20Reinforcement%20Learning%20and%20Space%20Reduction&body=Title%3A%20RL-MUL%202.0%3A%20Multiplier%20Design%20Optimization%20with%20Parallel%20Deep%0A%20%20Reinforcement%20Learning%20and%20Space%20Reduction%0AAuthor%3A%20Dongsheng%20Zuo%20and%20Jiadong%20Zhu%20and%20Yikang%20Ouyang%20and%20Yuzhe%20Ma%0AAbstract%3A%20%20%20Multiplication%20is%20a%20fundamental%20operation%20in%20many%20applications%2C%20and%0Amultipliers%20are%20widely%20adopted%20in%20various%20circuits.%20However%2C%20optimizing%0Amultipliers%20is%20challenging%20due%20to%20the%20extensive%20design%20space.%20In%20this%20paper%2C%20we%0Apropose%20a%20multiplier%20design%20optimization%20framework%20based%20on%20reinforcement%0Alearning.%20We%20utilize%20matrix%20and%20tensor%20representations%20for%20the%20compressor%20tree%0Aof%20a%20multiplier%2C%20enabling%20seamless%20integration%20of%20convolutional%20neural%20networks%0Aas%20the%20agent%20network.%20The%20agent%20optimizes%20the%20multiplier%20structure%20using%20a%0APareto-driven%20reward%20customized%20to%20balance%20area%20and%20delay.%20Furthermore%2C%20we%0Aenhance%20the%20original%20framework%20with%20parallel%20reinforcement%20learning%20and%20design%0Aspace%20pruning%20techniques%20and%20extend%20its%20capability%20to%20optimize%20fused%0Amultiply-accumulate%20%28MAC%29%20designs.%20Experiments%20conducted%20on%20different%20bit%0Awidths%20of%20multipliers%20demonstrate%20that%20multipliers%20produced%20by%20our%20approach%0Aoutperform%20all%20baseline%20designs%20in%20terms%20of%20area%2C%20power%2C%20and%20delay.%20The%0Aperformance%20gain%20is%20further%20validated%20by%20comparing%20the%20area%2C%20power%2C%20and%20delay%0Aof%20processing%20element%20arrays%20using%20multipliers%20from%20our%20approach%20and%20baseline%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00639v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-MUL%25202.0%253A%2520Multiplier%2520Design%2520Optimization%2520with%2520Parallel%2520Deep%250A%2520%2520Reinforcement%2520Learning%2520and%2520Space%2520Reduction%26entry.906535625%3DDongsheng%2520Zuo%2520and%2520Jiadong%2520Zhu%2520and%2520Yikang%2520Ouyang%2520and%2520Yuzhe%2520Ma%26entry.1292438233%3D%2520%2520Multiplication%2520is%2520a%2520fundamental%2520operation%2520in%2520many%2520applications%252C%2520and%250Amultipliers%2520are%2520widely%2520adopted%2520in%2520various%2520circuits.%2520However%252C%2520optimizing%250Amultipliers%2520is%2520challenging%2520due%2520to%2520the%2520extensive%2520design%2520space.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520multiplier%2520design%2520optimization%2520framework%2520based%2520on%2520reinforcement%250Alearning.%2520We%2520utilize%2520matrix%2520and%2520tensor%2520representations%2520for%2520the%2520compressor%2520tree%250Aof%2520a%2520multiplier%252C%2520enabling%2520seamless%2520integration%2520of%2520convolutional%2520neural%2520networks%250Aas%2520the%2520agent%2520network.%2520The%2520agent%2520optimizes%2520the%2520multiplier%2520structure%2520using%2520a%250APareto-driven%2520reward%2520customized%2520to%2520balance%2520area%2520and%2520delay.%2520Furthermore%252C%2520we%250Aenhance%2520the%2520original%2520framework%2520with%2520parallel%2520reinforcement%2520learning%2520and%2520design%250Aspace%2520pruning%2520techniques%2520and%2520extend%2520its%2520capability%2520to%2520optimize%2520fused%250Amultiply-accumulate%2520%2528MAC%2529%2520designs.%2520Experiments%2520conducted%2520on%2520different%2520bit%250Awidths%2520of%2520multipliers%2520demonstrate%2520that%2520multipliers%2520produced%2520by%2520our%2520approach%250Aoutperform%2520all%2520baseline%2520designs%2520in%2520terms%2520of%2520area%252C%2520power%252C%2520and%2520delay.%2520The%250Aperformance%2520gain%2520is%2520further%2520validated%2520by%2520comparing%2520the%2520area%252C%2520power%252C%2520and%2520delay%250Aof%2520processing%2520element%2520arrays%2520using%2520multipliers%2520from%2520our%2520approach%2520and%2520baseline%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00639v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-MUL%202.0%3A%20Multiplier%20Design%20Optimization%20with%20Parallel%20Deep%0A%20%20Reinforcement%20Learning%20and%20Space%20Reduction&entry.906535625=Dongsheng%20Zuo%20and%20Jiadong%20Zhu%20and%20Yikang%20Ouyang%20and%20Yuzhe%20Ma&entry.1292438233=%20%20Multiplication%20is%20a%20fundamental%20operation%20in%20many%20applications%2C%20and%0Amultipliers%20are%20widely%20adopted%20in%20various%20circuits.%20However%2C%20optimizing%0Amultipliers%20is%20challenging%20due%20to%20the%20extensive%20design%20space.%20In%20this%20paper%2C%20we%0Apropose%20a%20multiplier%20design%20optimization%20framework%20based%20on%20reinforcement%0Alearning.%20We%20utilize%20matrix%20and%20tensor%20representations%20for%20the%20compressor%20tree%0Aof%20a%20multiplier%2C%20enabling%20seamless%20integration%20of%20convolutional%20neural%20networks%0Aas%20the%20agent%20network.%20The%20agent%20optimizes%20the%20multiplier%20structure%20using%20a%0APareto-driven%20reward%20customized%20to%20balance%20area%20and%20delay.%20Furthermore%2C%20we%0Aenhance%20the%20original%20framework%20with%20parallel%20reinforcement%20learning%20and%20design%0Aspace%20pruning%20techniques%20and%20extend%20its%20capability%20to%20optimize%20fused%0Amultiply-accumulate%20%28MAC%29%20designs.%20Experiments%20conducted%20on%20different%20bit%0Awidths%20of%20multipliers%20demonstrate%20that%20multipliers%20produced%20by%20our%20approach%0Aoutperform%20all%20baseline%20designs%20in%20terms%20of%20area%2C%20power%2C%20and%20delay.%20The%0Aperformance%20gain%20is%20further%20validated%20by%20comparing%20the%20area%2C%20power%2C%20and%20delay%0Aof%20processing%20element%20arrays%20using%20multipliers%20from%20our%20approach%20and%20baseline%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00639v2&entry.124074799=Read"},
{"title": "Context-aware Inductive Knowledge Graph Completion with Latent Type\n  Constraints and Subgraph Reasoning", "author": "Muzhi Li and Cehao Yang and Chengjin Xu and Zixing Song and Xuhui Jiang and Jian Guo and Ho-fung Leung and Irwin King", "abstract": "  Inductive knowledge graph completion (KGC) aims to predict missing triples\nwith unseen entities. Recent works focus on modeling reasoning paths between\nthe head and tail entity as direct supporting evidence. However, these methods\ndepend heavily on the existence and quality of reasoning paths, which limits\ntheir general applicability in different scenarios. In addition, we observe\nthat latent type constraints and neighboring facts inherent in KGs are also\nvital in inferring missing triples. To effectively utilize all useful\ninformation in KGs, we introduce CATS, a novel context-aware inductive KGC\nsolution. With sufficient guidance from proper prompts and supervised\nfine-tuning, CATS activates the strong semantic understanding and reasoning\ncapabilities of large language models to assess the existence of query triples,\nwhich consist of two modules. First, the type-aware reasoning module evaluates\nwhether the candidate entity matches the latent entity type as required by the\nquery relation. Then, the subgraph reasoning module selects relevant reasoning\npaths and neighboring facts, and evaluates their correlation to the query\ntriple. Experiment results on three widely used datasets demonstrate that CATS\nsignificantly outperforms state-of-the-art methods in 16 out of 18\ntransductive, inductive, and few-shot settings with an average absolute MRR\nimprovement of 7.2%.\n", "link": "http://arxiv.org/abs/2410.16803v3", "date": "2024-12-27", "relevancy": 1.4655, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5059}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-aware%20Inductive%20Knowledge%20Graph%20Completion%20with%20Latent%20Type%0A%20%20Constraints%20and%20Subgraph%20Reasoning&body=Title%3A%20Context-aware%20Inductive%20Knowledge%20Graph%20Completion%20with%20Latent%20Type%0A%20%20Constraints%20and%20Subgraph%20Reasoning%0AAuthor%3A%20Muzhi%20Li%20and%20Cehao%20Yang%20and%20Chengjin%20Xu%20and%20Zixing%20Song%20and%20Xuhui%20Jiang%20and%20Jian%20Guo%20and%20Ho-fung%20Leung%20and%20Irwin%20King%0AAbstract%3A%20%20%20Inductive%20knowledge%20graph%20completion%20%28KGC%29%20aims%20to%20predict%20missing%20triples%0Awith%20unseen%20entities.%20Recent%20works%20focus%20on%20modeling%20reasoning%20paths%20between%0Athe%20head%20and%20tail%20entity%20as%20direct%20supporting%20evidence.%20However%2C%20these%20methods%0Adepend%20heavily%20on%20the%20existence%20and%20quality%20of%20reasoning%20paths%2C%20which%20limits%0Atheir%20general%20applicability%20in%20different%20scenarios.%20In%20addition%2C%20we%20observe%0Athat%20latent%20type%20constraints%20and%20neighboring%20facts%20inherent%20in%20KGs%20are%20also%0Avital%20in%20inferring%20missing%20triples.%20To%20effectively%20utilize%20all%20useful%0Ainformation%20in%20KGs%2C%20we%20introduce%20CATS%2C%20a%20novel%20context-aware%20inductive%20KGC%0Asolution.%20With%20sufficient%20guidance%20from%20proper%20prompts%20and%20supervised%0Afine-tuning%2C%20CATS%20activates%20the%20strong%20semantic%20understanding%20and%20reasoning%0Acapabilities%20of%20large%20language%20models%20to%20assess%20the%20existence%20of%20query%20triples%2C%0Awhich%20consist%20of%20two%20modules.%20First%2C%20the%20type-aware%20reasoning%20module%20evaluates%0Awhether%20the%20candidate%20entity%20matches%20the%20latent%20entity%20type%20as%20required%20by%20the%0Aquery%20relation.%20Then%2C%20the%20subgraph%20reasoning%20module%20selects%20relevant%20reasoning%0Apaths%20and%20neighboring%20facts%2C%20and%20evaluates%20their%20correlation%20to%20the%20query%0Atriple.%20Experiment%20results%20on%20three%20widely%20used%20datasets%20demonstrate%20that%20CATS%0Asignificantly%20outperforms%20state-of-the-art%20methods%20in%2016%20out%20of%2018%0Atransductive%2C%20inductive%2C%20and%20few-shot%20settings%20with%20an%20average%20absolute%20MRR%0Aimprovement%20of%207.2%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16803v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-aware%2520Inductive%2520Knowledge%2520Graph%2520Completion%2520with%2520Latent%2520Type%250A%2520%2520Constraints%2520and%2520Subgraph%2520Reasoning%26entry.906535625%3DMuzhi%2520Li%2520and%2520Cehao%2520Yang%2520and%2520Chengjin%2520Xu%2520and%2520Zixing%2520Song%2520and%2520Xuhui%2520Jiang%2520and%2520Jian%2520Guo%2520and%2520Ho-fung%2520Leung%2520and%2520Irwin%2520King%26entry.1292438233%3D%2520%2520Inductive%2520knowledge%2520graph%2520completion%2520%2528KGC%2529%2520aims%2520to%2520predict%2520missing%2520triples%250Awith%2520unseen%2520entities.%2520Recent%2520works%2520focus%2520on%2520modeling%2520reasoning%2520paths%2520between%250Athe%2520head%2520and%2520tail%2520entity%2520as%2520direct%2520supporting%2520evidence.%2520However%252C%2520these%2520methods%250Adepend%2520heavily%2520on%2520the%2520existence%2520and%2520quality%2520of%2520reasoning%2520paths%252C%2520which%2520limits%250Atheir%2520general%2520applicability%2520in%2520different%2520scenarios.%2520In%2520addition%252C%2520we%2520observe%250Athat%2520latent%2520type%2520constraints%2520and%2520neighboring%2520facts%2520inherent%2520in%2520KGs%2520are%2520also%250Avital%2520in%2520inferring%2520missing%2520triples.%2520To%2520effectively%2520utilize%2520all%2520useful%250Ainformation%2520in%2520KGs%252C%2520we%2520introduce%2520CATS%252C%2520a%2520novel%2520context-aware%2520inductive%2520KGC%250Asolution.%2520With%2520sufficient%2520guidance%2520from%2520proper%2520prompts%2520and%2520supervised%250Afine-tuning%252C%2520CATS%2520activates%2520the%2520strong%2520semantic%2520understanding%2520and%2520reasoning%250Acapabilities%2520of%2520large%2520language%2520models%2520to%2520assess%2520the%2520existence%2520of%2520query%2520triples%252C%250Awhich%2520consist%2520of%2520two%2520modules.%2520First%252C%2520the%2520type-aware%2520reasoning%2520module%2520evaluates%250Awhether%2520the%2520candidate%2520entity%2520matches%2520the%2520latent%2520entity%2520type%2520as%2520required%2520by%2520the%250Aquery%2520relation.%2520Then%252C%2520the%2520subgraph%2520reasoning%2520module%2520selects%2520relevant%2520reasoning%250Apaths%2520and%2520neighboring%2520facts%252C%2520and%2520evaluates%2520their%2520correlation%2520to%2520the%2520query%250Atriple.%2520Experiment%2520results%2520on%2520three%2520widely%2520used%2520datasets%2520demonstrate%2520that%2520CATS%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520in%252016%2520out%2520of%252018%250Atransductive%252C%2520inductive%252C%2520and%2520few-shot%2520settings%2520with%2520an%2520average%2520absolute%2520MRR%250Aimprovement%2520of%25207.2%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16803v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-aware%20Inductive%20Knowledge%20Graph%20Completion%20with%20Latent%20Type%0A%20%20Constraints%20and%20Subgraph%20Reasoning&entry.906535625=Muzhi%20Li%20and%20Cehao%20Yang%20and%20Chengjin%20Xu%20and%20Zixing%20Song%20and%20Xuhui%20Jiang%20and%20Jian%20Guo%20and%20Ho-fung%20Leung%20and%20Irwin%20King&entry.1292438233=%20%20Inductive%20knowledge%20graph%20completion%20%28KGC%29%20aims%20to%20predict%20missing%20triples%0Awith%20unseen%20entities.%20Recent%20works%20focus%20on%20modeling%20reasoning%20paths%20between%0Athe%20head%20and%20tail%20entity%20as%20direct%20supporting%20evidence.%20However%2C%20these%20methods%0Adepend%20heavily%20on%20the%20existence%20and%20quality%20of%20reasoning%20paths%2C%20which%20limits%0Atheir%20general%20applicability%20in%20different%20scenarios.%20In%20addition%2C%20we%20observe%0Athat%20latent%20type%20constraints%20and%20neighboring%20facts%20inherent%20in%20KGs%20are%20also%0Avital%20in%20inferring%20missing%20triples.%20To%20effectively%20utilize%20all%20useful%0Ainformation%20in%20KGs%2C%20we%20introduce%20CATS%2C%20a%20novel%20context-aware%20inductive%20KGC%0Asolution.%20With%20sufficient%20guidance%20from%20proper%20prompts%20and%20supervised%0Afine-tuning%2C%20CATS%20activates%20the%20strong%20semantic%20understanding%20and%20reasoning%0Acapabilities%20of%20large%20language%20models%20to%20assess%20the%20existence%20of%20query%20triples%2C%0Awhich%20consist%20of%20two%20modules.%20First%2C%20the%20type-aware%20reasoning%20module%20evaluates%0Awhether%20the%20candidate%20entity%20matches%20the%20latent%20entity%20type%20as%20required%20by%20the%0Aquery%20relation.%20Then%2C%20the%20subgraph%20reasoning%20module%20selects%20relevant%20reasoning%0Apaths%20and%20neighboring%20facts%2C%20and%20evaluates%20their%20correlation%20to%20the%20query%0Atriple.%20Experiment%20results%20on%20three%20widely%20used%20datasets%20demonstrate%20that%20CATS%0Asignificantly%20outperforms%20state-of-the-art%20methods%20in%2016%20out%20of%2018%0Atransductive%2C%20inductive%2C%20and%20few-shot%20settings%20with%20an%20average%20absolute%20MRR%0Aimprovement%20of%207.2%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16803v3&entry.124074799=Read"},
{"title": "A Comparative Study of Machine Unlearning Techniques for Image and Text\n  Classification Models", "author": "Omar M. Safa and Mahmoud M. Abdelaziz and Mustafa Eltawy and Mohamed Mamdouh and Moamen Gharib and Salaheldin Eltenihy and Nagia M. Ghanem and Mohamed M. Ismail", "abstract": "  Machine Unlearning has emerged as a critical area in artificial intelligence,\naddressing the need to selectively remove learned data from machine learning\nmodels in response to data privacy regulations. This paper provides a\ncomprehensive comparative analysis of six state-of-theart unlearning techniques\napplied to image and text classification tasks. We evaluate their performance,\nefficiency, and compliance with regulatory requirements, highlighting their\nstrengths and limitations in practical scenarios. By systematically analyzing\nthese methods, we aim to provide insights into their applicability,\nchallenges,and tradeoffs, fostering advancements in the field of ethical and\nadaptable machine learning.\n", "link": "http://arxiv.org/abs/2412.19583v1", "date": "2024-12-27", "relevancy": 1.4389, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4916}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4849}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Study%20of%20Machine%20Unlearning%20Techniques%20for%20Image%20and%20Text%0A%20%20Classification%20Models&body=Title%3A%20A%20Comparative%20Study%20of%20Machine%20Unlearning%20Techniques%20for%20Image%20and%20Text%0A%20%20Classification%20Models%0AAuthor%3A%20Omar%20M.%20Safa%20and%20Mahmoud%20M.%20Abdelaziz%20and%20Mustafa%20Eltawy%20and%20Mohamed%20Mamdouh%20and%20Moamen%20Gharib%20and%20Salaheldin%20Eltenihy%20and%20Nagia%20M.%20Ghanem%20and%20Mohamed%20M.%20Ismail%0AAbstract%3A%20%20%20Machine%20Unlearning%20has%20emerged%20as%20a%20critical%20area%20in%20artificial%20intelligence%2C%0Aaddressing%20the%20need%20to%20selectively%20remove%20learned%20data%20from%20machine%20learning%0Amodels%20in%20response%20to%20data%20privacy%20regulations.%20This%20paper%20provides%20a%0Acomprehensive%20comparative%20analysis%20of%20six%20state-of-theart%20unlearning%20techniques%0Aapplied%20to%20image%20and%20text%20classification%20tasks.%20We%20evaluate%20their%20performance%2C%0Aefficiency%2C%20and%20compliance%20with%20regulatory%20requirements%2C%20highlighting%20their%0Astrengths%20and%20limitations%20in%20practical%20scenarios.%20By%20systematically%20analyzing%0Athese%20methods%2C%20we%20aim%20to%20provide%20insights%20into%20their%20applicability%2C%0Achallenges%2Cand%20tradeoffs%2C%20fostering%20advancements%20in%20the%20field%20of%20ethical%20and%0Aadaptable%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Study%2520of%2520Machine%2520Unlearning%2520Techniques%2520for%2520Image%2520and%2520Text%250A%2520%2520Classification%2520Models%26entry.906535625%3DOmar%2520M.%2520Safa%2520and%2520Mahmoud%2520M.%2520Abdelaziz%2520and%2520Mustafa%2520Eltawy%2520and%2520Mohamed%2520Mamdouh%2520and%2520Moamen%2520Gharib%2520and%2520Salaheldin%2520Eltenihy%2520and%2520Nagia%2520M.%2520Ghanem%2520and%2520Mohamed%2520M.%2520Ismail%26entry.1292438233%3D%2520%2520Machine%2520Unlearning%2520has%2520emerged%2520as%2520a%2520critical%2520area%2520in%2520artificial%2520intelligence%252C%250Aaddressing%2520the%2520need%2520to%2520selectively%2520remove%2520learned%2520data%2520from%2520machine%2520learning%250Amodels%2520in%2520response%2520to%2520data%2520privacy%2520regulations.%2520This%2520paper%2520provides%2520a%250Acomprehensive%2520comparative%2520analysis%2520of%2520six%2520state-of-theart%2520unlearning%2520techniques%250Aapplied%2520to%2520image%2520and%2520text%2520classification%2520tasks.%2520We%2520evaluate%2520their%2520performance%252C%250Aefficiency%252C%2520and%2520compliance%2520with%2520regulatory%2520requirements%252C%2520highlighting%2520their%250Astrengths%2520and%2520limitations%2520in%2520practical%2520scenarios.%2520By%2520systematically%2520analyzing%250Athese%2520methods%252C%2520we%2520aim%2520to%2520provide%2520insights%2520into%2520their%2520applicability%252C%250Achallenges%252Cand%2520tradeoffs%252C%2520fostering%2520advancements%2520in%2520the%2520field%2520of%2520ethical%2520and%250Aadaptable%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Study%20of%20Machine%20Unlearning%20Techniques%20for%20Image%20and%20Text%0A%20%20Classification%20Models&entry.906535625=Omar%20M.%20Safa%20and%20Mahmoud%20M.%20Abdelaziz%20and%20Mustafa%20Eltawy%20and%20Mohamed%20Mamdouh%20and%20Moamen%20Gharib%20and%20Salaheldin%20Eltenihy%20and%20Nagia%20M.%20Ghanem%20and%20Mohamed%20M.%20Ismail&entry.1292438233=%20%20Machine%20Unlearning%20has%20emerged%20as%20a%20critical%20area%20in%20artificial%20intelligence%2C%0Aaddressing%20the%20need%20to%20selectively%20remove%20learned%20data%20from%20machine%20learning%0Amodels%20in%20response%20to%20data%20privacy%20regulations.%20This%20paper%20provides%20a%0Acomprehensive%20comparative%20analysis%20of%20six%20state-of-theart%20unlearning%20techniques%0Aapplied%20to%20image%20and%20text%20classification%20tasks.%20We%20evaluate%20their%20performance%2C%0Aefficiency%2C%20and%20compliance%20with%20regulatory%20requirements%2C%20highlighting%20their%0Astrengths%20and%20limitations%20in%20practical%20scenarios.%20By%20systematically%20analyzing%0Athese%20methods%2C%20we%20aim%20to%20provide%20insights%20into%20their%20applicability%2C%0Achallenges%2Cand%20tradeoffs%2C%20fostering%20advancements%20in%20the%20field%20of%20ethical%20and%0Aadaptable%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19583v1&entry.124074799=Read"},
{"title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback", "author": "Sijia Chen and Baochun Li", "abstract": "  Large language models (LLMs) have been routinely used to solve various tasks\nusing step-by-step reasoning. However, the structure of intermediate reasoning\nsteps, or thoughts, is rigid and unidirectional, such as chains, trees, or\nacyclic-directed graphs. Consequently, the resulting inflexible and\nforward-only reasoning may not address challenging tasks and fail when the LLM\nfrequently gives false responses, i.e., ``hallucinations''. This paper proposes\na new reasoning framework, called Thought Rollback (TR), allowing LLMs to\nadaptively build thought structure while maintaining effective reasoning toward\nproblem-solving under ``hallucinations''. The core mechanism of TR is rolling\nback thoughts, which allows LLMs to perform error analysis on thoughts, and\nthus roll back to any previously mistaken thought for revision. Subsequently,\nby including such trial-and-error in the prompt to guide the LLM, each rollback\nleads to one more reliable reasoning path. Therefore, starting with a simple\nprompt without human annotations, LLM with TR adaptively and gradually explores\nthoughts for a correct solution. Comprehensive experiments on mathematical\nproblems and multi-task reasoning demonstrate the state-of-the-art performance\nof TR in terms of problem-solving rate and interaction cost. For instance, the\nsolving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH\ndataset.\n", "link": "http://arxiv.org/abs/2412.19707v1", "date": "2024-12-27", "relevancy": 1.4294, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4915}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4628}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Adaptive%20Reasoning%20in%20Large%20Language%20Models%20with%20Thought%20Rollback&body=Title%3A%20Toward%20Adaptive%20Reasoning%20in%20Large%20Language%20Models%20with%20Thought%20Rollback%0AAuthor%3A%20Sijia%20Chen%20and%20Baochun%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20routinely%20used%20to%20solve%20various%20tasks%0Ausing%20step-by-step%20reasoning.%20However%2C%20the%20structure%20of%20intermediate%20reasoning%0Asteps%2C%20or%20thoughts%2C%20is%20rigid%20and%20unidirectional%2C%20such%20as%20chains%2C%20trees%2C%20or%0Aacyclic-directed%20graphs.%20Consequently%2C%20the%20resulting%20inflexible%20and%0Aforward-only%20reasoning%20may%20not%20address%20challenging%20tasks%20and%20fail%20when%20the%20LLM%0Afrequently%20gives%20false%20responses%2C%20i.e.%2C%20%60%60hallucinations%27%27.%20This%20paper%20proposes%0Aa%20new%20reasoning%20framework%2C%20called%20Thought%20Rollback%20%28TR%29%2C%20allowing%20LLMs%20to%0Aadaptively%20build%20thought%20structure%20while%20maintaining%20effective%20reasoning%20toward%0Aproblem-solving%20under%20%60%60hallucinations%27%27.%20The%20core%20mechanism%20of%20TR%20is%20rolling%0Aback%20thoughts%2C%20which%20allows%20LLMs%20to%20perform%20error%20analysis%20on%20thoughts%2C%20and%0Athus%20roll%20back%20to%20any%20previously%20mistaken%20thought%20for%20revision.%20Subsequently%2C%0Aby%20including%20such%20trial-and-error%20in%20the%20prompt%20to%20guide%20the%20LLM%2C%20each%20rollback%0Aleads%20to%20one%20more%20reliable%20reasoning%20path.%20Therefore%2C%20starting%20with%20a%20simple%0Aprompt%20without%20human%20annotations%2C%20LLM%20with%20TR%20adaptively%20and%20gradually%20explores%0Athoughts%20for%20a%20correct%20solution.%20Comprehensive%20experiments%20on%20mathematical%0Aproblems%20and%20multi-task%20reasoning%20demonstrate%20the%20state-of-the-art%20performance%0Aof%20TR%20in%20terms%20of%20problem-solving%20rate%20and%20interaction%20cost.%20For%20instance%2C%20the%0Asolving%20rate%20of%20GPT-4%20with%20TR%20outperforms%20the%20current%20best%20by%20%249%5C%25%24%20on%20the%20MATH%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Adaptive%2520Reasoning%2520in%2520Large%2520Language%2520Models%2520with%2520Thought%2520Rollback%26entry.906535625%3DSijia%2520Chen%2520and%2520Baochun%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520routinely%2520used%2520to%2520solve%2520various%2520tasks%250Ausing%2520step-by-step%2520reasoning.%2520However%252C%2520the%2520structure%2520of%2520intermediate%2520reasoning%250Asteps%252C%2520or%2520thoughts%252C%2520is%2520rigid%2520and%2520unidirectional%252C%2520such%2520as%2520chains%252C%2520trees%252C%2520or%250Aacyclic-directed%2520graphs.%2520Consequently%252C%2520the%2520resulting%2520inflexible%2520and%250Aforward-only%2520reasoning%2520may%2520not%2520address%2520challenging%2520tasks%2520and%2520fail%2520when%2520the%2520LLM%250Afrequently%2520gives%2520false%2520responses%252C%2520i.e.%252C%2520%2560%2560hallucinations%2527%2527.%2520This%2520paper%2520proposes%250Aa%2520new%2520reasoning%2520framework%252C%2520called%2520Thought%2520Rollback%2520%2528TR%2529%252C%2520allowing%2520LLMs%2520to%250Aadaptively%2520build%2520thought%2520structure%2520while%2520maintaining%2520effective%2520reasoning%2520toward%250Aproblem-solving%2520under%2520%2560%2560hallucinations%2527%2527.%2520The%2520core%2520mechanism%2520of%2520TR%2520is%2520rolling%250Aback%2520thoughts%252C%2520which%2520allows%2520LLMs%2520to%2520perform%2520error%2520analysis%2520on%2520thoughts%252C%2520and%250Athus%2520roll%2520back%2520to%2520any%2520previously%2520mistaken%2520thought%2520for%2520revision.%2520Subsequently%252C%250Aby%2520including%2520such%2520trial-and-error%2520in%2520the%2520prompt%2520to%2520guide%2520the%2520LLM%252C%2520each%2520rollback%250Aleads%2520to%2520one%2520more%2520reliable%2520reasoning%2520path.%2520Therefore%252C%2520starting%2520with%2520a%2520simple%250Aprompt%2520without%2520human%2520annotations%252C%2520LLM%2520with%2520TR%2520adaptively%2520and%2520gradually%2520explores%250Athoughts%2520for%2520a%2520correct%2520solution.%2520Comprehensive%2520experiments%2520on%2520mathematical%250Aproblems%2520and%2520multi-task%2520reasoning%2520demonstrate%2520the%2520state-of-the-art%2520performance%250Aof%2520TR%2520in%2520terms%2520of%2520problem-solving%2520rate%2520and%2520interaction%2520cost.%2520For%2520instance%252C%2520the%250Asolving%2520rate%2520of%2520GPT-4%2520with%2520TR%2520outperforms%2520the%2520current%2520best%2520by%2520%25249%255C%2525%2524%2520on%2520the%2520MATH%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Adaptive%20Reasoning%20in%20Large%20Language%20Models%20with%20Thought%20Rollback&entry.906535625=Sijia%20Chen%20and%20Baochun%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20routinely%20used%20to%20solve%20various%20tasks%0Ausing%20step-by-step%20reasoning.%20However%2C%20the%20structure%20of%20intermediate%20reasoning%0Asteps%2C%20or%20thoughts%2C%20is%20rigid%20and%20unidirectional%2C%20such%20as%20chains%2C%20trees%2C%20or%0Aacyclic-directed%20graphs.%20Consequently%2C%20the%20resulting%20inflexible%20and%0Aforward-only%20reasoning%20may%20not%20address%20challenging%20tasks%20and%20fail%20when%20the%20LLM%0Afrequently%20gives%20false%20responses%2C%20i.e.%2C%20%60%60hallucinations%27%27.%20This%20paper%20proposes%0Aa%20new%20reasoning%20framework%2C%20called%20Thought%20Rollback%20%28TR%29%2C%20allowing%20LLMs%20to%0Aadaptively%20build%20thought%20structure%20while%20maintaining%20effective%20reasoning%20toward%0Aproblem-solving%20under%20%60%60hallucinations%27%27.%20The%20core%20mechanism%20of%20TR%20is%20rolling%0Aback%20thoughts%2C%20which%20allows%20LLMs%20to%20perform%20error%20analysis%20on%20thoughts%2C%20and%0Athus%20roll%20back%20to%20any%20previously%20mistaken%20thought%20for%20revision.%20Subsequently%2C%0Aby%20including%20such%20trial-and-error%20in%20the%20prompt%20to%20guide%20the%20LLM%2C%20each%20rollback%0Aleads%20to%20one%20more%20reliable%20reasoning%20path.%20Therefore%2C%20starting%20with%20a%20simple%0Aprompt%20without%20human%20annotations%2C%20LLM%20with%20TR%20adaptively%20and%20gradually%20explores%0Athoughts%20for%20a%20correct%20solution.%20Comprehensive%20experiments%20on%20mathematical%0Aproblems%20and%20multi-task%20reasoning%20demonstrate%20the%20state-of-the-art%20performance%0Aof%20TR%20in%20terms%20of%20problem-solving%20rate%20and%20interaction%20cost.%20For%20instance%2C%20the%0Asolving%20rate%20of%20GPT-4%20with%20TR%20outperforms%20the%20current%20best%20by%20%249%5C%25%24%20on%20the%20MATH%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19707v1&entry.124074799=Read"},
{"title": "An Integrated Optimization and Deep Learning Pipeline for Predicting\n  Live Birth Success in IVF Using Feature Optimization and Transformer-Based\n  Models", "author": "Arezoo Borji and Hossam Haick and Birgit Pohn and Antonia Graf and Jana Zakall and S M Ragib Shahriar Islam and Gernot Kronreif and Daniel Kovatchki and Heinz Strohmer and Sepideh Hatamikia", "abstract": "  In vitro fertilization (IVF) is a widely utilized assisted reproductive\ntechnology, yet predicting its success remains challenging due to the\nmultifaceted interplay of clinical, demographic, and procedural factors. This\nstudy develops a robust artificial intelligence (AI) pipeline aimed at\npredicting live birth outcomes in IVF treatments. The pipeline uses anonymized\ndata from 2010 to 2018, obtained from the Human Fertilization and Embryology\nAuthority (HFEA). We evaluated the prediction performance of live birth success\nas a binary outcome (success/failure) by integrating different feature\nselection methods, such as principal component analysis (PCA) and particle\nswarm optimization (PSO), with different traditional machine learning-based\nclassifiers including random forest (RF) and decision tree, as well as deep\nlearning-based classifiers including custom transformer-based model and a tab\ntransformer model with an attention mechanism. Our research demonstrated that\nthe best performance was achieved by combining PSO for feature selection with\nthe TabTransformer-based deep learning model, yielding an accuracy of 99.50%\nand an AUC of 99.96%, highlighting its significant performance to predict live\nbirths. This study establishes a highly accurate AI pipeline for predicting\nlive birth outcomes in IVF, demonstrating its potential to enhance personalized\nfertility treatments.\n", "link": "http://arxiv.org/abs/2412.19696v1", "date": "2024-12-27", "relevancy": 1.3633, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4494}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Integrated%20Optimization%20and%20Deep%20Learning%20Pipeline%20for%20Predicting%0A%20%20Live%20Birth%20Success%20in%20IVF%20Using%20Feature%20Optimization%20and%20Transformer-Based%0A%20%20Models&body=Title%3A%20An%20Integrated%20Optimization%20and%20Deep%20Learning%20Pipeline%20for%20Predicting%0A%20%20Live%20Birth%20Success%20in%20IVF%20Using%20Feature%20Optimization%20and%20Transformer-Based%0A%20%20Models%0AAuthor%3A%20Arezoo%20Borji%20and%20Hossam%20Haick%20and%20Birgit%20Pohn%20and%20Antonia%20Graf%20and%20Jana%20Zakall%20and%20S%20M%20Ragib%20Shahriar%20Islam%20and%20Gernot%20Kronreif%20and%20Daniel%20Kovatchki%20and%20Heinz%20Strohmer%20and%20Sepideh%20Hatamikia%0AAbstract%3A%20%20%20In%20vitro%20fertilization%20%28IVF%29%20is%20a%20widely%20utilized%20assisted%20reproductive%0Atechnology%2C%20yet%20predicting%20its%20success%20remains%20challenging%20due%20to%20the%0Amultifaceted%20interplay%20of%20clinical%2C%20demographic%2C%20and%20procedural%20factors.%20This%0Astudy%20develops%20a%20robust%20artificial%20intelligence%20%28AI%29%20pipeline%20aimed%20at%0Apredicting%20live%20birth%20outcomes%20in%20IVF%20treatments.%20The%20pipeline%20uses%20anonymized%0Adata%20from%202010%20to%202018%2C%20obtained%20from%20the%20Human%20Fertilization%20and%20Embryology%0AAuthority%20%28HFEA%29.%20We%20evaluated%20the%20prediction%20performance%20of%20live%20birth%20success%0Aas%20a%20binary%20outcome%20%28success/failure%29%20by%20integrating%20different%20feature%0Aselection%20methods%2C%20such%20as%20principal%20component%20analysis%20%28PCA%29%20and%20particle%0Aswarm%20optimization%20%28PSO%29%2C%20with%20different%20traditional%20machine%20learning-based%0Aclassifiers%20including%20random%20forest%20%28RF%29%20and%20decision%20tree%2C%20as%20well%20as%20deep%0Alearning-based%20classifiers%20including%20custom%20transformer-based%20model%20and%20a%20tab%0Atransformer%20model%20with%20an%20attention%20mechanism.%20Our%20research%20demonstrated%20that%0Athe%20best%20performance%20was%20achieved%20by%20combining%20PSO%20for%20feature%20selection%20with%0Athe%20TabTransformer-based%20deep%20learning%20model%2C%20yielding%20an%20accuracy%20of%2099.50%25%0Aand%20an%20AUC%20of%2099.96%25%2C%20highlighting%20its%20significant%20performance%20to%20predict%20live%0Abirths.%20This%20study%20establishes%20a%20highly%20accurate%20AI%20pipeline%20for%20predicting%0Alive%20birth%20outcomes%20in%20IVF%2C%20demonstrating%20its%20potential%20to%20enhance%20personalized%0Afertility%20treatments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Integrated%2520Optimization%2520and%2520Deep%2520Learning%2520Pipeline%2520for%2520Predicting%250A%2520%2520Live%2520Birth%2520Success%2520in%2520IVF%2520Using%2520Feature%2520Optimization%2520and%2520Transformer-Based%250A%2520%2520Models%26entry.906535625%3DArezoo%2520Borji%2520and%2520Hossam%2520Haick%2520and%2520Birgit%2520Pohn%2520and%2520Antonia%2520Graf%2520and%2520Jana%2520Zakall%2520and%2520S%2520M%2520Ragib%2520Shahriar%2520Islam%2520and%2520Gernot%2520Kronreif%2520and%2520Daniel%2520Kovatchki%2520and%2520Heinz%2520Strohmer%2520and%2520Sepideh%2520Hatamikia%26entry.1292438233%3D%2520%2520In%2520vitro%2520fertilization%2520%2528IVF%2529%2520is%2520a%2520widely%2520utilized%2520assisted%2520reproductive%250Atechnology%252C%2520yet%2520predicting%2520its%2520success%2520remains%2520challenging%2520due%2520to%2520the%250Amultifaceted%2520interplay%2520of%2520clinical%252C%2520demographic%252C%2520and%2520procedural%2520factors.%2520This%250Astudy%2520develops%2520a%2520robust%2520artificial%2520intelligence%2520%2528AI%2529%2520pipeline%2520aimed%2520at%250Apredicting%2520live%2520birth%2520outcomes%2520in%2520IVF%2520treatments.%2520The%2520pipeline%2520uses%2520anonymized%250Adata%2520from%25202010%2520to%25202018%252C%2520obtained%2520from%2520the%2520Human%2520Fertilization%2520and%2520Embryology%250AAuthority%2520%2528HFEA%2529.%2520We%2520evaluated%2520the%2520prediction%2520performance%2520of%2520live%2520birth%2520success%250Aas%2520a%2520binary%2520outcome%2520%2528success/failure%2529%2520by%2520integrating%2520different%2520feature%250Aselection%2520methods%252C%2520such%2520as%2520principal%2520component%2520analysis%2520%2528PCA%2529%2520and%2520particle%250Aswarm%2520optimization%2520%2528PSO%2529%252C%2520with%2520different%2520traditional%2520machine%2520learning-based%250Aclassifiers%2520including%2520random%2520forest%2520%2528RF%2529%2520and%2520decision%2520tree%252C%2520as%2520well%2520as%2520deep%250Alearning-based%2520classifiers%2520including%2520custom%2520transformer-based%2520model%2520and%2520a%2520tab%250Atransformer%2520model%2520with%2520an%2520attention%2520mechanism.%2520Our%2520research%2520demonstrated%2520that%250Athe%2520best%2520performance%2520was%2520achieved%2520by%2520combining%2520PSO%2520for%2520feature%2520selection%2520with%250Athe%2520TabTransformer-based%2520deep%2520learning%2520model%252C%2520yielding%2520an%2520accuracy%2520of%252099.50%2525%250Aand%2520an%2520AUC%2520of%252099.96%2525%252C%2520highlighting%2520its%2520significant%2520performance%2520to%2520predict%2520live%250Abirths.%2520This%2520study%2520establishes%2520a%2520highly%2520accurate%2520AI%2520pipeline%2520for%2520predicting%250Alive%2520birth%2520outcomes%2520in%2520IVF%252C%2520demonstrating%2520its%2520potential%2520to%2520enhance%2520personalized%250Afertility%2520treatments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Integrated%20Optimization%20and%20Deep%20Learning%20Pipeline%20for%20Predicting%0A%20%20Live%20Birth%20Success%20in%20IVF%20Using%20Feature%20Optimization%20and%20Transformer-Based%0A%20%20Models&entry.906535625=Arezoo%20Borji%20and%20Hossam%20Haick%20and%20Birgit%20Pohn%20and%20Antonia%20Graf%20and%20Jana%20Zakall%20and%20S%20M%20Ragib%20Shahriar%20Islam%20and%20Gernot%20Kronreif%20and%20Daniel%20Kovatchki%20and%20Heinz%20Strohmer%20and%20Sepideh%20Hatamikia&entry.1292438233=%20%20In%20vitro%20fertilization%20%28IVF%29%20is%20a%20widely%20utilized%20assisted%20reproductive%0Atechnology%2C%20yet%20predicting%20its%20success%20remains%20challenging%20due%20to%20the%0Amultifaceted%20interplay%20of%20clinical%2C%20demographic%2C%20and%20procedural%20factors.%20This%0Astudy%20develops%20a%20robust%20artificial%20intelligence%20%28AI%29%20pipeline%20aimed%20at%0Apredicting%20live%20birth%20outcomes%20in%20IVF%20treatments.%20The%20pipeline%20uses%20anonymized%0Adata%20from%202010%20to%202018%2C%20obtained%20from%20the%20Human%20Fertilization%20and%20Embryology%0AAuthority%20%28HFEA%29.%20We%20evaluated%20the%20prediction%20performance%20of%20live%20birth%20success%0Aas%20a%20binary%20outcome%20%28success/failure%29%20by%20integrating%20different%20feature%0Aselection%20methods%2C%20such%20as%20principal%20component%20analysis%20%28PCA%29%20and%20particle%0Aswarm%20optimization%20%28PSO%29%2C%20with%20different%20traditional%20machine%20learning-based%0Aclassifiers%20including%20random%20forest%20%28RF%29%20and%20decision%20tree%2C%20as%20well%20as%20deep%0Alearning-based%20classifiers%20including%20custom%20transformer-based%20model%20and%20a%20tab%0Atransformer%20model%20with%20an%20attention%20mechanism.%20Our%20research%20demonstrated%20that%0Athe%20best%20performance%20was%20achieved%20by%20combining%20PSO%20for%20feature%20selection%20with%0Athe%20TabTransformer-based%20deep%20learning%20model%2C%20yielding%20an%20accuracy%20of%2099.50%25%0Aand%20an%20AUC%20of%2099.96%25%2C%20highlighting%20its%20significant%20performance%20to%20predict%20live%0Abirths.%20This%20study%20establishes%20a%20highly%20accurate%20AI%20pipeline%20for%20predicting%0Alive%20birth%20outcomes%20in%20IVF%2C%20demonstrating%20its%20potential%20to%20enhance%20personalized%0Afertility%20treatments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19696v1&entry.124074799=Read"},
{"title": "Language-Guided Diffusion Model for Visual Grounding", "author": "Sijia Chen and Baochun Li", "abstract": "  Visual grounding (VG) tasks involve explicit cross-modal alignment, as\nsemantically corresponding image regions are to be located for the language\nphrases provided. Existing approaches complete such visual-text reasoning in a\nsingle-step manner. Their performance causes high demands on large-scale\nanchors and over-designed multi-modal fusion modules based on human priors,\nleading to complicated frameworks that may be difficult to train and overfit to\nspecific scenarios. Even worse, such once-for-all reasoning mechanisms are\nincapable of refining boxes continuously to enhance query-region matching. In\ncontrast, in this paper, we formulate an iterative reasoning process by\ndenoising diffusion modeling. Specifically, we propose a language-guided\ndiffusion framework for visual grounding, LG-DVG, which trains the model to\nprogressively reason queried object boxes by denoising a set of noisy boxes\nwith the language guide. To achieve this, LG-DVG gradually perturbs\nquery-aligned ground truth boxes to noisy ones and reverses this process step\nby step, conditional on query semantics. Extensive experiments for our proposed\nframework on five widely used datasets validate the superior performance of\nsolving visual grounding, a cross-modal alignment task, in a generative way.\nThe source codes are available at\nhttps://github.com/iQua/vgbase/tree/main/examples/DiffusionVG.\n", "link": "http://arxiv.org/abs/2308.09599v2", "date": "2024-12-27", "relevancy": 1.1727, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5917}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5845}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Guided%20Diffusion%20Model%20for%20Visual%20Grounding&body=Title%3A%20Language-Guided%20Diffusion%20Model%20for%20Visual%20Grounding%0AAuthor%3A%20Sijia%20Chen%20and%20Baochun%20Li%0AAbstract%3A%20%20%20Visual%20grounding%20%28VG%29%20tasks%20involve%20explicit%20cross-modal%20alignment%2C%20as%0Asemantically%20corresponding%20image%20regions%20are%20to%20be%20located%20for%20the%20language%0Aphrases%20provided.%20Existing%20approaches%20complete%20such%20visual-text%20reasoning%20in%20a%0Asingle-step%20manner.%20Their%20performance%20causes%20high%20demands%20on%20large-scale%0Aanchors%20and%20over-designed%20multi-modal%20fusion%20modules%20based%20on%20human%20priors%2C%0Aleading%20to%20complicated%20frameworks%20that%20may%20be%20difficult%20to%20train%20and%20overfit%20to%0Aspecific%20scenarios.%20Even%20worse%2C%20such%20once-for-all%20reasoning%20mechanisms%20are%0Aincapable%20of%20refining%20boxes%20continuously%20to%20enhance%20query-region%20matching.%20In%0Acontrast%2C%20in%20this%20paper%2C%20we%20formulate%20an%20iterative%20reasoning%20process%20by%0Adenoising%20diffusion%20modeling.%20Specifically%2C%20we%20propose%20a%20language-guided%0Adiffusion%20framework%20for%20visual%20grounding%2C%20LG-DVG%2C%20which%20trains%20the%20model%20to%0Aprogressively%20reason%20queried%20object%20boxes%20by%20denoising%20a%20set%20of%20noisy%20boxes%0Awith%20the%20language%20guide.%20To%20achieve%20this%2C%20LG-DVG%20gradually%20perturbs%0Aquery-aligned%20ground%20truth%20boxes%20to%20noisy%20ones%20and%20reverses%20this%20process%20step%0Aby%20step%2C%20conditional%20on%20query%20semantics.%20Extensive%20experiments%20for%20our%20proposed%0Aframework%20on%20five%20widely%20used%20datasets%20validate%20the%20superior%20performance%20of%0Asolving%20visual%20grounding%2C%20a%20cross-modal%20alignment%20task%2C%20in%20a%20generative%20way.%0AThe%20source%20codes%20are%20available%20at%0Ahttps%3A//github.com/iQua/vgbase/tree/main/examples/DiffusionVG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09599v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Guided%2520Diffusion%2520Model%2520for%2520Visual%2520Grounding%26entry.906535625%3DSijia%2520Chen%2520and%2520Baochun%2520Li%26entry.1292438233%3D%2520%2520Visual%2520grounding%2520%2528VG%2529%2520tasks%2520involve%2520explicit%2520cross-modal%2520alignment%252C%2520as%250Asemantically%2520corresponding%2520image%2520regions%2520are%2520to%2520be%2520located%2520for%2520the%2520language%250Aphrases%2520provided.%2520Existing%2520approaches%2520complete%2520such%2520visual-text%2520reasoning%2520in%2520a%250Asingle-step%2520manner.%2520Their%2520performance%2520causes%2520high%2520demands%2520on%2520large-scale%250Aanchors%2520and%2520over-designed%2520multi-modal%2520fusion%2520modules%2520based%2520on%2520human%2520priors%252C%250Aleading%2520to%2520complicated%2520frameworks%2520that%2520may%2520be%2520difficult%2520to%2520train%2520and%2520overfit%2520to%250Aspecific%2520scenarios.%2520Even%2520worse%252C%2520such%2520once-for-all%2520reasoning%2520mechanisms%2520are%250Aincapable%2520of%2520refining%2520boxes%2520continuously%2520to%2520enhance%2520query-region%2520matching.%2520In%250Acontrast%252C%2520in%2520this%2520paper%252C%2520we%2520formulate%2520an%2520iterative%2520reasoning%2520process%2520by%250Adenoising%2520diffusion%2520modeling.%2520Specifically%252C%2520we%2520propose%2520a%2520language-guided%250Adiffusion%2520framework%2520for%2520visual%2520grounding%252C%2520LG-DVG%252C%2520which%2520trains%2520the%2520model%2520to%250Aprogressively%2520reason%2520queried%2520object%2520boxes%2520by%2520denoising%2520a%2520set%2520of%2520noisy%2520boxes%250Awith%2520the%2520language%2520guide.%2520To%2520achieve%2520this%252C%2520LG-DVG%2520gradually%2520perturbs%250Aquery-aligned%2520ground%2520truth%2520boxes%2520to%2520noisy%2520ones%2520and%2520reverses%2520this%2520process%2520step%250Aby%2520step%252C%2520conditional%2520on%2520query%2520semantics.%2520Extensive%2520experiments%2520for%2520our%2520proposed%250Aframework%2520on%2520five%2520widely%2520used%2520datasets%2520validate%2520the%2520superior%2520performance%2520of%250Asolving%2520visual%2520grounding%252C%2520a%2520cross-modal%2520alignment%2520task%252C%2520in%2520a%2520generative%2520way.%250AThe%2520source%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/iQua/vgbase/tree/main/examples/DiffusionVG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09599v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Guided%20Diffusion%20Model%20for%20Visual%20Grounding&entry.906535625=Sijia%20Chen%20and%20Baochun%20Li&entry.1292438233=%20%20Visual%20grounding%20%28VG%29%20tasks%20involve%20explicit%20cross-modal%20alignment%2C%20as%0Asemantically%20corresponding%20image%20regions%20are%20to%20be%20located%20for%20the%20language%0Aphrases%20provided.%20Existing%20approaches%20complete%20such%20visual-text%20reasoning%20in%20a%0Asingle-step%20manner.%20Their%20performance%20causes%20high%20demands%20on%20large-scale%0Aanchors%20and%20over-designed%20multi-modal%20fusion%20modules%20based%20on%20human%20priors%2C%0Aleading%20to%20complicated%20frameworks%20that%20may%20be%20difficult%20to%20train%20and%20overfit%20to%0Aspecific%20scenarios.%20Even%20worse%2C%20such%20once-for-all%20reasoning%20mechanisms%20are%0Aincapable%20of%20refining%20boxes%20continuously%20to%20enhance%20query-region%20matching.%20In%0Acontrast%2C%20in%20this%20paper%2C%20we%20formulate%20an%20iterative%20reasoning%20process%20by%0Adenoising%20diffusion%20modeling.%20Specifically%2C%20we%20propose%20a%20language-guided%0Adiffusion%20framework%20for%20visual%20grounding%2C%20LG-DVG%2C%20which%20trains%20the%20model%20to%0Aprogressively%20reason%20queried%20object%20boxes%20by%20denoising%20a%20set%20of%20noisy%20boxes%0Awith%20the%20language%20guide.%20To%20achieve%20this%2C%20LG-DVG%20gradually%20perturbs%0Aquery-aligned%20ground%20truth%20boxes%20to%20noisy%20ones%20and%20reverses%20this%20process%20step%0Aby%20step%2C%20conditional%20on%20query%20semantics.%20Extensive%20experiments%20for%20our%20proposed%0Aframework%20on%20five%20widely%20used%20datasets%20validate%20the%20superior%20performance%20of%0Asolving%20visual%20grounding%2C%20a%20cross-modal%20alignment%20task%2C%20in%20a%20generative%20way.%0AThe%20source%20codes%20are%20available%20at%0Ahttps%3A//github.com/iQua/vgbase/tree/main/examples/DiffusionVG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09599v2&entry.124074799=Read"},
{"title": "Can AI Help with Your Personal Finances?", "author": "Oudom Hean and Utsha Saha and Binita Saha", "abstract": "  In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.\n", "link": "http://arxiv.org/abs/2412.19784v1", "date": "2024-12-27", "relevancy": 1.2778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4451}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20AI%20Help%20with%20Your%20Personal%20Finances%3F&body=Title%3A%20Can%20AI%20Help%20with%20Your%20Personal%20Finances%3F%0AAuthor%3A%20Oudom%20Hean%20and%20Utsha%20Saha%20and%20Binita%20Saha%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%0Atransformative%20development%20in%20artificial%20intelligence%20%28AI%29%2C%20drawing%20significant%0Aattention%20from%20industry%20and%20academia.%20Trained%20on%20vast%20datasets%2C%20these%0Asophisticated%20AI%20systems%20exhibit%20impressive%20natural%20language%20processing%20and%0Acontent%20generation%20capabilities.%20This%20paper%20explores%20the%20potential%20of%20LLMs%20to%0Aaddress%20key%20challenges%20in%20personal%20finance%2C%20focusing%20on%20the%20United%20States.%20We%0Aevaluate%20several%20leading%20LLMs%2C%20including%20OpenAI%27s%20ChatGPT%2C%20Google%27s%20Gemini%2C%0AAnthropic%27s%20Claude%2C%20and%20Meta%27s%20Llama%2C%20to%20assess%20their%20effectiveness%20in%0Aproviding%20accurate%20financial%20advice%20on%20topics%20such%20as%20mortgages%2C%20taxes%2C%20loans%2C%0Aand%20investments.%20Our%20findings%20show%20that%20while%20these%20models%20achieve%20an%20average%0Aaccuracy%20rate%20of%20approximately%2070%25%2C%20they%20also%20display%20notable%20limitations%20in%0Acertain%20areas.%20Specifically%2C%20LLMs%20struggle%20to%20provide%20accurate%20responses%20for%0Acomplex%20financial%20queries%2C%20with%20performance%20varying%20significantly%20across%0Adifferent%20topics.%20Despite%20these%20limitations%2C%20the%20analysis%20reveals%20notable%0Aimprovements%20in%20newer%20versions%20of%20these%20models%2C%20highlighting%20their%20growing%0Autility%20for%20individuals%20and%20financial%20advisors.%20As%20these%20AI%20systems%20continue%20to%0Aevolve%2C%20their%20potential%20for%20advancing%20AI-driven%20applications%20in%20personal%0Afinance%20becomes%20increasingly%20promising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520AI%2520Help%2520with%2520Your%2520Personal%2520Finances%253F%26entry.906535625%3DOudom%2520Hean%2520and%2520Utsha%2520Saha%2520and%2520Binita%2520Saha%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520a%250Atransformative%2520development%2520in%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520drawing%2520significant%250Aattention%2520from%2520industry%2520and%2520academia.%2520Trained%2520on%2520vast%2520datasets%252C%2520these%250Asophisticated%2520AI%2520systems%2520exhibit%2520impressive%2520natural%2520language%2520processing%2520and%250Acontent%2520generation%2520capabilities.%2520This%2520paper%2520explores%2520the%2520potential%2520of%2520LLMs%2520to%250Aaddress%2520key%2520challenges%2520in%2520personal%2520finance%252C%2520focusing%2520on%2520the%2520United%2520States.%2520We%250Aevaluate%2520several%2520leading%2520LLMs%252C%2520including%2520OpenAI%2527s%2520ChatGPT%252C%2520Google%2527s%2520Gemini%252C%250AAnthropic%2527s%2520Claude%252C%2520and%2520Meta%2527s%2520Llama%252C%2520to%2520assess%2520their%2520effectiveness%2520in%250Aproviding%2520accurate%2520financial%2520advice%2520on%2520topics%2520such%2520as%2520mortgages%252C%2520taxes%252C%2520loans%252C%250Aand%2520investments.%2520Our%2520findings%2520show%2520that%2520while%2520these%2520models%2520achieve%2520an%2520average%250Aaccuracy%2520rate%2520of%2520approximately%252070%2525%252C%2520they%2520also%2520display%2520notable%2520limitations%2520in%250Acertain%2520areas.%2520Specifically%252C%2520LLMs%2520struggle%2520to%2520provide%2520accurate%2520responses%2520for%250Acomplex%2520financial%2520queries%252C%2520with%2520performance%2520varying%2520significantly%2520across%250Adifferent%2520topics.%2520Despite%2520these%2520limitations%252C%2520the%2520analysis%2520reveals%2520notable%250Aimprovements%2520in%2520newer%2520versions%2520of%2520these%2520models%252C%2520highlighting%2520their%2520growing%250Autility%2520for%2520individuals%2520and%2520financial%2520advisors.%2520As%2520these%2520AI%2520systems%2520continue%2520to%250Aevolve%252C%2520their%2520potential%2520for%2520advancing%2520AI-driven%2520applications%2520in%2520personal%250Afinance%2520becomes%2520increasingly%2520promising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20AI%20Help%20with%20Your%20Personal%20Finances%3F&entry.906535625=Oudom%20Hean%20and%20Utsha%20Saha%20and%20Binita%20Saha&entry.1292438233=%20%20In%20recent%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20a%0Atransformative%20development%20in%20artificial%20intelligence%20%28AI%29%2C%20drawing%20significant%0Aattention%20from%20industry%20and%20academia.%20Trained%20on%20vast%20datasets%2C%20these%0Asophisticated%20AI%20systems%20exhibit%20impressive%20natural%20language%20processing%20and%0Acontent%20generation%20capabilities.%20This%20paper%20explores%20the%20potential%20of%20LLMs%20to%0Aaddress%20key%20challenges%20in%20personal%20finance%2C%20focusing%20on%20the%20United%20States.%20We%0Aevaluate%20several%20leading%20LLMs%2C%20including%20OpenAI%27s%20ChatGPT%2C%20Google%27s%20Gemini%2C%0AAnthropic%27s%20Claude%2C%20and%20Meta%27s%20Llama%2C%20to%20assess%20their%20effectiveness%20in%0Aproviding%20accurate%20financial%20advice%20on%20topics%20such%20as%20mortgages%2C%20taxes%2C%20loans%2C%0Aand%20investments.%20Our%20findings%20show%20that%20while%20these%20models%20achieve%20an%20average%0Aaccuracy%20rate%20of%20approximately%2070%25%2C%20they%20also%20display%20notable%20limitations%20in%0Acertain%20areas.%20Specifically%2C%20LLMs%20struggle%20to%20provide%20accurate%20responses%20for%0Acomplex%20financial%20queries%2C%20with%20performance%20varying%20significantly%20across%0Adifferent%20topics.%20Despite%20these%20limitations%2C%20the%20analysis%20reveals%20notable%0Aimprovements%20in%20newer%20versions%20of%20these%20models%2C%20highlighting%20their%20growing%0Autility%20for%20individuals%20and%20financial%20advisors.%20As%20these%20AI%20systems%20continue%20to%0Aevolve%2C%20their%20potential%20for%20advancing%20AI-driven%20applications%20in%20personal%0Afinance%20becomes%20increasingly%20promising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19784v1&entry.124074799=Read"},
{"title": "Complement or substitute? How AI increases the demand for human skills", "author": "Elina M\u00e4kel\u00e4 and Fabian Stephany", "abstract": "  The question of whether AI substitutes or complements human work is central\nto debates on the future of work. This paper examines the impact of AI on skill\ndemand and compensation in the U.S. economy, analysing 12 million online job\nvacancies from 2018 to 2023. It investigates internal effects (within-job\nsubstitution and complementation) and external effects (across occupations,\nindustries, and regions). Our findings reveal a significant increase in demand\nfor AI-complementary skills, such as digital literacy, teamwork, and\nresilience, alongside rising wage premiums for these skills in AI roles like\nData Scientist. Conversely, substitute skills, including customer service and\ntext review, have declined in both demand and value within AI-related\npositions. Examining external effects, we find a notable rise in demand for\ncomplementary skills in non-AI roles linked to the growth of AI-related jobs in\nspecific industries or regions. At the same time, there is a moderate decline\nin non-AI roles requiring substitute skills. Overall, AI's complementary effect\nis up to 50% larger than its substitution effect, resulting in net positive\ndemand for skills. These results, replicated for the UK and Australia,\nhighlight AI's transformative impact on workforce skill requirements. They\nsuggest reskilling efforts should prioritise not only technical AI skills but\nalso complementary skills like ethics and digital literacy.\n", "link": "http://arxiv.org/abs/2412.19754v1", "date": "2024-12-27", "relevancy": 1.0768, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3732}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3604}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complement%20or%20substitute%3F%20How%20AI%20increases%20the%20demand%20for%20human%20skills&body=Title%3A%20Complement%20or%20substitute%3F%20How%20AI%20increases%20the%20demand%20for%20human%20skills%0AAuthor%3A%20Elina%20M%C3%A4kel%C3%A4%20and%20Fabian%20Stephany%0AAbstract%3A%20%20%20The%20question%20of%20whether%20AI%20substitutes%20or%20complements%20human%20work%20is%20central%0Ato%20debates%20on%20the%20future%20of%20work.%20This%20paper%20examines%20the%20impact%20of%20AI%20on%20skill%0Ademand%20and%20compensation%20in%20the%20U.S.%20economy%2C%20analysing%2012%20million%20online%20job%0Avacancies%20from%202018%20to%202023.%20It%20investigates%20internal%20effects%20%28within-job%0Asubstitution%20and%20complementation%29%20and%20external%20effects%20%28across%20occupations%2C%0Aindustries%2C%20and%20regions%29.%20Our%20findings%20reveal%20a%20significant%20increase%20in%20demand%0Afor%20AI-complementary%20skills%2C%20such%20as%20digital%20literacy%2C%20teamwork%2C%20and%0Aresilience%2C%20alongside%20rising%20wage%20premiums%20for%20these%20skills%20in%20AI%20roles%20like%0AData%20Scientist.%20Conversely%2C%20substitute%20skills%2C%20including%20customer%20service%20and%0Atext%20review%2C%20have%20declined%20in%20both%20demand%20and%20value%20within%20AI-related%0Apositions.%20Examining%20external%20effects%2C%20we%20find%20a%20notable%20rise%20in%20demand%20for%0Acomplementary%20skills%20in%20non-AI%20roles%20linked%20to%20the%20growth%20of%20AI-related%20jobs%20in%0Aspecific%20industries%20or%20regions.%20At%20the%20same%20time%2C%20there%20is%20a%20moderate%20decline%0Ain%20non-AI%20roles%20requiring%20substitute%20skills.%20Overall%2C%20AI%27s%20complementary%20effect%0Ais%20up%20to%2050%25%20larger%20than%20its%20substitution%20effect%2C%20resulting%20in%20net%20positive%0Ademand%20for%20skills.%20These%20results%2C%20replicated%20for%20the%20UK%20and%20Australia%2C%0Ahighlight%20AI%27s%20transformative%20impact%20on%20workforce%20skill%20requirements.%20They%0Asuggest%20reskilling%20efforts%20should%20prioritise%20not%20only%20technical%20AI%20skills%20but%0Aalso%20complementary%20skills%20like%20ethics%20and%20digital%20literacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplement%2520or%2520substitute%253F%2520How%2520AI%2520increases%2520the%2520demand%2520for%2520human%2520skills%26entry.906535625%3DElina%2520M%25C3%25A4kel%25C3%25A4%2520and%2520Fabian%2520Stephany%26entry.1292438233%3D%2520%2520The%2520question%2520of%2520whether%2520AI%2520substitutes%2520or%2520complements%2520human%2520work%2520is%2520central%250Ato%2520debates%2520on%2520the%2520future%2520of%2520work.%2520This%2520paper%2520examines%2520the%2520impact%2520of%2520AI%2520on%2520skill%250Ademand%2520and%2520compensation%2520in%2520the%2520U.S.%2520economy%252C%2520analysing%252012%2520million%2520online%2520job%250Avacancies%2520from%25202018%2520to%25202023.%2520It%2520investigates%2520internal%2520effects%2520%2528within-job%250Asubstitution%2520and%2520complementation%2529%2520and%2520external%2520effects%2520%2528across%2520occupations%252C%250Aindustries%252C%2520and%2520regions%2529.%2520Our%2520findings%2520reveal%2520a%2520significant%2520increase%2520in%2520demand%250Afor%2520AI-complementary%2520skills%252C%2520such%2520as%2520digital%2520literacy%252C%2520teamwork%252C%2520and%250Aresilience%252C%2520alongside%2520rising%2520wage%2520premiums%2520for%2520these%2520skills%2520in%2520AI%2520roles%2520like%250AData%2520Scientist.%2520Conversely%252C%2520substitute%2520skills%252C%2520including%2520customer%2520service%2520and%250Atext%2520review%252C%2520have%2520declined%2520in%2520both%2520demand%2520and%2520value%2520within%2520AI-related%250Apositions.%2520Examining%2520external%2520effects%252C%2520we%2520find%2520a%2520notable%2520rise%2520in%2520demand%2520for%250Acomplementary%2520skills%2520in%2520non-AI%2520roles%2520linked%2520to%2520the%2520growth%2520of%2520AI-related%2520jobs%2520in%250Aspecific%2520industries%2520or%2520regions.%2520At%2520the%2520same%2520time%252C%2520there%2520is%2520a%2520moderate%2520decline%250Ain%2520non-AI%2520roles%2520requiring%2520substitute%2520skills.%2520Overall%252C%2520AI%2527s%2520complementary%2520effect%250Ais%2520up%2520to%252050%2525%2520larger%2520than%2520its%2520substitution%2520effect%252C%2520resulting%2520in%2520net%2520positive%250Ademand%2520for%2520skills.%2520These%2520results%252C%2520replicated%2520for%2520the%2520UK%2520and%2520Australia%252C%250Ahighlight%2520AI%2527s%2520transformative%2520impact%2520on%2520workforce%2520skill%2520requirements.%2520They%250Asuggest%2520reskilling%2520efforts%2520should%2520prioritise%2520not%2520only%2520technical%2520AI%2520skills%2520but%250Aalso%2520complementary%2520skills%2520like%2520ethics%2520and%2520digital%2520literacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complement%20or%20substitute%3F%20How%20AI%20increases%20the%20demand%20for%20human%20skills&entry.906535625=Elina%20M%C3%A4kel%C3%A4%20and%20Fabian%20Stephany&entry.1292438233=%20%20The%20question%20of%20whether%20AI%20substitutes%20or%20complements%20human%20work%20is%20central%0Ato%20debates%20on%20the%20future%20of%20work.%20This%20paper%20examines%20the%20impact%20of%20AI%20on%20skill%0Ademand%20and%20compensation%20in%20the%20U.S.%20economy%2C%20analysing%2012%20million%20online%20job%0Avacancies%20from%202018%20to%202023.%20It%20investigates%20internal%20effects%20%28within-job%0Asubstitution%20and%20complementation%29%20and%20external%20effects%20%28across%20occupations%2C%0Aindustries%2C%20and%20regions%29.%20Our%20findings%20reveal%20a%20significant%20increase%20in%20demand%0Afor%20AI-complementary%20skills%2C%20such%20as%20digital%20literacy%2C%20teamwork%2C%20and%0Aresilience%2C%20alongside%20rising%20wage%20premiums%20for%20these%20skills%20in%20AI%20roles%20like%0AData%20Scientist.%20Conversely%2C%20substitute%20skills%2C%20including%20customer%20service%20and%0Atext%20review%2C%20have%20declined%20in%20both%20demand%20and%20value%20within%20AI-related%0Apositions.%20Examining%20external%20effects%2C%20we%20find%20a%20notable%20rise%20in%20demand%20for%0Acomplementary%20skills%20in%20non-AI%20roles%20linked%20to%20the%20growth%20of%20AI-related%20jobs%20in%0Aspecific%20industries%20or%20regions.%20At%20the%20same%20time%2C%20there%20is%20a%20moderate%20decline%0Ain%20non-AI%20roles%20requiring%20substitute%20skills.%20Overall%2C%20AI%27s%20complementary%20effect%0Ais%20up%20to%2050%25%20larger%20than%20its%20substitution%20effect%2C%20resulting%20in%20net%20positive%0Ademand%20for%20skills.%20These%20results%2C%20replicated%20for%20the%20UK%20and%20Australia%2C%0Ahighlight%20AI%27s%20transformative%20impact%20on%20workforce%20skill%20requirements.%20They%0Asuggest%20reskilling%20efforts%20should%20prioritise%20not%20only%20technical%20AI%20skills%20but%0Aalso%20complementary%20skills%20like%20ethics%20and%20digital%20literacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19754v1&entry.124074799=Read"},
{"title": "Convergence of SGD with momentum in the nonconvex case: A time\n  window-based analysis", "author": "Junwen Qiu and Bohao Ma and Andre Milzarek", "abstract": "  The stochastic gradient descent method with momentum (SGDM) is a common\napproach for solving large-scale and stochastic optimization problems. Despite\nits popularity, the convergence behavior of SGDM remains less understood in\nnonconvex scenarios. This is primarily due to the absence of a sufficient\ndescent property and challenges in simultaneously controlling the momentum and\nstochastic errors in an almost sure sense. To address these challenges, we\ninvestigate the behavior of SGDM over specific time windows, rather than\nexamining the descent of consecutive iterates as in traditional studies. This\ntime window-based approach simplifies the convergence analysis and enables us\nto establish the iterate convergence result for SGDM under the {\\L}ojasiewicz\nproperty. We further provide local convergence rates which depend on the\nunderlying {\\L}ojasiewicz exponent and the utilized step size schemes.\n", "link": "http://arxiv.org/abs/2405.16954v3", "date": "2024-12-27", "relevancy": 1.3353, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.451}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4388}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20SGD%20with%20momentum%20in%20the%20nonconvex%20case%3A%20A%20time%0A%20%20window-based%20analysis&body=Title%3A%20Convergence%20of%20SGD%20with%20momentum%20in%20the%20nonconvex%20case%3A%20A%20time%0A%20%20window-based%20analysis%0AAuthor%3A%20Junwen%20Qiu%20and%20Bohao%20Ma%20and%20Andre%20Milzarek%0AAbstract%3A%20%20%20The%20stochastic%20gradient%20descent%20method%20with%20momentum%20%28SGDM%29%20is%20a%20common%0Aapproach%20for%20solving%20large-scale%20and%20stochastic%20optimization%20problems.%20Despite%0Aits%20popularity%2C%20the%20convergence%20behavior%20of%20SGDM%20remains%20less%20understood%20in%0Anonconvex%20scenarios.%20This%20is%20primarily%20due%20to%20the%20absence%20of%20a%20sufficient%0Adescent%20property%20and%20challenges%20in%20simultaneously%20controlling%20the%20momentum%20and%0Astochastic%20errors%20in%20an%20almost%20sure%20sense.%20To%20address%20these%20challenges%2C%20we%0Ainvestigate%20the%20behavior%20of%20SGDM%20over%20specific%20time%20windows%2C%20rather%20than%0Aexamining%20the%20descent%20of%20consecutive%20iterates%20as%20in%20traditional%20studies.%20This%0Atime%20window-based%20approach%20simplifies%20the%20convergence%20analysis%20and%20enables%20us%0Ato%20establish%20the%20iterate%20convergence%20result%20for%20SGDM%20under%20the%20%7B%5CL%7Dojasiewicz%0Aproperty.%20We%20further%20provide%20local%20convergence%20rates%20which%20depend%20on%20the%0Aunderlying%20%7B%5CL%7Dojasiewicz%20exponent%20and%20the%20utilized%20step%20size%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16954v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520of%2520SGD%2520with%2520momentum%2520in%2520the%2520nonconvex%2520case%253A%2520A%2520time%250A%2520%2520window-based%2520analysis%26entry.906535625%3DJunwen%2520Qiu%2520and%2520Bohao%2520Ma%2520and%2520Andre%2520Milzarek%26entry.1292438233%3D%2520%2520The%2520stochastic%2520gradient%2520descent%2520method%2520with%2520momentum%2520%2528SGDM%2529%2520is%2520a%2520common%250Aapproach%2520for%2520solving%2520large-scale%2520and%2520stochastic%2520optimization%2520problems.%2520Despite%250Aits%2520popularity%252C%2520the%2520convergence%2520behavior%2520of%2520SGDM%2520remains%2520less%2520understood%2520in%250Anonconvex%2520scenarios.%2520This%2520is%2520primarily%2520due%2520to%2520the%2520absence%2520of%2520a%2520sufficient%250Adescent%2520property%2520and%2520challenges%2520in%2520simultaneously%2520controlling%2520the%2520momentum%2520and%250Astochastic%2520errors%2520in%2520an%2520almost%2520sure%2520sense.%2520To%2520address%2520these%2520challenges%252C%2520we%250Ainvestigate%2520the%2520behavior%2520of%2520SGDM%2520over%2520specific%2520time%2520windows%252C%2520rather%2520than%250Aexamining%2520the%2520descent%2520of%2520consecutive%2520iterates%2520as%2520in%2520traditional%2520studies.%2520This%250Atime%2520window-based%2520approach%2520simplifies%2520the%2520convergence%2520analysis%2520and%2520enables%2520us%250Ato%2520establish%2520the%2520iterate%2520convergence%2520result%2520for%2520SGDM%2520under%2520the%2520%257B%255CL%257Dojasiewicz%250Aproperty.%2520We%2520further%2520provide%2520local%2520convergence%2520rates%2520which%2520depend%2520on%2520the%250Aunderlying%2520%257B%255CL%257Dojasiewicz%2520exponent%2520and%2520the%2520utilized%2520step%2520size%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16954v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20SGD%20with%20momentum%20in%20the%20nonconvex%20case%3A%20A%20time%0A%20%20window-based%20analysis&entry.906535625=Junwen%20Qiu%20and%20Bohao%20Ma%20and%20Andre%20Milzarek&entry.1292438233=%20%20The%20stochastic%20gradient%20descent%20method%20with%20momentum%20%28SGDM%29%20is%20a%20common%0Aapproach%20for%20solving%20large-scale%20and%20stochastic%20optimization%20problems.%20Despite%0Aits%20popularity%2C%20the%20convergence%20behavior%20of%20SGDM%20remains%20less%20understood%20in%0Anonconvex%20scenarios.%20This%20is%20primarily%20due%20to%20the%20absence%20of%20a%20sufficient%0Adescent%20property%20and%20challenges%20in%20simultaneously%20controlling%20the%20momentum%20and%0Astochastic%20errors%20in%20an%20almost%20sure%20sense.%20To%20address%20these%20challenges%2C%20we%0Ainvestigate%20the%20behavior%20of%20SGDM%20over%20specific%20time%20windows%2C%20rather%20than%0Aexamining%20the%20descent%20of%20consecutive%20iterates%20as%20in%20traditional%20studies.%20This%0Atime%20window-based%20approach%20simplifies%20the%20convergence%20analysis%20and%20enables%20us%0Ato%20establish%20the%20iterate%20convergence%20result%20for%20SGDM%20under%20the%20%7B%5CL%7Dojasiewicz%0Aproperty.%20We%20further%20provide%20local%20convergence%20rates%20which%20depend%20on%20the%0Aunderlying%20%7B%5CL%7Dojasiewicz%20exponent%20and%20the%20utilized%20step%20size%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16954v3&entry.124074799=Read"},
{"title": "Bidding Games on Markov Decision Processes with Quantitative\n  Reachability Objectives", "author": "Guy Avni and Martin Kure\u010dka and Kaushik Mallik and Petr Novotn\u00fd and Suman Sadhukhan", "abstract": "  Graph games are fundamental in strategic reasoning of multi-agent systems and\ntheir environments. We study a new family of graph games which combine\nstochastic environmental uncertainties and auction-based interactions among the\nagents, formalized as bidding games on (finite) Markov decision processes\n(MDP). Normally, on MDPs, a single decision-maker chooses a sequence of\nactions, producing a probability distribution over infinite paths. In bidding\ngames on MDPs, two players -- called the reachability and safety players -- bid\nfor the privilege of choosing the next action at each step. The reachability\nplayer's goal is to maximize the probability of reaching a target vertex,\nwhereas the safety player's goal is to minimize it. These games generalize\ntraditional bidding games on graphs, and the existing analysis techniques do\nnot extend. For instance, the central property of traditional bidding games is\nthe existence of a threshold budget, which is a necessary and sufficient budget\nto guarantee winning for the reachability player. For MDPs, the threshold\nbecomes a relation between the budgets and probabilities of reaching the\ntarget. We devise value-iteration algorithms that approximate thresholds and\noptimal policies for general MDPs, and compute the exact solutions for acyclic\nMDPs, and show that finding thresholds is at least as hard as solving\nsimple-stochastic games.\n", "link": "http://arxiv.org/abs/2412.19609v1", "date": "2024-12-27", "relevancy": 0.8871, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4636}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4626}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bidding%20Games%20on%20Markov%20Decision%20Processes%20with%20Quantitative%0A%20%20Reachability%20Objectives&body=Title%3A%20Bidding%20Games%20on%20Markov%20Decision%20Processes%20with%20Quantitative%0A%20%20Reachability%20Objectives%0AAuthor%3A%20Guy%20Avni%20and%20Martin%20Kure%C4%8Dka%20and%20Kaushik%20Mallik%20and%20Petr%20Novotn%C3%BD%20and%20Suman%20Sadhukhan%0AAbstract%3A%20%20%20Graph%20games%20are%20fundamental%20in%20strategic%20reasoning%20of%20multi-agent%20systems%20and%0Atheir%20environments.%20We%20study%20a%20new%20family%20of%20graph%20games%20which%20combine%0Astochastic%20environmental%20uncertainties%20and%20auction-based%20interactions%20among%20the%0Aagents%2C%20formalized%20as%20bidding%20games%20on%20%28finite%29%20Markov%20decision%20processes%0A%28MDP%29.%20Normally%2C%20on%20MDPs%2C%20a%20single%20decision-maker%20chooses%20a%20sequence%20of%0Aactions%2C%20producing%20a%20probability%20distribution%20over%20infinite%20paths.%20In%20bidding%0Agames%20on%20MDPs%2C%20two%20players%20--%20called%20the%20reachability%20and%20safety%20players%20--%20bid%0Afor%20the%20privilege%20of%20choosing%20the%20next%20action%20at%20each%20step.%20The%20reachability%0Aplayer%27s%20goal%20is%20to%20maximize%20the%20probability%20of%20reaching%20a%20target%20vertex%2C%0Awhereas%20the%20safety%20player%27s%20goal%20is%20to%20minimize%20it.%20These%20games%20generalize%0Atraditional%20bidding%20games%20on%20graphs%2C%20and%20the%20existing%20analysis%20techniques%20do%0Anot%20extend.%20For%20instance%2C%20the%20central%20property%20of%20traditional%20bidding%20games%20is%0Athe%20existence%20of%20a%20threshold%20budget%2C%20which%20is%20a%20necessary%20and%20sufficient%20budget%0Ato%20guarantee%20winning%20for%20the%20reachability%20player.%20For%20MDPs%2C%20the%20threshold%0Abecomes%20a%20relation%20between%20the%20budgets%20and%20probabilities%20of%20reaching%20the%0Atarget.%20We%20devise%20value-iteration%20algorithms%20that%20approximate%20thresholds%20and%0Aoptimal%20policies%20for%20general%20MDPs%2C%20and%20compute%20the%20exact%20solutions%20for%20acyclic%0AMDPs%2C%20and%20show%20that%20finding%20thresholds%20is%20at%20least%20as%20hard%20as%20solving%0Asimple-stochastic%20games.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBidding%2520Games%2520on%2520Markov%2520Decision%2520Processes%2520with%2520Quantitative%250A%2520%2520Reachability%2520Objectives%26entry.906535625%3DGuy%2520Avni%2520and%2520Martin%2520Kure%25C4%258Dka%2520and%2520Kaushik%2520Mallik%2520and%2520Petr%2520Novotn%25C3%25BD%2520and%2520Suman%2520Sadhukhan%26entry.1292438233%3D%2520%2520Graph%2520games%2520are%2520fundamental%2520in%2520strategic%2520reasoning%2520of%2520multi-agent%2520systems%2520and%250Atheir%2520environments.%2520We%2520study%2520a%2520new%2520family%2520of%2520graph%2520games%2520which%2520combine%250Astochastic%2520environmental%2520uncertainties%2520and%2520auction-based%2520interactions%2520among%2520the%250Aagents%252C%2520formalized%2520as%2520bidding%2520games%2520on%2520%2528finite%2529%2520Markov%2520decision%2520processes%250A%2528MDP%2529.%2520Normally%252C%2520on%2520MDPs%252C%2520a%2520single%2520decision-maker%2520chooses%2520a%2520sequence%2520of%250Aactions%252C%2520producing%2520a%2520probability%2520distribution%2520over%2520infinite%2520paths.%2520In%2520bidding%250Agames%2520on%2520MDPs%252C%2520two%2520players%2520--%2520called%2520the%2520reachability%2520and%2520safety%2520players%2520--%2520bid%250Afor%2520the%2520privilege%2520of%2520choosing%2520the%2520next%2520action%2520at%2520each%2520step.%2520The%2520reachability%250Aplayer%2527s%2520goal%2520is%2520to%2520maximize%2520the%2520probability%2520of%2520reaching%2520a%2520target%2520vertex%252C%250Awhereas%2520the%2520safety%2520player%2527s%2520goal%2520is%2520to%2520minimize%2520it.%2520These%2520games%2520generalize%250Atraditional%2520bidding%2520games%2520on%2520graphs%252C%2520and%2520the%2520existing%2520analysis%2520techniques%2520do%250Anot%2520extend.%2520For%2520instance%252C%2520the%2520central%2520property%2520of%2520traditional%2520bidding%2520games%2520is%250Athe%2520existence%2520of%2520a%2520threshold%2520budget%252C%2520which%2520is%2520a%2520necessary%2520and%2520sufficient%2520budget%250Ato%2520guarantee%2520winning%2520for%2520the%2520reachability%2520player.%2520For%2520MDPs%252C%2520the%2520threshold%250Abecomes%2520a%2520relation%2520between%2520the%2520budgets%2520and%2520probabilities%2520of%2520reaching%2520the%250Atarget.%2520We%2520devise%2520value-iteration%2520algorithms%2520that%2520approximate%2520thresholds%2520and%250Aoptimal%2520policies%2520for%2520general%2520MDPs%252C%2520and%2520compute%2520the%2520exact%2520solutions%2520for%2520acyclic%250AMDPs%252C%2520and%2520show%2520that%2520finding%2520thresholds%2520is%2520at%2520least%2520as%2520hard%2520as%2520solving%250Asimple-stochastic%2520games.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidding%20Games%20on%20Markov%20Decision%20Processes%20with%20Quantitative%0A%20%20Reachability%20Objectives&entry.906535625=Guy%20Avni%20and%20Martin%20Kure%C4%8Dka%20and%20Kaushik%20Mallik%20and%20Petr%20Novotn%C3%BD%20and%20Suman%20Sadhukhan&entry.1292438233=%20%20Graph%20games%20are%20fundamental%20in%20strategic%20reasoning%20of%20multi-agent%20systems%20and%0Atheir%20environments.%20We%20study%20a%20new%20family%20of%20graph%20games%20which%20combine%0Astochastic%20environmental%20uncertainties%20and%20auction-based%20interactions%20among%20the%0Aagents%2C%20formalized%20as%20bidding%20games%20on%20%28finite%29%20Markov%20decision%20processes%0A%28MDP%29.%20Normally%2C%20on%20MDPs%2C%20a%20single%20decision-maker%20chooses%20a%20sequence%20of%0Aactions%2C%20producing%20a%20probability%20distribution%20over%20infinite%20paths.%20In%20bidding%0Agames%20on%20MDPs%2C%20two%20players%20--%20called%20the%20reachability%20and%20safety%20players%20--%20bid%0Afor%20the%20privilege%20of%20choosing%20the%20next%20action%20at%20each%20step.%20The%20reachability%0Aplayer%27s%20goal%20is%20to%20maximize%20the%20probability%20of%20reaching%20a%20target%20vertex%2C%0Awhereas%20the%20safety%20player%27s%20goal%20is%20to%20minimize%20it.%20These%20games%20generalize%0Atraditional%20bidding%20games%20on%20graphs%2C%20and%20the%20existing%20analysis%20techniques%20do%0Anot%20extend.%20For%20instance%2C%20the%20central%20property%20of%20traditional%20bidding%20games%20is%0Athe%20existence%20of%20a%20threshold%20budget%2C%20which%20is%20a%20necessary%20and%20sufficient%20budget%0Ato%20guarantee%20winning%20for%20the%20reachability%20player.%20For%20MDPs%2C%20the%20threshold%0Abecomes%20a%20relation%20between%20the%20budgets%20and%20probabilities%20of%20reaching%20the%0Atarget.%20We%20devise%20value-iteration%20algorithms%20that%20approximate%20thresholds%20and%0Aoptimal%20policies%20for%20general%20MDPs%2C%20and%20compute%20the%20exact%20solutions%20for%20acyclic%0AMDPs%2C%20and%20show%20that%20finding%20thresholds%20is%20at%20least%20as%20hard%20as%20solving%0Asimple-stochastic%20games.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19609v1&entry.124074799=Read"},
{"title": "Blessing or curse? A survey on the Impact of Generative AI on Fake News", "author": "Alexander Loth and Martin Kappes and Marc-Oliver Pahl", "abstract": "  Fake news significantly influence our society. They impact consumers, voters,\nand many other societal groups. While Fake News exist for a centuries,\nGenerative AI brings fake news on a new level. It is now possible to automate\nthe creation of masses of high-quality individually targeted Fake News. On the\nother end, Generative AI can also help detecting Fake News. Both fields are\nyoung but developing fast.\n  This survey provides a comprehensive examination of the research and\npractical use of Generative AI for Fake News detection and creation in 2024.\nFollowing the Structured Literature Survey approach, the paper synthesizes\ncurrent results in the following topic clusters 1) enabling technologies, 2)\ncreation of Fake News, 3) case study social media as most relevant distribution\nchannel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.\n  The article also identifies current challenges and open issues.\n", "link": "http://arxiv.org/abs/2404.03021v2", "date": "2024-12-27", "relevancy": 1.4037, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4838}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4653}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blessing%20or%20curse%3F%20A%20survey%20on%20the%20Impact%20of%20Generative%20AI%20on%20Fake%20News&body=Title%3A%20Blessing%20or%20curse%3F%20A%20survey%20on%20the%20Impact%20of%20Generative%20AI%20on%20Fake%20News%0AAuthor%3A%20Alexander%20Loth%20and%20Martin%20Kappes%20and%20Marc-Oliver%20Pahl%0AAbstract%3A%20%20%20Fake%20news%20significantly%20influence%20our%20society.%20They%20impact%20consumers%2C%20voters%2C%0Aand%20many%20other%20societal%20groups.%20While%20Fake%20News%20exist%20for%20a%20centuries%2C%0AGenerative%20AI%20brings%20fake%20news%20on%20a%20new%20level.%20It%20is%20now%20possible%20to%20automate%0Athe%20creation%20of%20masses%20of%20high-quality%20individually%20targeted%20Fake%20News.%20On%20the%0Aother%20end%2C%20Generative%20AI%20can%20also%20help%20detecting%20Fake%20News.%20Both%20fields%20are%0Ayoung%20but%20developing%20fast.%0A%20%20This%20survey%20provides%20a%20comprehensive%20examination%20of%20the%20research%20and%0Apractical%20use%20of%20Generative%20AI%20for%20Fake%20News%20detection%20and%20creation%20in%202024.%0AFollowing%20the%20Structured%20Literature%20Survey%20approach%2C%20the%20paper%20synthesizes%0Acurrent%20results%20in%20the%20following%20topic%20clusters%201%29%20enabling%20technologies%2C%202%29%0Acreation%20of%20Fake%20News%2C%203%29%20case%20study%20social%20media%20as%20most%20relevant%20distribution%0Achannel%2C%204%29%20detection%20of%20Fake%20News%2C%20and%205%29%20deepfakes%20as%20upcoming%20technology.%0A%20%20The%20article%20also%20identifies%20current%20challenges%20and%20open%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03021v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlessing%2520or%2520curse%253F%2520A%2520survey%2520on%2520the%2520Impact%2520of%2520Generative%2520AI%2520on%2520Fake%2520News%26entry.906535625%3DAlexander%2520Loth%2520and%2520Martin%2520Kappes%2520and%2520Marc-Oliver%2520Pahl%26entry.1292438233%3D%2520%2520Fake%2520news%2520significantly%2520influence%2520our%2520society.%2520They%2520impact%2520consumers%252C%2520voters%252C%250Aand%2520many%2520other%2520societal%2520groups.%2520While%2520Fake%2520News%2520exist%2520for%2520a%2520centuries%252C%250AGenerative%2520AI%2520brings%2520fake%2520news%2520on%2520a%2520new%2520level.%2520It%2520is%2520now%2520possible%2520to%2520automate%250Athe%2520creation%2520of%2520masses%2520of%2520high-quality%2520individually%2520targeted%2520Fake%2520News.%2520On%2520the%250Aother%2520end%252C%2520Generative%2520AI%2520can%2520also%2520help%2520detecting%2520Fake%2520News.%2520Both%2520fields%2520are%250Ayoung%2520but%2520developing%2520fast.%250A%2520%2520This%2520survey%2520provides%2520a%2520comprehensive%2520examination%2520of%2520the%2520research%2520and%250Apractical%2520use%2520of%2520Generative%2520AI%2520for%2520Fake%2520News%2520detection%2520and%2520creation%2520in%25202024.%250AFollowing%2520the%2520Structured%2520Literature%2520Survey%2520approach%252C%2520the%2520paper%2520synthesizes%250Acurrent%2520results%2520in%2520the%2520following%2520topic%2520clusters%25201%2529%2520enabling%2520technologies%252C%25202%2529%250Acreation%2520of%2520Fake%2520News%252C%25203%2529%2520case%2520study%2520social%2520media%2520as%2520most%2520relevant%2520distribution%250Achannel%252C%25204%2529%2520detection%2520of%2520Fake%2520News%252C%2520and%25205%2529%2520deepfakes%2520as%2520upcoming%2520technology.%250A%2520%2520The%2520article%2520also%2520identifies%2520current%2520challenges%2520and%2520open%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03021v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blessing%20or%20curse%3F%20A%20survey%20on%20the%20Impact%20of%20Generative%20AI%20on%20Fake%20News&entry.906535625=Alexander%20Loth%20and%20Martin%20Kappes%20and%20Marc-Oliver%20Pahl&entry.1292438233=%20%20Fake%20news%20significantly%20influence%20our%20society.%20They%20impact%20consumers%2C%20voters%2C%0Aand%20many%20other%20societal%20groups.%20While%20Fake%20News%20exist%20for%20a%20centuries%2C%0AGenerative%20AI%20brings%20fake%20news%20on%20a%20new%20level.%20It%20is%20now%20possible%20to%20automate%0Athe%20creation%20of%20masses%20of%20high-quality%20individually%20targeted%20Fake%20News.%20On%20the%0Aother%20end%2C%20Generative%20AI%20can%20also%20help%20detecting%20Fake%20News.%20Both%20fields%20are%0Ayoung%20but%20developing%20fast.%0A%20%20This%20survey%20provides%20a%20comprehensive%20examination%20of%20the%20research%20and%0Apractical%20use%20of%20Generative%20AI%20for%20Fake%20News%20detection%20and%20creation%20in%202024.%0AFollowing%20the%20Structured%20Literature%20Survey%20approach%2C%20the%20paper%20synthesizes%0Acurrent%20results%20in%20the%20following%20topic%20clusters%201%29%20enabling%20technologies%2C%202%29%0Acreation%20of%20Fake%20News%2C%203%29%20case%20study%20social%20media%20as%20most%20relevant%20distribution%0Achannel%2C%204%29%20detection%20of%20Fake%20News%2C%20and%205%29%20deepfakes%20as%20upcoming%20technology.%0A%20%20The%20article%20also%20identifies%20current%20challenges%20and%20open%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03021v2&entry.124074799=Read"},
{"title": "Causal machine learning for heterogeneous treatment effects in the\n  presence of missing outcome data", "author": "Matthew Pryce and Karla Diaz-Ordaz and Ruth H. Keogh and Stijn Vansteelandt", "abstract": "  When estimating heterogeneous treatment effects, missing outcome data can\ncomplicate treatment effect estimation, causing certain subgroups of the\npopulation to be poorly represented. In this work, we discuss this commonly\noverlooked problem and consider the impact that missing at random (MAR) outcome\ndata has on causal machine learning estimators for the conditional average\ntreatment effect (CATE). We then propose two de-biased machine learning\nestimators for the CATE, the mDR-learner and mEP-learner, which address the\nissue of under-representation by integrating inverse probability of censoring\nweights into the DR-learner and EP-learner respectively. We show that under\nreasonable conditions, these estimators are oracle efficient, and illustrate\ntheir favorable performance through simulated data settings, comparing them to\nexisting CATE estimators, including comparison to estimators which use common\nmissing data techniques. Guidance on the implementation of these estimators is\nprovided and we present an example of their application using the ACTG175\ntrial, exploring treatment effect heterogeneity when comparing Zidovudine\nmono-therapy against alternative antiretroviral therapies among HIV-1-infected\nindividuals.\n", "link": "http://arxiv.org/abs/2412.19711v1", "date": "2024-12-27", "relevancy": 1.2718, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4556}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4317}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20machine%20learning%20for%20heterogeneous%20treatment%20effects%20in%20the%0A%20%20presence%20of%20missing%20outcome%20data&body=Title%3A%20Causal%20machine%20learning%20for%20heterogeneous%20treatment%20effects%20in%20the%0A%20%20presence%20of%20missing%20outcome%20data%0AAuthor%3A%20Matthew%20Pryce%20and%20Karla%20Diaz-Ordaz%20and%20Ruth%20H.%20Keogh%20and%20Stijn%20Vansteelandt%0AAbstract%3A%20%20%20When%20estimating%20heterogeneous%20treatment%20effects%2C%20missing%20outcome%20data%20can%0Acomplicate%20treatment%20effect%20estimation%2C%20causing%20certain%20subgroups%20of%20the%0Apopulation%20to%20be%20poorly%20represented.%20In%20this%20work%2C%20we%20discuss%20this%20commonly%0Aoverlooked%20problem%20and%20consider%20the%20impact%20that%20missing%20at%20random%20%28MAR%29%20outcome%0Adata%20has%20on%20causal%20machine%20learning%20estimators%20for%20the%20conditional%20average%0Atreatment%20effect%20%28CATE%29.%20We%20then%20propose%20two%20de-biased%20machine%20learning%0Aestimators%20for%20the%20CATE%2C%20the%20mDR-learner%20and%20mEP-learner%2C%20which%20address%20the%0Aissue%20of%20under-representation%20by%20integrating%20inverse%20probability%20of%20censoring%0Aweights%20into%20the%20DR-learner%20and%20EP-learner%20respectively.%20We%20show%20that%20under%0Areasonable%20conditions%2C%20these%20estimators%20are%20oracle%20efficient%2C%20and%20illustrate%0Atheir%20favorable%20performance%20through%20simulated%20data%20settings%2C%20comparing%20them%20to%0Aexisting%20CATE%20estimators%2C%20including%20comparison%20to%20estimators%20which%20use%20common%0Amissing%20data%20techniques.%20Guidance%20on%20the%20implementation%20of%20these%20estimators%20is%0Aprovided%20and%20we%20present%20an%20example%20of%20their%20application%20using%20the%20ACTG175%0Atrial%2C%20exploring%20treatment%20effect%20heterogeneity%20when%20comparing%20Zidovudine%0Amono-therapy%20against%20alternative%20antiretroviral%20therapies%20among%20HIV-1-infected%0Aindividuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520machine%2520learning%2520for%2520heterogeneous%2520treatment%2520effects%2520in%2520the%250A%2520%2520presence%2520of%2520missing%2520outcome%2520data%26entry.906535625%3DMatthew%2520Pryce%2520and%2520Karla%2520Diaz-Ordaz%2520and%2520Ruth%2520H.%2520Keogh%2520and%2520Stijn%2520Vansteelandt%26entry.1292438233%3D%2520%2520When%2520estimating%2520heterogeneous%2520treatment%2520effects%252C%2520missing%2520outcome%2520data%2520can%250Acomplicate%2520treatment%2520effect%2520estimation%252C%2520causing%2520certain%2520subgroups%2520of%2520the%250Apopulation%2520to%2520be%2520poorly%2520represented.%2520In%2520this%2520work%252C%2520we%2520discuss%2520this%2520commonly%250Aoverlooked%2520problem%2520and%2520consider%2520the%2520impact%2520that%2520missing%2520at%2520random%2520%2528MAR%2529%2520outcome%250Adata%2520has%2520on%2520causal%2520machine%2520learning%2520estimators%2520for%2520the%2520conditional%2520average%250Atreatment%2520effect%2520%2528CATE%2529.%2520We%2520then%2520propose%2520two%2520de-biased%2520machine%2520learning%250Aestimators%2520for%2520the%2520CATE%252C%2520the%2520mDR-learner%2520and%2520mEP-learner%252C%2520which%2520address%2520the%250Aissue%2520of%2520under-representation%2520by%2520integrating%2520inverse%2520probability%2520of%2520censoring%250Aweights%2520into%2520the%2520DR-learner%2520and%2520EP-learner%2520respectively.%2520We%2520show%2520that%2520under%250Areasonable%2520conditions%252C%2520these%2520estimators%2520are%2520oracle%2520efficient%252C%2520and%2520illustrate%250Atheir%2520favorable%2520performance%2520through%2520simulated%2520data%2520settings%252C%2520comparing%2520them%2520to%250Aexisting%2520CATE%2520estimators%252C%2520including%2520comparison%2520to%2520estimators%2520which%2520use%2520common%250Amissing%2520data%2520techniques.%2520Guidance%2520on%2520the%2520implementation%2520of%2520these%2520estimators%2520is%250Aprovided%2520and%2520we%2520present%2520an%2520example%2520of%2520their%2520application%2520using%2520the%2520ACTG175%250Atrial%252C%2520exploring%2520treatment%2520effect%2520heterogeneity%2520when%2520comparing%2520Zidovudine%250Amono-therapy%2520against%2520alternative%2520antiretroviral%2520therapies%2520among%2520HIV-1-infected%250Aindividuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20machine%20learning%20for%20heterogeneous%20treatment%20effects%20in%20the%0A%20%20presence%20of%20missing%20outcome%20data&entry.906535625=Matthew%20Pryce%20and%20Karla%20Diaz-Ordaz%20and%20Ruth%20H.%20Keogh%20and%20Stijn%20Vansteelandt&entry.1292438233=%20%20When%20estimating%20heterogeneous%20treatment%20effects%2C%20missing%20outcome%20data%20can%0Acomplicate%20treatment%20effect%20estimation%2C%20causing%20certain%20subgroups%20of%20the%0Apopulation%20to%20be%20poorly%20represented.%20In%20this%20work%2C%20we%20discuss%20this%20commonly%0Aoverlooked%20problem%20and%20consider%20the%20impact%20that%20missing%20at%20random%20%28MAR%29%20outcome%0Adata%20has%20on%20causal%20machine%20learning%20estimators%20for%20the%20conditional%20average%0Atreatment%20effect%20%28CATE%29.%20We%20then%20propose%20two%20de-biased%20machine%20learning%0Aestimators%20for%20the%20CATE%2C%20the%20mDR-learner%20and%20mEP-learner%2C%20which%20address%20the%0Aissue%20of%20under-representation%20by%20integrating%20inverse%20probability%20of%20censoring%0Aweights%20into%20the%20DR-learner%20and%20EP-learner%20respectively.%20We%20show%20that%20under%0Areasonable%20conditions%2C%20these%20estimators%20are%20oracle%20efficient%2C%20and%20illustrate%0Atheir%20favorable%20performance%20through%20simulated%20data%20settings%2C%20comparing%20them%20to%0Aexisting%20CATE%20estimators%2C%20including%20comparison%20to%20estimators%20which%20use%20common%0Amissing%20data%20techniques.%20Guidance%20on%20the%20implementation%20of%20these%20estimators%20is%0Aprovided%20and%20we%20present%20an%20example%20of%20their%20application%20using%20the%20ACTG175%0Atrial%2C%20exploring%20treatment%20effect%20heterogeneity%20when%20comparing%20Zidovudine%0Amono-therapy%20against%20alternative%20antiretroviral%20therapies%20among%20HIV-1-infected%0Aindividuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19711v1&entry.124074799=Read"},
{"title": "A Mathematical Framework for the Problem of Security for Cognition in\n  Neurotechnology", "author": "Bryce Allen Bagley and Claudia K Petritsch", "abstract": "  The rapid advancement in neurotechnology in recent years has created an\nemerging critical intersection between neurotechnology and security.\nImplantable devices, non-invasive monitoring, and non-invasive therapies all\ncarry with them the prospect of violating the privacy and autonomy of\nindividuals' cognition. A growing number of scientists and physicians have made\ncalls to address this issue, but applied efforts have been relatively limited.\nA major barrier hampering scientific and engineering efforts to address these\nsecurity issues is the lack of a clear means of describing and analyzing\nrelevant problems. In this paper we develop Cognitive Neurosecurity, a\nmathematical framework which enables such description and analysis by drawing\non methods and results from multiple fields. We demonstrate certain statistical\nproperties which have significant implications for Cognitive Neurosecurity, and\nthen present descriptions of the algorithmic problems faced by attackers\nattempting to violate privacy and autonomy, and defenders attempting to\nobstruct such attempts.\n", "link": "http://arxiv.org/abs/2403.07945v3", "date": "2024-12-27", "relevancy": 1.2455, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4214}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4174}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mathematical%20Framework%20for%20the%20Problem%20of%20Security%20for%20Cognition%20in%0A%20%20Neurotechnology&body=Title%3A%20A%20Mathematical%20Framework%20for%20the%20Problem%20of%20Security%20for%20Cognition%20in%0A%20%20Neurotechnology%0AAuthor%3A%20Bryce%20Allen%20Bagley%20and%20Claudia%20K%20Petritsch%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20neurotechnology%20in%20recent%20years%20has%20created%20an%0Aemerging%20critical%20intersection%20between%20neurotechnology%20and%20security.%0AImplantable%20devices%2C%20non-invasive%20monitoring%2C%20and%20non-invasive%20therapies%20all%0Acarry%20with%20them%20the%20prospect%20of%20violating%20the%20privacy%20and%20autonomy%20of%0Aindividuals%27%20cognition.%20A%20growing%20number%20of%20scientists%20and%20physicians%20have%20made%0Acalls%20to%20address%20this%20issue%2C%20but%20applied%20efforts%20have%20been%20relatively%20limited.%0AA%20major%20barrier%20hampering%20scientific%20and%20engineering%20efforts%20to%20address%20these%0Asecurity%20issues%20is%20the%20lack%20of%20a%20clear%20means%20of%20describing%20and%20analyzing%0Arelevant%20problems.%20In%20this%20paper%20we%20develop%20Cognitive%20Neurosecurity%2C%20a%0Amathematical%20framework%20which%20enables%20such%20description%20and%20analysis%20by%20drawing%0Aon%20methods%20and%20results%20from%20multiple%20fields.%20We%20demonstrate%20certain%20statistical%0Aproperties%20which%20have%20significant%20implications%20for%20Cognitive%20Neurosecurity%2C%20and%0Athen%20present%20descriptions%20of%20the%20algorithmic%20problems%20faced%20by%20attackers%0Aattempting%20to%20violate%20privacy%20and%20autonomy%2C%20and%20defenders%20attempting%20to%0Aobstruct%20such%20attempts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07945v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mathematical%2520Framework%2520for%2520the%2520Problem%2520of%2520Security%2520for%2520Cognition%2520in%250A%2520%2520Neurotechnology%26entry.906535625%3DBryce%2520Allen%2520Bagley%2520and%2520Claudia%2520K%2520Petritsch%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520neurotechnology%2520in%2520recent%2520years%2520has%2520created%2520an%250Aemerging%2520critical%2520intersection%2520between%2520neurotechnology%2520and%2520security.%250AImplantable%2520devices%252C%2520non-invasive%2520monitoring%252C%2520and%2520non-invasive%2520therapies%2520all%250Acarry%2520with%2520them%2520the%2520prospect%2520of%2520violating%2520the%2520privacy%2520and%2520autonomy%2520of%250Aindividuals%2527%2520cognition.%2520A%2520growing%2520number%2520of%2520scientists%2520and%2520physicians%2520have%2520made%250Acalls%2520to%2520address%2520this%2520issue%252C%2520but%2520applied%2520efforts%2520have%2520been%2520relatively%2520limited.%250AA%2520major%2520barrier%2520hampering%2520scientific%2520and%2520engineering%2520efforts%2520to%2520address%2520these%250Asecurity%2520issues%2520is%2520the%2520lack%2520of%2520a%2520clear%2520means%2520of%2520describing%2520and%2520analyzing%250Arelevant%2520problems.%2520In%2520this%2520paper%2520we%2520develop%2520Cognitive%2520Neurosecurity%252C%2520a%250Amathematical%2520framework%2520which%2520enables%2520such%2520description%2520and%2520analysis%2520by%2520drawing%250Aon%2520methods%2520and%2520results%2520from%2520multiple%2520fields.%2520We%2520demonstrate%2520certain%2520statistical%250Aproperties%2520which%2520have%2520significant%2520implications%2520for%2520Cognitive%2520Neurosecurity%252C%2520and%250Athen%2520present%2520descriptions%2520of%2520the%2520algorithmic%2520problems%2520faced%2520by%2520attackers%250Aattempting%2520to%2520violate%2520privacy%2520and%2520autonomy%252C%2520and%2520defenders%2520attempting%2520to%250Aobstruct%2520such%2520attempts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07945v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mathematical%20Framework%20for%20the%20Problem%20of%20Security%20for%20Cognition%20in%0A%20%20Neurotechnology&entry.906535625=Bryce%20Allen%20Bagley%20and%20Claudia%20K%20Petritsch&entry.1292438233=%20%20The%20rapid%20advancement%20in%20neurotechnology%20in%20recent%20years%20has%20created%20an%0Aemerging%20critical%20intersection%20between%20neurotechnology%20and%20security.%0AImplantable%20devices%2C%20non-invasive%20monitoring%2C%20and%20non-invasive%20therapies%20all%0Acarry%20with%20them%20the%20prospect%20of%20violating%20the%20privacy%20and%20autonomy%20of%0Aindividuals%27%20cognition.%20A%20growing%20number%20of%20scientists%20and%20physicians%20have%20made%0Acalls%20to%20address%20this%20issue%2C%20but%20applied%20efforts%20have%20been%20relatively%20limited.%0AA%20major%20barrier%20hampering%20scientific%20and%20engineering%20efforts%20to%20address%20these%0Asecurity%20issues%20is%20the%20lack%20of%20a%20clear%20means%20of%20describing%20and%20analyzing%0Arelevant%20problems.%20In%20this%20paper%20we%20develop%20Cognitive%20Neurosecurity%2C%20a%0Amathematical%20framework%20which%20enables%20such%20description%20and%20analysis%20by%20drawing%0Aon%20methods%20and%20results%20from%20multiple%20fields.%20We%20demonstrate%20certain%20statistical%0Aproperties%20which%20have%20significant%20implications%20for%20Cognitive%20Neurosecurity%2C%20and%0Athen%20present%20descriptions%20of%20the%20algorithmic%20problems%20faced%20by%20attackers%0Aattempting%20to%20violate%20privacy%20and%20autonomy%2C%20and%20defenders%20attempting%20to%0Aobstruct%20such%20attempts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07945v3&entry.124074799=Read"},
{"title": "Analysis of Premature Death Rates in Texas Counties: The Impact of Air\n  Quality, Socioeconomic Factors, and COPD Prevalence", "author": "Richard Rich and Ernesto Diaz", "abstract": "  Understanding factors contributing to premature mortality is critical for\npublic health planning. This study examines the relationships between premature\ndeath rates and multiple risk factors across several Texas counties, utilizing\nEPA air quality data, Census information, and county health records from recent\nyears. We analyze the impact of air quality (PM2.5 levels), socioeconomic\nfactors (median household income), and health conditions (COPD prevalence)\nthrough statistical analysis and modeling techniques. Results reveal COPD\nprevalence as a strong predictor of premature death rates, with higher\nprevalence associated with a substantial increase in years of potential life\nlost. While socioeconomic factors show a significant negative correlation, air\nquality demonstrates more complex indirect relationships. These findings\nemphasize the need for integrated public health interventions that prioritize\nkey health conditions while addressing underlying socioeconomic disparities.\n", "link": "http://arxiv.org/abs/2412.19774v1", "date": "2024-12-27", "relevancy": 1.1653, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.2964}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.2913}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.2862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Premature%20Death%20Rates%20in%20Texas%20Counties%3A%20The%20Impact%20of%20Air%0A%20%20Quality%2C%20Socioeconomic%20Factors%2C%20and%20COPD%20Prevalence&body=Title%3A%20Analysis%20of%20Premature%20Death%20Rates%20in%20Texas%20Counties%3A%20The%20Impact%20of%20Air%0A%20%20Quality%2C%20Socioeconomic%20Factors%2C%20and%20COPD%20Prevalence%0AAuthor%3A%20Richard%20Rich%20and%20Ernesto%20Diaz%0AAbstract%3A%20%20%20Understanding%20factors%20contributing%20to%20premature%20mortality%20is%20critical%20for%0Apublic%20health%20planning.%20This%20study%20examines%20the%20relationships%20between%20premature%0Adeath%20rates%20and%20multiple%20risk%20factors%20across%20several%20Texas%20counties%2C%20utilizing%0AEPA%20air%20quality%20data%2C%20Census%20information%2C%20and%20county%20health%20records%20from%20recent%0Ayears.%20We%20analyze%20the%20impact%20of%20air%20quality%20%28PM2.5%20levels%29%2C%20socioeconomic%0Afactors%20%28median%20household%20income%29%2C%20and%20health%20conditions%20%28COPD%20prevalence%29%0Athrough%20statistical%20analysis%20and%20modeling%20techniques.%20Results%20reveal%20COPD%0Aprevalence%20as%20a%20strong%20predictor%20of%20premature%20death%20rates%2C%20with%20higher%0Aprevalence%20associated%20with%20a%20substantial%20increase%20in%20years%20of%20potential%20life%0Alost.%20While%20socioeconomic%20factors%20show%20a%20significant%20negative%20correlation%2C%20air%0Aquality%20demonstrates%20more%20complex%20indirect%20relationships.%20These%20findings%0Aemphasize%20the%20need%20for%20integrated%20public%20health%20interventions%20that%20prioritize%0Akey%20health%20conditions%20while%20addressing%20underlying%20socioeconomic%20disparities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Premature%2520Death%2520Rates%2520in%2520Texas%2520Counties%253A%2520The%2520Impact%2520of%2520Air%250A%2520%2520Quality%252C%2520Socioeconomic%2520Factors%252C%2520and%2520COPD%2520Prevalence%26entry.906535625%3DRichard%2520Rich%2520and%2520Ernesto%2520Diaz%26entry.1292438233%3D%2520%2520Understanding%2520factors%2520contributing%2520to%2520premature%2520mortality%2520is%2520critical%2520for%250Apublic%2520health%2520planning.%2520This%2520study%2520examines%2520the%2520relationships%2520between%2520premature%250Adeath%2520rates%2520and%2520multiple%2520risk%2520factors%2520across%2520several%2520Texas%2520counties%252C%2520utilizing%250AEPA%2520air%2520quality%2520data%252C%2520Census%2520information%252C%2520and%2520county%2520health%2520records%2520from%2520recent%250Ayears.%2520We%2520analyze%2520the%2520impact%2520of%2520air%2520quality%2520%2528PM2.5%2520levels%2529%252C%2520socioeconomic%250Afactors%2520%2528median%2520household%2520income%2529%252C%2520and%2520health%2520conditions%2520%2528COPD%2520prevalence%2529%250Athrough%2520statistical%2520analysis%2520and%2520modeling%2520techniques.%2520Results%2520reveal%2520COPD%250Aprevalence%2520as%2520a%2520strong%2520predictor%2520of%2520premature%2520death%2520rates%252C%2520with%2520higher%250Aprevalence%2520associated%2520with%2520a%2520substantial%2520increase%2520in%2520years%2520of%2520potential%2520life%250Alost.%2520While%2520socioeconomic%2520factors%2520show%2520a%2520significant%2520negative%2520correlation%252C%2520air%250Aquality%2520demonstrates%2520more%2520complex%2520indirect%2520relationships.%2520These%2520findings%250Aemphasize%2520the%2520need%2520for%2520integrated%2520public%2520health%2520interventions%2520that%2520prioritize%250Akey%2520health%2520conditions%2520while%2520addressing%2520underlying%2520socioeconomic%2520disparities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Premature%20Death%20Rates%20in%20Texas%20Counties%3A%20The%20Impact%20of%20Air%0A%20%20Quality%2C%20Socioeconomic%20Factors%2C%20and%20COPD%20Prevalence&entry.906535625=Richard%20Rich%20and%20Ernesto%20Diaz&entry.1292438233=%20%20Understanding%20factors%20contributing%20to%20premature%20mortality%20is%20critical%20for%0Apublic%20health%20planning.%20This%20study%20examines%20the%20relationships%20between%20premature%0Adeath%20rates%20and%20multiple%20risk%20factors%20across%20several%20Texas%20counties%2C%20utilizing%0AEPA%20air%20quality%20data%2C%20Census%20information%2C%20and%20county%20health%20records%20from%20recent%0Ayears.%20We%20analyze%20the%20impact%20of%20air%20quality%20%28PM2.5%20levels%29%2C%20socioeconomic%0Afactors%20%28median%20household%20income%29%2C%20and%20health%20conditions%20%28COPD%20prevalence%29%0Athrough%20statistical%20analysis%20and%20modeling%20techniques.%20Results%20reveal%20COPD%0Aprevalence%20as%20a%20strong%20predictor%20of%20premature%20death%20rates%2C%20with%20higher%0Aprevalence%20associated%20with%20a%20substantial%20increase%20in%20years%20of%20potential%20life%0Alost.%20While%20socioeconomic%20factors%20show%20a%20significant%20negative%20correlation%2C%20air%0Aquality%20demonstrates%20more%20complex%20indirect%20relationships.%20These%20findings%0Aemphasize%20the%20need%20for%20integrated%20public%20health%20interventions%20that%20prioritize%0Akey%20health%20conditions%20while%20addressing%20underlying%20socioeconomic%20disparities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19774v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


