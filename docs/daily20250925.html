<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250924.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Video models are zero-shot learners and reasoners", "author": "Thadd\u00e4us Wiedemer and Yuxuan Li and Paul Vicol and Shixiang Shane Gu and Nick Matarese and Kevin Swersky and Been Kim and Priyank Jaini and Robert Geirhos", "abstract": "  The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models.\n", "link": "http://arxiv.org/abs/2509.20328v1", "date": "2025-09-24", "relevancy": 3.092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.645}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20models%20are%20zero-shot%20learners%20and%20reasoners&body=Title%3A%20Video%20models%20are%20zero-shot%20learners%20and%20reasoners%0AAuthor%3A%20Thadd%C3%A4us%20Wiedemer%20and%20Yuxuan%20Li%20and%20Paul%20Vicol%20and%20Shixiang%20Shane%20Gu%20and%20Nick%20Matarese%20and%20Kevin%20Swersky%20and%20Been%20Kim%20and%20Priyank%20Jaini%20and%20Robert%20Geirhos%0AAbstract%3A%20%20%20The%20remarkable%20zero-shot%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20have%0Apropelled%20natural%20language%20processing%20from%20task-specific%20models%20to%20unified%2C%0Ageneralist%20foundation%20models.%20This%20transformation%20emerged%20from%20simple%0Aprimitives%3A%20large%2C%20generative%20models%20trained%20on%20web-scale%20data.%20Curiously%2C%20the%0Asame%20primitives%20apply%20to%20today%27s%20generative%20video%20models.%20Could%20video%20models%20be%0Aon%20a%20trajectory%20towards%20general-purpose%20vision%20understanding%2C%20much%20like%20LLMs%0Adeveloped%20general-purpose%20language%20understanding%3F%20We%20demonstrate%20that%20Veo%203%20can%0Asolve%20a%20broad%20variety%20of%20tasks%20it%20wasn%27t%20explicitly%20trained%20for%3A%20segmenting%0Aobjects%2C%20detecting%20edges%2C%20editing%20images%2C%20understanding%20physical%20properties%2C%0Arecognizing%20object%20affordances%2C%20simulating%20tool%20use%2C%20and%20more.%20These%20abilities%0Ato%20perceive%2C%20model%2C%20and%20manipulate%20the%20visual%20world%20enable%20early%20forms%20of%0Avisual%20reasoning%20like%20maze%20and%20symmetry%20solving.%20Veo%27s%20emergent%20zero-shot%0Acapabilities%20indicate%20that%20video%20models%20are%20on%20a%20path%20to%20becoming%20unified%2C%0Ageneralist%20vision%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520models%2520are%2520zero-shot%2520learners%2520and%2520reasoners%26entry.906535625%3DThadd%25C3%25A4us%2520Wiedemer%2520and%2520Yuxuan%2520Li%2520and%2520Paul%2520Vicol%2520and%2520Shixiang%2520Shane%2520Gu%2520and%2520Nick%2520Matarese%2520and%2520Kevin%2520Swersky%2520and%2520Been%2520Kim%2520and%2520Priyank%2520Jaini%2520and%2520Robert%2520Geirhos%26entry.1292438233%3D%2520%2520The%2520remarkable%2520zero-shot%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Apropelled%2520natural%2520language%2520processing%2520from%2520task-specific%2520models%2520to%2520unified%252C%250Ageneralist%2520foundation%2520models.%2520This%2520transformation%2520emerged%2520from%2520simple%250Aprimitives%253A%2520large%252C%2520generative%2520models%2520trained%2520on%2520web-scale%2520data.%2520Curiously%252C%2520the%250Asame%2520primitives%2520apply%2520to%2520today%2527s%2520generative%2520video%2520models.%2520Could%2520video%2520models%2520be%250Aon%2520a%2520trajectory%2520towards%2520general-purpose%2520vision%2520understanding%252C%2520much%2520like%2520LLMs%250Adeveloped%2520general-purpose%2520language%2520understanding%253F%2520We%2520demonstrate%2520that%2520Veo%25203%2520can%250Asolve%2520a%2520broad%2520variety%2520of%2520tasks%2520it%2520wasn%2527t%2520explicitly%2520trained%2520for%253A%2520segmenting%250Aobjects%252C%2520detecting%2520edges%252C%2520editing%2520images%252C%2520understanding%2520physical%2520properties%252C%250Arecognizing%2520object%2520affordances%252C%2520simulating%2520tool%2520use%252C%2520and%2520more.%2520These%2520abilities%250Ato%2520perceive%252C%2520model%252C%2520and%2520manipulate%2520the%2520visual%2520world%2520enable%2520early%2520forms%2520of%250Avisual%2520reasoning%2520like%2520maze%2520and%2520symmetry%2520solving.%2520Veo%2527s%2520emergent%2520zero-shot%250Acapabilities%2520indicate%2520that%2520video%2520models%2520are%2520on%2520a%2520path%2520to%2520becoming%2520unified%252C%250Ageneralist%2520vision%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20models%20are%20zero-shot%20learners%20and%20reasoners&entry.906535625=Thadd%C3%A4us%20Wiedemer%20and%20Yuxuan%20Li%20and%20Paul%20Vicol%20and%20Shixiang%20Shane%20Gu%20and%20Nick%20Matarese%20and%20Kevin%20Swersky%20and%20Been%20Kim%20and%20Priyank%20Jaini%20and%20Robert%20Geirhos&entry.1292438233=%20%20The%20remarkable%20zero-shot%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20have%0Apropelled%20natural%20language%20processing%20from%20task-specific%20models%20to%20unified%2C%0Ageneralist%20foundation%20models.%20This%20transformation%20emerged%20from%20simple%0Aprimitives%3A%20large%2C%20generative%20models%20trained%20on%20web-scale%20data.%20Curiously%2C%20the%0Asame%20primitives%20apply%20to%20today%27s%20generative%20video%20models.%20Could%20video%20models%20be%0Aon%20a%20trajectory%20towards%20general-purpose%20vision%20understanding%2C%20much%20like%20LLMs%0Adeveloped%20general-purpose%20language%20understanding%3F%20We%20demonstrate%20that%20Veo%203%20can%0Asolve%20a%20broad%20variety%20of%20tasks%20it%20wasn%27t%20explicitly%20trained%20for%3A%20segmenting%0Aobjects%2C%20detecting%20edges%2C%20editing%20images%2C%20understanding%20physical%20properties%2C%0Arecognizing%20object%20affordances%2C%20simulating%20tool%20use%2C%20and%20more.%20These%20abilities%0Ato%20perceive%2C%20model%2C%20and%20manipulate%20the%20visual%20world%20enable%20early%20forms%20of%0Avisual%20reasoning%20like%20maze%20and%20symmetry%20solving.%20Veo%27s%20emergent%20zero-shot%0Acapabilities%20indicate%20that%20video%20models%20are%20on%20a%20path%20to%20becoming%20unified%2C%0Ageneralist%20vision%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20328v1&entry.124074799=Read"},
{"title": "PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video\n  Generation", "author": "Chen Wang and Chuhao Chen and Yiming Huang and Zhiyang Dou and Yuan Liu and Jiatao Gu and Lingjie Liu", "abstract": "  Existing video generation models excel at producing photo-realistic videos\nfrom text or images, but often lack physical plausibility and 3D\ncontrollability. To overcome these limitations, we introduce PhysCtrl, a novel\nframework for physics-grounded image-to-video generation with physical\nparameters and force control. At its core is a generative physics network that\nlearns the distribution of physical dynamics across four materials (elastic,\nsand, plasticine, and rigid) via a diffusion model conditioned on physics\nparameters and applied forces. We represent physical dynamics as 3D point\ntrajectories and train on a large-scale synthetic dataset of 550K animations\ngenerated by physics simulators. We enhance the diffusion model with a novel\nspatiotemporal attention block that emulates particle interactions and\nincorporates physics-based constraints during training to enforce physical\nplausibility. Experiments show that PhysCtrl generates realistic,\nphysics-grounded motion trajectories which, when used to drive image-to-video\nmodels, yield high-fidelity, controllable videos that outperform existing\nmethods in both visual quality and physical plausibility. Project Page:\nhttps://cwchenwang.github.io/physctrl\n", "link": "http://arxiv.org/abs/2509.20358v1", "date": "2025-09-24", "relevancy": 2.894, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.832}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6478}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysCtrl%3A%20Generative%20Physics%20for%20Controllable%20and%20Physics-Grounded%20Video%0A%20%20Generation&body=Title%3A%20PhysCtrl%3A%20Generative%20Physics%20for%20Controllable%20and%20Physics-Grounded%20Video%0A%20%20Generation%0AAuthor%3A%20Chen%20Wang%20and%20Chuhao%20Chen%20and%20Yiming%20Huang%20and%20Zhiyang%20Dou%20and%20Yuan%20Liu%20and%20Jiatao%20Gu%20and%20Lingjie%20Liu%0AAbstract%3A%20%20%20Existing%20video%20generation%20models%20excel%20at%20producing%20photo-realistic%20videos%0Afrom%20text%20or%20images%2C%20but%20often%20lack%20physical%20plausibility%20and%203D%0Acontrollability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20PhysCtrl%2C%20a%20novel%0Aframework%20for%20physics-grounded%20image-to-video%20generation%20with%20physical%0Aparameters%20and%20force%20control.%20At%20its%20core%20is%20a%20generative%20physics%20network%20that%0Alearns%20the%20distribution%20of%20physical%20dynamics%20across%20four%20materials%20%28elastic%2C%0Asand%2C%20plasticine%2C%20and%20rigid%29%20via%20a%20diffusion%20model%20conditioned%20on%20physics%0Aparameters%20and%20applied%20forces.%20We%20represent%20physical%20dynamics%20as%203D%20point%0Atrajectories%20and%20train%20on%20a%20large-scale%20synthetic%20dataset%20of%20550K%20animations%0Agenerated%20by%20physics%20simulators.%20We%20enhance%20the%20diffusion%20model%20with%20a%20novel%0Aspatiotemporal%20attention%20block%20that%20emulates%20particle%20interactions%20and%0Aincorporates%20physics-based%20constraints%20during%20training%20to%20enforce%20physical%0Aplausibility.%20Experiments%20show%20that%20PhysCtrl%20generates%20realistic%2C%0Aphysics-grounded%20motion%20trajectories%20which%2C%20when%20used%20to%20drive%20image-to-video%0Amodels%2C%20yield%20high-fidelity%2C%20controllable%20videos%20that%20outperform%20existing%0Amethods%20in%20both%20visual%20quality%20and%20physical%20plausibility.%20Project%20Page%3A%0Ahttps%3A//cwchenwang.github.io/physctrl%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysCtrl%253A%2520Generative%2520Physics%2520for%2520Controllable%2520and%2520Physics-Grounded%2520Video%250A%2520%2520Generation%26entry.906535625%3DChen%2520Wang%2520and%2520Chuhao%2520Chen%2520and%2520Yiming%2520Huang%2520and%2520Zhiyang%2520Dou%2520and%2520Yuan%2520Liu%2520and%2520Jiatao%2520Gu%2520and%2520Lingjie%2520Liu%26entry.1292438233%3D%2520%2520Existing%2520video%2520generation%2520models%2520excel%2520at%2520producing%2520photo-realistic%2520videos%250Afrom%2520text%2520or%2520images%252C%2520but%2520often%2520lack%2520physical%2520plausibility%2520and%25203D%250Acontrollability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520PhysCtrl%252C%2520a%2520novel%250Aframework%2520for%2520physics-grounded%2520image-to-video%2520generation%2520with%2520physical%250Aparameters%2520and%2520force%2520control.%2520At%2520its%2520core%2520is%2520a%2520generative%2520physics%2520network%2520that%250Alearns%2520the%2520distribution%2520of%2520physical%2520dynamics%2520across%2520four%2520materials%2520%2528elastic%252C%250Asand%252C%2520plasticine%252C%2520and%2520rigid%2529%2520via%2520a%2520diffusion%2520model%2520conditioned%2520on%2520physics%250Aparameters%2520and%2520applied%2520forces.%2520We%2520represent%2520physical%2520dynamics%2520as%25203D%2520point%250Atrajectories%2520and%2520train%2520on%2520a%2520large-scale%2520synthetic%2520dataset%2520of%2520550K%2520animations%250Agenerated%2520by%2520physics%2520simulators.%2520We%2520enhance%2520the%2520diffusion%2520model%2520with%2520a%2520novel%250Aspatiotemporal%2520attention%2520block%2520that%2520emulates%2520particle%2520interactions%2520and%250Aincorporates%2520physics-based%2520constraints%2520during%2520training%2520to%2520enforce%2520physical%250Aplausibility.%2520Experiments%2520show%2520that%2520PhysCtrl%2520generates%2520realistic%252C%250Aphysics-grounded%2520motion%2520trajectories%2520which%252C%2520when%2520used%2520to%2520drive%2520image-to-video%250Amodels%252C%2520yield%2520high-fidelity%252C%2520controllable%2520videos%2520that%2520outperform%2520existing%250Amethods%2520in%2520both%2520visual%2520quality%2520and%2520physical%2520plausibility.%2520Project%2520Page%253A%250Ahttps%253A//cwchenwang.github.io/physctrl%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysCtrl%3A%20Generative%20Physics%20for%20Controllable%20and%20Physics-Grounded%20Video%0A%20%20Generation&entry.906535625=Chen%20Wang%20and%20Chuhao%20Chen%20and%20Yiming%20Huang%20and%20Zhiyang%20Dou%20and%20Yuan%20Liu%20and%20Jiatao%20Gu%20and%20Lingjie%20Liu&entry.1292438233=%20%20Existing%20video%20generation%20models%20excel%20at%20producing%20photo-realistic%20videos%0Afrom%20text%20or%20images%2C%20but%20often%20lack%20physical%20plausibility%20and%203D%0Acontrollability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20PhysCtrl%2C%20a%20novel%0Aframework%20for%20physics-grounded%20image-to-video%20generation%20with%20physical%0Aparameters%20and%20force%20control.%20At%20its%20core%20is%20a%20generative%20physics%20network%20that%0Alearns%20the%20distribution%20of%20physical%20dynamics%20across%20four%20materials%20%28elastic%2C%0Asand%2C%20plasticine%2C%20and%20rigid%29%20via%20a%20diffusion%20model%20conditioned%20on%20physics%0Aparameters%20and%20applied%20forces.%20We%20represent%20physical%20dynamics%20as%203D%20point%0Atrajectories%20and%20train%20on%20a%20large-scale%20synthetic%20dataset%20of%20550K%20animations%0Agenerated%20by%20physics%20simulators.%20We%20enhance%20the%20diffusion%20model%20with%20a%20novel%0Aspatiotemporal%20attention%20block%20that%20emulates%20particle%20interactions%20and%0Aincorporates%20physics-based%20constraints%20during%20training%20to%20enforce%20physical%0Aplausibility.%20Experiments%20show%20that%20PhysCtrl%20generates%20realistic%2C%0Aphysics-grounded%20motion%20trajectories%20which%2C%20when%20used%20to%20drive%20image-to-video%0Amodels%2C%20yield%20high-fidelity%2C%20controllable%20videos%20that%20outperform%20existing%0Amethods%20in%20both%20visual%20quality%20and%20physical%20plausibility.%20Project%20Page%3A%0Ahttps%3A//cwchenwang.github.io/physctrl%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20358v1&entry.124074799=Read"},
{"title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  via Intermediate Projector", "author": "Yiming Cao and Yanjie Li and Kaisheng Liang and Bin Xiao", "abstract": "  The growing deployment of Large Vision-Language Models (VLMs) raises safety\nconcerns, as adversaries may exploit model vulnerabilities to induce harmful\noutputs, with targeted black-box adversarial attacks posing a particularly\nsevere threat. However, existing methods primarily maximize encoder-level\nglobal similarity, which lacks the granularity for stealthy and practical\nfine-grained attacks, where only specific target should be altered (e.g.,\nmodifying a car while preserving its background). Moreover, they largely\nneglect the projector, a key semantic bridge in VLMs for multimodal alignment.\nTo address these limitations, we propose a novel black-box targeted attack\nframework that leverages the projector. Specifically, we utilize the widely\nadopted Querying Transformer (Q-Former) which transforms global image\nembeddings into fine-grained query outputs, to enhance attack effectiveness and\ngranularity. For standard global targeted attack scenarios, we propose the\nIntermediate Projector Guided Attack (IPGA), which aligns Q-Former fine-grained\nquery outputs with the target to enhance attack strength and exploits the\nintermediate pretrained Q-Former that is not fine-tuned for any specific Large\nLanguage Model (LLM) to improve attack transferability. For fine-grained attack\nscenarios, we augment IPGA with the Residual Query Alignment (RQA) module,\nwhich preserves unrelated content by constraining non-target query outputs to\nenhance attack granularity. Extensive experiments demonstrate that IPGA\nsignificantly outperforms baselines in global targeted attacks, and IPGA with\nRQA (IPGA-R) attains superior success rates and unrelated content preservation\nover baselines in fine-grained attacks. Our method also transfers effectively\nto commercial VLMs such as Google Gemini and OpenAI GPT.\n", "link": "http://arxiv.org/abs/2508.13739v2", "date": "2025-09-24", "relevancy": 2.6836, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Targeted%20Adversarial%20Attacks%20on%20Large%20Vision-Language%20Models%0A%20%20via%20Intermediate%20Projector&body=Title%3A%20Enhancing%20Targeted%20Adversarial%20Attacks%20on%20Large%20Vision-Language%20Models%0A%20%20via%20Intermediate%20Projector%0AAuthor%3A%20Yiming%20Cao%20and%20Yanjie%20Li%20and%20Kaisheng%20Liang%20and%20Bin%20Xiao%0AAbstract%3A%20%20%20The%20growing%20deployment%20of%20Large%20Vision-Language%20Models%20%28VLMs%29%20raises%20safety%0Aconcerns%2C%20as%20adversaries%20may%20exploit%20model%20vulnerabilities%20to%20induce%20harmful%0Aoutputs%2C%20with%20targeted%20black-box%20adversarial%20attacks%20posing%20a%20particularly%0Asevere%20threat.%20However%2C%20existing%20methods%20primarily%20maximize%20encoder-level%0Aglobal%20similarity%2C%20which%20lacks%20the%20granularity%20for%20stealthy%20and%20practical%0Afine-grained%20attacks%2C%20where%20only%20specific%20target%20should%20be%20altered%20%28e.g.%2C%0Amodifying%20a%20car%20while%20preserving%20its%20background%29.%20Moreover%2C%20they%20largely%0Aneglect%20the%20projector%2C%20a%20key%20semantic%20bridge%20in%20VLMs%20for%20multimodal%20alignment.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20black-box%20targeted%20attack%0Aframework%20that%20leverages%20the%20projector.%20Specifically%2C%20we%20utilize%20the%20widely%0Aadopted%20Querying%20Transformer%20%28Q-Former%29%20which%20transforms%20global%20image%0Aembeddings%20into%20fine-grained%20query%20outputs%2C%20to%20enhance%20attack%20effectiveness%20and%0Agranularity.%20For%20standard%20global%20targeted%20attack%20scenarios%2C%20we%20propose%20the%0AIntermediate%20Projector%20Guided%20Attack%20%28IPGA%29%2C%20which%20aligns%20Q-Former%20fine-grained%0Aquery%20outputs%20with%20the%20target%20to%20enhance%20attack%20strength%20and%20exploits%20the%0Aintermediate%20pretrained%20Q-Former%20that%20is%20not%20fine-tuned%20for%20any%20specific%20Large%0ALanguage%20Model%20%28LLM%29%20to%20improve%20attack%20transferability.%20For%20fine-grained%20attack%0Ascenarios%2C%20we%20augment%20IPGA%20with%20the%20Residual%20Query%20Alignment%20%28RQA%29%20module%2C%0Awhich%20preserves%20unrelated%20content%20by%20constraining%20non-target%20query%20outputs%20to%0Aenhance%20attack%20granularity.%20Extensive%20experiments%20demonstrate%20that%20IPGA%0Asignificantly%20outperforms%20baselines%20in%20global%20targeted%20attacks%2C%20and%20IPGA%20with%0ARQA%20%28IPGA-R%29%20attains%20superior%20success%20rates%20and%20unrelated%20content%20preservation%0Aover%20baselines%20in%20fine-grained%20attacks.%20Our%20method%20also%20transfers%20effectively%0Ato%20commercial%20VLMs%20such%20as%20Google%20Gemini%20and%20OpenAI%20GPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13739v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Targeted%2520Adversarial%2520Attacks%2520on%2520Large%2520Vision-Language%2520Models%250A%2520%2520via%2520Intermediate%2520Projector%26entry.906535625%3DYiming%2520Cao%2520and%2520Yanjie%2520Li%2520and%2520Kaisheng%2520Liang%2520and%2520Bin%2520Xiao%26entry.1292438233%3D%2520%2520The%2520growing%2520deployment%2520of%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520raises%2520safety%250Aconcerns%252C%2520as%2520adversaries%2520may%2520exploit%2520model%2520vulnerabilities%2520to%2520induce%2520harmful%250Aoutputs%252C%2520with%2520targeted%2520black-box%2520adversarial%2520attacks%2520posing%2520a%2520particularly%250Asevere%2520threat.%2520However%252C%2520existing%2520methods%2520primarily%2520maximize%2520encoder-level%250Aglobal%2520similarity%252C%2520which%2520lacks%2520the%2520granularity%2520for%2520stealthy%2520and%2520practical%250Afine-grained%2520attacks%252C%2520where%2520only%2520specific%2520target%2520should%2520be%2520altered%2520%2528e.g.%252C%250Amodifying%2520a%2520car%2520while%2520preserving%2520its%2520background%2529.%2520Moreover%252C%2520they%2520largely%250Aneglect%2520the%2520projector%252C%2520a%2520key%2520semantic%2520bridge%2520in%2520VLMs%2520for%2520multimodal%2520alignment.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520black-box%2520targeted%2520attack%250Aframework%2520that%2520leverages%2520the%2520projector.%2520Specifically%252C%2520we%2520utilize%2520the%2520widely%250Aadopted%2520Querying%2520Transformer%2520%2528Q-Former%2529%2520which%2520transforms%2520global%2520image%250Aembeddings%2520into%2520fine-grained%2520query%2520outputs%252C%2520to%2520enhance%2520attack%2520effectiveness%2520and%250Agranularity.%2520For%2520standard%2520global%2520targeted%2520attack%2520scenarios%252C%2520we%2520propose%2520the%250AIntermediate%2520Projector%2520Guided%2520Attack%2520%2528IPGA%2529%252C%2520which%2520aligns%2520Q-Former%2520fine-grained%250Aquery%2520outputs%2520with%2520the%2520target%2520to%2520enhance%2520attack%2520strength%2520and%2520exploits%2520the%250Aintermediate%2520pretrained%2520Q-Former%2520that%2520is%2520not%2520fine-tuned%2520for%2520any%2520specific%2520Large%250ALanguage%2520Model%2520%2528LLM%2529%2520to%2520improve%2520attack%2520transferability.%2520For%2520fine-grained%2520attack%250Ascenarios%252C%2520we%2520augment%2520IPGA%2520with%2520the%2520Residual%2520Query%2520Alignment%2520%2528RQA%2529%2520module%252C%250Awhich%2520preserves%2520unrelated%2520content%2520by%2520constraining%2520non-target%2520query%2520outputs%2520to%250Aenhance%2520attack%2520granularity.%2520Extensive%2520experiments%2520demonstrate%2520that%2520IPGA%250Asignificantly%2520outperforms%2520baselines%2520in%2520global%2520targeted%2520attacks%252C%2520and%2520IPGA%2520with%250ARQA%2520%2528IPGA-R%2529%2520attains%2520superior%2520success%2520rates%2520and%2520unrelated%2520content%2520preservation%250Aover%2520baselines%2520in%2520fine-grained%2520attacks.%2520Our%2520method%2520also%2520transfers%2520effectively%250Ato%2520commercial%2520VLMs%2520such%2520as%2520Google%2520Gemini%2520and%2520OpenAI%2520GPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13739v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Targeted%20Adversarial%20Attacks%20on%20Large%20Vision-Language%20Models%0A%20%20via%20Intermediate%20Projector&entry.906535625=Yiming%20Cao%20and%20Yanjie%20Li%20and%20Kaisheng%20Liang%20and%20Bin%20Xiao&entry.1292438233=%20%20The%20growing%20deployment%20of%20Large%20Vision-Language%20Models%20%28VLMs%29%20raises%20safety%0Aconcerns%2C%20as%20adversaries%20may%20exploit%20model%20vulnerabilities%20to%20induce%20harmful%0Aoutputs%2C%20with%20targeted%20black-box%20adversarial%20attacks%20posing%20a%20particularly%0Asevere%20threat.%20However%2C%20existing%20methods%20primarily%20maximize%20encoder-level%0Aglobal%20similarity%2C%20which%20lacks%20the%20granularity%20for%20stealthy%20and%20practical%0Afine-grained%20attacks%2C%20where%20only%20specific%20target%20should%20be%20altered%20%28e.g.%2C%0Amodifying%20a%20car%20while%20preserving%20its%20background%29.%20Moreover%2C%20they%20largely%0Aneglect%20the%20projector%2C%20a%20key%20semantic%20bridge%20in%20VLMs%20for%20multimodal%20alignment.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20black-box%20targeted%20attack%0Aframework%20that%20leverages%20the%20projector.%20Specifically%2C%20we%20utilize%20the%20widely%0Aadopted%20Querying%20Transformer%20%28Q-Former%29%20which%20transforms%20global%20image%0Aembeddings%20into%20fine-grained%20query%20outputs%2C%20to%20enhance%20attack%20effectiveness%20and%0Agranularity.%20For%20standard%20global%20targeted%20attack%20scenarios%2C%20we%20propose%20the%0AIntermediate%20Projector%20Guided%20Attack%20%28IPGA%29%2C%20which%20aligns%20Q-Former%20fine-grained%0Aquery%20outputs%20with%20the%20target%20to%20enhance%20attack%20strength%20and%20exploits%20the%0Aintermediate%20pretrained%20Q-Former%20that%20is%20not%20fine-tuned%20for%20any%20specific%20Large%0ALanguage%20Model%20%28LLM%29%20to%20improve%20attack%20transferability.%20For%20fine-grained%20attack%0Ascenarios%2C%20we%20augment%20IPGA%20with%20the%20Residual%20Query%20Alignment%20%28RQA%29%20module%2C%0Awhich%20preserves%20unrelated%20content%20by%20constraining%20non-target%20query%20outputs%20to%0Aenhance%20attack%20granularity.%20Extensive%20experiments%20demonstrate%20that%20IPGA%0Asignificantly%20outperforms%20baselines%20in%20global%20targeted%20attacks%2C%20and%20IPGA%20with%0ARQA%20%28IPGA-R%29%20attains%20superior%20success%20rates%20and%20unrelated%20content%20preservation%0Aover%20baselines%20in%20fine-grained%20attacks.%20Our%20method%20also%20transfers%20effectively%0Ato%20commercial%20VLMs%20such%20as%20Google%20Gemini%20and%20OpenAI%20GPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13739v2&entry.124074799=Read"},
{"title": "Efficient Encoder-Free Pose Conditioning and Pose Control for Virtual\n  Try-On", "author": "Qi Li and Shuwen Qiu and Julien Han and Xingzi Xu and Mehmet Saygin Seyfioglu and Kee Kiat Koo and Karim Bouyarmane", "abstract": "  As online shopping continues to grow, the demand for Virtual Try-On (VTON)\ntechnology has surged, allowing customers to visualize products on themselves\nby overlaying product images onto their own photos. An essential yet\nchallenging condition for effective VTON is pose control, which ensures\naccurate alignment of products with the user's body while supporting diverse\norientations for a more immersive experience. However, incorporating pose\nconditions into VTON models presents several challenges, including selecting\nthe optimal pose representation, integrating poses without additional\nparameters, and balancing pose preservation with flexible pose control.\n  In this work, we build upon a baseline VTON model that concatenates the\nreference image condition without external encoder, control network, or complex\nattention layers. We investigate methods to incorporate pose control into this\npure concatenation paradigm by spatially concatenating pose data, comparing\nperformance using pose maps and skeletons, without adding any additional\nparameters or module to the baseline model. Our experiments reveal that pose\nstitching with pose maps yields the best results, enhancing both pose\npreservation and output realism. Additionally, we introduce a mixed-mask\ntraining strategy using fine-grained and bounding box masks, allowing the model\nto support flexible product integration across varied poses and conditions.\n", "link": "http://arxiv.org/abs/2509.20343v1", "date": "2025-09-24", "relevancy": 2.5459, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6447}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6426}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Encoder-Free%20Pose%20Conditioning%20and%20Pose%20Control%20for%20Virtual%0A%20%20Try-On&body=Title%3A%20Efficient%20Encoder-Free%20Pose%20Conditioning%20and%20Pose%20Control%20for%20Virtual%0A%20%20Try-On%0AAuthor%3A%20Qi%20Li%20and%20Shuwen%20Qiu%20and%20Julien%20Han%20and%20Xingzi%20Xu%20and%20Mehmet%20Saygin%20Seyfioglu%20and%20Kee%20Kiat%20Koo%20and%20Karim%20Bouyarmane%0AAbstract%3A%20%20%20As%20online%20shopping%20continues%20to%20grow%2C%20the%20demand%20for%20Virtual%20Try-On%20%28VTON%29%0Atechnology%20has%20surged%2C%20allowing%20customers%20to%20visualize%20products%20on%20themselves%0Aby%20overlaying%20product%20images%20onto%20their%20own%20photos.%20An%20essential%20yet%0Achallenging%20condition%20for%20effective%20VTON%20is%20pose%20control%2C%20which%20ensures%0Aaccurate%20alignment%20of%20products%20with%20the%20user%27s%20body%20while%20supporting%20diverse%0Aorientations%20for%20a%20more%20immersive%20experience.%20However%2C%20incorporating%20pose%0Aconditions%20into%20VTON%20models%20presents%20several%20challenges%2C%20including%20selecting%0Athe%20optimal%20pose%20representation%2C%20integrating%20poses%20without%20additional%0Aparameters%2C%20and%20balancing%20pose%20preservation%20with%20flexible%20pose%20control.%0A%20%20In%20this%20work%2C%20we%20build%20upon%20a%20baseline%20VTON%20model%20that%20concatenates%20the%0Areference%20image%20condition%20without%20external%20encoder%2C%20control%20network%2C%20or%20complex%0Aattention%20layers.%20We%20investigate%20methods%20to%20incorporate%20pose%20control%20into%20this%0Apure%20concatenation%20paradigm%20by%20spatially%20concatenating%20pose%20data%2C%20comparing%0Aperformance%20using%20pose%20maps%20and%20skeletons%2C%20without%20adding%20any%20additional%0Aparameters%20or%20module%20to%20the%20baseline%20model.%20Our%20experiments%20reveal%20that%20pose%0Astitching%20with%20pose%20maps%20yields%20the%20best%20results%2C%20enhancing%20both%20pose%0Apreservation%20and%20output%20realism.%20Additionally%2C%20we%20introduce%20a%20mixed-mask%0Atraining%20strategy%20using%20fine-grained%20and%20bounding%20box%20masks%2C%20allowing%20the%20model%0Ato%20support%20flexible%20product%20integration%20across%20varied%20poses%20and%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Encoder-Free%2520Pose%2520Conditioning%2520and%2520Pose%2520Control%2520for%2520Virtual%250A%2520%2520Try-On%26entry.906535625%3DQi%2520Li%2520and%2520Shuwen%2520Qiu%2520and%2520Julien%2520Han%2520and%2520Xingzi%2520Xu%2520and%2520Mehmet%2520Saygin%2520Seyfioglu%2520and%2520Kee%2520Kiat%2520Koo%2520and%2520Karim%2520Bouyarmane%26entry.1292438233%3D%2520%2520As%2520online%2520shopping%2520continues%2520to%2520grow%252C%2520the%2520demand%2520for%2520Virtual%2520Try-On%2520%2528VTON%2529%250Atechnology%2520has%2520surged%252C%2520allowing%2520customers%2520to%2520visualize%2520products%2520on%2520themselves%250Aby%2520overlaying%2520product%2520images%2520onto%2520their%2520own%2520photos.%2520An%2520essential%2520yet%250Achallenging%2520condition%2520for%2520effective%2520VTON%2520is%2520pose%2520control%252C%2520which%2520ensures%250Aaccurate%2520alignment%2520of%2520products%2520with%2520the%2520user%2527s%2520body%2520while%2520supporting%2520diverse%250Aorientations%2520for%2520a%2520more%2520immersive%2520experience.%2520However%252C%2520incorporating%2520pose%250Aconditions%2520into%2520VTON%2520models%2520presents%2520several%2520challenges%252C%2520including%2520selecting%250Athe%2520optimal%2520pose%2520representation%252C%2520integrating%2520poses%2520without%2520additional%250Aparameters%252C%2520and%2520balancing%2520pose%2520preservation%2520with%2520flexible%2520pose%2520control.%250A%2520%2520In%2520this%2520work%252C%2520we%2520build%2520upon%2520a%2520baseline%2520VTON%2520model%2520that%2520concatenates%2520the%250Areference%2520image%2520condition%2520without%2520external%2520encoder%252C%2520control%2520network%252C%2520or%2520complex%250Aattention%2520layers.%2520We%2520investigate%2520methods%2520to%2520incorporate%2520pose%2520control%2520into%2520this%250Apure%2520concatenation%2520paradigm%2520by%2520spatially%2520concatenating%2520pose%2520data%252C%2520comparing%250Aperformance%2520using%2520pose%2520maps%2520and%2520skeletons%252C%2520without%2520adding%2520any%2520additional%250Aparameters%2520or%2520module%2520to%2520the%2520baseline%2520model.%2520Our%2520experiments%2520reveal%2520that%2520pose%250Astitching%2520with%2520pose%2520maps%2520yields%2520the%2520best%2520results%252C%2520enhancing%2520both%2520pose%250Apreservation%2520and%2520output%2520realism.%2520Additionally%252C%2520we%2520introduce%2520a%2520mixed-mask%250Atraining%2520strategy%2520using%2520fine-grained%2520and%2520bounding%2520box%2520masks%252C%2520allowing%2520the%2520model%250Ato%2520support%2520flexible%2520product%2520integration%2520across%2520varied%2520poses%2520and%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Encoder-Free%20Pose%20Conditioning%20and%20Pose%20Control%20for%20Virtual%0A%20%20Try-On&entry.906535625=Qi%20Li%20and%20Shuwen%20Qiu%20and%20Julien%20Han%20and%20Xingzi%20Xu%20and%20Mehmet%20Saygin%20Seyfioglu%20and%20Kee%20Kiat%20Koo%20and%20Karim%20Bouyarmane&entry.1292438233=%20%20As%20online%20shopping%20continues%20to%20grow%2C%20the%20demand%20for%20Virtual%20Try-On%20%28VTON%29%0Atechnology%20has%20surged%2C%20allowing%20customers%20to%20visualize%20products%20on%20themselves%0Aby%20overlaying%20product%20images%20onto%20their%20own%20photos.%20An%20essential%20yet%0Achallenging%20condition%20for%20effective%20VTON%20is%20pose%20control%2C%20which%20ensures%0Aaccurate%20alignment%20of%20products%20with%20the%20user%27s%20body%20while%20supporting%20diverse%0Aorientations%20for%20a%20more%20immersive%20experience.%20However%2C%20incorporating%20pose%0Aconditions%20into%20VTON%20models%20presents%20several%20challenges%2C%20including%20selecting%0Athe%20optimal%20pose%20representation%2C%20integrating%20poses%20without%20additional%0Aparameters%2C%20and%20balancing%20pose%20preservation%20with%20flexible%20pose%20control.%0A%20%20In%20this%20work%2C%20we%20build%20upon%20a%20baseline%20VTON%20model%20that%20concatenates%20the%0Areference%20image%20condition%20without%20external%20encoder%2C%20control%20network%2C%20or%20complex%0Aattention%20layers.%20We%20investigate%20methods%20to%20incorporate%20pose%20control%20into%20this%0Apure%20concatenation%20paradigm%20by%20spatially%20concatenating%20pose%20data%2C%20comparing%0Aperformance%20using%20pose%20maps%20and%20skeletons%2C%20without%20adding%20any%20additional%0Aparameters%20or%20module%20to%20the%20baseline%20model.%20Our%20experiments%20reveal%20that%20pose%0Astitching%20with%20pose%20maps%20yields%20the%20best%20results%2C%20enhancing%20both%20pose%0Apreservation%20and%20output%20realism.%20Additionally%2C%20we%20introduce%20a%20mixed-mask%0Atraining%20strategy%20using%20fine-grained%20and%20bounding%20box%20masks%2C%20allowing%20the%20model%0Ato%20support%20flexible%20product%20integration%20across%20varied%20poses%20and%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20343v1&entry.124074799=Read"},
{"title": "VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and\n  Generation", "author": "Shaofeng Yin and Yanjie Ze and Hong-Xing Yu and C. Karen Liu and Jiajun Wu", "abstract": "  Humanoid loco-manipulation in unstructured environments demands tight\nintegration of egocentric perception and whole-body control. However, existing\napproaches either depend on external motion capture systems or fail to\ngeneralize across diverse tasks. We introduce VisualMimic, a visual sim-to-real\nframework that unifies egocentric vision with hierarchical whole-body control\nfor humanoid robots. VisualMimic combines a task-agnostic low-level keypoint\ntracker -- trained from human motion data via a teacher-student scheme -- with\na task-specific high-level policy that generates keypoint commands from visual\nand proprioceptive input. To ensure stable training, we inject noise into the\nlow-level policy and clip high-level actions using human motion statistics.\nVisualMimic enables zero-shot transfer of visuomotor policies trained in\nsimulation to real humanoid robots, accomplishing a wide range of\nloco-manipulation tasks such as box lifting, pushing, football dribbling, and\nkicking. Beyond controlled laboratory settings, our policies also generalize\nrobustly to outdoor environments. Videos are available at:\nhttps://visualmimic.github.io .\n", "link": "http://arxiv.org/abs/2509.20322v1", "date": "2025-09-24", "relevancy": 2.5309, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.634}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6335}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualMimic%3A%20Visual%20Humanoid%20Loco-Manipulation%20via%20Motion%20Tracking%20and%0A%20%20Generation&body=Title%3A%20VisualMimic%3A%20Visual%20Humanoid%20Loco-Manipulation%20via%20Motion%20Tracking%20and%0A%20%20Generation%0AAuthor%3A%20Shaofeng%20Yin%20and%20Yanjie%20Ze%20and%20Hong-Xing%20Yu%20and%20C.%20Karen%20Liu%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20Humanoid%20loco-manipulation%20in%20unstructured%20environments%20demands%20tight%0Aintegration%20of%20egocentric%20perception%20and%20whole-body%20control.%20However%2C%20existing%0Aapproaches%20either%20depend%20on%20external%20motion%20capture%20systems%20or%20fail%20to%0Ageneralize%20across%20diverse%20tasks.%20We%20introduce%20VisualMimic%2C%20a%20visual%20sim-to-real%0Aframework%20that%20unifies%20egocentric%20vision%20with%20hierarchical%20whole-body%20control%0Afor%20humanoid%20robots.%20VisualMimic%20combines%20a%20task-agnostic%20low-level%20keypoint%0Atracker%20--%20trained%20from%20human%20motion%20data%20via%20a%20teacher-student%20scheme%20--%20with%0Aa%20task-specific%20high-level%20policy%20that%20generates%20keypoint%20commands%20from%20visual%0Aand%20proprioceptive%20input.%20To%20ensure%20stable%20training%2C%20we%20inject%20noise%20into%20the%0Alow-level%20policy%20and%20clip%20high-level%20actions%20using%20human%20motion%20statistics.%0AVisualMimic%20enables%20zero-shot%20transfer%20of%20visuomotor%20policies%20trained%20in%0Asimulation%20to%20real%20humanoid%20robots%2C%20accomplishing%20a%20wide%20range%20of%0Aloco-manipulation%20tasks%20such%20as%20box%20lifting%2C%20pushing%2C%20football%20dribbling%2C%20and%0Akicking.%20Beyond%20controlled%20laboratory%20settings%2C%20our%20policies%20also%20generalize%0Arobustly%20to%20outdoor%20environments.%20Videos%20are%20available%20at%3A%0Ahttps%3A//visualmimic.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualMimic%253A%2520Visual%2520Humanoid%2520Loco-Manipulation%2520via%2520Motion%2520Tracking%2520and%250A%2520%2520Generation%26entry.906535625%3DShaofeng%2520Yin%2520and%2520Yanjie%2520Ze%2520and%2520Hong-Xing%2520Yu%2520and%2520C.%2520Karen%2520Liu%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520Humanoid%2520loco-manipulation%2520in%2520unstructured%2520environments%2520demands%2520tight%250Aintegration%2520of%2520egocentric%2520perception%2520and%2520whole-body%2520control.%2520However%252C%2520existing%250Aapproaches%2520either%2520depend%2520on%2520external%2520motion%2520capture%2520systems%2520or%2520fail%2520to%250Ageneralize%2520across%2520diverse%2520tasks.%2520We%2520introduce%2520VisualMimic%252C%2520a%2520visual%2520sim-to-real%250Aframework%2520that%2520unifies%2520egocentric%2520vision%2520with%2520hierarchical%2520whole-body%2520control%250Afor%2520humanoid%2520robots.%2520VisualMimic%2520combines%2520a%2520task-agnostic%2520low-level%2520keypoint%250Atracker%2520--%2520trained%2520from%2520human%2520motion%2520data%2520via%2520a%2520teacher-student%2520scheme%2520--%2520with%250Aa%2520task-specific%2520high-level%2520policy%2520that%2520generates%2520keypoint%2520commands%2520from%2520visual%250Aand%2520proprioceptive%2520input.%2520To%2520ensure%2520stable%2520training%252C%2520we%2520inject%2520noise%2520into%2520the%250Alow-level%2520policy%2520and%2520clip%2520high-level%2520actions%2520using%2520human%2520motion%2520statistics.%250AVisualMimic%2520enables%2520zero-shot%2520transfer%2520of%2520visuomotor%2520policies%2520trained%2520in%250Asimulation%2520to%2520real%2520humanoid%2520robots%252C%2520accomplishing%2520a%2520wide%2520range%2520of%250Aloco-manipulation%2520tasks%2520such%2520as%2520box%2520lifting%252C%2520pushing%252C%2520football%2520dribbling%252C%2520and%250Akicking.%2520Beyond%2520controlled%2520laboratory%2520settings%252C%2520our%2520policies%2520also%2520generalize%250Arobustly%2520to%2520outdoor%2520environments.%2520Videos%2520are%2520available%2520at%253A%250Ahttps%253A//visualmimic.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualMimic%3A%20Visual%20Humanoid%20Loco-Manipulation%20via%20Motion%20Tracking%20and%0A%20%20Generation&entry.906535625=Shaofeng%20Yin%20and%20Yanjie%20Ze%20and%20Hong-Xing%20Yu%20and%20C.%20Karen%20Liu%20and%20Jiajun%20Wu&entry.1292438233=%20%20Humanoid%20loco-manipulation%20in%20unstructured%20environments%20demands%20tight%0Aintegration%20of%20egocentric%20perception%20and%20whole-body%20control.%20However%2C%20existing%0Aapproaches%20either%20depend%20on%20external%20motion%20capture%20systems%20or%20fail%20to%0Ageneralize%20across%20diverse%20tasks.%20We%20introduce%20VisualMimic%2C%20a%20visual%20sim-to-real%0Aframework%20that%20unifies%20egocentric%20vision%20with%20hierarchical%20whole-body%20control%0Afor%20humanoid%20robots.%20VisualMimic%20combines%20a%20task-agnostic%20low-level%20keypoint%0Atracker%20--%20trained%20from%20human%20motion%20data%20via%20a%20teacher-student%20scheme%20--%20with%0Aa%20task-specific%20high-level%20policy%20that%20generates%20keypoint%20commands%20from%20visual%0Aand%20proprioceptive%20input.%20To%20ensure%20stable%20training%2C%20we%20inject%20noise%20into%20the%0Alow-level%20policy%20and%20clip%20high-level%20actions%20using%20human%20motion%20statistics.%0AVisualMimic%20enables%20zero-shot%20transfer%20of%20visuomotor%20policies%20trained%20in%0Asimulation%20to%20real%20humanoid%20robots%2C%20accomplishing%20a%20wide%20range%20of%0Aloco-manipulation%20tasks%20such%20as%20box%20lifting%2C%20pushing%2C%20football%20dribbling%2C%20and%0Akicking.%20Beyond%20controlled%20laboratory%20settings%2C%20our%20policies%20also%20generalize%0Arobustly%20to%20outdoor%20environments.%20Videos%20are%20available%20at%3A%0Ahttps%3A//visualmimic.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20322v1&entry.124074799=Read"},
{"title": "Graph Variate Neural Networks", "author": "Om Roy and Yashar Moshfeghi and Keith Smith", "abstract": "  Modelling dynamically evolving spatio-temporal signals is a prominent\nchallenge in the Graph Neural Network (GNN) literature. Notably, GNNs assume an\nexisting underlying graph structure. While this underlying structure may not\nalways exist or is derived independently from the signal, a temporally evolving\nfunctional network can always be constructed from multi-channel data. Graph\nVariate Signal Analysis (GVSA) defines a unified framework consisting of a\nnetwork tensor of instantaneous connectivity profiles against a stable support\nusually constructed from the signal itself. Building on GVSA and tools from\ngraph signal processing, we introduce Graph-Variate Neural Networks (GVNNs):\nlayers that convolve spatio-temporal signals with a signal-dependent\nconnectivity tensor combining a stable long-term support with instantaneous,\ndata-driven interactions. This design captures dynamic statistical\ninterdependencies at each time step without ad hoc sliding windows and admits\nan efficient implementation with linear complexity in sequence length. Across\nforecasting benchmarks, GVNNs consistently outperform strong graph-based\nbaselines and are competitive with widely used sequence models such as LSTMs\nand Transformers. On EEG motor-imagery classification, GVNNs achieve strong\naccuracy highlighting their potential for brain-computer interface\napplications.\n", "link": "http://arxiv.org/abs/2509.20311v1", "date": "2025-09-24", "relevancy": 2.4729, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5109}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4911}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Variate%20Neural%20Networks&body=Title%3A%20Graph%20Variate%20Neural%20Networks%0AAuthor%3A%20Om%20Roy%20and%20Yashar%20Moshfeghi%20and%20Keith%20Smith%0AAbstract%3A%20%20%20Modelling%20dynamically%20evolving%20spatio-temporal%20signals%20is%20a%20prominent%0Achallenge%20in%20the%20Graph%20Neural%20Network%20%28GNN%29%20literature.%20Notably%2C%20GNNs%20assume%20an%0Aexisting%20underlying%20graph%20structure.%20While%20this%20underlying%20structure%20may%20not%0Aalways%20exist%20or%20is%20derived%20independently%20from%20the%20signal%2C%20a%20temporally%20evolving%0Afunctional%20network%20can%20always%20be%20constructed%20from%20multi-channel%20data.%20Graph%0AVariate%20Signal%20Analysis%20%28GVSA%29%20defines%20a%20unified%20framework%20consisting%20of%20a%0Anetwork%20tensor%20of%20instantaneous%20connectivity%20profiles%20against%20a%20stable%20support%0Ausually%20constructed%20from%20the%20signal%20itself.%20Building%20on%20GVSA%20and%20tools%20from%0Agraph%20signal%20processing%2C%20we%20introduce%20Graph-Variate%20Neural%20Networks%20%28GVNNs%29%3A%0Alayers%20that%20convolve%20spatio-temporal%20signals%20with%20a%20signal-dependent%0Aconnectivity%20tensor%20combining%20a%20stable%20long-term%20support%20with%20instantaneous%2C%0Adata-driven%20interactions.%20This%20design%20captures%20dynamic%20statistical%0Ainterdependencies%20at%20each%20time%20step%20without%20ad%20hoc%20sliding%20windows%20and%20admits%0Aan%20efficient%20implementation%20with%20linear%20complexity%20in%20sequence%20length.%20Across%0Aforecasting%20benchmarks%2C%20GVNNs%20consistently%20outperform%20strong%20graph-based%0Abaselines%20and%20are%20competitive%20with%20widely%20used%20sequence%20models%20such%20as%20LSTMs%0Aand%20Transformers.%20On%20EEG%20motor-imagery%20classification%2C%20GVNNs%20achieve%20strong%0Aaccuracy%20highlighting%20their%20potential%20for%20brain-computer%20interface%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Variate%2520Neural%2520Networks%26entry.906535625%3DOm%2520Roy%2520and%2520Yashar%2520Moshfeghi%2520and%2520Keith%2520Smith%26entry.1292438233%3D%2520%2520Modelling%2520dynamically%2520evolving%2520spatio-temporal%2520signals%2520is%2520a%2520prominent%250Achallenge%2520in%2520the%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520literature.%2520Notably%252C%2520GNNs%2520assume%2520an%250Aexisting%2520underlying%2520graph%2520structure.%2520While%2520this%2520underlying%2520structure%2520may%2520not%250Aalways%2520exist%2520or%2520is%2520derived%2520independently%2520from%2520the%2520signal%252C%2520a%2520temporally%2520evolving%250Afunctional%2520network%2520can%2520always%2520be%2520constructed%2520from%2520multi-channel%2520data.%2520Graph%250AVariate%2520Signal%2520Analysis%2520%2528GVSA%2529%2520defines%2520a%2520unified%2520framework%2520consisting%2520of%2520a%250Anetwork%2520tensor%2520of%2520instantaneous%2520connectivity%2520profiles%2520against%2520a%2520stable%2520support%250Ausually%2520constructed%2520from%2520the%2520signal%2520itself.%2520Building%2520on%2520GVSA%2520and%2520tools%2520from%250Agraph%2520signal%2520processing%252C%2520we%2520introduce%2520Graph-Variate%2520Neural%2520Networks%2520%2528GVNNs%2529%253A%250Alayers%2520that%2520convolve%2520spatio-temporal%2520signals%2520with%2520a%2520signal-dependent%250Aconnectivity%2520tensor%2520combining%2520a%2520stable%2520long-term%2520support%2520with%2520instantaneous%252C%250Adata-driven%2520interactions.%2520This%2520design%2520captures%2520dynamic%2520statistical%250Ainterdependencies%2520at%2520each%2520time%2520step%2520without%2520ad%2520hoc%2520sliding%2520windows%2520and%2520admits%250Aan%2520efficient%2520implementation%2520with%2520linear%2520complexity%2520in%2520sequence%2520length.%2520Across%250Aforecasting%2520benchmarks%252C%2520GVNNs%2520consistently%2520outperform%2520strong%2520graph-based%250Abaselines%2520and%2520are%2520competitive%2520with%2520widely%2520used%2520sequence%2520models%2520such%2520as%2520LSTMs%250Aand%2520Transformers.%2520On%2520EEG%2520motor-imagery%2520classification%252C%2520GVNNs%2520achieve%2520strong%250Aaccuracy%2520highlighting%2520their%2520potential%2520for%2520brain-computer%2520interface%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Variate%20Neural%20Networks&entry.906535625=Om%20Roy%20and%20Yashar%20Moshfeghi%20and%20Keith%20Smith&entry.1292438233=%20%20Modelling%20dynamically%20evolving%20spatio-temporal%20signals%20is%20a%20prominent%0Achallenge%20in%20the%20Graph%20Neural%20Network%20%28GNN%29%20literature.%20Notably%2C%20GNNs%20assume%20an%0Aexisting%20underlying%20graph%20structure.%20While%20this%20underlying%20structure%20may%20not%0Aalways%20exist%20or%20is%20derived%20independently%20from%20the%20signal%2C%20a%20temporally%20evolving%0Afunctional%20network%20can%20always%20be%20constructed%20from%20multi-channel%20data.%20Graph%0AVariate%20Signal%20Analysis%20%28GVSA%29%20defines%20a%20unified%20framework%20consisting%20of%20a%0Anetwork%20tensor%20of%20instantaneous%20connectivity%20profiles%20against%20a%20stable%20support%0Ausually%20constructed%20from%20the%20signal%20itself.%20Building%20on%20GVSA%20and%20tools%20from%0Agraph%20signal%20processing%2C%20we%20introduce%20Graph-Variate%20Neural%20Networks%20%28GVNNs%29%3A%0Alayers%20that%20convolve%20spatio-temporal%20signals%20with%20a%20signal-dependent%0Aconnectivity%20tensor%20combining%20a%20stable%20long-term%20support%20with%20instantaneous%2C%0Adata-driven%20interactions.%20This%20design%20captures%20dynamic%20statistical%0Ainterdependencies%20at%20each%20time%20step%20without%20ad%20hoc%20sliding%20windows%20and%20admits%0Aan%20efficient%20implementation%20with%20linear%20complexity%20in%20sequence%20length.%20Across%0Aforecasting%20benchmarks%2C%20GVNNs%20consistently%20outperform%20strong%20graph-based%0Abaselines%20and%20are%20competitive%20with%20widely%20used%20sequence%20models%20such%20as%20LSTMs%0Aand%20Transformers.%20On%20EEG%20motor-imagery%20classification%2C%20GVNNs%20achieve%20strong%0Aaccuracy%20highlighting%20their%20potential%20for%20brain-computer%20interface%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20311v1&entry.124074799=Read"},
{"title": "EmbeddingGemma: Powerful and Lightweight Text Representations", "author": "Henrique Schechter Vera and Sahil Dua and Biao Zhang and Daniel Salz and Ryan Mullins and Sindhu Raghuram Panyam and Sara Smoot and Iftekhar Naim and Joe Zou and Feiyang Chen and Daniel Cer and Alice Lisak and Min Choi and Lucas Gonzalez and Omar Sanseviero and Glenn Cameron and Ian Ballantyne and Kat Black and Kaifeng Chen and Weiyi Wang and Zhe Li and Gus Martins and Jinhyuk Lee and Mark Sherwood and Juyeong Ji and Renjie Wu and Jingxiao Zheng and Jyotinder Singh and Abheesht Sharma and Divya Sreepat and Aashi Jain and Adham Elarabawy and AJ Co and Andreas Doumanoglou and Babak Samari and Ben Hora and Brian Potetz and Dahun Kim and Enrique Alfonseca and Fedor Moiseev and Feng Han and Frank Palma Gomez and Gustavo Hern\u00e1ndez \u00c1brego and Hesen Zhang and Hui Hui and Jay Han and Karan Gill and Ke Chen and Koert Chen and Madhuri Shanbhogue and Michael Boratko and Paul Suganthan and Sai Meher Karthik Duddu and Sandeep Mariserla and Setareh Ariafar and Shanfeng Zhang and Shijie Zhang and Simon Baumgartner and Sonam Goenka and Steve Qiu and Tanmaya Dabral and Trevor Walker and Vikram Rao and Waleed Khawaja and Wenlei Zhou and Xiaoqi Ren and Ye Xia and Yichang Chen and Yi-Ting Chen and Zhe Dong and Zhongli Ding and Francesco Visin and Ga\u00ebl Liu and Jiageng Zhang and Kathleen Kenealy and Michelle Casbon and Ravin Kumar and Thomas Mesnard and Zach Gleicher and Cormac Brick and Olivier Lacombe and Adam Roberts and Yunhsuan Sung and Raphael Hoffmann and Tris Warkentin and Armand Joulin and Tom Duerig and Mojtaba Seyedhosseini", "abstract": "  We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.\n", "link": "http://arxiv.org/abs/2509.20354v1", "date": "2025-09-24", "relevancy": 2.4339, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.512}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4829}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmbeddingGemma%3A%20Powerful%20and%20Lightweight%20Text%20Representations&body=Title%3A%20EmbeddingGemma%3A%20Powerful%20and%20Lightweight%20Text%20Representations%0AAuthor%3A%20Henrique%20Schechter%20Vera%20and%20Sahil%20Dua%20and%20Biao%20Zhang%20and%20Daniel%20Salz%20and%20Ryan%20Mullins%20and%20Sindhu%20Raghuram%20Panyam%20and%20Sara%20Smoot%20and%20Iftekhar%20Naim%20and%20Joe%20Zou%20and%20Feiyang%20Chen%20and%20Daniel%20Cer%20and%20Alice%20Lisak%20and%20Min%20Choi%20and%20Lucas%20Gonzalez%20and%20Omar%20Sanseviero%20and%20Glenn%20Cameron%20and%20Ian%20Ballantyne%20and%20Kat%20Black%20and%20Kaifeng%20Chen%20and%20Weiyi%20Wang%20and%20Zhe%20Li%20and%20Gus%20Martins%20and%20Jinhyuk%20Lee%20and%20Mark%20Sherwood%20and%20Juyeong%20Ji%20and%20Renjie%20Wu%20and%20Jingxiao%20Zheng%20and%20Jyotinder%20Singh%20and%20Abheesht%20Sharma%20and%20Divya%20Sreepat%20and%20Aashi%20Jain%20and%20Adham%20Elarabawy%20and%20AJ%20Co%20and%20Andreas%20Doumanoglou%20and%20Babak%20Samari%20and%20Ben%20Hora%20and%20Brian%20Potetz%20and%20Dahun%20Kim%20and%20Enrique%20Alfonseca%20and%20Fedor%20Moiseev%20and%20Feng%20Han%20and%20Frank%20Palma%20Gomez%20and%20Gustavo%20Hern%C3%A1ndez%20%C3%81brego%20and%20Hesen%20Zhang%20and%20Hui%20Hui%20and%20Jay%20Han%20and%20Karan%20Gill%20and%20Ke%20Chen%20and%20Koert%20Chen%20and%20Madhuri%20Shanbhogue%20and%20Michael%20Boratko%20and%20Paul%20Suganthan%20and%20Sai%20Meher%20Karthik%20Duddu%20and%20Sandeep%20Mariserla%20and%20Setareh%20Ariafar%20and%20Shanfeng%20Zhang%20and%20Shijie%20Zhang%20and%20Simon%20Baumgartner%20and%20Sonam%20Goenka%20and%20Steve%20Qiu%20and%20Tanmaya%20Dabral%20and%20Trevor%20Walker%20and%20Vikram%20Rao%20and%20Waleed%20Khawaja%20and%20Wenlei%20Zhou%20and%20Xiaoqi%20Ren%20and%20Ye%20Xia%20and%20Yichang%20Chen%20and%20Yi-Ting%20Chen%20and%20Zhe%20Dong%20and%20Zhongli%20Ding%20and%20Francesco%20Visin%20and%20Ga%C3%ABl%20Liu%20and%20Jiageng%20Zhang%20and%20Kathleen%20Kenealy%20and%20Michelle%20Casbon%20and%20Ravin%20Kumar%20and%20Thomas%20Mesnard%20and%20Zach%20Gleicher%20and%20Cormac%20Brick%20and%20Olivier%20Lacombe%20and%20Adam%20Roberts%20and%20Yunhsuan%20Sung%20and%20Raphael%20Hoffmann%20and%20Tris%20Warkentin%20and%20Armand%20Joulin%20and%20Tom%20Duerig%20and%20Mojtaba%20Seyedhosseini%0AAbstract%3A%20%20%20We%20introduce%20EmbeddingGemma%2C%20a%20new%20lightweight%2C%20open%20text%20embedding%20model%0Abased%20on%20the%20Gemma%203%20language%20model%20family.%20Our%20innovative%20training%20recipe%0Astrategically%20captures%20knowledge%20from%20larger%20models%20via%20encoder-decoder%0Ainitialization%20and%20geometric%20embedding%20distillation.%20We%20improve%20model%0Arobustness%20and%20expressiveness%20with%20a%20spread-out%20regularizer%2C%20and%20ensure%0Ageneralizability%20by%20merging%20checkpoints%20from%20varied%2C%20optimized%20mixtures.%0AEvaluated%20on%20the%20Massive%20Text%20Embedding%20Benchmark%20%28MTEB%29%20across%20multilingual%2C%0AEnglish%2C%20and%20code%20domains%2C%20EmbeddingGemma%20%28300M%29%20achieves%20state-of-the-art%0Aresults.%20Notably%2C%20it%20outperforms%20prior%20top%20models%2C%20both%20proprietary%20and%20open%2C%0Awith%20fewer%20than%20500M%20parameters%2C%20and%20provides%20performance%20comparable%20to%20models%0Adouble%20its%20size%2C%20offering%20an%20exceptional%20performance-to-cost%20ratio.%20Remarkably%2C%0Athis%20lead%20persists%20when%20quantizing%20model%20weights%20or%20truncating%20embedding%0Aoutputs.%20This%20makes%20EmbeddingGemma%20particularly%20well-suited%20for%20low-latency%20and%0Ahigh-throughput%20use%20cases%20such%20as%20on-device%20applications.%20We%20provide%20ablation%0Astudies%20exploring%20our%20key%20design%20choices.%20We%20release%20EmbeddingGemma%20to%20the%0Acommunity%20to%20promote%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbeddingGemma%253A%2520Powerful%2520and%2520Lightweight%2520Text%2520Representations%26entry.906535625%3DHenrique%2520Schechter%2520Vera%2520and%2520Sahil%2520Dua%2520and%2520Biao%2520Zhang%2520and%2520Daniel%2520Salz%2520and%2520Ryan%2520Mullins%2520and%2520Sindhu%2520Raghuram%2520Panyam%2520and%2520Sara%2520Smoot%2520and%2520Iftekhar%2520Naim%2520and%2520Joe%2520Zou%2520and%2520Feiyang%2520Chen%2520and%2520Daniel%2520Cer%2520and%2520Alice%2520Lisak%2520and%2520Min%2520Choi%2520and%2520Lucas%2520Gonzalez%2520and%2520Omar%2520Sanseviero%2520and%2520Glenn%2520Cameron%2520and%2520Ian%2520Ballantyne%2520and%2520Kat%2520Black%2520and%2520Kaifeng%2520Chen%2520and%2520Weiyi%2520Wang%2520and%2520Zhe%2520Li%2520and%2520Gus%2520Martins%2520and%2520Jinhyuk%2520Lee%2520and%2520Mark%2520Sherwood%2520and%2520Juyeong%2520Ji%2520and%2520Renjie%2520Wu%2520and%2520Jingxiao%2520Zheng%2520and%2520Jyotinder%2520Singh%2520and%2520Abheesht%2520Sharma%2520and%2520Divya%2520Sreepat%2520and%2520Aashi%2520Jain%2520and%2520Adham%2520Elarabawy%2520and%2520AJ%2520Co%2520and%2520Andreas%2520Doumanoglou%2520and%2520Babak%2520Samari%2520and%2520Ben%2520Hora%2520and%2520Brian%2520Potetz%2520and%2520Dahun%2520Kim%2520and%2520Enrique%2520Alfonseca%2520and%2520Fedor%2520Moiseev%2520and%2520Feng%2520Han%2520and%2520Frank%2520Palma%2520Gomez%2520and%2520Gustavo%2520Hern%25C3%25A1ndez%2520%25C3%2581brego%2520and%2520Hesen%2520Zhang%2520and%2520Hui%2520Hui%2520and%2520Jay%2520Han%2520and%2520Karan%2520Gill%2520and%2520Ke%2520Chen%2520and%2520Koert%2520Chen%2520and%2520Madhuri%2520Shanbhogue%2520and%2520Michael%2520Boratko%2520and%2520Paul%2520Suganthan%2520and%2520Sai%2520Meher%2520Karthik%2520Duddu%2520and%2520Sandeep%2520Mariserla%2520and%2520Setareh%2520Ariafar%2520and%2520Shanfeng%2520Zhang%2520and%2520Shijie%2520Zhang%2520and%2520Simon%2520Baumgartner%2520and%2520Sonam%2520Goenka%2520and%2520Steve%2520Qiu%2520and%2520Tanmaya%2520Dabral%2520and%2520Trevor%2520Walker%2520and%2520Vikram%2520Rao%2520and%2520Waleed%2520Khawaja%2520and%2520Wenlei%2520Zhou%2520and%2520Xiaoqi%2520Ren%2520and%2520Ye%2520Xia%2520and%2520Yichang%2520Chen%2520and%2520Yi-Ting%2520Chen%2520and%2520Zhe%2520Dong%2520and%2520Zhongli%2520Ding%2520and%2520Francesco%2520Visin%2520and%2520Ga%25C3%25ABl%2520Liu%2520and%2520Jiageng%2520Zhang%2520and%2520Kathleen%2520Kenealy%2520and%2520Michelle%2520Casbon%2520and%2520Ravin%2520Kumar%2520and%2520Thomas%2520Mesnard%2520and%2520Zach%2520Gleicher%2520and%2520Cormac%2520Brick%2520and%2520Olivier%2520Lacombe%2520and%2520Adam%2520Roberts%2520and%2520Yunhsuan%2520Sung%2520and%2520Raphael%2520Hoffmann%2520and%2520Tris%2520Warkentin%2520and%2520Armand%2520Joulin%2520and%2520Tom%2520Duerig%2520and%2520Mojtaba%2520Seyedhosseini%26entry.1292438233%3D%2520%2520We%2520introduce%2520EmbeddingGemma%252C%2520a%2520new%2520lightweight%252C%2520open%2520text%2520embedding%2520model%250Abased%2520on%2520the%2520Gemma%25203%2520language%2520model%2520family.%2520Our%2520innovative%2520training%2520recipe%250Astrategically%2520captures%2520knowledge%2520from%2520larger%2520models%2520via%2520encoder-decoder%250Ainitialization%2520and%2520geometric%2520embedding%2520distillation.%2520We%2520improve%2520model%250Arobustness%2520and%2520expressiveness%2520with%2520a%2520spread-out%2520regularizer%252C%2520and%2520ensure%250Ageneralizability%2520by%2520merging%2520checkpoints%2520from%2520varied%252C%2520optimized%2520mixtures.%250AEvaluated%2520on%2520the%2520Massive%2520Text%2520Embedding%2520Benchmark%2520%2528MTEB%2529%2520across%2520multilingual%252C%250AEnglish%252C%2520and%2520code%2520domains%252C%2520EmbeddingGemma%2520%2528300M%2529%2520achieves%2520state-of-the-art%250Aresults.%2520Notably%252C%2520it%2520outperforms%2520prior%2520top%2520models%252C%2520both%2520proprietary%2520and%2520open%252C%250Awith%2520fewer%2520than%2520500M%2520parameters%252C%2520and%2520provides%2520performance%2520comparable%2520to%2520models%250Adouble%2520its%2520size%252C%2520offering%2520an%2520exceptional%2520performance-to-cost%2520ratio.%2520Remarkably%252C%250Athis%2520lead%2520persists%2520when%2520quantizing%2520model%2520weights%2520or%2520truncating%2520embedding%250Aoutputs.%2520This%2520makes%2520EmbeddingGemma%2520particularly%2520well-suited%2520for%2520low-latency%2520and%250Ahigh-throughput%2520use%2520cases%2520such%2520as%2520on-device%2520applications.%2520We%2520provide%2520ablation%250Astudies%2520exploring%2520our%2520key%2520design%2520choices.%2520We%2520release%2520EmbeddingGemma%2520to%2520the%250Acommunity%2520to%2520promote%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmbeddingGemma%3A%20Powerful%20and%20Lightweight%20Text%20Representations&entry.906535625=Henrique%20Schechter%20Vera%20and%20Sahil%20Dua%20and%20Biao%20Zhang%20and%20Daniel%20Salz%20and%20Ryan%20Mullins%20and%20Sindhu%20Raghuram%20Panyam%20and%20Sara%20Smoot%20and%20Iftekhar%20Naim%20and%20Joe%20Zou%20and%20Feiyang%20Chen%20and%20Daniel%20Cer%20and%20Alice%20Lisak%20and%20Min%20Choi%20and%20Lucas%20Gonzalez%20and%20Omar%20Sanseviero%20and%20Glenn%20Cameron%20and%20Ian%20Ballantyne%20and%20Kat%20Black%20and%20Kaifeng%20Chen%20and%20Weiyi%20Wang%20and%20Zhe%20Li%20and%20Gus%20Martins%20and%20Jinhyuk%20Lee%20and%20Mark%20Sherwood%20and%20Juyeong%20Ji%20and%20Renjie%20Wu%20and%20Jingxiao%20Zheng%20and%20Jyotinder%20Singh%20and%20Abheesht%20Sharma%20and%20Divya%20Sreepat%20and%20Aashi%20Jain%20and%20Adham%20Elarabawy%20and%20AJ%20Co%20and%20Andreas%20Doumanoglou%20and%20Babak%20Samari%20and%20Ben%20Hora%20and%20Brian%20Potetz%20and%20Dahun%20Kim%20and%20Enrique%20Alfonseca%20and%20Fedor%20Moiseev%20and%20Feng%20Han%20and%20Frank%20Palma%20Gomez%20and%20Gustavo%20Hern%C3%A1ndez%20%C3%81brego%20and%20Hesen%20Zhang%20and%20Hui%20Hui%20and%20Jay%20Han%20and%20Karan%20Gill%20and%20Ke%20Chen%20and%20Koert%20Chen%20and%20Madhuri%20Shanbhogue%20and%20Michael%20Boratko%20and%20Paul%20Suganthan%20and%20Sai%20Meher%20Karthik%20Duddu%20and%20Sandeep%20Mariserla%20and%20Setareh%20Ariafar%20and%20Shanfeng%20Zhang%20and%20Shijie%20Zhang%20and%20Simon%20Baumgartner%20and%20Sonam%20Goenka%20and%20Steve%20Qiu%20and%20Tanmaya%20Dabral%20and%20Trevor%20Walker%20and%20Vikram%20Rao%20and%20Waleed%20Khawaja%20and%20Wenlei%20Zhou%20and%20Xiaoqi%20Ren%20and%20Ye%20Xia%20and%20Yichang%20Chen%20and%20Yi-Ting%20Chen%20and%20Zhe%20Dong%20and%20Zhongli%20Ding%20and%20Francesco%20Visin%20and%20Ga%C3%ABl%20Liu%20and%20Jiageng%20Zhang%20and%20Kathleen%20Kenealy%20and%20Michelle%20Casbon%20and%20Ravin%20Kumar%20and%20Thomas%20Mesnard%20and%20Zach%20Gleicher%20and%20Cormac%20Brick%20and%20Olivier%20Lacombe%20and%20Adam%20Roberts%20and%20Yunhsuan%20Sung%20and%20Raphael%20Hoffmann%20and%20Tris%20Warkentin%20and%20Armand%20Joulin%20and%20Tom%20Duerig%20and%20Mojtaba%20Seyedhosseini&entry.1292438233=%20%20We%20introduce%20EmbeddingGemma%2C%20a%20new%20lightweight%2C%20open%20text%20embedding%20model%0Abased%20on%20the%20Gemma%203%20language%20model%20family.%20Our%20innovative%20training%20recipe%0Astrategically%20captures%20knowledge%20from%20larger%20models%20via%20encoder-decoder%0Ainitialization%20and%20geometric%20embedding%20distillation.%20We%20improve%20model%0Arobustness%20and%20expressiveness%20with%20a%20spread-out%20regularizer%2C%20and%20ensure%0Ageneralizability%20by%20merging%20checkpoints%20from%20varied%2C%20optimized%20mixtures.%0AEvaluated%20on%20the%20Massive%20Text%20Embedding%20Benchmark%20%28MTEB%29%20across%20multilingual%2C%0AEnglish%2C%20and%20code%20domains%2C%20EmbeddingGemma%20%28300M%29%20achieves%20state-of-the-art%0Aresults.%20Notably%2C%20it%20outperforms%20prior%20top%20models%2C%20both%20proprietary%20and%20open%2C%0Awith%20fewer%20than%20500M%20parameters%2C%20and%20provides%20performance%20comparable%20to%20models%0Adouble%20its%20size%2C%20offering%20an%20exceptional%20performance-to-cost%20ratio.%20Remarkably%2C%0Athis%20lead%20persists%20when%20quantizing%20model%20weights%20or%20truncating%20embedding%0Aoutputs.%20This%20makes%20EmbeddingGemma%20particularly%20well-suited%20for%20low-latency%20and%0Ahigh-throughput%20use%20cases%20such%20as%20on-device%20applications.%20We%20provide%20ablation%0Astudies%20exploring%20our%20key%20design%20choices.%20We%20release%20EmbeddingGemma%20to%20the%0Acommunity%20to%20promote%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20354v1&entry.124074799=Read"},
{"title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware\n  Reinforcement Learning on Imbalanced Data", "author": "Yuhang Zhou and Jing Zhu and Shengyi Qian and Zhuokai Zhao and Xiyao Wang and Xiaoyu Liu and Ming Li and Paiheng Xu and Wei Ai and Furong Huang", "abstract": "  Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups, assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks. Our code and data are available at\nhttps://github.com/Tonyzhou98/disco_grpo.\n", "link": "http://arxiv.org/abs/2505.15074v3", "date": "2025-09-24", "relevancy": 2.4294, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4979}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.486}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DISCO%20Balances%20the%20Scales%3A%20Adaptive%20Domain-%20and%20Difficulty-Aware%0A%20%20Reinforcement%20Learning%20on%20Imbalanced%20Data&body=Title%3A%20DISCO%20Balances%20the%20Scales%3A%20Adaptive%20Domain-%20and%20Difficulty-Aware%0A%20%20Reinforcement%20Learning%20on%20Imbalanced%20Data%0AAuthor%3A%20Yuhang%20Zhou%20and%20Jing%20Zhu%20and%20Shengyi%20Qian%20and%20Zhuokai%20Zhao%20and%20Xiyao%20Wang%20and%20Xiaoyu%20Liu%20and%20Ming%20Li%20and%20Paiheng%20Xu%20and%20Wei%20Ai%20and%20Furong%20Huang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20aligned%20with%20human%20preferences%0Athrough%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29.%20Among%20RLHF%20methods%2C%0AGroup%20Relative%20Policy%20Optimization%20%28GRPO%29%20has%20gained%20attention%20for%20its%0Asimplicity%20and%20strong%20performance%2C%20notably%20eliminating%20the%20need%20for%20a%20learned%0Avalue%20function.%20However%2C%20GRPO%20implicitly%20assumes%20a%20balanced%20domain%20distribution%0Aand%20uniform%20semantic%20alignment%20across%20groups%2C%20assumptions%20that%20rarely%20hold%20in%0Areal-world%20datasets.%20When%20applied%20to%20multi-domain%2C%20imbalanced%20data%2C%20GRPO%0Adisproportionately%20optimizes%20for%20dominant%20domains%2C%20neglecting%20underrepresented%0Aones%20and%20resulting%20in%20poor%20generalization%20and%20fairness.%20We%20propose%0ADomain-Informed%20Self-Consistency%20Policy%20Optimization%20%28DISCO%29%2C%20a%20principled%0Aextension%20to%20GRPO%20that%20addresses%20inter-group%20imbalance%20with%20two%20key%0Ainnovations.%20Domain-aware%20reward%20scaling%20counteracts%20frequency%20bias%20by%0Areweighting%20optimization%20based%20on%20domain%20prevalence.%20Difficulty-aware%20reward%0Ascaling%20leverages%20prompt-level%20self-consistency%20to%20identify%20and%20prioritize%0Auncertain%20prompts%20that%20offer%20greater%20learning%20value.%20Together%2C%20these%20strategies%0Apromote%20more%20equitable%20and%20effective%20policy%20learning%20across%20domains.%20Extensive%0Aexperiments%20across%20multiple%20LLMs%20and%20skewed%20training%20distributions%20show%20that%0ADISCO%20improves%20generalization%2C%20outperforms%20existing%20GRPO%20variants%20by%205%25%20on%0AQwen3%20models%2C%20and%20sets%20new%20state-of-the-art%20results%20on%20multi-domain%20alignment%0Abenchmarks.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Tonyzhou98/disco_grpo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15074v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDISCO%2520Balances%2520the%2520Scales%253A%2520Adaptive%2520Domain-%2520and%2520Difficulty-Aware%250A%2520%2520Reinforcement%2520Learning%2520on%2520Imbalanced%2520Data%26entry.906535625%3DYuhang%2520Zhou%2520and%2520Jing%2520Zhu%2520and%2520Shengyi%2520Qian%2520and%2520Zhuokai%2520Zhao%2520and%2520Xiyao%2520Wang%2520and%2520Xiaoyu%2520Liu%2520and%2520Ming%2520Li%2520and%2520Paiheng%2520Xu%2520and%2520Wei%2520Ai%2520and%2520Furong%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520aligned%2520with%2520human%2520preferences%250Athrough%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529.%2520Among%2520RLHF%2520methods%252C%250AGroup%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520has%2520gained%2520attention%2520for%2520its%250Asimplicity%2520and%2520strong%2520performance%252C%2520notably%2520eliminating%2520the%2520need%2520for%2520a%2520learned%250Avalue%2520function.%2520However%252C%2520GRPO%2520implicitly%2520assumes%2520a%2520balanced%2520domain%2520distribution%250Aand%2520uniform%2520semantic%2520alignment%2520across%2520groups%252C%2520assumptions%2520that%2520rarely%2520hold%2520in%250Areal-world%2520datasets.%2520When%2520applied%2520to%2520multi-domain%252C%2520imbalanced%2520data%252C%2520GRPO%250Adisproportionately%2520optimizes%2520for%2520dominant%2520domains%252C%2520neglecting%2520underrepresented%250Aones%2520and%2520resulting%2520in%2520poor%2520generalization%2520and%2520fairness.%2520We%2520propose%250ADomain-Informed%2520Self-Consistency%2520Policy%2520Optimization%2520%2528DISCO%2529%252C%2520a%2520principled%250Aextension%2520to%2520GRPO%2520that%2520addresses%2520inter-group%2520imbalance%2520with%2520two%2520key%250Ainnovations.%2520Domain-aware%2520reward%2520scaling%2520counteracts%2520frequency%2520bias%2520by%250Areweighting%2520optimization%2520based%2520on%2520domain%2520prevalence.%2520Difficulty-aware%2520reward%250Ascaling%2520leverages%2520prompt-level%2520self-consistency%2520to%2520identify%2520and%2520prioritize%250Auncertain%2520prompts%2520that%2520offer%2520greater%2520learning%2520value.%2520Together%252C%2520these%2520strategies%250Apromote%2520more%2520equitable%2520and%2520effective%2520policy%2520learning%2520across%2520domains.%2520Extensive%250Aexperiments%2520across%2520multiple%2520LLMs%2520and%2520skewed%2520training%2520distributions%2520show%2520that%250ADISCO%2520improves%2520generalization%252C%2520outperforms%2520existing%2520GRPO%2520variants%2520by%25205%2525%2520on%250AQwen3%2520models%252C%2520and%2520sets%2520new%2520state-of-the-art%2520results%2520on%2520multi-domain%2520alignment%250Abenchmarks.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/Tonyzhou98/disco_grpo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15074v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DISCO%20Balances%20the%20Scales%3A%20Adaptive%20Domain-%20and%20Difficulty-Aware%0A%20%20Reinforcement%20Learning%20on%20Imbalanced%20Data&entry.906535625=Yuhang%20Zhou%20and%20Jing%20Zhu%20and%20Shengyi%20Qian%20and%20Zhuokai%20Zhao%20and%20Xiyao%20Wang%20and%20Xiaoyu%20Liu%20and%20Ming%20Li%20and%20Paiheng%20Xu%20and%20Wei%20Ai%20and%20Furong%20Huang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20aligned%20with%20human%20preferences%0Athrough%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29.%20Among%20RLHF%20methods%2C%0AGroup%20Relative%20Policy%20Optimization%20%28GRPO%29%20has%20gained%20attention%20for%20its%0Asimplicity%20and%20strong%20performance%2C%20notably%20eliminating%20the%20need%20for%20a%20learned%0Avalue%20function.%20However%2C%20GRPO%20implicitly%20assumes%20a%20balanced%20domain%20distribution%0Aand%20uniform%20semantic%20alignment%20across%20groups%2C%20assumptions%20that%20rarely%20hold%20in%0Areal-world%20datasets.%20When%20applied%20to%20multi-domain%2C%20imbalanced%20data%2C%20GRPO%0Adisproportionately%20optimizes%20for%20dominant%20domains%2C%20neglecting%20underrepresented%0Aones%20and%20resulting%20in%20poor%20generalization%20and%20fairness.%20We%20propose%0ADomain-Informed%20Self-Consistency%20Policy%20Optimization%20%28DISCO%29%2C%20a%20principled%0Aextension%20to%20GRPO%20that%20addresses%20inter-group%20imbalance%20with%20two%20key%0Ainnovations.%20Domain-aware%20reward%20scaling%20counteracts%20frequency%20bias%20by%0Areweighting%20optimization%20based%20on%20domain%20prevalence.%20Difficulty-aware%20reward%0Ascaling%20leverages%20prompt-level%20self-consistency%20to%20identify%20and%20prioritize%0Auncertain%20prompts%20that%20offer%20greater%20learning%20value.%20Together%2C%20these%20strategies%0Apromote%20more%20equitable%20and%20effective%20policy%20learning%20across%20domains.%20Extensive%0Aexperiments%20across%20multiple%20LLMs%20and%20skewed%20training%20distributions%20show%20that%0ADISCO%20improves%20generalization%2C%20outperforms%20existing%20GRPO%20variants%20by%205%25%20on%0AQwen3%20models%2C%20and%20sets%20new%20state-of-the-art%20results%20on%20multi-domain%20alignment%0Abenchmarks.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Tonyzhou98/disco_grpo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15074v3&entry.124074799=Read"},
{"title": "EditVerse: Unifying Image and Video Editing and Generation with\n  In-Context Learning", "author": "Xuan Ju and Tianyu Wang and Yuqian Zhou and He Zhang and Qing Liu and Nanxuan Zhao and Zhifei Zhang and Yijun Li and Yuanhao Cai and Shaoteng Liu and Daniil Pakhomov and Zhe Lin and Soo Ye Kim and Qiang Xu", "abstract": "  Recent advances in foundation models highlight a clear trend toward\nunification and scaling, showing emergent capabilities across diverse domains.\nWhile image generation and editing have rapidly transitioned from task-specific\nto unified frameworks, video generation and editing remain fragmented due to\narchitectural limitations and data scarcity. In this work, we introduce\nEditVerse, a unified framework for image and video generation and editing\nwithin a single model. By representing all modalities, i.e., text, image, and\nvideo, as a unified token sequence, EditVerse leverages self-attention to\nachieve robust in-context learning, natural cross-modal knowledge transfer, and\nflexible handling of inputs and outputs with arbitrary resolutions and\ndurations. To address the lack of video editing training data, we design a\nscalable data pipeline that curates 232K video editing samples and combines\nthem with large-scale image and video datasets for joint training. Furthermore,\nwe present EditVerseBench, the first benchmark for instruction-based video\nediting covering diverse tasks and resolutions. Extensive experiments and user\nstudies demonstrate that EditVerse achieves state-of-the-art performance,\nsurpassing existing open-source and commercial models, while exhibiting\nemergent editing and generation abilities across modalities.\n", "link": "http://arxiv.org/abs/2509.20360v1", "date": "2025-09-24", "relevancy": 2.3492, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5891}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EditVerse%3A%20Unifying%20Image%20and%20Video%20Editing%20and%20Generation%20with%0A%20%20In-Context%20Learning&body=Title%3A%20EditVerse%3A%20Unifying%20Image%20and%20Video%20Editing%20and%20Generation%20with%0A%20%20In-Context%20Learning%0AAuthor%3A%20Xuan%20Ju%20and%20Tianyu%20Wang%20and%20Yuqian%20Zhou%20and%20He%20Zhang%20and%20Qing%20Liu%20and%20Nanxuan%20Zhao%20and%20Zhifei%20Zhang%20and%20Yijun%20Li%20and%20Yuanhao%20Cai%20and%20Shaoteng%20Liu%20and%20Daniil%20Pakhomov%20and%20Zhe%20Lin%20and%20Soo%20Ye%20Kim%20and%20Qiang%20Xu%0AAbstract%3A%20%20%20Recent%20advances%20in%20foundation%20models%20highlight%20a%20clear%20trend%20toward%0Aunification%20and%20scaling%2C%20showing%20emergent%20capabilities%20across%20diverse%20domains.%0AWhile%20image%20generation%20and%20editing%20have%20rapidly%20transitioned%20from%20task-specific%0Ato%20unified%20frameworks%2C%20video%20generation%20and%20editing%20remain%20fragmented%20due%20to%0Aarchitectural%20limitations%20and%20data%20scarcity.%20In%20this%20work%2C%20we%20introduce%0AEditVerse%2C%20a%20unified%20framework%20for%20image%20and%20video%20generation%20and%20editing%0Awithin%20a%20single%20model.%20By%20representing%20all%20modalities%2C%20i.e.%2C%20text%2C%20image%2C%20and%0Avideo%2C%20as%20a%20unified%20token%20sequence%2C%20EditVerse%20leverages%20self-attention%20to%0Aachieve%20robust%20in-context%20learning%2C%20natural%20cross-modal%20knowledge%20transfer%2C%20and%0Aflexible%20handling%20of%20inputs%20and%20outputs%20with%20arbitrary%20resolutions%20and%0Adurations.%20To%20address%20the%20lack%20of%20video%20editing%20training%20data%2C%20we%20design%20a%0Ascalable%20data%20pipeline%20that%20curates%20232K%20video%20editing%20samples%20and%20combines%0Athem%20with%20large-scale%20image%20and%20video%20datasets%20for%20joint%20training.%20Furthermore%2C%0Awe%20present%20EditVerseBench%2C%20the%20first%20benchmark%20for%20instruction-based%20video%0Aediting%20covering%20diverse%20tasks%20and%20resolutions.%20Extensive%20experiments%20and%20user%0Astudies%20demonstrate%20that%20EditVerse%20achieves%20state-of-the-art%20performance%2C%0Asurpassing%20existing%20open-source%20and%20commercial%20models%2C%20while%20exhibiting%0Aemergent%20editing%20and%20generation%20abilities%20across%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEditVerse%253A%2520Unifying%2520Image%2520and%2520Video%2520Editing%2520and%2520Generation%2520with%250A%2520%2520In-Context%2520Learning%26entry.906535625%3DXuan%2520Ju%2520and%2520Tianyu%2520Wang%2520and%2520Yuqian%2520Zhou%2520and%2520He%2520Zhang%2520and%2520Qing%2520Liu%2520and%2520Nanxuan%2520Zhao%2520and%2520Zhifei%2520Zhang%2520and%2520Yijun%2520Li%2520and%2520Yuanhao%2520Cai%2520and%2520Shaoteng%2520Liu%2520and%2520Daniil%2520Pakhomov%2520and%2520Zhe%2520Lin%2520and%2520Soo%2520Ye%2520Kim%2520and%2520Qiang%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520foundation%2520models%2520highlight%2520a%2520clear%2520trend%2520toward%250Aunification%2520and%2520scaling%252C%2520showing%2520emergent%2520capabilities%2520across%2520diverse%2520domains.%250AWhile%2520image%2520generation%2520and%2520editing%2520have%2520rapidly%2520transitioned%2520from%2520task-specific%250Ato%2520unified%2520frameworks%252C%2520video%2520generation%2520and%2520editing%2520remain%2520fragmented%2520due%2520to%250Aarchitectural%2520limitations%2520and%2520data%2520scarcity.%2520In%2520this%2520work%252C%2520we%2520introduce%250AEditVerse%252C%2520a%2520unified%2520framework%2520for%2520image%2520and%2520video%2520generation%2520and%2520editing%250Awithin%2520a%2520single%2520model.%2520By%2520representing%2520all%2520modalities%252C%2520i.e.%252C%2520text%252C%2520image%252C%2520and%250Avideo%252C%2520as%2520a%2520unified%2520token%2520sequence%252C%2520EditVerse%2520leverages%2520self-attention%2520to%250Aachieve%2520robust%2520in-context%2520learning%252C%2520natural%2520cross-modal%2520knowledge%2520transfer%252C%2520and%250Aflexible%2520handling%2520of%2520inputs%2520and%2520outputs%2520with%2520arbitrary%2520resolutions%2520and%250Adurations.%2520To%2520address%2520the%2520lack%2520of%2520video%2520editing%2520training%2520data%252C%2520we%2520design%2520a%250Ascalable%2520data%2520pipeline%2520that%2520curates%2520232K%2520video%2520editing%2520samples%2520and%2520combines%250Athem%2520with%2520large-scale%2520image%2520and%2520video%2520datasets%2520for%2520joint%2520training.%2520Furthermore%252C%250Awe%2520present%2520EditVerseBench%252C%2520the%2520first%2520benchmark%2520for%2520instruction-based%2520video%250Aediting%2520covering%2520diverse%2520tasks%2520and%2520resolutions.%2520Extensive%2520experiments%2520and%2520user%250Astudies%2520demonstrate%2520that%2520EditVerse%2520achieves%2520state-of-the-art%2520performance%252C%250Asurpassing%2520existing%2520open-source%2520and%2520commercial%2520models%252C%2520while%2520exhibiting%250Aemergent%2520editing%2520and%2520generation%2520abilities%2520across%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EditVerse%3A%20Unifying%20Image%20and%20Video%20Editing%20and%20Generation%20with%0A%20%20In-Context%20Learning&entry.906535625=Xuan%20Ju%20and%20Tianyu%20Wang%20and%20Yuqian%20Zhou%20and%20He%20Zhang%20and%20Qing%20Liu%20and%20Nanxuan%20Zhao%20and%20Zhifei%20Zhang%20and%20Yijun%20Li%20and%20Yuanhao%20Cai%20and%20Shaoteng%20Liu%20and%20Daniil%20Pakhomov%20and%20Zhe%20Lin%20and%20Soo%20Ye%20Kim%20and%20Qiang%20Xu&entry.1292438233=%20%20Recent%20advances%20in%20foundation%20models%20highlight%20a%20clear%20trend%20toward%0Aunification%20and%20scaling%2C%20showing%20emergent%20capabilities%20across%20diverse%20domains.%0AWhile%20image%20generation%20and%20editing%20have%20rapidly%20transitioned%20from%20task-specific%0Ato%20unified%20frameworks%2C%20video%20generation%20and%20editing%20remain%20fragmented%20due%20to%0Aarchitectural%20limitations%20and%20data%20scarcity.%20In%20this%20work%2C%20we%20introduce%0AEditVerse%2C%20a%20unified%20framework%20for%20image%20and%20video%20generation%20and%20editing%0Awithin%20a%20single%20model.%20By%20representing%20all%20modalities%2C%20i.e.%2C%20text%2C%20image%2C%20and%0Avideo%2C%20as%20a%20unified%20token%20sequence%2C%20EditVerse%20leverages%20self-attention%20to%0Aachieve%20robust%20in-context%20learning%2C%20natural%20cross-modal%20knowledge%20transfer%2C%20and%0Aflexible%20handling%20of%20inputs%20and%20outputs%20with%20arbitrary%20resolutions%20and%0Adurations.%20To%20address%20the%20lack%20of%20video%20editing%20training%20data%2C%20we%20design%20a%0Ascalable%20data%20pipeline%20that%20curates%20232K%20video%20editing%20samples%20and%20combines%0Athem%20with%20large-scale%20image%20and%20video%20datasets%20for%20joint%20training.%20Furthermore%2C%0Awe%20present%20EditVerseBench%2C%20the%20first%20benchmark%20for%20instruction-based%20video%0Aediting%20covering%20diverse%20tasks%20and%20resolutions.%20Extensive%20experiments%20and%20user%0Astudies%20demonstrate%20that%20EditVerse%20achieves%20state-of-the-art%20performance%2C%0Asurpassing%20existing%20open-source%20and%20commercial%20models%2C%20while%20exhibiting%0Aemergent%20editing%20and%20generation%20abilities%20across%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20360v1&entry.124074799=Read"},
{"title": "Multimodal Reference Visual Grounding", "author": "Yangxiao Lu and Ruosen Li and Liqiang Jing and Jikai Wang and Xinya Du and Yunhui Guo and Nicholas Ruozzi and Yu Xiang", "abstract": "  Visual grounding focuses on detecting objects from images based on language\nexpressions. Recent Large Vision-Language Models (LVLMs) have significantly\nadvanced visual grounding performance by training large models with large-scale\ndatasets. However, the problem remains challenging, especially when similar\nobjects appear in the input image. For example, an LVLM may not be able to\ndifferentiate Diet Coke and regular Coke in an image. In this case, if\nadditional reference images of Diet Coke and regular Coke are available, it can\nhelp the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual\nGrounding (MRVG). In this task, a model has access to a set of reference images\nof objects in a database. Based on these reference images and a language\nexpression, the model is required to detect a target object from a query image.\nWe first introduce a new dataset to study the MRVG problem. Then we introduce a\nnovel method, named MRVG-Net, to solve this visual grounding problem. We show\nthat by efficiently using reference images with few-shot object detection and\nusing Large Language Models (LLMs) for object matching, our method achieves\nsuperior visual grounding performance compared to the state-of-the-art LVLMs\nsuch as Qwen2.5-VL-72B. Our approach bridges the gap between few-shot detection\nand visual grounding, unlocking new capabilities for visual understanding,\nwhich has wide applications in robotics. Project page with our video, code, and\ndataset: https://irvlutd.github.io/MultiGrounding\n", "link": "http://arxiv.org/abs/2504.02876v2", "date": "2025-09-24", "relevancy": 2.3108, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5836}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Reference%20Visual%20Grounding&body=Title%3A%20Multimodal%20Reference%20Visual%20Grounding%0AAuthor%3A%20Yangxiao%20Lu%20and%20Ruosen%20Li%20and%20Liqiang%20Jing%20and%20Jikai%20Wang%20and%20Xinya%20Du%20and%20Yunhui%20Guo%20and%20Nicholas%20Ruozzi%20and%20Yu%20Xiang%0AAbstract%3A%20%20%20Visual%20grounding%20focuses%20on%20detecting%20objects%20from%20images%20based%20on%20language%0Aexpressions.%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20significantly%0Aadvanced%20visual%20grounding%20performance%20by%20training%20large%20models%20with%20large-scale%0Adatasets.%20However%2C%20the%20problem%20remains%20challenging%2C%20especially%20when%20similar%0Aobjects%20appear%20in%20the%20input%20image.%20For%20example%2C%20an%20LVLM%20may%20not%20be%20able%20to%0Adifferentiate%20Diet%20Coke%20and%20regular%20Coke%20in%20an%20image.%20In%20this%20case%2C%20if%0Aadditional%20reference%20images%20of%20Diet%20Coke%20and%20regular%20Coke%20are%20available%2C%20it%20can%0Ahelp%20the%20visual%20grounding%20of%20similar%20objects.%0A%20%20In%20this%20work%2C%20we%20introduce%20a%20new%20task%20named%20Multimodal%20Reference%20Visual%0AGrounding%20%28MRVG%29.%20In%20this%20task%2C%20a%20model%20has%20access%20to%20a%20set%20of%20reference%20images%0Aof%20objects%20in%20a%20database.%20Based%20on%20these%20reference%20images%20and%20a%20language%0Aexpression%2C%20the%20model%20is%20required%20to%20detect%20a%20target%20object%20from%20a%20query%20image.%0AWe%20first%20introduce%20a%20new%20dataset%20to%20study%20the%20MRVG%20problem.%20Then%20we%20introduce%20a%0Anovel%20method%2C%20named%20MRVG-Net%2C%20to%20solve%20this%20visual%20grounding%20problem.%20We%20show%0Athat%20by%20efficiently%20using%20reference%20images%20with%20few-shot%20object%20detection%20and%0Ausing%20Large%20Language%20Models%20%28LLMs%29%20for%20object%20matching%2C%20our%20method%20achieves%0Asuperior%20visual%20grounding%20performance%20compared%20to%20the%20state-of-the-art%20LVLMs%0Asuch%20as%20Qwen2.5-VL-72B.%20Our%20approach%20bridges%20the%20gap%20between%20few-shot%20detection%0Aand%20visual%20grounding%2C%20unlocking%20new%20capabilities%20for%20visual%20understanding%2C%0Awhich%20has%20wide%20applications%20in%20robotics.%20Project%20page%20with%20our%20video%2C%20code%2C%20and%0Adataset%3A%20https%3A//irvlutd.github.io/MultiGrounding%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02876v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Reference%2520Visual%2520Grounding%26entry.906535625%3DYangxiao%2520Lu%2520and%2520Ruosen%2520Li%2520and%2520Liqiang%2520Jing%2520and%2520Jikai%2520Wang%2520and%2520Xinya%2520Du%2520and%2520Yunhui%2520Guo%2520and%2520Nicholas%2520Ruozzi%2520and%2520Yu%2520Xiang%26entry.1292438233%3D%2520%2520Visual%2520grounding%2520focuses%2520on%2520detecting%2520objects%2520from%2520images%2520based%2520on%2520language%250Aexpressions.%2520Recent%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520significantly%250Aadvanced%2520visual%2520grounding%2520performance%2520by%2520training%2520large%2520models%2520with%2520large-scale%250Adatasets.%2520However%252C%2520the%2520problem%2520remains%2520challenging%252C%2520especially%2520when%2520similar%250Aobjects%2520appear%2520in%2520the%2520input%2520image.%2520For%2520example%252C%2520an%2520LVLM%2520may%2520not%2520be%2520able%2520to%250Adifferentiate%2520Diet%2520Coke%2520and%2520regular%2520Coke%2520in%2520an%2520image.%2520In%2520this%2520case%252C%2520if%250Aadditional%2520reference%2520images%2520of%2520Diet%2520Coke%2520and%2520regular%2520Coke%2520are%2520available%252C%2520it%2520can%250Ahelp%2520the%2520visual%2520grounding%2520of%2520similar%2520objects.%250A%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520task%2520named%2520Multimodal%2520Reference%2520Visual%250AGrounding%2520%2528MRVG%2529.%2520In%2520this%2520task%252C%2520a%2520model%2520has%2520access%2520to%2520a%2520set%2520of%2520reference%2520images%250Aof%2520objects%2520in%2520a%2520database.%2520Based%2520on%2520these%2520reference%2520images%2520and%2520a%2520language%250Aexpression%252C%2520the%2520model%2520is%2520required%2520to%2520detect%2520a%2520target%2520object%2520from%2520a%2520query%2520image.%250AWe%2520first%2520introduce%2520a%2520new%2520dataset%2520to%2520study%2520the%2520MRVG%2520problem.%2520Then%2520we%2520introduce%2520a%250Anovel%2520method%252C%2520named%2520MRVG-Net%252C%2520to%2520solve%2520this%2520visual%2520grounding%2520problem.%2520We%2520show%250Athat%2520by%2520efficiently%2520using%2520reference%2520images%2520with%2520few-shot%2520object%2520detection%2520and%250Ausing%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520object%2520matching%252C%2520our%2520method%2520achieves%250Asuperior%2520visual%2520grounding%2520performance%2520compared%2520to%2520the%2520state-of-the-art%2520LVLMs%250Asuch%2520as%2520Qwen2.5-VL-72B.%2520Our%2520approach%2520bridges%2520the%2520gap%2520between%2520few-shot%2520detection%250Aand%2520visual%2520grounding%252C%2520unlocking%2520new%2520capabilities%2520for%2520visual%2520understanding%252C%250Awhich%2520has%2520wide%2520applications%2520in%2520robotics.%2520Project%2520page%2520with%2520our%2520video%252C%2520code%252C%2520and%250Adataset%253A%2520https%253A//irvlutd.github.io/MultiGrounding%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02876v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Reference%20Visual%20Grounding&entry.906535625=Yangxiao%20Lu%20and%20Ruosen%20Li%20and%20Liqiang%20Jing%20and%20Jikai%20Wang%20and%20Xinya%20Du%20and%20Yunhui%20Guo%20and%20Nicholas%20Ruozzi%20and%20Yu%20Xiang&entry.1292438233=%20%20Visual%20grounding%20focuses%20on%20detecting%20objects%20from%20images%20based%20on%20language%0Aexpressions.%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20significantly%0Aadvanced%20visual%20grounding%20performance%20by%20training%20large%20models%20with%20large-scale%0Adatasets.%20However%2C%20the%20problem%20remains%20challenging%2C%20especially%20when%20similar%0Aobjects%20appear%20in%20the%20input%20image.%20For%20example%2C%20an%20LVLM%20may%20not%20be%20able%20to%0Adifferentiate%20Diet%20Coke%20and%20regular%20Coke%20in%20an%20image.%20In%20this%20case%2C%20if%0Aadditional%20reference%20images%20of%20Diet%20Coke%20and%20regular%20Coke%20are%20available%2C%20it%20can%0Ahelp%20the%20visual%20grounding%20of%20similar%20objects.%0A%20%20In%20this%20work%2C%20we%20introduce%20a%20new%20task%20named%20Multimodal%20Reference%20Visual%0AGrounding%20%28MRVG%29.%20In%20this%20task%2C%20a%20model%20has%20access%20to%20a%20set%20of%20reference%20images%0Aof%20objects%20in%20a%20database.%20Based%20on%20these%20reference%20images%20and%20a%20language%0Aexpression%2C%20the%20model%20is%20required%20to%20detect%20a%20target%20object%20from%20a%20query%20image.%0AWe%20first%20introduce%20a%20new%20dataset%20to%20study%20the%20MRVG%20problem.%20Then%20we%20introduce%20a%0Anovel%20method%2C%20named%20MRVG-Net%2C%20to%20solve%20this%20visual%20grounding%20problem.%20We%20show%0Athat%20by%20efficiently%20using%20reference%20images%20with%20few-shot%20object%20detection%20and%0Ausing%20Large%20Language%20Models%20%28LLMs%29%20for%20object%20matching%2C%20our%20method%20achieves%0Asuperior%20visual%20grounding%20performance%20compared%20to%20the%20state-of-the-art%20LVLMs%0Asuch%20as%20Qwen2.5-VL-72B.%20Our%20approach%20bridges%20the%20gap%20between%20few-shot%20detection%0Aand%20visual%20grounding%2C%20unlocking%20new%20capabilities%20for%20visual%20understanding%2C%0Awhich%20has%20wide%20applications%20in%20robotics.%20Project%20page%20with%20our%20video%2C%20code%2C%20and%0Adataset%3A%20https%3A//irvlutd.github.io/MultiGrounding%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02876v2&entry.124074799=Read"},
{"title": "A Recovery Guarantee for Sparse Neural Networks", "author": "Sara Fridovich-Keil and Mert Pilanci", "abstract": "  We prove the first guarantees of sparse recovery for ReLU neural networks,\nwhere the sparse network weights constitute the signal to be recovered.\nSpecifically, we study structural properties of the sparse network weights for\ntwo-layer, scalar-output networks under which a simple iterative hard\nthresholding algorithm recovers these weights exactly, using memory that grows\nlinearly in the number of nonzero weights. We validate this theoretical result\nwith simple experiments on recovery of sparse planted MLPs, MNIST\nclassification, and implicit neural representations. Experimentally, we find\nperformance that is competitive with, and often exceeds, a high-performing but\nmemory-inefficient baseline based on iterative magnitude pruning.\n", "link": "http://arxiv.org/abs/2509.20323v1", "date": "2025-09-24", "relevancy": 2.2458, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.475}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4379}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Recovery%20Guarantee%20for%20Sparse%20Neural%20Networks&body=Title%3A%20A%20Recovery%20Guarantee%20for%20Sparse%20Neural%20Networks%0AAuthor%3A%20Sara%20Fridovich-Keil%20and%20Mert%20Pilanci%0AAbstract%3A%20%20%20We%20prove%20the%20first%20guarantees%20of%20sparse%20recovery%20for%20ReLU%20neural%20networks%2C%0Awhere%20the%20sparse%20network%20weights%20constitute%20the%20signal%20to%20be%20recovered.%0ASpecifically%2C%20we%20study%20structural%20properties%20of%20the%20sparse%20network%20weights%20for%0Atwo-layer%2C%20scalar-output%20networks%20under%20which%20a%20simple%20iterative%20hard%0Athresholding%20algorithm%20recovers%20these%20weights%20exactly%2C%20using%20memory%20that%20grows%0Alinearly%20in%20the%20number%20of%20nonzero%20weights.%20We%20validate%20this%20theoretical%20result%0Awith%20simple%20experiments%20on%20recovery%20of%20sparse%20planted%20MLPs%2C%20MNIST%0Aclassification%2C%20and%20implicit%20neural%20representations.%20Experimentally%2C%20we%20find%0Aperformance%20that%20is%20competitive%20with%2C%20and%20often%20exceeds%2C%20a%20high-performing%20but%0Amemory-inefficient%20baseline%20based%20on%20iterative%20magnitude%20pruning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Recovery%2520Guarantee%2520for%2520Sparse%2520Neural%2520Networks%26entry.906535625%3DSara%2520Fridovich-Keil%2520and%2520Mert%2520Pilanci%26entry.1292438233%3D%2520%2520We%2520prove%2520the%2520first%2520guarantees%2520of%2520sparse%2520recovery%2520for%2520ReLU%2520neural%2520networks%252C%250Awhere%2520the%2520sparse%2520network%2520weights%2520constitute%2520the%2520signal%2520to%2520be%2520recovered.%250ASpecifically%252C%2520we%2520study%2520structural%2520properties%2520of%2520the%2520sparse%2520network%2520weights%2520for%250Atwo-layer%252C%2520scalar-output%2520networks%2520under%2520which%2520a%2520simple%2520iterative%2520hard%250Athresholding%2520algorithm%2520recovers%2520these%2520weights%2520exactly%252C%2520using%2520memory%2520that%2520grows%250Alinearly%2520in%2520the%2520number%2520of%2520nonzero%2520weights.%2520We%2520validate%2520this%2520theoretical%2520result%250Awith%2520simple%2520experiments%2520on%2520recovery%2520of%2520sparse%2520planted%2520MLPs%252C%2520MNIST%250Aclassification%252C%2520and%2520implicit%2520neural%2520representations.%2520Experimentally%252C%2520we%2520find%250Aperformance%2520that%2520is%2520competitive%2520with%252C%2520and%2520often%2520exceeds%252C%2520a%2520high-performing%2520but%250Amemory-inefficient%2520baseline%2520based%2520on%2520iterative%2520magnitude%2520pruning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Recovery%20Guarantee%20for%20Sparse%20Neural%20Networks&entry.906535625=Sara%20Fridovich-Keil%20and%20Mert%20Pilanci&entry.1292438233=%20%20We%20prove%20the%20first%20guarantees%20of%20sparse%20recovery%20for%20ReLU%20neural%20networks%2C%0Awhere%20the%20sparse%20network%20weights%20constitute%20the%20signal%20to%20be%20recovered.%0ASpecifically%2C%20we%20study%20structural%20properties%20of%20the%20sparse%20network%20weights%20for%0Atwo-layer%2C%20scalar-output%20networks%20under%20which%20a%20simple%20iterative%20hard%0Athresholding%20algorithm%20recovers%20these%20weights%20exactly%2C%20using%20memory%20that%20grows%0Alinearly%20in%20the%20number%20of%20nonzero%20weights.%20We%20validate%20this%20theoretical%20result%0Awith%20simple%20experiments%20on%20recovery%20of%20sparse%20planted%20MLPs%2C%20MNIST%0Aclassification%2C%20and%20implicit%20neural%20representations.%20Experimentally%2C%20we%20find%0Aperformance%20that%20is%20competitive%20with%2C%20and%20often%20exceeds%2C%20a%20high-performing%20but%0Amemory-inefficient%20baseline%20based%20on%20iterative%20magnitude%20pruning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20323v1&entry.124074799=Read"},
{"title": "Feature Dynamics as Implicit Data Augmentation: A Depth-Decomposed View\n  on Deep Neural Network Generalization", "author": "Tianyu Ruan and Kuo Gai and Shihua Zhang", "abstract": "  Why do deep networks generalize well? In contrast to classical generalization\ntheory, we approach this fundamental question by examining not only inputs and\noutputs, but the evolution of internal features. Our study suggests a\nphenomenon of temporal consistency that predictions remain stable when shallow\nfeatures from earlier checkpoints combine with deeper features from later ones.\nThis stability is not a trivial convergence artifact. It acts as a form of\nimplicit, structured augmentation that supports generalization. We show that\ntemporal consistency extends to unseen and corrupted data, but collapses when\nsemantic structure is destroyed (e.g., random labels). Statistical tests\nfurther reveal that SGD injects anisotropic noise aligned with a few principal\ndirections, reinforcing its role as a source of structured variability.\nTogether, these findings suggest a conceptual perspective that links feature\ndynamics to generalization, pointing toward future work on practical surrogates\nfor measuring temporal feature evolution.\n", "link": "http://arxiv.org/abs/2509.20334v1", "date": "2025-09-24", "relevancy": 2.1565, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5668}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5231}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Dynamics%20as%20Implicit%20Data%20Augmentation%3A%20A%20Depth-Decomposed%20View%0A%20%20on%20Deep%20Neural%20Network%20Generalization&body=Title%3A%20Feature%20Dynamics%20as%20Implicit%20Data%20Augmentation%3A%20A%20Depth-Decomposed%20View%0A%20%20on%20Deep%20Neural%20Network%20Generalization%0AAuthor%3A%20Tianyu%20Ruan%20and%20Kuo%20Gai%20and%20Shihua%20Zhang%0AAbstract%3A%20%20%20Why%20do%20deep%20networks%20generalize%20well%3F%20In%20contrast%20to%20classical%20generalization%0Atheory%2C%20we%20approach%20this%20fundamental%20question%20by%20examining%20not%20only%20inputs%20and%0Aoutputs%2C%20but%20the%20evolution%20of%20internal%20features.%20Our%20study%20suggests%20a%0Aphenomenon%20of%20temporal%20consistency%20that%20predictions%20remain%20stable%20when%20shallow%0Afeatures%20from%20earlier%20checkpoints%20combine%20with%20deeper%20features%20from%20later%20ones.%0AThis%20stability%20is%20not%20a%20trivial%20convergence%20artifact.%20It%20acts%20as%20a%20form%20of%0Aimplicit%2C%20structured%20augmentation%20that%20supports%20generalization.%20We%20show%20that%0Atemporal%20consistency%20extends%20to%20unseen%20and%20corrupted%20data%2C%20but%20collapses%20when%0Asemantic%20structure%20is%20destroyed%20%28e.g.%2C%20random%20labels%29.%20Statistical%20tests%0Afurther%20reveal%20that%20SGD%20injects%20anisotropic%20noise%20aligned%20with%20a%20few%20principal%0Adirections%2C%20reinforcing%20its%20role%20as%20a%20source%20of%20structured%20variability.%0ATogether%2C%20these%20findings%20suggest%20a%20conceptual%20perspective%20that%20links%20feature%0Adynamics%20to%20generalization%2C%20pointing%20toward%20future%20work%20on%20practical%20surrogates%0Afor%20measuring%20temporal%20feature%20evolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Dynamics%2520as%2520Implicit%2520Data%2520Augmentation%253A%2520A%2520Depth-Decomposed%2520View%250A%2520%2520on%2520Deep%2520Neural%2520Network%2520Generalization%26entry.906535625%3DTianyu%2520Ruan%2520and%2520Kuo%2520Gai%2520and%2520Shihua%2520Zhang%26entry.1292438233%3D%2520%2520Why%2520do%2520deep%2520networks%2520generalize%2520well%253F%2520In%2520contrast%2520to%2520classical%2520generalization%250Atheory%252C%2520we%2520approach%2520this%2520fundamental%2520question%2520by%2520examining%2520not%2520only%2520inputs%2520and%250Aoutputs%252C%2520but%2520the%2520evolution%2520of%2520internal%2520features.%2520Our%2520study%2520suggests%2520a%250Aphenomenon%2520of%2520temporal%2520consistency%2520that%2520predictions%2520remain%2520stable%2520when%2520shallow%250Afeatures%2520from%2520earlier%2520checkpoints%2520combine%2520with%2520deeper%2520features%2520from%2520later%2520ones.%250AThis%2520stability%2520is%2520not%2520a%2520trivial%2520convergence%2520artifact.%2520It%2520acts%2520as%2520a%2520form%2520of%250Aimplicit%252C%2520structured%2520augmentation%2520that%2520supports%2520generalization.%2520We%2520show%2520that%250Atemporal%2520consistency%2520extends%2520to%2520unseen%2520and%2520corrupted%2520data%252C%2520but%2520collapses%2520when%250Asemantic%2520structure%2520is%2520destroyed%2520%2528e.g.%252C%2520random%2520labels%2529.%2520Statistical%2520tests%250Afurther%2520reveal%2520that%2520SGD%2520injects%2520anisotropic%2520noise%2520aligned%2520with%2520a%2520few%2520principal%250Adirections%252C%2520reinforcing%2520its%2520role%2520as%2520a%2520source%2520of%2520structured%2520variability.%250ATogether%252C%2520these%2520findings%2520suggest%2520a%2520conceptual%2520perspective%2520that%2520links%2520feature%250Adynamics%2520to%2520generalization%252C%2520pointing%2520toward%2520future%2520work%2520on%2520practical%2520surrogates%250Afor%2520measuring%2520temporal%2520feature%2520evolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Dynamics%20as%20Implicit%20Data%20Augmentation%3A%20A%20Depth-Decomposed%20View%0A%20%20on%20Deep%20Neural%20Network%20Generalization&entry.906535625=Tianyu%20Ruan%20and%20Kuo%20Gai%20and%20Shihua%20Zhang&entry.1292438233=%20%20Why%20do%20deep%20networks%20generalize%20well%3F%20In%20contrast%20to%20classical%20generalization%0Atheory%2C%20we%20approach%20this%20fundamental%20question%20by%20examining%20not%20only%20inputs%20and%0Aoutputs%2C%20but%20the%20evolution%20of%20internal%20features.%20Our%20study%20suggests%20a%0Aphenomenon%20of%20temporal%20consistency%20that%20predictions%20remain%20stable%20when%20shallow%0Afeatures%20from%20earlier%20checkpoints%20combine%20with%20deeper%20features%20from%20later%20ones.%0AThis%20stability%20is%20not%20a%20trivial%20convergence%20artifact.%20It%20acts%20as%20a%20form%20of%0Aimplicit%2C%20structured%20augmentation%20that%20supports%20generalization.%20We%20show%20that%0Atemporal%20consistency%20extends%20to%20unseen%20and%20corrupted%20data%2C%20but%20collapses%20when%0Asemantic%20structure%20is%20destroyed%20%28e.g.%2C%20random%20labels%29.%20Statistical%20tests%0Afurther%20reveal%20that%20SGD%20injects%20anisotropic%20noise%20aligned%20with%20a%20few%20principal%0Adirections%2C%20reinforcing%20its%20role%20as%20a%20source%20of%20structured%20variability.%0ATogether%2C%20these%20findings%20suggest%20a%20conceptual%20perspective%20that%20links%20feature%0Adynamics%20to%20generalization%2C%20pointing%20toward%20future%20work%20on%20practical%20surrogates%0Afor%20measuring%20temporal%20feature%20evolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20334v1&entry.124074799=Read"},
{"title": "Morphological Synthesizer for Ge'ez Language: Addressing Morphological\n  Complexity and Resource Limitations", "author": "Gebrearegawi Gebremariam and Hailay Teklehaymanot and Gebregewergs Mezgebe", "abstract": "  Ge'ez is an ancient Semitic language renowned for its unique alphabet. It\nserves as the script for numerous languages, including Tigrinya and Amharic,\nand played a pivotal role in Ethiopia's cultural and religious development\nduring the Aksumite kingdom era. Ge'ez remains significant as a liturgical\nlanguage in Ethiopia and Eritrea, with much of the national identity\ndocumentation recorded in Ge'ez. These written materials are invaluable primary\nsources for studying Ethiopian and Eritrean philosophy, creativity, knowledge,\nand civilization. Ge'ez has a complex morphological structure with rich\ninflectional and derivational morphology, and no usable NLP has been developed\nand published until now due to the scarcity of annotated linguistic data,\ncorpora, labeled datasets, and lexicons. Therefore, we propose a rule-based\nGe'ez morphological synthesizer to generate surface words from root words\naccording to the morphological structures of the language. We used 1,102 sample\nverbs, representing all verb morphological structures, to test and evaluate the\nsystem. The system achieves a performance of 97.4%, outperforming the baseline\nmodel and suggesting that future work should build a comprehensive system\nconsidering morphological variations of the language.\n  Keywords: Ge'ez, NLP, morphology, morphological synthesizer, rule-based\n", "link": "http://arxiv.org/abs/2509.20341v1", "date": "2025-09-24", "relevancy": 2.1313, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4403}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4228}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morphological%20Synthesizer%20for%20Ge%27ez%20Language%3A%20Addressing%20Morphological%0A%20%20Complexity%20and%20Resource%20Limitations&body=Title%3A%20Morphological%20Synthesizer%20for%20Ge%27ez%20Language%3A%20Addressing%20Morphological%0A%20%20Complexity%20and%20Resource%20Limitations%0AAuthor%3A%20Gebrearegawi%20Gebremariam%20and%20Hailay%20Teklehaymanot%20and%20Gebregewergs%20Mezgebe%0AAbstract%3A%20%20%20Ge%27ez%20is%20an%20ancient%20Semitic%20language%20renowned%20for%20its%20unique%20alphabet.%20It%0Aserves%20as%20the%20script%20for%20numerous%20languages%2C%20including%20Tigrinya%20and%20Amharic%2C%0Aand%20played%20a%20pivotal%20role%20in%20Ethiopia%27s%20cultural%20and%20religious%20development%0Aduring%20the%20Aksumite%20kingdom%20era.%20Ge%27ez%20remains%20significant%20as%20a%20liturgical%0Alanguage%20in%20Ethiopia%20and%20Eritrea%2C%20with%20much%20of%20the%20national%20identity%0Adocumentation%20recorded%20in%20Ge%27ez.%20These%20written%20materials%20are%20invaluable%20primary%0Asources%20for%20studying%20Ethiopian%20and%20Eritrean%20philosophy%2C%20creativity%2C%20knowledge%2C%0Aand%20civilization.%20Ge%27ez%20has%20a%20complex%20morphological%20structure%20with%20rich%0Ainflectional%20and%20derivational%20morphology%2C%20and%20no%20usable%20NLP%20has%20been%20developed%0Aand%20published%20until%20now%20due%20to%20the%20scarcity%20of%20annotated%20linguistic%20data%2C%0Acorpora%2C%20labeled%20datasets%2C%20and%20lexicons.%20Therefore%2C%20we%20propose%20a%20rule-based%0AGe%27ez%20morphological%20synthesizer%20to%20generate%20surface%20words%20from%20root%20words%0Aaccording%20to%20the%20morphological%20structures%20of%20the%20language.%20We%20used%201%2C102%20sample%0Averbs%2C%20representing%20all%20verb%20morphological%20structures%2C%20to%20test%20and%20evaluate%20the%0Asystem.%20The%20system%20achieves%20a%20performance%20of%2097.4%25%2C%20outperforming%20the%20baseline%0Amodel%20and%20suggesting%20that%20future%20work%20should%20build%20a%20comprehensive%20system%0Aconsidering%20morphological%20variations%20of%20the%20language.%0A%20%20Keywords%3A%20Ge%27ez%2C%20NLP%2C%20morphology%2C%20morphological%20synthesizer%2C%20rule-based%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20341v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphological%2520Synthesizer%2520for%2520Ge%2527ez%2520Language%253A%2520Addressing%2520Morphological%250A%2520%2520Complexity%2520and%2520Resource%2520Limitations%26entry.906535625%3DGebrearegawi%2520Gebremariam%2520and%2520Hailay%2520Teklehaymanot%2520and%2520Gebregewergs%2520Mezgebe%26entry.1292438233%3D%2520%2520Ge%2527ez%2520is%2520an%2520ancient%2520Semitic%2520language%2520renowned%2520for%2520its%2520unique%2520alphabet.%2520It%250Aserves%2520as%2520the%2520script%2520for%2520numerous%2520languages%252C%2520including%2520Tigrinya%2520and%2520Amharic%252C%250Aand%2520played%2520a%2520pivotal%2520role%2520in%2520Ethiopia%2527s%2520cultural%2520and%2520religious%2520development%250Aduring%2520the%2520Aksumite%2520kingdom%2520era.%2520Ge%2527ez%2520remains%2520significant%2520as%2520a%2520liturgical%250Alanguage%2520in%2520Ethiopia%2520and%2520Eritrea%252C%2520with%2520much%2520of%2520the%2520national%2520identity%250Adocumentation%2520recorded%2520in%2520Ge%2527ez.%2520These%2520written%2520materials%2520are%2520invaluable%2520primary%250Asources%2520for%2520studying%2520Ethiopian%2520and%2520Eritrean%2520philosophy%252C%2520creativity%252C%2520knowledge%252C%250Aand%2520civilization.%2520Ge%2527ez%2520has%2520a%2520complex%2520morphological%2520structure%2520with%2520rich%250Ainflectional%2520and%2520derivational%2520morphology%252C%2520and%2520no%2520usable%2520NLP%2520has%2520been%2520developed%250Aand%2520published%2520until%2520now%2520due%2520to%2520the%2520scarcity%2520of%2520annotated%2520linguistic%2520data%252C%250Acorpora%252C%2520labeled%2520datasets%252C%2520and%2520lexicons.%2520Therefore%252C%2520we%2520propose%2520a%2520rule-based%250AGe%2527ez%2520morphological%2520synthesizer%2520to%2520generate%2520surface%2520words%2520from%2520root%2520words%250Aaccording%2520to%2520the%2520morphological%2520structures%2520of%2520the%2520language.%2520We%2520used%25201%252C102%2520sample%250Averbs%252C%2520representing%2520all%2520verb%2520morphological%2520structures%252C%2520to%2520test%2520and%2520evaluate%2520the%250Asystem.%2520The%2520system%2520achieves%2520a%2520performance%2520of%252097.4%2525%252C%2520outperforming%2520the%2520baseline%250Amodel%2520and%2520suggesting%2520that%2520future%2520work%2520should%2520build%2520a%2520comprehensive%2520system%250Aconsidering%2520morphological%2520variations%2520of%2520the%2520language.%250A%2520%2520Keywords%253A%2520Ge%2527ez%252C%2520NLP%252C%2520morphology%252C%2520morphological%2520synthesizer%252C%2520rule-based%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20341v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morphological%20Synthesizer%20for%20Ge%27ez%20Language%3A%20Addressing%20Morphological%0A%20%20Complexity%20and%20Resource%20Limitations&entry.906535625=Gebrearegawi%20Gebremariam%20and%20Hailay%20Teklehaymanot%20and%20Gebregewergs%20Mezgebe&entry.1292438233=%20%20Ge%27ez%20is%20an%20ancient%20Semitic%20language%20renowned%20for%20its%20unique%20alphabet.%20It%0Aserves%20as%20the%20script%20for%20numerous%20languages%2C%20including%20Tigrinya%20and%20Amharic%2C%0Aand%20played%20a%20pivotal%20role%20in%20Ethiopia%27s%20cultural%20and%20religious%20development%0Aduring%20the%20Aksumite%20kingdom%20era.%20Ge%27ez%20remains%20significant%20as%20a%20liturgical%0Alanguage%20in%20Ethiopia%20and%20Eritrea%2C%20with%20much%20of%20the%20national%20identity%0Adocumentation%20recorded%20in%20Ge%27ez.%20These%20written%20materials%20are%20invaluable%20primary%0Asources%20for%20studying%20Ethiopian%20and%20Eritrean%20philosophy%2C%20creativity%2C%20knowledge%2C%0Aand%20civilization.%20Ge%27ez%20has%20a%20complex%20morphological%20structure%20with%20rich%0Ainflectional%20and%20derivational%20morphology%2C%20and%20no%20usable%20NLP%20has%20been%20developed%0Aand%20published%20until%20now%20due%20to%20the%20scarcity%20of%20annotated%20linguistic%20data%2C%0Acorpora%2C%20labeled%20datasets%2C%20and%20lexicons.%20Therefore%2C%20we%20propose%20a%20rule-based%0AGe%27ez%20morphological%20synthesizer%20to%20generate%20surface%20words%20from%20root%20words%0Aaccording%20to%20the%20morphological%20structures%20of%20the%20language.%20We%20used%201%2C102%20sample%0Averbs%2C%20representing%20all%20verb%20morphological%20structures%2C%20to%20test%20and%20evaluate%20the%0Asystem.%20The%20system%20achieves%20a%20performance%20of%2097.4%25%2C%20outperforming%20the%20baseline%0Amodel%20and%20suggesting%20that%20future%20work%20should%20build%20a%20comprehensive%20system%0Aconsidering%20morphological%20variations%20of%20the%20language.%0A%20%20Keywords%3A%20Ge%27ez%2C%20NLP%2C%20morphology%2C%20morphological%20synthesizer%2C%20rule-based%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20341v1&entry.124074799=Read"},
{"title": "Uncovering Graph Reasoning in Decoder-only Transformers with Circuit\n  Tracing", "author": "Xinnan Dai and Chung-Hsiang Lo and Kai Guo and Shenglai Zeng and Dongsheng Luo and Jiliang Tang", "abstract": "  Transformer-based LLMs demonstrate strong performance on graph reasoning\ntasks, yet their internal mechanisms remain underexplored. To uncover these\nreasoning process mechanisms in a fundamental and unified view, we set the\nbasic decoder-only transformers and explain them using the circuit-tracer\nframework. Through this lens, we visualize reasoning traces and identify two\ncore mechanisms in graph reasoning: token merging and structural memorization,\nwhich underlie both path reasoning and substructure extraction tasks. We\nfurther quantify these behaviors and analyze how they are influenced by graph\ndensity and model size. Our study provides a unified interpretability framework\nfor understanding structural reasoning in decoder-only Transformers.\n", "link": "http://arxiv.org/abs/2509.20336v1", "date": "2025-09-24", "relevancy": 2.1278, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Graph%20Reasoning%20in%20Decoder-only%20Transformers%20with%20Circuit%0A%20%20Tracing&body=Title%3A%20Uncovering%20Graph%20Reasoning%20in%20Decoder-only%20Transformers%20with%20Circuit%0A%20%20Tracing%0AAuthor%3A%20Xinnan%20Dai%20and%20Chung-Hsiang%20Lo%20and%20Kai%20Guo%20and%20Shenglai%20Zeng%20and%20Dongsheng%20Luo%20and%20Jiliang%20Tang%0AAbstract%3A%20%20%20Transformer-based%20LLMs%20demonstrate%20strong%20performance%20on%20graph%20reasoning%0Atasks%2C%20yet%20their%20internal%20mechanisms%20remain%20underexplored.%20To%20uncover%20these%0Areasoning%20process%20mechanisms%20in%20a%20fundamental%20and%20unified%20view%2C%20we%20set%20the%0Abasic%20decoder-only%20transformers%20and%20explain%20them%20using%20the%20circuit-tracer%0Aframework.%20Through%20this%20lens%2C%20we%20visualize%20reasoning%20traces%20and%20identify%20two%0Acore%20mechanisms%20in%20graph%20reasoning%3A%20token%20merging%20and%20structural%20memorization%2C%0Awhich%20underlie%20both%20path%20reasoning%20and%20substructure%20extraction%20tasks.%20We%0Afurther%20quantify%20these%20behaviors%20and%20analyze%20how%20they%20are%20influenced%20by%20graph%0Adensity%20and%20model%20size.%20Our%20study%20provides%20a%20unified%20interpretability%20framework%0Afor%20understanding%20structural%20reasoning%20in%20decoder-only%20Transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Graph%2520Reasoning%2520in%2520Decoder-only%2520Transformers%2520with%2520Circuit%250A%2520%2520Tracing%26entry.906535625%3DXinnan%2520Dai%2520and%2520Chung-Hsiang%2520Lo%2520and%2520Kai%2520Guo%2520and%2520Shenglai%2520Zeng%2520and%2520Dongsheng%2520Luo%2520and%2520Jiliang%2520Tang%26entry.1292438233%3D%2520%2520Transformer-based%2520LLMs%2520demonstrate%2520strong%2520performance%2520on%2520graph%2520reasoning%250Atasks%252C%2520yet%2520their%2520internal%2520mechanisms%2520remain%2520underexplored.%2520To%2520uncover%2520these%250Areasoning%2520process%2520mechanisms%2520in%2520a%2520fundamental%2520and%2520unified%2520view%252C%2520we%2520set%2520the%250Abasic%2520decoder-only%2520transformers%2520and%2520explain%2520them%2520using%2520the%2520circuit-tracer%250Aframework.%2520Through%2520this%2520lens%252C%2520we%2520visualize%2520reasoning%2520traces%2520and%2520identify%2520two%250Acore%2520mechanisms%2520in%2520graph%2520reasoning%253A%2520token%2520merging%2520and%2520structural%2520memorization%252C%250Awhich%2520underlie%2520both%2520path%2520reasoning%2520and%2520substructure%2520extraction%2520tasks.%2520We%250Afurther%2520quantify%2520these%2520behaviors%2520and%2520analyze%2520how%2520they%2520are%2520influenced%2520by%2520graph%250Adensity%2520and%2520model%2520size.%2520Our%2520study%2520provides%2520a%2520unified%2520interpretability%2520framework%250Afor%2520understanding%2520structural%2520reasoning%2520in%2520decoder-only%2520Transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Graph%20Reasoning%20in%20Decoder-only%20Transformers%20with%20Circuit%0A%20%20Tracing&entry.906535625=Xinnan%20Dai%20and%20Chung-Hsiang%20Lo%20and%20Kai%20Guo%20and%20Shenglai%20Zeng%20and%20Dongsheng%20Luo%20and%20Jiliang%20Tang&entry.1292438233=%20%20Transformer-based%20LLMs%20demonstrate%20strong%20performance%20on%20graph%20reasoning%0Atasks%2C%20yet%20their%20internal%20mechanisms%20remain%20underexplored.%20To%20uncover%20these%0Areasoning%20process%20mechanisms%20in%20a%20fundamental%20and%20unified%20view%2C%20we%20set%20the%0Abasic%20decoder-only%20transformers%20and%20explain%20them%20using%20the%20circuit-tracer%0Aframework.%20Through%20this%20lens%2C%20we%20visualize%20reasoning%20traces%20and%20identify%20two%0Acore%20mechanisms%20in%20graph%20reasoning%3A%20token%20merging%20and%20structural%20memorization%2C%0Awhich%20underlie%20both%20path%20reasoning%20and%20substructure%20extraction%20tasks.%20We%0Afurther%20quantify%20these%20behaviors%20and%20analyze%20how%20they%20are%20influenced%20by%20graph%0Adensity%20and%20model%20size.%20Our%20study%20provides%20a%20unified%20interpretability%20framework%0Afor%20understanding%20structural%20reasoning%20in%20decoder-only%20Transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20336v1&entry.124074799=Read"},
{"title": "Data-fused Model Predictive Control with Guarantees: Application to\n  Flying Humanoid Robots", "author": "Davide Gorbani and Mohamed Elobaid and Giuseppe L'Erario and Hosameldin Awadalla Omer Mohamed and Daniele Pucci", "abstract": "  This paper introduces a Data-Fused Model Predictive Control (DFMPC) framework\nthat combines physics-based models with data-driven representations of unknown\ndynamics. Leveraging Willems' Fundamental Lemma and an artificial equilibrium\nformulation, the method enables tracking of changing, potentially unreachable\nsetpoints while explicitly handling measurement noise through slack variables\nand regularization. We provide guarantees of recursive feasibility and\npractical stability under input-output constraints for a specific class of\nreference signals. The approach is validated on the iRonCub flying humanoid\nrobot, integrating analytical momentum models with data-driven turbine\ndynamics. Simulations show improved tracking and robustness compared to a\npurely model-based MPC, while maintaining real-time feasibility.\n", "link": "http://arxiv.org/abs/2509.10353v3", "date": "2025-09-24", "relevancy": 2.1251, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5836}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5274}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-fused%20Model%20Predictive%20Control%20with%20Guarantees%3A%20Application%20to%0A%20%20Flying%20Humanoid%20Robots&body=Title%3A%20Data-fused%20Model%20Predictive%20Control%20with%20Guarantees%3A%20Application%20to%0A%20%20Flying%20Humanoid%20Robots%0AAuthor%3A%20Davide%20Gorbani%20and%20Mohamed%20Elobaid%20and%20Giuseppe%20L%27Erario%20and%20Hosameldin%20Awadalla%20Omer%20Mohamed%20and%20Daniele%20Pucci%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20Data-Fused%20Model%20Predictive%20Control%20%28DFMPC%29%20framework%0Athat%20combines%20physics-based%20models%20with%20data-driven%20representations%20of%20unknown%0Adynamics.%20Leveraging%20Willems%27%20Fundamental%20Lemma%20and%20an%20artificial%20equilibrium%0Aformulation%2C%20the%20method%20enables%20tracking%20of%20changing%2C%20potentially%20unreachable%0Asetpoints%20while%20explicitly%20handling%20measurement%20noise%20through%20slack%20variables%0Aand%20regularization.%20We%20provide%20guarantees%20of%20recursive%20feasibility%20and%0Apractical%20stability%20under%20input-output%20constraints%20for%20a%20specific%20class%20of%0Areference%20signals.%20The%20approach%20is%20validated%20on%20the%20iRonCub%20flying%20humanoid%0Arobot%2C%20integrating%20analytical%20momentum%20models%20with%20data-driven%20turbine%0Adynamics.%20Simulations%20show%20improved%20tracking%20and%20robustness%20compared%20to%20a%0Apurely%20model-based%20MPC%2C%20while%20maintaining%20real-time%20feasibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10353v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-fused%2520Model%2520Predictive%2520Control%2520with%2520Guarantees%253A%2520Application%2520to%250A%2520%2520Flying%2520Humanoid%2520Robots%26entry.906535625%3DDavide%2520Gorbani%2520and%2520Mohamed%2520Elobaid%2520and%2520Giuseppe%2520L%2527Erario%2520and%2520Hosameldin%2520Awadalla%2520Omer%2520Mohamed%2520and%2520Daniele%2520Pucci%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520Data-Fused%2520Model%2520Predictive%2520Control%2520%2528DFMPC%2529%2520framework%250Athat%2520combines%2520physics-based%2520models%2520with%2520data-driven%2520representations%2520of%2520unknown%250Adynamics.%2520Leveraging%2520Willems%2527%2520Fundamental%2520Lemma%2520and%2520an%2520artificial%2520equilibrium%250Aformulation%252C%2520the%2520method%2520enables%2520tracking%2520of%2520changing%252C%2520potentially%2520unreachable%250Asetpoints%2520while%2520explicitly%2520handling%2520measurement%2520noise%2520through%2520slack%2520variables%250Aand%2520regularization.%2520We%2520provide%2520guarantees%2520of%2520recursive%2520feasibility%2520and%250Apractical%2520stability%2520under%2520input-output%2520constraints%2520for%2520a%2520specific%2520class%2520of%250Areference%2520signals.%2520The%2520approach%2520is%2520validated%2520on%2520the%2520iRonCub%2520flying%2520humanoid%250Arobot%252C%2520integrating%2520analytical%2520momentum%2520models%2520with%2520data-driven%2520turbine%250Adynamics.%2520Simulations%2520show%2520improved%2520tracking%2520and%2520robustness%2520compared%2520to%2520a%250Apurely%2520model-based%2520MPC%252C%2520while%2520maintaining%2520real-time%2520feasibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10353v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-fused%20Model%20Predictive%20Control%20with%20Guarantees%3A%20Application%20to%0A%20%20Flying%20Humanoid%20Robots&entry.906535625=Davide%20Gorbani%20and%20Mohamed%20Elobaid%20and%20Giuseppe%20L%27Erario%20and%20Hosameldin%20Awadalla%20Omer%20Mohamed%20and%20Daniele%20Pucci&entry.1292438233=%20%20This%20paper%20introduces%20a%20Data-Fused%20Model%20Predictive%20Control%20%28DFMPC%29%20framework%0Athat%20combines%20physics-based%20models%20with%20data-driven%20representations%20of%20unknown%0Adynamics.%20Leveraging%20Willems%27%20Fundamental%20Lemma%20and%20an%20artificial%20equilibrium%0Aformulation%2C%20the%20method%20enables%20tracking%20of%20changing%2C%20potentially%20unreachable%0Asetpoints%20while%20explicitly%20handling%20measurement%20noise%20through%20slack%20variables%0Aand%20regularization.%20We%20provide%20guarantees%20of%20recursive%20feasibility%20and%0Apractical%20stability%20under%20input-output%20constraints%20for%20a%20specific%20class%20of%0Areference%20signals.%20The%20approach%20is%20validated%20on%20the%20iRonCub%20flying%20humanoid%0Arobot%2C%20integrating%20analytical%20momentum%20models%20with%20data-driven%20turbine%0Adynamics.%20Simulations%20show%20improved%20tracking%20and%20robustness%20compared%20to%20a%0Apurely%20model-based%20MPC%2C%20while%20maintaining%20real-time%20feasibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10353v3&entry.124074799=Read"},
{"title": "FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory\n  for Segmentation-oriented Anomaly Synthesis", "author": "Xichen Xu and Yanshu Wang and Jinbao Wang and Xiaoning Lei and Guoyang Xie and Guannan Jiang and Zhichao Lu", "abstract": "  Industrial anomaly segmentation relies heavily on pixel-level annotations,\nyet real-world anomalies are often scarce, diverse, and costly to label.\nSegmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a\npromising alternative; however, existing methods struggle to balance sampling\nefficiency and generation quality. Moreover, most approaches treat all spatial\nregions uniformly, overlooking the distinct statistical differences between\nanomaly and background areas. This uniform treatment hinders the synthesis of\ncontrollable, structure-specific anomalies tailored for segmentation tasks. In\nthis paper, we propose FAST, a foreground-aware diffusion framework featuring\ntwo novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the\nForeground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling\nalgorithm specifically designed for segmentation-oriented industrial anomaly\nsynthesis, which accelerates the reverse process through coarse-to-fine\naggregation and enables the synthesis of state-of-the-art segmentation-oriented\nanomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the\nanomaly-aware noise within the masked foreground regions at each sampling step,\npreserving localized anomaly signals throughout the denoising trajectory.\nExtensive experiments on multiple industrial benchmarks demonstrate that FAST\nconsistently outperforms existing anomaly synthesis methods in downstream\nsegmentation tasks. We release the code at:\nhttps://anonymous.4open.science/r/NeurIPS-938.\n", "link": "http://arxiv.org/abs/2509.20295v1", "date": "2025-09-24", "relevancy": 2.1205, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5433}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5399}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAST%3A%20Foreground-aware%20Diffusion%20with%20Accelerated%20Sampling%20Trajectory%0A%20%20for%20Segmentation-oriented%20Anomaly%20Synthesis&body=Title%3A%20FAST%3A%20Foreground-aware%20Diffusion%20with%20Accelerated%20Sampling%20Trajectory%0A%20%20for%20Segmentation-oriented%20Anomaly%20Synthesis%0AAuthor%3A%20Xichen%20Xu%20and%20Yanshu%20Wang%20and%20Jinbao%20Wang%20and%20Xiaoning%20Lei%20and%20Guoyang%20Xie%20and%20Guannan%20Jiang%20and%20Zhichao%20Lu%0AAbstract%3A%20%20%20Industrial%20anomaly%20segmentation%20relies%20heavily%20on%20pixel-level%20annotations%2C%0Ayet%20real-world%20anomalies%20are%20often%20scarce%2C%20diverse%2C%20and%20costly%20to%20label.%0ASegmentation-oriented%20industrial%20anomaly%20synthesis%20%28SIAS%29%20has%20emerged%20as%20a%0Apromising%20alternative%3B%20however%2C%20existing%20methods%20struggle%20to%20balance%20sampling%0Aefficiency%20and%20generation%20quality.%20Moreover%2C%20most%20approaches%20treat%20all%20spatial%0Aregions%20uniformly%2C%20overlooking%20the%20distinct%20statistical%20differences%20between%0Aanomaly%20and%20background%20areas.%20This%20uniform%20treatment%20hinders%20the%20synthesis%20of%0Acontrollable%2C%20structure-specific%20anomalies%20tailored%20for%20segmentation%20tasks.%20In%0Athis%20paper%2C%20we%20propose%20FAST%2C%20a%20foreground-aware%20diffusion%20framework%20featuring%0Atwo%20novel%20modules%3A%20the%20Anomaly-Informed%20Accelerated%20Sampling%20%28AIAS%29%20and%20the%0AForeground-Aware%20Reconstruction%20Module%20%28FARM%29.%20AIAS%20is%20a%20training-free%20sampling%0Aalgorithm%20specifically%20designed%20for%20segmentation-oriented%20industrial%20anomaly%0Asynthesis%2C%20which%20accelerates%20the%20reverse%20process%20through%20coarse-to-fine%0Aaggregation%20and%20enables%20the%20synthesis%20of%20state-of-the-art%20segmentation-oriented%0Aanomalies%20in%20as%20few%20as%2010%20steps.%20Meanwhile%2C%20FARM%20adaptively%20adjusts%20the%0Aanomaly-aware%20noise%20within%20the%20masked%20foreground%20regions%20at%20each%20sampling%20step%2C%0Apreserving%20localized%20anomaly%20signals%20throughout%20the%20denoising%20trajectory.%0AExtensive%20experiments%20on%20multiple%20industrial%20benchmarks%20demonstrate%20that%20FAST%0Aconsistently%20outperforms%20existing%20anomaly%20synthesis%20methods%20in%20downstream%0Asegmentation%20tasks.%20We%20release%20the%20code%20at%3A%0Ahttps%3A//anonymous.4open.science/r/NeurIPS-938.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAST%253A%2520Foreground-aware%2520Diffusion%2520with%2520Accelerated%2520Sampling%2520Trajectory%250A%2520%2520for%2520Segmentation-oriented%2520Anomaly%2520Synthesis%26entry.906535625%3DXichen%2520Xu%2520and%2520Yanshu%2520Wang%2520and%2520Jinbao%2520Wang%2520and%2520Xiaoning%2520Lei%2520and%2520Guoyang%2520Xie%2520and%2520Guannan%2520Jiang%2520and%2520Zhichao%2520Lu%26entry.1292438233%3D%2520%2520Industrial%2520anomaly%2520segmentation%2520relies%2520heavily%2520on%2520pixel-level%2520annotations%252C%250Ayet%2520real-world%2520anomalies%2520are%2520often%2520scarce%252C%2520diverse%252C%2520and%2520costly%2520to%2520label.%250ASegmentation-oriented%2520industrial%2520anomaly%2520synthesis%2520%2528SIAS%2529%2520has%2520emerged%2520as%2520a%250Apromising%2520alternative%253B%2520however%252C%2520existing%2520methods%2520struggle%2520to%2520balance%2520sampling%250Aefficiency%2520and%2520generation%2520quality.%2520Moreover%252C%2520most%2520approaches%2520treat%2520all%2520spatial%250Aregions%2520uniformly%252C%2520overlooking%2520the%2520distinct%2520statistical%2520differences%2520between%250Aanomaly%2520and%2520background%2520areas.%2520This%2520uniform%2520treatment%2520hinders%2520the%2520synthesis%2520of%250Acontrollable%252C%2520structure-specific%2520anomalies%2520tailored%2520for%2520segmentation%2520tasks.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520FAST%252C%2520a%2520foreground-aware%2520diffusion%2520framework%2520featuring%250Atwo%2520novel%2520modules%253A%2520the%2520Anomaly-Informed%2520Accelerated%2520Sampling%2520%2528AIAS%2529%2520and%2520the%250AForeground-Aware%2520Reconstruction%2520Module%2520%2528FARM%2529.%2520AIAS%2520is%2520a%2520training-free%2520sampling%250Aalgorithm%2520specifically%2520designed%2520for%2520segmentation-oriented%2520industrial%2520anomaly%250Asynthesis%252C%2520which%2520accelerates%2520the%2520reverse%2520process%2520through%2520coarse-to-fine%250Aaggregation%2520and%2520enables%2520the%2520synthesis%2520of%2520state-of-the-art%2520segmentation-oriented%250Aanomalies%2520in%2520as%2520few%2520as%252010%2520steps.%2520Meanwhile%252C%2520FARM%2520adaptively%2520adjusts%2520the%250Aanomaly-aware%2520noise%2520within%2520the%2520masked%2520foreground%2520regions%2520at%2520each%2520sampling%2520step%252C%250Apreserving%2520localized%2520anomaly%2520signals%2520throughout%2520the%2520denoising%2520trajectory.%250AExtensive%2520experiments%2520on%2520multiple%2520industrial%2520benchmarks%2520demonstrate%2520that%2520FAST%250Aconsistently%2520outperforms%2520existing%2520anomaly%2520synthesis%2520methods%2520in%2520downstream%250Asegmentation%2520tasks.%2520We%2520release%2520the%2520code%2520at%253A%250Ahttps%253A//anonymous.4open.science/r/NeurIPS-938.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAST%3A%20Foreground-aware%20Diffusion%20with%20Accelerated%20Sampling%20Trajectory%0A%20%20for%20Segmentation-oriented%20Anomaly%20Synthesis&entry.906535625=Xichen%20Xu%20and%20Yanshu%20Wang%20and%20Jinbao%20Wang%20and%20Xiaoning%20Lei%20and%20Guoyang%20Xie%20and%20Guannan%20Jiang%20and%20Zhichao%20Lu&entry.1292438233=%20%20Industrial%20anomaly%20segmentation%20relies%20heavily%20on%20pixel-level%20annotations%2C%0Ayet%20real-world%20anomalies%20are%20often%20scarce%2C%20diverse%2C%20and%20costly%20to%20label.%0ASegmentation-oriented%20industrial%20anomaly%20synthesis%20%28SIAS%29%20has%20emerged%20as%20a%0Apromising%20alternative%3B%20however%2C%20existing%20methods%20struggle%20to%20balance%20sampling%0Aefficiency%20and%20generation%20quality.%20Moreover%2C%20most%20approaches%20treat%20all%20spatial%0Aregions%20uniformly%2C%20overlooking%20the%20distinct%20statistical%20differences%20between%0Aanomaly%20and%20background%20areas.%20This%20uniform%20treatment%20hinders%20the%20synthesis%20of%0Acontrollable%2C%20structure-specific%20anomalies%20tailored%20for%20segmentation%20tasks.%20In%0Athis%20paper%2C%20we%20propose%20FAST%2C%20a%20foreground-aware%20diffusion%20framework%20featuring%0Atwo%20novel%20modules%3A%20the%20Anomaly-Informed%20Accelerated%20Sampling%20%28AIAS%29%20and%20the%0AForeground-Aware%20Reconstruction%20Module%20%28FARM%29.%20AIAS%20is%20a%20training-free%20sampling%0Aalgorithm%20specifically%20designed%20for%20segmentation-oriented%20industrial%20anomaly%0Asynthesis%2C%20which%20accelerates%20the%20reverse%20process%20through%20coarse-to-fine%0Aaggregation%20and%20enables%20the%20synthesis%20of%20state-of-the-art%20segmentation-oriented%0Aanomalies%20in%20as%20few%20as%2010%20steps.%20Meanwhile%2C%20FARM%20adaptively%20adjusts%20the%0Aanomaly-aware%20noise%20within%20the%20masked%20foreground%20regions%20at%20each%20sampling%20step%2C%0Apreserving%20localized%20anomaly%20signals%20throughout%20the%20denoising%20trajectory.%0AExtensive%20experiments%20on%20multiple%20industrial%20benchmarks%20demonstrate%20that%20FAST%0Aconsistently%20outperforms%20existing%20anomaly%20synthesis%20methods%20in%20downstream%0Asegmentation%20tasks.%20We%20release%20the%20code%20at%3A%0Ahttps%3A//anonymous.4open.science/r/NeurIPS-938.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20295v1&entry.124074799=Read"},
{"title": "Exploring Graph-Transformer Out-of-Distribution Generalization Abilities", "author": "Itay Niv and Neta Rabin", "abstract": "  Deep learning on graphs has shown remarkable success across numerous\napplications, including social networks, bio-physics, traffic networks, and\nrecommendation systems. Regardless of their successes, current methods\nfrequently depend on the assumption that training and testing data share the\nsame distribution, a condition rarely met in real-world scenarios. While\ngraph-transformer (GT) backbones have recently outperformed traditional\nmessage-passing neural networks (MPNNs) in multiple in-distribution (ID)\nbenchmarks, their effectiveness under distribution shifts remains largely\nunexplored. In this work, we address the challenge of out-of-distribution (OOD)\ngeneralization for graph neural networks, with a special focus on the impact of\nbackbone architecture. We systematically evaluate GT and hybrid backbones in\nOOD settings and compare them to MPNNs. To do so, we adapt several leading\ndomain generalization (DG) algorithms to work with GTs and assess their\nperformance on a benchmark designed to test a variety of distribution shifts.\nOur results reveal that GT and hybrid GT-MPNN backbones demonstrate stronger\ngeneralization ability compared to MPNNs, even without specialized DG\nalgorithms (on four out of six benchmarks). Additionally, we propose a novel\npost-training analysis approach that compares the clustering structure of the\nentire ID and OOD test datasets, specifically examining domain alignment and\nclass separation. Highlighting its model-agnostic design, the method yielded\nvaluable insights into both GT and MPNN backbones and appears well suited for\nbroader DG applications beyond graph learning, offering a deeper perspective on\ngeneralization abilities that goes beyond standard accuracy metrics. Together,\nour findings highlight the promise of graph-transformers for robust, real-world\ngraph learning and set a new direction for future research in OOD\ngeneralization.\n", "link": "http://arxiv.org/abs/2506.20575v2", "date": "2025-09-24", "relevancy": 2.1075, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5409}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5178}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Graph-Transformer%20Out-of-Distribution%20Generalization%20Abilities&body=Title%3A%20Exploring%20Graph-Transformer%20Out-of-Distribution%20Generalization%20Abilities%0AAuthor%3A%20Itay%20Niv%20and%20Neta%20Rabin%0AAbstract%3A%20%20%20Deep%20learning%20on%20graphs%20has%20shown%20remarkable%20success%20across%20numerous%0Aapplications%2C%20including%20social%20networks%2C%20bio-physics%2C%20traffic%20networks%2C%20and%0Arecommendation%20systems.%20Regardless%20of%20their%20successes%2C%20current%20methods%0Afrequently%20depend%20on%20the%20assumption%20that%20training%20and%20testing%20data%20share%20the%0Asame%20distribution%2C%20a%20condition%20rarely%20met%20in%20real-world%20scenarios.%20While%0Agraph-transformer%20%28GT%29%20backbones%20have%20recently%20outperformed%20traditional%0Amessage-passing%20neural%20networks%20%28MPNNs%29%20in%20multiple%20in-distribution%20%28ID%29%0Abenchmarks%2C%20their%20effectiveness%20under%20distribution%20shifts%20remains%20largely%0Aunexplored.%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20out-of-distribution%20%28OOD%29%0Ageneralization%20for%20graph%20neural%20networks%2C%20with%20a%20special%20focus%20on%20the%20impact%20of%0Abackbone%20architecture.%20We%20systematically%20evaluate%20GT%20and%20hybrid%20backbones%20in%0AOOD%20settings%20and%20compare%20them%20to%20MPNNs.%20To%20do%20so%2C%20we%20adapt%20several%20leading%0Adomain%20generalization%20%28DG%29%20algorithms%20to%20work%20with%20GTs%20and%20assess%20their%0Aperformance%20on%20a%20benchmark%20designed%20to%20test%20a%20variety%20of%20distribution%20shifts.%0AOur%20results%20reveal%20that%20GT%20and%20hybrid%20GT-MPNN%20backbones%20demonstrate%20stronger%0Ageneralization%20ability%20compared%20to%20MPNNs%2C%20even%20without%20specialized%20DG%0Aalgorithms%20%28on%20four%20out%20of%20six%20benchmarks%29.%20Additionally%2C%20we%20propose%20a%20novel%0Apost-training%20analysis%20approach%20that%20compares%20the%20clustering%20structure%20of%20the%0Aentire%20ID%20and%20OOD%20test%20datasets%2C%20specifically%20examining%20domain%20alignment%20and%0Aclass%20separation.%20Highlighting%20its%20model-agnostic%20design%2C%20the%20method%20yielded%0Avaluable%20insights%20into%20both%20GT%20and%20MPNN%20backbones%20and%20appears%20well%20suited%20for%0Abroader%20DG%20applications%20beyond%20graph%20learning%2C%20offering%20a%20deeper%20perspective%20on%0Ageneralization%20abilities%20that%20goes%20beyond%20standard%20accuracy%20metrics.%20Together%2C%0Aour%20findings%20highlight%20the%20promise%20of%20graph-transformers%20for%20robust%2C%20real-world%0Agraph%20learning%20and%20set%20a%20new%20direction%20for%20future%20research%20in%20OOD%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20575v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Graph-Transformer%2520Out-of-Distribution%2520Generalization%2520Abilities%26entry.906535625%3DItay%2520Niv%2520and%2520Neta%2520Rabin%26entry.1292438233%3D%2520%2520Deep%2520learning%2520on%2520graphs%2520has%2520shown%2520remarkable%2520success%2520across%2520numerous%250Aapplications%252C%2520including%2520social%2520networks%252C%2520bio-physics%252C%2520traffic%2520networks%252C%2520and%250Arecommendation%2520systems.%2520Regardless%2520of%2520their%2520successes%252C%2520current%2520methods%250Afrequently%2520depend%2520on%2520the%2520assumption%2520that%2520training%2520and%2520testing%2520data%2520share%2520the%250Asame%2520distribution%252C%2520a%2520condition%2520rarely%2520met%2520in%2520real-world%2520scenarios.%2520While%250Agraph-transformer%2520%2528GT%2529%2520backbones%2520have%2520recently%2520outperformed%2520traditional%250Amessage-passing%2520neural%2520networks%2520%2528MPNNs%2529%2520in%2520multiple%2520in-distribution%2520%2528ID%2529%250Abenchmarks%252C%2520their%2520effectiveness%2520under%2520distribution%2520shifts%2520remains%2520largely%250Aunexplored.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520challenge%2520of%2520out-of-distribution%2520%2528OOD%2529%250Ageneralization%2520for%2520graph%2520neural%2520networks%252C%2520with%2520a%2520special%2520focus%2520on%2520the%2520impact%2520of%250Abackbone%2520architecture.%2520We%2520systematically%2520evaluate%2520GT%2520and%2520hybrid%2520backbones%2520in%250AOOD%2520settings%2520and%2520compare%2520them%2520to%2520MPNNs.%2520To%2520do%2520so%252C%2520we%2520adapt%2520several%2520leading%250Adomain%2520generalization%2520%2528DG%2529%2520algorithms%2520to%2520work%2520with%2520GTs%2520and%2520assess%2520their%250Aperformance%2520on%2520a%2520benchmark%2520designed%2520to%2520test%2520a%2520variety%2520of%2520distribution%2520shifts.%250AOur%2520results%2520reveal%2520that%2520GT%2520and%2520hybrid%2520GT-MPNN%2520backbones%2520demonstrate%2520stronger%250Ageneralization%2520ability%2520compared%2520to%2520MPNNs%252C%2520even%2520without%2520specialized%2520DG%250Aalgorithms%2520%2528on%2520four%2520out%2520of%2520six%2520benchmarks%2529.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%250Apost-training%2520analysis%2520approach%2520that%2520compares%2520the%2520clustering%2520structure%2520of%2520the%250Aentire%2520ID%2520and%2520OOD%2520test%2520datasets%252C%2520specifically%2520examining%2520domain%2520alignment%2520and%250Aclass%2520separation.%2520Highlighting%2520its%2520model-agnostic%2520design%252C%2520the%2520method%2520yielded%250Avaluable%2520insights%2520into%2520both%2520GT%2520and%2520MPNN%2520backbones%2520and%2520appears%2520well%2520suited%2520for%250Abroader%2520DG%2520applications%2520beyond%2520graph%2520learning%252C%2520offering%2520a%2520deeper%2520perspective%2520on%250Ageneralization%2520abilities%2520that%2520goes%2520beyond%2520standard%2520accuracy%2520metrics.%2520Together%252C%250Aour%2520findings%2520highlight%2520the%2520promise%2520of%2520graph-transformers%2520for%2520robust%252C%2520real-world%250Agraph%2520learning%2520and%2520set%2520a%2520new%2520direction%2520for%2520future%2520research%2520in%2520OOD%250Ageneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20575v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Graph-Transformer%20Out-of-Distribution%20Generalization%20Abilities&entry.906535625=Itay%20Niv%20and%20Neta%20Rabin&entry.1292438233=%20%20Deep%20learning%20on%20graphs%20has%20shown%20remarkable%20success%20across%20numerous%0Aapplications%2C%20including%20social%20networks%2C%20bio-physics%2C%20traffic%20networks%2C%20and%0Arecommendation%20systems.%20Regardless%20of%20their%20successes%2C%20current%20methods%0Afrequently%20depend%20on%20the%20assumption%20that%20training%20and%20testing%20data%20share%20the%0Asame%20distribution%2C%20a%20condition%20rarely%20met%20in%20real-world%20scenarios.%20While%0Agraph-transformer%20%28GT%29%20backbones%20have%20recently%20outperformed%20traditional%0Amessage-passing%20neural%20networks%20%28MPNNs%29%20in%20multiple%20in-distribution%20%28ID%29%0Abenchmarks%2C%20their%20effectiveness%20under%20distribution%20shifts%20remains%20largely%0Aunexplored.%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20out-of-distribution%20%28OOD%29%0Ageneralization%20for%20graph%20neural%20networks%2C%20with%20a%20special%20focus%20on%20the%20impact%20of%0Abackbone%20architecture.%20We%20systematically%20evaluate%20GT%20and%20hybrid%20backbones%20in%0AOOD%20settings%20and%20compare%20them%20to%20MPNNs.%20To%20do%20so%2C%20we%20adapt%20several%20leading%0Adomain%20generalization%20%28DG%29%20algorithms%20to%20work%20with%20GTs%20and%20assess%20their%0Aperformance%20on%20a%20benchmark%20designed%20to%20test%20a%20variety%20of%20distribution%20shifts.%0AOur%20results%20reveal%20that%20GT%20and%20hybrid%20GT-MPNN%20backbones%20demonstrate%20stronger%0Ageneralization%20ability%20compared%20to%20MPNNs%2C%20even%20without%20specialized%20DG%0Aalgorithms%20%28on%20four%20out%20of%20six%20benchmarks%29.%20Additionally%2C%20we%20propose%20a%20novel%0Apost-training%20analysis%20approach%20that%20compares%20the%20clustering%20structure%20of%20the%0Aentire%20ID%20and%20OOD%20test%20datasets%2C%20specifically%20examining%20domain%20alignment%20and%0Aclass%20separation.%20Highlighting%20its%20model-agnostic%20design%2C%20the%20method%20yielded%0Avaluable%20insights%20into%20both%20GT%20and%20MPNN%20backbones%20and%20appears%20well%20suited%20for%0Abroader%20DG%20applications%20beyond%20graph%20learning%2C%20offering%20a%20deeper%20perspective%20on%0Ageneralization%20abilities%20that%20goes%20beyond%20standard%20accuracy%20metrics.%20Together%2C%0Aour%20findings%20highlight%20the%20promise%20of%20graph-transformers%20for%20robust%2C%20real-world%0Agraph%20learning%20and%20set%20a%20new%20direction%20for%20future%20research%20in%20OOD%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20575v2&entry.124074799=Read"},
{"title": "Structure As Search: Unsupervised Permutation Learning for Combinatorial\n  Optimization", "author": "Yimeng Min and Carla P. Gomes", "abstract": "  We propose a non-autoregressive framework for the Travelling Salesman Problem\nwhere solutions emerge directly from learned permutations, without requiring\nexplicit search. By applying a similarity transformation to Hamiltonian cycles,\nthe model learns to approximate permutation matrices via continuous\nrelaxations. Our unsupervised approach achieves competitive performance against\nclassical heuristics, demonstrating that the inherent structure of the problem\ncan effectively guide combinatorial optimization without sequential\ndecision-making. Our method offers concrete evidence that neural networks can\ndirectly capture and exploit combinatorial structure.\n", "link": "http://arxiv.org/abs/2507.04164v3", "date": "2025-09-24", "relevancy": 2.0681, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5637}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure%20As%20Search%3A%20Unsupervised%20Permutation%20Learning%20for%20Combinatorial%0A%20%20Optimization&body=Title%3A%20Structure%20As%20Search%3A%20Unsupervised%20Permutation%20Learning%20for%20Combinatorial%0A%20%20Optimization%0AAuthor%3A%20Yimeng%20Min%20and%20Carla%20P.%20Gomes%0AAbstract%3A%20%20%20We%20propose%20a%20non-autoregressive%20framework%20for%20the%20Travelling%20Salesman%20Problem%0Awhere%20solutions%20emerge%20directly%20from%20learned%20permutations%2C%20without%20requiring%0Aexplicit%20search.%20By%20applying%20a%20similarity%20transformation%20to%20Hamiltonian%20cycles%2C%0Athe%20model%20learns%20to%20approximate%20permutation%20matrices%20via%20continuous%0Arelaxations.%20Our%20unsupervised%20approach%20achieves%20competitive%20performance%20against%0Aclassical%20heuristics%2C%20demonstrating%20that%20the%20inherent%20structure%20of%20the%20problem%0Acan%20effectively%20guide%20combinatorial%20optimization%20without%20sequential%0Adecision-making.%20Our%20method%20offers%20concrete%20evidence%20that%20neural%20networks%20can%0Adirectly%20capture%20and%20exploit%20combinatorial%20structure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04164v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure%2520As%2520Search%253A%2520Unsupervised%2520Permutation%2520Learning%2520for%2520Combinatorial%250A%2520%2520Optimization%26entry.906535625%3DYimeng%2520Min%2520and%2520Carla%2520P.%2520Gomes%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520non-autoregressive%2520framework%2520for%2520the%2520Travelling%2520Salesman%2520Problem%250Awhere%2520solutions%2520emerge%2520directly%2520from%2520learned%2520permutations%252C%2520without%2520requiring%250Aexplicit%2520search.%2520By%2520applying%2520a%2520similarity%2520transformation%2520to%2520Hamiltonian%2520cycles%252C%250Athe%2520model%2520learns%2520to%2520approximate%2520permutation%2520matrices%2520via%2520continuous%250Arelaxations.%2520Our%2520unsupervised%2520approach%2520achieves%2520competitive%2520performance%2520against%250Aclassical%2520heuristics%252C%2520demonstrating%2520that%2520the%2520inherent%2520structure%2520of%2520the%2520problem%250Acan%2520effectively%2520guide%2520combinatorial%2520optimization%2520without%2520sequential%250Adecision-making.%2520Our%2520method%2520offers%2520concrete%2520evidence%2520that%2520neural%2520networks%2520can%250Adirectly%2520capture%2520and%2520exploit%2520combinatorial%2520structure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04164v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure%20As%20Search%3A%20Unsupervised%20Permutation%20Learning%20for%20Combinatorial%0A%20%20Optimization&entry.906535625=Yimeng%20Min%20and%20Carla%20P.%20Gomes&entry.1292438233=%20%20We%20propose%20a%20non-autoregressive%20framework%20for%20the%20Travelling%20Salesman%20Problem%0Awhere%20solutions%20emerge%20directly%20from%20learned%20permutations%2C%20without%20requiring%0Aexplicit%20search.%20By%20applying%20a%20similarity%20transformation%20to%20Hamiltonian%20cycles%2C%0Athe%20model%20learns%20to%20approximate%20permutation%20matrices%20via%20continuous%0Arelaxations.%20Our%20unsupervised%20approach%20achieves%20competitive%20performance%20against%0Aclassical%20heuristics%2C%20demonstrating%20that%20the%20inherent%20structure%20of%20the%20problem%0Acan%20effectively%20guide%20combinatorial%20optimization%20without%20sequential%0Adecision-making.%20Our%20method%20offers%20concrete%20evidence%20that%20neural%20networks%20can%0Adirectly%20capture%20and%20exploit%20combinatorial%20structure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04164v3&entry.124074799=Read"},
{"title": "A GEN AI Framework for Medical Note Generation", "author": "Hui Yi Leong and Yi Fan Gao and Shuai Ji and Bora Kalaycioglu and Uktu Pamuksuz", "abstract": "  The increasing administrative burden of medical documentation, particularly\nthrough Electronic Health Records (EHR), significantly reduces the time\navailable for direct patient care and contributes to physician burnout. To\naddress this issue, we propose MediNotes, an advanced generative AI framework\ndesigned to automate the creation of SOAP (Subjective, Objective, Assessment,\nPlan) notes from medical conversations. MediNotes integrates Large Language\nModels (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech\nRecognition (ASR) to capture and process both text and voice inputs in real\ntime or from recorded audio, generating structured and contextually accurate\nmedical notes. The framework also incorporates advanced techniques like\nQuantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning\n(PEFT) for efficient model fine-tuning in resource-constrained environments.\nAdditionally, MediNotes offers a query-based retrieval system, allowing\nhealthcare providers and patients to access relevant medical information\nquickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate\nthat MediNotes significantly improves the accuracy, efficiency, and usability\nof automated medical documentation, offering a robust solution to reduce the\nadministrative burden on healthcare professionals while improving the quality\nof clinical workflows.\n", "link": "http://arxiv.org/abs/2410.01841v2", "date": "2025-09-24", "relevancy": 2.0631, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5333}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5144}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20GEN%20AI%20Framework%20for%20Medical%20Note%20Generation&body=Title%3A%20A%20GEN%20AI%20Framework%20for%20Medical%20Note%20Generation%0AAuthor%3A%20Hui%20Yi%20Leong%20and%20Yi%20Fan%20Gao%20and%20Shuai%20Ji%20and%20Bora%20Kalaycioglu%20and%20Uktu%20Pamuksuz%0AAbstract%3A%20%20%20The%20increasing%20administrative%20burden%20of%20medical%20documentation%2C%20particularly%0Athrough%20Electronic%20Health%20Records%20%28EHR%29%2C%20significantly%20reduces%20the%20time%0Aavailable%20for%20direct%20patient%20care%20and%20contributes%20to%20physician%20burnout.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20MediNotes%2C%20an%20advanced%20generative%20AI%20framework%0Adesigned%20to%20automate%20the%20creation%20of%20SOAP%20%28Subjective%2C%20Objective%2C%20Assessment%2C%0APlan%29%20notes%20from%20medical%20conversations.%20MediNotes%20integrates%20Large%20Language%0AModels%20%28LLMs%29%2C%20Retrieval-Augmented%20Generation%20%28RAG%29%2C%20and%20Automatic%20Speech%0ARecognition%20%28ASR%29%20to%20capture%20and%20process%20both%20text%20and%20voice%20inputs%20in%20real%0Atime%20or%20from%20recorded%20audio%2C%20generating%20structured%20and%20contextually%20accurate%0Amedical%20notes.%20The%20framework%20also%20incorporates%20advanced%20techniques%20like%0AQuantized%20Low-Rank%20Adaptation%20%28QLoRA%29%20and%20Parameter-Efficient%20Fine-Tuning%0A%28PEFT%29%20for%20efficient%20model%20fine-tuning%20in%20resource-constrained%20environments.%0AAdditionally%2C%20MediNotes%20offers%20a%20query-based%20retrieval%20system%2C%20allowing%0Ahealthcare%20providers%20and%20patients%20to%20access%20relevant%20medical%20information%0Aquickly%20and%20accurately.%20Evaluations%20using%20the%20ACI-BENCH%20dataset%20demonstrate%0Athat%20MediNotes%20significantly%20improves%20the%20accuracy%2C%20efficiency%2C%20and%20usability%0Aof%20automated%20medical%20documentation%2C%20offering%20a%20robust%20solution%20to%20reduce%20the%0Aadministrative%20burden%20on%20healthcare%20professionals%20while%20improving%20the%20quality%0Aof%20clinical%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01841v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520GEN%2520AI%2520Framework%2520for%2520Medical%2520Note%2520Generation%26entry.906535625%3DHui%2520Yi%2520Leong%2520and%2520Yi%2520Fan%2520Gao%2520and%2520Shuai%2520Ji%2520and%2520Bora%2520Kalaycioglu%2520and%2520Uktu%2520Pamuksuz%26entry.1292438233%3D%2520%2520The%2520increasing%2520administrative%2520burden%2520of%2520medical%2520documentation%252C%2520particularly%250Athrough%2520Electronic%2520Health%2520Records%2520%2528EHR%2529%252C%2520significantly%2520reduces%2520the%2520time%250Aavailable%2520for%2520direct%2520patient%2520care%2520and%2520contributes%2520to%2520physician%2520burnout.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520MediNotes%252C%2520an%2520advanced%2520generative%2520AI%2520framework%250Adesigned%2520to%2520automate%2520the%2520creation%2520of%2520SOAP%2520%2528Subjective%252C%2520Objective%252C%2520Assessment%252C%250APlan%2529%2520notes%2520from%2520medical%2520conversations.%2520MediNotes%2520integrates%2520Large%2520Language%250AModels%2520%2528LLMs%2529%252C%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%252C%2520and%2520Automatic%2520Speech%250ARecognition%2520%2528ASR%2529%2520to%2520capture%2520and%2520process%2520both%2520text%2520and%2520voice%2520inputs%2520in%2520real%250Atime%2520or%2520from%2520recorded%2520audio%252C%2520generating%2520structured%2520and%2520contextually%2520accurate%250Amedical%2520notes.%2520The%2520framework%2520also%2520incorporates%2520advanced%2520techniques%2520like%250AQuantized%2520Low-Rank%2520Adaptation%2520%2528QLoRA%2529%2520and%2520Parameter-Efficient%2520Fine-Tuning%250A%2528PEFT%2529%2520for%2520efficient%2520model%2520fine-tuning%2520in%2520resource-constrained%2520environments.%250AAdditionally%252C%2520MediNotes%2520offers%2520a%2520query-based%2520retrieval%2520system%252C%2520allowing%250Ahealthcare%2520providers%2520and%2520patients%2520to%2520access%2520relevant%2520medical%2520information%250Aquickly%2520and%2520accurately.%2520Evaluations%2520using%2520the%2520ACI-BENCH%2520dataset%2520demonstrate%250Athat%2520MediNotes%2520significantly%2520improves%2520the%2520accuracy%252C%2520efficiency%252C%2520and%2520usability%250Aof%2520automated%2520medical%2520documentation%252C%2520offering%2520a%2520robust%2520solution%2520to%2520reduce%2520the%250Aadministrative%2520burden%2520on%2520healthcare%2520professionals%2520while%2520improving%2520the%2520quality%250Aof%2520clinical%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01841v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20GEN%20AI%20Framework%20for%20Medical%20Note%20Generation&entry.906535625=Hui%20Yi%20Leong%20and%20Yi%20Fan%20Gao%20and%20Shuai%20Ji%20and%20Bora%20Kalaycioglu%20and%20Uktu%20Pamuksuz&entry.1292438233=%20%20The%20increasing%20administrative%20burden%20of%20medical%20documentation%2C%20particularly%0Athrough%20Electronic%20Health%20Records%20%28EHR%29%2C%20significantly%20reduces%20the%20time%0Aavailable%20for%20direct%20patient%20care%20and%20contributes%20to%20physician%20burnout.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20MediNotes%2C%20an%20advanced%20generative%20AI%20framework%0Adesigned%20to%20automate%20the%20creation%20of%20SOAP%20%28Subjective%2C%20Objective%2C%20Assessment%2C%0APlan%29%20notes%20from%20medical%20conversations.%20MediNotes%20integrates%20Large%20Language%0AModels%20%28LLMs%29%2C%20Retrieval-Augmented%20Generation%20%28RAG%29%2C%20and%20Automatic%20Speech%0ARecognition%20%28ASR%29%20to%20capture%20and%20process%20both%20text%20and%20voice%20inputs%20in%20real%0Atime%20or%20from%20recorded%20audio%2C%20generating%20structured%20and%20contextually%20accurate%0Amedical%20notes.%20The%20framework%20also%20incorporates%20advanced%20techniques%20like%0AQuantized%20Low-Rank%20Adaptation%20%28QLoRA%29%20and%20Parameter-Efficient%20Fine-Tuning%0A%28PEFT%29%20for%20efficient%20model%20fine-tuning%20in%20resource-constrained%20environments.%0AAdditionally%2C%20MediNotes%20offers%20a%20query-based%20retrieval%20system%2C%20allowing%0Ahealthcare%20providers%20and%20patients%20to%20access%20relevant%20medical%20information%0Aquickly%20and%20accurately.%20Evaluations%20using%20the%20ACI-BENCH%20dataset%20demonstrate%0Athat%20MediNotes%20significantly%20improves%20the%20accuracy%2C%20efficiency%2C%20and%20usability%0Aof%20automated%20medical%20documentation%2C%20offering%20a%20robust%20solution%20to%20reduce%20the%0Aadministrative%20burden%20on%20healthcare%20professionals%20while%20improving%20the%20quality%0Aof%20clinical%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01841v2&entry.124074799=Read"},
{"title": "PerFace: Metric Learning in Perceptual Facial Similarity for Enhanced\n  Face Anonymization", "author": "Haruka Kumagai and Leslie W\u00f6hler and Satoshi Ikehata and Kiyoharu Aizawa", "abstract": "  In response to rising societal awareness of privacy concerns, face\nanonymization techniques have advanced, including the emergence of\nface-swapping methods that replace one identity with another. Achieving a\nbalance between anonymity and naturalness in face swapping requires careful\nselection of identities: overly similar faces compromise anonymity, while\ndissimilar ones reduce naturalness. Existing models, however, focus on binary\nidentity classification \"the same person or not\", making it difficult to\nmeasure nuanced similarities such as \"completely different\" versus \"highly\nsimilar but different.\" This paper proposes a human-perception-based face\nsimilarity metric, creating a dataset of 6,400 triplet annotations and metric\nlearning to predict the similarity. Experimental results demonstrate\nsignificant improvements in both face similarity prediction and attribute-based\nface classification tasks over existing methods.\n", "link": "http://arxiv.org/abs/2509.20281v1", "date": "2025-09-24", "relevancy": 2.0478, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5059}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerFace%3A%20Metric%20Learning%20in%20Perceptual%20Facial%20Similarity%20for%20Enhanced%0A%20%20Face%20Anonymization&body=Title%3A%20PerFace%3A%20Metric%20Learning%20in%20Perceptual%20Facial%20Similarity%20for%20Enhanced%0A%20%20Face%20Anonymization%0AAuthor%3A%20Haruka%20Kumagai%20and%20Leslie%20W%C3%B6hler%20and%20Satoshi%20Ikehata%20and%20Kiyoharu%20Aizawa%0AAbstract%3A%20%20%20In%20response%20to%20rising%20societal%20awareness%20of%20privacy%20concerns%2C%20face%0Aanonymization%20techniques%20have%20advanced%2C%20including%20the%20emergence%20of%0Aface-swapping%20methods%20that%20replace%20one%20identity%20with%20another.%20Achieving%20a%0Abalance%20between%20anonymity%20and%20naturalness%20in%20face%20swapping%20requires%20careful%0Aselection%20of%20identities%3A%20overly%20similar%20faces%20compromise%20anonymity%2C%20while%0Adissimilar%20ones%20reduce%20naturalness.%20Existing%20models%2C%20however%2C%20focus%20on%20binary%0Aidentity%20classification%20%22the%20same%20person%20or%20not%22%2C%20making%20it%20difficult%20to%0Ameasure%20nuanced%20similarities%20such%20as%20%22completely%20different%22%20versus%20%22highly%0Asimilar%20but%20different.%22%20This%20paper%20proposes%20a%20human-perception-based%20face%0Asimilarity%20metric%2C%20creating%20a%20dataset%20of%206%2C400%20triplet%20annotations%20and%20metric%0Alearning%20to%20predict%20the%20similarity.%20Experimental%20results%20demonstrate%0Asignificant%20improvements%20in%20both%20face%20similarity%20prediction%20and%20attribute-based%0Aface%20classification%20tasks%20over%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerFace%253A%2520Metric%2520Learning%2520in%2520Perceptual%2520Facial%2520Similarity%2520for%2520Enhanced%250A%2520%2520Face%2520Anonymization%26entry.906535625%3DHaruka%2520Kumagai%2520and%2520Leslie%2520W%25C3%25B6hler%2520and%2520Satoshi%2520Ikehata%2520and%2520Kiyoharu%2520Aizawa%26entry.1292438233%3D%2520%2520In%2520response%2520to%2520rising%2520societal%2520awareness%2520of%2520privacy%2520concerns%252C%2520face%250Aanonymization%2520techniques%2520have%2520advanced%252C%2520including%2520the%2520emergence%2520of%250Aface-swapping%2520methods%2520that%2520replace%2520one%2520identity%2520with%2520another.%2520Achieving%2520a%250Abalance%2520between%2520anonymity%2520and%2520naturalness%2520in%2520face%2520swapping%2520requires%2520careful%250Aselection%2520of%2520identities%253A%2520overly%2520similar%2520faces%2520compromise%2520anonymity%252C%2520while%250Adissimilar%2520ones%2520reduce%2520naturalness.%2520Existing%2520models%252C%2520however%252C%2520focus%2520on%2520binary%250Aidentity%2520classification%2520%2522the%2520same%2520person%2520or%2520not%2522%252C%2520making%2520it%2520difficult%2520to%250Ameasure%2520nuanced%2520similarities%2520such%2520as%2520%2522completely%2520different%2522%2520versus%2520%2522highly%250Asimilar%2520but%2520different.%2522%2520This%2520paper%2520proposes%2520a%2520human-perception-based%2520face%250Asimilarity%2520metric%252C%2520creating%2520a%2520dataset%2520of%25206%252C400%2520triplet%2520annotations%2520and%2520metric%250Alearning%2520to%2520predict%2520the%2520similarity.%2520Experimental%2520results%2520demonstrate%250Asignificant%2520improvements%2520in%2520both%2520face%2520similarity%2520prediction%2520and%2520attribute-based%250Aface%2520classification%2520tasks%2520over%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerFace%3A%20Metric%20Learning%20in%20Perceptual%20Facial%20Similarity%20for%20Enhanced%0A%20%20Face%20Anonymization&entry.906535625=Haruka%20Kumagai%20and%20Leslie%20W%C3%B6hler%20and%20Satoshi%20Ikehata%20and%20Kiyoharu%20Aizawa&entry.1292438233=%20%20In%20response%20to%20rising%20societal%20awareness%20of%20privacy%20concerns%2C%20face%0Aanonymization%20techniques%20have%20advanced%2C%20including%20the%20emergence%20of%0Aface-swapping%20methods%20that%20replace%20one%20identity%20with%20another.%20Achieving%20a%0Abalance%20between%20anonymity%20and%20naturalness%20in%20face%20swapping%20requires%20careful%0Aselection%20of%20identities%3A%20overly%20similar%20faces%20compromise%20anonymity%2C%20while%0Adissimilar%20ones%20reduce%20naturalness.%20Existing%20models%2C%20however%2C%20focus%20on%20binary%0Aidentity%20classification%20%22the%20same%20person%20or%20not%22%2C%20making%20it%20difficult%20to%0Ameasure%20nuanced%20similarities%20such%20as%20%22completely%20different%22%20versus%20%22highly%0Asimilar%20but%20different.%22%20This%20paper%20proposes%20a%20human-perception-based%20face%0Asimilarity%20metric%2C%20creating%20a%20dataset%20of%206%2C400%20triplet%20annotations%20and%20metric%0Alearning%20to%20predict%20the%20similarity.%20Experimental%20results%20demonstrate%0Asignificant%20improvements%20in%20both%20face%20similarity%20prediction%20and%20attribute-based%0Aface%20classification%20tasks%20over%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20281v1&entry.124074799=Read"},
{"title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective", "author": "Jing Xiong and Jianghan Shen and Fanghua Ye and Chaofan Tao and Zhongwei Wan and Jianqiao Lu and Xun Wu and Chuanyang Zheng and Zhijiang Guo and Min Yang and Lingpeng Kong and Ngai Wong", "abstract": "  Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.\n", "link": "http://arxiv.org/abs/2410.03090v2", "date": "2025-09-24", "relevancy": 2.0275, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5161}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNComp%3A%20Can%20Matrix%20Entropy%20Uncover%20Sparsity%3F%20--%20A%20Compressor%20Design%20from%0A%20%20an%20Uncertainty-Aware%20Perspective&body=Title%3A%20UNComp%3A%20Can%20Matrix%20Entropy%20Uncover%20Sparsity%3F%20--%20A%20Compressor%20Design%20from%0A%20%20an%20Uncertainty-Aware%20Perspective%0AAuthor%3A%20Jing%20Xiong%20and%20Jianghan%20Shen%20and%20Fanghua%20Ye%20and%20Chaofan%20Tao%20and%20Zhongwei%20Wan%20and%20Jianqiao%20Lu%20and%20Xun%20Wu%20and%20Chuanyang%20Zheng%20and%20Zhijiang%20Guo%20and%20Min%20Yang%20and%20Lingpeng%20Kong%20and%20Ngai%20Wong%0AAbstract%3A%20%20%20Deploying%20large%20language%20models%20%28LLMs%29%20for%20long-context%20inference%20remains%0Achallenging%20due%20to%20their%20substantial%20memory%20and%20computational%20demands.%20While%0Atechniques%20such%20as%20Key-Value%20%28KV%29%20cache%20compression%20are%20designed%20to%20reduce%0Amemory%20usage%2C%20they%20often%20neglect%20the%20structured%20sparsity%20inherent%20in%20the%0Arelationship%20between%20hidden%20states%20and%20their%20corresponding%20KV%20cache.%20In%20this%0Awork%2C%20we%20explore%20the%20role%20of%20uncertainty%20as%20a%20potential%20indicator%20of%20sparsity%0Awithin%20LLMs.%20We%20propose%20UNComp%2C%20an%20uncertainty-aware%20framework%20that%20leverages%0Atruncated%20matrix%20entropy%20to%20identify%20areas%20of%20low%20information%20content%2C%20thereby%0Arevealing%20sparsity%20patterns%20that%20can%20be%20used%20for%20adaptive%20compression.%20Unlike%0Atraditional%20methods%20that%20apply%20uniform%20compression%2C%20UNComp%20dynamically%20adjusts%0Aits%20approach%20to%20compression%2C%20guided%20by%20uncertainty%20measures%20that%20reflect%20the%0Aimportance%20of%20various%20model%20components.%20Our%20analysis%20shows%20that%20sparsity%0Apatterns%2C%20when%20derived%20from%20uncertainty%20estimates%2C%20can%20be%20exploited%20to%20reveal%0Aspecial%20long-range%20dependencies%2C%20such%20as%20retrieval%20heads%20and%20retrieval%20layers.%0AThis%20perspective%20not%20only%20enhances%20our%20understanding%20of%20how%20compression%20can%20be%0Aoptimized%20but%20also%20provides%20new%20insights%20into%20the%20inherent%20sparsity%20of%20LLMs%0Aduring%20long-context%20inference.%20By%20focusing%20on%20uncertainty%20to%20analyze%20the%0Asparsity%20pattern%20in%20detail%2C%20UNComp%20reduces%20the%20KV%20cache%20size%20to%204.74%25%20of%20the%0Aoriginal%2C%20achieves%20a%206%25%20prefill%20speedup%2C%20and%20improves%20throughput%20by%206.4x%20-%20not%0Aonly%20delivering%20strong%20lossless%20compression%20performance%2C%20but%20also%20validating%0Athe%20effectiveness%20of%20the%20underlying%20theoretical%20tool.%20We%20release%20the%20code%20at%0Ahttps%3A//github.com/menik1126/UNComp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNComp%253A%2520Can%2520Matrix%2520Entropy%2520Uncover%2520Sparsity%253F%2520--%2520A%2520Compressor%2520Design%2520from%250A%2520%2520an%2520Uncertainty-Aware%2520Perspective%26entry.906535625%3DJing%2520Xiong%2520and%2520Jianghan%2520Shen%2520and%2520Fanghua%2520Ye%2520and%2520Chaofan%2520Tao%2520and%2520Zhongwei%2520Wan%2520and%2520Jianqiao%2520Lu%2520and%2520Xun%2520Wu%2520and%2520Chuanyang%2520Zheng%2520and%2520Zhijiang%2520Guo%2520and%2520Min%2520Yang%2520and%2520Lingpeng%2520Kong%2520and%2520Ngai%2520Wong%26entry.1292438233%3D%2520%2520Deploying%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520long-context%2520inference%2520remains%250Achallenging%2520due%2520to%2520their%2520substantial%2520memory%2520and%2520computational%2520demands.%2520While%250Atechniques%2520such%2520as%2520Key-Value%2520%2528KV%2529%2520cache%2520compression%2520are%2520designed%2520to%2520reduce%250Amemory%2520usage%252C%2520they%2520often%2520neglect%2520the%2520structured%2520sparsity%2520inherent%2520in%2520the%250Arelationship%2520between%2520hidden%2520states%2520and%2520their%2520corresponding%2520KV%2520cache.%2520In%2520this%250Awork%252C%2520we%2520explore%2520the%2520role%2520of%2520uncertainty%2520as%2520a%2520potential%2520indicator%2520of%2520sparsity%250Awithin%2520LLMs.%2520We%2520propose%2520UNComp%252C%2520an%2520uncertainty-aware%2520framework%2520that%2520leverages%250Atruncated%2520matrix%2520entropy%2520to%2520identify%2520areas%2520of%2520low%2520information%2520content%252C%2520thereby%250Arevealing%2520sparsity%2520patterns%2520that%2520can%2520be%2520used%2520for%2520adaptive%2520compression.%2520Unlike%250Atraditional%2520methods%2520that%2520apply%2520uniform%2520compression%252C%2520UNComp%2520dynamically%2520adjusts%250Aits%2520approach%2520to%2520compression%252C%2520guided%2520by%2520uncertainty%2520measures%2520that%2520reflect%2520the%250Aimportance%2520of%2520various%2520model%2520components.%2520Our%2520analysis%2520shows%2520that%2520sparsity%250Apatterns%252C%2520when%2520derived%2520from%2520uncertainty%2520estimates%252C%2520can%2520be%2520exploited%2520to%2520reveal%250Aspecial%2520long-range%2520dependencies%252C%2520such%2520as%2520retrieval%2520heads%2520and%2520retrieval%2520layers.%250AThis%2520perspective%2520not%2520only%2520enhances%2520our%2520understanding%2520of%2520how%2520compression%2520can%2520be%250Aoptimized%2520but%2520also%2520provides%2520new%2520insights%2520into%2520the%2520inherent%2520sparsity%2520of%2520LLMs%250Aduring%2520long-context%2520inference.%2520By%2520focusing%2520on%2520uncertainty%2520to%2520analyze%2520the%250Asparsity%2520pattern%2520in%2520detail%252C%2520UNComp%2520reduces%2520the%2520KV%2520cache%2520size%2520to%25204.74%2525%2520of%2520the%250Aoriginal%252C%2520achieves%2520a%25206%2525%2520prefill%2520speedup%252C%2520and%2520improves%2520throughput%2520by%25206.4x%2520-%2520not%250Aonly%2520delivering%2520strong%2520lossless%2520compression%2520performance%252C%2520but%2520also%2520validating%250Athe%2520effectiveness%2520of%2520the%2520underlying%2520theoretical%2520tool.%2520We%2520release%2520the%2520code%2520at%250Ahttps%253A//github.com/menik1126/UNComp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNComp%3A%20Can%20Matrix%20Entropy%20Uncover%20Sparsity%3F%20--%20A%20Compressor%20Design%20from%0A%20%20an%20Uncertainty-Aware%20Perspective&entry.906535625=Jing%20Xiong%20and%20Jianghan%20Shen%20and%20Fanghua%20Ye%20and%20Chaofan%20Tao%20and%20Zhongwei%20Wan%20and%20Jianqiao%20Lu%20and%20Xun%20Wu%20and%20Chuanyang%20Zheng%20and%20Zhijiang%20Guo%20and%20Min%20Yang%20and%20Lingpeng%20Kong%20and%20Ngai%20Wong&entry.1292438233=%20%20Deploying%20large%20language%20models%20%28LLMs%29%20for%20long-context%20inference%20remains%0Achallenging%20due%20to%20their%20substantial%20memory%20and%20computational%20demands.%20While%0Atechniques%20such%20as%20Key-Value%20%28KV%29%20cache%20compression%20are%20designed%20to%20reduce%0Amemory%20usage%2C%20they%20often%20neglect%20the%20structured%20sparsity%20inherent%20in%20the%0Arelationship%20between%20hidden%20states%20and%20their%20corresponding%20KV%20cache.%20In%20this%0Awork%2C%20we%20explore%20the%20role%20of%20uncertainty%20as%20a%20potential%20indicator%20of%20sparsity%0Awithin%20LLMs.%20We%20propose%20UNComp%2C%20an%20uncertainty-aware%20framework%20that%20leverages%0Atruncated%20matrix%20entropy%20to%20identify%20areas%20of%20low%20information%20content%2C%20thereby%0Arevealing%20sparsity%20patterns%20that%20can%20be%20used%20for%20adaptive%20compression.%20Unlike%0Atraditional%20methods%20that%20apply%20uniform%20compression%2C%20UNComp%20dynamically%20adjusts%0Aits%20approach%20to%20compression%2C%20guided%20by%20uncertainty%20measures%20that%20reflect%20the%0Aimportance%20of%20various%20model%20components.%20Our%20analysis%20shows%20that%20sparsity%0Apatterns%2C%20when%20derived%20from%20uncertainty%20estimates%2C%20can%20be%20exploited%20to%20reveal%0Aspecial%20long-range%20dependencies%2C%20such%20as%20retrieval%20heads%20and%20retrieval%20layers.%0AThis%20perspective%20not%20only%20enhances%20our%20understanding%20of%20how%20compression%20can%20be%0Aoptimized%20but%20also%20provides%20new%20insights%20into%20the%20inherent%20sparsity%20of%20LLMs%0Aduring%20long-context%20inference.%20By%20focusing%20on%20uncertainty%20to%20analyze%20the%0Asparsity%20pattern%20in%20detail%2C%20UNComp%20reduces%20the%20KV%20cache%20size%20to%204.74%25%20of%20the%0Aoriginal%2C%20achieves%20a%206%25%20prefill%20speedup%2C%20and%20improves%20throughput%20by%206.4x%20-%20not%0Aonly%20delivering%20strong%20lossless%20compression%20performance%2C%20but%20also%20validating%0Athe%20effectiveness%20of%20the%20underlying%20theoretical%20tool.%20We%20release%20the%20code%20at%0Ahttps%3A//github.com/menik1126/UNComp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03090v2&entry.124074799=Read"},
{"title": "DRES: Benchmarking LLMs for Disfluency Removal", "author": "Maria Teleki and Sai Janjur and Haoran Liu and Oliver Grabner and Ketan Verma and Thomas Docog and Xiangjue Dong and Lingfeng Shi and Cong Wang and Stephanie Birkelbach and Jason Kim and Yin Zhang and James Caverlee", "abstract": "  Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems.\n", "link": "http://arxiv.org/abs/2509.20321v1", "date": "2025-09-24", "relevancy": 2.0117, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.504}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRES%3A%20Benchmarking%20LLMs%20for%20Disfluency%20Removal&body=Title%3A%20DRES%3A%20Benchmarking%20LLMs%20for%20Disfluency%20Removal%0AAuthor%3A%20Maria%20Teleki%20and%20Sai%20Janjur%20and%20Haoran%20Liu%20and%20Oliver%20Grabner%20and%20Ketan%20Verma%20and%20Thomas%20Docog%20and%20Xiangjue%20Dong%20and%20Lingfeng%20Shi%20and%20Cong%20Wang%20and%20Stephanie%20Birkelbach%20and%20Jason%20Kim%20and%20Yin%20Zhang%20and%20James%20Caverlee%0AAbstract%3A%20%20%20Disfluencies%20--%20such%20as%20%22um%2C%22%20%22uh%2C%22%20interjections%2C%20parentheticals%2C%20and%20edited%0Astatements%20--%20remain%20a%20persistent%20challenge%20for%20speech-driven%20systems%2C%0Adegrading%20accuracy%20in%20command%20interpretation%2C%20summarization%2C%20and%20conversational%0Aagents.%20We%20introduce%20DRES%20%28Disfluency%20Removal%20Evaluation%20Suite%29%2C%20a%20controlled%0Atext-level%20benchmark%20that%20establishes%20a%20reproducible%20semantic%20upper%20bound%20for%0Athis%20task.%20DRES%20builds%20on%20human-annotated%20Switchboard%20transcripts%2C%20isolating%0Adisfluency%20removal%20from%20ASR%20errors%20and%20acoustic%20variability.%20We%20systematically%0Aevaluate%20proprietary%20and%20open-source%20LLMs%20across%20scales%2C%20prompting%20strategies%2C%0Aand%20architectures.%20Our%20results%20reveal%20that%20%28i%29%20simple%20segmentation%20consistently%0Aimproves%20performance%2C%20even%20for%20long-context%20models%3B%20%28ii%29%20reasoning-oriented%0Amodels%20tend%20to%20over-delete%20fluent%20tokens%3B%20and%20%28iii%29%20fine-tuning%20achieves%20near%0Astate-of-the-art%20precision%20and%20recall%20but%20harms%20generalization%20abilities.%20We%0Afurther%20present%20a%20set%20of%20LLM-specific%20error%20modes%20and%20offer%20nine%20practical%0Arecommendations%20%28R1-R9%29%20for%20deploying%20disfluency%20removal%20in%20speech-driven%0Apipelines.%20DRES%20provides%20a%20reproducible%2C%20model-agnostic%20foundation%20for%0Aadvancing%20robust%20spoken-language%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRES%253A%2520Benchmarking%2520LLMs%2520for%2520Disfluency%2520Removal%26entry.906535625%3DMaria%2520Teleki%2520and%2520Sai%2520Janjur%2520and%2520Haoran%2520Liu%2520and%2520Oliver%2520Grabner%2520and%2520Ketan%2520Verma%2520and%2520Thomas%2520Docog%2520and%2520Xiangjue%2520Dong%2520and%2520Lingfeng%2520Shi%2520and%2520Cong%2520Wang%2520and%2520Stephanie%2520Birkelbach%2520and%2520Jason%2520Kim%2520and%2520Yin%2520Zhang%2520and%2520James%2520Caverlee%26entry.1292438233%3D%2520%2520Disfluencies%2520--%2520such%2520as%2520%2522um%252C%2522%2520%2522uh%252C%2522%2520interjections%252C%2520parentheticals%252C%2520and%2520edited%250Astatements%2520--%2520remain%2520a%2520persistent%2520challenge%2520for%2520speech-driven%2520systems%252C%250Adegrading%2520accuracy%2520in%2520command%2520interpretation%252C%2520summarization%252C%2520and%2520conversational%250Aagents.%2520We%2520introduce%2520DRES%2520%2528Disfluency%2520Removal%2520Evaluation%2520Suite%2529%252C%2520a%2520controlled%250Atext-level%2520benchmark%2520that%2520establishes%2520a%2520reproducible%2520semantic%2520upper%2520bound%2520for%250Athis%2520task.%2520DRES%2520builds%2520on%2520human-annotated%2520Switchboard%2520transcripts%252C%2520isolating%250Adisfluency%2520removal%2520from%2520ASR%2520errors%2520and%2520acoustic%2520variability.%2520We%2520systematically%250Aevaluate%2520proprietary%2520and%2520open-source%2520LLMs%2520across%2520scales%252C%2520prompting%2520strategies%252C%250Aand%2520architectures.%2520Our%2520results%2520reveal%2520that%2520%2528i%2529%2520simple%2520segmentation%2520consistently%250Aimproves%2520performance%252C%2520even%2520for%2520long-context%2520models%253B%2520%2528ii%2529%2520reasoning-oriented%250Amodels%2520tend%2520to%2520over-delete%2520fluent%2520tokens%253B%2520and%2520%2528iii%2529%2520fine-tuning%2520achieves%2520near%250Astate-of-the-art%2520precision%2520and%2520recall%2520but%2520harms%2520generalization%2520abilities.%2520We%250Afurther%2520present%2520a%2520set%2520of%2520LLM-specific%2520error%2520modes%2520and%2520offer%2520nine%2520practical%250Arecommendations%2520%2528R1-R9%2529%2520for%2520deploying%2520disfluency%2520removal%2520in%2520speech-driven%250Apipelines.%2520DRES%2520provides%2520a%2520reproducible%252C%2520model-agnostic%2520foundation%2520for%250Aadvancing%2520robust%2520spoken-language%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRES%3A%20Benchmarking%20LLMs%20for%20Disfluency%20Removal&entry.906535625=Maria%20Teleki%20and%20Sai%20Janjur%20and%20Haoran%20Liu%20and%20Oliver%20Grabner%20and%20Ketan%20Verma%20and%20Thomas%20Docog%20and%20Xiangjue%20Dong%20and%20Lingfeng%20Shi%20and%20Cong%20Wang%20and%20Stephanie%20Birkelbach%20and%20Jason%20Kim%20and%20Yin%20Zhang%20and%20James%20Caverlee&entry.1292438233=%20%20Disfluencies%20--%20such%20as%20%22um%2C%22%20%22uh%2C%22%20interjections%2C%20parentheticals%2C%20and%20edited%0Astatements%20--%20remain%20a%20persistent%20challenge%20for%20speech-driven%20systems%2C%0Adegrading%20accuracy%20in%20command%20interpretation%2C%20summarization%2C%20and%20conversational%0Aagents.%20We%20introduce%20DRES%20%28Disfluency%20Removal%20Evaluation%20Suite%29%2C%20a%20controlled%0Atext-level%20benchmark%20that%20establishes%20a%20reproducible%20semantic%20upper%20bound%20for%0Athis%20task.%20DRES%20builds%20on%20human-annotated%20Switchboard%20transcripts%2C%20isolating%0Adisfluency%20removal%20from%20ASR%20errors%20and%20acoustic%20variability.%20We%20systematically%0Aevaluate%20proprietary%20and%20open-source%20LLMs%20across%20scales%2C%20prompting%20strategies%2C%0Aand%20architectures.%20Our%20results%20reveal%20that%20%28i%29%20simple%20segmentation%20consistently%0Aimproves%20performance%2C%20even%20for%20long-context%20models%3B%20%28ii%29%20reasoning-oriented%0Amodels%20tend%20to%20over-delete%20fluent%20tokens%3B%20and%20%28iii%29%20fine-tuning%20achieves%20near%0Astate-of-the-art%20precision%20and%20recall%20but%20harms%20generalization%20abilities.%20We%0Afurther%20present%20a%20set%20of%20LLM-specific%20error%20modes%20and%20offer%20nine%20practical%0Arecommendations%20%28R1-R9%29%20for%20deploying%20disfluency%20removal%20in%20speech-driven%0Apipelines.%20DRES%20provides%20a%20reproducible%2C%20model-agnostic%20foundation%20for%0Aadvancing%20robust%20spoken-language%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20321v1&entry.124074799=Read"},
{"title": "SIM-CoT: Supervised Implicit Chain-of-Thought", "author": "Xilin Wei and Xiaoran Liu and Yuhang Zang and Xiaoyi Dong and Yuhang Cao and Jiaqi Wang and Xipeng Qiu and Dahua Lin", "abstract": "  Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient\nalternative to explicit CoT reasoning in Large Language Models (LLMs), but a\npersistent performance gap has limited the application of implicit CoT. We\nidentify a core latent instability issue by scaling the computational budget of\nimplicit CoT approaches: as we increase the number of implicit reasoning tokens\nto enhance performance, the training process often becomes unstable and\ncollapses. Our analysis reveals that this instability arises from the latent\nrepresentations becoming homogeneous and losing their semantic diversity, a\nfailure caused by insufficient step-level supervision in existing implicit CoT\napproaches. To address this issue, we propose SIM-CoT, a plug-and-play training\nmodule that introduces step-level supervision to stabilize and enrich the\nlatent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder\nduring training to align each implicit token with its corresponding explicit\nreasoning step, ensuring that latent states capture distinct and meaningful\ninformation. The proposed auxiliary decoder is removed during inference,\npreserving the computational efficiency of implicit CoT methods with no added\noverhead. In addition, the auxiliary decoder affords interpretability of\nimplicit reasoning by projecting each latent token onto an explicit reasoning\nvocabulary, enabling per-step visualization of semantic roles and diagnosis.\nSIM-CoT significantly enhances both the in-domain accuracy and out-of-domain\nstability of various implicit CoT methods, boosting baselines like Coconut by\n+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong\nscalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%\nwith 2.3\\times greater token efficiency, while substantially closing the\nperformance gap on larger models like LLaMA-3.1 8B.\n", "link": "http://arxiv.org/abs/2509.20317v1", "date": "2025-09-24", "relevancy": 1.9814, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIM-CoT%3A%20Supervised%20Implicit%20Chain-of-Thought&body=Title%3A%20SIM-CoT%3A%20Supervised%20Implicit%20Chain-of-Thought%0AAuthor%3A%20Xilin%20Wei%20and%20Xiaoran%20Liu%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Jiaqi%20Wang%20and%20Xipeng%20Qiu%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20Implicit%20Chain-of-Thought%20%28CoT%29%20methods%20present%20a%20promising%2C%20token-efficient%0Aalternative%20to%20explicit%20CoT%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29%2C%20but%20a%0Apersistent%20performance%20gap%20has%20limited%20the%20application%20of%20implicit%20CoT.%20We%0Aidentify%20a%20core%20latent%20instability%20issue%20by%20scaling%20the%20computational%20budget%20of%0Aimplicit%20CoT%20approaches%3A%20as%20we%20increase%20the%20number%20of%20implicit%20reasoning%20tokens%0Ato%20enhance%20performance%2C%20the%20training%20process%20often%20becomes%20unstable%20and%0Acollapses.%20Our%20analysis%20reveals%20that%20this%20instability%20arises%20from%20the%20latent%0Arepresentations%20becoming%20homogeneous%20and%20losing%20their%20semantic%20diversity%2C%20a%0Afailure%20caused%20by%20insufficient%20step-level%20supervision%20in%20existing%20implicit%20CoT%0Aapproaches.%20To%20address%20this%20issue%2C%20we%20propose%20SIM-CoT%2C%20a%20plug-and-play%20training%0Amodule%20that%20introduces%20step-level%20supervision%20to%20stabilize%20and%20enrich%20the%0Alatent%20reasoning%20space.%20Specifically%2C%20SIM-CoT%20employs%20an%20auxiliary%20decoder%0Aduring%20training%20to%20align%20each%20implicit%20token%20with%20its%20corresponding%20explicit%0Areasoning%20step%2C%20ensuring%20that%20latent%20states%20capture%20distinct%20and%20meaningful%0Ainformation.%20The%20proposed%20auxiliary%20decoder%20is%20removed%20during%20inference%2C%0Apreserving%20the%20computational%20efficiency%20of%20implicit%20CoT%20methods%20with%20no%20added%0Aoverhead.%20In%20addition%2C%20the%20auxiliary%20decoder%20affords%20interpretability%20of%0Aimplicit%20reasoning%20by%20projecting%20each%20latent%20token%20onto%20an%20explicit%20reasoning%0Avocabulary%2C%20enabling%20per-step%20visualization%20of%20semantic%20roles%20and%20diagnosis.%0ASIM-CoT%20significantly%20enhances%20both%20the%20in-domain%20accuracy%20and%20out-of-domain%0Astability%20of%20various%20implicit%20CoT%20methods%2C%20boosting%20baselines%20like%20Coconut%20by%0A%2B8.2%25%20on%20GPT-2%20and%20CODI%20by%20%2B3.0%25%20on%20LLaMA-3.1%208B.%20Demonstrating%20strong%0Ascalability%2C%20SIM-CoT%20also%20surpasses%20the%20explicit%20CoT%20baseline%20on%20GPT-2%20by%202.1%25%0Awith%202.3%5Ctimes%20greater%20token%20efficiency%2C%20while%20substantially%20closing%20the%0Aperformance%20gap%20on%20larger%20models%20like%20LLaMA-3.1%208B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIM-CoT%253A%2520Supervised%2520Implicit%2520Chain-of-Thought%26entry.906535625%3DXilin%2520Wei%2520and%2520Xiaoran%2520Liu%2520and%2520Yuhang%2520Zang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Cao%2520and%2520Jiaqi%2520Wang%2520and%2520Xipeng%2520Qiu%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520Implicit%2520Chain-of-Thought%2520%2528CoT%2529%2520methods%2520present%2520a%2520promising%252C%2520token-efficient%250Aalternative%2520to%2520explicit%2520CoT%2520reasoning%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520but%2520a%250Apersistent%2520performance%2520gap%2520has%2520limited%2520the%2520application%2520of%2520implicit%2520CoT.%2520We%250Aidentify%2520a%2520core%2520latent%2520instability%2520issue%2520by%2520scaling%2520the%2520computational%2520budget%2520of%250Aimplicit%2520CoT%2520approaches%253A%2520as%2520we%2520increase%2520the%2520number%2520of%2520implicit%2520reasoning%2520tokens%250Ato%2520enhance%2520performance%252C%2520the%2520training%2520process%2520often%2520becomes%2520unstable%2520and%250Acollapses.%2520Our%2520analysis%2520reveals%2520that%2520this%2520instability%2520arises%2520from%2520the%2520latent%250Arepresentations%2520becoming%2520homogeneous%2520and%2520losing%2520their%2520semantic%2520diversity%252C%2520a%250Afailure%2520caused%2520by%2520insufficient%2520step-level%2520supervision%2520in%2520existing%2520implicit%2520CoT%250Aapproaches.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520SIM-CoT%252C%2520a%2520plug-and-play%2520training%250Amodule%2520that%2520introduces%2520step-level%2520supervision%2520to%2520stabilize%2520and%2520enrich%2520the%250Alatent%2520reasoning%2520space.%2520Specifically%252C%2520SIM-CoT%2520employs%2520an%2520auxiliary%2520decoder%250Aduring%2520training%2520to%2520align%2520each%2520implicit%2520token%2520with%2520its%2520corresponding%2520explicit%250Areasoning%2520step%252C%2520ensuring%2520that%2520latent%2520states%2520capture%2520distinct%2520and%2520meaningful%250Ainformation.%2520The%2520proposed%2520auxiliary%2520decoder%2520is%2520removed%2520during%2520inference%252C%250Apreserving%2520the%2520computational%2520efficiency%2520of%2520implicit%2520CoT%2520methods%2520with%2520no%2520added%250Aoverhead.%2520In%2520addition%252C%2520the%2520auxiliary%2520decoder%2520affords%2520interpretability%2520of%250Aimplicit%2520reasoning%2520by%2520projecting%2520each%2520latent%2520token%2520onto%2520an%2520explicit%2520reasoning%250Avocabulary%252C%2520enabling%2520per-step%2520visualization%2520of%2520semantic%2520roles%2520and%2520diagnosis.%250ASIM-CoT%2520significantly%2520enhances%2520both%2520the%2520in-domain%2520accuracy%2520and%2520out-of-domain%250Astability%2520of%2520various%2520implicit%2520CoT%2520methods%252C%2520boosting%2520baselines%2520like%2520Coconut%2520by%250A%252B8.2%2525%2520on%2520GPT-2%2520and%2520CODI%2520by%2520%252B3.0%2525%2520on%2520LLaMA-3.1%25208B.%2520Demonstrating%2520strong%250Ascalability%252C%2520SIM-CoT%2520also%2520surpasses%2520the%2520explicit%2520CoT%2520baseline%2520on%2520GPT-2%2520by%25202.1%2525%250Awith%25202.3%255Ctimes%2520greater%2520token%2520efficiency%252C%2520while%2520substantially%2520closing%2520the%250Aperformance%2520gap%2520on%2520larger%2520models%2520like%2520LLaMA-3.1%25208B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIM-CoT%3A%20Supervised%20Implicit%20Chain-of-Thought&entry.906535625=Xilin%20Wei%20and%20Xiaoran%20Liu%20and%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Jiaqi%20Wang%20and%20Xipeng%20Qiu%20and%20Dahua%20Lin&entry.1292438233=%20%20Implicit%20Chain-of-Thought%20%28CoT%29%20methods%20present%20a%20promising%2C%20token-efficient%0Aalternative%20to%20explicit%20CoT%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29%2C%20but%20a%0Apersistent%20performance%20gap%20has%20limited%20the%20application%20of%20implicit%20CoT.%20We%0Aidentify%20a%20core%20latent%20instability%20issue%20by%20scaling%20the%20computational%20budget%20of%0Aimplicit%20CoT%20approaches%3A%20as%20we%20increase%20the%20number%20of%20implicit%20reasoning%20tokens%0Ato%20enhance%20performance%2C%20the%20training%20process%20often%20becomes%20unstable%20and%0Acollapses.%20Our%20analysis%20reveals%20that%20this%20instability%20arises%20from%20the%20latent%0Arepresentations%20becoming%20homogeneous%20and%20losing%20their%20semantic%20diversity%2C%20a%0Afailure%20caused%20by%20insufficient%20step-level%20supervision%20in%20existing%20implicit%20CoT%0Aapproaches.%20To%20address%20this%20issue%2C%20we%20propose%20SIM-CoT%2C%20a%20plug-and-play%20training%0Amodule%20that%20introduces%20step-level%20supervision%20to%20stabilize%20and%20enrich%20the%0Alatent%20reasoning%20space.%20Specifically%2C%20SIM-CoT%20employs%20an%20auxiliary%20decoder%0Aduring%20training%20to%20align%20each%20implicit%20token%20with%20its%20corresponding%20explicit%0Areasoning%20step%2C%20ensuring%20that%20latent%20states%20capture%20distinct%20and%20meaningful%0Ainformation.%20The%20proposed%20auxiliary%20decoder%20is%20removed%20during%20inference%2C%0Apreserving%20the%20computational%20efficiency%20of%20implicit%20CoT%20methods%20with%20no%20added%0Aoverhead.%20In%20addition%2C%20the%20auxiliary%20decoder%20affords%20interpretability%20of%0Aimplicit%20reasoning%20by%20projecting%20each%20latent%20token%20onto%20an%20explicit%20reasoning%0Avocabulary%2C%20enabling%20per-step%20visualization%20of%20semantic%20roles%20and%20diagnosis.%0ASIM-CoT%20significantly%20enhances%20both%20the%20in-domain%20accuracy%20and%20out-of-domain%0Astability%20of%20various%20implicit%20CoT%20methods%2C%20boosting%20baselines%20like%20Coconut%20by%0A%2B8.2%25%20on%20GPT-2%20and%20CODI%20by%20%2B3.0%25%20on%20LLaMA-3.1%208B.%20Demonstrating%20strong%0Ascalability%2C%20SIM-CoT%20also%20surpasses%20the%20explicit%20CoT%20baseline%20on%20GPT-2%20by%202.1%25%0Awith%202.3%5Ctimes%20greater%20token%20efficiency%2C%20while%20substantially%20closing%20the%0Aperformance%20gap%20on%20larger%20models%20like%20LLaMA-3.1%208B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20317v1&entry.124074799=Read"},
{"title": "Statistical Inference Leveraging Synthetic Data with Distribution-Free\n  Guarantees", "author": "Meshi Bashari and Yonghoon Lee and Roy Maor Lotan and Edgar Dobriban and Yaniv Romano", "abstract": "  The rapid proliferation of high-quality synthetic data -- generated by\nadvanced AI models or collected as auxiliary data from related tasks --\npresents both opportunities and challenges for statistical inference. This\npaper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that\nwraps around any statistical inference procedure to safely enhance sample\nefficiency by combining synthetic and real data. Our framework leverages\nhigh-quality synthetic data to boost statistical power, yet adaptively defaults\nto the standard inference method using only real data when synthetic data is of\nlow quality. The error of our method remains below a user-specified bound\nwithout any distributional assumptions on the synthetic data, and decreases as\nthe quality of the synthetic data improves. This flexibility enables seamless\nintegration with conformal prediction, risk control, hypothesis testing, and\nmultiple testing procedures, all without modifying the base inference method.\nWe demonstrate the benefits of our method on challenging tasks with limited\nlabeled data, including AlphaFold protein structure prediction, and comparing\nlarge reasoning models on complex math problems.\n", "link": "http://arxiv.org/abs/2509.20345v1", "date": "2025-09-24", "relevancy": 1.9523, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.483}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20Inference%20Leveraging%20Synthetic%20Data%20with%20Distribution-Free%0A%20%20Guarantees&body=Title%3A%20Statistical%20Inference%20Leveraging%20Synthetic%20Data%20with%20Distribution-Free%0A%20%20Guarantees%0AAuthor%3A%20Meshi%20Bashari%20and%20Yonghoon%20Lee%20and%20Roy%20Maor%20Lotan%20and%20Edgar%20Dobriban%20and%20Yaniv%20Romano%0AAbstract%3A%20%20%20The%20rapid%20proliferation%20of%20high-quality%20synthetic%20data%20--%20generated%20by%0Aadvanced%20AI%20models%20or%20collected%20as%20auxiliary%20data%20from%20related%20tasks%20--%0Apresents%20both%20opportunities%20and%20challenges%20for%20statistical%20inference.%20This%0Apaper%20introduces%20a%20GEneral%20Synthetic-Powered%20Inference%20%28GESPI%29%20framework%20that%0Awraps%20around%20any%20statistical%20inference%20procedure%20to%20safely%20enhance%20sample%0Aefficiency%20by%20combining%20synthetic%20and%20real%20data.%20Our%20framework%20leverages%0Ahigh-quality%20synthetic%20data%20to%20boost%20statistical%20power%2C%20yet%20adaptively%20defaults%0Ato%20the%20standard%20inference%20method%20using%20only%20real%20data%20when%20synthetic%20data%20is%20of%0Alow%20quality.%20The%20error%20of%20our%20method%20remains%20below%20a%20user-specified%20bound%0Awithout%20any%20distributional%20assumptions%20on%20the%20synthetic%20data%2C%20and%20decreases%20as%0Athe%20quality%20of%20the%20synthetic%20data%20improves.%20This%20flexibility%20enables%20seamless%0Aintegration%20with%20conformal%20prediction%2C%20risk%20control%2C%20hypothesis%20testing%2C%20and%0Amultiple%20testing%20procedures%2C%20all%20without%20modifying%20the%20base%20inference%20method.%0AWe%20demonstrate%20the%20benefits%20of%20our%20method%20on%20challenging%20tasks%20with%20limited%0Alabeled%20data%2C%20including%20AlphaFold%20protein%20structure%20prediction%2C%20and%20comparing%0Alarge%20reasoning%20models%20on%20complex%20math%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520Inference%2520Leveraging%2520Synthetic%2520Data%2520with%2520Distribution-Free%250A%2520%2520Guarantees%26entry.906535625%3DMeshi%2520Bashari%2520and%2520Yonghoon%2520Lee%2520and%2520Roy%2520Maor%2520Lotan%2520and%2520Edgar%2520Dobriban%2520and%2520Yaniv%2520Romano%26entry.1292438233%3D%2520%2520The%2520rapid%2520proliferation%2520of%2520high-quality%2520synthetic%2520data%2520--%2520generated%2520by%250Aadvanced%2520AI%2520models%2520or%2520collected%2520as%2520auxiliary%2520data%2520from%2520related%2520tasks%2520--%250Apresents%2520both%2520opportunities%2520and%2520challenges%2520for%2520statistical%2520inference.%2520This%250Apaper%2520introduces%2520a%2520GEneral%2520Synthetic-Powered%2520Inference%2520%2528GESPI%2529%2520framework%2520that%250Awraps%2520around%2520any%2520statistical%2520inference%2520procedure%2520to%2520safely%2520enhance%2520sample%250Aefficiency%2520by%2520combining%2520synthetic%2520and%2520real%2520data.%2520Our%2520framework%2520leverages%250Ahigh-quality%2520synthetic%2520data%2520to%2520boost%2520statistical%2520power%252C%2520yet%2520adaptively%2520defaults%250Ato%2520the%2520standard%2520inference%2520method%2520using%2520only%2520real%2520data%2520when%2520synthetic%2520data%2520is%2520of%250Alow%2520quality.%2520The%2520error%2520of%2520our%2520method%2520remains%2520below%2520a%2520user-specified%2520bound%250Awithout%2520any%2520distributional%2520assumptions%2520on%2520the%2520synthetic%2520data%252C%2520and%2520decreases%2520as%250Athe%2520quality%2520of%2520the%2520synthetic%2520data%2520improves.%2520This%2520flexibility%2520enables%2520seamless%250Aintegration%2520with%2520conformal%2520prediction%252C%2520risk%2520control%252C%2520hypothesis%2520testing%252C%2520and%250Amultiple%2520testing%2520procedures%252C%2520all%2520without%2520modifying%2520the%2520base%2520inference%2520method.%250AWe%2520demonstrate%2520the%2520benefits%2520of%2520our%2520method%2520on%2520challenging%2520tasks%2520with%2520limited%250Alabeled%2520data%252C%2520including%2520AlphaFold%2520protein%2520structure%2520prediction%252C%2520and%2520comparing%250Alarge%2520reasoning%2520models%2520on%2520complex%2520math%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20Inference%20Leveraging%20Synthetic%20Data%20with%20Distribution-Free%0A%20%20Guarantees&entry.906535625=Meshi%20Bashari%20and%20Yonghoon%20Lee%20and%20Roy%20Maor%20Lotan%20and%20Edgar%20Dobriban%20and%20Yaniv%20Romano&entry.1292438233=%20%20The%20rapid%20proliferation%20of%20high-quality%20synthetic%20data%20--%20generated%20by%0Aadvanced%20AI%20models%20or%20collected%20as%20auxiliary%20data%20from%20related%20tasks%20--%0Apresents%20both%20opportunities%20and%20challenges%20for%20statistical%20inference.%20This%0Apaper%20introduces%20a%20GEneral%20Synthetic-Powered%20Inference%20%28GESPI%29%20framework%20that%0Awraps%20around%20any%20statistical%20inference%20procedure%20to%20safely%20enhance%20sample%0Aefficiency%20by%20combining%20synthetic%20and%20real%20data.%20Our%20framework%20leverages%0Ahigh-quality%20synthetic%20data%20to%20boost%20statistical%20power%2C%20yet%20adaptively%20defaults%0Ato%20the%20standard%20inference%20method%20using%20only%20real%20data%20when%20synthetic%20data%20is%20of%0Alow%20quality.%20The%20error%20of%20our%20method%20remains%20below%20a%20user-specified%20bound%0Awithout%20any%20distributional%20assumptions%20on%20the%20synthetic%20data%2C%20and%20decreases%20as%0Athe%20quality%20of%20the%20synthetic%20data%20improves.%20This%20flexibility%20enables%20seamless%0Aintegration%20with%20conformal%20prediction%2C%20risk%20control%2C%20hypothesis%20testing%2C%20and%0Amultiple%20testing%20procedures%2C%20all%20without%20modifying%20the%20base%20inference%20method.%0AWe%20demonstrate%20the%20benefits%20of%20our%20method%20on%20challenging%20tasks%20with%20limited%0Alabeled%20data%2C%20including%20AlphaFold%20protein%20structure%20prediction%2C%20and%20comparing%0Alarge%20reasoning%20models%20on%20complex%20math%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20345v1&entry.124074799=Read"},
{"title": "White-Basilisk: A Hybrid Model for Code Vulnerability Detection", "author": "Ioannis Lamprou and Alexander Shevtsov and Ioannis Arapakis and Sotiris Ioannidis", "abstract": "  The proliferation of software vulnerabilities presents a significant\nchallenge to cybersecurity, necessitating more effective detection\nmethodologies. We introduce White-Basilisk, a novel approach to vulnerability\ndetection that demonstrates superior performance while challenging prevailing\nassumptions in AI model scaling. Utilizing an innovative architecture that\nintegrates Mamba layers, linear self-attention, and a Mixture of Experts\nframework, White-Basilisk achieves state-of-the-art results in vulnerability\ndetection tasks with a parameter count of only 200M. The model's capacity to\nprocess sequences of unprecedented length enables comprehensive analysis of\nextensive codebases in a single pass, surpassing the context limitations of\ncurrent Large Language Models (LLMs). White-Basilisk exhibits robust\nperformance on imbalanced, real-world datasets, while maintaining computational\nefficiency that facilitates deployment across diverse organizational scales.\nThis research not only establishes new benchmarks in code security but also\nprovides empirical evidence that compact, efficiently designed models can\noutperform larger counterparts in specialized tasks, potentially redefining\noptimization strategies in AI development for domain-specific applications.\n", "link": "http://arxiv.org/abs/2507.08540v3", "date": "2025-09-24", "relevancy": 1.9271, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20White-Basilisk%3A%20A%20Hybrid%20Model%20for%20Code%20Vulnerability%20Detection&body=Title%3A%20White-Basilisk%3A%20A%20Hybrid%20Model%20for%20Code%20Vulnerability%20Detection%0AAuthor%3A%20Ioannis%20Lamprou%20and%20Alexander%20Shevtsov%20and%20Ioannis%20Arapakis%20and%20Sotiris%20Ioannidis%0AAbstract%3A%20%20%20The%20proliferation%20of%20software%20vulnerabilities%20presents%20a%20significant%0Achallenge%20to%20cybersecurity%2C%20necessitating%20more%20effective%20detection%0Amethodologies.%20We%20introduce%20White-Basilisk%2C%20a%20novel%20approach%20to%20vulnerability%0Adetection%20that%20demonstrates%20superior%20performance%20while%20challenging%20prevailing%0Aassumptions%20in%20AI%20model%20scaling.%20Utilizing%20an%20innovative%20architecture%20that%0Aintegrates%20Mamba%20layers%2C%20linear%20self-attention%2C%20and%20a%20Mixture%20of%20Experts%0Aframework%2C%20White-Basilisk%20achieves%20state-of-the-art%20results%20in%20vulnerability%0Adetection%20tasks%20with%20a%20parameter%20count%20of%20only%20200M.%20The%20model%27s%20capacity%20to%0Aprocess%20sequences%20of%20unprecedented%20length%20enables%20comprehensive%20analysis%20of%0Aextensive%20codebases%20in%20a%20single%20pass%2C%20surpassing%20the%20context%20limitations%20of%0Acurrent%20Large%20Language%20Models%20%28LLMs%29.%20White-Basilisk%20exhibits%20robust%0Aperformance%20on%20imbalanced%2C%20real-world%20datasets%2C%20while%20maintaining%20computational%0Aefficiency%20that%20facilitates%20deployment%20across%20diverse%20organizational%20scales.%0AThis%20research%20not%20only%20establishes%20new%20benchmarks%20in%20code%20security%20but%20also%0Aprovides%20empirical%20evidence%20that%20compact%2C%20efficiently%20designed%20models%20can%0Aoutperform%20larger%20counterparts%20in%20specialized%20tasks%2C%20potentially%20redefining%0Aoptimization%20strategies%20in%20AI%20development%20for%20domain-specific%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08540v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhite-Basilisk%253A%2520A%2520Hybrid%2520Model%2520for%2520Code%2520Vulnerability%2520Detection%26entry.906535625%3DIoannis%2520Lamprou%2520and%2520Alexander%2520Shevtsov%2520and%2520Ioannis%2520Arapakis%2520and%2520Sotiris%2520Ioannidis%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520software%2520vulnerabilities%2520presents%2520a%2520significant%250Achallenge%2520to%2520cybersecurity%252C%2520necessitating%2520more%2520effective%2520detection%250Amethodologies.%2520We%2520introduce%2520White-Basilisk%252C%2520a%2520novel%2520approach%2520to%2520vulnerability%250Adetection%2520that%2520demonstrates%2520superior%2520performance%2520while%2520challenging%2520prevailing%250Aassumptions%2520in%2520AI%2520model%2520scaling.%2520Utilizing%2520an%2520innovative%2520architecture%2520that%250Aintegrates%2520Mamba%2520layers%252C%2520linear%2520self-attention%252C%2520and%2520a%2520Mixture%2520of%2520Experts%250Aframework%252C%2520White-Basilisk%2520achieves%2520state-of-the-art%2520results%2520in%2520vulnerability%250Adetection%2520tasks%2520with%2520a%2520parameter%2520count%2520of%2520only%2520200M.%2520The%2520model%2527s%2520capacity%2520to%250Aprocess%2520sequences%2520of%2520unprecedented%2520length%2520enables%2520comprehensive%2520analysis%2520of%250Aextensive%2520codebases%2520in%2520a%2520single%2520pass%252C%2520surpassing%2520the%2520context%2520limitations%2520of%250Acurrent%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520White-Basilisk%2520exhibits%2520robust%250Aperformance%2520on%2520imbalanced%252C%2520real-world%2520datasets%252C%2520while%2520maintaining%2520computational%250Aefficiency%2520that%2520facilitates%2520deployment%2520across%2520diverse%2520organizational%2520scales.%250AThis%2520research%2520not%2520only%2520establishes%2520new%2520benchmarks%2520in%2520code%2520security%2520but%2520also%250Aprovides%2520empirical%2520evidence%2520that%2520compact%252C%2520efficiently%2520designed%2520models%2520can%250Aoutperform%2520larger%2520counterparts%2520in%2520specialized%2520tasks%252C%2520potentially%2520redefining%250Aoptimization%2520strategies%2520in%2520AI%2520development%2520for%2520domain-specific%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08540v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=White-Basilisk%3A%20A%20Hybrid%20Model%20for%20Code%20Vulnerability%20Detection&entry.906535625=Ioannis%20Lamprou%20and%20Alexander%20Shevtsov%20and%20Ioannis%20Arapakis%20and%20Sotiris%20Ioannidis&entry.1292438233=%20%20The%20proliferation%20of%20software%20vulnerabilities%20presents%20a%20significant%0Achallenge%20to%20cybersecurity%2C%20necessitating%20more%20effective%20detection%0Amethodologies.%20We%20introduce%20White-Basilisk%2C%20a%20novel%20approach%20to%20vulnerability%0Adetection%20that%20demonstrates%20superior%20performance%20while%20challenging%20prevailing%0Aassumptions%20in%20AI%20model%20scaling.%20Utilizing%20an%20innovative%20architecture%20that%0Aintegrates%20Mamba%20layers%2C%20linear%20self-attention%2C%20and%20a%20Mixture%20of%20Experts%0Aframework%2C%20White-Basilisk%20achieves%20state-of-the-art%20results%20in%20vulnerability%0Adetection%20tasks%20with%20a%20parameter%20count%20of%20only%20200M.%20The%20model%27s%20capacity%20to%0Aprocess%20sequences%20of%20unprecedented%20length%20enables%20comprehensive%20analysis%20of%0Aextensive%20codebases%20in%20a%20single%20pass%2C%20surpassing%20the%20context%20limitations%20of%0Acurrent%20Large%20Language%20Models%20%28LLMs%29.%20White-Basilisk%20exhibits%20robust%0Aperformance%20on%20imbalanced%2C%20real-world%20datasets%2C%20while%20maintaining%20computational%0Aefficiency%20that%20facilitates%20deployment%20across%20diverse%20organizational%20scales.%0AThis%20research%20not%20only%20establishes%20new%20benchmarks%20in%20code%20security%20but%20also%0Aprovides%20empirical%20evidence%20that%20compact%2C%20efficiently%20designed%20models%20can%0Aoutperform%20larger%20counterparts%20in%20specialized%20tasks%2C%20potentially%20redefining%0Aoptimization%20strategies%20in%20AI%20development%20for%20domain-specific%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08540v3&entry.124074799=Read"},
{"title": "Spatio-Temporal Directed Graph Learning for Account Takeover Fraud\n  Detection", "author": "Mohsen Nayebi Kerdabadi and William Andrew Byron and Xin Sun and Amirfarrokh Iranitalab", "abstract": "  Account Takeover (ATO) fraud poses a significant challenge in consumer\nbanking, requiring high recall under strict latency while minimizing friction\nfor legitimate users. Production systems typically rely on tabular\ngradient-boosted decision trees (e.g., XGBoost) that score sessions\nindependently, overlooking the relational and temporal structure of online\nactivity that characterizes coordinated attacks and \"fraud rings.\" We introduce\nATLAS (Account Takeover Learning Across Spatio-Temporal Directed Graph), a\nframework that reformulates ATO detection as spatio-temporal node\nclassification on a time-respecting directed session graph. ATLAS links\nentities via shared identifiers (account, device, IP) and regulates\nconnectivity with time-window and recency constraints, enabling causal,\ntime-respecting message passing and latency-aware label propagation that uses\nonly labels available at scoring time, non-anticipative and leakage-free. We\noperationalize ATLAS with inductive GraphSAGE variants trained via neighbor\nsampling, at scale on a sessions graph with more than 100M nodes and around 1B\nedges. On a high-risk digital product at Capital One, ATLAS delivers 6.38\npercent AUC improvement and more than 50 percent reduction in customer\nfriction, improving fraud capture while reducing user friction.\n", "link": "http://arxiv.org/abs/2509.20339v1", "date": "2025-09-24", "relevancy": 1.9135, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4894}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4777}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20Directed%20Graph%20Learning%20for%20Account%20Takeover%20Fraud%0A%20%20Detection&body=Title%3A%20Spatio-Temporal%20Directed%20Graph%20Learning%20for%20Account%20Takeover%20Fraud%0A%20%20Detection%0AAuthor%3A%20Mohsen%20Nayebi%20Kerdabadi%20and%20William%20Andrew%20Byron%20and%20Xin%20Sun%20and%20Amirfarrokh%20Iranitalab%0AAbstract%3A%20%20%20Account%20Takeover%20%28ATO%29%20fraud%20poses%20a%20significant%20challenge%20in%20consumer%0Abanking%2C%20requiring%20high%20recall%20under%20strict%20latency%20while%20minimizing%20friction%0Afor%20legitimate%20users.%20Production%20systems%20typically%20rely%20on%20tabular%0Agradient-boosted%20decision%20trees%20%28e.g.%2C%20XGBoost%29%20that%20score%20sessions%0Aindependently%2C%20overlooking%20the%20relational%20and%20temporal%20structure%20of%20online%0Aactivity%20that%20characterizes%20coordinated%20attacks%20and%20%22fraud%20rings.%22%20We%20introduce%0AATLAS%20%28Account%20Takeover%20Learning%20Across%20Spatio-Temporal%20Directed%20Graph%29%2C%20a%0Aframework%20that%20reformulates%20ATO%20detection%20as%20spatio-temporal%20node%0Aclassification%20on%20a%20time-respecting%20directed%20session%20graph.%20ATLAS%20links%0Aentities%20via%20shared%20identifiers%20%28account%2C%20device%2C%20IP%29%20and%20regulates%0Aconnectivity%20with%20time-window%20and%20recency%20constraints%2C%20enabling%20causal%2C%0Atime-respecting%20message%20passing%20and%20latency-aware%20label%20propagation%20that%20uses%0Aonly%20labels%20available%20at%20scoring%20time%2C%20non-anticipative%20and%20leakage-free.%20We%0Aoperationalize%20ATLAS%20with%20inductive%20GraphSAGE%20variants%20trained%20via%20neighbor%0Asampling%2C%20at%20scale%20on%20a%20sessions%20graph%20with%20more%20than%20100M%20nodes%20and%20around%201B%0Aedges.%20On%20a%20high-risk%20digital%20product%20at%20Capital%20One%2C%20ATLAS%20delivers%206.38%0Apercent%20AUC%20improvement%20and%20more%20than%2050%20percent%20reduction%20in%20customer%0Afriction%2C%20improving%20fraud%20capture%20while%20reducing%20user%20friction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Temporal%2520Directed%2520Graph%2520Learning%2520for%2520Account%2520Takeover%2520Fraud%250A%2520%2520Detection%26entry.906535625%3DMohsen%2520Nayebi%2520Kerdabadi%2520and%2520William%2520Andrew%2520Byron%2520and%2520Xin%2520Sun%2520and%2520Amirfarrokh%2520Iranitalab%26entry.1292438233%3D%2520%2520Account%2520Takeover%2520%2528ATO%2529%2520fraud%2520poses%2520a%2520significant%2520challenge%2520in%2520consumer%250Abanking%252C%2520requiring%2520high%2520recall%2520under%2520strict%2520latency%2520while%2520minimizing%2520friction%250Afor%2520legitimate%2520users.%2520Production%2520systems%2520typically%2520rely%2520on%2520tabular%250Agradient-boosted%2520decision%2520trees%2520%2528e.g.%252C%2520XGBoost%2529%2520that%2520score%2520sessions%250Aindependently%252C%2520overlooking%2520the%2520relational%2520and%2520temporal%2520structure%2520of%2520online%250Aactivity%2520that%2520characterizes%2520coordinated%2520attacks%2520and%2520%2522fraud%2520rings.%2522%2520We%2520introduce%250AATLAS%2520%2528Account%2520Takeover%2520Learning%2520Across%2520Spatio-Temporal%2520Directed%2520Graph%2529%252C%2520a%250Aframework%2520that%2520reformulates%2520ATO%2520detection%2520as%2520spatio-temporal%2520node%250Aclassification%2520on%2520a%2520time-respecting%2520directed%2520session%2520graph.%2520ATLAS%2520links%250Aentities%2520via%2520shared%2520identifiers%2520%2528account%252C%2520device%252C%2520IP%2529%2520and%2520regulates%250Aconnectivity%2520with%2520time-window%2520and%2520recency%2520constraints%252C%2520enabling%2520causal%252C%250Atime-respecting%2520message%2520passing%2520and%2520latency-aware%2520label%2520propagation%2520that%2520uses%250Aonly%2520labels%2520available%2520at%2520scoring%2520time%252C%2520non-anticipative%2520and%2520leakage-free.%2520We%250Aoperationalize%2520ATLAS%2520with%2520inductive%2520GraphSAGE%2520variants%2520trained%2520via%2520neighbor%250Asampling%252C%2520at%2520scale%2520on%2520a%2520sessions%2520graph%2520with%2520more%2520than%2520100M%2520nodes%2520and%2520around%25201B%250Aedges.%2520On%2520a%2520high-risk%2520digital%2520product%2520at%2520Capital%2520One%252C%2520ATLAS%2520delivers%25206.38%250Apercent%2520AUC%2520improvement%2520and%2520more%2520than%252050%2520percent%2520reduction%2520in%2520customer%250Afriction%252C%2520improving%2520fraud%2520capture%2520while%2520reducing%2520user%2520friction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20Directed%20Graph%20Learning%20for%20Account%20Takeover%20Fraud%0A%20%20Detection&entry.906535625=Mohsen%20Nayebi%20Kerdabadi%20and%20William%20Andrew%20Byron%20and%20Xin%20Sun%20and%20Amirfarrokh%20Iranitalab&entry.1292438233=%20%20Account%20Takeover%20%28ATO%29%20fraud%20poses%20a%20significant%20challenge%20in%20consumer%0Abanking%2C%20requiring%20high%20recall%20under%20strict%20latency%20while%20minimizing%20friction%0Afor%20legitimate%20users.%20Production%20systems%20typically%20rely%20on%20tabular%0Agradient-boosted%20decision%20trees%20%28e.g.%2C%20XGBoost%29%20that%20score%20sessions%0Aindependently%2C%20overlooking%20the%20relational%20and%20temporal%20structure%20of%20online%0Aactivity%20that%20characterizes%20coordinated%20attacks%20and%20%22fraud%20rings.%22%20We%20introduce%0AATLAS%20%28Account%20Takeover%20Learning%20Across%20Spatio-Temporal%20Directed%20Graph%29%2C%20a%0Aframework%20that%20reformulates%20ATO%20detection%20as%20spatio-temporal%20node%0Aclassification%20on%20a%20time-respecting%20directed%20session%20graph.%20ATLAS%20links%0Aentities%20via%20shared%20identifiers%20%28account%2C%20device%2C%20IP%29%20and%20regulates%0Aconnectivity%20with%20time-window%20and%20recency%20constraints%2C%20enabling%20causal%2C%0Atime-respecting%20message%20passing%20and%20latency-aware%20label%20propagation%20that%20uses%0Aonly%20labels%20available%20at%20scoring%20time%2C%20non-anticipative%20and%20leakage-free.%20We%0Aoperationalize%20ATLAS%20with%20inductive%20GraphSAGE%20variants%20trained%20via%20neighbor%0Asampling%2C%20at%20scale%20on%20a%20sessions%20graph%20with%20more%20than%20100M%20nodes%20and%20around%201B%0Aedges.%20On%20a%20high-risk%20digital%20product%20at%20Capital%20One%2C%20ATLAS%20delivers%206.38%0Apercent%20AUC%20improvement%20and%20more%20than%2050%20percent%20reduction%20in%20customer%0Afriction%2C%20improving%20fraud%20capture%20while%20reducing%20user%20friction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20339v1&entry.124074799=Read"},
{"title": "Beyond the Pre-Service Horizon: Infusing In-Service Behavior for\n  Improved Financial Risk Forecasting", "author": "Senhao Liu and Zhiyu Guo and Zhiyuan Ji and Yueguo Chen and Yateng Tang and Yunhai Wang and Xuehao Zheng and Xiang Ao", "abstract": "  Typical financial risk management involves distinct phases for pre-service\nrisk assessment and in-service default detection, often modeled separately.\nThis paper proposes a novel framework, Multi-Granularity Knowledge Distillation\n(abbreviated as MGKD), aimed at improving pre-service risk prediction through\nthe integration of in-service user behavior data. MGKD follows the idea of\nknowledge distillation, where the teacher model, trained on historical\nin-service data, guides the student model, which is trained on pre-service\ndata. By using soft labels derived from in-service data, the teacher model\nhelps the student model improve its risk prediction prior to service\nactivation. Meanwhile, a multi-granularity distillation strategy is introduced,\nincluding coarse-grained, fine-grained, and self-distillation, to align the\nrepresentations and predictions of the teacher and student models. This\napproach not only reinforces the representation of default cases but also\nenables the transfer of key behavioral patterns associated with defaulters from\nthe teacher to the student model, thereby improving the overall performance of\npre-service risk assessment. Moreover, we adopt a re-weighting strategy to\nmitigate the model's bias towards the minority class. Experimental results on\nlarge-scale real-world datasets from Tencent Mobile Payment demonstrate the\neffectiveness of our proposed approach in both offline and online scenarios.\n", "link": "http://arxiv.org/abs/2509.06385v3", "date": "2025-09-24", "relevancy": 1.9099, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5043}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4732}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Pre-Service%20Horizon%3A%20Infusing%20In-Service%20Behavior%20for%0A%20%20Improved%20Financial%20Risk%20Forecasting&body=Title%3A%20Beyond%20the%20Pre-Service%20Horizon%3A%20Infusing%20In-Service%20Behavior%20for%0A%20%20Improved%20Financial%20Risk%20Forecasting%0AAuthor%3A%20Senhao%20Liu%20and%20Zhiyu%20Guo%20and%20Zhiyuan%20Ji%20and%20Yueguo%20Chen%20and%20Yateng%20Tang%20and%20Yunhai%20Wang%20and%20Xuehao%20Zheng%20and%20Xiang%20Ao%0AAbstract%3A%20%20%20Typical%20financial%20risk%20management%20involves%20distinct%20phases%20for%20pre-service%0Arisk%20assessment%20and%20in-service%20default%20detection%2C%20often%20modeled%20separately.%0AThis%20paper%20proposes%20a%20novel%20framework%2C%20Multi-Granularity%20Knowledge%20Distillation%0A%28abbreviated%20as%20MGKD%29%2C%20aimed%20at%20improving%20pre-service%20risk%20prediction%20through%0Athe%20integration%20of%20in-service%20user%20behavior%20data.%20MGKD%20follows%20the%20idea%20of%0Aknowledge%20distillation%2C%20where%20the%20teacher%20model%2C%20trained%20on%20historical%0Ain-service%20data%2C%20guides%20the%20student%20model%2C%20which%20is%20trained%20on%20pre-service%0Adata.%20By%20using%20soft%20labels%20derived%20from%20in-service%20data%2C%20the%20teacher%20model%0Ahelps%20the%20student%20model%20improve%20its%20risk%20prediction%20prior%20to%20service%0Aactivation.%20Meanwhile%2C%20a%20multi-granularity%20distillation%20strategy%20is%20introduced%2C%0Aincluding%20coarse-grained%2C%20fine-grained%2C%20and%20self-distillation%2C%20to%20align%20the%0Arepresentations%20and%20predictions%20of%20the%20teacher%20and%20student%20models.%20This%0Aapproach%20not%20only%20reinforces%20the%20representation%20of%20default%20cases%20but%20also%0Aenables%20the%20transfer%20of%20key%20behavioral%20patterns%20associated%20with%20defaulters%20from%0Athe%20teacher%20to%20the%20student%20model%2C%20thereby%20improving%20the%20overall%20performance%20of%0Apre-service%20risk%20assessment.%20Moreover%2C%20we%20adopt%20a%20re-weighting%20strategy%20to%0Amitigate%20the%20model%27s%20bias%20towards%20the%20minority%20class.%20Experimental%20results%20on%0Alarge-scale%20real-world%20datasets%20from%20Tencent%20Mobile%20Payment%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20approach%20in%20both%20offline%20and%20online%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06385v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Pre-Service%2520Horizon%253A%2520Infusing%2520In-Service%2520Behavior%2520for%250A%2520%2520Improved%2520Financial%2520Risk%2520Forecasting%26entry.906535625%3DSenhao%2520Liu%2520and%2520Zhiyu%2520Guo%2520and%2520Zhiyuan%2520Ji%2520and%2520Yueguo%2520Chen%2520and%2520Yateng%2520Tang%2520and%2520Yunhai%2520Wang%2520and%2520Xuehao%2520Zheng%2520and%2520Xiang%2520Ao%26entry.1292438233%3D%2520%2520Typical%2520financial%2520risk%2520management%2520involves%2520distinct%2520phases%2520for%2520pre-service%250Arisk%2520assessment%2520and%2520in-service%2520default%2520detection%252C%2520often%2520modeled%2520separately.%250AThis%2520paper%2520proposes%2520a%2520novel%2520framework%252C%2520Multi-Granularity%2520Knowledge%2520Distillation%250A%2528abbreviated%2520as%2520MGKD%2529%252C%2520aimed%2520at%2520improving%2520pre-service%2520risk%2520prediction%2520through%250Athe%2520integration%2520of%2520in-service%2520user%2520behavior%2520data.%2520MGKD%2520follows%2520the%2520idea%2520of%250Aknowledge%2520distillation%252C%2520where%2520the%2520teacher%2520model%252C%2520trained%2520on%2520historical%250Ain-service%2520data%252C%2520guides%2520the%2520student%2520model%252C%2520which%2520is%2520trained%2520on%2520pre-service%250Adata.%2520By%2520using%2520soft%2520labels%2520derived%2520from%2520in-service%2520data%252C%2520the%2520teacher%2520model%250Ahelps%2520the%2520student%2520model%2520improve%2520its%2520risk%2520prediction%2520prior%2520to%2520service%250Aactivation.%2520Meanwhile%252C%2520a%2520multi-granularity%2520distillation%2520strategy%2520is%2520introduced%252C%250Aincluding%2520coarse-grained%252C%2520fine-grained%252C%2520and%2520self-distillation%252C%2520to%2520align%2520the%250Arepresentations%2520and%2520predictions%2520of%2520the%2520teacher%2520and%2520student%2520models.%2520This%250Aapproach%2520not%2520only%2520reinforces%2520the%2520representation%2520of%2520default%2520cases%2520but%2520also%250Aenables%2520the%2520transfer%2520of%2520key%2520behavioral%2520patterns%2520associated%2520with%2520defaulters%2520from%250Athe%2520teacher%2520to%2520the%2520student%2520model%252C%2520thereby%2520improving%2520the%2520overall%2520performance%2520of%250Apre-service%2520risk%2520assessment.%2520Moreover%252C%2520we%2520adopt%2520a%2520re-weighting%2520strategy%2520to%250Amitigate%2520the%2520model%2527s%2520bias%2520towards%2520the%2520minority%2520class.%2520Experimental%2520results%2520on%250Alarge-scale%2520real-world%2520datasets%2520from%2520Tencent%2520Mobile%2520Payment%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520approach%2520in%2520both%2520offline%2520and%2520online%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06385v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Pre-Service%20Horizon%3A%20Infusing%20In-Service%20Behavior%20for%0A%20%20Improved%20Financial%20Risk%20Forecasting&entry.906535625=Senhao%20Liu%20and%20Zhiyu%20Guo%20and%20Zhiyuan%20Ji%20and%20Yueguo%20Chen%20and%20Yateng%20Tang%20and%20Yunhai%20Wang%20and%20Xuehao%20Zheng%20and%20Xiang%20Ao&entry.1292438233=%20%20Typical%20financial%20risk%20management%20involves%20distinct%20phases%20for%20pre-service%0Arisk%20assessment%20and%20in-service%20default%20detection%2C%20often%20modeled%20separately.%0AThis%20paper%20proposes%20a%20novel%20framework%2C%20Multi-Granularity%20Knowledge%20Distillation%0A%28abbreviated%20as%20MGKD%29%2C%20aimed%20at%20improving%20pre-service%20risk%20prediction%20through%0Athe%20integration%20of%20in-service%20user%20behavior%20data.%20MGKD%20follows%20the%20idea%20of%0Aknowledge%20distillation%2C%20where%20the%20teacher%20model%2C%20trained%20on%20historical%0Ain-service%20data%2C%20guides%20the%20student%20model%2C%20which%20is%20trained%20on%20pre-service%0Adata.%20By%20using%20soft%20labels%20derived%20from%20in-service%20data%2C%20the%20teacher%20model%0Ahelps%20the%20student%20model%20improve%20its%20risk%20prediction%20prior%20to%20service%0Aactivation.%20Meanwhile%2C%20a%20multi-granularity%20distillation%20strategy%20is%20introduced%2C%0Aincluding%20coarse-grained%2C%20fine-grained%2C%20and%20self-distillation%2C%20to%20align%20the%0Arepresentations%20and%20predictions%20of%20the%20teacher%20and%20student%20models.%20This%0Aapproach%20not%20only%20reinforces%20the%20representation%20of%20default%20cases%20but%20also%0Aenables%20the%20transfer%20of%20key%20behavioral%20patterns%20associated%20with%20defaulters%20from%0Athe%20teacher%20to%20the%20student%20model%2C%20thereby%20improving%20the%20overall%20performance%20of%0Apre-service%20risk%20assessment.%20Moreover%2C%20we%20adopt%20a%20re-weighting%20strategy%20to%0Amitigate%20the%20model%27s%20bias%20towards%20the%20minority%20class.%20Experimental%20results%20on%0Alarge-scale%20real-world%20datasets%20from%20Tencent%20Mobile%20Payment%20demonstrate%20the%0Aeffectiveness%20of%20our%20proposed%20approach%20in%20both%20offline%20and%20online%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06385v3&entry.124074799=Read"},
{"title": "Enhancing RAG Efficiency with Adaptive Context Compression", "author": "Shuyu Guo and Shuo Zhang and Zhaochun Ren", "abstract": "  Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.\n", "link": "http://arxiv.org/abs/2507.22931v3", "date": "2025-09-24", "relevancy": 1.904, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20RAG%20Efficiency%20with%20Adaptive%20Context%20Compression&body=Title%3A%20Enhancing%20RAG%20Efficiency%20with%20Adaptive%20Context%20Compression%0AAuthor%3A%20Shuyu%20Guo%20and%20Shuo%20Zhang%20and%20Zhaochun%20Ren%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20enhances%20large%20language%20models%20%28LLMs%29%0Awith%20external%20knowledge%20but%20incurs%20significant%20inference%20costs%20due%20to%20lengthy%0Aretrieved%20contexts.%20While%20context%20compression%20mitigates%20this%20issue%2C%20existing%0Amethods%20apply%20fixed%20compression%20rates%2C%20over-compressing%20simple%20queries%20or%0Aunder-compressing%20complex%20ones.%20We%20propose%20Adaptive%20Context%20Compression%20for%20RAG%0A%28ACC-RAG%29%2C%20a%20framework%20that%20dynamically%20adjusts%20compression%20rates%20based%20on%0Ainput%20complexity%2C%20optimizing%20inference%20efficiency%20without%20sacrificing%20accuracy.%0AACC-RAG%20combines%20a%20hierarchical%20compressor%20%28for%20multi-granular%20embeddings%29%20with%0Aa%20context%20selector%20to%20retain%20minimal%20sufficient%20information%2C%20akin%20to%20human%0Askimming.%20Evaluated%20on%20Wikipedia%20and%20five%20QA%20datasets%2C%20ACC-RAG%20outperforms%0Afixed-rate%20methods%20and%20matches/unlocks%20over%204%20times%20faster%20inference%20versus%0Astandard%20RAG%20while%20maintaining%20or%20improving%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22931v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520RAG%2520Efficiency%2520with%2520Adaptive%2520Context%2520Compression%26entry.906535625%3DShuyu%2520Guo%2520and%2520Shuo%2520Zhang%2520and%2520Zhaochun%2520Ren%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520enhances%2520large%2520language%2520models%2520%2528LLMs%2529%250Awith%2520external%2520knowledge%2520but%2520incurs%2520significant%2520inference%2520costs%2520due%2520to%2520lengthy%250Aretrieved%2520contexts.%2520While%2520context%2520compression%2520mitigates%2520this%2520issue%252C%2520existing%250Amethods%2520apply%2520fixed%2520compression%2520rates%252C%2520over-compressing%2520simple%2520queries%2520or%250Aunder-compressing%2520complex%2520ones.%2520We%2520propose%2520Adaptive%2520Context%2520Compression%2520for%2520RAG%250A%2528ACC-RAG%2529%252C%2520a%2520framework%2520that%2520dynamically%2520adjusts%2520compression%2520rates%2520based%2520on%250Ainput%2520complexity%252C%2520optimizing%2520inference%2520efficiency%2520without%2520sacrificing%2520accuracy.%250AACC-RAG%2520combines%2520a%2520hierarchical%2520compressor%2520%2528for%2520multi-granular%2520embeddings%2529%2520with%250Aa%2520context%2520selector%2520to%2520retain%2520minimal%2520sufficient%2520information%252C%2520akin%2520to%2520human%250Askimming.%2520Evaluated%2520on%2520Wikipedia%2520and%2520five%2520QA%2520datasets%252C%2520ACC-RAG%2520outperforms%250Afixed-rate%2520methods%2520and%2520matches/unlocks%2520over%25204%2520times%2520faster%2520inference%2520versus%250Astandard%2520RAG%2520while%2520maintaining%2520or%2520improving%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22931v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20RAG%20Efficiency%20with%20Adaptive%20Context%20Compression&entry.906535625=Shuyu%20Guo%20and%20Shuo%20Zhang%20and%20Zhaochun%20Ren&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20enhances%20large%20language%20models%20%28LLMs%29%0Awith%20external%20knowledge%20but%20incurs%20significant%20inference%20costs%20due%20to%20lengthy%0Aretrieved%20contexts.%20While%20context%20compression%20mitigates%20this%20issue%2C%20existing%0Amethods%20apply%20fixed%20compression%20rates%2C%20over-compressing%20simple%20queries%20or%0Aunder-compressing%20complex%20ones.%20We%20propose%20Adaptive%20Context%20Compression%20for%20RAG%0A%28ACC-RAG%29%2C%20a%20framework%20that%20dynamically%20adjusts%20compression%20rates%20based%20on%0Ainput%20complexity%2C%20optimizing%20inference%20efficiency%20without%20sacrificing%20accuracy.%0AACC-RAG%20combines%20a%20hierarchical%20compressor%20%28for%20multi-granular%20embeddings%29%20with%0Aa%20context%20selector%20to%20retain%20minimal%20sufficient%20information%2C%20akin%20to%20human%0Askimming.%20Evaluated%20on%20Wikipedia%20and%20five%20QA%20datasets%2C%20ACC-RAG%20outperforms%0Afixed-rate%20methods%20and%20matches/unlocks%20over%204%20times%20faster%20inference%20versus%0Astandard%20RAG%20while%20maintaining%20or%20improving%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22931v3&entry.124074799=Read"},
{"title": "Efficient Fine-Tuning of Large Language Models for Automated Medical\n  Documentation", "author": "Hui Yi Leong and Yi Fan Gao and Ji Shuai and Yang Zhang and Uktu Pamuksuz", "abstract": "  Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being.\n", "link": "http://arxiv.org/abs/2409.09324v3", "date": "2025-09-24", "relevancy": 1.8906, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Fine-Tuning%20of%20Large%20Language%20Models%20for%20Automated%20Medical%0A%20%20Documentation&body=Title%3A%20Efficient%20Fine-Tuning%20of%20Large%20Language%20Models%20for%20Automated%20Medical%0A%20%20Documentation%0AAuthor%3A%20Hui%20Yi%20Leong%20and%20Yi%20Fan%20Gao%20and%20Ji%20Shuai%20and%20Yang%20Zhang%20and%20Uktu%20Pamuksuz%0AAbstract%3A%20%20%20Scientific%20research%20indicates%20that%20for%20every%20hour%20spent%20in%20direct%20patient%0Acare%2C%20physicians%20spend%20nearly%20two%20additional%20hours%20on%20administrative%20tasks%2C%0Aparticularly%20on%20electronic%20health%20records%20%28EHRs%29%20and%20desk%20work.%20This%20excessive%0Aadministrative%20burden%20not%20only%20reduces%20the%20time%20available%20for%20patient%20care%20but%0Aalso%20contributes%20to%20physician%20burnout%20and%20inefficiencies%20in%20healthcare%0Adelivery.%20To%20address%20these%20challenges%2C%20this%20study%20introduces%20MediGen%2C%20a%0Afine-tuned%20large%20language%20model%20%28LLM%29%20designed%20to%20automate%20the%20generation%20of%0Amedical%20reports%20from%20medical%20dialogues.%20By%20leveraging%20state-of-the-art%0Amethodologies%20for%20fine-tuning%20open-source%20pretrained%20models%2C%20including%0ALLaMA3-8B%2C%20MediGen%20achieves%20high%20accuracy%20in%20transcribing%20and%20summarizing%0Aclinical%20interactions.%20The%20fine-tuned%20LLaMA3-8B%20model%20demonstrated%20promising%0Aresults%2C%20achieving%20a%20ROUGE%20score%20of%2058%25%20and%20a%20BERTScore-F1%20of%2072%25%2C%20indicating%0Aits%20effectiveness%20in%20generating%20accurate%20and%20clinically%20relevant%20medical%0Areports.%20These%20findings%20suggest%20that%20MediGen%20has%20the%20potential%20to%20significantly%0Areduce%20the%20administrative%20workload%20on%20physicians%2C%20improving%20both%20healthcare%0Aefficiency%20and%20physician%20well-being.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09324v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Fine-Tuning%2520of%2520Large%2520Language%2520Models%2520for%2520Automated%2520Medical%250A%2520%2520Documentation%26entry.906535625%3DHui%2520Yi%2520Leong%2520and%2520Yi%2520Fan%2520Gao%2520and%2520Ji%2520Shuai%2520and%2520Yang%2520Zhang%2520and%2520Uktu%2520Pamuksuz%26entry.1292438233%3D%2520%2520Scientific%2520research%2520indicates%2520that%2520for%2520every%2520hour%2520spent%2520in%2520direct%2520patient%250Acare%252C%2520physicians%2520spend%2520nearly%2520two%2520additional%2520hours%2520on%2520administrative%2520tasks%252C%250Aparticularly%2520on%2520electronic%2520health%2520records%2520%2528EHRs%2529%2520and%2520desk%2520work.%2520This%2520excessive%250Aadministrative%2520burden%2520not%2520only%2520reduces%2520the%2520time%2520available%2520for%2520patient%2520care%2520but%250Aalso%2520contributes%2520to%2520physician%2520burnout%2520and%2520inefficiencies%2520in%2520healthcare%250Adelivery.%2520To%2520address%2520these%2520challenges%252C%2520this%2520study%2520introduces%2520MediGen%252C%2520a%250Afine-tuned%2520large%2520language%2520model%2520%2528LLM%2529%2520designed%2520to%2520automate%2520the%2520generation%2520of%250Amedical%2520reports%2520from%2520medical%2520dialogues.%2520By%2520leveraging%2520state-of-the-art%250Amethodologies%2520for%2520fine-tuning%2520open-source%2520pretrained%2520models%252C%2520including%250ALLaMA3-8B%252C%2520MediGen%2520achieves%2520high%2520accuracy%2520in%2520transcribing%2520and%2520summarizing%250Aclinical%2520interactions.%2520The%2520fine-tuned%2520LLaMA3-8B%2520model%2520demonstrated%2520promising%250Aresults%252C%2520achieving%2520a%2520ROUGE%2520score%2520of%252058%2525%2520and%2520a%2520BERTScore-F1%2520of%252072%2525%252C%2520indicating%250Aits%2520effectiveness%2520in%2520generating%2520accurate%2520and%2520clinically%2520relevant%2520medical%250Areports.%2520These%2520findings%2520suggest%2520that%2520MediGen%2520has%2520the%2520potential%2520to%2520significantly%250Areduce%2520the%2520administrative%2520workload%2520on%2520physicians%252C%2520improving%2520both%2520healthcare%250Aefficiency%2520and%2520physician%2520well-being.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09324v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Fine-Tuning%20of%20Large%20Language%20Models%20for%20Automated%20Medical%0A%20%20Documentation&entry.906535625=Hui%20Yi%20Leong%20and%20Yi%20Fan%20Gao%20and%20Ji%20Shuai%20and%20Yang%20Zhang%20and%20Uktu%20Pamuksuz&entry.1292438233=%20%20Scientific%20research%20indicates%20that%20for%20every%20hour%20spent%20in%20direct%20patient%0Acare%2C%20physicians%20spend%20nearly%20two%20additional%20hours%20on%20administrative%20tasks%2C%0Aparticularly%20on%20electronic%20health%20records%20%28EHRs%29%20and%20desk%20work.%20This%20excessive%0Aadministrative%20burden%20not%20only%20reduces%20the%20time%20available%20for%20patient%20care%20but%0Aalso%20contributes%20to%20physician%20burnout%20and%20inefficiencies%20in%20healthcare%0Adelivery.%20To%20address%20these%20challenges%2C%20this%20study%20introduces%20MediGen%2C%20a%0Afine-tuned%20large%20language%20model%20%28LLM%29%20designed%20to%20automate%20the%20generation%20of%0Amedical%20reports%20from%20medical%20dialogues.%20By%20leveraging%20state-of-the-art%0Amethodologies%20for%20fine-tuning%20open-source%20pretrained%20models%2C%20including%0ALLaMA3-8B%2C%20MediGen%20achieves%20high%20accuracy%20in%20transcribing%20and%20summarizing%0Aclinical%20interactions.%20The%20fine-tuned%20LLaMA3-8B%20model%20demonstrated%20promising%0Aresults%2C%20achieving%20a%20ROUGE%20score%20of%2058%25%20and%20a%20BERTScore-F1%20of%2072%25%2C%20indicating%0Aits%20effectiveness%20in%20generating%20accurate%20and%20clinically%20relevant%20medical%0Areports.%20These%20findings%20suggest%20that%20MediGen%20has%20the%20potential%20to%20significantly%0Areduce%20the%20administrative%20workload%20on%20physicians%2C%20improving%20both%20healthcare%0Aefficiency%20and%20physician%20well-being.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09324v3&entry.124074799=Read"},
{"title": "Langevin Unlearning: A New Perspective of Noisy Gradient Descent for\n  Machine Unlearning", "author": "Eli Chien and Haoyu Wang and Ziang Chen and Pan Li", "abstract": "  Machine unlearning has raised significant interest with the adoption of laws\nensuring the ``right to be forgotten''. Researchers have provided a\nprobabilistic notion of approximate unlearning under a similar definition of\nDifferential Privacy (DP), where privacy is defined as statistical\nindistinguishability to retraining from scratch. We propose Langevin\nunlearning, an unlearning framework based on noisy gradient descent with\nprivacy guarantees for approximate unlearning problems. Langevin unlearning\nunifies the DP learning process and the privacy-certified unlearning process\nwith many algorithmic benefits. These include approximate certified unlearning\nfor non-convex problems, complexity saving compared to retraining, sequential\nand batch unlearning for multiple unlearning requests.\n", "link": "http://arxiv.org/abs/2401.10371v7", "date": "2025-09-24", "relevancy": 1.8647, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4835}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4694}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Langevin%20Unlearning%3A%20A%20New%20Perspective%20of%20Noisy%20Gradient%20Descent%20for%0A%20%20Machine%20Unlearning&body=Title%3A%20Langevin%20Unlearning%3A%20A%20New%20Perspective%20of%20Noisy%20Gradient%20Descent%20for%0A%20%20Machine%20Unlearning%0AAuthor%3A%20Eli%20Chien%20and%20Haoyu%20Wang%20and%20Ziang%20Chen%20and%20Pan%20Li%0AAbstract%3A%20%20%20Machine%20unlearning%20has%20raised%20significant%20interest%20with%20the%20adoption%20of%20laws%0Aensuring%20the%20%60%60right%20to%20be%20forgotten%27%27.%20Researchers%20have%20provided%20a%0Aprobabilistic%20notion%20of%20approximate%20unlearning%20under%20a%20similar%20definition%20of%0ADifferential%20Privacy%20%28DP%29%2C%20where%20privacy%20is%20defined%20as%20statistical%0Aindistinguishability%20to%20retraining%20from%20scratch.%20We%20propose%20Langevin%0Aunlearning%2C%20an%20unlearning%20framework%20based%20on%20noisy%20gradient%20descent%20with%0Aprivacy%20guarantees%20for%20approximate%20unlearning%20problems.%20Langevin%20unlearning%0Aunifies%20the%20DP%20learning%20process%20and%20the%20privacy-certified%20unlearning%20process%0Awith%20many%20algorithmic%20benefits.%20These%20include%20approximate%20certified%20unlearning%0Afor%20non-convex%20problems%2C%20complexity%20saving%20compared%20to%20retraining%2C%20sequential%0Aand%20batch%20unlearning%20for%20multiple%20unlearning%20requests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10371v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangevin%2520Unlearning%253A%2520A%2520New%2520Perspective%2520of%2520Noisy%2520Gradient%2520Descent%2520for%250A%2520%2520Machine%2520Unlearning%26entry.906535625%3DEli%2520Chien%2520and%2520Haoyu%2520Wang%2520and%2520Ziang%2520Chen%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520has%2520raised%2520significant%2520interest%2520with%2520the%2520adoption%2520of%2520laws%250Aensuring%2520the%2520%2560%2560right%2520to%2520be%2520forgotten%2527%2527.%2520Researchers%2520have%2520provided%2520a%250Aprobabilistic%2520notion%2520of%2520approximate%2520unlearning%2520under%2520a%2520similar%2520definition%2520of%250ADifferential%2520Privacy%2520%2528DP%2529%252C%2520where%2520privacy%2520is%2520defined%2520as%2520statistical%250Aindistinguishability%2520to%2520retraining%2520from%2520scratch.%2520We%2520propose%2520Langevin%250Aunlearning%252C%2520an%2520unlearning%2520framework%2520based%2520on%2520noisy%2520gradient%2520descent%2520with%250Aprivacy%2520guarantees%2520for%2520approximate%2520unlearning%2520problems.%2520Langevin%2520unlearning%250Aunifies%2520the%2520DP%2520learning%2520process%2520and%2520the%2520privacy-certified%2520unlearning%2520process%250Awith%2520many%2520algorithmic%2520benefits.%2520These%2520include%2520approximate%2520certified%2520unlearning%250Afor%2520non-convex%2520problems%252C%2520complexity%2520saving%2520compared%2520to%2520retraining%252C%2520sequential%250Aand%2520batch%2520unlearning%2520for%2520multiple%2520unlearning%2520requests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10371v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Langevin%20Unlearning%3A%20A%20New%20Perspective%20of%20Noisy%20Gradient%20Descent%20for%0A%20%20Machine%20Unlearning&entry.906535625=Eli%20Chien%20and%20Haoyu%20Wang%20and%20Ziang%20Chen%20and%20Pan%20Li&entry.1292438233=%20%20Machine%20unlearning%20has%20raised%20significant%20interest%20with%20the%20adoption%20of%20laws%0Aensuring%20the%20%60%60right%20to%20be%20forgotten%27%27.%20Researchers%20have%20provided%20a%0Aprobabilistic%20notion%20of%20approximate%20unlearning%20under%20a%20similar%20definition%20of%0ADifferential%20Privacy%20%28DP%29%2C%20where%20privacy%20is%20defined%20as%20statistical%0Aindistinguishability%20to%20retraining%20from%20scratch.%20We%20propose%20Langevin%0Aunlearning%2C%20an%20unlearning%20framework%20based%20on%20noisy%20gradient%20descent%20with%0Aprivacy%20guarantees%20for%20approximate%20unlearning%20problems.%20Langevin%20unlearning%0Aunifies%20the%20DP%20learning%20process%20and%20the%20privacy-certified%20unlearning%20process%0Awith%20many%20algorithmic%20benefits.%20These%20include%20approximate%20certified%20unlearning%0Afor%20non-convex%20problems%2C%20complexity%20saving%20compared%20to%20retraining%2C%20sequential%0Aand%20batch%20unlearning%20for%20multiple%20unlearning%20requests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10371v7&entry.124074799=Read"},
{"title": "Alignment-Sensitive Minimax Rates for Spectral Algorithms with Learned\n  Kernels", "author": "Dongming Huang and Zhifan Li and Yicheng Li and Qian Lin", "abstract": "  We study spectral algorithms in the setting where kernels are learned from\ndata. We introduce the effective span dimension (ESD), an alignment-sensitive\ncomplexity measure that depends jointly on the signal, spectrum, and noise\nlevel $\\sigma^2$. The ESD is well-defined for arbitrary kernels and signals\nwithout requiring eigen-decay conditions or source conditions. We prove that\nfor sequence models whose ESD is at most $K$, the minimax excess risk scales as\n$\\sigma^2 K$. Furthermore, we analyze over-parameterized gradient flow and\nprove that it can reduce the ESD. This finding establishes a connection between\nadaptive feature learning and provable improvements in generalization of\nspectral algorithms. We demonstrate the generality of the ESD framework by\nextending it to linear models and RKHS regression, and we support the theory\nwith numerical experiments. This framework provides a novel perspective on\ngeneralization beyond traditional fixed-kernel theories.\n", "link": "http://arxiv.org/abs/2509.20294v1", "date": "2025-09-24", "relevancy": 1.8626, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4955}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4488}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment-Sensitive%20Minimax%20Rates%20for%20Spectral%20Algorithms%20with%20Learned%0A%20%20Kernels&body=Title%3A%20Alignment-Sensitive%20Minimax%20Rates%20for%20Spectral%20Algorithms%20with%20Learned%0A%20%20Kernels%0AAuthor%3A%20Dongming%20Huang%20and%20Zhifan%20Li%20and%20Yicheng%20Li%20and%20Qian%20Lin%0AAbstract%3A%20%20%20We%20study%20spectral%20algorithms%20in%20the%20setting%20where%20kernels%20are%20learned%20from%0Adata.%20We%20introduce%20the%20effective%20span%20dimension%20%28ESD%29%2C%20an%20alignment-sensitive%0Acomplexity%20measure%20that%20depends%20jointly%20on%20the%20signal%2C%20spectrum%2C%20and%20noise%0Alevel%20%24%5Csigma%5E2%24.%20The%20ESD%20is%20well-defined%20for%20arbitrary%20kernels%20and%20signals%0Awithout%20requiring%20eigen-decay%20conditions%20or%20source%20conditions.%20We%20prove%20that%0Afor%20sequence%20models%20whose%20ESD%20is%20at%20most%20%24K%24%2C%20the%20minimax%20excess%20risk%20scales%20as%0A%24%5Csigma%5E2%20K%24.%20Furthermore%2C%20we%20analyze%20over-parameterized%20gradient%20flow%20and%0Aprove%20that%20it%20can%20reduce%20the%20ESD.%20This%20finding%20establishes%20a%20connection%20between%0Aadaptive%20feature%20learning%20and%20provable%20improvements%20in%20generalization%20of%0Aspectral%20algorithms.%20We%20demonstrate%20the%20generality%20of%20the%20ESD%20framework%20by%0Aextending%20it%20to%20linear%20models%20and%20RKHS%20regression%2C%20and%20we%20support%20the%20theory%0Awith%20numerical%20experiments.%20This%20framework%20provides%20a%20novel%20perspective%20on%0Ageneralization%20beyond%20traditional%20fixed-kernel%20theories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment-Sensitive%2520Minimax%2520Rates%2520for%2520Spectral%2520Algorithms%2520with%2520Learned%250A%2520%2520Kernels%26entry.906535625%3DDongming%2520Huang%2520and%2520Zhifan%2520Li%2520and%2520Yicheng%2520Li%2520and%2520Qian%2520Lin%26entry.1292438233%3D%2520%2520We%2520study%2520spectral%2520algorithms%2520in%2520the%2520setting%2520where%2520kernels%2520are%2520learned%2520from%250Adata.%2520We%2520introduce%2520the%2520effective%2520span%2520dimension%2520%2528ESD%2529%252C%2520an%2520alignment-sensitive%250Acomplexity%2520measure%2520that%2520depends%2520jointly%2520on%2520the%2520signal%252C%2520spectrum%252C%2520and%2520noise%250Alevel%2520%2524%255Csigma%255E2%2524.%2520The%2520ESD%2520is%2520well-defined%2520for%2520arbitrary%2520kernels%2520and%2520signals%250Awithout%2520requiring%2520eigen-decay%2520conditions%2520or%2520source%2520conditions.%2520We%2520prove%2520that%250Afor%2520sequence%2520models%2520whose%2520ESD%2520is%2520at%2520most%2520%2524K%2524%252C%2520the%2520minimax%2520excess%2520risk%2520scales%2520as%250A%2524%255Csigma%255E2%2520K%2524.%2520Furthermore%252C%2520we%2520analyze%2520over-parameterized%2520gradient%2520flow%2520and%250Aprove%2520that%2520it%2520can%2520reduce%2520the%2520ESD.%2520This%2520finding%2520establishes%2520a%2520connection%2520between%250Aadaptive%2520feature%2520learning%2520and%2520provable%2520improvements%2520in%2520generalization%2520of%250Aspectral%2520algorithms.%2520We%2520demonstrate%2520the%2520generality%2520of%2520the%2520ESD%2520framework%2520by%250Aextending%2520it%2520to%2520linear%2520models%2520and%2520RKHS%2520regression%252C%2520and%2520we%2520support%2520the%2520theory%250Awith%2520numerical%2520experiments.%2520This%2520framework%2520provides%2520a%2520novel%2520perspective%2520on%250Ageneralization%2520beyond%2520traditional%2520fixed-kernel%2520theories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment-Sensitive%20Minimax%20Rates%20for%20Spectral%20Algorithms%20with%20Learned%0A%20%20Kernels&entry.906535625=Dongming%20Huang%20and%20Zhifan%20Li%20and%20Yicheng%20Li%20and%20Qian%20Lin&entry.1292438233=%20%20We%20study%20spectral%20algorithms%20in%20the%20setting%20where%20kernels%20are%20learned%20from%0Adata.%20We%20introduce%20the%20effective%20span%20dimension%20%28ESD%29%2C%20an%20alignment-sensitive%0Acomplexity%20measure%20that%20depends%20jointly%20on%20the%20signal%2C%20spectrum%2C%20and%20noise%0Alevel%20%24%5Csigma%5E2%24.%20The%20ESD%20is%20well-defined%20for%20arbitrary%20kernels%20and%20signals%0Awithout%20requiring%20eigen-decay%20conditions%20or%20source%20conditions.%20We%20prove%20that%0Afor%20sequence%20models%20whose%20ESD%20is%20at%20most%20%24K%24%2C%20the%20minimax%20excess%20risk%20scales%20as%0A%24%5Csigma%5E2%20K%24.%20Furthermore%2C%20we%20analyze%20over-parameterized%20gradient%20flow%20and%0Aprove%20that%20it%20can%20reduce%20the%20ESD.%20This%20finding%20establishes%20a%20connection%20between%0Aadaptive%20feature%20learning%20and%20provable%20improvements%20in%20generalization%20of%0Aspectral%20algorithms.%20We%20demonstrate%20the%20generality%20of%20the%20ESD%20framework%20by%0Aextending%20it%20to%20linear%20models%20and%20RKHS%20regression%2C%20and%20we%20support%20the%20theory%0Awith%20numerical%20experiments.%20This%20framework%20provides%20a%20novel%20perspective%20on%0Ageneralization%20beyond%20traditional%20fixed-kernel%20theories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20294v1&entry.124074799=Read"},
{"title": "LEDiT: Your Length-Extrapolatable Diffusion Transformer without\n  Positional Encoding", "author": "Shen Zhang and Siyuan Liang and Yaning Tan and Zhaowei Chen and Linze Li and Ge Wu and Yuhao Chen and Shuheng Li and Zhenyu Zhao and Caihua Chen and Jiajun Liang and Yao Tang", "abstract": "  Diffusion transformers (DiTs) struggle to generate images at resolutions\nhigher than their training resolutions. The primary obstacle is that the\nexplicit positional encodings(PE), such as RoPE, need extrapolating to unseen\npositions which degrades performance when the inference resolution differs from\ntraining. In this paper, We propose a Length-Extrapolatable Diffusion\nTransformer~(LEDiT) to overcome this limitation. LEDiT needs no explicit PEs,\nthereby avoiding PE extrapolation. The key innovation of LEDiT lies in the use\nof causal attention. We demonstrate that causal attention can implicitly encode\nglobal positional information and show that such information facilitates\nextrapolation. We further introduce a locality enhancement module, which\ncaptures fine-grained local information to complement the global coarse-grained\nposition information encoded by causal attention. Experimental results on both\nconditional and text-to-image generation tasks demonstrate that LEDiT supports\nup to 4x resolution scaling (e.g., from 256x256 to 512x512), achieving better\nimage quality compared to the state-of-the-art length extrapolation methods. We\nbelieve that LEDiT marks a departure from the standard RoPE-based methods and\noffers a promising insight into length extrapolation. Project page:\nhttps://shenzhang2145.github.io/ledit/\n", "link": "http://arxiv.org/abs/2503.04344v3", "date": "2025-09-24", "relevancy": 1.8246, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6327}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6165}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEDiT%3A%20Your%20Length-Extrapolatable%20Diffusion%20Transformer%20without%0A%20%20Positional%20Encoding&body=Title%3A%20LEDiT%3A%20Your%20Length-Extrapolatable%20Diffusion%20Transformer%20without%0A%20%20Positional%20Encoding%0AAuthor%3A%20Shen%20Zhang%20and%20Siyuan%20Liang%20and%20Yaning%20Tan%20and%20Zhaowei%20Chen%20and%20Linze%20Li%20and%20Ge%20Wu%20and%20Yuhao%20Chen%20and%20Shuheng%20Li%20and%20Zhenyu%20Zhao%20and%20Caihua%20Chen%20and%20Jiajun%20Liang%20and%20Yao%20Tang%0AAbstract%3A%20%20%20Diffusion%20transformers%20%28DiTs%29%20struggle%20to%20generate%20images%20at%20resolutions%0Ahigher%20than%20their%20training%20resolutions.%20The%20primary%20obstacle%20is%20that%20the%0Aexplicit%20positional%20encodings%28PE%29%2C%20such%20as%20RoPE%2C%20need%20extrapolating%20to%20unseen%0Apositions%20which%20degrades%20performance%20when%20the%20inference%20resolution%20differs%20from%0Atraining.%20In%20this%20paper%2C%20We%20propose%20a%20Length-Extrapolatable%20Diffusion%0ATransformer~%28LEDiT%29%20to%20overcome%20this%20limitation.%20LEDiT%20needs%20no%20explicit%20PEs%2C%0Athereby%20avoiding%20PE%20extrapolation.%20The%20key%20innovation%20of%20LEDiT%20lies%20in%20the%20use%0Aof%20causal%20attention.%20We%20demonstrate%20that%20causal%20attention%20can%20implicitly%20encode%0Aglobal%20positional%20information%20and%20show%20that%20such%20information%20facilitates%0Aextrapolation.%20We%20further%20introduce%20a%20locality%20enhancement%20module%2C%20which%0Acaptures%20fine-grained%20local%20information%20to%20complement%20the%20global%20coarse-grained%0Aposition%20information%20encoded%20by%20causal%20attention.%20Experimental%20results%20on%20both%0Aconditional%20and%20text-to-image%20generation%20tasks%20demonstrate%20that%20LEDiT%20supports%0Aup%20to%204x%20resolution%20scaling%20%28e.g.%2C%20from%20256x256%20to%20512x512%29%2C%20achieving%20better%0Aimage%20quality%20compared%20to%20the%20state-of-the-art%20length%20extrapolation%20methods.%20We%0Abelieve%20that%20LEDiT%20marks%20a%20departure%20from%20the%20standard%20RoPE-based%20methods%20and%0Aoffers%20a%20promising%20insight%20into%20length%20extrapolation.%20Project%20page%3A%0Ahttps%3A//shenzhang2145.github.io/ledit/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEDiT%253A%2520Your%2520Length-Extrapolatable%2520Diffusion%2520Transformer%2520without%250A%2520%2520Positional%2520Encoding%26entry.906535625%3DShen%2520Zhang%2520and%2520Siyuan%2520Liang%2520and%2520Yaning%2520Tan%2520and%2520Zhaowei%2520Chen%2520and%2520Linze%2520Li%2520and%2520Ge%2520Wu%2520and%2520Yuhao%2520Chen%2520and%2520Shuheng%2520Li%2520and%2520Zhenyu%2520Zhao%2520and%2520Caihua%2520Chen%2520and%2520Jiajun%2520Liang%2520and%2520Yao%2520Tang%26entry.1292438233%3D%2520%2520Diffusion%2520transformers%2520%2528DiTs%2529%2520struggle%2520to%2520generate%2520images%2520at%2520resolutions%250Ahigher%2520than%2520their%2520training%2520resolutions.%2520The%2520primary%2520obstacle%2520is%2520that%2520the%250Aexplicit%2520positional%2520encodings%2528PE%2529%252C%2520such%2520as%2520RoPE%252C%2520need%2520extrapolating%2520to%2520unseen%250Apositions%2520which%2520degrades%2520performance%2520when%2520the%2520inference%2520resolution%2520differs%2520from%250Atraining.%2520In%2520this%2520paper%252C%2520We%2520propose%2520a%2520Length-Extrapolatable%2520Diffusion%250ATransformer~%2528LEDiT%2529%2520to%2520overcome%2520this%2520limitation.%2520LEDiT%2520needs%2520no%2520explicit%2520PEs%252C%250Athereby%2520avoiding%2520PE%2520extrapolation.%2520The%2520key%2520innovation%2520of%2520LEDiT%2520lies%2520in%2520the%2520use%250Aof%2520causal%2520attention.%2520We%2520demonstrate%2520that%2520causal%2520attention%2520can%2520implicitly%2520encode%250Aglobal%2520positional%2520information%2520and%2520show%2520that%2520such%2520information%2520facilitates%250Aextrapolation.%2520We%2520further%2520introduce%2520a%2520locality%2520enhancement%2520module%252C%2520which%250Acaptures%2520fine-grained%2520local%2520information%2520to%2520complement%2520the%2520global%2520coarse-grained%250Aposition%2520information%2520encoded%2520by%2520causal%2520attention.%2520Experimental%2520results%2520on%2520both%250Aconditional%2520and%2520text-to-image%2520generation%2520tasks%2520demonstrate%2520that%2520LEDiT%2520supports%250Aup%2520to%25204x%2520resolution%2520scaling%2520%2528e.g.%252C%2520from%2520256x256%2520to%2520512x512%2529%252C%2520achieving%2520better%250Aimage%2520quality%2520compared%2520to%2520the%2520state-of-the-art%2520length%2520extrapolation%2520methods.%2520We%250Abelieve%2520that%2520LEDiT%2520marks%2520a%2520departure%2520from%2520the%2520standard%2520RoPE-based%2520methods%2520and%250Aoffers%2520a%2520promising%2520insight%2520into%2520length%2520extrapolation.%2520Project%2520page%253A%250Ahttps%253A//shenzhang2145.github.io/ledit/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEDiT%3A%20Your%20Length-Extrapolatable%20Diffusion%20Transformer%20without%0A%20%20Positional%20Encoding&entry.906535625=Shen%20Zhang%20and%20Siyuan%20Liang%20and%20Yaning%20Tan%20and%20Zhaowei%20Chen%20and%20Linze%20Li%20and%20Ge%20Wu%20and%20Yuhao%20Chen%20and%20Shuheng%20Li%20and%20Zhenyu%20Zhao%20and%20Caihua%20Chen%20and%20Jiajun%20Liang%20and%20Yao%20Tang&entry.1292438233=%20%20Diffusion%20transformers%20%28DiTs%29%20struggle%20to%20generate%20images%20at%20resolutions%0Ahigher%20than%20their%20training%20resolutions.%20The%20primary%20obstacle%20is%20that%20the%0Aexplicit%20positional%20encodings%28PE%29%2C%20such%20as%20RoPE%2C%20need%20extrapolating%20to%20unseen%0Apositions%20which%20degrades%20performance%20when%20the%20inference%20resolution%20differs%20from%0Atraining.%20In%20this%20paper%2C%20We%20propose%20a%20Length-Extrapolatable%20Diffusion%0ATransformer~%28LEDiT%29%20to%20overcome%20this%20limitation.%20LEDiT%20needs%20no%20explicit%20PEs%2C%0Athereby%20avoiding%20PE%20extrapolation.%20The%20key%20innovation%20of%20LEDiT%20lies%20in%20the%20use%0Aof%20causal%20attention.%20We%20demonstrate%20that%20causal%20attention%20can%20implicitly%20encode%0Aglobal%20positional%20information%20and%20show%20that%20such%20information%20facilitates%0Aextrapolation.%20We%20further%20introduce%20a%20locality%20enhancement%20module%2C%20which%0Acaptures%20fine-grained%20local%20information%20to%20complement%20the%20global%20coarse-grained%0Aposition%20information%20encoded%20by%20causal%20attention.%20Experimental%20results%20on%20both%0Aconditional%20and%20text-to-image%20generation%20tasks%20demonstrate%20that%20LEDiT%20supports%0Aup%20to%204x%20resolution%20scaling%20%28e.g.%2C%20from%20256x256%20to%20512x512%29%2C%20achieving%20better%0Aimage%20quality%20compared%20to%20the%20state-of-the-art%20length%20extrapolation%20methods.%20We%0Abelieve%20that%20LEDiT%20marks%20a%20departure%20from%20the%20standard%20RoPE-based%20methods%20and%0Aoffers%20a%20promising%20insight%20into%20length%20extrapolation.%20Project%20page%3A%0Ahttps%3A//shenzhang2145.github.io/ledit/%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04344v3&entry.124074799=Read"},
{"title": "Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor\n  Policies from Single Human Video", "author": "Georgios Tziafas and Jiayun Zhang and Hamidreza Kasaei", "abstract": "  Learning visuomotor policies from expert demonstrations is an important\nfrontier in modern robotics research, however, most popular methods require\ncopious efforts for collecting teleoperation data and struggle to generalize\nout-ofdistribution. Scaling data collection has been explored through\nleveraging human videos, as well as demonstration augmentation techniques. The\nlatter approach typically requires expensive simulation rollouts and trains\npolicies with synthetic image data, therefore introducing a sim-to-real gap. In\nparallel, alternative state representations such as keypoints have shown great\npromise for category-level generalization. In this work, we bring these avenues\ntogether in a unified framework: PAD (Parse-AugmentDistill), for learning\ngeneralizable bimanual policies from a single human video. Our method relies on\nthree steps: (a) parsing a human video demo into a robot-executable\nkeypoint-action trajectory, (b) employing bimanual task-and-motion-planning to\naugment the demonstration at scale without simulators, and (c) distilling the\naugmented trajectories into a keypoint-conditioned policy. Empirically, we\nshowcase that PAD outperforms state-ofthe-art bimanual demonstration\naugmentation works relying on image policies with simulation rollouts, both in\nterms of success rate and sample/cost efficiency. We deploy our framework in\nsix diverse real-world bimanual tasks such as pouring drinks, cleaning trash\nand opening containers, producing one-shot policies that generalize in unseen\nspatial arrangements, object instances and background distractors.\nSupplementary material can be found in the project webpage\nhttps://gtziafas.github.io/PAD_project/.\n", "link": "http://arxiv.org/abs/2509.20286v1", "date": "2025-09-24", "relevancy": 1.7751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6255}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5933}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parse-Augment-Distill%3A%20Learning%20Generalizable%20Bimanual%20Visuomotor%0A%20%20Policies%20from%20Single%20Human%20Video&body=Title%3A%20Parse-Augment-Distill%3A%20Learning%20Generalizable%20Bimanual%20Visuomotor%0A%20%20Policies%20from%20Single%20Human%20Video%0AAuthor%3A%20Georgios%20Tziafas%20and%20Jiayun%20Zhang%20and%20Hamidreza%20Kasaei%0AAbstract%3A%20%20%20Learning%20visuomotor%20policies%20from%20expert%20demonstrations%20is%20an%20important%0Afrontier%20in%20modern%20robotics%20research%2C%20however%2C%20most%20popular%20methods%20require%0Acopious%20efforts%20for%20collecting%20teleoperation%20data%20and%20struggle%20to%20generalize%0Aout-ofdistribution.%20Scaling%20data%20collection%20has%20been%20explored%20through%0Aleveraging%20human%20videos%2C%20as%20well%20as%20demonstration%20augmentation%20techniques.%20The%0Alatter%20approach%20typically%20requires%20expensive%20simulation%20rollouts%20and%20trains%0Apolicies%20with%20synthetic%20image%20data%2C%20therefore%20introducing%20a%20sim-to-real%20gap.%20In%0Aparallel%2C%20alternative%20state%20representations%20such%20as%20keypoints%20have%20shown%20great%0Apromise%20for%20category-level%20generalization.%20In%20this%20work%2C%20we%20bring%20these%20avenues%0Atogether%20in%20a%20unified%20framework%3A%20PAD%20%28Parse-AugmentDistill%29%2C%20for%20learning%0Ageneralizable%20bimanual%20policies%20from%20a%20single%20human%20video.%20Our%20method%20relies%20on%0Athree%20steps%3A%20%28a%29%20parsing%20a%20human%20video%20demo%20into%20a%20robot-executable%0Akeypoint-action%20trajectory%2C%20%28b%29%20employing%20bimanual%20task-and-motion-planning%20to%0Aaugment%20the%20demonstration%20at%20scale%20without%20simulators%2C%20and%20%28c%29%20distilling%20the%0Aaugmented%20trajectories%20into%20a%20keypoint-conditioned%20policy.%20Empirically%2C%20we%0Ashowcase%20that%20PAD%20outperforms%20state-ofthe-art%20bimanual%20demonstration%0Aaugmentation%20works%20relying%20on%20image%20policies%20with%20simulation%20rollouts%2C%20both%20in%0Aterms%20of%20success%20rate%20and%20sample/cost%20efficiency.%20We%20deploy%20our%20framework%20in%0Asix%20diverse%20real-world%20bimanual%20tasks%20such%20as%20pouring%20drinks%2C%20cleaning%20trash%0Aand%20opening%20containers%2C%20producing%20one-shot%20policies%20that%20generalize%20in%20unseen%0Aspatial%20arrangements%2C%20object%20instances%20and%20background%20distractors.%0ASupplementary%20material%20can%20be%20found%20in%20the%20project%20webpage%0Ahttps%3A//gtziafas.github.io/PAD_project/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParse-Augment-Distill%253A%2520Learning%2520Generalizable%2520Bimanual%2520Visuomotor%250A%2520%2520Policies%2520from%2520Single%2520Human%2520Video%26entry.906535625%3DGeorgios%2520Tziafas%2520and%2520Jiayun%2520Zhang%2520and%2520Hamidreza%2520Kasaei%26entry.1292438233%3D%2520%2520Learning%2520visuomotor%2520policies%2520from%2520expert%2520demonstrations%2520is%2520an%2520important%250Afrontier%2520in%2520modern%2520robotics%2520research%252C%2520however%252C%2520most%2520popular%2520methods%2520require%250Acopious%2520efforts%2520for%2520collecting%2520teleoperation%2520data%2520and%2520struggle%2520to%2520generalize%250Aout-ofdistribution.%2520Scaling%2520data%2520collection%2520has%2520been%2520explored%2520through%250Aleveraging%2520human%2520videos%252C%2520as%2520well%2520as%2520demonstration%2520augmentation%2520techniques.%2520The%250Alatter%2520approach%2520typically%2520requires%2520expensive%2520simulation%2520rollouts%2520and%2520trains%250Apolicies%2520with%2520synthetic%2520image%2520data%252C%2520therefore%2520introducing%2520a%2520sim-to-real%2520gap.%2520In%250Aparallel%252C%2520alternative%2520state%2520representations%2520such%2520as%2520keypoints%2520have%2520shown%2520great%250Apromise%2520for%2520category-level%2520generalization.%2520In%2520this%2520work%252C%2520we%2520bring%2520these%2520avenues%250Atogether%2520in%2520a%2520unified%2520framework%253A%2520PAD%2520%2528Parse-AugmentDistill%2529%252C%2520for%2520learning%250Ageneralizable%2520bimanual%2520policies%2520from%2520a%2520single%2520human%2520video.%2520Our%2520method%2520relies%2520on%250Athree%2520steps%253A%2520%2528a%2529%2520parsing%2520a%2520human%2520video%2520demo%2520into%2520a%2520robot-executable%250Akeypoint-action%2520trajectory%252C%2520%2528b%2529%2520employing%2520bimanual%2520task-and-motion-planning%2520to%250Aaugment%2520the%2520demonstration%2520at%2520scale%2520without%2520simulators%252C%2520and%2520%2528c%2529%2520distilling%2520the%250Aaugmented%2520trajectories%2520into%2520a%2520keypoint-conditioned%2520policy.%2520Empirically%252C%2520we%250Ashowcase%2520that%2520PAD%2520outperforms%2520state-ofthe-art%2520bimanual%2520demonstration%250Aaugmentation%2520works%2520relying%2520on%2520image%2520policies%2520with%2520simulation%2520rollouts%252C%2520both%2520in%250Aterms%2520of%2520success%2520rate%2520and%2520sample/cost%2520efficiency.%2520We%2520deploy%2520our%2520framework%2520in%250Asix%2520diverse%2520real-world%2520bimanual%2520tasks%2520such%2520as%2520pouring%2520drinks%252C%2520cleaning%2520trash%250Aand%2520opening%2520containers%252C%2520producing%2520one-shot%2520policies%2520that%2520generalize%2520in%2520unseen%250Aspatial%2520arrangements%252C%2520object%2520instances%2520and%2520background%2520distractors.%250ASupplementary%2520material%2520can%2520be%2520found%2520in%2520the%2520project%2520webpage%250Ahttps%253A//gtziafas.github.io/PAD_project/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parse-Augment-Distill%3A%20Learning%20Generalizable%20Bimanual%20Visuomotor%0A%20%20Policies%20from%20Single%20Human%20Video&entry.906535625=Georgios%20Tziafas%20and%20Jiayun%20Zhang%20and%20Hamidreza%20Kasaei&entry.1292438233=%20%20Learning%20visuomotor%20policies%20from%20expert%20demonstrations%20is%20an%20important%0Afrontier%20in%20modern%20robotics%20research%2C%20however%2C%20most%20popular%20methods%20require%0Acopious%20efforts%20for%20collecting%20teleoperation%20data%20and%20struggle%20to%20generalize%0Aout-ofdistribution.%20Scaling%20data%20collection%20has%20been%20explored%20through%0Aleveraging%20human%20videos%2C%20as%20well%20as%20demonstration%20augmentation%20techniques.%20The%0Alatter%20approach%20typically%20requires%20expensive%20simulation%20rollouts%20and%20trains%0Apolicies%20with%20synthetic%20image%20data%2C%20therefore%20introducing%20a%20sim-to-real%20gap.%20In%0Aparallel%2C%20alternative%20state%20representations%20such%20as%20keypoints%20have%20shown%20great%0Apromise%20for%20category-level%20generalization.%20In%20this%20work%2C%20we%20bring%20these%20avenues%0Atogether%20in%20a%20unified%20framework%3A%20PAD%20%28Parse-AugmentDistill%29%2C%20for%20learning%0Ageneralizable%20bimanual%20policies%20from%20a%20single%20human%20video.%20Our%20method%20relies%20on%0Athree%20steps%3A%20%28a%29%20parsing%20a%20human%20video%20demo%20into%20a%20robot-executable%0Akeypoint-action%20trajectory%2C%20%28b%29%20employing%20bimanual%20task-and-motion-planning%20to%0Aaugment%20the%20demonstration%20at%20scale%20without%20simulators%2C%20and%20%28c%29%20distilling%20the%0Aaugmented%20trajectories%20into%20a%20keypoint-conditioned%20policy.%20Empirically%2C%20we%0Ashowcase%20that%20PAD%20outperforms%20state-ofthe-art%20bimanual%20demonstration%0Aaugmentation%20works%20relying%20on%20image%20policies%20with%20simulation%20rollouts%2C%20both%20in%0Aterms%20of%20success%20rate%20and%20sample/cost%20efficiency.%20We%20deploy%20our%20framework%20in%0Asix%20diverse%20real-world%20bimanual%20tasks%20such%20as%20pouring%20drinks%2C%20cleaning%20trash%0Aand%20opening%20containers%2C%20producing%20one-shot%20policies%20that%20generalize%20in%20unseen%0Aspatial%20arrangements%2C%20object%20instances%20and%20background%20distractors.%0ASupplementary%20material%20can%20be%20found%20in%20the%20project%20webpage%0Ahttps%3A//gtziafas.github.io/PAD_project/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20286v1&entry.124074799=Read"},
{"title": "PGCLODA: Prompt-Guided Graph Contrastive Learning for\n  Oligopeptide-Infectious Disease Association Prediction", "author": "Dayu Tan and Jing Chen and Xiaoping Zhou and Yansen Su and Chunhou Zheng", "abstract": "  Infectious diseases continue to pose a serious threat to public health,\nunderscoring the urgent need for effective computational approaches to screen\nnovel anti-infective agents. Oligopeptides have emerged as promising candidates\nin antimicrobial research due to their structural simplicity, high\nbioavailability, and low susceptibility to resistance. Despite their potential,\ncomputational models specifically designed to predict associations between\noligopeptides and infectious diseases remain scarce. This study introduces a\nprompt-guided graph-based contrastive learning framework (PGCLODA) to uncover\npotential associations. A tripartite graph is constructed with oligopeptides,\nmicrobes, and diseases as nodes, incorporating both structural and semantic\ninformation. To preserve critical regions during contrastive learning, a\nprompt-guided graph augmentation strategy is employed to generate meaningful\npaired views. A dual encoder architecture, integrating Graph Convolutional\nNetwork (GCN) and Transformer, is used to jointly capture local and global\nfeatures. The fused embeddings are subsequently input into a multilayer\nperceptron (MLP) classifier for final prediction. Experimental results on a\nbenchmark dataset indicate that PGCLODA consistently outperforms\nstate-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and\nhyperparameter studies confirm the contribution of each module. Case studies\nfurther validate the generalization ability of PGCLODA and its potential to\nuncover novel, biologically relevant associations. These findings offer\nvaluable insights for mechanism-driven discovery and oligopeptide-based drug\ndevelopment. The source code of PGCLODA is available online at\nhttps://github.com/jjnlcode/PGCLODA.\n", "link": "http://arxiv.org/abs/2509.20290v1", "date": "2025-09-24", "relevancy": 1.772, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4487}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4471}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PGCLODA%3A%20Prompt-Guided%20Graph%20Contrastive%20Learning%20for%0A%20%20Oligopeptide-Infectious%20Disease%20Association%20Prediction&body=Title%3A%20PGCLODA%3A%20Prompt-Guided%20Graph%20Contrastive%20Learning%20for%0A%20%20Oligopeptide-Infectious%20Disease%20Association%20Prediction%0AAuthor%3A%20Dayu%20Tan%20and%20Jing%20Chen%20and%20Xiaoping%20Zhou%20and%20Yansen%20Su%20and%20Chunhou%20Zheng%0AAbstract%3A%20%20%20Infectious%20diseases%20continue%20to%20pose%20a%20serious%20threat%20to%20public%20health%2C%0Aunderscoring%20the%20urgent%20need%20for%20effective%20computational%20approaches%20to%20screen%0Anovel%20anti-infective%20agents.%20Oligopeptides%20have%20emerged%20as%20promising%20candidates%0Ain%20antimicrobial%20research%20due%20to%20their%20structural%20simplicity%2C%20high%0Abioavailability%2C%20and%20low%20susceptibility%20to%20resistance.%20Despite%20their%20potential%2C%0Acomputational%20models%20specifically%20designed%20to%20predict%20associations%20between%0Aoligopeptides%20and%20infectious%20diseases%20remain%20scarce.%20This%20study%20introduces%20a%0Aprompt-guided%20graph-based%20contrastive%20learning%20framework%20%28PGCLODA%29%20to%20uncover%0Apotential%20associations.%20A%20tripartite%20graph%20is%20constructed%20with%20oligopeptides%2C%0Amicrobes%2C%20and%20diseases%20as%20nodes%2C%20incorporating%20both%20structural%20and%20semantic%0Ainformation.%20To%20preserve%20critical%20regions%20during%20contrastive%20learning%2C%20a%0Aprompt-guided%20graph%20augmentation%20strategy%20is%20employed%20to%20generate%20meaningful%0Apaired%20views.%20A%20dual%20encoder%20architecture%2C%20integrating%20Graph%20Convolutional%0ANetwork%20%28GCN%29%20and%20Transformer%2C%20is%20used%20to%20jointly%20capture%20local%20and%20global%0Afeatures.%20The%20fused%20embeddings%20are%20subsequently%20input%20into%20a%20multilayer%0Aperceptron%20%28MLP%29%20classifier%20for%20final%20prediction.%20Experimental%20results%20on%20a%0Abenchmark%20dataset%20indicate%20that%20PGCLODA%20consistently%20outperforms%0Astate-of-the-art%20models%20in%20AUROC%2C%20AUPRC%2C%20and%20accuracy.%20Ablation%20and%0Ahyperparameter%20studies%20confirm%20the%20contribution%20of%20each%20module.%20Case%20studies%0Afurther%20validate%20the%20generalization%20ability%20of%20PGCLODA%20and%20its%20potential%20to%0Auncover%20novel%2C%20biologically%20relevant%20associations.%20These%20findings%20offer%0Avaluable%20insights%20for%20mechanism-driven%20discovery%20and%20oligopeptide-based%20drug%0Adevelopment.%20The%20source%20code%20of%20PGCLODA%20is%20available%20online%20at%0Ahttps%3A//github.com/jjnlcode/PGCLODA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPGCLODA%253A%2520Prompt-Guided%2520Graph%2520Contrastive%2520Learning%2520for%250A%2520%2520Oligopeptide-Infectious%2520Disease%2520Association%2520Prediction%26entry.906535625%3DDayu%2520Tan%2520and%2520Jing%2520Chen%2520and%2520Xiaoping%2520Zhou%2520and%2520Yansen%2520Su%2520and%2520Chunhou%2520Zheng%26entry.1292438233%3D%2520%2520Infectious%2520diseases%2520continue%2520to%2520pose%2520a%2520serious%2520threat%2520to%2520public%2520health%252C%250Aunderscoring%2520the%2520urgent%2520need%2520for%2520effective%2520computational%2520approaches%2520to%2520screen%250Anovel%2520anti-infective%2520agents.%2520Oligopeptides%2520have%2520emerged%2520as%2520promising%2520candidates%250Ain%2520antimicrobial%2520research%2520due%2520to%2520their%2520structural%2520simplicity%252C%2520high%250Abioavailability%252C%2520and%2520low%2520susceptibility%2520to%2520resistance.%2520Despite%2520their%2520potential%252C%250Acomputational%2520models%2520specifically%2520designed%2520to%2520predict%2520associations%2520between%250Aoligopeptides%2520and%2520infectious%2520diseases%2520remain%2520scarce.%2520This%2520study%2520introduces%2520a%250Aprompt-guided%2520graph-based%2520contrastive%2520learning%2520framework%2520%2528PGCLODA%2529%2520to%2520uncover%250Apotential%2520associations.%2520A%2520tripartite%2520graph%2520is%2520constructed%2520with%2520oligopeptides%252C%250Amicrobes%252C%2520and%2520diseases%2520as%2520nodes%252C%2520incorporating%2520both%2520structural%2520and%2520semantic%250Ainformation.%2520To%2520preserve%2520critical%2520regions%2520during%2520contrastive%2520learning%252C%2520a%250Aprompt-guided%2520graph%2520augmentation%2520strategy%2520is%2520employed%2520to%2520generate%2520meaningful%250Apaired%2520views.%2520A%2520dual%2520encoder%2520architecture%252C%2520integrating%2520Graph%2520Convolutional%250ANetwork%2520%2528GCN%2529%2520and%2520Transformer%252C%2520is%2520used%2520to%2520jointly%2520capture%2520local%2520and%2520global%250Afeatures.%2520The%2520fused%2520embeddings%2520are%2520subsequently%2520input%2520into%2520a%2520multilayer%250Aperceptron%2520%2528MLP%2529%2520classifier%2520for%2520final%2520prediction.%2520Experimental%2520results%2520on%2520a%250Abenchmark%2520dataset%2520indicate%2520that%2520PGCLODA%2520consistently%2520outperforms%250Astate-of-the-art%2520models%2520in%2520AUROC%252C%2520AUPRC%252C%2520and%2520accuracy.%2520Ablation%2520and%250Ahyperparameter%2520studies%2520confirm%2520the%2520contribution%2520of%2520each%2520module.%2520Case%2520studies%250Afurther%2520validate%2520the%2520generalization%2520ability%2520of%2520PGCLODA%2520and%2520its%2520potential%2520to%250Auncover%2520novel%252C%2520biologically%2520relevant%2520associations.%2520These%2520findings%2520offer%250Avaluable%2520insights%2520for%2520mechanism-driven%2520discovery%2520and%2520oligopeptide-based%2520drug%250Adevelopment.%2520The%2520source%2520code%2520of%2520PGCLODA%2520is%2520available%2520online%2520at%250Ahttps%253A//github.com/jjnlcode/PGCLODA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PGCLODA%3A%20Prompt-Guided%20Graph%20Contrastive%20Learning%20for%0A%20%20Oligopeptide-Infectious%20Disease%20Association%20Prediction&entry.906535625=Dayu%20Tan%20and%20Jing%20Chen%20and%20Xiaoping%20Zhou%20and%20Yansen%20Su%20and%20Chunhou%20Zheng&entry.1292438233=%20%20Infectious%20diseases%20continue%20to%20pose%20a%20serious%20threat%20to%20public%20health%2C%0Aunderscoring%20the%20urgent%20need%20for%20effective%20computational%20approaches%20to%20screen%0Anovel%20anti-infective%20agents.%20Oligopeptides%20have%20emerged%20as%20promising%20candidates%0Ain%20antimicrobial%20research%20due%20to%20their%20structural%20simplicity%2C%20high%0Abioavailability%2C%20and%20low%20susceptibility%20to%20resistance.%20Despite%20their%20potential%2C%0Acomputational%20models%20specifically%20designed%20to%20predict%20associations%20between%0Aoligopeptides%20and%20infectious%20diseases%20remain%20scarce.%20This%20study%20introduces%20a%0Aprompt-guided%20graph-based%20contrastive%20learning%20framework%20%28PGCLODA%29%20to%20uncover%0Apotential%20associations.%20A%20tripartite%20graph%20is%20constructed%20with%20oligopeptides%2C%0Amicrobes%2C%20and%20diseases%20as%20nodes%2C%20incorporating%20both%20structural%20and%20semantic%0Ainformation.%20To%20preserve%20critical%20regions%20during%20contrastive%20learning%2C%20a%0Aprompt-guided%20graph%20augmentation%20strategy%20is%20employed%20to%20generate%20meaningful%0Apaired%20views.%20A%20dual%20encoder%20architecture%2C%20integrating%20Graph%20Convolutional%0ANetwork%20%28GCN%29%20and%20Transformer%2C%20is%20used%20to%20jointly%20capture%20local%20and%20global%0Afeatures.%20The%20fused%20embeddings%20are%20subsequently%20input%20into%20a%20multilayer%0Aperceptron%20%28MLP%29%20classifier%20for%20final%20prediction.%20Experimental%20results%20on%20a%0Abenchmark%20dataset%20indicate%20that%20PGCLODA%20consistently%20outperforms%0Astate-of-the-art%20models%20in%20AUROC%2C%20AUPRC%2C%20and%20accuracy.%20Ablation%20and%0Ahyperparameter%20studies%20confirm%20the%20contribution%20of%20each%20module.%20Case%20studies%0Afurther%20validate%20the%20generalization%20ability%20of%20PGCLODA%20and%20its%20potential%20to%0Auncover%20novel%2C%20biologically%20relevant%20associations.%20These%20findings%20offer%0Avaluable%20insights%20for%20mechanism-driven%20discovery%20and%20oligopeptide-based%20drug%0Adevelopment.%20The%20source%20code%20of%20PGCLODA%20is%20available%20online%20at%0Ahttps%3A//github.com/jjnlcode/PGCLODA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20290v1&entry.124074799=Read"},
{"title": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal", "author": "Maria Teleki and Sai Janjur and Haoran Liu and Oliver Grabner and Ketan Verma and Thomas Docog and Xiangjue Dong and Lingfeng Shi and Cong Wang and Stephanie Birkelbach and Jason Kim and Yin Zhang and James Caverlee", "abstract": "  Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies.\n", "link": "http://arxiv.org/abs/2509.20319v1", "date": "2025-09-24", "relevancy": 1.7714, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4461}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Z-Scores%3A%20A%20Metric%20for%20Linguistically%20Assessing%20Disfluency%20Removal&body=Title%3A%20Z-Scores%3A%20A%20Metric%20for%20Linguistically%20Assessing%20Disfluency%20Removal%0AAuthor%3A%20Maria%20Teleki%20and%20Sai%20Janjur%20and%20Haoran%20Liu%20and%20Oliver%20Grabner%20and%20Ketan%20Verma%20and%20Thomas%20Docog%20and%20Xiangjue%20Dong%20and%20Lingfeng%20Shi%20and%20Cong%20Wang%20and%20Stephanie%20Birkelbach%20and%20Jason%20Kim%20and%20Yin%20Zhang%20and%20James%20Caverlee%0AAbstract%3A%20%20%20Evaluating%20disfluency%20removal%20in%20speech%20requires%20more%20than%20aggregate%0Atoken-level%20scores.%20Traditional%20word-based%20metrics%20such%20as%20precision%2C%20recall%2C%0Aand%20F1%20%28E-Scores%29%20capture%20overall%20performance%20but%20cannot%20reveal%20why%20models%0Asucceed%20or%20fail.%20We%20introduce%20Z-Scores%2C%20a%20span-level%20linguistically-grounded%0Aevaluation%20metric%20that%20categorizes%20system%20behavior%20across%20distinct%20disfluency%0Atypes%20%28EDITED%2C%20INTJ%2C%20PRN%29.%20Our%20deterministic%20alignment%20module%20enables%20robust%0Amapping%20between%20generated%20text%20and%20disfluent%20transcripts%2C%20allowing%20Z-Scores%20to%0Aexpose%20systematic%20weaknesses%20that%20word-level%20metrics%20obscure.%20By%20providing%0Acategory-specific%20diagnostics%2C%20Z-Scores%20enable%20researchers%20to%20identify%20model%0Afailure%20modes%20and%20design%20targeted%20interventions%20--%20such%20as%20tailored%20prompts%20or%0Adata%20augmentation%20--%20yielding%20measurable%20performance%20improvements.%20A%20case%20study%0Awith%20LLMs%20shows%20that%20Z-Scores%20uncover%20challenges%20with%20INTJ%20and%20PRN%20disfluencies%0Ahidden%20in%20aggregate%20F1%2C%20directly%20informing%20model%20refinement%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZ-Scores%253A%2520A%2520Metric%2520for%2520Linguistically%2520Assessing%2520Disfluency%2520Removal%26entry.906535625%3DMaria%2520Teleki%2520and%2520Sai%2520Janjur%2520and%2520Haoran%2520Liu%2520and%2520Oliver%2520Grabner%2520and%2520Ketan%2520Verma%2520and%2520Thomas%2520Docog%2520and%2520Xiangjue%2520Dong%2520and%2520Lingfeng%2520Shi%2520and%2520Cong%2520Wang%2520and%2520Stephanie%2520Birkelbach%2520and%2520Jason%2520Kim%2520and%2520Yin%2520Zhang%2520and%2520James%2520Caverlee%26entry.1292438233%3D%2520%2520Evaluating%2520disfluency%2520removal%2520in%2520speech%2520requires%2520more%2520than%2520aggregate%250Atoken-level%2520scores.%2520Traditional%2520word-based%2520metrics%2520such%2520as%2520precision%252C%2520recall%252C%250Aand%2520F1%2520%2528E-Scores%2529%2520capture%2520overall%2520performance%2520but%2520cannot%2520reveal%2520why%2520models%250Asucceed%2520or%2520fail.%2520We%2520introduce%2520Z-Scores%252C%2520a%2520span-level%2520linguistically-grounded%250Aevaluation%2520metric%2520that%2520categorizes%2520system%2520behavior%2520across%2520distinct%2520disfluency%250Atypes%2520%2528EDITED%252C%2520INTJ%252C%2520PRN%2529.%2520Our%2520deterministic%2520alignment%2520module%2520enables%2520robust%250Amapping%2520between%2520generated%2520text%2520and%2520disfluent%2520transcripts%252C%2520allowing%2520Z-Scores%2520to%250Aexpose%2520systematic%2520weaknesses%2520that%2520word-level%2520metrics%2520obscure.%2520By%2520providing%250Acategory-specific%2520diagnostics%252C%2520Z-Scores%2520enable%2520researchers%2520to%2520identify%2520model%250Afailure%2520modes%2520and%2520design%2520targeted%2520interventions%2520--%2520such%2520as%2520tailored%2520prompts%2520or%250Adata%2520augmentation%2520--%2520yielding%2520measurable%2520performance%2520improvements.%2520A%2520case%2520study%250Awith%2520LLMs%2520shows%2520that%2520Z-Scores%2520uncover%2520challenges%2520with%2520INTJ%2520and%2520PRN%2520disfluencies%250Ahidden%2520in%2520aggregate%2520F1%252C%2520directly%2520informing%2520model%2520refinement%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Z-Scores%3A%20A%20Metric%20for%20Linguistically%20Assessing%20Disfluency%20Removal&entry.906535625=Maria%20Teleki%20and%20Sai%20Janjur%20and%20Haoran%20Liu%20and%20Oliver%20Grabner%20and%20Ketan%20Verma%20and%20Thomas%20Docog%20and%20Xiangjue%20Dong%20and%20Lingfeng%20Shi%20and%20Cong%20Wang%20and%20Stephanie%20Birkelbach%20and%20Jason%20Kim%20and%20Yin%20Zhang%20and%20James%20Caverlee&entry.1292438233=%20%20Evaluating%20disfluency%20removal%20in%20speech%20requires%20more%20than%20aggregate%0Atoken-level%20scores.%20Traditional%20word-based%20metrics%20such%20as%20precision%2C%20recall%2C%0Aand%20F1%20%28E-Scores%29%20capture%20overall%20performance%20but%20cannot%20reveal%20why%20models%0Asucceed%20or%20fail.%20We%20introduce%20Z-Scores%2C%20a%20span-level%20linguistically-grounded%0Aevaluation%20metric%20that%20categorizes%20system%20behavior%20across%20distinct%20disfluency%0Atypes%20%28EDITED%2C%20INTJ%2C%20PRN%29.%20Our%20deterministic%20alignment%20module%20enables%20robust%0Amapping%20between%20generated%20text%20and%20disfluent%20transcripts%2C%20allowing%20Z-Scores%20to%0Aexpose%20systematic%20weaknesses%20that%20word-level%20metrics%20obscure.%20By%20providing%0Acategory-specific%20diagnostics%2C%20Z-Scores%20enable%20researchers%20to%20identify%20model%0Afailure%20modes%20and%20design%20targeted%20interventions%20--%20such%20as%20tailored%20prompts%20or%0Adata%20augmentation%20--%20yielding%20measurable%20performance%20improvements.%20A%20case%20study%0Awith%20LLMs%20shows%20that%20Z-Scores%20uncover%20challenges%20with%20INTJ%20and%20PRN%20disfluencies%0Ahidden%20in%20aggregate%20F1%2C%20directly%20informing%20model%20refinement%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20319v1&entry.124074799=Read"},
{"title": "Efficiently learning depth-3 circuits via quantum agnostic boosting", "author": "Srinivasan Arunachalam and Arkopal Dutt and Alexandru Gheorghiu and Michael de Oliveira", "abstract": "  We initiate the study of quantum agnostic learning of phase states with\nrespect to a function class $\\mathsf{C}\\subseteq \\{c:\\{0,1\\}^n\\rightarrow\n\\{0,1\\}\\}$: given copies of an unknown $n$-qubit state $|\\psi\\rangle$ which has\nfidelity $\\textsf{opt}$ with a phase state\n$|\\phi_c\\rangle=\\frac{1}{\\sqrt{2^n}}\\sum_{x\\in \\{0,1\\}^n}(-1)^{c(x)}|x\\rangle$\nfor some $c\\in \\mathsf{C}$, output $|\\phi\\rangle$ which has fidelity $|\\langle\n\\phi | \\psi \\rangle|^2 \\geq \\textsf{opt}-\\varepsilon$. To this end, we give\nagnostic learning protocols for the following classes: (i) Size-$t$ decision\ntrees which runs in time $\\textsf{poly}(n,t,1/\\varepsilon)$. This also implies\n$k$-juntas can be agnostically learned in time\n$\\textsf{poly}(n,2^k,1/\\varepsilon)$. (ii) $s$-term DNF formulas in time\n$\\textsf{poly}(n,(s/\\varepsilon)^{\\log \\log (s/\\varepsilon) \\cdot\n\\log(1/\\varepsilon)})$.\n  Our main technical contribution is a quantum agnostic boosting protocol which\nconverts a weak agnostic learner, which outputs a parity state $|\\phi\\rangle$\nsuch that $|\\langle \\phi|\\psi\\rangle|^2\\geq \\textsf{opt}/\\textsf{poly}(n)$,\ninto a strong learner which outputs a superposition of parity states\n$|\\phi'\\rangle$ such that $|\\langle \\phi'|\\psi\\rangle|^2\\geq \\textsf{opt} -\n\\varepsilon$.\n  Using quantum agnostic boosting, we obtain a $n^{O(\\log \\log n \\cdot\n\\log(1/\\varepsilon))}$-time algorithm for learning $\\textsf{poly}(n)$-sized\ndepth-$3$ circuits (consisting of $\\textsf{AND}$, $\\textsf{OR}$, $\\textsf{NOT}$\ngates) in the uniform $\\textsf{PAC}$ model given quantum examples, which is\nnear-polynomial time for constant $\\varepsilon$. Classically, obtaining an\nalgorithm with a similar complexity has been an open question in the\n$\\textsf{PAC}$ model and our work answers this given quantum examples.\n", "link": "http://arxiv.org/abs/2509.14461v2", "date": "2025-09-24", "relevancy": 1.7292, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4454}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4239}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20learning%20depth-3%20circuits%20via%20quantum%20agnostic%20boosting&body=Title%3A%20Efficiently%20learning%20depth-3%20circuits%20via%20quantum%20agnostic%20boosting%0AAuthor%3A%20Srinivasan%20Arunachalam%20and%20Arkopal%20Dutt%20and%20Alexandru%20Gheorghiu%20and%20Michael%20de%20Oliveira%0AAbstract%3A%20%20%20We%20initiate%20the%20study%20of%20quantum%20agnostic%20learning%20of%20phase%20states%20with%0Arespect%20to%20a%20function%20class%20%24%5Cmathsf%7BC%7D%5Csubseteq%20%5C%7Bc%3A%5C%7B0%2C1%5C%7D%5En%5Crightarrow%0A%5C%7B0%2C1%5C%7D%5C%7D%24%3A%20given%20copies%20of%20an%20unknown%20%24n%24-qubit%20state%20%24%7C%5Cpsi%5Crangle%24%20which%20has%0Afidelity%20%24%5Ctextsf%7Bopt%7D%24%20with%20a%20phase%20state%0A%24%7C%5Cphi_c%5Crangle%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5En%7D%7D%5Csum_%7Bx%5Cin%20%5C%7B0%2C1%5C%7D%5En%7D%28-1%29%5E%7Bc%28x%29%7D%7Cx%5Crangle%24%0Afor%20some%20%24c%5Cin%20%5Cmathsf%7BC%7D%24%2C%20output%20%24%7C%5Cphi%5Crangle%24%20which%20has%20fidelity%20%24%7C%5Clangle%0A%5Cphi%20%7C%20%5Cpsi%20%5Crangle%7C%5E2%20%5Cgeq%20%5Ctextsf%7Bopt%7D-%5Cvarepsilon%24.%20To%20this%20end%2C%20we%20give%0Aagnostic%20learning%20protocols%20for%20the%20following%20classes%3A%20%28i%29%20Size-%24t%24%20decision%0Atrees%20which%20runs%20in%20time%20%24%5Ctextsf%7Bpoly%7D%28n%2Ct%2C1/%5Cvarepsilon%29%24.%20This%20also%20implies%0A%24k%24-juntas%20can%20be%20agnostically%20learned%20in%20time%0A%24%5Ctextsf%7Bpoly%7D%28n%2C2%5Ek%2C1/%5Cvarepsilon%29%24.%20%28ii%29%20%24s%24-term%20DNF%20formulas%20in%20time%0A%24%5Ctextsf%7Bpoly%7D%28n%2C%28s/%5Cvarepsilon%29%5E%7B%5Clog%20%5Clog%20%28s/%5Cvarepsilon%29%20%5Ccdot%0A%5Clog%281/%5Cvarepsilon%29%7D%29%24.%0A%20%20Our%20main%20technical%20contribution%20is%20a%20quantum%20agnostic%20boosting%20protocol%20which%0Aconverts%20a%20weak%20agnostic%20learner%2C%20which%20outputs%20a%20parity%20state%20%24%7C%5Cphi%5Crangle%24%0Asuch%20that%20%24%7C%5Clangle%20%5Cphi%7C%5Cpsi%5Crangle%7C%5E2%5Cgeq%20%5Ctextsf%7Bopt%7D/%5Ctextsf%7Bpoly%7D%28n%29%24%2C%0Ainto%20a%20strong%20learner%20which%20outputs%20a%20superposition%20of%20parity%20states%0A%24%7C%5Cphi%27%5Crangle%24%20such%20that%20%24%7C%5Clangle%20%5Cphi%27%7C%5Cpsi%5Crangle%7C%5E2%5Cgeq%20%5Ctextsf%7Bopt%7D%20-%0A%5Cvarepsilon%24.%0A%20%20Using%20quantum%20agnostic%20boosting%2C%20we%20obtain%20a%20%24n%5E%7BO%28%5Clog%20%5Clog%20n%20%5Ccdot%0A%5Clog%281/%5Cvarepsilon%29%29%7D%24-time%20algorithm%20for%20learning%20%24%5Ctextsf%7Bpoly%7D%28n%29%24-sized%0Adepth-%243%24%20circuits%20%28consisting%20of%20%24%5Ctextsf%7BAND%7D%24%2C%20%24%5Ctextsf%7BOR%7D%24%2C%20%24%5Ctextsf%7BNOT%7D%24%0Agates%29%20in%20the%20uniform%20%24%5Ctextsf%7BPAC%7D%24%20model%20given%20quantum%20examples%2C%20which%20is%0Anear-polynomial%20time%20for%20constant%20%24%5Cvarepsilon%24.%20Classically%2C%20obtaining%20an%0Aalgorithm%20with%20a%20similar%20complexity%20has%20been%20an%20open%20question%20in%20the%0A%24%5Ctextsf%7BPAC%7D%24%20model%20and%20our%20work%20answers%20this%20given%20quantum%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.14461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520learning%2520depth-3%2520circuits%2520via%2520quantum%2520agnostic%2520boosting%26entry.906535625%3DSrinivasan%2520Arunachalam%2520and%2520Arkopal%2520Dutt%2520and%2520Alexandru%2520Gheorghiu%2520and%2520Michael%2520de%2520Oliveira%26entry.1292438233%3D%2520%2520We%2520initiate%2520the%2520study%2520of%2520quantum%2520agnostic%2520learning%2520of%2520phase%2520states%2520with%250Arespect%2520to%2520a%2520function%2520class%2520%2524%255Cmathsf%257BC%257D%255Csubseteq%2520%255C%257Bc%253A%255C%257B0%252C1%255C%257D%255En%255Crightarrow%250A%255C%257B0%252C1%255C%257D%255C%257D%2524%253A%2520given%2520copies%2520of%2520an%2520unknown%2520%2524n%2524-qubit%2520state%2520%2524%257C%255Cpsi%255Crangle%2524%2520which%2520has%250Afidelity%2520%2524%255Ctextsf%257Bopt%257D%2524%2520with%2520a%2520phase%2520state%250A%2524%257C%255Cphi_c%255Crangle%253D%255Cfrac%257B1%257D%257B%255Csqrt%257B2%255En%257D%257D%255Csum_%257Bx%255Cin%2520%255C%257B0%252C1%255C%257D%255En%257D%2528-1%2529%255E%257Bc%2528x%2529%257D%257Cx%255Crangle%2524%250Afor%2520some%2520%2524c%255Cin%2520%255Cmathsf%257BC%257D%2524%252C%2520output%2520%2524%257C%255Cphi%255Crangle%2524%2520which%2520has%2520fidelity%2520%2524%257C%255Clangle%250A%255Cphi%2520%257C%2520%255Cpsi%2520%255Crangle%257C%255E2%2520%255Cgeq%2520%255Ctextsf%257Bopt%257D-%255Cvarepsilon%2524.%2520To%2520this%2520end%252C%2520we%2520give%250Aagnostic%2520learning%2520protocols%2520for%2520the%2520following%2520classes%253A%2520%2528i%2529%2520Size-%2524t%2524%2520decision%250Atrees%2520which%2520runs%2520in%2520time%2520%2524%255Ctextsf%257Bpoly%257D%2528n%252Ct%252C1/%255Cvarepsilon%2529%2524.%2520This%2520also%2520implies%250A%2524k%2524-juntas%2520can%2520be%2520agnostically%2520learned%2520in%2520time%250A%2524%255Ctextsf%257Bpoly%257D%2528n%252C2%255Ek%252C1/%255Cvarepsilon%2529%2524.%2520%2528ii%2529%2520%2524s%2524-term%2520DNF%2520formulas%2520in%2520time%250A%2524%255Ctextsf%257Bpoly%257D%2528n%252C%2528s/%255Cvarepsilon%2529%255E%257B%255Clog%2520%255Clog%2520%2528s/%255Cvarepsilon%2529%2520%255Ccdot%250A%255Clog%25281/%255Cvarepsilon%2529%257D%2529%2524.%250A%2520%2520Our%2520main%2520technical%2520contribution%2520is%2520a%2520quantum%2520agnostic%2520boosting%2520protocol%2520which%250Aconverts%2520a%2520weak%2520agnostic%2520learner%252C%2520which%2520outputs%2520a%2520parity%2520state%2520%2524%257C%255Cphi%255Crangle%2524%250Asuch%2520that%2520%2524%257C%255Clangle%2520%255Cphi%257C%255Cpsi%255Crangle%257C%255E2%255Cgeq%2520%255Ctextsf%257Bopt%257D/%255Ctextsf%257Bpoly%257D%2528n%2529%2524%252C%250Ainto%2520a%2520strong%2520learner%2520which%2520outputs%2520a%2520superposition%2520of%2520parity%2520states%250A%2524%257C%255Cphi%2527%255Crangle%2524%2520such%2520that%2520%2524%257C%255Clangle%2520%255Cphi%2527%257C%255Cpsi%255Crangle%257C%255E2%255Cgeq%2520%255Ctextsf%257Bopt%257D%2520-%250A%255Cvarepsilon%2524.%250A%2520%2520Using%2520quantum%2520agnostic%2520boosting%252C%2520we%2520obtain%2520a%2520%2524n%255E%257BO%2528%255Clog%2520%255Clog%2520n%2520%255Ccdot%250A%255Clog%25281/%255Cvarepsilon%2529%2529%257D%2524-time%2520algorithm%2520for%2520learning%2520%2524%255Ctextsf%257Bpoly%257D%2528n%2529%2524-sized%250Adepth-%25243%2524%2520circuits%2520%2528consisting%2520of%2520%2524%255Ctextsf%257BAND%257D%2524%252C%2520%2524%255Ctextsf%257BOR%257D%2524%252C%2520%2524%255Ctextsf%257BNOT%257D%2524%250Agates%2529%2520in%2520the%2520uniform%2520%2524%255Ctextsf%257BPAC%257D%2524%2520model%2520given%2520quantum%2520examples%252C%2520which%2520is%250Anear-polynomial%2520time%2520for%2520constant%2520%2524%255Cvarepsilon%2524.%2520Classically%252C%2520obtaining%2520an%250Aalgorithm%2520with%2520a%2520similar%2520complexity%2520has%2520been%2520an%2520open%2520question%2520in%2520the%250A%2524%255Ctextsf%257BPAC%257D%2524%2520model%2520and%2520our%2520work%2520answers%2520this%2520given%2520quantum%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20learning%20depth-3%20circuits%20via%20quantum%20agnostic%20boosting&entry.906535625=Srinivasan%20Arunachalam%20and%20Arkopal%20Dutt%20and%20Alexandru%20Gheorghiu%20and%20Michael%20de%20Oliveira&entry.1292438233=%20%20We%20initiate%20the%20study%20of%20quantum%20agnostic%20learning%20of%20phase%20states%20with%0Arespect%20to%20a%20function%20class%20%24%5Cmathsf%7BC%7D%5Csubseteq%20%5C%7Bc%3A%5C%7B0%2C1%5C%7D%5En%5Crightarrow%0A%5C%7B0%2C1%5C%7D%5C%7D%24%3A%20given%20copies%20of%20an%20unknown%20%24n%24-qubit%20state%20%24%7C%5Cpsi%5Crangle%24%20which%20has%0Afidelity%20%24%5Ctextsf%7Bopt%7D%24%20with%20a%20phase%20state%0A%24%7C%5Cphi_c%5Crangle%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5En%7D%7D%5Csum_%7Bx%5Cin%20%5C%7B0%2C1%5C%7D%5En%7D%28-1%29%5E%7Bc%28x%29%7D%7Cx%5Crangle%24%0Afor%20some%20%24c%5Cin%20%5Cmathsf%7BC%7D%24%2C%20output%20%24%7C%5Cphi%5Crangle%24%20which%20has%20fidelity%20%24%7C%5Clangle%0A%5Cphi%20%7C%20%5Cpsi%20%5Crangle%7C%5E2%20%5Cgeq%20%5Ctextsf%7Bopt%7D-%5Cvarepsilon%24.%20To%20this%20end%2C%20we%20give%0Aagnostic%20learning%20protocols%20for%20the%20following%20classes%3A%20%28i%29%20Size-%24t%24%20decision%0Atrees%20which%20runs%20in%20time%20%24%5Ctextsf%7Bpoly%7D%28n%2Ct%2C1/%5Cvarepsilon%29%24.%20This%20also%20implies%0A%24k%24-juntas%20can%20be%20agnostically%20learned%20in%20time%0A%24%5Ctextsf%7Bpoly%7D%28n%2C2%5Ek%2C1/%5Cvarepsilon%29%24.%20%28ii%29%20%24s%24-term%20DNF%20formulas%20in%20time%0A%24%5Ctextsf%7Bpoly%7D%28n%2C%28s/%5Cvarepsilon%29%5E%7B%5Clog%20%5Clog%20%28s/%5Cvarepsilon%29%20%5Ccdot%0A%5Clog%281/%5Cvarepsilon%29%7D%29%24.%0A%20%20Our%20main%20technical%20contribution%20is%20a%20quantum%20agnostic%20boosting%20protocol%20which%0Aconverts%20a%20weak%20agnostic%20learner%2C%20which%20outputs%20a%20parity%20state%20%24%7C%5Cphi%5Crangle%24%0Asuch%20that%20%24%7C%5Clangle%20%5Cphi%7C%5Cpsi%5Crangle%7C%5E2%5Cgeq%20%5Ctextsf%7Bopt%7D/%5Ctextsf%7Bpoly%7D%28n%29%24%2C%0Ainto%20a%20strong%20learner%20which%20outputs%20a%20superposition%20of%20parity%20states%0A%24%7C%5Cphi%27%5Crangle%24%20such%20that%20%24%7C%5Clangle%20%5Cphi%27%7C%5Cpsi%5Crangle%7C%5E2%5Cgeq%20%5Ctextsf%7Bopt%7D%20-%0A%5Cvarepsilon%24.%0A%20%20Using%20quantum%20agnostic%20boosting%2C%20we%20obtain%20a%20%24n%5E%7BO%28%5Clog%20%5Clog%20n%20%5Ccdot%0A%5Clog%281/%5Cvarepsilon%29%29%7D%24-time%20algorithm%20for%20learning%20%24%5Ctextsf%7Bpoly%7D%28n%29%24-sized%0Adepth-%243%24%20circuits%20%28consisting%20of%20%24%5Ctextsf%7BAND%7D%24%2C%20%24%5Ctextsf%7BOR%7D%24%2C%20%24%5Ctextsf%7BNOT%7D%24%0Agates%29%20in%20the%20uniform%20%24%5Ctextsf%7BPAC%7D%24%20model%20given%20quantum%20examples%2C%20which%20is%0Anear-polynomial%20time%20for%20constant%20%24%5Cvarepsilon%24.%20Classically%2C%20obtaining%20an%0Aalgorithm%20with%20a%20similar%20complexity%20has%20been%20an%20open%20question%20in%20the%0A%24%5Ctextsf%7BPAC%7D%24%20model%20and%20our%20work%20answers%20this%20given%20quantum%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.14461v2&entry.124074799=Read"},
{"title": "Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in\n  Evaluation and Meta-Evaluation of Machine Translation", "author": "Behzad Shayegh and Jan-Thorsten Peter and David Vilar and Tobias Domhan and Juraj Juraska and Markus Freitag and Lili Mou", "abstract": "  We investigate the tradeoff between adequacy and fluency in machine\ntranslation. We show the severity of this tradeoff at the evaluation level and\nanalyze where popular metrics fall within it. Essentially, current metrics\ngenerally lean toward adequacy, meaning that their scores correlate more\nstrongly with the adequacy of translations than with fluency. More importantly,\nwe find that this tradeoff also persists at the meta-evaluation level, and that\nthe standard WMT meta-evaluation favors adequacy-oriented metrics over\nfluency-oriented ones. We show that this bias is partially attributed to the\ncomposition of the systems included in the meta-evaluation datasets. To control\nthis bias, we propose a method that synthesizes translation systems in\nmeta-evaluation. Our findings highlight the importance of understanding this\ntradeoff in meta-evaluation and its impact on metric rankings.\n", "link": "http://arxiv.org/abs/2509.20287v1", "date": "2025-09-24", "relevancy": 1.6966, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feeding%20Two%20Birds%20or%20Favoring%20One%3F%20Adequacy-Fluency%20Tradeoffs%20in%0A%20%20Evaluation%20and%20Meta-Evaluation%20of%20Machine%20Translation&body=Title%3A%20Feeding%20Two%20Birds%20or%20Favoring%20One%3F%20Adequacy-Fluency%20Tradeoffs%20in%0A%20%20Evaluation%20and%20Meta-Evaluation%20of%20Machine%20Translation%0AAuthor%3A%20Behzad%20Shayegh%20and%20Jan-Thorsten%20Peter%20and%20David%20Vilar%20and%20Tobias%20Domhan%20and%20Juraj%20Juraska%20and%20Markus%20Freitag%20and%20Lili%20Mou%0AAbstract%3A%20%20%20We%20investigate%20the%20tradeoff%20between%20adequacy%20and%20fluency%20in%20machine%0Atranslation.%20We%20show%20the%20severity%20of%20this%20tradeoff%20at%20the%20evaluation%20level%20and%0Aanalyze%20where%20popular%20metrics%20fall%20within%20it.%20Essentially%2C%20current%20metrics%0Agenerally%20lean%20toward%20adequacy%2C%20meaning%20that%20their%20scores%20correlate%20more%0Astrongly%20with%20the%20adequacy%20of%20translations%20than%20with%20fluency.%20More%20importantly%2C%0Awe%20find%20that%20this%20tradeoff%20also%20persists%20at%20the%20meta-evaluation%20level%2C%20and%20that%0Athe%20standard%20WMT%20meta-evaluation%20favors%20adequacy-oriented%20metrics%20over%0Afluency-oriented%20ones.%20We%20show%20that%20this%20bias%20is%20partially%20attributed%20to%20the%0Acomposition%20of%20the%20systems%20included%20in%20the%20meta-evaluation%20datasets.%20To%20control%0Athis%20bias%2C%20we%20propose%20a%20method%20that%20synthesizes%20translation%20systems%20in%0Ameta-evaluation.%20Our%20findings%20highlight%20the%20importance%20of%20understanding%20this%0Atradeoff%20in%20meta-evaluation%20and%20its%20impact%20on%20metric%20rankings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeeding%2520Two%2520Birds%2520or%2520Favoring%2520One%253F%2520Adequacy-Fluency%2520Tradeoffs%2520in%250A%2520%2520Evaluation%2520and%2520Meta-Evaluation%2520of%2520Machine%2520Translation%26entry.906535625%3DBehzad%2520Shayegh%2520and%2520Jan-Thorsten%2520Peter%2520and%2520David%2520Vilar%2520and%2520Tobias%2520Domhan%2520and%2520Juraj%2520Juraska%2520and%2520Markus%2520Freitag%2520and%2520Lili%2520Mou%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520tradeoff%2520between%2520adequacy%2520and%2520fluency%2520in%2520machine%250Atranslation.%2520We%2520show%2520the%2520severity%2520of%2520this%2520tradeoff%2520at%2520the%2520evaluation%2520level%2520and%250Aanalyze%2520where%2520popular%2520metrics%2520fall%2520within%2520it.%2520Essentially%252C%2520current%2520metrics%250Agenerally%2520lean%2520toward%2520adequacy%252C%2520meaning%2520that%2520their%2520scores%2520correlate%2520more%250Astrongly%2520with%2520the%2520adequacy%2520of%2520translations%2520than%2520with%2520fluency.%2520More%2520importantly%252C%250Awe%2520find%2520that%2520this%2520tradeoff%2520also%2520persists%2520at%2520the%2520meta-evaluation%2520level%252C%2520and%2520that%250Athe%2520standard%2520WMT%2520meta-evaluation%2520favors%2520adequacy-oriented%2520metrics%2520over%250Afluency-oriented%2520ones.%2520We%2520show%2520that%2520this%2520bias%2520is%2520partially%2520attributed%2520to%2520the%250Acomposition%2520of%2520the%2520systems%2520included%2520in%2520the%2520meta-evaluation%2520datasets.%2520To%2520control%250Athis%2520bias%252C%2520we%2520propose%2520a%2520method%2520that%2520synthesizes%2520translation%2520systems%2520in%250Ameta-evaluation.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%2520understanding%2520this%250Atradeoff%2520in%2520meta-evaluation%2520and%2520its%2520impact%2520on%2520metric%2520rankings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feeding%20Two%20Birds%20or%20Favoring%20One%3F%20Adequacy-Fluency%20Tradeoffs%20in%0A%20%20Evaluation%20and%20Meta-Evaluation%20of%20Machine%20Translation&entry.906535625=Behzad%20Shayegh%20and%20Jan-Thorsten%20Peter%20and%20David%20Vilar%20and%20Tobias%20Domhan%20and%20Juraj%20Juraska%20and%20Markus%20Freitag%20and%20Lili%20Mou&entry.1292438233=%20%20We%20investigate%20the%20tradeoff%20between%20adequacy%20and%20fluency%20in%20machine%0Atranslation.%20We%20show%20the%20severity%20of%20this%20tradeoff%20at%20the%20evaluation%20level%20and%0Aanalyze%20where%20popular%20metrics%20fall%20within%20it.%20Essentially%2C%20current%20metrics%0Agenerally%20lean%20toward%20adequacy%2C%20meaning%20that%20their%20scores%20correlate%20more%0Astrongly%20with%20the%20adequacy%20of%20translations%20than%20with%20fluency.%20More%20importantly%2C%0Awe%20find%20that%20this%20tradeoff%20also%20persists%20at%20the%20meta-evaluation%20level%2C%20and%20that%0Athe%20standard%20WMT%20meta-evaluation%20favors%20adequacy-oriented%20metrics%20over%0Afluency-oriented%20ones.%20We%20show%20that%20this%20bias%20is%20partially%20attributed%20to%20the%0Acomposition%20of%20the%20systems%20included%20in%20the%20meta-evaluation%20datasets.%20To%20control%0Athis%20bias%2C%20we%20propose%20a%20method%20that%20synthesizes%20translation%20systems%20in%0Ameta-evaluation.%20Our%20findings%20highlight%20the%20importance%20of%20understanding%20this%0Atradeoff%20in%20meta-evaluation%20and%20its%20impact%20on%20metric%20rankings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20287v1&entry.124074799=Read"},
{"title": "Fair Clustering with Minimum Representation Constraints", "author": "Connor Lawless and Oktay Gunluk", "abstract": "  Clustering is a well-studied unsupervised learning task that aims to\npartition data points into a number of clusters. In many applications, these\nclusters correspond to real-world constructs (e.g., electoral districts,\nplaylists, TV channels), where a group (e.g., social or demographic) benefits\nonly if it reaches a minimum level of representation in the cluster (e.g., 50%\nto elect their preferred candidate). In this paper, we study the k-means and\nk-medians clustering problems under the additional fairness constraint that\neach group must attain a minimum level of representation in at least a\nspecified number of clusters. We formulate this problem as a mixed-integer\n(nonlinear) optimization problem and propose an alternating minimization\nalgorithm, called MiniReL, to solve it. Although incorporating fairness\nconstraints results in an NP-hard assignment problem within the MiniReL\nalgorithm, we present several heuristic strategies that make the approach\npractical even for large datasets. Numerical results demonstrate that our\nmethod yields fair clusters without increasing clustering cost across standard\nbenchmark datasets.\n", "link": "http://arxiv.org/abs/2409.02963v2", "date": "2025-09-24", "relevancy": 1.5954, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4009}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3977}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Clustering%20with%20Minimum%20Representation%20Constraints&body=Title%3A%20Fair%20Clustering%20with%20Minimum%20Representation%20Constraints%0AAuthor%3A%20Connor%20Lawless%20and%20Oktay%20Gunluk%0AAbstract%3A%20%20%20Clustering%20is%20a%20well-studied%20unsupervised%20learning%20task%20that%20aims%20to%0Apartition%20data%20points%20into%20a%20number%20of%20clusters.%20In%20many%20applications%2C%20these%0Aclusters%20correspond%20to%20real-world%20constructs%20%28e.g.%2C%20electoral%20districts%2C%0Aplaylists%2C%20TV%20channels%29%2C%20where%20a%20group%20%28e.g.%2C%20social%20or%20demographic%29%20benefits%0Aonly%20if%20it%20reaches%20a%20minimum%20level%20of%20representation%20in%20the%20cluster%20%28e.g.%2C%2050%25%0Ato%20elect%20their%20preferred%20candidate%29.%20In%20this%20paper%2C%20we%20study%20the%20k-means%20and%0Ak-medians%20clustering%20problems%20under%20the%20additional%20fairness%20constraint%20that%0Aeach%20group%20must%20attain%20a%20minimum%20level%20of%20representation%20in%20at%20least%20a%0Aspecified%20number%20of%20clusters.%20We%20formulate%20this%20problem%20as%20a%20mixed-integer%0A%28nonlinear%29%20optimization%20problem%20and%20propose%20an%20alternating%20minimization%0Aalgorithm%2C%20called%20MiniReL%2C%20to%20solve%20it.%20Although%20incorporating%20fairness%0Aconstraints%20results%20in%20an%20NP-hard%20assignment%20problem%20within%20the%20MiniReL%0Aalgorithm%2C%20we%20present%20several%20heuristic%20strategies%20that%20make%20the%20approach%0Apractical%20even%20for%20large%20datasets.%20Numerical%20results%20demonstrate%20that%20our%0Amethod%20yields%20fair%20clusters%20without%20increasing%20clustering%20cost%20across%20standard%0Abenchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.02963v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Clustering%2520with%2520Minimum%2520Representation%2520Constraints%26entry.906535625%3DConnor%2520Lawless%2520and%2520Oktay%2520Gunluk%26entry.1292438233%3D%2520%2520Clustering%2520is%2520a%2520well-studied%2520unsupervised%2520learning%2520task%2520that%2520aims%2520to%250Apartition%2520data%2520points%2520into%2520a%2520number%2520of%2520clusters.%2520In%2520many%2520applications%252C%2520these%250Aclusters%2520correspond%2520to%2520real-world%2520constructs%2520%2528e.g.%252C%2520electoral%2520districts%252C%250Aplaylists%252C%2520TV%2520channels%2529%252C%2520where%2520a%2520group%2520%2528e.g.%252C%2520social%2520or%2520demographic%2529%2520benefits%250Aonly%2520if%2520it%2520reaches%2520a%2520minimum%2520level%2520of%2520representation%2520in%2520the%2520cluster%2520%2528e.g.%252C%252050%2525%250Ato%2520elect%2520their%2520preferred%2520candidate%2529.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520k-means%2520and%250Ak-medians%2520clustering%2520problems%2520under%2520the%2520additional%2520fairness%2520constraint%2520that%250Aeach%2520group%2520must%2520attain%2520a%2520minimum%2520level%2520of%2520representation%2520in%2520at%2520least%2520a%250Aspecified%2520number%2520of%2520clusters.%2520We%2520formulate%2520this%2520problem%2520as%2520a%2520mixed-integer%250A%2528nonlinear%2529%2520optimization%2520problem%2520and%2520propose%2520an%2520alternating%2520minimization%250Aalgorithm%252C%2520called%2520MiniReL%252C%2520to%2520solve%2520it.%2520Although%2520incorporating%2520fairness%250Aconstraints%2520results%2520in%2520an%2520NP-hard%2520assignment%2520problem%2520within%2520the%2520MiniReL%250Aalgorithm%252C%2520we%2520present%2520several%2520heuristic%2520strategies%2520that%2520make%2520the%2520approach%250Apractical%2520even%2520for%2520large%2520datasets.%2520Numerical%2520results%2520demonstrate%2520that%2520our%250Amethod%2520yields%2520fair%2520clusters%2520without%2520increasing%2520clustering%2520cost%2520across%2520standard%250Abenchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.02963v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Clustering%20with%20Minimum%20Representation%20Constraints&entry.906535625=Connor%20Lawless%20and%20Oktay%20Gunluk&entry.1292438233=%20%20Clustering%20is%20a%20well-studied%20unsupervised%20learning%20task%20that%20aims%20to%0Apartition%20data%20points%20into%20a%20number%20of%20clusters.%20In%20many%20applications%2C%20these%0Aclusters%20correspond%20to%20real-world%20constructs%20%28e.g.%2C%20electoral%20districts%2C%0Aplaylists%2C%20TV%20channels%29%2C%20where%20a%20group%20%28e.g.%2C%20social%20or%20demographic%29%20benefits%0Aonly%20if%20it%20reaches%20a%20minimum%20level%20of%20representation%20in%20the%20cluster%20%28e.g.%2C%2050%25%0Ato%20elect%20their%20preferred%20candidate%29.%20In%20this%20paper%2C%20we%20study%20the%20k-means%20and%0Ak-medians%20clustering%20problems%20under%20the%20additional%20fairness%20constraint%20that%0Aeach%20group%20must%20attain%20a%20minimum%20level%20of%20representation%20in%20at%20least%20a%0Aspecified%20number%20of%20clusters.%20We%20formulate%20this%20problem%20as%20a%20mixed-integer%0A%28nonlinear%29%20optimization%20problem%20and%20propose%20an%20alternating%20minimization%0Aalgorithm%2C%20called%20MiniReL%2C%20to%20solve%20it.%20Although%20incorporating%20fairness%0Aconstraints%20results%20in%20an%20NP-hard%20assignment%20problem%20within%20the%20MiniReL%0Aalgorithm%2C%20we%20present%20several%20heuristic%20strategies%20that%20make%20the%20approach%0Apractical%20even%20for%20large%20datasets.%20Numerical%20results%20demonstrate%20that%20our%0Amethod%20yields%20fair%20clusters%20without%20increasing%20clustering%20cost%20across%20standard%0Abenchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.02963v2&entry.124074799=Read"},
{"title": "BBoE: Leveraging Bundle of Edges for Kinodynamic Bidirectional Motion\n  Planning", "author": "Srikrishna Bangalore Raghu and Alessandro Roncone", "abstract": "  In this work, we introduce BBoE, a bidirectional, kinodynamic, sampling-based\nmotion planner that consistently and quickly finds low-cost solutions in\nenvironments with varying obstacle clutter. The algorithm combines exploration\nand exploitation while relying on precomputed robot state traversals, resulting\nin efficient convergence towards the goal. Our key contributions include: i) a\nstrategy to navigate through obstacle-rich spaces by sorting and sequencing\npreprocessed forward propagations; and ii) BBoE, a robust bidirectional\nkinodynamic planner that utilizes this strategy to produce fast and feasible\nsolutions. The proposed framework reduces planning time, diminishes solution\ncost and increases success rate in comparison to previous approaches.\n", "link": "http://arxiv.org/abs/2509.20333v1", "date": "2025-09-24", "relevancy": 1.5691, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5451}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5293}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BBoE%3A%20Leveraging%20Bundle%20of%20Edges%20for%20Kinodynamic%20Bidirectional%20Motion%0A%20%20Planning&body=Title%3A%20BBoE%3A%20Leveraging%20Bundle%20of%20Edges%20for%20Kinodynamic%20Bidirectional%20Motion%0A%20%20Planning%0AAuthor%3A%20Srikrishna%20Bangalore%20Raghu%20and%20Alessandro%20Roncone%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20BBoE%2C%20a%20bidirectional%2C%20kinodynamic%2C%20sampling-based%0Amotion%20planner%20that%20consistently%20and%20quickly%20finds%20low-cost%20solutions%20in%0Aenvironments%20with%20varying%20obstacle%20clutter.%20The%20algorithm%20combines%20exploration%0Aand%20exploitation%20while%20relying%20on%20precomputed%20robot%20state%20traversals%2C%20resulting%0Ain%20efficient%20convergence%20towards%20the%20goal.%20Our%20key%20contributions%20include%3A%20i%29%20a%0Astrategy%20to%20navigate%20through%20obstacle-rich%20spaces%20by%20sorting%20and%20sequencing%0Apreprocessed%20forward%20propagations%3B%20and%20ii%29%20BBoE%2C%20a%20robust%20bidirectional%0Akinodynamic%20planner%20that%20utilizes%20this%20strategy%20to%20produce%20fast%20and%20feasible%0Asolutions.%20The%20proposed%20framework%20reduces%20planning%20time%2C%20diminishes%20solution%0Acost%20and%20increases%20success%20rate%20in%20comparison%20to%20previous%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBBoE%253A%2520Leveraging%2520Bundle%2520of%2520Edges%2520for%2520Kinodynamic%2520Bidirectional%2520Motion%250A%2520%2520Planning%26entry.906535625%3DSrikrishna%2520Bangalore%2520Raghu%2520and%2520Alessandro%2520Roncone%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520BBoE%252C%2520a%2520bidirectional%252C%2520kinodynamic%252C%2520sampling-based%250Amotion%2520planner%2520that%2520consistently%2520and%2520quickly%2520finds%2520low-cost%2520solutions%2520in%250Aenvironments%2520with%2520varying%2520obstacle%2520clutter.%2520The%2520algorithm%2520combines%2520exploration%250Aand%2520exploitation%2520while%2520relying%2520on%2520precomputed%2520robot%2520state%2520traversals%252C%2520resulting%250Ain%2520efficient%2520convergence%2520towards%2520the%2520goal.%2520Our%2520key%2520contributions%2520include%253A%2520i%2529%2520a%250Astrategy%2520to%2520navigate%2520through%2520obstacle-rich%2520spaces%2520by%2520sorting%2520and%2520sequencing%250Apreprocessed%2520forward%2520propagations%253B%2520and%2520ii%2529%2520BBoE%252C%2520a%2520robust%2520bidirectional%250Akinodynamic%2520planner%2520that%2520utilizes%2520this%2520strategy%2520to%2520produce%2520fast%2520and%2520feasible%250Asolutions.%2520The%2520proposed%2520framework%2520reduces%2520planning%2520time%252C%2520diminishes%2520solution%250Acost%2520and%2520increases%2520success%2520rate%2520in%2520comparison%2520to%2520previous%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BBoE%3A%20Leveraging%20Bundle%20of%20Edges%20for%20Kinodynamic%20Bidirectional%20Motion%0A%20%20Planning&entry.906535625=Srikrishna%20Bangalore%20Raghu%20and%20Alessandro%20Roncone&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20BBoE%2C%20a%20bidirectional%2C%20kinodynamic%2C%20sampling-based%0Amotion%20planner%20that%20consistently%20and%20quickly%20finds%20low-cost%20solutions%20in%0Aenvironments%20with%20varying%20obstacle%20clutter.%20The%20algorithm%20combines%20exploration%0Aand%20exploitation%20while%20relying%20on%20precomputed%20robot%20state%20traversals%2C%20resulting%0Ain%20efficient%20convergence%20towards%20the%20goal.%20Our%20key%20contributions%20include%3A%20i%29%20a%0Astrategy%20to%20navigate%20through%20obstacle-rich%20spaces%20by%20sorting%20and%20sequencing%0Apreprocessed%20forward%20propagations%3B%20and%20ii%29%20BBoE%2C%20a%20robust%20bidirectional%0Akinodynamic%20planner%20that%20utilizes%20this%20strategy%20to%20produce%20fast%20and%20feasible%0Asolutions.%20The%20proposed%20framework%20reduces%20planning%20time%2C%20diminishes%20solution%0Acost%20and%20increases%20success%20rate%20in%20comparison%20to%20previous%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20333v1&entry.124074799=Read"},
{"title": "Deep learning for exoplanet detection and characterization by direct\n  imaging at high contrast", "author": "Th\u00e9o Bodrito and Olivier Flasseur and Julien Mairal and Jean Ponce and Maud Langlois and Anne-Marie Lagrange", "abstract": "  Exoplanet imaging is a major challenge in astrophysics due to the need for\nhigh angular resolution and high contrast. We present a multi-scale statistical\nmodel for the nuisance component corrupting multivariate image series at high\ncontrast. Integrated into a learnable architecture, it leverages the physics of\nthe problem and enables the fusion of multiple observations of the same star in\na way that is optimal in terms of detection signal-to-noise ratio. Applied to\ndata from the VLT/SPHERE instrument, the method significantly improves the\ndetection sensitivity and the accuracy of astrometric and photometric\nestimation.\n", "link": "http://arxiv.org/abs/2509.20310v1", "date": "2025-09-24", "relevancy": 1.5545, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5485}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5219}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning%20for%20exoplanet%20detection%20and%20characterization%20by%20direct%0A%20%20imaging%20at%20high%20contrast&body=Title%3A%20Deep%20learning%20for%20exoplanet%20detection%20and%20characterization%20by%20direct%0A%20%20imaging%20at%20high%20contrast%0AAuthor%3A%20Th%C3%A9o%20Bodrito%20and%20Olivier%20Flasseur%20and%20Julien%20Mairal%20and%20Jean%20Ponce%20and%20Maud%20Langlois%20and%20Anne-Marie%20Lagrange%0AAbstract%3A%20%20%20Exoplanet%20imaging%20is%20a%20major%20challenge%20in%20astrophysics%20due%20to%20the%20need%20for%0Ahigh%20angular%20resolution%20and%20high%20contrast.%20We%20present%20a%20multi-scale%20statistical%0Amodel%20for%20the%20nuisance%20component%20corrupting%20multivariate%20image%20series%20at%20high%0Acontrast.%20Integrated%20into%20a%20learnable%20architecture%2C%20it%20leverages%20the%20physics%20of%0Athe%20problem%20and%20enables%20the%20fusion%20of%20multiple%20observations%20of%20the%20same%20star%20in%0Aa%20way%20that%20is%20optimal%20in%20terms%20of%20detection%20signal-to-noise%20ratio.%20Applied%20to%0Adata%20from%20the%20VLT/SPHERE%20instrument%2C%20the%20method%20significantly%20improves%20the%0Adetection%20sensitivity%20and%20the%20accuracy%20of%20astrometric%20and%20photometric%0Aestimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning%2520for%2520exoplanet%2520detection%2520and%2520characterization%2520by%2520direct%250A%2520%2520imaging%2520at%2520high%2520contrast%26entry.906535625%3DTh%25C3%25A9o%2520Bodrito%2520and%2520Olivier%2520Flasseur%2520and%2520Julien%2520Mairal%2520and%2520Jean%2520Ponce%2520and%2520Maud%2520Langlois%2520and%2520Anne-Marie%2520Lagrange%26entry.1292438233%3D%2520%2520Exoplanet%2520imaging%2520is%2520a%2520major%2520challenge%2520in%2520astrophysics%2520due%2520to%2520the%2520need%2520for%250Ahigh%2520angular%2520resolution%2520and%2520high%2520contrast.%2520We%2520present%2520a%2520multi-scale%2520statistical%250Amodel%2520for%2520the%2520nuisance%2520component%2520corrupting%2520multivariate%2520image%2520series%2520at%2520high%250Acontrast.%2520Integrated%2520into%2520a%2520learnable%2520architecture%252C%2520it%2520leverages%2520the%2520physics%2520of%250Athe%2520problem%2520and%2520enables%2520the%2520fusion%2520of%2520multiple%2520observations%2520of%2520the%2520same%2520star%2520in%250Aa%2520way%2520that%2520is%2520optimal%2520in%2520terms%2520of%2520detection%2520signal-to-noise%2520ratio.%2520Applied%2520to%250Adata%2520from%2520the%2520VLT/SPHERE%2520instrument%252C%2520the%2520method%2520significantly%2520improves%2520the%250Adetection%2520sensitivity%2520and%2520the%2520accuracy%2520of%2520astrometric%2520and%2520photometric%250Aestimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning%20for%20exoplanet%20detection%20and%20characterization%20by%20direct%0A%20%20imaging%20at%20high%20contrast&entry.906535625=Th%C3%A9o%20Bodrito%20and%20Olivier%20Flasseur%20and%20Julien%20Mairal%20and%20Jean%20Ponce%20and%20Maud%20Langlois%20and%20Anne-Marie%20Lagrange&entry.1292438233=%20%20Exoplanet%20imaging%20is%20a%20major%20challenge%20in%20astrophysics%20due%20to%20the%20need%20for%0Ahigh%20angular%20resolution%20and%20high%20contrast.%20We%20present%20a%20multi-scale%20statistical%0Amodel%20for%20the%20nuisance%20component%20corrupting%20multivariate%20image%20series%20at%20high%0Acontrast.%20Integrated%20into%20a%20learnable%20architecture%2C%20it%20leverages%20the%20physics%20of%0Athe%20problem%20and%20enables%20the%20fusion%20of%20multiple%20observations%20of%20the%20same%20star%20in%0Aa%20way%20that%20is%20optimal%20in%20terms%20of%20detection%20signal-to-noise%20ratio.%20Applied%20to%0Adata%20from%20the%20VLT/SPHERE%20instrument%2C%20the%20method%20significantly%20improves%20the%0Adetection%20sensitivity%20and%20the%20accuracy%20of%20astrometric%20and%20photometric%0Aestimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20310v1&entry.124074799=Read"},
{"title": "Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement\n  Learning", "author": "Umer Siddique and Abhinav Sinha and Yongcan Cao", "abstract": "  Conventional multi-agent reinforcement learning (MARL) methods rely on\ntime-triggered execution, where agents sample and communicate actions at fixed\nintervals. This approach is often computationally expensive and\ncommunication-intensive. To address this limitation, we propose ET-MAPG\n(Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a\nframework that jointly learns an agent's control policy and its\nevent-triggering policy. Unlike prior work that decouples these mechanisms,\nET-MAPG integrates them into a unified learning process, enabling agents to\nlearn not only what action to take but also when to execute it. For scenarios\nwith inter-agent communication, we introduce AET-MAPG, an attention-based\nvariant that leverages a self-attention mechanism to learn selective\ncommunication patterns. AET-MAPG empowers agents to determine not only when to\ntrigger an action but also with whom to communicate and what information to\nexchange, thereby optimizing coordination. Both methods can be integrated with\nany policy gradient MARL algorithm. Extensive experiments across diverse MARL\nbenchmarks demonstrate that our approaches achieve performance comparable to\nstate-of-the-art, time-triggered baselines while significantly reducing both\ncomputational load and communication overhead.\n", "link": "http://arxiv.org/abs/2509.20338v1", "date": "2025-09-24", "relevancy": 1.5233, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5158}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5118}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Event-Triggered%20Policy%20Gradient%20for%20Multi-Agent%20Reinforcement%0A%20%20Learning&body=Title%3A%20Adaptive%20Event-Triggered%20Policy%20Gradient%20for%20Multi-Agent%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Umer%20Siddique%20and%20Abhinav%20Sinha%20and%20Yongcan%20Cao%0AAbstract%3A%20%20%20Conventional%20multi-agent%20reinforcement%20learning%20%28MARL%29%20methods%20rely%20on%0Atime-triggered%20execution%2C%20where%20agents%20sample%20and%20communicate%20actions%20at%20fixed%0Aintervals.%20This%20approach%20is%20often%20computationally%20expensive%20and%0Acommunication-intensive.%20To%20address%20this%20limitation%2C%20we%20propose%20ET-MAPG%0A%28Event-Triggered%20Multi-Agent%20Policy%20Gradient%20reinforcement%20learning%29%2C%20a%0Aframework%20that%20jointly%20learns%20an%20agent%27s%20control%20policy%20and%20its%0Aevent-triggering%20policy.%20Unlike%20prior%20work%20that%20decouples%20these%20mechanisms%2C%0AET-MAPG%20integrates%20them%20into%20a%20unified%20learning%20process%2C%20enabling%20agents%20to%0Alearn%20not%20only%20what%20action%20to%20take%20but%20also%20when%20to%20execute%20it.%20For%20scenarios%0Awith%20inter-agent%20communication%2C%20we%20introduce%20AET-MAPG%2C%20an%20attention-based%0Avariant%20that%20leverages%20a%20self-attention%20mechanism%20to%20learn%20selective%0Acommunication%20patterns.%20AET-MAPG%20empowers%20agents%20to%20determine%20not%20only%20when%20to%0Atrigger%20an%20action%20but%20also%20with%20whom%20to%20communicate%20and%20what%20information%20to%0Aexchange%2C%20thereby%20optimizing%20coordination.%20Both%20methods%20can%20be%20integrated%20with%0Aany%20policy%20gradient%20MARL%20algorithm.%20Extensive%20experiments%20across%20diverse%20MARL%0Abenchmarks%20demonstrate%20that%20our%20approaches%20achieve%20performance%20comparable%20to%0Astate-of-the-art%2C%20time-triggered%20baselines%20while%20significantly%20reducing%20both%0Acomputational%20load%20and%20communication%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Event-Triggered%2520Policy%2520Gradient%2520for%2520Multi-Agent%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DUmer%2520Siddique%2520and%2520Abhinav%2520Sinha%2520and%2520Yongcan%2520Cao%26entry.1292438233%3D%2520%2520Conventional%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520methods%2520rely%2520on%250Atime-triggered%2520execution%252C%2520where%2520agents%2520sample%2520and%2520communicate%2520actions%2520at%2520fixed%250Aintervals.%2520This%2520approach%2520is%2520often%2520computationally%2520expensive%2520and%250Acommunication-intensive.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520ET-MAPG%250A%2528Event-Triggered%2520Multi-Agent%2520Policy%2520Gradient%2520reinforcement%2520learning%2529%252C%2520a%250Aframework%2520that%2520jointly%2520learns%2520an%2520agent%2527s%2520control%2520policy%2520and%2520its%250Aevent-triggering%2520policy.%2520Unlike%2520prior%2520work%2520that%2520decouples%2520these%2520mechanisms%252C%250AET-MAPG%2520integrates%2520them%2520into%2520a%2520unified%2520learning%2520process%252C%2520enabling%2520agents%2520to%250Alearn%2520not%2520only%2520what%2520action%2520to%2520take%2520but%2520also%2520when%2520to%2520execute%2520it.%2520For%2520scenarios%250Awith%2520inter-agent%2520communication%252C%2520we%2520introduce%2520AET-MAPG%252C%2520an%2520attention-based%250Avariant%2520that%2520leverages%2520a%2520self-attention%2520mechanism%2520to%2520learn%2520selective%250Acommunication%2520patterns.%2520AET-MAPG%2520empowers%2520agents%2520to%2520determine%2520not%2520only%2520when%2520to%250Atrigger%2520an%2520action%2520but%2520also%2520with%2520whom%2520to%2520communicate%2520and%2520what%2520information%2520to%250Aexchange%252C%2520thereby%2520optimizing%2520coordination.%2520Both%2520methods%2520can%2520be%2520integrated%2520with%250Aany%2520policy%2520gradient%2520MARL%2520algorithm.%2520Extensive%2520experiments%2520across%2520diverse%2520MARL%250Abenchmarks%2520demonstrate%2520that%2520our%2520approaches%2520achieve%2520performance%2520comparable%2520to%250Astate-of-the-art%252C%2520time-triggered%2520baselines%2520while%2520significantly%2520reducing%2520both%250Acomputational%2520load%2520and%2520communication%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Event-Triggered%20Policy%20Gradient%20for%20Multi-Agent%20Reinforcement%0A%20%20Learning&entry.906535625=Umer%20Siddique%20and%20Abhinav%20Sinha%20and%20Yongcan%20Cao&entry.1292438233=%20%20Conventional%20multi-agent%20reinforcement%20learning%20%28MARL%29%20methods%20rely%20on%0Atime-triggered%20execution%2C%20where%20agents%20sample%20and%20communicate%20actions%20at%20fixed%0Aintervals.%20This%20approach%20is%20often%20computationally%20expensive%20and%0Acommunication-intensive.%20To%20address%20this%20limitation%2C%20we%20propose%20ET-MAPG%0A%28Event-Triggered%20Multi-Agent%20Policy%20Gradient%20reinforcement%20learning%29%2C%20a%0Aframework%20that%20jointly%20learns%20an%20agent%27s%20control%20policy%20and%20its%0Aevent-triggering%20policy.%20Unlike%20prior%20work%20that%20decouples%20these%20mechanisms%2C%0AET-MAPG%20integrates%20them%20into%20a%20unified%20learning%20process%2C%20enabling%20agents%20to%0Alearn%20not%20only%20what%20action%20to%20take%20but%20also%20when%20to%20execute%20it.%20For%20scenarios%0Awith%20inter-agent%20communication%2C%20we%20introduce%20AET-MAPG%2C%20an%20attention-based%0Avariant%20that%20leverages%20a%20self-attention%20mechanism%20to%20learn%20selective%0Acommunication%20patterns.%20AET-MAPG%20empowers%20agents%20to%20determine%20not%20only%20when%20to%0Atrigger%20an%20action%20but%20also%20with%20whom%20to%20communicate%20and%20what%20information%20to%0Aexchange%2C%20thereby%20optimizing%20coordination.%20Both%20methods%20can%20be%20integrated%20with%0Aany%20policy%20gradient%20MARL%20algorithm.%20Extensive%20experiments%20across%20diverse%20MARL%0Abenchmarks%20demonstrate%20that%20our%20approaches%20achieve%20performance%20comparable%20to%0Astate-of-the-art%2C%20time-triggered%20baselines%20while%20significantly%20reducing%20both%0Acomputational%20load%20and%20communication%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20338v1&entry.124074799=Read"},
{"title": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices", "author": "Bishal Adhikari and Jiajia Li and Eric S. Michel and Jacob Dykes and Te-Ming Paul Tseng and Mary Love Tagert and Dong Chen", "abstract": "  The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies since these\nmethods are often labor-intensive, costly, and ineffective for modern farming\nsystems. To overcome this, there is a critical need for intelligent, autonomous\nsolutions which require accurate and efficient deer detection. But the progress\nin this field is impeded by a significant gap in the literature, mainly the\nlack of a domain-specific, practical dataset and limited study on the on-field\ndeployability of deer detection systems. Addressing this gap, this study\npresents a comprehensive evaluation of state-of-the-art deep learning models\nfor deer detection in challenging real-world scenarios. The contributions of\nthis work are threefold. First, we introduce a curated, publicly available\ndataset of 3,095 annotated images with bounding-box annotations of deer,\nderived from the Idaho Cameratraps project. Second, we provide an extensive\ncomparative analysis of 12 model variants across four recent YOLO\narchitectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a\nhigh-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing\nplatforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the\nreal-time detection is not feasible in Raspberry Pi without hardware-specific\nmodel optimization, while NVIDIA Jetson provides greater than 30 FPS with\nGPU-accelerated inference on 's' and 'n' series models. This study also reveals\nthat smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and\nYOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and\ncomputational efficiency (FPS > 30). To support further research, both the\nsource code and datasets are publicly available at\nhttps://github.com/WinnerBishal/track-the-deer.\n", "link": "http://arxiv.org/abs/2509.20318v1", "date": "2025-09-24", "relevancy": 1.5074, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5211}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Evaluation%20of%20YOLO-based%20Deer%20Detection%20Performance%20on%0A%20%20Edge%20Devices&body=Title%3A%20A%20Comprehensive%20Evaluation%20of%20YOLO-based%20Deer%20Detection%20Performance%20on%0A%20%20Edge%20Devices%0AAuthor%3A%20Bishal%20Adhikari%20and%20Jiajia%20Li%20and%20Eric%20S.%20Michel%20and%20Jacob%20Dykes%20and%20Te-Ming%20Paul%20Tseng%20and%20Mary%20Love%20Tagert%20and%20Dong%20Chen%0AAbstract%3A%20%20%20The%20escalating%20economic%20losses%20in%20agriculture%20due%20to%20deer%20intrusion%2C%0Aestimated%20to%20be%20in%20the%20hundreds%20of%20millions%20of%20dollars%20annually%20in%20the%20U.S.%2C%0Ahighlight%20the%20inadequacy%20of%20traditional%20mitigation%20strategies%20since%20these%0Amethods%20are%20often%20labor-intensive%2C%20costly%2C%20and%20ineffective%20for%20modern%20farming%0Asystems.%20To%20overcome%20this%2C%20there%20is%20a%20critical%20need%20for%20intelligent%2C%20autonomous%0Asolutions%20which%20require%20accurate%20and%20efficient%20deer%20detection.%20But%20the%20progress%0Ain%20this%20field%20is%20impeded%20by%20a%20significant%20gap%20in%20the%20literature%2C%20mainly%20the%0Alack%20of%20a%20domain-specific%2C%20practical%20dataset%20and%20limited%20study%20on%20the%20on-field%0Adeployability%20of%20deer%20detection%20systems.%20Addressing%20this%20gap%2C%20this%20study%0Apresents%20a%20comprehensive%20evaluation%20of%20state-of-the-art%20deep%20learning%20models%0Afor%20deer%20detection%20in%20challenging%20real-world%20scenarios.%20The%20contributions%20of%0Athis%20work%20are%20threefold.%20First%2C%20we%20introduce%20a%20curated%2C%20publicly%20available%0Adataset%20of%203%2C095%20annotated%20images%20with%20bounding-box%20annotations%20of%20deer%2C%0Aderived%20from%20the%20Idaho%20Cameratraps%20project.%20Second%2C%20we%20provide%20an%20extensive%0Acomparative%20analysis%20of%2012%20model%20variants%20across%20four%20recent%20YOLO%0Aarchitectures%28v8%2C%20v9%2C%20v10%2C%20and%20v11%29.%20Finally%2C%20we%20benchmarked%20performance%20on%20a%0Ahigh-end%20NVIDIA%20RTX%205090%20GPU%20and%20evaluated%20on%20two%20representative%20edge%20computing%0Aplatforms%3A%20Raspberry%20Pi%205%20and%20NVIDIA%20Jetson%20AGX%20Xavier.%20Results%20show%20that%20the%0Areal-time%20detection%20is%20not%20feasible%20in%20Raspberry%20Pi%20without%20hardware-specific%0Amodel%20optimization%2C%20while%20NVIDIA%20Jetson%20provides%20greater%20than%2030%20FPS%20with%0AGPU-accelerated%20inference%20on%20%27s%27%20and%20%27n%27%20series%20models.%20This%20study%20also%20reveals%0Athat%20smaller%2C%20architecturally%20advanced%20models%20such%20as%20YOLOv11n%2C%20YOLOv8s%2C%20and%0AYOLOv9s%20offer%20the%20optimal%20balance%20of%20high%20accuracy%20%28AP%40.5%20%3E%200.85%29%20and%0Acomputational%20efficiency%20%28FPS%20%3E%2030%29.%20To%20support%20further%20research%2C%20both%20the%0Asource%20code%20and%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//github.com/WinnerBishal/track-the-deer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Evaluation%2520of%2520YOLO-based%2520Deer%2520Detection%2520Performance%2520on%250A%2520%2520Edge%2520Devices%26entry.906535625%3DBishal%2520Adhikari%2520and%2520Jiajia%2520Li%2520and%2520Eric%2520S.%2520Michel%2520and%2520Jacob%2520Dykes%2520and%2520Te-Ming%2520Paul%2520Tseng%2520and%2520Mary%2520Love%2520Tagert%2520and%2520Dong%2520Chen%26entry.1292438233%3D%2520%2520The%2520escalating%2520economic%2520losses%2520in%2520agriculture%2520due%2520to%2520deer%2520intrusion%252C%250Aestimated%2520to%2520be%2520in%2520the%2520hundreds%2520of%2520millions%2520of%2520dollars%2520annually%2520in%2520the%2520U.S.%252C%250Ahighlight%2520the%2520inadequacy%2520of%2520traditional%2520mitigation%2520strategies%2520since%2520these%250Amethods%2520are%2520often%2520labor-intensive%252C%2520costly%252C%2520and%2520ineffective%2520for%2520modern%2520farming%250Asystems.%2520To%2520overcome%2520this%252C%2520there%2520is%2520a%2520critical%2520need%2520for%2520intelligent%252C%2520autonomous%250Asolutions%2520which%2520require%2520accurate%2520and%2520efficient%2520deer%2520detection.%2520But%2520the%2520progress%250Ain%2520this%2520field%2520is%2520impeded%2520by%2520a%2520significant%2520gap%2520in%2520the%2520literature%252C%2520mainly%2520the%250Alack%2520of%2520a%2520domain-specific%252C%2520practical%2520dataset%2520and%2520limited%2520study%2520on%2520the%2520on-field%250Adeployability%2520of%2520deer%2520detection%2520systems.%2520Addressing%2520this%2520gap%252C%2520this%2520study%250Apresents%2520a%2520comprehensive%2520evaluation%2520of%2520state-of-the-art%2520deep%2520learning%2520models%250Afor%2520deer%2520detection%2520in%2520challenging%2520real-world%2520scenarios.%2520The%2520contributions%2520of%250Athis%2520work%2520are%2520threefold.%2520First%252C%2520we%2520introduce%2520a%2520curated%252C%2520publicly%2520available%250Adataset%2520of%25203%252C095%2520annotated%2520images%2520with%2520bounding-box%2520annotations%2520of%2520deer%252C%250Aderived%2520from%2520the%2520Idaho%2520Cameratraps%2520project.%2520Second%252C%2520we%2520provide%2520an%2520extensive%250Acomparative%2520analysis%2520of%252012%2520model%2520variants%2520across%2520four%2520recent%2520YOLO%250Aarchitectures%2528v8%252C%2520v9%252C%2520v10%252C%2520and%2520v11%2529.%2520Finally%252C%2520we%2520benchmarked%2520performance%2520on%2520a%250Ahigh-end%2520NVIDIA%2520RTX%25205090%2520GPU%2520and%2520evaluated%2520on%2520two%2520representative%2520edge%2520computing%250Aplatforms%253A%2520Raspberry%2520Pi%25205%2520and%2520NVIDIA%2520Jetson%2520AGX%2520Xavier.%2520Results%2520show%2520that%2520the%250Areal-time%2520detection%2520is%2520not%2520feasible%2520in%2520Raspberry%2520Pi%2520without%2520hardware-specific%250Amodel%2520optimization%252C%2520while%2520NVIDIA%2520Jetson%2520provides%2520greater%2520than%252030%2520FPS%2520with%250AGPU-accelerated%2520inference%2520on%2520%2527s%2527%2520and%2520%2527n%2527%2520series%2520models.%2520This%2520study%2520also%2520reveals%250Athat%2520smaller%252C%2520architecturally%2520advanced%2520models%2520such%2520as%2520YOLOv11n%252C%2520YOLOv8s%252C%2520and%250AYOLOv9s%2520offer%2520the%2520optimal%2520balance%2520of%2520high%2520accuracy%2520%2528AP%2540.5%2520%253E%25200.85%2529%2520and%250Acomputational%2520efficiency%2520%2528FPS%2520%253E%252030%2529.%2520To%2520support%2520further%2520research%252C%2520both%2520the%250Asource%2520code%2520and%2520datasets%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/WinnerBishal/track-the-deer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Evaluation%20of%20YOLO-based%20Deer%20Detection%20Performance%20on%0A%20%20Edge%20Devices&entry.906535625=Bishal%20Adhikari%20and%20Jiajia%20Li%20and%20Eric%20S.%20Michel%20and%20Jacob%20Dykes%20and%20Te-Ming%20Paul%20Tseng%20and%20Mary%20Love%20Tagert%20and%20Dong%20Chen&entry.1292438233=%20%20The%20escalating%20economic%20losses%20in%20agriculture%20due%20to%20deer%20intrusion%2C%0Aestimated%20to%20be%20in%20the%20hundreds%20of%20millions%20of%20dollars%20annually%20in%20the%20U.S.%2C%0Ahighlight%20the%20inadequacy%20of%20traditional%20mitigation%20strategies%20since%20these%0Amethods%20are%20often%20labor-intensive%2C%20costly%2C%20and%20ineffective%20for%20modern%20farming%0Asystems.%20To%20overcome%20this%2C%20there%20is%20a%20critical%20need%20for%20intelligent%2C%20autonomous%0Asolutions%20which%20require%20accurate%20and%20efficient%20deer%20detection.%20But%20the%20progress%0Ain%20this%20field%20is%20impeded%20by%20a%20significant%20gap%20in%20the%20literature%2C%20mainly%20the%0Alack%20of%20a%20domain-specific%2C%20practical%20dataset%20and%20limited%20study%20on%20the%20on-field%0Adeployability%20of%20deer%20detection%20systems.%20Addressing%20this%20gap%2C%20this%20study%0Apresents%20a%20comprehensive%20evaluation%20of%20state-of-the-art%20deep%20learning%20models%0Afor%20deer%20detection%20in%20challenging%20real-world%20scenarios.%20The%20contributions%20of%0Athis%20work%20are%20threefold.%20First%2C%20we%20introduce%20a%20curated%2C%20publicly%20available%0Adataset%20of%203%2C095%20annotated%20images%20with%20bounding-box%20annotations%20of%20deer%2C%0Aderived%20from%20the%20Idaho%20Cameratraps%20project.%20Second%2C%20we%20provide%20an%20extensive%0Acomparative%20analysis%20of%2012%20model%20variants%20across%20four%20recent%20YOLO%0Aarchitectures%28v8%2C%20v9%2C%20v10%2C%20and%20v11%29.%20Finally%2C%20we%20benchmarked%20performance%20on%20a%0Ahigh-end%20NVIDIA%20RTX%205090%20GPU%20and%20evaluated%20on%20two%20representative%20edge%20computing%0Aplatforms%3A%20Raspberry%20Pi%205%20and%20NVIDIA%20Jetson%20AGX%20Xavier.%20Results%20show%20that%20the%0Areal-time%20detection%20is%20not%20feasible%20in%20Raspberry%20Pi%20without%20hardware-specific%0Amodel%20optimization%2C%20while%20NVIDIA%20Jetson%20provides%20greater%20than%2030%20FPS%20with%0AGPU-accelerated%20inference%20on%20%27s%27%20and%20%27n%27%20series%20models.%20This%20study%20also%20reveals%0Athat%20smaller%2C%20architecturally%20advanced%20models%20such%20as%20YOLOv11n%2C%20YOLOv8s%2C%20and%0AYOLOv9s%20offer%20the%20optimal%20balance%20of%20high%20accuracy%20%28AP%40.5%20%3E%200.85%29%20and%0Acomputational%20efficiency%20%28FPS%20%3E%2030%29.%20To%20support%20further%20research%2C%20both%20the%0Asource%20code%20and%20datasets%20are%20publicly%20available%20at%0Ahttps%3A//github.com/WinnerBishal/track-the-deer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20318v1&entry.124074799=Read"},
{"title": "AORRTC: Almost-Surely Asymptotically Optimal Planning with RRT-Connect", "author": "Tyler Wilson and Wil Thomason and Zachary Kingston and Jonathan Gammell", "abstract": "  Finding high-quality solutions quickly is an important objective in motion\nplanning. This is especially true for high-degree-of-freedom robots.\nSatisficing planners have traditionally found feasible solutions quickly but\nprovide no guarantees on their optimality, while almost-surely asymptotically\noptimal (a.s.a.o.) planners have probabilistic guarantees on their convergence\ntowards an optimal solution but are more computationally expensive.\n  This paper uses the AO-x meta-algorithm to extend the satisficing RRT-Connect\nplanner to optimal planning. The resulting Asymptotically Optimal RRT-Connect\n(AORRTC) finds initial solutions in similar times as RRT-Connect and uses any\nadditional planning time to converge towards the optimal solution in an anytime\nmanner. It is proven to be probabilistically complete and a.s.a.o.\n  AORRTC was tested with the Panda (7 DoF) and Fetch (8 DoF) robotic arms on\nthe MotionBenchMaker dataset. These experiments show that AORRTC finds initial\nsolutions as fast as RRT-Connect and faster than the tested state-of-the-art\na.s.a.o. algorithms while converging to better solutions faster. AORRTC finds\nsolutions to difficult high-DoF planning problems in milliseconds where the\nother a.s.a.o. planners could not consistently find solutions in seconds. This\nperformance was demonstrated both with and without single instruction/multiple\ndata (SIMD) acceleration.\n", "link": "http://arxiv.org/abs/2505.10542v4", "date": "2025-09-24", "relevancy": 1.4822, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5408}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4809}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AORRTC%3A%20Almost-Surely%20Asymptotically%20Optimal%20Planning%20with%20RRT-Connect&body=Title%3A%20AORRTC%3A%20Almost-Surely%20Asymptotically%20Optimal%20Planning%20with%20RRT-Connect%0AAuthor%3A%20Tyler%20Wilson%20and%20Wil%20Thomason%20and%20Zachary%20Kingston%20and%20Jonathan%20Gammell%0AAbstract%3A%20%20%20Finding%20high-quality%20solutions%20quickly%20is%20an%20important%20objective%20in%20motion%0Aplanning.%20This%20is%20especially%20true%20for%20high-degree-of-freedom%20robots.%0ASatisficing%20planners%20have%20traditionally%20found%20feasible%20solutions%20quickly%20but%0Aprovide%20no%20guarantees%20on%20their%20optimality%2C%20while%20almost-surely%20asymptotically%0Aoptimal%20%28a.s.a.o.%29%20planners%20have%20probabilistic%20guarantees%20on%20their%20convergence%0Atowards%20an%20optimal%20solution%20but%20are%20more%20computationally%20expensive.%0A%20%20This%20paper%20uses%20the%20AO-x%20meta-algorithm%20to%20extend%20the%20satisficing%20RRT-Connect%0Aplanner%20to%20optimal%20planning.%20The%20resulting%20Asymptotically%20Optimal%20RRT-Connect%0A%28AORRTC%29%20finds%20initial%20solutions%20in%20similar%20times%20as%20RRT-Connect%20and%20uses%20any%0Aadditional%20planning%20time%20to%20converge%20towards%20the%20optimal%20solution%20in%20an%20anytime%0Amanner.%20It%20is%20proven%20to%20be%20probabilistically%20complete%20and%20a.s.a.o.%0A%20%20AORRTC%20was%20tested%20with%20the%20Panda%20%287%20DoF%29%20and%20Fetch%20%288%20DoF%29%20robotic%20arms%20on%0Athe%20MotionBenchMaker%20dataset.%20These%20experiments%20show%20that%20AORRTC%20finds%20initial%0Asolutions%20as%20fast%20as%20RRT-Connect%20and%20faster%20than%20the%20tested%20state-of-the-art%0Aa.s.a.o.%20algorithms%20while%20converging%20to%20better%20solutions%20faster.%20AORRTC%20finds%0Asolutions%20to%20difficult%20high-DoF%20planning%20problems%20in%20milliseconds%20where%20the%0Aother%20a.s.a.o.%20planners%20could%20not%20consistently%20find%20solutions%20in%20seconds.%20This%0Aperformance%20was%20demonstrated%20both%20with%20and%20without%20single%20instruction/multiple%0Adata%20%28SIMD%29%20acceleration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10542v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAORRTC%253A%2520Almost-Surely%2520Asymptotically%2520Optimal%2520Planning%2520with%2520RRT-Connect%26entry.906535625%3DTyler%2520Wilson%2520and%2520Wil%2520Thomason%2520and%2520Zachary%2520Kingston%2520and%2520Jonathan%2520Gammell%26entry.1292438233%3D%2520%2520Finding%2520high-quality%2520solutions%2520quickly%2520is%2520an%2520important%2520objective%2520in%2520motion%250Aplanning.%2520This%2520is%2520especially%2520true%2520for%2520high-degree-of-freedom%2520robots.%250ASatisficing%2520planners%2520have%2520traditionally%2520found%2520feasible%2520solutions%2520quickly%2520but%250Aprovide%2520no%2520guarantees%2520on%2520their%2520optimality%252C%2520while%2520almost-surely%2520asymptotically%250Aoptimal%2520%2528a.s.a.o.%2529%2520planners%2520have%2520probabilistic%2520guarantees%2520on%2520their%2520convergence%250Atowards%2520an%2520optimal%2520solution%2520but%2520are%2520more%2520computationally%2520expensive.%250A%2520%2520This%2520paper%2520uses%2520the%2520AO-x%2520meta-algorithm%2520to%2520extend%2520the%2520satisficing%2520RRT-Connect%250Aplanner%2520to%2520optimal%2520planning.%2520The%2520resulting%2520Asymptotically%2520Optimal%2520RRT-Connect%250A%2528AORRTC%2529%2520finds%2520initial%2520solutions%2520in%2520similar%2520times%2520as%2520RRT-Connect%2520and%2520uses%2520any%250Aadditional%2520planning%2520time%2520to%2520converge%2520towards%2520the%2520optimal%2520solution%2520in%2520an%2520anytime%250Amanner.%2520It%2520is%2520proven%2520to%2520be%2520probabilistically%2520complete%2520and%2520a.s.a.o.%250A%2520%2520AORRTC%2520was%2520tested%2520with%2520the%2520Panda%2520%25287%2520DoF%2529%2520and%2520Fetch%2520%25288%2520DoF%2529%2520robotic%2520arms%2520on%250Athe%2520MotionBenchMaker%2520dataset.%2520These%2520experiments%2520show%2520that%2520AORRTC%2520finds%2520initial%250Asolutions%2520as%2520fast%2520as%2520RRT-Connect%2520and%2520faster%2520than%2520the%2520tested%2520state-of-the-art%250Aa.s.a.o.%2520algorithms%2520while%2520converging%2520to%2520better%2520solutions%2520faster.%2520AORRTC%2520finds%250Asolutions%2520to%2520difficult%2520high-DoF%2520planning%2520problems%2520in%2520milliseconds%2520where%2520the%250Aother%2520a.s.a.o.%2520planners%2520could%2520not%2520consistently%2520find%2520solutions%2520in%2520seconds.%2520This%250Aperformance%2520was%2520demonstrated%2520both%2520with%2520and%2520without%2520single%2520instruction/multiple%250Adata%2520%2528SIMD%2529%2520acceleration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10542v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AORRTC%3A%20Almost-Surely%20Asymptotically%20Optimal%20Planning%20with%20RRT-Connect&entry.906535625=Tyler%20Wilson%20and%20Wil%20Thomason%20and%20Zachary%20Kingston%20and%20Jonathan%20Gammell&entry.1292438233=%20%20Finding%20high-quality%20solutions%20quickly%20is%20an%20important%20objective%20in%20motion%0Aplanning.%20This%20is%20especially%20true%20for%20high-degree-of-freedom%20robots.%0ASatisficing%20planners%20have%20traditionally%20found%20feasible%20solutions%20quickly%20but%0Aprovide%20no%20guarantees%20on%20their%20optimality%2C%20while%20almost-surely%20asymptotically%0Aoptimal%20%28a.s.a.o.%29%20planners%20have%20probabilistic%20guarantees%20on%20their%20convergence%0Atowards%20an%20optimal%20solution%20but%20are%20more%20computationally%20expensive.%0A%20%20This%20paper%20uses%20the%20AO-x%20meta-algorithm%20to%20extend%20the%20satisficing%20RRT-Connect%0Aplanner%20to%20optimal%20planning.%20The%20resulting%20Asymptotically%20Optimal%20RRT-Connect%0A%28AORRTC%29%20finds%20initial%20solutions%20in%20similar%20times%20as%20RRT-Connect%20and%20uses%20any%0Aadditional%20planning%20time%20to%20converge%20towards%20the%20optimal%20solution%20in%20an%20anytime%0Amanner.%20It%20is%20proven%20to%20be%20probabilistically%20complete%20and%20a.s.a.o.%0A%20%20AORRTC%20was%20tested%20with%20the%20Panda%20%287%20DoF%29%20and%20Fetch%20%288%20DoF%29%20robotic%20arms%20on%0Athe%20MotionBenchMaker%20dataset.%20These%20experiments%20show%20that%20AORRTC%20finds%20initial%0Asolutions%20as%20fast%20as%20RRT-Connect%20and%20faster%20than%20the%20tested%20state-of-the-art%0Aa.s.a.o.%20algorithms%20while%20converging%20to%20better%20solutions%20faster.%20AORRTC%20finds%0Asolutions%20to%20difficult%20high-DoF%20planning%20problems%20in%20milliseconds%20where%20the%0Aother%20a.s.a.o.%20planners%20could%20not%20consistently%20find%20solutions%20in%20seconds.%20This%0Aperformance%20was%20demonstrated%20both%20with%20and%20without%20single%20instruction/multiple%0Adata%20%28SIMD%29%20acceleration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10542v4&entry.124074799=Read"},
{"title": "Multilingual Hope Speech Detection: A Comparative Study of Logistic\n  Regression, mBERT, and XLM-RoBERTa with Active Learning", "author": "T. O. Abiola and K. D. Abiodun and O. E. Olumide and O. O. Adebanji and O. Hiram Calvo and Grigori Sidorov", "abstract": "  Hope speech language that fosters encouragement and optimism plays a vital\nrole in promoting positive discourse online. However, its detection remains\nchallenging, especially in multilingual and low-resource settings. This paper\npresents a multilingual framework for hope speech detection using an active\nlearning approach and transformer-based models, including mBERT and\nXLM-RoBERTa. Experiments were conducted on datasets in English, Spanish,\nGerman, and Urdu, including benchmark test sets from recent shared tasks. Our\nresults show that transformer models significantly outperform traditional\nbaselines, with XLM-RoBERTa achieving the highest overall accuracy.\nFurthermore, our active learning strategy maintained strong performance even\nwith small annotated datasets. This study highlights the effectiveness of\ncombining multilingual transformers with data-efficient training strategies for\nhope speech detection.\n", "link": "http://arxiv.org/abs/2509.20315v1", "date": "2025-09-24", "relevancy": 1.4764, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5005}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Hope%20Speech%20Detection%3A%20A%20Comparative%20Study%20of%20Logistic%0A%20%20Regression%2C%20mBERT%2C%20and%20XLM-RoBERTa%20with%20Active%20Learning&body=Title%3A%20Multilingual%20Hope%20Speech%20Detection%3A%20A%20Comparative%20Study%20of%20Logistic%0A%20%20Regression%2C%20mBERT%2C%20and%20XLM-RoBERTa%20with%20Active%20Learning%0AAuthor%3A%20T.%20O.%20Abiola%20and%20K.%20D.%20Abiodun%20and%20O.%20E.%20Olumide%20and%20O.%20O.%20Adebanji%20and%20O.%20Hiram%20Calvo%20and%20Grigori%20Sidorov%0AAbstract%3A%20%20%20Hope%20speech%20language%20that%20fosters%20encouragement%20and%20optimism%20plays%20a%20vital%0Arole%20in%20promoting%20positive%20discourse%20online.%20However%2C%20its%20detection%20remains%0Achallenging%2C%20especially%20in%20multilingual%20and%20low-resource%20settings.%20This%20paper%0Apresents%20a%20multilingual%20framework%20for%20hope%20speech%20detection%20using%20an%20active%0Alearning%20approach%20and%20transformer-based%20models%2C%20including%20mBERT%20and%0AXLM-RoBERTa.%20Experiments%20were%20conducted%20on%20datasets%20in%20English%2C%20Spanish%2C%0AGerman%2C%20and%20Urdu%2C%20including%20benchmark%20test%20sets%20from%20recent%20shared%20tasks.%20Our%0Aresults%20show%20that%20transformer%20models%20significantly%20outperform%20traditional%0Abaselines%2C%20with%20XLM-RoBERTa%20achieving%20the%20highest%20overall%20accuracy.%0AFurthermore%2C%20our%20active%20learning%20strategy%20maintained%20strong%20performance%20even%0Awith%20small%20annotated%20datasets.%20This%20study%20highlights%20the%20effectiveness%20of%0Acombining%20multilingual%20transformers%20with%20data-efficient%20training%20strategies%20for%0Ahope%20speech%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Hope%2520Speech%2520Detection%253A%2520A%2520Comparative%2520Study%2520of%2520Logistic%250A%2520%2520Regression%252C%2520mBERT%252C%2520and%2520XLM-RoBERTa%2520with%2520Active%2520Learning%26entry.906535625%3DT.%2520O.%2520Abiola%2520and%2520K.%2520D.%2520Abiodun%2520and%2520O.%2520E.%2520Olumide%2520and%2520O.%2520O.%2520Adebanji%2520and%2520O.%2520Hiram%2520Calvo%2520and%2520Grigori%2520Sidorov%26entry.1292438233%3D%2520%2520Hope%2520speech%2520language%2520that%2520fosters%2520encouragement%2520and%2520optimism%2520plays%2520a%2520vital%250Arole%2520in%2520promoting%2520positive%2520discourse%2520online.%2520However%252C%2520its%2520detection%2520remains%250Achallenging%252C%2520especially%2520in%2520multilingual%2520and%2520low-resource%2520settings.%2520This%2520paper%250Apresents%2520a%2520multilingual%2520framework%2520for%2520hope%2520speech%2520detection%2520using%2520an%2520active%250Alearning%2520approach%2520and%2520transformer-based%2520models%252C%2520including%2520mBERT%2520and%250AXLM-RoBERTa.%2520Experiments%2520were%2520conducted%2520on%2520datasets%2520in%2520English%252C%2520Spanish%252C%250AGerman%252C%2520and%2520Urdu%252C%2520including%2520benchmark%2520test%2520sets%2520from%2520recent%2520shared%2520tasks.%2520Our%250Aresults%2520show%2520that%2520transformer%2520models%2520significantly%2520outperform%2520traditional%250Abaselines%252C%2520with%2520XLM-RoBERTa%2520achieving%2520the%2520highest%2520overall%2520accuracy.%250AFurthermore%252C%2520our%2520active%2520learning%2520strategy%2520maintained%2520strong%2520performance%2520even%250Awith%2520small%2520annotated%2520datasets.%2520This%2520study%2520highlights%2520the%2520effectiveness%2520of%250Acombining%2520multilingual%2520transformers%2520with%2520data-efficient%2520training%2520strategies%2520for%250Ahope%2520speech%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Hope%20Speech%20Detection%3A%20A%20Comparative%20Study%20of%20Logistic%0A%20%20Regression%2C%20mBERT%2C%20and%20XLM-RoBERTa%20with%20Active%20Learning&entry.906535625=T.%20O.%20Abiola%20and%20K.%20D.%20Abiodun%20and%20O.%20E.%20Olumide%20and%20O.%20O.%20Adebanji%20and%20O.%20Hiram%20Calvo%20and%20Grigori%20Sidorov&entry.1292438233=%20%20Hope%20speech%20language%20that%20fosters%20encouragement%20and%20optimism%20plays%20a%20vital%0Arole%20in%20promoting%20positive%20discourse%20online.%20However%2C%20its%20detection%20remains%0Achallenging%2C%20especially%20in%20multilingual%20and%20low-resource%20settings.%20This%20paper%0Apresents%20a%20multilingual%20framework%20for%20hope%20speech%20detection%20using%20an%20active%0Alearning%20approach%20and%20transformer-based%20models%2C%20including%20mBERT%20and%0AXLM-RoBERTa.%20Experiments%20were%20conducted%20on%20datasets%20in%20English%2C%20Spanish%2C%0AGerman%2C%20and%20Urdu%2C%20including%20benchmark%20test%20sets%20from%20recent%20shared%20tasks.%20Our%0Aresults%20show%20that%20transformer%20models%20significantly%20outperform%20traditional%0Abaselines%2C%20with%20XLM-RoBERTa%20achieving%20the%20highest%20overall%20accuracy.%0AFurthermore%2C%20our%20active%20learning%20strategy%20maintained%20strong%20performance%20even%0Awith%20small%20annotated%20datasets.%20This%20study%20highlights%20the%20effectiveness%20of%0Acombining%20multilingual%20transformers%20with%20data-efficient%20training%20strategies%20for%0Ahope%20speech%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20315v1&entry.124074799=Read"},
{"title": "Ads that Stick: Near-Optimal Ad Optimization through Psychological\n  Behavior Models", "author": "Kailash Gopal Darmasubramanian and Akash Pareek and Arindam Khan and Arpit Agarwal", "abstract": "  Optimizing the timing and frequency of ads is a central problem in digital\nadvertising, with significant economic consequences. Existing scheduling\npolicies rely on simple heuristics, such as uniform spacing and frequency caps,\nthat overlook long-term user interest. However, it is well-known that users'\nlong-term interest and engagement result from the interplay of several\npsychological effects (Curmei, Haupt, Recht, Hadfield-Menell, ACM CRS, 2022).\n  In this work, we model change in user interest upon showing ads based on\nthree key psychological principles: mere exposure, hedonic adaptation, and\noperant conditioning. The first two effects are modeled using a concave\nfunction of user interest with repeated exposure, while the third effect is\nmodeled using a temporal decay function, which explains the decline in user\ninterest due to overexposure. Under our psychological behavior model, we ask\nthe following question: Given a continuous time interval $T$, how many ads\nshould be shown, and at what times, to maximize the user interest towards the\nads?\n  Towards answering this question, we first show that, if the number of\ndisplayed ads is fixed, then the optimal ad-schedule only depends on the\noperant conditioning function. Our main result is a quasi-linear time algorithm\nthat outputs a near-optimal ad-schedule, i.e., the difference in the\nperformance of our schedule and the optimal schedule is exponentially small.\nOur algorithm leads to significant insights about optimal ad placement and\nshows that simple heuristics such as uniform spacing are sub-optimal under many\nnatural settings. The optimal number of ads to display, which also depends on\nthe mere exposure and hedonistic adaptation functions, can be found through a\nsimple linear search given the above algorithm. We further support our findings\nwith experimental results, demonstrating that our strategy outperforms various\nbaselines.\n", "link": "http://arxiv.org/abs/2509.20304v1", "date": "2025-09-24", "relevancy": 1.3558, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4538}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.452}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ads%20that%20Stick%3A%20Near-Optimal%20Ad%20Optimization%20through%20Psychological%0A%20%20Behavior%20Models&body=Title%3A%20Ads%20that%20Stick%3A%20Near-Optimal%20Ad%20Optimization%20through%20Psychological%0A%20%20Behavior%20Models%0AAuthor%3A%20Kailash%20Gopal%20Darmasubramanian%20and%20Akash%20Pareek%20and%20Arindam%20Khan%20and%20Arpit%20Agarwal%0AAbstract%3A%20%20%20Optimizing%20the%20timing%20and%20frequency%20of%20ads%20is%20a%20central%20problem%20in%20digital%0Aadvertising%2C%20with%20significant%20economic%20consequences.%20Existing%20scheduling%0Apolicies%20rely%20on%20simple%20heuristics%2C%20such%20as%20uniform%20spacing%20and%20frequency%20caps%2C%0Athat%20overlook%20long-term%20user%20interest.%20However%2C%20it%20is%20well-known%20that%20users%27%0Along-term%20interest%20and%20engagement%20result%20from%20the%20interplay%20of%20several%0Apsychological%20effects%20%28Curmei%2C%20Haupt%2C%20Recht%2C%20Hadfield-Menell%2C%20ACM%20CRS%2C%202022%29.%0A%20%20In%20this%20work%2C%20we%20model%20change%20in%20user%20interest%20upon%20showing%20ads%20based%20on%0Athree%20key%20psychological%20principles%3A%20mere%20exposure%2C%20hedonic%20adaptation%2C%20and%0Aoperant%20conditioning.%20The%20first%20two%20effects%20are%20modeled%20using%20a%20concave%0Afunction%20of%20user%20interest%20with%20repeated%20exposure%2C%20while%20the%20third%20effect%20is%0Amodeled%20using%20a%20temporal%20decay%20function%2C%20which%20explains%20the%20decline%20in%20user%0Ainterest%20due%20to%20overexposure.%20Under%20our%20psychological%20behavior%20model%2C%20we%20ask%0Athe%20following%20question%3A%20Given%20a%20continuous%20time%20interval%20%24T%24%2C%20how%20many%20ads%0Ashould%20be%20shown%2C%20and%20at%20what%20times%2C%20to%20maximize%20the%20user%20interest%20towards%20the%0Aads%3F%0A%20%20Towards%20answering%20this%20question%2C%20we%20first%20show%20that%2C%20if%20the%20number%20of%0Adisplayed%20ads%20is%20fixed%2C%20then%20the%20optimal%20ad-schedule%20only%20depends%20on%20the%0Aoperant%20conditioning%20function.%20Our%20main%20result%20is%20a%20quasi-linear%20time%20algorithm%0Athat%20outputs%20a%20near-optimal%20ad-schedule%2C%20i.e.%2C%20the%20difference%20in%20the%0Aperformance%20of%20our%20schedule%20and%20the%20optimal%20schedule%20is%20exponentially%20small.%0AOur%20algorithm%20leads%20to%20significant%20insights%20about%20optimal%20ad%20placement%20and%0Ashows%20that%20simple%20heuristics%20such%20as%20uniform%20spacing%20are%20sub-optimal%20under%20many%0Anatural%20settings.%20The%20optimal%20number%20of%20ads%20to%20display%2C%20which%20also%20depends%20on%0Athe%20mere%20exposure%20and%20hedonistic%20adaptation%20functions%2C%20can%20be%20found%20through%20a%0Asimple%20linear%20search%20given%20the%20above%20algorithm.%20We%20further%20support%20our%20findings%0Awith%20experimental%20results%2C%20demonstrating%20that%20our%20strategy%20outperforms%20various%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAds%2520that%2520Stick%253A%2520Near-Optimal%2520Ad%2520Optimization%2520through%2520Psychological%250A%2520%2520Behavior%2520Models%26entry.906535625%3DKailash%2520Gopal%2520Darmasubramanian%2520and%2520Akash%2520Pareek%2520and%2520Arindam%2520Khan%2520and%2520Arpit%2520Agarwal%26entry.1292438233%3D%2520%2520Optimizing%2520the%2520timing%2520and%2520frequency%2520of%2520ads%2520is%2520a%2520central%2520problem%2520in%2520digital%250Aadvertising%252C%2520with%2520significant%2520economic%2520consequences.%2520Existing%2520scheduling%250Apolicies%2520rely%2520on%2520simple%2520heuristics%252C%2520such%2520as%2520uniform%2520spacing%2520and%2520frequency%2520caps%252C%250Athat%2520overlook%2520long-term%2520user%2520interest.%2520However%252C%2520it%2520is%2520well-known%2520that%2520users%2527%250Along-term%2520interest%2520and%2520engagement%2520result%2520from%2520the%2520interplay%2520of%2520several%250Apsychological%2520effects%2520%2528Curmei%252C%2520Haupt%252C%2520Recht%252C%2520Hadfield-Menell%252C%2520ACM%2520CRS%252C%25202022%2529.%250A%2520%2520In%2520this%2520work%252C%2520we%2520model%2520change%2520in%2520user%2520interest%2520upon%2520showing%2520ads%2520based%2520on%250Athree%2520key%2520psychological%2520principles%253A%2520mere%2520exposure%252C%2520hedonic%2520adaptation%252C%2520and%250Aoperant%2520conditioning.%2520The%2520first%2520two%2520effects%2520are%2520modeled%2520using%2520a%2520concave%250Afunction%2520of%2520user%2520interest%2520with%2520repeated%2520exposure%252C%2520while%2520the%2520third%2520effect%2520is%250Amodeled%2520using%2520a%2520temporal%2520decay%2520function%252C%2520which%2520explains%2520the%2520decline%2520in%2520user%250Ainterest%2520due%2520to%2520overexposure.%2520Under%2520our%2520psychological%2520behavior%2520model%252C%2520we%2520ask%250Athe%2520following%2520question%253A%2520Given%2520a%2520continuous%2520time%2520interval%2520%2524T%2524%252C%2520how%2520many%2520ads%250Ashould%2520be%2520shown%252C%2520and%2520at%2520what%2520times%252C%2520to%2520maximize%2520the%2520user%2520interest%2520towards%2520the%250Aads%253F%250A%2520%2520Towards%2520answering%2520this%2520question%252C%2520we%2520first%2520show%2520that%252C%2520if%2520the%2520number%2520of%250Adisplayed%2520ads%2520is%2520fixed%252C%2520then%2520the%2520optimal%2520ad-schedule%2520only%2520depends%2520on%2520the%250Aoperant%2520conditioning%2520function.%2520Our%2520main%2520result%2520is%2520a%2520quasi-linear%2520time%2520algorithm%250Athat%2520outputs%2520a%2520near-optimal%2520ad-schedule%252C%2520i.e.%252C%2520the%2520difference%2520in%2520the%250Aperformance%2520of%2520our%2520schedule%2520and%2520the%2520optimal%2520schedule%2520is%2520exponentially%2520small.%250AOur%2520algorithm%2520leads%2520to%2520significant%2520insights%2520about%2520optimal%2520ad%2520placement%2520and%250Ashows%2520that%2520simple%2520heuristics%2520such%2520as%2520uniform%2520spacing%2520are%2520sub-optimal%2520under%2520many%250Anatural%2520settings.%2520The%2520optimal%2520number%2520of%2520ads%2520to%2520display%252C%2520which%2520also%2520depends%2520on%250Athe%2520mere%2520exposure%2520and%2520hedonistic%2520adaptation%2520functions%252C%2520can%2520be%2520found%2520through%2520a%250Asimple%2520linear%2520search%2520given%2520the%2520above%2520algorithm.%2520We%2520further%2520support%2520our%2520findings%250Awith%2520experimental%2520results%252C%2520demonstrating%2520that%2520our%2520strategy%2520outperforms%2520various%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ads%20that%20Stick%3A%20Near-Optimal%20Ad%20Optimization%20through%20Psychological%0A%20%20Behavior%20Models&entry.906535625=Kailash%20Gopal%20Darmasubramanian%20and%20Akash%20Pareek%20and%20Arindam%20Khan%20and%20Arpit%20Agarwal&entry.1292438233=%20%20Optimizing%20the%20timing%20and%20frequency%20of%20ads%20is%20a%20central%20problem%20in%20digital%0Aadvertising%2C%20with%20significant%20economic%20consequences.%20Existing%20scheduling%0Apolicies%20rely%20on%20simple%20heuristics%2C%20such%20as%20uniform%20spacing%20and%20frequency%20caps%2C%0Athat%20overlook%20long-term%20user%20interest.%20However%2C%20it%20is%20well-known%20that%20users%27%0Along-term%20interest%20and%20engagement%20result%20from%20the%20interplay%20of%20several%0Apsychological%20effects%20%28Curmei%2C%20Haupt%2C%20Recht%2C%20Hadfield-Menell%2C%20ACM%20CRS%2C%202022%29.%0A%20%20In%20this%20work%2C%20we%20model%20change%20in%20user%20interest%20upon%20showing%20ads%20based%20on%0Athree%20key%20psychological%20principles%3A%20mere%20exposure%2C%20hedonic%20adaptation%2C%20and%0Aoperant%20conditioning.%20The%20first%20two%20effects%20are%20modeled%20using%20a%20concave%0Afunction%20of%20user%20interest%20with%20repeated%20exposure%2C%20while%20the%20third%20effect%20is%0Amodeled%20using%20a%20temporal%20decay%20function%2C%20which%20explains%20the%20decline%20in%20user%0Ainterest%20due%20to%20overexposure.%20Under%20our%20psychological%20behavior%20model%2C%20we%20ask%0Athe%20following%20question%3A%20Given%20a%20continuous%20time%20interval%20%24T%24%2C%20how%20many%20ads%0Ashould%20be%20shown%2C%20and%20at%20what%20times%2C%20to%20maximize%20the%20user%20interest%20towards%20the%0Aads%3F%0A%20%20Towards%20answering%20this%20question%2C%20we%20first%20show%20that%2C%20if%20the%20number%20of%0Adisplayed%20ads%20is%20fixed%2C%20then%20the%20optimal%20ad-schedule%20only%20depends%20on%20the%0Aoperant%20conditioning%20function.%20Our%20main%20result%20is%20a%20quasi-linear%20time%20algorithm%0Athat%20outputs%20a%20near-optimal%20ad-schedule%2C%20i.e.%2C%20the%20difference%20in%20the%0Aperformance%20of%20our%20schedule%20and%20the%20optimal%20schedule%20is%20exponentially%20small.%0AOur%20algorithm%20leads%20to%20significant%20insights%20about%20optimal%20ad%20placement%20and%0Ashows%20that%20simple%20heuristics%20such%20as%20uniform%20spacing%20are%20sub-optimal%20under%20many%0Anatural%20settings.%20The%20optimal%20number%20of%20ads%20to%20display%2C%20which%20also%20depends%20on%0Athe%20mere%20exposure%20and%20hedonistic%20adaptation%20functions%2C%20can%20be%20found%20through%20a%0Asimple%20linear%20search%20given%20the%20above%20algorithm.%20We%20further%20support%20our%20findings%0Awith%20experimental%20results%2C%20demonstrating%20that%20our%20strategy%20outperforms%20various%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20304v1&entry.124074799=Read"},
{"title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks\n  Silently Undermine Validity", "author": "Benjamin Feuer and Chiung-Yi Tseng and Astitwa Sarthak Lathe and Oussama Elachqar and John P Dickerson", "abstract": "  LLM-judged benchmarks are increasingly used to evaluate complex model\nbehaviors, yet their design introduces failure modes absent in conventional\nground-truth based benchmarks. We argue that without tight objectives and\nverifiable constructions, benchmark rankings can produce high-confidence\nrankings that are in fact largely noise. We introduce two mechanisms to\ndiagnose these issues. Schematic adherence quantifies how much of a judge's\noverall verdict is explained by the explicit evaluation schema, revealing\nunexplained variance when judges deviate from their own rubric. Psychometric\nvalidity aggregates internal consistency and discriminant validity signals to\nquantify irreducible uncertainty in any benchmarking run. Applying these tools\nto Arena-Hard Auto, we find severe schema incoherence and factor collapse\nacross popular judges: for example, unexplained variance exceeding 90 percent\nfor DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We\nalso show that the ELO-style aggregation used by Arena-Hard Auto collapses and\nmasks genuine ranking uncertainty. Our results highlight design failures that\nundermine validity and offer actionable principles for building better-scoped,\nreliability-aware LLM-judged benchmarks. We release our code at\nhttps://anonymous.4open.science/r/judgment-to-noise-947D/README.md\n", "link": "http://arxiv.org/abs/2509.20293v1", "date": "2025-09-24", "relevancy": 1.3461, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4791}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4435}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Judgment%20Becomes%20Noise%3A%20How%20Design%20Failures%20in%20LLM%20Judge%20Benchmarks%0A%20%20Silently%20Undermine%20Validity&body=Title%3A%20When%20Judgment%20Becomes%20Noise%3A%20How%20Design%20Failures%20in%20LLM%20Judge%20Benchmarks%0A%20%20Silently%20Undermine%20Validity%0AAuthor%3A%20Benjamin%20Feuer%20and%20Chiung-Yi%20Tseng%20and%20Astitwa%20Sarthak%20Lathe%20and%20Oussama%20Elachqar%20and%20John%20P%20Dickerson%0AAbstract%3A%20%20%20LLM-judged%20benchmarks%20are%20increasingly%20used%20to%20evaluate%20complex%20model%0Abehaviors%2C%20yet%20their%20design%20introduces%20failure%20modes%20absent%20in%20conventional%0Aground-truth%20based%20benchmarks.%20We%20argue%20that%20without%20tight%20objectives%20and%0Averifiable%20constructions%2C%20benchmark%20rankings%20can%20produce%20high-confidence%0Arankings%20that%20are%20in%20fact%20largely%20noise.%20We%20introduce%20two%20mechanisms%20to%0Adiagnose%20these%20issues.%20Schematic%20adherence%20quantifies%20how%20much%20of%20a%20judge%27s%0Aoverall%20verdict%20is%20explained%20by%20the%20explicit%20evaluation%20schema%2C%20revealing%0Aunexplained%20variance%20when%20judges%20deviate%20from%20their%20own%20rubric.%20Psychometric%0Avalidity%20aggregates%20internal%20consistency%20and%20discriminant%20validity%20signals%20to%0Aquantify%20irreducible%20uncertainty%20in%20any%20benchmarking%20run.%20Applying%20these%20tools%0Ato%20Arena-Hard%20Auto%2C%20we%20find%20severe%20schema%20incoherence%20and%20factor%20collapse%0Aacross%20popular%20judges%3A%20for%20example%2C%20unexplained%20variance%20exceeding%2090%20percent%0Afor%20DeepSeek-R1-32B%20and%20factor%20correlations%20above%200.93%20for%20most%20criteria.%20We%0Aalso%20show%20that%20the%20ELO-style%20aggregation%20used%20by%20Arena-Hard%20Auto%20collapses%20and%0Amasks%20genuine%20ranking%20uncertainty.%20Our%20results%20highlight%20design%20failures%20that%0Aundermine%20validity%20and%20offer%20actionable%20principles%20for%20building%20better-scoped%2C%0Areliability-aware%20LLM-judged%20benchmarks.%20We%20release%20our%20code%20at%0Ahttps%3A//anonymous.4open.science/r/judgment-to-noise-947D/README.md%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Judgment%2520Becomes%2520Noise%253A%2520How%2520Design%2520Failures%2520in%2520LLM%2520Judge%2520Benchmarks%250A%2520%2520Silently%2520Undermine%2520Validity%26entry.906535625%3DBenjamin%2520Feuer%2520and%2520Chiung-Yi%2520Tseng%2520and%2520Astitwa%2520Sarthak%2520Lathe%2520and%2520Oussama%2520Elachqar%2520and%2520John%2520P%2520Dickerson%26entry.1292438233%3D%2520%2520LLM-judged%2520benchmarks%2520are%2520increasingly%2520used%2520to%2520evaluate%2520complex%2520model%250Abehaviors%252C%2520yet%2520their%2520design%2520introduces%2520failure%2520modes%2520absent%2520in%2520conventional%250Aground-truth%2520based%2520benchmarks.%2520We%2520argue%2520that%2520without%2520tight%2520objectives%2520and%250Averifiable%2520constructions%252C%2520benchmark%2520rankings%2520can%2520produce%2520high-confidence%250Arankings%2520that%2520are%2520in%2520fact%2520largely%2520noise.%2520We%2520introduce%2520two%2520mechanisms%2520to%250Adiagnose%2520these%2520issues.%2520Schematic%2520adherence%2520quantifies%2520how%2520much%2520of%2520a%2520judge%2527s%250Aoverall%2520verdict%2520is%2520explained%2520by%2520the%2520explicit%2520evaluation%2520schema%252C%2520revealing%250Aunexplained%2520variance%2520when%2520judges%2520deviate%2520from%2520their%2520own%2520rubric.%2520Psychometric%250Avalidity%2520aggregates%2520internal%2520consistency%2520and%2520discriminant%2520validity%2520signals%2520to%250Aquantify%2520irreducible%2520uncertainty%2520in%2520any%2520benchmarking%2520run.%2520Applying%2520these%2520tools%250Ato%2520Arena-Hard%2520Auto%252C%2520we%2520find%2520severe%2520schema%2520incoherence%2520and%2520factor%2520collapse%250Aacross%2520popular%2520judges%253A%2520for%2520example%252C%2520unexplained%2520variance%2520exceeding%252090%2520percent%250Afor%2520DeepSeek-R1-32B%2520and%2520factor%2520correlations%2520above%25200.93%2520for%2520most%2520criteria.%2520We%250Aalso%2520show%2520that%2520the%2520ELO-style%2520aggregation%2520used%2520by%2520Arena-Hard%2520Auto%2520collapses%2520and%250Amasks%2520genuine%2520ranking%2520uncertainty.%2520Our%2520results%2520highlight%2520design%2520failures%2520that%250Aundermine%2520validity%2520and%2520offer%2520actionable%2520principles%2520for%2520building%2520better-scoped%252C%250Areliability-aware%2520LLM-judged%2520benchmarks.%2520We%2520release%2520our%2520code%2520at%250Ahttps%253A//anonymous.4open.science/r/judgment-to-noise-947D/README.md%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Judgment%20Becomes%20Noise%3A%20How%20Design%20Failures%20in%20LLM%20Judge%20Benchmarks%0A%20%20Silently%20Undermine%20Validity&entry.906535625=Benjamin%20Feuer%20and%20Chiung-Yi%20Tseng%20and%20Astitwa%20Sarthak%20Lathe%20and%20Oussama%20Elachqar%20and%20John%20P%20Dickerson&entry.1292438233=%20%20LLM-judged%20benchmarks%20are%20increasingly%20used%20to%20evaluate%20complex%20model%0Abehaviors%2C%20yet%20their%20design%20introduces%20failure%20modes%20absent%20in%20conventional%0Aground-truth%20based%20benchmarks.%20We%20argue%20that%20without%20tight%20objectives%20and%0Averifiable%20constructions%2C%20benchmark%20rankings%20can%20produce%20high-confidence%0Arankings%20that%20are%20in%20fact%20largely%20noise.%20We%20introduce%20two%20mechanisms%20to%0Adiagnose%20these%20issues.%20Schematic%20adherence%20quantifies%20how%20much%20of%20a%20judge%27s%0Aoverall%20verdict%20is%20explained%20by%20the%20explicit%20evaluation%20schema%2C%20revealing%0Aunexplained%20variance%20when%20judges%20deviate%20from%20their%20own%20rubric.%20Psychometric%0Avalidity%20aggregates%20internal%20consistency%20and%20discriminant%20validity%20signals%20to%0Aquantify%20irreducible%20uncertainty%20in%20any%20benchmarking%20run.%20Applying%20these%20tools%0Ato%20Arena-Hard%20Auto%2C%20we%20find%20severe%20schema%20incoherence%20and%20factor%20collapse%0Aacross%20popular%20judges%3A%20for%20example%2C%20unexplained%20variance%20exceeding%2090%20percent%0Afor%20DeepSeek-R1-32B%20and%20factor%20correlations%20above%200.93%20for%20most%20criteria.%20We%0Aalso%20show%20that%20the%20ELO-style%20aggregation%20used%20by%20Arena-Hard%20Auto%20collapses%20and%0Amasks%20genuine%20ranking%20uncertainty.%20Our%20results%20highlight%20design%20failures%20that%0Aundermine%20validity%20and%20offer%20actionable%20principles%20for%20building%20better-scoped%2C%0Areliability-aware%20LLM-judged%20benchmarks.%20We%20release%20our%20code%20at%0Ahttps%3A//anonymous.4open.science/r/judgment-to-noise-947D/README.md%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20293v1&entry.124074799=Read"},
{"title": "RAG Security and Privacy: Formalizing the Threat Model and Attack\n  Surface", "author": "Atousa Arzanipour and Rouzbeh Behnia and Reza Ebrahimi and Kaushik Dutta", "abstract": "  Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems.\n", "link": "http://arxiv.org/abs/2509.20324v1", "date": "2025-09-24", "relevancy": 1.311, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4383}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4377}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAG%20Security%20and%20Privacy%3A%20Formalizing%20the%20Threat%20Model%20and%20Attack%0A%20%20Surface&body=Title%3A%20RAG%20Security%20and%20Privacy%3A%20Formalizing%20the%20Threat%20Model%20and%20Attack%0A%20%20Surface%0AAuthor%3A%20Atousa%20Arzanipour%20and%20Rouzbeh%20Behnia%20and%20Reza%20Ebrahimi%20and%20Kaushik%20Dutta%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20is%20an%20emerging%20approach%20in%20natural%0Alanguage%20processing%20that%20combines%20large%20language%20models%20%28LLMs%29%20with%20external%0Adocument%20retrieval%20to%20produce%20more%20accurate%20and%20grounded%20responses.%20While%20RAG%0Ahas%20shown%20strong%20potential%20in%20reducing%20hallucinations%20and%20improving%20factual%0Aconsistency%2C%20it%20also%20introduces%20new%20privacy%20and%20security%20challenges%20that%20differ%0Afrom%20those%20faced%20by%20traditional%20LLMs.%20Existing%20research%20has%20demonstrated%20that%0ALLMs%20can%20leak%20sensitive%20information%20through%20training%20data%20memorization%20or%0Aadversarial%20prompts%2C%20and%20RAG%20systems%20inherit%20many%20of%20these%20vulnerabilities.%20At%0Athe%20same%20time%2C%20reliance%20of%20RAG%20on%20an%20external%20knowledge%20base%20opens%20new%20attack%0Asurfaces%2C%20including%20the%20potential%20for%20leaking%20information%20about%20the%20presence%20or%0Acontent%20of%20retrieved%20documents%2C%20or%20for%20injecting%20malicious%20content%20to%0Amanipulate%20model%20behavior.%20Despite%20these%20risks%2C%20there%20is%20currently%20no%20formal%0Aframework%20that%20defines%20the%20threat%20landscape%20for%20RAG%20systems.%20In%20this%20paper%2C%20we%0Aaddress%20a%20critical%20gap%20in%20the%20literature%20by%20proposing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20formal%20threat%20model%20for%20retrieval-RAG%20systems.%20We%0Aintroduce%20a%20structured%20taxonomy%20of%20adversary%20types%20based%20on%20their%20access%20to%0Amodel%20components%20and%20data%2C%20and%20we%20formally%20define%20key%20threat%20vectors%20such%20as%0Adocument-level%20membership%20inference%20and%20data%20poisoning%2C%20which%20pose%20serious%0Aprivacy%20and%20integrity%20risks%20in%20real-world%20deployments.%20By%20establishing%20formal%0Adefinitions%20and%20attack%20models%2C%20our%20work%20lays%20the%20foundation%20for%20a%20more%20rigorous%0Aand%20principled%20understanding%20of%20privacy%20and%20security%20in%20RAG%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAG%2520Security%2520and%2520Privacy%253A%2520Formalizing%2520the%2520Threat%2520Model%2520and%2520Attack%250A%2520%2520Surface%26entry.906535625%3DAtousa%2520Arzanipour%2520and%2520Rouzbeh%2520Behnia%2520and%2520Reza%2520Ebrahimi%2520and%2520Kaushik%2520Dutta%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520is%2520an%2520emerging%2520approach%2520in%2520natural%250Alanguage%2520processing%2520that%2520combines%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520external%250Adocument%2520retrieval%2520to%2520produce%2520more%2520accurate%2520and%2520grounded%2520responses.%2520While%2520RAG%250Ahas%2520shown%2520strong%2520potential%2520in%2520reducing%2520hallucinations%2520and%2520improving%2520factual%250Aconsistency%252C%2520it%2520also%2520introduces%2520new%2520privacy%2520and%2520security%2520challenges%2520that%2520differ%250Afrom%2520those%2520faced%2520by%2520traditional%2520LLMs.%2520Existing%2520research%2520has%2520demonstrated%2520that%250ALLMs%2520can%2520leak%2520sensitive%2520information%2520through%2520training%2520data%2520memorization%2520or%250Aadversarial%2520prompts%252C%2520and%2520RAG%2520systems%2520inherit%2520many%2520of%2520these%2520vulnerabilities.%2520At%250Athe%2520same%2520time%252C%2520reliance%2520of%2520RAG%2520on%2520an%2520external%2520knowledge%2520base%2520opens%2520new%2520attack%250Asurfaces%252C%2520including%2520the%2520potential%2520for%2520leaking%2520information%2520about%2520the%2520presence%2520or%250Acontent%2520of%2520retrieved%2520documents%252C%2520or%2520for%2520injecting%2520malicious%2520content%2520to%250Amanipulate%2520model%2520behavior.%2520Despite%2520these%2520risks%252C%2520there%2520is%2520currently%2520no%2520formal%250Aframework%2520that%2520defines%2520the%2520threat%2520landscape%2520for%2520RAG%2520systems.%2520In%2520this%2520paper%252C%2520we%250Aaddress%2520a%2520critical%2520gap%2520in%2520the%2520literature%2520by%2520proposing%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520first%2520formal%2520threat%2520model%2520for%2520retrieval-RAG%2520systems.%2520We%250Aintroduce%2520a%2520structured%2520taxonomy%2520of%2520adversary%2520types%2520based%2520on%2520their%2520access%2520to%250Amodel%2520components%2520and%2520data%252C%2520and%2520we%2520formally%2520define%2520key%2520threat%2520vectors%2520such%2520as%250Adocument-level%2520membership%2520inference%2520and%2520data%2520poisoning%252C%2520which%2520pose%2520serious%250Aprivacy%2520and%2520integrity%2520risks%2520in%2520real-world%2520deployments.%2520By%2520establishing%2520formal%250Adefinitions%2520and%2520attack%2520models%252C%2520our%2520work%2520lays%2520the%2520foundation%2520for%2520a%2520more%2520rigorous%250Aand%2520principled%2520understanding%2520of%2520privacy%2520and%2520security%2520in%2520RAG%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAG%20Security%20and%20Privacy%3A%20Formalizing%20the%20Threat%20Model%20and%20Attack%0A%20%20Surface&entry.906535625=Atousa%20Arzanipour%20and%20Rouzbeh%20Behnia%20and%20Reza%20Ebrahimi%20and%20Kaushik%20Dutta&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20is%20an%20emerging%20approach%20in%20natural%0Alanguage%20processing%20that%20combines%20large%20language%20models%20%28LLMs%29%20with%20external%0Adocument%20retrieval%20to%20produce%20more%20accurate%20and%20grounded%20responses.%20While%20RAG%0Ahas%20shown%20strong%20potential%20in%20reducing%20hallucinations%20and%20improving%20factual%0Aconsistency%2C%20it%20also%20introduces%20new%20privacy%20and%20security%20challenges%20that%20differ%0Afrom%20those%20faced%20by%20traditional%20LLMs.%20Existing%20research%20has%20demonstrated%20that%0ALLMs%20can%20leak%20sensitive%20information%20through%20training%20data%20memorization%20or%0Aadversarial%20prompts%2C%20and%20RAG%20systems%20inherit%20many%20of%20these%20vulnerabilities.%20At%0Athe%20same%20time%2C%20reliance%20of%20RAG%20on%20an%20external%20knowledge%20base%20opens%20new%20attack%0Asurfaces%2C%20including%20the%20potential%20for%20leaking%20information%20about%20the%20presence%20or%0Acontent%20of%20retrieved%20documents%2C%20or%20for%20injecting%20malicious%20content%20to%0Amanipulate%20model%20behavior.%20Despite%20these%20risks%2C%20there%20is%20currently%20no%20formal%0Aframework%20that%20defines%20the%20threat%20landscape%20for%20RAG%20systems.%20In%20this%20paper%2C%20we%0Aaddress%20a%20critical%20gap%20in%20the%20literature%20by%20proposing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20formal%20threat%20model%20for%20retrieval-RAG%20systems.%20We%0Aintroduce%20a%20structured%20taxonomy%20of%20adversary%20types%20based%20on%20their%20access%20to%0Amodel%20components%20and%20data%2C%20and%20we%20formally%20define%20key%20threat%20vectors%20such%20as%0Adocument-level%20membership%20inference%20and%20data%20poisoning%2C%20which%20pose%20serious%0Aprivacy%20and%20integrity%20risks%20in%20real-world%20deployments.%20By%20establishing%20formal%0Adefinitions%20and%20attack%20models%2C%20our%20work%20lays%20the%20foundation%20for%20a%20more%20rigorous%0Aand%20principled%20understanding%20of%20privacy%20and%20security%20in%20RAG%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20324v1&entry.124074799=Read"},
{"title": "On Robustness of Consensus over Pseudo-Undirected Path Graphs", "author": "Abhinav Sinha and Dwaipayan Mukherjee and Shashi Ranjan Kumar", "abstract": "  Consensus over networked agents is typically studied using undirected or\ndirected communication graphs. Undirected graphs enforce symmetry in\ninformation exchange, leading to convergence to the average of initial states,\nwhile directed graphs permit asymmetry but make consensus dependent on root\nnodes and their influence. Both paradigms impose inherent restrictions on\nachievable consensus values and network robustness. This paper introduces a\ntheoretical framework for achieving consensus over a class of network\ntopologies, termed pseudo-undirected graphs, which retains bidirectional\nconnectivity between node pairs but allows the corresponding edge weights to\ndiffer, including the possibility of negative values under bounded conditions.\nThe resulting Laplacian is generally non-symmetric, yet it guarantees consensus\nunder connectivity assumptions, to expand the solution space, which enables the\nsystem to achieve a stable consensus value that can lie outside the convex hull\nof the initial state set. We derive admissibility bounds for negative weights\nfor a pseudo-undirected path graph, and show an application in the simultaneous\ninterception of a moving target.\n", "link": "http://arxiv.org/abs/2509.20314v1", "date": "2025-09-24", "relevancy": 1.3041, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4441}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4347}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Robustness%20of%20Consensus%20over%20Pseudo-Undirected%20Path%20Graphs&body=Title%3A%20On%20Robustness%20of%20Consensus%20over%20Pseudo-Undirected%20Path%20Graphs%0AAuthor%3A%20Abhinav%20Sinha%20and%20Dwaipayan%20Mukherjee%20and%20Shashi%20Ranjan%20Kumar%0AAbstract%3A%20%20%20Consensus%20over%20networked%20agents%20is%20typically%20studied%20using%20undirected%20or%0Adirected%20communication%20graphs.%20Undirected%20graphs%20enforce%20symmetry%20in%0Ainformation%20exchange%2C%20leading%20to%20convergence%20to%20the%20average%20of%20initial%20states%2C%0Awhile%20directed%20graphs%20permit%20asymmetry%20but%20make%20consensus%20dependent%20on%20root%0Anodes%20and%20their%20influence.%20Both%20paradigms%20impose%20inherent%20restrictions%20on%0Aachievable%20consensus%20values%20and%20network%20robustness.%20This%20paper%20introduces%20a%0Atheoretical%20framework%20for%20achieving%20consensus%20over%20a%20class%20of%20network%0Atopologies%2C%20termed%20pseudo-undirected%20graphs%2C%20which%20retains%20bidirectional%0Aconnectivity%20between%20node%20pairs%20but%20allows%20the%20corresponding%20edge%20weights%20to%0Adiffer%2C%20including%20the%20possibility%20of%20negative%20values%20under%20bounded%20conditions.%0AThe%20resulting%20Laplacian%20is%20generally%20non-symmetric%2C%20yet%20it%20guarantees%20consensus%0Aunder%20connectivity%20assumptions%2C%20to%20expand%20the%20solution%20space%2C%20which%20enables%20the%0Asystem%20to%20achieve%20a%20stable%20consensus%20value%20that%20can%20lie%20outside%20the%20convex%20hull%0Aof%20the%20initial%20state%20set.%20We%20derive%20admissibility%20bounds%20for%20negative%20weights%0Afor%20a%20pseudo-undirected%20path%20graph%2C%20and%20show%20an%20application%20in%20the%20simultaneous%0Ainterception%20of%20a%20moving%20target.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Robustness%2520of%2520Consensus%2520over%2520Pseudo-Undirected%2520Path%2520Graphs%26entry.906535625%3DAbhinav%2520Sinha%2520and%2520Dwaipayan%2520Mukherjee%2520and%2520Shashi%2520Ranjan%2520Kumar%26entry.1292438233%3D%2520%2520Consensus%2520over%2520networked%2520agents%2520is%2520typically%2520studied%2520using%2520undirected%2520or%250Adirected%2520communication%2520graphs.%2520Undirected%2520graphs%2520enforce%2520symmetry%2520in%250Ainformation%2520exchange%252C%2520leading%2520to%2520convergence%2520to%2520the%2520average%2520of%2520initial%2520states%252C%250Awhile%2520directed%2520graphs%2520permit%2520asymmetry%2520but%2520make%2520consensus%2520dependent%2520on%2520root%250Anodes%2520and%2520their%2520influence.%2520Both%2520paradigms%2520impose%2520inherent%2520restrictions%2520on%250Aachievable%2520consensus%2520values%2520and%2520network%2520robustness.%2520This%2520paper%2520introduces%2520a%250Atheoretical%2520framework%2520for%2520achieving%2520consensus%2520over%2520a%2520class%2520of%2520network%250Atopologies%252C%2520termed%2520pseudo-undirected%2520graphs%252C%2520which%2520retains%2520bidirectional%250Aconnectivity%2520between%2520node%2520pairs%2520but%2520allows%2520the%2520corresponding%2520edge%2520weights%2520to%250Adiffer%252C%2520including%2520the%2520possibility%2520of%2520negative%2520values%2520under%2520bounded%2520conditions.%250AThe%2520resulting%2520Laplacian%2520is%2520generally%2520non-symmetric%252C%2520yet%2520it%2520guarantees%2520consensus%250Aunder%2520connectivity%2520assumptions%252C%2520to%2520expand%2520the%2520solution%2520space%252C%2520which%2520enables%2520the%250Asystem%2520to%2520achieve%2520a%2520stable%2520consensus%2520value%2520that%2520can%2520lie%2520outside%2520the%2520convex%2520hull%250Aof%2520the%2520initial%2520state%2520set.%2520We%2520derive%2520admissibility%2520bounds%2520for%2520negative%2520weights%250Afor%2520a%2520pseudo-undirected%2520path%2520graph%252C%2520and%2520show%2520an%2520application%2520in%2520the%2520simultaneous%250Ainterception%2520of%2520a%2520moving%2520target.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Robustness%20of%20Consensus%20over%20Pseudo-Undirected%20Path%20Graphs&entry.906535625=Abhinav%20Sinha%20and%20Dwaipayan%20Mukherjee%20and%20Shashi%20Ranjan%20Kumar&entry.1292438233=%20%20Consensus%20over%20networked%20agents%20is%20typically%20studied%20using%20undirected%20or%0Adirected%20communication%20graphs.%20Undirected%20graphs%20enforce%20symmetry%20in%0Ainformation%20exchange%2C%20leading%20to%20convergence%20to%20the%20average%20of%20initial%20states%2C%0Awhile%20directed%20graphs%20permit%20asymmetry%20but%20make%20consensus%20dependent%20on%20root%0Anodes%20and%20their%20influence.%20Both%20paradigms%20impose%20inherent%20restrictions%20on%0Aachievable%20consensus%20values%20and%20network%20robustness.%20This%20paper%20introduces%20a%0Atheoretical%20framework%20for%20achieving%20consensus%20over%20a%20class%20of%20network%0Atopologies%2C%20termed%20pseudo-undirected%20graphs%2C%20which%20retains%20bidirectional%0Aconnectivity%20between%20node%20pairs%20but%20allows%20the%20corresponding%20edge%20weights%20to%0Adiffer%2C%20including%20the%20possibility%20of%20negative%20values%20under%20bounded%20conditions.%0AThe%20resulting%20Laplacian%20is%20generally%20non-symmetric%2C%20yet%20it%20guarantees%20consensus%0Aunder%20connectivity%20assumptions%2C%20to%20expand%20the%20solution%20space%2C%20which%20enables%20the%0Asystem%20to%20achieve%20a%20stable%20consensus%20value%20that%20can%20lie%20outside%20the%20convex%20hull%0Aof%20the%20initial%20state%20set.%20We%20derive%20admissibility%20bounds%20for%20negative%20weights%0Afor%20a%20pseudo-undirected%20path%20graph%2C%20and%20show%20an%20application%20in%20the%20simultaneous%0Ainterception%20of%20a%20moving%20target.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20314v1&entry.124074799=Read"},
{"title": "mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies", "author": "Remo Steiner and Alexander Millane and David Tingdahl and Clemens Volk and Vikram Ramasamy and Xinjie Yao and Peter Du and Soha Pouya and Shiwei Sheng", "abstract": "  End-to-end learning of robot control policies, structured as neural networks,\nhas emerged as a promising approach to robotic manipulation. To complete many\ncommon tasks, relevant objects are required to pass in and out of a robot's\nfield of view. In these settings, spatial memory - the ability to remember the\nspatial composition of the scene - is an important competency. However,\nbuilding such mechanisms into robot learning systems remains an open research\nproblem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D\nAction Policies), a 3D diffusion policy that generates robot trajectories based\non a semantic 3D reconstruction of the environment. We show in simulation\nexperiments that our approach is effective at solving tasks where\nstate-of-the-art approaches without memory mechanisms struggle. We release our\nreconstruction system, training code, and evaluation tasks to spur research in\nthis direction.\n", "link": "http://arxiv.org/abs/2509.20297v1", "date": "2025-09-24", "relevancy": 1.1895, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6045}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5976}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mindmap%3A%20Spatial%20Memory%20in%20Deep%20Feature%20Maps%20for%203D%20Action%20Policies&body=Title%3A%20mindmap%3A%20Spatial%20Memory%20in%20Deep%20Feature%20Maps%20for%203D%20Action%20Policies%0AAuthor%3A%20Remo%20Steiner%20and%20Alexander%20Millane%20and%20David%20Tingdahl%20and%20Clemens%20Volk%20and%20Vikram%20Ramasamy%20and%20Xinjie%20Yao%20and%20Peter%20Du%20and%20Soha%20Pouya%20and%20Shiwei%20Sheng%0AAbstract%3A%20%20%20End-to-end%20learning%20of%20robot%20control%20policies%2C%20structured%20as%20neural%20networks%2C%0Ahas%20emerged%20as%20a%20promising%20approach%20to%20robotic%20manipulation.%20To%20complete%20many%0Acommon%20tasks%2C%20relevant%20objects%20are%20required%20to%20pass%20in%20and%20out%20of%20a%20robot%27s%0Afield%20of%20view.%20In%20these%20settings%2C%20spatial%20memory%20-%20the%20ability%20to%20remember%20the%0Aspatial%20composition%20of%20the%20scene%20-%20is%20an%20important%20competency.%20However%2C%0Abuilding%20such%20mechanisms%20into%20robot%20learning%20systems%20remains%20an%20open%20research%0Aproblem.%20We%20introduce%20mindmap%20%28Spatial%20Memory%20in%20Deep%20Feature%20Maps%20for%203D%0AAction%20Policies%29%2C%20a%203D%20diffusion%20policy%20that%20generates%20robot%20trajectories%20based%0Aon%20a%20semantic%203D%20reconstruction%20of%20the%20environment.%20We%20show%20in%20simulation%0Aexperiments%20that%20our%20approach%20is%20effective%20at%20solving%20tasks%20where%0Astate-of-the-art%20approaches%20without%20memory%20mechanisms%20struggle.%20We%20release%20our%0Areconstruction%20system%2C%20training%20code%2C%20and%20evaluation%20tasks%20to%20spur%20research%20in%0Athis%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dmindmap%253A%2520Spatial%2520Memory%2520in%2520Deep%2520Feature%2520Maps%2520for%25203D%2520Action%2520Policies%26entry.906535625%3DRemo%2520Steiner%2520and%2520Alexander%2520Millane%2520and%2520David%2520Tingdahl%2520and%2520Clemens%2520Volk%2520and%2520Vikram%2520Ramasamy%2520and%2520Xinjie%2520Yao%2520and%2520Peter%2520Du%2520and%2520Soha%2520Pouya%2520and%2520Shiwei%2520Sheng%26entry.1292438233%3D%2520%2520End-to-end%2520learning%2520of%2520robot%2520control%2520policies%252C%2520structured%2520as%2520neural%2520networks%252C%250Ahas%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520robotic%2520manipulation.%2520To%2520complete%2520many%250Acommon%2520tasks%252C%2520relevant%2520objects%2520are%2520required%2520to%2520pass%2520in%2520and%2520out%2520of%2520a%2520robot%2527s%250Afield%2520of%2520view.%2520In%2520these%2520settings%252C%2520spatial%2520memory%2520-%2520the%2520ability%2520to%2520remember%2520the%250Aspatial%2520composition%2520of%2520the%2520scene%2520-%2520is%2520an%2520important%2520competency.%2520However%252C%250Abuilding%2520such%2520mechanisms%2520into%2520robot%2520learning%2520systems%2520remains%2520an%2520open%2520research%250Aproblem.%2520We%2520introduce%2520mindmap%2520%2528Spatial%2520Memory%2520in%2520Deep%2520Feature%2520Maps%2520for%25203D%250AAction%2520Policies%2529%252C%2520a%25203D%2520diffusion%2520policy%2520that%2520generates%2520robot%2520trajectories%2520based%250Aon%2520a%2520semantic%25203D%2520reconstruction%2520of%2520the%2520environment.%2520We%2520show%2520in%2520simulation%250Aexperiments%2520that%2520our%2520approach%2520is%2520effective%2520at%2520solving%2520tasks%2520where%250Astate-of-the-art%2520approaches%2520without%2520memory%2520mechanisms%2520struggle.%2520We%2520release%2520our%250Areconstruction%2520system%252C%2520training%2520code%252C%2520and%2520evaluation%2520tasks%2520to%2520spur%2520research%2520in%250Athis%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mindmap%3A%20Spatial%20Memory%20in%20Deep%20Feature%20Maps%20for%203D%20Action%20Policies&entry.906535625=Remo%20Steiner%20and%20Alexander%20Millane%20and%20David%20Tingdahl%20and%20Clemens%20Volk%20and%20Vikram%20Ramasamy%20and%20Xinjie%20Yao%20and%20Peter%20Du%20and%20Soha%20Pouya%20and%20Shiwei%20Sheng&entry.1292438233=%20%20End-to-end%20learning%20of%20robot%20control%20policies%2C%20structured%20as%20neural%20networks%2C%0Ahas%20emerged%20as%20a%20promising%20approach%20to%20robotic%20manipulation.%20To%20complete%20many%0Acommon%20tasks%2C%20relevant%20objects%20are%20required%20to%20pass%20in%20and%20out%20of%20a%20robot%27s%0Afield%20of%20view.%20In%20these%20settings%2C%20spatial%20memory%20-%20the%20ability%20to%20remember%20the%0Aspatial%20composition%20of%20the%20scene%20-%20is%20an%20important%20competency.%20However%2C%0Abuilding%20such%20mechanisms%20into%20robot%20learning%20systems%20remains%20an%20open%20research%0Aproblem.%20We%20introduce%20mindmap%20%28Spatial%20Memory%20in%20Deep%20Feature%20Maps%20for%203D%0AAction%20Policies%29%2C%20a%203D%20diffusion%20policy%20that%20generates%20robot%20trajectories%20based%0Aon%20a%20semantic%203D%20reconstruction%20of%20the%20environment.%20We%20show%20in%20simulation%0Aexperiments%20that%20our%20approach%20is%20effective%20at%20solving%20tasks%20where%0Astate-of-the-art%20approaches%20without%20memory%20mechanisms%20struggle.%20We%20release%20our%0Areconstruction%20system%2C%20training%20code%2C%20and%20evaluation%20tasks%20to%20spur%20research%20in%0Athis%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20297v1&entry.124074799=Read"},
{"title": "Process-Informed Forecasting of Complex Thermal Dynamics in\n  Pharmaceutical Manufacturing", "author": "Ramona Rubini and Siavash Khodakarami and Aniruddha Bora and George Em Karniadakis and Michele Dassisti", "abstract": "  Accurate time-series forecasting for complex physical systems is the backbone\nof modern industrial monitoring and control. While deep learning models excel\nat capturing complex dynamics, currently, their deployment is limited due to\nphysical inconsistency and robustness, hence constraining their reliability in\nregulated environments. We introduce process-informed forecasting (PIF) models\nfor temperature in pharmaceutical lyophilization. We investigate a wide range\nof models, from classical ones such as Autoregressive Integrated Moving Average\nModel (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning\narchitectures, including Kolmogorov-Arnold Networks (KANs). We compare three\ndifferent loss function formulations that integrate a process-informed\ntrajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a\nResidual-Based Attention (RBA) mechanism. We evaluate all models not only for\naccuracy and physical consistency but also for robustness to sensor noise.\nFurthermore, we test the practical generalizability of the best model in a\ntransfer learning scenario on a new process. Our results show that PIF models\noutperform their data-driven counterparts in terms of accuracy, physical\nplausibility and noise resilience. This work provides a roadmap for developing\nreliable and generalizable forecasting solutions for critical applications in\nthe pharmaceutical manufacturing landscape.\n", "link": "http://arxiv.org/abs/2509.20349v1", "date": "2025-09-24", "relevancy": 0.9627, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.498}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4817}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Process-Informed%20Forecasting%20of%20Complex%20Thermal%20Dynamics%20in%0A%20%20Pharmaceutical%20Manufacturing&body=Title%3A%20Process-Informed%20Forecasting%20of%20Complex%20Thermal%20Dynamics%20in%0A%20%20Pharmaceutical%20Manufacturing%0AAuthor%3A%20Ramona%20Rubini%20and%20Siavash%20Khodakarami%20and%20Aniruddha%20Bora%20and%20George%20Em%20Karniadakis%20and%20Michele%20Dassisti%0AAbstract%3A%20%20%20Accurate%20time-series%20forecasting%20for%20complex%20physical%20systems%20is%20the%20backbone%0Aof%20modern%20industrial%20monitoring%20and%20control.%20While%20deep%20learning%20models%20excel%0Aat%20capturing%20complex%20dynamics%2C%20currently%2C%20their%20deployment%20is%20limited%20due%20to%0Aphysical%20inconsistency%20and%20robustness%2C%20hence%20constraining%20their%20reliability%20in%0Aregulated%20environments.%20We%20introduce%20process-informed%20forecasting%20%28PIF%29%20models%0Afor%20temperature%20in%20pharmaceutical%20lyophilization.%20We%20investigate%20a%20wide%20range%0Aof%20models%2C%20from%20classical%20ones%20such%20as%20Autoregressive%20Integrated%20Moving%20Average%0AModel%20%28ARIMA%29%20and%20Exponential%20Smoothing%20Model%20%28ETS%29%2C%20to%20modern%20deep%20learning%0Aarchitectures%2C%20including%20Kolmogorov-Arnold%20Networks%20%28KANs%29.%20We%20compare%20three%0Adifferent%20loss%20function%20formulations%20that%20integrate%20a%20process-informed%0Atrajectory%20prior%3A%20a%20fixed-weight%20loss%2C%20a%20dynamic%20uncertainty-based%20loss%2C%20and%20a%0AResidual-Based%20Attention%20%28RBA%29%20mechanism.%20We%20evaluate%20all%20models%20not%20only%20for%0Aaccuracy%20and%20physical%20consistency%20but%20also%20for%20robustness%20to%20sensor%20noise.%0AFurthermore%2C%20we%20test%20the%20practical%20generalizability%20of%20the%20best%20model%20in%20a%0Atransfer%20learning%20scenario%20on%20a%20new%20process.%20Our%20results%20show%20that%20PIF%20models%0Aoutperform%20their%20data-driven%20counterparts%20in%20terms%20of%20accuracy%2C%20physical%0Aplausibility%20and%20noise%20resilience.%20This%20work%20provides%20a%20roadmap%20for%20developing%0Areliable%20and%20generalizable%20forecasting%20solutions%20for%20critical%20applications%20in%0Athe%20pharmaceutical%20manufacturing%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.20349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcess-Informed%2520Forecasting%2520of%2520Complex%2520Thermal%2520Dynamics%2520in%250A%2520%2520Pharmaceutical%2520Manufacturing%26entry.906535625%3DRamona%2520Rubini%2520and%2520Siavash%2520Khodakarami%2520and%2520Aniruddha%2520Bora%2520and%2520George%2520Em%2520Karniadakis%2520and%2520Michele%2520Dassisti%26entry.1292438233%3D%2520%2520Accurate%2520time-series%2520forecasting%2520for%2520complex%2520physical%2520systems%2520is%2520the%2520backbone%250Aof%2520modern%2520industrial%2520monitoring%2520and%2520control.%2520While%2520deep%2520learning%2520models%2520excel%250Aat%2520capturing%2520complex%2520dynamics%252C%2520currently%252C%2520their%2520deployment%2520is%2520limited%2520due%2520to%250Aphysical%2520inconsistency%2520and%2520robustness%252C%2520hence%2520constraining%2520their%2520reliability%2520in%250Aregulated%2520environments.%2520We%2520introduce%2520process-informed%2520forecasting%2520%2528PIF%2529%2520models%250Afor%2520temperature%2520in%2520pharmaceutical%2520lyophilization.%2520We%2520investigate%2520a%2520wide%2520range%250Aof%2520models%252C%2520from%2520classical%2520ones%2520such%2520as%2520Autoregressive%2520Integrated%2520Moving%2520Average%250AModel%2520%2528ARIMA%2529%2520and%2520Exponential%2520Smoothing%2520Model%2520%2528ETS%2529%252C%2520to%2520modern%2520deep%2520learning%250Aarchitectures%252C%2520including%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529.%2520We%2520compare%2520three%250Adifferent%2520loss%2520function%2520formulations%2520that%2520integrate%2520a%2520process-informed%250Atrajectory%2520prior%253A%2520a%2520fixed-weight%2520loss%252C%2520a%2520dynamic%2520uncertainty-based%2520loss%252C%2520and%2520a%250AResidual-Based%2520Attention%2520%2528RBA%2529%2520mechanism.%2520We%2520evaluate%2520all%2520models%2520not%2520only%2520for%250Aaccuracy%2520and%2520physical%2520consistency%2520but%2520also%2520for%2520robustness%2520to%2520sensor%2520noise.%250AFurthermore%252C%2520we%2520test%2520the%2520practical%2520generalizability%2520of%2520the%2520best%2520model%2520in%2520a%250Atransfer%2520learning%2520scenario%2520on%2520a%2520new%2520process.%2520Our%2520results%2520show%2520that%2520PIF%2520models%250Aoutperform%2520their%2520data-driven%2520counterparts%2520in%2520terms%2520of%2520accuracy%252C%2520physical%250Aplausibility%2520and%2520noise%2520resilience.%2520This%2520work%2520provides%2520a%2520roadmap%2520for%2520developing%250Areliable%2520and%2520generalizable%2520forecasting%2520solutions%2520for%2520critical%2520applications%2520in%250Athe%2520pharmaceutical%2520manufacturing%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Process-Informed%20Forecasting%20of%20Complex%20Thermal%20Dynamics%20in%0A%20%20Pharmaceutical%20Manufacturing&entry.906535625=Ramona%20Rubini%20and%20Siavash%20Khodakarami%20and%20Aniruddha%20Bora%20and%20George%20Em%20Karniadakis%20and%20Michele%20Dassisti&entry.1292438233=%20%20Accurate%20time-series%20forecasting%20for%20complex%20physical%20systems%20is%20the%20backbone%0Aof%20modern%20industrial%20monitoring%20and%20control.%20While%20deep%20learning%20models%20excel%0Aat%20capturing%20complex%20dynamics%2C%20currently%2C%20their%20deployment%20is%20limited%20due%20to%0Aphysical%20inconsistency%20and%20robustness%2C%20hence%20constraining%20their%20reliability%20in%0Aregulated%20environments.%20We%20introduce%20process-informed%20forecasting%20%28PIF%29%20models%0Afor%20temperature%20in%20pharmaceutical%20lyophilization.%20We%20investigate%20a%20wide%20range%0Aof%20models%2C%20from%20classical%20ones%20such%20as%20Autoregressive%20Integrated%20Moving%20Average%0AModel%20%28ARIMA%29%20and%20Exponential%20Smoothing%20Model%20%28ETS%29%2C%20to%20modern%20deep%20learning%0Aarchitectures%2C%20including%20Kolmogorov-Arnold%20Networks%20%28KANs%29.%20We%20compare%20three%0Adifferent%20loss%20function%20formulations%20that%20integrate%20a%20process-informed%0Atrajectory%20prior%3A%20a%20fixed-weight%20loss%2C%20a%20dynamic%20uncertainty-based%20loss%2C%20and%20a%0AResidual-Based%20Attention%20%28RBA%29%20mechanism.%20We%20evaluate%20all%20models%20not%20only%20for%0Aaccuracy%20and%20physical%20consistency%20but%20also%20for%20robustness%20to%20sensor%20noise.%0AFurthermore%2C%20we%20test%20the%20practical%20generalizability%20of%20the%20best%20model%20in%20a%0Atransfer%20learning%20scenario%20on%20a%20new%20process.%20Our%20results%20show%20that%20PIF%20models%0Aoutperform%20their%20data-driven%20counterparts%20in%20terms%20of%20accuracy%2C%20physical%0Aplausibility%20and%20noise%20resilience.%20This%20work%20provides%20a%20roadmap%20for%20developing%0Areliable%20and%20generalizable%20forecasting%20solutions%20for%20critical%20applications%20in%0Athe%20pharmaceutical%20manufacturing%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.20349v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


