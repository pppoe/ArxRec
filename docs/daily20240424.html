<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240423.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "X-3D: Explicit 3D Structure Modeling for Point Cloud Recognition", "author": "Shuofeng Sun and Yongming Rao and Jiwen Lu and Haibin Yan", "abstract": "  Numerous prior studies predominantly emphasize constructing relation vectors\nfor individual neighborhood points and generating dynamic kernels for each\nvector and embedding these into high-dimensional spaces to capture implicit\nlocal structures. However, we contend that such implicit high-dimensional\nstructure modeling approch inadequately represents the local geometric\nstructure of point clouds due to the absence of explicit structural\ninformation. Hence, we introduce X-3D, an explicit 3D structure modeling\napproach. X-3D functions by capturing the explicit local structural information\nwithin the input 3D space and employing it to produce dynamic kernels with\nshared weights for all neighborhood points within the current local region.\nThis modeling approach introduces effective geometric prior and significantly\ndiminishes the disparity between the local structure of the embedding space and\nthe original input point cloud, thereby improving the extraction of local\nfeatures. Experiments show that our method can be used on a variety of methods\nand achieves state-of-the-art performance on segmentation, classification,\ndetection tasks with lower extra computational cost, such as \\textbf{90.7\\%} on\nScanObjectNN for classification, \\textbf{79.2\\%} on S3DIS 6 fold and\n\\textbf{74.3\\%} on S3DIS Area 5 for segmentation, \\textbf{76.3\\%} on ScanNetV2\nfor segmentation and \\textbf{64.5\\%} mAP , \\textbf{46.9\\%} mAP on SUN RGB-D and\n\\textbf{69.0\\%} mAP , \\textbf{51.1\\%} mAP on ScanNetV2 . Our code is available\nat\n\\href{https://github.com/sunshuofeng/X-3D}{https://github.com/sunshuofeng/X-3D}.\n", "link": "http://arxiv.org/abs/2404.15010v1", "date": "2024-04-23", "relevancy": 2.9908, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6435}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5673}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20X-3D%3A%20Explicit%203D%20Structure%20Modeling%20for%20Point%20Cloud%20Recognition&body=Title%3A%20X-3D%3A%20Explicit%203D%20Structure%20Modeling%20for%20Point%20Cloud%20Recognition%0AAuthor%3A%20Shuofeng%20Sun%20and%20Yongming%20Rao%20and%20Jiwen%20Lu%20and%20Haibin%20Yan%0AAbstract%3A%20%20%20Numerous%20prior%20studies%20predominantly%20emphasize%20constructing%20relation%20vectors%0Afor%20individual%20neighborhood%20points%20and%20generating%20dynamic%20kernels%20for%20each%0Avector%20and%20embedding%20these%20into%20high-dimensional%20spaces%20to%20capture%20implicit%0Alocal%20structures.%20However%2C%20we%20contend%20that%20such%20implicit%20high-dimensional%0Astructure%20modeling%20approch%20inadequately%20represents%20the%20local%20geometric%0Astructure%20of%20point%20clouds%20due%20to%20the%20absence%20of%20explicit%20structural%0Ainformation.%20Hence%2C%20we%20introduce%20X-3D%2C%20an%20explicit%203D%20structure%20modeling%0Aapproach.%20X-3D%20functions%20by%20capturing%20the%20explicit%20local%20structural%20information%0Awithin%20the%20input%203D%20space%20and%20employing%20it%20to%20produce%20dynamic%20kernels%20with%0Ashared%20weights%20for%20all%20neighborhood%20points%20within%20the%20current%20local%20region.%0AThis%20modeling%20approach%20introduces%20effective%20geometric%20prior%20and%20significantly%0Adiminishes%20the%20disparity%20between%20the%20local%20structure%20of%20the%20embedding%20space%20and%0Athe%20original%20input%20point%20cloud%2C%20thereby%20improving%20the%20extraction%20of%20local%0Afeatures.%20Experiments%20show%20that%20our%20method%20can%20be%20used%20on%20a%20variety%20of%20methods%0Aand%20achieves%20state-of-the-art%20performance%20on%20segmentation%2C%20classification%2C%0Adetection%20tasks%20with%20lower%20extra%20computational%20cost%2C%20such%20as%20%5Ctextbf%7B90.7%5C%25%7D%20on%0AScanObjectNN%20for%20classification%2C%20%5Ctextbf%7B79.2%5C%25%7D%20on%20S3DIS%206%20fold%20and%0A%5Ctextbf%7B74.3%5C%25%7D%20on%20S3DIS%20Area%205%20for%20segmentation%2C%20%5Ctextbf%7B76.3%5C%25%7D%20on%20ScanNetV2%0Afor%20segmentation%20and%20%5Ctextbf%7B64.5%5C%25%7D%20mAP%20%2C%20%5Ctextbf%7B46.9%5C%25%7D%20mAP%20on%20SUN%20RGB-D%20and%0A%5Ctextbf%7B69.0%5C%25%7D%20mAP%20%2C%20%5Ctextbf%7B51.1%5C%25%7D%20mAP%20on%20ScanNetV2%20.%20Our%20code%20is%20available%0Aat%0A%5Chref%7Bhttps%3A//github.com/sunshuofeng/X-3D%7D%7Bhttps%3A//github.com/sunshuofeng/X-3D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15010v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-3D%3A%20Explicit%203D%20Structure%20Modeling%20for%20Point%20Cloud%20Recognition&entry.906535625=Shuofeng%20Sun%20and%20Yongming%20Rao%20and%20Jiwen%20Lu%20and%20Haibin%20Yan&entry.1292438233=%20%20Numerous%20prior%20studies%20predominantly%20emphasize%20constructing%20relation%20vectors%0Afor%20individual%20neighborhood%20points%20and%20generating%20dynamic%20kernels%20for%20each%0Avector%20and%20embedding%20these%20into%20high-dimensional%20spaces%20to%20capture%20implicit%0Alocal%20structures.%20However%2C%20we%20contend%20that%20such%20implicit%20high-dimensional%0Astructure%20modeling%20approch%20inadequately%20represents%20the%20local%20geometric%0Astructure%20of%20point%20clouds%20due%20to%20the%20absence%20of%20explicit%20structural%0Ainformation.%20Hence%2C%20we%20introduce%20X-3D%2C%20an%20explicit%203D%20structure%20modeling%0Aapproach.%20X-3D%20functions%20by%20capturing%20the%20explicit%20local%20structural%20information%0Awithin%20the%20input%203D%20space%20and%20employing%20it%20to%20produce%20dynamic%20kernels%20with%0Ashared%20weights%20for%20all%20neighborhood%20points%20within%20the%20current%20local%20region.%0AThis%20modeling%20approach%20introduces%20effective%20geometric%20prior%20and%20significantly%0Adiminishes%20the%20disparity%20between%20the%20local%20structure%20of%20the%20embedding%20space%20and%0Athe%20original%20input%20point%20cloud%2C%20thereby%20improving%20the%20extraction%20of%20local%0Afeatures.%20Experiments%20show%20that%20our%20method%20can%20be%20used%20on%20a%20variety%20of%20methods%0Aand%20achieves%20state-of-the-art%20performance%20on%20segmentation%2C%20classification%2C%0Adetection%20tasks%20with%20lower%20extra%20computational%20cost%2C%20such%20as%20%5Ctextbf%7B90.7%5C%25%7D%20on%0AScanObjectNN%20for%20classification%2C%20%5Ctextbf%7B79.2%5C%25%7D%20on%20S3DIS%206%20fold%20and%0A%5Ctextbf%7B74.3%5C%25%7D%20on%20S3DIS%20Area%205%20for%20segmentation%2C%20%5Ctextbf%7B76.3%5C%25%7D%20on%20ScanNetV2%0Afor%20segmentation%20and%20%5Ctextbf%7B64.5%5C%25%7D%20mAP%20%2C%20%5Ctextbf%7B46.9%5C%25%7D%20mAP%20on%20SUN%20RGB-D%20and%0A%5Ctextbf%7B69.0%5C%25%7D%20mAP%20%2C%20%5Ctextbf%7B51.1%5C%25%7D%20mAP%20on%20ScanNetV2%20.%20Our%20code%20is%20available%0Aat%0A%5Chref%7Bhttps%3A//github.com/sunshuofeng/X-3D%7D%7Bhttps%3A//github.com/sunshuofeng/X-3D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15010v1&entry.124074799=Read"},
{"title": "Are Semi-Dense Detector-Free Methods Good at Matching Local Features?", "author": "Matthieu Vilain and R\u00e9mi Giraud and Hugo Germain and Guillaume Bourmaud", "abstract": "  Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among\nthe most popular image matching methods. While SDF methods are trained to\nestablish correspondences between two images, their performances are almost\nexclusively evaluated using relative pose estimation metrics. Thus, the link\nbetween their ability to establish correspondences and the quality of the\nresulting estimated pose has thus far received little attention. This paper is\na first attempt to study this link. We start with proposing a novel structured\nattention-based image matching architecture (SAM). It allows us to show a\ncounter-intuitive result on two datasets (MegaDepth and HPatches): on the one\nhand SAM either outperforms or is on par with SDF methods in terms of\npose/homography estimation metrics, but on the other hand SDF approaches are\nsignificantly better than SAM in terms of matching accuracy. We then propose to\nlimit the computation of the matching accuracy to textured regions, and show\nthat in this case SAM often surpasses SDF methods. Our findings highlight a\nstrong correlation between the ability to establish accurate correspondences in\ntextured regions and the accuracy of the resulting estimated pose/homography.\nOur code will be made available.\n", "link": "http://arxiv.org/abs/2402.08671v2", "date": "2024-04-23", "relevancy": 2.8877, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6244}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5613}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.547}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Are%20Semi-Dense%20Detector-Free%20Methods%20Good%20at%20Matching%20Local%20Features%3F&body=Title%3A%20Are%20Semi-Dense%20Detector-Free%20Methods%20Good%20at%20Matching%20Local%20Features%3F%0AAuthor%3A%20Matthieu%20Vilain%20and%20R%C3%A9mi%20Giraud%20and%20Hugo%20Germain%20and%20Guillaume%20Bourmaud%0AAbstract%3A%20%20%20Semi-dense%20detector-free%20approaches%20%28SDF%29%2C%20such%20as%20LoFTR%2C%20are%20currently%20among%0Athe%20most%20popular%20image%20matching%20methods.%20While%20SDF%20methods%20are%20trained%20to%0Aestablish%20correspondences%20between%20two%20images%2C%20their%20performances%20are%20almost%0Aexclusively%20evaluated%20using%20relative%20pose%20estimation%20metrics.%20Thus%2C%20the%20link%0Abetween%20their%20ability%20to%20establish%20correspondences%20and%20the%20quality%20of%20the%0Aresulting%20estimated%20pose%20has%20thus%20far%20received%20little%20attention.%20This%20paper%20is%0Aa%20first%20attempt%20to%20study%20this%20link.%20We%20start%20with%20proposing%20a%20novel%20structured%0Aattention-based%20image%20matching%20architecture%20%28SAM%29.%20It%20allows%20us%20to%20show%20a%0Acounter-intuitive%20result%20on%20two%20datasets%20%28MegaDepth%20and%20HPatches%29%3A%20on%20the%20one%0Ahand%20SAM%20either%20outperforms%20or%20is%20on%20par%20with%20SDF%20methods%20in%20terms%20of%0Apose/homography%20estimation%20metrics%2C%20but%20on%20the%20other%20hand%20SDF%20approaches%20are%0Asignificantly%20better%20than%20SAM%20in%20terms%20of%20matching%20accuracy.%20We%20then%20propose%20to%0Alimit%20the%20computation%20of%20the%20matching%20accuracy%20to%20textured%20regions%2C%20and%20show%0Athat%20in%20this%20case%20SAM%20often%20surpasses%20SDF%20methods.%20Our%20findings%20highlight%20a%0Astrong%20correlation%20between%20the%20ability%20to%20establish%20accurate%20correspondences%20in%0Atextured%20regions%20and%20the%20accuracy%20of%20the%20resulting%20estimated%20pose/homography.%0AOur%20code%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08671v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Semi-Dense%20Detector-Free%20Methods%20Good%20at%20Matching%20Local%20Features%3F&entry.906535625=Matthieu%20Vilain%20and%20R%C3%A9mi%20Giraud%20and%20Hugo%20Germain%20and%20Guillaume%20Bourmaud&entry.1292438233=%20%20Semi-dense%20detector-free%20approaches%20%28SDF%29%2C%20such%20as%20LoFTR%2C%20are%20currently%20among%0Athe%20most%20popular%20image%20matching%20methods.%20While%20SDF%20methods%20are%20trained%20to%0Aestablish%20correspondences%20between%20two%20images%2C%20their%20performances%20are%20almost%0Aexclusively%20evaluated%20using%20relative%20pose%20estimation%20metrics.%20Thus%2C%20the%20link%0Abetween%20their%20ability%20to%20establish%20correspondences%20and%20the%20quality%20of%20the%0Aresulting%20estimated%20pose%20has%20thus%20far%20received%20little%20attention.%20This%20paper%20is%0Aa%20first%20attempt%20to%20study%20this%20link.%20We%20start%20with%20proposing%20a%20novel%20structured%0Aattention-based%20image%20matching%20architecture%20%28SAM%29.%20It%20allows%20us%20to%20show%20a%0Acounter-intuitive%20result%20on%20two%20datasets%20%28MegaDepth%20and%20HPatches%29%3A%20on%20the%20one%0Ahand%20SAM%20either%20outperforms%20or%20is%20on%20par%20with%20SDF%20methods%20in%20terms%20of%0Apose/homography%20estimation%20metrics%2C%20but%20on%20the%20other%20hand%20SDF%20approaches%20are%0Asignificantly%20better%20than%20SAM%20in%20terms%20of%20matching%20accuracy.%20We%20then%20propose%20to%0Alimit%20the%20computation%20of%20the%20matching%20accuracy%20to%20textured%20regions%2C%20and%20show%0Athat%20in%20this%20case%20SAM%20often%20surpasses%20SDF%20methods.%20Our%20findings%20highlight%20a%0Astrong%20correlation%20between%20the%20ability%20to%20establish%20accurate%20correspondences%20in%0Atextured%20regions%20and%20the%20accuracy%20of%20the%20resulting%20estimated%20pose/homography.%0AOur%20code%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08671v2&entry.124074799=Read"},
{"title": "DAWN: Domain-Adaptive Weakly Supervised Nuclei Segmentation via\n  Cross-Task Interactions", "author": "Ye Zhang and Yifeng Wang and Zijie Fang and Hao Bian and Linghan Cai and Ziyue Wang and Yongbing Zhang", "abstract": "  Weakly supervised segmentation methods have gained significant attention due\nto their ability to reduce the reliance on costly pixel-level annotations\nduring model training. However, the current weakly supervised nuclei\nsegmentation approaches typically follow a two-stage pseudo-label generation\nand network training process. The performance of the nuclei segmentation\nheavily relies on the quality of the generated pseudo-labels, thereby limiting\nits effectiveness. This paper introduces a novel domain-adaptive weakly\nsupervised nuclei segmentation framework using cross-task interaction\nstrategies to overcome the challenge of pseudo-label generation. Specifically,\nwe utilize weakly annotated data to train an auxiliary detection task, which\nassists the domain adaptation of the segmentation network. To enhance the\nefficiency of domain adaptation, we design a consistent feature constraint\nmodule integrating prior knowledge from the source domain. Furthermore, we\ndevelop pseudo-label optimization and interactive training methods to improve\nthe domain transfer capability. To validate the effectiveness of our proposed\nmethod, we conduct extensive comparative and ablation experiments on six\ndatasets. The results demonstrate the superiority of our approach over existing\nweakly supervised approaches. Remarkably, our method achieves comparable or\neven better performance than fully supervised methods. Our code will be\nreleased in https://github.com/zhangye-zoe/DAWN.\n", "link": "http://arxiv.org/abs/2404.14956v1", "date": "2024-04-23", "relevancy": 2.7697, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5848}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DAWN%3A%20Domain-Adaptive%20Weakly%20Supervised%20Nuclei%20Segmentation%20via%0A%20%20Cross-Task%20Interactions&body=Title%3A%20DAWN%3A%20Domain-Adaptive%20Weakly%20Supervised%20Nuclei%20Segmentation%20via%0A%20%20Cross-Task%20Interactions%0AAuthor%3A%20Ye%20Zhang%20and%20Yifeng%20Wang%20and%20Zijie%20Fang%20and%20Hao%20Bian%20and%20Linghan%20Cai%20and%20Ziyue%20Wang%20and%20Yongbing%20Zhang%0AAbstract%3A%20%20%20Weakly%20supervised%20segmentation%20methods%20have%20gained%20significant%20attention%20due%0Ato%20their%20ability%20to%20reduce%20the%20reliance%20on%20costly%20pixel-level%20annotations%0Aduring%20model%20training.%20However%2C%20the%20current%20weakly%20supervised%20nuclei%0Asegmentation%20approaches%20typically%20follow%20a%20two-stage%20pseudo-label%20generation%0Aand%20network%20training%20process.%20The%20performance%20of%20the%20nuclei%20segmentation%0Aheavily%20relies%20on%20the%20quality%20of%20the%20generated%20pseudo-labels%2C%20thereby%20limiting%0Aits%20effectiveness.%20This%20paper%20introduces%20a%20novel%20domain-adaptive%20weakly%0Asupervised%20nuclei%20segmentation%20framework%20using%20cross-task%20interaction%0Astrategies%20to%20overcome%20the%20challenge%20of%20pseudo-label%20generation.%20Specifically%2C%0Awe%20utilize%20weakly%20annotated%20data%20to%20train%20an%20auxiliary%20detection%20task%2C%20which%0Aassists%20the%20domain%20adaptation%20of%20the%20segmentation%20network.%20To%20enhance%20the%0Aefficiency%20of%20domain%20adaptation%2C%20we%20design%20a%20consistent%20feature%20constraint%0Amodule%20integrating%20prior%20knowledge%20from%20the%20source%20domain.%20Furthermore%2C%20we%0Adevelop%20pseudo-label%20optimization%20and%20interactive%20training%20methods%20to%20improve%0Athe%20domain%20transfer%20capability.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%0Amethod%2C%20we%20conduct%20extensive%20comparative%20and%20ablation%20experiments%20on%20six%0Adatasets.%20The%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%0Aweakly%20supervised%20approaches.%20Remarkably%2C%20our%20method%20achieves%20comparable%20or%0Aeven%20better%20performance%20than%20fully%20supervised%20methods.%20Our%20code%20will%20be%0Areleased%20in%20https%3A//github.com/zhangye-zoe/DAWN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14956v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAWN%3A%20Domain-Adaptive%20Weakly%20Supervised%20Nuclei%20Segmentation%20via%0A%20%20Cross-Task%20Interactions&entry.906535625=Ye%20Zhang%20and%20Yifeng%20Wang%20and%20Zijie%20Fang%20and%20Hao%20Bian%20and%20Linghan%20Cai%20and%20Ziyue%20Wang%20and%20Yongbing%20Zhang&entry.1292438233=%20%20Weakly%20supervised%20segmentation%20methods%20have%20gained%20significant%20attention%20due%0Ato%20their%20ability%20to%20reduce%20the%20reliance%20on%20costly%20pixel-level%20annotations%0Aduring%20model%20training.%20However%2C%20the%20current%20weakly%20supervised%20nuclei%0Asegmentation%20approaches%20typically%20follow%20a%20two-stage%20pseudo-label%20generation%0Aand%20network%20training%20process.%20The%20performance%20of%20the%20nuclei%20segmentation%0Aheavily%20relies%20on%20the%20quality%20of%20the%20generated%20pseudo-labels%2C%20thereby%20limiting%0Aits%20effectiveness.%20This%20paper%20introduces%20a%20novel%20domain-adaptive%20weakly%0Asupervised%20nuclei%20segmentation%20framework%20using%20cross-task%20interaction%0Astrategies%20to%20overcome%20the%20challenge%20of%20pseudo-label%20generation.%20Specifically%2C%0Awe%20utilize%20weakly%20annotated%20data%20to%20train%20an%20auxiliary%20detection%20task%2C%20which%0Aassists%20the%20domain%20adaptation%20of%20the%20segmentation%20network.%20To%20enhance%20the%0Aefficiency%20of%20domain%20adaptation%2C%20we%20design%20a%20consistent%20feature%20constraint%0Amodule%20integrating%20prior%20knowledge%20from%20the%20source%20domain.%20Furthermore%2C%20we%0Adevelop%20pseudo-label%20optimization%20and%20interactive%20training%20methods%20to%20improve%0Athe%20domain%20transfer%20capability.%20To%20validate%20the%20effectiveness%20of%20our%20proposed%0Amethod%2C%20we%20conduct%20extensive%20comparative%20and%20ablation%20experiments%20on%20six%0Adatasets.%20The%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%0Aweakly%20supervised%20approaches.%20Remarkably%2C%20our%20method%20achieves%20comparable%20or%0Aeven%20better%20performance%20than%20fully%20supervised%20methods.%20Our%20code%20will%20be%0Areleased%20in%20https%3A//github.com/zhangye-zoe/DAWN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14956v1&entry.124074799=Read"},
{"title": "Mamba3D: Enhancing Local Features for 3D Point Cloud Analysis via State\n  Space Model", "author": "Xu Han and Yuan Tang and Zhaoxuan Wang and Xianzhi Li", "abstract": "  Existing Transformer-based models for point cloud analysis suffer from\nquadratic complexity, leading to compromised point cloud resolution and\ninformation loss. In contrast, the newly proposed Mamba model, based on state\nspace models (SSM), outperforms Transformer in multiple areas with only linear\ncomplexity. However, the straightforward adoption of Mamba does not achieve\nsatisfactory performance on point cloud tasks. In this work, we present\nMamba3D, a state space model tailored for point cloud learning to enhance local\nfeature extraction, achieving superior performance, high efficiency, and\nscalability potential. Specifically, we propose a simple yet effective Local\nNorm Pooling (LNP) block to extract local geometric features. Additionally, to\nobtain better global features, we introduce a bidirectional SSM (bi-SSM) with\nboth a token forward SSM and a novel backward SSM that operates on the feature\nchannel. Extensive experimental results show that Mamba3D surpasses\nTransformer-based counterparts and concurrent works in multiple tasks, with or\nwithout pre-training. Notably, Mamba3D achieves multiple SoTA, including an\noverall accuracy of 92.6% (train from scratch) on the ScanObjectNN and 95.1%\n(with single-modal pre-training) on the ModelNet40 classification task, with\nonly linear complexity.\n", "link": "http://arxiv.org/abs/2404.14966v1", "date": "2024-04-23", "relevancy": 2.7429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5608}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5598}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5251}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mamba3D%3A%20Enhancing%20Local%20Features%20for%203D%20Point%20Cloud%20Analysis%20via%20State%0A%20%20Space%20Model&body=Title%3A%20Mamba3D%3A%20Enhancing%20Local%20Features%20for%203D%20Point%20Cloud%20Analysis%20via%20State%0A%20%20Space%20Model%0AAuthor%3A%20Xu%20Han%20and%20Yuan%20Tang%20and%20Zhaoxuan%20Wang%20and%20Xianzhi%20Li%0AAbstract%3A%20%20%20Existing%20Transformer-based%20models%20for%20point%20cloud%20analysis%20suffer%20from%0Aquadratic%20complexity%2C%20leading%20to%20compromised%20point%20cloud%20resolution%20and%0Ainformation%20loss.%20In%20contrast%2C%20the%20newly%20proposed%20Mamba%20model%2C%20based%20on%20state%0Aspace%20models%20%28SSM%29%2C%20outperforms%20Transformer%20in%20multiple%20areas%20with%20only%20linear%0Acomplexity.%20However%2C%20the%20straightforward%20adoption%20of%20Mamba%20does%20not%20achieve%0Asatisfactory%20performance%20on%20point%20cloud%20tasks.%20In%20this%20work%2C%20we%20present%0AMamba3D%2C%20a%20state%20space%20model%20tailored%20for%20point%20cloud%20learning%20to%20enhance%20local%0Afeature%20extraction%2C%20achieving%20superior%20performance%2C%20high%20efficiency%2C%20and%0Ascalability%20potential.%20Specifically%2C%20we%20propose%20a%20simple%20yet%20effective%20Local%0ANorm%20Pooling%20%28LNP%29%20block%20to%20extract%20local%20geometric%20features.%20Additionally%2C%20to%0Aobtain%20better%20global%20features%2C%20we%20introduce%20a%20bidirectional%20SSM%20%28bi-SSM%29%20with%0Aboth%20a%20token%20forward%20SSM%20and%20a%20novel%20backward%20SSM%20that%20operates%20on%20the%20feature%0Achannel.%20Extensive%20experimental%20results%20show%20that%20Mamba3D%20surpasses%0ATransformer-based%20counterparts%20and%20concurrent%20works%20in%20multiple%20tasks%2C%20with%20or%0Awithout%20pre-training.%20Notably%2C%20Mamba3D%20achieves%20multiple%20SoTA%2C%20including%20an%0Aoverall%20accuracy%20of%2092.6%25%20%28train%20from%20scratch%29%20on%20the%20ScanObjectNN%20and%2095.1%25%0A%28with%20single-modal%20pre-training%29%20on%20the%20ModelNet40%20classification%20task%2C%20with%0Aonly%20linear%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14966v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba3D%3A%20Enhancing%20Local%20Features%20for%203D%20Point%20Cloud%20Analysis%20via%20State%0A%20%20Space%20Model&entry.906535625=Xu%20Han%20and%20Yuan%20Tang%20and%20Zhaoxuan%20Wang%20and%20Xianzhi%20Li&entry.1292438233=%20%20Existing%20Transformer-based%20models%20for%20point%20cloud%20analysis%20suffer%20from%0Aquadratic%20complexity%2C%20leading%20to%20compromised%20point%20cloud%20resolution%20and%0Ainformation%20loss.%20In%20contrast%2C%20the%20newly%20proposed%20Mamba%20model%2C%20based%20on%20state%0Aspace%20models%20%28SSM%29%2C%20outperforms%20Transformer%20in%20multiple%20areas%20with%20only%20linear%0Acomplexity.%20However%2C%20the%20straightforward%20adoption%20of%20Mamba%20does%20not%20achieve%0Asatisfactory%20performance%20on%20point%20cloud%20tasks.%20In%20this%20work%2C%20we%20present%0AMamba3D%2C%20a%20state%20space%20model%20tailored%20for%20point%20cloud%20learning%20to%20enhance%20local%0Afeature%20extraction%2C%20achieving%20superior%20performance%2C%20high%20efficiency%2C%20and%0Ascalability%20potential.%20Specifically%2C%20we%20propose%20a%20simple%20yet%20effective%20Local%0ANorm%20Pooling%20%28LNP%29%20block%20to%20extract%20local%20geometric%20features.%20Additionally%2C%20to%0Aobtain%20better%20global%20features%2C%20we%20introduce%20a%20bidirectional%20SSM%20%28bi-SSM%29%20with%0Aboth%20a%20token%20forward%20SSM%20and%20a%20novel%20backward%20SSM%20that%20operates%20on%20the%20feature%0Achannel.%20Extensive%20experimental%20results%20show%20that%20Mamba3D%20surpasses%0ATransformer-based%20counterparts%20and%20concurrent%20works%20in%20multiple%20tasks%2C%20with%20or%0Awithout%20pre-training.%20Notably%2C%20Mamba3D%20achieves%20multiple%20SoTA%2C%20including%20an%0Aoverall%20accuracy%20of%2092.6%25%20%28train%20from%20scratch%29%20on%20the%20ScanObjectNN%20and%2095.1%25%0A%28with%20single-modal%20pre-training%29%20on%20the%20ModelNet40%20classification%20task%2C%20with%0Aonly%20linear%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14966v1&entry.124074799=Read"},
{"title": "DP-Net: Learning Discriminative Parts for image recognition", "author": "Ronan Sicre and Hanwei Zhang and Julien Dejasmin and Chiheb Daaloul and St\u00e9phane Ayache and Thierry Arti\u00e8res", "abstract": "  This paper presents Discriminative Part Network (DP-Net), a deep architecture\nwith strong interpretation capabilities, which exploits a pretrained\nConvolutional Neural Network (CNN) combined with a part-based recognition\nmodule. This system learns and detects parts in the images that are\ndiscriminative among categories, without the need for fine-tuning the CNN,\nmaking it more scalable than other part-based models. While part-based\napproaches naturally offer interpretable representations, we propose\nexplanations at image and category levels and introduce specific constraints on\nthe part learning process to make them more discrimative.\n", "link": "http://arxiv.org/abs/2404.15037v1", "date": "2024-04-23", "relevancy": 2.7048, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5935}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5308}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DP-Net%3A%20Learning%20Discriminative%20Parts%20for%20image%20recognition&body=Title%3A%20DP-Net%3A%20Learning%20Discriminative%20Parts%20for%20image%20recognition%0AAuthor%3A%20Ronan%20Sicre%20and%20Hanwei%20Zhang%20and%20Julien%20Dejasmin%20and%20Chiheb%20Daaloul%20and%20St%C3%A9phane%20Ayache%20and%20Thierry%20Arti%C3%A8res%0AAbstract%3A%20%20%20This%20paper%20presents%20Discriminative%20Part%20Network%20%28DP-Net%29%2C%20a%20deep%20architecture%0Awith%20strong%20interpretation%20capabilities%2C%20which%20exploits%20a%20pretrained%0AConvolutional%20Neural%20Network%20%28CNN%29%20combined%20with%20a%20part-based%20recognition%0Amodule.%20This%20system%20learns%20and%20detects%20parts%20in%20the%20images%20that%20are%0Adiscriminative%20among%20categories%2C%20without%20the%20need%20for%20fine-tuning%20the%20CNN%2C%0Amaking%20it%20more%20scalable%20than%20other%20part-based%20models.%20While%20part-based%0Aapproaches%20naturally%20offer%20interpretable%20representations%2C%20we%20propose%0Aexplanations%20at%20image%20and%20category%20levels%20and%20introduce%20specific%20constraints%20on%0Athe%20part%20learning%20process%20to%20make%20them%20more%20discrimative.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15037v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-Net%3A%20Learning%20Discriminative%20Parts%20for%20image%20recognition&entry.906535625=Ronan%20Sicre%20and%20Hanwei%20Zhang%20and%20Julien%20Dejasmin%20and%20Chiheb%20Daaloul%20and%20St%C3%A9phane%20Ayache%20and%20Thierry%20Arti%C3%A8res&entry.1292438233=%20%20This%20paper%20presents%20Discriminative%20Part%20Network%20%28DP-Net%29%2C%20a%20deep%20architecture%0Awith%20strong%20interpretation%20capabilities%2C%20which%20exploits%20a%20pretrained%0AConvolutional%20Neural%20Network%20%28CNN%29%20combined%20with%20a%20part-based%20recognition%0Amodule.%20This%20system%20learns%20and%20detects%20parts%20in%20the%20images%20that%20are%0Adiscriminative%20among%20categories%2C%20without%20the%20need%20for%20fine-tuning%20the%20CNN%2C%0Amaking%20it%20more%20scalable%20than%20other%20part-based%20models.%20While%20part-based%0Aapproaches%20naturally%20offer%20interpretable%20representations%2C%20we%20propose%0Aexplanations%20at%20image%20and%20category%20levels%20and%20introduce%20specific%20constraints%20on%0Athe%20part%20learning%20process%20to%20make%20them%20more%20discrimative.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15037v1&entry.124074799=Read"},
{"title": "G3R: Generating Rich and Fine-grained mmWave Radar Data from 2D Videos\n  for Generalized Gesture Recognition", "author": "Kaikai Deng and Dong Zhao and Wenxin Zheng and Yue Ling and Kangwen Yin and Huadong Ma", "abstract": "  Millimeter wave radar is gaining traction recently as a promising modality\nfor enabling pervasive and privacy-preserving gesture recognition. However, the\nlack of rich and fine-grained radar datasets hinders progress in developing\ngeneralized deep learning models for gesture recognition across various user\npostures (e.g., standing, sitting), positions, and scenes. To remedy this, we\nresort to designing a software pipeline that exploits wealthy 2D videos to\ngenerate realistic radar data, but it needs to address the challenge of\nsimulating diversified and fine-grained reflection properties of user gestures.\nTo this end, we design G3R with three key components: (i) a gesture reflection\npoint generator expands the arm's skeleton points to form human reflection\npoints; (ii) a signal simulation model simulates the multipath reflection and\nattenuation of radar signals to output the human intensity map; (iii) an\nencoder-decoder model combines a sampling module and a fitting module to\naddress the differences in number and distribution of points between generated\nand real-world radar data for generating realistic radar data. We implement and\nevaluate G3R using 2D videos from public data sources and self-collected\nreal-world radar data, demonstrating its superiority over other\nstate-of-the-art approaches for gesture recognition.\n", "link": "http://arxiv.org/abs/2404.14934v1", "date": "2024-04-23", "relevancy": 2.6705, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5636}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5197}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20G3R%3A%20Generating%20Rich%20and%20Fine-grained%20mmWave%20Radar%20Data%20from%202D%20Videos%0A%20%20for%20Generalized%20Gesture%20Recognition&body=Title%3A%20G3R%3A%20Generating%20Rich%20and%20Fine-grained%20mmWave%20Radar%20Data%20from%202D%20Videos%0A%20%20for%20Generalized%20Gesture%20Recognition%0AAuthor%3A%20Kaikai%20Deng%20and%20Dong%20Zhao%20and%20Wenxin%20Zheng%20and%20Yue%20Ling%20and%20Kangwen%20Yin%20and%20Huadong%20Ma%0AAbstract%3A%20%20%20Millimeter%20wave%20radar%20is%20gaining%20traction%20recently%20as%20a%20promising%20modality%0Afor%20enabling%20pervasive%20and%20privacy-preserving%20gesture%20recognition.%20However%2C%20the%0Alack%20of%20rich%20and%20fine-grained%20radar%20datasets%20hinders%20progress%20in%20developing%0Ageneralized%20deep%20learning%20models%20for%20gesture%20recognition%20across%20various%20user%0Apostures%20%28e.g.%2C%20standing%2C%20sitting%29%2C%20positions%2C%20and%20scenes.%20To%20remedy%20this%2C%20we%0Aresort%20to%20designing%20a%20software%20pipeline%20that%20exploits%20wealthy%202D%20videos%20to%0Agenerate%20realistic%20radar%20data%2C%20but%20it%20needs%20to%20address%20the%20challenge%20of%0Asimulating%20diversified%20and%20fine-grained%20reflection%20properties%20of%20user%20gestures.%0ATo%20this%20end%2C%20we%20design%20G3R%20with%20three%20key%20components%3A%20%28i%29%20a%20gesture%20reflection%0Apoint%20generator%20expands%20the%20arm%27s%20skeleton%20points%20to%20form%20human%20reflection%0Apoints%3B%20%28ii%29%20a%20signal%20simulation%20model%20simulates%20the%20multipath%20reflection%20and%0Aattenuation%20of%20radar%20signals%20to%20output%20the%20human%20intensity%20map%3B%20%28iii%29%20an%0Aencoder-decoder%20model%20combines%20a%20sampling%20module%20and%20a%20fitting%20module%20to%0Aaddress%20the%20differences%20in%20number%20and%20distribution%20of%20points%20between%20generated%0Aand%20real-world%20radar%20data%20for%20generating%20realistic%20radar%20data.%20We%20implement%20and%0Aevaluate%20G3R%20using%202D%20videos%20from%20public%20data%20sources%20and%20self-collected%0Areal-world%20radar%20data%2C%20demonstrating%20its%20superiority%20over%20other%0Astate-of-the-art%20approaches%20for%20gesture%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14934v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G3R%3A%20Generating%20Rich%20and%20Fine-grained%20mmWave%20Radar%20Data%20from%202D%20Videos%0A%20%20for%20Generalized%20Gesture%20Recognition&entry.906535625=Kaikai%20Deng%20and%20Dong%20Zhao%20and%20Wenxin%20Zheng%20and%20Yue%20Ling%20and%20Kangwen%20Yin%20and%20Huadong%20Ma&entry.1292438233=%20%20Millimeter%20wave%20radar%20is%20gaining%20traction%20recently%20as%20a%20promising%20modality%0Afor%20enabling%20pervasive%20and%20privacy-preserving%20gesture%20recognition.%20However%2C%20the%0Alack%20of%20rich%20and%20fine-grained%20radar%20datasets%20hinders%20progress%20in%20developing%0Ageneralized%20deep%20learning%20models%20for%20gesture%20recognition%20across%20various%20user%0Apostures%20%28e.g.%2C%20standing%2C%20sitting%29%2C%20positions%2C%20and%20scenes.%20To%20remedy%20this%2C%20we%0Aresort%20to%20designing%20a%20software%20pipeline%20that%20exploits%20wealthy%202D%20videos%20to%0Agenerate%20realistic%20radar%20data%2C%20but%20it%20needs%20to%20address%20the%20challenge%20of%0Asimulating%20diversified%20and%20fine-grained%20reflection%20properties%20of%20user%20gestures.%0ATo%20this%20end%2C%20we%20design%20G3R%20with%20three%20key%20components%3A%20%28i%29%20a%20gesture%20reflection%0Apoint%20generator%20expands%20the%20arm%27s%20skeleton%20points%20to%20form%20human%20reflection%0Apoints%3B%20%28ii%29%20a%20signal%20simulation%20model%20simulates%20the%20multipath%20reflection%20and%0Aattenuation%20of%20radar%20signals%20to%20output%20the%20human%20intensity%20map%3B%20%28iii%29%20an%0Aencoder-decoder%20model%20combines%20a%20sampling%20module%20and%20a%20fitting%20module%20to%0Aaddress%20the%20differences%20in%20number%20and%20distribution%20of%20points%20between%20generated%0Aand%20real-world%20radar%20data%20for%20generating%20realistic%20radar%20data.%20We%20implement%20and%0Aevaluate%20G3R%20using%202D%20videos%20from%20public%20data%20sources%20and%20self-collected%0Areal-world%20radar%20data%2C%20demonstrating%20its%20superiority%20over%20other%0Astate-of-the-art%20approaches%20for%20gesture%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14934v1&entry.124074799=Read"},
{"title": "ID-Animator: Zero-Shot Identity-Preserving Human Video Generation", "author": "Xuanhua He and Quande Liu and Shengju Qian and Xin Wang and Tao Hu and Ke Cao and Keyu Yan and Man Zhou and Jie Zhang", "abstract": "  Generating high fidelity human video with specified identities has attracted\nsignificant attention in the content generation community. However, existing\ntechniques struggle to strike a balance between training efficiency and\nidentity preservation, either requiring tedious case-by-case finetuning or\nusually missing the identity details in video generation process. In this\nstudy, we present ID-Animator, a zero-shot human-video generation approach that\ncan perform personalized video generation given single reference facial image\nwithout further training. ID-Animator inherits existing diffusion-based video\ngeneration backbones with a face adapter to encode the ID-relevant embeddings\nfrom learnable facial latent queries. To facilitate the extraction of identity\ninformation in video generation, we introduce an ID-oriented dataset\nconstruction pipeline, which incorporates decoupled human attribute and action\ncaptioning technique from a constructed facial image pool. Based on this\npipeline, a random face reference training method is further devised to\nprecisely capture the ID-relevant embeddings from reference images, thus\nimproving the fidelity and generalization capacity of our model for ID-specific\nvideo generation. Extensive experiments demonstrate the superiority of\nID-Animator to generate personalized human videos over previous models.\nMoreover, our method is highly compatible with popular pre-trained T2V models\nlike animatediff and various community backbone models, showing high\nextendability in real-world applications for video generation where identity\npreservation is highly desired. Our codes and checkpoints will be released at\nhttps://github.com/ID-Animator/ID-Animator.\n", "link": "http://arxiv.org/abs/2404.15275v1", "date": "2024-04-23", "relevancy": 2.6665, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7532}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6228}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5975}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ID-Animator%3A%20Zero-Shot%20Identity-Preserving%20Human%20Video%20Generation&body=Title%3A%20ID-Animator%3A%20Zero-Shot%20Identity-Preserving%20Human%20Video%20Generation%0AAuthor%3A%20Xuanhua%20He%20and%20Quande%20Liu%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Tao%20Hu%20and%20Ke%20Cao%20and%20Keyu%20Yan%20and%20Man%20Zhou%20and%20Jie%20Zhang%0AAbstract%3A%20%20%20Generating%20high%20fidelity%20human%20video%20with%20specified%20identities%20has%20attracted%0Asignificant%20attention%20in%20the%20content%20generation%20community.%20However%2C%20existing%0Atechniques%20struggle%20to%20strike%20a%20balance%20between%20training%20efficiency%20and%0Aidentity%20preservation%2C%20either%20requiring%20tedious%20case-by-case%20finetuning%20or%0Ausually%20missing%20the%20identity%20details%20in%20video%20generation%20process.%20In%20this%0Astudy%2C%20we%20present%20ID-Animator%2C%20a%20zero-shot%20human-video%20generation%20approach%20that%0Acan%20perform%20personalized%20video%20generation%20given%20single%20reference%20facial%20image%0Awithout%20further%20training.%20ID-Animator%20inherits%20existing%20diffusion-based%20video%0Ageneration%20backbones%20with%20a%20face%20adapter%20to%20encode%20the%20ID-relevant%20embeddings%0Afrom%20learnable%20facial%20latent%20queries.%20To%20facilitate%20the%20extraction%20of%20identity%0Ainformation%20in%20video%20generation%2C%20we%20introduce%20an%20ID-oriented%20dataset%0Aconstruction%20pipeline%2C%20which%20incorporates%20decoupled%20human%20attribute%20and%20action%0Acaptioning%20technique%20from%20a%20constructed%20facial%20image%20pool.%20Based%20on%20this%0Apipeline%2C%20a%20random%20face%20reference%20training%20method%20is%20further%20devised%20to%0Aprecisely%20capture%20the%20ID-relevant%20embeddings%20from%20reference%20images%2C%20thus%0Aimproving%20the%20fidelity%20and%20generalization%20capacity%20of%20our%20model%20for%20ID-specific%0Avideo%20generation.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%0AID-Animator%20to%20generate%20personalized%20human%20videos%20over%20previous%20models.%0AMoreover%2C%20our%20method%20is%20highly%20compatible%20with%20popular%20pre-trained%20T2V%20models%0Alike%20animatediff%20and%20various%20community%20backbone%20models%2C%20showing%20high%0Aextendability%20in%20real-world%20applications%20for%20video%20generation%20where%20identity%0Apreservation%20is%20highly%20desired.%20Our%20codes%20and%20checkpoints%20will%20be%20released%20at%0Ahttps%3A//github.com/ID-Animator/ID-Animator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15275v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ID-Animator%3A%20Zero-Shot%20Identity-Preserving%20Human%20Video%20Generation&entry.906535625=Xuanhua%20He%20and%20Quande%20Liu%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Tao%20Hu%20and%20Ke%20Cao%20and%20Keyu%20Yan%20and%20Man%20Zhou%20and%20Jie%20Zhang&entry.1292438233=%20%20Generating%20high%20fidelity%20human%20video%20with%20specified%20identities%20has%20attracted%0Asignificant%20attention%20in%20the%20content%20generation%20community.%20However%2C%20existing%0Atechniques%20struggle%20to%20strike%20a%20balance%20between%20training%20efficiency%20and%0Aidentity%20preservation%2C%20either%20requiring%20tedious%20case-by-case%20finetuning%20or%0Ausually%20missing%20the%20identity%20details%20in%20video%20generation%20process.%20In%20this%0Astudy%2C%20we%20present%20ID-Animator%2C%20a%20zero-shot%20human-video%20generation%20approach%20that%0Acan%20perform%20personalized%20video%20generation%20given%20single%20reference%20facial%20image%0Awithout%20further%20training.%20ID-Animator%20inherits%20existing%20diffusion-based%20video%0Ageneration%20backbones%20with%20a%20face%20adapter%20to%20encode%20the%20ID-relevant%20embeddings%0Afrom%20learnable%20facial%20latent%20queries.%20To%20facilitate%20the%20extraction%20of%20identity%0Ainformation%20in%20video%20generation%2C%20we%20introduce%20an%20ID-oriented%20dataset%0Aconstruction%20pipeline%2C%20which%20incorporates%20decoupled%20human%20attribute%20and%20action%0Acaptioning%20technique%20from%20a%20constructed%20facial%20image%20pool.%20Based%20on%20this%0Apipeline%2C%20a%20random%20face%20reference%20training%20method%20is%20further%20devised%20to%0Aprecisely%20capture%20the%20ID-relevant%20embeddings%20from%20reference%20images%2C%20thus%0Aimproving%20the%20fidelity%20and%20generalization%20capacity%20of%20our%20model%20for%20ID-specific%0Avideo%20generation.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%0AID-Animator%20to%20generate%20personalized%20human%20videos%20over%20previous%20models.%0AMoreover%2C%20our%20method%20is%20highly%20compatible%20with%20popular%20pre-trained%20T2V%20models%0Alike%20animatediff%20and%20various%20community%20backbone%20models%2C%20showing%20high%0Aextendability%20in%20real-world%20applications%20for%20video%20generation%20where%20identity%0Apreservation%20is%20highly%20desired.%20Our%20codes%20and%20checkpoints%20will%20be%20released%20at%0Ahttps%3A//github.com/ID-Animator/ID-Animator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15275v1&entry.124074799=Read"},
{"title": "Continual Learning with Pre-Trained Models: A Survey", "author": "Da-Wei Zhou and Hai-Long Sun and Jingyi Ning and Han-Jia Ye and De-Chuan Zhan", "abstract": "  Nowadays, real-world applications often face streaming data, which requires\nthe learning system to absorb new knowledge as data evolves. Continual Learning\n(CL) aims to achieve this goal and meanwhile overcome the catastrophic\nforgetting of former knowledge when learning new ones. Typical CL methods build\nthe model from scratch to grow with incoming data. However, the advent of the\npre-trained model (PTM) era has sparked immense research interest, particularly\nin leveraging PTMs' robust representational capabilities. This paper presents a\ncomprehensive survey of the latest advancements in PTM-based CL. We categorize\nexisting methodologies into three distinct groups, providing a comparative\nanalysis of their similarities, differences, and respective advantages and\ndisadvantages. Additionally, we offer an empirical study contrasting various\nstate-of-the-art methods to highlight concerns regarding fairness in\ncomparisons. The source code to reproduce these evaluations is available at:\nhttps://github.com/sun-hailong/LAMDA-PILOT\n", "link": "http://arxiv.org/abs/2401.16386v2", "date": "2024-04-23", "relevancy": 2.636, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5825}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5008}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4983}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%20with%20Pre-Trained%20Models%3A%20A%20Survey&body=Title%3A%20Continual%20Learning%20with%20Pre-Trained%20Models%3A%20A%20Survey%0AAuthor%3A%20Da-Wei%20Zhou%20and%20Hai-Long%20Sun%20and%20Jingyi%20Ning%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20Nowadays%2C%20real-world%20applications%20often%20face%20streaming%20data%2C%20which%20requires%0Athe%20learning%20system%20to%20absorb%20new%20knowledge%20as%20data%20evolves.%20Continual%20Learning%0A%28CL%29%20aims%20to%20achieve%20this%20goal%20and%20meanwhile%20overcome%20the%20catastrophic%0Aforgetting%20of%20former%20knowledge%20when%20learning%20new%20ones.%20Typical%20CL%20methods%20build%0Athe%20model%20from%20scratch%20to%20grow%20with%20incoming%20data.%20However%2C%20the%20advent%20of%20the%0Apre-trained%20model%20%28PTM%29%20era%20has%20sparked%20immense%20research%20interest%2C%20particularly%0Ain%20leveraging%20PTMs%27%20robust%20representational%20capabilities.%20This%20paper%20presents%20a%0Acomprehensive%20survey%20of%20the%20latest%20advancements%20in%20PTM-based%20CL.%20We%20categorize%0Aexisting%20methodologies%20into%20three%20distinct%20groups%2C%20providing%20a%20comparative%0Aanalysis%20of%20their%20similarities%2C%20differences%2C%20and%20respective%20advantages%20and%0Adisadvantages.%20Additionally%2C%20we%20offer%20an%20empirical%20study%20contrasting%20various%0Astate-of-the-art%20methods%20to%20highlight%20concerns%20regarding%20fairness%20in%0Acomparisons.%20The%20source%20code%20to%20reproduce%20these%20evaluations%20is%20available%20at%3A%0Ahttps%3A//github.com/sun-hailong/LAMDA-PILOT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16386v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%20with%20Pre-Trained%20Models%3A%20A%20Survey&entry.906535625=Da-Wei%20Zhou%20and%20Hai-Long%20Sun%20and%20Jingyi%20Ning%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20Nowadays%2C%20real-world%20applications%20often%20face%20streaming%20data%2C%20which%20requires%0Athe%20learning%20system%20to%20absorb%20new%20knowledge%20as%20data%20evolves.%20Continual%20Learning%0A%28CL%29%20aims%20to%20achieve%20this%20goal%20and%20meanwhile%20overcome%20the%20catastrophic%0Aforgetting%20of%20former%20knowledge%20when%20learning%20new%20ones.%20Typical%20CL%20methods%20build%0Athe%20model%20from%20scratch%20to%20grow%20with%20incoming%20data.%20However%2C%20the%20advent%20of%20the%0Apre-trained%20model%20%28PTM%29%20era%20has%20sparked%20immense%20research%20interest%2C%20particularly%0Ain%20leveraging%20PTMs%27%20robust%20representational%20capabilities.%20This%20paper%20presents%20a%0Acomprehensive%20survey%20of%20the%20latest%20advancements%20in%20PTM-based%20CL.%20We%20categorize%0Aexisting%20methodologies%20into%20three%20distinct%20groups%2C%20providing%20a%20comparative%0Aanalysis%20of%20their%20similarities%2C%20differences%2C%20and%20respective%20advantages%20and%0Adisadvantages.%20Additionally%2C%20we%20offer%20an%20empirical%20study%20contrasting%20various%0Astate-of-the-art%20methods%20to%20highlight%20concerns%20regarding%20fairness%20in%0Acomparisons.%20The%20source%20code%20to%20reproduce%20these%20evaluations%20is%20available%20at%3A%0Ahttps%3A//github.com/sun-hailong/LAMDA-PILOT%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16386v2&entry.124074799=Read"},
{"title": "Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph\n  Neural Networks", "author": "Zhe Zhao and Pengkun Wang and Xu Wang and Haibin Wen and Xiaolong Xie and Zhengyang Zhou and Qingfu Zhang and Yang Wang", "abstract": "  Pre-training GNNs to extract transferable knowledge and apply it to\ndownstream tasks has become the de facto standard of graph representation\nlearning. Recent works focused on designing self-supervised pre-training tasks\nto extract useful and universal transferable knowledge from large-scale\nunlabeled data. However, they have to face an inevitable question: traditional\npre-training strategies that aim at extracting useful information about\npre-training tasks, may not extract all useful information about the downstream\ntask. In this paper, we reexamine the pre-training process within traditional\npre-training and fine-tuning frameworks from the perspective of Information\nBottleneck (IB) and confirm that the forgetting phenomenon in pre-training\nphase may cause detrimental effects on downstream tasks. Therefore, we propose\na novel \\underline{D}elayed \\underline{B}ottlenecking \\underline{P}re-training\n(DBP) framework which maintains as much as possible mutual information between\nlatent representations and training data during pre-training phase by\nsuppressing the compression operation and delays the compression operation to\nfine-tuning phase to make sure the compression can be guided with labeled\nfine-tuning data and downstream tasks. To achieve this, we design two\ninformation control objectives that can be directly optimized and further\nintegrate them into the actual model design. Extensive experiments on both\nchemistry and biology domains demonstrate the effectiveness of DBP.\n", "link": "http://arxiv.org/abs/2404.14941v1", "date": "2024-04-23", "relevancy": 2.6238, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5318}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5302}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5123}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Delayed%20Bottlenecking%3A%20Alleviating%20Forgetting%20in%20Pre-trained%20Graph%0A%20%20Neural%20Networks&body=Title%3A%20Delayed%20Bottlenecking%3A%20Alleviating%20Forgetting%20in%20Pre-trained%20Graph%0A%20%20Neural%20Networks%0AAuthor%3A%20Zhe%20Zhao%20and%20Pengkun%20Wang%20and%20Xu%20Wang%20and%20Haibin%20Wen%20and%20Xiaolong%20Xie%20and%20Zhengyang%20Zhou%20and%20Qingfu%20Zhang%20and%20Yang%20Wang%0AAbstract%3A%20%20%20Pre-training%20GNNs%20to%20extract%20transferable%20knowledge%20and%20apply%20it%20to%0Adownstream%20tasks%20has%20become%20the%20de%20facto%20standard%20of%20graph%20representation%0Alearning.%20Recent%20works%20focused%20on%20designing%20self-supervised%20pre-training%20tasks%0Ato%20extract%20useful%20and%20universal%20transferable%20knowledge%20from%20large-scale%0Aunlabeled%20data.%20However%2C%20they%20have%20to%20face%20an%20inevitable%20question%3A%20traditional%0Apre-training%20strategies%20that%20aim%20at%20extracting%20useful%20information%20about%0Apre-training%20tasks%2C%20may%20not%20extract%20all%20useful%20information%20about%20the%20downstream%0Atask.%20In%20this%20paper%2C%20we%20reexamine%20the%20pre-training%20process%20within%20traditional%0Apre-training%20and%20fine-tuning%20frameworks%20from%20the%20perspective%20of%20Information%0ABottleneck%20%28IB%29%20and%20confirm%20that%20the%20forgetting%20phenomenon%20in%20pre-training%0Aphase%20may%20cause%20detrimental%20effects%20on%20downstream%20tasks.%20Therefore%2C%20we%20propose%0Aa%20novel%20%5Cunderline%7BD%7Delayed%20%5Cunderline%7BB%7Dottlenecking%20%5Cunderline%7BP%7Dre-training%0A%28DBP%29%20framework%20which%20maintains%20as%20much%20as%20possible%20mutual%20information%20between%0Alatent%20representations%20and%20training%20data%20during%20pre-training%20phase%20by%0Asuppressing%20the%20compression%20operation%20and%20delays%20the%20compression%20operation%20to%0Afine-tuning%20phase%20to%20make%20sure%20the%20compression%20can%20be%20guided%20with%20labeled%0Afine-tuning%20data%20and%20downstream%20tasks.%20To%20achieve%20this%2C%20we%20design%20two%0Ainformation%20control%20objectives%20that%20can%20be%20directly%20optimized%20and%20further%0Aintegrate%20them%20into%20the%20actual%20model%20design.%20Extensive%20experiments%20on%20both%0Achemistry%20and%20biology%20domains%20demonstrate%20the%20effectiveness%20of%20DBP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14941v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delayed%20Bottlenecking%3A%20Alleviating%20Forgetting%20in%20Pre-trained%20Graph%0A%20%20Neural%20Networks&entry.906535625=Zhe%20Zhao%20and%20Pengkun%20Wang%20and%20Xu%20Wang%20and%20Haibin%20Wen%20and%20Xiaolong%20Xie%20and%20Zhengyang%20Zhou%20and%20Qingfu%20Zhang%20and%20Yang%20Wang&entry.1292438233=%20%20Pre-training%20GNNs%20to%20extract%20transferable%20knowledge%20and%20apply%20it%20to%0Adownstream%20tasks%20has%20become%20the%20de%20facto%20standard%20of%20graph%20representation%0Alearning.%20Recent%20works%20focused%20on%20designing%20self-supervised%20pre-training%20tasks%0Ato%20extract%20useful%20and%20universal%20transferable%20knowledge%20from%20large-scale%0Aunlabeled%20data.%20However%2C%20they%20have%20to%20face%20an%20inevitable%20question%3A%20traditional%0Apre-training%20strategies%20that%20aim%20at%20extracting%20useful%20information%20about%0Apre-training%20tasks%2C%20may%20not%20extract%20all%20useful%20information%20about%20the%20downstream%0Atask.%20In%20this%20paper%2C%20we%20reexamine%20the%20pre-training%20process%20within%20traditional%0Apre-training%20and%20fine-tuning%20frameworks%20from%20the%20perspective%20of%20Information%0ABottleneck%20%28IB%29%20and%20confirm%20that%20the%20forgetting%20phenomenon%20in%20pre-training%0Aphase%20may%20cause%20detrimental%20effects%20on%20downstream%20tasks.%20Therefore%2C%20we%20propose%0Aa%20novel%20%5Cunderline%7BD%7Delayed%20%5Cunderline%7BB%7Dottlenecking%20%5Cunderline%7BP%7Dre-training%0A%28DBP%29%20framework%20which%20maintains%20as%20much%20as%20possible%20mutual%20information%20between%0Alatent%20representations%20and%20training%20data%20during%20pre-training%20phase%20by%0Asuppressing%20the%20compression%20operation%20and%20delays%20the%20compression%20operation%20to%0Afine-tuning%20phase%20to%20make%20sure%20the%20compression%20can%20be%20guided%20with%20labeled%0Afine-tuning%20data%20and%20downstream%20tasks.%20To%20achieve%20this%2C%20we%20design%20two%0Ainformation%20control%20objectives%20that%20can%20be%20directly%20optimized%20and%20further%0Aintegrate%20them%20into%20the%20actual%20model%20design.%20Extensive%20experiments%20on%20both%0Achemistry%20and%20biology%20domains%20demonstrate%20the%20effectiveness%20of%20DBP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14941v1&entry.124074799=Read"},
{"title": "CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with\n  Complex Trajectory", "author": "Yunlong Ran and Yanxu Li and Qi Ye and Yuchi Huo and Zechun Bai and Jiahao Sun and Jiming Chen", "abstract": "  Neural radiance field (NeRF) has achieved impressive results in high-quality\n3D scene reconstruction. However, NeRF heavily relies on precise camera poses.\nWhile recent works like BARF have introduced camera pose optimization within\nNeRF, their applicability is limited to simple trajectory scenes. Existing\nmethods struggle while tackling complex trajectories involving large rotations.\nTo address this limitation, we propose CT-NeRF, an incremental reconstruction\noptimization pipeline using only RGB images without pose and depth input. In\nthis pipeline, we first propose a local-global bundle adjustment under a pose\ngraph connecting neighboring frames to enforce the consistency between poses to\nescape the local minima caused by only pose consistency with the scene\nstructure. Further, we instantiate the consistency between poses as a\nreprojected geometric image distance constraint resulting from pixel-level\ncorrespondences between input image pairs. Through the incremental\nreconstruction, CT-NeRF enables the recovery of both camera poses and scene\nstructure and is capable of handling scenes with complex trajectories. We\nevaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and\nFree-Dataset, which feature complex trajectories. Results show CT-NeRF\noutperforms existing methods in novel view synthesis and pose estimation\naccuracy.\n", "link": "http://arxiv.org/abs/2404.13896v2", "date": "2024-04-23", "relevancy": 2.6132, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5439}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5196}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5044}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CT-NeRF%3A%20Incremental%20Optimizing%20Neural%20Radiance%20Field%20and%20Poses%20with%0A%20%20Complex%20Trajectory&body=Title%3A%20CT-NeRF%3A%20Incremental%20Optimizing%20Neural%20Radiance%20Field%20and%20Poses%20with%0A%20%20Complex%20Trajectory%0AAuthor%3A%20Yunlong%20Ran%20and%20Yanxu%20Li%20and%20Qi%20Ye%20and%20Yuchi%20Huo%20and%20Zechun%20Bai%20and%20Jiahao%20Sun%20and%20Jiming%20Chen%0AAbstract%3A%20%20%20Neural%20radiance%20field%20%28NeRF%29%20has%20achieved%20impressive%20results%20in%20high-quality%0A3D%20scene%20reconstruction.%20However%2C%20NeRF%20heavily%20relies%20on%20precise%20camera%20poses.%0AWhile%20recent%20works%20like%20BARF%20have%20introduced%20camera%20pose%20optimization%20within%0ANeRF%2C%20their%20applicability%20is%20limited%20to%20simple%20trajectory%20scenes.%20Existing%0Amethods%20struggle%20while%20tackling%20complex%20trajectories%20involving%20large%20rotations.%0ATo%20address%20this%20limitation%2C%20we%20propose%20CT-NeRF%2C%20an%20incremental%20reconstruction%0Aoptimization%20pipeline%20using%20only%20RGB%20images%20without%20pose%20and%20depth%20input.%20In%0Athis%20pipeline%2C%20we%20first%20propose%20a%20local-global%20bundle%20adjustment%20under%20a%20pose%0Agraph%20connecting%20neighboring%20frames%20to%20enforce%20the%20consistency%20between%20poses%20to%0Aescape%20the%20local%20minima%20caused%20by%20only%20pose%20consistency%20with%20the%20scene%0Astructure.%20Further%2C%20we%20instantiate%20the%20consistency%20between%20poses%20as%20a%0Areprojected%20geometric%20image%20distance%20constraint%20resulting%20from%20pixel-level%0Acorrespondences%20between%20input%20image%20pairs.%20Through%20the%20incremental%0Areconstruction%2C%20CT-NeRF%20enables%20the%20recovery%20of%20both%20camera%20poses%20and%20scene%0Astructure%20and%20is%20capable%20of%20handling%20scenes%20with%20complex%20trajectories.%20We%0Aevaluate%20the%20performance%20of%20CT-NeRF%20on%20two%20real-world%20datasets%2C%20NeRFBuster%20and%0AFree-Dataset%2C%20which%20feature%20complex%20trajectories.%20Results%20show%20CT-NeRF%0Aoutperforms%20existing%20methods%20in%20novel%20view%20synthesis%20and%20pose%20estimation%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13896v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-NeRF%3A%20Incremental%20Optimizing%20Neural%20Radiance%20Field%20and%20Poses%20with%0A%20%20Complex%20Trajectory&entry.906535625=Yunlong%20Ran%20and%20Yanxu%20Li%20and%20Qi%20Ye%20and%20Yuchi%20Huo%20and%20Zechun%20Bai%20and%20Jiahao%20Sun%20and%20Jiming%20Chen&entry.1292438233=%20%20Neural%20radiance%20field%20%28NeRF%29%20has%20achieved%20impressive%20results%20in%20high-quality%0A3D%20scene%20reconstruction.%20However%2C%20NeRF%20heavily%20relies%20on%20precise%20camera%20poses.%0AWhile%20recent%20works%20like%20BARF%20have%20introduced%20camera%20pose%20optimization%20within%0ANeRF%2C%20their%20applicability%20is%20limited%20to%20simple%20trajectory%20scenes.%20Existing%0Amethods%20struggle%20while%20tackling%20complex%20trajectories%20involving%20large%20rotations.%0ATo%20address%20this%20limitation%2C%20we%20propose%20CT-NeRF%2C%20an%20incremental%20reconstruction%0Aoptimization%20pipeline%20using%20only%20RGB%20images%20without%20pose%20and%20depth%20input.%20In%0Athis%20pipeline%2C%20we%20first%20propose%20a%20local-global%20bundle%20adjustment%20under%20a%20pose%0Agraph%20connecting%20neighboring%20frames%20to%20enforce%20the%20consistency%20between%20poses%20to%0Aescape%20the%20local%20minima%20caused%20by%20only%20pose%20consistency%20with%20the%20scene%0Astructure.%20Further%2C%20we%20instantiate%20the%20consistency%20between%20poses%20as%20a%0Areprojected%20geometric%20image%20distance%20constraint%20resulting%20from%20pixel-level%0Acorrespondences%20between%20input%20image%20pairs.%20Through%20the%20incremental%0Areconstruction%2C%20CT-NeRF%20enables%20the%20recovery%20of%20both%20camera%20poses%20and%20scene%0Astructure%20and%20is%20capable%20of%20handling%20scenes%20with%20complex%20trajectories.%20We%0Aevaluate%20the%20performance%20of%20CT-NeRF%20on%20two%20real-world%20datasets%2C%20NeRFBuster%20and%0AFree-Dataset%2C%20which%20feature%20complex%20trajectories.%20Results%20show%20CT-NeRF%0Aoutperforms%20existing%20methods%20in%20novel%20view%20synthesis%20and%20pose%20estimation%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13896v2&entry.124074799=Read"},
{"title": "Attention-Map Augmentation for Hypercomplex Breast Cancer Classification", "author": "Eleonora Lopez and Filippo Betello and Federico Carmignani and Eleonora Grassucci and Danilo Comminiello", "abstract": "  Breast cancer is the most widespread neoplasm among women and early detection\nof this disease is critical. Deep learning techniques have become of great\ninterest to improve diagnostic performance. However, distinguishing between\nmalignant and benign masses in whole mammograms poses a challenge, as they\nappear nearly identical to an untrained eye, and the region of interest (ROI)\nconstitutes only a small fraction of the entire image. In this paper, we\npropose a framework, parameterized hypercomplex attention maps (PHAM), to\novercome these problems. Specifically, we deploy an augmentation step based on\ncomputing attention maps. Then, the attention maps are used to condition the\nclassification step by constructing a multi-dimensional input comprised of the\noriginal breast cancer image and the corresponding attention map. In this step,\na parameterized hypercomplex neural network (PHNN) is employed to perform\nbreast cancer classification. The framework offers two main advantages. First,\nattention maps provide critical information regarding the ROI and allow the\nneural model to concentrate on it. Second, the hypercomplex architecture has\nthe ability to model local relations between input dimensions thanks to\nhypercomplex algebra rules, thus properly exploiting the information provided\nby the attention map. We demonstrate the efficacy of the proposed framework on\nboth mammography images as well as histopathological ones. We surpass\nattention-based state-of-the-art networks and the real-valued counterpart of\nour approach. The code of our work is available at\nhttps://github.com/ispamm/AttentionBCS.\n", "link": "http://arxiv.org/abs/2310.07633v2", "date": "2024-04-23", "relevancy": 2.5619, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4991}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Attention-Map%20Augmentation%20for%20Hypercomplex%20Breast%20Cancer%20Classification&body=Title%3A%20Attention-Map%20Augmentation%20for%20Hypercomplex%20Breast%20Cancer%20Classification%0AAuthor%3A%20Eleonora%20Lopez%20and%20Filippo%20Betello%20and%20Federico%20Carmignani%20and%20Eleonora%20Grassucci%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20Breast%20cancer%20is%20the%20most%20widespread%20neoplasm%20among%20women%20and%20early%20detection%0Aof%20this%20disease%20is%20critical.%20Deep%20learning%20techniques%20have%20become%20of%20great%0Ainterest%20to%20improve%20diagnostic%20performance.%20However%2C%20distinguishing%20between%0Amalignant%20and%20benign%20masses%20in%20whole%20mammograms%20poses%20a%20challenge%2C%20as%20they%0Aappear%20nearly%20identical%20to%20an%20untrained%20eye%2C%20and%20the%20region%20of%20interest%20%28ROI%29%0Aconstitutes%20only%20a%20small%20fraction%20of%20the%20entire%20image.%20In%20this%20paper%2C%20we%0Apropose%20a%20framework%2C%20parameterized%20hypercomplex%20attention%20maps%20%28PHAM%29%2C%20to%0Aovercome%20these%20problems.%20Specifically%2C%20we%20deploy%20an%20augmentation%20step%20based%20on%0Acomputing%20attention%20maps.%20Then%2C%20the%20attention%20maps%20are%20used%20to%20condition%20the%0Aclassification%20step%20by%20constructing%20a%20multi-dimensional%20input%20comprised%20of%20the%0Aoriginal%20breast%20cancer%20image%20and%20the%20corresponding%20attention%20map.%20In%20this%20step%2C%0Aa%20parameterized%20hypercomplex%20neural%20network%20%28PHNN%29%20is%20employed%20to%20perform%0Abreast%20cancer%20classification.%20The%20framework%20offers%20two%20main%20advantages.%20First%2C%0Aattention%20maps%20provide%20critical%20information%20regarding%20the%20ROI%20and%20allow%20the%0Aneural%20model%20to%20concentrate%20on%20it.%20Second%2C%20the%20hypercomplex%20architecture%20has%0Athe%20ability%20to%20model%20local%20relations%20between%20input%20dimensions%20thanks%20to%0Ahypercomplex%20algebra%20rules%2C%20thus%20properly%20exploiting%20the%20information%20provided%0Aby%20the%20attention%20map.%20We%20demonstrate%20the%20efficacy%20of%20the%20proposed%20framework%20on%0Aboth%20mammography%20images%20as%20well%20as%20histopathological%20ones.%20We%20surpass%0Aattention-based%20state-of-the-art%20networks%20and%20the%20real-valued%20counterpart%20of%0Aour%20approach.%20The%20code%20of%20our%20work%20is%20available%20at%0Ahttps%3A//github.com/ispamm/AttentionBCS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07633v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-Map%20Augmentation%20for%20Hypercomplex%20Breast%20Cancer%20Classification&entry.906535625=Eleonora%20Lopez%20and%20Filippo%20Betello%20and%20Federico%20Carmignani%20and%20Eleonora%20Grassucci%20and%20Danilo%20Comminiello&entry.1292438233=%20%20Breast%20cancer%20is%20the%20most%20widespread%20neoplasm%20among%20women%20and%20early%20detection%0Aof%20this%20disease%20is%20critical.%20Deep%20learning%20techniques%20have%20become%20of%20great%0Ainterest%20to%20improve%20diagnostic%20performance.%20However%2C%20distinguishing%20between%0Amalignant%20and%20benign%20masses%20in%20whole%20mammograms%20poses%20a%20challenge%2C%20as%20they%0Aappear%20nearly%20identical%20to%20an%20untrained%20eye%2C%20and%20the%20region%20of%20interest%20%28ROI%29%0Aconstitutes%20only%20a%20small%20fraction%20of%20the%20entire%20image.%20In%20this%20paper%2C%20we%0Apropose%20a%20framework%2C%20parameterized%20hypercomplex%20attention%20maps%20%28PHAM%29%2C%20to%0Aovercome%20these%20problems.%20Specifically%2C%20we%20deploy%20an%20augmentation%20step%20based%20on%0Acomputing%20attention%20maps.%20Then%2C%20the%20attention%20maps%20are%20used%20to%20condition%20the%0Aclassification%20step%20by%20constructing%20a%20multi-dimensional%20input%20comprised%20of%20the%0Aoriginal%20breast%20cancer%20image%20and%20the%20corresponding%20attention%20map.%20In%20this%20step%2C%0Aa%20parameterized%20hypercomplex%20neural%20network%20%28PHNN%29%20is%20employed%20to%20perform%0Abreast%20cancer%20classification.%20The%20framework%20offers%20two%20main%20advantages.%20First%2C%0Aattention%20maps%20provide%20critical%20information%20regarding%20the%20ROI%20and%20allow%20the%0Aneural%20model%20to%20concentrate%20on%20it.%20Second%2C%20the%20hypercomplex%20architecture%20has%0Athe%20ability%20to%20model%20local%20relations%20between%20input%20dimensions%20thanks%20to%0Ahypercomplex%20algebra%20rules%2C%20thus%20properly%20exploiting%20the%20information%20provided%0Aby%20the%20attention%20map.%20We%20demonstrate%20the%20efficacy%20of%20the%20proposed%20framework%20on%0Aboth%20mammography%20images%20as%20well%20as%20histopathological%20ones.%20We%20surpass%0Aattention-based%20state-of-the-art%20networks%20and%20the%20real-valued%20counterpart%20of%0Aour%20approach.%20The%20code%20of%20our%20work%20is%20available%20at%0Ahttps%3A//github.com/ispamm/AttentionBCS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07633v2&entry.124074799=Read"},
{"title": "Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance", "author": "Kuan-Chih Huang and Yi-Hsuan Tsai and Ming-Hsuan Yang", "abstract": "  Weakly supervised 3D object detection aims to learn a 3D detector with lower\nannotation cost, e.g., 2D labels. Unlike prior work which still relies on few\naccurate 3D annotations, we propose a framework to study how to leverage\nconstraints between 2D and 3D domains without requiring any 3D labels.\nSpecifically, we employ visual data from three perspectives to establish\nconnections between 2D and 3D domains. First, we design a feature-level\nconstraint to align LiDAR and image features based on object-aware regions.\nSecond, the output-level constraint is developed to enforce the overlap between\n2D and projected 3D box estimations. Finally, the training-level constraint is\nutilized by producing accurate and consistent 3D pseudo-labels that align with\nthe visual data. We conduct extensive experiments on the KITTI dataset to\nvalidate the effectiveness of the proposed three constraints. Without using any\n3D labels, our method achieves favorable performance against state-of-the-art\napproaches and is competitive with the method that uses 500-frame 3D\nannotations. Code and models will be made publicly available at\nhttps://github.com/kuanchihhuang/VG-W3D.\n", "link": "http://arxiv.org/abs/2312.07530v2", "date": "2024-04-23", "relevancy": 2.5477, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6554}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6356}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.619}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Weakly%20Supervised%203D%20Object%20Detection%20via%20Multi-Level%20Visual%20Guidance&body=Title%3A%20Weakly%20Supervised%203D%20Object%20Detection%20via%20Multi-Level%20Visual%20Guidance%0AAuthor%3A%20Kuan-Chih%20Huang%20and%20Yi-Hsuan%20Tsai%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Weakly%20supervised%203D%20object%20detection%20aims%20to%20learn%20a%203D%20detector%20with%20lower%0Aannotation%20cost%2C%20e.g.%2C%202D%20labels.%20Unlike%20prior%20work%20which%20still%20relies%20on%20few%0Aaccurate%203D%20annotations%2C%20we%20propose%20a%20framework%20to%20study%20how%20to%20leverage%0Aconstraints%20between%202D%20and%203D%20domains%20without%20requiring%20any%203D%20labels.%0ASpecifically%2C%20we%20employ%20visual%20data%20from%20three%20perspectives%20to%20establish%0Aconnections%20between%202D%20and%203D%20domains.%20First%2C%20we%20design%20a%20feature-level%0Aconstraint%20to%20align%20LiDAR%20and%20image%20features%20based%20on%20object-aware%20regions.%0ASecond%2C%20the%20output-level%20constraint%20is%20developed%20to%20enforce%20the%20overlap%20between%0A2D%20and%20projected%203D%20box%20estimations.%20Finally%2C%20the%20training-level%20constraint%20is%0Autilized%20by%20producing%20accurate%20and%20consistent%203D%20pseudo-labels%20that%20align%20with%0Athe%20visual%20data.%20We%20conduct%20extensive%20experiments%20on%20the%20KITTI%20dataset%20to%0Avalidate%20the%20effectiveness%20of%20the%20proposed%20three%20constraints.%20Without%20using%20any%0A3D%20labels%2C%20our%20method%20achieves%20favorable%20performance%20against%20state-of-the-art%0Aapproaches%20and%20is%20competitive%20with%20the%20method%20that%20uses%20500-frame%203D%0Aannotations.%20Code%20and%20models%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/kuanchihhuang/VG-W3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07530v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20Supervised%203D%20Object%20Detection%20via%20Multi-Level%20Visual%20Guidance&entry.906535625=Kuan-Chih%20Huang%20and%20Yi-Hsuan%20Tsai%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Weakly%20supervised%203D%20object%20detection%20aims%20to%20learn%20a%203D%20detector%20with%20lower%0Aannotation%20cost%2C%20e.g.%2C%202D%20labels.%20Unlike%20prior%20work%20which%20still%20relies%20on%20few%0Aaccurate%203D%20annotations%2C%20we%20propose%20a%20framework%20to%20study%20how%20to%20leverage%0Aconstraints%20between%202D%20and%203D%20domains%20without%20requiring%20any%203D%20labels.%0ASpecifically%2C%20we%20employ%20visual%20data%20from%20three%20perspectives%20to%20establish%0Aconnections%20between%202D%20and%203D%20domains.%20First%2C%20we%20design%20a%20feature-level%0Aconstraint%20to%20align%20LiDAR%20and%20image%20features%20based%20on%20object-aware%20regions.%0ASecond%2C%20the%20output-level%20constraint%20is%20developed%20to%20enforce%20the%20overlap%20between%0A2D%20and%20projected%203D%20box%20estimations.%20Finally%2C%20the%20training-level%20constraint%20is%0Autilized%20by%20producing%20accurate%20and%20consistent%203D%20pseudo-labels%20that%20align%20with%0Athe%20visual%20data.%20We%20conduct%20extensive%20experiments%20on%20the%20KITTI%20dataset%20to%0Avalidate%20the%20effectiveness%20of%20the%20proposed%20three%20constraints.%20Without%20using%20any%0A3D%20labels%2C%20our%20method%20achieves%20favorable%20performance%20against%20state-of-the-art%0Aapproaches%20and%20is%20competitive%20with%20the%20method%20that%20uses%20500-frame%203D%0Aannotations.%20Code%20and%20models%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/kuanchihhuang/VG-W3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07530v2&entry.124074799=Read"},
{"title": "Estimation Network Design framework for efficient distributed\n  optimization", "author": "Mattia Bianchi and Sergio Grammatico", "abstract": "  Distributed decision problems features a group of agents that can only\ncommunicate over a peer-to-peer network, without a central memory. In\napplications such as network control and data ranking, each agent is only\naffected by a small portion of the decision vector: this sparsity is typically\nignored in distributed algorithms, while it could be leveraged to improve\nefficiency and scalability. To address this issue, our recent paper introduces\nEstimation Network Design (END), a graph theoretical language for the analysis\nand design of distributed iterations. END algorithms can be tuned to exploit\nthe sparsity of specific problem instances, reducing communication overhead and\nminimizing redundancy, yet without requiring case-by-case convergence analysis.\nIn this paper, we showcase the flexility of END in the context of distributed\noptimization. In particular, we study the sparsity-aware version of many\nestablished methods, including ADMM, AugDGM and Push-Sum DGD. Simulations on an\nestimation problem in sensor networks demonstrate that END algorithms can boost\nconvergence speed and greatly reduce the communication and memory cost.\n", "link": "http://arxiv.org/abs/2404.15273v1", "date": "2024-04-23", "relevancy": 2.5175, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5312}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4991}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4802}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Estimation%20Network%20Design%20framework%20for%20efficient%20distributed%0A%20%20optimization&body=Title%3A%20Estimation%20Network%20Design%20framework%20for%20efficient%20distributed%0A%20%20optimization%0AAuthor%3A%20Mattia%20Bianchi%20and%20Sergio%20Grammatico%0AAbstract%3A%20%20%20Distributed%20decision%20problems%20features%20a%20group%20of%20agents%20that%20can%20only%0Acommunicate%20over%20a%20peer-to-peer%20network%2C%20without%20a%20central%20memory.%20In%0Aapplications%20such%20as%20network%20control%20and%20data%20ranking%2C%20each%20agent%20is%20only%0Aaffected%20by%20a%20small%20portion%20of%20the%20decision%20vector%3A%20this%20sparsity%20is%20typically%0Aignored%20in%20distributed%20algorithms%2C%20while%20it%20could%20be%20leveraged%20to%20improve%0Aefficiency%20and%20scalability.%20To%20address%20this%20issue%2C%20our%20recent%20paper%20introduces%0AEstimation%20Network%20Design%20%28END%29%2C%20a%20graph%20theoretical%20language%20for%20the%20analysis%0Aand%20design%20of%20distributed%20iterations.%20END%20algorithms%20can%20be%20tuned%20to%20exploit%0Athe%20sparsity%20of%20specific%20problem%20instances%2C%20reducing%20communication%20overhead%20and%0Aminimizing%20redundancy%2C%20yet%20without%20requiring%20case-by-case%20convergence%20analysis.%0AIn%20this%20paper%2C%20we%20showcase%20the%20flexility%20of%20END%20in%20the%20context%20of%20distributed%0Aoptimization.%20In%20particular%2C%20we%20study%20the%20sparsity-aware%20version%20of%20many%0Aestablished%20methods%2C%20including%20ADMM%2C%20AugDGM%20and%20Push-Sum%20DGD.%20Simulations%20on%20an%0Aestimation%20problem%20in%20sensor%20networks%20demonstrate%20that%20END%20algorithms%20can%20boost%0Aconvergence%20speed%20and%20greatly%20reduce%20the%20communication%20and%20memory%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15273v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimation%20Network%20Design%20framework%20for%20efficient%20distributed%0A%20%20optimization&entry.906535625=Mattia%20Bianchi%20and%20Sergio%20Grammatico&entry.1292438233=%20%20Distributed%20decision%20problems%20features%20a%20group%20of%20agents%20that%20can%20only%0Acommunicate%20over%20a%20peer-to-peer%20network%2C%20without%20a%20central%20memory.%20In%0Aapplications%20such%20as%20network%20control%20and%20data%20ranking%2C%20each%20agent%20is%20only%0Aaffected%20by%20a%20small%20portion%20of%20the%20decision%20vector%3A%20this%20sparsity%20is%20typically%0Aignored%20in%20distributed%20algorithms%2C%20while%20it%20could%20be%20leveraged%20to%20improve%0Aefficiency%20and%20scalability.%20To%20address%20this%20issue%2C%20our%20recent%20paper%20introduces%0AEstimation%20Network%20Design%20%28END%29%2C%20a%20graph%20theoretical%20language%20for%20the%20analysis%0Aand%20design%20of%20distributed%20iterations.%20END%20algorithms%20can%20be%20tuned%20to%20exploit%0Athe%20sparsity%20of%20specific%20problem%20instances%2C%20reducing%20communication%20overhead%20and%0Aminimizing%20redundancy%2C%20yet%20without%20requiring%20case-by-case%20convergence%20analysis.%0AIn%20this%20paper%2C%20we%20showcase%20the%20flexility%20of%20END%20in%20the%20context%20of%20distributed%0Aoptimization.%20In%20particular%2C%20we%20study%20the%20sparsity-aware%20version%20of%20many%0Aestablished%20methods%2C%20including%20ADMM%2C%20AugDGM%20and%20Push-Sum%20DGD.%20Simulations%20on%20an%0Aestimation%20problem%20in%20sensor%20networks%20demonstrate%20that%20END%20algorithms%20can%20boost%0Aconvergence%20speed%20and%20greatly%20reduce%20the%20communication%20and%20memory%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15273v1&entry.124074799=Read"},
{"title": "Additive Margin in Contrastive Self-Supervised Frameworks to Learn\n  Discriminative Speaker Representations", "author": "Theo Lepage and Reda Dehak", "abstract": "  Self-Supervised Learning (SSL) frameworks became the standard for learning\nrobust class representations by benefiting from large unlabeled datasets. For\nSpeaker Verification (SV), most SSL systems rely on contrastive-based loss\nfunctions. We explore different ways to improve the performance of these\ntechniques by revisiting the NT-Xent contrastive loss. Our main contribution is\nthe definition of the NT-Xent-AM loss and the study of the importance of\nAdditive Margin (AM) in SimCLR and MoCo SSL methods to further separate\npositive from negative pairs. Despite class collisions, we show that AM\nenhances the compactness of same-speaker embeddings and reduces the number of\nfalse negatives and false positives on SV. Additionally, we demonstrate the\neffectiveness of the symmetric contrastive loss, which provides more\nsupervision for the SSL task. Implementing these two modifications to SimCLR\nimproves performance and results in 7.85% EER on VoxCeleb1-O, outperforming\nother equivalent methods.\n", "link": "http://arxiv.org/abs/2404.14913v1", "date": "2024-04-23", "relevancy": 2.5102, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4934}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.478}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Additive%20Margin%20in%20Contrastive%20Self-Supervised%20Frameworks%20to%20Learn%0A%20%20Discriminative%20Speaker%20Representations&body=Title%3A%20Additive%20Margin%20in%20Contrastive%20Self-Supervised%20Frameworks%20to%20Learn%0A%20%20Discriminative%20Speaker%20Representations%0AAuthor%3A%20Theo%20Lepage%20and%20Reda%20Dehak%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20frameworks%20became%20the%20standard%20for%20learning%0Arobust%20class%20representations%20by%20benefiting%20from%20large%20unlabeled%20datasets.%20For%0ASpeaker%20Verification%20%28SV%29%2C%20most%20SSL%20systems%20rely%20on%20contrastive-based%20loss%0Afunctions.%20We%20explore%20different%20ways%20to%20improve%20the%20performance%20of%20these%0Atechniques%20by%20revisiting%20the%20NT-Xent%20contrastive%20loss.%20Our%20main%20contribution%20is%0Athe%20definition%20of%20the%20NT-Xent-AM%20loss%20and%20the%20study%20of%20the%20importance%20of%0AAdditive%20Margin%20%28AM%29%20in%20SimCLR%20and%20MoCo%20SSL%20methods%20to%20further%20separate%0Apositive%20from%20negative%20pairs.%20Despite%20class%20collisions%2C%20we%20show%20that%20AM%0Aenhances%20the%20compactness%20of%20same-speaker%20embeddings%20and%20reduces%20the%20number%20of%0Afalse%20negatives%20and%20false%20positives%20on%20SV.%20Additionally%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20the%20symmetric%20contrastive%20loss%2C%20which%20provides%20more%0Asupervision%20for%20the%20SSL%20task.%20Implementing%20these%20two%20modifications%20to%20SimCLR%0Aimproves%20performance%20and%20results%20in%207.85%25%20EER%20on%20VoxCeleb1-O%2C%20outperforming%0Aother%20equivalent%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14913v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Additive%20Margin%20in%20Contrastive%20Self-Supervised%20Frameworks%20to%20Learn%0A%20%20Discriminative%20Speaker%20Representations&entry.906535625=Theo%20Lepage%20and%20Reda%20Dehak&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20frameworks%20became%20the%20standard%20for%20learning%0Arobust%20class%20representations%20by%20benefiting%20from%20large%20unlabeled%20datasets.%20For%0ASpeaker%20Verification%20%28SV%29%2C%20most%20SSL%20systems%20rely%20on%20contrastive-based%20loss%0Afunctions.%20We%20explore%20different%20ways%20to%20improve%20the%20performance%20of%20these%0Atechniques%20by%20revisiting%20the%20NT-Xent%20contrastive%20loss.%20Our%20main%20contribution%20is%0Athe%20definition%20of%20the%20NT-Xent-AM%20loss%20and%20the%20study%20of%20the%20importance%20of%0AAdditive%20Margin%20%28AM%29%20in%20SimCLR%20and%20MoCo%20SSL%20methods%20to%20further%20separate%0Apositive%20from%20negative%20pairs.%20Despite%20class%20collisions%2C%20we%20show%20that%20AM%0Aenhances%20the%20compactness%20of%20same-speaker%20embeddings%20and%20reduces%20the%20number%20of%0Afalse%20negatives%20and%20false%20positives%20on%20SV.%20Additionally%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20the%20symmetric%20contrastive%20loss%2C%20which%20provides%20more%0Asupervision%20for%20the%20SSL%20task.%20Implementing%20these%20two%20modifications%20to%20SimCLR%0Aimproves%20performance%20and%20results%20in%207.85%25%20EER%20on%20VoxCeleb1-O%2C%20outperforming%0Aother%20equivalent%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14913v1&entry.124074799=Read"},
{"title": "TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and\n  Proprioception Estimation", "author": "Junli Ren and Yikai Liu and Yingru Dai and Guijin Wang", "abstract": "  Legged navigation is typically examined within open-world, off-road, and\nchallenging environments. In these scenarios, estimating external disturbances\nrequires a complex synthesis of multi-modal information. This underlines a\nmajor limitation in existing works that primarily focus on avoiding obstacles.\nIn this work, we propose TOP-Nav, a novel legged navigation framework that\nintegrates a comprehensive path planner with Terrain awareness, Obstacle\navoidance and close-loop Proprioception. TOP-Nav underscores the synergies\nbetween vision and proprioception in both path and motion planning. Within the\npath planner, we present and integrate a terrain estimator that enables the\nrobot to select waypoints on terrains with higher traversability while\neffectively avoiding obstacles. In the motion planning level, we not only\nimplement a locomotion controller to track the navigation commands, but also\nconstruct a proprioception advisor to provide motion evaluations for the path\nplanner. Based on the close-loop motion feedback, we make online corrections\nfor the vision-based terrain and obstacle estimations. Consequently, TOP-Nav\nachieves open-world navigation that the robot can handle terrains or\ndisturbances beyond the distribution of prior knowledge and overcomes\nconstraints imposed by visual conditions. Building upon extensive experiments\nconducted in both simulation and real-world environments, TOP-Nav demonstrates\nsuperior performance in open-world navigation compared to existing methods.\n", "link": "http://arxiv.org/abs/2404.15256v1", "date": "2024-04-23", "relevancy": 2.4861, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6341}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6247}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6077}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TOP-Nav%3A%20Legged%20Navigation%20Integrating%20Terrain%2C%20Obstacle%20and%0A%20%20Proprioception%20Estimation&body=Title%3A%20TOP-Nav%3A%20Legged%20Navigation%20Integrating%20Terrain%2C%20Obstacle%20and%0A%20%20Proprioception%20Estimation%0AAuthor%3A%20Junli%20Ren%20and%20Yikai%20Liu%20and%20Yingru%20Dai%20and%20Guijin%20Wang%0AAbstract%3A%20%20%20Legged%20navigation%20is%20typically%20examined%20within%20open-world%2C%20off-road%2C%20and%0Achallenging%20environments.%20In%20these%20scenarios%2C%20estimating%20external%20disturbances%0Arequires%20a%20complex%20synthesis%20of%20multi-modal%20information.%20This%20underlines%20a%0Amajor%20limitation%20in%20existing%20works%20that%20primarily%20focus%20on%20avoiding%20obstacles.%0AIn%20this%20work%2C%20we%20propose%20TOP-Nav%2C%20a%20novel%20legged%20navigation%20framework%20that%0Aintegrates%20a%20comprehensive%20path%20planner%20with%20Terrain%20awareness%2C%20Obstacle%0Aavoidance%20and%20close-loop%20Proprioception.%20TOP-Nav%20underscores%20the%20synergies%0Abetween%20vision%20and%20proprioception%20in%20both%20path%20and%20motion%20planning.%20Within%20the%0Apath%20planner%2C%20we%20present%20and%20integrate%20a%20terrain%20estimator%20that%20enables%20the%0Arobot%20to%20select%20waypoints%20on%20terrains%20with%20higher%20traversability%20while%0Aeffectively%20avoiding%20obstacles.%20In%20the%20motion%20planning%20level%2C%20we%20not%20only%0Aimplement%20a%20locomotion%20controller%20to%20track%20the%20navigation%20commands%2C%20but%20also%0Aconstruct%20a%20proprioception%20advisor%20to%20provide%20motion%20evaluations%20for%20the%20path%0Aplanner.%20Based%20on%20the%20close-loop%20motion%20feedback%2C%20we%20make%20online%20corrections%0Afor%20the%20vision-based%20terrain%20and%20obstacle%20estimations.%20Consequently%2C%20TOP-Nav%0Aachieves%20open-world%20navigation%20that%20the%20robot%20can%20handle%20terrains%20or%0Adisturbances%20beyond%20the%20distribution%20of%20prior%20knowledge%20and%20overcomes%0Aconstraints%20imposed%20by%20visual%20conditions.%20Building%20upon%20extensive%20experiments%0Aconducted%20in%20both%20simulation%20and%20real-world%20environments%2C%20TOP-Nav%20demonstrates%0Asuperior%20performance%20in%20open-world%20navigation%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15256v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TOP-Nav%3A%20Legged%20Navigation%20Integrating%20Terrain%2C%20Obstacle%20and%0A%20%20Proprioception%20Estimation&entry.906535625=Junli%20Ren%20and%20Yikai%20Liu%20and%20Yingru%20Dai%20and%20Guijin%20Wang&entry.1292438233=%20%20Legged%20navigation%20is%20typically%20examined%20within%20open-world%2C%20off-road%2C%20and%0Achallenging%20environments.%20In%20these%20scenarios%2C%20estimating%20external%20disturbances%0Arequires%20a%20complex%20synthesis%20of%20multi-modal%20information.%20This%20underlines%20a%0Amajor%20limitation%20in%20existing%20works%20that%20primarily%20focus%20on%20avoiding%20obstacles.%0AIn%20this%20work%2C%20we%20propose%20TOP-Nav%2C%20a%20novel%20legged%20navigation%20framework%20that%0Aintegrates%20a%20comprehensive%20path%20planner%20with%20Terrain%20awareness%2C%20Obstacle%0Aavoidance%20and%20close-loop%20Proprioception.%20TOP-Nav%20underscores%20the%20synergies%0Abetween%20vision%20and%20proprioception%20in%20both%20path%20and%20motion%20planning.%20Within%20the%0Apath%20planner%2C%20we%20present%20and%20integrate%20a%20terrain%20estimator%20that%20enables%20the%0Arobot%20to%20select%20waypoints%20on%20terrains%20with%20higher%20traversability%20while%0Aeffectively%20avoiding%20obstacles.%20In%20the%20motion%20planning%20level%2C%20we%20not%20only%0Aimplement%20a%20locomotion%20controller%20to%20track%20the%20navigation%20commands%2C%20but%20also%0Aconstruct%20a%20proprioception%20advisor%20to%20provide%20motion%20evaluations%20for%20the%20path%0Aplanner.%20Based%20on%20the%20close-loop%20motion%20feedback%2C%20we%20make%20online%20corrections%0Afor%20the%20vision-based%20terrain%20and%20obstacle%20estimations.%20Consequently%2C%20TOP-Nav%0Aachieves%20open-world%20navigation%20that%20the%20robot%20can%20handle%20terrains%20or%0Adisturbances%20beyond%20the%20distribution%20of%20prior%20knowledge%20and%20overcomes%0Aconstraints%20imposed%20by%20visual%20conditions.%20Building%20upon%20extensive%20experiments%0Aconducted%20in%20both%20simulation%20and%20real-world%20environments%2C%20TOP-Nav%20demonstrates%0Asuperior%20performance%20in%20open-world%20navigation%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15256v1&entry.124074799=Read"},
{"title": "Regularized Gauss-Newton for Optimizing Overparameterized Neural\n  Networks", "author": "Adeyemi D. Adeoye and Philipp Christian Petersen and Alberto Bemporad", "abstract": "  The generalized Gauss-Newton (GGN) optimization method incorporates curvature\nestimates into its solution steps, and provides a good approximation to the\nNewton method for large-scale optimization problems. GGN has been found\nparticularly interesting for practical training of deep neural networks, not\nonly for its impressive convergence speed, but also for its close relation with\nneural tangent kernel regression, which is central to recent studies that aim\nto understand the optimization and generalization properties of neural\nnetworks. This work studies a GGN method for optimizing a two-layer neural\nnetwork with explicit regularization. In particular, we consider a class of\ngeneralized self-concordant (GSC) functions that provide smooth approximations\nto commonly-used penalty terms in the objective function of the optimization\nproblem. This approach provides an adaptive learning rate selection technique\nthat requires little to no tuning for optimal performance. We study the\nconvergence of the two-layer neural network, considered to be\noverparameterized, in the optimization loop of the resulting GGN method for a\ngiven scaling of the network parameters. Our numerical experiments highlight\nspecific aspects of GSC regularization that help to improve generalization of\nthe optimized neural network. The code to reproduce the experimental results is\navailable at https://github.com/adeyemiadeoye/ggn-score-nn.\n", "link": "http://arxiv.org/abs/2404.14875v1", "date": "2024-04-23", "relevancy": 2.4747, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5375}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4796}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4677}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Regularized%20Gauss-Newton%20for%20Optimizing%20Overparameterized%20Neural%0A%20%20Networks&body=Title%3A%20Regularized%20Gauss-Newton%20for%20Optimizing%20Overparameterized%20Neural%0A%20%20Networks%0AAuthor%3A%20Adeyemi%20D.%20Adeoye%20and%20Philipp%20Christian%20Petersen%20and%20Alberto%20Bemporad%0AAbstract%3A%20%20%20The%20generalized%20Gauss-Newton%20%28GGN%29%20optimization%20method%20incorporates%20curvature%0Aestimates%20into%20its%20solution%20steps%2C%20and%20provides%20a%20good%20approximation%20to%20the%0ANewton%20method%20for%20large-scale%20optimization%20problems.%20GGN%20has%20been%20found%0Aparticularly%20interesting%20for%20practical%20training%20of%20deep%20neural%20networks%2C%20not%0Aonly%20for%20its%20impressive%20convergence%20speed%2C%20but%20also%20for%20its%20close%20relation%20with%0Aneural%20tangent%20kernel%20regression%2C%20which%20is%20central%20to%20recent%20studies%20that%20aim%0Ato%20understand%20the%20optimization%20and%20generalization%20properties%20of%20neural%0Anetworks.%20This%20work%20studies%20a%20GGN%20method%20for%20optimizing%20a%20two-layer%20neural%0Anetwork%20with%20explicit%20regularization.%20In%20particular%2C%20we%20consider%20a%20class%20of%0Ageneralized%20self-concordant%20%28GSC%29%20functions%20that%20provide%20smooth%20approximations%0Ato%20commonly-used%20penalty%20terms%20in%20the%20objective%20function%20of%20the%20optimization%0Aproblem.%20This%20approach%20provides%20an%20adaptive%20learning%20rate%20selection%20technique%0Athat%20requires%20little%20to%20no%20tuning%20for%20optimal%20performance.%20We%20study%20the%0Aconvergence%20of%20the%20two-layer%20neural%20network%2C%20considered%20to%20be%0Aoverparameterized%2C%20in%20the%20optimization%20loop%20of%20the%20resulting%20GGN%20method%20for%20a%0Agiven%20scaling%20of%20the%20network%20parameters.%20Our%20numerical%20experiments%20highlight%0Aspecific%20aspects%20of%20GSC%20regularization%20that%20help%20to%20improve%20generalization%20of%0Athe%20optimized%20neural%20network.%20The%20code%20to%20reproduce%20the%20experimental%20results%20is%0Aavailable%20at%20https%3A//github.com/adeyemiadeoye/ggn-score-nn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14875v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Gauss-Newton%20for%20Optimizing%20Overparameterized%20Neural%0A%20%20Networks&entry.906535625=Adeyemi%20D.%20Adeoye%20and%20Philipp%20Christian%20Petersen%20and%20Alberto%20Bemporad&entry.1292438233=%20%20The%20generalized%20Gauss-Newton%20%28GGN%29%20optimization%20method%20incorporates%20curvature%0Aestimates%20into%20its%20solution%20steps%2C%20and%20provides%20a%20good%20approximation%20to%20the%0ANewton%20method%20for%20large-scale%20optimization%20problems.%20GGN%20has%20been%20found%0Aparticularly%20interesting%20for%20practical%20training%20of%20deep%20neural%20networks%2C%20not%0Aonly%20for%20its%20impressive%20convergence%20speed%2C%20but%20also%20for%20its%20close%20relation%20with%0Aneural%20tangent%20kernel%20regression%2C%20which%20is%20central%20to%20recent%20studies%20that%20aim%0Ato%20understand%20the%20optimization%20and%20generalization%20properties%20of%20neural%0Anetworks.%20This%20work%20studies%20a%20GGN%20method%20for%20optimizing%20a%20two-layer%20neural%0Anetwork%20with%20explicit%20regularization.%20In%20particular%2C%20we%20consider%20a%20class%20of%0Ageneralized%20self-concordant%20%28GSC%29%20functions%20that%20provide%20smooth%20approximations%0Ato%20commonly-used%20penalty%20terms%20in%20the%20objective%20function%20of%20the%20optimization%0Aproblem.%20This%20approach%20provides%20an%20adaptive%20learning%20rate%20selection%20technique%0Athat%20requires%20little%20to%20no%20tuning%20for%20optimal%20performance.%20We%20study%20the%0Aconvergence%20of%20the%20two-layer%20neural%20network%2C%20considered%20to%20be%0Aoverparameterized%2C%20in%20the%20optimization%20loop%20of%20the%20resulting%20GGN%20method%20for%20a%0Agiven%20scaling%20of%20the%20network%20parameters.%20Our%20numerical%20experiments%20highlight%0Aspecific%20aspects%20of%20GSC%20regularization%20that%20help%20to%20improve%20generalization%20of%0Athe%20optimized%20neural%20network.%20The%20code%20to%20reproduce%20the%20experimental%20results%20is%0Aavailable%20at%20https%3A//github.com/adeyemiadeoye/ggn-score-nn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14875v1&entry.124074799=Read"},
{"title": "OccGen: Generative Multi-modal 3D Occupancy Prediction for Autonomous\n  Driving", "author": "Guoqing Wang and Zhongdao Wang and Pin Tang and Jilai Zheng and Xiangxuan Ren and Bailan Feng and Chao Ma", "abstract": "  Existing solutions for 3D semantic occupancy prediction typically treat the\ntask as a one-shot 3D voxel-wise segmentation perception problem. These\ndiscriminative methods focus on learning the mapping between the inputs and\noccupancy map in a single step, lacking the ability to gradually refine the\noccupancy map and the reasonable scene imaginative capacity to complete the\nlocal regions somewhere. In this paper, we introduce OccGen, a simple yet\npowerful generative perception model for the task of 3D semantic occupancy\nprediction. OccGen adopts a ''noise-to-occupancy'' generative paradigm,\nprogressively inferring and refining the occupancy map by predicting and\neliminating noise originating from a random Gaussian distribution. OccGen\nconsists of two main components: a conditional encoder that is capable of\nprocessing multi-modal inputs, and a progressive refinement decoder that\napplies diffusion denoising using the multi-modal features as conditions. A key\ninsight of this generative pipeline is that the diffusion denoising process is\nnaturally able to model the coarse-to-fine refinement of the dense 3D occupancy\nmap, therefore producing more detailed predictions. Extensive experiments on\nseveral occupancy benchmarks demonstrate the effectiveness of the proposed\nmethod compared to the state-of-the-art methods. For instance, OccGen\nrelatively enhances the mIoU by 9.5%, 6.3%, and 13.3% on nuScenes-Occupancy\ndataset under the muli-modal, LiDAR-only, and camera-only settings,\nrespectively. Moreover, as a generative perception model, OccGen exhibits\ndesirable properties that discriminative models cannot achieve, such as\nproviding uncertainty estimates alongside its multiple-step predictions.\n", "link": "http://arxiv.org/abs/2404.15014v1", "date": "2024-04-23", "relevancy": 2.4122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6254}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6053}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5919}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OccGen%3A%20Generative%20Multi-modal%203D%20Occupancy%20Prediction%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20OccGen%3A%20Generative%20Multi-modal%203D%20Occupancy%20Prediction%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Guoqing%20Wang%20and%20Zhongdao%20Wang%20and%20Pin%20Tang%20and%20Jilai%20Zheng%20and%20Xiangxuan%20Ren%20and%20Bailan%20Feng%20and%20Chao%20Ma%0AAbstract%3A%20%20%20Existing%20solutions%20for%203D%20semantic%20occupancy%20prediction%20typically%20treat%20the%0Atask%20as%20a%20one-shot%203D%20voxel-wise%20segmentation%20perception%20problem.%20These%0Adiscriminative%20methods%20focus%20on%20learning%20the%20mapping%20between%20the%20inputs%20and%0Aoccupancy%20map%20in%20a%20single%20step%2C%20lacking%20the%20ability%20to%20gradually%20refine%20the%0Aoccupancy%20map%20and%20the%20reasonable%20scene%20imaginative%20capacity%20to%20complete%20the%0Alocal%20regions%20somewhere.%20In%20this%20paper%2C%20we%20introduce%20OccGen%2C%20a%20simple%20yet%0Apowerful%20generative%20perception%20model%20for%20the%20task%20of%203D%20semantic%20occupancy%0Aprediction.%20OccGen%20adopts%20a%20%27%27noise-to-occupancy%27%27%20generative%20paradigm%2C%0Aprogressively%20inferring%20and%20refining%20the%20occupancy%20map%20by%20predicting%20and%0Aeliminating%20noise%20originating%20from%20a%20random%20Gaussian%20distribution.%20OccGen%0Aconsists%20of%20two%20main%20components%3A%20a%20conditional%20encoder%20that%20is%20capable%20of%0Aprocessing%20multi-modal%20inputs%2C%20and%20a%20progressive%20refinement%20decoder%20that%0Aapplies%20diffusion%20denoising%20using%20the%20multi-modal%20features%20as%20conditions.%20A%20key%0Ainsight%20of%20this%20generative%20pipeline%20is%20that%20the%20diffusion%20denoising%20process%20is%0Anaturally%20able%20to%20model%20the%20coarse-to-fine%20refinement%20of%20the%20dense%203D%20occupancy%0Amap%2C%20therefore%20producing%20more%20detailed%20predictions.%20Extensive%20experiments%20on%0Aseveral%20occupancy%20benchmarks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20compared%20to%20the%20state-of-the-art%20methods.%20For%20instance%2C%20OccGen%0Arelatively%20enhances%20the%20mIoU%20by%209.5%25%2C%206.3%25%2C%20and%2013.3%25%20on%20nuScenes-Occupancy%0Adataset%20under%20the%20muli-modal%2C%20LiDAR-only%2C%20and%20camera-only%20settings%2C%0Arespectively.%20Moreover%2C%20as%20a%20generative%20perception%20model%2C%20OccGen%20exhibits%0Adesirable%20properties%20that%20discriminative%20models%20cannot%20achieve%2C%20such%20as%0Aproviding%20uncertainty%20estimates%20alongside%20its%20multiple-step%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15014v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccGen%3A%20Generative%20Multi-modal%203D%20Occupancy%20Prediction%20for%20Autonomous%0A%20%20Driving&entry.906535625=Guoqing%20Wang%20and%20Zhongdao%20Wang%20and%20Pin%20Tang%20and%20Jilai%20Zheng%20and%20Xiangxuan%20Ren%20and%20Bailan%20Feng%20and%20Chao%20Ma&entry.1292438233=%20%20Existing%20solutions%20for%203D%20semantic%20occupancy%20prediction%20typically%20treat%20the%0Atask%20as%20a%20one-shot%203D%20voxel-wise%20segmentation%20perception%20problem.%20These%0Adiscriminative%20methods%20focus%20on%20learning%20the%20mapping%20between%20the%20inputs%20and%0Aoccupancy%20map%20in%20a%20single%20step%2C%20lacking%20the%20ability%20to%20gradually%20refine%20the%0Aoccupancy%20map%20and%20the%20reasonable%20scene%20imaginative%20capacity%20to%20complete%20the%0Alocal%20regions%20somewhere.%20In%20this%20paper%2C%20we%20introduce%20OccGen%2C%20a%20simple%20yet%0Apowerful%20generative%20perception%20model%20for%20the%20task%20of%203D%20semantic%20occupancy%0Aprediction.%20OccGen%20adopts%20a%20%27%27noise-to-occupancy%27%27%20generative%20paradigm%2C%0Aprogressively%20inferring%20and%20refining%20the%20occupancy%20map%20by%20predicting%20and%0Aeliminating%20noise%20originating%20from%20a%20random%20Gaussian%20distribution.%20OccGen%0Aconsists%20of%20two%20main%20components%3A%20a%20conditional%20encoder%20that%20is%20capable%20of%0Aprocessing%20multi-modal%20inputs%2C%20and%20a%20progressive%20refinement%20decoder%20that%0Aapplies%20diffusion%20denoising%20using%20the%20multi-modal%20features%20as%20conditions.%20A%20key%0Ainsight%20of%20this%20generative%20pipeline%20is%20that%20the%20diffusion%20denoising%20process%20is%0Anaturally%20able%20to%20model%20the%20coarse-to-fine%20refinement%20of%20the%20dense%203D%20occupancy%0Amap%2C%20therefore%20producing%20more%20detailed%20predictions.%20Extensive%20experiments%20on%0Aseveral%20occupancy%20benchmarks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20compared%20to%20the%20state-of-the-art%20methods.%20For%20instance%2C%20OccGen%0Arelatively%20enhances%20the%20mIoU%20by%209.5%25%2C%206.3%25%2C%20and%2013.3%25%20on%20nuScenes-Occupancy%0Adataset%20under%20the%20muli-modal%2C%20LiDAR-only%2C%20and%20camera-only%20settings%2C%0Arespectively.%20Moreover%2C%20as%20a%20generative%20perception%20model%2C%20OccGen%20exhibits%0Adesirable%20properties%20that%20discriminative%20models%20cannot%20achieve%2C%20such%20as%0Aproviding%20uncertainty%20estimates%20alongside%20its%20multiple-step%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15014v1&entry.124074799=Read"},
{"title": "Rethinking LLM Memorization through the Lens of Adversarial Compression", "author": "Avi Schwarzschild and Zhili Feng and Pratyush Maini and Zachary C. Lipton and J. Zico Kolter", "abstract": "  Large language models (LLMs) trained on web-scale datasets raise substantial\nconcerns regarding permissible data usage. One major question is whether these\nmodels \"memorize\" all their training data or they integrate many data sources\nin some way more akin to how a human would learn and synthesize information.\nThe answer hinges, to a large degree, on $\\textit{how we define memorization}$.\nIn this work, we propose the Adversarial Compression Ratio (ACR) as a metric\nfor assessing memorization in LLMs -- a given string from the training data is\nconsidered memorized if it can be elicited by a prompt shorter than the string\nitself. In other words, these strings can be \"compressed\" with the model by\ncomputing adversarial prompts of fewer tokens. We outline the limitations of\nexisting notions of memorization and show how the ACR overcomes these\nchallenges by (i) offering an adversarial view to measuring memorization,\nespecially for monitoring unlearning and compliance; and (ii) allowing for the\nflexibility to measure memorization for arbitrary strings at a reasonably low\ncompute. Our definition serves as a valuable and practical tool for determining\nwhen model owners may be violating terms around data usage, providing a\npotential legal tool and a critical lens through which to address such\nscenarios. Project page: https://locuslab.github.io/acr-memorization.\n", "link": "http://arxiv.org/abs/2404.15146v1", "date": "2024-04-23", "relevancy": 2.4001, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.49}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Rethinking%20LLM%20Memorization%20through%20the%20Lens%20of%20Adversarial%20Compression&body=Title%3A%20Rethinking%20LLM%20Memorization%20through%20the%20Lens%20of%20Adversarial%20Compression%0AAuthor%3A%20Avi%20Schwarzschild%20and%20Zhili%20Feng%20and%20Pratyush%20Maini%20and%20Zachary%20C.%20Lipton%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20trained%20on%20web-scale%20datasets%20raise%20substantial%0Aconcerns%20regarding%20permissible%20data%20usage.%20One%20major%20question%20is%20whether%20these%0Amodels%20%22memorize%22%20all%20their%20training%20data%20or%20they%20integrate%20many%20data%20sources%0Ain%20some%20way%20more%20akin%20to%20how%20a%20human%20would%20learn%20and%20synthesize%20information.%0AThe%20answer%20hinges%2C%20to%20a%20large%20degree%2C%20on%20%24%5Ctextit%7Bhow%20we%20define%20memorization%7D%24.%0AIn%20this%20work%2C%20we%20propose%20the%20Adversarial%20Compression%20Ratio%20%28ACR%29%20as%20a%20metric%0Afor%20assessing%20memorization%20in%20LLMs%20--%20a%20given%20string%20from%20the%20training%20data%20is%0Aconsidered%20memorized%20if%20it%20can%20be%20elicited%20by%20a%20prompt%20shorter%20than%20the%20string%0Aitself.%20In%20other%20words%2C%20these%20strings%20can%20be%20%22compressed%22%20with%20the%20model%20by%0Acomputing%20adversarial%20prompts%20of%20fewer%20tokens.%20We%20outline%20the%20limitations%20of%0Aexisting%20notions%20of%20memorization%20and%20show%20how%20the%20ACR%20overcomes%20these%0Achallenges%20by%20%28i%29%20offering%20an%20adversarial%20view%20to%20measuring%20memorization%2C%0Aespecially%20for%20monitoring%20unlearning%20and%20compliance%3B%20and%20%28ii%29%20allowing%20for%20the%0Aflexibility%20to%20measure%20memorization%20for%20arbitrary%20strings%20at%20a%20reasonably%20low%0Acompute.%20Our%20definition%20serves%20as%20a%20valuable%20and%20practical%20tool%20for%20determining%0Awhen%20model%20owners%20may%20be%20violating%20terms%20around%20data%20usage%2C%20providing%20a%0Apotential%20legal%20tool%20and%20a%20critical%20lens%20through%20which%20to%20address%20such%0Ascenarios.%20Project%20page%3A%20https%3A//locuslab.github.io/acr-memorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15146v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20LLM%20Memorization%20through%20the%20Lens%20of%20Adversarial%20Compression&entry.906535625=Avi%20Schwarzschild%20and%20Zhili%20Feng%20and%20Pratyush%20Maini%20and%20Zachary%20C.%20Lipton%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20trained%20on%20web-scale%20datasets%20raise%20substantial%0Aconcerns%20regarding%20permissible%20data%20usage.%20One%20major%20question%20is%20whether%20these%0Amodels%20%22memorize%22%20all%20their%20training%20data%20or%20they%20integrate%20many%20data%20sources%0Ain%20some%20way%20more%20akin%20to%20how%20a%20human%20would%20learn%20and%20synthesize%20information.%0AThe%20answer%20hinges%2C%20to%20a%20large%20degree%2C%20on%20%24%5Ctextit%7Bhow%20we%20define%20memorization%7D%24.%0AIn%20this%20work%2C%20we%20propose%20the%20Adversarial%20Compression%20Ratio%20%28ACR%29%20as%20a%20metric%0Afor%20assessing%20memorization%20in%20LLMs%20--%20a%20given%20string%20from%20the%20training%20data%20is%0Aconsidered%20memorized%20if%20it%20can%20be%20elicited%20by%20a%20prompt%20shorter%20than%20the%20string%0Aitself.%20In%20other%20words%2C%20these%20strings%20can%20be%20%22compressed%22%20with%20the%20model%20by%0Acomputing%20adversarial%20prompts%20of%20fewer%20tokens.%20We%20outline%20the%20limitations%20of%0Aexisting%20notions%20of%20memorization%20and%20show%20how%20the%20ACR%20overcomes%20these%0Achallenges%20by%20%28i%29%20offering%20an%20adversarial%20view%20to%20measuring%20memorization%2C%0Aespecially%20for%20monitoring%20unlearning%20and%20compliance%3B%20and%20%28ii%29%20allowing%20for%20the%0Aflexibility%20to%20measure%20memorization%20for%20arbitrary%20strings%20at%20a%20reasonably%20low%0Acompute.%20Our%20definition%20serves%20as%20a%20valuable%20and%20practical%20tool%20for%20determining%0Awhen%20model%20owners%20may%20be%20violating%20terms%20around%20data%20usage%2C%20providing%20a%0Apotential%20legal%20tool%20and%20a%20critical%20lens%20through%20which%20to%20address%20such%0Ascenarios.%20Project%20page%3A%20https%3A//locuslab.github.io/acr-memorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15146v1&entry.124074799=Read"},
{"title": "Automatic Layout Planning for Visually-Rich Documents with\n  Instruction-Following Models", "author": "Wanrong Zhu and Jennifer Healey and Ruiyi Zhang and William Yang Wang and Tong Sun", "abstract": "  Recent advancements in instruction-following models have made user\ninteractions with models more user-friendly and efficient, broadening their\napplicability. In graphic design, non-professional users often struggle to\ncreate visually appealing layouts due to limited skills and resources. In this\nwork, we introduce a novel multimodal instruction-following framework for\nlayout planning, allowing users to easily arrange visual elements into tailored\nlayouts by specifying canvas size and design purpose, such as for book covers,\nposters, brochures, or menus. We developed three layout reasoning tasks to\ntrain the model in understanding and executing layout instructions. Experiments\non two benchmarks show that our method not only simplifies the design process\nfor non-professionals but also surpasses the performance of few-shot GPT-4V\nmodels, with mIoU higher by 12% on Crello. This progress highlights the\npotential of multimodal instruction-following models to automate and simplify\nthe design process, providing an approachable solution for a wide range of\ndesign tasks on visually-rich documents.\n", "link": "http://arxiv.org/abs/2404.15271v1", "date": "2024-04-23", "relevancy": 2.3924, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6636}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5529}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Layout%20Planning%20for%20Visually-Rich%20Documents%20with%0A%20%20Instruction-Following%20Models&body=Title%3A%20Automatic%20Layout%20Planning%20for%20Visually-Rich%20Documents%20with%0A%20%20Instruction-Following%20Models%0AAuthor%3A%20Wanrong%20Zhu%20and%20Jennifer%20Healey%20and%20Ruiyi%20Zhang%20and%20William%20Yang%20Wang%20and%20Tong%20Sun%0AAbstract%3A%20%20%20Recent%20advancements%20in%20instruction-following%20models%20have%20made%20user%0Ainteractions%20with%20models%20more%20user-friendly%20and%20efficient%2C%20broadening%20their%0Aapplicability.%20In%20graphic%20design%2C%20non-professional%20users%20often%20struggle%20to%0Acreate%20visually%20appealing%20layouts%20due%20to%20limited%20skills%20and%20resources.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20multimodal%20instruction-following%20framework%20for%0Alayout%20planning%2C%20allowing%20users%20to%20easily%20arrange%20visual%20elements%20into%20tailored%0Alayouts%20by%20specifying%20canvas%20size%20and%20design%20purpose%2C%20such%20as%20for%20book%20covers%2C%0Aposters%2C%20brochures%2C%20or%20menus.%20We%20developed%20three%20layout%20reasoning%20tasks%20to%0Atrain%20the%20model%20in%20understanding%20and%20executing%20layout%20instructions.%20Experiments%0Aon%20two%20benchmarks%20show%20that%20our%20method%20not%20only%20simplifies%20the%20design%20process%0Afor%20non-professionals%20but%20also%20surpasses%20the%20performance%20of%20few-shot%20GPT-4V%0Amodels%2C%20with%20mIoU%20higher%20by%2012%25%20on%20Crello.%20This%20progress%20highlights%20the%0Apotential%20of%20multimodal%20instruction-following%20models%20to%20automate%20and%20simplify%0Athe%20design%20process%2C%20providing%20an%20approachable%20solution%20for%20a%20wide%20range%20of%0Adesign%20tasks%20on%20visually-rich%20documents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15271v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Layout%20Planning%20for%20Visually-Rich%20Documents%20with%0A%20%20Instruction-Following%20Models&entry.906535625=Wanrong%20Zhu%20and%20Jennifer%20Healey%20and%20Ruiyi%20Zhang%20and%20William%20Yang%20Wang%20and%20Tong%20Sun&entry.1292438233=%20%20Recent%20advancements%20in%20instruction-following%20models%20have%20made%20user%0Ainteractions%20with%20models%20more%20user-friendly%20and%20efficient%2C%20broadening%20their%0Aapplicability.%20In%20graphic%20design%2C%20non-professional%20users%20often%20struggle%20to%0Acreate%20visually%20appealing%20layouts%20due%20to%20limited%20skills%20and%20resources.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20multimodal%20instruction-following%20framework%20for%0Alayout%20planning%2C%20allowing%20users%20to%20easily%20arrange%20visual%20elements%20into%20tailored%0Alayouts%20by%20specifying%20canvas%20size%20and%20design%20purpose%2C%20such%20as%20for%20book%20covers%2C%0Aposters%2C%20brochures%2C%20or%20menus.%20We%20developed%20three%20layout%20reasoning%20tasks%20to%0Atrain%20the%20model%20in%20understanding%20and%20executing%20layout%20instructions.%20Experiments%0Aon%20two%20benchmarks%20show%20that%20our%20method%20not%20only%20simplifies%20the%20design%20process%0Afor%20non-professionals%20but%20also%20surpasses%20the%20performance%20of%20few-shot%20GPT-4V%0Amodels%2C%20with%20mIoU%20higher%20by%2012%25%20on%20Crello.%20This%20progress%20highlights%20the%0Apotential%20of%20multimodal%20instruction-following%20models%20to%20automate%20and%20simplify%0Athe%20design%20process%2C%20providing%20an%20approachable%20solution%20for%20a%20wide%20range%20of%0Adesign%20tasks%20on%20visually-rich%20documents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15271v1&entry.124074799=Read"},
{"title": "Combating Missing Modalities in Egocentric Videos at Test Time", "author": "Merey Ramazanova and Alejandro Pardo and Bernard Ghanem and Motasem Alfarra", "abstract": "  Understanding videos that contain multiple modalities is crucial, especially\nin egocentric videos, where combining various sensory inputs significantly\nimproves tasks like action recognition and moment localization. However,\nreal-world applications often face challenges with incomplete modalities due to\nprivacy concerns, efficiency needs, or hardware issues. Current methods, while\neffective, often necessitate retraining the model entirely to handle missing\nmodalities, making them computationally intensive, particularly with large\ntraining datasets. In this study, we propose a novel approach to address this\nissue at test time without requiring retraining. We frame the problem as a\ntest-time adaptation task, where the model adjusts to the available unlabeled\ndata at test time. Our method, MiDl~(Mutual information with\nself-Distillation), encourages the model to be insensitive to the specific\nmodality source present during testing by minimizing the mutual information\nbetween the prediction and the available modality. Additionally, we incorporate\nself-distillation to maintain the model's original performance when both\nmodalities are available. MiDl represents the first self-supervised, online\nsolution for handling missing modalities exclusively at test time. Through\nexperiments with various pretrained models and datasets, MiDl demonstrates\nsubstantial performance improvement without the need for retraining.\n", "link": "http://arxiv.org/abs/2404.15161v1", "date": "2024-04-23", "relevancy": 2.3705, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5979}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5929}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5902}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Combating%20Missing%20Modalities%20in%20Egocentric%20Videos%20at%20Test%20Time&body=Title%3A%20Combating%20Missing%20Modalities%20in%20Egocentric%20Videos%20at%20Test%20Time%0AAuthor%3A%20Merey%20Ramazanova%20and%20Alejandro%20Pardo%20and%20Bernard%20Ghanem%20and%20Motasem%20Alfarra%0AAbstract%3A%20%20%20Understanding%20videos%20that%20contain%20multiple%20modalities%20is%20crucial%2C%20especially%0Ain%20egocentric%20videos%2C%20where%20combining%20various%20sensory%20inputs%20significantly%0Aimproves%20tasks%20like%20action%20recognition%20and%20moment%20localization.%20However%2C%0Areal-world%20applications%20often%20face%20challenges%20with%20incomplete%20modalities%20due%20to%0Aprivacy%20concerns%2C%20efficiency%20needs%2C%20or%20hardware%20issues.%20Current%20methods%2C%20while%0Aeffective%2C%20often%20necessitate%20retraining%20the%20model%20entirely%20to%20handle%20missing%0Amodalities%2C%20making%20them%20computationally%20intensive%2C%20particularly%20with%20large%0Atraining%20datasets.%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20to%20address%20this%0Aissue%20at%20test%20time%20without%20requiring%20retraining.%20We%20frame%20the%20problem%20as%20a%0Atest-time%20adaptation%20task%2C%20where%20the%20model%20adjusts%20to%20the%20available%20unlabeled%0Adata%20at%20test%20time.%20Our%20method%2C%20MiDl~%28Mutual%20information%20with%0Aself-Distillation%29%2C%20encourages%20the%20model%20to%20be%20insensitive%20to%20the%20specific%0Amodality%20source%20present%20during%20testing%20by%20minimizing%20the%20mutual%20information%0Abetween%20the%20prediction%20and%20the%20available%20modality.%20Additionally%2C%20we%20incorporate%0Aself-distillation%20to%20maintain%20the%20model%27s%20original%20performance%20when%20both%0Amodalities%20are%20available.%20MiDl%20represents%20the%20first%20self-supervised%2C%20online%0Asolution%20for%20handling%20missing%20modalities%20exclusively%20at%20test%20time.%20Through%0Aexperiments%20with%20various%20pretrained%20models%20and%20datasets%2C%20MiDl%20demonstrates%0Asubstantial%20performance%20improvement%20without%20the%20need%20for%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15161v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combating%20Missing%20Modalities%20in%20Egocentric%20Videos%20at%20Test%20Time&entry.906535625=Merey%20Ramazanova%20and%20Alejandro%20Pardo%20and%20Bernard%20Ghanem%20and%20Motasem%20Alfarra&entry.1292438233=%20%20Understanding%20videos%20that%20contain%20multiple%20modalities%20is%20crucial%2C%20especially%0Ain%20egocentric%20videos%2C%20where%20combining%20various%20sensory%20inputs%20significantly%0Aimproves%20tasks%20like%20action%20recognition%20and%20moment%20localization.%20However%2C%0Areal-world%20applications%20often%20face%20challenges%20with%20incomplete%20modalities%20due%20to%0Aprivacy%20concerns%2C%20efficiency%20needs%2C%20or%20hardware%20issues.%20Current%20methods%2C%20while%0Aeffective%2C%20often%20necessitate%20retraining%20the%20model%20entirely%20to%20handle%20missing%0Amodalities%2C%20making%20them%20computationally%20intensive%2C%20particularly%20with%20large%0Atraining%20datasets.%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20to%20address%20this%0Aissue%20at%20test%20time%20without%20requiring%20retraining.%20We%20frame%20the%20problem%20as%20a%0Atest-time%20adaptation%20task%2C%20where%20the%20model%20adjusts%20to%20the%20available%20unlabeled%0Adata%20at%20test%20time.%20Our%20method%2C%20MiDl~%28Mutual%20information%20with%0Aself-Distillation%29%2C%20encourages%20the%20model%20to%20be%20insensitive%20to%20the%20specific%0Amodality%20source%20present%20during%20testing%20by%20minimizing%20the%20mutual%20information%0Abetween%20the%20prediction%20and%20the%20available%20modality.%20Additionally%2C%20we%20incorporate%0Aself-distillation%20to%20maintain%20the%20model%27s%20original%20performance%20when%20both%0Amodalities%20are%20available.%20MiDl%20represents%20the%20first%20self-supervised%2C%20online%0Asolution%20for%20handling%20missing%20modalities%20exclusively%20at%20test%20time.%20Through%0Aexperiments%20with%20various%20pretrained%20models%20and%20datasets%2C%20MiDl%20demonstrates%0Asubstantial%20performance%20improvement%20without%20the%20need%20for%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15161v1&entry.124074799=Read"},
{"title": "RoboFusion: Towards Robust Multi-Modal 3D Object Detection via SAM", "author": "Ziying Song and Guoxing Zhang and Lin Liu and Lei Yang and Shaoqing Xu and Caiyan Jia and Feiyang Jia and Li Wang", "abstract": "  Multi-modal 3D object detectors are dedicated to exploring secure and\nreliable perception systems for autonomous driving (AD).Although achieving\nstate-of-the-art (SOTA) performance on clean benchmark datasets, they tend to\noverlook the complexity and harsh conditions of real-world environments. With\nthe emergence of visual foundation models (VFMs), opportunities and challenges\nare presented for improving the robustness and generalization of multi-modal 3D\nobject detection in AD. Therefore, we propose RoboFusion, a robust framework\nthat leverages VFMs like SAM to tackle out-of-distribution (OOD) noise\nscenarios. We first adapt the original SAM for AD scenarios named SAM-AD. To\nalign SAM or SAM-AD with multi-modal methods, we then introduce AD-FPN for\nupsampling the image features extracted by SAM. We employ wavelet decomposition\nto denoise the depth-guided images for further noise reduction and weather\ninterference. At last, we employ self-attention mechanisms to adaptively\nreweight the fused features, enhancing informative features while suppressing\nexcess noise. In summary, RoboFusion significantly reduces noise by leveraging\nthe generalization and robustness of VFMs, thereby enhancing the resilience of\nmulti-modal 3D object detection. Consequently, RoboFusion achieves SOTA\nperformance in noisy scenarios, as demonstrated by the KITTI-C and nuScenes-C\nbenchmarks. Code is available at https://github.com/adept-thu/RoboFusion.\n", "link": "http://arxiv.org/abs/2401.03907v4", "date": "2024-04-23", "relevancy": 2.3692, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6093}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.59}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5763}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RoboFusion%3A%20Towards%20Robust%20Multi-Modal%203D%20Object%20Detection%20via%20SAM&body=Title%3A%20RoboFusion%3A%20Towards%20Robust%20Multi-Modal%203D%20Object%20Detection%20via%20SAM%0AAuthor%3A%20Ziying%20Song%20and%20Guoxing%20Zhang%20and%20Lin%20Liu%20and%20Lei%20Yang%20and%20Shaoqing%20Xu%20and%20Caiyan%20Jia%20and%20Feiyang%20Jia%20and%20Li%20Wang%0AAbstract%3A%20%20%20Multi-modal%203D%20object%20detectors%20are%20dedicated%20to%20exploring%20secure%20and%0Areliable%20perception%20systems%20for%20autonomous%20driving%20%28AD%29.Although%20achieving%0Astate-of-the-art%20%28SOTA%29%20performance%20on%20clean%20benchmark%20datasets%2C%20they%20tend%20to%0Aoverlook%20the%20complexity%20and%20harsh%20conditions%20of%20real-world%20environments.%20With%0Athe%20emergence%20of%20visual%20foundation%20models%20%28VFMs%29%2C%20opportunities%20and%20challenges%0Aare%20presented%20for%20improving%20the%20robustness%20and%20generalization%20of%20multi-modal%203D%0Aobject%20detection%20in%20AD.%20Therefore%2C%20we%20propose%20RoboFusion%2C%20a%20robust%20framework%0Athat%20leverages%20VFMs%20like%20SAM%20to%20tackle%20out-of-distribution%20%28OOD%29%20noise%0Ascenarios.%20We%20first%20adapt%20the%20original%20SAM%20for%20AD%20scenarios%20named%20SAM-AD.%20To%0Aalign%20SAM%20or%20SAM-AD%20with%20multi-modal%20methods%2C%20we%20then%20introduce%20AD-FPN%20for%0Aupsampling%20the%20image%20features%20extracted%20by%20SAM.%20We%20employ%20wavelet%20decomposition%0Ato%20denoise%20the%20depth-guided%20images%20for%20further%20noise%20reduction%20and%20weather%0Ainterference.%20At%20last%2C%20we%20employ%20self-attention%20mechanisms%20to%20adaptively%0Areweight%20the%20fused%20features%2C%20enhancing%20informative%20features%20while%20suppressing%0Aexcess%20noise.%20In%20summary%2C%20RoboFusion%20significantly%20reduces%20noise%20by%20leveraging%0Athe%20generalization%20and%20robustness%20of%20VFMs%2C%20thereby%20enhancing%20the%20resilience%20of%0Amulti-modal%203D%20object%20detection.%20Consequently%2C%20RoboFusion%20achieves%20SOTA%0Aperformance%20in%20noisy%20scenarios%2C%20as%20demonstrated%20by%20the%20KITTI-C%20and%20nuScenes-C%0Abenchmarks.%20Code%20is%20available%20at%20https%3A//github.com/adept-thu/RoboFusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03907v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboFusion%3A%20Towards%20Robust%20Multi-Modal%203D%20Object%20Detection%20via%20SAM&entry.906535625=Ziying%20Song%20and%20Guoxing%20Zhang%20and%20Lin%20Liu%20and%20Lei%20Yang%20and%20Shaoqing%20Xu%20and%20Caiyan%20Jia%20and%20Feiyang%20Jia%20and%20Li%20Wang&entry.1292438233=%20%20Multi-modal%203D%20object%20detectors%20are%20dedicated%20to%20exploring%20secure%20and%0Areliable%20perception%20systems%20for%20autonomous%20driving%20%28AD%29.Although%20achieving%0Astate-of-the-art%20%28SOTA%29%20performance%20on%20clean%20benchmark%20datasets%2C%20they%20tend%20to%0Aoverlook%20the%20complexity%20and%20harsh%20conditions%20of%20real-world%20environments.%20With%0Athe%20emergence%20of%20visual%20foundation%20models%20%28VFMs%29%2C%20opportunities%20and%20challenges%0Aare%20presented%20for%20improving%20the%20robustness%20and%20generalization%20of%20multi-modal%203D%0Aobject%20detection%20in%20AD.%20Therefore%2C%20we%20propose%20RoboFusion%2C%20a%20robust%20framework%0Athat%20leverages%20VFMs%20like%20SAM%20to%20tackle%20out-of-distribution%20%28OOD%29%20noise%0Ascenarios.%20We%20first%20adapt%20the%20original%20SAM%20for%20AD%20scenarios%20named%20SAM-AD.%20To%0Aalign%20SAM%20or%20SAM-AD%20with%20multi-modal%20methods%2C%20we%20then%20introduce%20AD-FPN%20for%0Aupsampling%20the%20image%20features%20extracted%20by%20SAM.%20We%20employ%20wavelet%20decomposition%0Ato%20denoise%20the%20depth-guided%20images%20for%20further%20noise%20reduction%20and%20weather%0Ainterference.%20At%20last%2C%20we%20employ%20self-attention%20mechanisms%20to%20adaptively%0Areweight%20the%20fused%20features%2C%20enhancing%20informative%20features%20while%20suppressing%0Aexcess%20noise.%20In%20summary%2C%20RoboFusion%20significantly%20reduces%20noise%20by%20leveraging%0Athe%20generalization%20and%20robustness%20of%20VFMs%2C%20thereby%20enhancing%20the%20resilience%20of%0Amulti-modal%203D%20object%20detection.%20Consequently%2C%20RoboFusion%20achieves%20SOTA%0Aperformance%20in%20noisy%20scenarios%2C%20as%20demonstrated%20by%20the%20KITTI-C%20and%20nuScenes-C%0Abenchmarks.%20Code%20is%20available%20at%20https%3A//github.com/adept-thu/RoboFusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03907v4&entry.124074799=Read"},
{"title": "Mining Supervision for Dynamic Regions in Self-Supervised Monocular\n  Depth Estimation", "author": "Hoang Chuong Nguyen and Tianyu Wang and Jose M. Alvarez and Miaomiao Liu", "abstract": "  This paper focuses on self-supervised monocular depth estimation in dynamic\nscenes trained on monocular videos. Existing methods jointly estimate\npixel-wise depth and motion, relying mainly on an image reconstruction loss.\nDynamic regions1 remain a critical challenge for these methods due to the\ninherent ambiguity in depth and motion estimation, resulting in inaccurate\ndepth estimation. This paper proposes a self-supervised training framework\nexploiting pseudo depth labels for dynamic regions from training data. The key\ncontribution of our framework is to decouple depth estimation for static and\ndynamic regions of images in the training data. We start with an unsupervised\ndepth estimation approach, which provides reliable depth estimates for static\nregions and motion cues for dynamic regions and allows us to extract moving\nobject information at the instance level. In the next stage, we use an object\nnetwork to estimate the depth of those moving objects assuming rigid motions.\nThen, we propose a new scale alignment module to address the scale ambiguity\nbetween estimated depths for static and dynamic regions. We can then use the\ndepth labels generated to train an end-to-end depth estimation network and\nimprove its performance. Extensive experiments on the Cityscapes and KITTI\ndatasets show that our self-training strategy consistently outperforms existing\nself/unsupervised depth estimation methods.\n", "link": "http://arxiv.org/abs/2404.14908v1", "date": "2024-04-23", "relevancy": 2.3485, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6075}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5868}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5669}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mining%20Supervision%20for%20Dynamic%20Regions%20in%20Self-Supervised%20Monocular%0A%20%20Depth%20Estimation&body=Title%3A%20Mining%20Supervision%20for%20Dynamic%20Regions%20in%20Self-Supervised%20Monocular%0A%20%20Depth%20Estimation%0AAuthor%3A%20Hoang%20Chuong%20Nguyen%20and%20Tianyu%20Wang%20and%20Jose%20M.%20Alvarez%20and%20Miaomiao%20Liu%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20self-supervised%20monocular%20depth%20estimation%20in%20dynamic%0Ascenes%20trained%20on%20monocular%20videos.%20Existing%20methods%20jointly%20estimate%0Apixel-wise%20depth%20and%20motion%2C%20relying%20mainly%20on%20an%20image%20reconstruction%20loss.%0ADynamic%20regions1%20remain%20a%20critical%20challenge%20for%20these%20methods%20due%20to%20the%0Ainherent%20ambiguity%20in%20depth%20and%20motion%20estimation%2C%20resulting%20in%20inaccurate%0Adepth%20estimation.%20This%20paper%20proposes%20a%20self-supervised%20training%20framework%0Aexploiting%20pseudo%20depth%20labels%20for%20dynamic%20regions%20from%20training%20data.%20The%20key%0Acontribution%20of%20our%20framework%20is%20to%20decouple%20depth%20estimation%20for%20static%20and%0Adynamic%20regions%20of%20images%20in%20the%20training%20data.%20We%20start%20with%20an%20unsupervised%0Adepth%20estimation%20approach%2C%20which%20provides%20reliable%20depth%20estimates%20for%20static%0Aregions%20and%20motion%20cues%20for%20dynamic%20regions%20and%20allows%20us%20to%20extract%20moving%0Aobject%20information%20at%20the%20instance%20level.%20In%20the%20next%20stage%2C%20we%20use%20an%20object%0Anetwork%20to%20estimate%20the%20depth%20of%20those%20moving%20objects%20assuming%20rigid%20motions.%0AThen%2C%20we%20propose%20a%20new%20scale%20alignment%20module%20to%20address%20the%20scale%20ambiguity%0Abetween%20estimated%20depths%20for%20static%20and%20dynamic%20regions.%20We%20can%20then%20use%20the%0Adepth%20labels%20generated%20to%20train%20an%20end-to-end%20depth%20estimation%20network%20and%0Aimprove%20its%20performance.%20Extensive%20experiments%20on%20the%20Cityscapes%20and%20KITTI%0Adatasets%20show%20that%20our%20self-training%20strategy%20consistently%20outperforms%20existing%0Aself/unsupervised%20depth%20estimation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14908v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mining%20Supervision%20for%20Dynamic%20Regions%20in%20Self-Supervised%20Monocular%0A%20%20Depth%20Estimation&entry.906535625=Hoang%20Chuong%20Nguyen%20and%20Tianyu%20Wang%20and%20Jose%20M.%20Alvarez%20and%20Miaomiao%20Liu&entry.1292438233=%20%20This%20paper%20focuses%20on%20self-supervised%20monocular%20depth%20estimation%20in%20dynamic%0Ascenes%20trained%20on%20monocular%20videos.%20Existing%20methods%20jointly%20estimate%0Apixel-wise%20depth%20and%20motion%2C%20relying%20mainly%20on%20an%20image%20reconstruction%20loss.%0ADynamic%20regions1%20remain%20a%20critical%20challenge%20for%20these%20methods%20due%20to%20the%0Ainherent%20ambiguity%20in%20depth%20and%20motion%20estimation%2C%20resulting%20in%20inaccurate%0Adepth%20estimation.%20This%20paper%20proposes%20a%20self-supervised%20training%20framework%0Aexploiting%20pseudo%20depth%20labels%20for%20dynamic%20regions%20from%20training%20data.%20The%20key%0Acontribution%20of%20our%20framework%20is%20to%20decouple%20depth%20estimation%20for%20static%20and%0Adynamic%20regions%20of%20images%20in%20the%20training%20data.%20We%20start%20with%20an%20unsupervised%0Adepth%20estimation%20approach%2C%20which%20provides%20reliable%20depth%20estimates%20for%20static%0Aregions%20and%20motion%20cues%20for%20dynamic%20regions%20and%20allows%20us%20to%20extract%20moving%0Aobject%20information%20at%20the%20instance%20level.%20In%20the%20next%20stage%2C%20we%20use%20an%20object%0Anetwork%20to%20estimate%20the%20depth%20of%20those%20moving%20objects%20assuming%20rigid%20motions.%0AThen%2C%20we%20propose%20a%20new%20scale%20alignment%20module%20to%20address%20the%20scale%20ambiguity%0Abetween%20estimated%20depths%20for%20static%20and%20dynamic%20regions.%20We%20can%20then%20use%20the%0Adepth%20labels%20generated%20to%20train%20an%20end-to-end%20depth%20estimation%20network%20and%0Aimprove%20its%20performance.%20Extensive%20experiments%20on%20the%20Cityscapes%20and%20KITTI%0Adatasets%20show%20that%20our%20self-training%20strategy%20consistently%20outperforms%20existing%0Aself/unsupervised%20depth%20estimation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14908v1&entry.124074799=Read"},
{"title": "Multimodal Large Language Model is a Human-Aligned Annotator for\n  Text-to-Image Generation", "author": "Xun Wu and Shaohan Huang and Furu Wei", "abstract": "  Recent studies have demonstrated the exceptional potentials of leveraging\nhuman preference datasets to refine text-to-image generative models, enhancing\nthe alignment between generated images and textual prompts. Despite these\nadvances, current human preference datasets are either prohibitively expensive\nto construct or suffer from a lack of diversity in preference dimensions,\nresulting in limited applicability for instruction tuning in open-source\ntext-to-image generative models and hinder further exploration. To address\nthese challenges and promote the alignment of generative models through\ninstruction tuning, we leverage multimodal large language models to create\nVisionPrefer, a high-quality and fine-grained preference dataset that captures\nmultiple preference aspects. We aggregate feedback from AI annotators across\nfour aspects: prompt-following, aesthetic, fidelity, and harmlessness to\nconstruct VisionPrefer. To validate the effectiveness of VisionPrefer, we train\na reward model VP-Score over VisionPrefer to guide the training of\ntext-to-image generative models and the preference prediction accuracy of\nVP-Score is comparable to human annotators. Furthermore, we use two\nreinforcement learning methods to supervised fine-tune generative models to\nevaluate the performance of VisionPrefer, and extensive experimental results\ndemonstrate that VisionPrefer significantly improves text-image alignment in\ncompositional image generation across diverse aspects, e.g., aesthetic, and\ngeneralizes better than previous human-preference metrics across various image\ndistributions. Moreover, VisionPrefer indicates that the integration of\nAI-generated synthetic data as a supervisory signal is a promising avenue for\nachieving improved alignment with human preferences in vision generative\nmodels.\n", "link": "http://arxiv.org/abs/2404.15100v1", "date": "2024-04-23", "relevancy": 2.336, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5949}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5601}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Large%20Language%20Model%20is%20a%20Human-Aligned%20Annotator%20for%0A%20%20Text-to-Image%20Generation&body=Title%3A%20Multimodal%20Large%20Language%20Model%20is%20a%20Human-Aligned%20Annotator%20for%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Xun%20Wu%20and%20Shaohan%20Huang%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Recent%20studies%20have%20demonstrated%20the%20exceptional%20potentials%20of%20leveraging%0Ahuman%20preference%20datasets%20to%20refine%20text-to-image%20generative%20models%2C%20enhancing%0Athe%20alignment%20between%20generated%20images%20and%20textual%20prompts.%20Despite%20these%0Aadvances%2C%20current%20human%20preference%20datasets%20are%20either%20prohibitively%20expensive%0Ato%20construct%20or%20suffer%20from%20a%20lack%20of%20diversity%20in%20preference%20dimensions%2C%0Aresulting%20in%20limited%20applicability%20for%20instruction%20tuning%20in%20open-source%0Atext-to-image%20generative%20models%20and%20hinder%20further%20exploration.%20To%20address%0Athese%20challenges%20and%20promote%20the%20alignment%20of%20generative%20models%20through%0Ainstruction%20tuning%2C%20we%20leverage%20multimodal%20large%20language%20models%20to%20create%0AVisionPrefer%2C%20a%20high-quality%20and%20fine-grained%20preference%20dataset%20that%20captures%0Amultiple%20preference%20aspects.%20We%20aggregate%20feedback%20from%20AI%20annotators%20across%0Afour%20aspects%3A%20prompt-following%2C%20aesthetic%2C%20fidelity%2C%20and%20harmlessness%20to%0Aconstruct%20VisionPrefer.%20To%20validate%20the%20effectiveness%20of%20VisionPrefer%2C%20we%20train%0Aa%20reward%20model%20VP-Score%20over%20VisionPrefer%20to%20guide%20the%20training%20of%0Atext-to-image%20generative%20models%20and%20the%20preference%20prediction%20accuracy%20of%0AVP-Score%20is%20comparable%20to%20human%20annotators.%20Furthermore%2C%20we%20use%20two%0Areinforcement%20learning%20methods%20to%20supervised%20fine-tune%20generative%20models%20to%0Aevaluate%20the%20performance%20of%20VisionPrefer%2C%20and%20extensive%20experimental%20results%0Ademonstrate%20that%20VisionPrefer%20significantly%20improves%20text-image%20alignment%20in%0Acompositional%20image%20generation%20across%20diverse%20aspects%2C%20e.g.%2C%20aesthetic%2C%20and%0Ageneralizes%20better%20than%20previous%20human-preference%20metrics%20across%20various%20image%0Adistributions.%20Moreover%2C%20VisionPrefer%20indicates%20that%20the%20integration%20of%0AAI-generated%20synthetic%20data%20as%20a%20supervisory%20signal%20is%20a%20promising%20avenue%20for%0Aachieving%20improved%20alignment%20with%20human%20preferences%20in%20vision%20generative%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15100v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Large%20Language%20Model%20is%20a%20Human-Aligned%20Annotator%20for%0A%20%20Text-to-Image%20Generation&entry.906535625=Xun%20Wu%20and%20Shaohan%20Huang%20and%20Furu%20Wei&entry.1292438233=%20%20Recent%20studies%20have%20demonstrated%20the%20exceptional%20potentials%20of%20leveraging%0Ahuman%20preference%20datasets%20to%20refine%20text-to-image%20generative%20models%2C%20enhancing%0Athe%20alignment%20between%20generated%20images%20and%20textual%20prompts.%20Despite%20these%0Aadvances%2C%20current%20human%20preference%20datasets%20are%20either%20prohibitively%20expensive%0Ato%20construct%20or%20suffer%20from%20a%20lack%20of%20diversity%20in%20preference%20dimensions%2C%0Aresulting%20in%20limited%20applicability%20for%20instruction%20tuning%20in%20open-source%0Atext-to-image%20generative%20models%20and%20hinder%20further%20exploration.%20To%20address%0Athese%20challenges%20and%20promote%20the%20alignment%20of%20generative%20models%20through%0Ainstruction%20tuning%2C%20we%20leverage%20multimodal%20large%20language%20models%20to%20create%0AVisionPrefer%2C%20a%20high-quality%20and%20fine-grained%20preference%20dataset%20that%20captures%0Amultiple%20preference%20aspects.%20We%20aggregate%20feedback%20from%20AI%20annotators%20across%0Afour%20aspects%3A%20prompt-following%2C%20aesthetic%2C%20fidelity%2C%20and%20harmlessness%20to%0Aconstruct%20VisionPrefer.%20To%20validate%20the%20effectiveness%20of%20VisionPrefer%2C%20we%20train%0Aa%20reward%20model%20VP-Score%20over%20VisionPrefer%20to%20guide%20the%20training%20of%0Atext-to-image%20generative%20models%20and%20the%20preference%20prediction%20accuracy%20of%0AVP-Score%20is%20comparable%20to%20human%20annotators.%20Furthermore%2C%20we%20use%20two%0Areinforcement%20learning%20methods%20to%20supervised%20fine-tune%20generative%20models%20to%0Aevaluate%20the%20performance%20of%20VisionPrefer%2C%20and%20extensive%20experimental%20results%0Ademonstrate%20that%20VisionPrefer%20significantly%20improves%20text-image%20alignment%20in%0Acompositional%20image%20generation%20across%20diverse%20aspects%2C%20e.g.%2C%20aesthetic%2C%20and%0Ageneralizes%20better%20than%20previous%20human-preference%20metrics%20across%20various%20image%0Adistributions.%20Moreover%2C%20VisionPrefer%20indicates%20that%20the%20integration%20of%0AAI-generated%20synthetic%20data%20as%20a%20supervisory%20signal%20is%20a%20promising%20avenue%20for%0Aachieving%20improved%20alignment%20with%20human%20preferences%20in%20vision%20generative%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15100v1&entry.124074799=Read"},
{"title": "GCEPNet: Graph Convolution-Enhanced Expectation Propagation for Massive\n  MIMO Detection", "author": "Qincheng Lu and Sitao Luan and Xiao-Wen Chang", "abstract": "  Massive MIMO (multiple-input multiple-output) detection is an important topic\nin wireless communication and various machine learning based methods have been\ndeveloped recently for this task. Expectation propagation (EP) and its variants\nare widely used for MIMO detection and have achieved the best performance.\nHowever, EP-based solvers fail to capture the correlation between unknown\nvariables, leading to loss of information, and in addition, they are\ncomputationally expensive. In this paper, we show that the real-valued system\ncan be modeled as spectral signal convolution on graph, through which the\ncorrelation between unknown variables can be captured. Based on this analysis,\nwe propose graph convolution-enhanced expectation propagation (GCEPNet), a\ngraph convolution-enhanced EP detector. GCEPNet incorporates data-dependent\nattention scores into Chebyshev polynomial for powerful graph convolution with\nbetter generalization capacity. It enables a better estimation of the cavity\ndistribution for EP and empirically achieves the state-of-the-art (SOTA) MIMO\ndetection performance with much faster inference speed. To our knowledge, we\nare the first to shed light on the connection between the system model and\ngraph convolution, and the first to design the data-dependent attention scores\nfor graph convolution.\n", "link": "http://arxiv.org/abs/2404.14886v1", "date": "2024-04-23", "relevancy": 2.2934, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4723}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4555}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4482}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GCEPNet%3A%20Graph%20Convolution-Enhanced%20Expectation%20Propagation%20for%20Massive%0A%20%20MIMO%20Detection&body=Title%3A%20GCEPNet%3A%20Graph%20Convolution-Enhanced%20Expectation%20Propagation%20for%20Massive%0A%20%20MIMO%20Detection%0AAuthor%3A%20Qincheng%20Lu%20and%20Sitao%20Luan%20and%20Xiao-Wen%20Chang%0AAbstract%3A%20%20%20Massive%20MIMO%20%28multiple-input%20multiple-output%29%20detection%20is%20an%20important%20topic%0Ain%20wireless%20communication%20and%20various%20machine%20learning%20based%20methods%20have%20been%0Adeveloped%20recently%20for%20this%20task.%20Expectation%20propagation%20%28EP%29%20and%20its%20variants%0Aare%20widely%20used%20for%20MIMO%20detection%20and%20have%20achieved%20the%20best%20performance.%0AHowever%2C%20EP-based%20solvers%20fail%20to%20capture%20the%20correlation%20between%20unknown%0Avariables%2C%20leading%20to%20loss%20of%20information%2C%20and%20in%20addition%2C%20they%20are%0Acomputationally%20expensive.%20In%20this%20paper%2C%20we%20show%20that%20the%20real-valued%20system%0Acan%20be%20modeled%20as%20spectral%20signal%20convolution%20on%20graph%2C%20through%20which%20the%0Acorrelation%20between%20unknown%20variables%20can%20be%20captured.%20Based%20on%20this%20analysis%2C%0Awe%20propose%20graph%20convolution-enhanced%20expectation%20propagation%20%28GCEPNet%29%2C%20a%0Agraph%20convolution-enhanced%20EP%20detector.%20GCEPNet%20incorporates%20data-dependent%0Aattention%20scores%20into%20Chebyshev%20polynomial%20for%20powerful%20graph%20convolution%20with%0Abetter%20generalization%20capacity.%20It%20enables%20a%20better%20estimation%20of%20the%20cavity%0Adistribution%20for%20EP%20and%20empirically%20achieves%20the%20state-of-the-art%20%28SOTA%29%20MIMO%0Adetection%20performance%20with%20much%20faster%20inference%20speed.%20To%20our%20knowledge%2C%20we%0Aare%20the%20first%20to%20shed%20light%20on%20the%20connection%20between%20the%20system%20model%20and%0Agraph%20convolution%2C%20and%20the%20first%20to%20design%20the%20data-dependent%20attention%20scores%0Afor%20graph%20convolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14886v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCEPNet%3A%20Graph%20Convolution-Enhanced%20Expectation%20Propagation%20for%20Massive%0A%20%20MIMO%20Detection&entry.906535625=Qincheng%20Lu%20and%20Sitao%20Luan%20and%20Xiao-Wen%20Chang&entry.1292438233=%20%20Massive%20MIMO%20%28multiple-input%20multiple-output%29%20detection%20is%20an%20important%20topic%0Ain%20wireless%20communication%20and%20various%20machine%20learning%20based%20methods%20have%20been%0Adeveloped%20recently%20for%20this%20task.%20Expectation%20propagation%20%28EP%29%20and%20its%20variants%0Aare%20widely%20used%20for%20MIMO%20detection%20and%20have%20achieved%20the%20best%20performance.%0AHowever%2C%20EP-based%20solvers%20fail%20to%20capture%20the%20correlation%20between%20unknown%0Avariables%2C%20leading%20to%20loss%20of%20information%2C%20and%20in%20addition%2C%20they%20are%0Acomputationally%20expensive.%20In%20this%20paper%2C%20we%20show%20that%20the%20real-valued%20system%0Acan%20be%20modeled%20as%20spectral%20signal%20convolution%20on%20graph%2C%20through%20which%20the%0Acorrelation%20between%20unknown%20variables%20can%20be%20captured.%20Based%20on%20this%20analysis%2C%0Awe%20propose%20graph%20convolution-enhanced%20expectation%20propagation%20%28GCEPNet%29%2C%20a%0Agraph%20convolution-enhanced%20EP%20detector.%20GCEPNet%20incorporates%20data-dependent%0Aattention%20scores%20into%20Chebyshev%20polynomial%20for%20powerful%20graph%20convolution%20with%0Abetter%20generalization%20capacity.%20It%20enables%20a%20better%20estimation%20of%20the%20cavity%0Adistribution%20for%20EP%20and%20empirically%20achieves%20the%20state-of-the-art%20%28SOTA%29%20MIMO%0Adetection%20performance%20with%20much%20faster%20inference%20speed.%20To%20our%20knowledge%2C%20we%0Aare%20the%20first%20to%20shed%20light%20on%20the%20connection%20between%20the%20system%20model%20and%0Agraph%20convolution%2C%20and%20the%20first%20to%20design%20the%20data-dependent%20attention%20scores%0Afor%20graph%20convolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14886v1&entry.124074799=Read"},
{"title": "Problem space structural adversarial attacks for Network Intrusion\n  Detection Systems based on Graph Neural Networks", "author": "Andrea Venturi and Dario Stabili and Mirco Marchetti", "abstract": "  Machine Learning (ML) algorithms have become increasingly popular for\nsupporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive\nresearch has shown their vulnerability to adversarial attacks, which involve\nsubtle perturbations to the inputs of the models aimed at compromising their\nperformance. Recent proposals have effectively leveraged Graph Neural Networks\n(GNN) to produce predictions based also on the structural patterns exhibited by\nintrusions to enhance the detection robustness. However, the adoption of\nGNN-based NIDS introduces new types of risks. In this paper, we propose the\nfirst formalization of adversarial attacks specifically tailored for GNN in\nnetwork intrusion detection. Moreover, we outline and model the problem space\nconstraints that attackers need to consider to carry out feasible structural\nattacks in real-world scenarios. As a final contribution, we conduct an\nextensive experimental campaign in which we launch the proposed attacks against\nstate-of-the-art GNN-based NIDS. Our findings demonstrate the increased\nrobustness of the models against classical feature-based adversarial attacks,\nwhile highlighting their susceptibility to structure-based attacks.\n", "link": "http://arxiv.org/abs/2403.11830v2", "date": "2024-04-23", "relevancy": 2.2853, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4687}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4602}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4423}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Problem%20space%20structural%20adversarial%20attacks%20for%20Network%20Intrusion%0A%20%20Detection%20Systems%20based%20on%20Graph%20Neural%20Networks&body=Title%3A%20Problem%20space%20structural%20adversarial%20attacks%20for%20Network%20Intrusion%0A%20%20Detection%20Systems%20based%20on%20Graph%20Neural%20Networks%0AAuthor%3A%20Andrea%20Venturi%20and%20Dario%20Stabili%20and%20Mirco%20Marchetti%0AAbstract%3A%20%20%20Machine%20Learning%20%28ML%29%20algorithms%20have%20become%20increasingly%20popular%20for%0Asupporting%20Network%20Intrusion%20Detection%20Systems%20%28NIDS%29.%20Nevertheless%2C%20extensive%0Aresearch%20has%20shown%20their%20vulnerability%20to%20adversarial%20attacks%2C%20which%20involve%0Asubtle%20perturbations%20to%20the%20inputs%20of%20the%20models%20aimed%20at%20compromising%20their%0Aperformance.%20Recent%20proposals%20have%20effectively%20leveraged%20Graph%20Neural%20Networks%0A%28GNN%29%20to%20produce%20predictions%20based%20also%20on%20the%20structural%20patterns%20exhibited%20by%0Aintrusions%20to%20enhance%20the%20detection%20robustness.%20However%2C%20the%20adoption%20of%0AGNN-based%20NIDS%20introduces%20new%20types%20of%20risks.%20In%20this%20paper%2C%20we%20propose%20the%0Afirst%20formalization%20of%20adversarial%20attacks%20specifically%20tailored%20for%20GNN%20in%0Anetwork%20intrusion%20detection.%20Moreover%2C%20we%20outline%20and%20model%20the%20problem%20space%0Aconstraints%20that%20attackers%20need%20to%20consider%20to%20carry%20out%20feasible%20structural%0Aattacks%20in%20real-world%20scenarios.%20As%20a%20final%20contribution%2C%20we%20conduct%20an%0Aextensive%20experimental%20campaign%20in%20which%20we%20launch%20the%20proposed%20attacks%20against%0Astate-of-the-art%20GNN-based%20NIDS.%20Our%20findings%20demonstrate%20the%20increased%0Arobustness%20of%20the%20models%20against%20classical%20feature-based%20adversarial%20attacks%2C%0Awhile%20highlighting%20their%20susceptibility%20to%20structure-based%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11830v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Problem%20space%20structural%20adversarial%20attacks%20for%20Network%20Intrusion%0A%20%20Detection%20Systems%20based%20on%20Graph%20Neural%20Networks&entry.906535625=Andrea%20Venturi%20and%20Dario%20Stabili%20and%20Mirco%20Marchetti&entry.1292438233=%20%20Machine%20Learning%20%28ML%29%20algorithms%20have%20become%20increasingly%20popular%20for%0Asupporting%20Network%20Intrusion%20Detection%20Systems%20%28NIDS%29.%20Nevertheless%2C%20extensive%0Aresearch%20has%20shown%20their%20vulnerability%20to%20adversarial%20attacks%2C%20which%20involve%0Asubtle%20perturbations%20to%20the%20inputs%20of%20the%20models%20aimed%20at%20compromising%20their%0Aperformance.%20Recent%20proposals%20have%20effectively%20leveraged%20Graph%20Neural%20Networks%0A%28GNN%29%20to%20produce%20predictions%20based%20also%20on%20the%20structural%20patterns%20exhibited%20by%0Aintrusions%20to%20enhance%20the%20detection%20robustness.%20However%2C%20the%20adoption%20of%0AGNN-based%20NIDS%20introduces%20new%20types%20of%20risks.%20In%20this%20paper%2C%20we%20propose%20the%0Afirst%20formalization%20of%20adversarial%20attacks%20specifically%20tailored%20for%20GNN%20in%0Anetwork%20intrusion%20detection.%20Moreover%2C%20we%20outline%20and%20model%20the%20problem%20space%0Aconstraints%20that%20attackers%20need%20to%20consider%20to%20carry%20out%20feasible%20structural%0Aattacks%20in%20real-world%20scenarios.%20As%20a%20final%20contribution%2C%20we%20conduct%20an%0Aextensive%20experimental%20campaign%20in%20which%20we%20launch%20the%20proposed%20attacks%20against%0Astate-of-the-art%20GNN-based%20NIDS.%20Our%20findings%20demonstrate%20the%20increased%0Arobustness%20of%20the%20models%20against%20classical%20feature-based%20adversarial%20attacks%2C%0Awhile%20highlighting%20their%20susceptibility%20to%20structure-based%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11830v2&entry.124074799=Read"},
{"title": "Domain adaptive pose estimation via multi-level alignment", "author": "Yugan Chen and Lin Zhao and Yalong Xu and Honglei Zu and Xiaoqi An and Guangyu Li", "abstract": "  Domain adaptive pose estimation aims to enable deep models trained on source\ndomain (synthesized) datasets produce similar results on the target domain\n(real-world) datasets. The existing methods have made significant progress by\nconducting image-level or feature-level alignment. However, only aligning at a\nsingle level is not sufficient to fully bridge the domain gap and achieve\nexcellent domain adaptive results. In this paper, we propose a multi-level\ndomain adaptation aproach, which aligns different domains at the image,\nfeature, and pose levels. Specifically, we first utilize image style transer to\nensure that images from the source and target domains have a similar\ndistribution. Subsequently, at the feature level, we employ adversarial\ntraining to make the features from the source and target domains preserve\ndomain-invariant characeristics as much as possible. Finally, at the pose\nlevel, a self-supervised approach is utilized to enable the model to learn\ndiverse knowledge, implicitly addressing the domain gap. Experimental results\ndemonstrate that significant imrovement can be achieved by the proposed\nmulti-level alignment method in pose estimation, which outperforms previous\nstate-of-the-art in human pose by up to 2.4% and animal pose estimation by up\nto 3.1% for dogs and 1.4% for sheep.\n", "link": "http://arxiv.org/abs/2404.14885v1", "date": "2024-04-23", "relevancy": 2.2689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5918}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5583}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5282}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Domain%20adaptive%20pose%20estimation%20via%20multi-level%20alignment&body=Title%3A%20Domain%20adaptive%20pose%20estimation%20via%20multi-level%20alignment%0AAuthor%3A%20Yugan%20Chen%20and%20Lin%20Zhao%20and%20Yalong%20Xu%20and%20Honglei%20Zu%20and%20Xiaoqi%20An%20and%20Guangyu%20Li%0AAbstract%3A%20%20%20Domain%20adaptive%20pose%20estimation%20aims%20to%20enable%20deep%20models%20trained%20on%20source%0Adomain%20%28synthesized%29%20datasets%20produce%20similar%20results%20on%20the%20target%20domain%0A%28real-world%29%20datasets.%20The%20existing%20methods%20have%20made%20significant%20progress%20by%0Aconducting%20image-level%20or%20feature-level%20alignment.%20However%2C%20only%20aligning%20at%20a%0Asingle%20level%20is%20not%20sufficient%20to%20fully%20bridge%20the%20domain%20gap%20and%20achieve%0Aexcellent%20domain%20adaptive%20results.%20In%20this%20paper%2C%20we%20propose%20a%20multi-level%0Adomain%20adaptation%20aproach%2C%20which%20aligns%20different%20domains%20at%20the%20image%2C%0Afeature%2C%20and%20pose%20levels.%20Specifically%2C%20we%20first%20utilize%20image%20style%20transer%20to%0Aensure%20that%20images%20from%20the%20source%20and%20target%20domains%20have%20a%20similar%0Adistribution.%20Subsequently%2C%20at%20the%20feature%20level%2C%20we%20employ%20adversarial%0Atraining%20to%20make%20the%20features%20from%20the%20source%20and%20target%20domains%20preserve%0Adomain-invariant%20characeristics%20as%20much%20as%20possible.%20Finally%2C%20at%20the%20pose%0Alevel%2C%20a%20self-supervised%20approach%20is%20utilized%20to%20enable%20the%20model%20to%20learn%0Adiverse%20knowledge%2C%20implicitly%20addressing%20the%20domain%20gap.%20Experimental%20results%0Ademonstrate%20that%20significant%20imrovement%20can%20be%20achieved%20by%20the%20proposed%0Amulti-level%20alignment%20method%20in%20pose%20estimation%2C%20which%20outperforms%20previous%0Astate-of-the-art%20in%20human%20pose%20by%20up%20to%202.4%25%20and%20animal%20pose%20estimation%20by%20up%0Ato%203.1%25%20for%20dogs%20and%201.4%25%20for%20sheep.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14885v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20adaptive%20pose%20estimation%20via%20multi-level%20alignment&entry.906535625=Yugan%20Chen%20and%20Lin%20Zhao%20and%20Yalong%20Xu%20and%20Honglei%20Zu%20and%20Xiaoqi%20An%20and%20Guangyu%20Li&entry.1292438233=%20%20Domain%20adaptive%20pose%20estimation%20aims%20to%20enable%20deep%20models%20trained%20on%20source%0Adomain%20%28synthesized%29%20datasets%20produce%20similar%20results%20on%20the%20target%20domain%0A%28real-world%29%20datasets.%20The%20existing%20methods%20have%20made%20significant%20progress%20by%0Aconducting%20image-level%20or%20feature-level%20alignment.%20However%2C%20only%20aligning%20at%20a%0Asingle%20level%20is%20not%20sufficient%20to%20fully%20bridge%20the%20domain%20gap%20and%20achieve%0Aexcellent%20domain%20adaptive%20results.%20In%20this%20paper%2C%20we%20propose%20a%20multi-level%0Adomain%20adaptation%20aproach%2C%20which%20aligns%20different%20domains%20at%20the%20image%2C%0Afeature%2C%20and%20pose%20levels.%20Specifically%2C%20we%20first%20utilize%20image%20style%20transer%20to%0Aensure%20that%20images%20from%20the%20source%20and%20target%20domains%20have%20a%20similar%0Adistribution.%20Subsequently%2C%20at%20the%20feature%20level%2C%20we%20employ%20adversarial%0Atraining%20to%20make%20the%20features%20from%20the%20source%20and%20target%20domains%20preserve%0Adomain-invariant%20characeristics%20as%20much%20as%20possible.%20Finally%2C%20at%20the%20pose%0Alevel%2C%20a%20self-supervised%20approach%20is%20utilized%20to%20enable%20the%20model%20to%20learn%0Adiverse%20knowledge%2C%20implicitly%20addressing%20the%20domain%20gap.%20Experimental%20results%0Ademonstrate%20that%20significant%20imrovement%20can%20be%20achieved%20by%20the%20proposed%0Amulti-level%20alignment%20method%20in%20pose%20estimation%2C%20which%20outperforms%20previous%0Astate-of-the-art%20in%20human%20pose%20by%20up%20to%202.4%25%20and%20animal%20pose%20estimation%20by%20up%0Ato%203.1%25%20for%20dogs%20and%201.4%25%20for%20sheep.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14885v1&entry.124074799=Read"},
{"title": "Multi-Session SLAM with Differentiable Wide-Baseline Pose Optimization", "author": "Lahav Lipson and Jia Deng", "abstract": "  We introduce a new system for Multi-Session SLAM, which tracks camera motion\nacross multiple disjoint videos under a single global reference. Our approach\ncouples the prediction of optical flow with solver layers to estimate camera\npose. The backbone is trained end-to-end using a novel differentiable solver\nfor wide-baseline two-view pose. The full system can connect disjoint\nsequences, perform visual odometry, and global optimization. Compared to\nexisting approaches, our design is accurate and robust to catastrophic\nfailures. Code is available at github.com/princeton-vl/MultiSlam_DiffPose\n", "link": "http://arxiv.org/abs/2404.15263v1", "date": "2024-04-23", "relevancy": 2.2568, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5894}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Session%20SLAM%20with%20Differentiable%20Wide-Baseline%20Pose%20Optimization&body=Title%3A%20Multi-Session%20SLAM%20with%20Differentiable%20Wide-Baseline%20Pose%20Optimization%0AAuthor%3A%20Lahav%20Lipson%20and%20Jia%20Deng%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20system%20for%20Multi-Session%20SLAM%2C%20which%20tracks%20camera%20motion%0Aacross%20multiple%20disjoint%20videos%20under%20a%20single%20global%20reference.%20Our%20approach%0Acouples%20the%20prediction%20of%20optical%20flow%20with%20solver%20layers%20to%20estimate%20camera%0Apose.%20The%20backbone%20is%20trained%20end-to-end%20using%20a%20novel%20differentiable%20solver%0Afor%20wide-baseline%20two-view%20pose.%20The%20full%20system%20can%20connect%20disjoint%0Asequences%2C%20perform%20visual%20odometry%2C%20and%20global%20optimization.%20Compared%20to%0Aexisting%20approaches%2C%20our%20design%20is%20accurate%20and%20robust%20to%20catastrophic%0Afailures.%20Code%20is%20available%20at%20github.com/princeton-vl/MultiSlam_DiffPose%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15263v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Session%20SLAM%20with%20Differentiable%20Wide-Baseline%20Pose%20Optimization&entry.906535625=Lahav%20Lipson%20and%20Jia%20Deng&entry.1292438233=%20%20We%20introduce%20a%20new%20system%20for%20Multi-Session%20SLAM%2C%20which%20tracks%20camera%20motion%0Aacross%20multiple%20disjoint%20videos%20under%20a%20single%20global%20reference.%20Our%20approach%0Acouples%20the%20prediction%20of%20optical%20flow%20with%20solver%20layers%20to%20estimate%20camera%0Apose.%20The%20backbone%20is%20trained%20end-to-end%20using%20a%20novel%20differentiable%20solver%0Afor%20wide-baseline%20two-view%20pose.%20The%20full%20system%20can%20connect%20disjoint%0Asequences%2C%20perform%20visual%20odometry%2C%20and%20global%20optimization.%20Compared%20to%0Aexisting%20approaches%2C%20our%20design%20is%20accurate%20and%20robust%20to%20catastrophic%0Afailures.%20Code%20is%20available%20at%20github.com/princeton-vl/MultiSlam_DiffPose%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15263v1&entry.124074799=Read"},
{"title": "Deep Models for Multi-View 3D Object Recognition: A Review", "author": "Mona Alzahrani and Muhammad Usman and Salma Kammoun and Saeed Anwar and Tarek Helmy", "abstract": "  Human decision-making often relies on visual information from multiple\nperspectives or views. In contrast, machine learning-based object recognition\nutilizes information from a single image of the object. However, the\ninformation conveyed by a single image may not be sufficient for accurate\ndecision-making, particularly in complex recognition problems. The utilization\nof multi-view 3D representations for object recognition has thus far\ndemonstrated the most promising results for achieving state-of-the-art\nperformance. This review paper comprehensively covers recent progress in\nmulti-view 3D object recognition methods for 3D classification and retrieval\ntasks. Specifically, we focus on deep learning-based and transformer-based\ntechniques, as they are widely utilized and have achieved state-of-the-art\nperformance. We provide detailed information about existing deep learning-based\nand transformer-based multi-view 3D object recognition models, including the\nmost commonly used 3D datasets, camera configurations and number of views, view\nselection strategies, pre-trained CNN architectures, fusion strategies, and\nrecognition performance on 3D classification and 3D retrieval tasks.\nAdditionally, we examine various computer vision applications that use\nmulti-view classification. Finally, we highlight key findings and future\ndirections for developing multi-view 3D object recognition methods to provide\nreaders with a comprehensive understanding of the field.\n", "link": "http://arxiv.org/abs/2404.15224v1", "date": "2024-04-23", "relevancy": 2.2466, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5936}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5456}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5361}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Models%20for%20Multi-View%203D%20Object%20Recognition%3A%20A%20Review&body=Title%3A%20Deep%20Models%20for%20Multi-View%203D%20Object%20Recognition%3A%20A%20Review%0AAuthor%3A%20Mona%20Alzahrani%20and%20Muhammad%20Usman%20and%20Salma%20Kammoun%20and%20Saeed%20Anwar%20and%20Tarek%20Helmy%0AAbstract%3A%20%20%20Human%20decision-making%20often%20relies%20on%20visual%20information%20from%20multiple%0Aperspectives%20or%20views.%20In%20contrast%2C%20machine%20learning-based%20object%20recognition%0Autilizes%20information%20from%20a%20single%20image%20of%20the%20object.%20However%2C%20the%0Ainformation%20conveyed%20by%20a%20single%20image%20may%20not%20be%20sufficient%20for%20accurate%0Adecision-making%2C%20particularly%20in%20complex%20recognition%20problems.%20The%20utilization%0Aof%20multi-view%203D%20representations%20for%20object%20recognition%20has%20thus%20far%0Ademonstrated%20the%20most%20promising%20results%20for%20achieving%20state-of-the-art%0Aperformance.%20This%20review%20paper%20comprehensively%20covers%20recent%20progress%20in%0Amulti-view%203D%20object%20recognition%20methods%20for%203D%20classification%20and%20retrieval%0Atasks.%20Specifically%2C%20we%20focus%20on%20deep%20learning-based%20and%20transformer-based%0Atechniques%2C%20as%20they%20are%20widely%20utilized%20and%20have%20achieved%20state-of-the-art%0Aperformance.%20We%20provide%20detailed%20information%20about%20existing%20deep%20learning-based%0Aand%20transformer-based%20multi-view%203D%20object%20recognition%20models%2C%20including%20the%0Amost%20commonly%20used%203D%20datasets%2C%20camera%20configurations%20and%20number%20of%20views%2C%20view%0Aselection%20strategies%2C%20pre-trained%20CNN%20architectures%2C%20fusion%20strategies%2C%20and%0Arecognition%20performance%20on%203D%20classification%20and%203D%20retrieval%20tasks.%0AAdditionally%2C%20we%20examine%20various%20computer%20vision%20applications%20that%20use%0Amulti-view%20classification.%20Finally%2C%20we%20highlight%20key%20findings%20and%20future%0Adirections%20for%20developing%20multi-view%203D%20object%20recognition%20methods%20to%20provide%0Areaders%20with%20a%20comprehensive%20understanding%20of%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15224v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Models%20for%20Multi-View%203D%20Object%20Recognition%3A%20A%20Review&entry.906535625=Mona%20Alzahrani%20and%20Muhammad%20Usman%20and%20Salma%20Kammoun%20and%20Saeed%20Anwar%20and%20Tarek%20Helmy&entry.1292438233=%20%20Human%20decision-making%20often%20relies%20on%20visual%20information%20from%20multiple%0Aperspectives%20or%20views.%20In%20contrast%2C%20machine%20learning-based%20object%20recognition%0Autilizes%20information%20from%20a%20single%20image%20of%20the%20object.%20However%2C%20the%0Ainformation%20conveyed%20by%20a%20single%20image%20may%20not%20be%20sufficient%20for%20accurate%0Adecision-making%2C%20particularly%20in%20complex%20recognition%20problems.%20The%20utilization%0Aof%20multi-view%203D%20representations%20for%20object%20recognition%20has%20thus%20far%0Ademonstrated%20the%20most%20promising%20results%20for%20achieving%20state-of-the-art%0Aperformance.%20This%20review%20paper%20comprehensively%20covers%20recent%20progress%20in%0Amulti-view%203D%20object%20recognition%20methods%20for%203D%20classification%20and%20retrieval%0Atasks.%20Specifically%2C%20we%20focus%20on%20deep%20learning-based%20and%20transformer-based%0Atechniques%2C%20as%20they%20are%20widely%20utilized%20and%20have%20achieved%20state-of-the-art%0Aperformance.%20We%20provide%20detailed%20information%20about%20existing%20deep%20learning-based%0Aand%20transformer-based%20multi-view%203D%20object%20recognition%20models%2C%20including%20the%0Amost%20commonly%20used%203D%20datasets%2C%20camera%20configurations%20and%20number%20of%20views%2C%20view%0Aselection%20strategies%2C%20pre-trained%20CNN%20architectures%2C%20fusion%20strategies%2C%20and%0Arecognition%20performance%20on%203D%20classification%20and%203D%20retrieval%20tasks.%0AAdditionally%2C%20we%20examine%20various%20computer%20vision%20applications%20that%20use%0Amulti-view%20classification.%20Finally%2C%20we%20highlight%20key%20findings%20and%20future%0Adirections%20for%20developing%20multi-view%203D%20object%20recognition%20methods%20to%20provide%0Areaders%20with%20a%20comprehensive%20understanding%20of%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15224v1&entry.124074799=Read"},
{"title": "Fully Automatic Neural Network Reduction for Formal Verification", "author": "Tobias Ladner and Matthias Althoff", "abstract": "  Formal verification of neural networks is essential before their deployment\nin safety-critical applications. However, existing methods for formally\nverifying neural networks are not yet scalable enough to handle practical\nproblems involving a large number of neurons. We address this challenge by\nintroducing a fully automatic and sound reduction of neural networks using\nreachability analysis. The soundness ensures that the verification of the\nreduced network entails the verification of the original network. To the best\nof our knowledge, we present the first sound reduction approach that is\napplicable to neural networks with any type of element-wise activation\nfunction, such as ReLU, sigmoid, and tanh. The network reduction is computed on\nthe fly while simultaneously verifying the original network and its\nspecifications. All parameters are automatically tuned to minimize the network\nsize without compromising verifiability. We further show the applicability of\nour approach to convolutional neural networks by explicitly exploiting similar\nneighboring pixels. Our evaluation shows that our approach can reduce the\nnumber of neurons to a fraction of the original number of neurons with minor\nouter-approximation and thus reduce the verification time to a similar degree.\n", "link": "http://arxiv.org/abs/2305.01932v2", "date": "2024-04-23", "relevancy": 2.2411, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4575}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4438}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4434}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fully%20Automatic%20Neural%20Network%20Reduction%20for%20Formal%20Verification&body=Title%3A%20Fully%20Automatic%20Neural%20Network%20Reduction%20for%20Formal%20Verification%0AAuthor%3A%20Tobias%20Ladner%20and%20Matthias%20Althoff%0AAbstract%3A%20%20%20Formal%20verification%20of%20neural%20networks%20is%20essential%20before%20their%20deployment%0Ain%20safety-critical%20applications.%20However%2C%20existing%20methods%20for%20formally%0Averifying%20neural%20networks%20are%20not%20yet%20scalable%20enough%20to%20handle%20practical%0Aproblems%20involving%20a%20large%20number%20of%20neurons.%20We%20address%20this%20challenge%20by%0Aintroducing%20a%20fully%20automatic%20and%20sound%20reduction%20of%20neural%20networks%20using%0Areachability%20analysis.%20The%20soundness%20ensures%20that%20the%20verification%20of%20the%0Areduced%20network%20entails%20the%20verification%20of%20the%20original%20network.%20To%20the%20best%0Aof%20our%20knowledge%2C%20we%20present%20the%20first%20sound%20reduction%20approach%20that%20is%0Aapplicable%20to%20neural%20networks%20with%20any%20type%20of%20element-wise%20activation%0Afunction%2C%20such%20as%20ReLU%2C%20sigmoid%2C%20and%20tanh.%20The%20network%20reduction%20is%20computed%20on%0Athe%20fly%20while%20simultaneously%20verifying%20the%20original%20network%20and%20its%0Aspecifications.%20All%20parameters%20are%20automatically%20tuned%20to%20minimize%20the%20network%0Asize%20without%20compromising%20verifiability.%20We%20further%20show%20the%20applicability%20of%0Aour%20approach%20to%20convolutional%20neural%20networks%20by%20explicitly%20exploiting%20similar%0Aneighboring%20pixels.%20Our%20evaluation%20shows%20that%20our%20approach%20can%20reduce%20the%0Anumber%20of%20neurons%20to%20a%20fraction%20of%20the%20original%20number%20of%20neurons%20with%20minor%0Aouter-approximation%20and%20thus%20reduce%20the%20verification%20time%20to%20a%20similar%20degree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.01932v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Automatic%20Neural%20Network%20Reduction%20for%20Formal%20Verification&entry.906535625=Tobias%20Ladner%20and%20Matthias%20Althoff&entry.1292438233=%20%20Formal%20verification%20of%20neural%20networks%20is%20essential%20before%20their%20deployment%0Ain%20safety-critical%20applications.%20However%2C%20existing%20methods%20for%20formally%0Averifying%20neural%20networks%20are%20not%20yet%20scalable%20enough%20to%20handle%20practical%0Aproblems%20involving%20a%20large%20number%20of%20neurons.%20We%20address%20this%20challenge%20by%0Aintroducing%20a%20fully%20automatic%20and%20sound%20reduction%20of%20neural%20networks%20using%0Areachability%20analysis.%20The%20soundness%20ensures%20that%20the%20verification%20of%20the%0Areduced%20network%20entails%20the%20verification%20of%20the%20original%20network.%20To%20the%20best%0Aof%20our%20knowledge%2C%20we%20present%20the%20first%20sound%20reduction%20approach%20that%20is%0Aapplicable%20to%20neural%20networks%20with%20any%20type%20of%20element-wise%20activation%0Afunction%2C%20such%20as%20ReLU%2C%20sigmoid%2C%20and%20tanh.%20The%20network%20reduction%20is%20computed%20on%0Athe%20fly%20while%20simultaneously%20verifying%20the%20original%20network%20and%20its%0Aspecifications.%20All%20parameters%20are%20automatically%20tuned%20to%20minimize%20the%20network%0Asize%20without%20compromising%20verifiability.%20We%20further%20show%20the%20applicability%20of%0Aour%20approach%20to%20convolutional%20neural%20networks%20by%20explicitly%20exploiting%20similar%0Aneighboring%20pixels.%20Our%20evaluation%20shows%20that%20our%20approach%20can%20reduce%20the%0Anumber%20of%20neurons%20to%20a%20fraction%20of%20the%20original%20number%20of%20neurons%20with%20minor%0Aouter-approximation%20and%20thus%20reduce%20the%20verification%20time%20to%20a%20similar%20degree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.01932v2&entry.124074799=Read"},
{"title": "Source-free Domain Adaptation for Video Object Detection Under Adverse\n  Image Conditions", "author": "Xingguang Zhang and Chih-Hsien Chou", "abstract": "  When deploying pre-trained video object detectors in real-world scenarios,\nthe domain gap between training and testing data caused by adverse image\nconditions often leads to performance degradation. Addressing this issue\nbecomes particularly challenging when only the pre-trained model and degraded\nvideos are available. Although various source-free domain adaptation (SFDA)\nmethods have been proposed for single-frame object detectors, SFDA for video\nobject detection (VOD) remains unexplored. Moreover, most unsupervised domain\nadaptation works for object detection rely on two-stage detectors, while SFDA\nfor one-stage detectors, which are more vulnerable to fine-tuning, is not well\naddressed in the literature. In this paper, we propose Spatial-Temporal\nAlternate Refinement with Mean Teacher (STAR-MT), a simple yet effective SFDA\nmethod for VOD. Specifically, we aim to improve the performance of the\none-stage VOD method, YOLOV, under adverse image conditions, including noise,\nair turbulence, and haze. Extensive experiments on the ImageNetVOD dataset and\nits degraded versions demonstrate that our method consistently improves video\nobject detection performance in challenging imaging conditions, showcasing its\npotential for real-world applications.\n", "link": "http://arxiv.org/abs/2404.15252v1", "date": "2024-04-23", "relevancy": 2.2374, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5626}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5608}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5475}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Source-free%20Domain%20Adaptation%20for%20Video%20Object%20Detection%20Under%20Adverse%0A%20%20Image%20Conditions&body=Title%3A%20Source-free%20Domain%20Adaptation%20for%20Video%20Object%20Detection%20Under%20Adverse%0A%20%20Image%20Conditions%0AAuthor%3A%20Xingguang%20Zhang%20and%20Chih-Hsien%20Chou%0AAbstract%3A%20%20%20When%20deploying%20pre-trained%20video%20object%20detectors%20in%20real-world%20scenarios%2C%0Athe%20domain%20gap%20between%20training%20and%20testing%20data%20caused%20by%20adverse%20image%0Aconditions%20often%20leads%20to%20performance%20degradation.%20Addressing%20this%20issue%0Abecomes%20particularly%20challenging%20when%20only%20the%20pre-trained%20model%20and%20degraded%0Avideos%20are%20available.%20Although%20various%20source-free%20domain%20adaptation%20%28SFDA%29%0Amethods%20have%20been%20proposed%20for%20single-frame%20object%20detectors%2C%20SFDA%20for%20video%0Aobject%20detection%20%28VOD%29%20remains%20unexplored.%20Moreover%2C%20most%20unsupervised%20domain%0Aadaptation%20works%20for%20object%20detection%20rely%20on%20two-stage%20detectors%2C%20while%20SFDA%0Afor%20one-stage%20detectors%2C%20which%20are%20more%20vulnerable%20to%20fine-tuning%2C%20is%20not%20well%0Aaddressed%20in%20the%20literature.%20In%20this%20paper%2C%20we%20propose%20Spatial-Temporal%0AAlternate%20Refinement%20with%20Mean%20Teacher%20%28STAR-MT%29%2C%20a%20simple%20yet%20effective%20SFDA%0Amethod%20for%20VOD.%20Specifically%2C%20we%20aim%20to%20improve%20the%20performance%20of%20the%0Aone-stage%20VOD%20method%2C%20YOLOV%2C%20under%20adverse%20image%20conditions%2C%20including%20noise%2C%0Aair%20turbulence%2C%20and%20haze.%20Extensive%20experiments%20on%20the%20ImageNetVOD%20dataset%20and%0Aits%20degraded%20versions%20demonstrate%20that%20our%20method%20consistently%20improves%20video%0Aobject%20detection%20performance%20in%20challenging%20imaging%20conditions%2C%20showcasing%20its%0Apotential%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15252v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source-free%20Domain%20Adaptation%20for%20Video%20Object%20Detection%20Under%20Adverse%0A%20%20Image%20Conditions&entry.906535625=Xingguang%20Zhang%20and%20Chih-Hsien%20Chou&entry.1292438233=%20%20When%20deploying%20pre-trained%20video%20object%20detectors%20in%20real-world%20scenarios%2C%0Athe%20domain%20gap%20between%20training%20and%20testing%20data%20caused%20by%20adverse%20image%0Aconditions%20often%20leads%20to%20performance%20degradation.%20Addressing%20this%20issue%0Abecomes%20particularly%20challenging%20when%20only%20the%20pre-trained%20model%20and%20degraded%0Avideos%20are%20available.%20Although%20various%20source-free%20domain%20adaptation%20%28SFDA%29%0Amethods%20have%20been%20proposed%20for%20single-frame%20object%20detectors%2C%20SFDA%20for%20video%0Aobject%20detection%20%28VOD%29%20remains%20unexplored.%20Moreover%2C%20most%20unsupervised%20domain%0Aadaptation%20works%20for%20object%20detection%20rely%20on%20two-stage%20detectors%2C%20while%20SFDA%0Afor%20one-stage%20detectors%2C%20which%20are%20more%20vulnerable%20to%20fine-tuning%2C%20is%20not%20well%0Aaddressed%20in%20the%20literature.%20In%20this%20paper%2C%20we%20propose%20Spatial-Temporal%0AAlternate%20Refinement%20with%20Mean%20Teacher%20%28STAR-MT%29%2C%20a%20simple%20yet%20effective%20SFDA%0Amethod%20for%20VOD.%20Specifically%2C%20we%20aim%20to%20improve%20the%20performance%20of%20the%0Aone-stage%20VOD%20method%2C%20YOLOV%2C%20under%20adverse%20image%20conditions%2C%20including%20noise%2C%0Aair%20turbulence%2C%20and%20haze.%20Extensive%20experiments%20on%20the%20ImageNetVOD%20dataset%20and%0Aits%20degraded%20versions%20demonstrate%20that%20our%20method%20consistently%20improves%20video%0Aobject%20detection%20performance%20in%20challenging%20imaging%20conditions%2C%20showcasing%20its%0Apotential%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15252v1&entry.124074799=Read"},
{"title": "FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient\n  Descent", "author": "Cameron Smith and David Charatan and Ayush Tewari and Vincent Sitzmann", "abstract": "  This paper introduces FlowMap, an end-to-end differentiable method that\nsolves for precise camera poses, camera intrinsics, and per-frame dense depth\nof a video sequence. Our method performs per-video gradient-descent\nminimization of a simple least-squares objective that compares the optical flow\ninduced by depth, intrinsics, and poses against correspondences obtained via\noff-the-shelf optical flow and point tracking. Alongside the use of point\ntracks to encourage long-term geometric consistency, we introduce\ndifferentiable re-parameterizations of depth, intrinsics, and pose that are\namenable to first-order optimization. We empirically show that camera\nparameters and dense depth recovered by our method enable photo-realistic novel\nview synthesis on 360-degree trajectories using Gaussian Splatting. Our method\nnot only far outperforms prior gradient-descent based bundle adjustment\nmethods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM\nmethod, on the downstream task of 360-degree novel view synthesis (even though\nour method is purely gradient-descent based, fully differentiable, and presents\na complete departure from conventional SfM).\n", "link": "http://arxiv.org/abs/2404.15259v1", "date": "2024-04-23", "relevancy": 2.2299, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5679}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5464}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FlowMap%3A%20High-Quality%20Camera%20Poses%2C%20Intrinsics%2C%20and%20Depth%20via%20Gradient%0A%20%20Descent&body=Title%3A%20FlowMap%3A%20High-Quality%20Camera%20Poses%2C%20Intrinsics%2C%20and%20Depth%20via%20Gradient%0A%20%20Descent%0AAuthor%3A%20Cameron%20Smith%20and%20David%20Charatan%20and%20Ayush%20Tewari%20and%20Vincent%20Sitzmann%0AAbstract%3A%20%20%20This%20paper%20introduces%20FlowMap%2C%20an%20end-to-end%20differentiable%20method%20that%0Asolves%20for%20precise%20camera%20poses%2C%20camera%20intrinsics%2C%20and%20per-frame%20dense%20depth%0Aof%20a%20video%20sequence.%20Our%20method%20performs%20per-video%20gradient-descent%0Aminimization%20of%20a%20simple%20least-squares%20objective%20that%20compares%20the%20optical%20flow%0Ainduced%20by%20depth%2C%20intrinsics%2C%20and%20poses%20against%20correspondences%20obtained%20via%0Aoff-the-shelf%20optical%20flow%20and%20point%20tracking.%20Alongside%20the%20use%20of%20point%0Atracks%20to%20encourage%20long-term%20geometric%20consistency%2C%20we%20introduce%0Adifferentiable%20re-parameterizations%20of%20depth%2C%20intrinsics%2C%20and%20pose%20that%20are%0Aamenable%20to%20first-order%20optimization.%20We%20empirically%20show%20that%20camera%0Aparameters%20and%20dense%20depth%20recovered%20by%20our%20method%20enable%20photo-realistic%20novel%0Aview%20synthesis%20on%20360-degree%20trajectories%20using%20Gaussian%20Splatting.%20Our%20method%0Anot%20only%20far%20outperforms%20prior%20gradient-descent%20based%20bundle%20adjustment%0Amethods%2C%20but%20surprisingly%20performs%20on%20par%20with%20COLMAP%2C%20the%20state-of-the-art%20SfM%0Amethod%2C%20on%20the%20downstream%20task%20of%20360-degree%20novel%20view%20synthesis%20%28even%20though%0Aour%20method%20is%20purely%20gradient-descent%20based%2C%20fully%20differentiable%2C%20and%20presents%0Aa%20complete%20departure%20from%20conventional%20SfM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15259v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowMap%3A%20High-Quality%20Camera%20Poses%2C%20Intrinsics%2C%20and%20Depth%20via%20Gradient%0A%20%20Descent&entry.906535625=Cameron%20Smith%20and%20David%20Charatan%20and%20Ayush%20Tewari%20and%20Vincent%20Sitzmann&entry.1292438233=%20%20This%20paper%20introduces%20FlowMap%2C%20an%20end-to-end%20differentiable%20method%20that%0Asolves%20for%20precise%20camera%20poses%2C%20camera%20intrinsics%2C%20and%20per-frame%20dense%20depth%0Aof%20a%20video%20sequence.%20Our%20method%20performs%20per-video%20gradient-descent%0Aminimization%20of%20a%20simple%20least-squares%20objective%20that%20compares%20the%20optical%20flow%0Ainduced%20by%20depth%2C%20intrinsics%2C%20and%20poses%20against%20correspondences%20obtained%20via%0Aoff-the-shelf%20optical%20flow%20and%20point%20tracking.%20Alongside%20the%20use%20of%20point%0Atracks%20to%20encourage%20long-term%20geometric%20consistency%2C%20we%20introduce%0Adifferentiable%20re-parameterizations%20of%20depth%2C%20intrinsics%2C%20and%20pose%20that%20are%0Aamenable%20to%20first-order%20optimization.%20We%20empirically%20show%20that%20camera%0Aparameters%20and%20dense%20depth%20recovered%20by%20our%20method%20enable%20photo-realistic%20novel%0Aview%20synthesis%20on%20360-degree%20trajectories%20using%20Gaussian%20Splatting.%20Our%20method%0Anot%20only%20far%20outperforms%20prior%20gradient-descent%20based%20bundle%20adjustment%0Amethods%2C%20but%20surprisingly%20performs%20on%20par%20with%20COLMAP%2C%20the%20state-of-the-art%20SfM%0Amethod%2C%20on%20the%20downstream%20task%20of%20360-degree%20novel%20view%20synthesis%20%28even%20though%0Aour%20method%20is%20purely%20gradient-descent%20based%2C%20fully%20differentiable%2C%20and%20presents%0Aa%20complete%20departure%20from%20conventional%20SfM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15259v1&entry.124074799=Read"},
{"title": "CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and\n  Radiology Reports for Full-Body Scenarios", "author": "Jingyang Lin and Yingda Xia and Jianpeng Zhang and Ke Yan and Le Lu and Jiebo Luo and Ling Zhang", "abstract": "  Medical Vision-Language Pretraining (Med-VLP) establishes a connection\nbetween visual content from medical images and the relevant textual\ndescriptions. Existing Med-VLP methods primarily focus on 2D images depicting a\nsingle body part, notably chest X-rays. In this paper, we extend the scope of\nMed-VLP to encompass 3D images, specifically targeting full-body scenarios, by\nusing a multimodal dataset of CT images and reports. Compared with the 2D\ncounterpart, 3D VLP is required to effectively capture essential semantics from\nsignificantly sparser representation in 3D imaging. In this paper, we introduce\nCT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method\nthat constructs organ-level image-text pairs to enhance multimodal contrastive\nlearning, aligning grounded visual features with precise diagnostic text.\nAdditionally, we developed an abnormality dictionary to augment contrastive\nlearning with diverse negative samples. Our method, trained on a multimodal CT\ndataset comprising 44,011 organ-level vision-text pairs from 17,702 patients\nacross 104 organs, demonstrates it can identify organs and abnormalities in a\nzero-shot manner using natural languages. The performance of CT-GLIP is\nvalidated on a separate test set of 1,130 patients, focusing on the 16 most\nfrequent abnormalities across 7 organs. The experimental results show our\nmodel's superior performance over the standard CLIP framework across zero-shot\nand fine-tuning scenarios, using both CNN and ViT architectures.\n", "link": "http://arxiv.org/abs/2404.15272v1", "date": "2024-04-23", "relevancy": 2.2297, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6227}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5382}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4999}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CT-GLIP%3A%203D%20Grounded%20Language-Image%20Pretraining%20with%20CT%20Scans%20and%0A%20%20Radiology%20Reports%20for%20Full-Body%20Scenarios&body=Title%3A%20CT-GLIP%3A%203D%20Grounded%20Language-Image%20Pretraining%20with%20CT%20Scans%20and%0A%20%20Radiology%20Reports%20for%20Full-Body%20Scenarios%0AAuthor%3A%20Jingyang%20Lin%20and%20Yingda%20Xia%20and%20Jianpeng%20Zhang%20and%20Ke%20Yan%20and%20Le%20Lu%20and%20Jiebo%20Luo%20and%20Ling%20Zhang%0AAbstract%3A%20%20%20Medical%20Vision-Language%20Pretraining%20%28Med-VLP%29%20establishes%20a%20connection%0Abetween%20visual%20content%20from%20medical%20images%20and%20the%20relevant%20textual%0Adescriptions.%20Existing%20Med-VLP%20methods%20primarily%20focus%20on%202D%20images%20depicting%20a%0Asingle%20body%20part%2C%20notably%20chest%20X-rays.%20In%20this%20paper%2C%20we%20extend%20the%20scope%20of%0AMed-VLP%20to%20encompass%203D%20images%2C%20specifically%20targeting%20full-body%20scenarios%2C%20by%0Ausing%20a%20multimodal%20dataset%20of%20CT%20images%20and%20reports.%20Compared%20with%20the%202D%0Acounterpart%2C%203D%20VLP%20is%20required%20to%20effectively%20capture%20essential%20semantics%20from%0Asignificantly%20sparser%20representation%20in%203D%20imaging.%20In%20this%20paper%2C%20we%20introduce%0ACT-GLIP%20%28Grounded%20Language-Image%20Pretraining%20with%20CT%20scans%29%2C%20a%20novel%20method%0Athat%20constructs%20organ-level%20image-text%20pairs%20to%20enhance%20multimodal%20contrastive%0Alearning%2C%20aligning%20grounded%20visual%20features%20with%20precise%20diagnostic%20text.%0AAdditionally%2C%20we%20developed%20an%20abnormality%20dictionary%20to%20augment%20contrastive%0Alearning%20with%20diverse%20negative%20samples.%20Our%20method%2C%20trained%20on%20a%20multimodal%20CT%0Adataset%20comprising%2044%2C011%20organ-level%20vision-text%20pairs%20from%2017%2C702%20patients%0Aacross%20104%20organs%2C%20demonstrates%20it%20can%20identify%20organs%20and%20abnormalities%20in%20a%0Azero-shot%20manner%20using%20natural%20languages.%20The%20performance%20of%20CT-GLIP%20is%0Avalidated%20on%20a%20separate%20test%20set%20of%201%2C130%20patients%2C%20focusing%20on%20the%2016%20most%0Afrequent%20abnormalities%20across%207%20organs.%20The%20experimental%20results%20show%20our%0Amodel%27s%20superior%20performance%20over%20the%20standard%20CLIP%20framework%20across%20zero-shot%0Aand%20fine-tuning%20scenarios%2C%20using%20both%20CNN%20and%20ViT%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15272v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-GLIP%3A%203D%20Grounded%20Language-Image%20Pretraining%20with%20CT%20Scans%20and%0A%20%20Radiology%20Reports%20for%20Full-Body%20Scenarios&entry.906535625=Jingyang%20Lin%20and%20Yingda%20Xia%20and%20Jianpeng%20Zhang%20and%20Ke%20Yan%20and%20Le%20Lu%20and%20Jiebo%20Luo%20and%20Ling%20Zhang&entry.1292438233=%20%20Medical%20Vision-Language%20Pretraining%20%28Med-VLP%29%20establishes%20a%20connection%0Abetween%20visual%20content%20from%20medical%20images%20and%20the%20relevant%20textual%0Adescriptions.%20Existing%20Med-VLP%20methods%20primarily%20focus%20on%202D%20images%20depicting%20a%0Asingle%20body%20part%2C%20notably%20chest%20X-rays.%20In%20this%20paper%2C%20we%20extend%20the%20scope%20of%0AMed-VLP%20to%20encompass%203D%20images%2C%20specifically%20targeting%20full-body%20scenarios%2C%20by%0Ausing%20a%20multimodal%20dataset%20of%20CT%20images%20and%20reports.%20Compared%20with%20the%202D%0Acounterpart%2C%203D%20VLP%20is%20required%20to%20effectively%20capture%20essential%20semantics%20from%0Asignificantly%20sparser%20representation%20in%203D%20imaging.%20In%20this%20paper%2C%20we%20introduce%0ACT-GLIP%20%28Grounded%20Language-Image%20Pretraining%20with%20CT%20scans%29%2C%20a%20novel%20method%0Athat%20constructs%20organ-level%20image-text%20pairs%20to%20enhance%20multimodal%20contrastive%0Alearning%2C%20aligning%20grounded%20visual%20features%20with%20precise%20diagnostic%20text.%0AAdditionally%2C%20we%20developed%20an%20abnormality%20dictionary%20to%20augment%20contrastive%0Alearning%20with%20diverse%20negative%20samples.%20Our%20method%2C%20trained%20on%20a%20multimodal%20CT%0Adataset%20comprising%2044%2C011%20organ-level%20vision-text%20pairs%20from%2017%2C702%20patients%0Aacross%20104%20organs%2C%20demonstrates%20it%20can%20identify%20organs%20and%20abnormalities%20in%20a%0Azero-shot%20manner%20using%20natural%20languages.%20The%20performance%20of%20CT-GLIP%20is%0Avalidated%20on%20a%20separate%20test%20set%20of%201%2C130%20patients%2C%20focusing%20on%20the%2016%20most%0Afrequent%20abnormalities%20across%207%20organs.%20The%20experimental%20results%20show%20our%0Amodel%27s%20superior%20performance%20over%20the%20standard%20CLIP%20framework%20across%20zero-shot%0Aand%20fine-tuning%20scenarios%2C%20using%20both%20CNN%20and%20ViT%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15272v1&entry.124074799=Read"},
{"title": "VideoXum: Cross-modal Visual and Textural Summarization of Videos", "author": "Jingyang Lin and Hang Hua and Ming Chen and Yikang Li and Jenhao Hsiao and Chiuman Ho and Jiebo Luo", "abstract": "  Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n", "link": "http://arxiv.org/abs/2303.12060v3", "date": "2024-04-23", "relevancy": 2.2216, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5724}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5637}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5351}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VideoXum%3A%20Cross-modal%20Visual%20and%20Textural%20Summarization%20of%20Videos&body=Title%3A%20VideoXum%3A%20Cross-modal%20Visual%20and%20Textural%20Summarization%20of%20Videos%0AAuthor%3A%20Jingyang%20Lin%20and%20Hang%20Hua%20and%20Ming%20Chen%20and%20Yikang%20Li%20and%20Jenhao%20Hsiao%20and%20Chiuman%20Ho%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Video%20summarization%20aims%20to%20distill%20the%20most%20important%20information%20from%20a%0Asource%20video%20to%20produce%20either%20an%20abridged%20clip%20or%20a%20textual%20narrative.%0ATraditionally%2C%20different%20methods%20have%20been%20proposed%20depending%20on%20whether%20the%0Aoutput%20is%20a%20video%20or%20text%2C%20thus%20ignoring%20the%20correlation%20between%20the%20two%0Asemantically%20related%20tasks%20of%20visual%20summarization%20and%20textual%20summarization.%0AWe%20propose%20a%20new%20joint%20video%20and%20text%20summarization%20task.%20The%20goal%20is%20to%0Agenerate%20both%20a%20shortened%20video%20clip%20along%20with%20the%20corresponding%20textual%0Asummary%20from%20a%20long%20video%2C%20collectively%20referred%20to%20as%20a%20cross-modal%20summary.%0AThe%20generated%20shortened%20video%20clip%20and%20text%20narratives%20should%20be%20semantically%0Awell%20aligned.%20To%20this%20end%2C%20we%20first%20build%20a%20large-scale%20human-annotated%20dataset%0A--%20VideoXum%20%28X%20refers%20to%20different%20modalities%29.%20The%20dataset%20is%20reannotated%0Abased%20on%20ActivityNet.%20After%20we%20filter%20out%20the%20videos%20that%20do%20not%20meet%20the%0Alength%20requirements%2C%2014%2C001%20long%20videos%20remain%20in%20our%20new%20dataset.%20Each%20video%0Ain%20our%20reannotated%20dataset%20has%20human-annotated%20video%20summaries%20and%20the%0Acorresponding%20narrative%20summaries.%20We%20then%20design%20a%20novel%20end-to-end%20model%20--%0AVTSUM-BILP%20to%20address%20the%20challenges%20of%20our%20proposed%20task.%20Moreover%2C%20we%20propose%0Aa%20new%20metric%20called%20VT-CLIPScore%20to%20help%20evaluate%20the%20semantic%20consistency%20of%0Across-modality%20summary.%20The%20proposed%20model%20achieves%20promising%20performance%20on%0Athis%20new%20task%20and%20establishes%20a%20benchmark%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.12060v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoXum%3A%20Cross-modal%20Visual%20and%20Textural%20Summarization%20of%20Videos&entry.906535625=Jingyang%20Lin%20and%20Hang%20Hua%20and%20Ming%20Chen%20and%20Yikang%20Li%20and%20Jenhao%20Hsiao%20and%20Chiuman%20Ho%20and%20Jiebo%20Luo&entry.1292438233=%20%20Video%20summarization%20aims%20to%20distill%20the%20most%20important%20information%20from%20a%0Asource%20video%20to%20produce%20either%20an%20abridged%20clip%20or%20a%20textual%20narrative.%0ATraditionally%2C%20different%20methods%20have%20been%20proposed%20depending%20on%20whether%20the%0Aoutput%20is%20a%20video%20or%20text%2C%20thus%20ignoring%20the%20correlation%20between%20the%20two%0Asemantically%20related%20tasks%20of%20visual%20summarization%20and%20textual%20summarization.%0AWe%20propose%20a%20new%20joint%20video%20and%20text%20summarization%20task.%20The%20goal%20is%20to%0Agenerate%20both%20a%20shortened%20video%20clip%20along%20with%20the%20corresponding%20textual%0Asummary%20from%20a%20long%20video%2C%20collectively%20referred%20to%20as%20a%20cross-modal%20summary.%0AThe%20generated%20shortened%20video%20clip%20and%20text%20narratives%20should%20be%20semantically%0Awell%20aligned.%20To%20this%20end%2C%20we%20first%20build%20a%20large-scale%20human-annotated%20dataset%0A--%20VideoXum%20%28X%20refers%20to%20different%20modalities%29.%20The%20dataset%20is%20reannotated%0Abased%20on%20ActivityNet.%20After%20we%20filter%20out%20the%20videos%20that%20do%20not%20meet%20the%0Alength%20requirements%2C%2014%2C001%20long%20videos%20remain%20in%20our%20new%20dataset.%20Each%20video%0Ain%20our%20reannotated%20dataset%20has%20human-annotated%20video%20summaries%20and%20the%0Acorresponding%20narrative%20summaries.%20We%20then%20design%20a%20novel%20end-to-end%20model%20--%0AVTSUM-BILP%20to%20address%20the%20challenges%20of%20our%20proposed%20task.%20Moreover%2C%20we%20propose%0Aa%20new%20metric%20called%20VT-CLIPScore%20to%20help%20evaluate%20the%20semantic%20consistency%20of%0Across-modality%20summary.%20The%20proposed%20model%20achieves%20promising%20performance%20on%0Athis%20new%20task%20and%20establishes%20a%20benchmark%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.12060v3&entry.124074799=Read"},
{"title": "RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose\n  Estimation", "author": "Pavel Petracek and Kostas Alexis and Martin Saska", "abstract": "  The typical point cloud sampling methods used in state estimation for mobile\nrobots preserve a high level of point redundancy. This redundancy unnecessarily\nslows down the estimation pipeline and may cause drift under real-time\nconstraints. Such undue latency becomes a bottleneck for resource-constrained\nrobots (especially UAVs), requiring minimal delay for agile and accurate\noperation. We propose a novel, deterministic, uninformed, and single-parameter\npoint cloud sampling method named RMS that minimizes redundancy within a 3D\npoint cloud. In contrast to the state of the art, RMS balances the\ntranslation-space observability by leveraging the fact that linear and planar\nsurfaces inherently exhibit high redundancy propagated into iterative\nestimation pipelines. We define the concept of gradient flow, quantifying the\nlocal surface underlying a point. We also show that maximizing the entropy of\nthe gradient flow minimizes point redundancy for robot ego-motion estimation.\nWe integrate RMS into the point-based KISS-ICP and feature-based LOAM odometry\npipelines and evaluate experimentally on KITTI, Hilti-Oxford, and custom\ndatasets from multirotor UAVs. The experiments demonstrate that RMS outperforms\nstate-of-the-art methods in speed, compression, and accuracy in\nwell-conditioned as well as in geometrically-degenerated settings.\n", "link": "http://arxiv.org/abs/2312.07337v3", "date": "2024-04-23", "relevancy": 2.2139, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.576}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5272}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RMS%3A%20Redundancy-Minimizing%20Point%20Cloud%20Sampling%20for%20Real-Time%20Pose%0A%20%20Estimation&body=Title%3A%20RMS%3A%20Redundancy-Minimizing%20Point%20Cloud%20Sampling%20for%20Real-Time%20Pose%0A%20%20Estimation%0AAuthor%3A%20Pavel%20Petracek%20and%20Kostas%20Alexis%20and%20Martin%20Saska%0AAbstract%3A%20%20%20The%20typical%20point%20cloud%20sampling%20methods%20used%20in%20state%20estimation%20for%20mobile%0Arobots%20preserve%20a%20high%20level%20of%20point%20redundancy.%20This%20redundancy%20unnecessarily%0Aslows%20down%20the%20estimation%20pipeline%20and%20may%20cause%20drift%20under%20real-time%0Aconstraints.%20Such%20undue%20latency%20becomes%20a%20bottleneck%20for%20resource-constrained%0Arobots%20%28especially%20UAVs%29%2C%20requiring%20minimal%20delay%20for%20agile%20and%20accurate%0Aoperation.%20We%20propose%20a%20novel%2C%20deterministic%2C%20uninformed%2C%20and%20single-parameter%0Apoint%20cloud%20sampling%20method%20named%20RMS%20that%20minimizes%20redundancy%20within%20a%203D%0Apoint%20cloud.%20In%20contrast%20to%20the%20state%20of%20the%20art%2C%20RMS%20balances%20the%0Atranslation-space%20observability%20by%20leveraging%20the%20fact%20that%20linear%20and%20planar%0Asurfaces%20inherently%20exhibit%20high%20redundancy%20propagated%20into%20iterative%0Aestimation%20pipelines.%20We%20define%20the%20concept%20of%20gradient%20flow%2C%20quantifying%20the%0Alocal%20surface%20underlying%20a%20point.%20We%20also%20show%20that%20maximizing%20the%20entropy%20of%0Athe%20gradient%20flow%20minimizes%20point%20redundancy%20for%20robot%20ego-motion%20estimation.%0AWe%20integrate%20RMS%20into%20the%20point-based%20KISS-ICP%20and%20feature-based%20LOAM%20odometry%0Apipelines%20and%20evaluate%20experimentally%20on%20KITTI%2C%20Hilti-Oxford%2C%20and%20custom%0Adatasets%20from%20multirotor%20UAVs.%20The%20experiments%20demonstrate%20that%20RMS%20outperforms%0Astate-of-the-art%20methods%20in%20speed%2C%20compression%2C%20and%20accuracy%20in%0Awell-conditioned%20as%20well%20as%20in%20geometrically-degenerated%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07337v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RMS%3A%20Redundancy-Minimizing%20Point%20Cloud%20Sampling%20for%20Real-Time%20Pose%0A%20%20Estimation&entry.906535625=Pavel%20Petracek%20and%20Kostas%20Alexis%20and%20Martin%20Saska&entry.1292438233=%20%20The%20typical%20point%20cloud%20sampling%20methods%20used%20in%20state%20estimation%20for%20mobile%0Arobots%20preserve%20a%20high%20level%20of%20point%20redundancy.%20This%20redundancy%20unnecessarily%0Aslows%20down%20the%20estimation%20pipeline%20and%20may%20cause%20drift%20under%20real-time%0Aconstraints.%20Such%20undue%20latency%20becomes%20a%20bottleneck%20for%20resource-constrained%0Arobots%20%28especially%20UAVs%29%2C%20requiring%20minimal%20delay%20for%20agile%20and%20accurate%0Aoperation.%20We%20propose%20a%20novel%2C%20deterministic%2C%20uninformed%2C%20and%20single-parameter%0Apoint%20cloud%20sampling%20method%20named%20RMS%20that%20minimizes%20redundancy%20within%20a%203D%0Apoint%20cloud.%20In%20contrast%20to%20the%20state%20of%20the%20art%2C%20RMS%20balances%20the%0Atranslation-space%20observability%20by%20leveraging%20the%20fact%20that%20linear%20and%20planar%0Asurfaces%20inherently%20exhibit%20high%20redundancy%20propagated%20into%20iterative%0Aestimation%20pipelines.%20We%20define%20the%20concept%20of%20gradient%20flow%2C%20quantifying%20the%0Alocal%20surface%20underlying%20a%20point.%20We%20also%20show%20that%20maximizing%20the%20entropy%20of%0Athe%20gradient%20flow%20minimizes%20point%20redundancy%20for%20robot%20ego-motion%20estimation.%0AWe%20integrate%20RMS%20into%20the%20point-based%20KISS-ICP%20and%20feature-based%20LOAM%20odometry%0Apipelines%20and%20evaluate%20experimentally%20on%20KITTI%2C%20Hilti-Oxford%2C%20and%20custom%0Adatasets%20from%20multirotor%20UAVs.%20The%20experiments%20demonstrate%20that%20RMS%20outperforms%0Astate-of-the-art%20methods%20in%20speed%2C%20compression%2C%20and%20accuracy%20in%0Awell-conditioned%20as%20well%20as%20in%20geometrically-degenerated%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07337v3&entry.124074799=Read"},
{"title": "RingID: Rethinking Tree-Ring Watermarking for Enhanced Multi-Key\n  Identification", "author": "Hai Ci and Pei Yang and Yiren Song and Mike Zheng Shou", "abstract": "  We revisit Tree-Ring Watermarking, a recent diffusion model watermarking\nmethod that demonstrates great robustness to various attacks. We conduct an\nin-depth study on it and reveal that the distribution shift unintentionally\nintroduced by the watermarking process, apart from watermark pattern matching,\ncontributes to its exceptional robustness. Our investigation further exposes\ninherent flaws in its original design, particularly in its ability to identify\nmultiple distinct keys, where distribution shift offers no assistance. Based on\nthese findings and analysis, we present RingID for enhanced multi-key\nidentification. It consists of a novel multi-channel heterogeneous watermarking\napproach designed to seamlessly amalgamate distinctive advantages from diverse\nwatermarks. Coupled with a series of suggested enhancements, RingID exhibits\nsubstantial advancements in multi-key identification. Github Page:\nhttps://github.com/showlab/RingID\n", "link": "http://arxiv.org/abs/2404.14055v2", "date": "2024-04-23", "relevancy": 2.2076, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4887}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4257}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4102}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification&body=Title%3A%20RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification%0AAuthor%3A%20Hai%20Ci%20and%20Pei%20Yang%20and%20Yiren%20Song%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20We%20revisit%20Tree-Ring%20Watermarking%2C%20a%20recent%20diffusion%20model%20watermarking%0Amethod%20that%20demonstrates%20great%20robustness%20to%20various%20attacks.%20We%20conduct%20an%0Ain-depth%20study%20on%20it%20and%20reveal%20that%20the%20distribution%20shift%20unintentionally%0Aintroduced%20by%20the%20watermarking%20process%2C%20apart%20from%20watermark%20pattern%20matching%2C%0Acontributes%20to%20its%20exceptional%20robustness.%20Our%20investigation%20further%20exposes%0Ainherent%20flaws%20in%20its%20original%20design%2C%20particularly%20in%20its%20ability%20to%20identify%0Amultiple%20distinct%20keys%2C%20where%20distribution%20shift%20offers%20no%20assistance.%20Based%20on%0Athese%20findings%20and%20analysis%2C%20we%20present%20RingID%20for%20enhanced%20multi-key%0Aidentification.%20It%20consists%20of%20a%20novel%20multi-channel%20heterogeneous%20watermarking%0Aapproach%20designed%20to%20seamlessly%20amalgamate%20distinctive%20advantages%20from%20diverse%0Awatermarks.%20Coupled%20with%20a%20series%20of%20suggested%20enhancements%2C%20RingID%20exhibits%0Asubstantial%20advancements%20in%20multi-key%20identification.%20Github%20Page%3A%0Ahttps%3A//github.com/showlab/RingID%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14055v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RingID%3A%20Rethinking%20Tree-Ring%20Watermarking%20for%20Enhanced%20Multi-Key%0A%20%20Identification&entry.906535625=Hai%20Ci%20and%20Pei%20Yang%20and%20Yiren%20Song%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20We%20revisit%20Tree-Ring%20Watermarking%2C%20a%20recent%20diffusion%20model%20watermarking%0Amethod%20that%20demonstrates%20great%20robustness%20to%20various%20attacks.%20We%20conduct%20an%0Ain-depth%20study%20on%20it%20and%20reveal%20that%20the%20distribution%20shift%20unintentionally%0Aintroduced%20by%20the%20watermarking%20process%2C%20apart%20from%20watermark%20pattern%20matching%2C%0Acontributes%20to%20its%20exceptional%20robustness.%20Our%20investigation%20further%20exposes%0Ainherent%20flaws%20in%20its%20original%20design%2C%20particularly%20in%20its%20ability%20to%20identify%0Amultiple%20distinct%20keys%2C%20where%20distribution%20shift%20offers%20no%20assistance.%20Based%20on%0Athese%20findings%20and%20analysis%2C%20we%20present%20RingID%20for%20enhanced%20multi-key%0Aidentification.%20It%20consists%20of%20a%20novel%20multi-channel%20heterogeneous%20watermarking%0Aapproach%20designed%20to%20seamlessly%20amalgamate%20distinctive%20advantages%20from%20diverse%0Awatermarks.%20Coupled%20with%20a%20series%20of%20suggested%20enhancements%2C%20RingID%20exhibits%0Asubstantial%20advancements%20in%20multi-key%20identification.%20Github%20Page%3A%0Ahttps%3A//github.com/showlab/RingID%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14055v2&entry.124074799=Read"},
{"title": "Co-Speech Gesture Detection through Multi-Phase Sequence Labeling", "author": "Esam Ghaleb and Ilya Burenko and Marlou Rasenberg and Wim Pouw and Peter Uhrig and Judith Holler and Ivan Toni and Asl\u0131 \u00d6zy\u00fcrek and Raquel Fern\u00e1ndez", "abstract": "  Gestures are integral components of face-to-face communication. They unfold\nover time, often following predictable movement phases of preparation, stroke,\nand retraction. Yet, the prevalent approach to automatic gesture detection\ntreats the problem as binary classification, classifying a segment as either\ncontaining a gesture or not, thus failing to capture its inherently sequential\nand contextual nature. To address this, we introduce a novel framework that\nreframes the task as a multi-phase sequence labeling problem rather than binary\nclassification. Our model processes sequences of skeletal movements over time\nwindows, uses Transformer encoders to learn contextual embeddings, and\nleverages Conditional Random Fields to perform sequence labeling. We evaluate\nour proposal on a large dataset of diverse co-speech gestures in task-oriented\nface-to-face dialogues. The results consistently demonstrate that our method\nsignificantly outperforms strong baseline models in detecting gesture strokes.\nFurthermore, applying Transformer encoders to learn contextual embeddings from\nmovement sequences substantially improves gesture unit detection. These results\nhighlight our framework's capacity to capture the fine-grained dynamics of\nco-speech gesture phases, paving the way for more nuanced and accurate gesture\ndetection and analysis.\n", "link": "http://arxiv.org/abs/2308.10680v2", "date": "2024-04-23", "relevancy": 2.1892, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Co-Speech%20Gesture%20Detection%20through%20Multi-Phase%20Sequence%20Labeling&body=Title%3A%20Co-Speech%20Gesture%20Detection%20through%20Multi-Phase%20Sequence%20Labeling%0AAuthor%3A%20Esam%20Ghaleb%20and%20Ilya%20Burenko%20and%20Marlou%20Rasenberg%20and%20Wim%20Pouw%20and%20Peter%20Uhrig%20and%20Judith%20Holler%20and%20Ivan%20Toni%20and%20Asl%C4%B1%20%C3%96zy%C3%BCrek%20and%20Raquel%20Fern%C3%A1ndez%0AAbstract%3A%20%20%20Gestures%20are%20integral%20components%20of%20face-to-face%20communication.%20They%20unfold%0Aover%20time%2C%20often%20following%20predictable%20movement%20phases%20of%20preparation%2C%20stroke%2C%0Aand%20retraction.%20Yet%2C%20the%20prevalent%20approach%20to%20automatic%20gesture%20detection%0Atreats%20the%20problem%20as%20binary%20classification%2C%20classifying%20a%20segment%20as%20either%0Acontaining%20a%20gesture%20or%20not%2C%20thus%20failing%20to%20capture%20its%20inherently%20sequential%0Aand%20contextual%20nature.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20framework%20that%0Areframes%20the%20task%20as%20a%20multi-phase%20sequence%20labeling%20problem%20rather%20than%20binary%0Aclassification.%20Our%20model%20processes%20sequences%20of%20skeletal%20movements%20over%20time%0Awindows%2C%20uses%20Transformer%20encoders%20to%20learn%20contextual%20embeddings%2C%20and%0Aleverages%20Conditional%20Random%20Fields%20to%20perform%20sequence%20labeling.%20We%20evaluate%0Aour%20proposal%20on%20a%20large%20dataset%20of%20diverse%20co-speech%20gestures%20in%20task-oriented%0Aface-to-face%20dialogues.%20The%20results%20consistently%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20strong%20baseline%20models%20in%20detecting%20gesture%20strokes.%0AFurthermore%2C%20applying%20Transformer%20encoders%20to%20learn%20contextual%20embeddings%20from%0Amovement%20sequences%20substantially%20improves%20gesture%20unit%20detection.%20These%20results%0Ahighlight%20our%20framework%27s%20capacity%20to%20capture%20the%20fine-grained%20dynamics%20of%0Aco-speech%20gesture%20phases%2C%20paving%20the%20way%20for%20more%20nuanced%20and%20accurate%20gesture%0Adetection%20and%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10680v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Speech%20Gesture%20Detection%20through%20Multi-Phase%20Sequence%20Labeling&entry.906535625=Esam%20Ghaleb%20and%20Ilya%20Burenko%20and%20Marlou%20Rasenberg%20and%20Wim%20Pouw%20and%20Peter%20Uhrig%20and%20Judith%20Holler%20and%20Ivan%20Toni%20and%20Asl%C4%B1%20%C3%96zy%C3%BCrek%20and%20Raquel%20Fern%C3%A1ndez&entry.1292438233=%20%20Gestures%20are%20integral%20components%20of%20face-to-face%20communication.%20They%20unfold%0Aover%20time%2C%20often%20following%20predictable%20movement%20phases%20of%20preparation%2C%20stroke%2C%0Aand%20retraction.%20Yet%2C%20the%20prevalent%20approach%20to%20automatic%20gesture%20detection%0Atreats%20the%20problem%20as%20binary%20classification%2C%20classifying%20a%20segment%20as%20either%0Acontaining%20a%20gesture%20or%20not%2C%20thus%20failing%20to%20capture%20its%20inherently%20sequential%0Aand%20contextual%20nature.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20framework%20that%0Areframes%20the%20task%20as%20a%20multi-phase%20sequence%20labeling%20problem%20rather%20than%20binary%0Aclassification.%20Our%20model%20processes%20sequences%20of%20skeletal%20movements%20over%20time%0Awindows%2C%20uses%20Transformer%20encoders%20to%20learn%20contextual%20embeddings%2C%20and%0Aleverages%20Conditional%20Random%20Fields%20to%20perform%20sequence%20labeling.%20We%20evaluate%0Aour%20proposal%20on%20a%20large%20dataset%20of%20diverse%20co-speech%20gestures%20in%20task-oriented%0Aface-to-face%20dialogues.%20The%20results%20consistently%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20strong%20baseline%20models%20in%20detecting%20gesture%20strokes.%0AFurthermore%2C%20applying%20Transformer%20encoders%20to%20learn%20contextual%20embeddings%20from%0Amovement%20sequences%20substantially%20improves%20gesture%20unit%20detection.%20These%20results%0Ahighlight%20our%20framework%27s%20capacity%20to%20capture%20the%20fine-grained%20dynamics%20of%0Aco-speech%20gesture%20phases%2C%20paving%20the%20way%20for%20more%20nuanced%20and%20accurate%20gesture%0Adetection%20and%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10680v2&entry.124074799=Read"},
{"title": "Leverage Variational Graph Representation For Model Poisoning on\n  Federated Learning", "author": "Kai Li and Xin Yuan and Jingjing Zheng and Wei Ni and Falko Dressler and Abbas Jamalipour", "abstract": "  This paper puts forth a new training data-untethered model poisoning (MP)\nattack on federated learning (FL). The new MP attack extends an adversarial\nvariational graph autoencoder (VGAE) to create malicious local models based\nsolely on the benign local models overheard without any access to the training\ndata of FL. Such an advancement leads to the VGAE-MP attack that is not only\nefficacious but also remains elusive to detection. VGAE-MP attack extracts\ngraph structural correlations among the benign local models and the training\ndata features, adversarially regenerates the graph structure, and generates\nmalicious local models using the adversarial graph structure and benign models'\nfeatures. Moreover, a new attacking algorithm is presented to train the\nmalicious local models using VGAE and sub-gradient descent, while enabling an\noptimal selection of the benign local models for training the VGAE. Experiments\ndemonstrate a gradual drop in FL accuracy under the proposed VGAE-MP attack and\nthe ineffectiveness of existing defense mechanisms in detecting the attack,\nposing a severe threat to FL.\n", "link": "http://arxiv.org/abs/2404.15042v1", "date": "2024-04-23", "relevancy": 2.1843, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4423}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4406}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4277}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leverage%20Variational%20Graph%20Representation%20For%20Model%20Poisoning%20on%0A%20%20Federated%20Learning&body=Title%3A%20Leverage%20Variational%20Graph%20Representation%20For%20Model%20Poisoning%20on%0A%20%20Federated%20Learning%0AAuthor%3A%20Kai%20Li%20and%20Xin%20Yuan%20and%20Jingjing%20Zheng%20and%20Wei%20Ni%20and%20Falko%20Dressler%20and%20Abbas%20Jamalipour%0AAbstract%3A%20%20%20This%20paper%20puts%20forth%20a%20new%20training%20data-untethered%20model%20poisoning%20%28MP%29%0Aattack%20on%20federated%20learning%20%28FL%29.%20The%20new%20MP%20attack%20extends%20an%20adversarial%0Avariational%20graph%20autoencoder%20%28VGAE%29%20to%20create%20malicious%20local%20models%20based%0Asolely%20on%20the%20benign%20local%20models%20overheard%20without%20any%20access%20to%20the%20training%0Adata%20of%20FL.%20Such%20an%20advancement%20leads%20to%20the%20VGAE-MP%20attack%20that%20is%20not%20only%0Aefficacious%20but%20also%20remains%20elusive%20to%20detection.%20VGAE-MP%20attack%20extracts%0Agraph%20structural%20correlations%20among%20the%20benign%20local%20models%20and%20the%20training%0Adata%20features%2C%20adversarially%20regenerates%20the%20graph%20structure%2C%20and%20generates%0Amalicious%20local%20models%20using%20the%20adversarial%20graph%20structure%20and%20benign%20models%27%0Afeatures.%20Moreover%2C%20a%20new%20attacking%20algorithm%20is%20presented%20to%20train%20the%0Amalicious%20local%20models%20using%20VGAE%20and%20sub-gradient%20descent%2C%20while%20enabling%20an%0Aoptimal%20selection%20of%20the%20benign%20local%20models%20for%20training%20the%20VGAE.%20Experiments%0Ademonstrate%20a%20gradual%20drop%20in%20FL%20accuracy%20under%20the%20proposed%20VGAE-MP%20attack%20and%0Athe%20ineffectiveness%20of%20existing%20defense%20mechanisms%20in%20detecting%20the%20attack%2C%0Aposing%20a%20severe%20threat%20to%20FL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15042v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leverage%20Variational%20Graph%20Representation%20For%20Model%20Poisoning%20on%0A%20%20Federated%20Learning&entry.906535625=Kai%20Li%20and%20Xin%20Yuan%20and%20Jingjing%20Zheng%20and%20Wei%20Ni%20and%20Falko%20Dressler%20and%20Abbas%20Jamalipour&entry.1292438233=%20%20This%20paper%20puts%20forth%20a%20new%20training%20data-untethered%20model%20poisoning%20%28MP%29%0Aattack%20on%20federated%20learning%20%28FL%29.%20The%20new%20MP%20attack%20extends%20an%20adversarial%0Avariational%20graph%20autoencoder%20%28VGAE%29%20to%20create%20malicious%20local%20models%20based%0Asolely%20on%20the%20benign%20local%20models%20overheard%20without%20any%20access%20to%20the%20training%0Adata%20of%20FL.%20Such%20an%20advancement%20leads%20to%20the%20VGAE-MP%20attack%20that%20is%20not%20only%0Aefficacious%20but%20also%20remains%20elusive%20to%20detection.%20VGAE-MP%20attack%20extracts%0Agraph%20structural%20correlations%20among%20the%20benign%20local%20models%20and%20the%20training%0Adata%20features%2C%20adversarially%20regenerates%20the%20graph%20structure%2C%20and%20generates%0Amalicious%20local%20models%20using%20the%20adversarial%20graph%20structure%20and%20benign%20models%27%0Afeatures.%20Moreover%2C%20a%20new%20attacking%20algorithm%20is%20presented%20to%20train%20the%0Amalicious%20local%20models%20using%20VGAE%20and%20sub-gradient%20descent%2C%20while%20enabling%20an%0Aoptimal%20selection%20of%20the%20benign%20local%20models%20for%20training%20the%20VGAE.%20Experiments%0Ademonstrate%20a%20gradual%20drop%20in%20FL%20accuracy%20under%20the%20proposed%20VGAE-MP%20attack%20and%0Athe%20ineffectiveness%20of%20existing%20defense%20mechanisms%20in%20detecting%20the%20attack%2C%0Aposing%20a%20severe%20threat%20to%20FL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15042v1&entry.124074799=Read"},
{"title": "Other Tokens Matter: Exploring Global and Local Features of Vision\n  Transformers for Object Re-Identification", "author": "Yingquan Wang and Pingping Zhang and Dong Wang and Huchuan Lu", "abstract": "  Object Re-Identification (Re-ID) aims to identify and retrieve specific\nobjects from images captured at different places and times. Recently, object\nRe-ID has achieved great success with the advances of Vision Transformers\n(ViT). However, the effects of the global-local relation have not been fully\nexplored in Transformers for object Re-ID. In this work, we first explore the\ninfluence of global and local features of ViT and then further propose a novel\nGlobal-Local Transformer (GLTrans) for high-performance object Re-ID. We find\nthat the features from last few layers of ViT already have a strong\nrepresentational ability, and the global and local information can mutually\nenhance each other. Based on this fact, we propose a Global Aggregation Encoder\n(GAE) to utilize the class tokens of the last few Transformer layers and learn\ncomprehensive global features effectively. Meanwhile, we propose the Local\nMulti-layer Fusion (LMF) which leverages both the global cues from GAE and\nmulti-layer patch tokens to explore the discriminative local representations.\nExtensive experiments demonstrate that our proposed method achieves superior\nperformance on four object Re-ID benchmarks.\n", "link": "http://arxiv.org/abs/2404.14985v1", "date": "2024-04-23", "relevancy": 2.1656, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5655}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5359}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Other%20Tokens%20Matter%3A%20Exploring%20Global%20and%20Local%20Features%20of%20Vision%0A%20%20Transformers%20for%20Object%20Re-Identification&body=Title%3A%20Other%20Tokens%20Matter%3A%20Exploring%20Global%20and%20Local%20Features%20of%20Vision%0A%20%20Transformers%20for%20Object%20Re-Identification%0AAuthor%3A%20Yingquan%20Wang%20and%20Pingping%20Zhang%20and%20Dong%20Wang%20and%20Huchuan%20Lu%0AAbstract%3A%20%20%20Object%20Re-Identification%20%28Re-ID%29%20aims%20to%20identify%20and%20retrieve%20specific%0Aobjects%20from%20images%20captured%20at%20different%20places%20and%20times.%20Recently%2C%20object%0ARe-ID%20has%20achieved%20great%20success%20with%20the%20advances%20of%20Vision%20Transformers%0A%28ViT%29.%20However%2C%20the%20effects%20of%20the%20global-local%20relation%20have%20not%20been%20fully%0Aexplored%20in%20Transformers%20for%20object%20Re-ID.%20In%20this%20work%2C%20we%20first%20explore%20the%0Ainfluence%20of%20global%20and%20local%20features%20of%20ViT%20and%20then%20further%20propose%20a%20novel%0AGlobal-Local%20Transformer%20%28GLTrans%29%20for%20high-performance%20object%20Re-ID.%20We%20find%0Athat%20the%20features%20from%20last%20few%20layers%20of%20ViT%20already%20have%20a%20strong%0Arepresentational%20ability%2C%20and%20the%20global%20and%20local%20information%20can%20mutually%0Aenhance%20each%20other.%20Based%20on%20this%20fact%2C%20we%20propose%20a%20Global%20Aggregation%20Encoder%0A%28GAE%29%20to%20utilize%20the%20class%20tokens%20of%20the%20last%20few%20Transformer%20layers%20and%20learn%0Acomprehensive%20global%20features%20effectively.%20Meanwhile%2C%20we%20propose%20the%20Local%0AMulti-layer%20Fusion%20%28LMF%29%20which%20leverages%20both%20the%20global%20cues%20from%20GAE%20and%0Amulti-layer%20patch%20tokens%20to%20explore%20the%20discriminative%20local%20representations.%0AExtensive%20experiments%20demonstrate%20that%20our%20proposed%20method%20achieves%20superior%0Aperformance%20on%20four%20object%20Re-ID%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14985v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Other%20Tokens%20Matter%3A%20Exploring%20Global%20and%20Local%20Features%20of%20Vision%0A%20%20Transformers%20for%20Object%20Re-Identification&entry.906535625=Yingquan%20Wang%20and%20Pingping%20Zhang%20and%20Dong%20Wang%20and%20Huchuan%20Lu&entry.1292438233=%20%20Object%20Re-Identification%20%28Re-ID%29%20aims%20to%20identify%20and%20retrieve%20specific%0Aobjects%20from%20images%20captured%20at%20different%20places%20and%20times.%20Recently%2C%20object%0ARe-ID%20has%20achieved%20great%20success%20with%20the%20advances%20of%20Vision%20Transformers%0A%28ViT%29.%20However%2C%20the%20effects%20of%20the%20global-local%20relation%20have%20not%20been%20fully%0Aexplored%20in%20Transformers%20for%20object%20Re-ID.%20In%20this%20work%2C%20we%20first%20explore%20the%0Ainfluence%20of%20global%20and%20local%20features%20of%20ViT%20and%20then%20further%20propose%20a%20novel%0AGlobal-Local%20Transformer%20%28GLTrans%29%20for%20high-performance%20object%20Re-ID.%20We%20find%0Athat%20the%20features%20from%20last%20few%20layers%20of%20ViT%20already%20have%20a%20strong%0Arepresentational%20ability%2C%20and%20the%20global%20and%20local%20information%20can%20mutually%0Aenhance%20each%20other.%20Based%20on%20this%20fact%2C%20we%20propose%20a%20Global%20Aggregation%20Encoder%0A%28GAE%29%20to%20utilize%20the%20class%20tokens%20of%20the%20last%20few%20Transformer%20layers%20and%20learn%0Acomprehensive%20global%20features%20effectively.%20Meanwhile%2C%20we%20propose%20the%20Local%0AMulti-layer%20Fusion%20%28LMF%29%20which%20leverages%20both%20the%20global%20cues%20from%20GAE%20and%0Amulti-layer%20patch%20tokens%20to%20explore%20the%20discriminative%20local%20representations.%0AExtensive%20experiments%20demonstrate%20that%20our%20proposed%20method%20achieves%20superior%0Aperformance%20on%20four%20object%20Re-ID%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14985v1&entry.124074799=Read"},
{"title": "Mixtures of Gaussians are Privately Learnable with a Polynomial Number\n  of Samples", "author": "Mohammad Afzali and Hassan Ashtiani and Christopher Liaw", "abstract": "  We study the problem of estimating mixtures of Gaussians under the constraint\nof differential privacy (DP). Our main result is that\n$\\text{poly}(k,d,1/\\alpha,1/\\varepsilon,\\log(1/\\delta))$ samples are sufficient\nto estimate a mixture of $k$ Gaussians in $\\mathbb{R}^d$ up to total variation\ndistance $\\alpha$ while satisfying $(\\varepsilon, \\delta)$-DP. This is the\nfirst finite sample complexity upper bound for the problem that does not make\nany structural assumptions on the GMMs.\n  To solve the problem, we devise a new framework which may be useful for other\ntasks. On a high level, we show that if a class of distributions (such as\nGaussians) is (1) list decodable and (2) admits a \"locally small'' cover (Bun\net al., 2021) with respect to total variation distance, then the class of its\nmixtures is privately learnable. The proof circumvents a known barrier\nindicating that, unlike Gaussians, GMMs do not admit a locally small cover\n(Aden-Ali et al., 2021b).\n", "link": "http://arxiv.org/abs/2309.03847v3", "date": "2024-04-23", "relevancy": 2.1448, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4516}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4205}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4147}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mixtures%20of%20Gaussians%20are%20Privately%20Learnable%20with%20a%20Polynomial%20Number%0A%20%20of%20Samples&body=Title%3A%20Mixtures%20of%20Gaussians%20are%20Privately%20Learnable%20with%20a%20Polynomial%20Number%0A%20%20of%20Samples%0AAuthor%3A%20Mohammad%20Afzali%20and%20Hassan%20Ashtiani%20and%20Christopher%20Liaw%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20estimating%20mixtures%20of%20Gaussians%20under%20the%20constraint%0Aof%20differential%20privacy%20%28DP%29.%20Our%20main%20result%20is%20that%0A%24%5Ctext%7Bpoly%7D%28k%2Cd%2C1/%5Calpha%2C1/%5Cvarepsilon%2C%5Clog%281/%5Cdelta%29%29%24%20samples%20are%20sufficient%0Ato%20estimate%20a%20mixture%20of%20%24k%24%20Gaussians%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20up%20to%20total%20variation%0Adistance%20%24%5Calpha%24%20while%20satisfying%20%24%28%5Cvarepsilon%2C%20%5Cdelta%29%24-DP.%20This%20is%20the%0Afirst%20finite%20sample%20complexity%20upper%20bound%20for%20the%20problem%20that%20does%20not%20make%0Aany%20structural%20assumptions%20on%20the%20GMMs.%0A%20%20To%20solve%20the%20problem%2C%20we%20devise%20a%20new%20framework%20which%20may%20be%20useful%20for%20other%0Atasks.%20On%20a%20high%20level%2C%20we%20show%20that%20if%20a%20class%20of%20distributions%20%28such%20as%0AGaussians%29%20is%20%281%29%20list%20decodable%20and%20%282%29%20admits%20a%20%22locally%20small%27%27%20cover%20%28Bun%0Aet%20al.%2C%202021%29%20with%20respect%20to%20total%20variation%20distance%2C%20then%20the%20class%20of%20its%0Amixtures%20is%20privately%20learnable.%20The%20proof%20circumvents%20a%20known%20barrier%0Aindicating%20that%2C%20unlike%20Gaussians%2C%20GMMs%20do%20not%20admit%20a%20locally%20small%20cover%0A%28Aden-Ali%20et%20al.%2C%202021b%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03847v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixtures%20of%20Gaussians%20are%20Privately%20Learnable%20with%20a%20Polynomial%20Number%0A%20%20of%20Samples&entry.906535625=Mohammad%20Afzali%20and%20Hassan%20Ashtiani%20and%20Christopher%20Liaw&entry.1292438233=%20%20We%20study%20the%20problem%20of%20estimating%20mixtures%20of%20Gaussians%20under%20the%20constraint%0Aof%20differential%20privacy%20%28DP%29.%20Our%20main%20result%20is%20that%0A%24%5Ctext%7Bpoly%7D%28k%2Cd%2C1/%5Calpha%2C1/%5Cvarepsilon%2C%5Clog%281/%5Cdelta%29%29%24%20samples%20are%20sufficient%0Ato%20estimate%20a%20mixture%20of%20%24k%24%20Gaussians%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%20up%20to%20total%20variation%0Adistance%20%24%5Calpha%24%20while%20satisfying%20%24%28%5Cvarepsilon%2C%20%5Cdelta%29%24-DP.%20This%20is%20the%0Afirst%20finite%20sample%20complexity%20upper%20bound%20for%20the%20problem%20that%20does%20not%20make%0Aany%20structural%20assumptions%20on%20the%20GMMs.%0A%20%20To%20solve%20the%20problem%2C%20we%20devise%20a%20new%20framework%20which%20may%20be%20useful%20for%20other%0Atasks.%20On%20a%20high%20level%2C%20we%20show%20that%20if%20a%20class%20of%20distributions%20%28such%20as%0AGaussians%29%20is%20%281%29%20list%20decodable%20and%20%282%29%20admits%20a%20%22locally%20small%27%27%20cover%20%28Bun%0Aet%20al.%2C%202021%29%20with%20respect%20to%20total%20variation%20distance%2C%20then%20the%20class%20of%20its%0Amixtures%20is%20privately%20learnable.%20The%20proof%20circumvents%20a%20known%20barrier%0Aindicating%20that%2C%20unlike%20Gaussians%2C%20GMMs%20do%20not%20admit%20a%20locally%20small%20cover%0A%28Aden-Ali%20et%20al.%2C%202021b%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03847v3&entry.124074799=Read"},
{"title": "A Spatiotemporal Hand-Eye Calibration for Trajectory Alignment in\n  Visual(-Inertial) Odometry Evaluation", "author": "Zichao Shu and Lijun Li and Rui Wang and Zetao Chen", "abstract": "  A common prerequisite for evaluating a visual(-inertial) odometry (VO/VIO)\nalgorithm is to align the timestamps and the reference frame of its estimated\ntrajectory with a reference ground-truth derived from a system of superior\nprecision, such as a motion capture system. The trajectory-based alignment,\ntypically modeled as a classic hand-eye calibration, significantly influences\nthe accuracy of evaluation metrics. However, traditional calibration methods\nare susceptible to the quality of the input poses. Few studies have taken this\ninto account when evaluating VO/VIO trajectories that usually suffer from noise\nand drift. To fill this gap, we propose a novel spatiotemporal hand-eye\ncalibration algorithm that fully leverages multiple constraints from screw\ntheory for enhanced accuracy and robustness. Experimental results show that our\nalgorithm has better performance and is less noise-prone than state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2404.14894v1", "date": "2024-04-23", "relevancy": 2.1365, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5475}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Spatiotemporal%20Hand-Eye%20Calibration%20for%20Trajectory%20Alignment%20in%0A%20%20Visual%28-Inertial%29%20Odometry%20Evaluation&body=Title%3A%20A%20Spatiotemporal%20Hand-Eye%20Calibration%20for%20Trajectory%20Alignment%20in%0A%20%20Visual%28-Inertial%29%20Odometry%20Evaluation%0AAuthor%3A%20Zichao%20Shu%20and%20Lijun%20Li%20and%20Rui%20Wang%20and%20Zetao%20Chen%0AAbstract%3A%20%20%20A%20common%20prerequisite%20for%20evaluating%20a%20visual%28-inertial%29%20odometry%20%28VO/VIO%29%0Aalgorithm%20is%20to%20align%20the%20timestamps%20and%20the%20reference%20frame%20of%20its%20estimated%0Atrajectory%20with%20a%20reference%20ground-truth%20derived%20from%20a%20system%20of%20superior%0Aprecision%2C%20such%20as%20a%20motion%20capture%20system.%20The%20trajectory-based%20alignment%2C%0Atypically%20modeled%20as%20a%20classic%20hand-eye%20calibration%2C%20significantly%20influences%0Athe%20accuracy%20of%20evaluation%20metrics.%20However%2C%20traditional%20calibration%20methods%0Aare%20susceptible%20to%20the%20quality%20of%20the%20input%20poses.%20Few%20studies%20have%20taken%20this%0Ainto%20account%20when%20evaluating%20VO/VIO%20trajectories%20that%20usually%20suffer%20from%20noise%0Aand%20drift.%20To%20fill%20this%20gap%2C%20we%20propose%20a%20novel%20spatiotemporal%20hand-eye%0Acalibration%20algorithm%20that%20fully%20leverages%20multiple%20constraints%20from%20screw%0Atheory%20for%20enhanced%20accuracy%20and%20robustness.%20Experimental%20results%20show%20that%20our%0Aalgorithm%20has%20better%20performance%20and%20is%20less%20noise-prone%20than%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14894v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Spatiotemporal%20Hand-Eye%20Calibration%20for%20Trajectory%20Alignment%20in%0A%20%20Visual%28-Inertial%29%20Odometry%20Evaluation&entry.906535625=Zichao%20Shu%20and%20Lijun%20Li%20and%20Rui%20Wang%20and%20Zetao%20Chen&entry.1292438233=%20%20A%20common%20prerequisite%20for%20evaluating%20a%20visual%28-inertial%29%20odometry%20%28VO/VIO%29%0Aalgorithm%20is%20to%20align%20the%20timestamps%20and%20the%20reference%20frame%20of%20its%20estimated%0Atrajectory%20with%20a%20reference%20ground-truth%20derived%20from%20a%20system%20of%20superior%0Aprecision%2C%20such%20as%20a%20motion%20capture%20system.%20The%20trajectory-based%20alignment%2C%0Atypically%20modeled%20as%20a%20classic%20hand-eye%20calibration%2C%20significantly%20influences%0Athe%20accuracy%20of%20evaluation%20metrics.%20However%2C%20traditional%20calibration%20methods%0Aare%20susceptible%20to%20the%20quality%20of%20the%20input%20poses.%20Few%20studies%20have%20taken%20this%0Ainto%20account%20when%20evaluating%20VO/VIO%20trajectories%20that%20usually%20suffer%20from%20noise%0Aand%20drift.%20To%20fill%20this%20gap%2C%20we%20propose%20a%20novel%20spatiotemporal%20hand-eye%0Acalibration%20algorithm%20that%20fully%20leverages%20multiple%20constraints%20from%20screw%0Atheory%20for%20enhanced%20accuracy%20and%20robustness.%20Experimental%20results%20show%20that%20our%0Aalgorithm%20has%20better%20performance%20and%20is%20less%20noise-prone%20than%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14894v1&entry.124074799=Read"},
{"title": "Adaptive Mixed-Scale Feature Fusion Network for Blind AI-Generated Image\n  Quality Assessment", "author": "Tianwei Zhou and Songbai Tan and Wei Zhou and Yu Luo and Yuan-Gen Wang and Guanghui Yue", "abstract": "  With the increasing maturity of the text-to-image and image-to-image\ngenerative models, AI-generated images (AGIs) have shown great application\npotential in advertisement, entertainment, education, social media, etc.\nAlthough remarkable advancements have been achieved in generative models, very\nfew efforts have been paid to design relevant quality assessment models. In\nthis paper, we propose a novel blind image quality assessment (IQA) network,\nnamed AMFF-Net, for AGIs. AMFF-Net evaluates AGI quality from three dimensions,\ni.e., \"visual quality\", \"authenticity\", and \"consistency\". Specifically,\ninspired by the characteristics of the human visual system and motivated by the\nobservation that \"visual quality\" and \"authenticity\" are characterized by both\nlocal and global aspects, AMFF-Net scales the image up and down and takes the\nscaled images and original-sized image as the inputs to obtain multi-scale\nfeatures. After that, an Adaptive Feature Fusion (AFF) block is used to\nadaptively fuse the multi-scale features with learnable weights. In addition,\nconsidering the correlation between the image and prompt, AMFF-Net compares the\nsemantic features from text encoder and image encoder to evaluate the\ntext-to-image alignment. We carry out extensive experiments on three AGI\nquality assessment databases, and the experimental results show that our\nAMFF-Net obtains better performance than nine state-of-the-art blind IQA\nmethods. The results of ablation experiments further demonstrate the\neffectiveness of the proposed multi-scale input strategy and AFF block.\n", "link": "http://arxiv.org/abs/2404.15163v1", "date": "2024-04-23", "relevancy": 2.1364, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5472}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5387}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5191}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Mixed-Scale%20Feature%20Fusion%20Network%20for%20Blind%20AI-Generated%20Image%0A%20%20Quality%20Assessment&body=Title%3A%20Adaptive%20Mixed-Scale%20Feature%20Fusion%20Network%20for%20Blind%20AI-Generated%20Image%0A%20%20Quality%20Assessment%0AAuthor%3A%20Tianwei%20Zhou%20and%20Songbai%20Tan%20and%20Wei%20Zhou%20and%20Yu%20Luo%20and%20Yuan-Gen%20Wang%20and%20Guanghui%20Yue%0AAbstract%3A%20%20%20With%20the%20increasing%20maturity%20of%20the%20text-to-image%20and%20image-to-image%0Agenerative%20models%2C%20AI-generated%20images%20%28AGIs%29%20have%20shown%20great%20application%0Apotential%20in%20advertisement%2C%20entertainment%2C%20education%2C%20social%20media%2C%20etc.%0AAlthough%20remarkable%20advancements%20have%20been%20achieved%20in%20generative%20models%2C%20very%0Afew%20efforts%20have%20been%20paid%20to%20design%20relevant%20quality%20assessment%20models.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20blind%20image%20quality%20assessment%20%28IQA%29%20network%2C%0Anamed%20AMFF-Net%2C%20for%20AGIs.%20AMFF-Net%20evaluates%20AGI%20quality%20from%20three%20dimensions%2C%0Ai.e.%2C%20%22visual%20quality%22%2C%20%22authenticity%22%2C%20and%20%22consistency%22.%20Specifically%2C%0Ainspired%20by%20the%20characteristics%20of%20the%20human%20visual%20system%20and%20motivated%20by%20the%0Aobservation%20that%20%22visual%20quality%22%20and%20%22authenticity%22%20are%20characterized%20by%20both%0Alocal%20and%20global%20aspects%2C%20AMFF-Net%20scales%20the%20image%20up%20and%20down%20and%20takes%20the%0Ascaled%20images%20and%20original-sized%20image%20as%20the%20inputs%20to%20obtain%20multi-scale%0Afeatures.%20After%20that%2C%20an%20Adaptive%20Feature%20Fusion%20%28AFF%29%20block%20is%20used%20to%0Aadaptively%20fuse%20the%20multi-scale%20features%20with%20learnable%20weights.%20In%20addition%2C%0Aconsidering%20the%20correlation%20between%20the%20image%20and%20prompt%2C%20AMFF-Net%20compares%20the%0Asemantic%20features%20from%20text%20encoder%20and%20image%20encoder%20to%20evaluate%20the%0Atext-to-image%20alignment.%20We%20carry%20out%20extensive%20experiments%20on%20three%20AGI%0Aquality%20assessment%20databases%2C%20and%20the%20experimental%20results%20show%20that%20our%0AAMFF-Net%20obtains%20better%20performance%20than%20nine%20state-of-the-art%20blind%20IQA%0Amethods.%20The%20results%20of%20ablation%20experiments%20further%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20multi-scale%20input%20strategy%20and%20AFF%20block.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15163v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Mixed-Scale%20Feature%20Fusion%20Network%20for%20Blind%20AI-Generated%20Image%0A%20%20Quality%20Assessment&entry.906535625=Tianwei%20Zhou%20and%20Songbai%20Tan%20and%20Wei%20Zhou%20and%20Yu%20Luo%20and%20Yuan-Gen%20Wang%20and%20Guanghui%20Yue&entry.1292438233=%20%20With%20the%20increasing%20maturity%20of%20the%20text-to-image%20and%20image-to-image%0Agenerative%20models%2C%20AI-generated%20images%20%28AGIs%29%20have%20shown%20great%20application%0Apotential%20in%20advertisement%2C%20entertainment%2C%20education%2C%20social%20media%2C%20etc.%0AAlthough%20remarkable%20advancements%20have%20been%20achieved%20in%20generative%20models%2C%20very%0Afew%20efforts%20have%20been%20paid%20to%20design%20relevant%20quality%20assessment%20models.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20blind%20image%20quality%20assessment%20%28IQA%29%20network%2C%0Anamed%20AMFF-Net%2C%20for%20AGIs.%20AMFF-Net%20evaluates%20AGI%20quality%20from%20three%20dimensions%2C%0Ai.e.%2C%20%22visual%20quality%22%2C%20%22authenticity%22%2C%20and%20%22consistency%22.%20Specifically%2C%0Ainspired%20by%20the%20characteristics%20of%20the%20human%20visual%20system%20and%20motivated%20by%20the%0Aobservation%20that%20%22visual%20quality%22%20and%20%22authenticity%22%20are%20characterized%20by%20both%0Alocal%20and%20global%20aspects%2C%20AMFF-Net%20scales%20the%20image%20up%20and%20down%20and%20takes%20the%0Ascaled%20images%20and%20original-sized%20image%20as%20the%20inputs%20to%20obtain%20multi-scale%0Afeatures.%20After%20that%2C%20an%20Adaptive%20Feature%20Fusion%20%28AFF%29%20block%20is%20used%20to%0Aadaptively%20fuse%20the%20multi-scale%20features%20with%20learnable%20weights.%20In%20addition%2C%0Aconsidering%20the%20correlation%20between%20the%20image%20and%20prompt%2C%20AMFF-Net%20compares%20the%0Asemantic%20features%20from%20text%20encoder%20and%20image%20encoder%20to%20evaluate%20the%0Atext-to-image%20alignment.%20We%20carry%20out%20extensive%20experiments%20on%20three%20AGI%0Aquality%20assessment%20databases%2C%20and%20the%20experimental%20results%20show%20that%20our%0AAMFF-Net%20obtains%20better%20performance%20than%20nine%20state-of-the-art%20blind%20IQA%0Amethods.%20The%20results%20of%20ablation%20experiments%20further%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20multi-scale%20input%20strategy%20and%20AFF%20block.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15163v1&entry.124074799=Read"},
{"title": "LEAF: Unveiling Two Sides of the Same Coin in Semi-supervised Facial\n  Expression Recognition", "author": "Fan Zhang and Zhi-Qi Cheng and Jian Zhao and Xiaojiang Peng and Xuelong Li", "abstract": "  Semi-supervised learning has emerged as a promising approach to tackle the\nchallenge of label scarcity in facial expression recognition (FER) task.\nHowever, current state-of-the-art methods primarily focus on one side of the\ncoin, i.e., generating high-quality pseudo-labels, while overlooking the other\nside: enhancing expression-relevant representations. In this paper, we unveil\nboth sides of the coin by proposing a unified framework termed hierarchicaL\ndEcoupling And Fusing (LEAF) to coordinate expression-relevant representations\nand pseudo-labels for semi-supervised FER. LEAF introduces a hierarchical\nexpression-aware aggregation strategy that operates at three levels: semantic,\ninstance, and category. (1) At the semantic and instance levels, LEAF decouples\nrepresentations into expression-agnostic and expression-relevant components,\nand adaptively fuses them using learnable gating weights. (2) At the category\nlevel, LEAF assigns ambiguous pseudo-labels by decoupling predictions into\npositive and negative parts, and employs a consistency loss to ensure agreement\nbetween two augmented views of the same image. Extensive experiments on\nbenchmark datasets demonstrate that by unveiling and harmonizing both sides of\nthe coin, LEAF outperforms state-of-the-art semi-supervised FER methods,\neffectively leveraging both labeled and unlabeled data. Moreover, the proposed\nexpression-aware aggregation strategy can be seamlessly integrated into\nexisting semi-supervised frameworks, leading to significant performance gains.\n", "link": "http://arxiv.org/abs/2404.15041v1", "date": "2024-04-23", "relevancy": 2.127, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5463}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5316}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5261}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LEAF%3A%20Unveiling%20Two%20Sides%20of%20the%20Same%20Coin%20in%20Semi-supervised%20Facial%0A%20%20Expression%20Recognition&body=Title%3A%20LEAF%3A%20Unveiling%20Two%20Sides%20of%20the%20Same%20Coin%20in%20Semi-supervised%20Facial%0A%20%20Expression%20Recognition%0AAuthor%3A%20Fan%20Zhang%20and%20Zhi-Qi%20Cheng%20and%20Jian%20Zhao%20and%20Xiaojiang%20Peng%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Semi-supervised%20learning%20has%20emerged%20as%20a%20promising%20approach%20to%20tackle%20the%0Achallenge%20of%20label%20scarcity%20in%20facial%20expression%20recognition%20%28FER%29%20task.%0AHowever%2C%20current%20state-of-the-art%20methods%20primarily%20focus%20on%20one%20side%20of%20the%0Acoin%2C%20i.e.%2C%20generating%20high-quality%20pseudo-labels%2C%20while%20overlooking%20the%20other%0Aside%3A%20enhancing%20expression-relevant%20representations.%20In%20this%20paper%2C%20we%20unveil%0Aboth%20sides%20of%20the%20coin%20by%20proposing%20a%20unified%20framework%20termed%20hierarchicaL%0AdEcoupling%20And%20Fusing%20%28LEAF%29%20to%20coordinate%20expression-relevant%20representations%0Aand%20pseudo-labels%20for%20semi-supervised%20FER.%20LEAF%20introduces%20a%20hierarchical%0Aexpression-aware%20aggregation%20strategy%20that%20operates%20at%20three%20levels%3A%20semantic%2C%0Ainstance%2C%20and%20category.%20%281%29%20At%20the%20semantic%20and%20instance%20levels%2C%20LEAF%20decouples%0Arepresentations%20into%20expression-agnostic%20and%20expression-relevant%20components%2C%0Aand%20adaptively%20fuses%20them%20using%20learnable%20gating%20weights.%20%282%29%20At%20the%20category%0Alevel%2C%20LEAF%20assigns%20ambiguous%20pseudo-labels%20by%20decoupling%20predictions%20into%0Apositive%20and%20negative%20parts%2C%20and%20employs%20a%20consistency%20loss%20to%20ensure%20agreement%0Abetween%20two%20augmented%20views%20of%20the%20same%20image.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20that%20by%20unveiling%20and%20harmonizing%20both%20sides%20of%0Athe%20coin%2C%20LEAF%20outperforms%20state-of-the-art%20semi-supervised%20FER%20methods%2C%0Aeffectively%20leveraging%20both%20labeled%20and%20unlabeled%20data.%20Moreover%2C%20the%20proposed%0Aexpression-aware%20aggregation%20strategy%20can%20be%20seamlessly%20integrated%20into%0Aexisting%20semi-supervised%20frameworks%2C%20leading%20to%20significant%20performance%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15041v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAF%3A%20Unveiling%20Two%20Sides%20of%20the%20Same%20Coin%20in%20Semi-supervised%20Facial%0A%20%20Expression%20Recognition&entry.906535625=Fan%20Zhang%20and%20Zhi-Qi%20Cheng%20and%20Jian%20Zhao%20and%20Xiaojiang%20Peng%20and%20Xuelong%20Li&entry.1292438233=%20%20Semi-supervised%20learning%20has%20emerged%20as%20a%20promising%20approach%20to%20tackle%20the%0Achallenge%20of%20label%20scarcity%20in%20facial%20expression%20recognition%20%28FER%29%20task.%0AHowever%2C%20current%20state-of-the-art%20methods%20primarily%20focus%20on%20one%20side%20of%20the%0Acoin%2C%20i.e.%2C%20generating%20high-quality%20pseudo-labels%2C%20while%20overlooking%20the%20other%0Aside%3A%20enhancing%20expression-relevant%20representations.%20In%20this%20paper%2C%20we%20unveil%0Aboth%20sides%20of%20the%20coin%20by%20proposing%20a%20unified%20framework%20termed%20hierarchicaL%0AdEcoupling%20And%20Fusing%20%28LEAF%29%20to%20coordinate%20expression-relevant%20representations%0Aand%20pseudo-labels%20for%20semi-supervised%20FER.%20LEAF%20introduces%20a%20hierarchical%0Aexpression-aware%20aggregation%20strategy%20that%20operates%20at%20three%20levels%3A%20semantic%2C%0Ainstance%2C%20and%20category.%20%281%29%20At%20the%20semantic%20and%20instance%20levels%2C%20LEAF%20decouples%0Arepresentations%20into%20expression-agnostic%20and%20expression-relevant%20components%2C%0Aand%20adaptively%20fuses%20them%20using%20learnable%20gating%20weights.%20%282%29%20At%20the%20category%0Alevel%2C%20LEAF%20assigns%20ambiguous%20pseudo-labels%20by%20decoupling%20predictions%20into%0Apositive%20and%20negative%20parts%2C%20and%20employs%20a%20consistency%20loss%20to%20ensure%20agreement%0Abetween%20two%20augmented%20views%20of%20the%20same%20image.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20that%20by%20unveiling%20and%20harmonizing%20both%20sides%20of%0Athe%20coin%2C%20LEAF%20outperforms%20state-of-the-art%20semi-supervised%20FER%20methods%2C%0Aeffectively%20leveraging%20both%20labeled%20and%20unlabeled%20data.%20Moreover%2C%20the%20proposed%0Aexpression-aware%20aggregation%20strategy%20can%20be%20seamlessly%20integrated%20into%0Aexisting%20semi-supervised%20frameworks%2C%20leading%20to%20significant%20performance%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15041v1&entry.124074799=Read"},
{"title": "SGFormer: Spherical Geometry Transformer for 360 Depth Estimation", "author": "Junsong Zhang and Zisong Chen and Chunyu Lin and Lang Nie and Zhijie Shen and Junda Huang and Yao Zhao", "abstract": "  Panoramic distortion poses a significant challenge in 360 depth estimation,\nparticularly pronounced at the north and south poles. Existing methods either\nadopt a bi-projection fusion strategy to remove distortions or model long-range\ndependencies to capture global structures, which can result in either unclear\nstructure or insufficient local perception. In this paper, we propose a\nspherical geometry transformer, named SGFormer, to address the above issues,\nwith an innovative step to integrate spherical geometric priors into vision\ntransformers. To this end, we retarget the transformer decoder to a spherical\nprior decoder (termed SPDecoder), which endeavors to uphold the integrity of\nspherical structures during decoding. Concretely, we leverage bipolar\nre-projection, circular rotation, and curve local embedding to preserve the\nspherical characteristics of equidistortion, continuity, and surface distance,\nrespectively. Furthermore, we present a query-based global conditional position\nembedding to compensate for spatial structure at varying resolutions. It not\nonly boosts the global perception of spatial position but also sharpens the\ndepth structure across different patches. Finally, we conduct extensive\nexperiments on popular benchmarks, demonstrating our superiority over\nstate-of-the-art solutions.\n", "link": "http://arxiv.org/abs/2404.14979v1", "date": "2024-04-23", "relevancy": 2.1234, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5403}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5399}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5178}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SGFormer%3A%20Spherical%20Geometry%20Transformer%20for%20360%20Depth%20Estimation&body=Title%3A%20SGFormer%3A%20Spherical%20Geometry%20Transformer%20for%20360%20Depth%20Estimation%0AAuthor%3A%20Junsong%20Zhang%20and%20Zisong%20Chen%20and%20Chunyu%20Lin%20and%20Lang%20Nie%20and%20Zhijie%20Shen%20and%20Junda%20Huang%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20Panoramic%20distortion%20poses%20a%20significant%20challenge%20in%20360%20depth%20estimation%2C%0Aparticularly%20pronounced%20at%20the%20north%20and%20south%20poles.%20Existing%20methods%20either%0Aadopt%20a%20bi-projection%20fusion%20strategy%20to%20remove%20distortions%20or%20model%20long-range%0Adependencies%20to%20capture%20global%20structures%2C%20which%20can%20result%20in%20either%20unclear%0Astructure%20or%20insufficient%20local%20perception.%20In%20this%20paper%2C%20we%20propose%20a%0Aspherical%20geometry%20transformer%2C%20named%20SGFormer%2C%20to%20address%20the%20above%20issues%2C%0Awith%20an%20innovative%20step%20to%20integrate%20spherical%20geometric%20priors%20into%20vision%0Atransformers.%20To%20this%20end%2C%20we%20retarget%20the%20transformer%20decoder%20to%20a%20spherical%0Aprior%20decoder%20%28termed%20SPDecoder%29%2C%20which%20endeavors%20to%20uphold%20the%20integrity%20of%0Aspherical%20structures%20during%20decoding.%20Concretely%2C%20we%20leverage%20bipolar%0Are-projection%2C%20circular%20rotation%2C%20and%20curve%20local%20embedding%20to%20preserve%20the%0Aspherical%20characteristics%20of%20equidistortion%2C%20continuity%2C%20and%20surface%20distance%2C%0Arespectively.%20Furthermore%2C%20we%20present%20a%20query-based%20global%20conditional%20position%0Aembedding%20to%20compensate%20for%20spatial%20structure%20at%20varying%20resolutions.%20It%20not%0Aonly%20boosts%20the%20global%20perception%20of%20spatial%20position%20but%20also%20sharpens%20the%0Adepth%20structure%20across%20different%20patches.%20Finally%2C%20we%20conduct%20extensive%0Aexperiments%20on%20popular%20benchmarks%2C%20demonstrating%20our%20superiority%20over%0Astate-of-the-art%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14979v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGFormer%3A%20Spherical%20Geometry%20Transformer%20for%20360%20Depth%20Estimation&entry.906535625=Junsong%20Zhang%20and%20Zisong%20Chen%20and%20Chunyu%20Lin%20and%20Lang%20Nie%20and%20Zhijie%20Shen%20and%20Junda%20Huang%20and%20Yao%20Zhao&entry.1292438233=%20%20Panoramic%20distortion%20poses%20a%20significant%20challenge%20in%20360%20depth%20estimation%2C%0Aparticularly%20pronounced%20at%20the%20north%20and%20south%20poles.%20Existing%20methods%20either%0Aadopt%20a%20bi-projection%20fusion%20strategy%20to%20remove%20distortions%20or%20model%20long-range%0Adependencies%20to%20capture%20global%20structures%2C%20which%20can%20result%20in%20either%20unclear%0Astructure%20or%20insufficient%20local%20perception.%20In%20this%20paper%2C%20we%20propose%20a%0Aspherical%20geometry%20transformer%2C%20named%20SGFormer%2C%20to%20address%20the%20above%20issues%2C%0Awith%20an%20innovative%20step%20to%20integrate%20spherical%20geometric%20priors%20into%20vision%0Atransformers.%20To%20this%20end%2C%20we%20retarget%20the%20transformer%20decoder%20to%20a%20spherical%0Aprior%20decoder%20%28termed%20SPDecoder%29%2C%20which%20endeavors%20to%20uphold%20the%20integrity%20of%0Aspherical%20structures%20during%20decoding.%20Concretely%2C%20we%20leverage%20bipolar%0Are-projection%2C%20circular%20rotation%2C%20and%20curve%20local%20embedding%20to%20preserve%20the%0Aspherical%20characteristics%20of%20equidistortion%2C%20continuity%2C%20and%20surface%20distance%2C%0Arespectively.%20Furthermore%2C%20we%20present%20a%20query-based%20global%20conditional%20position%0Aembedding%20to%20compensate%20for%20spatial%20structure%20at%20varying%20resolutions.%20It%20not%0Aonly%20boosts%20the%20global%20perception%20of%20spatial%20position%20but%20also%20sharpens%20the%0Adepth%20structure%20across%20different%20patches.%20Finally%2C%20we%20conduct%20extensive%0Aexperiments%20on%20popular%20benchmarks%2C%20demonstrating%20our%20superiority%20over%0Astate-of-the-art%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14979v1&entry.124074799=Read"},
{"title": "Improving Video Corpus Moment Retrieval with Partial Relevance\n  Enhancement", "author": "Danyang Hou and Liang Pang and Huawei Shen and Xueqi Cheng", "abstract": "  Video Corpus Moment Retrieval (VCMR) is a new video retrieval task aimed at\nretrieving a relevant moment from a large corpus of untrimmed videos using a\ntext query. The relevance between the video and query is partial, mainly\nevident in two aspects:~(1)~Scope: The untrimmed video contains many frames,\nbut not all are relevant to the query. Strong relevance is typically observed\nonly within the relevant moment.~(2)~Modality: The relevance of the query\nvaries with different modalities. Action descriptions align more with visual\nelements, while character conversations are more related to textual\ninformation.Existing methods often treat all video contents equally, leading to\nsub-optimal moment retrieval. We argue that effectively capturing the partial\nrelevance between the query and video is essential for the VCMR task. To this\nend, we propose a Partial Relevance Enhanced Model~(PREM) to improve VCMR. VCMR\ninvolves two sub-tasks: video retrieval and moment localization. To align with\ntheir distinct objectives, we implement specialized partial relevance\nenhancement strategies. For video retrieval, we introduce a multi-modal\ncollaborative video retriever, generating different query representations for\nthe two modalities by modality-specific pooling, ensuring a more effective\nmatch. For moment localization, we propose the focus-then-fuse moment\nlocalizer, utilizing modality-specific gates to capture essential content. We\nalso introduce relevant content-enhanced training methods for both retriever\nand localizer to enhance the ability of model to capture relevant content.\nExperimental results on TVR and DiDeMo datasets show that the proposed model\noutperforms the baselines, achieving a new state-of-the-art of VCMR. The code\nis available at \\url{https://github.com/hdy007007/PREM}.\n", "link": "http://arxiv.org/abs/2402.13576v2", "date": "2024-04-23", "relevancy": 2.1226, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5479}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Video%20Corpus%20Moment%20Retrieval%20with%20Partial%20Relevance%0A%20%20Enhancement&body=Title%3A%20Improving%20Video%20Corpus%20Moment%20Retrieval%20with%20Partial%20Relevance%0A%20%20Enhancement%0AAuthor%3A%20Danyang%20Hou%20and%20Liang%20Pang%20and%20Huawei%20Shen%20and%20Xueqi%20Cheng%0AAbstract%3A%20%20%20Video%20Corpus%20Moment%20Retrieval%20%28VCMR%29%20is%20a%20new%20video%20retrieval%20task%20aimed%20at%0Aretrieving%20a%20relevant%20moment%20from%20a%20large%20corpus%20of%20untrimmed%20videos%20using%20a%0Atext%20query.%20The%20relevance%20between%20the%20video%20and%20query%20is%20partial%2C%20mainly%0Aevident%20in%20two%20aspects%3A~%281%29~Scope%3A%20The%20untrimmed%20video%20contains%20many%20frames%2C%0Abut%20not%20all%20are%20relevant%20to%20the%20query.%20Strong%20relevance%20is%20typically%20observed%0Aonly%20within%20the%20relevant%20moment.~%282%29~Modality%3A%20The%20relevance%20of%20the%20query%0Avaries%20with%20different%20modalities.%20Action%20descriptions%20align%20more%20with%20visual%0Aelements%2C%20while%20character%20conversations%20are%20more%20related%20to%20textual%0Ainformation.Existing%20methods%20often%20treat%20all%20video%20contents%20equally%2C%20leading%20to%0Asub-optimal%20moment%20retrieval.%20We%20argue%20that%20effectively%20capturing%20the%20partial%0Arelevance%20between%20the%20query%20and%20video%20is%20essential%20for%20the%20VCMR%20task.%20To%20this%0Aend%2C%20we%20propose%20a%20Partial%20Relevance%20Enhanced%20Model~%28PREM%29%20to%20improve%20VCMR.%20VCMR%0Ainvolves%20two%20sub-tasks%3A%20video%20retrieval%20and%20moment%20localization.%20To%20align%20with%0Atheir%20distinct%20objectives%2C%20we%20implement%20specialized%20partial%20relevance%0Aenhancement%20strategies.%20For%20video%20retrieval%2C%20we%20introduce%20a%20multi-modal%0Acollaborative%20video%20retriever%2C%20generating%20different%20query%20representations%20for%0Athe%20two%20modalities%20by%20modality-specific%20pooling%2C%20ensuring%20a%20more%20effective%0Amatch.%20For%20moment%20localization%2C%20we%20propose%20the%20focus-then-fuse%20moment%0Alocalizer%2C%20utilizing%20modality-specific%20gates%20to%20capture%20essential%20content.%20We%0Aalso%20introduce%20relevant%20content-enhanced%20training%20methods%20for%20both%20retriever%0Aand%20localizer%20to%20enhance%20the%20ability%20of%20model%20to%20capture%20relevant%20content.%0AExperimental%20results%20on%20TVR%20and%20DiDeMo%20datasets%20show%20that%20the%20proposed%20model%0Aoutperforms%20the%20baselines%2C%20achieving%20a%20new%20state-of-the-art%20of%20VCMR.%20The%20code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/hdy007007/PREM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13576v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Video%20Corpus%20Moment%20Retrieval%20with%20Partial%20Relevance%0A%20%20Enhancement&entry.906535625=Danyang%20Hou%20and%20Liang%20Pang%20and%20Huawei%20Shen%20and%20Xueqi%20Cheng&entry.1292438233=%20%20Video%20Corpus%20Moment%20Retrieval%20%28VCMR%29%20is%20a%20new%20video%20retrieval%20task%20aimed%20at%0Aretrieving%20a%20relevant%20moment%20from%20a%20large%20corpus%20of%20untrimmed%20videos%20using%20a%0Atext%20query.%20The%20relevance%20between%20the%20video%20and%20query%20is%20partial%2C%20mainly%0Aevident%20in%20two%20aspects%3A~%281%29~Scope%3A%20The%20untrimmed%20video%20contains%20many%20frames%2C%0Abut%20not%20all%20are%20relevant%20to%20the%20query.%20Strong%20relevance%20is%20typically%20observed%0Aonly%20within%20the%20relevant%20moment.~%282%29~Modality%3A%20The%20relevance%20of%20the%20query%0Avaries%20with%20different%20modalities.%20Action%20descriptions%20align%20more%20with%20visual%0Aelements%2C%20while%20character%20conversations%20are%20more%20related%20to%20textual%0Ainformation.Existing%20methods%20often%20treat%20all%20video%20contents%20equally%2C%20leading%20to%0Asub-optimal%20moment%20retrieval.%20We%20argue%20that%20effectively%20capturing%20the%20partial%0Arelevance%20between%20the%20query%20and%20video%20is%20essential%20for%20the%20VCMR%20task.%20To%20this%0Aend%2C%20we%20propose%20a%20Partial%20Relevance%20Enhanced%20Model~%28PREM%29%20to%20improve%20VCMR.%20VCMR%0Ainvolves%20two%20sub-tasks%3A%20video%20retrieval%20and%20moment%20localization.%20To%20align%20with%0Atheir%20distinct%20objectives%2C%20we%20implement%20specialized%20partial%20relevance%0Aenhancement%20strategies.%20For%20video%20retrieval%2C%20we%20introduce%20a%20multi-modal%0Acollaborative%20video%20retriever%2C%20generating%20different%20query%20representations%20for%0Athe%20two%20modalities%20by%20modality-specific%20pooling%2C%20ensuring%20a%20more%20effective%0Amatch.%20For%20moment%20localization%2C%20we%20propose%20the%20focus-then-fuse%20moment%0Alocalizer%2C%20utilizing%20modality-specific%20gates%20to%20capture%20essential%20content.%20We%0Aalso%20introduce%20relevant%20content-enhanced%20training%20methods%20for%20both%20retriever%0Aand%20localizer%20to%20enhance%20the%20ability%20of%20model%20to%20capture%20relevant%20content.%0AExperimental%20results%20on%20TVR%20and%20DiDeMo%20datasets%20show%20that%20the%20proposed%20model%0Aoutperforms%20the%20baselines%2C%20achieving%20a%20new%20state-of-the-art%20of%20VCMR.%20The%20code%0Ais%20available%20at%20%5Curl%7Bhttps%3A//github.com/hdy007007/PREM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13576v2&entry.124074799=Read"},
{"title": "Metric-guided Image Reconstruction Bounds via Conformal Prediction", "author": "Matt Y Cheung and Tucker J Netherton and Laurence E Court and Ashok Veeraraghavan and Guha Balakrishnan", "abstract": "  Recent advancements in machine learning have led to novel imaging systems and\nalgorithms that address ill-posed problems. Assessing their trustworthiness and\nunderstanding how to deploy them safely at test time remains an important and\nopen problem. We propose a method that leverages conformal prediction to\nretrieve upper/lower bounds and statistical inliers/outliers of reconstructions\nbased on the prediction intervals of downstream metrics. We apply our method to\nsparse-view CT for downstream radiotherapy planning and show 1) that\nmetric-guided bounds have valid coverage for downstream metrics while\nconventional pixel-wise bounds do not and 2) anatomical differences of\nupper/lower bounds between metric-guided and pixel-wise methods. Our work paves\nthe way for more meaningful reconstruction bounds. Code available at\nhttps://github.com/matthewyccheung/conformal-metric\n", "link": "http://arxiv.org/abs/2404.15274v1", "date": "2024-04-23", "relevancy": 2.1224, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5376}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5265}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Metric-guided%20Image%20Reconstruction%20Bounds%20via%20Conformal%20Prediction&body=Title%3A%20Metric-guided%20Image%20Reconstruction%20Bounds%20via%20Conformal%20Prediction%0AAuthor%3A%20Matt%20Y%20Cheung%20and%20Tucker%20J%20Netherton%20and%20Laurence%20E%20Court%20and%20Ashok%20Veeraraghavan%20and%20Guha%20Balakrishnan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20machine%20learning%20have%20led%20to%20novel%20imaging%20systems%20and%0Aalgorithms%20that%20address%20ill-posed%20problems.%20Assessing%20their%20trustworthiness%20and%0Aunderstanding%20how%20to%20deploy%20them%20safely%20at%20test%20time%20remains%20an%20important%20and%0Aopen%20problem.%20We%20propose%20a%20method%20that%20leverages%20conformal%20prediction%20to%0Aretrieve%20upper/lower%20bounds%20and%20statistical%20inliers/outliers%20of%20reconstructions%0Abased%20on%20the%20prediction%20intervals%20of%20downstream%20metrics.%20We%20apply%20our%20method%20to%0Asparse-view%20CT%20for%20downstream%20radiotherapy%20planning%20and%20show%201%29%20that%0Ametric-guided%20bounds%20have%20valid%20coverage%20for%20downstream%20metrics%20while%0Aconventional%20pixel-wise%20bounds%20do%20not%20and%202%29%20anatomical%20differences%20of%0Aupper/lower%20bounds%20between%20metric-guided%20and%20pixel-wise%20methods.%20Our%20work%20paves%0Athe%20way%20for%20more%20meaningful%20reconstruction%20bounds.%20Code%20available%20at%0Ahttps%3A//github.com/matthewyccheung/conformal-metric%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15274v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric-guided%20Image%20Reconstruction%20Bounds%20via%20Conformal%20Prediction&entry.906535625=Matt%20Y%20Cheung%20and%20Tucker%20J%20Netherton%20and%20Laurence%20E%20Court%20and%20Ashok%20Veeraraghavan%20and%20Guha%20Balakrishnan&entry.1292438233=%20%20Recent%20advancements%20in%20machine%20learning%20have%20led%20to%20novel%20imaging%20systems%20and%0Aalgorithms%20that%20address%20ill-posed%20problems.%20Assessing%20their%20trustworthiness%20and%0Aunderstanding%20how%20to%20deploy%20them%20safely%20at%20test%20time%20remains%20an%20important%20and%0Aopen%20problem.%20We%20propose%20a%20method%20that%20leverages%20conformal%20prediction%20to%0Aretrieve%20upper/lower%20bounds%20and%20statistical%20inliers/outliers%20of%20reconstructions%0Abased%20on%20the%20prediction%20intervals%20of%20downstream%20metrics.%20We%20apply%20our%20method%20to%0Asparse-view%20CT%20for%20downstream%20radiotherapy%20planning%20and%20show%201%29%20that%0Ametric-guided%20bounds%20have%20valid%20coverage%20for%20downstream%20metrics%20while%0Aconventional%20pixel-wise%20bounds%20do%20not%20and%202%29%20anatomical%20differences%20of%0Aupper/lower%20bounds%20between%20metric-guided%20and%20pixel-wise%20methods.%20Our%20work%20paves%0Athe%20way%20for%20more%20meaningful%20reconstruction%20bounds.%20Code%20available%20at%0Ahttps%3A//github.com/matthewyccheung/conformal-metric%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15274v1&entry.124074799=Read"},
{"title": "Importance of Disjoint Sampling in Conventional and Transformer Models\n  for Hyperspectral Image Classification", "author": "Muhammad Ahmad and Manuel Mazzara and Salvatore Distifano", "abstract": "  Disjoint sampling is critical for rigorous and unbiased evaluation of\nstate-of-the-art (SOTA) models. When training, validation, and test sets\noverlap or share data, it introduces a bias that inflates performance metrics\nand prevents accurate assessment of a model's true ability to generalize to new\nexamples. This paper presents an innovative disjoint sampling approach for\ntraining SOTA models on Hyperspectral image classification (HSIC) tasks. By\nseparating training, validation, and test data without overlap, the proposed\nmethod facilitates a fairer evaluation of how well a model can classify pixels\nit was not exposed to during training or validation. Experiments demonstrate\nthe approach significantly improves a model's generalization compared to\nalternatives that include training and validation data in test data. By\neliminating data leakage between sets, disjoint sampling provides reliable\nmetrics for benchmarking progress in HSIC. Researchers can have confidence that\nreported performance truly reflects a model's capabilities for classifying new\nscenes, not just memorized pixels. This rigorous methodology is critical for\nadvancing SOTA models and their real-world application to large-scale land\nmapping with Hyperspectral sensors.\n  The source code is available at\nhttps://github.com/mahmad00/Disjoint-Sampling-for-Hyperspectral-Image-Classification.\n", "link": "http://arxiv.org/abs/2404.14944v1", "date": "2024-04-23", "relevancy": 2.1186, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5471}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5291}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5125}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Importance%20of%20Disjoint%20Sampling%20in%20Conventional%20and%20Transformer%20Models%0A%20%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20Importance%20of%20Disjoint%20Sampling%20in%20Conventional%20and%20Transformer%20Models%0A%20%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Manuel%20Mazzara%20and%20Salvatore%20Distifano%0AAbstract%3A%20%20%20Disjoint%20sampling%20is%20critical%20for%20rigorous%20and%20unbiased%20evaluation%20of%0Astate-of-the-art%20%28SOTA%29%20models.%20When%20training%2C%20validation%2C%20and%20test%20sets%0Aoverlap%20or%20share%20data%2C%20it%20introduces%20a%20bias%20that%20inflates%20performance%20metrics%0Aand%20prevents%20accurate%20assessment%20of%20a%20model%27s%20true%20ability%20to%20generalize%20to%20new%0Aexamples.%20This%20paper%20presents%20an%20innovative%20disjoint%20sampling%20approach%20for%0Atraining%20SOTA%20models%20on%20Hyperspectral%20image%20classification%20%28HSIC%29%20tasks.%20By%0Aseparating%20training%2C%20validation%2C%20and%20test%20data%20without%20overlap%2C%20the%20proposed%0Amethod%20facilitates%20a%20fairer%20evaluation%20of%20how%20well%20a%20model%20can%20classify%20pixels%0Ait%20was%20not%20exposed%20to%20during%20training%20or%20validation.%20Experiments%20demonstrate%0Athe%20approach%20significantly%20improves%20a%20model%27s%20generalization%20compared%20to%0Aalternatives%20that%20include%20training%20and%20validation%20data%20in%20test%20data.%20By%0Aeliminating%20data%20leakage%20between%20sets%2C%20disjoint%20sampling%20provides%20reliable%0Ametrics%20for%20benchmarking%20progress%20in%20HSIC.%20Researchers%20can%20have%20confidence%20that%0Areported%20performance%20truly%20reflects%20a%20model%27s%20capabilities%20for%20classifying%20new%0Ascenes%2C%20not%20just%20memorized%20pixels.%20This%20rigorous%20methodology%20is%20critical%20for%0Aadvancing%20SOTA%20models%20and%20their%20real-world%20application%20to%20large-scale%20land%0Amapping%20with%20Hyperspectral%20sensors.%0A%20%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/mahmad00/Disjoint-Sampling-for-Hyperspectral-Image-Classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14944v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Importance%20of%20Disjoint%20Sampling%20in%20Conventional%20and%20Transformer%20Models%0A%20%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Muhammad%20Ahmad%20and%20Manuel%20Mazzara%20and%20Salvatore%20Distifano&entry.1292438233=%20%20Disjoint%20sampling%20is%20critical%20for%20rigorous%20and%20unbiased%20evaluation%20of%0Astate-of-the-art%20%28SOTA%29%20models.%20When%20training%2C%20validation%2C%20and%20test%20sets%0Aoverlap%20or%20share%20data%2C%20it%20introduces%20a%20bias%20that%20inflates%20performance%20metrics%0Aand%20prevents%20accurate%20assessment%20of%20a%20model%27s%20true%20ability%20to%20generalize%20to%20new%0Aexamples.%20This%20paper%20presents%20an%20innovative%20disjoint%20sampling%20approach%20for%0Atraining%20SOTA%20models%20on%20Hyperspectral%20image%20classification%20%28HSIC%29%20tasks.%20By%0Aseparating%20training%2C%20validation%2C%20and%20test%20data%20without%20overlap%2C%20the%20proposed%0Amethod%20facilitates%20a%20fairer%20evaluation%20of%20how%20well%20a%20model%20can%20classify%20pixels%0Ait%20was%20not%20exposed%20to%20during%20training%20or%20validation.%20Experiments%20demonstrate%0Athe%20approach%20significantly%20improves%20a%20model%27s%20generalization%20compared%20to%0Aalternatives%20that%20include%20training%20and%20validation%20data%20in%20test%20data.%20By%0Aeliminating%20data%20leakage%20between%20sets%2C%20disjoint%20sampling%20provides%20reliable%0Ametrics%20for%20benchmarking%20progress%20in%20HSIC.%20Researchers%20can%20have%20confidence%20that%0Areported%20performance%20truly%20reflects%20a%20model%27s%20capabilities%20for%20classifying%20new%0Ascenes%2C%20not%20just%20memorized%20pixels.%20This%20rigorous%20methodology%20is%20critical%20for%0Aadvancing%20SOTA%20models%20and%20their%20real-world%20application%20to%20large-scale%20land%0Amapping%20with%20Hyperspectral%20sensors.%0A%20%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/mahmad00/Disjoint-Sampling-for-Hyperspectral-Image-Classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14944v1&entry.124074799=Read"},
{"title": "Subobject-level Image Tokenization", "author": "Delong Chen and Samuel Cahyawijaya and Jianfeng Liu and Baoyuan Wang and Pascale Fung", "abstract": "  Transformer-based vision models typically tokenize images into fixed-size\nsquare patches as input units, which lacks the adaptability to image content\nand overlooks the inherent pixel grouping structure. Inspired by the subword\ntokenization widely adopted in language models, we propose an image tokenizer\nat a subobject level, where the subobjects are represented by semantically\nmeaningful image segments obtained by segmentation models (e.g., segment\nanything models). To implement a learning system based on subobject\ntokenization, we first introduced a Direct Segment Anything Model (DirectSAM)\nthat efficiently produces comprehensive segmentation of subobjects, then embed\nsubobjects into compact latent vectors and fed them into a large language model\nfor vision language learning. Empirical results demonstrated that our\nsubobject-level tokenization significantly facilitates efficient learning of\ntranslating images into object and attribute descriptions compared to the\ntraditional patch-level tokenization. Codes and models are open-sourced at\nhttps://github.com/ChenDelong1999/subobjects.\n", "link": "http://arxiv.org/abs/2402.14327v2", "date": "2024-04-23", "relevancy": 2.113, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5374}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.504}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Subobject-level%20Image%20Tokenization&body=Title%3A%20Subobject-level%20Image%20Tokenization%0AAuthor%3A%20Delong%20Chen%20and%20Samuel%20Cahyawijaya%20and%20Jianfeng%20Liu%20and%20Baoyuan%20Wang%20and%20Pascale%20Fung%0AAbstract%3A%20%20%20Transformer-based%20vision%20models%20typically%20tokenize%20images%20into%20fixed-size%0Asquare%20patches%20as%20input%20units%2C%20which%20lacks%20the%20adaptability%20to%20image%20content%0Aand%20overlooks%20the%20inherent%20pixel%20grouping%20structure.%20Inspired%20by%20the%20subword%0Atokenization%20widely%20adopted%20in%20language%20models%2C%20we%20propose%20an%20image%20tokenizer%0Aat%20a%20subobject%20level%2C%20where%20the%20subobjects%20are%20represented%20by%20semantically%0Ameaningful%20image%20segments%20obtained%20by%20segmentation%20models%20%28e.g.%2C%20segment%0Aanything%20models%29.%20To%20implement%20a%20learning%20system%20based%20on%20subobject%0Atokenization%2C%20we%20first%20introduced%20a%20Direct%20Segment%20Anything%20Model%20%28DirectSAM%29%0Athat%20efficiently%20produces%20comprehensive%20segmentation%20of%20subobjects%2C%20then%20embed%0Asubobjects%20into%20compact%20latent%20vectors%20and%20fed%20them%20into%20a%20large%20language%20model%0Afor%20vision%20language%20learning.%20Empirical%20results%20demonstrated%20that%20our%0Asubobject-level%20tokenization%20significantly%20facilitates%20efficient%20learning%20of%0Atranslating%20images%20into%20object%20and%20attribute%20descriptions%20compared%20to%20the%0Atraditional%20patch-level%20tokenization.%20Codes%20and%20models%20are%20open-sourced%20at%0Ahttps%3A//github.com/ChenDelong1999/subobjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14327v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subobject-level%20Image%20Tokenization&entry.906535625=Delong%20Chen%20and%20Samuel%20Cahyawijaya%20and%20Jianfeng%20Liu%20and%20Baoyuan%20Wang%20and%20Pascale%20Fung&entry.1292438233=%20%20Transformer-based%20vision%20models%20typically%20tokenize%20images%20into%20fixed-size%0Asquare%20patches%20as%20input%20units%2C%20which%20lacks%20the%20adaptability%20to%20image%20content%0Aand%20overlooks%20the%20inherent%20pixel%20grouping%20structure.%20Inspired%20by%20the%20subword%0Atokenization%20widely%20adopted%20in%20language%20models%2C%20we%20propose%20an%20image%20tokenizer%0Aat%20a%20subobject%20level%2C%20where%20the%20subobjects%20are%20represented%20by%20semantically%0Ameaningful%20image%20segments%20obtained%20by%20segmentation%20models%20%28e.g.%2C%20segment%0Aanything%20models%29.%20To%20implement%20a%20learning%20system%20based%20on%20subobject%0Atokenization%2C%20we%20first%20introduced%20a%20Direct%20Segment%20Anything%20Model%20%28DirectSAM%29%0Athat%20efficiently%20produces%20comprehensive%20segmentation%20of%20subobjects%2C%20then%20embed%0Asubobjects%20into%20compact%20latent%20vectors%20and%20fed%20them%20into%20a%20large%20language%20model%0Afor%20vision%20language%20learning.%20Empirical%20results%20demonstrated%20that%20our%0Asubobject-level%20tokenization%20significantly%20facilitates%20efficient%20learning%20of%0Atranslating%20images%20into%20object%20and%20attribute%20descriptions%20compared%20to%20the%0Atraditional%20patch-level%20tokenization.%20Codes%20and%20models%20are%20open-sourced%20at%0Ahttps%3A//github.com/ChenDelong1999/subobjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14327v2&entry.124074799=Read"},
{"title": "Driver Activity Classification Using Generalizable Representations from\n  Vision-Language Models", "author": "Ross Greer and Mathias Viborg Andersen and Andreas M\u00f8gelmose and Mohan Trivedi", "abstract": "  Driver activity classification is crucial for ensuring road safety, with\napplications ranging from driver assistance systems to autonomous vehicle\ncontrol transitions. In this paper, we present a novel approach leveraging\ngeneralizable representations from vision-language models for driver activity\nclassification. Our method employs a Semantic Representation Late Fusion Neural\nNetwork (SRLF-Net) to process synchronized video frames from multiple\nperspectives. Each frame is encoded using a pretrained vision-language encoder,\nand the resulting embeddings are fused to generate class probability\npredictions. By leveraging contrastively-learned vision-language\nrepresentations, our approach achieves robust performance across diverse driver\nactivities. We evaluate our method on the Naturalistic Driving Action\nRecognition Dataset, demonstrating strong accuracy across many classes. Our\nresults suggest that vision-language representations offer a promising avenue\nfor driver monitoring systems, providing both accuracy and interpretability\nthrough natural language descriptors.\n", "link": "http://arxiv.org/abs/2404.14906v1", "date": "2024-04-23", "relevancy": 2.1065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5574}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5286}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5124}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Driver%20Activity%20Classification%20Using%20Generalizable%20Representations%20from%0A%20%20Vision-Language%20Models&body=Title%3A%20Driver%20Activity%20Classification%20Using%20Generalizable%20Representations%20from%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Ross%20Greer%20and%20Mathias%20Viborg%20Andersen%20and%20Andreas%20M%C3%B8gelmose%20and%20Mohan%20Trivedi%0AAbstract%3A%20%20%20Driver%20activity%20classification%20is%20crucial%20for%20ensuring%20road%20safety%2C%20with%0Aapplications%20ranging%20from%20driver%20assistance%20systems%20to%20autonomous%20vehicle%0Acontrol%20transitions.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20leveraging%0Ageneralizable%20representations%20from%20vision-language%20models%20for%20driver%20activity%0Aclassification.%20Our%20method%20employs%20a%20Semantic%20Representation%20Late%20Fusion%20Neural%0ANetwork%20%28SRLF-Net%29%20to%20process%20synchronized%20video%20frames%20from%20multiple%0Aperspectives.%20Each%20frame%20is%20encoded%20using%20a%20pretrained%20vision-language%20encoder%2C%0Aand%20the%20resulting%20embeddings%20are%20fused%20to%20generate%20class%20probability%0Apredictions.%20By%20leveraging%20contrastively-learned%20vision-language%0Arepresentations%2C%20our%20approach%20achieves%20robust%20performance%20across%20diverse%20driver%0Aactivities.%20We%20evaluate%20our%20method%20on%20the%20Naturalistic%20Driving%20Action%0ARecognition%20Dataset%2C%20demonstrating%20strong%20accuracy%20across%20many%20classes.%20Our%0Aresults%20suggest%20that%20vision-language%20representations%20offer%20a%20promising%20avenue%0Afor%20driver%20monitoring%20systems%2C%20providing%20both%20accuracy%20and%20interpretability%0Athrough%20natural%20language%20descriptors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14906v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driver%20Activity%20Classification%20Using%20Generalizable%20Representations%20from%0A%20%20Vision-Language%20Models&entry.906535625=Ross%20Greer%20and%20Mathias%20Viborg%20Andersen%20and%20Andreas%20M%C3%B8gelmose%20and%20Mohan%20Trivedi&entry.1292438233=%20%20Driver%20activity%20classification%20is%20crucial%20for%20ensuring%20road%20safety%2C%20with%0Aapplications%20ranging%20from%20driver%20assistance%20systems%20to%20autonomous%20vehicle%0Acontrol%20transitions.%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20leveraging%0Ageneralizable%20representations%20from%20vision-language%20models%20for%20driver%20activity%0Aclassification.%20Our%20method%20employs%20a%20Semantic%20Representation%20Late%20Fusion%20Neural%0ANetwork%20%28SRLF-Net%29%20to%20process%20synchronized%20video%20frames%20from%20multiple%0Aperspectives.%20Each%20frame%20is%20encoded%20using%20a%20pretrained%20vision-language%20encoder%2C%0Aand%20the%20resulting%20embeddings%20are%20fused%20to%20generate%20class%20probability%0Apredictions.%20By%20leveraging%20contrastively-learned%20vision-language%0Arepresentations%2C%20our%20approach%20achieves%20robust%20performance%20across%20diverse%20driver%0Aactivities.%20We%20evaluate%20our%20method%20on%20the%20Naturalistic%20Driving%20Action%0ARecognition%20Dataset%2C%20demonstrating%20strong%20accuracy%20across%20many%20classes.%20Our%0Aresults%20suggest%20that%20vision-language%20representations%20offer%20a%20promising%20avenue%0Afor%20driver%20monitoring%20systems%2C%20providing%20both%20accuracy%20and%20interpretability%0Athrough%20natural%20language%20descriptors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14906v1&entry.124074799=Read"},
{"title": "Direct Zernike Coefficient Prediction from Point Spread Functions and\n  Extended Images using Deep Learning", "author": "Yong En Kok and Alexander Bentley and Andrew Parkes and Amanda J. Wright and Michael G. Somekh and Michael Pound", "abstract": "  Optical imaging quality can be severely degraded by system and sample induced\naberrations. Existing adaptive optics systems typically rely on iterative\nsearch algorithm to correct for aberrations and improve images. This study\ndemonstrates the application of convolutional neural networks to characterise\nthe optical aberration by directly predicting the Zernike coefficients from two\nto three phase-diverse optical images. We evaluated our network on 600,000\nsimulated Point Spread Function (PSF) datasets randomly generated within the\nrange of -1 to 1 radians using the first 25 Zernike coefficients. The results\nshow that using only three phase-diverse images captured above, below and at\nthe focal plane with an amplitude of 1 achieves a low RMSE of 0.10 radians on\nthe simulated PSF dataset. Furthermore, this approach directly predicts Zernike\nmodes simulated extended 2D samples, while maintaining a comparable RMSE of\n0.15 radians. We demonstrate that this approach is effective using only a\nsingle prediction step, or can be iterated a small number of times. This simple\nand straightforward technique provides rapid and accurate method for predicting\nthe aberration correction using three or less phase-diverse images, paving the\nway for evaluation on real-world dataset.\n", "link": "http://arxiv.org/abs/2404.15231v1", "date": "2024-04-23", "relevancy": 2.1037, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5354}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5277}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5158}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Direct%20Zernike%20Coefficient%20Prediction%20from%20Point%20Spread%20Functions%20and%0A%20%20Extended%20Images%20using%20Deep%20Learning&body=Title%3A%20Direct%20Zernike%20Coefficient%20Prediction%20from%20Point%20Spread%20Functions%20and%0A%20%20Extended%20Images%20using%20Deep%20Learning%0AAuthor%3A%20Yong%20En%20Kok%20and%20Alexander%20Bentley%20and%20Andrew%20Parkes%20and%20Amanda%20J.%20Wright%20and%20Michael%20G.%20Somekh%20and%20Michael%20Pound%0AAbstract%3A%20%20%20Optical%20imaging%20quality%20can%20be%20severely%20degraded%20by%20system%20and%20sample%20induced%0Aaberrations.%20Existing%20adaptive%20optics%20systems%20typically%20rely%20on%20iterative%0Asearch%20algorithm%20to%20correct%20for%20aberrations%20and%20improve%20images.%20This%20study%0Ademonstrates%20the%20application%20of%20convolutional%20neural%20networks%20to%20characterise%0Athe%20optical%20aberration%20by%20directly%20predicting%20the%20Zernike%20coefficients%20from%20two%0Ato%20three%20phase-diverse%20optical%20images.%20We%20evaluated%20our%20network%20on%20600%2C000%0Asimulated%20Point%20Spread%20Function%20%28PSF%29%20datasets%20randomly%20generated%20within%20the%0Arange%20of%20-1%20to%201%20radians%20using%20the%20first%2025%20Zernike%20coefficients.%20The%20results%0Ashow%20that%20using%20only%20three%20phase-diverse%20images%20captured%20above%2C%20below%20and%20at%0Athe%20focal%20plane%20with%20an%20amplitude%20of%201%20achieves%20a%20low%20RMSE%20of%200.10%20radians%20on%0Athe%20simulated%20PSF%20dataset.%20Furthermore%2C%20this%20approach%20directly%20predicts%20Zernike%0Amodes%20simulated%20extended%202D%20samples%2C%20while%20maintaining%20a%20comparable%20RMSE%20of%0A0.15%20radians.%20We%20demonstrate%20that%20this%20approach%20is%20effective%20using%20only%20a%0Asingle%20prediction%20step%2C%20or%20can%20be%20iterated%20a%20small%20number%20of%20times.%20This%20simple%0Aand%20straightforward%20technique%20provides%20rapid%20and%20accurate%20method%20for%20predicting%0Athe%20aberration%20correction%20using%20three%20or%20less%20phase-diverse%20images%2C%20paving%20the%0Away%20for%20evaluation%20on%20real-world%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15231v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Zernike%20Coefficient%20Prediction%20from%20Point%20Spread%20Functions%20and%0A%20%20Extended%20Images%20using%20Deep%20Learning&entry.906535625=Yong%20En%20Kok%20and%20Alexander%20Bentley%20and%20Andrew%20Parkes%20and%20Amanda%20J.%20Wright%20and%20Michael%20G.%20Somekh%20and%20Michael%20Pound&entry.1292438233=%20%20Optical%20imaging%20quality%20can%20be%20severely%20degraded%20by%20system%20and%20sample%20induced%0Aaberrations.%20Existing%20adaptive%20optics%20systems%20typically%20rely%20on%20iterative%0Asearch%20algorithm%20to%20correct%20for%20aberrations%20and%20improve%20images.%20This%20study%0Ademonstrates%20the%20application%20of%20convolutional%20neural%20networks%20to%20characterise%0Athe%20optical%20aberration%20by%20directly%20predicting%20the%20Zernike%20coefficients%20from%20two%0Ato%20three%20phase-diverse%20optical%20images.%20We%20evaluated%20our%20network%20on%20600%2C000%0Asimulated%20Point%20Spread%20Function%20%28PSF%29%20datasets%20randomly%20generated%20within%20the%0Arange%20of%20-1%20to%201%20radians%20using%20the%20first%2025%20Zernike%20coefficients.%20The%20results%0Ashow%20that%20using%20only%20three%20phase-diverse%20images%20captured%20above%2C%20below%20and%20at%0Athe%20focal%20plane%20with%20an%20amplitude%20of%201%20achieves%20a%20low%20RMSE%20of%200.10%20radians%20on%0Athe%20simulated%20PSF%20dataset.%20Furthermore%2C%20this%20approach%20directly%20predicts%20Zernike%0Amodes%20simulated%20extended%202D%20samples%2C%20while%20maintaining%20a%20comparable%20RMSE%20of%0A0.15%20radians.%20We%20demonstrate%20that%20this%20approach%20is%20effective%20using%20only%20a%0Asingle%20prediction%20step%2C%20or%20can%20be%20iterated%20a%20small%20number%20of%20times.%20This%20simple%0Aand%20straightforward%20technique%20provides%20rapid%20and%20accurate%20method%20for%20predicting%0Athe%20aberration%20correction%20using%20three%20or%20less%20phase-diverse%20images%2C%20paving%20the%0Away%20for%20evaluation%20on%20real-world%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15231v1&entry.124074799=Read"},
{"title": "StreakNet-Arch: An Anti-scattering Network-based Architecture for\n  Underwater Carrier LiDAR-Radar Imaging", "author": "Xuelong Li and Hongjun An and Guangying Li and Xing Wang and Guanghua Cheng and Zhe Sun", "abstract": "  In this paper, we introduce StreakNet-Arch, a novel signal processing\narchitecture designed for Underwater Carrier LiDAR-Radar (UCLR) imaging\nsystems, to address the limitations in scatter suppression and real-time\nimaging. StreakNet-Arch formulates the signal processing as a real-time,\nend-to-end binary classification task, enabling real-time image acquisition. To\nachieve this, we leverage Self-Attention networks and propose a novel Double\nBranch Cross Attention (DBC-Attention) mechanism that surpasses the performance\nof traditional methods. Furthermore, we present a method for embedding\nstreak-tube camera images into attention networks, effectively acting as a\nlearned bandpass filter. To facilitate further research, we contribute a\npublicly available streak-tube camera image dataset. The dataset contains\n2,695,168 real-world underwater 3D point cloud data. These advancements\nsignificantly improve UCLR capabilities, enhancing its performance and\napplicability in underwater imaging tasks. The source code and dataset can be\nfound at https://github.com/BestAnHongjun/StreakNet .\n", "link": "http://arxiv.org/abs/2404.09158v2", "date": "2024-04-23", "relevancy": 2.0805, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5326}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5215}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5138}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20StreakNet-Arch%3A%20An%20Anti-scattering%20Network-based%20Architecture%20for%0A%20%20Underwater%20Carrier%20LiDAR-Radar%20Imaging&body=Title%3A%20StreakNet-Arch%3A%20An%20Anti-scattering%20Network-based%20Architecture%20for%0A%20%20Underwater%20Carrier%20LiDAR-Radar%20Imaging%0AAuthor%3A%20Xuelong%20Li%20and%20Hongjun%20An%20and%20Guangying%20Li%20and%20Xing%20Wang%20and%20Guanghua%20Cheng%20and%20Zhe%20Sun%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20StreakNet-Arch%2C%20a%20novel%20signal%20processing%0Aarchitecture%20designed%20for%20Underwater%20Carrier%20LiDAR-Radar%20%28UCLR%29%20imaging%0Asystems%2C%20to%20address%20the%20limitations%20in%20scatter%20suppression%20and%20real-time%0Aimaging.%20StreakNet-Arch%20formulates%20the%20signal%20processing%20as%20a%20real-time%2C%0Aend-to-end%20binary%20classification%20task%2C%20enabling%20real-time%20image%20acquisition.%20To%0Aachieve%20this%2C%20we%20leverage%20Self-Attention%20networks%20and%20propose%20a%20novel%20Double%0ABranch%20Cross%20Attention%20%28DBC-Attention%29%20mechanism%20that%20surpasses%20the%20performance%0Aof%20traditional%20methods.%20Furthermore%2C%20we%20present%20a%20method%20for%20embedding%0Astreak-tube%20camera%20images%20into%20attention%20networks%2C%20effectively%20acting%20as%20a%0Alearned%20bandpass%20filter.%20To%20facilitate%20further%20research%2C%20we%20contribute%20a%0Apublicly%20available%20streak-tube%20camera%20image%20dataset.%20The%20dataset%20contains%0A2%2C695%2C168%20real-world%20underwater%203D%20point%20cloud%20data.%20These%20advancements%0Asignificantly%20improve%20UCLR%20capabilities%2C%20enhancing%20its%20performance%20and%0Aapplicability%20in%20underwater%20imaging%20tasks.%20The%20source%20code%20and%20dataset%20can%20be%0Afound%20at%20https%3A//github.com/BestAnHongjun/StreakNet%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09158v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreakNet-Arch%3A%20An%20Anti-scattering%20Network-based%20Architecture%20for%0A%20%20Underwater%20Carrier%20LiDAR-Radar%20Imaging&entry.906535625=Xuelong%20Li%20and%20Hongjun%20An%20and%20Guangying%20Li%20and%20Xing%20Wang%20and%20Guanghua%20Cheng%20and%20Zhe%20Sun&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20StreakNet-Arch%2C%20a%20novel%20signal%20processing%0Aarchitecture%20designed%20for%20Underwater%20Carrier%20LiDAR-Radar%20%28UCLR%29%20imaging%0Asystems%2C%20to%20address%20the%20limitations%20in%20scatter%20suppression%20and%20real-time%0Aimaging.%20StreakNet-Arch%20formulates%20the%20signal%20processing%20as%20a%20real-time%2C%0Aend-to-end%20binary%20classification%20task%2C%20enabling%20real-time%20image%20acquisition.%20To%0Aachieve%20this%2C%20we%20leverage%20Self-Attention%20networks%20and%20propose%20a%20novel%20Double%0ABranch%20Cross%20Attention%20%28DBC-Attention%29%20mechanism%20that%20surpasses%20the%20performance%0Aof%20traditional%20methods.%20Furthermore%2C%20we%20present%20a%20method%20for%20embedding%0Astreak-tube%20camera%20images%20into%20attention%20networks%2C%20effectively%20acting%20as%20a%0Alearned%20bandpass%20filter.%20To%20facilitate%20further%20research%2C%20we%20contribute%20a%0Apublicly%20available%20streak-tube%20camera%20image%20dataset.%20The%20dataset%20contains%0A2%2C695%2C168%20real-world%20underwater%203D%20point%20cloud%20data.%20These%20advancements%0Asignificantly%20improve%20UCLR%20capabilities%2C%20enhancing%20its%20performance%20and%0Aapplicability%20in%20underwater%20imaging%20tasks.%20The%20source%20code%20and%20dataset%20can%20be%0Afound%20at%20https%3A//github.com/BestAnHongjun/StreakNet%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09158v2&entry.124074799=Read"},
{"title": "Decoupling Long- and Short-Term Patterns in Spatiotemporal Inference", "author": "Junfeng Hu and Yuxuan Liang and Zhencheng Fan and Li Liu and Yifang Yin and Roger Zimmermann", "abstract": "  Sensors are the key to environmental monitoring, which impart benefits to\nsmart cities in many aspects, such as providing real-time air quality\ninformation to assist human decision-making. However, it is impractical to\ndeploy massive sensors due to the expensive costs, resulting in sparse data\ncollection. Therefore, how to get fine-grained data measurement has long been a\npressing issue. In this paper, we aim to infer values at non-sensor locations\nbased on observations from available sensors (termed spatiotemporal inference),\nwhere capturing spatiotemporal relationships among the data plays a critical\nrole. Our investigations reveal two significant insights that have not been\nexplored by previous works. Firstly, data exhibits distinct patterns at both\nlong- and short-term temporal scales, which should be analyzed separately.\nSecondly, short-term patterns contain more delicate relations including those\nacross spatial and temporal dimensions simultaneously, while long-term patterns\ninvolve high-level temporal trends. Based on these observations, we propose to\ndecouple the modeling of short-term and long-term patterns. Specifically, we\nintroduce a joint spatiotemporal graph attention network to learn the relations\nacross space and time for short-term patterns. Furthermore, we propose a graph\nrecurrent network with a time skip strategy to alleviate the gradient vanishing\nproblem and model the long-term dependencies. Experimental results on four\npublic real-world datasets demonstrate that our method effectively captures\nboth long- and short-term relations, achieving state-of-the-art performance\nagainst existing methods.\n", "link": "http://arxiv.org/abs/2109.09506v3", "date": "2024-04-23", "relevancy": 2.0565, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5277}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5053}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5023}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Long-%20and%20Short-Term%20Patterns%20in%20Spatiotemporal%20Inference&body=Title%3A%20Decoupling%20Long-%20and%20Short-Term%20Patterns%20in%20Spatiotemporal%20Inference%0AAuthor%3A%20Junfeng%20Hu%20and%20Yuxuan%20Liang%20and%20Zhencheng%20Fan%20and%20Li%20Liu%20and%20Yifang%20Yin%20and%20Roger%20Zimmermann%0AAbstract%3A%20%20%20Sensors%20are%20the%20key%20to%20environmental%20monitoring%2C%20which%20impart%20benefits%20to%0Asmart%20cities%20in%20many%20aspects%2C%20such%20as%20providing%20real-time%20air%20quality%0Ainformation%20to%20assist%20human%20decision-making.%20However%2C%20it%20is%20impractical%20to%0Adeploy%20massive%20sensors%20due%20to%20the%20expensive%20costs%2C%20resulting%20in%20sparse%20data%0Acollection.%20Therefore%2C%20how%20to%20get%20fine-grained%20data%20measurement%20has%20long%20been%20a%0Apressing%20issue.%20In%20this%20paper%2C%20we%20aim%20to%20infer%20values%20at%20non-sensor%20locations%0Abased%20on%20observations%20from%20available%20sensors%20%28termed%20spatiotemporal%20inference%29%2C%0Awhere%20capturing%20spatiotemporal%20relationships%20among%20the%20data%20plays%20a%20critical%0Arole.%20Our%20investigations%20reveal%20two%20significant%20insights%20that%20have%20not%20been%0Aexplored%20by%20previous%20works.%20Firstly%2C%20data%20exhibits%20distinct%20patterns%20at%20both%0Along-%20and%20short-term%20temporal%20scales%2C%20which%20should%20be%20analyzed%20separately.%0ASecondly%2C%20short-term%20patterns%20contain%20more%20delicate%20relations%20including%20those%0Aacross%20spatial%20and%20temporal%20dimensions%20simultaneously%2C%20while%20long-term%20patterns%0Ainvolve%20high-level%20temporal%20trends.%20Based%20on%20these%20observations%2C%20we%20propose%20to%0Adecouple%20the%20modeling%20of%20short-term%20and%20long-term%20patterns.%20Specifically%2C%20we%0Aintroduce%20a%20joint%20spatiotemporal%20graph%20attention%20network%20to%20learn%20the%20relations%0Aacross%20space%20and%20time%20for%20short-term%20patterns.%20Furthermore%2C%20we%20propose%20a%20graph%0Arecurrent%20network%20with%20a%20time%20skip%20strategy%20to%20alleviate%20the%20gradient%20vanishing%0Aproblem%20and%20model%20the%20long-term%20dependencies.%20Experimental%20results%20on%20four%0Apublic%20real-world%20datasets%20demonstrate%20that%20our%20method%20effectively%20captures%0Aboth%20long-%20and%20short-term%20relations%2C%20achieving%20state-of-the-art%20performance%0Aagainst%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2109.09506v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Long-%20and%20Short-Term%20Patterns%20in%20Spatiotemporal%20Inference&entry.906535625=Junfeng%20Hu%20and%20Yuxuan%20Liang%20and%20Zhencheng%20Fan%20and%20Li%20Liu%20and%20Yifang%20Yin%20and%20Roger%20Zimmermann&entry.1292438233=%20%20Sensors%20are%20the%20key%20to%20environmental%20monitoring%2C%20which%20impart%20benefits%20to%0Asmart%20cities%20in%20many%20aspects%2C%20such%20as%20providing%20real-time%20air%20quality%0Ainformation%20to%20assist%20human%20decision-making.%20However%2C%20it%20is%20impractical%20to%0Adeploy%20massive%20sensors%20due%20to%20the%20expensive%20costs%2C%20resulting%20in%20sparse%20data%0Acollection.%20Therefore%2C%20how%20to%20get%20fine-grained%20data%20measurement%20has%20long%20been%20a%0Apressing%20issue.%20In%20this%20paper%2C%20we%20aim%20to%20infer%20values%20at%20non-sensor%20locations%0Abased%20on%20observations%20from%20available%20sensors%20%28termed%20spatiotemporal%20inference%29%2C%0Awhere%20capturing%20spatiotemporal%20relationships%20among%20the%20data%20plays%20a%20critical%0Arole.%20Our%20investigations%20reveal%20two%20significant%20insights%20that%20have%20not%20been%0Aexplored%20by%20previous%20works.%20Firstly%2C%20data%20exhibits%20distinct%20patterns%20at%20both%0Along-%20and%20short-term%20temporal%20scales%2C%20which%20should%20be%20analyzed%20separately.%0ASecondly%2C%20short-term%20patterns%20contain%20more%20delicate%20relations%20including%20those%0Aacross%20spatial%20and%20temporal%20dimensions%20simultaneously%2C%20while%20long-term%20patterns%0Ainvolve%20high-level%20temporal%20trends.%20Based%20on%20these%20observations%2C%20we%20propose%20to%0Adecouple%20the%20modeling%20of%20short-term%20and%20long-term%20patterns.%20Specifically%2C%20we%0Aintroduce%20a%20joint%20spatiotemporal%20graph%20attention%20network%20to%20learn%20the%20relations%0Aacross%20space%20and%20time%20for%20short-term%20patterns.%20Furthermore%2C%20we%20propose%20a%20graph%0Arecurrent%20network%20with%20a%20time%20skip%20strategy%20to%20alleviate%20the%20gradient%20vanishing%0Aproblem%20and%20model%20the%20long-term%20dependencies.%20Experimental%20results%20on%20four%0Apublic%20real-world%20datasets%20demonstrate%20that%20our%20method%20effectively%20captures%0Aboth%20long-%20and%20short-term%20relations%2C%20achieving%20state-of-the-art%20performance%0Aagainst%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2109.09506v3&entry.124074799=Read"},
{"title": "Streamlining the Image Stitching Pipeline: Integrating Fusion and\n  Rectangling into a Unified Model", "author": "Ziqi Xie", "abstract": "  Learning-based image stitching techniques typically involve three distinct\nstages: registration, fusion, and rectangling. These stages are often performed\nsequentially, each trained independently, leading to potential cascading error\npropagation and complex parameter tuning challenges. In rethinking the\nmathematical modeling of the fusion and rectangling stages, we discovered that\nthese processes can be effectively combined into a single, variety-intensity\ninpainting problem. Therefore, we propose the Simple and Robust Stitcher\n(SRStitcher), an efficient training-free image stitching method that merges the\nfusion and rectangling stages into a unified model. By employing the weighted\nmask and large-scale generative model, SRStitcher can solve the fusion and\nrectangling problems in a single inference, without additional training or\nfine-tuning of other models. Our method not only simplifies the stitching\npipeline but also enhances fault tolerance towards misregistration errors.\nExtensive experiments demonstrate that SRStitcher outperforms state-of-the-art\n(SOTA) methods in both quantitative assessments and qualitative evaluations.\nThe code is released at https://github.com/yayoyo66/SRStitcher\n", "link": "http://arxiv.org/abs/2404.14951v1", "date": "2024-04-23", "relevancy": 2.0553, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5344}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Streamlining%20the%20Image%20Stitching%20Pipeline%3A%20Integrating%20Fusion%20and%0A%20%20Rectangling%20into%20a%20Unified%20Model&body=Title%3A%20Streamlining%20the%20Image%20Stitching%20Pipeline%3A%20Integrating%20Fusion%20and%0A%20%20Rectangling%20into%20a%20Unified%20Model%0AAuthor%3A%20Ziqi%20Xie%0AAbstract%3A%20%20%20Learning-based%20image%20stitching%20techniques%20typically%20involve%20three%20distinct%0Astages%3A%20registration%2C%20fusion%2C%20and%20rectangling.%20These%20stages%20are%20often%20performed%0Asequentially%2C%20each%20trained%20independently%2C%20leading%20to%20potential%20cascading%20error%0Apropagation%20and%20complex%20parameter%20tuning%20challenges.%20In%20rethinking%20the%0Amathematical%20modeling%20of%20the%20fusion%20and%20rectangling%20stages%2C%20we%20discovered%20that%0Athese%20processes%20can%20be%20effectively%20combined%20into%20a%20single%2C%20variety-intensity%0Ainpainting%20problem.%20Therefore%2C%20we%20propose%20the%20Simple%20and%20Robust%20Stitcher%0A%28SRStitcher%29%2C%20an%20efficient%20training-free%20image%20stitching%20method%20that%20merges%20the%0Afusion%20and%20rectangling%20stages%20into%20a%20unified%20model.%20By%20employing%20the%20weighted%0Amask%20and%20large-scale%20generative%20model%2C%20SRStitcher%20can%20solve%20the%20fusion%20and%0Arectangling%20problems%20in%20a%20single%20inference%2C%20without%20additional%20training%20or%0Afine-tuning%20of%20other%20models.%20Our%20method%20not%20only%20simplifies%20the%20stitching%0Apipeline%20but%20also%20enhances%20fault%20tolerance%20towards%20misregistration%20errors.%0AExtensive%20experiments%20demonstrate%20that%20SRStitcher%20outperforms%20state-of-the-art%0A%28SOTA%29%20methods%20in%20both%20quantitative%20assessments%20and%20qualitative%20evaluations.%0AThe%20code%20is%20released%20at%20https%3A//github.com/yayoyo66/SRStitcher%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14951v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streamlining%20the%20Image%20Stitching%20Pipeline%3A%20Integrating%20Fusion%20and%0A%20%20Rectangling%20into%20a%20Unified%20Model&entry.906535625=Ziqi%20Xie&entry.1292438233=%20%20Learning-based%20image%20stitching%20techniques%20typically%20involve%20three%20distinct%0Astages%3A%20registration%2C%20fusion%2C%20and%20rectangling.%20These%20stages%20are%20often%20performed%0Asequentially%2C%20each%20trained%20independently%2C%20leading%20to%20potential%20cascading%20error%0Apropagation%20and%20complex%20parameter%20tuning%20challenges.%20In%20rethinking%20the%0Amathematical%20modeling%20of%20the%20fusion%20and%20rectangling%20stages%2C%20we%20discovered%20that%0Athese%20processes%20can%20be%20effectively%20combined%20into%20a%20single%2C%20variety-intensity%0Ainpainting%20problem.%20Therefore%2C%20we%20propose%20the%20Simple%20and%20Robust%20Stitcher%0A%28SRStitcher%29%2C%20an%20efficient%20training-free%20image%20stitching%20method%20that%20merges%20the%0Afusion%20and%20rectangling%20stages%20into%20a%20unified%20model.%20By%20employing%20the%20weighted%0Amask%20and%20large-scale%20generative%20model%2C%20SRStitcher%20can%20solve%20the%20fusion%20and%0Arectangling%20problems%20in%20a%20single%20inference%2C%20without%20additional%20training%20or%0Afine-tuning%20of%20other%20models.%20Our%20method%20not%20only%20simplifies%20the%20stitching%0Apipeline%20but%20also%20enhances%20fault%20tolerance%20towards%20misregistration%20errors.%0AExtensive%20experiments%20demonstrate%20that%20SRStitcher%20outperforms%20state-of-the-art%0A%28SOTA%29%20methods%20in%20both%20quantitative%20assessments%20and%20qualitative%20evaluations.%0AThe%20code%20is%20released%20at%20https%3A//github.com/yayoyo66/SRStitcher%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14951v1&entry.124074799=Read"},
{"title": "PRISM: A Promptable and Robust Interactive Segmentation Model with\n  Visual Prompts", "author": "Hao Li and Han Liu and Dewei Hu and Jiacheng Wang and Ipek Oguz", "abstract": "  In this paper, we present PRISM, a Promptable and Robust Interactive\nSegmentation Model, aiming for precise segmentation of 3D medical images. PRISM\naccepts various visual inputs, including points, boxes, and scribbles as sparse\nprompts, as well as masks as dense prompts. Specifically, PRISM is designed\nwith four principles to achieve robustness: (1) Iterative learning. The model\nproduces segmentations by using visual prompts from previous iterations to\nachieve progressive improvement. (2) Confidence learning. PRISM employs\nmultiple segmentation heads per input image, each generating a continuous map\nand a confidence score to optimize predictions. (3) Corrective learning.\nFollowing each segmentation iteration, PRISM employs a shallow corrective\nrefinement network to reassign mislabeled voxels. (4) Hybrid design. PRISM\nintegrates hybrid encoders to better capture both the local and global\ninformation. Comprehensive validation of PRISM is conducted using four public\ndatasets for tumor segmentation in the colon, pancreas, liver, and kidney,\nhighlighting challenges caused by anatomical variations and ambiguous\nboundaries in accurate tumor identification. Compared to state-of-the-art\nmethods, both with and without prompt engineering, PRISM significantly improves\nperformance, achieving results that are close to human levels. The code is\npublicly available at https://github.com/MedICL-VU/PRISM.\n", "link": "http://arxiv.org/abs/2404.15028v1", "date": "2024-04-23", "relevancy": 2.0491, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5026}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4978}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PRISM%3A%20A%20Promptable%20and%20Robust%20Interactive%20Segmentation%20Model%20with%0A%20%20Visual%20Prompts&body=Title%3A%20PRISM%3A%20A%20Promptable%20and%20Robust%20Interactive%20Segmentation%20Model%20with%0A%20%20Visual%20Prompts%0AAuthor%3A%20Hao%20Li%20and%20Han%20Liu%20and%20Dewei%20Hu%20and%20Jiacheng%20Wang%20and%20Ipek%20Oguz%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20PRISM%2C%20a%20Promptable%20and%20Robust%20Interactive%0ASegmentation%20Model%2C%20aiming%20for%20precise%20segmentation%20of%203D%20medical%20images.%20PRISM%0Aaccepts%20various%20visual%20inputs%2C%20including%20points%2C%20boxes%2C%20and%20scribbles%20as%20sparse%0Aprompts%2C%20as%20well%20as%20masks%20as%20dense%20prompts.%20Specifically%2C%20PRISM%20is%20designed%0Awith%20four%20principles%20to%20achieve%20robustness%3A%20%281%29%20Iterative%20learning.%20The%20model%0Aproduces%20segmentations%20by%20using%20visual%20prompts%20from%20previous%20iterations%20to%0Aachieve%20progressive%20improvement.%20%282%29%20Confidence%20learning.%20PRISM%20employs%0Amultiple%20segmentation%20heads%20per%20input%20image%2C%20each%20generating%20a%20continuous%20map%0Aand%20a%20confidence%20score%20to%20optimize%20predictions.%20%283%29%20Corrective%20learning.%0AFollowing%20each%20segmentation%20iteration%2C%20PRISM%20employs%20a%20shallow%20corrective%0Arefinement%20network%20to%20reassign%20mislabeled%20voxels.%20%284%29%20Hybrid%20design.%20PRISM%0Aintegrates%20hybrid%20encoders%20to%20better%20capture%20both%20the%20local%20and%20global%0Ainformation.%20Comprehensive%20validation%20of%20PRISM%20is%20conducted%20using%20four%20public%0Adatasets%20for%20tumor%20segmentation%20in%20the%20colon%2C%20pancreas%2C%20liver%2C%20and%20kidney%2C%0Ahighlighting%20challenges%20caused%20by%20anatomical%20variations%20and%20ambiguous%0Aboundaries%20in%20accurate%20tumor%20identification.%20Compared%20to%20state-of-the-art%0Amethods%2C%20both%20with%20and%20without%20prompt%20engineering%2C%20PRISM%20significantly%20improves%0Aperformance%2C%20achieving%20results%20that%20are%20close%20to%20human%20levels.%20The%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/MedICL-VU/PRISM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15028v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISM%3A%20A%20Promptable%20and%20Robust%20Interactive%20Segmentation%20Model%20with%0A%20%20Visual%20Prompts&entry.906535625=Hao%20Li%20and%20Han%20Liu%20and%20Dewei%20Hu%20and%20Jiacheng%20Wang%20and%20Ipek%20Oguz&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20PRISM%2C%20a%20Promptable%20and%20Robust%20Interactive%0ASegmentation%20Model%2C%20aiming%20for%20precise%20segmentation%20of%203D%20medical%20images.%20PRISM%0Aaccepts%20various%20visual%20inputs%2C%20including%20points%2C%20boxes%2C%20and%20scribbles%20as%20sparse%0Aprompts%2C%20as%20well%20as%20masks%20as%20dense%20prompts.%20Specifically%2C%20PRISM%20is%20designed%0Awith%20four%20principles%20to%20achieve%20robustness%3A%20%281%29%20Iterative%20learning.%20The%20model%0Aproduces%20segmentations%20by%20using%20visual%20prompts%20from%20previous%20iterations%20to%0Aachieve%20progressive%20improvement.%20%282%29%20Confidence%20learning.%20PRISM%20employs%0Amultiple%20segmentation%20heads%20per%20input%20image%2C%20each%20generating%20a%20continuous%20map%0Aand%20a%20confidence%20score%20to%20optimize%20predictions.%20%283%29%20Corrective%20learning.%0AFollowing%20each%20segmentation%20iteration%2C%20PRISM%20employs%20a%20shallow%20corrective%0Arefinement%20network%20to%20reassign%20mislabeled%20voxels.%20%284%29%20Hybrid%20design.%20PRISM%0Aintegrates%20hybrid%20encoders%20to%20better%20capture%20both%20the%20local%20and%20global%0Ainformation.%20Comprehensive%20validation%20of%20PRISM%20is%20conducted%20using%20four%20public%0Adatasets%20for%20tumor%20segmentation%20in%20the%20colon%2C%20pancreas%2C%20liver%2C%20and%20kidney%2C%0Ahighlighting%20challenges%20caused%20by%20anatomical%20variations%20and%20ambiguous%0Aboundaries%20in%20accurate%20tumor%20identification.%20Compared%20to%20state-of-the-art%0Amethods%2C%20both%20with%20and%20without%20prompt%20engineering%2C%20PRISM%20significantly%20improves%0Aperformance%2C%20achieving%20results%20that%20are%20close%20to%20human%20levels.%20The%20code%20is%0Apublicly%20available%20at%20https%3A//github.com/MedICL-VU/PRISM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15028v1&entry.124074799=Read"},
{"title": "PSO-Based Optimal Coverage Path Planning for Surface Defect Inspection\n  of 3C Components with a Robotic Line Scanner", "author": "Hongpeng Chen and Shengzeng Huo and Muhammad Muddassir and Hoi-Yin Lee and Anqing Duan and Pai Zheng and Hongsheng Pan and David Navarro-Alarcon", "abstract": "  The automatic inspection of surface defects is an important task for quality\ncontrol in the computers, communications, and consumer electronics (3C)\nindustry. Conventional devices for defect inspection (viz. line-scan sensors)\nhave a limited field of view, thus, a robot-aided defect inspection system\nneeds to scan the object from multiple viewpoints. Optimally selecting the\nrobot's viewpoints and planning a path is regarded as coverage path planning\n(CPP), a problem that enables inspecting the object's complete surface while\nreducing the scanning time and avoiding misdetection of defects. However, the\ndevelopment of CPP strategies for robotic line scanners has not been\nsufficiently studied by researchers. To fill this gap in the literature, in\nthis paper, we present a new approach for robotic line scanners to detect\nsurface defects of 3C free-form objects automatically. Our proposed solution\nconsists of generating a local path by a new hybrid region segmentation method\nand an adaptive planning algorithm to ensure the coverage of the complete\nobject surface. An optimization method for the global path sequence is\ndeveloped to maximize the scanning efficiency. To verify our proposed\nmethodology, we conduct detailed simulation-based and experimental studies on\nvarious free-form workpieces, and compare its performance with a\nstate-of-the-art solution. The reported results demonstrate the feasibility and\neffectiveness of our approach.\n", "link": "http://arxiv.org/abs/2307.04431v2", "date": "2024-04-23", "relevancy": 2.0379, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5186}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5087}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5066}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PSO-Based%20Optimal%20Coverage%20Path%20Planning%20for%20Surface%20Defect%20Inspection%0A%20%20of%203C%20Components%20with%20a%20Robotic%20Line%20Scanner&body=Title%3A%20PSO-Based%20Optimal%20Coverage%20Path%20Planning%20for%20Surface%20Defect%20Inspection%0A%20%20of%203C%20Components%20with%20a%20Robotic%20Line%20Scanner%0AAuthor%3A%20Hongpeng%20Chen%20and%20Shengzeng%20Huo%20and%20Muhammad%20Muddassir%20and%20Hoi-Yin%20Lee%20and%20Anqing%20Duan%20and%20Pai%20Zheng%20and%20Hongsheng%20Pan%20and%20David%20Navarro-Alarcon%0AAbstract%3A%20%20%20The%20automatic%20inspection%20of%20surface%20defects%20is%20an%20important%20task%20for%20quality%0Acontrol%20in%20the%20computers%2C%20communications%2C%20and%20consumer%20electronics%20%283C%29%0Aindustry.%20Conventional%20devices%20for%20defect%20inspection%20%28viz.%20line-scan%20sensors%29%0Ahave%20a%20limited%20field%20of%20view%2C%20thus%2C%20a%20robot-aided%20defect%20inspection%20system%0Aneeds%20to%20scan%20the%20object%20from%20multiple%20viewpoints.%20Optimally%20selecting%20the%0Arobot%27s%20viewpoints%20and%20planning%20a%20path%20is%20regarded%20as%20coverage%20path%20planning%0A%28CPP%29%2C%20a%20problem%20that%20enables%20inspecting%20the%20object%27s%20complete%20surface%20while%0Areducing%20the%20scanning%20time%20and%20avoiding%20misdetection%20of%20defects.%20However%2C%20the%0Adevelopment%20of%20CPP%20strategies%20for%20robotic%20line%20scanners%20has%20not%20been%0Asufficiently%20studied%20by%20researchers.%20To%20fill%20this%20gap%20in%20the%20literature%2C%20in%0Athis%20paper%2C%20we%20present%20a%20new%20approach%20for%20robotic%20line%20scanners%20to%20detect%0Asurface%20defects%20of%203C%20free-form%20objects%20automatically.%20Our%20proposed%20solution%0Aconsists%20of%20generating%20a%20local%20path%20by%20a%20new%20hybrid%20region%20segmentation%20method%0Aand%20an%20adaptive%20planning%20algorithm%20to%20ensure%20the%20coverage%20of%20the%20complete%0Aobject%20surface.%20An%20optimization%20method%20for%20the%20global%20path%20sequence%20is%0Adeveloped%20to%20maximize%20the%20scanning%20efficiency.%20To%20verify%20our%20proposed%0Amethodology%2C%20we%20conduct%20detailed%20simulation-based%20and%20experimental%20studies%20on%0Avarious%20free-form%20workpieces%2C%20and%20compare%20its%20performance%20with%20a%0Astate-of-the-art%20solution.%20The%20reported%20results%20demonstrate%20the%20feasibility%20and%0Aeffectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.04431v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSO-Based%20Optimal%20Coverage%20Path%20Planning%20for%20Surface%20Defect%20Inspection%0A%20%20of%203C%20Components%20with%20a%20Robotic%20Line%20Scanner&entry.906535625=Hongpeng%20Chen%20and%20Shengzeng%20Huo%20and%20Muhammad%20Muddassir%20and%20Hoi-Yin%20Lee%20and%20Anqing%20Duan%20and%20Pai%20Zheng%20and%20Hongsheng%20Pan%20and%20David%20Navarro-Alarcon&entry.1292438233=%20%20The%20automatic%20inspection%20of%20surface%20defects%20is%20an%20important%20task%20for%20quality%0Acontrol%20in%20the%20computers%2C%20communications%2C%20and%20consumer%20electronics%20%283C%29%0Aindustry.%20Conventional%20devices%20for%20defect%20inspection%20%28viz.%20line-scan%20sensors%29%0Ahave%20a%20limited%20field%20of%20view%2C%20thus%2C%20a%20robot-aided%20defect%20inspection%20system%0Aneeds%20to%20scan%20the%20object%20from%20multiple%20viewpoints.%20Optimally%20selecting%20the%0Arobot%27s%20viewpoints%20and%20planning%20a%20path%20is%20regarded%20as%20coverage%20path%20planning%0A%28CPP%29%2C%20a%20problem%20that%20enables%20inspecting%20the%20object%27s%20complete%20surface%20while%0Areducing%20the%20scanning%20time%20and%20avoiding%20misdetection%20of%20defects.%20However%2C%20the%0Adevelopment%20of%20CPP%20strategies%20for%20robotic%20line%20scanners%20has%20not%20been%0Asufficiently%20studied%20by%20researchers.%20To%20fill%20this%20gap%20in%20the%20literature%2C%20in%0Athis%20paper%2C%20we%20present%20a%20new%20approach%20for%20robotic%20line%20scanners%20to%20detect%0Asurface%20defects%20of%203C%20free-form%20objects%20automatically.%20Our%20proposed%20solution%0Aconsists%20of%20generating%20a%20local%20path%20by%20a%20new%20hybrid%20region%20segmentation%20method%0Aand%20an%20adaptive%20planning%20algorithm%20to%20ensure%20the%20coverage%20of%20the%20complete%0Aobject%20surface.%20An%20optimization%20method%20for%20the%20global%20path%20sequence%20is%0Adeveloped%20to%20maximize%20the%20scanning%20efficiency.%20To%20verify%20our%20proposed%0Amethodology%2C%20we%20conduct%20detailed%20simulation-based%20and%20experimental%20studies%20on%0Avarious%20free-form%20workpieces%2C%20and%20compare%20its%20performance%20with%20a%0Astate-of-the-art%20solution.%20The%20reported%20results%20demonstrate%20the%20feasibility%20and%0Aeffectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.04431v2&entry.124074799=Read"},
{"title": "CLIP-QDA: An Explainable Concept Bottleneck Model", "author": "R\u00e9mi Kazmierczak and Elo\u00efse Berthier and Goran Frehse and Gianni Franchi", "abstract": "  In this paper, we introduce an explainable algorithm designed from a\nmulti-modal foundation model, that performs fast and explainable image\nclassification. Drawing inspiration from CLIP-based Concept Bottleneck Models\n(CBMs), our method creates a latent space where each neuron is linked to a\nspecific word. Observing that this latent space can be modeled with simple\ndistributions, we use a Mixture of Gaussians (MoG) formalism to enhance the\ninterpretability of this latent space. Then, we introduce CLIP-QDA, a\nclassifier that only uses statistical values to infer labels from the concepts.\nIn addition, this formalism allows for both local and global explanations.\nThese explanations come from the inner design of our architecture, our work is\npart of a new family of greybox models, combining performances of opaque\nfoundation models and the interpretability of transparent models. Our empirical\nfindings show that in instances where the MoG assumption holds, CLIP-QDA\nachieves similar accuracy with state-of-the-art methods CBMs. Our explanations\ncompete with existing XAI methods while being faster to compute.\n", "link": "http://arxiv.org/abs/2312.00110v2", "date": "2024-04-23", "relevancy": 2.0346, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5186}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5114}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4976}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLIP-QDA%3A%20An%20Explainable%20Concept%20Bottleneck%20Model&body=Title%3A%20CLIP-QDA%3A%20An%20Explainable%20Concept%20Bottleneck%20Model%0AAuthor%3A%20R%C3%A9mi%20Kazmierczak%20and%20Elo%C3%AFse%20Berthier%20and%20Goran%20Frehse%20and%20Gianni%20Franchi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20explainable%20algorithm%20designed%20from%20a%0Amulti-modal%20foundation%20model%2C%20that%20performs%20fast%20and%20explainable%20image%0Aclassification.%20Drawing%20inspiration%20from%20CLIP-based%20Concept%20Bottleneck%20Models%0A%28CBMs%29%2C%20our%20method%20creates%20a%20latent%20space%20where%20each%20neuron%20is%20linked%20to%20a%0Aspecific%20word.%20Observing%20that%20this%20latent%20space%20can%20be%20modeled%20with%20simple%0Adistributions%2C%20we%20use%20a%20Mixture%20of%20Gaussians%20%28MoG%29%20formalism%20to%20enhance%20the%0Ainterpretability%20of%20this%20latent%20space.%20Then%2C%20we%20introduce%20CLIP-QDA%2C%20a%0Aclassifier%20that%20only%20uses%20statistical%20values%20to%20infer%20labels%20from%20the%20concepts.%0AIn%20addition%2C%20this%20formalism%20allows%20for%20both%20local%20and%20global%20explanations.%0AThese%20explanations%20come%20from%20the%20inner%20design%20of%20our%20architecture%2C%20our%20work%20is%0Apart%20of%20a%20new%20family%20of%20greybox%20models%2C%20combining%20performances%20of%20opaque%0Afoundation%20models%20and%20the%20interpretability%20of%20transparent%20models.%20Our%20empirical%0Afindings%20show%20that%20in%20instances%20where%20the%20MoG%20assumption%20holds%2C%20CLIP-QDA%0Aachieves%20similar%20accuracy%20with%20state-of-the-art%20methods%20CBMs.%20Our%20explanations%0Acompete%20with%20existing%20XAI%20methods%20while%20being%20faster%20to%20compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00110v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-QDA%3A%20An%20Explainable%20Concept%20Bottleneck%20Model&entry.906535625=R%C3%A9mi%20Kazmierczak%20and%20Elo%C3%AFse%20Berthier%20and%20Goran%20Frehse%20and%20Gianni%20Franchi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20explainable%20algorithm%20designed%20from%20a%0Amulti-modal%20foundation%20model%2C%20that%20performs%20fast%20and%20explainable%20image%0Aclassification.%20Drawing%20inspiration%20from%20CLIP-based%20Concept%20Bottleneck%20Models%0A%28CBMs%29%2C%20our%20method%20creates%20a%20latent%20space%20where%20each%20neuron%20is%20linked%20to%20a%0Aspecific%20word.%20Observing%20that%20this%20latent%20space%20can%20be%20modeled%20with%20simple%0Adistributions%2C%20we%20use%20a%20Mixture%20of%20Gaussians%20%28MoG%29%20formalism%20to%20enhance%20the%0Ainterpretability%20of%20this%20latent%20space.%20Then%2C%20we%20introduce%20CLIP-QDA%2C%20a%0Aclassifier%20that%20only%20uses%20statistical%20values%20to%20infer%20labels%20from%20the%20concepts.%0AIn%20addition%2C%20this%20formalism%20allows%20for%20both%20local%20and%20global%20explanations.%0AThese%20explanations%20come%20from%20the%20inner%20design%20of%20our%20architecture%2C%20our%20work%20is%0Apart%20of%20a%20new%20family%20of%20greybox%20models%2C%20combining%20performances%20of%20opaque%0Afoundation%20models%20and%20the%20interpretability%20of%20transparent%20models.%20Our%20empirical%0Afindings%20show%20that%20in%20instances%20where%20the%20MoG%20assumption%20holds%2C%20CLIP-QDA%0Aachieves%20similar%20accuracy%20with%20state-of-the-art%20methods%20CBMs.%20Our%20explanations%0Acompete%20with%20existing%20XAI%20methods%20while%20being%20faster%20to%20compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00110v2&entry.124074799=Read"},
{"title": "Differentially-Private Data Synthetisation for Efficient\n  Re-Identification Risk Control", "author": "T\u00e2nia Carvalho and Nuno Moniz and Lu\u00eds Antunes and Nitesh Chawla", "abstract": "  Protecting user data privacy can be achieved via many methods, from\nstatistical transformations to generative models. However, all of them have\ncritical drawbacks. For example, creating a transformed data set using\ntraditional techniques is highly time-consuming. Also, recent deep\nlearning-based solutions require significant computational resources in\naddition to long training phases, and differentially private-based solutions\nmay undermine data utility. In this paper, we propose $\\epsilon$-PrivateSMOTE,\na technique designed for safeguarding against re-identification and linkage\nattacks, particularly addressing cases with a high \\sloppy re-identification\nrisk. Our proposal combines synthetic data generation via noise-induced\ninterpolation with differential privacy principles to obfuscate high-risk\ncases. We demonstrate how $\\epsilon$-PrivateSMOTE is capable of achieving\ncompetitive results in privacy risk and better predictive performance when\ncompared to multiple traditional and state-of-the-art privacy-preservation\nmethods, including generative adversarial networks, variational autoencoders,\nand differential privacy baselines. We also show how our method improves time\nrequirements by at least a factor of 9 and is a resource-efficient solution\nthat ensures high performance without specialised hardware.\n", "link": "http://arxiv.org/abs/2212.00484v3", "date": "2024-04-23", "relevancy": 2.0161, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5412}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4768}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Differentially-Private%20Data%20Synthetisation%20for%20Efficient%0A%20%20Re-Identification%20Risk%20Control&body=Title%3A%20Differentially-Private%20Data%20Synthetisation%20for%20Efficient%0A%20%20Re-Identification%20Risk%20Control%0AAuthor%3A%20T%C3%A2nia%20Carvalho%20and%20Nuno%20Moniz%20and%20Lu%C3%ADs%20Antunes%20and%20Nitesh%20Chawla%0AAbstract%3A%20%20%20Protecting%20user%20data%20privacy%20can%20be%20achieved%20via%20many%20methods%2C%20from%0Astatistical%20transformations%20to%20generative%20models.%20However%2C%20all%20of%20them%20have%0Acritical%20drawbacks.%20For%20example%2C%20creating%20a%20transformed%20data%20set%20using%0Atraditional%20techniques%20is%20highly%20time-consuming.%20Also%2C%20recent%20deep%0Alearning-based%20solutions%20require%20significant%20computational%20resources%20in%0Aaddition%20to%20long%20training%20phases%2C%20and%20differentially%20private-based%20solutions%0Amay%20undermine%20data%20utility.%20In%20this%20paper%2C%20we%20propose%20%24%5Cepsilon%24-PrivateSMOTE%2C%0Aa%20technique%20designed%20for%20safeguarding%20against%20re-identification%20and%20linkage%0Aattacks%2C%20particularly%20addressing%20cases%20with%20a%20high%20%5Csloppy%20re-identification%0Arisk.%20Our%20proposal%20combines%20synthetic%20data%20generation%20via%20noise-induced%0Ainterpolation%20with%20differential%20privacy%20principles%20to%20obfuscate%20high-risk%0Acases.%20We%20demonstrate%20how%20%24%5Cepsilon%24-PrivateSMOTE%20is%20capable%20of%20achieving%0Acompetitive%20results%20in%20privacy%20risk%20and%20better%20predictive%20performance%20when%0Acompared%20to%20multiple%20traditional%20and%20state-of-the-art%20privacy-preservation%0Amethods%2C%20including%20generative%20adversarial%20networks%2C%20variational%20autoencoders%2C%0Aand%20differential%20privacy%20baselines.%20We%20also%20show%20how%20our%20method%20improves%20time%0Arequirements%20by%20at%20least%20a%20factor%20of%209%20and%20is%20a%20resource-efficient%20solution%0Athat%20ensures%20high%20performance%20without%20specialised%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.00484v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially-Private%20Data%20Synthetisation%20for%20Efficient%0A%20%20Re-Identification%20Risk%20Control&entry.906535625=T%C3%A2nia%20Carvalho%20and%20Nuno%20Moniz%20and%20Lu%C3%ADs%20Antunes%20and%20Nitesh%20Chawla&entry.1292438233=%20%20Protecting%20user%20data%20privacy%20can%20be%20achieved%20via%20many%20methods%2C%20from%0Astatistical%20transformations%20to%20generative%20models.%20However%2C%20all%20of%20them%20have%0Acritical%20drawbacks.%20For%20example%2C%20creating%20a%20transformed%20data%20set%20using%0Atraditional%20techniques%20is%20highly%20time-consuming.%20Also%2C%20recent%20deep%0Alearning-based%20solutions%20require%20significant%20computational%20resources%20in%0Aaddition%20to%20long%20training%20phases%2C%20and%20differentially%20private-based%20solutions%0Amay%20undermine%20data%20utility.%20In%20this%20paper%2C%20we%20propose%20%24%5Cepsilon%24-PrivateSMOTE%2C%0Aa%20technique%20designed%20for%20safeguarding%20against%20re-identification%20and%20linkage%0Aattacks%2C%20particularly%20addressing%20cases%20with%20a%20high%20%5Csloppy%20re-identification%0Arisk.%20Our%20proposal%20combines%20synthetic%20data%20generation%20via%20noise-induced%0Ainterpolation%20with%20differential%20privacy%20principles%20to%20obfuscate%20high-risk%0Acases.%20We%20demonstrate%20how%20%24%5Cepsilon%24-PrivateSMOTE%20is%20capable%20of%20achieving%0Acompetitive%20results%20in%20privacy%20risk%20and%20better%20predictive%20performance%20when%0Acompared%20to%20multiple%20traditional%20and%20state-of-the-art%20privacy-preservation%0Amethods%2C%20including%20generative%20adversarial%20networks%2C%20variational%20autoencoders%2C%0Aand%20differential%20privacy%20baselines.%20We%20also%20show%20how%20our%20method%20improves%20time%0Arequirements%20by%20at%20least%20a%20factor%20of%209%20and%20is%20a%20resource-efficient%20solution%0Athat%20ensures%20high%20performance%20without%20specialised%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.00484v3&entry.124074799=Read"},
{"title": "CA-Stream: Attention-based pooling for interpretable image recognition", "author": "Felipe Torres and Hanwei Zhang and Ronan Sicre and St\u00e9phane Ayache and Yannis Avrithis", "abstract": "  Explanations obtained from transformer-based architectures in the form of raw\nattention, can be seen as a class-agnostic saliency map. Additionally,\nattention-based pooling serves as a form of masking the in feature space.\nMotivated by this observation, we design an attention-based pooling mechanism\nintended to replace Global Average Pooling (GAP) at inference. This mechanism,\ncalled Cross-Attention Stream (CA-Stream), comprises a stream of cross\nattention blocks interacting with features at different network depths.\nCA-Stream enhances interpretability in models, while preserving recognition\nperformance.\n", "link": "http://arxiv.org/abs/2404.14996v1", "date": "2024-04-23", "relevancy": 2.016, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.516}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5138}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CA-Stream%3A%20Attention-based%20pooling%20for%20interpretable%20image%20recognition&body=Title%3A%20CA-Stream%3A%20Attention-based%20pooling%20for%20interpretable%20image%20recognition%0AAuthor%3A%20Felipe%20Torres%20and%20Hanwei%20Zhang%20and%20Ronan%20Sicre%20and%20St%C3%A9phane%20Ayache%20and%20Yannis%20Avrithis%0AAbstract%3A%20%20%20Explanations%20obtained%20from%20transformer-based%20architectures%20in%20the%20form%20of%20raw%0Aattention%2C%20can%20be%20seen%20as%20a%20class-agnostic%20saliency%20map.%20Additionally%2C%0Aattention-based%20pooling%20serves%20as%20a%20form%20of%20masking%20the%20in%20feature%20space.%0AMotivated%20by%20this%20observation%2C%20we%20design%20an%20attention-based%20pooling%20mechanism%0Aintended%20to%20replace%20Global%20Average%20Pooling%20%28GAP%29%20at%20inference.%20This%20mechanism%2C%0Acalled%20Cross-Attention%20Stream%20%28CA-Stream%29%2C%20comprises%20a%20stream%20of%20cross%0Aattention%20blocks%20interacting%20with%20features%20at%20different%20network%20depths.%0ACA-Stream%20enhances%20interpretability%20in%20models%2C%20while%20preserving%20recognition%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14996v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CA-Stream%3A%20Attention-based%20pooling%20for%20interpretable%20image%20recognition&entry.906535625=Felipe%20Torres%20and%20Hanwei%20Zhang%20and%20Ronan%20Sicre%20and%20St%C3%A9phane%20Ayache%20and%20Yannis%20Avrithis&entry.1292438233=%20%20Explanations%20obtained%20from%20transformer-based%20architectures%20in%20the%20form%20of%20raw%0Aattention%2C%20can%20be%20seen%20as%20a%20class-agnostic%20saliency%20map.%20Additionally%2C%0Aattention-based%20pooling%20serves%20as%20a%20form%20of%20masking%20the%20in%20feature%20space.%0AMotivated%20by%20this%20observation%2C%20we%20design%20an%20attention-based%20pooling%20mechanism%0Aintended%20to%20replace%20Global%20Average%20Pooling%20%28GAP%29%20at%20inference.%20This%20mechanism%2C%0Acalled%20Cross-Attention%20Stream%20%28CA-Stream%29%2C%20comprises%20a%20stream%20of%20cross%0Aattention%20blocks%20interacting%20with%20features%20at%20different%20network%20depths.%0ACA-Stream%20enhances%20interpretability%20in%20models%2C%20while%20preserving%20recognition%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14996v1&entry.124074799=Read"},
{"title": "An Economic Solution to Copyright Challenges of Generative AI", "author": "Jiachen T. Wang and Zhun Deng and Hiroaki Chiba-Okabe and Boaz Barak and Weijie J. Su", "abstract": "  Generative artificial intelligence (AI) systems are trained on large data\ncorpora to generate new pieces of text, images, videos, and other media. There\nis growing concern that such systems may infringe on the copyright interests of\ntraining data contributors. To address the copyright challenges of generative\nAI, we propose a framework that compensates copyright owners proportionally to\ntheir contributions to the creation of AI-generated content. The metric for\ncontributions is quantitatively determined by leveraging the probabilistic\nnature of modern generative AI models and using techniques from cooperative\ngame theory in economics. This framework enables a platform where AI developers\nbenefit from access to high-quality training data, thus improving model\nperformance. Meanwhile, copyright owners receive fair compensation, driving the\ncontinued provision of relevant data for generative model training. Experiments\ndemonstrate that our framework successfully identifies the most relevant data\nsources used in artwork generation, ensuring a fair and interpretable\ndistribution of revenues among copyright owners.\n", "link": "http://arxiv.org/abs/2404.13964v2", "date": "2024-04-23", "relevancy": 2.0054, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5114}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4943}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4936}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Economic%20Solution%20to%20Copyright%20Challenges%20of%20Generative%20AI&body=Title%3A%20An%20Economic%20Solution%20to%20Copyright%20Challenges%20of%20Generative%20AI%0AAuthor%3A%20Jiachen%20T.%20Wang%20and%20Zhun%20Deng%20and%20Hiroaki%20Chiba-Okabe%20and%20Boaz%20Barak%20and%20Weijie%20J.%20Su%0AAbstract%3A%20%20%20Generative%20artificial%20intelligence%20%28AI%29%20systems%20are%20trained%20on%20large%20data%0Acorpora%20to%20generate%20new%20pieces%20of%20text%2C%20images%2C%20videos%2C%20and%20other%20media.%20There%0Ais%20growing%20concern%20that%20such%20systems%20may%20infringe%20on%20the%20copyright%20interests%20of%0Atraining%20data%20contributors.%20To%20address%20the%20copyright%20challenges%20of%20generative%0AAI%2C%20we%20propose%20a%20framework%20that%20compensates%20copyright%20owners%20proportionally%20to%0Atheir%20contributions%20to%20the%20creation%20of%20AI-generated%20content.%20The%20metric%20for%0Acontributions%20is%20quantitatively%20determined%20by%20leveraging%20the%20probabilistic%0Anature%20of%20modern%20generative%20AI%20models%20and%20using%20techniques%20from%20cooperative%0Agame%20theory%20in%20economics.%20This%20framework%20enables%20a%20platform%20where%20AI%20developers%0Abenefit%20from%20access%20to%20high-quality%20training%20data%2C%20thus%20improving%20model%0Aperformance.%20Meanwhile%2C%20copyright%20owners%20receive%20fair%20compensation%2C%20driving%20the%0Acontinued%20provision%20of%20relevant%20data%20for%20generative%20model%20training.%20Experiments%0Ademonstrate%20that%20our%20framework%20successfully%20identifies%20the%20most%20relevant%20data%0Asources%20used%20in%20artwork%20generation%2C%20ensuring%20a%20fair%20and%20interpretable%0Adistribution%20of%20revenues%20among%20copyright%20owners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13964v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Economic%20Solution%20to%20Copyright%20Challenges%20of%20Generative%20AI&entry.906535625=Jiachen%20T.%20Wang%20and%20Zhun%20Deng%20and%20Hiroaki%20Chiba-Okabe%20and%20Boaz%20Barak%20and%20Weijie%20J.%20Su&entry.1292438233=%20%20Generative%20artificial%20intelligence%20%28AI%29%20systems%20are%20trained%20on%20large%20data%0Acorpora%20to%20generate%20new%20pieces%20of%20text%2C%20images%2C%20videos%2C%20and%20other%20media.%20There%0Ais%20growing%20concern%20that%20such%20systems%20may%20infringe%20on%20the%20copyright%20interests%20of%0Atraining%20data%20contributors.%20To%20address%20the%20copyright%20challenges%20of%20generative%0AAI%2C%20we%20propose%20a%20framework%20that%20compensates%20copyright%20owners%20proportionally%20to%0Atheir%20contributions%20to%20the%20creation%20of%20AI-generated%20content.%20The%20metric%20for%0Acontributions%20is%20quantitatively%20determined%20by%20leveraging%20the%20probabilistic%0Anature%20of%20modern%20generative%20AI%20models%20and%20using%20techniques%20from%20cooperative%0Agame%20theory%20in%20economics.%20This%20framework%20enables%20a%20platform%20where%20AI%20developers%0Abenefit%20from%20access%20to%20high-quality%20training%20data%2C%20thus%20improving%20model%0Aperformance.%20Meanwhile%2C%20copyright%20owners%20receive%20fair%20compensation%2C%20driving%20the%0Acontinued%20provision%20of%20relevant%20data%20for%20generative%20model%20training.%20Experiments%0Ademonstrate%20that%20our%20framework%20successfully%20identifies%20the%20most%20relevant%20data%0Asources%20used%20in%20artwork%20generation%2C%20ensuring%20a%20fair%20and%20interpretable%0Adistribution%20of%20revenues%20among%20copyright%20owners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13964v2&entry.124074799=Read"},
{"title": "Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging\n  Perturbations That Efficiently Fool Customized Diffusion Models", "author": "Jingyao Xu and Yuetong Lu and Yandong Li and Siyang Lu and Dongdong Wang and Xiang Wei", "abstract": "  Diffusion models (DMs) embark a new era of generative modeling and offer more\nopportunities for efficient generating high-quality and realistic data samples.\nHowever, their widespread use has also brought forth new challenges in model\nsecurity, which motivates the creation of more effective adversarial attackers\non DMs to understand its vulnerability. We propose CAAT, a simple but generic\nand efficient approach that does not require costly training to effectively\nfool latent diffusion models (LDMs). The approach is based on the observation\nthat cross-attention layers exhibits higher sensitivity to gradient change,\nallowing for leveraging subtle perturbations on published images to\nsignificantly corrupt the generated images. We show that a subtle perturbation\non an image can significantly impact the cross-attention layers, thus changing\nthe mapping between text and image during the fine-tuning of customized\ndiffusion models. Extensive experiments demonstrate that CAAT is compatible\nwith diverse diffusion models and outperforms baseline attack methods in a more\neffective (more noise) and efficient (twice as fast as Anti-DreamBooth and\nMist) manner.\n", "link": "http://arxiv.org/abs/2404.15081v1", "date": "2024-04-23", "relevancy": 1.98, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6716}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6678}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6231}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Perturbing%20Attention%20Gives%20You%20More%20Bang%20for%20the%20Buck%3A%20Subtle%20Imaging%0A%20%20Perturbations%20That%20Efficiently%20Fool%20Customized%20Diffusion%20Models&body=Title%3A%20Perturbing%20Attention%20Gives%20You%20More%20Bang%20for%20the%20Buck%3A%20Subtle%20Imaging%0A%20%20Perturbations%20That%20Efficiently%20Fool%20Customized%20Diffusion%20Models%0AAuthor%3A%20Jingyao%20Xu%20and%20Yuetong%20Lu%20and%20Yandong%20Li%20and%20Siyang%20Lu%20and%20Dongdong%20Wang%20and%20Xiang%20Wei%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20embark%20a%20new%20era%20of%20generative%20modeling%20and%20offer%20more%0Aopportunities%20for%20efficient%20generating%20high-quality%20and%20realistic%20data%20samples.%0AHowever%2C%20their%20widespread%20use%20has%20also%20brought%20forth%20new%20challenges%20in%20model%0Asecurity%2C%20which%20motivates%20the%20creation%20of%20more%20effective%20adversarial%20attackers%0Aon%20DMs%20to%20understand%20its%20vulnerability.%20We%20propose%20CAAT%2C%20a%20simple%20but%20generic%0Aand%20efficient%20approach%20that%20does%20not%20require%20costly%20training%20to%20effectively%0Afool%20latent%20diffusion%20models%20%28LDMs%29.%20The%20approach%20is%20based%20on%20the%20observation%0Athat%20cross-attention%20layers%20exhibits%20higher%20sensitivity%20to%20gradient%20change%2C%0Aallowing%20for%20leveraging%20subtle%20perturbations%20on%20published%20images%20to%0Asignificantly%20corrupt%20the%20generated%20images.%20We%20show%20that%20a%20subtle%20perturbation%0Aon%20an%20image%20can%20significantly%20impact%20the%20cross-attention%20layers%2C%20thus%20changing%0Athe%20mapping%20between%20text%20and%20image%20during%20the%20fine-tuning%20of%20customized%0Adiffusion%20models.%20Extensive%20experiments%20demonstrate%20that%20CAAT%20is%20compatible%0Awith%20diverse%20diffusion%20models%20and%20outperforms%20baseline%20attack%20methods%20in%20a%20more%0Aeffective%20%28more%20noise%29%20and%20efficient%20%28twice%20as%20fast%20as%20Anti-DreamBooth%20and%0AMist%29%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15081v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perturbing%20Attention%20Gives%20You%20More%20Bang%20for%20the%20Buck%3A%20Subtle%20Imaging%0A%20%20Perturbations%20That%20Efficiently%20Fool%20Customized%20Diffusion%20Models&entry.906535625=Jingyao%20Xu%20and%20Yuetong%20Lu%20and%20Yandong%20Li%20and%20Siyang%20Lu%20and%20Dongdong%20Wang%20and%20Xiang%20Wei&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20embark%20a%20new%20era%20of%20generative%20modeling%20and%20offer%20more%0Aopportunities%20for%20efficient%20generating%20high-quality%20and%20realistic%20data%20samples.%0AHowever%2C%20their%20widespread%20use%20has%20also%20brought%20forth%20new%20challenges%20in%20model%0Asecurity%2C%20which%20motivates%20the%20creation%20of%20more%20effective%20adversarial%20attackers%0Aon%20DMs%20to%20understand%20its%20vulnerability.%20We%20propose%20CAAT%2C%20a%20simple%20but%20generic%0Aand%20efficient%20approach%20that%20does%20not%20require%20costly%20training%20to%20effectively%0Afool%20latent%20diffusion%20models%20%28LDMs%29.%20The%20approach%20is%20based%20on%20the%20observation%0Athat%20cross-attention%20layers%20exhibits%20higher%20sensitivity%20to%20gradient%20change%2C%0Aallowing%20for%20leveraging%20subtle%20perturbations%20on%20published%20images%20to%0Asignificantly%20corrupt%20the%20generated%20images.%20We%20show%20that%20a%20subtle%20perturbation%0Aon%20an%20image%20can%20significantly%20impact%20the%20cross-attention%20layers%2C%20thus%20changing%0Athe%20mapping%20between%20text%20and%20image%20during%20the%20fine-tuning%20of%20customized%0Adiffusion%20models.%20Extensive%20experiments%20demonstrate%20that%20CAAT%20is%20compatible%0Awith%20diverse%20diffusion%20models%20and%20outperforms%20baseline%20attack%20methods%20in%20a%20more%0Aeffective%20%28more%20noise%29%20and%20efficient%20%28twice%20as%20fast%20as%20Anti-DreamBooth%20and%0AMist%29%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15081v1&entry.124074799=Read"},
{"title": "Traditional to Transformers: A Survey on Current Trends and Future\n  Prospects for Hyperspectral Image Classification", "author": "Muhammad Ahmad and Salvatore Distifano and Manuel Mazzara and Adil Mehmood Khan", "abstract": "  Hyperspectral image classification is a challenging task due to the high\ndimensionality and complex nature of hyperspectral data. In recent years, deep\nlearning techniques have emerged as powerful tools for addressing these\nchallenges. This survey provides a comprehensive overview of the current trends\nand future prospects in hyperspectral image classification, focusing on the\nadvancements from deep learning models to the emerging use of transformers. We\nreview the key concepts, methodologies, and state-of-the-art approaches in deep\nlearning for hyperspectral image classification. Additionally, we discuss the\npotential of transformer-based models in this field and highlight the\nadvantages and challenges associated with these approaches. Comprehensive\nexperimental results have been undertaken using three Hyperspectral datasets to\nverify the efficacy of various conventional deep-learning models and\nTransformers. Finally, we outline future research directions and potential\napplications that can further enhance the accuracy and efficiency of\nhyperspectral image classification.\n  The Source code is available at\nhttps://github.com/mahmad00/Conventional-to-Transformer-for-Hyperspectral-Image-Classification-Survey-2024.\n", "link": "http://arxiv.org/abs/2404.14955v1", "date": "2024-04-23", "relevancy": 1.9754, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5327}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4823}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Traditional%20to%20Transformers%3A%20A%20Survey%20on%20Current%20Trends%20and%20Future%0A%20%20Prospects%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20Traditional%20to%20Transformers%3A%20A%20Survey%20on%20Current%20Trends%20and%20Future%0A%20%20Prospects%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Salvatore%20Distifano%20and%20Manuel%20Mazzara%20and%20Adil%20Mehmood%20Khan%0AAbstract%3A%20%20%20Hyperspectral%20image%20classification%20is%20a%20challenging%20task%20due%20to%20the%20high%0Adimensionality%20and%20complex%20nature%20of%20hyperspectral%20data.%20In%20recent%20years%2C%20deep%0Alearning%20techniques%20have%20emerged%20as%20powerful%20tools%20for%20addressing%20these%0Achallenges.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20the%20current%20trends%0Aand%20future%20prospects%20in%20hyperspectral%20image%20classification%2C%20focusing%20on%20the%0Aadvancements%20from%20deep%20learning%20models%20to%20the%20emerging%20use%20of%20transformers.%20We%0Areview%20the%20key%20concepts%2C%20methodologies%2C%20and%20state-of-the-art%20approaches%20in%20deep%0Alearning%20for%20hyperspectral%20image%20classification.%20Additionally%2C%20we%20discuss%20the%0Apotential%20of%20transformer-based%20models%20in%20this%20field%20and%20highlight%20the%0Aadvantages%20and%20challenges%20associated%20with%20these%20approaches.%20Comprehensive%0Aexperimental%20results%20have%20been%20undertaken%20using%20three%20Hyperspectral%20datasets%20to%0Averify%20the%20efficacy%20of%20various%20conventional%20deep-learning%20models%20and%0ATransformers.%20Finally%2C%20we%20outline%20future%20research%20directions%20and%20potential%0Aapplications%20that%20can%20further%20enhance%20the%20accuracy%20and%20efficiency%20of%0Ahyperspectral%20image%20classification.%0A%20%20The%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/mahmad00/Conventional-to-Transformer-for-Hyperspectral-Image-Classification-Survey-2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14955v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traditional%20to%20Transformers%3A%20A%20Survey%20on%20Current%20Trends%20and%20Future%0A%20%20Prospects%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Muhammad%20Ahmad%20and%20Salvatore%20Distifano%20and%20Manuel%20Mazzara%20and%20Adil%20Mehmood%20Khan&entry.1292438233=%20%20Hyperspectral%20image%20classification%20is%20a%20challenging%20task%20due%20to%20the%20high%0Adimensionality%20and%20complex%20nature%20of%20hyperspectral%20data.%20In%20recent%20years%2C%20deep%0Alearning%20techniques%20have%20emerged%20as%20powerful%20tools%20for%20addressing%20these%0Achallenges.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20the%20current%20trends%0Aand%20future%20prospects%20in%20hyperspectral%20image%20classification%2C%20focusing%20on%20the%0Aadvancements%20from%20deep%20learning%20models%20to%20the%20emerging%20use%20of%20transformers.%20We%0Areview%20the%20key%20concepts%2C%20methodologies%2C%20and%20state-of-the-art%20approaches%20in%20deep%0Alearning%20for%20hyperspectral%20image%20classification.%20Additionally%2C%20we%20discuss%20the%0Apotential%20of%20transformer-based%20models%20in%20this%20field%20and%20highlight%20the%0Aadvantages%20and%20challenges%20associated%20with%20these%20approaches.%20Comprehensive%0Aexperimental%20results%20have%20been%20undertaken%20using%20three%20Hyperspectral%20datasets%20to%0Averify%20the%20efficacy%20of%20various%20conventional%20deep-learning%20models%20and%0ATransformers.%20Finally%2C%20we%20outline%20future%20research%20directions%20and%20potential%0Aapplications%20that%20can%20further%20enhance%20the%20accuracy%20and%20efficiency%20of%0Ahyperspectral%20image%20classification.%0A%20%20The%20Source%20code%20is%20available%20at%0Ahttps%3A//github.com/mahmad00/Conventional-to-Transformer-for-Hyperspectral-Image-Classification-Survey-2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14955v1&entry.124074799=Read"},
{"title": "CLEANing Cygnus A deep and fast with R2D2", "author": "Arwa Dabbech and Amir Aghabiglou and Chung San Chu and Yves Wiaux", "abstract": "  A novel deep learning paradigm for synthesis imaging by radio interferometry\nin astronomy was recently proposed, dubbed \"Residual-to-Residual DNN series for\nhigh-Dynamic range imaging\" (R2D2). In this work, we start by shedding light on\nR2D2's algorithmic structure, interpreting it as a learned version of CLEAN\nwith minor cycles substituted with a deep neural network (DNN) whose training\nis iteration-specific. We then proceed with R2D2's first demonstration on real\ndata, for monochromatic intensity imaging of the radio galaxy Cygnus A from S\nband observations with the Very Large Array (VLA). We show that the modeling\npower of R2D2's learning approach enables delivering high-precision imaging,\nsuperseding the resolution of CLEAN, and matching the precision of modern\noptimization and plug-and-play algorithms, respectively uSARA and AIRI.\nRequiring few major-cycle iterations only, R2D2 provides a much faster\nreconstruction than uSARA and AIRI, known to be highly iterative, and is at\nleast as fast as CLEAN.\n", "link": "http://arxiv.org/abs/2309.03291v3", "date": "2024-04-23", "relevancy": 1.9685, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5015}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4901}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4835}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CLEANing%20Cygnus%20A%20deep%20and%20fast%20with%20R2D2&body=Title%3A%20CLEANing%20Cygnus%20A%20deep%20and%20fast%20with%20R2D2%0AAuthor%3A%20Arwa%20Dabbech%20and%20Amir%20Aghabiglou%20and%20Chung%20San%20Chu%20and%20Yves%20Wiaux%0AAbstract%3A%20%20%20A%20novel%20deep%20learning%20paradigm%20for%20synthesis%20imaging%20by%20radio%20interferometry%0Ain%20astronomy%20was%20recently%20proposed%2C%20dubbed%20%22Residual-to-Residual%20DNN%20series%20for%0Ahigh-Dynamic%20range%20imaging%22%20%28R2D2%29.%20In%20this%20work%2C%20we%20start%20by%20shedding%20light%20on%0AR2D2%27s%20algorithmic%20structure%2C%20interpreting%20it%20as%20a%20learned%20version%20of%20CLEAN%0Awith%20minor%20cycles%20substituted%20with%20a%20deep%20neural%20network%20%28DNN%29%20whose%20training%0Ais%20iteration-specific.%20We%20then%20proceed%20with%20R2D2%27s%20first%20demonstration%20on%20real%0Adata%2C%20for%20monochromatic%20intensity%20imaging%20of%20the%20radio%20galaxy%20Cygnus%20A%20from%20S%0Aband%20observations%20with%20the%20Very%20Large%20Array%20%28VLA%29.%20We%20show%20that%20the%20modeling%0Apower%20of%20R2D2%27s%20learning%20approach%20enables%20delivering%20high-precision%20imaging%2C%0Asuperseding%20the%20resolution%20of%20CLEAN%2C%20and%20matching%20the%20precision%20of%20modern%0Aoptimization%20and%20plug-and-play%20algorithms%2C%20respectively%20uSARA%20and%20AIRI.%0ARequiring%20few%20major-cycle%20iterations%20only%2C%20R2D2%20provides%20a%20much%20faster%0Areconstruction%20than%20uSARA%20and%20AIRI%2C%20known%20to%20be%20highly%20iterative%2C%20and%20is%20at%0Aleast%20as%20fast%20as%20CLEAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03291v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLEANing%20Cygnus%20A%20deep%20and%20fast%20with%20R2D2&entry.906535625=Arwa%20Dabbech%20and%20Amir%20Aghabiglou%20and%20Chung%20San%20Chu%20and%20Yves%20Wiaux&entry.1292438233=%20%20A%20novel%20deep%20learning%20paradigm%20for%20synthesis%20imaging%20by%20radio%20interferometry%0Ain%20astronomy%20was%20recently%20proposed%2C%20dubbed%20%22Residual-to-Residual%20DNN%20series%20for%0Ahigh-Dynamic%20range%20imaging%22%20%28R2D2%29.%20In%20this%20work%2C%20we%20start%20by%20shedding%20light%20on%0AR2D2%27s%20algorithmic%20structure%2C%20interpreting%20it%20as%20a%20learned%20version%20of%20CLEAN%0Awith%20minor%20cycles%20substituted%20with%20a%20deep%20neural%20network%20%28DNN%29%20whose%20training%0Ais%20iteration-specific.%20We%20then%20proceed%20with%20R2D2%27s%20first%20demonstration%20on%20real%0Adata%2C%20for%20monochromatic%20intensity%20imaging%20of%20the%20radio%20galaxy%20Cygnus%20A%20from%20S%0Aband%20observations%20with%20the%20Very%20Large%20Array%20%28VLA%29.%20We%20show%20that%20the%20modeling%0Apower%20of%20R2D2%27s%20learning%20approach%20enables%20delivering%20high-precision%20imaging%2C%0Asuperseding%20the%20resolution%20of%20CLEAN%2C%20and%20matching%20the%20precision%20of%20modern%0Aoptimization%20and%20plug-and-play%20algorithms%2C%20respectively%20uSARA%20and%20AIRI.%0ARequiring%20few%20major-cycle%20iterations%20only%2C%20R2D2%20provides%20a%20much%20faster%0Areconstruction%20than%20uSARA%20and%20AIRI%2C%20known%20to%20be%20highly%20iterative%2C%20and%20is%20at%0Aleast%20as%20fast%20as%20CLEAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03291v3&entry.124074799=Read"},
{"title": "UniMERNet: A Universal Network for Real-World Mathematical Expression\n  Recognition", "author": "Bin Wang and Zhuangcheng Gu and Chao Xu and Bo Zhang and Botian Shi and Conghui He", "abstract": "  This paper presents the UniMER dataset to provide the first study on\nMathematical Expression Recognition (MER) towards complex real-world scenarios.\nThe UniMER dataset consists of a large-scale training set UniMER-1M offering an\nunprecedented scale and diversity with one million training instances and a\nmeticulously designed test set UniMER-Test that reflects a diverse range of\nformula distributions prevalent in real-world scenarios. Therefore, the UniMER\ndataset enables the training of a robust and high-accuracy MER model and\ncomprehensive evaluation of model performance. Moreover, we introduce the\nUniversal Mathematical Expression Recognition Network (UniMERNet), an\ninnovative framework designed to enhance MER in practical scenarios. UniMERNet\nincorporates a Length-Aware Module to process formulas of varied lengths\nefficiently, thereby enabling the model to handle complex mathematical\nexpressions with greater accuracy. In addition, UniMERNet employs our UniMER-1M\ndata and image augmentation techniques to improve the model's robustness under\ndifferent noise conditions. Our extensive experiments demonstrate that\nUniMERNet outperforms existing MER models, setting a new benchmark in various\nscenarios and ensuring superior recognition quality in real-world applications.\nThe dataset and model are available at\nhttps://github.com/opendatalab/UniMERNet.\n", "link": "http://arxiv.org/abs/2404.15254v1", "date": "2024-04-23", "relevancy": 1.9642, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5439}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.467}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20UniMERNet%3A%20A%20Universal%20Network%20for%20Real-World%20Mathematical%20Expression%0A%20%20Recognition&body=Title%3A%20UniMERNet%3A%20A%20Universal%20Network%20for%20Real-World%20Mathematical%20Expression%0A%20%20Recognition%0AAuthor%3A%20Bin%20Wang%20and%20Zhuangcheng%20Gu%20and%20Chao%20Xu%20and%20Bo%20Zhang%20and%20Botian%20Shi%20and%20Conghui%20He%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20UniMER%20dataset%20to%20provide%20the%20first%20study%20on%0AMathematical%20Expression%20Recognition%20%28MER%29%20towards%20complex%20real-world%20scenarios.%0AThe%20UniMER%20dataset%20consists%20of%20a%20large-scale%20training%20set%20UniMER-1M%20offering%20an%0Aunprecedented%20scale%20and%20diversity%20with%20one%20million%20training%20instances%20and%20a%0Ameticulously%20designed%20test%20set%20UniMER-Test%20that%20reflects%20a%20diverse%20range%20of%0Aformula%20distributions%20prevalent%20in%20real-world%20scenarios.%20Therefore%2C%20the%20UniMER%0Adataset%20enables%20the%20training%20of%20a%20robust%20and%20high-accuracy%20MER%20model%20and%0Acomprehensive%20evaluation%20of%20model%20performance.%20Moreover%2C%20we%20introduce%20the%0AUniversal%20Mathematical%20Expression%20Recognition%20Network%20%28UniMERNet%29%2C%20an%0Ainnovative%20framework%20designed%20to%20enhance%20MER%20in%20practical%20scenarios.%20UniMERNet%0Aincorporates%20a%20Length-Aware%20Module%20to%20process%20formulas%20of%20varied%20lengths%0Aefficiently%2C%20thereby%20enabling%20the%20model%20to%20handle%20complex%20mathematical%0Aexpressions%20with%20greater%20accuracy.%20In%20addition%2C%20UniMERNet%20employs%20our%20UniMER-1M%0Adata%20and%20image%20augmentation%20techniques%20to%20improve%20the%20model%27s%20robustness%20under%0Adifferent%20noise%20conditions.%20Our%20extensive%20experiments%20demonstrate%20that%0AUniMERNet%20outperforms%20existing%20MER%20models%2C%20setting%20a%20new%20benchmark%20in%20various%0Ascenarios%20and%20ensuring%20superior%20recognition%20quality%20in%20real-world%20applications.%0AThe%20dataset%20and%20model%20are%20available%20at%0Ahttps%3A//github.com/opendatalab/UniMERNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15254v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMERNet%3A%20A%20Universal%20Network%20for%20Real-World%20Mathematical%20Expression%0A%20%20Recognition&entry.906535625=Bin%20Wang%20and%20Zhuangcheng%20Gu%20and%20Chao%20Xu%20and%20Bo%20Zhang%20and%20Botian%20Shi%20and%20Conghui%20He&entry.1292438233=%20%20This%20paper%20presents%20the%20UniMER%20dataset%20to%20provide%20the%20first%20study%20on%0AMathematical%20Expression%20Recognition%20%28MER%29%20towards%20complex%20real-world%20scenarios.%0AThe%20UniMER%20dataset%20consists%20of%20a%20large-scale%20training%20set%20UniMER-1M%20offering%20an%0Aunprecedented%20scale%20and%20diversity%20with%20one%20million%20training%20instances%20and%20a%0Ameticulously%20designed%20test%20set%20UniMER-Test%20that%20reflects%20a%20diverse%20range%20of%0Aformula%20distributions%20prevalent%20in%20real-world%20scenarios.%20Therefore%2C%20the%20UniMER%0Adataset%20enables%20the%20training%20of%20a%20robust%20and%20high-accuracy%20MER%20model%20and%0Acomprehensive%20evaluation%20of%20model%20performance.%20Moreover%2C%20we%20introduce%20the%0AUniversal%20Mathematical%20Expression%20Recognition%20Network%20%28UniMERNet%29%2C%20an%0Ainnovative%20framework%20designed%20to%20enhance%20MER%20in%20practical%20scenarios.%20UniMERNet%0Aincorporates%20a%20Length-Aware%20Module%20to%20process%20formulas%20of%20varied%20lengths%0Aefficiently%2C%20thereby%20enabling%20the%20model%20to%20handle%20complex%20mathematical%0Aexpressions%20with%20greater%20accuracy.%20In%20addition%2C%20UniMERNet%20employs%20our%20UniMER-1M%0Adata%20and%20image%20augmentation%20techniques%20to%20improve%20the%20model%27s%20robustness%20under%0Adifferent%20noise%20conditions.%20Our%20extensive%20experiments%20demonstrate%20that%0AUniMERNet%20outperforms%20existing%20MER%20models%2C%20setting%20a%20new%20benchmark%20in%20various%0Ascenarios%20and%20ensuring%20superior%20recognition%20quality%20in%20real-world%20applications.%0AThe%20dataset%20and%20model%20are%20available%20at%0Ahttps%3A//github.com/opendatalab/UniMERNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15254v1&entry.124074799=Read"},
{"title": "Explicit Second-Order Min-Max Optimization Methods with Optimal\n  Convergence Guarantee", "author": "Tianyi Lin and Panayotis Mertikopoulos and Michael I. Jordan", "abstract": "  We propose and analyze several inexact regularized Newton-type methods for\nfinding a global saddle point of \\emph{convex-concave} unconstrained min-max\noptimization problems. Compared to first-order methods, our understanding of\nsecond-order methods for min-max optimization is relatively limited, as\nobtaining global rates of convergence with second-order information is much\nmore involved. In this paper, we examine how second-order information can be\nused to speed up extra-gradient methods, even under inexactness. Specifically,\nwe show that the proposed methods generate iterates that remain within a\nbounded set and that the averaged iterates converge to an $\\epsilon$-saddle\npoint within $O(\\epsilon^{-2/3})$ iterations in terms of a restricted gap\nfunction. This matched the theoretically established lower bound in this\ncontext. We also provide a simple routine for solving the subproblem at each\niteration, requiring a single Schur decomposition and $O(\\log\\log(1/\\epsilon))$\ncalls to a linear system solver in a quasi-upper-triangular system. Thus, our\nmethod improves the existing line-search-based second-order min-max\noptimization methods by shaving off an $O(\\log\\log(1/\\epsilon))$ factor in the\nrequired number of Schur decompositions. Finally, we present numerical\nexperiments on synthetic and real data that demonstrate the efficiency of the\nproposed methods.\n", "link": "http://arxiv.org/abs/2210.12860v4", "date": "2024-04-23", "relevancy": 1.9621, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3941}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3922}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3909}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Explicit%20Second-Order%20Min-Max%20Optimization%20Methods%20with%20Optimal%0A%20%20Convergence%20Guarantee&body=Title%3A%20Explicit%20Second-Order%20Min-Max%20Optimization%20Methods%20with%20Optimal%0A%20%20Convergence%20Guarantee%0AAuthor%3A%20Tianyi%20Lin%20and%20Panayotis%20Mertikopoulos%20and%20Michael%20I.%20Jordan%0AAbstract%3A%20%20%20We%20propose%20and%20analyze%20several%20inexact%20regularized%20Newton-type%20methods%20for%0Afinding%20a%20global%20saddle%20point%20of%20%5Cemph%7Bconvex-concave%7D%20unconstrained%20min-max%0Aoptimization%20problems.%20Compared%20to%20first-order%20methods%2C%20our%20understanding%20of%0Asecond-order%20methods%20for%20min-max%20optimization%20is%20relatively%20limited%2C%20as%0Aobtaining%20global%20rates%20of%20convergence%20with%20second-order%20information%20is%20much%0Amore%20involved.%20In%20this%20paper%2C%20we%20examine%20how%20second-order%20information%20can%20be%0Aused%20to%20speed%20up%20extra-gradient%20methods%2C%20even%20under%20inexactness.%20Specifically%2C%0Awe%20show%20that%20the%20proposed%20methods%20generate%20iterates%20that%20remain%20within%20a%0Abounded%20set%20and%20that%20the%20averaged%20iterates%20converge%20to%20an%20%24%5Cepsilon%24-saddle%0Apoint%20within%20%24O%28%5Cepsilon%5E%7B-2/3%7D%29%24%20iterations%20in%20terms%20of%20a%20restricted%20gap%0Afunction.%20This%20matched%20the%20theoretically%20established%20lower%20bound%20in%20this%0Acontext.%20We%20also%20provide%20a%20simple%20routine%20for%20solving%20the%20subproblem%20at%20each%0Aiteration%2C%20requiring%20a%20single%20Schur%20decomposition%20and%20%24O%28%5Clog%5Clog%281/%5Cepsilon%29%29%24%0Acalls%20to%20a%20linear%20system%20solver%20in%20a%20quasi-upper-triangular%20system.%20Thus%2C%20our%0Amethod%20improves%20the%20existing%20line-search-based%20second-order%20min-max%0Aoptimization%20methods%20by%20shaving%20off%20an%20%24O%28%5Clog%5Clog%281/%5Cepsilon%29%29%24%20factor%20in%20the%0Arequired%20number%20of%20Schur%20decompositions.%20Finally%2C%20we%20present%20numerical%0Aexperiments%20on%20synthetic%20and%20real%20data%20that%20demonstrate%20the%20efficiency%20of%20the%0Aproposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.12860v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Second-Order%20Min-Max%20Optimization%20Methods%20with%20Optimal%0A%20%20Convergence%20Guarantee&entry.906535625=Tianyi%20Lin%20and%20Panayotis%20Mertikopoulos%20and%20Michael%20I.%20Jordan&entry.1292438233=%20%20We%20propose%20and%20analyze%20several%20inexact%20regularized%20Newton-type%20methods%20for%0Afinding%20a%20global%20saddle%20point%20of%20%5Cemph%7Bconvex-concave%7D%20unconstrained%20min-max%0Aoptimization%20problems.%20Compared%20to%20first-order%20methods%2C%20our%20understanding%20of%0Asecond-order%20methods%20for%20min-max%20optimization%20is%20relatively%20limited%2C%20as%0Aobtaining%20global%20rates%20of%20convergence%20with%20second-order%20information%20is%20much%0Amore%20involved.%20In%20this%20paper%2C%20we%20examine%20how%20second-order%20information%20can%20be%0Aused%20to%20speed%20up%20extra-gradient%20methods%2C%20even%20under%20inexactness.%20Specifically%2C%0Awe%20show%20that%20the%20proposed%20methods%20generate%20iterates%20that%20remain%20within%20a%0Abounded%20set%20and%20that%20the%20averaged%20iterates%20converge%20to%20an%20%24%5Cepsilon%24-saddle%0Apoint%20within%20%24O%28%5Cepsilon%5E%7B-2/3%7D%29%24%20iterations%20in%20terms%20of%20a%20restricted%20gap%0Afunction.%20This%20matched%20the%20theoretically%20established%20lower%20bound%20in%20this%0Acontext.%20We%20also%20provide%20a%20simple%20routine%20for%20solving%20the%20subproblem%20at%20each%0Aiteration%2C%20requiring%20a%20single%20Schur%20decomposition%20and%20%24O%28%5Clog%5Clog%281/%5Cepsilon%29%29%24%0Acalls%20to%20a%20linear%20system%20solver%20in%20a%20quasi-upper-triangular%20system.%20Thus%2C%20our%0Amethod%20improves%20the%20existing%20line-search-based%20second-order%20min-max%0Aoptimization%20methods%20by%20shaving%20off%20an%20%24O%28%5Clog%5Clog%281/%5Cepsilon%29%29%24%20factor%20in%20the%0Arequired%20number%20of%20Schur%20decompositions.%20Finally%2C%20we%20present%20numerical%0Aexperiments%20on%20synthetic%20and%20real%20data%20that%20demonstrate%20the%20efficiency%20of%20the%0Aproposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.12860v4&entry.124074799=Read"},
{"title": "PHLP: Sole Persistent Homology for Link Prediction -- Interpretable\n  Feature Extraction", "author": "Junwon You and Eunwoo Heo and Jae-Hun Jung", "abstract": "  Link prediction (LP), inferring the connectivity between nodes, is a\nsignificant research area in graph data, where a link represents essential\ninformation on relationships between nodes. Although graph neural network\n(GNN)-based models have achieved high performance in LP, understanding why they\nperform well is challenging because most comprise complex neural networks. We\nemploy persistent homology (PH), a topological data analysis method that helps\nanalyze the topological information of graphs, to explain the reasons for the\nhigh performance. We propose a novel method that employs PH for LP (PHLP)\nfocusing on how the presence or absence of target links influences the overall\ntopology. The PHLP utilizes the angle hop subgraph and new node labeling called\ndegree double radius node labeling (Degree DRNL), distinguishing the\ninformation of graphs better than DRNL. Using only a classifier, PHLP performs\nsimilarly to state-of-the-art (SOTA) models on most benchmark datasets.\nIncorporating the outputs calculated using PHLP into the existing GNN-based\nSOTA models improves performance across all benchmark datasets. To the best of\nour knowledge, PHLP is the first method of applying PH to LP without GNNs. The\nproposed approach, employing PH while not relying on neural networks, enables\nthe identification of crucial factors for improving performance.\n", "link": "http://arxiv.org/abs/2404.15225v1", "date": "2024-04-23", "relevancy": 1.9609, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5111}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4769}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4713}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PHLP%3A%20Sole%20Persistent%20Homology%20for%20Link%20Prediction%20--%20Interpretable%0A%20%20Feature%20Extraction&body=Title%3A%20PHLP%3A%20Sole%20Persistent%20Homology%20for%20Link%20Prediction%20--%20Interpretable%0A%20%20Feature%20Extraction%0AAuthor%3A%20Junwon%20You%20and%20Eunwoo%20Heo%20and%20Jae-Hun%20Jung%0AAbstract%3A%20%20%20Link%20prediction%20%28LP%29%2C%20inferring%20the%20connectivity%20between%20nodes%2C%20is%20a%0Asignificant%20research%20area%20in%20graph%20data%2C%20where%20a%20link%20represents%20essential%0Ainformation%20on%20relationships%20between%20nodes.%20Although%20graph%20neural%20network%0A%28GNN%29-based%20models%20have%20achieved%20high%20performance%20in%20LP%2C%20understanding%20why%20they%0Aperform%20well%20is%20challenging%20because%20most%20comprise%20complex%20neural%20networks.%20We%0Aemploy%20persistent%20homology%20%28PH%29%2C%20a%20topological%20data%20analysis%20method%20that%20helps%0Aanalyze%20the%20topological%20information%20of%20graphs%2C%20to%20explain%20the%20reasons%20for%20the%0Ahigh%20performance.%20We%20propose%20a%20novel%20method%20that%20employs%20PH%20for%20LP%20%28PHLP%29%0Afocusing%20on%20how%20the%20presence%20or%20absence%20of%20target%20links%20influences%20the%20overall%0Atopology.%20The%20PHLP%20utilizes%20the%20angle%20hop%20subgraph%20and%20new%20node%20labeling%20called%0Adegree%20double%20radius%20node%20labeling%20%28Degree%20DRNL%29%2C%20distinguishing%20the%0Ainformation%20of%20graphs%20better%20than%20DRNL.%20Using%20only%20a%20classifier%2C%20PHLP%20performs%0Asimilarly%20to%20state-of-the-art%20%28SOTA%29%20models%20on%20most%20benchmark%20datasets.%0AIncorporating%20the%20outputs%20calculated%20using%20PHLP%20into%20the%20existing%20GNN-based%0ASOTA%20models%20improves%20performance%20across%20all%20benchmark%20datasets.%20To%20the%20best%20of%0Aour%20knowledge%2C%20PHLP%20is%20the%20first%20method%20of%20applying%20PH%20to%20LP%20without%20GNNs.%20The%0Aproposed%20approach%2C%20employing%20PH%20while%20not%20relying%20on%20neural%20networks%2C%20enables%0Athe%20identification%20of%20crucial%20factors%20for%20improving%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15225v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PHLP%3A%20Sole%20Persistent%20Homology%20for%20Link%20Prediction%20--%20Interpretable%0A%20%20Feature%20Extraction&entry.906535625=Junwon%20You%20and%20Eunwoo%20Heo%20and%20Jae-Hun%20Jung&entry.1292438233=%20%20Link%20prediction%20%28LP%29%2C%20inferring%20the%20connectivity%20between%20nodes%2C%20is%20a%0Asignificant%20research%20area%20in%20graph%20data%2C%20where%20a%20link%20represents%20essential%0Ainformation%20on%20relationships%20between%20nodes.%20Although%20graph%20neural%20network%0A%28GNN%29-based%20models%20have%20achieved%20high%20performance%20in%20LP%2C%20understanding%20why%20they%0Aperform%20well%20is%20challenging%20because%20most%20comprise%20complex%20neural%20networks.%20We%0Aemploy%20persistent%20homology%20%28PH%29%2C%20a%20topological%20data%20analysis%20method%20that%20helps%0Aanalyze%20the%20topological%20information%20of%20graphs%2C%20to%20explain%20the%20reasons%20for%20the%0Ahigh%20performance.%20We%20propose%20a%20novel%20method%20that%20employs%20PH%20for%20LP%20%28PHLP%29%0Afocusing%20on%20how%20the%20presence%20or%20absence%20of%20target%20links%20influences%20the%20overall%0Atopology.%20The%20PHLP%20utilizes%20the%20angle%20hop%20subgraph%20and%20new%20node%20labeling%20called%0Adegree%20double%20radius%20node%20labeling%20%28Degree%20DRNL%29%2C%20distinguishing%20the%0Ainformation%20of%20graphs%20better%20than%20DRNL.%20Using%20only%20a%20classifier%2C%20PHLP%20performs%0Asimilarly%20to%20state-of-the-art%20%28SOTA%29%20models%20on%20most%20benchmark%20datasets.%0AIncorporating%20the%20outputs%20calculated%20using%20PHLP%20into%20the%20existing%20GNN-based%0ASOTA%20models%20improves%20performance%20across%20all%20benchmark%20datasets.%20To%20the%20best%20of%0Aour%20knowledge%2C%20PHLP%20is%20the%20first%20method%20of%20applying%20PH%20to%20LP%20without%20GNNs.%20The%0Aproposed%20approach%2C%20employing%20PH%20while%20not%20relying%20on%20neural%20networks%2C%20enables%0Athe%20identification%20of%20crucial%20factors%20for%20improving%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15225v1&entry.124074799=Read"},
{"title": "A Learning Paradigm for Interpretable Gradients", "author": "Felipe Torres Figueroa and Hanwei Zhang and Ronan Sicre and Yannis Avrithis and Stephane Ayache", "abstract": "  This paper studies interpretability of convolutional networks by means of\nsaliency maps. Most approaches based on Class Activation Maps (CAM) combine\ninformation from fully connected layers and gradient through variants of\nbackpropagation. However, it is well understood that gradients are noisy and\nalternatives like guided backpropagation have been proposed to obtain better\nvisualization at inference. In this work, we present a novel training approach\nto improve the quality of gradients for interpretability. In particular, we\nintroduce a regularization loss such that the gradient with respect to the\ninput image obtained by standard backpropagation is similar to the gradient\nobtained by guided backpropagation. We find that the resulting gradient is\nqualitatively less noisy and improves quantitatively the interpretability\nproperties of different networks, using several interpretability methods.\n", "link": "http://arxiv.org/abs/2404.15024v1", "date": "2024-04-23", "relevancy": 1.959, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.495}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4945}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4826}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Learning%20Paradigm%20for%20Interpretable%20Gradients&body=Title%3A%20A%20Learning%20Paradigm%20for%20Interpretable%20Gradients%0AAuthor%3A%20Felipe%20Torres%20Figueroa%20and%20Hanwei%20Zhang%20and%20Ronan%20Sicre%20and%20Yannis%20Avrithis%20and%20Stephane%20Ayache%0AAbstract%3A%20%20%20This%20paper%20studies%20interpretability%20of%20convolutional%20networks%20by%20means%20of%0Asaliency%20maps.%20Most%20approaches%20based%20on%20Class%20Activation%20Maps%20%28CAM%29%20combine%0Ainformation%20from%20fully%20connected%20layers%20and%20gradient%20through%20variants%20of%0Abackpropagation.%20However%2C%20it%20is%20well%20understood%20that%20gradients%20are%20noisy%20and%0Aalternatives%20like%20guided%20backpropagation%20have%20been%20proposed%20to%20obtain%20better%0Avisualization%20at%20inference.%20In%20this%20work%2C%20we%20present%20a%20novel%20training%20approach%0Ato%20improve%20the%20quality%20of%20gradients%20for%20interpretability.%20In%20particular%2C%20we%0Aintroduce%20a%20regularization%20loss%20such%20that%20the%20gradient%20with%20respect%20to%20the%0Ainput%20image%20obtained%20by%20standard%20backpropagation%20is%20similar%20to%20the%20gradient%0Aobtained%20by%20guided%20backpropagation.%20We%20find%20that%20the%20resulting%20gradient%20is%0Aqualitatively%20less%20noisy%20and%20improves%20quantitatively%20the%20interpretability%0Aproperties%20of%20different%20networks%2C%20using%20several%20interpretability%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15024v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Learning%20Paradigm%20for%20Interpretable%20Gradients&entry.906535625=Felipe%20Torres%20Figueroa%20and%20Hanwei%20Zhang%20and%20Ronan%20Sicre%20and%20Yannis%20Avrithis%20and%20Stephane%20Ayache&entry.1292438233=%20%20This%20paper%20studies%20interpretability%20of%20convolutional%20networks%20by%20means%20of%0Asaliency%20maps.%20Most%20approaches%20based%20on%20Class%20Activation%20Maps%20%28CAM%29%20combine%0Ainformation%20from%20fully%20connected%20layers%20and%20gradient%20through%20variants%20of%0Abackpropagation.%20However%2C%20it%20is%20well%20understood%20that%20gradients%20are%20noisy%20and%0Aalternatives%20like%20guided%20backpropagation%20have%20been%20proposed%20to%20obtain%20better%0Avisualization%20at%20inference.%20In%20this%20work%2C%20we%20present%20a%20novel%20training%20approach%0Ato%20improve%20the%20quality%20of%20gradients%20for%20interpretability.%20In%20particular%2C%20we%0Aintroduce%20a%20regularization%20loss%20such%20that%20the%20gradient%20with%20respect%20to%20the%0Ainput%20image%20obtained%20by%20standard%20backpropagation%20is%20similar%20to%20the%20gradient%0Aobtained%20by%20guided%20backpropagation.%20We%20find%20that%20the%20resulting%20gradient%20is%0Aqualitatively%20less%20noisy%20and%20improves%20quantitatively%20the%20interpretability%0Aproperties%20of%20different%20networks%2C%20using%20several%20interpretability%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15024v1&entry.124074799=Read"},
{"title": "Formal Verification of Graph Convolutional Networks with Uncertain Node\n  Features and Uncertain Graph Structure", "author": "Tobias Ladner and Michael Eichelbeck and Matthias Althoff", "abstract": "  Graph neural networks are becoming increasingly popular in the field of\nmachine learning due to their unique ability to process data structured in\ngraphs. They have also been applied in safety-critical environments where\nperturbations inherently occur. However, these perturbations require us to\nformally verify neural networks before their deployment in safety-critical\nenvironments as neural networks are prone to adversarial attacks. While there\nexists research on the formal verification of neural networks, there is no work\nverifying the robustness of generic graph convolutional network architectures\nwith uncertainty in the node features and in the graph structure over multiple\nmessage-passing steps. This work addresses this research gap by explicitly\npreserving the non-convex dependencies of all elements in the underlying\ncomputations through reachability analysis with (matrix) polynomial zonotopes.\nWe demonstrate our approach on three popular benchmark datasets.\n", "link": "http://arxiv.org/abs/2404.15065v1", "date": "2024-04-23", "relevancy": 1.9561, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5074}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4794}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4671}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Formal%20Verification%20of%20Graph%20Convolutional%20Networks%20with%20Uncertain%20Node%0A%20%20Features%20and%20Uncertain%20Graph%20Structure&body=Title%3A%20Formal%20Verification%20of%20Graph%20Convolutional%20Networks%20with%20Uncertain%20Node%0A%20%20Features%20and%20Uncertain%20Graph%20Structure%0AAuthor%3A%20Tobias%20Ladner%20and%20Michael%20Eichelbeck%20and%20Matthias%20Althoff%0AAbstract%3A%20%20%20Graph%20neural%20networks%20are%20becoming%20increasingly%20popular%20in%20the%20field%20of%0Amachine%20learning%20due%20to%20their%20unique%20ability%20to%20process%20data%20structured%20in%0Agraphs.%20They%20have%20also%20been%20applied%20in%20safety-critical%20environments%20where%0Aperturbations%20inherently%20occur.%20However%2C%20these%20perturbations%20require%20us%20to%0Aformally%20verify%20neural%20networks%20before%20their%20deployment%20in%20safety-critical%0Aenvironments%20as%20neural%20networks%20are%20prone%20to%20adversarial%20attacks.%20While%20there%0Aexists%20research%20on%20the%20formal%20verification%20of%20neural%20networks%2C%20there%20is%20no%20work%0Averifying%20the%20robustness%20of%20generic%20graph%20convolutional%20network%20architectures%0Awith%20uncertainty%20in%20the%20node%20features%20and%20in%20the%20graph%20structure%20over%20multiple%0Amessage-passing%20steps.%20This%20work%20addresses%20this%20research%20gap%20by%20explicitly%0Apreserving%20the%20non-convex%20dependencies%20of%20all%20elements%20in%20the%20underlying%0Acomputations%20through%20reachability%20analysis%20with%20%28matrix%29%20polynomial%20zonotopes.%0AWe%20demonstrate%20our%20approach%20on%20three%20popular%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15065v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Formal%20Verification%20of%20Graph%20Convolutional%20Networks%20with%20Uncertain%20Node%0A%20%20Features%20and%20Uncertain%20Graph%20Structure&entry.906535625=Tobias%20Ladner%20and%20Michael%20Eichelbeck%20and%20Matthias%20Althoff&entry.1292438233=%20%20Graph%20neural%20networks%20are%20becoming%20increasingly%20popular%20in%20the%20field%20of%0Amachine%20learning%20due%20to%20their%20unique%20ability%20to%20process%20data%20structured%20in%0Agraphs.%20They%20have%20also%20been%20applied%20in%20safety-critical%20environments%20where%0Aperturbations%20inherently%20occur.%20However%2C%20these%20perturbations%20require%20us%20to%0Aformally%20verify%20neural%20networks%20before%20their%20deployment%20in%20safety-critical%0Aenvironments%20as%20neural%20networks%20are%20prone%20to%20adversarial%20attacks.%20While%20there%0Aexists%20research%20on%20the%20formal%20verification%20of%20neural%20networks%2C%20there%20is%20no%20work%0Averifying%20the%20robustness%20of%20generic%20graph%20convolutional%20network%20architectures%0Awith%20uncertainty%20in%20the%20node%20features%20and%20in%20the%20graph%20structure%20over%20multiple%0Amessage-passing%20steps.%20This%20work%20addresses%20this%20research%20gap%20by%20explicitly%0Apreserving%20the%20non-convex%20dependencies%20of%20all%20elements%20in%20the%20underlying%0Acomputations%20through%20reachability%20analysis%20with%20%28matrix%29%20polynomial%20zonotopes.%0AWe%20demonstrate%20our%20approach%20on%20three%20popular%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15065v1&entry.124074799=Read"},
{"title": "Visual Grounding Methods for VQA are Working for the Wrong Reasons!", "author": "Robik Shrestha and Kushal Kafle and Christopher Kanan", "abstract": "  Existing Visual Question Answering (VQA) methods tend to exploit dataset\nbiases and spurious statistical correlations, instead of producing right\nanswers for the right reasons. To address this issue, recent bias mitigation\nmethods for VQA propose to incorporate visual cues (e.g., human attention maps)\nto better ground the VQA models, showcasing impressive gains. However, we show\nthat the performance improvements are not a result of improved visual\ngrounding, but a regularization effect which prevents over-fitting to\nlinguistic priors. For instance, we find that it is not actually necessary to\nprovide proper, human-based cues; random, insensible cues also result in\nsimilar improvements. Based on this observation, we propose a simpler\nregularization scheme that does not require any external annotations and yet\nachieves near state-of-the-art performance on VQA-CPv2.\n", "link": "http://arxiv.org/abs/2004.05704v4", "date": "2024-04-23", "relevancy": 1.9543, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4982}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4943}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4767}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visual%20Grounding%20Methods%20for%20VQA%20are%20Working%20for%20the%20Wrong%20Reasons%21&body=Title%3A%20Visual%20Grounding%20Methods%20for%20VQA%20are%20Working%20for%20the%20Wrong%20Reasons%21%0AAuthor%3A%20Robik%20Shrestha%20and%20Kushal%20Kafle%20and%20Christopher%20Kanan%0AAbstract%3A%20%20%20Existing%20Visual%20Question%20Answering%20%28VQA%29%20methods%20tend%20to%20exploit%20dataset%0Abiases%20and%20spurious%20statistical%20correlations%2C%20instead%20of%20producing%20right%0Aanswers%20for%20the%20right%20reasons.%20To%20address%20this%20issue%2C%20recent%20bias%20mitigation%0Amethods%20for%20VQA%20propose%20to%20incorporate%20visual%20cues%20%28e.g.%2C%20human%20attention%20maps%29%0Ato%20better%20ground%20the%20VQA%20models%2C%20showcasing%20impressive%20gains.%20However%2C%20we%20show%0Athat%20the%20performance%20improvements%20are%20not%20a%20result%20of%20improved%20visual%0Agrounding%2C%20but%20a%20regularization%20effect%20which%20prevents%20over-fitting%20to%0Alinguistic%20priors.%20For%20instance%2C%20we%20find%20that%20it%20is%20not%20actually%20necessary%20to%0Aprovide%20proper%2C%20human-based%20cues%3B%20random%2C%20insensible%20cues%20also%20result%20in%0Asimilar%20improvements.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20simpler%0Aregularization%20scheme%20that%20does%20not%20require%20any%20external%20annotations%20and%20yet%0Aachieves%20near%20state-of-the-art%20performance%20on%20VQA-CPv2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2004.05704v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Grounding%20Methods%20for%20VQA%20are%20Working%20for%20the%20Wrong%20Reasons%21&entry.906535625=Robik%20Shrestha%20and%20Kushal%20Kafle%20and%20Christopher%20Kanan&entry.1292438233=%20%20Existing%20Visual%20Question%20Answering%20%28VQA%29%20methods%20tend%20to%20exploit%20dataset%0Abiases%20and%20spurious%20statistical%20correlations%2C%20instead%20of%20producing%20right%0Aanswers%20for%20the%20right%20reasons.%20To%20address%20this%20issue%2C%20recent%20bias%20mitigation%0Amethods%20for%20VQA%20propose%20to%20incorporate%20visual%20cues%20%28e.g.%2C%20human%20attention%20maps%29%0Ato%20better%20ground%20the%20VQA%20models%2C%20showcasing%20impressive%20gains.%20However%2C%20we%20show%0Athat%20the%20performance%20improvements%20are%20not%20a%20result%20of%20improved%20visual%0Agrounding%2C%20but%20a%20regularization%20effect%20which%20prevents%20over-fitting%20to%0Alinguistic%20priors.%20For%20instance%2C%20we%20find%20that%20it%20is%20not%20actually%20necessary%20to%0Aprovide%20proper%2C%20human-based%20cues%3B%20random%2C%20insensible%20cues%20also%20result%20in%0Asimilar%20improvements.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20simpler%0Aregularization%20scheme%20that%20does%20not%20require%20any%20external%20annotations%20and%20yet%0Aachieves%20near%20state-of-the-art%20performance%20on%20VQA-CPv2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2004.05704v4&entry.124074799=Read"},
{"title": "Linear Optimal Partial Transport Embedding", "author": "Yikun Bai and Ivan Medri and Rocio Diaz Martin and Rana Muhammad Shahroz Khan and Soheil Kolouri", "abstract": "  Optimal transport (OT) has gained popularity due to its various applications\nin fields such as machine learning, statistics, and signal processing. However,\nthe balanced mass requirement limits its performance in practical problems. To\naddress these limitations, variants of the OT problem, including unbalanced OT,\nOptimal partial transport (OPT), and Hellinger Kantorovich (HK), have been\nproposed. In this paper, we propose the Linear optimal partial transport (LOPT)\nembedding, which extends the (local) linearization technique on OT and HK to\nthe OPT problem. The proposed embedding allows for faster computation of OPT\ndistance between pairs of positive measures. Besides our theoretical\ncontributions, we demonstrate the LOPT embedding technique in point-cloud\ninterpolation and PCA analysis.\n", "link": "http://arxiv.org/abs/2302.03232v5", "date": "2024-04-23", "relevancy": 1.9504, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5065}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4747}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4739}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Linear%20Optimal%20Partial%20Transport%20Embedding&body=Title%3A%20Linear%20Optimal%20Partial%20Transport%20Embedding%0AAuthor%3A%20Yikun%20Bai%20and%20Ivan%20Medri%20and%20Rocio%20Diaz%20Martin%20and%20Rana%20Muhammad%20Shahroz%20Khan%20and%20Soheil%20Kolouri%0AAbstract%3A%20%20%20Optimal%20transport%20%28OT%29%20has%20gained%20popularity%20due%20to%20its%20various%20applications%0Ain%20fields%20such%20as%20machine%20learning%2C%20statistics%2C%20and%20signal%20processing.%20However%2C%0Athe%20balanced%20mass%20requirement%20limits%20its%20performance%20in%20practical%20problems.%20To%0Aaddress%20these%20limitations%2C%20variants%20of%20the%20OT%20problem%2C%20including%20unbalanced%20OT%2C%0AOptimal%20partial%20transport%20%28OPT%29%2C%20and%20Hellinger%20Kantorovich%20%28HK%29%2C%20have%20been%0Aproposed.%20In%20this%20paper%2C%20we%20propose%20the%20Linear%20optimal%20partial%20transport%20%28LOPT%29%0Aembedding%2C%20which%20extends%20the%20%28local%29%20linearization%20technique%20on%20OT%20and%20HK%20to%0Athe%20OPT%20problem.%20The%20proposed%20embedding%20allows%20for%20faster%20computation%20of%20OPT%0Adistance%20between%20pairs%20of%20positive%20measures.%20Besides%20our%20theoretical%0Acontributions%2C%20we%20demonstrate%20the%20LOPT%20embedding%20technique%20in%20point-cloud%0Ainterpolation%20and%20PCA%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03232v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Optimal%20Partial%20Transport%20Embedding&entry.906535625=Yikun%20Bai%20and%20Ivan%20Medri%20and%20Rocio%20Diaz%20Martin%20and%20Rana%20Muhammad%20Shahroz%20Khan%20and%20Soheil%20Kolouri&entry.1292438233=%20%20Optimal%20transport%20%28OT%29%20has%20gained%20popularity%20due%20to%20its%20various%20applications%0Ain%20fields%20such%20as%20machine%20learning%2C%20statistics%2C%20and%20signal%20processing.%20However%2C%0Athe%20balanced%20mass%20requirement%20limits%20its%20performance%20in%20practical%20problems.%20To%0Aaddress%20these%20limitations%2C%20variants%20of%20the%20OT%20problem%2C%20including%20unbalanced%20OT%2C%0AOptimal%20partial%20transport%20%28OPT%29%2C%20and%20Hellinger%20Kantorovich%20%28HK%29%2C%20have%20been%0Aproposed.%20In%20this%20paper%2C%20we%20propose%20the%20Linear%20optimal%20partial%20transport%20%28LOPT%29%0Aembedding%2C%20which%20extends%20the%20%28local%29%20linearization%20technique%20on%20OT%20and%20HK%20to%0Athe%20OPT%20problem.%20The%20proposed%20embedding%20allows%20for%20faster%20computation%20of%20OPT%0Adistance%20between%20pairs%20of%20positive%20measures.%20Besides%20our%20theoretical%0Acontributions%2C%20we%20demonstrate%20the%20LOPT%20embedding%20technique%20in%20point-cloud%0Ainterpolation%20and%20PCA%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03232v5&entry.124074799=Read"},
{"title": "Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition\n  Against Model Inversion Attack", "author": "Yinggui Wang and Yuanqing Huang and Jianshu Li and Le Yang and Kai Song and Lei Wang", "abstract": "  The utilization of personal sensitive data in training face recognition (FR)\nmodels poses significant privacy concerns, as adversaries can employ model\ninversion attacks (MIA) to infer the original training data. Existing defense\nmethods, such as data augmentation and differential privacy, have been employed\nto mitigate this issue. However, these methods often fail to strike an optimal\nbalance between privacy and accuracy. To address this limitation, this paper\nintroduces an adaptive hybrid masking algorithm against MIA. Specifically, face\nimages are masked in the frequency domain using an adaptive MixUp strategy.\nUnlike the traditional MixUp algorithm, which is predominantly used for data\naugmentation, our modified approach incorporates frequency domain mixing.\nPrevious studies have shown that increasing the number of images mixed in MixUp\ncan enhance privacy preservation but at the expense of reduced face recognition\naccuracy. To overcome this trade-off, we develop an enhanced adaptive MixUp\nstrategy based on reinforcement learning, which enables us to mix a larger\nnumber of images while maintaining satisfactory recognition accuracy. To\noptimize privacy protection, we propose maximizing the reward function (i.e.,\nthe loss function of the FR system) during the training of the strategy\nnetwork. While the loss function of the FR network is minimized in the phase of\ntraining the FR network. The strategy network and the face recognition network\ncan be viewed as antagonistic entities in the training process, ultimately\nreaching a more balanced trade-off. Experimental results demonstrate that our\nproposed hybrid masking scheme outperforms existing defense algorithms in terms\nof privacy preservation and recognition accuracy against MIA.\n", "link": "http://arxiv.org/abs/2403.10558v2", "date": "2024-04-23", "relevancy": 1.9502, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5034}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4827}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Hybrid%20Masking%20Strategy%20for%20Privacy-Preserving%20Face%20Recognition%0A%20%20Against%20Model%20Inversion%20Attack&body=Title%3A%20Adaptive%20Hybrid%20Masking%20Strategy%20for%20Privacy-Preserving%20Face%20Recognition%0A%20%20Against%20Model%20Inversion%20Attack%0AAuthor%3A%20Yinggui%20Wang%20and%20Yuanqing%20Huang%20and%20Jianshu%20Li%20and%20Le%20Yang%20and%20Kai%20Song%20and%20Lei%20Wang%0AAbstract%3A%20%20%20The%20utilization%20of%20personal%20sensitive%20data%20in%20training%20face%20recognition%20%28FR%29%0Amodels%20poses%20significant%20privacy%20concerns%2C%20as%20adversaries%20can%20employ%20model%0Ainversion%20attacks%20%28MIA%29%20to%20infer%20the%20original%20training%20data.%20Existing%20defense%0Amethods%2C%20such%20as%20data%20augmentation%20and%20differential%20privacy%2C%20have%20been%20employed%0Ato%20mitigate%20this%20issue.%20However%2C%20these%20methods%20often%20fail%20to%20strike%20an%20optimal%0Abalance%20between%20privacy%20and%20accuracy.%20To%20address%20this%20limitation%2C%20this%20paper%0Aintroduces%20an%20adaptive%20hybrid%20masking%20algorithm%20against%20MIA.%20Specifically%2C%20face%0Aimages%20are%20masked%20in%20the%20frequency%20domain%20using%20an%20adaptive%20MixUp%20strategy.%0AUnlike%20the%20traditional%20MixUp%20algorithm%2C%20which%20is%20predominantly%20used%20for%20data%0Aaugmentation%2C%20our%20modified%20approach%20incorporates%20frequency%20domain%20mixing.%0APrevious%20studies%20have%20shown%20that%20increasing%20the%20number%20of%20images%20mixed%20in%20MixUp%0Acan%20enhance%20privacy%20preservation%20but%20at%20the%20expense%20of%20reduced%20face%20recognition%0Aaccuracy.%20To%20overcome%20this%20trade-off%2C%20we%20develop%20an%20enhanced%20adaptive%20MixUp%0Astrategy%20based%20on%20reinforcement%20learning%2C%20which%20enables%20us%20to%20mix%20a%20larger%0Anumber%20of%20images%20while%20maintaining%20satisfactory%20recognition%20accuracy.%20To%0Aoptimize%20privacy%20protection%2C%20we%20propose%20maximizing%20the%20reward%20function%20%28i.e.%2C%0Athe%20loss%20function%20of%20the%20FR%20system%29%20during%20the%20training%20of%20the%20strategy%0Anetwork.%20While%20the%20loss%20function%20of%20the%20FR%20network%20is%20minimized%20in%20the%20phase%20of%0Atraining%20the%20FR%20network.%20The%20strategy%20network%20and%20the%20face%20recognition%20network%0Acan%20be%20viewed%20as%20antagonistic%20entities%20in%20the%20training%20process%2C%20ultimately%0Areaching%20a%20more%20balanced%20trade-off.%20Experimental%20results%20demonstrate%20that%20our%0Aproposed%20hybrid%20masking%20scheme%20outperforms%20existing%20defense%20algorithms%20in%20terms%0Aof%20privacy%20preservation%20and%20recognition%20accuracy%20against%20MIA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10558v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Hybrid%20Masking%20Strategy%20for%20Privacy-Preserving%20Face%20Recognition%0A%20%20Against%20Model%20Inversion%20Attack&entry.906535625=Yinggui%20Wang%20and%20Yuanqing%20Huang%20and%20Jianshu%20Li%20and%20Le%20Yang%20and%20Kai%20Song%20and%20Lei%20Wang&entry.1292438233=%20%20The%20utilization%20of%20personal%20sensitive%20data%20in%20training%20face%20recognition%20%28FR%29%0Amodels%20poses%20significant%20privacy%20concerns%2C%20as%20adversaries%20can%20employ%20model%0Ainversion%20attacks%20%28MIA%29%20to%20infer%20the%20original%20training%20data.%20Existing%20defense%0Amethods%2C%20such%20as%20data%20augmentation%20and%20differential%20privacy%2C%20have%20been%20employed%0Ato%20mitigate%20this%20issue.%20However%2C%20these%20methods%20often%20fail%20to%20strike%20an%20optimal%0Abalance%20between%20privacy%20and%20accuracy.%20To%20address%20this%20limitation%2C%20this%20paper%0Aintroduces%20an%20adaptive%20hybrid%20masking%20algorithm%20against%20MIA.%20Specifically%2C%20face%0Aimages%20are%20masked%20in%20the%20frequency%20domain%20using%20an%20adaptive%20MixUp%20strategy.%0AUnlike%20the%20traditional%20MixUp%20algorithm%2C%20which%20is%20predominantly%20used%20for%20data%0Aaugmentation%2C%20our%20modified%20approach%20incorporates%20frequency%20domain%20mixing.%0APrevious%20studies%20have%20shown%20that%20increasing%20the%20number%20of%20images%20mixed%20in%20MixUp%0Acan%20enhance%20privacy%20preservation%20but%20at%20the%20expense%20of%20reduced%20face%20recognition%0Aaccuracy.%20To%20overcome%20this%20trade-off%2C%20we%20develop%20an%20enhanced%20adaptive%20MixUp%0Astrategy%20based%20on%20reinforcement%20learning%2C%20which%20enables%20us%20to%20mix%20a%20larger%0Anumber%20of%20images%20while%20maintaining%20satisfactory%20recognition%20accuracy.%20To%0Aoptimize%20privacy%20protection%2C%20we%20propose%20maximizing%20the%20reward%20function%20%28i.e.%2C%0Athe%20loss%20function%20of%20the%20FR%20system%29%20during%20the%20training%20of%20the%20strategy%0Anetwork.%20While%20the%20loss%20function%20of%20the%20FR%20network%20is%20minimized%20in%20the%20phase%20of%0Atraining%20the%20FR%20network.%20The%20strategy%20network%20and%20the%20face%20recognition%20network%0Acan%20be%20viewed%20as%20antagonistic%20entities%20in%20the%20training%20process%2C%20ultimately%0Areaching%20a%20more%20balanced%20trade-off.%20Experimental%20results%20demonstrate%20that%20our%0Aproposed%20hybrid%20masking%20scheme%20outperforms%20existing%20defense%20algorithms%20in%20terms%0Aof%20privacy%20preservation%20and%20recognition%20accuracy%20against%20MIA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10558v2&entry.124074799=Read"},
{"title": "Fin-Fed-OD: Federated Outlier Detection on Financial Tabular Data", "author": "Dayananda Herurkar and Sebastian Palacio and Ahmed Anwar and Joern Hees and Andreas Dengel", "abstract": "  Anomaly detection in real-world scenarios poses challenges due to dynamic and\noften unknown anomaly distributions, requiring robust methods that operate\nunder an open-world assumption. This challenge is exacerbated in practical\nsettings, where models are employed by private organizations, precluding data\nsharing due to privacy and competitive concerns. Despite potential benefits,\nthe sharing of anomaly information across organizations is restricted. This\npaper addresses the question of enhancing outlier detection within individual\norganizations without compromising data confidentiality. We propose a novel\nmethod leveraging representation learning and federated learning techniques to\nimprove the detection of unknown anomalies. Specifically, our approach utilizes\nlatent representations obtained from client-owned autoencoders to refine the\ndecision boundary of inliers. Notably, only model parameters are shared between\norganizations, preserving data privacy. The efficacy of our proposed method is\nevaluated on two standard financial tabular datasets and an image dataset for\nanomaly detection in a distributed setting. The results demonstrate a strong\nimprovement in the classification of unknown outliers during the inference\nphase for each organization's model.\n", "link": "http://arxiv.org/abs/2404.14933v1", "date": "2024-04-23", "relevancy": 1.9498, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5033}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5006}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4664}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fin-Fed-OD%3A%20Federated%20Outlier%20Detection%20on%20Financial%20Tabular%20Data&body=Title%3A%20Fin-Fed-OD%3A%20Federated%20Outlier%20Detection%20on%20Financial%20Tabular%20Data%0AAuthor%3A%20Dayananda%20Herurkar%20and%20Sebastian%20Palacio%20and%20Ahmed%20Anwar%20and%20Joern%20Hees%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20real-world%20scenarios%20poses%20challenges%20due%20to%20dynamic%20and%0Aoften%20unknown%20anomaly%20distributions%2C%20requiring%20robust%20methods%20that%20operate%0Aunder%20an%20open-world%20assumption.%20This%20challenge%20is%20exacerbated%20in%20practical%0Asettings%2C%20where%20models%20are%20employed%20by%20private%20organizations%2C%20precluding%20data%0Asharing%20due%20to%20privacy%20and%20competitive%20concerns.%20Despite%20potential%20benefits%2C%0Athe%20sharing%20of%20anomaly%20information%20across%20organizations%20is%20restricted.%20This%0Apaper%20addresses%20the%20question%20of%20enhancing%20outlier%20detection%20within%20individual%0Aorganizations%20without%20compromising%20data%20confidentiality.%20We%20propose%20a%20novel%0Amethod%20leveraging%20representation%20learning%20and%20federated%20learning%20techniques%20to%0Aimprove%20the%20detection%20of%20unknown%20anomalies.%20Specifically%2C%20our%20approach%20utilizes%0Alatent%20representations%20obtained%20from%20client-owned%20autoencoders%20to%20refine%20the%0Adecision%20boundary%20of%20inliers.%20Notably%2C%20only%20model%20parameters%20are%20shared%20between%0Aorganizations%2C%20preserving%20data%20privacy.%20The%20efficacy%20of%20our%20proposed%20method%20is%0Aevaluated%20on%20two%20standard%20financial%20tabular%20datasets%20and%20an%20image%20dataset%20for%0Aanomaly%20detection%20in%20a%20distributed%20setting.%20The%20results%20demonstrate%20a%20strong%0Aimprovement%20in%20the%20classification%20of%20unknown%20outliers%20during%20the%20inference%0Aphase%20for%20each%20organization%27s%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14933v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fin-Fed-OD%3A%20Federated%20Outlier%20Detection%20on%20Financial%20Tabular%20Data&entry.906535625=Dayananda%20Herurkar%20and%20Sebastian%20Palacio%20and%20Ahmed%20Anwar%20and%20Joern%20Hees%20and%20Andreas%20Dengel&entry.1292438233=%20%20Anomaly%20detection%20in%20real-world%20scenarios%20poses%20challenges%20due%20to%20dynamic%20and%0Aoften%20unknown%20anomaly%20distributions%2C%20requiring%20robust%20methods%20that%20operate%0Aunder%20an%20open-world%20assumption.%20This%20challenge%20is%20exacerbated%20in%20practical%0Asettings%2C%20where%20models%20are%20employed%20by%20private%20organizations%2C%20precluding%20data%0Asharing%20due%20to%20privacy%20and%20competitive%20concerns.%20Despite%20potential%20benefits%2C%0Athe%20sharing%20of%20anomaly%20information%20across%20organizations%20is%20restricted.%20This%0Apaper%20addresses%20the%20question%20of%20enhancing%20outlier%20detection%20within%20individual%0Aorganizations%20without%20compromising%20data%20confidentiality.%20We%20propose%20a%20novel%0Amethod%20leveraging%20representation%20learning%20and%20federated%20learning%20techniques%20to%0Aimprove%20the%20detection%20of%20unknown%20anomalies.%20Specifically%2C%20our%20approach%20utilizes%0Alatent%20representations%20obtained%20from%20client-owned%20autoencoders%20to%20refine%20the%0Adecision%20boundary%20of%20inliers.%20Notably%2C%20only%20model%20parameters%20are%20shared%20between%0Aorganizations%2C%20preserving%20data%20privacy.%20The%20efficacy%20of%20our%20proposed%20method%20is%0Aevaluated%20on%20two%20standard%20financial%20tabular%20datasets%20and%20an%20image%20dataset%20for%0Aanomaly%20detection%20in%20a%20distributed%20setting.%20The%20results%20demonstrate%20a%20strong%0Aimprovement%20in%20the%20classification%20of%20unknown%20outliers%20during%20the%20inference%0Aphase%20for%20each%20organization%27s%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14933v1&entry.124074799=Read"},
{"title": "Multi-Head Mixture-of-Experts", "author": "Xun Wu and Shaohan Huang and Wenhui Wang and Furu Wei", "abstract": "  Sparse Mixtures of Experts (SMoE) scales model capacity without significant\nincreases in training and inference costs, but exhibits the following two\nissues: (1) Low expert activation, where only a small subset of experts are\nactivated for optimization. (2) Lacking fine-grained analytical capabilities\nfor multiple semantic concepts within individual tokens. We propose Multi-Head\nMixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each\ntoken into multiple sub-tokens. These sub-tokens are then assigned to and\nprocessed by a diverse set of experts in parallel, and seamlessly reintegrated\ninto the original token form. The multi-head mechanism enables the model to\ncollectively attend to information from various representation spaces within\ndifferent experts, while significantly enhances expert activation, thus deepens\ncontext understanding and alleviate overfitting. Moreover, our MH-MoE is\nstraightforward to implement and decouples from other SMoE optimization\nmethods, making it easy to integrate with other SMoE models for enhanced\nperformance. Extensive experimental results across three tasks: English-focused\nlanguage modeling, Multi-lingual language modeling and Masked multi-modality\nmodeling tasks, demonstrate the effectiveness of MH-MoE.\n", "link": "http://arxiv.org/abs/2404.15045v1", "date": "2024-04-23", "relevancy": 1.9416, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4759}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4754}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Head%20Mixture-of-Experts&body=Title%3A%20Multi-Head%20Mixture-of-Experts%0AAuthor%3A%20Xun%20Wu%20and%20Shaohan%20Huang%20and%20Wenhui%20Wang%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Sparse%20Mixtures%20of%20Experts%20%28SMoE%29%20scales%20model%20capacity%20without%20significant%0Aincreases%20in%20training%20and%20inference%20costs%2C%20but%20exhibits%20the%20following%20two%0Aissues%3A%20%281%29%20Low%20expert%20activation%2C%20where%20only%20a%20small%20subset%20of%20experts%20are%0Aactivated%20for%20optimization.%20%282%29%20Lacking%20fine-grained%20analytical%20capabilities%0Afor%20multiple%20semantic%20concepts%20within%20individual%20tokens.%20We%20propose%20Multi-Head%0AMixture-of-Experts%20%28MH-MoE%29%2C%20which%20employs%20a%20multi-head%20mechanism%20to%20split%20each%0Atoken%20into%20multiple%20sub-tokens.%20These%20sub-tokens%20are%20then%20assigned%20to%20and%0Aprocessed%20by%20a%20diverse%20set%20of%20experts%20in%20parallel%2C%20and%20seamlessly%20reintegrated%0Ainto%20the%20original%20token%20form.%20The%20multi-head%20mechanism%20enables%20the%20model%20to%0Acollectively%20attend%20to%20information%20from%20various%20representation%20spaces%20within%0Adifferent%20experts%2C%20while%20significantly%20enhances%20expert%20activation%2C%20thus%20deepens%0Acontext%20understanding%20and%20alleviate%20overfitting.%20Moreover%2C%20our%20MH-MoE%20is%0Astraightforward%20to%20implement%20and%20decouples%20from%20other%20SMoE%20optimization%0Amethods%2C%20making%20it%20easy%20to%20integrate%20with%20other%20SMoE%20models%20for%20enhanced%0Aperformance.%20Extensive%20experimental%20results%20across%20three%20tasks%3A%20English-focused%0Alanguage%20modeling%2C%20Multi-lingual%20language%20modeling%20and%20Masked%20multi-modality%0Amodeling%20tasks%2C%20demonstrate%20the%20effectiveness%20of%20MH-MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15045v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Head%20Mixture-of-Experts&entry.906535625=Xun%20Wu%20and%20Shaohan%20Huang%20and%20Wenhui%20Wang%20and%20Furu%20Wei&entry.1292438233=%20%20Sparse%20Mixtures%20of%20Experts%20%28SMoE%29%20scales%20model%20capacity%20without%20significant%0Aincreases%20in%20training%20and%20inference%20costs%2C%20but%20exhibits%20the%20following%20two%0Aissues%3A%20%281%29%20Low%20expert%20activation%2C%20where%20only%20a%20small%20subset%20of%20experts%20are%0Aactivated%20for%20optimization.%20%282%29%20Lacking%20fine-grained%20analytical%20capabilities%0Afor%20multiple%20semantic%20concepts%20within%20individual%20tokens.%20We%20propose%20Multi-Head%0AMixture-of-Experts%20%28MH-MoE%29%2C%20which%20employs%20a%20multi-head%20mechanism%20to%20split%20each%0Atoken%20into%20multiple%20sub-tokens.%20These%20sub-tokens%20are%20then%20assigned%20to%20and%0Aprocessed%20by%20a%20diverse%20set%20of%20experts%20in%20parallel%2C%20and%20seamlessly%20reintegrated%0Ainto%20the%20original%20token%20form.%20The%20multi-head%20mechanism%20enables%20the%20model%20to%0Acollectively%20attend%20to%20information%20from%20various%20representation%20spaces%20within%0Adifferent%20experts%2C%20while%20significantly%20enhances%20expert%20activation%2C%20thus%20deepens%0Acontext%20understanding%20and%20alleviate%20overfitting.%20Moreover%2C%20our%20MH-MoE%20is%0Astraightforward%20to%20implement%20and%20decouples%20from%20other%20SMoE%20optimization%0Amethods%2C%20making%20it%20easy%20to%20integrate%20with%20other%20SMoE%20models%20for%20enhanced%0Aperformance.%20Extensive%20experimental%20results%20across%20three%20tasks%3A%20English-focused%0Alanguage%20modeling%2C%20Multi-lingual%20language%20modeling%20and%20Masked%20multi-modality%0Amodeling%20tasks%2C%20demonstrate%20the%20effectiveness%20of%20MH-MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15045v1&entry.124074799=Read"},
{"title": "LLMs4OM: Matching Ontologies with Large Language Models", "author": "Hamed Babaei Giglou and Jennifer D'Souza and Felix Engel and S\u00f6ren Auer", "abstract": "  Ontology Matching (OM), is a critical task in knowledge integration, where\naligning heterogeneous ontologies facilitates data interoperability and\nknowledge sharing. Traditional OM systems often rely on expert knowledge or\npredictive models, with limited exploration of the potential of Large Language\nModels (LLMs). We present the LLMs4OM framework, a novel approach to evaluate\nthe effectiveness of LLMs in OM tasks. This framework utilizes two modules for\nretrieval and matching, respectively, enhanced by zero-shot prompting across\nthree ontology representations: concept, concept-parent, and concept-children.\nThrough comprehensive evaluations using 20 OM datasets from various domains, we\ndemonstrate that LLMs, under the LLMs4OM framework, can match and even surpass\nthe performance of traditional OM systems, particularly in complex matching\nscenarios. Our results highlight the potential of LLMs to significantly\ncontribute to the field of OM.\n", "link": "http://arxiv.org/abs/2404.10317v2", "date": "2024-04-23", "relevancy": 1.9355, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5103}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4783}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4597}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLMs4OM%3A%20Matching%20Ontologies%20with%20Large%20Language%20Models&body=Title%3A%20LLMs4OM%3A%20Matching%20Ontologies%20with%20Large%20Language%20Models%0AAuthor%3A%20Hamed%20Babaei%20Giglou%20and%20Jennifer%20D%27Souza%20and%20Felix%20Engel%20and%20S%C3%B6ren%20Auer%0AAbstract%3A%20%20%20Ontology%20Matching%20%28OM%29%2C%20is%20a%20critical%20task%20in%20knowledge%20integration%2C%20where%0Aaligning%20heterogeneous%20ontologies%20facilitates%20data%20interoperability%20and%0Aknowledge%20sharing.%20Traditional%20OM%20systems%20often%20rely%20on%20expert%20knowledge%20or%0Apredictive%20models%2C%20with%20limited%20exploration%20of%20the%20potential%20of%20Large%20Language%0AModels%20%28LLMs%29.%20We%20present%20the%20LLMs4OM%20framework%2C%20a%20novel%20approach%20to%20evaluate%0Athe%20effectiveness%20of%20LLMs%20in%20OM%20tasks.%20This%20framework%20utilizes%20two%20modules%20for%0Aretrieval%20and%20matching%2C%20respectively%2C%20enhanced%20by%20zero-shot%20prompting%20across%0Athree%20ontology%20representations%3A%20concept%2C%20concept-parent%2C%20and%20concept-children.%0AThrough%20comprehensive%20evaluations%20using%2020%20OM%20datasets%20from%20various%20domains%2C%20we%0Ademonstrate%20that%20LLMs%2C%20under%20the%20LLMs4OM%20framework%2C%20can%20match%20and%20even%20surpass%0Athe%20performance%20of%20traditional%20OM%20systems%2C%20particularly%20in%20complex%20matching%0Ascenarios.%20Our%20results%20highlight%20the%20potential%20of%20LLMs%20to%20significantly%0Acontribute%20to%20the%20field%20of%20OM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10317v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs4OM%3A%20Matching%20Ontologies%20with%20Large%20Language%20Models&entry.906535625=Hamed%20Babaei%20Giglou%20and%20Jennifer%20D%27Souza%20and%20Felix%20Engel%20and%20S%C3%B6ren%20Auer&entry.1292438233=%20%20Ontology%20Matching%20%28OM%29%2C%20is%20a%20critical%20task%20in%20knowledge%20integration%2C%20where%0Aaligning%20heterogeneous%20ontologies%20facilitates%20data%20interoperability%20and%0Aknowledge%20sharing.%20Traditional%20OM%20systems%20often%20rely%20on%20expert%20knowledge%20or%0Apredictive%20models%2C%20with%20limited%20exploration%20of%20the%20potential%20of%20Large%20Language%0AModels%20%28LLMs%29.%20We%20present%20the%20LLMs4OM%20framework%2C%20a%20novel%20approach%20to%20evaluate%0Athe%20effectiveness%20of%20LLMs%20in%20OM%20tasks.%20This%20framework%20utilizes%20two%20modules%20for%0Aretrieval%20and%20matching%2C%20respectively%2C%20enhanced%20by%20zero-shot%20prompting%20across%0Athree%20ontology%20representations%3A%20concept%2C%20concept-parent%2C%20and%20concept-children.%0AThrough%20comprehensive%20evaluations%20using%2020%20OM%20datasets%20from%20various%20domains%2C%20we%0Ademonstrate%20that%20LLMs%2C%20under%20the%20LLMs4OM%20framework%2C%20can%20match%20and%20even%20surpass%0Athe%20performance%20of%20traditional%20OM%20systems%2C%20particularly%20in%20complex%20matching%0Ascenarios.%20Our%20results%20highlight%20the%20potential%20of%20LLMs%20to%20significantly%0Acontribute%20to%20the%20field%20of%20OM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10317v2&entry.124074799=Read"},
{"title": "Aligning LLM Agents by Learning Latent Preference from User Edits", "author": "Ge Gao and Alexey Taymanov and Eduardo Salinas and Paul Mineiro and Dipendra Misra", "abstract": "  We study interactive learning of language agents based on user edits made to\nthe agent's output. In a typical setting such as writing assistants, the user\ninteracts with a language agent to generate a response given a context, and may\noptionally edit the agent response to personalize it based on their latent\npreference, in addition to improving the correctness. The edit feedback is\nnaturally generated, making it a suitable candidate for improving the agent's\nalignment with the user's preference, and for reducing the cost of user edits\nover time. We propose a learning framework, PRELUDE that infers a description\nof the user's latent preference based on historic edit data and using it to\ndefine a prompt policy that drives future response generation. This avoids\nfine-tuning the agent, which is costly, challenging to scale with the number of\nusers, and may even degrade its performance on other tasks. Furthermore,\nlearning descriptive preference improves interpretability, allowing the user to\nview and modify the learned preference. However, user preference can be complex\nand vary based on context, making it challenging to learn. To address this, we\npropose a simple yet effective algorithm named CIPHER that leverages a large\nlanguage model (LLM) to infer the user preference for a given context based on\nuser edits. In the future, CIPHER retrieves inferred preferences from the\nk-closest contexts in the history, and forms an aggregate preference for\nresponse generation. We introduce two interactive environments -- summarization\nand email writing, for evaluation using a GPT-4 simulated user. We compare with\nalgorithms that directly retrieve user edits but do not learn descriptive\npreference, and algorithms that learn context-agnostic preference. On both\ntasks, CIPHER achieves the lowest edit distance cost and learns preferences\nthat show significant similarity to the ground truth preferences\n", "link": "http://arxiv.org/abs/2404.15269v1", "date": "2024-04-23", "relevancy": 1.9319, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4912}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.488}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4728}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Aligning%20LLM%20Agents%20by%20Learning%20Latent%20Preference%20from%20User%20Edits&body=Title%3A%20Aligning%20LLM%20Agents%20by%20Learning%20Latent%20Preference%20from%20User%20Edits%0AAuthor%3A%20Ge%20Gao%20and%20Alexey%20Taymanov%20and%20Eduardo%20Salinas%20and%20Paul%20Mineiro%20and%20Dipendra%20Misra%0AAbstract%3A%20%20%20We%20study%20interactive%20learning%20of%20language%20agents%20based%20on%20user%20edits%20made%20to%0Athe%20agent%27s%20output.%20In%20a%20typical%20setting%20such%20as%20writing%20assistants%2C%20the%20user%0Ainteracts%20with%20a%20language%20agent%20to%20generate%20a%20response%20given%20a%20context%2C%20and%20may%0Aoptionally%20edit%20the%20agent%20response%20to%20personalize%20it%20based%20on%20their%20latent%0Apreference%2C%20in%20addition%20to%20improving%20the%20correctness.%20The%20edit%20feedback%20is%0Anaturally%20generated%2C%20making%20it%20a%20suitable%20candidate%20for%20improving%20the%20agent%27s%0Aalignment%20with%20the%20user%27s%20preference%2C%20and%20for%20reducing%20the%20cost%20of%20user%20edits%0Aover%20time.%20We%20propose%20a%20learning%20framework%2C%20PRELUDE%20that%20infers%20a%20description%0Aof%20the%20user%27s%20latent%20preference%20based%20on%20historic%20edit%20data%20and%20using%20it%20to%0Adefine%20a%20prompt%20policy%20that%20drives%20future%20response%20generation.%20This%20avoids%0Afine-tuning%20the%20agent%2C%20which%20is%20costly%2C%20challenging%20to%20scale%20with%20the%20number%20of%0Ausers%2C%20and%20may%20even%20degrade%20its%20performance%20on%20other%20tasks.%20Furthermore%2C%0Alearning%20descriptive%20preference%20improves%20interpretability%2C%20allowing%20the%20user%20to%0Aview%20and%20modify%20the%20learned%20preference.%20However%2C%20user%20preference%20can%20be%20complex%0Aand%20vary%20based%20on%20context%2C%20making%20it%20challenging%20to%20learn.%20To%20address%20this%2C%20we%0Apropose%20a%20simple%20yet%20effective%20algorithm%20named%20CIPHER%20that%20leverages%20a%20large%0Alanguage%20model%20%28LLM%29%20to%20infer%20the%20user%20preference%20for%20a%20given%20context%20based%20on%0Auser%20edits.%20In%20the%20future%2C%20CIPHER%20retrieves%20inferred%20preferences%20from%20the%0Ak-closest%20contexts%20in%20the%20history%2C%20and%20forms%20an%20aggregate%20preference%20for%0Aresponse%20generation.%20We%20introduce%20two%20interactive%20environments%20--%20summarization%0Aand%20email%20writing%2C%20for%20evaluation%20using%20a%20GPT-4%20simulated%20user.%20We%20compare%20with%0Aalgorithms%20that%20directly%20retrieve%20user%20edits%20but%20do%20not%20learn%20descriptive%0Apreference%2C%20and%20algorithms%20that%20learn%20context-agnostic%20preference.%20On%20both%0Atasks%2C%20CIPHER%20achieves%20the%20lowest%20edit%20distance%20cost%20and%20learns%20preferences%0Athat%20show%20significant%20similarity%20to%20the%20ground%20truth%20preferences%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15269v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20LLM%20Agents%20by%20Learning%20Latent%20Preference%20from%20User%20Edits&entry.906535625=Ge%20Gao%20and%20Alexey%20Taymanov%20and%20Eduardo%20Salinas%20and%20Paul%20Mineiro%20and%20Dipendra%20Misra&entry.1292438233=%20%20We%20study%20interactive%20learning%20of%20language%20agents%20based%20on%20user%20edits%20made%20to%0Athe%20agent%27s%20output.%20In%20a%20typical%20setting%20such%20as%20writing%20assistants%2C%20the%20user%0Ainteracts%20with%20a%20language%20agent%20to%20generate%20a%20response%20given%20a%20context%2C%20and%20may%0Aoptionally%20edit%20the%20agent%20response%20to%20personalize%20it%20based%20on%20their%20latent%0Apreference%2C%20in%20addition%20to%20improving%20the%20correctness.%20The%20edit%20feedback%20is%0Anaturally%20generated%2C%20making%20it%20a%20suitable%20candidate%20for%20improving%20the%20agent%27s%0Aalignment%20with%20the%20user%27s%20preference%2C%20and%20for%20reducing%20the%20cost%20of%20user%20edits%0Aover%20time.%20We%20propose%20a%20learning%20framework%2C%20PRELUDE%20that%20infers%20a%20description%0Aof%20the%20user%27s%20latent%20preference%20based%20on%20historic%20edit%20data%20and%20using%20it%20to%0Adefine%20a%20prompt%20policy%20that%20drives%20future%20response%20generation.%20This%20avoids%0Afine-tuning%20the%20agent%2C%20which%20is%20costly%2C%20challenging%20to%20scale%20with%20the%20number%20of%0Ausers%2C%20and%20may%20even%20degrade%20its%20performance%20on%20other%20tasks.%20Furthermore%2C%0Alearning%20descriptive%20preference%20improves%20interpretability%2C%20allowing%20the%20user%20to%0Aview%20and%20modify%20the%20learned%20preference.%20However%2C%20user%20preference%20can%20be%20complex%0Aand%20vary%20based%20on%20context%2C%20making%20it%20challenging%20to%20learn.%20To%20address%20this%2C%20we%0Apropose%20a%20simple%20yet%20effective%20algorithm%20named%20CIPHER%20that%20leverages%20a%20large%0Alanguage%20model%20%28LLM%29%20to%20infer%20the%20user%20preference%20for%20a%20given%20context%20based%20on%0Auser%20edits.%20In%20the%20future%2C%20CIPHER%20retrieves%20inferred%20preferences%20from%20the%0Ak-closest%20contexts%20in%20the%20history%2C%20and%20forms%20an%20aggregate%20preference%20for%0Aresponse%20generation.%20We%20introduce%20two%20interactive%20environments%20--%20summarization%0Aand%20email%20writing%2C%20for%20evaluation%20using%20a%20GPT-4%20simulated%20user.%20We%20compare%20with%0Aalgorithms%20that%20directly%20retrieve%20user%20edits%20but%20do%20not%20learn%20descriptive%0Apreference%2C%20and%20algorithms%20that%20learn%20context-agnostic%20preference.%20On%20both%0Atasks%2C%20CIPHER%20achieves%20the%20lowest%20edit%20distance%20cost%20and%20learns%20preferences%0Athat%20show%20significant%20similarity%20to%20the%20ground%20truth%20preferences%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15269v1&entry.124074799=Read"},
{"title": "Goldfish: An Efficient Federated Unlearning Framework", "author": "Houzhe Wang and Xiaojie Zhu and Chi Chen and Paulo Esteves-Ver\u00edssimo", "abstract": "  With recent legislation on the right to be forgotten, machine unlearning has\nemerged as a crucial research area. It facilitates the removal of a user's data\nfrom federated trained machine learning models without the necessity for\nretraining from scratch. However, current machine unlearning algorithms are\nconfronted with challenges of efficiency and validity. To address the above\nissues, we propose a new framework, named Goldfish. It comprises four modules:\nbasic model, loss function, optimization, and extension. To address the\nchallenge of low validity in existing machine unlearning algorithms, we propose\na novel loss function. It takes into account the loss arising from the\ndiscrepancy between predictions and actual labels in the remaining dataset.\nSimultaneously, it takes into consideration the bias of predicted results on\nthe removed dataset. Moreover, it accounts for the confidence level of\npredicted results. Additionally, to enhance efficiency, we adopt knowledge a\ndistillation technique in the basic model and introduce an optimization module\nthat encompasses the early termination mechanism guided by empirical risk and\nthe data partition mechanism. Furthermore, to bolster the robustness of the\naggregated model, we propose an extension module that incorporates a mechanism\nusing adaptive distillation temperature to address the heterogeneity of user\nlocal data and a mechanism using adaptive weight to handle the variety in the\nquality of uploaded models. Finally, we conduct comprehensive experiments to\nillustrate the effectiveness of proposed approach.\n", "link": "http://arxiv.org/abs/2404.03180v2", "date": "2024-04-23", "relevancy": 1.9308, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5314}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4733}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4727}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Goldfish%3A%20An%20Efficient%20Federated%20Unlearning%20Framework&body=Title%3A%20Goldfish%3A%20An%20Efficient%20Federated%20Unlearning%20Framework%0AAuthor%3A%20Houzhe%20Wang%20and%20Xiaojie%20Zhu%20and%20Chi%20Chen%20and%20Paulo%20Esteves-Ver%C3%ADssimo%0AAbstract%3A%20%20%20With%20recent%20legislation%20on%20the%20right%20to%20be%20forgotten%2C%20machine%20unlearning%20has%0Aemerged%20as%20a%20crucial%20research%20area.%20It%20facilitates%20the%20removal%20of%20a%20user%27s%20data%0Afrom%20federated%20trained%20machine%20learning%20models%20without%20the%20necessity%20for%0Aretraining%20from%20scratch.%20However%2C%20current%20machine%20unlearning%20algorithms%20are%0Aconfronted%20with%20challenges%20of%20efficiency%20and%20validity.%20To%20address%20the%20above%0Aissues%2C%20we%20propose%20a%20new%20framework%2C%20named%20Goldfish.%20It%20comprises%20four%20modules%3A%0Abasic%20model%2C%20loss%20function%2C%20optimization%2C%20and%20extension.%20To%20address%20the%0Achallenge%20of%20low%20validity%20in%20existing%20machine%20unlearning%20algorithms%2C%20we%20propose%0Aa%20novel%20loss%20function.%20It%20takes%20into%20account%20the%20loss%20arising%20from%20the%0Adiscrepancy%20between%20predictions%20and%20actual%20labels%20in%20the%20remaining%20dataset.%0ASimultaneously%2C%20it%20takes%20into%20consideration%20the%20bias%20of%20predicted%20results%20on%0Athe%20removed%20dataset.%20Moreover%2C%20it%20accounts%20for%20the%20confidence%20level%20of%0Apredicted%20results.%20Additionally%2C%20to%20enhance%20efficiency%2C%20we%20adopt%20knowledge%20a%0Adistillation%20technique%20in%20the%20basic%20model%20and%20introduce%20an%20optimization%20module%0Athat%20encompasses%20the%20early%20termination%20mechanism%20guided%20by%20empirical%20risk%20and%0Athe%20data%20partition%20mechanism.%20Furthermore%2C%20to%20bolster%20the%20robustness%20of%20the%0Aaggregated%20model%2C%20we%20propose%20an%20extension%20module%20that%20incorporates%20a%20mechanism%0Ausing%20adaptive%20distillation%20temperature%20to%20address%20the%20heterogeneity%20of%20user%0Alocal%20data%20and%20a%20mechanism%20using%20adaptive%20weight%20to%20handle%20the%20variety%20in%20the%0Aquality%20of%20uploaded%20models.%20Finally%2C%20we%20conduct%20comprehensive%20experiments%20to%0Aillustrate%20the%20effectiveness%20of%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03180v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goldfish%3A%20An%20Efficient%20Federated%20Unlearning%20Framework&entry.906535625=Houzhe%20Wang%20and%20Xiaojie%20Zhu%20and%20Chi%20Chen%20and%20Paulo%20Esteves-Ver%C3%ADssimo&entry.1292438233=%20%20With%20recent%20legislation%20on%20the%20right%20to%20be%20forgotten%2C%20machine%20unlearning%20has%0Aemerged%20as%20a%20crucial%20research%20area.%20It%20facilitates%20the%20removal%20of%20a%20user%27s%20data%0Afrom%20federated%20trained%20machine%20learning%20models%20without%20the%20necessity%20for%0Aretraining%20from%20scratch.%20However%2C%20current%20machine%20unlearning%20algorithms%20are%0Aconfronted%20with%20challenges%20of%20efficiency%20and%20validity.%20To%20address%20the%20above%0Aissues%2C%20we%20propose%20a%20new%20framework%2C%20named%20Goldfish.%20It%20comprises%20four%20modules%3A%0Abasic%20model%2C%20loss%20function%2C%20optimization%2C%20and%20extension.%20To%20address%20the%0Achallenge%20of%20low%20validity%20in%20existing%20machine%20unlearning%20algorithms%2C%20we%20propose%0Aa%20novel%20loss%20function.%20It%20takes%20into%20account%20the%20loss%20arising%20from%20the%0Adiscrepancy%20between%20predictions%20and%20actual%20labels%20in%20the%20remaining%20dataset.%0ASimultaneously%2C%20it%20takes%20into%20consideration%20the%20bias%20of%20predicted%20results%20on%0Athe%20removed%20dataset.%20Moreover%2C%20it%20accounts%20for%20the%20confidence%20level%20of%0Apredicted%20results.%20Additionally%2C%20to%20enhance%20efficiency%2C%20we%20adopt%20knowledge%20a%0Adistillation%20technique%20in%20the%20basic%20model%20and%20introduce%20an%20optimization%20module%0Athat%20encompasses%20the%20early%20termination%20mechanism%20guided%20by%20empirical%20risk%20and%0Athe%20data%20partition%20mechanism.%20Furthermore%2C%20to%20bolster%20the%20robustness%20of%20the%0Aaggregated%20model%2C%20we%20propose%20an%20extension%20module%20that%20incorporates%20a%20mechanism%0Ausing%20adaptive%20distillation%20temperature%20to%20address%20the%20heterogeneity%20of%20user%0Alocal%20data%20and%20a%20mechanism%20using%20adaptive%20weight%20to%20handle%20the%20variety%20in%20the%0Aquality%20of%20uploaded%20models.%20Finally%2C%20we%20conduct%20comprehensive%20experiments%20to%0Aillustrate%20the%20effectiveness%20of%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03180v2&entry.124074799=Read"},
{"title": "ALI-DPFL: Differentially Private Federated Learning with Adaptive Local\n  Iterations", "author": "Xinpeng Ling and Jie Fu and Kuncan Wang and Haitao Liu and Zhili Chen", "abstract": "  Federated Learning (FL) is a distributed machine learning technique that\nallows model training among multiple devices or organizations by sharing\ntraining parameters instead of raw data. However, adversaries can still infer\nindividual information through inference attacks (e.g. differential attacks) on\nthese training parameters. As a result, Differential Privacy (DP) has been\nwidely used in FL to prevent such attacks.\n  We consider differentially private federated learning in a\nresource-constrained scenario, where both privacy budget and communication\nrounds are constrained. By theoretically analyzing the convergence, we can find\nthe optimal number of local DPSGD iterations for clients between any two\nsequential global updates. Based on this, we design an algorithm of\nDifferentially Private Federated Learning with Adaptive Local Iterations\n(ALI-DPFL). We experiment our algorithm on the MNIST, FashionMNIST and Cifar10\ndatasets, and demonstrate significantly better performances than previous work\nin the resource-constraint scenario. Code is available at\nhttps://github.com/KnightWan/ALI-DPFL.\n", "link": "http://arxiv.org/abs/2308.10457v6", "date": "2024-04-23", "relevancy": 1.9274, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4851}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4835}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4789}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ALI-DPFL%3A%20Differentially%20Private%20Federated%20Learning%20with%20Adaptive%20Local%0A%20%20Iterations&body=Title%3A%20ALI-DPFL%3A%20Differentially%20Private%20Federated%20Learning%20with%20Adaptive%20Local%0A%20%20Iterations%0AAuthor%3A%20Xinpeng%20Ling%20and%20Jie%20Fu%20and%20Kuncan%20Wang%20and%20Haitao%20Liu%20and%20Zhili%20Chen%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20technique%20that%0Aallows%20model%20training%20among%20multiple%20devices%20or%20organizations%20by%20sharing%0Atraining%20parameters%20instead%20of%20raw%20data.%20However%2C%20adversaries%20can%20still%20infer%0Aindividual%20information%20through%20inference%20attacks%20%28e.g.%20differential%20attacks%29%20on%0Athese%20training%20parameters.%20As%20a%20result%2C%20Differential%20Privacy%20%28DP%29%20has%20been%0Awidely%20used%20in%20FL%20to%20prevent%20such%20attacks.%0A%20%20We%20consider%20differentially%20private%20federated%20learning%20in%20a%0Aresource-constrained%20scenario%2C%20where%20both%20privacy%20budget%20and%20communication%0Arounds%20are%20constrained.%20By%20theoretically%20analyzing%20the%20convergence%2C%20we%20can%20find%0Athe%20optimal%20number%20of%20local%20DPSGD%20iterations%20for%20clients%20between%20any%20two%0Asequential%20global%20updates.%20Based%20on%20this%2C%20we%20design%20an%20algorithm%20of%0ADifferentially%20Private%20Federated%20Learning%20with%20Adaptive%20Local%20Iterations%0A%28ALI-DPFL%29.%20We%20experiment%20our%20algorithm%20on%20the%20MNIST%2C%20FashionMNIST%20and%20Cifar10%0Adatasets%2C%20and%20demonstrate%20significantly%20better%20performances%20than%20previous%20work%0Ain%20the%20resource-constraint%20scenario.%20Code%20is%20available%20at%0Ahttps%3A//github.com/KnightWan/ALI-DPFL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10457v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ALI-DPFL%3A%20Differentially%20Private%20Federated%20Learning%20with%20Adaptive%20Local%0A%20%20Iterations&entry.906535625=Xinpeng%20Ling%20and%20Jie%20Fu%20and%20Kuncan%20Wang%20and%20Haitao%20Liu%20and%20Zhili%20Chen&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20technique%20that%0Aallows%20model%20training%20among%20multiple%20devices%20or%20organizations%20by%20sharing%0Atraining%20parameters%20instead%20of%20raw%20data.%20However%2C%20adversaries%20can%20still%20infer%0Aindividual%20information%20through%20inference%20attacks%20%28e.g.%20differential%20attacks%29%20on%0Athese%20training%20parameters.%20As%20a%20result%2C%20Differential%20Privacy%20%28DP%29%20has%20been%0Awidely%20used%20in%20FL%20to%20prevent%20such%20attacks.%0A%20%20We%20consider%20differentially%20private%20federated%20learning%20in%20a%0Aresource-constrained%20scenario%2C%20where%20both%20privacy%20budget%20and%20communication%0Arounds%20are%20constrained.%20By%20theoretically%20analyzing%20the%20convergence%2C%20we%20can%20find%0Athe%20optimal%20number%20of%20local%20DPSGD%20iterations%20for%20clients%20between%20any%20two%0Asequential%20global%20updates.%20Based%20on%20this%2C%20we%20design%20an%20algorithm%20of%0ADifferentially%20Private%20Federated%20Learning%20with%20Adaptive%20Local%20Iterations%0A%28ALI-DPFL%29.%20We%20experiment%20our%20algorithm%20on%20the%20MNIST%2C%20FashionMNIST%20and%20Cifar10%0Adatasets%2C%20and%20demonstrate%20significantly%20better%20performances%20than%20previous%20work%0Ain%20the%20resource-constraint%20scenario.%20Code%20is%20available%20at%0Ahttps%3A//github.com/KnightWan/ALI-DPFL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10457v6&entry.124074799=Read"},
{"title": "Convergence of a model-free entropy-regularized inverse reinforcement\n  learning algorithm", "author": "Titouan Renard and Andreas Schlaginhaufen and Tingting Ni and Maryam Kamgarpour", "abstract": "  Given a dataset of expert demonstrations, inverse reinforcement learning\n(IRL) aims to recover a reward for which the expert is optimal. This work\nproposes a model-free algorithm to solve entropy-regularized IRL problem. In\nparticular, we employ a stochastic gradient descent update for the reward and a\nstochastic soft policy iteration update for the policy. Assuming access to a\ngenerative model, we prove that our algorithm is guaranteed to recover a reward\nfor which the expert is $\\varepsilon$-optimal using\n$\\mathcal{O}(1/\\varepsilon^{2})$ samples of the Markov decision process (MDP).\nFurthermore, with $\\mathcal{O}(1/\\varepsilon^{4})$ samples we prove that the\noptimal policy corresponding to the recovered reward is $\\varepsilon$-close to\nthe expert policy in total variation distance.\n", "link": "http://arxiv.org/abs/2403.16829v2", "date": "2024-04-23", "relevancy": 1.927, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4936}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4846}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4687}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20a%20model-free%20entropy-regularized%20inverse%20reinforcement%0A%20%20learning%20algorithm&body=Title%3A%20Convergence%20of%20a%20model-free%20entropy-regularized%20inverse%20reinforcement%0A%20%20learning%20algorithm%0AAuthor%3A%20Titouan%20Renard%20and%20Andreas%20Schlaginhaufen%20and%20Tingting%20Ni%20and%20Maryam%20Kamgarpour%0AAbstract%3A%20%20%20Given%20a%20dataset%20of%20expert%20demonstrations%2C%20inverse%20reinforcement%20learning%0A%28IRL%29%20aims%20to%20recover%20a%20reward%20for%20which%20the%20expert%20is%20optimal.%20This%20work%0Aproposes%20a%20model-free%20algorithm%20to%20solve%20entropy-regularized%20IRL%20problem.%20In%0Aparticular%2C%20we%20employ%20a%20stochastic%20gradient%20descent%20update%20for%20the%20reward%20and%20a%0Astochastic%20soft%20policy%20iteration%20update%20for%20the%20policy.%20Assuming%20access%20to%20a%0Agenerative%20model%2C%20we%20prove%20that%20our%20algorithm%20is%20guaranteed%20to%20recover%20a%20reward%0Afor%20which%20the%20expert%20is%20%24%5Cvarepsilon%24-optimal%20using%0A%24%5Cmathcal%7BO%7D%281/%5Cvarepsilon%5E%7B2%7D%29%24%20samples%20of%20the%20Markov%20decision%20process%20%28MDP%29.%0AFurthermore%2C%20with%20%24%5Cmathcal%7BO%7D%281/%5Cvarepsilon%5E%7B4%7D%29%24%20samples%20we%20prove%20that%20the%0Aoptimal%20policy%20corresponding%20to%20the%20recovered%20reward%20is%20%24%5Cvarepsilon%24-close%20to%0Athe%20expert%20policy%20in%20total%20variation%20distance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16829v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20a%20model-free%20entropy-regularized%20inverse%20reinforcement%0A%20%20learning%20algorithm&entry.906535625=Titouan%20Renard%20and%20Andreas%20Schlaginhaufen%20and%20Tingting%20Ni%20and%20Maryam%20Kamgarpour&entry.1292438233=%20%20Given%20a%20dataset%20of%20expert%20demonstrations%2C%20inverse%20reinforcement%20learning%0A%28IRL%29%20aims%20to%20recover%20a%20reward%20for%20which%20the%20expert%20is%20optimal.%20This%20work%0Aproposes%20a%20model-free%20algorithm%20to%20solve%20entropy-regularized%20IRL%20problem.%20In%0Aparticular%2C%20we%20employ%20a%20stochastic%20gradient%20descent%20update%20for%20the%20reward%20and%20a%0Astochastic%20soft%20policy%20iteration%20update%20for%20the%20policy.%20Assuming%20access%20to%20a%0Agenerative%20model%2C%20we%20prove%20that%20our%20algorithm%20is%20guaranteed%20to%20recover%20a%20reward%0Afor%20which%20the%20expert%20is%20%24%5Cvarepsilon%24-optimal%20using%0A%24%5Cmathcal%7BO%7D%281/%5Cvarepsilon%5E%7B2%7D%29%24%20samples%20of%20the%20Markov%20decision%20process%20%28MDP%29.%0AFurthermore%2C%20with%20%24%5Cmathcal%7BO%7D%281/%5Cvarepsilon%5E%7B4%7D%29%24%20samples%20we%20prove%20that%20the%0Aoptimal%20policy%20corresponding%20to%20the%20recovered%20reward%20is%20%24%5Cvarepsilon%24-close%20to%0Athe%20expert%20policy%20in%20total%20variation%20distance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16829v2&entry.124074799=Read"},
{"title": "Unknown Object Grasping for Assistive Robotics", "author": "Elle Miller and Maximilian Durner and Matthias Humt and Gabriel Quere and Wout Boerdijk and Ashok M. Sundaram and Freek Stulp and Jorn Vogel", "abstract": "  We propose a novel pipeline for unknown object grasping in shared robotic\nautonomy scenarios. State-of-the-art methods for fully autonomous scenarios are\ntypically learning-based approaches optimised for a specific end-effector, that\ngenerate grasp poses directly from sensor input. In the domain of assistive\nrobotics, we seek instead to utilise the user's cognitive abilities for\nenhanced satisfaction, grasping performance, and alignment with their high\nlevel task-specific goals. Given a pair of stereo images, we perform unknown\nobject instance segmentation and generate a 3D reconstruction of the object of\ninterest. In shared control, the user then guides the robot end-effector across\na virtual hemisphere centered around the object to their desired approach\ndirection. A physics-based grasp planner finds the most stable local grasp on\nthe reconstruction, and finally the user is guided by shared control to this\ngrasp. In experiments on the DLR EDAN platform, we report a grasp success rate\nof 87% for 10 unknown objects, and demonstrate the method's capability to grasp\nobjects in structured clutter and from shelves.\n", "link": "http://arxiv.org/abs/2404.15001v1", "date": "2024-04-23", "relevancy": 1.9203, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6565}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6351}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6041}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unknown%20Object%20Grasping%20for%20Assistive%20Robotics&body=Title%3A%20Unknown%20Object%20Grasping%20for%20Assistive%20Robotics%0AAuthor%3A%20Elle%20Miller%20and%20Maximilian%20Durner%20and%20Matthias%20Humt%20and%20Gabriel%20Quere%20and%20Wout%20Boerdijk%20and%20Ashok%20M.%20Sundaram%20and%20Freek%20Stulp%20and%20Jorn%20Vogel%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20pipeline%20for%20unknown%20object%20grasping%20in%20shared%20robotic%0Aautonomy%20scenarios.%20State-of-the-art%20methods%20for%20fully%20autonomous%20scenarios%20are%0Atypically%20learning-based%20approaches%20optimised%20for%20a%20specific%20end-effector%2C%20that%0Agenerate%20grasp%20poses%20directly%20from%20sensor%20input.%20In%20the%20domain%20of%20assistive%0Arobotics%2C%20we%20seek%20instead%20to%20utilise%20the%20user%27s%20cognitive%20abilities%20for%0Aenhanced%20satisfaction%2C%20grasping%20performance%2C%20and%20alignment%20with%20their%20high%0Alevel%20task-specific%20goals.%20Given%20a%20pair%20of%20stereo%20images%2C%20we%20perform%20unknown%0Aobject%20instance%20segmentation%20and%20generate%20a%203D%20reconstruction%20of%20the%20object%20of%0Ainterest.%20In%20shared%20control%2C%20the%20user%20then%20guides%20the%20robot%20end-effector%20across%0Aa%20virtual%20hemisphere%20centered%20around%20the%20object%20to%20their%20desired%20approach%0Adirection.%20A%20physics-based%20grasp%20planner%20finds%20the%20most%20stable%20local%20grasp%20on%0Athe%20reconstruction%2C%20and%20finally%20the%20user%20is%20guided%20by%20shared%20control%20to%20this%0Agrasp.%20In%20experiments%20on%20the%20DLR%20EDAN%20platform%2C%20we%20report%20a%20grasp%20success%20rate%0Aof%2087%25%20for%2010%20unknown%20objects%2C%20and%20demonstrate%20the%20method%27s%20capability%20to%20grasp%0Aobjects%20in%20structured%20clutter%20and%20from%20shelves.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15001v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unknown%20Object%20Grasping%20for%20Assistive%20Robotics&entry.906535625=Elle%20Miller%20and%20Maximilian%20Durner%20and%20Matthias%20Humt%20and%20Gabriel%20Quere%20and%20Wout%20Boerdijk%20and%20Ashok%20M.%20Sundaram%20and%20Freek%20Stulp%20and%20Jorn%20Vogel&entry.1292438233=%20%20We%20propose%20a%20novel%20pipeline%20for%20unknown%20object%20grasping%20in%20shared%20robotic%0Aautonomy%20scenarios.%20State-of-the-art%20methods%20for%20fully%20autonomous%20scenarios%20are%0Atypically%20learning-based%20approaches%20optimised%20for%20a%20specific%20end-effector%2C%20that%0Agenerate%20grasp%20poses%20directly%20from%20sensor%20input.%20In%20the%20domain%20of%20assistive%0Arobotics%2C%20we%20seek%20instead%20to%20utilise%20the%20user%27s%20cognitive%20abilities%20for%0Aenhanced%20satisfaction%2C%20grasping%20performance%2C%20and%20alignment%20with%20their%20high%0Alevel%20task-specific%20goals.%20Given%20a%20pair%20of%20stereo%20images%2C%20we%20perform%20unknown%0Aobject%20instance%20segmentation%20and%20generate%20a%203D%20reconstruction%20of%20the%20object%20of%0Ainterest.%20In%20shared%20control%2C%20the%20user%20then%20guides%20the%20robot%20end-effector%20across%0Aa%20virtual%20hemisphere%20centered%20around%20the%20object%20to%20their%20desired%20approach%0Adirection.%20A%20physics-based%20grasp%20planner%20finds%20the%20most%20stable%20local%20grasp%20on%0Athe%20reconstruction%2C%20and%20finally%20the%20user%20is%20guided%20by%20shared%20control%20to%20this%0Agrasp.%20In%20experiments%20on%20the%20DLR%20EDAN%20platform%2C%20we%20report%20a%20grasp%20success%20rate%0Aof%2087%25%20for%2010%20unknown%20objects%2C%20and%20demonstrate%20the%20method%27s%20capability%20to%20grasp%0Aobjects%20in%20structured%20clutter%20and%20from%20shelves.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15001v1&entry.124074799=Read"},
{"title": "EEGEncoder: Advancing BCI with Transformer-Based Motor Imagery\n  Classification", "author": "Wangdan Liao", "abstract": "  Brain-computer interfaces (BCIs) harness electroencephalographic signals for\ndirect neural control of devices, offering a significant benefit for\nindividuals with motor impairments. Traditional machine learning methods for\nEEG-based motor imagery (MI) classification encounter challenges such as manual\nfeature extraction and susceptibility to noise. This paper introduces\nEEGEncoder, a deep learning framework that employs transformer models to\nsurmount these limitations. Our innovative multi-scale fusion architecture\ncaptures both immediate and extended temporal features, thereby enhancing MI\ntask classification precision. EEGEncoder's key innovations include the\ninaugural application of transformers in MI-EEG signal classification, a mixup\ndata augmentation strategy for bolstered generalization, and a multi-task\nlearning approach for refined predictive accuracy. When tested on the BCI\nCompetition IV dataset 2a, our model established a new benchmark with its\nstate-of-the-art performance. EEGEncoder signifies a substantial advancement in\nBCI technology, offering a robust, efficient, and effective tool for\ntransforming thought into action, with the potential to significantly enhance\nthe quality of life for those dependent on BCIs.\n", "link": "http://arxiv.org/abs/2404.14869v1", "date": "2024-04-23", "relevancy": 1.9194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.497}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4804}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4724}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EEGEncoder%3A%20Advancing%20BCI%20with%20Transformer-Based%20Motor%20Imagery%0A%20%20Classification&body=Title%3A%20EEGEncoder%3A%20Advancing%20BCI%20with%20Transformer-Based%20Motor%20Imagery%0A%20%20Classification%0AAuthor%3A%20Wangdan%20Liao%0AAbstract%3A%20%20%20Brain-computer%20interfaces%20%28BCIs%29%20harness%20electroencephalographic%20signals%20for%0Adirect%20neural%20control%20of%20devices%2C%20offering%20a%20significant%20benefit%20for%0Aindividuals%20with%20motor%20impairments.%20Traditional%20machine%20learning%20methods%20for%0AEEG-based%20motor%20imagery%20%28MI%29%20classification%20encounter%20challenges%20such%20as%20manual%0Afeature%20extraction%20and%20susceptibility%20to%20noise.%20This%20paper%20introduces%0AEEGEncoder%2C%20a%20deep%20learning%20framework%20that%20employs%20transformer%20models%20to%0Asurmount%20these%20limitations.%20Our%20innovative%20multi-scale%20fusion%20architecture%0Acaptures%20both%20immediate%20and%20extended%20temporal%20features%2C%20thereby%20enhancing%20MI%0Atask%20classification%20precision.%20EEGEncoder%27s%20key%20innovations%20include%20the%0Ainaugural%20application%20of%20transformers%20in%20MI-EEG%20signal%20classification%2C%20a%20mixup%0Adata%20augmentation%20strategy%20for%20bolstered%20generalization%2C%20and%20a%20multi-task%0Alearning%20approach%20for%20refined%20predictive%20accuracy.%20When%20tested%20on%20the%20BCI%0ACompetition%20IV%20dataset%202a%2C%20our%20model%20established%20a%20new%20benchmark%20with%20its%0Astate-of-the-art%20performance.%20EEGEncoder%20signifies%20a%20substantial%20advancement%20in%0ABCI%20technology%2C%20offering%20a%20robust%2C%20efficient%2C%20and%20effective%20tool%20for%0Atransforming%20thought%20into%20action%2C%20with%20the%20potential%20to%20significantly%20enhance%0Athe%20quality%20of%20life%20for%20those%20dependent%20on%20BCIs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14869v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEGEncoder%3A%20Advancing%20BCI%20with%20Transformer-Based%20Motor%20Imagery%0A%20%20Classification&entry.906535625=Wangdan%20Liao&entry.1292438233=%20%20Brain-computer%20interfaces%20%28BCIs%29%20harness%20electroencephalographic%20signals%20for%0Adirect%20neural%20control%20of%20devices%2C%20offering%20a%20significant%20benefit%20for%0Aindividuals%20with%20motor%20impairments.%20Traditional%20machine%20learning%20methods%20for%0AEEG-based%20motor%20imagery%20%28MI%29%20classification%20encounter%20challenges%20such%20as%20manual%0Afeature%20extraction%20and%20susceptibility%20to%20noise.%20This%20paper%20introduces%0AEEGEncoder%2C%20a%20deep%20learning%20framework%20that%20employs%20transformer%20models%20to%0Asurmount%20these%20limitations.%20Our%20innovative%20multi-scale%20fusion%20architecture%0Acaptures%20both%20immediate%20and%20extended%20temporal%20features%2C%20thereby%20enhancing%20MI%0Atask%20classification%20precision.%20EEGEncoder%27s%20key%20innovations%20include%20the%0Ainaugural%20application%20of%20transformers%20in%20MI-EEG%20signal%20classification%2C%20a%20mixup%0Adata%20augmentation%20strategy%20for%20bolstered%20generalization%2C%20and%20a%20multi-task%0Alearning%20approach%20for%20refined%20predictive%20accuracy.%20When%20tested%20on%20the%20BCI%0ACompetition%20IV%20dataset%202a%2C%20our%20model%20established%20a%20new%20benchmark%20with%20its%0Astate-of-the-art%20performance.%20EEGEncoder%20signifies%20a%20substantial%20advancement%20in%0ABCI%20technology%2C%20offering%20a%20robust%2C%20efficient%2C%20and%20effective%20tool%20for%0Atransforming%20thought%20into%20action%2C%20with%20the%20potential%20to%20significantly%20enhance%0Athe%20quality%20of%20life%20for%20those%20dependent%20on%20BCIs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14869v1&entry.124074799=Read"},
{"title": "Dynamicity-aware Social Bot Detection with Dynamic Graph Transformers", "author": "Buyun He and Yingguang Yang and Qi Wu and Hao Liu and Renyu Yang and Hao Peng and Xiang Wang and Yong Liao and Pengyuan Zhou", "abstract": "  Detecting social bots has evolved into a pivotal yet intricate task, aimed at\ncombating the dissemination of misinformation and preserving the authenticity\nof online interactions. While earlier graph-based approaches, which leverage\ntopological structure of social networks, yielded notable outcomes, they\noverlooked the inherent dynamicity of social networks -- In reality, they\nlargely depicted the social network as a static graph and solely relied on its\nmost recent state. Due to the absence of dynamicity modeling, such approaches\nare vulnerable to evasion, particularly when advanced social bots interact with\nother users to camouflage identities and escape detection. To tackle these\nchallenges, we propose BotDGT, a novel framework that not only considers the\ntopological structure, but also effectively incorporates dynamic nature of\nsocial network. Specifically, we characterize a social network as a dynamic\ngraph. A structural module is employed to acquire topological information from\neach historical snapshot. Additionally, a temporal module is proposed to\nintegrate historical context and model the evolving behavior patterns exhibited\nby social bots and legitimate users. Experimental results demonstrate the\nsuperiority of BotDGT against the leading methods that neglected the dynamic\nnature of social networks in terms of accuracy, recall, and F1-score.\n", "link": "http://arxiv.org/abs/2404.15070v1", "date": "2024-04-23", "relevancy": 1.9157, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5028}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4736}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4571}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamicity-aware%20Social%20Bot%20Detection%20with%20Dynamic%20Graph%20Transformers&body=Title%3A%20Dynamicity-aware%20Social%20Bot%20Detection%20with%20Dynamic%20Graph%20Transformers%0AAuthor%3A%20Buyun%20He%20and%20Yingguang%20Yang%20and%20Qi%20Wu%20and%20Hao%20Liu%20and%20Renyu%20Yang%20and%20Hao%20Peng%20and%20Xiang%20Wang%20and%20Yong%20Liao%20and%20Pengyuan%20Zhou%0AAbstract%3A%20%20%20Detecting%20social%20bots%20has%20evolved%20into%20a%20pivotal%20yet%20intricate%20task%2C%20aimed%20at%0Acombating%20the%20dissemination%20of%20misinformation%20and%20preserving%20the%20authenticity%0Aof%20online%20interactions.%20While%20earlier%20graph-based%20approaches%2C%20which%20leverage%0Atopological%20structure%20of%20social%20networks%2C%20yielded%20notable%20outcomes%2C%20they%0Aoverlooked%20the%20inherent%20dynamicity%20of%20social%20networks%20--%20In%20reality%2C%20they%0Alargely%20depicted%20the%20social%20network%20as%20a%20static%20graph%20and%20solely%20relied%20on%20its%0Amost%20recent%20state.%20Due%20to%20the%20absence%20of%20dynamicity%20modeling%2C%20such%20approaches%0Aare%20vulnerable%20to%20evasion%2C%20particularly%20when%20advanced%20social%20bots%20interact%20with%0Aother%20users%20to%20camouflage%20identities%20and%20escape%20detection.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20BotDGT%2C%20a%20novel%20framework%20that%20not%20only%20considers%20the%0Atopological%20structure%2C%20but%20also%20effectively%20incorporates%20dynamic%20nature%20of%0Asocial%20network.%20Specifically%2C%20we%20characterize%20a%20social%20network%20as%20a%20dynamic%0Agraph.%20A%20structural%20module%20is%20employed%20to%20acquire%20topological%20information%20from%0Aeach%20historical%20snapshot.%20Additionally%2C%20a%20temporal%20module%20is%20proposed%20to%0Aintegrate%20historical%20context%20and%20model%20the%20evolving%20behavior%20patterns%20exhibited%0Aby%20social%20bots%20and%20legitimate%20users.%20Experimental%20results%20demonstrate%20the%0Asuperiority%20of%20BotDGT%20against%20the%20leading%20methods%20that%20neglected%20the%20dynamic%0Anature%20of%20social%20networks%20in%20terms%20of%20accuracy%2C%20recall%2C%20and%20F1-score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15070v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamicity-aware%20Social%20Bot%20Detection%20with%20Dynamic%20Graph%20Transformers&entry.906535625=Buyun%20He%20and%20Yingguang%20Yang%20and%20Qi%20Wu%20and%20Hao%20Liu%20and%20Renyu%20Yang%20and%20Hao%20Peng%20and%20Xiang%20Wang%20and%20Yong%20Liao%20and%20Pengyuan%20Zhou&entry.1292438233=%20%20Detecting%20social%20bots%20has%20evolved%20into%20a%20pivotal%20yet%20intricate%20task%2C%20aimed%20at%0Acombating%20the%20dissemination%20of%20misinformation%20and%20preserving%20the%20authenticity%0Aof%20online%20interactions.%20While%20earlier%20graph-based%20approaches%2C%20which%20leverage%0Atopological%20structure%20of%20social%20networks%2C%20yielded%20notable%20outcomes%2C%20they%0Aoverlooked%20the%20inherent%20dynamicity%20of%20social%20networks%20--%20In%20reality%2C%20they%0Alargely%20depicted%20the%20social%20network%20as%20a%20static%20graph%20and%20solely%20relied%20on%20its%0Amost%20recent%20state.%20Due%20to%20the%20absence%20of%20dynamicity%20modeling%2C%20such%20approaches%0Aare%20vulnerable%20to%20evasion%2C%20particularly%20when%20advanced%20social%20bots%20interact%20with%0Aother%20users%20to%20camouflage%20identities%20and%20escape%20detection.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20BotDGT%2C%20a%20novel%20framework%20that%20not%20only%20considers%20the%0Atopological%20structure%2C%20but%20also%20effectively%20incorporates%20dynamic%20nature%20of%0Asocial%20network.%20Specifically%2C%20we%20characterize%20a%20social%20network%20as%20a%20dynamic%0Agraph.%20A%20structural%20module%20is%20employed%20to%20acquire%20topological%20information%20from%0Aeach%20historical%20snapshot.%20Additionally%2C%20a%20temporal%20module%20is%20proposed%20to%0Aintegrate%20historical%20context%20and%20model%20the%20evolving%20behavior%20patterns%20exhibited%0Aby%20social%20bots%20and%20legitimate%20users.%20Experimental%20results%20demonstrate%20the%0Asuperiority%20of%20BotDGT%20against%20the%20leading%20methods%20that%20neglected%20the%20dynamic%0Anature%20of%20social%20networks%20in%20terms%20of%20accuracy%2C%20recall%2C%20and%20F1-score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15070v1&entry.124074799=Read"},
{"title": "Filtered Direct Preference Optimization", "author": "Tetsuro Morimura and Mitsuki Sakamoto and Yuu Jinnai and Kenshi Abe and Kaito Ariu", "abstract": "  Reinforcement learning from human feedback (RLHF) plays a crucial role in\naligning language models with human preferences. While the significance of\ndataset quality is generally recognized, explicit investigations into its\nimpact within the RLHF framework, to our knowledge, have been limited. This\npaper addresses the issue of text quality within the preference dataset by\nfocusing on Direct Preference Optimization (DPO), an increasingly adopted\nreward-model-free RLHF method. We confirm that text quality significantly\ninfluences the performance of models optimized with DPO more than those\noptimized with reward-model-based RLHF. Building on this new insight, we\npropose an extension of DPO, termed filtered direct preference optimization\n(fDPO). fDPO uses a trained reward model to monitor the quality of texts within\nthe preference dataset during DPO training. Samples of lower quality are\ndiscarded based on comparisons with texts generated by the model being\noptimized, resulting in a more accurate dataset. Experimental results\ndemonstrate that fDPO enhances the final model performance. Our code is\navailable at https://github.com/CyberAgentAILab/filtered-dpo.\n", "link": "http://arxiv.org/abs/2404.13846v2", "date": "2024-04-23", "relevancy": 1.9137, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5011}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4783}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4695}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Filtered%20Direct%20Preference%20Optimization&body=Title%3A%20Filtered%20Direct%20Preference%20Optimization%0AAuthor%3A%20Tetsuro%20Morimura%20and%20Mitsuki%20Sakamoto%20and%20Yuu%20Jinnai%20and%20Kenshi%20Abe%20and%20Kaito%20Ariu%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20plays%20a%20crucial%20role%20in%0Aaligning%20language%20models%20with%20human%20preferences.%20While%20the%20significance%20of%0Adataset%20quality%20is%20generally%20recognized%2C%20explicit%20investigations%20into%20its%0Aimpact%20within%20the%20RLHF%20framework%2C%20to%20our%20knowledge%2C%20have%20been%20limited.%20This%0Apaper%20addresses%20the%20issue%20of%20text%20quality%20within%20the%20preference%20dataset%20by%0Afocusing%20on%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20an%20increasingly%20adopted%0Areward-model-free%20RLHF%20method.%20We%20confirm%20that%20text%20quality%20significantly%0Ainfluences%20the%20performance%20of%20models%20optimized%20with%20DPO%20more%20than%20those%0Aoptimized%20with%20reward-model-based%20RLHF.%20Building%20on%20this%20new%20insight%2C%20we%0Apropose%20an%20extension%20of%20DPO%2C%20termed%20filtered%20direct%20preference%20optimization%0A%28fDPO%29.%20fDPO%20uses%20a%20trained%20reward%20model%20to%20monitor%20the%20quality%20of%20texts%20within%0Athe%20preference%20dataset%20during%20DPO%20training.%20Samples%20of%20lower%20quality%20are%0Adiscarded%20based%20on%20comparisons%20with%20texts%20generated%20by%20the%20model%20being%0Aoptimized%2C%20resulting%20in%20a%20more%20accurate%20dataset.%20Experimental%20results%0Ademonstrate%20that%20fDPO%20enhances%20the%20final%20model%20performance.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/CyberAgentAILab/filtered-dpo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13846v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Filtered%20Direct%20Preference%20Optimization&entry.906535625=Tetsuro%20Morimura%20and%20Mitsuki%20Sakamoto%20and%20Yuu%20Jinnai%20and%20Kenshi%20Abe%20and%20Kaito%20Ariu&entry.1292438233=%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20plays%20a%20crucial%20role%20in%0Aaligning%20language%20models%20with%20human%20preferences.%20While%20the%20significance%20of%0Adataset%20quality%20is%20generally%20recognized%2C%20explicit%20investigations%20into%20its%0Aimpact%20within%20the%20RLHF%20framework%2C%20to%20our%20knowledge%2C%20have%20been%20limited.%20This%0Apaper%20addresses%20the%20issue%20of%20text%20quality%20within%20the%20preference%20dataset%20by%0Afocusing%20on%20Direct%20Preference%20Optimization%20%28DPO%29%2C%20an%20increasingly%20adopted%0Areward-model-free%20RLHF%20method.%20We%20confirm%20that%20text%20quality%20significantly%0Ainfluences%20the%20performance%20of%20models%20optimized%20with%20DPO%20more%20than%20those%0Aoptimized%20with%20reward-model-based%20RLHF.%20Building%20on%20this%20new%20insight%2C%20we%0Apropose%20an%20extension%20of%20DPO%2C%20termed%20filtered%20direct%20preference%20optimization%0A%28fDPO%29.%20fDPO%20uses%20a%20trained%20reward%20model%20to%20monitor%20the%20quality%20of%20texts%20within%0Athe%20preference%20dataset%20during%20DPO%20training.%20Samples%20of%20lower%20quality%20are%0Adiscarded%20based%20on%20comparisons%20with%20texts%20generated%20by%20the%20model%20being%0Aoptimized%2C%20resulting%20in%20a%20more%20accurate%20dataset.%20Experimental%20results%0Ademonstrate%20that%20fDPO%20enhances%20the%20final%20model%20performance.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/CyberAgentAILab/filtered-dpo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13846v2&entry.124074799=Read"},
{"title": "Graph Machine Learning in the Era of Large Language Models (LLMs)", "author": "Wenqi Fan and Shijie Wang and Jiani Huang and Zhikai Chen and Yu Song and Wenzhuo Tang and Haitao Mao and Hui Liu and Xiaorui Liu and Dawei Yin and Qing Li", "abstract": "  Graphs play an important role in representing complex relationships in\nvarious domains like social networks, knowledge graphs, and molecular\ndiscovery. With the advent of deep learning, Graph Neural Networks (GNNs) have\nemerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the\nrepresentation and processing of graph structures. Recently, LLMs have\ndemonstrated unprecedented capabilities in language tasks and are widely\nadopted in a variety of applications such as computer vision and recommender\nsystems. This remarkable success has also attracted interest in applying LLMs\nto the graph domain. Increasing efforts have been made to explore the potential\nof LLMs in advancing Graph ML's generalization, transferability, and few-shot\nlearning ability. Meanwhile, graphs, especially knowledge graphs, are rich in\nreliable factual knowledge, which can be utilized to enhance the reasoning\ncapabilities of LLMs and potentially alleviate their limitations such as\nhallucinations and the lack of explainability. Given the rapid progress of this\nresearch direction, a systematic review summarizing the latest advancements for\nGraph ML in the era of LLMs is necessary to provide an in-depth understanding\nto researchers and practitioners. Therefore, in this survey, we first review\nthe recent developments in Graph ML. We then explore how LLMs can be utilized\nto enhance the quality of graph features, alleviate the reliance on labeled\ndata, and address challenges such as graph heterogeneity and\nout-of-distribution (OOD) generalization. Afterward, we delve into how graphs\ncan enhance LLMs, highlighting their abilities to enhance LLM pre-training and\ninference. Furthermore, we investigate various applications and discuss the\npotential future directions in this promising field.\n", "link": "http://arxiv.org/abs/2404.14928v1", "date": "2024-04-23", "relevancy": 1.9075, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5167}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4498}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4449}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Machine%20Learning%20in%20the%20Era%20of%20Large%20Language%20Models%20%28LLMs%29&body=Title%3A%20Graph%20Machine%20Learning%20in%20the%20Era%20of%20Large%20Language%20Models%20%28LLMs%29%0AAuthor%3A%20Wenqi%20Fan%20and%20Shijie%20Wang%20and%20Jiani%20Huang%20and%20Zhikai%20Chen%20and%20Yu%20Song%20and%20Wenzhuo%20Tang%20and%20Haitao%20Mao%20and%20Hui%20Liu%20and%20Xiaorui%20Liu%20and%20Dawei%20Yin%20and%20Qing%20Li%0AAbstract%3A%20%20%20Graphs%20play%20an%20important%20role%20in%20representing%20complex%20relationships%20in%0Avarious%20domains%20like%20social%20networks%2C%20knowledge%20graphs%2C%20and%20molecular%0Adiscovery.%20With%20the%20advent%20of%20deep%20learning%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%0Aemerged%20as%20a%20cornerstone%20in%20Graph%20Machine%20Learning%20%28Graph%20ML%29%2C%20facilitating%20the%0Arepresentation%20and%20processing%20of%20graph%20structures.%20Recently%2C%20LLMs%20have%0Ademonstrated%20unprecedented%20capabilities%20in%20language%20tasks%20and%20are%20widely%0Aadopted%20in%20a%20variety%20of%20applications%20such%20as%20computer%20vision%20and%20recommender%0Asystems.%20This%20remarkable%20success%20has%20also%20attracted%20interest%20in%20applying%20LLMs%0Ato%20the%20graph%20domain.%20Increasing%20efforts%20have%20been%20made%20to%20explore%20the%20potential%0Aof%20LLMs%20in%20advancing%20Graph%20ML%27s%20generalization%2C%20transferability%2C%20and%20few-shot%0Alearning%20ability.%20Meanwhile%2C%20graphs%2C%20especially%20knowledge%20graphs%2C%20are%20rich%20in%0Areliable%20factual%20knowledge%2C%20which%20can%20be%20utilized%20to%20enhance%20the%20reasoning%0Acapabilities%20of%20LLMs%20and%20potentially%20alleviate%20their%20limitations%20such%20as%0Ahallucinations%20and%20the%20lack%20of%20explainability.%20Given%20the%20rapid%20progress%20of%20this%0Aresearch%20direction%2C%20a%20systematic%20review%20summarizing%20the%20latest%20advancements%20for%0AGraph%20ML%20in%20the%20era%20of%20LLMs%20is%20necessary%20to%20provide%20an%20in-depth%20understanding%0Ato%20researchers%20and%20practitioners.%20Therefore%2C%20in%20this%20survey%2C%20we%20first%20review%0Athe%20recent%20developments%20in%20Graph%20ML.%20We%20then%20explore%20how%20LLMs%20can%20be%20utilized%0Ato%20enhance%20the%20quality%20of%20graph%20features%2C%20alleviate%20the%20reliance%20on%20labeled%0Adata%2C%20and%20address%20challenges%20such%20as%20graph%20heterogeneity%20and%0Aout-of-distribution%20%28OOD%29%20generalization.%20Afterward%2C%20we%20delve%20into%20how%20graphs%0Acan%20enhance%20LLMs%2C%20highlighting%20their%20abilities%20to%20enhance%20LLM%20pre-training%20and%0Ainference.%20Furthermore%2C%20we%20investigate%20various%20applications%20and%20discuss%20the%0Apotential%20future%20directions%20in%20this%20promising%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14928v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Machine%20Learning%20in%20the%20Era%20of%20Large%20Language%20Models%20%28LLMs%29&entry.906535625=Wenqi%20Fan%20and%20Shijie%20Wang%20and%20Jiani%20Huang%20and%20Zhikai%20Chen%20and%20Yu%20Song%20and%20Wenzhuo%20Tang%20and%20Haitao%20Mao%20and%20Hui%20Liu%20and%20Xiaorui%20Liu%20and%20Dawei%20Yin%20and%20Qing%20Li&entry.1292438233=%20%20Graphs%20play%20an%20important%20role%20in%20representing%20complex%20relationships%20in%0Avarious%20domains%20like%20social%20networks%2C%20knowledge%20graphs%2C%20and%20molecular%0Adiscovery.%20With%20the%20advent%20of%20deep%20learning%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%0Aemerged%20as%20a%20cornerstone%20in%20Graph%20Machine%20Learning%20%28Graph%20ML%29%2C%20facilitating%20the%0Arepresentation%20and%20processing%20of%20graph%20structures.%20Recently%2C%20LLMs%20have%0Ademonstrated%20unprecedented%20capabilities%20in%20language%20tasks%20and%20are%20widely%0Aadopted%20in%20a%20variety%20of%20applications%20such%20as%20computer%20vision%20and%20recommender%0Asystems.%20This%20remarkable%20success%20has%20also%20attracted%20interest%20in%20applying%20LLMs%0Ato%20the%20graph%20domain.%20Increasing%20efforts%20have%20been%20made%20to%20explore%20the%20potential%0Aof%20LLMs%20in%20advancing%20Graph%20ML%27s%20generalization%2C%20transferability%2C%20and%20few-shot%0Alearning%20ability.%20Meanwhile%2C%20graphs%2C%20especially%20knowledge%20graphs%2C%20are%20rich%20in%0Areliable%20factual%20knowledge%2C%20which%20can%20be%20utilized%20to%20enhance%20the%20reasoning%0Acapabilities%20of%20LLMs%20and%20potentially%20alleviate%20their%20limitations%20such%20as%0Ahallucinations%20and%20the%20lack%20of%20explainability.%20Given%20the%20rapid%20progress%20of%20this%0Aresearch%20direction%2C%20a%20systematic%20review%20summarizing%20the%20latest%20advancements%20for%0AGraph%20ML%20in%20the%20era%20of%20LLMs%20is%20necessary%20to%20provide%20an%20in-depth%20understanding%0Ato%20researchers%20and%20practitioners.%20Therefore%2C%20in%20this%20survey%2C%20we%20first%20review%0Athe%20recent%20developments%20in%20Graph%20ML.%20We%20then%20explore%20how%20LLMs%20can%20be%20utilized%0Ato%20enhance%20the%20quality%20of%20graph%20features%2C%20alleviate%20the%20reliance%20on%20labeled%0Adata%2C%20and%20address%20challenges%20such%20as%20graph%20heterogeneity%20and%0Aout-of-distribution%20%28OOD%29%20generalization.%20Afterward%2C%20we%20delve%20into%20how%20graphs%0Acan%20enhance%20LLMs%2C%20highlighting%20their%20abilities%20to%20enhance%20LLM%20pre-training%20and%0Ainference.%20Furthermore%2C%20we%20investigate%20various%20applications%20and%20discuss%20the%0Apotential%20future%20directions%20in%20this%20promising%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14928v1&entry.124074799=Read"},
{"title": "From Reactive to Proactive Volatility Modeling with Hemisphere Neural\n  Networks", "author": "Philippe Goulet Coulombe and Mikael Frenette and Karin Klieber", "abstract": "  We reinvigorate maximum likelihood estimation (MLE) for macroeconomic density\nforecasting through a novel neural network architecture with dedicated mean and\nvariance hemispheres. Our architecture features several key ingredients making\nMLE work in this context. First, the hemispheres share a common core at the\nentrance of the network which accommodates for various forms of time variation\nin the error variance. Second, we introduce a volatility emphasis constraint\nthat breaks mean/variance indeterminacy in this class of overparametrized\nnonlinear models. Third, we conduct a blocked out-of-bag reality check to curb\noverfitting in both conditional moments. Fourth, the algorithm utilizes\nstandard deep learning software and thus handles large data sets - both\ncomputationally and statistically. Ergo, our Hemisphere Neural Network (HNN)\nprovides proactive volatility forecasts based on leading indicators when it\ncan, and reactive volatility based on the magnitude of previous prediction\nerrors when it must. We evaluate point and density forecasts with an extensive\nout-of-sample experiment and benchmark against a suite of models ranging from\nclassics to more modern machine learning-based offerings. In all cases, HNN\nfares well by consistently providing accurate mean/variance forecasts for all\ntargets and horizons. Studying the resulting volatility paths reveals its\nversatility, while probabilistic forecasting evaluation metrics showcase its\nenviable reliability. Finally, we also demonstrate how this machinery can be\nmerged with other structured deep learning models by revisiting Goulet Coulombe\n(2022)'s Neural Phillips Curve.\n", "link": "http://arxiv.org/abs/2311.16333v2", "date": "2024-04-23", "relevancy": 1.8983, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4869}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.453}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20Reactive%20to%20Proactive%20Volatility%20Modeling%20with%20Hemisphere%20Neural%0A%20%20Networks&body=Title%3A%20From%20Reactive%20to%20Proactive%20Volatility%20Modeling%20with%20Hemisphere%20Neural%0A%20%20Networks%0AAuthor%3A%20Philippe%20Goulet%20Coulombe%20and%20Mikael%20Frenette%20and%20Karin%20Klieber%0AAbstract%3A%20%20%20We%20reinvigorate%20maximum%20likelihood%20estimation%20%28MLE%29%20for%20macroeconomic%20density%0Aforecasting%20through%20a%20novel%20neural%20network%20architecture%20with%20dedicated%20mean%20and%0Avariance%20hemispheres.%20Our%20architecture%20features%20several%20key%20ingredients%20making%0AMLE%20work%20in%20this%20context.%20First%2C%20the%20hemispheres%20share%20a%20common%20core%20at%20the%0Aentrance%20of%20the%20network%20which%20accommodates%20for%20various%20forms%20of%20time%20variation%0Ain%20the%20error%20variance.%20Second%2C%20we%20introduce%20a%20volatility%20emphasis%20constraint%0Athat%20breaks%20mean/variance%20indeterminacy%20in%20this%20class%20of%20overparametrized%0Anonlinear%20models.%20Third%2C%20we%20conduct%20a%20blocked%20out-of-bag%20reality%20check%20to%20curb%0Aoverfitting%20in%20both%20conditional%20moments.%20Fourth%2C%20the%20algorithm%20utilizes%0Astandard%20deep%20learning%20software%20and%20thus%20handles%20large%20data%20sets%20-%20both%0Acomputationally%20and%20statistically.%20Ergo%2C%20our%20Hemisphere%20Neural%20Network%20%28HNN%29%0Aprovides%20proactive%20volatility%20forecasts%20based%20on%20leading%20indicators%20when%20it%0Acan%2C%20and%20reactive%20volatility%20based%20on%20the%20magnitude%20of%20previous%20prediction%0Aerrors%20when%20it%20must.%20We%20evaluate%20point%20and%20density%20forecasts%20with%20an%20extensive%0Aout-of-sample%20experiment%20and%20benchmark%20against%20a%20suite%20of%20models%20ranging%20from%0Aclassics%20to%20more%20modern%20machine%20learning-based%20offerings.%20In%20all%20cases%2C%20HNN%0Afares%20well%20by%20consistently%20providing%20accurate%20mean/variance%20forecasts%20for%20all%0Atargets%20and%20horizons.%20Studying%20the%20resulting%20volatility%20paths%20reveals%20its%0Aversatility%2C%20while%20probabilistic%20forecasting%20evaluation%20metrics%20showcase%20its%0Aenviable%20reliability.%20Finally%2C%20we%20also%20demonstrate%20how%20this%20machinery%20can%20be%0Amerged%20with%20other%20structured%20deep%20learning%20models%20by%20revisiting%20Goulet%20Coulombe%0A%282022%29%27s%20Neural%20Phillips%20Curve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16333v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Reactive%20to%20Proactive%20Volatility%20Modeling%20with%20Hemisphere%20Neural%0A%20%20Networks&entry.906535625=Philippe%20Goulet%20Coulombe%20and%20Mikael%20Frenette%20and%20Karin%20Klieber&entry.1292438233=%20%20We%20reinvigorate%20maximum%20likelihood%20estimation%20%28MLE%29%20for%20macroeconomic%20density%0Aforecasting%20through%20a%20novel%20neural%20network%20architecture%20with%20dedicated%20mean%20and%0Avariance%20hemispheres.%20Our%20architecture%20features%20several%20key%20ingredients%20making%0AMLE%20work%20in%20this%20context.%20First%2C%20the%20hemispheres%20share%20a%20common%20core%20at%20the%0Aentrance%20of%20the%20network%20which%20accommodates%20for%20various%20forms%20of%20time%20variation%0Ain%20the%20error%20variance.%20Second%2C%20we%20introduce%20a%20volatility%20emphasis%20constraint%0Athat%20breaks%20mean/variance%20indeterminacy%20in%20this%20class%20of%20overparametrized%0Anonlinear%20models.%20Third%2C%20we%20conduct%20a%20blocked%20out-of-bag%20reality%20check%20to%20curb%0Aoverfitting%20in%20both%20conditional%20moments.%20Fourth%2C%20the%20algorithm%20utilizes%0Astandard%20deep%20learning%20software%20and%20thus%20handles%20large%20data%20sets%20-%20both%0Acomputationally%20and%20statistically.%20Ergo%2C%20our%20Hemisphere%20Neural%20Network%20%28HNN%29%0Aprovides%20proactive%20volatility%20forecasts%20based%20on%20leading%20indicators%20when%20it%0Acan%2C%20and%20reactive%20volatility%20based%20on%20the%20magnitude%20of%20previous%20prediction%0Aerrors%20when%20it%20must.%20We%20evaluate%20point%20and%20density%20forecasts%20with%20an%20extensive%0Aout-of-sample%20experiment%20and%20benchmark%20against%20a%20suite%20of%20models%20ranging%20from%0Aclassics%20to%20more%20modern%20machine%20learning-based%20offerings.%20In%20all%20cases%2C%20HNN%0Afares%20well%20by%20consistently%20providing%20accurate%20mean/variance%20forecasts%20for%20all%0Atargets%20and%20horizons.%20Studying%20the%20resulting%20volatility%20paths%20reveals%20its%0Aversatility%2C%20while%20probabilistic%20forecasting%20evaluation%20metrics%20showcase%20its%0Aenviable%20reliability.%20Finally%2C%20we%20also%20demonstrate%20how%20this%20machinery%20can%20be%0Amerged%20with%20other%20structured%20deep%20learning%20models%20by%20revisiting%20Goulet%20Coulombe%0A%282022%29%27s%20Neural%20Phillips%20Curve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16333v2&entry.124074799=Read"},
{"title": "Tenplex: Dynamic Parallelism for Deep Learning using Parallelizable\n  Tensor Collections", "author": "Marcel Wagenl\u00e4nder and Guo Li and Bo Zhao and Luo Mai and Peter Pietzuch", "abstract": "  Deep learning (DL) jobs use multi-dimensional parallelism, i.e. combining\ndata, model, and pipeline parallelism, to use large GPU clusters efficiently.\nLong-running jobs may experience changes to their GPU allocation: (i) resource\nelasticity during training adds or removes GPUs; (ii) hardware maintenance may\nrequire redeployment on different GPUs; and (iii) GPU failures force jobs to\nrun with fewer devices. Current DL frameworks tie jobs to a set of GPUs and\nthus lack support for these scenarios. In particular, they cannot change the\nmulti-dimensional parallelism of an already-running job in an efficient and\nmodel-independent way.\n  We describe Scalai, a state management library for DL systems that enables\njobs to change their parallelism dynamically after the GPU allocation is\nupdated at runtime. Scalai achieves this through a new abstraction, a\nparallelizable tensor collection (PTC), that externalizes the job state during\ntraining. After a GPU change, Scalai uses the PTC to transform the job state:\nthe PTC repartitions the dataset state under data parallelism and exposes it to\nDL workers through a virtual file system; and the PTC obtains the model state\nas partitioned checkpoints and transforms them to reflect the new\nparallelization configuration. For efficiency, Scalai executes PTC\ntransformations in parallel with minimum data movement between workers. Our\nexperiments show that Scalai enables DL jobs to support dynamic parallelization\nwith low overhead.\n", "link": "http://arxiv.org/abs/2312.05181v2", "date": "2024-04-23", "relevancy": 1.8833, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4778}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4735}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4653}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Tenplex%3A%20Dynamic%20Parallelism%20for%20Deep%20Learning%20using%20Parallelizable%0A%20%20Tensor%20Collections&body=Title%3A%20Tenplex%3A%20Dynamic%20Parallelism%20for%20Deep%20Learning%20using%20Parallelizable%0A%20%20Tensor%20Collections%0AAuthor%3A%20Marcel%20Wagenl%C3%A4nder%20and%20Guo%20Li%20and%20Bo%20Zhao%20and%20Luo%20Mai%20and%20Peter%20Pietzuch%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20jobs%20use%20multi-dimensional%20parallelism%2C%20i.e.%20combining%0Adata%2C%20model%2C%20and%20pipeline%20parallelism%2C%20to%20use%20large%20GPU%20clusters%20efficiently.%0ALong-running%20jobs%20may%20experience%20changes%20to%20their%20GPU%20allocation%3A%20%28i%29%20resource%0Aelasticity%20during%20training%20adds%20or%20removes%20GPUs%3B%20%28ii%29%20hardware%20maintenance%20may%0Arequire%20redeployment%20on%20different%20GPUs%3B%20and%20%28iii%29%20GPU%20failures%20force%20jobs%20to%0Arun%20with%20fewer%20devices.%20Current%20DL%20frameworks%20tie%20jobs%20to%20a%20set%20of%20GPUs%20and%0Athus%20lack%20support%20for%20these%20scenarios.%20In%20particular%2C%20they%20cannot%20change%20the%0Amulti-dimensional%20parallelism%20of%20an%20already-running%20job%20in%20an%20efficient%20and%0Amodel-independent%20way.%0A%20%20We%20describe%20Scalai%2C%20a%20state%20management%20library%20for%20DL%20systems%20that%20enables%0Ajobs%20to%20change%20their%20parallelism%20dynamically%20after%20the%20GPU%20allocation%20is%0Aupdated%20at%20runtime.%20Scalai%20achieves%20this%20through%20a%20new%20abstraction%2C%20a%0Aparallelizable%20tensor%20collection%20%28PTC%29%2C%20that%20externalizes%20the%20job%20state%20during%0Atraining.%20After%20a%20GPU%20change%2C%20Scalai%20uses%20the%20PTC%20to%20transform%20the%20job%20state%3A%0Athe%20PTC%20repartitions%20the%20dataset%20state%20under%20data%20parallelism%20and%20exposes%20it%20to%0ADL%20workers%20through%20a%20virtual%20file%20system%3B%20and%20the%20PTC%20obtains%20the%20model%20state%0Aas%20partitioned%20checkpoints%20and%20transforms%20them%20to%20reflect%20the%20new%0Aparallelization%20configuration.%20For%20efficiency%2C%20Scalai%20executes%20PTC%0Atransformations%20in%20parallel%20with%20minimum%20data%20movement%20between%20workers.%20Our%0Aexperiments%20show%20that%20Scalai%20enables%20DL%20jobs%20to%20support%20dynamic%20parallelization%0Awith%20low%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05181v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tenplex%3A%20Dynamic%20Parallelism%20for%20Deep%20Learning%20using%20Parallelizable%0A%20%20Tensor%20Collections&entry.906535625=Marcel%20Wagenl%C3%A4nder%20and%20Guo%20Li%20and%20Bo%20Zhao%20and%20Luo%20Mai%20and%20Peter%20Pietzuch&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20jobs%20use%20multi-dimensional%20parallelism%2C%20i.e.%20combining%0Adata%2C%20model%2C%20and%20pipeline%20parallelism%2C%20to%20use%20large%20GPU%20clusters%20efficiently.%0ALong-running%20jobs%20may%20experience%20changes%20to%20their%20GPU%20allocation%3A%20%28i%29%20resource%0Aelasticity%20during%20training%20adds%20or%20removes%20GPUs%3B%20%28ii%29%20hardware%20maintenance%20may%0Arequire%20redeployment%20on%20different%20GPUs%3B%20and%20%28iii%29%20GPU%20failures%20force%20jobs%20to%0Arun%20with%20fewer%20devices.%20Current%20DL%20frameworks%20tie%20jobs%20to%20a%20set%20of%20GPUs%20and%0Athus%20lack%20support%20for%20these%20scenarios.%20In%20particular%2C%20they%20cannot%20change%20the%0Amulti-dimensional%20parallelism%20of%20an%20already-running%20job%20in%20an%20efficient%20and%0Amodel-independent%20way.%0A%20%20We%20describe%20Scalai%2C%20a%20state%20management%20library%20for%20DL%20systems%20that%20enables%0Ajobs%20to%20change%20their%20parallelism%20dynamically%20after%20the%20GPU%20allocation%20is%0Aupdated%20at%20runtime.%20Scalai%20achieves%20this%20through%20a%20new%20abstraction%2C%20a%0Aparallelizable%20tensor%20collection%20%28PTC%29%2C%20that%20externalizes%20the%20job%20state%20during%0Atraining.%20After%20a%20GPU%20change%2C%20Scalai%20uses%20the%20PTC%20to%20transform%20the%20job%20state%3A%0Athe%20PTC%20repartitions%20the%20dataset%20state%20under%20data%20parallelism%20and%20exposes%20it%20to%0ADL%20workers%20through%20a%20virtual%20file%20system%3B%20and%20the%20PTC%20obtains%20the%20model%20state%0Aas%20partitioned%20checkpoints%20and%20transforms%20them%20to%20reflect%20the%20new%0Aparallelization%20configuration.%20For%20efficiency%2C%20Scalai%20executes%20PTC%0Atransformations%20in%20parallel%20with%20minimum%20data%20movement%20between%20workers.%20Our%0Aexperiments%20show%20that%20Scalai%20enables%20DL%20jobs%20to%20support%20dynamic%20parallelization%0Awith%20low%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05181v2&entry.124074799=Read"},
{"title": "A Hybrid Kernel-Free Boundary Integral Method with Operator Learning for\n  Solving Parametric Partial Differential Equations In Complex Domains", "author": "Shuo Ling and Liwei Tan and Wenjun Ying", "abstract": "  The Kernel-Free Boundary Integral (KFBI) method presents an iterative\nsolution to boundary integral equations arising from elliptic partial\ndifferential equations (PDEs). This method effectively addresses elliptic PDEs\non irregular domains, including the modified Helmholtz, Stokes, and elasticity\nequations. The rapid evolution of neural networks and deep learning has\ninvigorated the exploration of numerical PDEs. An increasing interest is\nobserved in deep learning approaches that seamlessly integrate mathematical\nprinciples for investigating numerical PDEs. We propose a hybrid KFBI method,\nintegrating the foundational principles of the KFBI method with the\ncapabilities of deep learning. This approach, within the framework of the\nboundary integral method, designs a network to approximate the solution\noperator for the corresponding integral equations by mapping the parameters,\ninhomogeneous terms and boundary information of PDEs to the boundary density\nfunctions, which can be regarded as the solution of the integral equations. The\nmodels are trained using data generated by the Cartesian grid-based KFBI\nalgorithm, exhibiting robust generalization capabilities. It accurately\npredicts density functions across diverse boundary conditions and parameters\nwithin the same class of equations. Experimental results demonstrate that the\ntrained model can directly infer the boundary density function with\nsatisfactory precision, obviating the need for iterative steps in solving\nboundary integral equations. Furthermore, applying the inference results of the\nmodel as initial values for iterations is also reasonable; this approach can\nretain the inherent second-order accuracy of the KFBI method while accelerating\nthe traditional KFBI approach by reducing about 50% iterations.\n", "link": "http://arxiv.org/abs/2404.15242v1", "date": "2024-04-23", "relevancy": 1.8812, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.483}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.483}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4526}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Kernel-Free%20Boundary%20Integral%20Method%20with%20Operator%20Learning%20for%0A%20%20Solving%20Parametric%20Partial%20Differential%20Equations%20In%20Complex%20Domains&body=Title%3A%20A%20Hybrid%20Kernel-Free%20Boundary%20Integral%20Method%20with%20Operator%20Learning%20for%0A%20%20Solving%20Parametric%20Partial%20Differential%20Equations%20In%20Complex%20Domains%0AAuthor%3A%20Shuo%20Ling%20and%20Liwei%20Tan%20and%20Wenjun%20Ying%0AAbstract%3A%20%20%20The%20Kernel-Free%20Boundary%20Integral%20%28KFBI%29%20method%20presents%20an%20iterative%0Asolution%20to%20boundary%20integral%20equations%20arising%20from%20elliptic%20partial%0Adifferential%20equations%20%28PDEs%29.%20This%20method%20effectively%20addresses%20elliptic%20PDEs%0Aon%20irregular%20domains%2C%20including%20the%20modified%20Helmholtz%2C%20Stokes%2C%20and%20elasticity%0Aequations.%20The%20rapid%20evolution%20of%20neural%20networks%20and%20deep%20learning%20has%0Ainvigorated%20the%20exploration%20of%20numerical%20PDEs.%20An%20increasing%20interest%20is%0Aobserved%20in%20deep%20learning%20approaches%20that%20seamlessly%20integrate%20mathematical%0Aprinciples%20for%20investigating%20numerical%20PDEs.%20We%20propose%20a%20hybrid%20KFBI%20method%2C%0Aintegrating%20the%20foundational%20principles%20of%20the%20KFBI%20method%20with%20the%0Acapabilities%20of%20deep%20learning.%20This%20approach%2C%20within%20the%20framework%20of%20the%0Aboundary%20integral%20method%2C%20designs%20a%20network%20to%20approximate%20the%20solution%0Aoperator%20for%20the%20corresponding%20integral%20equations%20by%20mapping%20the%20parameters%2C%0Ainhomogeneous%20terms%20and%20boundary%20information%20of%20PDEs%20to%20the%20boundary%20density%0Afunctions%2C%20which%20can%20be%20regarded%20as%20the%20solution%20of%20the%20integral%20equations.%20The%0Amodels%20are%20trained%20using%20data%20generated%20by%20the%20Cartesian%20grid-based%20KFBI%0Aalgorithm%2C%20exhibiting%20robust%20generalization%20capabilities.%20It%20accurately%0Apredicts%20density%20functions%20across%20diverse%20boundary%20conditions%20and%20parameters%0Awithin%20the%20same%20class%20of%20equations.%20Experimental%20results%20demonstrate%20that%20the%0Atrained%20model%20can%20directly%20infer%20the%20boundary%20density%20function%20with%0Asatisfactory%20precision%2C%20obviating%20the%20need%20for%20iterative%20steps%20in%20solving%0Aboundary%20integral%20equations.%20Furthermore%2C%20applying%20the%20inference%20results%20of%20the%0Amodel%20as%20initial%20values%20for%20iterations%20is%20also%20reasonable%3B%20this%20approach%20can%0Aretain%20the%20inherent%20second-order%20accuracy%20of%20the%20KFBI%20method%20while%20accelerating%0Athe%20traditional%20KFBI%20approach%20by%20reducing%20about%2050%25%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15242v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Kernel-Free%20Boundary%20Integral%20Method%20with%20Operator%20Learning%20for%0A%20%20Solving%20Parametric%20Partial%20Differential%20Equations%20In%20Complex%20Domains&entry.906535625=Shuo%20Ling%20and%20Liwei%20Tan%20and%20Wenjun%20Ying&entry.1292438233=%20%20The%20Kernel-Free%20Boundary%20Integral%20%28KFBI%29%20method%20presents%20an%20iterative%0Asolution%20to%20boundary%20integral%20equations%20arising%20from%20elliptic%20partial%0Adifferential%20equations%20%28PDEs%29.%20This%20method%20effectively%20addresses%20elliptic%20PDEs%0Aon%20irregular%20domains%2C%20including%20the%20modified%20Helmholtz%2C%20Stokes%2C%20and%20elasticity%0Aequations.%20The%20rapid%20evolution%20of%20neural%20networks%20and%20deep%20learning%20has%0Ainvigorated%20the%20exploration%20of%20numerical%20PDEs.%20An%20increasing%20interest%20is%0Aobserved%20in%20deep%20learning%20approaches%20that%20seamlessly%20integrate%20mathematical%0Aprinciples%20for%20investigating%20numerical%20PDEs.%20We%20propose%20a%20hybrid%20KFBI%20method%2C%0Aintegrating%20the%20foundational%20principles%20of%20the%20KFBI%20method%20with%20the%0Acapabilities%20of%20deep%20learning.%20This%20approach%2C%20within%20the%20framework%20of%20the%0Aboundary%20integral%20method%2C%20designs%20a%20network%20to%20approximate%20the%20solution%0Aoperator%20for%20the%20corresponding%20integral%20equations%20by%20mapping%20the%20parameters%2C%0Ainhomogeneous%20terms%20and%20boundary%20information%20of%20PDEs%20to%20the%20boundary%20density%0Afunctions%2C%20which%20can%20be%20regarded%20as%20the%20solution%20of%20the%20integral%20equations.%20The%0Amodels%20are%20trained%20using%20data%20generated%20by%20the%20Cartesian%20grid-based%20KFBI%0Aalgorithm%2C%20exhibiting%20robust%20generalization%20capabilities.%20It%20accurately%0Apredicts%20density%20functions%20across%20diverse%20boundary%20conditions%20and%20parameters%0Awithin%20the%20same%20class%20of%20equations.%20Experimental%20results%20demonstrate%20that%20the%0Atrained%20model%20can%20directly%20infer%20the%20boundary%20density%20function%20with%0Asatisfactory%20precision%2C%20obviating%20the%20need%20for%20iterative%20steps%20in%20solving%0Aboundary%20integral%20equations.%20Furthermore%2C%20applying%20the%20inference%20results%20of%20the%0Amodel%20as%20initial%20values%20for%20iterations%20is%20also%20reasonable%3B%20this%20approach%20can%0Aretain%20the%20inherent%20second-order%20accuracy%20of%20the%20KFBI%20method%20while%20accelerating%0Athe%20traditional%20KFBI%20approach%20by%20reducing%20about%2050%25%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15242v1&entry.124074799=Read"},
{"title": "DreamMatcher: Appearance Matching Self-Attention for\n  Semantically-Consistent Text-to-Image Personalization", "author": "Jisu Nam and Heesu Kim and DongJae Lee and Siyoon Jin and Seungryong Kim and Seunggyu Chang", "abstract": "  The objective of text-to-image (T2I) personalization is to customize a\ndiffusion model to a user-provided reference concept, generating diverse images\nof the concept aligned with the target prompts. Conventional methods\nrepresenting the reference concepts using unique text embeddings often fail to\naccurately mimic the appearance of the reference. To address this, one solution\nmay be explicitly conditioning the reference images into the target denoising\nprocess, known as key-value replacement. However, prior works are constrained\nto local editing since they disrupt the structure path of the pre-trained T2I\nmodel. To overcome this, we propose a novel plug-in method, called\nDreamMatcher, which reformulates T2I personalization as semantic matching.\nSpecifically, DreamMatcher replaces the target values with reference values\naligned by semantic matching, while leaving the structure path unchanged to\npreserve the versatile capability of pre-trained T2I models for generating\ndiverse structures. We also introduce a semantic-consistent masking strategy to\nisolate the personalized concept from irrelevant regions introduced by the\ntarget prompts. Compatible with existing T2I models, DreamMatcher shows\nsignificant improvements in complex scenarios. Intensive analyses demonstrate\nthe effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2402.09812v2", "date": "2024-04-23", "relevancy": 1.876, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6623}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6189}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6131}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DreamMatcher%3A%20Appearance%20Matching%20Self-Attention%20for%0A%20%20Semantically-Consistent%20Text-to-Image%20Personalization&body=Title%3A%20DreamMatcher%3A%20Appearance%20Matching%20Self-Attention%20for%0A%20%20Semantically-Consistent%20Text-to-Image%20Personalization%0AAuthor%3A%20Jisu%20Nam%20and%20Heesu%20Kim%20and%20DongJae%20Lee%20and%20Siyoon%20Jin%20and%20Seungryong%20Kim%20and%20Seunggyu%20Chang%0AAbstract%3A%20%20%20The%20objective%20of%20text-to-image%20%28T2I%29%20personalization%20is%20to%20customize%20a%0Adiffusion%20model%20to%20a%20user-provided%20reference%20concept%2C%20generating%20diverse%20images%0Aof%20the%20concept%20aligned%20with%20the%20target%20prompts.%20Conventional%20methods%0Arepresenting%20the%20reference%20concepts%20using%20unique%20text%20embeddings%20often%20fail%20to%0Aaccurately%20mimic%20the%20appearance%20of%20the%20reference.%20To%20address%20this%2C%20one%20solution%0Amay%20be%20explicitly%20conditioning%20the%20reference%20images%20into%20the%20target%20denoising%0Aprocess%2C%20known%20as%20key-value%20replacement.%20However%2C%20prior%20works%20are%20constrained%0Ato%20local%20editing%20since%20they%20disrupt%20the%20structure%20path%20of%20the%20pre-trained%20T2I%0Amodel.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%20plug-in%20method%2C%20called%0ADreamMatcher%2C%20which%20reformulates%20T2I%20personalization%20as%20semantic%20matching.%0ASpecifically%2C%20DreamMatcher%20replaces%20the%20target%20values%20with%20reference%20values%0Aaligned%20by%20semantic%20matching%2C%20while%20leaving%20the%20structure%20path%20unchanged%20to%0Apreserve%20the%20versatile%20capability%20of%20pre-trained%20T2I%20models%20for%20generating%0Adiverse%20structures.%20We%20also%20introduce%20a%20semantic-consistent%20masking%20strategy%20to%0Aisolate%20the%20personalized%20concept%20from%20irrelevant%20regions%20introduced%20by%20the%0Atarget%20prompts.%20Compatible%20with%20existing%20T2I%20models%2C%20DreamMatcher%20shows%0Asignificant%20improvements%20in%20complex%20scenarios.%20Intensive%20analyses%20demonstrate%0Athe%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09812v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamMatcher%3A%20Appearance%20Matching%20Self-Attention%20for%0A%20%20Semantically-Consistent%20Text-to-Image%20Personalization&entry.906535625=Jisu%20Nam%20and%20Heesu%20Kim%20and%20DongJae%20Lee%20and%20Siyoon%20Jin%20and%20Seungryong%20Kim%20and%20Seunggyu%20Chang&entry.1292438233=%20%20The%20objective%20of%20text-to-image%20%28T2I%29%20personalization%20is%20to%20customize%20a%0Adiffusion%20model%20to%20a%20user-provided%20reference%20concept%2C%20generating%20diverse%20images%0Aof%20the%20concept%20aligned%20with%20the%20target%20prompts.%20Conventional%20methods%0Arepresenting%20the%20reference%20concepts%20using%20unique%20text%20embeddings%20often%20fail%20to%0Aaccurately%20mimic%20the%20appearance%20of%20the%20reference.%20To%20address%20this%2C%20one%20solution%0Amay%20be%20explicitly%20conditioning%20the%20reference%20images%20into%20the%20target%20denoising%0Aprocess%2C%20known%20as%20key-value%20replacement.%20However%2C%20prior%20works%20are%20constrained%0Ato%20local%20editing%20since%20they%20disrupt%20the%20structure%20path%20of%20the%20pre-trained%20T2I%0Amodel.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%20plug-in%20method%2C%20called%0ADreamMatcher%2C%20which%20reformulates%20T2I%20personalization%20as%20semantic%20matching.%0ASpecifically%2C%20DreamMatcher%20replaces%20the%20target%20values%20with%20reference%20values%0Aaligned%20by%20semantic%20matching%2C%20while%20leaving%20the%20structure%20path%20unchanged%20to%0Apreserve%20the%20versatile%20capability%20of%20pre-trained%20T2I%20models%20for%20generating%0Adiverse%20structures.%20We%20also%20introduce%20a%20semantic-consistent%20masking%20strategy%20to%0Aisolate%20the%20personalized%20concept%20from%20irrelevant%20regions%20introduced%20by%20the%0Atarget%20prompts.%20Compatible%20with%20existing%20T2I%20models%2C%20DreamMatcher%20shows%0Asignificant%20improvements%20in%20complex%20scenarios.%20Intensive%20analyses%20demonstrate%0Athe%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09812v2&entry.124074799=Read"},
{"title": "XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\n  Upcycled Mixture-of-Experts", "author": "Yifeng Ding and Jiawei Liu and Yuxiang Wei and Terry Yue Zhuo and Lingming Zhang", "abstract": "  We introduce XFT, a simple yet powerful training scheme, by simply merging\nupcycled Mixture-of-Experts (MoE) to unleash the performance limit of\ninstruction-tuned code Large Language Models (LLMs). While vanilla sparse\nupcycling fails to improve instruction tuning, XFT introduces a shared expert\nmechanism with a novel routing weight normalization strategy into sparse\nupcycling, which significantly boosts instruction tuning. After fine-tuning the\nupcycled MoE model, XFT introduces a learnable model merging mechanism to\ncompile the upcycled MoE model back to a dense model, achieving upcycled\nMoE-level performance with only dense-model compute. By applying XFT to a 1.3B\nmodel, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6\npass@1 on HumanEval and HumanEval+ respectively. With the same data and model\narchitecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+,\nalong with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and\nDS-1000, demonstrating its generalizability. XFT is fully orthogonal to\nexisting techniques such as Evol-Instruct and OSS-Instruct, opening a new\ndimension for improving code instruction tuning. Codes are available at\nhttps://github.com/ise-uiuc/xft .\n", "link": "http://arxiv.org/abs/2404.15247v1", "date": "2024-04-23", "relevancy": 1.8744, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4741}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4664}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4603}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20XFT%3A%20Unlocking%20the%20Power%20of%20Code%20Instruction%20Tuning%20by%20Simply%20Merging%0A%20%20Upcycled%20Mixture-of-Experts&body=Title%3A%20XFT%3A%20Unlocking%20the%20Power%20of%20Code%20Instruction%20Tuning%20by%20Simply%20Merging%0A%20%20Upcycled%20Mixture-of-Experts%0AAuthor%3A%20Yifeng%20Ding%20and%20Jiawei%20Liu%20and%20Yuxiang%20Wei%20and%20Terry%20Yue%20Zhuo%20and%20Lingming%20Zhang%0AAbstract%3A%20%20%20We%20introduce%20XFT%2C%20a%20simple%20yet%20powerful%20training%20scheme%2C%20by%20simply%20merging%0Aupcycled%20Mixture-of-Experts%20%28MoE%29%20to%20unleash%20the%20performance%20limit%20of%0Ainstruction-tuned%20code%20Large%20Language%20Models%20%28LLMs%29.%20While%20vanilla%20sparse%0Aupcycling%20fails%20to%20improve%20instruction%20tuning%2C%20XFT%20introduces%20a%20shared%20expert%0Amechanism%20with%20a%20novel%20routing%20weight%20normalization%20strategy%20into%20sparse%0Aupcycling%2C%20which%20significantly%20boosts%20instruction%20tuning.%20After%20fine-tuning%20the%0Aupcycled%20MoE%20model%2C%20XFT%20introduces%20a%20learnable%20model%20merging%20mechanism%20to%0Acompile%20the%20upcycled%20MoE%20model%20back%20to%20a%20dense%20model%2C%20achieving%20upcycled%0AMoE-level%20performance%20with%20only%20dense-model%20compute.%20By%20applying%20XFT%20to%20a%201.3B%0Amodel%2C%20we%20create%20a%20new%20state-of-the-art%20tiny%20code%20LLM%20%28%3C3B%29%20with%2067.1%20and%2064.6%0Apass%401%20on%20HumanEval%20and%20HumanEval%2B%20respectively.%20With%20the%20same%20data%20and%20model%0Aarchitecture%2C%20XFT%20improves%20supervised%20fine-tuning%20%28SFT%29%20by%2013%25%20on%20HumanEval%2B%2C%0Aalong%20with%20consistent%20improvements%20from%202%25%20to%2013%25%20on%20MBPP%2B%2C%20MultiPL-E%2C%20and%0ADS-1000%2C%20demonstrating%20its%20generalizability.%20XFT%20is%20fully%20orthogonal%20to%0Aexisting%20techniques%20such%20as%20Evol-Instruct%20and%20OSS-Instruct%2C%20opening%20a%20new%0Adimension%20for%20improving%20code%20instruction%20tuning.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/ise-uiuc/xft%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15247v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XFT%3A%20Unlocking%20the%20Power%20of%20Code%20Instruction%20Tuning%20by%20Simply%20Merging%0A%20%20Upcycled%20Mixture-of-Experts&entry.906535625=Yifeng%20Ding%20and%20Jiawei%20Liu%20and%20Yuxiang%20Wei%20and%20Terry%20Yue%20Zhuo%20and%20Lingming%20Zhang&entry.1292438233=%20%20We%20introduce%20XFT%2C%20a%20simple%20yet%20powerful%20training%20scheme%2C%20by%20simply%20merging%0Aupcycled%20Mixture-of-Experts%20%28MoE%29%20to%20unleash%20the%20performance%20limit%20of%0Ainstruction-tuned%20code%20Large%20Language%20Models%20%28LLMs%29.%20While%20vanilla%20sparse%0Aupcycling%20fails%20to%20improve%20instruction%20tuning%2C%20XFT%20introduces%20a%20shared%20expert%0Amechanism%20with%20a%20novel%20routing%20weight%20normalization%20strategy%20into%20sparse%0Aupcycling%2C%20which%20significantly%20boosts%20instruction%20tuning.%20After%20fine-tuning%20the%0Aupcycled%20MoE%20model%2C%20XFT%20introduces%20a%20learnable%20model%20merging%20mechanism%20to%0Acompile%20the%20upcycled%20MoE%20model%20back%20to%20a%20dense%20model%2C%20achieving%20upcycled%0AMoE-level%20performance%20with%20only%20dense-model%20compute.%20By%20applying%20XFT%20to%20a%201.3B%0Amodel%2C%20we%20create%20a%20new%20state-of-the-art%20tiny%20code%20LLM%20%28%3C3B%29%20with%2067.1%20and%2064.6%0Apass%401%20on%20HumanEval%20and%20HumanEval%2B%20respectively.%20With%20the%20same%20data%20and%20model%0Aarchitecture%2C%20XFT%20improves%20supervised%20fine-tuning%20%28SFT%29%20by%2013%25%20on%20HumanEval%2B%2C%0Aalong%20with%20consistent%20improvements%20from%202%25%20to%2013%25%20on%20MBPP%2B%2C%20MultiPL-E%2C%20and%0ADS-1000%2C%20demonstrating%20its%20generalizability.%20XFT%20is%20fully%20orthogonal%20to%0Aexisting%20techniques%20such%20as%20Evol-Instruct%20and%20OSS-Instruct%2C%20opening%20a%20new%0Adimension%20for%20improving%20code%20instruction%20tuning.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/ise-uiuc/xft%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15247v1&entry.124074799=Read"},
{"title": "Using deep reinforcement learning to promote sustainable human behaviour\n  on a common pool resource problem", "author": "Raphael Koster and Miruna P\u00eeslar and Andrea Tacchetti and Jan Balaguer and Leqi Liu and Romuald Elie and Oliver P. Hauser and Karl Tuyls and Matt Botvinick and Christopher Summerfield", "abstract": "  A canonical social dilemma arises when finite resources are allocated to a\ngroup of people, who can choose to either reciprocate with interest, or keep\nthe proceeds for themselves. What resource allocation mechanisms will encourage\nlevels of reciprocation that sustain the commons? Here, in an iterated\nmultiplayer trust game, we use deep reinforcement learning (RL) to design an\nallocation mechanism that endogenously promotes sustainable contributions from\nhuman participants to a common pool resource. We first trained neural networks\nto behave like human players, creating a stimulated economy that allowed us to\nstudy how different mechanisms influenced the dynamics of receipt and\nreciprocation. We then used RL to train a social planner to maximise aggregate\nreturn to players. The social planner discovered a redistributive policy that\nled to a large surplus and an inclusive economy, in which players made roughly\nequal gains. The RL agent increased human surplus over baseline mechanisms\nbased on unrestricted welfare or conditional cooperation, by conditioning its\ngenerosity on available resources and temporarily sanctioning defectors by\nallocating fewer resources to them. Examining the AI policy allowed us to\ndevelop an explainable mechanism that performed similarly and was more popular\namong players. Deep reinforcement learning can be used to discover mechanisms\nthat promote sustainable human behaviour.\n", "link": "http://arxiv.org/abs/2404.15059v1", "date": "2024-04-23", "relevancy": 1.8743, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4768}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4707}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4631}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20deep%20reinforcement%20learning%20to%20promote%20sustainable%20human%20behaviour%0A%20%20on%20a%20common%20pool%20resource%20problem&body=Title%3A%20Using%20deep%20reinforcement%20learning%20to%20promote%20sustainable%20human%20behaviour%0A%20%20on%20a%20common%20pool%20resource%20problem%0AAuthor%3A%20Raphael%20Koster%20and%20Miruna%20P%C3%AEslar%20and%20Andrea%20Tacchetti%20and%20Jan%20Balaguer%20and%20Leqi%20Liu%20and%20Romuald%20Elie%20and%20Oliver%20P.%20Hauser%20and%20Karl%20Tuyls%20and%20Matt%20Botvinick%20and%20Christopher%20Summerfield%0AAbstract%3A%20%20%20A%20canonical%20social%20dilemma%20arises%20when%20finite%20resources%20are%20allocated%20to%20a%0Agroup%20of%20people%2C%20who%20can%20choose%20to%20either%20reciprocate%20with%20interest%2C%20or%20keep%0Athe%20proceeds%20for%20themselves.%20What%20resource%20allocation%20mechanisms%20will%20encourage%0Alevels%20of%20reciprocation%20that%20sustain%20the%20commons%3F%20Here%2C%20in%20an%20iterated%0Amultiplayer%20trust%20game%2C%20we%20use%20deep%20reinforcement%20learning%20%28RL%29%20to%20design%20an%0Aallocation%20mechanism%20that%20endogenously%20promotes%20sustainable%20contributions%20from%0Ahuman%20participants%20to%20a%20common%20pool%20resource.%20We%20first%20trained%20neural%20networks%0Ato%20behave%20like%20human%20players%2C%20creating%20a%20stimulated%20economy%20that%20allowed%20us%20to%0Astudy%20how%20different%20mechanisms%20influenced%20the%20dynamics%20of%20receipt%20and%0Areciprocation.%20We%20then%20used%20RL%20to%20train%20a%20social%20planner%20to%20maximise%20aggregate%0Areturn%20to%20players.%20The%20social%20planner%20discovered%20a%20redistributive%20policy%20that%0Aled%20to%20a%20large%20surplus%20and%20an%20inclusive%20economy%2C%20in%20which%20players%20made%20roughly%0Aequal%20gains.%20The%20RL%20agent%20increased%20human%20surplus%20over%20baseline%20mechanisms%0Abased%20on%20unrestricted%20welfare%20or%20conditional%20cooperation%2C%20by%20conditioning%20its%0Agenerosity%20on%20available%20resources%20and%20temporarily%20sanctioning%20defectors%20by%0Aallocating%20fewer%20resources%20to%20them.%20Examining%20the%20AI%20policy%20allowed%20us%20to%0Adevelop%20an%20explainable%20mechanism%20that%20performed%20similarly%20and%20was%20more%20popular%0Aamong%20players.%20Deep%20reinforcement%20learning%20can%20be%20used%20to%20discover%20mechanisms%0Athat%20promote%20sustainable%20human%20behaviour.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15059v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20deep%20reinforcement%20learning%20to%20promote%20sustainable%20human%20behaviour%0A%20%20on%20a%20common%20pool%20resource%20problem&entry.906535625=Raphael%20Koster%20and%20Miruna%20P%C3%AEslar%20and%20Andrea%20Tacchetti%20and%20Jan%20Balaguer%20and%20Leqi%20Liu%20and%20Romuald%20Elie%20and%20Oliver%20P.%20Hauser%20and%20Karl%20Tuyls%20and%20Matt%20Botvinick%20and%20Christopher%20Summerfield&entry.1292438233=%20%20A%20canonical%20social%20dilemma%20arises%20when%20finite%20resources%20are%20allocated%20to%20a%0Agroup%20of%20people%2C%20who%20can%20choose%20to%20either%20reciprocate%20with%20interest%2C%20or%20keep%0Athe%20proceeds%20for%20themselves.%20What%20resource%20allocation%20mechanisms%20will%20encourage%0Alevels%20of%20reciprocation%20that%20sustain%20the%20commons%3F%20Here%2C%20in%20an%20iterated%0Amultiplayer%20trust%20game%2C%20we%20use%20deep%20reinforcement%20learning%20%28RL%29%20to%20design%20an%0Aallocation%20mechanism%20that%20endogenously%20promotes%20sustainable%20contributions%20from%0Ahuman%20participants%20to%20a%20common%20pool%20resource.%20We%20first%20trained%20neural%20networks%0Ato%20behave%20like%20human%20players%2C%20creating%20a%20stimulated%20economy%20that%20allowed%20us%20to%0Astudy%20how%20different%20mechanisms%20influenced%20the%20dynamics%20of%20receipt%20and%0Areciprocation.%20We%20then%20used%20RL%20to%20train%20a%20social%20planner%20to%20maximise%20aggregate%0Areturn%20to%20players.%20The%20social%20planner%20discovered%20a%20redistributive%20policy%20that%0Aled%20to%20a%20large%20surplus%20and%20an%20inclusive%20economy%2C%20in%20which%20players%20made%20roughly%0Aequal%20gains.%20The%20RL%20agent%20increased%20human%20surplus%20over%20baseline%20mechanisms%0Abased%20on%20unrestricted%20welfare%20or%20conditional%20cooperation%2C%20by%20conditioning%20its%0Agenerosity%20on%20available%20resources%20and%20temporarily%20sanctioning%20defectors%20by%0Aallocating%20fewer%20resources%20to%20them.%20Examining%20the%20AI%20policy%20allowed%20us%20to%0Adevelop%20an%20explainable%20mechanism%20that%20performed%20similarly%20and%20was%20more%20popular%0Aamong%20players.%20Deep%20reinforcement%20learning%20can%20be%20used%20to%20discover%20mechanisms%0Athat%20promote%20sustainable%20human%20behaviour.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15059v1&entry.124074799=Read"},
{"title": "Evaluation of Teleoperation Concepts to solve Automated Vehicle\n  Disengagements", "author": "David Brecht and Nils Gehrke and Tobias Kerbl and Niklas Krauss and Domagoj Majstorovic and Florian Pfab and Maria-Magdalena Wolf and Frank Diermeyer", "abstract": "  Teleoperation is a popular solution to remotely support highly automated\nvehicles through a human remote operator whenever a disengagement of the\nautomated driving system is present. The remote operator wirelessly connects to\nthe vehicle and solves the disengagement through support or substitution of\nautomated driving functions and therefore enables the vehicle to resume\nautomation. There are different approaches to support automated driving\nfunctions on various levels, commonly known as teleoperation concepts. A\nvariety of teleoperation concepts is described in the literature, yet there has\nbeen no comprehensive and structured comparison of these concepts, and it is\nnot clear what subset of teleoperation concepts is suitable to enable safe and\nefficient remote support of highly automated vehicles in a broad spectrum of\ndisengagements. The following work establishes a basis for comparing\nteleoperation concepts through a literature overview on automated vehicle\ndisengagements and on already conducted studies on the comparison of\nteleoperation concepts and metrics used to evaluate teleoperation performance.\nAn evaluation of the teleoperation concepts is carried out in an expert\nworkshop, comparing different teleoperation concepts using a selection of\nautomated vehicle disengagement scenarios and metrics. Based on the workshop\nresults, a set of teleoperation concepts is derived that can be used to address\na wide variety of automated vehicle disengagements in a safe and efficient way.\n", "link": "http://arxiv.org/abs/2404.15030v1", "date": "2024-04-23", "relevancy": 1.8741, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5032}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4536}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4399}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Teleoperation%20Concepts%20to%20solve%20Automated%20Vehicle%0A%20%20Disengagements&body=Title%3A%20Evaluation%20of%20Teleoperation%20Concepts%20to%20solve%20Automated%20Vehicle%0A%20%20Disengagements%0AAuthor%3A%20David%20Brecht%20and%20Nils%20Gehrke%20and%20Tobias%20Kerbl%20and%20Niklas%20Krauss%20and%20Domagoj%20Majstorovic%20and%20Florian%20Pfab%20and%20Maria-Magdalena%20Wolf%20and%20Frank%20Diermeyer%0AAbstract%3A%20%20%20Teleoperation%20is%20a%20popular%20solution%20to%20remotely%20support%20highly%20automated%0Avehicles%20through%20a%20human%20remote%20operator%20whenever%20a%20disengagement%20of%20the%0Aautomated%20driving%20system%20is%20present.%20The%20remote%20operator%20wirelessly%20connects%20to%0Athe%20vehicle%20and%20solves%20the%20disengagement%20through%20support%20or%20substitution%20of%0Aautomated%20driving%20functions%20and%20therefore%20enables%20the%20vehicle%20to%20resume%0Aautomation.%20There%20are%20different%20approaches%20to%20support%20automated%20driving%0Afunctions%20on%20various%20levels%2C%20commonly%20known%20as%20teleoperation%20concepts.%20A%0Avariety%20of%20teleoperation%20concepts%20is%20described%20in%20the%20literature%2C%20yet%20there%20has%0Abeen%20no%20comprehensive%20and%20structured%20comparison%20of%20these%20concepts%2C%20and%20it%20is%0Anot%20clear%20what%20subset%20of%20teleoperation%20concepts%20is%20suitable%20to%20enable%20safe%20and%0Aefficient%20remote%20support%20of%20highly%20automated%20vehicles%20in%20a%20broad%20spectrum%20of%0Adisengagements.%20The%20following%20work%20establishes%20a%20basis%20for%20comparing%0Ateleoperation%20concepts%20through%20a%20literature%20overview%20on%20automated%20vehicle%0Adisengagements%20and%20on%20already%20conducted%20studies%20on%20the%20comparison%20of%0Ateleoperation%20concepts%20and%20metrics%20used%20to%20evaluate%20teleoperation%20performance.%0AAn%20evaluation%20of%20the%20teleoperation%20concepts%20is%20carried%20out%20in%20an%20expert%0Aworkshop%2C%20comparing%20different%20teleoperation%20concepts%20using%20a%20selection%20of%0Aautomated%20vehicle%20disengagement%20scenarios%20and%20metrics.%20Based%20on%20the%20workshop%0Aresults%2C%20a%20set%20of%20teleoperation%20concepts%20is%20derived%20that%20can%20be%20used%20to%20address%0Aa%20wide%20variety%20of%20automated%20vehicle%20disengagements%20in%20a%20safe%20and%20efficient%20way.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15030v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Teleoperation%20Concepts%20to%20solve%20Automated%20Vehicle%0A%20%20Disengagements&entry.906535625=David%20Brecht%20and%20Nils%20Gehrke%20and%20Tobias%20Kerbl%20and%20Niklas%20Krauss%20and%20Domagoj%20Majstorovic%20and%20Florian%20Pfab%20and%20Maria-Magdalena%20Wolf%20and%20Frank%20Diermeyer&entry.1292438233=%20%20Teleoperation%20is%20a%20popular%20solution%20to%20remotely%20support%20highly%20automated%0Avehicles%20through%20a%20human%20remote%20operator%20whenever%20a%20disengagement%20of%20the%0Aautomated%20driving%20system%20is%20present.%20The%20remote%20operator%20wirelessly%20connects%20to%0Athe%20vehicle%20and%20solves%20the%20disengagement%20through%20support%20or%20substitution%20of%0Aautomated%20driving%20functions%20and%20therefore%20enables%20the%20vehicle%20to%20resume%0Aautomation.%20There%20are%20different%20approaches%20to%20support%20automated%20driving%0Afunctions%20on%20various%20levels%2C%20commonly%20known%20as%20teleoperation%20concepts.%20A%0Avariety%20of%20teleoperation%20concepts%20is%20described%20in%20the%20literature%2C%20yet%20there%20has%0Abeen%20no%20comprehensive%20and%20structured%20comparison%20of%20these%20concepts%2C%20and%20it%20is%0Anot%20clear%20what%20subset%20of%20teleoperation%20concepts%20is%20suitable%20to%20enable%20safe%20and%0Aefficient%20remote%20support%20of%20highly%20automated%20vehicles%20in%20a%20broad%20spectrum%20of%0Adisengagements.%20The%20following%20work%20establishes%20a%20basis%20for%20comparing%0Ateleoperation%20concepts%20through%20a%20literature%20overview%20on%20automated%20vehicle%0Adisengagements%20and%20on%20already%20conducted%20studies%20on%20the%20comparison%20of%0Ateleoperation%20concepts%20and%20metrics%20used%20to%20evaluate%20teleoperation%20performance.%0AAn%20evaluation%20of%20the%20teleoperation%20concepts%20is%20carried%20out%20in%20an%20expert%0Aworkshop%2C%20comparing%20different%20teleoperation%20concepts%20using%20a%20selection%20of%0Aautomated%20vehicle%20disengagement%20scenarios%20and%20metrics.%20Based%20on%20the%20workshop%0Aresults%2C%20a%20set%20of%20teleoperation%20concepts%20is%20derived%20that%20can%20be%20used%20to%20address%0Aa%20wide%20variety%20of%20automated%20vehicle%20disengagements%20in%20a%20safe%20and%20efficient%20way.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15030v1&entry.124074799=Read"},
{"title": "IPAD: Industrial Process Anomaly Detection Dataset", "author": "Jinfan Liu and Yichao Yan and Junjie Li and Weiming Zhao and Pengzhi Chu and Xingdong Sheng and Yunhui Liu and Xiaokang Yang", "abstract": "  Video anomaly detection (VAD) is a challenging task aiming to recognize\nanomalies in video frames, and existing large-scale VAD researches primarily\nfocus on road traffic and human activity scenes. In industrial scenes, there\nare often a variety of unpredictable anomalies, and the VAD method can play a\nsignificant role in these scenarios. However, there is a lack of applicable\ndatasets and methods specifically tailored for industrial production scenarios\ndue to concerns regarding privacy and security. To bridge this gap, we propose\na new dataset, IPAD, specifically designed for VAD in industrial scenarios. The\nindustrial processes in our dataset are chosen through on-site factory research\nand discussions with engineers. This dataset covers 16 different industrial\ndevices and contains over 6 hours of both synthetic and real-world video\nfootage. Moreover, we annotate the key feature of the industrial process, ie,\nperiodicity. Based on the proposed dataset, we introduce a period memory module\nand a sliding window inspection mechanism to effectively investigate the\nperiodic information in a basic reconstruction model. Our framework leverages\nLoRA adapter to explore the effective migration of pretrained models, which are\ninitially trained using synthetic data, into real-world scenarios. Our proposed\ndataset and method will fill the gap in the field of industrial video anomaly\ndetection and drive the process of video understanding tasks as well as smart\nfactory deployment.\n", "link": "http://arxiv.org/abs/2404.15033v1", "date": "2024-04-23", "relevancy": 1.8634, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4801}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4599}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4539}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IPAD%3A%20Industrial%20Process%20Anomaly%20Detection%20Dataset&body=Title%3A%20IPAD%3A%20Industrial%20Process%20Anomaly%20Detection%20Dataset%0AAuthor%3A%20Jinfan%20Liu%20and%20Yichao%20Yan%20and%20Junjie%20Li%20and%20Weiming%20Zhao%20and%20Pengzhi%20Chu%20and%20Xingdong%20Sheng%20and%20Yunhui%20Liu%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Video%20anomaly%20detection%20%28VAD%29%20is%20a%20challenging%20task%20aiming%20to%20recognize%0Aanomalies%20in%20video%20frames%2C%20and%20existing%20large-scale%20VAD%20researches%20primarily%0Afocus%20on%20road%20traffic%20and%20human%20activity%20scenes.%20In%20industrial%20scenes%2C%20there%0Aare%20often%20a%20variety%20of%20unpredictable%20anomalies%2C%20and%20the%20VAD%20method%20can%20play%20a%0Asignificant%20role%20in%20these%20scenarios.%20However%2C%20there%20is%20a%20lack%20of%20applicable%0Adatasets%20and%20methods%20specifically%20tailored%20for%20industrial%20production%20scenarios%0Adue%20to%20concerns%20regarding%20privacy%20and%20security.%20To%20bridge%20this%20gap%2C%20we%20propose%0Aa%20new%20dataset%2C%20IPAD%2C%20specifically%20designed%20for%20VAD%20in%20industrial%20scenarios.%20The%0Aindustrial%20processes%20in%20our%20dataset%20are%20chosen%20through%20on-site%20factory%20research%0Aand%20discussions%20with%20engineers.%20This%20dataset%20covers%2016%20different%20industrial%0Adevices%20and%20contains%20over%206%20hours%20of%20both%20synthetic%20and%20real-world%20video%0Afootage.%20Moreover%2C%20we%20annotate%20the%20key%20feature%20of%20the%20industrial%20process%2C%20ie%2C%0Aperiodicity.%20Based%20on%20the%20proposed%20dataset%2C%20we%20introduce%20a%20period%20memory%20module%0Aand%20a%20sliding%20window%20inspection%20mechanism%20to%20effectively%20investigate%20the%0Aperiodic%20information%20in%20a%20basic%20reconstruction%20model.%20Our%20framework%20leverages%0ALoRA%20adapter%20to%20explore%20the%20effective%20migration%20of%20pretrained%20models%2C%20which%20are%0Ainitially%20trained%20using%20synthetic%20data%2C%20into%20real-world%20scenarios.%20Our%20proposed%0Adataset%20and%20method%20will%20fill%20the%20gap%20in%20the%20field%20of%20industrial%20video%20anomaly%0Adetection%20and%20drive%20the%20process%20of%20video%20understanding%20tasks%20as%20well%20as%20smart%0Afactory%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15033v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPAD%3A%20Industrial%20Process%20Anomaly%20Detection%20Dataset&entry.906535625=Jinfan%20Liu%20and%20Yichao%20Yan%20and%20Junjie%20Li%20and%20Weiming%20Zhao%20and%20Pengzhi%20Chu%20and%20Xingdong%20Sheng%20and%20Yunhui%20Liu%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Video%20anomaly%20detection%20%28VAD%29%20is%20a%20challenging%20task%20aiming%20to%20recognize%0Aanomalies%20in%20video%20frames%2C%20and%20existing%20large-scale%20VAD%20researches%20primarily%0Afocus%20on%20road%20traffic%20and%20human%20activity%20scenes.%20In%20industrial%20scenes%2C%20there%0Aare%20often%20a%20variety%20of%20unpredictable%20anomalies%2C%20and%20the%20VAD%20method%20can%20play%20a%0Asignificant%20role%20in%20these%20scenarios.%20However%2C%20there%20is%20a%20lack%20of%20applicable%0Adatasets%20and%20methods%20specifically%20tailored%20for%20industrial%20production%20scenarios%0Adue%20to%20concerns%20regarding%20privacy%20and%20security.%20To%20bridge%20this%20gap%2C%20we%20propose%0Aa%20new%20dataset%2C%20IPAD%2C%20specifically%20designed%20for%20VAD%20in%20industrial%20scenarios.%20The%0Aindustrial%20processes%20in%20our%20dataset%20are%20chosen%20through%20on-site%20factory%20research%0Aand%20discussions%20with%20engineers.%20This%20dataset%20covers%2016%20different%20industrial%0Adevices%20and%20contains%20over%206%20hours%20of%20both%20synthetic%20and%20real-world%20video%0Afootage.%20Moreover%2C%20we%20annotate%20the%20key%20feature%20of%20the%20industrial%20process%2C%20ie%2C%0Aperiodicity.%20Based%20on%20the%20proposed%20dataset%2C%20we%20introduce%20a%20period%20memory%20module%0Aand%20a%20sliding%20window%20inspection%20mechanism%20to%20effectively%20investigate%20the%0Aperiodic%20information%20in%20a%20basic%20reconstruction%20model.%20Our%20framework%20leverages%0ALoRA%20adapter%20to%20explore%20the%20effective%20migration%20of%20pretrained%20models%2C%20which%20are%0Ainitially%20trained%20using%20synthetic%20data%2C%20into%20real-world%20scenarios.%20Our%20proposed%0Adataset%20and%20method%20will%20fill%20the%20gap%20in%20the%20field%20of%20industrial%20video%20anomaly%0Adetection%20and%20drive%20the%20process%20of%20video%20understanding%20tasks%20as%20well%20as%20smart%0Afactory%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15033v1&entry.124074799=Read"},
{"title": "Mining Invariance from Nonlinear Multi-Environment Data: Binary\n  Classification", "author": "Austin Goddard and Kang Du and Yu Xiang", "abstract": "  Making predictions in an unseen environment given data from multiple training\nenvironments is a challenging task. We approach this problem from an invariance\nperspective, focusing on binary classification to shed light on general\nnonlinear data generation mechanisms. We identify a unique form of invariance\nthat exists solely in a binary setting that allows us to train models invariant\nover environments. We provide sufficient conditions for such invariance and\nshow it is robust even when environmental conditions vary greatly. Our\nformulation admits a causal interpretation, allowing us to compare it with\nvarious frameworks. Finally, we propose a heuristic prediction method and\nconduct experiments using real and synthetic datasets.\n", "link": "http://arxiv.org/abs/2404.15245v1", "date": "2024-04-23", "relevancy": 1.8582, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5067}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4663}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4459}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mining%20Invariance%20from%20Nonlinear%20Multi-Environment%20Data%3A%20Binary%0A%20%20Classification&body=Title%3A%20Mining%20Invariance%20from%20Nonlinear%20Multi-Environment%20Data%3A%20Binary%0A%20%20Classification%0AAuthor%3A%20Austin%20Goddard%20and%20Kang%20Du%20and%20Yu%20Xiang%0AAbstract%3A%20%20%20Making%20predictions%20in%20an%20unseen%20environment%20given%20data%20from%20multiple%20training%0Aenvironments%20is%20a%20challenging%20task.%20We%20approach%20this%20problem%20from%20an%20invariance%0Aperspective%2C%20focusing%20on%20binary%20classification%20to%20shed%20light%20on%20general%0Anonlinear%20data%20generation%20mechanisms.%20We%20identify%20a%20unique%20form%20of%20invariance%0Athat%20exists%20solely%20in%20a%20binary%20setting%20that%20allows%20us%20to%20train%20models%20invariant%0Aover%20environments.%20We%20provide%20sufficient%20conditions%20for%20such%20invariance%20and%0Ashow%20it%20is%20robust%20even%20when%20environmental%20conditions%20vary%20greatly.%20Our%0Aformulation%20admits%20a%20causal%20interpretation%2C%20allowing%20us%20to%20compare%20it%20with%0Avarious%20frameworks.%20Finally%2C%20we%20propose%20a%20heuristic%20prediction%20method%20and%0Aconduct%20experiments%20using%20real%20and%20synthetic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15245v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mining%20Invariance%20from%20Nonlinear%20Multi-Environment%20Data%3A%20Binary%0A%20%20Classification&entry.906535625=Austin%20Goddard%20and%20Kang%20Du%20and%20Yu%20Xiang&entry.1292438233=%20%20Making%20predictions%20in%20an%20unseen%20environment%20given%20data%20from%20multiple%20training%0Aenvironments%20is%20a%20challenging%20task.%20We%20approach%20this%20problem%20from%20an%20invariance%0Aperspective%2C%20focusing%20on%20binary%20classification%20to%20shed%20light%20on%20general%0Anonlinear%20data%20generation%20mechanisms.%20We%20identify%20a%20unique%20form%20of%20invariance%0Athat%20exists%20solely%20in%20a%20binary%20setting%20that%20allows%20us%20to%20train%20models%20invariant%0Aover%20environments.%20We%20provide%20sufficient%20conditions%20for%20such%20invariance%20and%0Ashow%20it%20is%20robust%20even%20when%20environmental%20conditions%20vary%20greatly.%20Our%0Aformulation%20admits%20a%20causal%20interpretation%2C%20allowing%20us%20to%20compare%20it%20with%0Avarious%20frameworks.%20Finally%2C%20we%20propose%20a%20heuristic%20prediction%20method%20and%0Aconduct%20experiments%20using%20real%20and%20synthetic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15245v1&entry.124074799=Read"},
{"title": "Enhancing Trust in Autonomous Agents: An Architecture for Accountability\n  and Explainability through Blockchain and Large Language Models", "author": "Laura Fern\u00e1ndez-Becerra and Miguel \u00c1ngel Gonz\u00e1lez-Santamarta and \u00c1ngel Manuel Guerrero-Higueras and Francisco Javier Rodr\u00edguez-Lera and Vicente Matell\u00e1n Olivera", "abstract": "  The deployment of autonomous agents in environments involving human\ninteraction has increasingly raised security concerns. Consequently,\nunderstanding the circumstances behind an event becomes critical, requiring the\ndevelopment of capabilities to justify their behaviors to non-expert users.\nSuch explanations are essential in enhancing trustworthiness and safety, acting\nas a preventive measure against failures, errors, and misunderstandings.\nAdditionally, they contribute to improving communication, bridging the gap\nbetween the agent and the user, thereby improving the effectiveness of their\ninteractions. This work presents an accountability and explainability\narchitecture implemented for ROS-based mobile robots. The proposed solution\nconsists of two main components. Firstly, a black box-like element to provide\naccountability, featuring anti-tampering properties achieved through blockchain\ntechnology. Secondly, a component in charge of generating natural language\nexplanations by harnessing the capabilities of Large Language Models (LLMs)\nover the data contained within the previously mentioned black box. The study\nevaluates the performance of our solution in three different scenarios, each\ninvolving autonomous agent navigation functionalities. This evaluation includes\na thorough examination of accountability and explainability metrics,\ndemonstrating the effectiveness of our approach in using accountable data from\nrobot actions to obtain coherent, accurate and understandable explanations,\neven when facing challenges inherent in the use of autonomous agents in\nreal-world scenarios.\n", "link": "http://arxiv.org/abs/2403.09567v2", "date": "2024-04-23", "relevancy": 1.5966, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5496}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Trust%20in%20Autonomous%20Agents%3A%20An%20Architecture%20for%20Accountability%0A%20%20and%20Explainability%20through%20Blockchain%20and%20Large%20Language%20Models&body=Title%3A%20Enhancing%20Trust%20in%20Autonomous%20Agents%3A%20An%20Architecture%20for%20Accountability%0A%20%20and%20Explainability%20through%20Blockchain%20and%20Large%20Language%20Models%0AAuthor%3A%20Laura%20Fern%C3%A1ndez-Becerra%20and%20Miguel%20%C3%81ngel%20Gonz%C3%A1lez-Santamarta%20and%20%C3%81ngel%20Manuel%20Guerrero-Higueras%20and%20Francisco%20Javier%20Rodr%C3%ADguez-Lera%20and%20Vicente%20Matell%C3%A1n%20Olivera%0AAbstract%3A%20%20%20The%20deployment%20of%20autonomous%20agents%20in%20environments%20involving%20human%0Ainteraction%20has%20increasingly%20raised%20security%20concerns.%20Consequently%2C%0Aunderstanding%20the%20circumstances%20behind%20an%20event%20becomes%20critical%2C%20requiring%20the%0Adevelopment%20of%20capabilities%20to%20justify%20their%20behaviors%20to%20non-expert%20users.%0ASuch%20explanations%20are%20essential%20in%20enhancing%20trustworthiness%20and%20safety%2C%20acting%0Aas%20a%20preventive%20measure%20against%20failures%2C%20errors%2C%20and%20misunderstandings.%0AAdditionally%2C%20they%20contribute%20to%20improving%20communication%2C%20bridging%20the%20gap%0Abetween%20the%20agent%20and%20the%20user%2C%20thereby%20improving%20the%20effectiveness%20of%20their%0Ainteractions.%20This%20work%20presents%20an%20accountability%20and%20explainability%0Aarchitecture%20implemented%20for%20ROS-based%20mobile%20robots.%20The%20proposed%20solution%0Aconsists%20of%20two%20main%20components.%20Firstly%2C%20a%20black%20box-like%20element%20to%20provide%0Aaccountability%2C%20featuring%20anti-tampering%20properties%20achieved%20through%20blockchain%0Atechnology.%20Secondly%2C%20a%20component%20in%20charge%20of%20generating%20natural%20language%0Aexplanations%20by%20harnessing%20the%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%0Aover%20the%20data%20contained%20within%20the%20previously%20mentioned%20black%20box.%20The%20study%0Aevaluates%20the%20performance%20of%20our%20solution%20in%20three%20different%20scenarios%2C%20each%0Ainvolving%20autonomous%20agent%20navigation%20functionalities.%20This%20evaluation%20includes%0Aa%20thorough%20examination%20of%20accountability%20and%20explainability%20metrics%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20approach%20in%20using%20accountable%20data%20from%0Arobot%20actions%20to%20obtain%20coherent%2C%20accurate%20and%20understandable%20explanations%2C%0Aeven%20when%20facing%20challenges%20inherent%20in%20the%20use%20of%20autonomous%20agents%20in%0Areal-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09567v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Trust%20in%20Autonomous%20Agents%3A%20An%20Architecture%20for%20Accountability%0A%20%20and%20Explainability%20through%20Blockchain%20and%20Large%20Language%20Models&entry.906535625=Laura%20Fern%C3%A1ndez-Becerra%20and%20Miguel%20%C3%81ngel%20Gonz%C3%A1lez-Santamarta%20and%20%C3%81ngel%20Manuel%20Guerrero-Higueras%20and%20Francisco%20Javier%20Rodr%C3%ADguez-Lera%20and%20Vicente%20Matell%C3%A1n%20Olivera&entry.1292438233=%20%20The%20deployment%20of%20autonomous%20agents%20in%20environments%20involving%20human%0Ainteraction%20has%20increasingly%20raised%20security%20concerns.%20Consequently%2C%0Aunderstanding%20the%20circumstances%20behind%20an%20event%20becomes%20critical%2C%20requiring%20the%0Adevelopment%20of%20capabilities%20to%20justify%20their%20behaviors%20to%20non-expert%20users.%0ASuch%20explanations%20are%20essential%20in%20enhancing%20trustworthiness%20and%20safety%2C%20acting%0Aas%20a%20preventive%20measure%20against%20failures%2C%20errors%2C%20and%20misunderstandings.%0AAdditionally%2C%20they%20contribute%20to%20improving%20communication%2C%20bridging%20the%20gap%0Abetween%20the%20agent%20and%20the%20user%2C%20thereby%20improving%20the%20effectiveness%20of%20their%0Ainteractions.%20This%20work%20presents%20an%20accountability%20and%20explainability%0Aarchitecture%20implemented%20for%20ROS-based%20mobile%20robots.%20The%20proposed%20solution%0Aconsists%20of%20two%20main%20components.%20Firstly%2C%20a%20black%20box-like%20element%20to%20provide%0Aaccountability%2C%20featuring%20anti-tampering%20properties%20achieved%20through%20blockchain%0Atechnology.%20Secondly%2C%20a%20component%20in%20charge%20of%20generating%20natural%20language%0Aexplanations%20by%20harnessing%20the%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%0Aover%20the%20data%20contained%20within%20the%20previously%20mentioned%20black%20box.%20The%20study%0Aevaluates%20the%20performance%20of%20our%20solution%20in%20three%20different%20scenarios%2C%20each%0Ainvolving%20autonomous%20agent%20navigation%20functionalities.%20This%20evaluation%20includes%0Aa%20thorough%20examination%20of%20accountability%20and%20explainability%20metrics%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20approach%20in%20using%20accountable%20data%20from%0Arobot%20actions%20to%20obtain%20coherent%2C%20accurate%20and%20understandable%20explanations%2C%0Aeven%20when%20facing%20challenges%20inherent%20in%20the%20use%20of%20autonomous%20agents%20in%0Areal-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09567v2&entry.124074799=Read"},
{"title": "Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs\n  Tree LSTMs", "author": "Rashid Barket and Matthew England and J\u00fcrgen Gerhard", "abstract": "  Computer Algebra Systems (e.g. Maple) are used in research, education, and\nindustrial settings. One of their key functionalities is symbolic integration,\nwhere there are many sub-algorithms to choose from that can affect the form of\nthe output integral, and the runtime. Choosing the right sub-algorithm for a\ngiven problem is challenging: we hypothesise that Machine Learning can guide\nthis sub-algorithm choice. A key consideration of our methodology is how to\nrepresent the mathematics to the ML model: we hypothesise that a representation\nwhich encodes the tree structure of mathematical expressions would be well\nsuited. We trained both an LSTM and a TreeLSTM model for sub-algorithm\nprediction and compared them to Maple's existing approach. Our TreeLSTM\nperforms much better than the LSTM, highlighting the benefit of using an\ninformed representation of mathematical expressions. It is able to produce\nbetter outputs than Maple's current state-of-the-art meta-algorithm, giving a\nstrong basis for further research.\n", "link": "http://arxiv.org/abs/2404.14973v1", "date": "2024-04-23", "relevancy": 1.7259, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4404}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4362}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4232}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Symbolic%20Integration%20Algorithm%20Selection%20with%20Machine%20Learning%3A%20LSTMs%20vs%0A%20%20Tree%20LSTMs&body=Title%3A%20Symbolic%20Integration%20Algorithm%20Selection%20with%20Machine%20Learning%3A%20LSTMs%20vs%0A%20%20Tree%20LSTMs%0AAuthor%3A%20Rashid%20Barket%20and%20Matthew%20England%20and%20J%C3%BCrgen%20Gerhard%0AAbstract%3A%20%20%20Computer%20Algebra%20Systems%20%28e.g.%20Maple%29%20are%20used%20in%20research%2C%20education%2C%20and%0Aindustrial%20settings.%20One%20of%20their%20key%20functionalities%20is%20symbolic%20integration%2C%0Awhere%20there%20are%20many%20sub-algorithms%20to%20choose%20from%20that%20can%20affect%20the%20form%20of%0Athe%20output%20integral%2C%20and%20the%20runtime.%20Choosing%20the%20right%20sub-algorithm%20for%20a%0Agiven%20problem%20is%20challenging%3A%20we%20hypothesise%20that%20Machine%20Learning%20can%20guide%0Athis%20sub-algorithm%20choice.%20A%20key%20consideration%20of%20our%20methodology%20is%20how%20to%0Arepresent%20the%20mathematics%20to%20the%20ML%20model%3A%20we%20hypothesise%20that%20a%20representation%0Awhich%20encodes%20the%20tree%20structure%20of%20mathematical%20expressions%20would%20be%20well%0Asuited.%20We%20trained%20both%20an%20LSTM%20and%20a%20TreeLSTM%20model%20for%20sub-algorithm%0Aprediction%20and%20compared%20them%20to%20Maple%27s%20existing%20approach.%20Our%20TreeLSTM%0Aperforms%20much%20better%20than%20the%20LSTM%2C%20highlighting%20the%20benefit%20of%20using%20an%0Ainformed%20representation%20of%20mathematical%20expressions.%20It%20is%20able%20to%20produce%0Abetter%20outputs%20than%20Maple%27s%20current%20state-of-the-art%20meta-algorithm%2C%20giving%20a%0Astrong%20basis%20for%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14973v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20Integration%20Algorithm%20Selection%20with%20Machine%20Learning%3A%20LSTMs%20vs%0A%20%20Tree%20LSTMs&entry.906535625=Rashid%20Barket%20and%20Matthew%20England%20and%20J%C3%BCrgen%20Gerhard&entry.1292438233=%20%20Computer%20Algebra%20Systems%20%28e.g.%20Maple%29%20are%20used%20in%20research%2C%20education%2C%20and%0Aindustrial%20settings.%20One%20of%20their%20key%20functionalities%20is%20symbolic%20integration%2C%0Awhere%20there%20are%20many%20sub-algorithms%20to%20choose%20from%20that%20can%20affect%20the%20form%20of%0Athe%20output%20integral%2C%20and%20the%20runtime.%20Choosing%20the%20right%20sub-algorithm%20for%20a%0Agiven%20problem%20is%20challenging%3A%20we%20hypothesise%20that%20Machine%20Learning%20can%20guide%0Athis%20sub-algorithm%20choice.%20A%20key%20consideration%20of%20our%20methodology%20is%20how%20to%0Arepresent%20the%20mathematics%20to%20the%20ML%20model%3A%20we%20hypothesise%20that%20a%20representation%0Awhich%20encodes%20the%20tree%20structure%20of%20mathematical%20expressions%20would%20be%20well%0Asuited.%20We%20trained%20both%20an%20LSTM%20and%20a%20TreeLSTM%20model%20for%20sub-algorithm%0Aprediction%20and%20compared%20them%20to%20Maple%27s%20existing%20approach.%20Our%20TreeLSTM%0Aperforms%20much%20better%20than%20the%20LSTM%2C%20highlighting%20the%20benefit%20of%20using%20an%0Ainformed%20representation%20of%20mathematical%20expressions.%20It%20is%20able%20to%20produce%0Abetter%20outputs%20than%20Maple%27s%20current%20state-of-the-art%20meta-algorithm%2C%20giving%20a%0Astrong%20basis%20for%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14973v1&entry.124074799=Read"},
{"title": "Taming Diffusion Probabilistic Models for Character Control", "author": "Rui Chen and Mingyi Shi and Shaoli Huang and Ping Tan and Taku Komura and Xuelin Chen", "abstract": "  We present a novel character control framework that effectively utilizes\nmotion diffusion probabilistic models to generate high-quality and diverse\ncharacter animations, responding in real-time to a variety of dynamic\nuser-supplied control signals. At the heart of our method lies a\ntransformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM),\nwhich takes as input the character's historical motion and can generate a range\nof diverse potential future motions conditioned on high-level, coarse user\ncontrol. To meet the demands for diversity, controllability, and computational\nefficiency required by a real-time controller, we incorporate several key\nalgorithmic designs. These include separate condition tokenization,\nclassifier-free guidance on past motion, and heuristic future trajectory\nextension, all designed to address the challenges associated with taming motion\ndiffusion probabilistic models for character control. As a result, our work\nrepresents the first model that enables real-time generation of high-quality,\ndiverse character animations based on user interactive control, supporting\nanimating the character in multiple styles with a single unified model. We\nevaluate our method on a diverse set of locomotion skills, demonstrating the\nmerits of our method over existing character controllers. Project page and\nsource codes: https://aiganimation.github.io/CAMDM/\n", "link": "http://arxiv.org/abs/2404.15121v1", "date": "2024-04-23", "relevancy": 1.766, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6047}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5869}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5772}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Taming%20Diffusion%20Probabilistic%20Models%20for%20Character%20Control&body=Title%3A%20Taming%20Diffusion%20Probabilistic%20Models%20for%20Character%20Control%0AAuthor%3A%20Rui%20Chen%20and%20Mingyi%20Shi%20and%20Shaoli%20Huang%20and%20Ping%20Tan%20and%20Taku%20Komura%20and%20Xuelin%20Chen%0AAbstract%3A%20%20%20We%20present%20a%20novel%20character%20control%20framework%20that%20effectively%20utilizes%0Amotion%20diffusion%20probabilistic%20models%20to%20generate%20high-quality%20and%20diverse%0Acharacter%20animations%2C%20responding%20in%20real-time%20to%20a%20variety%20of%20dynamic%0Auser-supplied%20control%20signals.%20At%20the%20heart%20of%20our%20method%20lies%20a%0Atransformer-based%20Conditional%20Autoregressive%20Motion%20Diffusion%20Model%20%28CAMDM%29%2C%0Awhich%20takes%20as%20input%20the%20character%27s%20historical%20motion%20and%20can%20generate%20a%20range%0Aof%20diverse%20potential%20future%20motions%20conditioned%20on%20high-level%2C%20coarse%20user%0Acontrol.%20To%20meet%20the%20demands%20for%20diversity%2C%20controllability%2C%20and%20computational%0Aefficiency%20required%20by%20a%20real-time%20controller%2C%20we%20incorporate%20several%20key%0Aalgorithmic%20designs.%20These%20include%20separate%20condition%20tokenization%2C%0Aclassifier-free%20guidance%20on%20past%20motion%2C%20and%20heuristic%20future%20trajectory%0Aextension%2C%20all%20designed%20to%20address%20the%20challenges%20associated%20with%20taming%20motion%0Adiffusion%20probabilistic%20models%20for%20character%20control.%20As%20a%20result%2C%20our%20work%0Arepresents%20the%20first%20model%20that%20enables%20real-time%20generation%20of%20high-quality%2C%0Adiverse%20character%20animations%20based%20on%20user%20interactive%20control%2C%20supporting%0Aanimating%20the%20character%20in%20multiple%20styles%20with%20a%20single%20unified%20model.%20We%0Aevaluate%20our%20method%20on%20a%20diverse%20set%20of%20locomotion%20skills%2C%20demonstrating%20the%0Amerits%20of%20our%20method%20over%20existing%20character%20controllers.%20Project%20page%20and%0Asource%20codes%3A%20https%3A//aiganimation.github.io/CAMDM/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15121v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Diffusion%20Probabilistic%20Models%20for%20Character%20Control&entry.906535625=Rui%20Chen%20and%20Mingyi%20Shi%20and%20Shaoli%20Huang%20and%20Ping%20Tan%20and%20Taku%20Komura%20and%20Xuelin%20Chen&entry.1292438233=%20%20We%20present%20a%20novel%20character%20control%20framework%20that%20effectively%20utilizes%0Amotion%20diffusion%20probabilistic%20models%20to%20generate%20high-quality%20and%20diverse%0Acharacter%20animations%2C%20responding%20in%20real-time%20to%20a%20variety%20of%20dynamic%0Auser-supplied%20control%20signals.%20At%20the%20heart%20of%20our%20method%20lies%20a%0Atransformer-based%20Conditional%20Autoregressive%20Motion%20Diffusion%20Model%20%28CAMDM%29%2C%0Awhich%20takes%20as%20input%20the%20character%27s%20historical%20motion%20and%20can%20generate%20a%20range%0Aof%20diverse%20potential%20future%20motions%20conditioned%20on%20high-level%2C%20coarse%20user%0Acontrol.%20To%20meet%20the%20demands%20for%20diversity%2C%20controllability%2C%20and%20computational%0Aefficiency%20required%20by%20a%20real-time%20controller%2C%20we%20incorporate%20several%20key%0Aalgorithmic%20designs.%20These%20include%20separate%20condition%20tokenization%2C%0Aclassifier-free%20guidance%20on%20past%20motion%2C%20and%20heuristic%20future%20trajectory%0Aextension%2C%20all%20designed%20to%20address%20the%20challenges%20associated%20with%20taming%20motion%0Adiffusion%20probabilistic%20models%20for%20character%20control.%20As%20a%20result%2C%20our%20work%0Arepresents%20the%20first%20model%20that%20enables%20real-time%20generation%20of%20high-quality%2C%0Adiverse%20character%20animations%20based%20on%20user%20interactive%20control%2C%20supporting%0Aanimating%20the%20character%20in%20multiple%20styles%20with%20a%20single%20unified%20model.%20We%0Aevaluate%20our%20method%20on%20a%20diverse%20set%20of%20locomotion%20skills%2C%20demonstrating%20the%0Amerits%20of%20our%20method%20over%20existing%20character%20controllers.%20Project%20page%20and%0Asource%20codes%3A%20https%3A//aiganimation.github.io/CAMDM/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15121v1&entry.124074799=Read"},
{"title": "Computational Dualism and Objective Superintelligence", "author": "Michael Timothy Bennett", "abstract": "  The concept of intelligent software is flawed. The behaviour of software\ndepends upon the hardware that interprets it. This undermines claims regarding\nthe behaviour of theorised, software superintelligence. Here we characterise\nthis problem as ``computational dualism'', where instead of mental and physical\nsubstance, we have software and hardware. We argue that to make objective\nclaims regarding performance we must avoid computational dualism. We propose\nusing an alternative based upon pancomputationalism, wherein every aspect of\nthe environment is a relation between irreducible states. We formalise systems\nas behaviour (inputs and outputs), and cognition as embodied, embedded,\nextended and enactive. The result is cognition formalised as a part of the\nenvironment, rather than as a disembodied policy interacting with the\nenvironment though an interpreter. This allows us to make objective claims\nregarding intelligence, which we argue is the ability to ``generalise'',\nidentify causes and adapt. We then propose objective upper bounds for\nintelligent behaviour.\n", "link": "http://arxiv.org/abs/2302.00843v4", "date": "2024-04-23", "relevancy": 1.67, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4663}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4118}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4037}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Computational%20Dualism%20and%20Objective%20Superintelligence&body=Title%3A%20Computational%20Dualism%20and%20Objective%20Superintelligence%0AAuthor%3A%20Michael%20Timothy%20Bennett%0AAbstract%3A%20%20%20The%20concept%20of%20intelligent%20software%20is%20flawed.%20The%20behaviour%20of%20software%0Adepends%20upon%20the%20hardware%20that%20interprets%20it.%20This%20undermines%20claims%20regarding%0Athe%20behaviour%20of%20theorised%2C%20software%20superintelligence.%20Here%20we%20characterise%0Athis%20problem%20as%20%60%60computational%20dualism%27%27%2C%20where%20instead%20of%20mental%20and%20physical%0Asubstance%2C%20we%20have%20software%20and%20hardware.%20We%20argue%20that%20to%20make%20objective%0Aclaims%20regarding%20performance%20we%20must%20avoid%20computational%20dualism.%20We%20propose%0Ausing%20an%20alternative%20based%20upon%20pancomputationalism%2C%20wherein%20every%20aspect%20of%0Athe%20environment%20is%20a%20relation%20between%20irreducible%20states.%20We%20formalise%20systems%0Aas%20behaviour%20%28inputs%20and%20outputs%29%2C%20and%20cognition%20as%20embodied%2C%20embedded%2C%0Aextended%20and%20enactive.%20The%20result%20is%20cognition%20formalised%20as%20a%20part%20of%20the%0Aenvironment%2C%20rather%20than%20as%20a%20disembodied%20policy%20interacting%20with%20the%0Aenvironment%20though%20an%20interpreter.%20This%20allows%20us%20to%20make%20objective%20claims%0Aregarding%20intelligence%2C%20which%20we%20argue%20is%20the%20ability%20to%20%60%60generalise%27%27%2C%0Aidentify%20causes%20and%20adapt.%20We%20then%20propose%20objective%20upper%20bounds%20for%0Aintelligent%20behaviour.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.00843v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20Dualism%20and%20Objective%20Superintelligence&entry.906535625=Michael%20Timothy%20Bennett&entry.1292438233=%20%20The%20concept%20of%20intelligent%20software%20is%20flawed.%20The%20behaviour%20of%20software%0Adepends%20upon%20the%20hardware%20that%20interprets%20it.%20This%20undermines%20claims%20regarding%0Athe%20behaviour%20of%20theorised%2C%20software%20superintelligence.%20Here%20we%20characterise%0Athis%20problem%20as%20%60%60computational%20dualism%27%27%2C%20where%20instead%20of%20mental%20and%20physical%0Asubstance%2C%20we%20have%20software%20and%20hardware.%20We%20argue%20that%20to%20make%20objective%0Aclaims%20regarding%20performance%20we%20must%20avoid%20computational%20dualism.%20We%20propose%0Ausing%20an%20alternative%20based%20upon%20pancomputationalism%2C%20wherein%20every%20aspect%20of%0Athe%20environment%20is%20a%20relation%20between%20irreducible%20states.%20We%20formalise%20systems%0Aas%20behaviour%20%28inputs%20and%20outputs%29%2C%20and%20cognition%20as%20embodied%2C%20embedded%2C%0Aextended%20and%20enactive.%20The%20result%20is%20cognition%20formalised%20as%20a%20part%20of%20the%0Aenvironment%2C%20rather%20than%20as%20a%20disembodied%20policy%20interacting%20with%20the%0Aenvironment%20though%20an%20interpreter.%20This%20allows%20us%20to%20make%20objective%20claims%0Aregarding%20intelligence%2C%20which%20we%20argue%20is%20the%20ability%20to%20%60%60generalise%27%27%2C%0Aidentify%20causes%20and%20adapt.%20We%20then%20propose%20objective%20upper%20bounds%20for%0Aintelligent%20behaviour.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.00843v4&entry.124074799=Read"},
{"title": "From Parts to Whole: A Unified Reference Framework for Controllable\n  Human Image Generation", "author": "Zehuan Huang and Hongxing Fan and Lipeng Wang and Lu Sheng", "abstract": "  Recent advancements in controllable human image generation have led to\nzero-shot generation using structural signals (e.g., pose, depth) or facial\nappearance. Yet, generating human images conditioned on multiple parts of human\nappearance remains challenging. Addressing this, we introduce Parts2Whole, a\nnovel framework designed for generating customized portraits from multiple\nreference images, including pose images and various aspects of human\nappearance. To achieve this, we first develop a semantic-aware appearance\nencoder to retain details of different human parts, which processes each image\nbased on its textual label to a series of multi-scale feature maps rather than\none image token, preserving the image dimension. Second, our framework supports\nmulti-image conditioned generation through a shared self-attention mechanism\nthat operates across reference and target features during the diffusion\nprocess. We enhance the vanilla attention mechanism by incorporating mask\ninformation from the reference human images, allowing for the precise selection\nof any part. Extensive experiments demonstrate the superiority of our approach\nover existing alternatives, offering advanced capabilities for multi-part\ncontrollable human image customization. See our project page at\nhttps://huanngzh.github.io/Parts2Whole/.\n", "link": "http://arxiv.org/abs/2404.15267v1", "date": "2024-04-23", "relevancy": 1.8333, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6497}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6073}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5821}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20Parts%20to%20Whole%3A%20A%20Unified%20Reference%20Framework%20for%20Controllable%0A%20%20Human%20Image%20Generation&body=Title%3A%20From%20Parts%20to%20Whole%3A%20A%20Unified%20Reference%20Framework%20for%20Controllable%0A%20%20Human%20Image%20Generation%0AAuthor%3A%20Zehuan%20Huang%20and%20Hongxing%20Fan%20and%20Lipeng%20Wang%20and%20Lu%20Sheng%0AAbstract%3A%20%20%20Recent%20advancements%20in%20controllable%20human%20image%20generation%20have%20led%20to%0Azero-shot%20generation%20using%20structural%20signals%20%28e.g.%2C%20pose%2C%20depth%29%20or%20facial%0Aappearance.%20Yet%2C%20generating%20human%20images%20conditioned%20on%20multiple%20parts%20of%20human%0Aappearance%20remains%20challenging.%20Addressing%20this%2C%20we%20introduce%20Parts2Whole%2C%20a%0Anovel%20framework%20designed%20for%20generating%20customized%20portraits%20from%20multiple%0Areference%20images%2C%20including%20pose%20images%20and%20various%20aspects%20of%20human%0Aappearance.%20To%20achieve%20this%2C%20we%20first%20develop%20a%20semantic-aware%20appearance%0Aencoder%20to%20retain%20details%20of%20different%20human%20parts%2C%20which%20processes%20each%20image%0Abased%20on%20its%20textual%20label%20to%20a%20series%20of%20multi-scale%20feature%20maps%20rather%20than%0Aone%20image%20token%2C%20preserving%20the%20image%20dimension.%20Second%2C%20our%20framework%20supports%0Amulti-image%20conditioned%20generation%20through%20a%20shared%20self-attention%20mechanism%0Athat%20operates%20across%20reference%20and%20target%20features%20during%20the%20diffusion%0Aprocess.%20We%20enhance%20the%20vanilla%20attention%20mechanism%20by%20incorporating%20mask%0Ainformation%20from%20the%20reference%20human%20images%2C%20allowing%20for%20the%20precise%20selection%0Aof%20any%20part.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20approach%0Aover%20existing%20alternatives%2C%20offering%20advanced%20capabilities%20for%20multi-part%0Acontrollable%20human%20image%20customization.%20See%20our%20project%20page%20at%0Ahttps%3A//huanngzh.github.io/Parts2Whole/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15267v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Parts%20to%20Whole%3A%20A%20Unified%20Reference%20Framework%20for%20Controllable%0A%20%20Human%20Image%20Generation&entry.906535625=Zehuan%20Huang%20and%20Hongxing%20Fan%20and%20Lipeng%20Wang%20and%20Lu%20Sheng&entry.1292438233=%20%20Recent%20advancements%20in%20controllable%20human%20image%20generation%20have%20led%20to%0Azero-shot%20generation%20using%20structural%20signals%20%28e.g.%2C%20pose%2C%20depth%29%20or%20facial%0Aappearance.%20Yet%2C%20generating%20human%20images%20conditioned%20on%20multiple%20parts%20of%20human%0Aappearance%20remains%20challenging.%20Addressing%20this%2C%20we%20introduce%20Parts2Whole%2C%20a%0Anovel%20framework%20designed%20for%20generating%20customized%20portraits%20from%20multiple%0Areference%20images%2C%20including%20pose%20images%20and%20various%20aspects%20of%20human%0Aappearance.%20To%20achieve%20this%2C%20we%20first%20develop%20a%20semantic-aware%20appearance%0Aencoder%20to%20retain%20details%20of%20different%20human%20parts%2C%20which%20processes%20each%20image%0Abased%20on%20its%20textual%20label%20to%20a%20series%20of%20multi-scale%20feature%20maps%20rather%20than%0Aone%20image%20token%2C%20preserving%20the%20image%20dimension.%20Second%2C%20our%20framework%20supports%0Amulti-image%20conditioned%20generation%20through%20a%20shared%20self-attention%20mechanism%0Athat%20operates%20across%20reference%20and%20target%20features%20during%20the%20diffusion%0Aprocess.%20We%20enhance%20the%20vanilla%20attention%20mechanism%20by%20incorporating%20mask%0Ainformation%20from%20the%20reference%20human%20images%2C%20allowing%20for%20the%20precise%20selection%0Aof%20any%20part.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20our%20approach%0Aover%20existing%20alternatives%2C%20offering%20advanced%20capabilities%20for%20multi-part%0Acontrollable%20human%20image%20customization.%20See%20our%20project%20page%20at%0Ahttps%3A//huanngzh.github.io/Parts2Whole/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15267v1&entry.124074799=Read"},
{"title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed\n  according to the U.S. Executive Order", "author": "Taishi Nakamura and Mayank Mishra and Simone Tedeschi and Yekun Chai and Jason T Stillerman and Felix Friedrich and Prateek Yadav and Tanmay Laud and Vu Minh Chien and Terry Yue Zhuo and Diganta Misra and Ben Bogin and Xuan-Son Vu and Marzena Karpinska and Arnav Varma Dantuluri and Wojciech Kusa and Tommaso Furlanello and Rio Yokota and Niklas Muennighoff and Suhas Pai and Tosin Adewumi and Veronika Laippala and Xiaozhe Yao and Adalberto Junior and Alpay Ariyak and Aleksandr Drozd and Jordan Clive and Kshitij Gupta and Liangyu Chen and Qi Sun and Ken Tsui and Noah Persaud and Nour Fahmy and Tianlong Chen and Mohit Bansal and Nicolo Monti and Tai Dang and Ziyang Luo and Tien-Tung Bui and Roberto Navigli and Virendra Mehta and Matthew Blumberg and Victor May and Huu Nguyen and Sampo Pyysalo", "abstract": "  Pretrained language models underpin several AI applications, but their high\ncomputational cost for training limits accessibility. Initiatives such as BLOOM\nand StarCoder aim to democratize access to pretrained models for collaborative\ncommunity development. However, such existing models face challenges: limited\nmultilingual capabilities, continual pretraining causing catastrophic\nforgetting, whereas pretraining from scratch is computationally expensive, and\ncompliance with AI safety and development laws. This paper presents Aurora-M, a\n15B parameter multilingual open-source model trained on English, Finnish,\nHindi, Japanese, Vietnamese, and code. Continually pretrained from\nStarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion\ntokens in total training token count. It is the first open-source multilingual\nmodel fine-tuned on human-reviewed safety instructions, thus aligning its\ndevelopment not only with conventional red-teaming considerations, but also\nwith the specific concerns articulated in the Biden-Harris Executive Order on\nthe Safe, Secure, and Trustworthy Development and Use of Artificial\nIntelligence. Aurora-M is rigorously evaluated across various tasks and\nlanguages, demonstrating robustness against catastrophic forgetting and\noutperforming alternatives in multilingual settings, particularly in safety\nevaluations. To promote responsible open-source LLM development, Aurora-M and\nits variants are released at\nhttps://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 .\n", "link": "http://arxiv.org/abs/2404.00399v2", "date": "2024-04-23", "relevancy": 1.7898, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4648}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4534}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4346}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Aurora-M%3A%20The%20First%20Open%20Source%20Multilingual%20Language%20Model%20Red-teamed%0A%20%20according%20to%20the%20U.S.%20Executive%20Order&body=Title%3A%20Aurora-M%3A%20The%20First%20Open%20Source%20Multilingual%20Language%20Model%20Red-teamed%0A%20%20according%20to%20the%20U.S.%20Executive%20Order%0AAuthor%3A%20Taishi%20Nakamura%20and%20Mayank%20Mishra%20and%20Simone%20Tedeschi%20and%20Yekun%20Chai%20and%20Jason%20T%20Stillerman%20and%20Felix%20Friedrich%20and%20Prateek%20Yadav%20and%20Tanmay%20Laud%20and%20Vu%20Minh%20Chien%20and%20Terry%20Yue%20Zhuo%20and%20Diganta%20Misra%20and%20Ben%20Bogin%20and%20Xuan-Son%20Vu%20and%20Marzena%20Karpinska%20and%20Arnav%20Varma%20Dantuluri%20and%20Wojciech%20Kusa%20and%20Tommaso%20Furlanello%20and%20Rio%20Yokota%20and%20Niklas%20Muennighoff%20and%20Suhas%20Pai%20and%20Tosin%20Adewumi%20and%20Veronika%20Laippala%20and%20Xiaozhe%20Yao%20and%20Adalberto%20Junior%20and%20Alpay%20Ariyak%20and%20Aleksandr%20Drozd%20and%20Jordan%20Clive%20and%20Kshitij%20Gupta%20and%20Liangyu%20Chen%20and%20Qi%20Sun%20and%20Ken%20Tsui%20and%20Noah%20Persaud%20and%20Nour%20Fahmy%20and%20Tianlong%20Chen%20and%20Mohit%20Bansal%20and%20Nicolo%20Monti%20and%20Tai%20Dang%20and%20Ziyang%20Luo%20and%20Tien-Tung%20Bui%20and%20Roberto%20Navigli%20and%20Virendra%20Mehta%20and%20Matthew%20Blumberg%20and%20Victor%20May%20and%20Huu%20Nguyen%20and%20Sampo%20Pyysalo%0AAbstract%3A%20%20%20Pretrained%20language%20models%20underpin%20several%20AI%20applications%2C%20but%20their%20high%0Acomputational%20cost%20for%20training%20limits%20accessibility.%20Initiatives%20such%20as%20BLOOM%0Aand%20StarCoder%20aim%20to%20democratize%20access%20to%20pretrained%20models%20for%20collaborative%0Acommunity%20development.%20However%2C%20such%20existing%20models%20face%20challenges%3A%20limited%0Amultilingual%20capabilities%2C%20continual%20pretraining%20causing%20catastrophic%0Aforgetting%2C%20whereas%20pretraining%20from%20scratch%20is%20computationally%20expensive%2C%20and%0Acompliance%20with%20AI%20safety%20and%20development%20laws.%20This%20paper%20presents%20Aurora-M%2C%20a%0A15B%20parameter%20multilingual%20open-source%20model%20trained%20on%20English%2C%20Finnish%2C%0AHindi%2C%20Japanese%2C%20Vietnamese%2C%20and%20code.%20Continually%20pretrained%20from%0AStarCoderPlus%20on%20435%20billion%20additional%20tokens%2C%20Aurora-M%20surpasses%202%20trillion%0Atokens%20in%20total%20training%20token%20count.%20It%20is%20the%20first%20open-source%20multilingual%0Amodel%20fine-tuned%20on%20human-reviewed%20safety%20instructions%2C%20thus%20aligning%20its%0Adevelopment%20not%20only%20with%20conventional%20red-teaming%20considerations%2C%20but%20also%0Awith%20the%20specific%20concerns%20articulated%20in%20the%20Biden-Harris%20Executive%20Order%20on%0Athe%20Safe%2C%20Secure%2C%20and%20Trustworthy%20Development%20and%20Use%20of%20Artificial%0AIntelligence.%20Aurora-M%20is%20rigorously%20evaluated%20across%20various%20tasks%20and%0Alanguages%2C%20demonstrating%20robustness%20against%20catastrophic%20forgetting%20and%0Aoutperforming%20alternatives%20in%20multilingual%20settings%2C%20particularly%20in%20safety%0Aevaluations.%20To%20promote%20responsible%20open-source%20LLM%20development%2C%20Aurora-M%20and%0Aits%20variants%20are%20released%20at%0Ahttps%3A//huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00399v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aurora-M%3A%20The%20First%20Open%20Source%20Multilingual%20Language%20Model%20Red-teamed%0A%20%20according%20to%20the%20U.S.%20Executive%20Order&entry.906535625=Taishi%20Nakamura%20and%20Mayank%20Mishra%20and%20Simone%20Tedeschi%20and%20Yekun%20Chai%20and%20Jason%20T%20Stillerman%20and%20Felix%20Friedrich%20and%20Prateek%20Yadav%20and%20Tanmay%20Laud%20and%20Vu%20Minh%20Chien%20and%20Terry%20Yue%20Zhuo%20and%20Diganta%20Misra%20and%20Ben%20Bogin%20and%20Xuan-Son%20Vu%20and%20Marzena%20Karpinska%20and%20Arnav%20Varma%20Dantuluri%20and%20Wojciech%20Kusa%20and%20Tommaso%20Furlanello%20and%20Rio%20Yokota%20and%20Niklas%20Muennighoff%20and%20Suhas%20Pai%20and%20Tosin%20Adewumi%20and%20Veronika%20Laippala%20and%20Xiaozhe%20Yao%20and%20Adalberto%20Junior%20and%20Alpay%20Ariyak%20and%20Aleksandr%20Drozd%20and%20Jordan%20Clive%20and%20Kshitij%20Gupta%20and%20Liangyu%20Chen%20and%20Qi%20Sun%20and%20Ken%20Tsui%20and%20Noah%20Persaud%20and%20Nour%20Fahmy%20and%20Tianlong%20Chen%20and%20Mohit%20Bansal%20and%20Nicolo%20Monti%20and%20Tai%20Dang%20and%20Ziyang%20Luo%20and%20Tien-Tung%20Bui%20and%20Roberto%20Navigli%20and%20Virendra%20Mehta%20and%20Matthew%20Blumberg%20and%20Victor%20May%20and%20Huu%20Nguyen%20and%20Sampo%20Pyysalo&entry.1292438233=%20%20Pretrained%20language%20models%20underpin%20several%20AI%20applications%2C%20but%20their%20high%0Acomputational%20cost%20for%20training%20limits%20accessibility.%20Initiatives%20such%20as%20BLOOM%0Aand%20StarCoder%20aim%20to%20democratize%20access%20to%20pretrained%20models%20for%20collaborative%0Acommunity%20development.%20However%2C%20such%20existing%20models%20face%20challenges%3A%20limited%0Amultilingual%20capabilities%2C%20continual%20pretraining%20causing%20catastrophic%0Aforgetting%2C%20whereas%20pretraining%20from%20scratch%20is%20computationally%20expensive%2C%20and%0Acompliance%20with%20AI%20safety%20and%20development%20laws.%20This%20paper%20presents%20Aurora-M%2C%20a%0A15B%20parameter%20multilingual%20open-source%20model%20trained%20on%20English%2C%20Finnish%2C%0AHindi%2C%20Japanese%2C%20Vietnamese%2C%20and%20code.%20Continually%20pretrained%20from%0AStarCoderPlus%20on%20435%20billion%20additional%20tokens%2C%20Aurora-M%20surpasses%202%20trillion%0Atokens%20in%20total%20training%20token%20count.%20It%20is%20the%20first%20open-source%20multilingual%0Amodel%20fine-tuned%20on%20human-reviewed%20safety%20instructions%2C%20thus%20aligning%20its%0Adevelopment%20not%20only%20with%20conventional%20red-teaming%20considerations%2C%20but%20also%0Awith%20the%20specific%20concerns%20articulated%20in%20the%20Biden-Harris%20Executive%20Order%20on%0Athe%20Safe%2C%20Secure%2C%20and%20Trustworthy%20Development%20and%20Use%20of%20Artificial%0AIntelligence.%20Aurora-M%20is%20rigorously%20evaluated%20across%20various%20tasks%20and%0Alanguages%2C%20demonstrating%20robustness%20against%20catastrophic%20forgetting%20and%0Aoutperforming%20alternatives%20in%20multilingual%20settings%2C%20particularly%20in%20safety%0Aevaluations.%20To%20promote%20responsible%20open-source%20LLM%20development%2C%20Aurora-M%20and%0Aits%20variants%20are%20released%20at%0Ahttps%3A//huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00399v2&entry.124074799=Read"},
{"title": "Harnessing Optical Imaging Limit through Atmospheric Scattering Media", "author": "Libang Chen and Jun Yang and Lingye Chen and Yuyang Shui and Yikun Liu and Jianying Zhou", "abstract": "  Recording and identifying faint objects through atmospheric scattering media\nby an optical system are fundamentally interesting and technologically\nimportant. In this work, we introduce a comprehensive model that incorporates\ncontributions from target characteristics, atmospheric effects, imaging system,\ndigital processing, and visual perception to assess the ultimate perceptible\nlimit of geometrical imaging, specifically the angular resolution at the\nboundary of visible distance. The model allows to reevaluate the effectiveness\nof conventional imaging recording, processing, and perception and to analyze\nthe limiting factors that constrain image recognition capabilities in\natmospheric media. The simulations were compared with the experimental results\nmeasured in a fog chamber and outdoor settings. The results reveal general good\nagreement between analysis and experimental, pointing out the way to harnessing\nthe physical limit for optical imaging in scattering media. An immediate\napplication of the study is the extension of the image range by an amount of\n1.2 times with noise reduction via multi-frame averaging, hence greatly\nenhancing the capability of optical imaging in the atmosphere.\n", "link": "http://arxiv.org/abs/2404.15082v1", "date": "2024-04-23", "relevancy": 1.4046, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4809}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4656}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4621}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Optical%20Imaging%20Limit%20through%20Atmospheric%20Scattering%20Media&body=Title%3A%20Harnessing%20Optical%20Imaging%20Limit%20through%20Atmospheric%20Scattering%20Media%0AAuthor%3A%20Libang%20Chen%20and%20Jun%20Yang%20and%20Lingye%20Chen%20and%20Yuyang%20Shui%20and%20Yikun%20Liu%20and%20Jianying%20Zhou%0AAbstract%3A%20%20%20Recording%20and%20identifying%20faint%20objects%20through%20atmospheric%20scattering%20media%0Aby%20an%20optical%20system%20are%20fundamentally%20interesting%20and%20technologically%0Aimportant.%20In%20this%20work%2C%20we%20introduce%20a%20comprehensive%20model%20that%20incorporates%0Acontributions%20from%20target%20characteristics%2C%20atmospheric%20effects%2C%20imaging%20system%2C%0Adigital%20processing%2C%20and%20visual%20perception%20to%20assess%20the%20ultimate%20perceptible%0Alimit%20of%20geometrical%20imaging%2C%20specifically%20the%20angular%20resolution%20at%20the%0Aboundary%20of%20visible%20distance.%20The%20model%20allows%20to%20reevaluate%20the%20effectiveness%0Aof%20conventional%20imaging%20recording%2C%20processing%2C%20and%20perception%20and%20to%20analyze%0Athe%20limiting%20factors%20that%20constrain%20image%20recognition%20capabilities%20in%0Aatmospheric%20media.%20The%20simulations%20were%20compared%20with%20the%20experimental%20results%0Ameasured%20in%20a%20fog%20chamber%20and%20outdoor%20settings.%20The%20results%20reveal%20general%20good%0Aagreement%20between%20analysis%20and%20experimental%2C%20pointing%20out%20the%20way%20to%20harnessing%0Athe%20physical%20limit%20for%20optical%20imaging%20in%20scattering%20media.%20An%20immediate%0Aapplication%20of%20the%20study%20is%20the%20extension%20of%20the%20image%20range%20by%20an%20amount%20of%0A1.2%20times%20with%20noise%20reduction%20via%20multi-frame%20averaging%2C%20hence%20greatly%0Aenhancing%20the%20capability%20of%20optical%20imaging%20in%20the%20atmosphere.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15082v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Optical%20Imaging%20Limit%20through%20Atmospheric%20Scattering%20Media&entry.906535625=Libang%20Chen%20and%20Jun%20Yang%20and%20Lingye%20Chen%20and%20Yuyang%20Shui%20and%20Yikun%20Liu%20and%20Jianying%20Zhou&entry.1292438233=%20%20Recording%20and%20identifying%20faint%20objects%20through%20atmospheric%20scattering%20media%0Aby%20an%20optical%20system%20are%20fundamentally%20interesting%20and%20technologically%0Aimportant.%20In%20this%20work%2C%20we%20introduce%20a%20comprehensive%20model%20that%20incorporates%0Acontributions%20from%20target%20characteristics%2C%20atmospheric%20effects%2C%20imaging%20system%2C%0Adigital%20processing%2C%20and%20visual%20perception%20to%20assess%20the%20ultimate%20perceptible%0Alimit%20of%20geometrical%20imaging%2C%20specifically%20the%20angular%20resolution%20at%20the%0Aboundary%20of%20visible%20distance.%20The%20model%20allows%20to%20reevaluate%20the%20effectiveness%0Aof%20conventional%20imaging%20recording%2C%20processing%2C%20and%20perception%20and%20to%20analyze%0Athe%20limiting%20factors%20that%20constrain%20image%20recognition%20capabilities%20in%0Aatmospheric%20media.%20The%20simulations%20were%20compared%20with%20the%20experimental%20results%0Ameasured%20in%20a%20fog%20chamber%20and%20outdoor%20settings.%20The%20results%20reveal%20general%20good%0Aagreement%20between%20analysis%20and%20experimental%2C%20pointing%20out%20the%20way%20to%20harnessing%0Athe%20physical%20limit%20for%20optical%20imaging%20in%20scattering%20media.%20An%20immediate%0Aapplication%20of%20the%20study%20is%20the%20extension%20of%20the%20image%20range%20by%20an%20amount%20of%0A1.2%20times%20with%20noise%20reduction%20via%20multi-frame%20averaging%2C%20hence%20greatly%0Aenhancing%20the%20capability%20of%20optical%20imaging%20in%20the%20atmosphere.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15082v1&entry.124074799=Read"},
{"title": "SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose\n  Estimation", "author": "Xiangyu Xu and Lijuan Liu and Shuicheng Yan", "abstract": "  Existing Transformers for monocular 3D human shape and pose estimation\ntypically have a quadratic computation and memory complexity with respect to\nthe feature length, which hinders the exploitation of fine-grained information\nin high-resolution features that is beneficial for accurate reconstruction. In\nthis work, we propose an SMPL-based Transformer framework (SMPLer) to address\nthis issue. SMPLer incorporates two key ingredients: a decoupled attention\noperation and an SMPL-based target representation, which allow effective\nutilization of high-resolution features in the Transformer. In addition, based\non these two designs, we also introduce several novel modules including a\nmulti-scale attention and a joint-aware attention to further boost the\nreconstruction performance. Extensive experiments demonstrate the effectiveness\nof SMPLer against existing 3D human shape and pose estimation methods both\nquantitatively and qualitatively. Notably, the proposed algorithm achieves an\nMPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by\nmore than 10% with fewer than one-third of the parameters. Code and pretrained\nmodels are available at https://github.com/xuxy09/SMPLer.\n", "link": "http://arxiv.org/abs/2404.15276v1", "date": "2024-04-23", "relevancy": 1.765, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6162}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5623}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5447}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SMPLer%3A%20Taming%20Transformers%20for%20Monocular%203D%20Human%20Shape%20and%20Pose%0A%20%20Estimation&body=Title%3A%20SMPLer%3A%20Taming%20Transformers%20for%20Monocular%203D%20Human%20Shape%20and%20Pose%0A%20%20Estimation%0AAuthor%3A%20Xiangyu%20Xu%20and%20Lijuan%20Liu%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Existing%20Transformers%20for%20monocular%203D%20human%20shape%20and%20pose%20estimation%0Atypically%20have%20a%20quadratic%20computation%20and%20memory%20complexity%20with%20respect%20to%0Athe%20feature%20length%2C%20which%20hinders%20the%20exploitation%20of%20fine-grained%20information%0Ain%20high-resolution%20features%20that%20is%20beneficial%20for%20accurate%20reconstruction.%20In%0Athis%20work%2C%20we%20propose%20an%20SMPL-based%20Transformer%20framework%20%28SMPLer%29%20to%20address%0Athis%20issue.%20SMPLer%20incorporates%20two%20key%20ingredients%3A%20a%20decoupled%20attention%0Aoperation%20and%20an%20SMPL-based%20target%20representation%2C%20which%20allow%20effective%0Autilization%20of%20high-resolution%20features%20in%20the%20Transformer.%20In%20addition%2C%20based%0Aon%20these%20two%20designs%2C%20we%20also%20introduce%20several%20novel%20modules%20including%20a%0Amulti-scale%20attention%20and%20a%20joint-aware%20attention%20to%20further%20boost%20the%0Areconstruction%20performance.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20SMPLer%20against%20existing%203D%20human%20shape%20and%20pose%20estimation%20methods%20both%0Aquantitatively%20and%20qualitatively.%20Notably%2C%20the%20proposed%20algorithm%20achieves%20an%0AMPJPE%20of%2045.2%20mm%20on%20the%20Human3.6M%20dataset%2C%20improving%20upon%20Mesh%20Graphormer%20by%0Amore%20than%2010%25%20with%20fewer%20than%20one-third%20of%20the%20parameters.%20Code%20and%20pretrained%0Amodels%20are%20available%20at%20https%3A//github.com/xuxy09/SMPLer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15276v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMPLer%3A%20Taming%20Transformers%20for%20Monocular%203D%20Human%20Shape%20and%20Pose%0A%20%20Estimation&entry.906535625=Xiangyu%20Xu%20and%20Lijuan%20Liu%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Existing%20Transformers%20for%20monocular%203D%20human%20shape%20and%20pose%20estimation%0Atypically%20have%20a%20quadratic%20computation%20and%20memory%20complexity%20with%20respect%20to%0Athe%20feature%20length%2C%20which%20hinders%20the%20exploitation%20of%20fine-grained%20information%0Ain%20high-resolution%20features%20that%20is%20beneficial%20for%20accurate%20reconstruction.%20In%0Athis%20work%2C%20we%20propose%20an%20SMPL-based%20Transformer%20framework%20%28SMPLer%29%20to%20address%0Athis%20issue.%20SMPLer%20incorporates%20two%20key%20ingredients%3A%20a%20decoupled%20attention%0Aoperation%20and%20an%20SMPL-based%20target%20representation%2C%20which%20allow%20effective%0Autilization%20of%20high-resolution%20features%20in%20the%20Transformer.%20In%20addition%2C%20based%0Aon%20these%20two%20designs%2C%20we%20also%20introduce%20several%20novel%20modules%20including%20a%0Amulti-scale%20attention%20and%20a%20joint-aware%20attention%20to%20further%20boost%20the%0Areconstruction%20performance.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20SMPLer%20against%20existing%203D%20human%20shape%20and%20pose%20estimation%20methods%20both%0Aquantitatively%20and%20qualitatively.%20Notably%2C%20the%20proposed%20algorithm%20achieves%20an%0AMPJPE%20of%2045.2%20mm%20on%20the%20Human3.6M%20dataset%2C%20improving%20upon%20Mesh%20Graphormer%20by%0Amore%20than%2010%25%20with%20fewer%20than%20one-third%20of%20the%20parameters.%20Code%20and%20pretrained%0Amodels%20are%20available%20at%20https%3A//github.com/xuxy09/SMPLer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15276v1&entry.124074799=Read"},
{"title": "Using ARIMA to Predict the Expansion of Subscriber Data Consumption", "author": "Mike Wa Nkongolo", "abstract": "  This study discusses how insights retrieved from subscriber data can impact\ndecision-making in telecommunications, focusing on predictive modeling using\nmachine learning techniques such as the ARIMA model. The study explores time\nseries forecasting to predict subscriber usage trends, evaluating the ARIMA\nmodel's performance using various metrics. It also compares ARIMA with\nConvolutional Neural Network (CNN) models, highlighting ARIMA's superiority in\naccuracy and execution speed. The study suggests future directions for\nresearch, including exploring additional forecasting models and considering\nother factors affecting subscriber data usage.\n", "link": "http://arxiv.org/abs/2404.15095v1", "date": "2024-04-23", "relevancy": 0.7283, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3699}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.358}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Using%20ARIMA%20to%20Predict%20the%20Expansion%20of%20Subscriber%20Data%20Consumption&body=Title%3A%20Using%20ARIMA%20to%20Predict%20the%20Expansion%20of%20Subscriber%20Data%20Consumption%0AAuthor%3A%20Mike%20Wa%20Nkongolo%0AAbstract%3A%20%20%20This%20study%20discusses%20how%20insights%20retrieved%20from%20subscriber%20data%20can%20impact%0Adecision-making%20in%20telecommunications%2C%20focusing%20on%20predictive%20modeling%20using%0Amachine%20learning%20techniques%20such%20as%20the%20ARIMA%20model.%20The%20study%20explores%20time%0Aseries%20forecasting%20to%20predict%20subscriber%20usage%20trends%2C%20evaluating%20the%20ARIMA%0Amodel%27s%20performance%20using%20various%20metrics.%20It%20also%20compares%20ARIMA%20with%0AConvolutional%20Neural%20Network%20%28CNN%29%20models%2C%20highlighting%20ARIMA%27s%20superiority%20in%0Aaccuracy%20and%20execution%20speed.%20The%20study%20suggests%20future%20directions%20for%0Aresearch%2C%20including%20exploring%20additional%20forecasting%20models%20and%20considering%0Aother%20factors%20affecting%20subscriber%20data%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15095v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20ARIMA%20to%20Predict%20the%20Expansion%20of%20Subscriber%20Data%20Consumption&entry.906535625=Mike%20Wa%20Nkongolo&entry.1292438233=%20%20This%20study%20discusses%20how%20insights%20retrieved%20from%20subscriber%20data%20can%20impact%0Adecision-making%20in%20telecommunications%2C%20focusing%20on%20predictive%20modeling%20using%0Amachine%20learning%20techniques%20such%20as%20the%20ARIMA%20model.%20The%20study%20explores%20time%0Aseries%20forecasting%20to%20predict%20subscriber%20usage%20trends%2C%20evaluating%20the%20ARIMA%0Amodel%27s%20performance%20using%20various%20metrics.%20It%20also%20compares%20ARIMA%20with%0AConvolutional%20Neural%20Network%20%28CNN%29%20models%2C%20highlighting%20ARIMA%27s%20superiority%20in%0Aaccuracy%20and%20execution%20speed.%20The%20study%20suggests%20future%20directions%20for%0Aresearch%2C%20including%20exploring%20additional%20forecasting%20models%20and%20considering%0Aother%20factors%20affecting%20subscriber%20data%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15095v1&entry.124074799=Read"},
{"title": "Neural Graph Generator: Feature-Conditioned Graph Generation using\n  Latent Diffusion Models", "author": "Iakovos Evdaimon and Giannis Nikolentzos and Michail Chatzianastasis and Hadi Abdine and Michalis Vazirgiannis", "abstract": "  Graph generation has emerged as a crucial task in machine learning, with\nsignificant challenges in generating graphs that accurately reflect specific\nproperties. Existing methods often fall short in efficiently addressing this\nneed as they struggle with the high-dimensional complexity and varied nature of\ngraph properties. In this paper, we introduce the Neural Graph Generator (NGG),\na novel approach which utilizes conditioned latent diffusion models for graph\ngeneration. NGG demonstrates a remarkable capacity to model complex graph\npatterns, offering control over the graph generation process. NGG employs a\nvariational graph autoencoder for graph compression and a diffusion process in\nthe latent vector space, guided by vectors summarizing graph statistics. We\ndemonstrate NGG's versatility across various graph generation tasks, showing\nits capability to capture desired graph properties and generalize to unseen\ngraphs. This work signifies a significant shift in graph generation\nmethodologies, offering a more practical and efficient solution for generating\ndiverse types of graphs with specific characteristics.\n", "link": "http://arxiv.org/abs/2403.01535v2", "date": "2024-04-23", "relevancy": 1.6598, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5792}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.554}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5426}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Graph%20Generator%3A%20Feature-Conditioned%20Graph%20Generation%20using%0A%20%20Latent%20Diffusion%20Models&body=Title%3A%20Neural%20Graph%20Generator%3A%20Feature-Conditioned%20Graph%20Generation%20using%0A%20%20Latent%20Diffusion%20Models%0AAuthor%3A%20Iakovos%20Evdaimon%20and%20Giannis%20Nikolentzos%20and%20Michail%20Chatzianastasis%20and%20Hadi%20Abdine%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Graph%20generation%20has%20emerged%20as%20a%20crucial%20task%20in%20machine%20learning%2C%20with%0Asignificant%20challenges%20in%20generating%20graphs%20that%20accurately%20reflect%20specific%0Aproperties.%20Existing%20methods%20often%20fall%20short%20in%20efficiently%20addressing%20this%0Aneed%20as%20they%20struggle%20with%20the%20high-dimensional%20complexity%20and%20varied%20nature%20of%0Agraph%20properties.%20In%20this%20paper%2C%20we%20introduce%20the%20Neural%20Graph%20Generator%20%28NGG%29%2C%0Aa%20novel%20approach%20which%20utilizes%20conditioned%20latent%20diffusion%20models%20for%20graph%0Ageneration.%20NGG%20demonstrates%20a%20remarkable%20capacity%20to%20model%20complex%20graph%0Apatterns%2C%20offering%20control%20over%20the%20graph%20generation%20process.%20NGG%20employs%20a%0Avariational%20graph%20autoencoder%20for%20graph%20compression%20and%20a%20diffusion%20process%20in%0Athe%20latent%20vector%20space%2C%20guided%20by%20vectors%20summarizing%20graph%20statistics.%20We%0Ademonstrate%20NGG%27s%20versatility%20across%20various%20graph%20generation%20tasks%2C%20showing%0Aits%20capability%20to%20capture%20desired%20graph%20properties%20and%20generalize%20to%20unseen%0Agraphs.%20This%20work%20signifies%20a%20significant%20shift%20in%20graph%20generation%0Amethodologies%2C%20offering%20a%20more%20practical%20and%20efficient%20solution%20for%20generating%0Adiverse%20types%20of%20graphs%20with%20specific%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01535v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Graph%20Generator%3A%20Feature-Conditioned%20Graph%20Generation%20using%0A%20%20Latent%20Diffusion%20Models&entry.906535625=Iakovos%20Evdaimon%20and%20Giannis%20Nikolentzos%20and%20Michail%20Chatzianastasis%20and%20Hadi%20Abdine%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Graph%20generation%20has%20emerged%20as%20a%20crucial%20task%20in%20machine%20learning%2C%20with%0Asignificant%20challenges%20in%20generating%20graphs%20that%20accurately%20reflect%20specific%0Aproperties.%20Existing%20methods%20often%20fall%20short%20in%20efficiently%20addressing%20this%0Aneed%20as%20they%20struggle%20with%20the%20high-dimensional%20complexity%20and%20varied%20nature%20of%0Agraph%20properties.%20In%20this%20paper%2C%20we%20introduce%20the%20Neural%20Graph%20Generator%20%28NGG%29%2C%0Aa%20novel%20approach%20which%20utilizes%20conditioned%20latent%20diffusion%20models%20for%20graph%0Ageneration.%20NGG%20demonstrates%20a%20remarkable%20capacity%20to%20model%20complex%20graph%0Apatterns%2C%20offering%20control%20over%20the%20graph%20generation%20process.%20NGG%20employs%20a%0Avariational%20graph%20autoencoder%20for%20graph%20compression%20and%20a%20diffusion%20process%20in%0Athe%20latent%20vector%20space%2C%20guided%20by%20vectors%20summarizing%20graph%20statistics.%20We%0Ademonstrate%20NGG%27s%20versatility%20across%20various%20graph%20generation%20tasks%2C%20showing%0Aits%20capability%20to%20capture%20desired%20graph%20properties%20and%20generalize%20to%20unseen%0Agraphs.%20This%20work%20signifies%20a%20significant%20shift%20in%20graph%20generation%0Amethodologies%2C%20offering%20a%20more%20practical%20and%20efficient%20solution%20for%20generating%0Adiverse%20types%20of%20graphs%20with%20specific%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01535v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


