<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud\n  SLAM", "author": "Ganlin Zhang and Erik Sandstr\u00f6m and Youmin Zhang and Manthan Patel and Luc Van Gool and Martin R. Oswald", "abstract": "  Recent advancements in RGB-only dense Simultaneous Localization and Mapping\n(SLAM) have predominantly utilized grid-based neural implicit encodings and/or\nstruggle to efficiently realize global map and pose consistency. To this end,\nwe propose an efficient RGB-only dense SLAM system using a flexible neural\npoint cloud scene representation that adapts to keyframe poses and depth\nupdates, without needing costly backpropagation. Another critical challenge of\nRGB-only SLAM is the lack of geometric priors. To alleviate this issue, with\nthe aid of a monocular depth estimator, we introduce a novel DSPO layer for\nbundle adjustment which optimizes the pose and depth of keyframes along with\nthe scale of the monocular depth. Finally, our system benefits from loop\nclosure and online global bundle adjustment and performs either better or\ncompetitive to existing dense neural RGB SLAM methods in tracking, mapping and\nrendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source\ncode will be made available.\n", "link": "http://arxiv.org/abs/2403.19549v1", "date": "2024-03-28", "relevancy": 2.9336, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.634}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5579}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GlORIE-SLAM%3A%20Globally%20Optimized%20RGB-only%20Implicit%20Encoding%20Point%20Cloud%0A%20%20SLAM&body=Title%3A%20GlORIE-SLAM%3A%20Globally%20Optimized%20RGB-only%20Implicit%20Encoding%20Point%20Cloud%0A%20%20SLAM%0AAuthor%3A%20Ganlin%20Zhang%20and%20Erik%20Sandstr%C3%B6m%20and%20Youmin%20Zhang%20and%20Manthan%20Patel%20and%20Luc%20Van%20Gool%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Recent%20advancements%20in%20RGB-only%20dense%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20have%20predominantly%20utilized%20grid-based%20neural%20implicit%20encodings%20and/or%0Astruggle%20to%20efficiently%20realize%20global%20map%20and%20pose%20consistency.%20To%20this%20end%2C%0Awe%20propose%20an%20efficient%20RGB-only%20dense%20SLAM%20system%20using%20a%20flexible%20neural%0Apoint%20cloud%20scene%20representation%20that%20adapts%20to%20keyframe%20poses%20and%20depth%0Aupdates%2C%20without%20needing%20costly%20backpropagation.%20Another%20critical%20challenge%20of%0ARGB-only%20SLAM%20is%20the%20lack%20of%20geometric%20priors.%20To%20alleviate%20this%20issue%2C%20with%0Athe%20aid%20of%20a%20monocular%20depth%20estimator%2C%20we%20introduce%20a%20novel%20DSPO%20layer%20for%0Abundle%20adjustment%20which%20optimizes%20the%20pose%20and%20depth%20of%20keyframes%20along%20with%0Athe%20scale%20of%20the%20monocular%20depth.%20Finally%2C%20our%20system%20benefits%20from%20loop%0Aclosure%20and%20online%20global%20bundle%20adjustment%20and%20performs%20either%20better%20or%0Acompetitive%20to%20existing%20dense%20neural%20RGB%20SLAM%20methods%20in%20tracking%2C%20mapping%20and%0Arendering%20accuracy%20on%20the%20Replica%2C%20TUM-RGBD%20and%20ScanNet%20datasets.%20The%20source%0Acode%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19549v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlORIE-SLAM%3A%20Globally%20Optimized%20RGB-only%20Implicit%20Encoding%20Point%20Cloud%0A%20%20SLAM&entry.906535625=Ganlin%20Zhang%20and%20Erik%20Sandstr%C3%B6m%20and%20Youmin%20Zhang%20and%20Manthan%20Patel%20and%20Luc%20Van%20Gool%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Recent%20advancements%20in%20RGB-only%20dense%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20have%20predominantly%20utilized%20grid-based%20neural%20implicit%20encodings%20and/or%0Astruggle%20to%20efficiently%20realize%20global%20map%20and%20pose%20consistency.%20To%20this%20end%2C%0Awe%20propose%20an%20efficient%20RGB-only%20dense%20SLAM%20system%20using%20a%20flexible%20neural%0Apoint%20cloud%20scene%20representation%20that%20adapts%20to%20keyframe%20poses%20and%20depth%0Aupdates%2C%20without%20needing%20costly%20backpropagation.%20Another%20critical%20challenge%20of%0ARGB-only%20SLAM%20is%20the%20lack%20of%20geometric%20priors.%20To%20alleviate%20this%20issue%2C%20with%0Athe%20aid%20of%20a%20monocular%20depth%20estimator%2C%20we%20introduce%20a%20novel%20DSPO%20layer%20for%0Abundle%20adjustment%20which%20optimizes%20the%20pose%20and%20depth%20of%20keyframes%20along%20with%0Athe%20scale%20of%20the%20monocular%20depth.%20Finally%2C%20our%20system%20benefits%20from%20loop%0Aclosure%20and%20online%20global%20bundle%20adjustment%20and%20performs%20either%20better%20or%0Acompetitive%20to%20existing%20dense%20neural%20RGB%20SLAM%20methods%20in%20tracking%2C%20mapping%20and%0Arendering%20accuracy%20on%20the%20Replica%2C%20TUM-RGBD%20and%20ScanNet%20datasets.%20The%20source%0Acode%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19549v1&entry.124074799=Read"},
{"title": "Sparse Generation: Making Pseudo Labels Sparse for weakly supervision\n  with points", "author": "Tian Ma and Chuyang Shang and Wanzhu Ren and Yuancheng Li and Jiiayi Yang and Jiali Qian", "abstract": "  In recent years, research on point weakly supervised object detection (PWSOD)\nmethods in the field of computer vision has attracted people's attention.\nHowever, existing pseudo labels generation methods perform poorly in a small\namount of supervised annotation data and dense object detection tasks. We\nconsider the generation of weakly supervised pseudo labels as the result of\nmodel's sparse output, and propose a method called Sparse Generation to make\npseudo labels sparse. It constructs dense tensors through the relationship\nbetween data and detector model, optimizes three of its parameters, and obtains\na sparse tensor via coordinated calculation, thereby indirectly obtaining\nhigher quality pseudo labels, and solving the model's density problem in the\nsituation of only a small amount of supervised annotation data can be used. On\ntwo broadly used open-source datasets (RSOD, SIMD) and a self-built dataset\n(Bullet-Hole), the experimental results showed that the proposed method has a\nsignificant advantage in terms of overall performance metrics, comparing to\nthat state-of-the-art method.\n", "link": "http://arxiv.org/abs/2403.19306v1", "date": "2024-03-28", "relevancy": 2.7998, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5647}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5605}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5548}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sparse%20Generation%3A%20Making%20Pseudo%20Labels%20Sparse%20for%20weakly%20supervision%0A%20%20with%20points&body=Title%3A%20Sparse%20Generation%3A%20Making%20Pseudo%20Labels%20Sparse%20for%20weakly%20supervision%0A%20%20with%20points%0AAuthor%3A%20Tian%20Ma%20and%20Chuyang%20Shang%20and%20Wanzhu%20Ren%20and%20Yuancheng%20Li%20and%20Jiiayi%20Yang%20and%20Jiali%20Qian%0AAbstract%3A%20%20%20In%20recent%20years%2C%20research%20on%20point%20weakly%20supervised%20object%20detection%20%28PWSOD%29%0Amethods%20in%20the%20field%20of%20computer%20vision%20has%20attracted%20people%27s%20attention.%0AHowever%2C%20existing%20pseudo%20labels%20generation%20methods%20perform%20poorly%20in%20a%20small%0Aamount%20of%20supervised%20annotation%20data%20and%20dense%20object%20detection%20tasks.%20We%0Aconsider%20the%20generation%20of%20weakly%20supervised%20pseudo%20labels%20as%20the%20result%20of%0Amodel%27s%20sparse%20output%2C%20and%20propose%20a%20method%20called%20Sparse%20Generation%20to%20make%0Apseudo%20labels%20sparse.%20It%20constructs%20dense%20tensors%20through%20the%20relationship%0Abetween%20data%20and%20detector%20model%2C%20optimizes%20three%20of%20its%20parameters%2C%20and%20obtains%0Aa%20sparse%20tensor%20via%20coordinated%20calculation%2C%20thereby%20indirectly%20obtaining%0Ahigher%20quality%20pseudo%20labels%2C%20and%20solving%20the%20model%27s%20density%20problem%20in%20the%0Asituation%20of%20only%20a%20small%20amount%20of%20supervised%20annotation%20data%20can%20be%20used.%20On%0Atwo%20broadly%20used%20open-source%20datasets%20%28RSOD%2C%20SIMD%29%20and%20a%20self-built%20dataset%0A%28Bullet-Hole%29%2C%20the%20experimental%20results%20showed%20that%20the%20proposed%20method%20has%20a%0Asignificant%20advantage%20in%20terms%20of%20overall%20performance%20metrics%2C%20comparing%20to%0Athat%20state-of-the-art%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19306v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Generation%3A%20Making%20Pseudo%20Labels%20Sparse%20for%20weakly%20supervision%0A%20%20with%20points&entry.906535625=Tian%20Ma%20and%20Chuyang%20Shang%20and%20Wanzhu%20Ren%20and%20Yuancheng%20Li%20and%20Jiiayi%20Yang%20and%20Jiali%20Qian&entry.1292438233=%20%20In%20recent%20years%2C%20research%20on%20point%20weakly%20supervised%20object%20detection%20%28PWSOD%29%0Amethods%20in%20the%20field%20of%20computer%20vision%20has%20attracted%20people%27s%20attention.%0AHowever%2C%20existing%20pseudo%20labels%20generation%20methods%20perform%20poorly%20in%20a%20small%0Aamount%20of%20supervised%20annotation%20data%20and%20dense%20object%20detection%20tasks.%20We%0Aconsider%20the%20generation%20of%20weakly%20supervised%20pseudo%20labels%20as%20the%20result%20of%0Amodel%27s%20sparse%20output%2C%20and%20propose%20a%20method%20called%20Sparse%20Generation%20to%20make%0Apseudo%20labels%20sparse.%20It%20constructs%20dense%20tensors%20through%20the%20relationship%0Abetween%20data%20and%20detector%20model%2C%20optimizes%20three%20of%20its%20parameters%2C%20and%20obtains%0Aa%20sparse%20tensor%20via%20coordinated%20calculation%2C%20thereby%20indirectly%20obtaining%0Ahigher%20quality%20pseudo%20labels%2C%20and%20solving%20the%20model%27s%20density%20problem%20in%20the%0Asituation%20of%20only%20a%20small%20amount%20of%20supervised%20annotation%20data%20can%20be%20used.%20On%0Atwo%20broadly%20used%20open-source%20datasets%20%28RSOD%2C%20SIMD%29%20and%20a%20self-built%20dataset%0A%28Bullet-Hole%29%2C%20the%20experimental%20results%20showed%20that%20the%20proposed%20method%20has%20a%0Asignificant%20advantage%20in%20terms%20of%20overall%20performance%20metrics%2C%20comparing%20to%0Athat%20state-of-the-art%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19306v1&entry.124074799=Read"},
{"title": "ILPO-NET: Network for the invariant recognition of arbitrary volumetric\n  patterns in 3D", "author": "Dmitrii Zhemchuzhnikov and Sergei Grudinin", "abstract": "  Effective recognition of spatial patterns and learning their hierarchy is\ncrucial in modern spatial data analysis. Volumetric data applications seek\ntechniques ensuring invariance not only to shifts but also to pattern\nrotations. While traditional methods can readily achieve translational\ninvariance, rotational invariance possesses multiple challenges and remains an\nactive area of research. Here, we present ILPO-Net (Invariant to Local Patterns\nOrientation Network), a novel approach that handles arbitrarily shaped patterns\nwith the convolutional operation inherently invariant to local spatial pattern\norientations using the Wigner matrix expansions. Our architecture seamlessly\nintegrates the new convolution operator and, when benchmarked on diverse\nvolumetric datasets such as MedMNIST and CATH, demonstrates superior\nperformance over the baselines with significantly reduced parameter counts - up\nto 1000 times fewer in the case of MedMNIST. Beyond these demonstrations,\nILPO-Net's rotational invariance paves the way for other applications across\nmultiple disciplines. Our code is publicly available at\nhttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.\n", "link": "http://arxiv.org/abs/2403.19612v1", "date": "2024-03-28", "relevancy": 2.7784, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5666}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5606}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5398}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ILPO-NET%3A%20Network%20for%20the%20invariant%20recognition%20of%20arbitrary%20volumetric%0A%20%20patterns%20in%203D&body=Title%3A%20ILPO-NET%3A%20Network%20for%20the%20invariant%20recognition%20of%20arbitrary%20volumetric%0A%20%20patterns%20in%203D%0AAuthor%3A%20Dmitrii%20Zhemchuzhnikov%20and%20Sergei%20Grudinin%0AAbstract%3A%20%20%20Effective%20recognition%20of%20spatial%20patterns%20and%20learning%20their%20hierarchy%20is%0Acrucial%20in%20modern%20spatial%20data%20analysis.%20Volumetric%20data%20applications%20seek%0Atechniques%20ensuring%20invariance%20not%20only%20to%20shifts%20but%20also%20to%20pattern%0Arotations.%20While%20traditional%20methods%20can%20readily%20achieve%20translational%0Ainvariance%2C%20rotational%20invariance%20possesses%20multiple%20challenges%20and%20remains%20an%0Aactive%20area%20of%20research.%20Here%2C%20we%20present%20ILPO-Net%20%28Invariant%20to%20Local%20Patterns%0AOrientation%20Network%29%2C%20a%20novel%20approach%20that%20handles%20arbitrarily%20shaped%20patterns%0Awith%20the%20convolutional%20operation%20inherently%20invariant%20to%20local%20spatial%20pattern%0Aorientations%20using%20the%20Wigner%20matrix%20expansions.%20Our%20architecture%20seamlessly%0Aintegrates%20the%20new%20convolution%20operator%20and%2C%20when%20benchmarked%20on%20diverse%0Avolumetric%20datasets%20such%20as%20MedMNIST%20and%20CATH%2C%20demonstrates%20superior%0Aperformance%20over%20the%20baselines%20with%20significantly%20reduced%20parameter%20counts%20-%20up%0Ato%201000%20times%20fewer%20in%20the%20case%20of%20MedMNIST.%20Beyond%20these%20demonstrations%2C%0AILPO-Net%27s%20rotational%20invariance%20paves%20the%20way%20for%20other%20applications%20across%0Amultiple%20disciplines.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19612v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ILPO-NET%3A%20Network%20for%20the%20invariant%20recognition%20of%20arbitrary%20volumetric%0A%20%20patterns%20in%203D&entry.906535625=Dmitrii%20Zhemchuzhnikov%20and%20Sergei%20Grudinin&entry.1292438233=%20%20Effective%20recognition%20of%20spatial%20patterns%20and%20learning%20their%20hierarchy%20is%0Acrucial%20in%20modern%20spatial%20data%20analysis.%20Volumetric%20data%20applications%20seek%0Atechniques%20ensuring%20invariance%20not%20only%20to%20shifts%20but%20also%20to%20pattern%0Arotations.%20While%20traditional%20methods%20can%20readily%20achieve%20translational%0Ainvariance%2C%20rotational%20invariance%20possesses%20multiple%20challenges%20and%20remains%20an%0Aactive%20area%20of%20research.%20Here%2C%20we%20present%20ILPO-Net%20%28Invariant%20to%20Local%20Patterns%0AOrientation%20Network%29%2C%20a%20novel%20approach%20that%20handles%20arbitrarily%20shaped%20patterns%0Awith%20the%20convolutional%20operation%20inherently%20invariant%20to%20local%20spatial%20pattern%0Aorientations%20using%20the%20Wigner%20matrix%20expansions.%20Our%20architecture%20seamlessly%0Aintegrates%20the%20new%20convolution%20operator%20and%2C%20when%20benchmarked%20on%20diverse%0Avolumetric%20datasets%20such%20as%20MedMNIST%20and%20CATH%2C%20demonstrates%20superior%0Aperformance%20over%20the%20baselines%20with%20significantly%20reduced%20parameter%20counts%20-%20up%0Ato%201000%20times%20fewer%20in%20the%20case%20of%20MedMNIST.%20Beyond%20these%20demonstrations%2C%0AILPO-Net%27s%20rotational%20invariance%20paves%20the%20way%20for%20other%20applications%20across%0Amultiple%20disciplines.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19612v1&entry.124074799=Read"},
{"title": "Img2Loc: Revisiting Image Geolocalization using Multi-modality\n  Foundation Models and Image-based Retrieval-Augmented Generation", "author": "Zhongliang Zhou and Jielu Zhang and Zihan Guan and Mengxuan Hu and Ni Lao and Lan Mu and Sheng Li and Gengchen Mai", "abstract": "  Geolocating precise locations from images presents a challenging problem in\ncomputer vision and information retrieval.Traditional methods typically employ\neither classification, which dividing the Earth surface into grid cells and\nclassifying images accordingly, or retrieval, which identifying locations by\nmatching images with a database of image-location pairs. However,\nclassification-based approaches are limited by the cell size and cannot yield\nprecise predictions, while retrieval-based systems usually suffer from poor\nsearch quality and inadequate coverage of the global landscape at varied scale\nand aggregation levels. To overcome these drawbacks, we present Img2Loc, a\nnovel system that redefines image geolocalization as a text generation task.\nThis is achieved using cutting-edge large multi-modality models like GPT4V or\nLLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based\nrepresentations to generate an image-based coordinate query database. It then\nuniquely combines query results with images itself, forming elaborate prompts\ncustomized for LMMs. When tested on benchmark datasets such as Im2GPS3k and\nYFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art\nmodels but does so without any model training.\n", "link": "http://arxiv.org/abs/2403.19584v1", "date": "2024-03-28", "relevancy": 2.7657, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5701}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5623}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Img2Loc%3A%20Revisiting%20Image%20Geolocalization%20using%20Multi-modality%0A%20%20Foundation%20Models%20and%20Image-based%20Retrieval-Augmented%20Generation&body=Title%3A%20Img2Loc%3A%20Revisiting%20Image%20Geolocalization%20using%20Multi-modality%0A%20%20Foundation%20Models%20and%20Image-based%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Zhongliang%20Zhou%20and%20Jielu%20Zhang%20and%20Zihan%20Guan%20and%20Mengxuan%20Hu%20and%20Ni%20Lao%20and%20Lan%20Mu%20and%20Sheng%20Li%20and%20Gengchen%20Mai%0AAbstract%3A%20%20%20Geolocating%20precise%20locations%20from%20images%20presents%20a%20challenging%20problem%20in%0Acomputer%20vision%20and%20information%20retrieval.Traditional%20methods%20typically%20employ%0Aeither%20classification%2C%20which%20dividing%20the%20Earth%20surface%20into%20grid%20cells%20and%0Aclassifying%20images%20accordingly%2C%20or%20retrieval%2C%20which%20identifying%20locations%20by%0Amatching%20images%20with%20a%20database%20of%20image-location%20pairs.%20However%2C%0Aclassification-based%20approaches%20are%20limited%20by%20the%20cell%20size%20and%20cannot%20yield%0Aprecise%20predictions%2C%20while%20retrieval-based%20systems%20usually%20suffer%20from%20poor%0Asearch%20quality%20and%20inadequate%20coverage%20of%20the%20global%20landscape%20at%20varied%20scale%0Aand%20aggregation%20levels.%20To%20overcome%20these%20drawbacks%2C%20we%20present%20Img2Loc%2C%20a%0Anovel%20system%20that%20redefines%20image%20geolocalization%20as%20a%20text%20generation%20task.%0AThis%20is%20achieved%20using%20cutting-edge%20large%20multi-modality%20models%20like%20GPT4V%20or%0ALLaVA%20with%20retrieval%20augmented%20generation.%20Img2Loc%20first%20employs%20CLIP-based%0Arepresentations%20to%20generate%20an%20image-based%20coordinate%20query%20database.%20It%20then%0Auniquely%20combines%20query%20results%20with%20images%20itself%2C%20forming%20elaborate%20prompts%0Acustomized%20for%20LMMs.%20When%20tested%20on%20benchmark%20datasets%20such%20as%20Im2GPS3k%20and%0AYFCC4k%2C%20Img2Loc%20not%20only%20surpasses%20the%20performance%20of%20previous%20state-of-the-art%0Amodels%20but%20does%20so%20without%20any%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19584v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Img2Loc%3A%20Revisiting%20Image%20Geolocalization%20using%20Multi-modality%0A%20%20Foundation%20Models%20and%20Image-based%20Retrieval-Augmented%20Generation&entry.906535625=Zhongliang%20Zhou%20and%20Jielu%20Zhang%20and%20Zihan%20Guan%20and%20Mengxuan%20Hu%20and%20Ni%20Lao%20and%20Lan%20Mu%20and%20Sheng%20Li%20and%20Gengchen%20Mai&entry.1292438233=%20%20Geolocating%20precise%20locations%20from%20images%20presents%20a%20challenging%20problem%20in%0Acomputer%20vision%20and%20information%20retrieval.Traditional%20methods%20typically%20employ%0Aeither%20classification%2C%20which%20dividing%20the%20Earth%20surface%20into%20grid%20cells%20and%0Aclassifying%20images%20accordingly%2C%20or%20retrieval%2C%20which%20identifying%20locations%20by%0Amatching%20images%20with%20a%20database%20of%20image-location%20pairs.%20However%2C%0Aclassification-based%20approaches%20are%20limited%20by%20the%20cell%20size%20and%20cannot%20yield%0Aprecise%20predictions%2C%20while%20retrieval-based%20systems%20usually%20suffer%20from%20poor%0Asearch%20quality%20and%20inadequate%20coverage%20of%20the%20global%20landscape%20at%20varied%20scale%0Aand%20aggregation%20levels.%20To%20overcome%20these%20drawbacks%2C%20we%20present%20Img2Loc%2C%20a%0Anovel%20system%20that%20redefines%20image%20geolocalization%20as%20a%20text%20generation%20task.%0AThis%20is%20achieved%20using%20cutting-edge%20large%20multi-modality%20models%20like%20GPT4V%20or%0ALLaVA%20with%20retrieval%20augmented%20generation.%20Img2Loc%20first%20employs%20CLIP-based%0Arepresentations%20to%20generate%20an%20image-based%20coordinate%20query%20database.%20It%20then%0Auniquely%20combines%20query%20results%20with%20images%20itself%2C%20forming%20elaborate%20prompts%0Acustomized%20for%20LMMs.%20When%20tested%20on%20benchmark%20datasets%20such%20as%20Im2GPS3k%20and%0AYFCC4k%2C%20Img2Loc%20not%20only%20surpasses%20the%20performance%20of%20previous%20state-of-the-art%0Amodels%20but%20does%20so%20without%20any%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19584v1&entry.124074799=Read"},
{"title": "The Bad Batches: Enhancing Self-Supervised Learning in Image\n  Classification Through Representative Batch Curation", "author": "Ozgu Goksu and Nicolas Pugeault", "abstract": "  The pursuit of learning robust representations without human supervision is a\nlongstanding challenge. The recent advancements in self-supervised contrastive\nlearning approaches have demonstrated high performance across various\nrepresentation learning challenges. However, current methods depend on the\nrandom transformation of training examples, resulting in some cases of\nunrepresentative positive pairs that can have a large impact on learning. This\nlimitation not only impedes the convergence of the learning process but the\nrobustness of the learnt representation as well as requiring larger batch sizes\nto improve robustness to such bad batches. This paper attempts to alleviate the\ninfluence of false positive and false negative pairs by employing pairwise\nsimilarity calculations through the Fr\\'echet ResNet Distance (FRD), thereby\nobtaining robust representations from unlabelled data. The effectiveness of the\nproposed method is substantiated by empirical results, where a linear\nclassifier trained on self-supervised contrastive representations achieved an\nimpressive 87.74\\% top-1 accuracy on STL10 and 99.31\\% on the Flower102\ndataset. These results emphasize the potential of the proposed approach in\npushing the boundaries of the state-of-the-art in self-supervised contrastive\nlearning, particularly for image classification tasks.\n", "link": "http://arxiv.org/abs/2403.19579v1", "date": "2024-03-28", "relevancy": 2.7549, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6017}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Bad%20Batches%3A%20Enhancing%20Self-Supervised%20Learning%20in%20Image%0A%20%20Classification%20Through%20Representative%20Batch%20Curation&body=Title%3A%20The%20Bad%20Batches%3A%20Enhancing%20Self-Supervised%20Learning%20in%20Image%0A%20%20Classification%20Through%20Representative%20Batch%20Curation%0AAuthor%3A%20Ozgu%20Goksu%20and%20Nicolas%20Pugeault%0AAbstract%3A%20%20%20The%20pursuit%20of%20learning%20robust%20representations%20without%20human%20supervision%20is%20a%0Alongstanding%20challenge.%20The%20recent%20advancements%20in%20self-supervised%20contrastive%0Alearning%20approaches%20have%20demonstrated%20high%20performance%20across%20various%0Arepresentation%20learning%20challenges.%20However%2C%20current%20methods%20depend%20on%20the%0Arandom%20transformation%20of%20training%20examples%2C%20resulting%20in%20some%20cases%20of%0Aunrepresentative%20positive%20pairs%20that%20can%20have%20a%20large%20impact%20on%20learning.%20This%0Alimitation%20not%20only%20impedes%20the%20convergence%20of%20the%20learning%20process%20but%20the%0Arobustness%20of%20the%20learnt%20representation%20as%20well%20as%20requiring%20larger%20batch%20sizes%0Ato%20improve%20robustness%20to%20such%20bad%20batches.%20This%20paper%20attempts%20to%20alleviate%20the%0Ainfluence%20of%20false%20positive%20and%20false%20negative%20pairs%20by%20employing%20pairwise%0Asimilarity%20calculations%20through%20the%20Fr%5C%27echet%20ResNet%20Distance%20%28FRD%29%2C%20thereby%0Aobtaining%20robust%20representations%20from%20unlabelled%20data.%20The%20effectiveness%20of%20the%0Aproposed%20method%20is%20substantiated%20by%20empirical%20results%2C%20where%20a%20linear%0Aclassifier%20trained%20on%20self-supervised%20contrastive%20representations%20achieved%20an%0Aimpressive%2087.74%5C%25%20top-1%20accuracy%20on%20STL10%20and%2099.31%5C%25%20on%20the%20Flower102%0Adataset.%20These%20results%20emphasize%20the%20potential%20of%20the%20proposed%20approach%20in%0Apushing%20the%20boundaries%20of%20the%20state-of-the-art%20in%20self-supervised%20contrastive%0Alearning%2C%20particularly%20for%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19579v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Bad%20Batches%3A%20Enhancing%20Self-Supervised%20Learning%20in%20Image%0A%20%20Classification%20Through%20Representative%20Batch%20Curation&entry.906535625=Ozgu%20Goksu%20and%20Nicolas%20Pugeault&entry.1292438233=%20%20The%20pursuit%20of%20learning%20robust%20representations%20without%20human%20supervision%20is%20a%0Alongstanding%20challenge.%20The%20recent%20advancements%20in%20self-supervised%20contrastive%0Alearning%20approaches%20have%20demonstrated%20high%20performance%20across%20various%0Arepresentation%20learning%20challenges.%20However%2C%20current%20methods%20depend%20on%20the%0Arandom%20transformation%20of%20training%20examples%2C%20resulting%20in%20some%20cases%20of%0Aunrepresentative%20positive%20pairs%20that%20can%20have%20a%20large%20impact%20on%20learning.%20This%0Alimitation%20not%20only%20impedes%20the%20convergence%20of%20the%20learning%20process%20but%20the%0Arobustness%20of%20the%20learnt%20representation%20as%20well%20as%20requiring%20larger%20batch%20sizes%0Ato%20improve%20robustness%20to%20such%20bad%20batches.%20This%20paper%20attempts%20to%20alleviate%20the%0Ainfluence%20of%20false%20positive%20and%20false%20negative%20pairs%20by%20employing%20pairwise%0Asimilarity%20calculations%20through%20the%20Fr%5C%27echet%20ResNet%20Distance%20%28FRD%29%2C%20thereby%0Aobtaining%20robust%20representations%20from%20unlabelled%20data.%20The%20effectiveness%20of%20the%0Aproposed%20method%20is%20substantiated%20by%20empirical%20results%2C%20where%20a%20linear%0Aclassifier%20trained%20on%20self-supervised%20contrastive%20representations%20achieved%20an%0Aimpressive%2087.74%5C%25%20top-1%20accuracy%20on%20STL10%20and%2099.31%5C%25%20on%20the%20Flower102%0Adataset.%20These%20results%20emphasize%20the%20potential%20of%20the%20proposed%20approach%20in%0Apushing%20the%20boundaries%20of%20the%20state-of-the-art%20in%20self-supervised%20contrastive%0Alearning%2C%20particularly%20for%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19579v1&entry.124074799=Read"},
{"title": "PointCloud-Text Matching: Benchmark Datasets and a Baseline", "author": "Yanglin Feng and Yang Qin and Dezhong Peng and Hongyuan Zhu and Xi Peng and Peng Hu", "abstract": "  In this paper, we present and study a new instance-level retrieval task:\nPointCloud-Text Matching~(PTM), which aims to find the exact cross-modal\ninstance that matches a given point-cloud query or text query. PTM could be\napplied to various scenarios, such as indoor/urban-canyon localization and\nscene retrieval. However, there exists no suitable and targeted dataset for PTM\nin practice. Therefore, we construct three new PTM benchmark datasets, namely\n3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with\nnoisy correspondence due to the sparsity, noise, or disorder of point clouds\nand the ambiguity, vagueness, or incompleteness of texts, which make existing\ncross-modal matching methods ineffective for PTM. To tackle these challenges,\nwe propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa).\nRoMa consists of two modules: a Dual Attention Perception module (DAP) and a\nRobust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages\ntoken-level and feature-level attention to adaptively focus on useful local and\nglobal features, and aggregate them into common representations, thereby\nreducing the adverse impact of noise and ambiguity. To handle noisy\ncorrespondence, RNCL divides negative pairs, which are much less error-prone\nthan positive pairs, into clean and noisy subsets, and assigns them forward and\nreverse optimization directions respectively, thus enhancing robustness against\nnoisy correspondence. We conduct extensive experiments on our benchmarks and\ndemonstrate the superiority of our RoMa.\n", "link": "http://arxiv.org/abs/2403.19386v1", "date": "2024-03-28", "relevancy": 2.7388, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.532}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PointCloud-Text%20Matching%3A%20Benchmark%20Datasets%20and%20a%20Baseline&body=Title%3A%20PointCloud-Text%20Matching%3A%20Benchmark%20Datasets%20and%20a%20Baseline%0AAuthor%3A%20Yanglin%20Feng%20and%20Yang%20Qin%20and%20Dezhong%20Peng%20and%20Hongyuan%20Zhu%20and%20Xi%20Peng%20and%20Peng%20Hu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20and%20study%20a%20new%20instance-level%20retrieval%20task%3A%0APointCloud-Text%20Matching~%28PTM%29%2C%20which%20aims%20to%20find%20the%20exact%20cross-modal%0Ainstance%20that%20matches%20a%20given%20point-cloud%20query%20or%20text%20query.%20PTM%20could%20be%0Aapplied%20to%20various%20scenarios%2C%20such%20as%20indoor/urban-canyon%20localization%20and%0Ascene%20retrieval.%20However%2C%20there%20exists%20no%20suitable%20and%20targeted%20dataset%20for%20PTM%0Ain%20practice.%20Therefore%2C%20we%20construct%20three%20new%20PTM%20benchmark%20datasets%2C%20namely%0A3D2T-SR%2C%203D2T-NR%2C%20and%203D2T-QA.%20We%20observe%20that%20the%20data%20is%20challenging%20and%20with%0Anoisy%20correspondence%20due%20to%20the%20sparsity%2C%20noise%2C%20or%20disorder%20of%20point%20clouds%0Aand%20the%20ambiguity%2C%20vagueness%2C%20or%20incompleteness%20of%20texts%2C%20which%20make%20existing%0Across-modal%20matching%20methods%20ineffective%20for%20PTM.%20To%20tackle%20these%20challenges%2C%0Awe%20propose%20a%20PTM%20baseline%2C%20named%20Robust%20PointCloud-Text%20Matching%20method%20%28RoMa%29.%0ARoMa%20consists%20of%20two%20modules%3A%20a%20Dual%20Attention%20Perception%20module%20%28DAP%29%20and%20a%0ARobust%20Negative%20Contrastive%20Learning%20module%20%28RNCL%29.%20Specifically%2C%20DAP%20leverages%0Atoken-level%20and%20feature-level%20attention%20to%20adaptively%20focus%20on%20useful%20local%20and%0Aglobal%20features%2C%20and%20aggregate%20them%20into%20common%20representations%2C%20thereby%0Areducing%20the%20adverse%20impact%20of%20noise%20and%20ambiguity.%20To%20handle%20noisy%0Acorrespondence%2C%20RNCL%20divides%20negative%20pairs%2C%20which%20are%20much%20less%20error-prone%0Athan%20positive%20pairs%2C%20into%20clean%20and%20noisy%20subsets%2C%20and%20assigns%20them%20forward%20and%0Areverse%20optimization%20directions%20respectively%2C%20thus%20enhancing%20robustness%20against%0Anoisy%20correspondence.%20We%20conduct%20extensive%20experiments%20on%20our%20benchmarks%20and%0Ademonstrate%20the%20superiority%20of%20our%20RoMa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19386v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointCloud-Text%20Matching%3A%20Benchmark%20Datasets%20and%20a%20Baseline&entry.906535625=Yanglin%20Feng%20and%20Yang%20Qin%20and%20Dezhong%20Peng%20and%20Hongyuan%20Zhu%20and%20Xi%20Peng%20and%20Peng%20Hu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20and%20study%20a%20new%20instance-level%20retrieval%20task%3A%0APointCloud-Text%20Matching~%28PTM%29%2C%20which%20aims%20to%20find%20the%20exact%20cross-modal%0Ainstance%20that%20matches%20a%20given%20point-cloud%20query%20or%20text%20query.%20PTM%20could%20be%0Aapplied%20to%20various%20scenarios%2C%20such%20as%20indoor/urban-canyon%20localization%20and%0Ascene%20retrieval.%20However%2C%20there%20exists%20no%20suitable%20and%20targeted%20dataset%20for%20PTM%0Ain%20practice.%20Therefore%2C%20we%20construct%20three%20new%20PTM%20benchmark%20datasets%2C%20namely%0A3D2T-SR%2C%203D2T-NR%2C%20and%203D2T-QA.%20We%20observe%20that%20the%20data%20is%20challenging%20and%20with%0Anoisy%20correspondence%20due%20to%20the%20sparsity%2C%20noise%2C%20or%20disorder%20of%20point%20clouds%0Aand%20the%20ambiguity%2C%20vagueness%2C%20or%20incompleteness%20of%20texts%2C%20which%20make%20existing%0Across-modal%20matching%20methods%20ineffective%20for%20PTM.%20To%20tackle%20these%20challenges%2C%0Awe%20propose%20a%20PTM%20baseline%2C%20named%20Robust%20PointCloud-Text%20Matching%20method%20%28RoMa%29.%0ARoMa%20consists%20of%20two%20modules%3A%20a%20Dual%20Attention%20Perception%20module%20%28DAP%29%20and%20a%0ARobust%20Negative%20Contrastive%20Learning%20module%20%28RNCL%29.%20Specifically%2C%20DAP%20leverages%0Atoken-level%20and%20feature-level%20attention%20to%20adaptively%20focus%20on%20useful%20local%20and%0Aglobal%20features%2C%20and%20aggregate%20them%20into%20common%20representations%2C%20thereby%0Areducing%20the%20adverse%20impact%20of%20noise%20and%20ambiguity.%20To%20handle%20noisy%0Acorrespondence%2C%20RNCL%20divides%20negative%20pairs%2C%20which%20are%20much%20less%20error-prone%0Athan%20positive%20pairs%2C%20into%20clean%20and%20noisy%20subsets%2C%20and%20assigns%20them%20forward%20and%0Areverse%20optimization%20directions%20respectively%2C%20thus%20enhancing%20robustness%20against%0Anoisy%20correspondence.%20We%20conduct%20extensive%20experiments%20on%20our%20benchmarks%20and%0Ademonstrate%20the%20superiority%20of%20our%20RoMa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19386v1&entry.124074799=Read"},
{"title": "Direct Superpoints Matching for Robust Point Cloud Registration", "author": "Aniket Gupta and Yiming Xie and Hanumant Singh and Huaizu Jiang", "abstract": "  Deep neural networks endow the downsampled superpoints with highly\ndiscriminative feature representations. Previous dominant point cloud\nregistration approaches match these feature representations as the first step,\ne.g., using the Sinkhorn algorithm. A RANSAC-like method is then usually\nadopted as a post-processing refinement to filter the outliers. Other dominant\nmethod is to directly predict the superpoint matchings using learned MLP\nlayers. Both of them have drawbacks: RANSAC-based methods are computationally\nintensive and prediction-based methods suffer from outputing non-existing\npoints in the point cloud. In this paper, we propose a straightforward and\neffective baseline to find correspondences of superpoints in a global matching\nmanner. We employ the normalized matching scores as weights for each\ncorrespondence, allowing us to reject the outliers and further weigh the rest\ninliers when fitting the transformation matrix without relying on the\ncumbersome RANSAC. Moreover, the entire model can be trained in an end-to-end\nfashion, leading to better accuracy. Our simple yet effective baseline shows\ncomparable or even better results than state-of-the-art methods on three\ndatasets including ModelNet, 3DMatch, and KITTI. We do not advocate our\napproach to be \\emph{the} solution for point cloud registration but use the\nresults to emphasize the role of matching strategy for point cloud\nregistration. The code and models are available at\nhttps://github.com/neu-vi/Superpoints_Registration.\n", "link": "http://arxiv.org/abs/2307.01362v3", "date": "2024-03-28", "relevancy": 2.7381, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6218}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5168}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5042}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Direct%20Superpoints%20Matching%20for%20Robust%20Point%20Cloud%20Registration&body=Title%3A%20Direct%20Superpoints%20Matching%20for%20Robust%20Point%20Cloud%20Registration%0AAuthor%3A%20Aniket%20Gupta%20and%20Yiming%20Xie%20and%20Hanumant%20Singh%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Deep%20neural%20networks%20endow%20the%20downsampled%20superpoints%20with%20highly%0Adiscriminative%20feature%20representations.%20Previous%20dominant%20point%20cloud%0Aregistration%20approaches%20match%20these%20feature%20representations%20as%20the%20first%20step%2C%0Ae.g.%2C%20using%20the%20Sinkhorn%20algorithm.%20A%20RANSAC-like%20method%20is%20then%20usually%0Aadopted%20as%20a%20post-processing%20refinement%20to%20filter%20the%20outliers.%20Other%20dominant%0Amethod%20is%20to%20directly%20predict%20the%20superpoint%20matchings%20using%20learned%20MLP%0Alayers.%20Both%20of%20them%20have%20drawbacks%3A%20RANSAC-based%20methods%20are%20computationally%0Aintensive%20and%20prediction-based%20methods%20suffer%20from%20outputing%20non-existing%0Apoints%20in%20the%20point%20cloud.%20In%20this%20paper%2C%20we%20propose%20a%20straightforward%20and%0Aeffective%20baseline%20to%20find%20correspondences%20of%20superpoints%20in%20a%20global%20matching%0Amanner.%20We%20employ%20the%20normalized%20matching%20scores%20as%20weights%20for%20each%0Acorrespondence%2C%20allowing%20us%20to%20reject%20the%20outliers%20and%20further%20weigh%20the%20rest%0Ainliers%20when%20fitting%20the%20transformation%20matrix%20without%20relying%20on%20the%0Acumbersome%20RANSAC.%20Moreover%2C%20the%20entire%20model%20can%20be%20trained%20in%20an%20end-to-end%0Afashion%2C%20leading%20to%20better%20accuracy.%20Our%20simple%20yet%20effective%20baseline%20shows%0Acomparable%20or%20even%20better%20results%20than%20state-of-the-art%20methods%20on%20three%0Adatasets%20including%20ModelNet%2C%203DMatch%2C%20and%20KITTI.%20We%20do%20not%20advocate%20our%0Aapproach%20to%20be%20%5Cemph%7Bthe%7D%20solution%20for%20point%20cloud%20registration%20but%20use%20the%0Aresults%20to%20emphasize%20the%20role%20of%20matching%20strategy%20for%20point%20cloud%0Aregistration.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/neu-vi/Superpoints_Registration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01362v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Superpoints%20Matching%20for%20Robust%20Point%20Cloud%20Registration&entry.906535625=Aniket%20Gupta%20and%20Yiming%20Xie%20and%20Hanumant%20Singh%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Deep%20neural%20networks%20endow%20the%20downsampled%20superpoints%20with%20highly%0Adiscriminative%20feature%20representations.%20Previous%20dominant%20point%20cloud%0Aregistration%20approaches%20match%20these%20feature%20representations%20as%20the%20first%20step%2C%0Ae.g.%2C%20using%20the%20Sinkhorn%20algorithm.%20A%20RANSAC-like%20method%20is%20then%20usually%0Aadopted%20as%20a%20post-processing%20refinement%20to%20filter%20the%20outliers.%20Other%20dominant%0Amethod%20is%20to%20directly%20predict%20the%20superpoint%20matchings%20using%20learned%20MLP%0Alayers.%20Both%20of%20them%20have%20drawbacks%3A%20RANSAC-based%20methods%20are%20computationally%0Aintensive%20and%20prediction-based%20methods%20suffer%20from%20outputing%20non-existing%0Apoints%20in%20the%20point%20cloud.%20In%20this%20paper%2C%20we%20propose%20a%20straightforward%20and%0Aeffective%20baseline%20to%20find%20correspondences%20of%20superpoints%20in%20a%20global%20matching%0Amanner.%20We%20employ%20the%20normalized%20matching%20scores%20as%20weights%20for%20each%0Acorrespondence%2C%20allowing%20us%20to%20reject%20the%20outliers%20and%20further%20weigh%20the%20rest%0Ainliers%20when%20fitting%20the%20transformation%20matrix%20without%20relying%20on%20the%0Acumbersome%20RANSAC.%20Moreover%2C%20the%20entire%20model%20can%20be%20trained%20in%20an%20end-to-end%0Afashion%2C%20leading%20to%20better%20accuracy.%20Our%20simple%20yet%20effective%20baseline%20shows%0Acomparable%20or%20even%20better%20results%20than%20state-of-the-art%20methods%20on%20three%0Adatasets%20including%20ModelNet%2C%203DMatch%2C%20and%20KITTI.%20We%20do%20not%20advocate%20our%0Aapproach%20to%20be%20%5Cemph%7Bthe%7D%20solution%20for%20point%20cloud%20registration%20but%20use%20the%0Aresults%20to%20emphasize%20the%20role%20of%20matching%20strategy%20for%20point%20cloud%0Aregistration.%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/neu-vi/Superpoints_Registration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01362v3&entry.124074799=Read"},
{"title": "Zero-shot Referring Expression Comprehension via Structural Similarity\n  Between Images and Captions", "author": "Zeyu Han and Fangrui Zhu and Qianru Lao and Huaizu Jiang", "abstract": "  Zero-shot referring expression comprehension aims at localizing bounding\nboxes in an image corresponding to provided textual prompts, which requires:\n(i) a fine-grained disentanglement of complex visual scene and textual context,\nand (ii) a capacity to understand relationships among disentangled entities.\nUnfortunately, existing large vision-language alignment (VLA) models, e.g.,\nCLIP, struggle with both aspects so cannot be directly used for this task. To\nmitigate this gap, we leverage large foundation models to disentangle both\nimages and texts into triplets in the format of (subject, predicate, object).\nAfter that, grounding is accomplished by calculating the structural similarity\nmatrix between visual and textual triplets with a VLA model, and subsequently\npropagate it to an instance-level similarity matrix. Furthermore, to equip VLA\nmodels with the ability of relationship understanding, we design a\ntriplet-matching objective to fine-tune the VLA models on a collection of\ncurated dataset containing abundant entity relationships. Experiments\ndemonstrate that our visual grounding performance increase of up to 19.5% over\nthe SOTA zero-shot model on RefCOCO/+/g. On the more challenging Who's Waldo\ndataset, our zero-shot approach achieves comparable accuracy to the fully\nsupervised model. Code is available at\nhttps://github.com/Show-han/Zeroshot_REC.\n", "link": "http://arxiv.org/abs/2311.17048v2", "date": "2024-03-28", "relevancy": 2.7334, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5857}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5442}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Referring%20Expression%20Comprehension%20via%20Structural%20Similarity%0A%20%20Between%20Images%20and%20Captions&body=Title%3A%20Zero-shot%20Referring%20Expression%20Comprehension%20via%20Structural%20Similarity%0A%20%20Between%20Images%20and%20Captions%0AAuthor%3A%20Zeyu%20Han%20and%20Fangrui%20Zhu%20and%20Qianru%20Lao%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Zero-shot%20referring%20expression%20comprehension%20aims%20at%20localizing%20bounding%0Aboxes%20in%20an%20image%20corresponding%20to%20provided%20textual%20prompts%2C%20which%20requires%3A%0A%28i%29%20a%20fine-grained%20disentanglement%20of%20complex%20visual%20scene%20and%20textual%20context%2C%0Aand%20%28ii%29%20a%20capacity%20to%20understand%20relationships%20among%20disentangled%20entities.%0AUnfortunately%2C%20existing%20large%20vision-language%20alignment%20%28VLA%29%20models%2C%20e.g.%2C%0ACLIP%2C%20struggle%20with%20both%20aspects%20so%20cannot%20be%20directly%20used%20for%20this%20task.%20To%0Amitigate%20this%20gap%2C%20we%20leverage%20large%20foundation%20models%20to%20disentangle%20both%0Aimages%20and%20texts%20into%20triplets%20in%20the%20format%20of%20%28subject%2C%20predicate%2C%20object%29.%0AAfter%20that%2C%20grounding%20is%20accomplished%20by%20calculating%20the%20structural%20similarity%0Amatrix%20between%20visual%20and%20textual%20triplets%20with%20a%20VLA%20model%2C%20and%20subsequently%0Apropagate%20it%20to%20an%20instance-level%20similarity%20matrix.%20Furthermore%2C%20to%20equip%20VLA%0Amodels%20with%20the%20ability%20of%20relationship%20understanding%2C%20we%20design%20a%0Atriplet-matching%20objective%20to%20fine-tune%20the%20VLA%20models%20on%20a%20collection%20of%0Acurated%20dataset%20containing%20abundant%20entity%20relationships.%20Experiments%0Ademonstrate%20that%20our%20visual%20grounding%20performance%20increase%20of%20up%20to%2019.5%25%20over%0Athe%20SOTA%20zero-shot%20model%20on%20RefCOCO/%2B/g.%20On%20the%20more%20challenging%20Who%27s%20Waldo%0Adataset%2C%20our%20zero-shot%20approach%20achieves%20comparable%20accuracy%20to%20the%20fully%0Asupervised%20model.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Show-han/Zeroshot_REC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17048v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Referring%20Expression%20Comprehension%20via%20Structural%20Similarity%0A%20%20Between%20Images%20and%20Captions&entry.906535625=Zeyu%20Han%20and%20Fangrui%20Zhu%20and%20Qianru%20Lao%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Zero-shot%20referring%20expression%20comprehension%20aims%20at%20localizing%20bounding%0Aboxes%20in%20an%20image%20corresponding%20to%20provided%20textual%20prompts%2C%20which%20requires%3A%0A%28i%29%20a%20fine-grained%20disentanglement%20of%20complex%20visual%20scene%20and%20textual%20context%2C%0Aand%20%28ii%29%20a%20capacity%20to%20understand%20relationships%20among%20disentangled%20entities.%0AUnfortunately%2C%20existing%20large%20vision-language%20alignment%20%28VLA%29%20models%2C%20e.g.%2C%0ACLIP%2C%20struggle%20with%20both%20aspects%20so%20cannot%20be%20directly%20used%20for%20this%20task.%20To%0Amitigate%20this%20gap%2C%20we%20leverage%20large%20foundation%20models%20to%20disentangle%20both%0Aimages%20and%20texts%20into%20triplets%20in%20the%20format%20of%20%28subject%2C%20predicate%2C%20object%29.%0AAfter%20that%2C%20grounding%20is%20accomplished%20by%20calculating%20the%20structural%20similarity%0Amatrix%20between%20visual%20and%20textual%20triplets%20with%20a%20VLA%20model%2C%20and%20subsequently%0Apropagate%20it%20to%20an%20instance-level%20similarity%20matrix.%20Furthermore%2C%20to%20equip%20VLA%0Amodels%20with%20the%20ability%20of%20relationship%20understanding%2C%20we%20design%20a%0Atriplet-matching%20objective%20to%20fine-tune%20the%20VLA%20models%20on%20a%20collection%20of%0Acurated%20dataset%20containing%20abundant%20entity%20relationships.%20Experiments%0Ademonstrate%20that%20our%20visual%20grounding%20performance%20increase%20of%20up%20to%2019.5%25%20over%0Athe%20SOTA%20zero-shot%20model%20on%20RefCOCO/%2B/g.%20On%20the%20more%20challenging%20Who%27s%20Waldo%0Adataset%2C%20our%20zero-shot%20approach%20achieves%20comparable%20accuracy%20to%20the%20fully%0Asupervised%20model.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Show-han/Zeroshot_REC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17048v2&entry.124074799=Read"},
{"title": "Intrinsic Image Decomposition Using Point Cloud Representation", "author": "Xiaoyan Xing and Konrad Groh and Sezer Karaoglu and Theo Gevers", "abstract": "  The purpose of intrinsic decomposition is to separate an image into its\nalbedo (reflective properties) and shading components (illumination\nproperties). This is challenging because it's an ill-posed problem.\nConventional approaches primarily concentrate on 2D imagery and fail to fully\nexploit the capabilities of 3D data representation. 3D point clouds offer a\nmore comprehensive format for representing scenes, as they combine geometric\nand color information effectively. To this end, in this paper, we introduce\nPoint Intrinsic Net (PoInt-Net), which leverages 3D point cloud data to\nconcurrently estimate albedo and shading maps. The merits of PoInt-Net include\nthe following aspects. First, the model is efficient, achieving consistent\nperformance across point clouds of any size with training only required on\nsmall-scale point clouds. Second, it exhibits remarkable robustness; even when\ntrained exclusively on datasets comprising individual objects, PoInt-Net\ndemonstrates strong generalization to unseen objects and scenes. Third, it\ndelivers superior accuracy over conventional 2D approaches, demonstrating\nenhanced performance across various metrics on different datasets. (Code\nReleased)\n", "link": "http://arxiv.org/abs/2307.10924v2", "date": "2024-03-28", "relevancy": 2.7298, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5985}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20Image%20Decomposition%20Using%20Point%20Cloud%20Representation&body=Title%3A%20Intrinsic%20Image%20Decomposition%20Using%20Point%20Cloud%20Representation%0AAuthor%3A%20Xiaoyan%20Xing%20and%20Konrad%20Groh%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers%0AAbstract%3A%20%20%20The%20purpose%20of%20intrinsic%20decomposition%20is%20to%20separate%20an%20image%20into%20its%0Aalbedo%20%28reflective%20properties%29%20and%20shading%20components%20%28illumination%0Aproperties%29.%20This%20is%20challenging%20because%20it%27s%20an%20ill-posed%20problem.%0AConventional%20approaches%20primarily%20concentrate%20on%202D%20imagery%20and%20fail%20to%20fully%0Aexploit%20the%20capabilities%20of%203D%20data%20representation.%203D%20point%20clouds%20offer%20a%0Amore%20comprehensive%20format%20for%20representing%20scenes%2C%20as%20they%20combine%20geometric%0Aand%20color%20information%20effectively.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20introduce%0APoint%20Intrinsic%20Net%20%28PoInt-Net%29%2C%20which%20leverages%203D%20point%20cloud%20data%20to%0Aconcurrently%20estimate%20albedo%20and%20shading%20maps.%20The%20merits%20of%20PoInt-Net%20include%0Athe%20following%20aspects.%20First%2C%20the%20model%20is%20efficient%2C%20achieving%20consistent%0Aperformance%20across%20point%20clouds%20of%20any%20size%20with%20training%20only%20required%20on%0Asmall-scale%20point%20clouds.%20Second%2C%20it%20exhibits%20remarkable%20robustness%3B%20even%20when%0Atrained%20exclusively%20on%20datasets%20comprising%20individual%20objects%2C%20PoInt-Net%0Ademonstrates%20strong%20generalization%20to%20unseen%20objects%20and%20scenes.%20Third%2C%20it%0Adelivers%20superior%20accuracy%20over%20conventional%202D%20approaches%2C%20demonstrating%0Aenhanced%20performance%20across%20various%20metrics%20on%20different%20datasets.%20%28Code%0AReleased%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.10924v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20Image%20Decomposition%20Using%20Point%20Cloud%20Representation&entry.906535625=Xiaoyan%20Xing%20and%20Konrad%20Groh%20and%20Sezer%20Karaoglu%20and%20Theo%20Gevers&entry.1292438233=%20%20The%20purpose%20of%20intrinsic%20decomposition%20is%20to%20separate%20an%20image%20into%20its%0Aalbedo%20%28reflective%20properties%29%20and%20shading%20components%20%28illumination%0Aproperties%29.%20This%20is%20challenging%20because%20it%27s%20an%20ill-posed%20problem.%0AConventional%20approaches%20primarily%20concentrate%20on%202D%20imagery%20and%20fail%20to%20fully%0Aexploit%20the%20capabilities%20of%203D%20data%20representation.%203D%20point%20clouds%20offer%20a%0Amore%20comprehensive%20format%20for%20representing%20scenes%2C%20as%20they%20combine%20geometric%0Aand%20color%20information%20effectively.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20introduce%0APoint%20Intrinsic%20Net%20%28PoInt-Net%29%2C%20which%20leverages%203D%20point%20cloud%20data%20to%0Aconcurrently%20estimate%20albedo%20and%20shading%20maps.%20The%20merits%20of%20PoInt-Net%20include%0Athe%20following%20aspects.%20First%2C%20the%20model%20is%20efficient%2C%20achieving%20consistent%0Aperformance%20across%20point%20clouds%20of%20any%20size%20with%20training%20only%20required%20on%0Asmall-scale%20point%20clouds.%20Second%2C%20it%20exhibits%20remarkable%20robustness%3B%20even%20when%0Atrained%20exclusively%20on%20datasets%20comprising%20individual%20objects%2C%20PoInt-Net%0Ademonstrates%20strong%20generalization%20to%20unseen%20objects%20and%20scenes.%20Third%2C%20it%0Adelivers%20superior%20accuracy%20over%20conventional%202D%20approaches%2C%20demonstrating%0Aenhanced%20performance%20across%20various%20metrics%20on%20different%20datasets.%20%28Code%0AReleased%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.10924v2&entry.124074799=Read"},
{"title": "Learnable Earth Parser: Discovering 3D Prototypes in Aerial Scans", "author": "Romain Loiseau and Elliot Vincent and Mathieu Aubry and Loic Landrieu", "abstract": "  We propose an unsupervised method for parsing large 3D scans of real-world\nscenes with easily-interpretable shapes. This work aims to provide a practical\ntool for analyzing 3D scenes in the context of aerial surveying and mapping,\nwithout the need for user annotations. Our approach is based on a probabilistic\nreconstruction model that decomposes an input 3D point cloud into a small set\nof learned prototypical 3D shapes. The resulting reconstruction is visually\ninterpretable and can be used to perform unsupervised instance and low-shot\nsemantic segmentation of complex scenes. We demonstrate the usefulness of our\nmodel on a novel dataset of seven large aerial LiDAR scans from diverse\nreal-world scenarios. Our approach outperforms state-of-the-art unsupervised\nmethods in terms of decomposition accuracy while remaining visually\ninterpretable. Our code and dataset are available at\nhttps://romainloiseau.fr/learnable-earth-parser/\n", "link": "http://arxiv.org/abs/2304.09704v2", "date": "2024-03-28", "relevancy": 2.7186, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5656}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5193}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learnable%20Earth%20Parser%3A%20Discovering%203D%20Prototypes%20in%20Aerial%20Scans&body=Title%3A%20Learnable%20Earth%20Parser%3A%20Discovering%203D%20Prototypes%20in%20Aerial%20Scans%0AAuthor%3A%20Romain%20Loiseau%20and%20Elliot%20Vincent%20and%20Mathieu%20Aubry%20and%20Loic%20Landrieu%0AAbstract%3A%20%20%20We%20propose%20an%20unsupervised%20method%20for%20parsing%20large%203D%20scans%20of%20real-world%0Ascenes%20with%20easily-interpretable%20shapes.%20This%20work%20aims%20to%20provide%20a%20practical%0Atool%20for%20analyzing%203D%20scenes%20in%20the%20context%20of%20aerial%20surveying%20and%20mapping%2C%0Awithout%20the%20need%20for%20user%20annotations.%20Our%20approach%20is%20based%20on%20a%20probabilistic%0Areconstruction%20model%20that%20decomposes%20an%20input%203D%20point%20cloud%20into%20a%20small%20set%0Aof%20learned%20prototypical%203D%20shapes.%20The%20resulting%20reconstruction%20is%20visually%0Ainterpretable%20and%20can%20be%20used%20to%20perform%20unsupervised%20instance%20and%20low-shot%0Asemantic%20segmentation%20of%20complex%20scenes.%20We%20demonstrate%20the%20usefulness%20of%20our%0Amodel%20on%20a%20novel%20dataset%20of%20seven%20large%20aerial%20LiDAR%20scans%20from%20diverse%0Areal-world%20scenarios.%20Our%20approach%20outperforms%20state-of-the-art%20unsupervised%0Amethods%20in%20terms%20of%20decomposition%20accuracy%20while%20remaining%20visually%0Ainterpretable.%20Our%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//romainloiseau.fr/learnable-earth-parser/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09704v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Earth%20Parser%3A%20Discovering%203D%20Prototypes%20in%20Aerial%20Scans&entry.906535625=Romain%20Loiseau%20and%20Elliot%20Vincent%20and%20Mathieu%20Aubry%20and%20Loic%20Landrieu&entry.1292438233=%20%20We%20propose%20an%20unsupervised%20method%20for%20parsing%20large%203D%20scans%20of%20real-world%0Ascenes%20with%20easily-interpretable%20shapes.%20This%20work%20aims%20to%20provide%20a%20practical%0Atool%20for%20analyzing%203D%20scenes%20in%20the%20context%20of%20aerial%20surveying%20and%20mapping%2C%0Awithout%20the%20need%20for%20user%20annotations.%20Our%20approach%20is%20based%20on%20a%20probabilistic%0Areconstruction%20model%20that%20decomposes%20an%20input%203D%20point%20cloud%20into%20a%20small%20set%0Aof%20learned%20prototypical%203D%20shapes.%20The%20resulting%20reconstruction%20is%20visually%0Ainterpretable%20and%20can%20be%20used%20to%20perform%20unsupervised%20instance%20and%20low-shot%0Asemantic%20segmentation%20of%20complex%20scenes.%20We%20demonstrate%20the%20usefulness%20of%20our%0Amodel%20on%20a%20novel%20dataset%20of%20seven%20large%20aerial%20LiDAR%20scans%20from%20diverse%0Areal-world%20scenarios.%20Our%20approach%20outperforms%20state-of-the-art%20unsupervised%0Amethods%20in%20terms%20of%20decomposition%20accuracy%20while%20remaining%20visually%0Ainterpretable.%20Our%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//romainloiseau.fr/learnable-earth-parser/%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09704v2&entry.124074799=Read"},
{"title": "CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object\n  Detection", "author": "Mikhail Kennerley and Jian-Gang Wang and Bharadwaj Veeravalli and Robby T. Tan", "abstract": "  Domain adaptive object detection aims to adapt detection models to domains\nwhere annotated data is unavailable. Existing methods have been proposed to\naddress the domain gap using the semi-supervised student-teacher framework.\nHowever, a fundamental issue arises from the class imbalance in the labelled\ntraining set, which can result in inaccurate pseudo-labels. The relationship\nbetween classes, especially where one class is a majority and the other\nminority, has a large impact on class bias. We propose Class-Aware Teacher\n(CAT) to address the class bias issue in the domain adaptation setting. In our\nwork, we approximate the class relationships with our Inter-Class Relation\nmodule (ICRm) and exploit it to reduce the bias within the model. In this way,\nwe are able to apply augmentations to highly related classes, both inter- and\nintra-domain, to boost the performance of minority classes while having minimal\nimpact on majority classes. We further reduce the bias by implementing a\nclass-relation weight to our classification loss. Experiments conducted on\nvarious datasets and ablation studies show that our method is able to address\nthe class bias in the domain adaptation setting. On the Cityscapes to Foggy\nCityscapes dataset, we attained a 52.5 mAP, a substantial improvement over the\n51.2 mAP achieved by the state-of-the-art method.\n", "link": "http://arxiv.org/abs/2403.19278v1", "date": "2024-03-28", "relevancy": 2.7075, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5636}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5369}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.524}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CAT%3A%20Exploiting%20Inter-Class%20Dynamics%20for%20Domain%20Adaptive%20Object%0A%20%20Detection&body=Title%3A%20CAT%3A%20Exploiting%20Inter-Class%20Dynamics%20for%20Domain%20Adaptive%20Object%0A%20%20Detection%0AAuthor%3A%20Mikhail%20Kennerley%20and%20Jian-Gang%20Wang%20and%20Bharadwaj%20Veeravalli%20and%20Robby%20T.%20Tan%0AAbstract%3A%20%20%20Domain%20adaptive%20object%20detection%20aims%20to%20adapt%20detection%20models%20to%20domains%0Awhere%20annotated%20data%20is%20unavailable.%20Existing%20methods%20have%20been%20proposed%20to%0Aaddress%20the%20domain%20gap%20using%20the%20semi-supervised%20student-teacher%20framework.%0AHowever%2C%20a%20fundamental%20issue%20arises%20from%20the%20class%20imbalance%20in%20the%20labelled%0Atraining%20set%2C%20which%20can%20result%20in%20inaccurate%20pseudo-labels.%20The%20relationship%0Abetween%20classes%2C%20especially%20where%20one%20class%20is%20a%20majority%20and%20the%20other%0Aminority%2C%20has%20a%20large%20impact%20on%20class%20bias.%20We%20propose%20Class-Aware%20Teacher%0A%28CAT%29%20to%20address%20the%20class%20bias%20issue%20in%20the%20domain%20adaptation%20setting.%20In%20our%0Awork%2C%20we%20approximate%20the%20class%20relationships%20with%20our%20Inter-Class%20Relation%0Amodule%20%28ICRm%29%20and%20exploit%20it%20to%20reduce%20the%20bias%20within%20the%20model.%20In%20this%20way%2C%0Awe%20are%20able%20to%20apply%20augmentations%20to%20highly%20related%20classes%2C%20both%20inter-%20and%0Aintra-domain%2C%20to%20boost%20the%20performance%20of%20minority%20classes%20while%20having%20minimal%0Aimpact%20on%20majority%20classes.%20We%20further%20reduce%20the%20bias%20by%20implementing%20a%0Aclass-relation%20weight%20to%20our%20classification%20loss.%20Experiments%20conducted%20on%0Avarious%20datasets%20and%20ablation%20studies%20show%20that%20our%20method%20is%20able%20to%20address%0Athe%20class%20bias%20in%20the%20domain%20adaptation%20setting.%20On%20the%20Cityscapes%20to%20Foggy%0ACityscapes%20dataset%2C%20we%20attained%20a%2052.5%20mAP%2C%20a%20substantial%20improvement%20over%20the%0A51.2%20mAP%20achieved%20by%20the%20state-of-the-art%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19278v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT%3A%20Exploiting%20Inter-Class%20Dynamics%20for%20Domain%20Adaptive%20Object%0A%20%20Detection&entry.906535625=Mikhail%20Kennerley%20and%20Jian-Gang%20Wang%20and%20Bharadwaj%20Veeravalli%20and%20Robby%20T.%20Tan&entry.1292438233=%20%20Domain%20adaptive%20object%20detection%20aims%20to%20adapt%20detection%20models%20to%20domains%0Awhere%20annotated%20data%20is%20unavailable.%20Existing%20methods%20have%20been%20proposed%20to%0Aaddress%20the%20domain%20gap%20using%20the%20semi-supervised%20student-teacher%20framework.%0AHowever%2C%20a%20fundamental%20issue%20arises%20from%20the%20class%20imbalance%20in%20the%20labelled%0Atraining%20set%2C%20which%20can%20result%20in%20inaccurate%20pseudo-labels.%20The%20relationship%0Abetween%20classes%2C%20especially%20where%20one%20class%20is%20a%20majority%20and%20the%20other%0Aminority%2C%20has%20a%20large%20impact%20on%20class%20bias.%20We%20propose%20Class-Aware%20Teacher%0A%28CAT%29%20to%20address%20the%20class%20bias%20issue%20in%20the%20domain%20adaptation%20setting.%20In%20our%0Awork%2C%20we%20approximate%20the%20class%20relationships%20with%20our%20Inter-Class%20Relation%0Amodule%20%28ICRm%29%20and%20exploit%20it%20to%20reduce%20the%20bias%20within%20the%20model.%20In%20this%20way%2C%0Awe%20are%20able%20to%20apply%20augmentations%20to%20highly%20related%20classes%2C%20both%20inter-%20and%0Aintra-domain%2C%20to%20boost%20the%20performance%20of%20minority%20classes%20while%20having%20minimal%0Aimpact%20on%20majority%20classes.%20We%20further%20reduce%20the%20bias%20by%20implementing%20a%0Aclass-relation%20weight%20to%20our%20classification%20loss.%20Experiments%20conducted%20on%0Avarious%20datasets%20and%20ablation%20studies%20show%20that%20our%20method%20is%20able%20to%20address%0Athe%20class%20bias%20in%20the%20domain%20adaptation%20setting.%20On%20the%20Cityscapes%20to%20Foggy%0ACityscapes%20dataset%2C%20we%20attained%20a%2052.5%20mAP%2C%20a%20substantial%20improvement%20over%20the%0A51.2%20mAP%20achieved%20by%20the%20state-of-the-art%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19278v1&entry.124074799=Read"},
{"title": "SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion\n  for 3D Scene Graph Alignment and Its Downstream Tasks", "author": "Yaxu Xie and Alain Pagani and Didier Stricker", "abstract": "  Scene graphs have been recently introduced into 3D spatial understanding as a\ncomprehensive representation of the scene. The alignment between 3D scene\ngraphs is the first step of many downstream tasks such as scene graph aided\npoint cloud registration, mosaicking, overlap checking, and robot navigation.\nIn this work, we treat 3D scene graph alignment as a partial graph-matching\nproblem and propose to solve it with a graph neural network. We reuse the\ngeometric features learned by a point cloud registration method and associate\nthe clustered point-level geometric features with the node-level semantic\nfeature via our designed feature fusion module. Partial matching is enabled by\nusing a learnable method to select the top-k similar node pairs. Subsequent\ndownstream tasks such as point cloud registration are achieved by running a\npre-trained registration network within the matched regions. We further propose\na point-matching rescoring method, that uses the node-wise alignment of the 3D\nscene graph to reweight the matching candidates from a pre-trained point cloud\nregistration method. It reduces the false point correspondences estimated\nespecially in low-overlapping cases. Experiments show that our method improves\nthe alignment accuracy by 10~20% in low-overlap and random transformation\nscenarios and outperforms the existing work in multiple downstream tasks.\n", "link": "http://arxiv.org/abs/2403.19474v1", "date": "2024-03-28", "relevancy": 2.7072, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6167}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5105}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4971}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SG-PGM%3A%20Partial%20Graph%20Matching%20Network%20with%20Semantic%20Geometric%20Fusion%0A%20%20for%203D%20Scene%20Graph%20Alignment%20and%20Its%20Downstream%20Tasks&body=Title%3A%20SG-PGM%3A%20Partial%20Graph%20Matching%20Network%20with%20Semantic%20Geometric%20Fusion%0A%20%20for%203D%20Scene%20Graph%20Alignment%20and%20Its%20Downstream%20Tasks%0AAuthor%3A%20Yaxu%20Xie%20and%20Alain%20Pagani%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20Scene%20graphs%20have%20been%20recently%20introduced%20into%203D%20spatial%20understanding%20as%20a%0Acomprehensive%20representation%20of%20the%20scene.%20The%20alignment%20between%203D%20scene%0Agraphs%20is%20the%20first%20step%20of%20many%20downstream%20tasks%20such%20as%20scene%20graph%20aided%0Apoint%20cloud%20registration%2C%20mosaicking%2C%20overlap%20checking%2C%20and%20robot%20navigation.%0AIn%20this%20work%2C%20we%20treat%203D%20scene%20graph%20alignment%20as%20a%20partial%20graph-matching%0Aproblem%20and%20propose%20to%20solve%20it%20with%20a%20graph%20neural%20network.%20We%20reuse%20the%0Ageometric%20features%20learned%20by%20a%20point%20cloud%20registration%20method%20and%20associate%0Athe%20clustered%20point-level%20geometric%20features%20with%20the%20node-level%20semantic%0Afeature%20via%20our%20designed%20feature%20fusion%20module.%20Partial%20matching%20is%20enabled%20by%0Ausing%20a%20learnable%20method%20to%20select%20the%20top-k%20similar%20node%20pairs.%20Subsequent%0Adownstream%20tasks%20such%20as%20point%20cloud%20registration%20are%20achieved%20by%20running%20a%0Apre-trained%20registration%20network%20within%20the%20matched%20regions.%20We%20further%20propose%0Aa%20point-matching%20rescoring%20method%2C%20that%20uses%20the%20node-wise%20alignment%20of%20the%203D%0Ascene%20graph%20to%20reweight%20the%20matching%20candidates%20from%20a%20pre-trained%20point%20cloud%0Aregistration%20method.%20It%20reduces%20the%20false%20point%20correspondences%20estimated%0Aespecially%20in%20low-overlapping%20cases.%20Experiments%20show%20that%20our%20method%20improves%0Athe%20alignment%20accuracy%20by%2010~20%25%20in%20low-overlap%20and%20random%20transformation%0Ascenarios%20and%20outperforms%20the%20existing%20work%20in%20multiple%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19474v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG-PGM%3A%20Partial%20Graph%20Matching%20Network%20with%20Semantic%20Geometric%20Fusion%0A%20%20for%203D%20Scene%20Graph%20Alignment%20and%20Its%20Downstream%20Tasks&entry.906535625=Yaxu%20Xie%20and%20Alain%20Pagani%20and%20Didier%20Stricker&entry.1292438233=%20%20Scene%20graphs%20have%20been%20recently%20introduced%20into%203D%20spatial%20understanding%20as%20a%0Acomprehensive%20representation%20of%20the%20scene.%20The%20alignment%20between%203D%20scene%0Agraphs%20is%20the%20first%20step%20of%20many%20downstream%20tasks%20such%20as%20scene%20graph%20aided%0Apoint%20cloud%20registration%2C%20mosaicking%2C%20overlap%20checking%2C%20and%20robot%20navigation.%0AIn%20this%20work%2C%20we%20treat%203D%20scene%20graph%20alignment%20as%20a%20partial%20graph-matching%0Aproblem%20and%20propose%20to%20solve%20it%20with%20a%20graph%20neural%20network.%20We%20reuse%20the%0Ageometric%20features%20learned%20by%20a%20point%20cloud%20registration%20method%20and%20associate%0Athe%20clustered%20point-level%20geometric%20features%20with%20the%20node-level%20semantic%0Afeature%20via%20our%20designed%20feature%20fusion%20module.%20Partial%20matching%20is%20enabled%20by%0Ausing%20a%20learnable%20method%20to%20select%20the%20top-k%20similar%20node%20pairs.%20Subsequent%0Adownstream%20tasks%20such%20as%20point%20cloud%20registration%20are%20achieved%20by%20running%20a%0Apre-trained%20registration%20network%20within%20the%20matched%20regions.%20We%20further%20propose%0Aa%20point-matching%20rescoring%20method%2C%20that%20uses%20the%20node-wise%20alignment%20of%20the%203D%0Ascene%20graph%20to%20reweight%20the%20matching%20candidates%20from%20a%20pre-trained%20point%20cloud%0Aregistration%20method.%20It%20reduces%20the%20false%20point%20correspondences%20estimated%0Aespecially%20in%20low-overlapping%20cases.%20Experiments%20show%20that%20our%20method%20improves%0Athe%20alignment%20accuracy%20by%2010~20%25%20in%20low-overlap%20and%20random%20transformation%0Ascenarios%20and%20outperforms%20the%20existing%20work%20in%20multiple%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19474v1&entry.124074799=Read"},
{"title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions", "author": "Kai Zhang and Yi Luan and Hexiang Hu and Kenton Lee and Siyuan Qiao and Wenhu Chen and Yu Su and Ming-Wei Chang", "abstract": "  Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.\n", "link": "http://arxiv.org/abs/2403.19651v1", "date": "2024-03-28", "relevancy": 2.6709, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5487}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5325}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MagicLens%3A%20Self-Supervised%20Image%20Retrieval%20with%20Open-Ended%20Instructions&body=Title%3A%20MagicLens%3A%20Self-Supervised%20Image%20Retrieval%20with%20Open-Ended%20Instructions%0AAuthor%3A%20Kai%20Zhang%20and%20Yi%20Luan%20and%20Hexiang%20Hu%20and%20Kenton%20Lee%20and%20Siyuan%20Qiao%20and%20Wenhu%20Chen%20and%20Yu%20Su%20and%20Ming-Wei%20Chang%0AAbstract%3A%20%20%20Image%20retrieval%2C%20i.e.%2C%20finding%20desired%20images%20given%20a%20reference%20image%2C%0Ainherently%20encompasses%20rich%2C%20multi-faceted%20search%20intents%20that%20are%20difficult%20to%0Acapture%20solely%20using%20image-based%20measures.%20Recent%20work%20leverages%20text%0Ainstructions%20to%20allow%20users%20to%20more%20freely%20express%20their%20search%20intents.%0AHowever%2C%20existing%20work%20primarily%20focuses%20on%20image%20pairs%20that%20are%20visually%0Asimilar%20and/or%20can%20be%20characterized%20by%20a%20small%20set%20of%20pre-defined%20relations.%0AThe%20core%20thesis%20of%20this%20paper%20is%20that%20text%20instructions%20can%20enable%20retrieving%0Aimages%20with%20richer%20relations%20beyond%20visual%20similarity.%20To%20show%20this%2C%20we%0Aintroduce%20MagicLens%2C%20a%20series%20of%20self-supervised%20image%20retrieval%20models%20that%0Asupport%20open-ended%20instructions.%20MagicLens%20is%20built%20on%20a%20key%20novel%20insight%3A%0Aimage%20pairs%20that%20naturally%20occur%20on%20the%20same%20web%20pages%20contain%20a%20wide%20range%20of%0Aimplicit%20relations%20%28e.g.%2C%20inside%20view%20of%29%2C%20and%20we%20can%20bring%20those%20implicit%0Arelations%20explicit%20by%20synthesizing%20instructions%20via%20large%20multimodal%20models%0A%28LMMs%29%20and%20large%20language%20models%20%28LLMs%29.%20Trained%20on%2036.7M%20%28query%20image%2C%0Ainstruction%2C%20target%20image%29%20triplets%20with%20rich%20semantic%20relations%20mined%20from%20the%0Aweb%2C%20MagicLens%20achieves%20comparable%20or%20better%20results%20on%20eight%20benchmarks%20of%0Avarious%20image%20retrieval%20tasks%20than%20prior%20state-of-the-art%20%28SOTA%29%20methods.%0ARemarkably%2C%20it%20outperforms%20previous%20SOTA%20but%20with%20a%2050X%20smaller%20model%20size%20on%0Amultiple%20benchmarks.%20Additional%20human%20analyses%20on%20a%201.4M-image%20unseen%20corpus%0Afurther%20demonstrate%20the%20diversity%20of%20search%20intents%20supported%20by%20MagicLens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19651v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicLens%3A%20Self-Supervised%20Image%20Retrieval%20with%20Open-Ended%20Instructions&entry.906535625=Kai%20Zhang%20and%20Yi%20Luan%20and%20Hexiang%20Hu%20and%20Kenton%20Lee%20and%20Siyuan%20Qiao%20and%20Wenhu%20Chen%20and%20Yu%20Su%20and%20Ming-Wei%20Chang&entry.1292438233=%20%20Image%20retrieval%2C%20i.e.%2C%20finding%20desired%20images%20given%20a%20reference%20image%2C%0Ainherently%20encompasses%20rich%2C%20multi-faceted%20search%20intents%20that%20are%20difficult%20to%0Acapture%20solely%20using%20image-based%20measures.%20Recent%20work%20leverages%20text%0Ainstructions%20to%20allow%20users%20to%20more%20freely%20express%20their%20search%20intents.%0AHowever%2C%20existing%20work%20primarily%20focuses%20on%20image%20pairs%20that%20are%20visually%0Asimilar%20and/or%20can%20be%20characterized%20by%20a%20small%20set%20of%20pre-defined%20relations.%0AThe%20core%20thesis%20of%20this%20paper%20is%20that%20text%20instructions%20can%20enable%20retrieving%0Aimages%20with%20richer%20relations%20beyond%20visual%20similarity.%20To%20show%20this%2C%20we%0Aintroduce%20MagicLens%2C%20a%20series%20of%20self-supervised%20image%20retrieval%20models%20that%0Asupport%20open-ended%20instructions.%20MagicLens%20is%20built%20on%20a%20key%20novel%20insight%3A%0Aimage%20pairs%20that%20naturally%20occur%20on%20the%20same%20web%20pages%20contain%20a%20wide%20range%20of%0Aimplicit%20relations%20%28e.g.%2C%20inside%20view%20of%29%2C%20and%20we%20can%20bring%20those%20implicit%0Arelations%20explicit%20by%20synthesizing%20instructions%20via%20large%20multimodal%20models%0A%28LMMs%29%20and%20large%20language%20models%20%28LLMs%29.%20Trained%20on%2036.7M%20%28query%20image%2C%0Ainstruction%2C%20target%20image%29%20triplets%20with%20rich%20semantic%20relations%20mined%20from%20the%0Aweb%2C%20MagicLens%20achieves%20comparable%20or%20better%20results%20on%20eight%20benchmarks%20of%0Avarious%20image%20retrieval%20tasks%20than%20prior%20state-of-the-art%20%28SOTA%29%20methods.%0ARemarkably%2C%20it%20outperforms%20previous%20SOTA%20but%20with%20a%2050X%20smaller%20model%20size%20on%0Amultiple%20benchmarks.%20Additional%20human%20analyses%20on%20a%201.4M-image%20unseen%20corpus%0Afurther%20demonstrate%20the%20diversity%20of%20search%20intents%20supported%20by%20MagicLens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19651v1&entry.124074799=Read"},
{"title": "Sparse 3D Reconstruction via Object-Centric Ray Sampling", "author": "Llukman Cerkezi and Paolo Favaro", "abstract": "  We propose a novel method for 3D object reconstruction from a sparse set of\nviews captured from a 360-degree calibrated camera rig. We represent the object\nsurface through a hybrid model that uses both an MLP-based neural\nrepresentation and a triangle mesh. A key contribution in our work is a novel\nobject-centric sampling scheme of the neural representation, where rays are\nshared among all views. This efficiently concentrates and reduces the number of\nsamples used to update the neural model at each iteration. This sampling scheme\nrelies on the mesh representation to ensure also that samples are\nwell-distributed along its normals. The rendering is then performed efficiently\nby a differentiable renderer. We demonstrate that this sampling scheme results\nin a more effective training of the neural representation, does not require the\nadditional supervision of segmentation masks, yields state of the art 3D\nreconstructions, and works with sparse views on the Google's Scanned Objects,\nTank and Temples and MVMC Car datasets. Code available at:\nhttps://github.com/llukmancerkezi/ROSTER\n", "link": "http://arxiv.org/abs/2309.03008v2", "date": "2024-03-28", "relevancy": 2.6476, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5357}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5309}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.522}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sparse%203D%20Reconstruction%20via%20Object-Centric%20Ray%20Sampling&body=Title%3A%20Sparse%203D%20Reconstruction%20via%20Object-Centric%20Ray%20Sampling%0AAuthor%3A%20Llukman%20Cerkezi%20and%20Paolo%20Favaro%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%20for%203D%20object%20reconstruction%20from%20a%20sparse%20set%20of%0Aviews%20captured%20from%20a%20360-degree%20calibrated%20camera%20rig.%20We%20represent%20the%20object%0Asurface%20through%20a%20hybrid%20model%20that%20uses%20both%20an%20MLP-based%20neural%0Arepresentation%20and%20a%20triangle%20mesh.%20A%20key%20contribution%20in%20our%20work%20is%20a%20novel%0Aobject-centric%20sampling%20scheme%20of%20the%20neural%20representation%2C%20where%20rays%20are%0Ashared%20among%20all%20views.%20This%20efficiently%20concentrates%20and%20reduces%20the%20number%20of%0Asamples%20used%20to%20update%20the%20neural%20model%20at%20each%20iteration.%20This%20sampling%20scheme%0Arelies%20on%20the%20mesh%20representation%20to%20ensure%20also%20that%20samples%20are%0Awell-distributed%20along%20its%20normals.%20The%20rendering%20is%20then%20performed%20efficiently%0Aby%20a%20differentiable%20renderer.%20We%20demonstrate%20that%20this%20sampling%20scheme%20results%0Ain%20a%20more%20effective%20training%20of%20the%20neural%20representation%2C%20does%20not%20require%20the%0Aadditional%20supervision%20of%20segmentation%20masks%2C%20yields%20state%20of%20the%20art%203D%0Areconstructions%2C%20and%20works%20with%20sparse%20views%20on%20the%20Google%27s%20Scanned%20Objects%2C%0ATank%20and%20Temples%20and%20MVMC%20Car%20datasets.%20Code%20available%20at%3A%0Ahttps%3A//github.com/llukmancerkezi/ROSTER%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.03008v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%203D%20Reconstruction%20via%20Object-Centric%20Ray%20Sampling&entry.906535625=Llukman%20Cerkezi%20and%20Paolo%20Favaro&entry.1292438233=%20%20We%20propose%20a%20novel%20method%20for%203D%20object%20reconstruction%20from%20a%20sparse%20set%20of%0Aviews%20captured%20from%20a%20360-degree%20calibrated%20camera%20rig.%20We%20represent%20the%20object%0Asurface%20through%20a%20hybrid%20model%20that%20uses%20both%20an%20MLP-based%20neural%0Arepresentation%20and%20a%20triangle%20mesh.%20A%20key%20contribution%20in%20our%20work%20is%20a%20novel%0Aobject-centric%20sampling%20scheme%20of%20the%20neural%20representation%2C%20where%20rays%20are%0Ashared%20among%20all%20views.%20This%20efficiently%20concentrates%20and%20reduces%20the%20number%20of%0Asamples%20used%20to%20update%20the%20neural%20model%20at%20each%20iteration.%20This%20sampling%20scheme%0Arelies%20on%20the%20mesh%20representation%20to%20ensure%20also%20that%20samples%20are%0Awell-distributed%20along%20its%20normals.%20The%20rendering%20is%20then%20performed%20efficiently%0Aby%20a%20differentiable%20renderer.%20We%20demonstrate%20that%20this%20sampling%20scheme%20results%0Ain%20a%20more%20effective%20training%20of%20the%20neural%20representation%2C%20does%20not%20require%20the%0Aadditional%20supervision%20of%20segmentation%20masks%2C%20yields%20state%20of%20the%20art%203D%0Areconstructions%2C%20and%20works%20with%20sparse%20views%20on%20the%20Google%27s%20Scanned%20Objects%2C%0ATank%20and%20Temples%20and%20MVMC%20Car%20datasets.%20Code%20available%20at%3A%0Ahttps%3A//github.com/llukmancerkezi/ROSTER%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.03008v2&entry.124074799=Read"},
{"title": "Jointly Training and Pruning CNNs via Learnable Agent Guidance and\n  Alignment", "author": "Alireza Ganjdanesh and Shangqian Gao and Heng Huang", "abstract": "  Structural model pruning is a prominent approach used for reducing the\ncomputational cost of Convolutional Neural Networks (CNNs) before their\ndeployment on resource-constrained devices. Yet, the majority of proposed ideas\nrequire a pretrained model before pruning, which is costly to secure. In this\npaper, we propose a novel structural pruning approach to jointly learn the\nweights and structurally prune architectures of CNN models. The core element of\nour method is a Reinforcement Learning (RL) agent whose actions determine the\npruning ratios of the CNN model's layers, and the resulting model's accuracy\nserves as its reward. We conduct the joint training and pruning by iteratively\ntraining the model's weights and the agent's policy, and we regularize the\nmodel's weights to align with the selected structure by the agent. The evolving\nmodel's weights result in a dynamic reward function for the agent, which\nprevents using prominent episodic RL methods with stationary environment\nassumption for our purpose. We address this challenge by designing a mechanism\nto model the complex changing dynamics of the reward function and provide a\nrepresentation of it to the RL agent. To do so, we take a learnable embedding\nfor each training epoch and employ a recurrent model to calculate a\nrepresentation of the changing environment. We train the recurrent model and\nembeddings using a decoder model to reconstruct observed rewards. Such a design\nempowers our agent to effectively leverage episodic observations along with the\nenvironment representations to learn a proper policy to determine performant\nsub-networks of the CNN model. Our extensive experiments on CIFAR-10 and\nImageNet using ResNets and MobileNets demonstrate the effectiveness of our\nmethod.\n", "link": "http://arxiv.org/abs/2403.19490v1", "date": "2024-03-28", "relevancy": 2.6409, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5357}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5068}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Jointly%20Training%20and%20Pruning%20CNNs%20via%20Learnable%20Agent%20Guidance%20and%0A%20%20Alignment&body=Title%3A%20Jointly%20Training%20and%20Pruning%20CNNs%20via%20Learnable%20Agent%20Guidance%20and%0A%20%20Alignment%0AAuthor%3A%20Alireza%20Ganjdanesh%20and%20Shangqian%20Gao%20and%20Heng%20Huang%0AAbstract%3A%20%20%20Structural%20model%20pruning%20is%20a%20prominent%20approach%20used%20for%20reducing%20the%0Acomputational%20cost%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29%20before%20their%0Adeployment%20on%20resource-constrained%20devices.%20Yet%2C%20the%20majority%20of%20proposed%20ideas%0Arequire%20a%20pretrained%20model%20before%20pruning%2C%20which%20is%20costly%20to%20secure.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20structural%20pruning%20approach%20to%20jointly%20learn%20the%0Aweights%20and%20structurally%20prune%20architectures%20of%20CNN%20models.%20The%20core%20element%20of%0Aour%20method%20is%20a%20Reinforcement%20Learning%20%28RL%29%20agent%20whose%20actions%20determine%20the%0Apruning%20ratios%20of%20the%20CNN%20model%27s%20layers%2C%20and%20the%20resulting%20model%27s%20accuracy%0Aserves%20as%20its%20reward.%20We%20conduct%20the%20joint%20training%20and%20pruning%20by%20iteratively%0Atraining%20the%20model%27s%20weights%20and%20the%20agent%27s%20policy%2C%20and%20we%20regularize%20the%0Amodel%27s%20weights%20to%20align%20with%20the%20selected%20structure%20by%20the%20agent.%20The%20evolving%0Amodel%27s%20weights%20result%20in%20a%20dynamic%20reward%20function%20for%20the%20agent%2C%20which%0Aprevents%20using%20prominent%20episodic%20RL%20methods%20with%20stationary%20environment%0Aassumption%20for%20our%20purpose.%20We%20address%20this%20challenge%20by%20designing%20a%20mechanism%0Ato%20model%20the%20complex%20changing%20dynamics%20of%20the%20reward%20function%20and%20provide%20a%0Arepresentation%20of%20it%20to%20the%20RL%20agent.%20To%20do%20so%2C%20we%20take%20a%20learnable%20embedding%0Afor%20each%20training%20epoch%20and%20employ%20a%20recurrent%20model%20to%20calculate%20a%0Arepresentation%20of%20the%20changing%20environment.%20We%20train%20the%20recurrent%20model%20and%0Aembeddings%20using%20a%20decoder%20model%20to%20reconstruct%20observed%20rewards.%20Such%20a%20design%0Aempowers%20our%20agent%20to%20effectively%20leverage%20episodic%20observations%20along%20with%20the%0Aenvironment%20representations%20to%20learn%20a%20proper%20policy%20to%20determine%20performant%0Asub-networks%20of%20the%20CNN%20model.%20Our%20extensive%20experiments%20on%20CIFAR-10%20and%0AImageNet%20using%20ResNets%20and%20MobileNets%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19490v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jointly%20Training%20and%20Pruning%20CNNs%20via%20Learnable%20Agent%20Guidance%20and%0A%20%20Alignment&entry.906535625=Alireza%20Ganjdanesh%20and%20Shangqian%20Gao%20and%20Heng%20Huang&entry.1292438233=%20%20Structural%20model%20pruning%20is%20a%20prominent%20approach%20used%20for%20reducing%20the%0Acomputational%20cost%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29%20before%20their%0Adeployment%20on%20resource-constrained%20devices.%20Yet%2C%20the%20majority%20of%20proposed%20ideas%0Arequire%20a%20pretrained%20model%20before%20pruning%2C%20which%20is%20costly%20to%20secure.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20structural%20pruning%20approach%20to%20jointly%20learn%20the%0Aweights%20and%20structurally%20prune%20architectures%20of%20CNN%20models.%20The%20core%20element%20of%0Aour%20method%20is%20a%20Reinforcement%20Learning%20%28RL%29%20agent%20whose%20actions%20determine%20the%0Apruning%20ratios%20of%20the%20CNN%20model%27s%20layers%2C%20and%20the%20resulting%20model%27s%20accuracy%0Aserves%20as%20its%20reward.%20We%20conduct%20the%20joint%20training%20and%20pruning%20by%20iteratively%0Atraining%20the%20model%27s%20weights%20and%20the%20agent%27s%20policy%2C%20and%20we%20regularize%20the%0Amodel%27s%20weights%20to%20align%20with%20the%20selected%20structure%20by%20the%20agent.%20The%20evolving%0Amodel%27s%20weights%20result%20in%20a%20dynamic%20reward%20function%20for%20the%20agent%2C%20which%0Aprevents%20using%20prominent%20episodic%20RL%20methods%20with%20stationary%20environment%0Aassumption%20for%20our%20purpose.%20We%20address%20this%20challenge%20by%20designing%20a%20mechanism%0Ato%20model%20the%20complex%20changing%20dynamics%20of%20the%20reward%20function%20and%20provide%20a%0Arepresentation%20of%20it%20to%20the%20RL%20agent.%20To%20do%20so%2C%20we%20take%20a%20learnable%20embedding%0Afor%20each%20training%20epoch%20and%20employ%20a%20recurrent%20model%20to%20calculate%20a%0Arepresentation%20of%20the%20changing%20environment.%20We%20train%20the%20recurrent%20model%20and%0Aembeddings%20using%20a%20decoder%20model%20to%20reconstruct%20observed%20rewards.%20Such%20a%20design%0Aempowers%20our%20agent%20to%20effectively%20leverage%20episodic%20observations%20along%20with%20the%0Aenvironment%20representations%20to%20learn%20a%20proper%20policy%20to%20determine%20performant%0Asub-networks%20of%20the%20CNN%20model.%20Our%20extensive%20experiments%20on%20CIFAR-10%20and%0AImageNet%20using%20ResNets%20and%20MobileNets%20demonstrate%20the%20effectiveness%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19490v1&entry.124074799=Read"},
{"title": "LocCa: Visual Pretraining with Location-aware Captioners", "author": "Bo Wan and Michael Tschannen and Yongqin Xian and Filip Pavetic and Ibrahim Alabdulmohsin and Xiao Wang and Andr\u00e9 Susano Pinto and Andreas Steiner and Lucas Beyer and Xiaohua Zhai", "abstract": "  Image captioning has been shown as an effective pretraining method similar to\ncontrastive pretraining. However, the incorporation of location-aware\ninformation into visual pretraining remains an area with limited research. In\nthis paper, we propose a simple visual pretraining method with location-aware\ncaptioners (LocCa). LocCa uses a simple image captioner task interface, to\nteach a model to read out rich information, i.e. bounding box coordinates, and\ncaptions, conditioned on the image pixel input. Thanks to the multitask\ncapabilities of an encoder-decoder architecture, we show that an image\ncaptioner can easily handle multiple tasks during pretraining. Our experiments\ndemonstrate that LocCa outperforms standard captioners significantly on\nlocalization downstream tasks while maintaining comparable performance on\nholistic tasks.\n", "link": "http://arxiv.org/abs/2403.19596v1", "date": "2024-03-28", "relevancy": 2.6384, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5473}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5207}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LocCa%3A%20Visual%20Pretraining%20with%20Location-aware%20Captioners&body=Title%3A%20LocCa%3A%20Visual%20Pretraining%20with%20Location-aware%20Captioners%0AAuthor%3A%20Bo%20Wan%20and%20Michael%20Tschannen%20and%20Yongqin%20Xian%20and%20Filip%20Pavetic%20and%20Ibrahim%20Alabdulmohsin%20and%20Xiao%20Wang%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Andreas%20Steiner%20and%20Lucas%20Beyer%20and%20Xiaohua%20Zhai%0AAbstract%3A%20%20%20Image%20captioning%20has%20been%20shown%20as%20an%20effective%20pretraining%20method%20similar%20to%0Acontrastive%20pretraining.%20However%2C%20the%20incorporation%20of%20location-aware%0Ainformation%20into%20visual%20pretraining%20remains%20an%20area%20with%20limited%20research.%20In%0Athis%20paper%2C%20we%20propose%20a%20simple%20visual%20pretraining%20method%20with%20location-aware%0Acaptioners%20%28LocCa%29.%20LocCa%20uses%20a%20simple%20image%20captioner%20task%20interface%2C%20to%0Ateach%20a%20model%20to%20read%20out%20rich%20information%2C%20i.e.%20bounding%20box%20coordinates%2C%20and%0Acaptions%2C%20conditioned%20on%20the%20image%20pixel%20input.%20Thanks%20to%20the%20multitask%0Acapabilities%20of%20an%20encoder-decoder%20architecture%2C%20we%20show%20that%20an%20image%0Acaptioner%20can%20easily%20handle%20multiple%20tasks%20during%20pretraining.%20Our%20experiments%0Ademonstrate%20that%20LocCa%20outperforms%20standard%20captioners%20significantly%20on%0Alocalization%20downstream%20tasks%20while%20maintaining%20comparable%20performance%20on%0Aholistic%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19596v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocCa%3A%20Visual%20Pretraining%20with%20Location-aware%20Captioners&entry.906535625=Bo%20Wan%20and%20Michael%20Tschannen%20and%20Yongqin%20Xian%20and%20Filip%20Pavetic%20and%20Ibrahim%20Alabdulmohsin%20and%20Xiao%20Wang%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Andreas%20Steiner%20and%20Lucas%20Beyer%20and%20Xiaohua%20Zhai&entry.1292438233=%20%20Image%20captioning%20has%20been%20shown%20as%20an%20effective%20pretraining%20method%20similar%20to%0Acontrastive%20pretraining.%20However%2C%20the%20incorporation%20of%20location-aware%0Ainformation%20into%20visual%20pretraining%20remains%20an%20area%20with%20limited%20research.%20In%0Athis%20paper%2C%20we%20propose%20a%20simple%20visual%20pretraining%20method%20with%20location-aware%0Acaptioners%20%28LocCa%29.%20LocCa%20uses%20a%20simple%20image%20captioner%20task%20interface%2C%20to%0Ateach%20a%20model%20to%20read%20out%20rich%20information%2C%20i.e.%20bounding%20box%20coordinates%2C%20and%0Acaptions%2C%20conditioned%20on%20the%20image%20pixel%20input.%20Thanks%20to%20the%20multitask%0Acapabilities%20of%20an%20encoder-decoder%20architecture%2C%20we%20show%20that%20an%20image%0Acaptioner%20can%20easily%20handle%20multiple%20tasks%20during%20pretraining.%20Our%20experiments%0Ademonstrate%20that%20LocCa%20outperforms%20standard%20captioners%20significantly%20on%0Alocalization%20downstream%20tasks%20while%20maintaining%20comparable%20performance%20on%0Aholistic%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19596v1&entry.124074799=Read"},
{"title": "Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets", "author": "Patrick Grommelt and Louis Weiss and Franz-Josef Pfreundt and Janis Keuper", "abstract": "  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n", "link": "http://arxiv.org/abs/2403.17608v2", "date": "2024-03-28", "relevancy": 2.6253, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5394}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5201}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5157}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fake%20or%20JPEG%3F%20Revealing%20Common%20Biases%20in%20Generated%20Image%20Detection%0A%20%20Datasets&body=Title%3A%20Fake%20or%20JPEG%3F%20Revealing%20Common%20Biases%20in%20Generated%20Image%20Detection%0A%20%20Datasets%0AAuthor%3A%20Patrick%20Grommelt%20and%20Louis%20Weiss%20and%20Franz-Josef%20Pfreundt%20and%20Janis%20Keuper%0AAbstract%3A%20%20%20The%20widespread%20adoption%20of%20generative%20image%20models%20has%20highlighted%20the%20urgent%0Aneed%20to%20detect%20artificial%20content%2C%20which%20is%20a%20crucial%20step%20in%20combating%0Awidespread%20manipulation%20and%20misinformation.%20Consequently%2C%20numerous%20detectors%0Aand%20associated%20datasets%20have%20emerged.%20However%2C%20many%20of%20these%20datasets%0Ainadvertently%20introduce%20undesirable%20biases%2C%20thereby%20impacting%20the%20effectiveness%0Aand%20evaluation%20of%20detectors.%20In%20this%20paper%2C%20we%20emphasize%20that%20many%20datasets%20for%0AAI-generated%20image%20detection%20contain%20biases%20related%20to%20JPEG%20compression%20and%0Aimage%20size.%20Using%20the%20GenImage%20dataset%2C%20we%20demonstrate%20that%20detectors%20indeed%0Alearn%20from%20these%20undesired%20factors.%20Furthermore%2C%20we%20show%20that%20removing%20the%0Anamed%20biases%20substantially%20increases%20robustness%20to%20JPEG%20compression%20and%0Asignificantly%20alters%20the%20cross-generator%20performance%20of%20evaluated%20detectors.%0ASpecifically%2C%20it%20leads%20to%20more%20than%2011%20percentage%20points%20increase%20in%0Across-generator%20performance%20for%20ResNet50%20and%20Swin-T%20detectors%20on%20the%20GenImage%0Adataset%2C%20achieving%20state-of-the-art%20results.%0A%20%20We%20provide%20the%20dataset%20and%20source%20codes%20of%20this%20paper%20on%20the%20anonymous%0Awebsite%3A%20https%3A//www.unbiased-genimage.org%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17608v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fake%20or%20JPEG%3F%20Revealing%20Common%20Biases%20in%20Generated%20Image%20Detection%0A%20%20Datasets&entry.906535625=Patrick%20Grommelt%20and%20Louis%20Weiss%20and%20Franz-Josef%20Pfreundt%20and%20Janis%20Keuper&entry.1292438233=%20%20The%20widespread%20adoption%20of%20generative%20image%20models%20has%20highlighted%20the%20urgent%0Aneed%20to%20detect%20artificial%20content%2C%20which%20is%20a%20crucial%20step%20in%20combating%0Awidespread%20manipulation%20and%20misinformation.%20Consequently%2C%20numerous%20detectors%0Aand%20associated%20datasets%20have%20emerged.%20However%2C%20many%20of%20these%20datasets%0Ainadvertently%20introduce%20undesirable%20biases%2C%20thereby%20impacting%20the%20effectiveness%0Aand%20evaluation%20of%20detectors.%20In%20this%20paper%2C%20we%20emphasize%20that%20many%20datasets%20for%0AAI-generated%20image%20detection%20contain%20biases%20related%20to%20JPEG%20compression%20and%0Aimage%20size.%20Using%20the%20GenImage%20dataset%2C%20we%20demonstrate%20that%20detectors%20indeed%0Alearn%20from%20these%20undesired%20factors.%20Furthermore%2C%20we%20show%20that%20removing%20the%0Anamed%20biases%20substantially%20increases%20robustness%20to%20JPEG%20compression%20and%0Asignificantly%20alters%20the%20cross-generator%20performance%20of%20evaluated%20detectors.%0ASpecifically%2C%20it%20leads%20to%20more%20than%2011%20percentage%20points%20increase%20in%0Across-generator%20performance%20for%20ResNet50%20and%20Swin-T%20detectors%20on%20the%20GenImage%0Adataset%2C%20achieving%20state-of-the-art%20results.%0A%20%20We%20provide%20the%20dataset%20and%20source%20codes%20of%20this%20paper%20on%20the%20anonymous%0Awebsite%3A%20https%3A//www.unbiased-genimage.org%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17608v2&entry.124074799=Read"},
{"title": "CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians", "author": "Avinash Paliwal and Wei Ye and Jinhui Xiong and Dmytro Kotovenko and Rakesh Ranjan and Vikas Chandra and Nima Khademi Kalantari", "abstract": "  The field of 3D reconstruction from images has rapidly evolved in the past\nfew years, first with the introduction of Neural Radiance Field (NeRF) and more\nrecently with 3D Gaussian Splatting (3DGS). The latter provides a significant\nedge over NeRF in terms of the training and inference speed, as well as the\nreconstruction quality. Although 3DGS works well for dense input images, the\nunstructured point-cloud like representation quickly overfits to the more\nchallenging setup of extremely sparse input images (e.g., 3 images), creating a\nrepresentation that appears as a jumble of needles from novel views. To address\nthis issue, we propose regularized optimization and depth-based initialization.\nOur key idea is to introduce a structured Gaussian representation that can be\ncontrolled in 2D image space. We then constraint the Gaussians, in particular\ntheir position, and prevent them from moving independently during optimization.\nSpecifically, we introduce single and multiview constraints through an implicit\nconvolutional decoder and a total variation loss, respectively. With the\ncoherency introduced to the Gaussians, we further constrain the optimization\nthrough a flow-based loss function. To support our regularized optimization, we\npropose an approach to initialize the Gaussians using monocular depth estimates\nat each input view. We demonstrate significant improvements compared to the\nstate-of-the-art sparse-view NeRF-based approaches on a variety of scenes.\n", "link": "http://arxiv.org/abs/2403.19495v1", "date": "2024-03-28", "relevancy": 2.6081, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5334}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5217}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5098}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CoherentGS%3A%20Sparse%20Novel%20View%20Synthesis%20with%20Coherent%203D%20Gaussians&body=Title%3A%20CoherentGS%3A%20Sparse%20Novel%20View%20Synthesis%20with%20Coherent%203D%20Gaussians%0AAuthor%3A%20Avinash%20Paliwal%20and%20Wei%20Ye%20and%20Jinhui%20Xiong%20and%20Dmytro%20Kotovenko%20and%20Rakesh%20Ranjan%20and%20Vikas%20Chandra%20and%20Nima%20Khademi%20Kalantari%0AAbstract%3A%20%20%20The%20field%20of%203D%20reconstruction%20from%20images%20has%20rapidly%20evolved%20in%20the%20past%0Afew%20years%2C%20first%20with%20the%20introduction%20of%20Neural%20Radiance%20Field%20%28NeRF%29%20and%20more%0Arecently%20with%203D%20Gaussian%20Splatting%20%283DGS%29.%20The%20latter%20provides%20a%20significant%0Aedge%20over%20NeRF%20in%20terms%20of%20the%20training%20and%20inference%20speed%2C%20as%20well%20as%20the%0Areconstruction%20quality.%20Although%203DGS%20works%20well%20for%20dense%20input%20images%2C%20the%0Aunstructured%20point-cloud%20like%20representation%20quickly%20overfits%20to%20the%20more%0Achallenging%20setup%20of%20extremely%20sparse%20input%20images%20%28e.g.%2C%203%20images%29%2C%20creating%20a%0Arepresentation%20that%20appears%20as%20a%20jumble%20of%20needles%20from%20novel%20views.%20To%20address%0Athis%20issue%2C%20we%20propose%20regularized%20optimization%20and%20depth-based%20initialization.%0AOur%20key%20idea%20is%20to%20introduce%20a%20structured%20Gaussian%20representation%20that%20can%20be%0Acontrolled%20in%202D%20image%20space.%20We%20then%20constraint%20the%20Gaussians%2C%20in%20particular%0Atheir%20position%2C%20and%20prevent%20them%20from%20moving%20independently%20during%20optimization.%0ASpecifically%2C%20we%20introduce%20single%20and%20multiview%20constraints%20through%20an%20implicit%0Aconvolutional%20decoder%20and%20a%20total%20variation%20loss%2C%20respectively.%20With%20the%0Acoherency%20introduced%20to%20the%20Gaussians%2C%20we%20further%20constrain%20the%20optimization%0Athrough%20a%20flow-based%20loss%20function.%20To%20support%20our%20regularized%20optimization%2C%20we%0Apropose%20an%20approach%20to%20initialize%20the%20Gaussians%20using%20monocular%20depth%20estimates%0Aat%20each%20input%20view.%20We%20demonstrate%20significant%20improvements%20compared%20to%20the%0Astate-of-the-art%20sparse-view%20NeRF-based%20approaches%20on%20a%20variety%20of%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19495v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoherentGS%3A%20Sparse%20Novel%20View%20Synthesis%20with%20Coherent%203D%20Gaussians&entry.906535625=Avinash%20Paliwal%20and%20Wei%20Ye%20and%20Jinhui%20Xiong%20and%20Dmytro%20Kotovenko%20and%20Rakesh%20Ranjan%20and%20Vikas%20Chandra%20and%20Nima%20Khademi%20Kalantari&entry.1292438233=%20%20The%20field%20of%203D%20reconstruction%20from%20images%20has%20rapidly%20evolved%20in%20the%20past%0Afew%20years%2C%20first%20with%20the%20introduction%20of%20Neural%20Radiance%20Field%20%28NeRF%29%20and%20more%0Arecently%20with%203D%20Gaussian%20Splatting%20%283DGS%29.%20The%20latter%20provides%20a%20significant%0Aedge%20over%20NeRF%20in%20terms%20of%20the%20training%20and%20inference%20speed%2C%20as%20well%20as%20the%0Areconstruction%20quality.%20Although%203DGS%20works%20well%20for%20dense%20input%20images%2C%20the%0Aunstructured%20point-cloud%20like%20representation%20quickly%20overfits%20to%20the%20more%0Achallenging%20setup%20of%20extremely%20sparse%20input%20images%20%28e.g.%2C%203%20images%29%2C%20creating%20a%0Arepresentation%20that%20appears%20as%20a%20jumble%20of%20needles%20from%20novel%20views.%20To%20address%0Athis%20issue%2C%20we%20propose%20regularized%20optimization%20and%20depth-based%20initialization.%0AOur%20key%20idea%20is%20to%20introduce%20a%20structured%20Gaussian%20representation%20that%20can%20be%0Acontrolled%20in%202D%20image%20space.%20We%20then%20constraint%20the%20Gaussians%2C%20in%20particular%0Atheir%20position%2C%20and%20prevent%20them%20from%20moving%20independently%20during%20optimization.%0ASpecifically%2C%20we%20introduce%20single%20and%20multiview%20constraints%20through%20an%20implicit%0Aconvolutional%20decoder%20and%20a%20total%20variation%20loss%2C%20respectively.%20With%20the%0Acoherency%20introduced%20to%20the%20Gaussians%2C%20we%20further%20constrain%20the%20optimization%0Athrough%20a%20flow-based%20loss%20function.%20To%20support%20our%20regularized%20optimization%2C%20we%0Apropose%20an%20approach%20to%20initialize%20the%20Gaussians%20using%20monocular%20depth%20estimates%0Aat%20each%20input%20view.%20We%20demonstrate%20significant%20improvements%20compared%20to%20the%0Astate-of-the-art%20sparse-view%20NeRF-based%20approaches%20on%20a%20variety%20of%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19495v1&entry.124074799=Read"},
{"title": "EgoThink: Evaluating First-Person Perspective Thinking Capability of\n  Vision-Language Models", "author": "Sijie Cheng and Zhicheng Guo and Jingwen Wu and Kechen Fang and Peng Li and Huaping Liu and Yang Liu", "abstract": "  Vision-language models (VLMs) have recently shown promising results in\ntraditional downstream tasks. Evaluation studies have emerged to assess their\nabilities, with the majority focusing on the third-person perspective, and only\na few addressing specific tasks from the first-person perspective. However, the\ncapability of VLMs to \"think\" from a first-person perspective, a crucial\nattribute for advancing autonomous agents and robotics, remains largely\nunexplored. To bridge this research gap, we introduce EgoThink, a novel visual\nquestion-answering benchmark that encompasses six core capabilities with twelve\ndetailed dimensions. The benchmark is constructed using selected clips from\negocentric videos, with manually annotated question-answer pairs containing\nfirst-person information. To comprehensively assess VLMs, we evaluate eighteen\npopular VLMs on EgoThink. Moreover, given the open-ended format of the answers,\nwe use GPT-4 as the automatic judge to compute single-answer grading.\nExperimental results indicate that although GPT-4V leads in numerous\ndimensions, all evaluated VLMs still possess considerable potential for\nimprovement in first-person perspective tasks. Meanwhile, enlarging the number\nof trainable parameters has the most significant impact on model performance on\nEgoThink. In conclusion, EgoThink serves as a valuable addition to existing\nevaluation benchmarks for VLMs, providing an indispensable resource for future\nresearch in the realm of embodied artificial intelligence and robotics.\n", "link": "http://arxiv.org/abs/2311.15596v2", "date": "2024-03-28", "relevancy": 2.5544, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5285}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5034}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5008}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EgoThink%3A%20Evaluating%20First-Person%20Perspective%20Thinking%20Capability%20of%0A%20%20Vision-Language%20Models&body=Title%3A%20EgoThink%3A%20Evaluating%20First-Person%20Perspective%20Thinking%20Capability%20of%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Sijie%20Cheng%20and%20Zhicheng%20Guo%20and%20Jingwen%20Wu%20and%20Kechen%20Fang%20and%20Peng%20Li%20and%20Huaping%20Liu%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20recently%20shown%20promising%20results%20in%0Atraditional%20downstream%20tasks.%20Evaluation%20studies%20have%20emerged%20to%20assess%20their%0Aabilities%2C%20with%20the%20majority%20focusing%20on%20the%20third-person%20perspective%2C%20and%20only%0Aa%20few%20addressing%20specific%20tasks%20from%20the%20first-person%20perspective.%20However%2C%20the%0Acapability%20of%20VLMs%20to%20%22think%22%20from%20a%20first-person%20perspective%2C%20a%20crucial%0Aattribute%20for%20advancing%20autonomous%20agents%20and%20robotics%2C%20remains%20largely%0Aunexplored.%20To%20bridge%20this%20research%20gap%2C%20we%20introduce%20EgoThink%2C%20a%20novel%20visual%0Aquestion-answering%20benchmark%20that%20encompasses%20six%20core%20capabilities%20with%20twelve%0Adetailed%20dimensions.%20The%20benchmark%20is%20constructed%20using%20selected%20clips%20from%0Aegocentric%20videos%2C%20with%20manually%20annotated%20question-answer%20pairs%20containing%0Afirst-person%20information.%20To%20comprehensively%20assess%20VLMs%2C%20we%20evaluate%20eighteen%0Apopular%20VLMs%20on%20EgoThink.%20Moreover%2C%20given%20the%20open-ended%20format%20of%20the%20answers%2C%0Awe%20use%20GPT-4%20as%20the%20automatic%20judge%20to%20compute%20single-answer%20grading.%0AExperimental%20results%20indicate%20that%20although%20GPT-4V%20leads%20in%20numerous%0Adimensions%2C%20all%20evaluated%20VLMs%20still%20possess%20considerable%20potential%20for%0Aimprovement%20in%20first-person%20perspective%20tasks.%20Meanwhile%2C%20enlarging%20the%20number%0Aof%20trainable%20parameters%20has%20the%20most%20significant%20impact%20on%20model%20performance%20on%0AEgoThink.%20In%20conclusion%2C%20EgoThink%20serves%20as%20a%20valuable%20addition%20to%20existing%0Aevaluation%20benchmarks%20for%20VLMs%2C%20providing%20an%20indispensable%20resource%20for%20future%0Aresearch%20in%20the%20realm%20of%20embodied%20artificial%20intelligence%20and%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15596v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoThink%3A%20Evaluating%20First-Person%20Perspective%20Thinking%20Capability%20of%0A%20%20Vision-Language%20Models&entry.906535625=Sijie%20Cheng%20and%20Zhicheng%20Guo%20and%20Jingwen%20Wu%20and%20Kechen%20Fang%20and%20Peng%20Li%20and%20Huaping%20Liu%20and%20Yang%20Liu&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20recently%20shown%20promising%20results%20in%0Atraditional%20downstream%20tasks.%20Evaluation%20studies%20have%20emerged%20to%20assess%20their%0Aabilities%2C%20with%20the%20majority%20focusing%20on%20the%20third-person%20perspective%2C%20and%20only%0Aa%20few%20addressing%20specific%20tasks%20from%20the%20first-person%20perspective.%20However%2C%20the%0Acapability%20of%20VLMs%20to%20%22think%22%20from%20a%20first-person%20perspective%2C%20a%20crucial%0Aattribute%20for%20advancing%20autonomous%20agents%20and%20robotics%2C%20remains%20largely%0Aunexplored.%20To%20bridge%20this%20research%20gap%2C%20we%20introduce%20EgoThink%2C%20a%20novel%20visual%0Aquestion-answering%20benchmark%20that%20encompasses%20six%20core%20capabilities%20with%20twelve%0Adetailed%20dimensions.%20The%20benchmark%20is%20constructed%20using%20selected%20clips%20from%0Aegocentric%20videos%2C%20with%20manually%20annotated%20question-answer%20pairs%20containing%0Afirst-person%20information.%20To%20comprehensively%20assess%20VLMs%2C%20we%20evaluate%20eighteen%0Apopular%20VLMs%20on%20EgoThink.%20Moreover%2C%20given%20the%20open-ended%20format%20of%20the%20answers%2C%0Awe%20use%20GPT-4%20as%20the%20automatic%20judge%20to%20compute%20single-answer%20grading.%0AExperimental%20results%20indicate%20that%20although%20GPT-4V%20leads%20in%20numerous%0Adimensions%2C%20all%20evaluated%20VLMs%20still%20possess%20considerable%20potential%20for%0Aimprovement%20in%20first-person%20perspective%20tasks.%20Meanwhile%2C%20enlarging%20the%20number%0Aof%20trainable%20parameters%20has%20the%20most%20significant%20impact%20on%20model%20performance%20on%0AEgoThink.%20In%20conclusion%2C%20EgoThink%20serves%20as%20a%20valuable%20addition%20to%20existing%0Aevaluation%20benchmarks%20for%20VLMs%2C%20providing%20an%20indispensable%20resource%20for%20future%0Aresearch%20in%20the%20realm%20of%20embodied%20artificial%20intelligence%20and%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15596v2&entry.124074799=Read"},
{"title": "CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network", "author": "Jie Wen and Zheng Zhang and Yong Xu and Bob Zhang and Lunke Fei and Guo-Sen Xie", "abstract": "  In recent years, incomplete multi-view clustering, which studies the\nchallenging multi-view clustering problem on missing views, has received\ngrowing research interests. Although a series of methods have been proposed to\naddress this issue, the following problems still exist: 1) Almost all of the\nexisting methods are based on shallow models, which is difficult to obtain\ndiscriminative common representations. 2) These methods are generally sensitive\nto noise or outliers since the negative samples are treated equally as the\nimportant samples. In this paper, we propose a novel incomplete multi-view\nclustering network, called Cognitive Deep Incomplete Multi-view Clustering\nNetwork (CDIMC-net), to address these issues. Specifically, it captures the\nhigh-level features and local structure of each view by incorporating the\nview-specific deep encoders and graph embedding strategy into a framework.\nMoreover, based on the human cognition, i.e., learning from easy to hard, it\nintroduces a self-paced strategy to select the most confident samples for model\ntraining, which can reduce the negative influence of outliers. Experimental\nresults on several incomplete datasets show that CDIMC-net outperforms the\nstate-of-the-art incomplete multi-view clustering methods.\n", "link": "http://arxiv.org/abs/2403.19514v1", "date": "2024-03-28", "relevancy": 2.5465, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5034}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4957}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CDIMC-net%3A%20Cognitive%20Deep%20Incomplete%20Multi-view%20Clustering%20Network&body=Title%3A%20CDIMC-net%3A%20Cognitive%20Deep%20Incomplete%20Multi-view%20Clustering%20Network%0AAuthor%3A%20Jie%20Wen%20and%20Zheng%20Zhang%20and%20Yong%20Xu%20and%20Bob%20Zhang%20and%20Lunke%20Fei%20and%20Guo-Sen%20Xie%0AAbstract%3A%20%20%20In%20recent%20years%2C%20incomplete%20multi-view%20clustering%2C%20which%20studies%20the%0Achallenging%20multi-view%20clustering%20problem%20on%20missing%20views%2C%20has%20received%0Agrowing%20research%20interests.%20Although%20a%20series%20of%20methods%20have%20been%20proposed%20to%0Aaddress%20this%20issue%2C%20the%20following%20problems%20still%20exist%3A%201%29%20Almost%20all%20of%20the%0Aexisting%20methods%20are%20based%20on%20shallow%20models%2C%20which%20is%20difficult%20to%20obtain%0Adiscriminative%20common%20representations.%202%29%20These%20methods%20are%20generally%20sensitive%0Ato%20noise%20or%20outliers%20since%20the%20negative%20samples%20are%20treated%20equally%20as%20the%0Aimportant%20samples.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20incomplete%20multi-view%0Aclustering%20network%2C%20called%20Cognitive%20Deep%20Incomplete%20Multi-view%20Clustering%0ANetwork%20%28CDIMC-net%29%2C%20to%20address%20these%20issues.%20Specifically%2C%20it%20captures%20the%0Ahigh-level%20features%20and%20local%20structure%20of%20each%20view%20by%20incorporating%20the%0Aview-specific%20deep%20encoders%20and%20graph%20embedding%20strategy%20into%20a%20framework.%0AMoreover%2C%20based%20on%20the%20human%20cognition%2C%20i.e.%2C%20learning%20from%20easy%20to%20hard%2C%20it%0Aintroduces%20a%20self-paced%20strategy%20to%20select%20the%20most%20confident%20samples%20for%20model%0Atraining%2C%20which%20can%20reduce%20the%20negative%20influence%20of%20outliers.%20Experimental%0Aresults%20on%20several%20incomplete%20datasets%20show%20that%20CDIMC-net%20outperforms%20the%0Astate-of-the-art%20incomplete%20multi-view%20clustering%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19514v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDIMC-net%3A%20Cognitive%20Deep%20Incomplete%20Multi-view%20Clustering%20Network&entry.906535625=Jie%20Wen%20and%20Zheng%20Zhang%20and%20Yong%20Xu%20and%20Bob%20Zhang%20and%20Lunke%20Fei%20and%20Guo-Sen%20Xie&entry.1292438233=%20%20In%20recent%20years%2C%20incomplete%20multi-view%20clustering%2C%20which%20studies%20the%0Achallenging%20multi-view%20clustering%20problem%20on%20missing%20views%2C%20has%20received%0Agrowing%20research%20interests.%20Although%20a%20series%20of%20methods%20have%20been%20proposed%20to%0Aaddress%20this%20issue%2C%20the%20following%20problems%20still%20exist%3A%201%29%20Almost%20all%20of%20the%0Aexisting%20methods%20are%20based%20on%20shallow%20models%2C%20which%20is%20difficult%20to%20obtain%0Adiscriminative%20common%20representations.%202%29%20These%20methods%20are%20generally%20sensitive%0Ato%20noise%20or%20outliers%20since%20the%20negative%20samples%20are%20treated%20equally%20as%20the%0Aimportant%20samples.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20incomplete%20multi-view%0Aclustering%20network%2C%20called%20Cognitive%20Deep%20Incomplete%20Multi-view%20Clustering%0ANetwork%20%28CDIMC-net%29%2C%20to%20address%20these%20issues.%20Specifically%2C%20it%20captures%20the%0Ahigh-level%20features%20and%20local%20structure%20of%20each%20view%20by%20incorporating%20the%0Aview-specific%20deep%20encoders%20and%20graph%20embedding%20strategy%20into%20a%20framework.%0AMoreover%2C%20based%20on%20the%20human%20cognition%2C%20i.e.%2C%20learning%20from%20easy%20to%20hard%2C%20it%0Aintroduces%20a%20self-paced%20strategy%20to%20select%20the%20most%20confident%20samples%20for%20model%0Atraining%2C%20which%20can%20reduce%20the%20negative%20influence%20of%20outliers.%20Experimental%0Aresults%20on%20several%20incomplete%20datasets%20show%20that%20CDIMC-net%20outperforms%20the%0Astate-of-the-art%20incomplete%20multi-view%20clustering%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19514v1&entry.124074799=Read"},
{"title": "DeepSample: DNN sampling-based testing for operational accuracy\n  assessment", "author": "Antonio Guerriero and Roberto Pietrantuono and Stefano Russo", "abstract": "  Deep Neural Networks (DNN) are core components for classification and\nregression tasks of many software systems. Companies incur in high costs for\ntesting DNN with datasets representative of the inputs expected in operation,\nas these need to be manually labelled. The challenge is to select a\nrepresentative set of test inputs as small as possible to reduce the labelling\ncost, while sufficing to yield unbiased high-confidence estimates of the\nexpected DNN accuracy. At the same time, testers are interested in exposing as\nmany DNN mispredictions as possible to improve the DNN, ending up in the need\nfor techniques pursuing a threefold aim: small dataset size, trustworthy\nestimates, mispredictions exposure. This study presents DeepSample, a family of\nDNN testing techniques for cost-effective accuracy assessment based on\nprobabilistic sampling. We investigate whether, to what extent, and under which\nconditions probabilistic sampling can help to tackle the outlined challenge. We\nimplement five new sampling-based testing techniques, and perform a\ncomprehensive comparison of such techniques and of three further\nstate-of-the-art techniques for both DNN classification and regression tasks.\nResults serve as guidance for best use of sampling-based testing for faithful\nand high-confidence estimates of DNN accuracy in operation at low cost.\n", "link": "http://arxiv.org/abs/2403.19271v1", "date": "2024-03-28", "relevancy": 2.4888, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5185}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4979}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4769}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DeepSample%3A%20DNN%20sampling-based%20testing%20for%20operational%20accuracy%0A%20%20assessment&body=Title%3A%20DeepSample%3A%20DNN%20sampling-based%20testing%20for%20operational%20accuracy%0A%20%20assessment%0AAuthor%3A%20Antonio%20Guerriero%20and%20Roberto%20Pietrantuono%20and%20Stefano%20Russo%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNN%29%20are%20core%20components%20for%20classification%20and%0Aregression%20tasks%20of%20many%20software%20systems.%20Companies%20incur%20in%20high%20costs%20for%0Atesting%20DNN%20with%20datasets%20representative%20of%20the%20inputs%20expected%20in%20operation%2C%0Aas%20these%20need%20to%20be%20manually%20labelled.%20The%20challenge%20is%20to%20select%20a%0Arepresentative%20set%20of%20test%20inputs%20as%20small%20as%20possible%20to%20reduce%20the%20labelling%0Acost%2C%20while%20sufficing%20to%20yield%20unbiased%20high-confidence%20estimates%20of%20the%0Aexpected%20DNN%20accuracy.%20At%20the%20same%20time%2C%20testers%20are%20interested%20in%20exposing%20as%0Amany%20DNN%20mispredictions%20as%20possible%20to%20improve%20the%20DNN%2C%20ending%20up%20in%20the%20need%0Afor%20techniques%20pursuing%20a%20threefold%20aim%3A%20small%20dataset%20size%2C%20trustworthy%0Aestimates%2C%20mispredictions%20exposure.%20This%20study%20presents%20DeepSample%2C%20a%20family%20of%0ADNN%20testing%20techniques%20for%20cost-effective%20accuracy%20assessment%20based%20on%0Aprobabilistic%20sampling.%20We%20investigate%20whether%2C%20to%20what%20extent%2C%20and%20under%20which%0Aconditions%20probabilistic%20sampling%20can%20help%20to%20tackle%20the%20outlined%20challenge.%20We%0Aimplement%20five%20new%20sampling-based%20testing%20techniques%2C%20and%20perform%20a%0Acomprehensive%20comparison%20of%20such%20techniques%20and%20of%20three%20further%0Astate-of-the-art%20techniques%20for%20both%20DNN%20classification%20and%20regression%20tasks.%0AResults%20serve%20as%20guidance%20for%20best%20use%20of%20sampling-based%20testing%20for%20faithful%0Aand%20high-confidence%20estimates%20of%20DNN%20accuracy%20in%20operation%20at%20low%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19271v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSample%3A%20DNN%20sampling-based%20testing%20for%20operational%20accuracy%0A%20%20assessment&entry.906535625=Antonio%20Guerriero%20and%20Roberto%20Pietrantuono%20and%20Stefano%20Russo&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNN%29%20are%20core%20components%20for%20classification%20and%0Aregression%20tasks%20of%20many%20software%20systems.%20Companies%20incur%20in%20high%20costs%20for%0Atesting%20DNN%20with%20datasets%20representative%20of%20the%20inputs%20expected%20in%20operation%2C%0Aas%20these%20need%20to%20be%20manually%20labelled.%20The%20challenge%20is%20to%20select%20a%0Arepresentative%20set%20of%20test%20inputs%20as%20small%20as%20possible%20to%20reduce%20the%20labelling%0Acost%2C%20while%20sufficing%20to%20yield%20unbiased%20high-confidence%20estimates%20of%20the%0Aexpected%20DNN%20accuracy.%20At%20the%20same%20time%2C%20testers%20are%20interested%20in%20exposing%20as%0Amany%20DNN%20mispredictions%20as%20possible%20to%20improve%20the%20DNN%2C%20ending%20up%20in%20the%20need%0Afor%20techniques%20pursuing%20a%20threefold%20aim%3A%20small%20dataset%20size%2C%20trustworthy%0Aestimates%2C%20mispredictions%20exposure.%20This%20study%20presents%20DeepSample%2C%20a%20family%20of%0ADNN%20testing%20techniques%20for%20cost-effective%20accuracy%20assessment%20based%20on%0Aprobabilistic%20sampling.%20We%20investigate%20whether%2C%20to%20what%20extent%2C%20and%20under%20which%0Aconditions%20probabilistic%20sampling%20can%20help%20to%20tackle%20the%20outlined%20challenge.%20We%0Aimplement%20five%20new%20sampling-based%20testing%20techniques%2C%20and%20perform%20a%0Acomprehensive%20comparison%20of%20such%20techniques%20and%20of%20three%20further%0Astate-of-the-art%20techniques%20for%20both%20DNN%20classification%20and%20regression%20tasks.%0AResults%20serve%20as%20guidance%20for%20best%20use%20of%20sampling-based%20testing%20for%20faithful%0Aand%20high-confidence%20estimates%20of%20DNN%20accuracy%20in%20operation%20at%20low%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19271v1&entry.124074799=Read"},
{"title": "MedBN: Robust Test-Time Adaptation against Malicious Test Samples", "author": "Hyejin Park and Jeongyeon Hwang and Sunung Mun and Sangdon Park and Jungseul Ok", "abstract": "  Test-time adaptation (TTA) has emerged as a promising solution to address\nperformance decay due to unforeseen distribution shifts between training and\ntest data. While recent TTA methods excel in adapting to test data variations,\nsuch adaptability exposes a model to vulnerability against malicious examples,\nan aspect that has received limited attention. Previous studies have uncovered\nsecurity vulnerabilities within TTA even when a small proportion of the test\nbatch is maliciously manipulated. In response to the emerging threat, we\npropose median batch normalization (MedBN), leveraging the robustness of the\nmedian for statistics estimation within the batch normalization layer during\ntest-time inference. Our method is algorithm-agnostic, thus allowing seamless\nintegration with existing TTA frameworks. Our experimental results on benchmark\ndatasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently\ndemonstrate that MedBN outperforms existing approaches in maintaining robust\nperformance across different attack scenarios, encompassing both instant and\ncumulative attacks. Through extensive experiments, we show that our approach\nsustains the performance even in the absence of attacks, achieving a practical\nbalance between robustness and performance.\n", "link": "http://arxiv.org/abs/2403.19326v1", "date": "2024-03-28", "relevancy": 2.4613, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5073}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4899}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4796}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MedBN%3A%20Robust%20Test-Time%20Adaptation%20against%20Malicious%20Test%20Samples&body=Title%3A%20MedBN%3A%20Robust%20Test-Time%20Adaptation%20against%20Malicious%20Test%20Samples%0AAuthor%3A%20Hyejin%20Park%20and%20Jeongyeon%20Hwang%20and%20Sunung%20Mun%20and%20Sangdon%20Park%20and%20Jungseul%20Ok%0AAbstract%3A%20%20%20Test-time%20adaptation%20%28TTA%29%20has%20emerged%20as%20a%20promising%20solution%20to%20address%0Aperformance%20decay%20due%20to%20unforeseen%20distribution%20shifts%20between%20training%20and%0Atest%20data.%20While%20recent%20TTA%20methods%20excel%20in%20adapting%20to%20test%20data%20variations%2C%0Asuch%20adaptability%20exposes%20a%20model%20to%20vulnerability%20against%20malicious%20examples%2C%0Aan%20aspect%20that%20has%20received%20limited%20attention.%20Previous%20studies%20have%20uncovered%0Asecurity%20vulnerabilities%20within%20TTA%20even%20when%20a%20small%20proportion%20of%20the%20test%0Abatch%20is%20maliciously%20manipulated.%20In%20response%20to%20the%20emerging%20threat%2C%20we%0Apropose%20median%20batch%20normalization%20%28MedBN%29%2C%20leveraging%20the%20robustness%20of%20the%0Amedian%20for%20statistics%20estimation%20within%20the%20batch%20normalization%20layer%20during%0Atest-time%20inference.%20Our%20method%20is%20algorithm-agnostic%2C%20thus%20allowing%20seamless%0Aintegration%20with%20existing%20TTA%20frameworks.%20Our%20experimental%20results%20on%20benchmark%0Adatasets%2C%20including%20CIFAR10-C%2C%20CIFAR100-C%20and%20ImageNet-C%2C%20consistently%0Ademonstrate%20that%20MedBN%20outperforms%20existing%20approaches%20in%20maintaining%20robust%0Aperformance%20across%20different%20attack%20scenarios%2C%20encompassing%20both%20instant%20and%0Acumulative%20attacks.%20Through%20extensive%20experiments%2C%20we%20show%20that%20our%20approach%0Asustains%20the%20performance%20even%20in%20the%20absence%20of%20attacks%2C%20achieving%20a%20practical%0Abalance%20between%20robustness%20and%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19326v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedBN%3A%20Robust%20Test-Time%20Adaptation%20against%20Malicious%20Test%20Samples&entry.906535625=Hyejin%20Park%20and%20Jeongyeon%20Hwang%20and%20Sunung%20Mun%20and%20Sangdon%20Park%20and%20Jungseul%20Ok&entry.1292438233=%20%20Test-time%20adaptation%20%28TTA%29%20has%20emerged%20as%20a%20promising%20solution%20to%20address%0Aperformance%20decay%20due%20to%20unforeseen%20distribution%20shifts%20between%20training%20and%0Atest%20data.%20While%20recent%20TTA%20methods%20excel%20in%20adapting%20to%20test%20data%20variations%2C%0Asuch%20adaptability%20exposes%20a%20model%20to%20vulnerability%20against%20malicious%20examples%2C%0Aan%20aspect%20that%20has%20received%20limited%20attention.%20Previous%20studies%20have%20uncovered%0Asecurity%20vulnerabilities%20within%20TTA%20even%20when%20a%20small%20proportion%20of%20the%20test%0Abatch%20is%20maliciously%20manipulated.%20In%20response%20to%20the%20emerging%20threat%2C%20we%0Apropose%20median%20batch%20normalization%20%28MedBN%29%2C%20leveraging%20the%20robustness%20of%20the%0Amedian%20for%20statistics%20estimation%20within%20the%20batch%20normalization%20layer%20during%0Atest-time%20inference.%20Our%20method%20is%20algorithm-agnostic%2C%20thus%20allowing%20seamless%0Aintegration%20with%20existing%20TTA%20frameworks.%20Our%20experimental%20results%20on%20benchmark%0Adatasets%2C%20including%20CIFAR10-C%2C%20CIFAR100-C%20and%20ImageNet-C%2C%20consistently%0Ademonstrate%20that%20MedBN%20outperforms%20existing%20approaches%20in%20maintaining%20robust%0Aperformance%20across%20different%20attack%20scenarios%2C%20encompassing%20both%20instant%20and%0Acumulative%20attacks.%20Through%20extensive%20experiments%2C%20we%20show%20that%20our%20approach%0Asustains%20the%20performance%20even%20in%20the%20absence%20of%20attacks%2C%20achieving%20a%20practical%0Abalance%20between%20robustness%20and%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19326v1&entry.124074799=Read"},
{"title": "SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping", "author": "Hana Sebia and Thomas Guyet and Etienne Audureau", "abstract": "  Tensor decomposition has recently been gaining attention in the machine\nlearning community for the analysis of individual traces, such as Electronic\nHealth Records (EHR). However, this task becomes significantly more difficult\nwhen the data follows complex temporal patterns. This paper introduces the\nnotion of a temporal phenotype as an arrangement of features over time and it\nproposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel\nmethod to discover hidden temporal patterns. SWoTTeD integrates several\nconstraints and regularizations to enhance the interpretability of the\nextracted phenotypes. We validate our proposal using both synthetic and\nreal-world datasets, and we present an original usecase using data from the\nGreater Paris University Hospital. The results show that SWoTTeD achieves at\nleast as accurate reconstruction as recent state-of-the-art tensor\ndecomposition models, and extracts temporal phenotypes that are meaningful for\nclinicians.\n", "link": "http://arxiv.org/abs/2310.01201v3", "date": "2024-03-28", "relevancy": 2.4594, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5245}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4896}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4615}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SWoTTeD%3A%20An%20Extension%20of%20Tensor%20Decomposition%20to%20Temporal%20Phenotyping&body=Title%3A%20SWoTTeD%3A%20An%20Extension%20of%20Tensor%20Decomposition%20to%20Temporal%20Phenotyping%0AAuthor%3A%20Hana%20Sebia%20and%20Thomas%20Guyet%20and%20Etienne%20Audureau%0AAbstract%3A%20%20%20Tensor%20decomposition%20has%20recently%20been%20gaining%20attention%20in%20the%20machine%0Alearning%20community%20for%20the%20analysis%20of%20individual%20traces%2C%20such%20as%20Electronic%0AHealth%20Records%20%28EHR%29.%20However%2C%20this%20task%20becomes%20significantly%20more%20difficult%0Awhen%20the%20data%20follows%20complex%20temporal%20patterns.%20This%20paper%20introduces%20the%0Anotion%20of%20a%20temporal%20phenotype%20as%20an%20arrangement%20of%20features%20over%20time%20and%20it%0Aproposes%20SWoTTeD%20%28Sliding%20Window%20for%20Temporal%20Tensor%20Decomposition%29%2C%20a%20novel%0Amethod%20to%20discover%20hidden%20temporal%20patterns.%20SWoTTeD%20integrates%20several%0Aconstraints%20and%20regularizations%20to%20enhance%20the%20interpretability%20of%20the%0Aextracted%20phenotypes.%20We%20validate%20our%20proposal%20using%20both%20synthetic%20and%0Areal-world%20datasets%2C%20and%20we%20present%20an%20original%20usecase%20using%20data%20from%20the%0AGreater%20Paris%20University%20Hospital.%20The%20results%20show%20that%20SWoTTeD%20achieves%20at%0Aleast%20as%20accurate%20reconstruction%20as%20recent%20state-of-the-art%20tensor%0Adecomposition%20models%2C%20and%20extracts%20temporal%20phenotypes%20that%20are%20meaningful%20for%0Aclinicians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01201v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWoTTeD%3A%20An%20Extension%20of%20Tensor%20Decomposition%20to%20Temporal%20Phenotyping&entry.906535625=Hana%20Sebia%20and%20Thomas%20Guyet%20and%20Etienne%20Audureau&entry.1292438233=%20%20Tensor%20decomposition%20has%20recently%20been%20gaining%20attention%20in%20the%20machine%0Alearning%20community%20for%20the%20analysis%20of%20individual%20traces%2C%20such%20as%20Electronic%0AHealth%20Records%20%28EHR%29.%20However%2C%20this%20task%20becomes%20significantly%20more%20difficult%0Awhen%20the%20data%20follows%20complex%20temporal%20patterns.%20This%20paper%20introduces%20the%0Anotion%20of%20a%20temporal%20phenotype%20as%20an%20arrangement%20of%20features%20over%20time%20and%20it%0Aproposes%20SWoTTeD%20%28Sliding%20Window%20for%20Temporal%20Tensor%20Decomposition%29%2C%20a%20novel%0Amethod%20to%20discover%20hidden%20temporal%20patterns.%20SWoTTeD%20integrates%20several%0Aconstraints%20and%20regularizations%20to%20enhance%20the%20interpretability%20of%20the%0Aextracted%20phenotypes.%20We%20validate%20our%20proposal%20using%20both%20synthetic%20and%0Areal-world%20datasets%2C%20and%20we%20present%20an%20original%20usecase%20using%20data%20from%20the%0AGreater%20Paris%20University%20Hospital.%20The%20results%20show%20that%20SWoTTeD%20achieves%20at%0Aleast%20as%20accurate%20reconstruction%20as%20recent%20state-of-the-art%20tensor%0Adecomposition%20models%2C%20and%20extracts%20temporal%20phenotypes%20that%20are%20meaningful%20for%0Aclinicians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01201v3&entry.124074799=Read"},
{"title": "Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A\n  GRU LSTM Hybrid Approach", "author": "Ramin Mousa and Mitra Khezli and Mohamadreza Azadi and Vahid Nikoofard and Saba Hesaraki", "abstract": "  Accurate classification of objects in 3D point clouds is a significant\nproblem in several applications, such as autonomous navigation and\naugmented/virtual reality scenarios, which has become a research hot spot. In\nthis paper, we presented a deep learning strategy for 3D object classification\nin augmented reality. The proposed approach is a combination of the GRU and\nLSTM. LSTM networks learn longer dependencies well, but due to the number of\ngates, it takes longer to train; on the other hand, GRU networks have a weaker\nperformance than LSTM, but their training speed is much higher than GRU, which\nis The speed is due to its fewer gates. The proposed approach used the\ncombination of speed and accuracy of these two networks. The proposed approach\nachieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes\neight classes (unlabeled, man-made terrain, natural terrain, high vegetation,\nlow vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the\ntraditional machine learning approaches could achieve a maximum accuracy of\n0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality,\nHybrid Model, GRULSTM, GRU, LSTM\n", "link": "http://arxiv.org/abs/2403.05950v2", "date": "2024-03-28", "relevancy": 2.4585, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4989}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4882}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Classifying%20Objects%20in%203D%20Point%20Clouds%20Using%20Recurrent%20Neural%20Network%3A%20A%0A%20%20GRU%20LSTM%20Hybrid%20Approach&body=Title%3A%20Classifying%20Objects%20in%203D%20Point%20Clouds%20Using%20Recurrent%20Neural%20Network%3A%20A%0A%20%20GRU%20LSTM%20Hybrid%20Approach%0AAuthor%3A%20Ramin%20Mousa%20and%20Mitra%20Khezli%20and%20Mohamadreza%20Azadi%20and%20Vahid%20Nikoofard%20and%20Saba%20Hesaraki%0AAbstract%3A%20%20%20Accurate%20classification%20of%20objects%20in%203D%20point%20clouds%20is%20a%20significant%0Aproblem%20in%20several%20applications%2C%20such%20as%20autonomous%20navigation%20and%0Aaugmented/virtual%20reality%20scenarios%2C%20which%20has%20become%20a%20research%20hot%20spot.%20In%0Athis%20paper%2C%20we%20presented%20a%20deep%20learning%20strategy%20for%203D%20object%20classification%0Ain%20augmented%20reality.%20The%20proposed%20approach%20is%20a%20combination%20of%20the%20GRU%20and%0ALSTM.%20LSTM%20networks%20learn%20longer%20dependencies%20well%2C%20but%20due%20to%20the%20number%20of%0Agates%2C%20it%20takes%20longer%20to%20train%3B%20on%20the%20other%20hand%2C%20GRU%20networks%20have%20a%20weaker%0Aperformance%20than%20LSTM%2C%20but%20their%20training%20speed%20is%20much%20higher%20than%20GRU%2C%20which%0Ais%20The%20speed%20is%20due%20to%20its%20fewer%20gates.%20The%20proposed%20approach%20used%20the%0Acombination%20of%20speed%20and%20accuracy%20of%20these%20two%20networks.%20The%20proposed%20approach%0Aachieved%20an%20accuracy%20of%200.99%20in%20the%204%2C499%2C0641%20points%20dataset%2C%20which%20includes%0Aeight%20classes%20%28unlabeled%2C%20man-made%20terrain%2C%20natural%20terrain%2C%20high%20vegetation%2C%0Alow%20vegetation%2C%20buildings%2C%20hardscape%2C%20scanning%20artifacts%2C%20cars%29.%20Meanwhile%2C%20the%0Atraditional%20machine%20learning%20approaches%20could%20achieve%20a%20maximum%20accuracy%20of%0A0.9489%20in%20the%20best%20case.%20Keywords%3A%20Point%20Cloud%20Classification%2C%20Virtual%20Reality%2C%0AHybrid%20Model%2C%20GRULSTM%2C%20GRU%2C%20LSTM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05950v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classifying%20Objects%20in%203D%20Point%20Clouds%20Using%20Recurrent%20Neural%20Network%3A%20A%0A%20%20GRU%20LSTM%20Hybrid%20Approach&entry.906535625=Ramin%20Mousa%20and%20Mitra%20Khezli%20and%20Mohamadreza%20Azadi%20and%20Vahid%20Nikoofard%20and%20Saba%20Hesaraki&entry.1292438233=%20%20Accurate%20classification%20of%20objects%20in%203D%20point%20clouds%20is%20a%20significant%0Aproblem%20in%20several%20applications%2C%20such%20as%20autonomous%20navigation%20and%0Aaugmented/virtual%20reality%20scenarios%2C%20which%20has%20become%20a%20research%20hot%20spot.%20In%0Athis%20paper%2C%20we%20presented%20a%20deep%20learning%20strategy%20for%203D%20object%20classification%0Ain%20augmented%20reality.%20The%20proposed%20approach%20is%20a%20combination%20of%20the%20GRU%20and%0ALSTM.%20LSTM%20networks%20learn%20longer%20dependencies%20well%2C%20but%20due%20to%20the%20number%20of%0Agates%2C%20it%20takes%20longer%20to%20train%3B%20on%20the%20other%20hand%2C%20GRU%20networks%20have%20a%20weaker%0Aperformance%20than%20LSTM%2C%20but%20their%20training%20speed%20is%20much%20higher%20than%20GRU%2C%20which%0Ais%20The%20speed%20is%20due%20to%20its%20fewer%20gates.%20The%20proposed%20approach%20used%20the%0Acombination%20of%20speed%20and%20accuracy%20of%20these%20two%20networks.%20The%20proposed%20approach%0Aachieved%20an%20accuracy%20of%200.99%20in%20the%204%2C499%2C0641%20points%20dataset%2C%20which%20includes%0Aeight%20classes%20%28unlabeled%2C%20man-made%20terrain%2C%20natural%20terrain%2C%20high%20vegetation%2C%0Alow%20vegetation%2C%20buildings%2C%20hardscape%2C%20scanning%20artifacts%2C%20cars%29.%20Meanwhile%2C%20the%0Atraditional%20machine%20learning%20approaches%20could%20achieve%20a%20maximum%20accuracy%20of%0A0.9489%20in%20the%20best%20case.%20Keywords%3A%20Point%20Cloud%20Classification%2C%20Virtual%20Reality%2C%0AHybrid%20Model%2C%20GRULSTM%2C%20GRU%2C%20LSTM%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05950v2&entry.124074799=Read"},
{"title": "Enhance Image Classification via Inter-Class Image Mixup with Diffusion\n  Model", "author": "Zhicai Wang and Longhui Wei and Tan Wang and Heyu Chen and Yanbin Hao and Xiang Wang and Xiangnan He and Qi Tian", "abstract": "  Text-to-image (T2I) generative models have recently emerged as a powerful\ntool, enabling the creation of photo-realistic images and giving rise to a\nmultitude of applications. However, the effective integration of T2I models\ninto fundamental image classification tasks remains an open question. A\nprevalent strategy to bolster image classification performance is through\naugmenting the training set with synthetic images generated by T2I models. In\nthis study, we scrutinize the shortcomings of both current generative and\nconventional data augmentation techniques. Our analysis reveals that these\nmethods struggle to produce images that are both faithful (in terms of\nforeground objects) and diverse (in terms of background contexts) for\ndomain-specific concepts. To tackle this challenge, we introduce an innovative\ninter-class data augmentation method known as Diff-Mix\n(https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset by\nperforming image translations between classes. Our empirical results\ndemonstrate that Diff-Mix achieves a better balance between faithfulness and\ndiversity, leading to a marked improvement in performance across diverse image\nclassification scenarios, including few-shot, conventional, and long-tail\nclassifications for domain-specific datasets.\n", "link": "http://arxiv.org/abs/2403.19600v1", "date": "2024-03-28", "relevancy": 2.4417, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6321}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5982}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5937}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhance%20Image%20Classification%20via%20Inter-Class%20Image%20Mixup%20with%20Diffusion%0A%20%20Model&body=Title%3A%20Enhance%20Image%20Classification%20via%20Inter-Class%20Image%20Mixup%20with%20Diffusion%0A%20%20Model%0AAuthor%3A%20Zhicai%20Wang%20and%20Longhui%20Wei%20and%20Tan%20Wang%20and%20Heyu%20Chen%20and%20Yanbin%20Hao%20and%20Xiang%20Wang%20and%20Xiangnan%20He%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generative%20models%20have%20recently%20emerged%20as%20a%20powerful%0Atool%2C%20enabling%20the%20creation%20of%20photo-realistic%20images%20and%20giving%20rise%20to%20a%0Amultitude%20of%20applications.%20However%2C%20the%20effective%20integration%20of%20T2I%20models%0Ainto%20fundamental%20image%20classification%20tasks%20remains%20an%20open%20question.%20A%0Aprevalent%20strategy%20to%20bolster%20image%20classification%20performance%20is%20through%0Aaugmenting%20the%20training%20set%20with%20synthetic%20images%20generated%20by%20T2I%20models.%20In%0Athis%20study%2C%20we%20scrutinize%20the%20shortcomings%20of%20both%20current%20generative%20and%0Aconventional%20data%20augmentation%20techniques.%20Our%20analysis%20reveals%20that%20these%0Amethods%20struggle%20to%20produce%20images%20that%20are%20both%20faithful%20%28in%20terms%20of%0Aforeground%20objects%29%20and%20diverse%20%28in%20terms%20of%20background%20contexts%29%20for%0Adomain-specific%20concepts.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20an%20innovative%0Ainter-class%20data%20augmentation%20method%20known%20as%20Diff-Mix%0A%28https%3A//github.com/Zhicaiwww/Diff-Mix%29%2C%20which%20enriches%20the%20dataset%20by%0Aperforming%20image%20translations%20between%20classes.%20Our%20empirical%20results%0Ademonstrate%20that%20Diff-Mix%20achieves%20a%20better%20balance%20between%20faithfulness%20and%0Adiversity%2C%20leading%20to%20a%20marked%20improvement%20in%20performance%20across%20diverse%20image%0Aclassification%20scenarios%2C%20including%20few-shot%2C%20conventional%2C%20and%20long-tail%0Aclassifications%20for%20domain-specific%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19600v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhance%20Image%20Classification%20via%20Inter-Class%20Image%20Mixup%20with%20Diffusion%0A%20%20Model&entry.906535625=Zhicai%20Wang%20and%20Longhui%20Wei%20and%20Tan%20Wang%20and%20Heyu%20Chen%20and%20Yanbin%20Hao%20and%20Xiang%20Wang%20and%20Xiangnan%20He%20and%20Qi%20Tian&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generative%20models%20have%20recently%20emerged%20as%20a%20powerful%0Atool%2C%20enabling%20the%20creation%20of%20photo-realistic%20images%20and%20giving%20rise%20to%20a%0Amultitude%20of%20applications.%20However%2C%20the%20effective%20integration%20of%20T2I%20models%0Ainto%20fundamental%20image%20classification%20tasks%20remains%20an%20open%20question.%20A%0Aprevalent%20strategy%20to%20bolster%20image%20classification%20performance%20is%20through%0Aaugmenting%20the%20training%20set%20with%20synthetic%20images%20generated%20by%20T2I%20models.%20In%0Athis%20study%2C%20we%20scrutinize%20the%20shortcomings%20of%20both%20current%20generative%20and%0Aconventional%20data%20augmentation%20techniques.%20Our%20analysis%20reveals%20that%20these%0Amethods%20struggle%20to%20produce%20images%20that%20are%20both%20faithful%20%28in%20terms%20of%0Aforeground%20objects%29%20and%20diverse%20%28in%20terms%20of%20background%20contexts%29%20for%0Adomain-specific%20concepts.%20To%20tackle%20this%20challenge%2C%20we%20introduce%20an%20innovative%0Ainter-class%20data%20augmentation%20method%20known%20as%20Diff-Mix%0A%28https%3A//github.com/Zhicaiwww/Diff-Mix%29%2C%20which%20enriches%20the%20dataset%20by%0Aperforming%20image%20translations%20between%20classes.%20Our%20empirical%20results%0Ademonstrate%20that%20Diff-Mix%20achieves%20a%20better%20balance%20between%20faithfulness%20and%0Adiversity%2C%20leading%20to%20a%20marked%20improvement%20in%20performance%20across%20diverse%20image%0Aclassification%20scenarios%2C%20including%20few-shot%2C%20conventional%2C%20and%20long-tail%0Aclassifications%20for%20domain-specific%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19600v1&entry.124074799=Read"},
{"title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot\n  Navigation", "author": "Jiacui Huang and Hongtao Zhang and Mingbo Zhao and Zhou Wu", "abstract": "  Vision-and-Language Navigation (VLN) is a challenging task that requires a\nrobot to navigate in photo-realistic environments with human natural language\npromptings. Recent studies aim to handle this task by constructing the semantic\nspatial map representation of the environment, and then leveraging the strong\nability of reasoning in large language models for generalizing code for guiding\nthe robot navigation. However, these methods face limitations in instance-level\nand attribute-level navigation tasks as they cannot distinguish different\ninstances of the same object. To address this challenge, we propose a new\nmethod, namely, Instance-aware Visual Language Map (IVLMap), to empower the\nrobot with instance-level and attribute-level semantic mapping, where it is\nautonomously constructed by fusing the RGBD video data collected from the robot\nagent with special-designed natural language map indexing in the bird's-in-eye\nview. Such indexing is instance-level and attribute-level. In particular, when\nintegrated with a large language model, IVLMap demonstrates the capability to\ni) transform natural language into navigation targets with instance and\nattribute information, enabling precise localization, and ii) accomplish\nzero-shot end-to-end navigation tasks based on natural language commands.\nExtensive navigation experiments are conducted. Simulation results illustrate\nthat our method can achieve an average improvement of 14.4\\% in navigation\naccuracy. Code and demo are released at https://ivlmap.github.io/.\n", "link": "http://arxiv.org/abs/2403.19336v1", "date": "2024-03-28", "relevancy": 2.4102, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6057}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.603}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6009}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IVLMap%3A%20Instance-Aware%20Visual%20Language%20Grounding%20for%20Consumer%20Robot%0A%20%20Navigation&body=Title%3A%20IVLMap%3A%20Instance-Aware%20Visual%20Language%20Grounding%20for%20Consumer%20Robot%0A%20%20Navigation%0AAuthor%3A%20Jiacui%20Huang%20and%20Hongtao%20Zhang%20and%20Mingbo%20Zhao%20and%20Zhou%20Wu%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20is%20a%20challenging%20task%20that%20requires%20a%0Arobot%20to%20navigate%20in%20photo-realistic%20environments%20with%20human%20natural%20language%0Apromptings.%20Recent%20studies%20aim%20to%20handle%20this%20task%20by%20constructing%20the%20semantic%0Aspatial%20map%20representation%20of%20the%20environment%2C%20and%20then%20leveraging%20the%20strong%0Aability%20of%20reasoning%20in%20large%20language%20models%20for%20generalizing%20code%20for%20guiding%0Athe%20robot%20navigation.%20However%2C%20these%20methods%20face%20limitations%20in%20instance-level%0Aand%20attribute-level%20navigation%20tasks%20as%20they%20cannot%20distinguish%20different%0Ainstances%20of%20the%20same%20object.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20new%0Amethod%2C%20namely%2C%20Instance-aware%20Visual%20Language%20Map%20%28IVLMap%29%2C%20to%20empower%20the%0Arobot%20with%20instance-level%20and%20attribute-level%20semantic%20mapping%2C%20where%20it%20is%0Aautonomously%20constructed%20by%20fusing%20the%20RGBD%20video%20data%20collected%20from%20the%20robot%0Aagent%20with%20special-designed%20natural%20language%20map%20indexing%20in%20the%20bird%27s-in-eye%0Aview.%20Such%20indexing%20is%20instance-level%20and%20attribute-level.%20In%20particular%2C%20when%0Aintegrated%20with%20a%20large%20language%20model%2C%20IVLMap%20demonstrates%20the%20capability%20to%0Ai%29%20transform%20natural%20language%20into%20navigation%20targets%20with%20instance%20and%0Aattribute%20information%2C%20enabling%20precise%20localization%2C%20and%20ii%29%20accomplish%0Azero-shot%20end-to-end%20navigation%20tasks%20based%20on%20natural%20language%20commands.%0AExtensive%20navigation%20experiments%20are%20conducted.%20Simulation%20results%20illustrate%0Athat%20our%20method%20can%20achieve%20an%20average%20improvement%20of%2014.4%5C%25%20in%20navigation%0Aaccuracy.%20Code%20and%20demo%20are%20released%20at%20https%3A//ivlmap.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19336v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IVLMap%3A%20Instance-Aware%20Visual%20Language%20Grounding%20for%20Consumer%20Robot%0A%20%20Navigation&entry.906535625=Jiacui%20Huang%20and%20Hongtao%20Zhang%20and%20Mingbo%20Zhao%20and%20Zhou%20Wu&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20is%20a%20challenging%20task%20that%20requires%20a%0Arobot%20to%20navigate%20in%20photo-realistic%20environments%20with%20human%20natural%20language%0Apromptings.%20Recent%20studies%20aim%20to%20handle%20this%20task%20by%20constructing%20the%20semantic%0Aspatial%20map%20representation%20of%20the%20environment%2C%20and%20then%20leveraging%20the%20strong%0Aability%20of%20reasoning%20in%20large%20language%20models%20for%20generalizing%20code%20for%20guiding%0Athe%20robot%20navigation.%20However%2C%20these%20methods%20face%20limitations%20in%20instance-level%0Aand%20attribute-level%20navigation%20tasks%20as%20they%20cannot%20distinguish%20different%0Ainstances%20of%20the%20same%20object.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20new%0Amethod%2C%20namely%2C%20Instance-aware%20Visual%20Language%20Map%20%28IVLMap%29%2C%20to%20empower%20the%0Arobot%20with%20instance-level%20and%20attribute-level%20semantic%20mapping%2C%20where%20it%20is%0Aautonomously%20constructed%20by%20fusing%20the%20RGBD%20video%20data%20collected%20from%20the%20robot%0Aagent%20with%20special-designed%20natural%20language%20map%20indexing%20in%20the%20bird%27s-in-eye%0Aview.%20Such%20indexing%20is%20instance-level%20and%20attribute-level.%20In%20particular%2C%20when%0Aintegrated%20with%20a%20large%20language%20model%2C%20IVLMap%20demonstrates%20the%20capability%20to%0Ai%29%20transform%20natural%20language%20into%20navigation%20targets%20with%20instance%20and%0Aattribute%20information%2C%20enabling%20precise%20localization%2C%20and%20ii%29%20accomplish%0Azero-shot%20end-to-end%20navigation%20tasks%20based%20on%20natural%20language%20commands.%0AExtensive%20navigation%20experiments%20are%20conducted.%20Simulation%20results%20illustrate%0Athat%20our%20method%20can%20achieve%20an%20average%20improvement%20of%2014.4%5C%25%20in%20navigation%0Aaccuracy.%20Code%20and%20demo%20are%20released%20at%20https%3A//ivlmap.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19336v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning: A Convex Optimization Approach", "author": "Ather Gattami", "abstract": "  In this paper, we consider reinforcement learning of nonlinear systems with\ncontinuous state and action spaces. We present an episodic learning algorithm,\nwhere we for each episode use convex optimization to find a two-layer neural\nnetwork approximation of the optimal $Q$-function. The convex optimization\napproach guarantees that the weights calculated at each episode are optimal,\nwith respect to the given sampled states and actions of the current episode.\nFor stable nonlinear systems, we show that the algorithm converges and that the\nconverging parameters of the trained neural network can be made arbitrarily\nclose to the optimal neural network parameters. In particular, if the\nregularization parameter is $\\rho$ and the time horizon is $T$, then the\nparameters of the trained neural network converge to $w$, where the distance\nbetween $w$ from the optimal parameters $w^\\star$ is bounded by\n$\\mathcal{O}(\\rho T^{-1})$. That is, when the number of episodes goes to\ninfinity, there exists a constant $C$ such that \\[\\|w-w^\\star\\| \\le\nC\\cdot\\frac{\\rho}{T}.\\] In particular, our algorithm converges arbitrarily\nclose to the optimal neural network parameters as the time horizon increases or\nas the regularization parameter decreases.\n", "link": "http://arxiv.org/abs/2402.19212v4", "date": "2024-03-28", "relevancy": 2.4035, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.505}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4844}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&body=Title%3A%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach%0AAuthor%3A%20Ather%20Gattami%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20is%20%24%5Crho%24%20and%20the%20time%20horizon%20is%20%24T%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20from%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%20T%5E%7B-1%7D%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%0Ainfinity%2C%20there%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%0AC%5Ccdot%5Cfrac%7B%5Crho%7D%7BT%7D.%5C%5D%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters%20as%20the%20time%20horizon%20increases%20or%0Aas%20the%20regularization%20parameter%20decreases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19212v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&entry.906535625=Ather%20Gattami&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20is%20%24%5Crho%24%20and%20the%20time%20horizon%20is%20%24T%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20from%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%20T%5E%7B-1%7D%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%0Ainfinity%2C%20there%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%0AC%5Ccdot%5Cfrac%7B%5Crho%7D%7BT%7D.%5C%5D%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters%20as%20the%20time%20horizon%20increases%20or%0Aas%20the%20regularization%20parameter%20decreases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19212v4&entry.124074799=Read"},
{"title": "Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair\n  Mining Approach", "author": "Xiang Lan and Hanshu Yan and Shenda Hong and Mengling Feng", "abstract": "  Not all positive pairs are beneficial to time series contrastive learning. In\nthis paper, we study two types of bad positive pairs that can impair the\nquality of time series representation learned through contrastive learning: the\nnoisy positive pair and the faulty positive pair. We observe that, with the\npresence of noisy positive pairs, the model tends to simply learn the pattern\nof noise (Noisy Alignment). Meanwhile, when faulty positive pairs arise, the\nmodel wastes considerable amount of effort aligning non-representative patterns\n(Faulty Alignment). To address this problem, we propose a Dynamic Bad Pair\nMining (DBPM) algorithm, which reliably identifies and suppresses bad positive\npairs in time series contrastive learning. Specifically, DBPM utilizes a memory\nmodule to dynamically track the training behavior of each positive pair along\ntraining process. This allows us to identify potential bad positive pairs at\neach epoch based on their historical training behaviors. The identified bad\npairs are subsequently down-weighted through a transformation module, thereby\nmitigating their negative impact on the representation learning process. DBPM\nis a simple algorithm designed as a lightweight plug-in without learnable\nparameters to enhance the performance of existing state-of-the-art methods.\nThrough extensive experiments conducted on four large-scale, real-world time\nseries datasets, we demonstrate DBPM's efficacy in mitigating the adverse\neffects of bad positive pairs.\n", "link": "http://arxiv.org/abs/2302.03357v3", "date": "2024-03-28", "relevancy": 2.3996, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5001}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4742}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4655}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Enhancing%20Time%20Series%20Contrastive%20Learning%3A%20A%20Dynamic%20Bad%20Pair%0A%20%20Mining%20Approach&body=Title%3A%20Towards%20Enhancing%20Time%20Series%20Contrastive%20Learning%3A%20A%20Dynamic%20Bad%20Pair%0A%20%20Mining%20Approach%0AAuthor%3A%20Xiang%20Lan%20and%20Hanshu%20Yan%20and%20Shenda%20Hong%20and%20Mengling%20Feng%0AAbstract%3A%20%20%20Not%20all%20positive%20pairs%20are%20beneficial%20to%20time%20series%20contrastive%20learning.%20In%0Athis%20paper%2C%20we%20study%20two%20types%20of%20bad%20positive%20pairs%20that%20can%20impair%20the%0Aquality%20of%20time%20series%20representation%20learned%20through%20contrastive%20learning%3A%20the%0Anoisy%20positive%20pair%20and%20the%20faulty%20positive%20pair.%20We%20observe%20that%2C%20with%20the%0Apresence%20of%20noisy%20positive%20pairs%2C%20the%20model%20tends%20to%20simply%20learn%20the%20pattern%0Aof%20noise%20%28Noisy%20Alignment%29.%20Meanwhile%2C%20when%20faulty%20positive%20pairs%20arise%2C%20the%0Amodel%20wastes%20considerable%20amount%20of%20effort%20aligning%20non-representative%20patterns%0A%28Faulty%20Alignment%29.%20To%20address%20this%20problem%2C%20we%20propose%20a%20Dynamic%20Bad%20Pair%0AMining%20%28DBPM%29%20algorithm%2C%20which%20reliably%20identifies%20and%20suppresses%20bad%20positive%0Apairs%20in%20time%20series%20contrastive%20learning.%20Specifically%2C%20DBPM%20utilizes%20a%20memory%0Amodule%20to%20dynamically%20track%20the%20training%20behavior%20of%20each%20positive%20pair%20along%0Atraining%20process.%20This%20allows%20us%20to%20identify%20potential%20bad%20positive%20pairs%20at%0Aeach%20epoch%20based%20on%20their%20historical%20training%20behaviors.%20The%20identified%20bad%0Apairs%20are%20subsequently%20down-weighted%20through%20a%20transformation%20module%2C%20thereby%0Amitigating%20their%20negative%20impact%20on%20the%20representation%20learning%20process.%20DBPM%0Ais%20a%20simple%20algorithm%20designed%20as%20a%20lightweight%20plug-in%20without%20learnable%0Aparameters%20to%20enhance%20the%20performance%20of%20existing%20state-of-the-art%20methods.%0AThrough%20extensive%20experiments%20conducted%20on%20four%20large-scale%2C%20real-world%20time%0Aseries%20datasets%2C%20we%20demonstrate%20DBPM%27s%20efficacy%20in%20mitigating%20the%20adverse%0Aeffects%20of%20bad%20positive%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03357v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Enhancing%20Time%20Series%20Contrastive%20Learning%3A%20A%20Dynamic%20Bad%20Pair%0A%20%20Mining%20Approach&entry.906535625=Xiang%20Lan%20and%20Hanshu%20Yan%20and%20Shenda%20Hong%20and%20Mengling%20Feng&entry.1292438233=%20%20Not%20all%20positive%20pairs%20are%20beneficial%20to%20time%20series%20contrastive%20learning.%20In%0Athis%20paper%2C%20we%20study%20two%20types%20of%20bad%20positive%20pairs%20that%20can%20impair%20the%0Aquality%20of%20time%20series%20representation%20learned%20through%20contrastive%20learning%3A%20the%0Anoisy%20positive%20pair%20and%20the%20faulty%20positive%20pair.%20We%20observe%20that%2C%20with%20the%0Apresence%20of%20noisy%20positive%20pairs%2C%20the%20model%20tends%20to%20simply%20learn%20the%20pattern%0Aof%20noise%20%28Noisy%20Alignment%29.%20Meanwhile%2C%20when%20faulty%20positive%20pairs%20arise%2C%20the%0Amodel%20wastes%20considerable%20amount%20of%20effort%20aligning%20non-representative%20patterns%0A%28Faulty%20Alignment%29.%20To%20address%20this%20problem%2C%20we%20propose%20a%20Dynamic%20Bad%20Pair%0AMining%20%28DBPM%29%20algorithm%2C%20which%20reliably%20identifies%20and%20suppresses%20bad%20positive%0Apairs%20in%20time%20series%20contrastive%20learning.%20Specifically%2C%20DBPM%20utilizes%20a%20memory%0Amodule%20to%20dynamically%20track%20the%20training%20behavior%20of%20each%20positive%20pair%20along%0Atraining%20process.%20This%20allows%20us%20to%20identify%20potential%20bad%20positive%20pairs%20at%0Aeach%20epoch%20based%20on%20their%20historical%20training%20behaviors.%20The%20identified%20bad%0Apairs%20are%20subsequently%20down-weighted%20through%20a%20transformation%20module%2C%20thereby%0Amitigating%20their%20negative%20impact%20on%20the%20representation%20learning%20process.%20DBPM%0Ais%20a%20simple%20algorithm%20designed%20as%20a%20lightweight%20plug-in%20without%20learnable%0Aparameters%20to%20enhance%20the%20performance%20of%20existing%20state-of-the-art%20methods.%0AThrough%20extensive%20experiments%20conducted%20on%20four%20large-scale%2C%20real-world%20time%0Aseries%20datasets%2C%20we%20demonstrate%20DBPM%27s%20efficacy%20in%20mitigating%20the%20adverse%0Aeffects%20of%20bad%20positive%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03357v3&entry.124074799=Read"},
{"title": "OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via\n  Cycle-Modality Propagation", "author": "Zhenyu Wang and Yali Li and Taichi Liu and Hengshuang Zhao and Shengjin Wang", "abstract": "  In the current state of 3D object detection research, the severe scarcity of\nannotated 3D data, substantial disparities across different data modalities,\nand the absence of a unified architecture, have impeded the progress towards\nthe goal of universality. In this paper, we propose \\textbf{OV-Uni3DETR}, a\nunified open-vocabulary 3D detector via cycle-modality propagation. Compared\nwith existing 3D detectors, OV-Uni3DETR offers distinct advantages: 1)\nOpen-vocabulary 3D detection: During training, it leverages various accessible\ndata, especially extensive 2D detection images, to boost training diversity.\nDuring inference, it can detect both seen and unseen classes. 2) Modality\nunifying: It seamlessly accommodates input data from any given modality,\neffectively addressing scenarios involving disparate modalities or missing\nsensor information, thereby supporting test-time modality switching. 3) Scene\nunifying: It provides a unified multi-modal model architecture for diverse\nscenes collected by distinct sensors. Specifically, we propose the\ncycle-modality propagation, aimed at propagating knowledge bridging 2D and 3D\nmodalities, to support the aforementioned functionalities. 2D semantic\nknowledge from large-vocabulary learning guides novel class discovery in the 3D\ndomain, and 3D geometric knowledge provides localization supervision for 2D\ndetection images. OV-Uni3DETR achieves the state-of-the-art performance on\nvarious scenarios, surpassing existing methods by more than 6\\% on average. Its\nperformance using only RGB images is on par with or even surpasses that of\nprevious point cloud based methods. Code and pre-trained models will be\nreleased later.\n", "link": "http://arxiv.org/abs/2403.19580v1", "date": "2024-03-28", "relevancy": 2.3925, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6076}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6072}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5852}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OV-Uni3DETR%3A%20Towards%20Unified%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Cycle-Modality%20Propagation&body=Title%3A%20OV-Uni3DETR%3A%20Towards%20Unified%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Cycle-Modality%20Propagation%0AAuthor%3A%20Zhenyu%20Wang%20and%20Yali%20Li%20and%20Taichi%20Liu%20and%20Hengshuang%20Zhao%20and%20Shengjin%20Wang%0AAbstract%3A%20%20%20In%20the%20current%20state%20of%203D%20object%20detection%20research%2C%20the%20severe%20scarcity%20of%0Aannotated%203D%20data%2C%20substantial%20disparities%20across%20different%20data%20modalities%2C%0Aand%20the%20absence%20of%20a%20unified%20architecture%2C%20have%20impeded%20the%20progress%20towards%0Athe%20goal%20of%20universality.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BOV-Uni3DETR%7D%2C%20a%0Aunified%20open-vocabulary%203D%20detector%20via%20cycle-modality%20propagation.%20Compared%0Awith%20existing%203D%20detectors%2C%20OV-Uni3DETR%20offers%20distinct%20advantages%3A%201%29%0AOpen-vocabulary%203D%20detection%3A%20During%20training%2C%20it%20leverages%20various%20accessible%0Adata%2C%20especially%20extensive%202D%20detection%20images%2C%20to%20boost%20training%20diversity.%0ADuring%20inference%2C%20it%20can%20detect%20both%20seen%20and%20unseen%20classes.%202%29%20Modality%0Aunifying%3A%20It%20seamlessly%20accommodates%20input%20data%20from%20any%20given%20modality%2C%0Aeffectively%20addressing%20scenarios%20involving%20disparate%20modalities%20or%20missing%0Asensor%20information%2C%20thereby%20supporting%20test-time%20modality%20switching.%203%29%20Scene%0Aunifying%3A%20It%20provides%20a%20unified%20multi-modal%20model%20architecture%20for%20diverse%0Ascenes%20collected%20by%20distinct%20sensors.%20Specifically%2C%20we%20propose%20the%0Acycle-modality%20propagation%2C%20aimed%20at%20propagating%20knowledge%20bridging%202D%20and%203D%0Amodalities%2C%20to%20support%20the%20aforementioned%20functionalities.%202D%20semantic%0Aknowledge%20from%20large-vocabulary%20learning%20guides%20novel%20class%20discovery%20in%20the%203D%0Adomain%2C%20and%203D%20geometric%20knowledge%20provides%20localization%20supervision%20for%202D%0Adetection%20images.%20OV-Uni3DETR%20achieves%20the%20state-of-the-art%20performance%20on%0Avarious%20scenarios%2C%20surpassing%20existing%20methods%20by%20more%20than%206%5C%25%20on%20average.%20Its%0Aperformance%20using%20only%20RGB%20images%20is%20on%20par%20with%20or%20even%20surpasses%20that%20of%0Aprevious%20point%20cloud%20based%20methods.%20Code%20and%20pre-trained%20models%20will%20be%0Areleased%20later.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19580v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OV-Uni3DETR%3A%20Towards%20Unified%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Cycle-Modality%20Propagation&entry.906535625=Zhenyu%20Wang%20and%20Yali%20Li%20and%20Taichi%20Liu%20and%20Hengshuang%20Zhao%20and%20Shengjin%20Wang&entry.1292438233=%20%20In%20the%20current%20state%20of%203D%20object%20detection%20research%2C%20the%20severe%20scarcity%20of%0Aannotated%203D%20data%2C%20substantial%20disparities%20across%20different%20data%20modalities%2C%0Aand%20the%20absence%20of%20a%20unified%20architecture%2C%20have%20impeded%20the%20progress%20towards%0Athe%20goal%20of%20universality.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BOV-Uni3DETR%7D%2C%20a%0Aunified%20open-vocabulary%203D%20detector%20via%20cycle-modality%20propagation.%20Compared%0Awith%20existing%203D%20detectors%2C%20OV-Uni3DETR%20offers%20distinct%20advantages%3A%201%29%0AOpen-vocabulary%203D%20detection%3A%20During%20training%2C%20it%20leverages%20various%20accessible%0Adata%2C%20especially%20extensive%202D%20detection%20images%2C%20to%20boost%20training%20diversity.%0ADuring%20inference%2C%20it%20can%20detect%20both%20seen%20and%20unseen%20classes.%202%29%20Modality%0Aunifying%3A%20It%20seamlessly%20accommodates%20input%20data%20from%20any%20given%20modality%2C%0Aeffectively%20addressing%20scenarios%20involving%20disparate%20modalities%20or%20missing%0Asensor%20information%2C%20thereby%20supporting%20test-time%20modality%20switching.%203%29%20Scene%0Aunifying%3A%20It%20provides%20a%20unified%20multi-modal%20model%20architecture%20for%20diverse%0Ascenes%20collected%20by%20distinct%20sensors.%20Specifically%2C%20we%20propose%20the%0Acycle-modality%20propagation%2C%20aimed%20at%20propagating%20knowledge%20bridging%202D%20and%203D%0Amodalities%2C%20to%20support%20the%20aforementioned%20functionalities.%202D%20semantic%0Aknowledge%20from%20large-vocabulary%20learning%20guides%20novel%20class%20discovery%20in%20the%203D%0Adomain%2C%20and%203D%20geometric%20knowledge%20provides%20localization%20supervision%20for%202D%0Adetection%20images.%20OV-Uni3DETR%20achieves%20the%20state-of-the-art%20performance%20on%0Avarious%20scenarios%2C%20surpassing%20existing%20methods%20by%20more%20than%206%5C%25%20on%20average.%20Its%0Aperformance%20using%20only%20RGB%20images%20is%20on%20par%20with%20or%20even%20surpasses%20that%20of%0Aprevious%20point%20cloud%20based%20methods.%20Code%20and%20pre-trained%20models%20will%20be%0Areleased%20later.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19580v1&entry.124074799=Read"},
{"title": "Nearest Neighbor Classication for Classical Image Upsampling", "author": "Evan Matthews and Nicolas Prate", "abstract": "  Given a set of ordered pixel data in the form of an image, our goal is to\nperform upsampling on the data such that: the resulting resolution is improved\nby some factor, the final result passes the human test, having added new,\nbelievable, and realistic information and detail to the image, the time\ncomplexity for upscaling is relatively close to that of lossy upscaling\nimplementations.\n", "link": "http://arxiv.org/abs/2403.19611v1", "date": "2024-03-28", "relevancy": 2.3491, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4903}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4735}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4457}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Nearest%20Neighbor%20Classication%20for%20Classical%20Image%20Upsampling&body=Title%3A%20Nearest%20Neighbor%20Classication%20for%20Classical%20Image%20Upsampling%0AAuthor%3A%20Evan%20Matthews%20and%20Nicolas%20Prate%0AAbstract%3A%20%20%20Given%20a%20set%20of%20ordered%20pixel%20data%20in%20the%20form%20of%20an%20image%2C%20our%20goal%20is%20to%0Aperform%20upsampling%20on%20the%20data%20such%20that%3A%20the%20resulting%20resolution%20is%20improved%0Aby%20some%20factor%2C%20the%20final%20result%20passes%20the%20human%20test%2C%20having%20added%20new%2C%0Abelievable%2C%20and%20realistic%20information%20and%20detail%20to%20the%20image%2C%20the%20time%0Acomplexity%20for%20upscaling%20is%20relatively%20close%20to%20that%20of%20lossy%20upscaling%0Aimplementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19611v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nearest%20Neighbor%20Classication%20for%20Classical%20Image%20Upsampling&entry.906535625=Evan%20Matthews%20and%20Nicolas%20Prate&entry.1292438233=%20%20Given%20a%20set%20of%20ordered%20pixel%20data%20in%20the%20form%20of%20an%20image%2C%20our%20goal%20is%20to%0Aperform%20upsampling%20on%20the%20data%20such%20that%3A%20the%20resulting%20resolution%20is%20improved%0Aby%20some%20factor%2C%20the%20final%20result%20passes%20the%20human%20test%2C%20having%20added%20new%2C%0Abelievable%2C%20and%20realistic%20information%20and%20detail%20to%20the%20image%2C%20the%20time%0Acomplexity%20for%20upscaling%20is%20relatively%20close%20to%20that%20of%20lossy%20upscaling%0Aimplementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19611v1&entry.124074799=Read"},
{"title": "MANUS: Markerless Grasp Capture using Articulated 3D Gaussians", "author": "Chandradeep Pokhariya and Ishaan N Shah and Angela Xing and Zekun Li and Kefan Chen and Avinash Sharma and Srinath Sridhar", "abstract": "  Understanding how we grasp objects with our hands has important applications\nin areas like robotics and mixed reality. However, this challenging problem\nrequires accurate modeling of the contact between hands and objects. To capture\ngrasps, existing methods use skeletons, meshes, or parametric models that does\nnot represent hand shape accurately resulting in inaccurate contacts. We\npresent MANUS, a method for Markerless Hand-Object Grasp Capture using\nArticulated 3D Gaussians. We build a novel articulated 3D Gaussians\nrepresentation that extends 3D Gaussian splatting for high-fidelity\nrepresentation of articulating hands. Since our representation uses Gaussian\nprimitives, it enables us to efficiently and accurately estimate contacts\nbetween the hand and the object. For the most accurate results, our method\nrequires tens of camera views that current datasets do not provide. We\ntherefore build MANUS-Grasps, a new dataset that contains hand-object grasps\nviewed from 50+ cameras across 30+ scenes, 3 subjects, and comprising over 7M\nframes. In addition to extensive qualitative results, we also show that our\nmethod outperforms others on a quantitative contact evaluation method that uses\npaint transfer from the object to the hand.\n", "link": "http://arxiv.org/abs/2312.02137v2", "date": "2024-03-28", "relevancy": 2.318, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6561}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MANUS%3A%20Markerless%20Grasp%20Capture%20using%20Articulated%203D%20Gaussians&body=Title%3A%20MANUS%3A%20Markerless%20Grasp%20Capture%20using%20Articulated%203D%20Gaussians%0AAuthor%3A%20Chandradeep%20Pokhariya%20and%20Ishaan%20N%20Shah%20and%20Angela%20Xing%20and%20Zekun%20Li%20and%20Kefan%20Chen%20and%20Avinash%20Sharma%20and%20Srinath%20Sridhar%0AAbstract%3A%20%20%20Understanding%20how%20we%20grasp%20objects%20with%20our%20hands%20has%20important%20applications%0Ain%20areas%20like%20robotics%20and%20mixed%20reality.%20However%2C%20this%20challenging%20problem%0Arequires%20accurate%20modeling%20of%20the%20contact%20between%20hands%20and%20objects.%20To%20capture%0Agrasps%2C%20existing%20methods%20use%20skeletons%2C%20meshes%2C%20or%20parametric%20models%20that%20does%0Anot%20represent%20hand%20shape%20accurately%20resulting%20in%20inaccurate%20contacts.%20We%0Apresent%20MANUS%2C%20a%20method%20for%20Markerless%20Hand-Object%20Grasp%20Capture%20using%0AArticulated%203D%20Gaussians.%20We%20build%20a%20novel%20articulated%203D%20Gaussians%0Arepresentation%20that%20extends%203D%20Gaussian%20splatting%20for%20high-fidelity%0Arepresentation%20of%20articulating%20hands.%20Since%20our%20representation%20uses%20Gaussian%0Aprimitives%2C%20it%20enables%20us%20to%20efficiently%20and%20accurately%20estimate%20contacts%0Abetween%20the%20hand%20and%20the%20object.%20For%20the%20most%20accurate%20results%2C%20our%20method%0Arequires%20tens%20of%20camera%20views%20that%20current%20datasets%20do%20not%20provide.%20We%0Atherefore%20build%20MANUS-Grasps%2C%20a%20new%20dataset%20that%20contains%20hand-object%20grasps%0Aviewed%20from%2050%2B%20cameras%20across%2030%2B%20scenes%2C%203%20subjects%2C%20and%20comprising%20over%207M%0Aframes.%20In%20addition%20to%20extensive%20qualitative%20results%2C%20we%20also%20show%20that%20our%0Amethod%20outperforms%20others%20on%20a%20quantitative%20contact%20evaluation%20method%20that%20uses%0Apaint%20transfer%20from%20the%20object%20to%20the%20hand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02137v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MANUS%3A%20Markerless%20Grasp%20Capture%20using%20Articulated%203D%20Gaussians&entry.906535625=Chandradeep%20Pokhariya%20and%20Ishaan%20N%20Shah%20and%20Angela%20Xing%20and%20Zekun%20Li%20and%20Kefan%20Chen%20and%20Avinash%20Sharma%20and%20Srinath%20Sridhar&entry.1292438233=%20%20Understanding%20how%20we%20grasp%20objects%20with%20our%20hands%20has%20important%20applications%0Ain%20areas%20like%20robotics%20and%20mixed%20reality.%20However%2C%20this%20challenging%20problem%0Arequires%20accurate%20modeling%20of%20the%20contact%20between%20hands%20and%20objects.%20To%20capture%0Agrasps%2C%20existing%20methods%20use%20skeletons%2C%20meshes%2C%20or%20parametric%20models%20that%20does%0Anot%20represent%20hand%20shape%20accurately%20resulting%20in%20inaccurate%20contacts.%20We%0Apresent%20MANUS%2C%20a%20method%20for%20Markerless%20Hand-Object%20Grasp%20Capture%20using%0AArticulated%203D%20Gaussians.%20We%20build%20a%20novel%20articulated%203D%20Gaussians%0Arepresentation%20that%20extends%203D%20Gaussian%20splatting%20for%20high-fidelity%0Arepresentation%20of%20articulating%20hands.%20Since%20our%20representation%20uses%20Gaussian%0Aprimitives%2C%20it%20enables%20us%20to%20efficiently%20and%20accurately%20estimate%20contacts%0Abetween%20the%20hand%20and%20the%20object.%20For%20the%20most%20accurate%20results%2C%20our%20method%0Arequires%20tens%20of%20camera%20views%20that%20current%20datasets%20do%20not%20provide.%20We%0Atherefore%20build%20MANUS-Grasps%2C%20a%20new%20dataset%20that%20contains%20hand-object%20grasps%0Aviewed%20from%2050%2B%20cameras%20across%2030%2B%20scenes%2C%203%20subjects%2C%20and%20comprising%20over%207M%0Aframes.%20In%20addition%20to%20extensive%20qualitative%20results%2C%20we%20also%20show%20that%20our%0Amethod%20outperforms%20others%20on%20a%20quantitative%20contact%20evaluation%20method%20that%20uses%0Apaint%20transfer%20from%20the%20object%20to%20the%20hand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02137v2&entry.124074799=Read"},
{"title": "Benchmarking Implicit Neural Representation and Geometric Rendering in\n  Real-Time RGB-D SLAM", "author": "Tongyan Hua and Lin Wang", "abstract": "  Implicit neural representation (INR), in combination with geometric\nrendering, has recently been employed in real-time dense RGB-D SLAM. Despite\nactive research endeavors being made, there lacks a unified protocol for fair\nevaluation, impeding the evolution of this area. In this work, we establish, to\nour knowledge, the first open-source benchmark framework to evaluate the\nperformance of a wide spectrum of commonly used INRs and rendering functions\nfor mapping and localization. The goal of our benchmark is to 1) gain an\nintuition of how different INRs and rendering functions impact mapping and\nlocalization and 2) establish a unified evaluation protocol w.r.t. the design\nchoices that may impact the mapping and localization. With the framework, we\nconduct a large suite of experiments, offering various insights in choosing the\nINRs and geometric rendering functions: for example, the dense feature grid\noutperforms other INRs (e.g. tri-plane and hash grid), even when geometric and\ncolor features are jointly encoded for memory efficiency. To extend the\nfindings into the practical scenario, a hybrid encoding strategy is proposed to\nbring the best of the accuracy and completion from the grid-based and\ndecomposition-based INRs. We further propose explicit hybrid encoding for\nhigh-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts\nthe premise on robustness and computation efficiency.\n", "link": "http://arxiv.org/abs/2403.19473v1", "date": "2024-03-28", "relevancy": 2.3141, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5953}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5774}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5393}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Implicit%20Neural%20Representation%20and%20Geometric%20Rendering%20in%0A%20%20Real-Time%20RGB-D%20SLAM&body=Title%3A%20Benchmarking%20Implicit%20Neural%20Representation%20and%20Geometric%20Rendering%20in%0A%20%20Real-Time%20RGB-D%20SLAM%0AAuthor%3A%20Tongyan%20Hua%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Implicit%20neural%20representation%20%28INR%29%2C%20in%20combination%20with%20geometric%0Arendering%2C%20has%20recently%20been%20employed%20in%20real-time%20dense%20RGB-D%20SLAM.%20Despite%0Aactive%20research%20endeavors%20being%20made%2C%20there%20lacks%20a%20unified%20protocol%20for%20fair%0Aevaluation%2C%20impeding%20the%20evolution%20of%20this%20area.%20In%20this%20work%2C%20we%20establish%2C%20to%0Aour%20knowledge%2C%20the%20first%20open-source%20benchmark%20framework%20to%20evaluate%20the%0Aperformance%20of%20a%20wide%20spectrum%20of%20commonly%20used%20INRs%20and%20rendering%20functions%0Afor%20mapping%20and%20localization.%20The%20goal%20of%20our%20benchmark%20is%20to%201%29%20gain%20an%0Aintuition%20of%20how%20different%20INRs%20and%20rendering%20functions%20impact%20mapping%20and%0Alocalization%20and%202%29%20establish%20a%20unified%20evaluation%20protocol%20w.r.t.%20the%20design%0Achoices%20that%20may%20impact%20the%20mapping%20and%20localization.%20With%20the%20framework%2C%20we%0Aconduct%20a%20large%20suite%20of%20experiments%2C%20offering%20various%20insights%20in%20choosing%20the%0AINRs%20and%20geometric%20rendering%20functions%3A%20for%20example%2C%20the%20dense%20feature%20grid%0Aoutperforms%20other%20INRs%20%28e.g.%20tri-plane%20and%20hash%20grid%29%2C%20even%20when%20geometric%20and%0Acolor%20features%20are%20jointly%20encoded%20for%20memory%20efficiency.%20To%20extend%20the%0Afindings%20into%20the%20practical%20scenario%2C%20a%20hybrid%20encoding%20strategy%20is%20proposed%20to%0Abring%20the%20best%20of%20the%20accuracy%20and%20completion%20from%20the%20grid-based%20and%0Adecomposition-based%20INRs.%20We%20further%20propose%20explicit%20hybrid%20encoding%20for%0Ahigh-fidelity%20dense%20grid%20mapping%20to%20comply%20with%20the%20RGB-D%20SLAM%20system%20that%20puts%0Athe%20premise%20on%20robustness%20and%20computation%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19473v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Implicit%20Neural%20Representation%20and%20Geometric%20Rendering%20in%0A%20%20Real-Time%20RGB-D%20SLAM&entry.906535625=Tongyan%20Hua%20and%20Lin%20Wang&entry.1292438233=%20%20Implicit%20neural%20representation%20%28INR%29%2C%20in%20combination%20with%20geometric%0Arendering%2C%20has%20recently%20been%20employed%20in%20real-time%20dense%20RGB-D%20SLAM.%20Despite%0Aactive%20research%20endeavors%20being%20made%2C%20there%20lacks%20a%20unified%20protocol%20for%20fair%0Aevaluation%2C%20impeding%20the%20evolution%20of%20this%20area.%20In%20this%20work%2C%20we%20establish%2C%20to%0Aour%20knowledge%2C%20the%20first%20open-source%20benchmark%20framework%20to%20evaluate%20the%0Aperformance%20of%20a%20wide%20spectrum%20of%20commonly%20used%20INRs%20and%20rendering%20functions%0Afor%20mapping%20and%20localization.%20The%20goal%20of%20our%20benchmark%20is%20to%201%29%20gain%20an%0Aintuition%20of%20how%20different%20INRs%20and%20rendering%20functions%20impact%20mapping%20and%0Alocalization%20and%202%29%20establish%20a%20unified%20evaluation%20protocol%20w.r.t.%20the%20design%0Achoices%20that%20may%20impact%20the%20mapping%20and%20localization.%20With%20the%20framework%2C%20we%0Aconduct%20a%20large%20suite%20of%20experiments%2C%20offering%20various%20insights%20in%20choosing%20the%0AINRs%20and%20geometric%20rendering%20functions%3A%20for%20example%2C%20the%20dense%20feature%20grid%0Aoutperforms%20other%20INRs%20%28e.g.%20tri-plane%20and%20hash%20grid%29%2C%20even%20when%20geometric%20and%0Acolor%20features%20are%20jointly%20encoded%20for%20memory%20efficiency.%20To%20extend%20the%0Afindings%20into%20the%20practical%20scenario%2C%20a%20hybrid%20encoding%20strategy%20is%20proposed%20to%0Abring%20the%20best%20of%20the%20accuracy%20and%20completion%20from%20the%20grid-based%20and%0Adecomposition-based%20INRs.%20We%20further%20propose%20explicit%20hybrid%20encoding%20for%0Ahigh-fidelity%20dense%20grid%20mapping%20to%20comply%20with%20the%20RGB-D%20SLAM%20system%20that%20puts%0Athe%20premise%20on%20robustness%20and%20computation%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19473v1&entry.124074799=Read"},
{"title": "Instance-Adaptive and Geometric-Aware Keypoint Learning for\n  Category-Level 6D Object Pose Estimation", "author": "Xiao Lin and Wenfei Yang and Yuan Gao and Tianzhu Zhang", "abstract": "  Category-level 6D object pose estimation aims to estimate the rotation,\ntranslation and size of unseen instances within specific categories. In this\narea, dense correspondence-based methods have achieved leading performance.\nHowever, they do not explicitly consider the local and global geometric\ninformation of different instances, resulting in poor generalization ability to\nunseen instances with significant shape variations. To deal with this problem,\nwe propose a novel Instance-Adaptive and Geometric-Aware Keypoint Learning\nmethod for category-level 6D object pose estimation (AG-Pose), which includes\ntwo key designs: (1) The first design is an Instance-Adaptive Keypoint\nDetection module, which can adaptively detect a set of sparse keypoints for\nvarious instances to represent their geometric structures. (2) The second\ndesign is a Geometric-Aware Feature Aggregation module, which can efficiently\nintegrate the local and global geometric information into keypoint features.\nThese two modules can work together to establish robust keypoint-level\ncorrespondences for unseen instances, thus enhancing the generalization ability\nof the model.Experimental results on CAMERA25 and REAL275 datasets show that\nthe proposed AG-Pose outperforms state-of-the-art methods by a large margin\nwithout category-specific shape priors.\n", "link": "http://arxiv.org/abs/2403.19527v1", "date": "2024-03-28", "relevancy": 2.3117, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6224}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5475}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Instance-Adaptive%20and%20Geometric-Aware%20Keypoint%20Learning%20for%0A%20%20Category-Level%206D%20Object%20Pose%20Estimation&body=Title%3A%20Instance-Adaptive%20and%20Geometric-Aware%20Keypoint%20Learning%20for%0A%20%20Category-Level%206D%20Object%20Pose%20Estimation%0AAuthor%3A%20Xiao%20Lin%20and%20Wenfei%20Yang%20and%20Yuan%20Gao%20and%20Tianzhu%20Zhang%0AAbstract%3A%20%20%20Category-level%206D%20object%20pose%20estimation%20aims%20to%20estimate%20the%20rotation%2C%0Atranslation%20and%20size%20of%20unseen%20instances%20within%20specific%20categories.%20In%20this%0Aarea%2C%20dense%20correspondence-based%20methods%20have%20achieved%20leading%20performance.%0AHowever%2C%20they%20do%20not%20explicitly%20consider%20the%20local%20and%20global%20geometric%0Ainformation%20of%20different%20instances%2C%20resulting%20in%20poor%20generalization%20ability%20to%0Aunseen%20instances%20with%20significant%20shape%20variations.%20To%20deal%20with%20this%20problem%2C%0Awe%20propose%20a%20novel%20Instance-Adaptive%20and%20Geometric-Aware%20Keypoint%20Learning%0Amethod%20for%20category-level%206D%20object%20pose%20estimation%20%28AG-Pose%29%2C%20which%20includes%0Atwo%20key%20designs%3A%20%281%29%20The%20first%20design%20is%20an%20Instance-Adaptive%20Keypoint%0ADetection%20module%2C%20which%20can%20adaptively%20detect%20a%20set%20of%20sparse%20keypoints%20for%0Avarious%20instances%20to%20represent%20their%20geometric%20structures.%20%282%29%20The%20second%0Adesign%20is%20a%20Geometric-Aware%20Feature%20Aggregation%20module%2C%20which%20can%20efficiently%0Aintegrate%20the%20local%20and%20global%20geometric%20information%20into%20keypoint%20features.%0AThese%20two%20modules%20can%20work%20together%20to%20establish%20robust%20keypoint-level%0Acorrespondences%20for%20unseen%20instances%2C%20thus%20enhancing%20the%20generalization%20ability%0Aof%20the%20model.Experimental%20results%20on%20CAMERA25%20and%20REAL275%20datasets%20show%20that%0Athe%20proposed%20AG-Pose%20outperforms%20state-of-the-art%20methods%20by%20a%20large%20margin%0Awithout%20category-specific%20shape%20priors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19527v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Adaptive%20and%20Geometric-Aware%20Keypoint%20Learning%20for%0A%20%20Category-Level%206D%20Object%20Pose%20Estimation&entry.906535625=Xiao%20Lin%20and%20Wenfei%20Yang%20and%20Yuan%20Gao%20and%20Tianzhu%20Zhang&entry.1292438233=%20%20Category-level%206D%20object%20pose%20estimation%20aims%20to%20estimate%20the%20rotation%2C%0Atranslation%20and%20size%20of%20unseen%20instances%20within%20specific%20categories.%20In%20this%0Aarea%2C%20dense%20correspondence-based%20methods%20have%20achieved%20leading%20performance.%0AHowever%2C%20they%20do%20not%20explicitly%20consider%20the%20local%20and%20global%20geometric%0Ainformation%20of%20different%20instances%2C%20resulting%20in%20poor%20generalization%20ability%20to%0Aunseen%20instances%20with%20significant%20shape%20variations.%20To%20deal%20with%20this%20problem%2C%0Awe%20propose%20a%20novel%20Instance-Adaptive%20and%20Geometric-Aware%20Keypoint%20Learning%0Amethod%20for%20category-level%206D%20object%20pose%20estimation%20%28AG-Pose%29%2C%20which%20includes%0Atwo%20key%20designs%3A%20%281%29%20The%20first%20design%20is%20an%20Instance-Adaptive%20Keypoint%0ADetection%20module%2C%20which%20can%20adaptively%20detect%20a%20set%20of%20sparse%20keypoints%20for%0Avarious%20instances%20to%20represent%20their%20geometric%20structures.%20%282%29%20The%20second%0Adesign%20is%20a%20Geometric-Aware%20Feature%20Aggregation%20module%2C%20which%20can%20efficiently%0Aintegrate%20the%20local%20and%20global%20geometric%20information%20into%20keypoint%20features.%0AThese%20two%20modules%20can%20work%20together%20to%20establish%20robust%20keypoint-level%0Acorrespondences%20for%20unseen%20instances%2C%20thus%20enhancing%20the%20generalization%20ability%0Aof%20the%20model.Experimental%20results%20on%20CAMERA25%20and%20REAL275%20datasets%20show%20that%0Athe%20proposed%20AG-Pose%20outperforms%20state-of-the-art%20methods%20by%20a%20large%20margin%0Awithout%20category-specific%20shape%20priors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19527v1&entry.124074799=Read"},
{"title": "OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in\n  Large-Scale Outdoor Environments", "author": "Yinan Deng and Jiahui Wang and Jingyu Zhao and Xinyu Tian and Guangyan Chen and Yi Yang and Yufeng Yue", "abstract": "  Environment representations endowed with sophisticated semantics are pivotal\nfor facilitating seamless interaction between robots and humans, enabling them\nto effectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including zero-shot\nlearning and support for open-set classes. However, existing open-vocabulary\nmaps are primarily designed for small-scale environments, such as desktops or\nrooms, and are typically geared towards limited-area tasks involving robotic\nindoor navigation or in-place manipulation. They face challenges in direct\ngeneralization to outdoor environments characterized by numerous objects and\ncomplex tasks, owing to limitations in both understanding level and map\nstructure. In this work, we propose OpenGraph, the first open-vocabulary\nhierarchical graph representation designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images, enhancing textual reasoning by encoding them. Subsequently, it\nachieves 3D incremental object-centric mapping with feature embedding by\nprojecting images onto LiDAR point clouds. Finally, the environment is\nsegmented based on lane graph connectivity to construct a hierarchical graph.\nValidation results from public dataset SemanticKITTI demonstrate that OpenGraph\nachieves the highest segmentation and query accuracy. The source code of\nOpenGraph is publicly available at https://github.com/BIT-DYN/OpenGraph.\n", "link": "http://arxiv.org/abs/2403.09412v2", "date": "2024-03-28", "relevancy": 2.2856, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6189}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5758}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.548}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OpenGraph%3A%20Open-Vocabulary%20Hierarchical%203D%20Graph%20Representation%20in%0A%20%20Large-Scale%20Outdoor%20Environments&body=Title%3A%20OpenGraph%3A%20Open-Vocabulary%20Hierarchical%203D%20Graph%20Representation%20in%0A%20%20Large-Scale%20Outdoor%20Environments%0AAuthor%3A%20Yinan%20Deng%20and%20Jiahui%20Wang%20and%20Jingyu%20Zhao%20and%20Xinyu%20Tian%20and%20Guangyan%20Chen%20and%20Yi%20Yang%20and%20Yufeng%20Yue%0AAbstract%3A%20%20%20Environment%20representations%20endowed%20with%20sophisticated%20semantics%20are%20pivotal%0Afor%20facilitating%20seamless%20interaction%20between%20robots%20and%20humans%2C%20enabling%20them%0Ato%20effectively%20carry%20out%20various%20tasks.%20Open-vocabulary%20maps%2C%20powered%20by%0AVisual-Language%20models%20%28VLMs%29%2C%20possess%20inherent%20advantages%2C%20including%20zero-shot%0Alearning%20and%20support%20for%20open-set%20classes.%20However%2C%20existing%20open-vocabulary%0Amaps%20are%20primarily%20designed%20for%20small-scale%20environments%2C%20such%20as%20desktops%20or%0Arooms%2C%20and%20are%20typically%20geared%20towards%20limited-area%20tasks%20involving%20robotic%0Aindoor%20navigation%20or%20in-place%20manipulation.%20They%20face%20challenges%20in%20direct%0Ageneralization%20to%20outdoor%20environments%20characterized%20by%20numerous%20objects%20and%0Acomplex%20tasks%2C%20owing%20to%20limitations%20in%20both%20understanding%20level%20and%20map%0Astructure.%20In%20this%20work%2C%20we%20propose%20OpenGraph%2C%20the%20first%20open-vocabulary%0Ahierarchical%20graph%20representation%20designed%20for%20large-scale%20outdoor%0Aenvironments.%20OpenGraph%20initially%20extracts%20instances%20and%20their%20captions%20from%0Avisual%20images%2C%20enhancing%20textual%20reasoning%20by%20encoding%20them.%20Subsequently%2C%20it%0Aachieves%203D%20incremental%20object-centric%20mapping%20with%20feature%20embedding%20by%0Aprojecting%20images%20onto%20LiDAR%20point%20clouds.%20Finally%2C%20the%20environment%20is%0Asegmented%20based%20on%20lane%20graph%20connectivity%20to%20construct%20a%20hierarchical%20graph.%0AValidation%20results%20from%20public%20dataset%20SemanticKITTI%20demonstrate%20that%20OpenGraph%0Aachieves%20the%20highest%20segmentation%20and%20query%20accuracy.%20The%20source%20code%20of%0AOpenGraph%20is%20publicly%20available%20at%20https%3A//github.com/BIT-DYN/OpenGraph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09412v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenGraph%3A%20Open-Vocabulary%20Hierarchical%203D%20Graph%20Representation%20in%0A%20%20Large-Scale%20Outdoor%20Environments&entry.906535625=Yinan%20Deng%20and%20Jiahui%20Wang%20and%20Jingyu%20Zhao%20and%20Xinyu%20Tian%20and%20Guangyan%20Chen%20and%20Yi%20Yang%20and%20Yufeng%20Yue&entry.1292438233=%20%20Environment%20representations%20endowed%20with%20sophisticated%20semantics%20are%20pivotal%0Afor%20facilitating%20seamless%20interaction%20between%20robots%20and%20humans%2C%20enabling%20them%0Ato%20effectively%20carry%20out%20various%20tasks.%20Open-vocabulary%20maps%2C%20powered%20by%0AVisual-Language%20models%20%28VLMs%29%2C%20possess%20inherent%20advantages%2C%20including%20zero-shot%0Alearning%20and%20support%20for%20open-set%20classes.%20However%2C%20existing%20open-vocabulary%0Amaps%20are%20primarily%20designed%20for%20small-scale%20environments%2C%20such%20as%20desktops%20or%0Arooms%2C%20and%20are%20typically%20geared%20towards%20limited-area%20tasks%20involving%20robotic%0Aindoor%20navigation%20or%20in-place%20manipulation.%20They%20face%20challenges%20in%20direct%0Ageneralization%20to%20outdoor%20environments%20characterized%20by%20numerous%20objects%20and%0Acomplex%20tasks%2C%20owing%20to%20limitations%20in%20both%20understanding%20level%20and%20map%0Astructure.%20In%20this%20work%2C%20we%20propose%20OpenGraph%2C%20the%20first%20open-vocabulary%0Ahierarchical%20graph%20representation%20designed%20for%20large-scale%20outdoor%0Aenvironments.%20OpenGraph%20initially%20extracts%20instances%20and%20their%20captions%20from%0Avisual%20images%2C%20enhancing%20textual%20reasoning%20by%20encoding%20them.%20Subsequently%2C%20it%0Aachieves%203D%20incremental%20object-centric%20mapping%20with%20feature%20embedding%20by%0Aprojecting%20images%20onto%20LiDAR%20point%20clouds.%20Finally%2C%20the%20environment%20is%0Asegmented%20based%20on%20lane%20graph%20connectivity%20to%20construct%20a%20hierarchical%20graph.%0AValidation%20results%20from%20public%20dataset%20SemanticKITTI%20demonstrate%20that%20OpenGraph%0Aachieves%20the%20highest%20segmentation%20and%20query%20accuracy.%20The%20source%20code%20of%0AOpenGraph%20is%20publicly%20available%20at%20https%3A//github.com/BIT-DYN/OpenGraph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09412v2&entry.124074799=Read"},
{"title": "Collaborative Interactive Evolution of Art in the Latent Space of Deep\n  Generative Models", "author": "Ole Hall and Anil Yaman", "abstract": "  Generative Adversarial Networks (GANs) have shown great success in generating\nhigh quality images and are thus used as one of the main approaches to generate\nart images. However, usually the image generation process involves sampling\nfrom the latent space of the learned art representations, allowing little\ncontrol over the output. In this work, we first employ GANs that are trained to\nproduce creative images using an architecture known as Creative Adversarial\nNetworks (CANs), then, we employ an evolutionary approach to navigate within\nthe latent space of the models to discover images. We use automatic aesthetic\nand collaborative interactive human evaluation metrics to assess the generated\nimages. In the human interactive evaluation case, we propose a collaborative\nevaluation based on the assessments of several participants. Furthermore, we\nalso experiment with an intelligent mutation operator that aims to improve the\nquality of the images through local search based on an aesthetic measure. We\nevaluate the effectiveness of this approach by comparing the results produced\nby the automatic and collaborative interactive evolution. The results show that\nthe proposed approach can generate highly attractive art images when the\nevolution is guided by collaborative human feedback.\n", "link": "http://arxiv.org/abs/2403.19620v1", "date": "2024-03-28", "relevancy": 2.2834, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6035}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5486}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5449}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Interactive%20Evolution%20of%20Art%20in%20the%20Latent%20Space%20of%20Deep%0A%20%20Generative%20Models&body=Title%3A%20Collaborative%20Interactive%20Evolution%20of%20Art%20in%20the%20Latent%20Space%20of%20Deep%0A%20%20Generative%20Models%0AAuthor%3A%20Ole%20Hall%20and%20Anil%20Yaman%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20shown%20great%20success%20in%20generating%0Ahigh%20quality%20images%20and%20are%20thus%20used%20as%20one%20of%20the%20main%20approaches%20to%20generate%0Aart%20images.%20However%2C%20usually%20the%20image%20generation%20process%20involves%20sampling%0Afrom%20the%20latent%20space%20of%20the%20learned%20art%20representations%2C%20allowing%20little%0Acontrol%20over%20the%20output.%20In%20this%20work%2C%20we%20first%20employ%20GANs%20that%20are%20trained%20to%0Aproduce%20creative%20images%20using%20an%20architecture%20known%20as%20Creative%20Adversarial%0ANetworks%20%28CANs%29%2C%20then%2C%20we%20employ%20an%20evolutionary%20approach%20to%20navigate%20within%0Athe%20latent%20space%20of%20the%20models%20to%20discover%20images.%20We%20use%20automatic%20aesthetic%0Aand%20collaborative%20interactive%20human%20evaluation%20metrics%20to%20assess%20the%20generated%0Aimages.%20In%20the%20human%20interactive%20evaluation%20case%2C%20we%20propose%20a%20collaborative%0Aevaluation%20based%20on%20the%20assessments%20of%20several%20participants.%20Furthermore%2C%20we%0Aalso%20experiment%20with%20an%20intelligent%20mutation%20operator%20that%20aims%20to%20improve%20the%0Aquality%20of%20the%20images%20through%20local%20search%20based%20on%20an%20aesthetic%20measure.%20We%0Aevaluate%20the%20effectiveness%20of%20this%20approach%20by%20comparing%20the%20results%20produced%0Aby%20the%20automatic%20and%20collaborative%20interactive%20evolution.%20The%20results%20show%20that%0Athe%20proposed%20approach%20can%20generate%20highly%20attractive%20art%20images%20when%20the%0Aevolution%20is%20guided%20by%20collaborative%20human%20feedback.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19620v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Interactive%20Evolution%20of%20Art%20in%20the%20Latent%20Space%20of%20Deep%0A%20%20Generative%20Models&entry.906535625=Ole%20Hall%20and%20Anil%20Yaman&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20shown%20great%20success%20in%20generating%0Ahigh%20quality%20images%20and%20are%20thus%20used%20as%20one%20of%20the%20main%20approaches%20to%20generate%0Aart%20images.%20However%2C%20usually%20the%20image%20generation%20process%20involves%20sampling%0Afrom%20the%20latent%20space%20of%20the%20learned%20art%20representations%2C%20allowing%20little%0Acontrol%20over%20the%20output.%20In%20this%20work%2C%20we%20first%20employ%20GANs%20that%20are%20trained%20to%0Aproduce%20creative%20images%20using%20an%20architecture%20known%20as%20Creative%20Adversarial%0ANetworks%20%28CANs%29%2C%20then%2C%20we%20employ%20an%20evolutionary%20approach%20to%20navigate%20within%0Athe%20latent%20space%20of%20the%20models%20to%20discover%20images.%20We%20use%20automatic%20aesthetic%0Aand%20collaborative%20interactive%20human%20evaluation%20metrics%20to%20assess%20the%20generated%0Aimages.%20In%20the%20human%20interactive%20evaluation%20case%2C%20we%20propose%20a%20collaborative%0Aevaluation%20based%20on%20the%20assessments%20of%20several%20participants.%20Furthermore%2C%20we%0Aalso%20experiment%20with%20an%20intelligent%20mutation%20operator%20that%20aims%20to%20improve%20the%0Aquality%20of%20the%20images%20through%20local%20search%20based%20on%20an%20aesthetic%20measure.%20We%0Aevaluate%20the%20effectiveness%20of%20this%20approach%20by%20comparing%20the%20results%20produced%0Aby%20the%20automatic%20and%20collaborative%20interactive%20evolution.%20The%20results%20show%20that%0Athe%20proposed%20approach%20can%20generate%20highly%20attractive%20art%20images%20when%20the%0Aevolution%20is%20guided%20by%20collaborative%20human%20feedback.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19620v1&entry.124074799=Read"},
{"title": "TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes", "author": "Bu Jin and Yupeng Zheng and Pengfei Li and Weize Li and Yuhang Zheng and Sujie Hu and Xinyu Liu and Jinwei Zhu and Zhijie Yan and Haiyang Sun and Kun Zhan and Peng Jia and Xiaoxiao Long and Yilun Chen and Hao Zhao", "abstract": "  3D dense captioning stands as a cornerstone in achieving a comprehensive\nunderstanding of 3D scenes through natural language. It has recently witnessed\nremarkable achievements, particularly in indoor settings. However, the\nexploration of 3D dense captioning in outdoor scenes is hindered by two major\nchallenges: 1) the \\textbf{domain gap} between indoor and outdoor scenes, such\nas dynamics and sparse visual inputs, makes it difficult to directly adapt\nexisting indoor methods; 2) the \\textbf{lack of data} with comprehensive\nbox-caption pair annotations specifically tailored for outdoor scenes. To this\nend, we introduce the new task of outdoor 3D dense captioning. As input, we\nassume a LiDAR point cloud and a set of RGB images captured by the panoramic\ncamera rig. The expected output is a set of object boxes with captions. To\ntackle this task, we propose the TOD3Cap network, which leverages the BEV\nrepresentation to generate object box proposals and integrates Relation\nQ-Former with LLaMA-Adapter to generate rich captions for these objects. We\nalso introduce the TOD3Cap dataset, the largest one to our knowledge for 3D\ndense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K\noutdoor objects from 850 scenes. Notably, our TOD3Cap network can effectively\nlocalize and caption 3D objects in outdoor scenes, which outperforms baseline\nmethods by a significant margin (+9.6 CiDEr@0.5IoU). Code, data, and models are\npublicly available at https://github.com/jxbbb/TOD3Cap.\n", "link": "http://arxiv.org/abs/2403.19589v1", "date": "2024-03-28", "relevancy": 2.2713, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5704}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5662}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5659}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TOD3Cap%3A%20Towards%203D%20Dense%20Captioning%20in%20Outdoor%20Scenes&body=Title%3A%20TOD3Cap%3A%20Towards%203D%20Dense%20Captioning%20in%20Outdoor%20Scenes%0AAuthor%3A%20Bu%20Jin%20and%20Yupeng%20Zheng%20and%20Pengfei%20Li%20and%20Weize%20Li%20and%20Yuhang%20Zheng%20and%20Sujie%20Hu%20and%20Xinyu%20Liu%20and%20Jinwei%20Zhu%20and%20Zhijie%20Yan%20and%20Haiyang%20Sun%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Xiaoxiao%20Long%20and%20Yilun%20Chen%20and%20Hao%20Zhao%0AAbstract%3A%20%20%203D%20dense%20captioning%20stands%20as%20a%20cornerstone%20in%20achieving%20a%20comprehensive%0Aunderstanding%20of%203D%20scenes%20through%20natural%20language.%20It%20has%20recently%20witnessed%0Aremarkable%20achievements%2C%20particularly%20in%20indoor%20settings.%20However%2C%20the%0Aexploration%20of%203D%20dense%20captioning%20in%20outdoor%20scenes%20is%20hindered%20by%20two%20major%0Achallenges%3A%201%29%20the%20%5Ctextbf%7Bdomain%20gap%7D%20between%20indoor%20and%20outdoor%20scenes%2C%20such%0Aas%20dynamics%20and%20sparse%20visual%20inputs%2C%20makes%20it%20difficult%20to%20directly%20adapt%0Aexisting%20indoor%20methods%3B%202%29%20the%20%5Ctextbf%7Black%20of%20data%7D%20with%20comprehensive%0Abox-caption%20pair%20annotations%20specifically%20tailored%20for%20outdoor%20scenes.%20To%20this%0Aend%2C%20we%20introduce%20the%20new%20task%20of%20outdoor%203D%20dense%20captioning.%20As%20input%2C%20we%0Aassume%20a%20LiDAR%20point%20cloud%20and%20a%20set%20of%20RGB%20images%20captured%20by%20the%20panoramic%0Acamera%20rig.%20The%20expected%20output%20is%20a%20set%20of%20object%20boxes%20with%20captions.%20To%0Atackle%20this%20task%2C%20we%20propose%20the%20TOD3Cap%20network%2C%20which%20leverages%20the%20BEV%0Arepresentation%20to%20generate%20object%20box%20proposals%20and%20integrates%20Relation%0AQ-Former%20with%20LLaMA-Adapter%20to%20generate%20rich%20captions%20for%20these%20objects.%20We%0Aalso%20introduce%20the%20TOD3Cap%20dataset%2C%20the%20largest%20one%20to%20our%20knowledge%20for%203D%0Adense%20captioning%20in%20outdoor%20scenes%2C%20which%20contains%202.3M%20descriptions%20of%2064.3K%0Aoutdoor%20objects%20from%20850%20scenes.%20Notably%2C%20our%20TOD3Cap%20network%20can%20effectively%0Alocalize%20and%20caption%203D%20objects%20in%20outdoor%20scenes%2C%20which%20outperforms%20baseline%0Amethods%20by%20a%20significant%20margin%20%28%2B9.6%20CiDEr%400.5IoU%29.%20Code%2C%20data%2C%20and%20models%20are%0Apublicly%20available%20at%20https%3A//github.com/jxbbb/TOD3Cap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19589v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TOD3Cap%3A%20Towards%203D%20Dense%20Captioning%20in%20Outdoor%20Scenes&entry.906535625=Bu%20Jin%20and%20Yupeng%20Zheng%20and%20Pengfei%20Li%20and%20Weize%20Li%20and%20Yuhang%20Zheng%20and%20Sujie%20Hu%20and%20Xinyu%20Liu%20and%20Jinwei%20Zhu%20and%20Zhijie%20Yan%20and%20Haiyang%20Sun%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Xiaoxiao%20Long%20and%20Yilun%20Chen%20and%20Hao%20Zhao&entry.1292438233=%20%203D%20dense%20captioning%20stands%20as%20a%20cornerstone%20in%20achieving%20a%20comprehensive%0Aunderstanding%20of%203D%20scenes%20through%20natural%20language.%20It%20has%20recently%20witnessed%0Aremarkable%20achievements%2C%20particularly%20in%20indoor%20settings.%20However%2C%20the%0Aexploration%20of%203D%20dense%20captioning%20in%20outdoor%20scenes%20is%20hindered%20by%20two%20major%0Achallenges%3A%201%29%20the%20%5Ctextbf%7Bdomain%20gap%7D%20between%20indoor%20and%20outdoor%20scenes%2C%20such%0Aas%20dynamics%20and%20sparse%20visual%20inputs%2C%20makes%20it%20difficult%20to%20directly%20adapt%0Aexisting%20indoor%20methods%3B%202%29%20the%20%5Ctextbf%7Black%20of%20data%7D%20with%20comprehensive%0Abox-caption%20pair%20annotations%20specifically%20tailored%20for%20outdoor%20scenes.%20To%20this%0Aend%2C%20we%20introduce%20the%20new%20task%20of%20outdoor%203D%20dense%20captioning.%20As%20input%2C%20we%0Aassume%20a%20LiDAR%20point%20cloud%20and%20a%20set%20of%20RGB%20images%20captured%20by%20the%20panoramic%0Acamera%20rig.%20The%20expected%20output%20is%20a%20set%20of%20object%20boxes%20with%20captions.%20To%0Atackle%20this%20task%2C%20we%20propose%20the%20TOD3Cap%20network%2C%20which%20leverages%20the%20BEV%0Arepresentation%20to%20generate%20object%20box%20proposals%20and%20integrates%20Relation%0AQ-Former%20with%20LLaMA-Adapter%20to%20generate%20rich%20captions%20for%20these%20objects.%20We%0Aalso%20introduce%20the%20TOD3Cap%20dataset%2C%20the%20largest%20one%20to%20our%20knowledge%20for%203D%0Adense%20captioning%20in%20outdoor%20scenes%2C%20which%20contains%202.3M%20descriptions%20of%2064.3K%0Aoutdoor%20objects%20from%20850%20scenes.%20Notably%2C%20our%20TOD3Cap%20network%20can%20effectively%0Alocalize%20and%20caption%203D%20objects%20in%20outdoor%20scenes%2C%20which%20outperforms%20baseline%0Amethods%20by%20a%20significant%20margin%20%28%2B9.6%20CiDEr%400.5IoU%29.%20Code%2C%20data%2C%20and%20models%20are%0Apublicly%20available%20at%20https%3A//github.com/jxbbb/TOD3Cap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19589v1&entry.124074799=Read"},
{"title": "Break-for-Make: Modular Low-Rank Adaptations for Composable\n  Content-Style Customization", "author": "Yu Xu and Fan Tang and Juan Cao and Yuxin Zhang and Oliver Deussen and Weiming Dong and Jintao Li and Tong-Yee Lee", "abstract": "  Personalized generation paradigms empower designers to customize visual\nintellectual properties with the help of textual descriptions by tuning or\nadapting pre-trained text-to-image models on a few images. Recent works explore\napproaches for concurrently customizing both content and detailed visual style\nappearance. However, these existing approaches often generate images where the\ncontent and style are entangled. In this study, we reconsider the customization\nof content and style concepts from the perspective of parameter space\nconstruction. Unlike existing methods that utilize a shared parameter space for\ncontent and style, we propose a learning framework that separates the parameter\nspace to facilitate individual learning of content and style, thereby enabling\ndisentangled content and style. To achieve this goal, we introduce \"partly\nlearnable projection\" (PLP) matrices to separate the original adapters into\ndivided sub-parameter spaces. We propose \"break-for-make\" customization\nlearning pipeline based on PLP, which is simple yet effective. We break the\noriginal adapters into \"up projection\" and \"down projection\", train content and\nstyle PLPs individually with the guidance of corresponding textual prompts in\nthe separate adapters, and maintain generalization by employing a\nmulti-correspondence projection learning strategy. Based on the adapters broken\napart for separate training content and style, we then make the entity\nparameter space by reconstructing the content and style PLPs matrices, followed\nby fine-tuning the combined adapter to generate the target object with the\ndesired appearance. Experiments on various styles, including textures,\nmaterials, and artistic style, show that our method outperforms\nstate-of-the-art single/multiple concept learning pipelines in terms of\ncontent-style-prompt alignment.\n", "link": "http://arxiv.org/abs/2403.19456v1", "date": "2024-03-28", "relevancy": 2.2685, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5924}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5672}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5569}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Break-for-Make%3A%20Modular%20Low-Rank%20Adaptations%20for%20Composable%0A%20%20Content-Style%20Customization&body=Title%3A%20Break-for-Make%3A%20Modular%20Low-Rank%20Adaptations%20for%20Composable%0A%20%20Content-Style%20Customization%0AAuthor%3A%20Yu%20Xu%20and%20Fan%20Tang%20and%20Juan%20Cao%20and%20Yuxin%20Zhang%20and%20Oliver%20Deussen%20and%20Weiming%20Dong%20and%20Jintao%20Li%20and%20Tong-Yee%20Lee%0AAbstract%3A%20%20%20Personalized%20generation%20paradigms%20empower%20designers%20to%20customize%20visual%0Aintellectual%20properties%20with%20the%20help%20of%20textual%20descriptions%20by%20tuning%20or%0Aadapting%20pre-trained%20text-to-image%20models%20on%20a%20few%20images.%20Recent%20works%20explore%0Aapproaches%20for%20concurrently%20customizing%20both%20content%20and%20detailed%20visual%20style%0Aappearance.%20However%2C%20these%20existing%20approaches%20often%20generate%20images%20where%20the%0Acontent%20and%20style%20are%20entangled.%20In%20this%20study%2C%20we%20reconsider%20the%20customization%0Aof%20content%20and%20style%20concepts%20from%20the%20perspective%20of%20parameter%20space%0Aconstruction.%20Unlike%20existing%20methods%20that%20utilize%20a%20shared%20parameter%20space%20for%0Acontent%20and%20style%2C%20we%20propose%20a%20learning%20framework%20that%20separates%20the%20parameter%0Aspace%20to%20facilitate%20individual%20learning%20of%20content%20and%20style%2C%20thereby%20enabling%0Adisentangled%20content%20and%20style.%20To%20achieve%20this%20goal%2C%20we%20introduce%20%22partly%0Alearnable%20projection%22%20%28PLP%29%20matrices%20to%20separate%20the%20original%20adapters%20into%0Adivided%20sub-parameter%20spaces.%20We%20propose%20%22break-for-make%22%20customization%0Alearning%20pipeline%20based%20on%20PLP%2C%20which%20is%20simple%20yet%20effective.%20We%20break%20the%0Aoriginal%20adapters%20into%20%22up%20projection%22%20and%20%22down%20projection%22%2C%20train%20content%20and%0Astyle%20PLPs%20individually%20with%20the%20guidance%20of%20corresponding%20textual%20prompts%20in%0Athe%20separate%20adapters%2C%20and%20maintain%20generalization%20by%20employing%20a%0Amulti-correspondence%20projection%20learning%20strategy.%20Based%20on%20the%20adapters%20broken%0Aapart%20for%20separate%20training%20content%20and%20style%2C%20we%20then%20make%20the%20entity%0Aparameter%20space%20by%20reconstructing%20the%20content%20and%20style%20PLPs%20matrices%2C%20followed%0Aby%20fine-tuning%20the%20combined%20adapter%20to%20generate%20the%20target%20object%20with%20the%0Adesired%20appearance.%20Experiments%20on%20various%20styles%2C%20including%20textures%2C%0Amaterials%2C%20and%20artistic%20style%2C%20show%20that%20our%20method%20outperforms%0Astate-of-the-art%20single/multiple%20concept%20learning%20pipelines%20in%20terms%20of%0Acontent-style-prompt%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19456v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Break-for-Make%3A%20Modular%20Low-Rank%20Adaptations%20for%20Composable%0A%20%20Content-Style%20Customization&entry.906535625=Yu%20Xu%20and%20Fan%20Tang%20and%20Juan%20Cao%20and%20Yuxin%20Zhang%20and%20Oliver%20Deussen%20and%20Weiming%20Dong%20and%20Jintao%20Li%20and%20Tong-Yee%20Lee&entry.1292438233=%20%20Personalized%20generation%20paradigms%20empower%20designers%20to%20customize%20visual%0Aintellectual%20properties%20with%20the%20help%20of%20textual%20descriptions%20by%20tuning%20or%0Aadapting%20pre-trained%20text-to-image%20models%20on%20a%20few%20images.%20Recent%20works%20explore%0Aapproaches%20for%20concurrently%20customizing%20both%20content%20and%20detailed%20visual%20style%0Aappearance.%20However%2C%20these%20existing%20approaches%20often%20generate%20images%20where%20the%0Acontent%20and%20style%20are%20entangled.%20In%20this%20study%2C%20we%20reconsider%20the%20customization%0Aof%20content%20and%20style%20concepts%20from%20the%20perspective%20of%20parameter%20space%0Aconstruction.%20Unlike%20existing%20methods%20that%20utilize%20a%20shared%20parameter%20space%20for%0Acontent%20and%20style%2C%20we%20propose%20a%20learning%20framework%20that%20separates%20the%20parameter%0Aspace%20to%20facilitate%20individual%20learning%20of%20content%20and%20style%2C%20thereby%20enabling%0Adisentangled%20content%20and%20style.%20To%20achieve%20this%20goal%2C%20we%20introduce%20%22partly%0Alearnable%20projection%22%20%28PLP%29%20matrices%20to%20separate%20the%20original%20adapters%20into%0Adivided%20sub-parameter%20spaces.%20We%20propose%20%22break-for-make%22%20customization%0Alearning%20pipeline%20based%20on%20PLP%2C%20which%20is%20simple%20yet%20effective.%20We%20break%20the%0Aoriginal%20adapters%20into%20%22up%20projection%22%20and%20%22down%20projection%22%2C%20train%20content%20and%0Astyle%20PLPs%20individually%20with%20the%20guidance%20of%20corresponding%20textual%20prompts%20in%0Athe%20separate%20adapters%2C%20and%20maintain%20generalization%20by%20employing%20a%0Amulti-correspondence%20projection%20learning%20strategy.%20Based%20on%20the%20adapters%20broken%0Aapart%20for%20separate%20training%20content%20and%20style%2C%20we%20then%20make%20the%20entity%0Aparameter%20space%20by%20reconstructing%20the%20content%20and%20style%20PLPs%20matrices%2C%20followed%0Aby%20fine-tuning%20the%20combined%20adapter%20to%20generate%20the%20target%20object%20with%20the%0Adesired%20appearance.%20Experiments%20on%20various%20styles%2C%20including%20textures%2C%0Amaterials%2C%20and%20artistic%20style%2C%20show%20that%20our%20method%20outperforms%0Astate-of-the-art%20single/multiple%20concept%20learning%20pipelines%20in%20terms%20of%0Acontent-style-prompt%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19456v1&entry.124074799=Read"},
{"title": "Locate, Assign, Refine: Taming Customized Image Inpainting with\n  Text-Subject Guidance", "author": "Yulin Pan and Chaojie Mao and Zeyinzi Jiang and Zhen Han and Jingfeng Zhang", "abstract": "  Prior studies have made significant progress in image inpainting guided by\neither text or subject image. However, the research on editing with their\ncombined guidance is still in the early stages. To tackle this challenge, we\npresent LAR-Gen, a novel approach for image inpainting that enables seamless\ninpainting of masked scene images, incorporating both the textual prompts and\nspecified subjects. Our approach adopts a coarse-to-fine manner to ensure\nsubject identity preservation and local semantic coherence. The process\ninvolves (i) Locate: concatenating the noise with masked scene image to achieve\nprecise regional editing, (ii) Assign: employing decoupled cross-attention\nmechanism to accommodate multi-modal guidance, and (iii) Refine: using a novel\nRefineNet to supplement subject details. Additionally, to address the issue of\nscarce training data, we introduce a novel data construction pipeline. This\npipeline extracts substantial pairs of data consisting of local text prompts\nand corresponding visual instances from a vast image dataset, leveraging\npublicly available large models. Extensive experiments and varied application\nscenarios demonstrate the superiority of LAR-Gen in terms of both identity\npreservation and text semantic consistency. Project page can be found at\n\\url{https://ali-vilab.github.io/largen-page/}.\n", "link": "http://arxiv.org/abs/2403.19534v1", "date": "2024-03-28", "relevancy": 2.2674, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5816}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5698}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.558}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Locate%2C%20Assign%2C%20Refine%3A%20Taming%20Customized%20Image%20Inpainting%20with%0A%20%20Text-Subject%20Guidance&body=Title%3A%20Locate%2C%20Assign%2C%20Refine%3A%20Taming%20Customized%20Image%20Inpainting%20with%0A%20%20Text-Subject%20Guidance%0AAuthor%3A%20Yulin%20Pan%20and%20Chaojie%20Mao%20and%20Zeyinzi%20Jiang%20and%20Zhen%20Han%20and%20Jingfeng%20Zhang%0AAbstract%3A%20%20%20Prior%20studies%20have%20made%20significant%20progress%20in%20image%20inpainting%20guided%20by%0Aeither%20text%20or%20subject%20image.%20However%2C%20the%20research%20on%20editing%20with%20their%0Acombined%20guidance%20is%20still%20in%20the%20early%20stages.%20To%20tackle%20this%20challenge%2C%20we%0Apresent%20LAR-Gen%2C%20a%20novel%20approach%20for%20image%20inpainting%20that%20enables%20seamless%0Ainpainting%20of%20masked%20scene%20images%2C%20incorporating%20both%20the%20textual%20prompts%20and%0Aspecified%20subjects.%20Our%20approach%20adopts%20a%20coarse-to-fine%20manner%20to%20ensure%0Asubject%20identity%20preservation%20and%20local%20semantic%20coherence.%20The%20process%0Ainvolves%20%28i%29%20Locate%3A%20concatenating%20the%20noise%20with%20masked%20scene%20image%20to%20achieve%0Aprecise%20regional%20editing%2C%20%28ii%29%20Assign%3A%20employing%20decoupled%20cross-attention%0Amechanism%20to%20accommodate%20multi-modal%20guidance%2C%20and%20%28iii%29%20Refine%3A%20using%20a%20novel%0ARefineNet%20to%20supplement%20subject%20details.%20Additionally%2C%20to%20address%20the%20issue%20of%0Ascarce%20training%20data%2C%20we%20introduce%20a%20novel%20data%20construction%20pipeline.%20This%0Apipeline%20extracts%20substantial%20pairs%20of%20data%20consisting%20of%20local%20text%20prompts%0Aand%20corresponding%20visual%20instances%20from%20a%20vast%20image%20dataset%2C%20leveraging%0Apublicly%20available%20large%20models.%20Extensive%20experiments%20and%20varied%20application%0Ascenarios%20demonstrate%20the%20superiority%20of%20LAR-Gen%20in%20terms%20of%20both%20identity%0Apreservation%20and%20text%20semantic%20consistency.%20Project%20page%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//ali-vilab.github.io/largen-page/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19534v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locate%2C%20Assign%2C%20Refine%3A%20Taming%20Customized%20Image%20Inpainting%20with%0A%20%20Text-Subject%20Guidance&entry.906535625=Yulin%20Pan%20and%20Chaojie%20Mao%20and%20Zeyinzi%20Jiang%20and%20Zhen%20Han%20and%20Jingfeng%20Zhang&entry.1292438233=%20%20Prior%20studies%20have%20made%20significant%20progress%20in%20image%20inpainting%20guided%20by%0Aeither%20text%20or%20subject%20image.%20However%2C%20the%20research%20on%20editing%20with%20their%0Acombined%20guidance%20is%20still%20in%20the%20early%20stages.%20To%20tackle%20this%20challenge%2C%20we%0Apresent%20LAR-Gen%2C%20a%20novel%20approach%20for%20image%20inpainting%20that%20enables%20seamless%0Ainpainting%20of%20masked%20scene%20images%2C%20incorporating%20both%20the%20textual%20prompts%20and%0Aspecified%20subjects.%20Our%20approach%20adopts%20a%20coarse-to-fine%20manner%20to%20ensure%0Asubject%20identity%20preservation%20and%20local%20semantic%20coherence.%20The%20process%0Ainvolves%20%28i%29%20Locate%3A%20concatenating%20the%20noise%20with%20masked%20scene%20image%20to%20achieve%0Aprecise%20regional%20editing%2C%20%28ii%29%20Assign%3A%20employing%20decoupled%20cross-attention%0Amechanism%20to%20accommodate%20multi-modal%20guidance%2C%20and%20%28iii%29%20Refine%3A%20using%20a%20novel%0ARefineNet%20to%20supplement%20subject%20details.%20Additionally%2C%20to%20address%20the%20issue%20of%0Ascarce%20training%20data%2C%20we%20introduce%20a%20novel%20data%20construction%20pipeline.%20This%0Apipeline%20extracts%20substantial%20pairs%20of%20data%20consisting%20of%20local%20text%20prompts%0Aand%20corresponding%20visual%20instances%20from%20a%20vast%20image%20dataset%2C%20leveraging%0Apublicly%20available%20large%20models.%20Extensive%20experiments%20and%20varied%20application%0Ascenarios%20demonstrate%20the%20superiority%20of%20LAR-Gen%20in%20terms%20of%20both%20identity%0Apreservation%20and%20text%20semantic%20consistency.%20Project%20page%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//ali-vilab.github.io/largen-page/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19534v1&entry.124074799=Read"},
{"title": "ViTAR: Vision Transformer with Any Resolution", "author": "Qihang Fan and Quanzeng You and Xiaotian Han and Yongfei Liu and Yunzhe Tao and Huaibo Huang and Ran He and Hongxia Yang", "abstract": "  This paper tackles a significant challenge faced by Vision Transformers\n(ViTs): their constrained scalability across different image resolutions.\nTypically, ViTs experience a performance decline when processing resolutions\ndifferent from those seen during training. Our work introduces two key\ninnovations to address this issue. Firstly, we propose a novel module for\ndynamic resolution adjustment, designed with a single Transformer block,\nspecifically to achieve highly efficient incremental token integration.\nSecondly, we introduce fuzzy positional encoding in the Vision Transformer to\nprovide consistent positional awareness across multiple resolutions, thereby\npreventing overfitting to any single training resolution. Our resulting model,\nViTAR (Vision Transformer with Any Resolution), demonstrates impressive\nadaptability, achieving 83.3\\% top-1 accuracy at a 1120x1120 resolution and\n80.4\\% accuracy at a 4032x4032 resolution, all while reducing computational\ncosts. ViTAR also shows strong performance in downstream tasks such as instance\nand semantic segmentation and can easily combined with self-supervised learning\ntechniques like Masked AutoEncoder. Our work provides a cost-effective solution\nfor enhancing the resolution scalability of ViTs, paving the way for more\nversatile and efficient high-resolution image processing.\n", "link": "http://arxiv.org/abs/2403.18361v2", "date": "2024-03-28", "relevancy": 2.2639, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5864}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5778}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ViTAR%3A%20Vision%20Transformer%20with%20Any%20Resolution&body=Title%3A%20ViTAR%3A%20Vision%20Transformer%20with%20Any%20Resolution%0AAuthor%3A%20Qihang%20Fan%20and%20Quanzeng%20You%20and%20Xiaotian%20Han%20and%20Yongfei%20Liu%20and%20Yunzhe%20Tao%20and%20Huaibo%20Huang%20and%20Ran%20He%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20This%20paper%20tackles%20a%20significant%20challenge%20faced%20by%20Vision%20Transformers%0A%28ViTs%29%3A%20their%20constrained%20scalability%20across%20different%20image%20resolutions.%0ATypically%2C%20ViTs%20experience%20a%20performance%20decline%20when%20processing%20resolutions%0Adifferent%20from%20those%20seen%20during%20training.%20Our%20work%20introduces%20two%20key%0Ainnovations%20to%20address%20this%20issue.%20Firstly%2C%20we%20propose%20a%20novel%20module%20for%0Adynamic%20resolution%20adjustment%2C%20designed%20with%20a%20single%20Transformer%20block%2C%0Aspecifically%20to%20achieve%20highly%20efficient%20incremental%20token%20integration.%0ASecondly%2C%20we%20introduce%20fuzzy%20positional%20encoding%20in%20the%20Vision%20Transformer%20to%0Aprovide%20consistent%20positional%20awareness%20across%20multiple%20resolutions%2C%20thereby%0Apreventing%20overfitting%20to%20any%20single%20training%20resolution.%20Our%20resulting%20model%2C%0AViTAR%20%28Vision%20Transformer%20with%20Any%20Resolution%29%2C%20demonstrates%20impressive%0Aadaptability%2C%20achieving%2083.3%5C%25%20top-1%20accuracy%20at%20a%201120x1120%20resolution%20and%0A80.4%5C%25%20accuracy%20at%20a%204032x4032%20resolution%2C%20all%20while%20reducing%20computational%0Acosts.%20ViTAR%20also%20shows%20strong%20performance%20in%20downstream%20tasks%20such%20as%20instance%0Aand%20semantic%20segmentation%20and%20can%20easily%20combined%20with%20self-supervised%20learning%0Atechniques%20like%20Masked%20AutoEncoder.%20Our%20work%20provides%20a%20cost-effective%20solution%0Afor%20enhancing%20the%20resolution%20scalability%20of%20ViTs%2C%20paving%20the%20way%20for%20more%0Aversatile%20and%20efficient%20high-resolution%20image%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18361v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTAR%3A%20Vision%20Transformer%20with%20Any%20Resolution&entry.906535625=Qihang%20Fan%20and%20Quanzeng%20You%20and%20Xiaotian%20Han%20and%20Yongfei%20Liu%20and%20Yunzhe%20Tao%20and%20Huaibo%20Huang%20and%20Ran%20He%20and%20Hongxia%20Yang&entry.1292438233=%20%20This%20paper%20tackles%20a%20significant%20challenge%20faced%20by%20Vision%20Transformers%0A%28ViTs%29%3A%20their%20constrained%20scalability%20across%20different%20image%20resolutions.%0ATypically%2C%20ViTs%20experience%20a%20performance%20decline%20when%20processing%20resolutions%0Adifferent%20from%20those%20seen%20during%20training.%20Our%20work%20introduces%20two%20key%0Ainnovations%20to%20address%20this%20issue.%20Firstly%2C%20we%20propose%20a%20novel%20module%20for%0Adynamic%20resolution%20adjustment%2C%20designed%20with%20a%20single%20Transformer%20block%2C%0Aspecifically%20to%20achieve%20highly%20efficient%20incremental%20token%20integration.%0ASecondly%2C%20we%20introduce%20fuzzy%20positional%20encoding%20in%20the%20Vision%20Transformer%20to%0Aprovide%20consistent%20positional%20awareness%20across%20multiple%20resolutions%2C%20thereby%0Apreventing%20overfitting%20to%20any%20single%20training%20resolution.%20Our%20resulting%20model%2C%0AViTAR%20%28Vision%20Transformer%20with%20Any%20Resolution%29%2C%20demonstrates%20impressive%0Aadaptability%2C%20achieving%2083.3%5C%25%20top-1%20accuracy%20at%20a%201120x1120%20resolution%20and%0A80.4%5C%25%20accuracy%20at%20a%204032x4032%20resolution%2C%20all%20while%20reducing%20computational%0Acosts.%20ViTAR%20also%20shows%20strong%20performance%20in%20downstream%20tasks%20such%20as%20instance%0Aand%20semantic%20segmentation%20and%20can%20easily%20combined%20with%20self-supervised%20learning%0Atechniques%20like%20Masked%20AutoEncoder.%20Our%20work%20provides%20a%20cost-effective%20solution%0Afor%20enhancing%20the%20resolution%20scalability%20of%20ViTs%2C%20paving%20the%20way%20for%20more%0Aversatile%20and%20efficient%20high-resolution%20image%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18361v2&entry.124074799=Read"},
{"title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long\n  Video Understanding", "author": "Shuhuai Ren and Linli Yao and Shicheng Li and Xu Sun and Lu Hou", "abstract": "  This work proposes TimeChat, a time-sensitive multimodal large language model\nspecifically designed for long video understanding. Our model incorporates two\nkey architectural contributions: (1) a timestamp-aware frame encoder that binds\nvisual content with the timestamp of each frame, and (2) a sliding video\nQ-Former that produces a video token sequence of varying lengths to accommodate\nvideos of various durations. Additionally, we construct an instruction-tuning\ndataset, encompassing 6 tasks and a total of 125K instances, to further enhance\nTimeChat's instruction-following performance. Experiment results across various\nvideo understanding tasks, such as dense captioning, temporal grounding, and\nhighlight detection, demonstrate TimeChat's strong zero-shot temporal\nlocalization and reasoning capabilities. For example, it achieves +9.2 F1 score\nand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)\non Charades-STA, compared to state-of-the-art video large language models,\nholding the potential to serve as a versatile video assistant for long-form\nvideo comprehension tasks and satisfy realistic user requirements.\n", "link": "http://arxiv.org/abs/2312.02051v2", "date": "2024-03-28", "relevancy": 2.2533, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5772}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5715}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5497}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TimeChat%3A%20A%20Time-sensitive%20Multimodal%20Large%20Language%20Model%20for%20Long%0A%20%20Video%20Understanding&body=Title%3A%20TimeChat%3A%20A%20Time-sensitive%20Multimodal%20Large%20Language%20Model%20for%20Long%0A%20%20Video%20Understanding%0AAuthor%3A%20Shuhuai%20Ren%20and%20Linli%20Yao%20and%20Shicheng%20Li%20and%20Xu%20Sun%20and%20Lu%20Hou%0AAbstract%3A%20%20%20This%20work%20proposes%20TimeChat%2C%20a%20time-sensitive%20multimodal%20large%20language%20model%0Aspecifically%20designed%20for%20long%20video%20understanding.%20Our%20model%20incorporates%20two%0Akey%20architectural%20contributions%3A%20%281%29%20a%20timestamp-aware%20frame%20encoder%20that%20binds%0Avisual%20content%20with%20the%20timestamp%20of%20each%20frame%2C%20and%20%282%29%20a%20sliding%20video%0AQ-Former%20that%20produces%20a%20video%20token%20sequence%20of%20varying%20lengths%20to%20accommodate%0Avideos%20of%20various%20durations.%20Additionally%2C%20we%20construct%20an%20instruction-tuning%0Adataset%2C%20encompassing%206%20tasks%20and%20a%20total%20of%20125K%20instances%2C%20to%20further%20enhance%0ATimeChat%27s%20instruction-following%20performance.%20Experiment%20results%20across%20various%0Avideo%20understanding%20tasks%2C%20such%20as%20dense%20captioning%2C%20temporal%20grounding%2C%20and%0Ahighlight%20detection%2C%20demonstrate%20TimeChat%27s%20strong%20zero-shot%20temporal%0Alocalization%20and%20reasoning%20capabilities.%20For%20example%2C%20it%20achieves%20%2B9.2%20F1%20score%0Aand%20%2B2.8%20CIDEr%20on%20YouCook2%2C%20%2B5.8%20HIT%401%20on%20QVHighlights%2C%20and%20%2B27.5%20R%401%20%28IoU%3D0.5%29%0Aon%20Charades-STA%2C%20compared%20to%20state-of-the-art%20video%20large%20language%20models%2C%0Aholding%20the%20potential%20to%20serve%20as%20a%20versatile%20video%20assistant%20for%20long-form%0Avideo%20comprehension%20tasks%20and%20satisfy%20realistic%20user%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02051v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeChat%3A%20A%20Time-sensitive%20Multimodal%20Large%20Language%20Model%20for%20Long%0A%20%20Video%20Understanding&entry.906535625=Shuhuai%20Ren%20and%20Linli%20Yao%20and%20Shicheng%20Li%20and%20Xu%20Sun%20and%20Lu%20Hou&entry.1292438233=%20%20This%20work%20proposes%20TimeChat%2C%20a%20time-sensitive%20multimodal%20large%20language%20model%0Aspecifically%20designed%20for%20long%20video%20understanding.%20Our%20model%20incorporates%20two%0Akey%20architectural%20contributions%3A%20%281%29%20a%20timestamp-aware%20frame%20encoder%20that%20binds%0Avisual%20content%20with%20the%20timestamp%20of%20each%20frame%2C%20and%20%282%29%20a%20sliding%20video%0AQ-Former%20that%20produces%20a%20video%20token%20sequence%20of%20varying%20lengths%20to%20accommodate%0Avideos%20of%20various%20durations.%20Additionally%2C%20we%20construct%20an%20instruction-tuning%0Adataset%2C%20encompassing%206%20tasks%20and%20a%20total%20of%20125K%20instances%2C%20to%20further%20enhance%0ATimeChat%27s%20instruction-following%20performance.%20Experiment%20results%20across%20various%0Avideo%20understanding%20tasks%2C%20such%20as%20dense%20captioning%2C%20temporal%20grounding%2C%20and%0Ahighlight%20detection%2C%20demonstrate%20TimeChat%27s%20strong%20zero-shot%20temporal%0Alocalization%20and%20reasoning%20capabilities.%20For%20example%2C%20it%20achieves%20%2B9.2%20F1%20score%0Aand%20%2B2.8%20CIDEr%20on%20YouCook2%2C%20%2B5.8%20HIT%401%20on%20QVHighlights%2C%20and%20%2B27.5%20R%401%20%28IoU%3D0.5%29%0Aon%20Charades-STA%2C%20compared%20to%20state-of-the-art%20video%20large%20language%20models%2C%0Aholding%20the%20potential%20to%20serve%20as%20a%20versatile%20video%20assistant%20for%20long-form%0Avideo%20comprehension%20tasks%20and%20satisfy%20realistic%20user%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02051v2&entry.124074799=Read"},
{"title": "Semantic Map-based Generation of Navigation Instructions", "author": "Chengzu Li and Chao Zhang and Simone Teufel and Rama Sanand Doddipatla and Svetlana Stoyanchev", "abstract": "  We are interested in the generation of navigation instructions, either in\ntheir own right or as training material for robotic navigation task. In this\npaper, we propose a new approach to navigation instruction generation by\nframing the problem as an image captioning task using semantic maps as visual\ninput. Conventional approaches employ a sequence of panorama images to generate\nnavigation instructions. Semantic maps abstract away from visual details and\nfuse the information in multiple panorama images into a single top-down\nrepresentation, thereby reducing computational complexity to process the input.\nWe present a benchmark dataset for instruction generation using semantic maps,\npropose an initial model and ask human subjects to manually assess the quality\nof generated instructions. Our initial investigations show promise in using\nsemantic maps for instruction generation instead of a sequence of panorama\nimages, but there is vast scope for improvement. We release the code for data\npreparation and model training at https://github.com/chengzu-li/VLGen.\n", "link": "http://arxiv.org/abs/2403.19603v1", "date": "2024-03-28", "relevancy": 2.2445, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.586}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5636}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5487}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Semantic%20Map-based%20Generation%20of%20Navigation%20Instructions&body=Title%3A%20Semantic%20Map-based%20Generation%20of%20Navigation%20Instructions%0AAuthor%3A%20Chengzu%20Li%20and%20Chao%20Zhang%20and%20Simone%20Teufel%20and%20Rama%20Sanand%20Doddipatla%20and%20Svetlana%20Stoyanchev%0AAbstract%3A%20%20%20We%20are%20interested%20in%20the%20generation%20of%20navigation%20instructions%2C%20either%20in%0Atheir%20own%20right%20or%20as%20training%20material%20for%20robotic%20navigation%20task.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20approach%20to%20navigation%20instruction%20generation%20by%0Aframing%20the%20problem%20as%20an%20image%20captioning%20task%20using%20semantic%20maps%20as%20visual%0Ainput.%20Conventional%20approaches%20employ%20a%20sequence%20of%20panorama%20images%20to%20generate%0Anavigation%20instructions.%20Semantic%20maps%20abstract%20away%20from%20visual%20details%20and%0Afuse%20the%20information%20in%20multiple%20panorama%20images%20into%20a%20single%20top-down%0Arepresentation%2C%20thereby%20reducing%20computational%20complexity%20to%20process%20the%20input.%0AWe%20present%20a%20benchmark%20dataset%20for%20instruction%20generation%20using%20semantic%20maps%2C%0Apropose%20an%20initial%20model%20and%20ask%20human%20subjects%20to%20manually%20assess%20the%20quality%0Aof%20generated%20instructions.%20Our%20initial%20investigations%20show%20promise%20in%20using%0Asemantic%20maps%20for%20instruction%20generation%20instead%20of%20a%20sequence%20of%20panorama%0Aimages%2C%20but%20there%20is%20vast%20scope%20for%20improvement.%20We%20release%20the%20code%20for%20data%0Apreparation%20and%20model%20training%20at%20https%3A//github.com/chengzu-li/VLGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19603v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Map-based%20Generation%20of%20Navigation%20Instructions&entry.906535625=Chengzu%20Li%20and%20Chao%20Zhang%20and%20Simone%20Teufel%20and%20Rama%20Sanand%20Doddipatla%20and%20Svetlana%20Stoyanchev&entry.1292438233=%20%20We%20are%20interested%20in%20the%20generation%20of%20navigation%20instructions%2C%20either%20in%0Atheir%20own%20right%20or%20as%20training%20material%20for%20robotic%20navigation%20task.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20approach%20to%20navigation%20instruction%20generation%20by%0Aframing%20the%20problem%20as%20an%20image%20captioning%20task%20using%20semantic%20maps%20as%20visual%0Ainput.%20Conventional%20approaches%20employ%20a%20sequence%20of%20panorama%20images%20to%20generate%0Anavigation%20instructions.%20Semantic%20maps%20abstract%20away%20from%20visual%20details%20and%0Afuse%20the%20information%20in%20multiple%20panorama%20images%20into%20a%20single%20top-down%0Arepresentation%2C%20thereby%20reducing%20computational%20complexity%20to%20process%20the%20input.%0AWe%20present%20a%20benchmark%20dataset%20for%20instruction%20generation%20using%20semantic%20maps%2C%0Apropose%20an%20initial%20model%20and%20ask%20human%20subjects%20to%20manually%20assess%20the%20quality%0Aof%20generated%20instructions.%20Our%20initial%20investigations%20show%20promise%20in%20using%0Asemantic%20maps%20for%20instruction%20generation%20instead%20of%20a%20sequence%20of%20panorama%0Aimages%2C%20but%20there%20is%20vast%20scope%20for%20improvement.%20We%20release%20the%20code%20for%20data%0Apreparation%20and%20model%20training%20at%20https%3A//github.com/chengzu-li/VLGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19603v1&entry.124074799=Read"},
{"title": "SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject\n  Control", "author": "Binyuan Huang and Yuqing Wen and Yucheng Zhao and Yaosi Hu and Yingfei Liu and Fan Jia and Weixin Mao and Tiancai Wang and Chi Zhang and Chang Wen Chen and Zhenzhong Chen and Xiangyu Zhang", "abstract": "  Autonomous driving progress relies on large-scale annotated datasets. In this\nwork, we explore the potential of generative models to produce vast quantities\nof freely-labeled data for autonomous driving applications and present\nSubjectDrive, the first model proven to scale generative data production in a\nway that could continuously improve autonomous driving applications. We\ninvestigate the impact of scaling up the quantity of generative data on the\nperformance of downstream perception models and find that enhancing data\ndiversity plays a crucial role in effectively scaling generative data\nproduction. Therefore, we have developed a novel model equipped with a subject\ncontrol mechanism, which allows the generative model to leverage diverse\nexternal data sources for producing varied and useful data. Extensive\nevaluations confirm SubjectDrive's efficacy in generating scalable autonomous\ndriving training data, marking a significant step toward revolutionizing data\nproduction methods in this field.\n", "link": "http://arxiv.org/abs/2403.19438v1", "date": "2024-03-28", "relevancy": 2.2377, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5776}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5491}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5398}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SubjectDrive%3A%20Scaling%20Generative%20Data%20in%20Autonomous%20Driving%20via%20Subject%0A%20%20Control&body=Title%3A%20SubjectDrive%3A%20Scaling%20Generative%20Data%20in%20Autonomous%20Driving%20via%20Subject%0A%20%20Control%0AAuthor%3A%20Binyuan%20Huang%20and%20Yuqing%20Wen%20and%20Yucheng%20Zhao%20and%20Yaosi%20Hu%20and%20Yingfei%20Liu%20and%20Fan%20Jia%20and%20Weixin%20Mao%20and%20Tiancai%20Wang%20and%20Chi%20Zhang%20and%20Chang%20Wen%20Chen%20and%20Zhenzhong%20Chen%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20Autonomous%20driving%20progress%20relies%20on%20large-scale%20annotated%20datasets.%20In%20this%0Awork%2C%20we%20explore%20the%20potential%20of%20generative%20models%20to%20produce%20vast%20quantities%0Aof%20freely-labeled%20data%20for%20autonomous%20driving%20applications%20and%20present%0ASubjectDrive%2C%20the%20first%20model%20proven%20to%20scale%20generative%20data%20production%20in%20a%0Away%20that%20could%20continuously%20improve%20autonomous%20driving%20applications.%20We%0Ainvestigate%20the%20impact%20of%20scaling%20up%20the%20quantity%20of%20generative%20data%20on%20the%0Aperformance%20of%20downstream%20perception%20models%20and%20find%20that%20enhancing%20data%0Adiversity%20plays%20a%20crucial%20role%20in%20effectively%20scaling%20generative%20data%0Aproduction.%20Therefore%2C%20we%20have%20developed%20a%20novel%20model%20equipped%20with%20a%20subject%0Acontrol%20mechanism%2C%20which%20allows%20the%20generative%20model%20to%20leverage%20diverse%0Aexternal%20data%20sources%20for%20producing%20varied%20and%20useful%20data.%20Extensive%0Aevaluations%20confirm%20SubjectDrive%27s%20efficacy%20in%20generating%20scalable%20autonomous%0Adriving%20training%20data%2C%20marking%20a%20significant%20step%20toward%20revolutionizing%20data%0Aproduction%20methods%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19438v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SubjectDrive%3A%20Scaling%20Generative%20Data%20in%20Autonomous%20Driving%20via%20Subject%0A%20%20Control&entry.906535625=Binyuan%20Huang%20and%20Yuqing%20Wen%20and%20Yucheng%20Zhao%20and%20Yaosi%20Hu%20and%20Yingfei%20Liu%20and%20Fan%20Jia%20and%20Weixin%20Mao%20and%20Tiancai%20Wang%20and%20Chi%20Zhang%20and%20Chang%20Wen%20Chen%20and%20Zhenzhong%20Chen%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20Autonomous%20driving%20progress%20relies%20on%20large-scale%20annotated%20datasets.%20In%20this%0Awork%2C%20we%20explore%20the%20potential%20of%20generative%20models%20to%20produce%20vast%20quantities%0Aof%20freely-labeled%20data%20for%20autonomous%20driving%20applications%20and%20present%0ASubjectDrive%2C%20the%20first%20model%20proven%20to%20scale%20generative%20data%20production%20in%20a%0Away%20that%20could%20continuously%20improve%20autonomous%20driving%20applications.%20We%0Ainvestigate%20the%20impact%20of%20scaling%20up%20the%20quantity%20of%20generative%20data%20on%20the%0Aperformance%20of%20downstream%20perception%20models%20and%20find%20that%20enhancing%20data%0Adiversity%20plays%20a%20crucial%20role%20in%20effectively%20scaling%20generative%20data%0Aproduction.%20Therefore%2C%20we%20have%20developed%20a%20novel%20model%20equipped%20with%20a%20subject%0Acontrol%20mechanism%2C%20which%20allows%20the%20generative%20model%20to%20leverage%20diverse%0Aexternal%20data%20sources%20for%20producing%20varied%20and%20useful%20data.%20Extensive%0Aevaluations%20confirm%20SubjectDrive%27s%20efficacy%20in%20generating%20scalable%20autonomous%0Adriving%20training%20data%2C%20marking%20a%20significant%20step%20toward%20revolutionizing%20data%0Aproduction%20methods%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19438v1&entry.124074799=Read"},
{"title": "A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose\n  Relocalization", "author": "Hongwei Ren and Jiadong Zhu and Yue Zhou and Haotian FU and Yulong Huang and Bojun Cheng", "abstract": "  Event cameras exhibit remarkable attributes such as high dynamic range,\nasynchronicity, and low latency, making them highly suitable for vision tasks\nthat involve high-speed motion in challenging lighting conditions. These\ncameras implicitly capture movement and depth information in events, making\nthem appealing sensors for Camera Pose Relocalization (CPR) tasks.\nNevertheless, existing CPR networks based on events neglect the pivotal\nfine-grained temporal information in events, resulting in unsatisfactory\nperformance. Moreover, the energy-efficient features are further compromised by\nthe use of excessively complex models, hindering efficient deployment on edge\ndevices. In this paper, we introduce PEPNet, a simple and effective point-based\nnetwork designed to regress six degrees of freedom (6-DOFs) event camera poses.\nWe rethink the relationship between the event camera and CPR tasks, leveraging\nthe raw Point Cloud directly as network input to harness the high-temporal\nresolution and inherent sparsity of events. PEPNet is adept at abstracting the\nspatial and implicit temporal features through hierarchical structure and\nexplicit temporal features by Attentive Bi-directional Long Short-Term Memory\n(A-Bi-LSTM). By employing a carefully crafted lightweight design, PEPNet\ndelivers state-of-the-art (SOTA) performance on both indoor and outdoor\ndatasets with meager computational resources. Specifically, PEPNet attains a\nsignificant 38% and 33% performance improvement on the random split IJRR and\nM3ED datasets, respectively. Moreover, the lightweight design version\nPEPNet$_{tiny}$ accomplishes results comparable to the SOTA while employing a\nmere 0.5% of the parameters.\n", "link": "http://arxiv.org/abs/2403.19412v1", "date": "2024-03-28", "relevancy": 2.2246, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5671}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5091}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20and%20Effective%20Point-based%20Network%20for%20Event%20Camera%206-DOFs%20Pose%0A%20%20Relocalization&body=Title%3A%20A%20Simple%20and%20Effective%20Point-based%20Network%20for%20Event%20Camera%206-DOFs%20Pose%0A%20%20Relocalization%0AAuthor%3A%20Hongwei%20Ren%20and%20Jiadong%20Zhu%20and%20Yue%20Zhou%20and%20Haotian%20FU%20and%20Yulong%20Huang%20and%20Bojun%20Cheng%0AAbstract%3A%20%20%20Event%20cameras%20exhibit%20remarkable%20attributes%20such%20as%20high%20dynamic%20range%2C%0Aasynchronicity%2C%20and%20low%20latency%2C%20making%20them%20highly%20suitable%20for%20vision%20tasks%0Athat%20involve%20high-speed%20motion%20in%20challenging%20lighting%20conditions.%20These%0Acameras%20implicitly%20capture%20movement%20and%20depth%20information%20in%20events%2C%20making%0Athem%20appealing%20sensors%20for%20Camera%20Pose%20Relocalization%20%28CPR%29%20tasks.%0ANevertheless%2C%20existing%20CPR%20networks%20based%20on%20events%20neglect%20the%20pivotal%0Afine-grained%20temporal%20information%20in%20events%2C%20resulting%20in%20unsatisfactory%0Aperformance.%20Moreover%2C%20the%20energy-efficient%20features%20are%20further%20compromised%20by%0Athe%20use%20of%20excessively%20complex%20models%2C%20hindering%20efficient%20deployment%20on%20edge%0Adevices.%20In%20this%20paper%2C%20we%20introduce%20PEPNet%2C%20a%20simple%20and%20effective%20point-based%0Anetwork%20designed%20to%20regress%20six%20degrees%20of%20freedom%20%286-DOFs%29%20event%20camera%20poses.%0AWe%20rethink%20the%20relationship%20between%20the%20event%20camera%20and%20CPR%20tasks%2C%20leveraging%0Athe%20raw%20Point%20Cloud%20directly%20as%20network%20input%20to%20harness%20the%20high-temporal%0Aresolution%20and%20inherent%20sparsity%20of%20events.%20PEPNet%20is%20adept%20at%20abstracting%20the%0Aspatial%20and%20implicit%20temporal%20features%20through%20hierarchical%20structure%20and%0Aexplicit%20temporal%20features%20by%20Attentive%20Bi-directional%20Long%20Short-Term%20Memory%0A%28A-Bi-LSTM%29.%20By%20employing%20a%20carefully%20crafted%20lightweight%20design%2C%20PEPNet%0Adelivers%20state-of-the-art%20%28SOTA%29%20performance%20on%20both%20indoor%20and%20outdoor%0Adatasets%20with%20meager%20computational%20resources.%20Specifically%2C%20PEPNet%20attains%20a%0Asignificant%2038%25%20and%2033%25%20performance%20improvement%20on%20the%20random%20split%20IJRR%20and%0AM3ED%20datasets%2C%20respectively.%20Moreover%2C%20the%20lightweight%20design%20version%0APEPNet%24_%7Btiny%7D%24%20accomplishes%20results%20comparable%20to%20the%20SOTA%20while%20employing%20a%0Amere%200.5%25%20of%20the%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19412v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20and%20Effective%20Point-based%20Network%20for%20Event%20Camera%206-DOFs%20Pose%0A%20%20Relocalization&entry.906535625=Hongwei%20Ren%20and%20Jiadong%20Zhu%20and%20Yue%20Zhou%20and%20Haotian%20FU%20and%20Yulong%20Huang%20and%20Bojun%20Cheng&entry.1292438233=%20%20Event%20cameras%20exhibit%20remarkable%20attributes%20such%20as%20high%20dynamic%20range%2C%0Aasynchronicity%2C%20and%20low%20latency%2C%20making%20them%20highly%20suitable%20for%20vision%20tasks%0Athat%20involve%20high-speed%20motion%20in%20challenging%20lighting%20conditions.%20These%0Acameras%20implicitly%20capture%20movement%20and%20depth%20information%20in%20events%2C%20making%0Athem%20appealing%20sensors%20for%20Camera%20Pose%20Relocalization%20%28CPR%29%20tasks.%0ANevertheless%2C%20existing%20CPR%20networks%20based%20on%20events%20neglect%20the%20pivotal%0Afine-grained%20temporal%20information%20in%20events%2C%20resulting%20in%20unsatisfactory%0Aperformance.%20Moreover%2C%20the%20energy-efficient%20features%20are%20further%20compromised%20by%0Athe%20use%20of%20excessively%20complex%20models%2C%20hindering%20efficient%20deployment%20on%20edge%0Adevices.%20In%20this%20paper%2C%20we%20introduce%20PEPNet%2C%20a%20simple%20and%20effective%20point-based%0Anetwork%20designed%20to%20regress%20six%20degrees%20of%20freedom%20%286-DOFs%29%20event%20camera%20poses.%0AWe%20rethink%20the%20relationship%20between%20the%20event%20camera%20and%20CPR%20tasks%2C%20leveraging%0Athe%20raw%20Point%20Cloud%20directly%20as%20network%20input%20to%20harness%20the%20high-temporal%0Aresolution%20and%20inherent%20sparsity%20of%20events.%20PEPNet%20is%20adept%20at%20abstracting%20the%0Aspatial%20and%20implicit%20temporal%20features%20through%20hierarchical%20structure%20and%0Aexplicit%20temporal%20features%20by%20Attentive%20Bi-directional%20Long%20Short-Term%20Memory%0A%28A-Bi-LSTM%29.%20By%20employing%20a%20carefully%20crafted%20lightweight%20design%2C%20PEPNet%0Adelivers%20state-of-the-art%20%28SOTA%29%20performance%20on%20both%20indoor%20and%20outdoor%0Adatasets%20with%20meager%20computational%20resources.%20Specifically%2C%20PEPNet%20attains%20a%0Asignificant%2038%25%20and%2033%25%20performance%20improvement%20on%20the%20random%20split%20IJRR%20and%0AM3ED%20datasets%2C%20respectively.%20Moreover%2C%20the%20lightweight%20design%20version%0APEPNet%24_%7Btiny%7D%24%20accomplishes%20results%20comparable%20to%20the%20SOTA%20while%20employing%20a%0Amere%200.5%25%20of%20the%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19412v1&entry.124074799=Read"},
{"title": "Siamese Vision Transformers are Scalable Audio-visual Learners", "author": "Yan-Bo Lin and Gedas Bertasius", "abstract": "  Traditional audio-visual methods rely on independent audio and visual\nbackbones, which is costly and not scalable. In this work, we investigate using\nan audio-visual siamese network (AVSiam) for efficient and scalable\naudio-visual pretraining. Our framework uses a single shared vision transformer\nbackbone to process audio and visual inputs, improving its parameter\nefficiency, reducing the GPU memory footprint, and allowing us to scale our\nmethod to larger datasets and model sizes. We pretrain our model using a\ncontrastive audio-visual matching objective with a multi-ratio random masking\nscheme, which enables our model to process larger audio-visual instance\nbatches, helpful for contrastive learning. Unlike prior audio-visual methods,\nour method can robustly handle audio, visual, and audio-visual inputs with a\nsingle shared ViT backbone. Furthermore, despite using the shared backbone for\nboth modalities, AVSiam achieves competitive or even better results than prior\nmethods on AudioSet and VGGSound for audio-visual classification and retrieval.\nOur code is available at https://github.com/GenjiB/AVSiam\n", "link": "http://arxiv.org/abs/2403.19638v1", "date": "2024-03-28", "relevancy": 2.2116, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5756}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5536}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5299}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Siamese%20Vision%20Transformers%20are%20Scalable%20Audio-visual%20Learners&body=Title%3A%20Siamese%20Vision%20Transformers%20are%20Scalable%20Audio-visual%20Learners%0AAuthor%3A%20Yan-Bo%20Lin%20and%20Gedas%20Bertasius%0AAbstract%3A%20%20%20Traditional%20audio-visual%20methods%20rely%20on%20independent%20audio%20and%20visual%0Abackbones%2C%20which%20is%20costly%20and%20not%20scalable.%20In%20this%20work%2C%20we%20investigate%20using%0Aan%20audio-visual%20siamese%20network%20%28AVSiam%29%20for%20efficient%20and%20scalable%0Aaudio-visual%20pretraining.%20Our%20framework%20uses%20a%20single%20shared%20vision%20transformer%0Abackbone%20to%20process%20audio%20and%20visual%20inputs%2C%20improving%20its%20parameter%0Aefficiency%2C%20reducing%20the%20GPU%20memory%20footprint%2C%20and%20allowing%20us%20to%20scale%20our%0Amethod%20to%20larger%20datasets%20and%20model%20sizes.%20We%20pretrain%20our%20model%20using%20a%0Acontrastive%20audio-visual%20matching%20objective%20with%20a%20multi-ratio%20random%20masking%0Ascheme%2C%20which%20enables%20our%20model%20to%20process%20larger%20audio-visual%20instance%0Abatches%2C%20helpful%20for%20contrastive%20learning.%20Unlike%20prior%20audio-visual%20methods%2C%0Aour%20method%20can%20robustly%20handle%20audio%2C%20visual%2C%20and%20audio-visual%20inputs%20with%20a%0Asingle%20shared%20ViT%20backbone.%20Furthermore%2C%20despite%20using%20the%20shared%20backbone%20for%0Aboth%20modalities%2C%20AVSiam%20achieves%20competitive%20or%20even%20better%20results%20than%20prior%0Amethods%20on%20AudioSet%20and%20VGGSound%20for%20audio-visual%20classification%20and%20retrieval.%0AOur%20code%20is%20available%20at%20https%3A//github.com/GenjiB/AVSiam%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19638v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Siamese%20Vision%20Transformers%20are%20Scalable%20Audio-visual%20Learners&entry.906535625=Yan-Bo%20Lin%20and%20Gedas%20Bertasius&entry.1292438233=%20%20Traditional%20audio-visual%20methods%20rely%20on%20independent%20audio%20and%20visual%0Abackbones%2C%20which%20is%20costly%20and%20not%20scalable.%20In%20this%20work%2C%20we%20investigate%20using%0Aan%20audio-visual%20siamese%20network%20%28AVSiam%29%20for%20efficient%20and%20scalable%0Aaudio-visual%20pretraining.%20Our%20framework%20uses%20a%20single%20shared%20vision%20transformer%0Abackbone%20to%20process%20audio%20and%20visual%20inputs%2C%20improving%20its%20parameter%0Aefficiency%2C%20reducing%20the%20GPU%20memory%20footprint%2C%20and%20allowing%20us%20to%20scale%20our%0Amethod%20to%20larger%20datasets%20and%20model%20sizes.%20We%20pretrain%20our%20model%20using%20a%0Acontrastive%20audio-visual%20matching%20objective%20with%20a%20multi-ratio%20random%20masking%0Ascheme%2C%20which%20enables%20our%20model%20to%20process%20larger%20audio-visual%20instance%0Abatches%2C%20helpful%20for%20contrastive%20learning.%20Unlike%20prior%20audio-visual%20methods%2C%0Aour%20method%20can%20robustly%20handle%20audio%2C%20visual%2C%20and%20audio-visual%20inputs%20with%20a%0Asingle%20shared%20ViT%20backbone.%20Furthermore%2C%20despite%20using%20the%20shared%20backbone%20for%0Aboth%20modalities%2C%20AVSiam%20achieves%20competitive%20or%20even%20better%20results%20than%20prior%0Amethods%20on%20AudioSet%20and%20VGGSound%20for%20audio-visual%20classification%20and%20retrieval.%0AOur%20code%20is%20available%20at%20https%3A//github.com/GenjiB/AVSiam%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19638v1&entry.124074799=Read"},
{"title": "BAMM: Bidirectional Autoregressive Motion Model", "author": "Ekkasit Pinyoanuntapong and Muhammad Usama Saleem and Pu Wang and Minwoo Lee and Srijan Das and Chen Chen", "abstract": "  Generating human motion from text has been dominated by denoising motion\nmodels either through diffusion or generative masking process. However, these\nmodels face great limitations in usability by requiring prior knowledge of the\nmotion length. Conversely, autoregressive motion models address this limitation\nby adaptively predicting motion endpoints, at the cost of degraded generation\nquality and editing capabilities. To address these challenges, we propose\nBidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion\ngeneration framework. BAMM consists of two key components: (1) a motion\ntokenizer that transforms 3D human motion into discrete tokens in latent space,\nand (2) a masked self-attention transformer that autoregressively predicts\nrandomly masked tokens via a hybrid attention masking strategy. By unifying\ngenerative masked modeling and autoregressive modeling, BAMM captures rich and\nbidirectional dependencies among motion tokens, while learning the\nprobabilistic mapping from textual inputs to motion outputs with\ndynamically-adjusted motion sequence length. This feature enables BAMM to\nsimultaneously achieving high-quality motion generation with enhanced usability\nand built-in motion editability. Extensive experiments on HumanML3D and KIT-ML\ndatasets demonstrate that BAMM surpasses current state-of-the-art methods in\nboth qualitative and quantitative measures.\n", "link": "http://arxiv.org/abs/2403.19435v1", "date": "2024-03-28", "relevancy": 2.1889, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5899}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5333}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BAMM%3A%20Bidirectional%20Autoregressive%20Motion%20Model&body=Title%3A%20BAMM%3A%20Bidirectional%20Autoregressive%20Motion%20Model%0AAuthor%3A%20Ekkasit%20Pinyoanuntapong%20and%20Muhammad%20Usama%20Saleem%20and%20Pu%20Wang%20and%20Minwoo%20Lee%20and%20Srijan%20Das%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Generating%20human%20motion%20from%20text%20has%20been%20dominated%20by%20denoising%20motion%0Amodels%20either%20through%20diffusion%20or%20generative%20masking%20process.%20However%2C%20these%0Amodels%20face%20great%20limitations%20in%20usability%20by%20requiring%20prior%20knowledge%20of%20the%0Amotion%20length.%20Conversely%2C%20autoregressive%20motion%20models%20address%20this%20limitation%0Aby%20adaptively%20predicting%20motion%20endpoints%2C%20at%20the%20cost%20of%20degraded%20generation%0Aquality%20and%20editing%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%0ABidirectional%20Autoregressive%20Motion%20Model%20%28BAMM%29%2C%20a%20novel%20text-to-motion%0Ageneration%20framework.%20BAMM%20consists%20of%20two%20key%20components%3A%20%281%29%20a%20motion%0Atokenizer%20that%20transforms%203D%20human%20motion%20into%20discrete%20tokens%20in%20latent%20space%2C%0Aand%20%282%29%20a%20masked%20self-attention%20transformer%20that%20autoregressively%20predicts%0Arandomly%20masked%20tokens%20via%20a%20hybrid%20attention%20masking%20strategy.%20By%20unifying%0Agenerative%20masked%20modeling%20and%20autoregressive%20modeling%2C%20BAMM%20captures%20rich%20and%0Abidirectional%20dependencies%20among%20motion%20tokens%2C%20while%20learning%20the%0Aprobabilistic%20mapping%20from%20textual%20inputs%20to%20motion%20outputs%20with%0Adynamically-adjusted%20motion%20sequence%20length.%20This%20feature%20enables%20BAMM%20to%0Asimultaneously%20achieving%20high-quality%20motion%20generation%20with%20enhanced%20usability%0Aand%20built-in%20motion%20editability.%20Extensive%20experiments%20on%20HumanML3D%20and%20KIT-ML%0Adatasets%20demonstrate%20that%20BAMM%20surpasses%20current%20state-of-the-art%20methods%20in%0Aboth%20qualitative%20and%20quantitative%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19435v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAMM%3A%20Bidirectional%20Autoregressive%20Motion%20Model&entry.906535625=Ekkasit%20Pinyoanuntapong%20and%20Muhammad%20Usama%20Saleem%20and%20Pu%20Wang%20and%20Minwoo%20Lee%20and%20Srijan%20Das%20and%20Chen%20Chen&entry.1292438233=%20%20Generating%20human%20motion%20from%20text%20has%20been%20dominated%20by%20denoising%20motion%0Amodels%20either%20through%20diffusion%20or%20generative%20masking%20process.%20However%2C%20these%0Amodels%20face%20great%20limitations%20in%20usability%20by%20requiring%20prior%20knowledge%20of%20the%0Amotion%20length.%20Conversely%2C%20autoregressive%20motion%20models%20address%20this%20limitation%0Aby%20adaptively%20predicting%20motion%20endpoints%2C%20at%20the%20cost%20of%20degraded%20generation%0Aquality%20and%20editing%20capabilities.%20To%20address%20these%20challenges%2C%20we%20propose%0ABidirectional%20Autoregressive%20Motion%20Model%20%28BAMM%29%2C%20a%20novel%20text-to-motion%0Ageneration%20framework.%20BAMM%20consists%20of%20two%20key%20components%3A%20%281%29%20a%20motion%0Atokenizer%20that%20transforms%203D%20human%20motion%20into%20discrete%20tokens%20in%20latent%20space%2C%0Aand%20%282%29%20a%20masked%20self-attention%20transformer%20that%20autoregressively%20predicts%0Arandomly%20masked%20tokens%20via%20a%20hybrid%20attention%20masking%20strategy.%20By%20unifying%0Agenerative%20masked%20modeling%20and%20autoregressive%20modeling%2C%20BAMM%20captures%20rich%20and%0Abidirectional%20dependencies%20among%20motion%20tokens%2C%20while%20learning%20the%0Aprobabilistic%20mapping%20from%20textual%20inputs%20to%20motion%20outputs%20with%0Adynamically-adjusted%20motion%20sequence%20length.%20This%20feature%20enables%20BAMM%20to%0Asimultaneously%20achieving%20high-quality%20motion%20generation%20with%20enhanced%20usability%0Aand%20built-in%20motion%20editability.%20Extensive%20experiments%20on%20HumanML3D%20and%20KIT-ML%0Adatasets%20demonstrate%20that%20BAMM%20surpasses%20current%20state-of-the-art%20methods%20in%0Aboth%20qualitative%20and%20quantitative%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19435v1&entry.124074799=Read"},
{"title": "Segmentation tool for images of cracks", "author": "Andrii Kompanets and Remco Duits and Davide Leonetti and Nicky van den Berg and H. H. and  Snijder", "abstract": "  Safety-critical infrastructures, such as bridges, are periodically inspected\nto check for existing damage, such as fatigue cracks and corrosion, and to\nguarantee the safe use of the infrastructure. Visual inspection is the most\nfrequent type of general inspection, despite the fact that its detection\ncapability is rather limited, especially for fatigue cracks. Machine learning\nalgorithms can be used for augmenting the capability of classical visual\ninspection of bridge structures, however, the implementation of such an\nalgorithm requires a massive annotated training dataset, which is\ntime-consuming to produce. This paper proposes a semi-automatic crack\nsegmentation tool that eases the manual segmentation of cracks on images needed\nto create a training dataset for a machine learning algorithm. Also, it can be\nused to measure the geometry of the crack. This tool makes use of an image\nprocessing algorithm, which was initially developed for the analysis of\nvascular systems on retinal images. The algorithm relies on a multi-orientation\nwavelet transform, which is applied to the image to construct the so-called\n\"orientation scores\", i.e. a modified version of the image. Afterwards, the\nfiltered orientation scores are used to formulate an optimal path problem that\nidentifies the crack. The globally optimal path between manually selected crack\nendpoints is computed, using a state-of-the-art geometric tracking method. The\npixel-wise segmentation is done afterwards using the obtained crack path. The\nproposed method outperforms fully automatic methods and shows potential to be\nan adequate alternative to the manual data annotation.\n", "link": "http://arxiv.org/abs/2403.19492v1", "date": "2024-03-28", "relevancy": 2.1827, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.439}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4314}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Segmentation%20tool%20for%20images%20of%20cracks&body=Title%3A%20Segmentation%20tool%20for%20images%20of%20cracks%0AAuthor%3A%20Andrii%20Kompanets%20and%20Remco%20Duits%20and%20Davide%20Leonetti%20and%20Nicky%20van%20den%20Berg%20and%20H.%20H.%20and%20%20Snijder%0AAbstract%3A%20%20%20Safety-critical%20infrastructures%2C%20such%20as%20bridges%2C%20are%20periodically%20inspected%0Ato%20check%20for%20existing%20damage%2C%20such%20as%20fatigue%20cracks%20and%20corrosion%2C%20and%20to%0Aguarantee%20the%20safe%20use%20of%20the%20infrastructure.%20Visual%20inspection%20is%20the%20most%0Afrequent%20type%20of%20general%20inspection%2C%20despite%20the%20fact%20that%20its%20detection%0Acapability%20is%20rather%20limited%2C%20especially%20for%20fatigue%20cracks.%20Machine%20learning%0Aalgorithms%20can%20be%20used%20for%20augmenting%20the%20capability%20of%20classical%20visual%0Ainspection%20of%20bridge%20structures%2C%20however%2C%20the%20implementation%20of%20such%20an%0Aalgorithm%20requires%20a%20massive%20annotated%20training%20dataset%2C%20which%20is%0Atime-consuming%20to%20produce.%20This%20paper%20proposes%20a%20semi-automatic%20crack%0Asegmentation%20tool%20that%20eases%20the%20manual%20segmentation%20of%20cracks%20on%20images%20needed%0Ato%20create%20a%20training%20dataset%20for%20a%20machine%20learning%20algorithm.%20Also%2C%20it%20can%20be%0Aused%20to%20measure%20the%20geometry%20of%20the%20crack.%20This%20tool%20makes%20use%20of%20an%20image%0Aprocessing%20algorithm%2C%20which%20was%20initially%20developed%20for%20the%20analysis%20of%0Avascular%20systems%20on%20retinal%20images.%20The%20algorithm%20relies%20on%20a%20multi-orientation%0Awavelet%20transform%2C%20which%20is%20applied%20to%20the%20image%20to%20construct%20the%20so-called%0A%22orientation%20scores%22%2C%20i.e.%20a%20modified%20version%20of%20the%20image.%20Afterwards%2C%20the%0Afiltered%20orientation%20scores%20are%20used%20to%20formulate%20an%20optimal%20path%20problem%20that%0Aidentifies%20the%20crack.%20The%20globally%20optimal%20path%20between%20manually%20selected%20crack%0Aendpoints%20is%20computed%2C%20using%20a%20state-of-the-art%20geometric%20tracking%20method.%20The%0Apixel-wise%20segmentation%20is%20done%20afterwards%20using%20the%20obtained%20crack%20path.%20The%0Aproposed%20method%20outperforms%20fully%20automatic%20methods%20and%20shows%20potential%20to%20be%0Aan%20adequate%20alternative%20to%20the%20manual%20data%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19492v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20tool%20for%20images%20of%20cracks&entry.906535625=Andrii%20Kompanets%20and%20Remco%20Duits%20and%20Davide%20Leonetti%20and%20Nicky%20van%20den%20Berg%20and%20H.%20H.%20and%20%20Snijder&entry.1292438233=%20%20Safety-critical%20infrastructures%2C%20such%20as%20bridges%2C%20are%20periodically%20inspected%0Ato%20check%20for%20existing%20damage%2C%20such%20as%20fatigue%20cracks%20and%20corrosion%2C%20and%20to%0Aguarantee%20the%20safe%20use%20of%20the%20infrastructure.%20Visual%20inspection%20is%20the%20most%0Afrequent%20type%20of%20general%20inspection%2C%20despite%20the%20fact%20that%20its%20detection%0Acapability%20is%20rather%20limited%2C%20especially%20for%20fatigue%20cracks.%20Machine%20learning%0Aalgorithms%20can%20be%20used%20for%20augmenting%20the%20capability%20of%20classical%20visual%0Ainspection%20of%20bridge%20structures%2C%20however%2C%20the%20implementation%20of%20such%20an%0Aalgorithm%20requires%20a%20massive%20annotated%20training%20dataset%2C%20which%20is%0Atime-consuming%20to%20produce.%20This%20paper%20proposes%20a%20semi-automatic%20crack%0Asegmentation%20tool%20that%20eases%20the%20manual%20segmentation%20of%20cracks%20on%20images%20needed%0Ato%20create%20a%20training%20dataset%20for%20a%20machine%20learning%20algorithm.%20Also%2C%20it%20can%20be%0Aused%20to%20measure%20the%20geometry%20of%20the%20crack.%20This%20tool%20makes%20use%20of%20an%20image%0Aprocessing%20algorithm%2C%20which%20was%20initially%20developed%20for%20the%20analysis%20of%0Avascular%20systems%20on%20retinal%20images.%20The%20algorithm%20relies%20on%20a%20multi-orientation%0Awavelet%20transform%2C%20which%20is%20applied%20to%20the%20image%20to%20construct%20the%20so-called%0A%22orientation%20scores%22%2C%20i.e.%20a%20modified%20version%20of%20the%20image.%20Afterwards%2C%20the%0Afiltered%20orientation%20scores%20are%20used%20to%20formulate%20an%20optimal%20path%20problem%20that%0Aidentifies%20the%20crack.%20The%20globally%20optimal%20path%20between%20manually%20selected%20crack%0Aendpoints%20is%20computed%2C%20using%20a%20state-of-the-art%20geometric%20tracking%20method.%20The%0Apixel-wise%20segmentation%20is%20done%20afterwards%20using%20the%20obtained%20crack%20path.%20The%0Aproposed%20method%20outperforms%20fully%20automatic%20methods%20and%20shows%20potential%20to%20be%0Aan%20adequate%20alternative%20to%20the%20manual%20data%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19492v1&entry.124074799=Read"},
{"title": "H2ASeg: Hierarchical Adaptive Interaction and Weighting Network for\n  Tumor Segmentation in PET/CT Images", "author": "Jinpeng Lu and Jingyun Chen and Linghan Cai and Songhan Jiang and Yongbing Zhang", "abstract": "  Positron emission tomography (PET) combined with computed tomography (CT)\nimaging is routinely used in cancer diagnosis and prognosis by providing\ncomplementary information. Automatically segmenting tumors in PET/CT images can\nsignificantly improve examination efficiency. Traditional multi-modal\nsegmentation solutions mainly rely on concatenation operations for modality\nfusion, which fail to effectively model the non-linear dependencies between PET\nand CT modalities. Recent studies have investigated various approaches to\noptimize the fusion of modality-specific features for enhancing joint\nrepresentations. However, modality-specific encoders used in these methods\noperate independently, inadequately leveraging the synergistic relationships\ninherent in PET and CT modalities, for example, the complementarity between\nsemantics and structure. To address these issues, we propose a Hierarchical\nAdaptive Interaction and Weighting Network termed H2ASeg to explore the\nintrinsic cross-modal correlations and transfer potential complementary\ninformation. Specifically, we design a Modality-Cooperative Spatial Attention\n(MCSA) module that performs intra- and inter-modal interactions globally and\nlocally. Additionally, a Target-Aware Modality Weighting (TAMW) module is\ndeveloped to highlight tumor-related features within multi-modal features,\nthereby refining tumor segmentation. By embedding these modules across\ndifferent layers, H2ASeg can hierarchically model cross-modal correlations,\nenabling a nuanced understanding of both semantic and structural tumor\nfeatures. Extensive experiments demonstrate the superiority of H2ASeg,\noutperforming state-of-the-art methods on AutoPet-II and Hecktor2022\nbenchmarks. The code is released at https://github.com/JinPLu/H2ASeg.\n", "link": "http://arxiv.org/abs/2403.18339v2", "date": "2024-03-28", "relevancy": 2.1759, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5806}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5414}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5083}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20H2ASeg%3A%20Hierarchical%20Adaptive%20Interaction%20and%20Weighting%20Network%20for%0A%20%20Tumor%20Segmentation%20in%20PET/CT%20Images&body=Title%3A%20H2ASeg%3A%20Hierarchical%20Adaptive%20Interaction%20and%20Weighting%20Network%20for%0A%20%20Tumor%20Segmentation%20in%20PET/CT%20Images%0AAuthor%3A%20Jinpeng%20Lu%20and%20Jingyun%20Chen%20and%20Linghan%20Cai%20and%20Songhan%20Jiang%20and%20Yongbing%20Zhang%0AAbstract%3A%20%20%20Positron%20emission%20tomography%20%28PET%29%20combined%20with%20computed%20tomography%20%28CT%29%0Aimaging%20is%20routinely%20used%20in%20cancer%20diagnosis%20and%20prognosis%20by%20providing%0Acomplementary%20information.%20Automatically%20segmenting%20tumors%20in%20PET/CT%20images%20can%0Asignificantly%20improve%20examination%20efficiency.%20Traditional%20multi-modal%0Asegmentation%20solutions%20mainly%20rely%20on%20concatenation%20operations%20for%20modality%0Afusion%2C%20which%20fail%20to%20effectively%20model%20the%20non-linear%20dependencies%20between%20PET%0Aand%20CT%20modalities.%20Recent%20studies%20have%20investigated%20various%20approaches%20to%0Aoptimize%20the%20fusion%20of%20modality-specific%20features%20for%20enhancing%20joint%0Arepresentations.%20However%2C%20modality-specific%20encoders%20used%20in%20these%20methods%0Aoperate%20independently%2C%20inadequately%20leveraging%20the%20synergistic%20relationships%0Ainherent%20in%20PET%20and%20CT%20modalities%2C%20for%20example%2C%20the%20complementarity%20between%0Asemantics%20and%20structure.%20To%20address%20these%20issues%2C%20we%20propose%20a%20Hierarchical%0AAdaptive%20Interaction%20and%20Weighting%20Network%20termed%20H2ASeg%20to%20explore%20the%0Aintrinsic%20cross-modal%20correlations%20and%20transfer%20potential%20complementary%0Ainformation.%20Specifically%2C%20we%20design%20a%20Modality-Cooperative%20Spatial%20Attention%0A%28MCSA%29%20module%20that%20performs%20intra-%20and%20inter-modal%20interactions%20globally%20and%0Alocally.%20Additionally%2C%20a%20Target-Aware%20Modality%20Weighting%20%28TAMW%29%20module%20is%0Adeveloped%20to%20highlight%20tumor-related%20features%20within%20multi-modal%20features%2C%0Athereby%20refining%20tumor%20segmentation.%20By%20embedding%20these%20modules%20across%0Adifferent%20layers%2C%20H2ASeg%20can%20hierarchically%20model%20cross-modal%20correlations%2C%0Aenabling%20a%20nuanced%20understanding%20of%20both%20semantic%20and%20structural%20tumor%0Afeatures.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20H2ASeg%2C%0Aoutperforming%20state-of-the-art%20methods%20on%20AutoPet-II%20and%20Hecktor2022%0Abenchmarks.%20The%20code%20is%20released%20at%20https%3A//github.com/JinPLu/H2ASeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18339v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H2ASeg%3A%20Hierarchical%20Adaptive%20Interaction%20and%20Weighting%20Network%20for%0A%20%20Tumor%20Segmentation%20in%20PET/CT%20Images&entry.906535625=Jinpeng%20Lu%20and%20Jingyun%20Chen%20and%20Linghan%20Cai%20and%20Songhan%20Jiang%20and%20Yongbing%20Zhang&entry.1292438233=%20%20Positron%20emission%20tomography%20%28PET%29%20combined%20with%20computed%20tomography%20%28CT%29%0Aimaging%20is%20routinely%20used%20in%20cancer%20diagnosis%20and%20prognosis%20by%20providing%0Acomplementary%20information.%20Automatically%20segmenting%20tumors%20in%20PET/CT%20images%20can%0Asignificantly%20improve%20examination%20efficiency.%20Traditional%20multi-modal%0Asegmentation%20solutions%20mainly%20rely%20on%20concatenation%20operations%20for%20modality%0Afusion%2C%20which%20fail%20to%20effectively%20model%20the%20non-linear%20dependencies%20between%20PET%0Aand%20CT%20modalities.%20Recent%20studies%20have%20investigated%20various%20approaches%20to%0Aoptimize%20the%20fusion%20of%20modality-specific%20features%20for%20enhancing%20joint%0Arepresentations.%20However%2C%20modality-specific%20encoders%20used%20in%20these%20methods%0Aoperate%20independently%2C%20inadequately%20leveraging%20the%20synergistic%20relationships%0Ainherent%20in%20PET%20and%20CT%20modalities%2C%20for%20example%2C%20the%20complementarity%20between%0Asemantics%20and%20structure.%20To%20address%20these%20issues%2C%20we%20propose%20a%20Hierarchical%0AAdaptive%20Interaction%20and%20Weighting%20Network%20termed%20H2ASeg%20to%20explore%20the%0Aintrinsic%20cross-modal%20correlations%20and%20transfer%20potential%20complementary%0Ainformation.%20Specifically%2C%20we%20design%20a%20Modality-Cooperative%20Spatial%20Attention%0A%28MCSA%29%20module%20that%20performs%20intra-%20and%20inter-modal%20interactions%20globally%20and%0Alocally.%20Additionally%2C%20a%20Target-Aware%20Modality%20Weighting%20%28TAMW%29%20module%20is%0Adeveloped%20to%20highlight%20tumor-related%20features%20within%20multi-modal%20features%2C%0Athereby%20refining%20tumor%20segmentation.%20By%20embedding%20these%20modules%20across%0Adifferent%20layers%2C%20H2ASeg%20can%20hierarchically%20model%20cross-modal%20correlations%2C%0Aenabling%20a%20nuanced%20understanding%20of%20both%20semantic%20and%20structural%20tumor%0Afeatures.%20Extensive%20experiments%20demonstrate%20the%20superiority%20of%20H2ASeg%2C%0Aoutperforming%20state-of-the-art%20methods%20on%20AutoPet-II%20and%20Hecktor2022%0Abenchmarks.%20The%20code%20is%20released%20at%20https%3A//github.com/JinPLu/H2ASeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18339v2&entry.124074799=Read"},
{"title": "Evaluating gesture generation in a large-scale open challenge: The GENEA\n  Challenge 2022", "author": "Taras Kucherenko and Pieter Wolfert and Youngwoo Yoon and Carla Viegas and Teodor Nikolov and Mihail Tsakov and Gustav Eje Henter", "abstract": "  This paper reports on the second GENEA Challenge to benchmark data-driven\nautomatic co-speech gesture generation. Participating teams used the same\nspeech and motion dataset to build gesture-generation systems. Motion generated\nby all these systems was rendered to video using a standardised visualisation\npipeline and evaluated in several large, crowdsourced user studies. Unlike when\ncomparing different research papers, differences in results are here only due\nto differences between methods, enabling direct comparison between systems. The\ndataset was based on 18 hours of full-body motion capture, including fingers,\nof different persons engaging in a dyadic conversation. Ten teams participated\nin the challenge across two tiers: full-body and upper-body gesticulation. For\neach tier, we evaluated both the human-likeness of the gesture motion and its\nappropriateness for the specific speech signal. Our evaluations decouple\nhuman-likeness from gesture appropriateness, which has been a difficult problem\nin the field.\n  The evaluation results show some synthetic gesture conditions being rated as\nsignificantly more human-like than 3D human motion capture. To the best of our\nknowledge, this has not been demonstrated before. On the other hand, all\nsynthetic motion is found to be vastly less appropriate for the speech than the\noriginal motion-capture recordings. We also find that conventional objective\nmetrics do not correlate well with subjective human-likeness ratings in this\nlarge evaluation. The one exception is the Fr\\'echet gesture distance (FGD),\nwhich achieves a Kendall's tau rank correlation of around $-0.5$. Based on the\nchallenge results we formulate numerous recommendations for system building and\nevaluation.\n", "link": "http://arxiv.org/abs/2303.08737v2", "date": "2024-03-28", "relevancy": 2.1737, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5579}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5491}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.493}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluating%20gesture%20generation%20in%20a%20large-scale%20open%20challenge%3A%20The%20GENEA%0A%20%20Challenge%202022&body=Title%3A%20Evaluating%20gesture%20generation%20in%20a%20large-scale%20open%20challenge%3A%20The%20GENEA%0A%20%20Challenge%202022%0AAuthor%3A%20Taras%20Kucherenko%20and%20Pieter%20Wolfert%20and%20Youngwoo%20Yoon%20and%20Carla%20Viegas%20and%20Teodor%20Nikolov%20and%20Mihail%20Tsakov%20and%20Gustav%20Eje%20Henter%0AAbstract%3A%20%20%20This%20paper%20reports%20on%20the%20second%20GENEA%20Challenge%20to%20benchmark%20data-driven%0Aautomatic%20co-speech%20gesture%20generation.%20Participating%20teams%20used%20the%20same%0Aspeech%20and%20motion%20dataset%20to%20build%20gesture-generation%20systems.%20Motion%20generated%0Aby%20all%20these%20systems%20was%20rendered%20to%20video%20using%20a%20standardised%20visualisation%0Apipeline%20and%20evaluated%20in%20several%20large%2C%20crowdsourced%20user%20studies.%20Unlike%20when%0Acomparing%20different%20research%20papers%2C%20differences%20in%20results%20are%20here%20only%20due%0Ato%20differences%20between%20methods%2C%20enabling%20direct%20comparison%20between%20systems.%20The%0Adataset%20was%20based%20on%2018%20hours%20of%20full-body%20motion%20capture%2C%20including%20fingers%2C%0Aof%20different%20persons%20engaging%20in%20a%20dyadic%20conversation.%20Ten%20teams%20participated%0Ain%20the%20challenge%20across%20two%20tiers%3A%20full-body%20and%20upper-body%20gesticulation.%20For%0Aeach%20tier%2C%20we%20evaluated%20both%20the%20human-likeness%20of%20the%20gesture%20motion%20and%20its%0Aappropriateness%20for%20the%20specific%20speech%20signal.%20Our%20evaluations%20decouple%0Ahuman-likeness%20from%20gesture%20appropriateness%2C%20which%20has%20been%20a%20difficult%20problem%0Ain%20the%20field.%0A%20%20The%20evaluation%20results%20show%20some%20synthetic%20gesture%20conditions%20being%20rated%20as%0Asignificantly%20more%20human-like%20than%203D%20human%20motion%20capture.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20has%20not%20been%20demonstrated%20before.%20On%20the%20other%20hand%2C%20all%0Asynthetic%20motion%20is%20found%20to%20be%20vastly%20less%20appropriate%20for%20the%20speech%20than%20the%0Aoriginal%20motion-capture%20recordings.%20We%20also%20find%20that%20conventional%20objective%0Ametrics%20do%20not%20correlate%20well%20with%20subjective%20human-likeness%20ratings%20in%20this%0Alarge%20evaluation.%20The%20one%20exception%20is%20the%20Fr%5C%27echet%20gesture%20distance%20%28FGD%29%2C%0Awhich%20achieves%20a%20Kendall%27s%20tau%20rank%20correlation%20of%20around%20%24-0.5%24.%20Based%20on%20the%0Achallenge%20results%20we%20formulate%20numerous%20recommendations%20for%20system%20building%20and%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.08737v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20gesture%20generation%20in%20a%20large-scale%20open%20challenge%3A%20The%20GENEA%0A%20%20Challenge%202022&entry.906535625=Taras%20Kucherenko%20and%20Pieter%20Wolfert%20and%20Youngwoo%20Yoon%20and%20Carla%20Viegas%20and%20Teodor%20Nikolov%20and%20Mihail%20Tsakov%20and%20Gustav%20Eje%20Henter&entry.1292438233=%20%20This%20paper%20reports%20on%20the%20second%20GENEA%20Challenge%20to%20benchmark%20data-driven%0Aautomatic%20co-speech%20gesture%20generation.%20Participating%20teams%20used%20the%20same%0Aspeech%20and%20motion%20dataset%20to%20build%20gesture-generation%20systems.%20Motion%20generated%0Aby%20all%20these%20systems%20was%20rendered%20to%20video%20using%20a%20standardised%20visualisation%0Apipeline%20and%20evaluated%20in%20several%20large%2C%20crowdsourced%20user%20studies.%20Unlike%20when%0Acomparing%20different%20research%20papers%2C%20differences%20in%20results%20are%20here%20only%20due%0Ato%20differences%20between%20methods%2C%20enabling%20direct%20comparison%20between%20systems.%20The%0Adataset%20was%20based%20on%2018%20hours%20of%20full-body%20motion%20capture%2C%20including%20fingers%2C%0Aof%20different%20persons%20engaging%20in%20a%20dyadic%20conversation.%20Ten%20teams%20participated%0Ain%20the%20challenge%20across%20two%20tiers%3A%20full-body%20and%20upper-body%20gesticulation.%20For%0Aeach%20tier%2C%20we%20evaluated%20both%20the%20human-likeness%20of%20the%20gesture%20motion%20and%20its%0Aappropriateness%20for%20the%20specific%20speech%20signal.%20Our%20evaluations%20decouple%0Ahuman-likeness%20from%20gesture%20appropriateness%2C%20which%20has%20been%20a%20difficult%20problem%0Ain%20the%20field.%0A%20%20The%20evaluation%20results%20show%20some%20synthetic%20gesture%20conditions%20being%20rated%20as%0Asignificantly%20more%20human-like%20than%203D%20human%20motion%20capture.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20has%20not%20been%20demonstrated%20before.%20On%20the%20other%20hand%2C%20all%0Asynthetic%20motion%20is%20found%20to%20be%20vastly%20less%20appropriate%20for%20the%20speech%20than%20the%0Aoriginal%20motion-capture%20recordings.%20We%20also%20find%20that%20conventional%20objective%0Ametrics%20do%20not%20correlate%20well%20with%20subjective%20human-likeness%20ratings%20in%20this%0Alarge%20evaluation.%20The%20one%20exception%20is%20the%20Fr%5C%27echet%20gesture%20distance%20%28FGD%29%2C%0Awhich%20achieves%20a%20Kendall%27s%20tau%20rank%20correlation%20of%20around%20%24-0.5%24.%20Based%20on%20the%0Achallenge%20results%20we%20formulate%20numerous%20recommendations%20for%20system%20building%20and%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.08737v2&entry.124074799=Read"},
{"title": "RFAConv: Innovating Spatial Attention and Standard Convolutional\n  Operation", "author": "Xin Zhang and Chen Liu and Degang Yang and Tingting Song and Yichen Ye and Ke Li and Yingze Song", "abstract": "  Spatial attention has been widely used to improve the performance of\nconvolutional neural networks. However, it has certain limitations. In this\npaper, we propose a new perspective on the effectiveness of spatial attention,\nwhich is that the spatial attention mechanism essentially solves the problem of\nconvolutional kernel parameter sharing. However, the information contained in\nthe attention map generated by spatial attention is not sufficient for\nlarge-size convolutional kernels. Therefore, we propose a novel attention\nmechanism called Receptive-Field Attention (RFA). Existing spatial attention,\nsuch as Convolutional Block Attention Module (CBAM) and Coordinated Attention\n(CA) focus only on spatial features, which does not fully address the problem\nof convolutional kernel parameter sharing. In contrast, RFA not only focuses on\nthe receptive-field spatial feature but also provides effective attention\nweights for large-size convolutional kernels. The Receptive-Field Attention\nconvolutional operation (RFAConv), developed by RFA, represents a new approach\nto replace the standard convolution operation. It offers nearly negligible\nincrement of computational cost and parameters, while significantly improving\nnetwork performance. We conducted a series of experiments on ImageNet-1k, COCO,\nand VOC datasets to demonstrate the superiority of our approach. Of particular\nimportance, we believe that it is time to shift focus from spatial features to\nreceptive-field spatial features for current spatial attention mechanisms. In\nthis way, we can further improve network performance and achieve even better\nresults. The code and pre-trained models for the relevant tasks can be found at\nhttps://github.com/Liuchen1997/RFAConv.\n", "link": "http://arxiv.org/abs/2304.03198v6", "date": "2024-03-28", "relevancy": 2.1533, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.563}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5274}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5181}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RFAConv%3A%20Innovating%20Spatial%20Attention%20and%20Standard%20Convolutional%0A%20%20Operation&body=Title%3A%20RFAConv%3A%20Innovating%20Spatial%20Attention%20and%20Standard%20Convolutional%0A%20%20Operation%0AAuthor%3A%20Xin%20Zhang%20and%20Chen%20Liu%20and%20Degang%20Yang%20and%20Tingting%20Song%20and%20Yichen%20Ye%20and%20Ke%20Li%20and%20Yingze%20Song%0AAbstract%3A%20%20%20Spatial%20attention%20has%20been%20widely%20used%20to%20improve%20the%20performance%20of%0Aconvolutional%20neural%20networks.%20However%2C%20it%20has%20certain%20limitations.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20perspective%20on%20the%20effectiveness%20of%20spatial%20attention%2C%0Awhich%20is%20that%20the%20spatial%20attention%20mechanism%20essentially%20solves%20the%20problem%20of%0Aconvolutional%20kernel%20parameter%20sharing.%20However%2C%20the%20information%20contained%20in%0Athe%20attention%20map%20generated%20by%20spatial%20attention%20is%20not%20sufficient%20for%0Alarge-size%20convolutional%20kernels.%20Therefore%2C%20we%20propose%20a%20novel%20attention%0Amechanism%20called%20Receptive-Field%20Attention%20%28RFA%29.%20Existing%20spatial%20attention%2C%0Asuch%20as%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20and%20Coordinated%20Attention%0A%28CA%29%20focus%20only%20on%20spatial%20features%2C%20which%20does%20not%20fully%20address%20the%20problem%0Aof%20convolutional%20kernel%20parameter%20sharing.%20In%20contrast%2C%20RFA%20not%20only%20focuses%20on%0Athe%20receptive-field%20spatial%20feature%20but%20also%20provides%20effective%20attention%0Aweights%20for%20large-size%20convolutional%20kernels.%20The%20Receptive-Field%20Attention%0Aconvolutional%20operation%20%28RFAConv%29%2C%20developed%20by%20RFA%2C%20represents%20a%20new%20approach%0Ato%20replace%20the%20standard%20convolution%20operation.%20It%20offers%20nearly%20negligible%0Aincrement%20of%20computational%20cost%20and%20parameters%2C%20while%20significantly%20improving%0Anetwork%20performance.%20We%20conducted%20a%20series%20of%20experiments%20on%20ImageNet-1k%2C%20COCO%2C%0Aand%20VOC%20datasets%20to%20demonstrate%20the%20superiority%20of%20our%20approach.%20Of%20particular%0Aimportance%2C%20we%20believe%20that%20it%20is%20time%20to%20shift%20focus%20from%20spatial%20features%20to%0Areceptive-field%20spatial%20features%20for%20current%20spatial%20attention%20mechanisms.%20In%0Athis%20way%2C%20we%20can%20further%20improve%20network%20performance%20and%20achieve%20even%20better%0Aresults.%20The%20code%20and%20pre-trained%20models%20for%20the%20relevant%20tasks%20can%20be%20found%20at%0Ahttps%3A//github.com/Liuchen1997/RFAConv.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03198v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RFAConv%3A%20Innovating%20Spatial%20Attention%20and%20Standard%20Convolutional%0A%20%20Operation&entry.906535625=Xin%20Zhang%20and%20Chen%20Liu%20and%20Degang%20Yang%20and%20Tingting%20Song%20and%20Yichen%20Ye%20and%20Ke%20Li%20and%20Yingze%20Song&entry.1292438233=%20%20Spatial%20attention%20has%20been%20widely%20used%20to%20improve%20the%20performance%20of%0Aconvolutional%20neural%20networks.%20However%2C%20it%20has%20certain%20limitations.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20perspective%20on%20the%20effectiveness%20of%20spatial%20attention%2C%0Awhich%20is%20that%20the%20spatial%20attention%20mechanism%20essentially%20solves%20the%20problem%20of%0Aconvolutional%20kernel%20parameter%20sharing.%20However%2C%20the%20information%20contained%20in%0Athe%20attention%20map%20generated%20by%20spatial%20attention%20is%20not%20sufficient%20for%0Alarge-size%20convolutional%20kernels.%20Therefore%2C%20we%20propose%20a%20novel%20attention%0Amechanism%20called%20Receptive-Field%20Attention%20%28RFA%29.%20Existing%20spatial%20attention%2C%0Asuch%20as%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%20and%20Coordinated%20Attention%0A%28CA%29%20focus%20only%20on%20spatial%20features%2C%20which%20does%20not%20fully%20address%20the%20problem%0Aof%20convolutional%20kernel%20parameter%20sharing.%20In%20contrast%2C%20RFA%20not%20only%20focuses%20on%0Athe%20receptive-field%20spatial%20feature%20but%20also%20provides%20effective%20attention%0Aweights%20for%20large-size%20convolutional%20kernels.%20The%20Receptive-Field%20Attention%0Aconvolutional%20operation%20%28RFAConv%29%2C%20developed%20by%20RFA%2C%20represents%20a%20new%20approach%0Ato%20replace%20the%20standard%20convolution%20operation.%20It%20offers%20nearly%20negligible%0Aincrement%20of%20computational%20cost%20and%20parameters%2C%20while%20significantly%20improving%0Anetwork%20performance.%20We%20conducted%20a%20series%20of%20experiments%20on%20ImageNet-1k%2C%20COCO%2C%0Aand%20VOC%20datasets%20to%20demonstrate%20the%20superiority%20of%20our%20approach.%20Of%20particular%0Aimportance%2C%20we%20believe%20that%20it%20is%20time%20to%20shift%20focus%20from%20spatial%20features%20to%0Areceptive-field%20spatial%20features%20for%20current%20spatial%20attention%20mechanisms.%20In%0Athis%20way%2C%20we%20can%20further%20improve%20network%20performance%20and%20achieve%20even%20better%0Aresults.%20The%20code%20and%20pre-trained%20models%20for%20the%20relevant%20tasks%20can%20be%20found%20at%0Ahttps%3A//github.com/Liuchen1997/RFAConv.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03198v6&entry.124074799=Read"},
{"title": "Beyond Talking -- Generating Holistic 3D Human Dyadic Motion for\n  Communication", "author": "Mingze Sun and Chao Xu and Xinyu Jiang and Yang Liu and Baigui Sun and Ruqi Huang", "abstract": "  In this paper, we introduce an innovative task focused on human\ncommunication, aiming to generate 3D holistic human motions for both speakers\nand listeners. Central to our approach is the incorporation of factorization to\ndecouple audio features and the combination of textual semantic information,\nthereby facilitating the creation of more realistic and coordinated movements.\nWe separately train VQ-VAEs with respect to the holistic motions of both\nspeaker and listener. We consider the real-time mutual influence between the\nspeaker and the listener and propose a novel chain-like transformer-based\nauto-regressive model specifically designed to characterize real-world\ncommunication scenarios effectively which can generate the motions of both the\nspeaker and the listener simultaneously. These designs ensure that the results\nwe generate are both coordinated and diverse. Our approach demonstrates\nstate-of-the-art performance on two benchmark datasets. Furthermore, we\nintroduce the HoCo holistic communication dataset, which is a valuable resource\nfor future research. Our HoCo dataset and code will be released for research\npurposes upon acceptance.\n", "link": "http://arxiv.org/abs/2403.19467v1", "date": "2024-03-28", "relevancy": 2.1405, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5415}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5321}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Beyond%20Talking%20--%20Generating%20Holistic%203D%20Human%20Dyadic%20Motion%20for%0A%20%20Communication&body=Title%3A%20Beyond%20Talking%20--%20Generating%20Holistic%203D%20Human%20Dyadic%20Motion%20for%0A%20%20Communication%0AAuthor%3A%20Mingze%20Sun%20and%20Chao%20Xu%20and%20Xinyu%20Jiang%20and%20Yang%20Liu%20and%20Baigui%20Sun%20and%20Ruqi%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20innovative%20task%20focused%20on%20human%0Acommunication%2C%20aiming%20to%20generate%203D%20holistic%20human%20motions%20for%20both%20speakers%0Aand%20listeners.%20Central%20to%20our%20approach%20is%20the%20incorporation%20of%20factorization%20to%0Adecouple%20audio%20features%20and%20the%20combination%20of%20textual%20semantic%20information%2C%0Athereby%20facilitating%20the%20creation%20of%20more%20realistic%20and%20coordinated%20movements.%0AWe%20separately%20train%20VQ-VAEs%20with%20respect%20to%20the%20holistic%20motions%20of%20both%0Aspeaker%20and%20listener.%20We%20consider%20the%20real-time%20mutual%20influence%20between%20the%0Aspeaker%20and%20the%20listener%20and%20propose%20a%20novel%20chain-like%20transformer-based%0Aauto-regressive%20model%20specifically%20designed%20to%20characterize%20real-world%0Acommunication%20scenarios%20effectively%20which%20can%20generate%20the%20motions%20of%20both%20the%0Aspeaker%20and%20the%20listener%20simultaneously.%20These%20designs%20ensure%20that%20the%20results%0Awe%20generate%20are%20both%20coordinated%20and%20diverse.%20Our%20approach%20demonstrates%0Astate-of-the-art%20performance%20on%20two%20benchmark%20datasets.%20Furthermore%2C%20we%0Aintroduce%20the%20HoCo%20holistic%20communication%20dataset%2C%20which%20is%20a%20valuable%20resource%0Afor%20future%20research.%20Our%20HoCo%20dataset%20and%20code%20will%20be%20released%20for%20research%0Apurposes%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19467v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Talking%20--%20Generating%20Holistic%203D%20Human%20Dyadic%20Motion%20for%0A%20%20Communication&entry.906535625=Mingze%20Sun%20and%20Chao%20Xu%20and%20Xinyu%20Jiang%20and%20Yang%20Liu%20and%20Baigui%20Sun%20and%20Ruqi%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20innovative%20task%20focused%20on%20human%0Acommunication%2C%20aiming%20to%20generate%203D%20holistic%20human%20motions%20for%20both%20speakers%0Aand%20listeners.%20Central%20to%20our%20approach%20is%20the%20incorporation%20of%20factorization%20to%0Adecouple%20audio%20features%20and%20the%20combination%20of%20textual%20semantic%20information%2C%0Athereby%20facilitating%20the%20creation%20of%20more%20realistic%20and%20coordinated%20movements.%0AWe%20separately%20train%20VQ-VAEs%20with%20respect%20to%20the%20holistic%20motions%20of%20both%0Aspeaker%20and%20listener.%20We%20consider%20the%20real-time%20mutual%20influence%20between%20the%0Aspeaker%20and%20the%20listener%20and%20propose%20a%20novel%20chain-like%20transformer-based%0Aauto-regressive%20model%20specifically%20designed%20to%20characterize%20real-world%0Acommunication%20scenarios%20effectively%20which%20can%20generate%20the%20motions%20of%20both%20the%0Aspeaker%20and%20the%20listener%20simultaneously.%20These%20designs%20ensure%20that%20the%20results%0Awe%20generate%20are%20both%20coordinated%20and%20diverse.%20Our%20approach%20demonstrates%0Astate-of-the-art%20performance%20on%20two%20benchmark%20datasets.%20Furthermore%2C%20we%0Aintroduce%20the%20HoCo%20holistic%20communication%20dataset%2C%20which%20is%20a%20valuable%20resource%0Afor%20future%20research.%20Our%20HoCo%20dataset%20and%20code%20will%20be%20released%20for%20research%0Apurposes%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19467v1&entry.124074799=Read"},
{"title": "Human Gaussian Splatting: Real-time Rendering of Animatable Avatars", "author": "Arthur Moreau and Jifei Song and Helisa Dhamo and Richard Shaw and Yiren Zhou and Eduardo P\u00e9rez-Pellitero", "abstract": "  This work addresses the problem of real-time rendering of photorealistic\nhuman body avatars learned from multi-view videos. While the classical\napproaches to model and render virtual humans generally use a textured mesh,\nrecent research has developed neural body representations that achieve\nimpressive visual quality. However, these models are difficult to render in\nreal-time and their quality degrades when the character is animated with body\nposes different than the training observations. We propose an animatable human\nmodel based on 3D Gaussian Splatting, that has recently emerged as a very\nefficient alternative to neural radiance fields. The body is represented by a\nset of gaussian primitives in a canonical space which is deformed with a coarse\nto fine approach that combines forward skinning and local non-rigid refinement.\nWe describe how to learn our Human Gaussian Splatting (HuGS) model in an\nend-to-end fashion from multi-view observations, and evaluate it against the\nstate-of-the-art approaches for novel pose synthesis of clothed body. Our\nmethod achieves 1.5 dB PSNR improvement over the state-of-the-art on THuman4\ndataset while being able to render in real-time (80 fps for 512x512\nresolution).\n", "link": "http://arxiv.org/abs/2311.17113v2", "date": "2024-03-28", "relevancy": 2.1395, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5366}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5353}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5295}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Human%20Gaussian%20Splatting%3A%20Real-time%20Rendering%20of%20Animatable%20Avatars&body=Title%3A%20Human%20Gaussian%20Splatting%3A%20Real-time%20Rendering%20of%20Animatable%20Avatars%0AAuthor%3A%20Arthur%20Moreau%20and%20Jifei%20Song%20and%20Helisa%20Dhamo%20and%20Richard%20Shaw%20and%20Yiren%20Zhou%20and%20Eduardo%20P%C3%A9rez-Pellitero%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20problem%20of%20real-time%20rendering%20of%20photorealistic%0Ahuman%20body%20avatars%20learned%20from%20multi-view%20videos.%20While%20the%20classical%0Aapproaches%20to%20model%20and%20render%20virtual%20humans%20generally%20use%20a%20textured%20mesh%2C%0Arecent%20research%20has%20developed%20neural%20body%20representations%20that%20achieve%0Aimpressive%20visual%20quality.%20However%2C%20these%20models%20are%20difficult%20to%20render%20in%0Areal-time%20and%20their%20quality%20degrades%20when%20the%20character%20is%20animated%20with%20body%0Aposes%20different%20than%20the%20training%20observations.%20We%20propose%20an%20animatable%20human%0Amodel%20based%20on%203D%20Gaussian%20Splatting%2C%20that%20has%20recently%20emerged%20as%20a%20very%0Aefficient%20alternative%20to%20neural%20radiance%20fields.%20The%20body%20is%20represented%20by%20a%0Aset%20of%20gaussian%20primitives%20in%20a%20canonical%20space%20which%20is%20deformed%20with%20a%20coarse%0Ato%20fine%20approach%20that%20combines%20forward%20skinning%20and%20local%20non-rigid%20refinement.%0AWe%20describe%20how%20to%20learn%20our%20Human%20Gaussian%20Splatting%20%28HuGS%29%20model%20in%20an%0Aend-to-end%20fashion%20from%20multi-view%20observations%2C%20and%20evaluate%20it%20against%20the%0Astate-of-the-art%20approaches%20for%20novel%20pose%20synthesis%20of%20clothed%20body.%20Our%0Amethod%20achieves%201.5%20dB%20PSNR%20improvement%20over%20the%20state-of-the-art%20on%20THuman4%0Adataset%20while%20being%20able%20to%20render%20in%20real-time%20%2880%20fps%20for%20512x512%0Aresolution%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17113v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Gaussian%20Splatting%3A%20Real-time%20Rendering%20of%20Animatable%20Avatars&entry.906535625=Arthur%20Moreau%20and%20Jifei%20Song%20and%20Helisa%20Dhamo%20and%20Richard%20Shaw%20and%20Yiren%20Zhou%20and%20Eduardo%20P%C3%A9rez-Pellitero&entry.1292438233=%20%20This%20work%20addresses%20the%20problem%20of%20real-time%20rendering%20of%20photorealistic%0Ahuman%20body%20avatars%20learned%20from%20multi-view%20videos.%20While%20the%20classical%0Aapproaches%20to%20model%20and%20render%20virtual%20humans%20generally%20use%20a%20textured%20mesh%2C%0Arecent%20research%20has%20developed%20neural%20body%20representations%20that%20achieve%0Aimpressive%20visual%20quality.%20However%2C%20these%20models%20are%20difficult%20to%20render%20in%0Areal-time%20and%20their%20quality%20degrades%20when%20the%20character%20is%20animated%20with%20body%0Aposes%20different%20than%20the%20training%20observations.%20We%20propose%20an%20animatable%20human%0Amodel%20based%20on%203D%20Gaussian%20Splatting%2C%20that%20has%20recently%20emerged%20as%20a%20very%0Aefficient%20alternative%20to%20neural%20radiance%20fields.%20The%20body%20is%20represented%20by%20a%0Aset%20of%20gaussian%20primitives%20in%20a%20canonical%20space%20which%20is%20deformed%20with%20a%20coarse%0Ato%20fine%20approach%20that%20combines%20forward%20skinning%20and%20local%20non-rigid%20refinement.%0AWe%20describe%20how%20to%20learn%20our%20Human%20Gaussian%20Splatting%20%28HuGS%29%20model%20in%20an%0Aend-to-end%20fashion%20from%20multi-view%20observations%2C%20and%20evaluate%20it%20against%20the%0Astate-of-the-art%20approaches%20for%20novel%20pose%20synthesis%20of%20clothed%20body.%20Our%0Amethod%20achieves%201.5%20dB%20PSNR%20improvement%20over%20the%20state-of-the-art%20on%20THuman4%0Adataset%20while%20being%20able%20to%20render%20in%20real-time%20%2880%20fps%20for%20512x512%0Aresolution%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17113v2&entry.124074799=Read"},
{"title": "Optimal Transport Perturbations for Safe Reinforcement Learning with\n  Robustness Guarantees", "author": "James Queeney and Erhan Can Ozcan and Ioannis Ch. Paschalidis and Christos G. Cassandras", "abstract": "  Robustness and safety are critical for the trustworthy deployment of deep\nreinforcement learning. Real-world decision making applications require\nalgorithms that can guarantee robust performance and safety in the presence of\ngeneral environment disturbances, while making limited assumptions on the data\ncollection process during training. In order to accomplish this goal, we\nintroduce a safe reinforcement learning framework that incorporates robustness\nthrough the use of an optimal transport cost uncertainty set. We provide an\nefficient implementation based on applying Optimal Transport Perturbations to\nconstruct worst-case virtual state transitions, which does not impact data\ncollection during training and does not require detailed simulator access. In\nexperiments on continuous control tasks with safety constraints, our approach\ndemonstrates robust performance while significantly improving safety at\ndeployment time compared to standard safe reinforcement learning.\n", "link": "http://arxiv.org/abs/2301.13375v2", "date": "2024-03-28", "relevancy": 2.1363, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5397}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5346}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5282}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Optimal%20Transport%20Perturbations%20for%20Safe%20Reinforcement%20Learning%20with%0A%20%20Robustness%20Guarantees&body=Title%3A%20Optimal%20Transport%20Perturbations%20for%20Safe%20Reinforcement%20Learning%20with%0A%20%20Robustness%20Guarantees%0AAuthor%3A%20James%20Queeney%20and%20Erhan%20Can%20Ozcan%20and%20Ioannis%20Ch.%20Paschalidis%20and%20Christos%20G.%20Cassandras%0AAbstract%3A%20%20%20Robustness%20and%20safety%20are%20critical%20for%20the%20trustworthy%20deployment%20of%20deep%0Areinforcement%20learning.%20Real-world%20decision%20making%20applications%20require%0Aalgorithms%20that%20can%20guarantee%20robust%20performance%20and%20safety%20in%20the%20presence%20of%0Ageneral%20environment%20disturbances%2C%20while%20making%20limited%20assumptions%20on%20the%20data%0Acollection%20process%20during%20training.%20In%20order%20to%20accomplish%20this%20goal%2C%20we%0Aintroduce%20a%20safe%20reinforcement%20learning%20framework%20that%20incorporates%20robustness%0Athrough%20the%20use%20of%20an%20optimal%20transport%20cost%20uncertainty%20set.%20We%20provide%20an%0Aefficient%20implementation%20based%20on%20applying%20Optimal%20Transport%20Perturbations%20to%0Aconstruct%20worst-case%20virtual%20state%20transitions%2C%20which%20does%20not%20impact%20data%0Acollection%20during%20training%20and%20does%20not%20require%20detailed%20simulator%20access.%20In%0Aexperiments%20on%20continuous%20control%20tasks%20with%20safety%20constraints%2C%20our%20approach%0Ademonstrates%20robust%20performance%20while%20significantly%20improving%20safety%20at%0Adeployment%20time%20compared%20to%20standard%20safe%20reinforcement%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13375v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Transport%20Perturbations%20for%20Safe%20Reinforcement%20Learning%20with%0A%20%20Robustness%20Guarantees&entry.906535625=James%20Queeney%20and%20Erhan%20Can%20Ozcan%20and%20Ioannis%20Ch.%20Paschalidis%20and%20Christos%20G.%20Cassandras&entry.1292438233=%20%20Robustness%20and%20safety%20are%20critical%20for%20the%20trustworthy%20deployment%20of%20deep%0Areinforcement%20learning.%20Real-world%20decision%20making%20applications%20require%0Aalgorithms%20that%20can%20guarantee%20robust%20performance%20and%20safety%20in%20the%20presence%20of%0Ageneral%20environment%20disturbances%2C%20while%20making%20limited%20assumptions%20on%20the%20data%0Acollection%20process%20during%20training.%20In%20order%20to%20accomplish%20this%20goal%2C%20we%0Aintroduce%20a%20safe%20reinforcement%20learning%20framework%20that%20incorporates%20robustness%0Athrough%20the%20use%20of%20an%20optimal%20transport%20cost%20uncertainty%20set.%20We%20provide%20an%0Aefficient%20implementation%20based%20on%20applying%20Optimal%20Transport%20Perturbations%20to%0Aconstruct%20worst-case%20virtual%20state%20transitions%2C%20which%20does%20not%20impact%20data%0Acollection%20during%20training%20and%20does%20not%20require%20detailed%20simulator%20access.%20In%0Aexperiments%20on%20continuous%20control%20tasks%20with%20safety%20constraints%2C%20our%20approach%0Ademonstrates%20robust%20performance%20while%20significantly%20improving%20safety%20at%0Adeployment%20time%20compared%20to%20standard%20safe%20reinforcement%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13375v2&entry.124074799=Read"},
{"title": "Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction", "author": "Xiaoyang Lyu and Chirui Chang and Peng Dai and Yang-tian Sun and Xiaojuang Qi", "abstract": "  Scene reconstruction from multi-view images is a fundamental problem in\ncomputer vision and graphics. Recent neural implicit surface reconstruction\nmethods have achieved high-quality results; however, editing and manipulating\nthe 3D geometry of reconstructed scenes remains challenging due to the absence\nof naturally decomposed object entities and complex object/background\ncompositions. In this paper, we present Total-Decom, a novel method for\ndecomposed 3D reconstruction with minimal human interaction. Our approach\nseamlessly integrates the Segment Anything Model (SAM) with hybrid\nimplicit-explicit neural surface representations and a mesh-based\nregion-growing technique for accurate 3D object decomposition. Total-Decom\nrequires minimal human annotations while providing users with real-time control\nover the granularity and quality of decomposition. We extensively evaluate our\nmethod on benchmark datasets and demonstrate its potential for downstream\napplications, such as animation and scene editing. The code is available at\n\\href{https://github.com/CVMI-Lab/Total-Decom.git}{https://github.com/CVMI-Lab/Total-Decom.git}.\n", "link": "http://arxiv.org/abs/2403.19314v1", "date": "2024-03-28", "relevancy": 2.1288, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5489}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5417}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5117}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Total-Decom%3A%20Decomposed%203D%20Scene%20Reconstruction%20with%20Minimal%20Interaction&body=Title%3A%20Total-Decom%3A%20Decomposed%203D%20Scene%20Reconstruction%20with%20Minimal%20Interaction%0AAuthor%3A%20Xiaoyang%20Lyu%20and%20Chirui%20Chang%20and%20Peng%20Dai%20and%20Yang-tian%20Sun%20and%20Xiaojuang%20Qi%0AAbstract%3A%20%20%20Scene%20reconstruction%20from%20multi-view%20images%20is%20a%20fundamental%20problem%20in%0Acomputer%20vision%20and%20graphics.%20Recent%20neural%20implicit%20surface%20reconstruction%0Amethods%20have%20achieved%20high-quality%20results%3B%20however%2C%20editing%20and%20manipulating%0Athe%203D%20geometry%20of%20reconstructed%20scenes%20remains%20challenging%20due%20to%20the%20absence%0Aof%20naturally%20decomposed%20object%20entities%20and%20complex%20object/background%0Acompositions.%20In%20this%20paper%2C%20we%20present%20Total-Decom%2C%20a%20novel%20method%20for%0Adecomposed%203D%20reconstruction%20with%20minimal%20human%20interaction.%20Our%20approach%0Aseamlessly%20integrates%20the%20Segment%20Anything%20Model%20%28SAM%29%20with%20hybrid%0Aimplicit-explicit%20neural%20surface%20representations%20and%20a%20mesh-based%0Aregion-growing%20technique%20for%20accurate%203D%20object%20decomposition.%20Total-Decom%0Arequires%20minimal%20human%20annotations%20while%20providing%20users%20with%20real-time%20control%0Aover%20the%20granularity%20and%20quality%20of%20decomposition.%20We%20extensively%20evaluate%20our%0Amethod%20on%20benchmark%20datasets%20and%20demonstrate%20its%20potential%20for%20downstream%0Aapplications%2C%20such%20as%20animation%20and%20scene%20editing.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CVMI-Lab/Total-Decom.git%7D%7Bhttps%3A//github.com/CVMI-Lab/Total-Decom.git%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19314v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Total-Decom%3A%20Decomposed%203D%20Scene%20Reconstruction%20with%20Minimal%20Interaction&entry.906535625=Xiaoyang%20Lyu%20and%20Chirui%20Chang%20and%20Peng%20Dai%20and%20Yang-tian%20Sun%20and%20Xiaojuang%20Qi&entry.1292438233=%20%20Scene%20reconstruction%20from%20multi-view%20images%20is%20a%20fundamental%20problem%20in%0Acomputer%20vision%20and%20graphics.%20Recent%20neural%20implicit%20surface%20reconstruction%0Amethods%20have%20achieved%20high-quality%20results%3B%20however%2C%20editing%20and%20manipulating%0Athe%203D%20geometry%20of%20reconstructed%20scenes%20remains%20challenging%20due%20to%20the%20absence%0Aof%20naturally%20decomposed%20object%20entities%20and%20complex%20object/background%0Acompositions.%20In%20this%20paper%2C%20we%20present%20Total-Decom%2C%20a%20novel%20method%20for%0Adecomposed%203D%20reconstruction%20with%20minimal%20human%20interaction.%20Our%20approach%0Aseamlessly%20integrates%20the%20Segment%20Anything%20Model%20%28SAM%29%20with%20hybrid%0Aimplicit-explicit%20neural%20surface%20representations%20and%20a%20mesh-based%0Aregion-growing%20technique%20for%20accurate%203D%20object%20decomposition.%20Total-Decom%0Arequires%20minimal%20human%20annotations%20while%20providing%20users%20with%20real-time%20control%0Aover%20the%20granularity%20and%20quality%20of%20decomposition.%20We%20extensively%20evaluate%20our%0Amethod%20on%20benchmark%20datasets%20and%20demonstrate%20its%20potential%20for%20downstream%0Aapplications%2C%20such%20as%20animation%20and%20scene%20editing.%20The%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CVMI-Lab/Total-Decom.git%7D%7Bhttps%3A//github.com/CVMI-Lab/Total-Decom.git%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19314v1&entry.124074799=Read"},
{"title": "Self-Improved Learning for Scalable Neural Combinatorial Optimization", "author": "Fu Luo and Xi Lin and Zhenkun Wang and Tong Xialiang and Mingxuan Yuan and Qingfu Zhang", "abstract": "  The end-to-end neural combinatorial optimization (NCO) method shows promising\nperformance in solving complex combinatorial optimization problems without the\nneed for expert design. However, existing methods struggle with large-scale\nproblems, hindering their practical applicability. To overcome this limitation,\nthis work proposes a novel Self-Improved Learning (SIL) method for better\nscalability of neural combinatorial optimization. Specifically, we develop an\nefficient self-improved mechanism that enables direct model training on\nlarge-scale problem instances without any labeled data. Powered by an\ninnovative local reconstruction approach, this method can iteratively generate\nbetter solutions by itself as pseudo-labels to guide efficient model training.\nIn addition, we design a linear complexity attention mechanism for the model to\nefficiently handle large-scale combinatorial problem instances with low\ncomputation overhead. Comprehensive experiments on the Travelling Salesman\nProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to\n100K nodes in both uniform and real-world distributions demonstrate the\nsuperior scalability of our method.\n", "link": "http://arxiv.org/abs/2403.19561v1", "date": "2024-03-28", "relevancy": 2.1243, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5714}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5098}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4836}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Improved%20Learning%20for%20Scalable%20Neural%20Combinatorial%20Optimization&body=Title%3A%20Self-Improved%20Learning%20for%20Scalable%20Neural%20Combinatorial%20Optimization%0AAuthor%3A%20Fu%20Luo%20and%20Xi%20Lin%20and%20Zhenkun%20Wang%20and%20Tong%20Xialiang%20and%20Mingxuan%20Yuan%20and%20Qingfu%20Zhang%0AAbstract%3A%20%20%20The%20end-to-end%20neural%20combinatorial%20optimization%20%28NCO%29%20method%20shows%20promising%0Aperformance%20in%20solving%20complex%20combinatorial%20optimization%20problems%20without%20the%0Aneed%20for%20expert%20design.%20However%2C%20existing%20methods%20struggle%20with%20large-scale%0Aproblems%2C%20hindering%20their%20practical%20applicability.%20To%20overcome%20this%20limitation%2C%0Athis%20work%20proposes%20a%20novel%20Self-Improved%20Learning%20%28SIL%29%20method%20for%20better%0Ascalability%20of%20neural%20combinatorial%20optimization.%20Specifically%2C%20we%20develop%20an%0Aefficient%20self-improved%20mechanism%20that%20enables%20direct%20model%20training%20on%0Alarge-scale%20problem%20instances%20without%20any%20labeled%20data.%20Powered%20by%20an%0Ainnovative%20local%20reconstruction%20approach%2C%20this%20method%20can%20iteratively%20generate%0Abetter%20solutions%20by%20itself%20as%20pseudo-labels%20to%20guide%20efficient%20model%20training.%0AIn%20addition%2C%20we%20design%20a%20linear%20complexity%20attention%20mechanism%20for%20the%20model%20to%0Aefficiently%20handle%20large-scale%20combinatorial%20problem%20instances%20with%20low%0Acomputation%20overhead.%20Comprehensive%20experiments%20on%20the%20Travelling%20Salesman%0AProblem%20%28TSP%29%20and%20the%20Capacitated%20Vehicle%20Routing%20Problem%20%28CVRP%29%20with%20up%20to%0A100K%20nodes%20in%20both%20uniform%20and%20real-world%20distributions%20demonstrate%20the%0Asuperior%20scalability%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19561v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Improved%20Learning%20for%20Scalable%20Neural%20Combinatorial%20Optimization&entry.906535625=Fu%20Luo%20and%20Xi%20Lin%20and%20Zhenkun%20Wang%20and%20Tong%20Xialiang%20and%20Mingxuan%20Yuan%20and%20Qingfu%20Zhang&entry.1292438233=%20%20The%20end-to-end%20neural%20combinatorial%20optimization%20%28NCO%29%20method%20shows%20promising%0Aperformance%20in%20solving%20complex%20combinatorial%20optimization%20problems%20without%20the%0Aneed%20for%20expert%20design.%20However%2C%20existing%20methods%20struggle%20with%20large-scale%0Aproblems%2C%20hindering%20their%20practical%20applicability.%20To%20overcome%20this%20limitation%2C%0Athis%20work%20proposes%20a%20novel%20Self-Improved%20Learning%20%28SIL%29%20method%20for%20better%0Ascalability%20of%20neural%20combinatorial%20optimization.%20Specifically%2C%20we%20develop%20an%0Aefficient%20self-improved%20mechanism%20that%20enables%20direct%20model%20training%20on%0Alarge-scale%20problem%20instances%20without%20any%20labeled%20data.%20Powered%20by%20an%0Ainnovative%20local%20reconstruction%20approach%2C%20this%20method%20can%20iteratively%20generate%0Abetter%20solutions%20by%20itself%20as%20pseudo-labels%20to%20guide%20efficient%20model%20training.%0AIn%20addition%2C%20we%20design%20a%20linear%20complexity%20attention%20mechanism%20for%20the%20model%20to%0Aefficiently%20handle%20large-scale%20combinatorial%20problem%20instances%20with%20low%0Acomputation%20overhead.%20Comprehensive%20experiments%20on%20the%20Travelling%20Salesman%0AProblem%20%28TSP%29%20and%20the%20Capacitated%20Vehicle%20Routing%20Problem%20%28CVRP%29%20with%20up%20to%0A100K%20nodes%20in%20both%20uniform%20and%20real-world%20distributions%20demonstrate%20the%0Asuperior%20scalability%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19561v1&entry.124074799=Read"},
{"title": "Towards Temporally Consistent Referring Video Object Segmentation", "author": "Bo Miao and Mohammed Bennamoun and Yongsheng Gao and Mubarak Shah and Ajmal Mian", "abstract": "  Referring Video Object Segmentation (R-VOS) methods face challenges in\nmaintaining consistent object segmentation due to temporal context variability\nand the presence of other visually similar objects. We propose an end-to-end\nR-VOS paradigm that explicitly models temporal instance consistency alongside\nthe referring segmentation. Specifically, we introduce a novel hybrid memory\nthat facilitates inter-frame collaboration for robust spatio-temporal matching\nand propagation. Features of frames with automatically generated high-quality\nreference masks are propagated to segment the remaining frames based on\nmulti-granularity association to achieve temporally consistent R-VOS.\nFurthermore, we propose a new Mask Consistency Score (MCS) metric to evaluate\nthe temporal consistency of video segmentation. Extensive experiments\ndemonstrate that our approach enhances temporal consistency by a significant\nmargin, leading to top-ranked performance on popular R-VOS benchmarks, i.e.,\nRef-YouTube-VOS (67.1%) and Ref-DAVIS17 (65.6%).\n", "link": "http://arxiv.org/abs/2403.19407v1", "date": "2024-03-28", "relevancy": 2.1198, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5416}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5408}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Temporally%20Consistent%20Referring%20Video%20Object%20Segmentation&body=Title%3A%20Towards%20Temporally%20Consistent%20Referring%20Video%20Object%20Segmentation%0AAuthor%3A%20Bo%20Miao%20and%20Mohammed%20Bennamoun%20and%20Yongsheng%20Gao%20and%20Mubarak%20Shah%20and%20Ajmal%20Mian%0AAbstract%3A%20%20%20Referring%20Video%20Object%20Segmentation%20%28R-VOS%29%20methods%20face%20challenges%20in%0Amaintaining%20consistent%20object%20segmentation%20due%20to%20temporal%20context%20variability%0Aand%20the%20presence%20of%20other%20visually%20similar%20objects.%20We%20propose%20an%20end-to-end%0AR-VOS%20paradigm%20that%20explicitly%20models%20temporal%20instance%20consistency%20alongside%0Athe%20referring%20segmentation.%20Specifically%2C%20we%20introduce%20a%20novel%20hybrid%20memory%0Athat%20facilitates%20inter-frame%20collaboration%20for%20robust%20spatio-temporal%20matching%0Aand%20propagation.%20Features%20of%20frames%20with%20automatically%20generated%20high-quality%0Areference%20masks%20are%20propagated%20to%20segment%20the%20remaining%20frames%20based%20on%0Amulti-granularity%20association%20to%20achieve%20temporally%20consistent%20R-VOS.%0AFurthermore%2C%20we%20propose%20a%20new%20Mask%20Consistency%20Score%20%28MCS%29%20metric%20to%20evaluate%0Athe%20temporal%20consistency%20of%20video%20segmentation.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20enhances%20temporal%20consistency%20by%20a%20significant%0Amargin%2C%20leading%20to%20top-ranked%20performance%20on%20popular%20R-VOS%20benchmarks%2C%20i.e.%2C%0ARef-YouTube-VOS%20%2867.1%25%29%20and%20Ref-DAVIS17%20%2865.6%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19407v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Temporally%20Consistent%20Referring%20Video%20Object%20Segmentation&entry.906535625=Bo%20Miao%20and%20Mohammed%20Bennamoun%20and%20Yongsheng%20Gao%20and%20Mubarak%20Shah%20and%20Ajmal%20Mian&entry.1292438233=%20%20Referring%20Video%20Object%20Segmentation%20%28R-VOS%29%20methods%20face%20challenges%20in%0Amaintaining%20consistent%20object%20segmentation%20due%20to%20temporal%20context%20variability%0Aand%20the%20presence%20of%20other%20visually%20similar%20objects.%20We%20propose%20an%20end-to-end%0AR-VOS%20paradigm%20that%20explicitly%20models%20temporal%20instance%20consistency%20alongside%0Athe%20referring%20segmentation.%20Specifically%2C%20we%20introduce%20a%20novel%20hybrid%20memory%0Athat%20facilitates%20inter-frame%20collaboration%20for%20robust%20spatio-temporal%20matching%0Aand%20propagation.%20Features%20of%20frames%20with%20automatically%20generated%20high-quality%0Areference%20masks%20are%20propagated%20to%20segment%20the%20remaining%20frames%20based%20on%0Amulti-granularity%20association%20to%20achieve%20temporally%20consistent%20R-VOS.%0AFurthermore%2C%20we%20propose%20a%20new%20Mask%20Consistency%20Score%20%28MCS%29%20metric%20to%20evaluate%0Athe%20temporal%20consistency%20of%20video%20segmentation.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20enhances%20temporal%20consistency%20by%20a%20significant%0Amargin%2C%20leading%20to%20top-ranked%20performance%20on%20popular%20R-VOS%20benchmarks%2C%20i.e.%2C%0ARef-YouTube-VOS%20%2867.1%25%29%20and%20Ref-DAVIS17%20%2865.6%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19407v1&entry.124074799=Read"},
{"title": "Hypergraph-based Multi-View Action Recognition using Event Cameras", "author": "Yue Gao and Jiaxuan Lu and Siqi Li and Yipeng Li and Shaoyi Du", "abstract": "  Action recognition from video data forms a cornerstone with wide-ranging\napplications. Single-view action recognition faces limitations due to its\nreliance on a single viewpoint. In contrast, multi-view approaches capture\ncomplementary information from various viewpoints for improved accuracy.\nRecently, event cameras have emerged as innovative bio-inspired sensors,\nleading to advancements in event-based action recognition. However, existing\nworks predominantly focus on single-view scenarios, leaving a gap in multi-view\nevent data exploitation, particularly in challenges like information deficit\nand semantic misalignment. To bridge this gap, we introduce HyperMV, a\nmulti-view event-based action recognition framework. HyperMV converts discrete\nevent data into frame-like representations and extracts view-related features\nusing a shared convolutional network. By treating segments as vertices and\nconstructing hyperedges using rule-based and KNN-based strategies, a multi-view\nhypergraph neural network that captures relationships across viewpoint and\ntemporal features is established. The vertex attention hypergraph propagation\nis also introduced for enhanced feature fusion. To prompt research in this\narea, we present the largest multi-view event-based action dataset\n$\\text{THU}^{\\text{MV-EACT}}\\text{-50}$, comprising 50 actions from 6\nviewpoints, which surpasses existing datasets by over tenfold. Experimental\nresults show that HyperMV significantly outperforms baselines in both\ncross-subject and cross-view scenarios, and also exceeds the state-of-the-arts\nin frame-based multi-view action recognition.\n", "link": "http://arxiv.org/abs/2403.19316v1", "date": "2024-03-28", "relevancy": 2.1196, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5383}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5257}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5232}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hypergraph-based%20Multi-View%20Action%20Recognition%20using%20Event%20Cameras&body=Title%3A%20Hypergraph-based%20Multi-View%20Action%20Recognition%20using%20Event%20Cameras%0AAuthor%3A%20Yue%20Gao%20and%20Jiaxuan%20Lu%20and%20Siqi%20Li%20and%20Yipeng%20Li%20and%20Shaoyi%20Du%0AAbstract%3A%20%20%20Action%20recognition%20from%20video%20data%20forms%20a%20cornerstone%20with%20wide-ranging%0Aapplications.%20Single-view%20action%20recognition%20faces%20limitations%20due%20to%20its%0Areliance%20on%20a%20single%20viewpoint.%20In%20contrast%2C%20multi-view%20approaches%20capture%0Acomplementary%20information%20from%20various%20viewpoints%20for%20improved%20accuracy.%0ARecently%2C%20event%20cameras%20have%20emerged%20as%20innovative%20bio-inspired%20sensors%2C%0Aleading%20to%20advancements%20in%20event-based%20action%20recognition.%20However%2C%20existing%0Aworks%20predominantly%20focus%20on%20single-view%20scenarios%2C%20leaving%20a%20gap%20in%20multi-view%0Aevent%20data%20exploitation%2C%20particularly%20in%20challenges%20like%20information%20deficit%0Aand%20semantic%20misalignment.%20To%20bridge%20this%20gap%2C%20we%20introduce%20HyperMV%2C%20a%0Amulti-view%20event-based%20action%20recognition%20framework.%20HyperMV%20converts%20discrete%0Aevent%20data%20into%20frame-like%20representations%20and%20extracts%20view-related%20features%0Ausing%20a%20shared%20convolutional%20network.%20By%20treating%20segments%20as%20vertices%20and%0Aconstructing%20hyperedges%20using%20rule-based%20and%20KNN-based%20strategies%2C%20a%20multi-view%0Ahypergraph%20neural%20network%20that%20captures%20relationships%20across%20viewpoint%20and%0Atemporal%20features%20is%20established.%20The%20vertex%20attention%20hypergraph%20propagation%0Ais%20also%20introduced%20for%20enhanced%20feature%20fusion.%20To%20prompt%20research%20in%20this%0Aarea%2C%20we%20present%20the%20largest%20multi-view%20event-based%20action%20dataset%0A%24%5Ctext%7BTHU%7D%5E%7B%5Ctext%7BMV-EACT%7D%7D%5Ctext%7B-50%7D%24%2C%20comprising%2050%20actions%20from%206%0Aviewpoints%2C%20which%20surpasses%20existing%20datasets%20by%20over%20tenfold.%20Experimental%0Aresults%20show%20that%20HyperMV%20significantly%20outperforms%20baselines%20in%20both%0Across-subject%20and%20cross-view%20scenarios%2C%20and%20also%20exceeds%20the%20state-of-the-arts%0Ain%20frame-based%20multi-view%20action%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19316v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraph-based%20Multi-View%20Action%20Recognition%20using%20Event%20Cameras&entry.906535625=Yue%20Gao%20and%20Jiaxuan%20Lu%20and%20Siqi%20Li%20and%20Yipeng%20Li%20and%20Shaoyi%20Du&entry.1292438233=%20%20Action%20recognition%20from%20video%20data%20forms%20a%20cornerstone%20with%20wide-ranging%0Aapplications.%20Single-view%20action%20recognition%20faces%20limitations%20due%20to%20its%0Areliance%20on%20a%20single%20viewpoint.%20In%20contrast%2C%20multi-view%20approaches%20capture%0Acomplementary%20information%20from%20various%20viewpoints%20for%20improved%20accuracy.%0ARecently%2C%20event%20cameras%20have%20emerged%20as%20innovative%20bio-inspired%20sensors%2C%0Aleading%20to%20advancements%20in%20event-based%20action%20recognition.%20However%2C%20existing%0Aworks%20predominantly%20focus%20on%20single-view%20scenarios%2C%20leaving%20a%20gap%20in%20multi-view%0Aevent%20data%20exploitation%2C%20particularly%20in%20challenges%20like%20information%20deficit%0Aand%20semantic%20misalignment.%20To%20bridge%20this%20gap%2C%20we%20introduce%20HyperMV%2C%20a%0Amulti-view%20event-based%20action%20recognition%20framework.%20HyperMV%20converts%20discrete%0Aevent%20data%20into%20frame-like%20representations%20and%20extracts%20view-related%20features%0Ausing%20a%20shared%20convolutional%20network.%20By%20treating%20segments%20as%20vertices%20and%0Aconstructing%20hyperedges%20using%20rule-based%20and%20KNN-based%20strategies%2C%20a%20multi-view%0Ahypergraph%20neural%20network%20that%20captures%20relationships%20across%20viewpoint%20and%0Atemporal%20features%20is%20established.%20The%20vertex%20attention%20hypergraph%20propagation%0Ais%20also%20introduced%20for%20enhanced%20feature%20fusion.%20To%20prompt%20research%20in%20this%0Aarea%2C%20we%20present%20the%20largest%20multi-view%20event-based%20action%20dataset%0A%24%5Ctext%7BTHU%7D%5E%7B%5Ctext%7BMV-EACT%7D%7D%5Ctext%7B-50%7D%24%2C%20comprising%2050%20actions%20from%206%0Aviewpoints%2C%20which%20surpasses%20existing%20datasets%20by%20over%20tenfold.%20Experimental%0Aresults%20show%20that%20HyperMV%20significantly%20outperforms%20baselines%20in%20both%0Across-subject%20and%20cross-view%20scenarios%2C%20and%20also%20exceeds%20the%20state-of-the-arts%0Ain%20frame-based%20multi-view%20action%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19316v1&entry.124074799=Read"},
{"title": "Frequency-Adaptive Dilated Convolution for Semantic Segmentation", "author": "Linwei Chen and Lin Gu and Ying Fu", "abstract": "  Dilated convolution, which expands the receptive field by inserting gaps\nbetween its consecutive elements, is widely employed in computer vision. In\nthis study, we propose three strategies to improve individual phases of dilated\nconvolution from the view of spectrum analysis. Departing from the conventional\npractice of fixing a global dilation rate as a hyperparameter, we introduce\nFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts\ndilation rates spatially based on local frequency components. Subsequently, we\ndesign two plug-in modules to directly enhance effective bandwidth and\nreceptive field size. The Adaptive Kernel (AdaKern) module decomposes\nconvolution weights into low-frequency and high-frequency components,\ndynamically adjusting the ratio between these components on a per-channel\nbasis. By increasing the high-frequency part of convolution weights, AdaKern\ncaptures more high-frequency components, thereby improving effective bandwidth.\nThe Frequency Selection (FreqSelect) module optimally balances high- and\nlow-frequency components in feature representations through spatially variant\nreweighting. It suppresses high frequencies in the background to encourage FADC\nto learn a larger dilation, thereby increasing the receptive field for an\nexpanded scope. Extensive experiments on segmentation and object detection\nconsistently validate the efficacy of our approach. The code is publicly\navailable at \\url{https://github.com/Linwei-Chen/FADC}.\n", "link": "http://arxiv.org/abs/2403.05369v4", "date": "2024-03-28", "relevancy": 2.1005, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5375}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5349}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5089}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Frequency-Adaptive%20Dilated%20Convolution%20for%20Semantic%20Segmentation&body=Title%3A%20Frequency-Adaptive%20Dilated%20Convolution%20for%20Semantic%20Segmentation%0AAuthor%3A%20Linwei%20Chen%20and%20Lin%20Gu%20and%20Ying%20Fu%0AAbstract%3A%20%20%20Dilated%20convolution%2C%20which%20expands%20the%20receptive%20field%20by%20inserting%20gaps%0Abetween%20its%20consecutive%20elements%2C%20is%20widely%20employed%20in%20computer%20vision.%20In%0Athis%20study%2C%20we%20propose%20three%20strategies%20to%20improve%20individual%20phases%20of%20dilated%0Aconvolution%20from%20the%20view%20of%20spectrum%20analysis.%20Departing%20from%20the%20conventional%0Apractice%20of%20fixing%20a%20global%20dilation%20rate%20as%20a%20hyperparameter%2C%20we%20introduce%0AFrequency-Adaptive%20Dilated%20Convolution%20%28FADC%29%2C%20which%20dynamically%20adjusts%0Adilation%20rates%20spatially%20based%20on%20local%20frequency%20components.%20Subsequently%2C%20we%0Adesign%20two%20plug-in%20modules%20to%20directly%20enhance%20effective%20bandwidth%20and%0Areceptive%20field%20size.%20The%20Adaptive%20Kernel%20%28AdaKern%29%20module%20decomposes%0Aconvolution%20weights%20into%20low-frequency%20and%20high-frequency%20components%2C%0Adynamically%20adjusting%20the%20ratio%20between%20these%20components%20on%20a%20per-channel%0Abasis.%20By%20increasing%20the%20high-frequency%20part%20of%20convolution%20weights%2C%20AdaKern%0Acaptures%20more%20high-frequency%20components%2C%20thereby%20improving%20effective%20bandwidth.%0AThe%20Frequency%20Selection%20%28FreqSelect%29%20module%20optimally%20balances%20high-%20and%0Alow-frequency%20components%20in%20feature%20representations%20through%20spatially%20variant%0Areweighting.%20It%20suppresses%20high%20frequencies%20in%20the%20background%20to%20encourage%20FADC%0Ato%20learn%20a%20larger%20dilation%2C%20thereby%20increasing%20the%20receptive%20field%20for%20an%0Aexpanded%20scope.%20Extensive%20experiments%20on%20segmentation%20and%20object%20detection%0Aconsistently%20validate%20the%20efficacy%20of%20our%20approach.%20The%20code%20is%20publicly%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/Linwei-Chen/FADC%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05369v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Adaptive%20Dilated%20Convolution%20for%20Semantic%20Segmentation&entry.906535625=Linwei%20Chen%20and%20Lin%20Gu%20and%20Ying%20Fu&entry.1292438233=%20%20Dilated%20convolution%2C%20which%20expands%20the%20receptive%20field%20by%20inserting%20gaps%0Abetween%20its%20consecutive%20elements%2C%20is%20widely%20employed%20in%20computer%20vision.%20In%0Athis%20study%2C%20we%20propose%20three%20strategies%20to%20improve%20individual%20phases%20of%20dilated%0Aconvolution%20from%20the%20view%20of%20spectrum%20analysis.%20Departing%20from%20the%20conventional%0Apractice%20of%20fixing%20a%20global%20dilation%20rate%20as%20a%20hyperparameter%2C%20we%20introduce%0AFrequency-Adaptive%20Dilated%20Convolution%20%28FADC%29%2C%20which%20dynamically%20adjusts%0Adilation%20rates%20spatially%20based%20on%20local%20frequency%20components.%20Subsequently%2C%20we%0Adesign%20two%20plug-in%20modules%20to%20directly%20enhance%20effective%20bandwidth%20and%0Areceptive%20field%20size.%20The%20Adaptive%20Kernel%20%28AdaKern%29%20module%20decomposes%0Aconvolution%20weights%20into%20low-frequency%20and%20high-frequency%20components%2C%0Adynamically%20adjusting%20the%20ratio%20between%20these%20components%20on%20a%20per-channel%0Abasis.%20By%20increasing%20the%20high-frequency%20part%20of%20convolution%20weights%2C%20AdaKern%0Acaptures%20more%20high-frequency%20components%2C%20thereby%20improving%20effective%20bandwidth.%0AThe%20Frequency%20Selection%20%28FreqSelect%29%20module%20optimally%20balances%20high-%20and%0Alow-frequency%20components%20in%20feature%20representations%20through%20spatially%20variant%0Areweighting.%20It%20suppresses%20high%20frequencies%20in%20the%20background%20to%20encourage%20FADC%0Ato%20learn%20a%20larger%20dilation%2C%20thereby%20increasing%20the%20receptive%20field%20for%20an%0Aexpanded%20scope.%20Extensive%20experiments%20on%20segmentation%20and%20object%20detection%0Aconsistently%20validate%20the%20efficacy%20of%20our%20approach.%20The%20code%20is%20publicly%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/Linwei-Chen/FADC%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05369v4&entry.124074799=Read"},
{"title": "OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task\n  Completion", "author": "Xinyu Zhan and Lixin Yang and Yifei Zhao and Kangrui Mao and Hanlin Xu and Zenan Lin and Kailin Li and Cewu Lu", "abstract": "  We present OAKINK2, a dataset of bimanual object manipulation tasks for\ncomplex daily activities. In pursuit of constructing the complex tasks into a\nstructured representation, OAKINK2 introduces three level of abstraction to\norganize the manipulation tasks: Affordance, Primitive Task, and Complex Task.\nOAKINK2 features on an object-centric perspective for decoding the complex\ntasks, treating them as a sequence of object affordance fulfillment. The first\nlevel, Affordance, outlines the functionalities that objects in the scene can\nafford, the second level, Primitive Task, describes the minimal interaction\nunits that humans interact with the object to achieve its affordance, and the\nthird level, Complex Task, illustrates how Primitive Tasks are composed and\ninterdependent. OAKINK2 dataset provides multi-view image streams and precise\npose annotations for the human body, hands and various interacting objects.\nThis extensive collection supports applications such as interaction\nreconstruction and motion synthesis. Based on the 3-level abstraction of\nOAKINK2, we explore a task-oriented framework for Complex Task Completion\n(CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task\nobjectives. Within the CTC framework, we employ Large Language Models (LLMs) to\ndecompose the complex task objectives into sequences of Primitive Tasks and\nhave developed a Motion Fulfillment Model that generates bimanual hand motion\nfor each Primitive Task. OAKINK2 datasets and models are available at\nhttps://oakink.net/v2.\n", "link": "http://arxiv.org/abs/2403.19417v1", "date": "2024-03-28", "relevancy": 2.0891, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5114}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OAKINK2%3A%20A%20Dataset%20of%20Bimanual%20Hands-Object%20Manipulation%20in%20Complex%20Task%0A%20%20Completion&body=Title%3A%20OAKINK2%3A%20A%20Dataset%20of%20Bimanual%20Hands-Object%20Manipulation%20in%20Complex%20Task%0A%20%20Completion%0AAuthor%3A%20Xinyu%20Zhan%20and%20Lixin%20Yang%20and%20Yifei%20Zhao%20and%20Kangrui%20Mao%20and%20Hanlin%20Xu%20and%20Zenan%20Lin%20and%20Kailin%20Li%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20We%20present%20OAKINK2%2C%20a%20dataset%20of%20bimanual%20object%20manipulation%20tasks%20for%0Acomplex%20daily%20activities.%20In%20pursuit%20of%20constructing%20the%20complex%20tasks%20into%20a%0Astructured%20representation%2C%20OAKINK2%20introduces%20three%20level%20of%20abstraction%20to%0Aorganize%20the%20manipulation%20tasks%3A%20Affordance%2C%20Primitive%20Task%2C%20and%20Complex%20Task.%0AOAKINK2%20features%20on%20an%20object-centric%20perspective%20for%20decoding%20the%20complex%0Atasks%2C%20treating%20them%20as%20a%20sequence%20of%20object%20affordance%20fulfillment.%20The%20first%0Alevel%2C%20Affordance%2C%20outlines%20the%20functionalities%20that%20objects%20in%20the%20scene%20can%0Aafford%2C%20the%20second%20level%2C%20Primitive%20Task%2C%20describes%20the%20minimal%20interaction%0Aunits%20that%20humans%20interact%20with%20the%20object%20to%20achieve%20its%20affordance%2C%20and%20the%0Athird%20level%2C%20Complex%20Task%2C%20illustrates%20how%20Primitive%20Tasks%20are%20composed%20and%0Ainterdependent.%20OAKINK2%20dataset%20provides%20multi-view%20image%20streams%20and%20precise%0Apose%20annotations%20for%20the%20human%20body%2C%20hands%20and%20various%20interacting%20objects.%0AThis%20extensive%20collection%20supports%20applications%20such%20as%20interaction%0Areconstruction%20and%20motion%20synthesis.%20Based%20on%20the%203-level%20abstraction%20of%0AOAKINK2%2C%20we%20explore%20a%20task-oriented%20framework%20for%20Complex%20Task%20Completion%0A%28CTC%29.%20CTC%20aims%20to%20generate%20a%20sequence%20of%20bimanual%20manipulation%20to%20achieve%20task%0Aobjectives.%20Within%20the%20CTC%20framework%2C%20we%20employ%20Large%20Language%20Models%20%28LLMs%29%20to%0Adecompose%20the%20complex%20task%20objectives%20into%20sequences%20of%20Primitive%20Tasks%20and%0Ahave%20developed%20a%20Motion%20Fulfillment%20Model%20that%20generates%20bimanual%20hand%20motion%0Afor%20each%20Primitive%20Task.%20OAKINK2%20datasets%20and%20models%20are%20available%20at%0Ahttps%3A//oakink.net/v2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19417v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OAKINK2%3A%20A%20Dataset%20of%20Bimanual%20Hands-Object%20Manipulation%20in%20Complex%20Task%0A%20%20Completion&entry.906535625=Xinyu%20Zhan%20and%20Lixin%20Yang%20and%20Yifei%20Zhao%20and%20Kangrui%20Mao%20and%20Hanlin%20Xu%20and%20Zenan%20Lin%20and%20Kailin%20Li%20and%20Cewu%20Lu&entry.1292438233=%20%20We%20present%20OAKINK2%2C%20a%20dataset%20of%20bimanual%20object%20manipulation%20tasks%20for%0Acomplex%20daily%20activities.%20In%20pursuit%20of%20constructing%20the%20complex%20tasks%20into%20a%0Astructured%20representation%2C%20OAKINK2%20introduces%20three%20level%20of%20abstraction%20to%0Aorganize%20the%20manipulation%20tasks%3A%20Affordance%2C%20Primitive%20Task%2C%20and%20Complex%20Task.%0AOAKINK2%20features%20on%20an%20object-centric%20perspective%20for%20decoding%20the%20complex%0Atasks%2C%20treating%20them%20as%20a%20sequence%20of%20object%20affordance%20fulfillment.%20The%20first%0Alevel%2C%20Affordance%2C%20outlines%20the%20functionalities%20that%20objects%20in%20the%20scene%20can%0Aafford%2C%20the%20second%20level%2C%20Primitive%20Task%2C%20describes%20the%20minimal%20interaction%0Aunits%20that%20humans%20interact%20with%20the%20object%20to%20achieve%20its%20affordance%2C%20and%20the%0Athird%20level%2C%20Complex%20Task%2C%20illustrates%20how%20Primitive%20Tasks%20are%20composed%20and%0Ainterdependent.%20OAKINK2%20dataset%20provides%20multi-view%20image%20streams%20and%20precise%0Apose%20annotations%20for%20the%20human%20body%2C%20hands%20and%20various%20interacting%20objects.%0AThis%20extensive%20collection%20supports%20applications%20such%20as%20interaction%0Areconstruction%20and%20motion%20synthesis.%20Based%20on%20the%203-level%20abstraction%20of%0AOAKINK2%2C%20we%20explore%20a%20task-oriented%20framework%20for%20Complex%20Task%20Completion%0A%28CTC%29.%20CTC%20aims%20to%20generate%20a%20sequence%20of%20bimanual%20manipulation%20to%20achieve%20task%0Aobjectives.%20Within%20the%20CTC%20framework%2C%20we%20employ%20Large%20Language%20Models%20%28LLMs%29%20to%0Adecompose%20the%20complex%20task%20objectives%20into%20sequences%20of%20Primitive%20Tasks%20and%0Ahave%20developed%20a%20Motion%20Fulfillment%20Model%20that%20generates%20bimanual%20hand%20motion%0Afor%20each%20Primitive%20Task.%20OAKINK2%20datasets%20and%20models%20are%20available%20at%0Ahttps%3A//oakink.net/v2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19417v1&entry.124074799=Read"},
{"title": "Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free\n  Prediction", "author": "Drew T. Nguyen and Reese Pathak and Anastasios N. Angelopoulos and Stephen Bates and Michael I. Jordan", "abstract": "  Decision-making pipelines are generally characterized by tradeoffs among\nvarious risk functions. It is often desirable to manage such tradeoffs in a\ndata-adaptive manner. As we demonstrate, if this is done naively, state-of-the\nart uncertainty quantification methods can lead to significant violations of\nputative risk guarantees.\n  To address this issue, we develop methods that permit valid control of risk\nwhen threshold and tradeoff parameters are chosen adaptively. Our methodology\nsupports monotone and nearly-monotone risks, but otherwise makes no\ndistributional assumptions.\n  To illustrate the benefits of our approach, we carry out numerical\nexperiments on synthetic data and the large-scale vision dataset MS-COCO.\n", "link": "http://arxiv.org/abs/2403.19605v1", "date": "2024-03-28", "relevancy": 2.0882, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5527}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5164}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5155}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Data-Adaptive%20Tradeoffs%20among%20Multiple%20Risks%20in%20Distribution-Free%0A%20%20Prediction&body=Title%3A%20Data-Adaptive%20Tradeoffs%20among%20Multiple%20Risks%20in%20Distribution-Free%0A%20%20Prediction%0AAuthor%3A%20Drew%20T.%20Nguyen%20and%20Reese%20Pathak%20and%20Anastasios%20N.%20Angelopoulos%20and%20Stephen%20Bates%20and%20Michael%20I.%20Jordan%0AAbstract%3A%20%20%20Decision-making%20pipelines%20are%20generally%20characterized%20by%20tradeoffs%20among%0Avarious%20risk%20functions.%20It%20is%20often%20desirable%20to%20manage%20such%20tradeoffs%20in%20a%0Adata-adaptive%20manner.%20As%20we%20demonstrate%2C%20if%20this%20is%20done%20naively%2C%20state-of-the%0Aart%20uncertainty%20quantification%20methods%20can%20lead%20to%20significant%20violations%20of%0Aputative%20risk%20guarantees.%0A%20%20To%20address%20this%20issue%2C%20we%20develop%20methods%20that%20permit%20valid%20control%20of%20risk%0Awhen%20threshold%20and%20tradeoff%20parameters%20are%20chosen%20adaptively.%20Our%20methodology%0Asupports%20monotone%20and%20nearly-monotone%20risks%2C%20but%20otherwise%20makes%20no%0Adistributional%20assumptions.%0A%20%20To%20illustrate%20the%20benefits%20of%20our%20approach%2C%20we%20carry%20out%20numerical%0Aexperiments%20on%20synthetic%20data%20and%20the%20large-scale%20vision%20dataset%20MS-COCO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19605v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Adaptive%20Tradeoffs%20among%20Multiple%20Risks%20in%20Distribution-Free%0A%20%20Prediction&entry.906535625=Drew%20T.%20Nguyen%20and%20Reese%20Pathak%20and%20Anastasios%20N.%20Angelopoulos%20and%20Stephen%20Bates%20and%20Michael%20I.%20Jordan&entry.1292438233=%20%20Decision-making%20pipelines%20are%20generally%20characterized%20by%20tradeoffs%20among%0Avarious%20risk%20functions.%20It%20is%20often%20desirable%20to%20manage%20such%20tradeoffs%20in%20a%0Adata-adaptive%20manner.%20As%20we%20demonstrate%2C%20if%20this%20is%20done%20naively%2C%20state-of-the%0Aart%20uncertainty%20quantification%20methods%20can%20lead%20to%20significant%20violations%20of%0Aputative%20risk%20guarantees.%0A%20%20To%20address%20this%20issue%2C%20we%20develop%20methods%20that%20permit%20valid%20control%20of%20risk%0Awhen%20threshold%20and%20tradeoff%20parameters%20are%20chosen%20adaptively.%20Our%20methodology%0Asupports%20monotone%20and%20nearly-monotone%20risks%2C%20but%20otherwise%20makes%20no%0Adistributional%20assumptions.%0A%20%20To%20illustrate%20the%20benefits%20of%20our%20approach%2C%20we%20carry%20out%20numerical%0Aexperiments%20on%20synthetic%20data%20and%20the%20large-scale%20vision%20dataset%20MS-COCO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19605v1&entry.124074799=Read"},
{"title": "Infrared Small Target Detection with Scale and Location Sensitivity", "author": "Qiankun Liu and Rui Liu and Bolun Zheng and Hongkui Wang and Ying Fu", "abstract": "  Recently, infrared small target detection (IRSTD) has been dominated by\ndeep-learning-based methods. However, these methods mainly focus on the design\nof complex model structures to extract discriminative features, leaving the\nloss functions for IRSTD under-explored. For example, the widely used\nIntersection over Union (IoU) and Dice losses lack sensitivity to the scales\nand locations of targets, limiting the detection performance of detectors. In\nthis paper, we focus on boosting detection performance with a more effective\nloss but a simpler model structure. Specifically, we first propose a novel\nScale and Location Sensitive (SLS) loss to handle the limitations of existing\nlosses: 1) for scale sensitivity, we compute a weight for the IoU loss based on\ntarget scales to help the detector distinguish targets with different scales:\n2) for location sensitivity, we introduce a penalty term based on the center\npoints of targets to help the detector localize targets more precisely. Then,\nwe design a simple Multi-Scale Head to the plain U-Net (MSHNet). By applying\nSLS loss to each scale of the predictions, our MSHNet outperforms existing\nstate-of-the-art methods by a large margin. In addition, the detection\nperformance of existing detectors can be further improved when trained with our\nSLS loss, demonstrating the effectiveness and generalization of our SLS loss.\nThe code is available at https://github.com/ying-fu/MSHNet.\n", "link": "http://arxiv.org/abs/2403.19366v1", "date": "2024-03-28", "relevancy": 2.074, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5358}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5096}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4975}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Infrared%20Small%20Target%20Detection%20with%20Scale%20and%20Location%20Sensitivity&body=Title%3A%20Infrared%20Small%20Target%20Detection%20with%20Scale%20and%20Location%20Sensitivity%0AAuthor%3A%20Qiankun%20Liu%20and%20Rui%20Liu%20and%20Bolun%20Zheng%20and%20Hongkui%20Wang%20and%20Ying%20Fu%0AAbstract%3A%20%20%20Recently%2C%20infrared%20small%20target%20detection%20%28IRSTD%29%20has%20been%20dominated%20by%0Adeep-learning-based%20methods.%20However%2C%20these%20methods%20mainly%20focus%20on%20the%20design%0Aof%20complex%20model%20structures%20to%20extract%20discriminative%20features%2C%20leaving%20the%0Aloss%20functions%20for%20IRSTD%20under-explored.%20For%20example%2C%20the%20widely%20used%0AIntersection%20over%20Union%20%28IoU%29%20and%20Dice%20losses%20lack%20sensitivity%20to%20the%20scales%0Aand%20locations%20of%20targets%2C%20limiting%20the%20detection%20performance%20of%20detectors.%20In%0Athis%20paper%2C%20we%20focus%20on%20boosting%20detection%20performance%20with%20a%20more%20effective%0Aloss%20but%20a%20simpler%20model%20structure.%20Specifically%2C%20we%20first%20propose%20a%20novel%0AScale%20and%20Location%20Sensitive%20%28SLS%29%20loss%20to%20handle%20the%20limitations%20of%20existing%0Alosses%3A%201%29%20for%20scale%20sensitivity%2C%20we%20compute%20a%20weight%20for%20the%20IoU%20loss%20based%20on%0Atarget%20scales%20to%20help%20the%20detector%20distinguish%20targets%20with%20different%20scales%3A%0A2%29%20for%20location%20sensitivity%2C%20we%20introduce%20a%20penalty%20term%20based%20on%20the%20center%0Apoints%20of%20targets%20to%20help%20the%20detector%20localize%20targets%20more%20precisely.%20Then%2C%0Awe%20design%20a%20simple%20Multi-Scale%20Head%20to%20the%20plain%20U-Net%20%28MSHNet%29.%20By%20applying%0ASLS%20loss%20to%20each%20scale%20of%20the%20predictions%2C%20our%20MSHNet%20outperforms%20existing%0Astate-of-the-art%20methods%20by%20a%20large%20margin.%20In%20addition%2C%20the%20detection%0Aperformance%20of%20existing%20detectors%20can%20be%20further%20improved%20when%20trained%20with%20our%0ASLS%20loss%2C%20demonstrating%20the%20effectiveness%20and%20generalization%20of%20our%20SLS%20loss.%0AThe%20code%20is%20available%20at%20https%3A//github.com/ying-fu/MSHNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19366v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infrared%20Small%20Target%20Detection%20with%20Scale%20and%20Location%20Sensitivity&entry.906535625=Qiankun%20Liu%20and%20Rui%20Liu%20and%20Bolun%20Zheng%20and%20Hongkui%20Wang%20and%20Ying%20Fu&entry.1292438233=%20%20Recently%2C%20infrared%20small%20target%20detection%20%28IRSTD%29%20has%20been%20dominated%20by%0Adeep-learning-based%20methods.%20However%2C%20these%20methods%20mainly%20focus%20on%20the%20design%0Aof%20complex%20model%20structures%20to%20extract%20discriminative%20features%2C%20leaving%20the%0Aloss%20functions%20for%20IRSTD%20under-explored.%20For%20example%2C%20the%20widely%20used%0AIntersection%20over%20Union%20%28IoU%29%20and%20Dice%20losses%20lack%20sensitivity%20to%20the%20scales%0Aand%20locations%20of%20targets%2C%20limiting%20the%20detection%20performance%20of%20detectors.%20In%0Athis%20paper%2C%20we%20focus%20on%20boosting%20detection%20performance%20with%20a%20more%20effective%0Aloss%20but%20a%20simpler%20model%20structure.%20Specifically%2C%20we%20first%20propose%20a%20novel%0AScale%20and%20Location%20Sensitive%20%28SLS%29%20loss%20to%20handle%20the%20limitations%20of%20existing%0Alosses%3A%201%29%20for%20scale%20sensitivity%2C%20we%20compute%20a%20weight%20for%20the%20IoU%20loss%20based%20on%0Atarget%20scales%20to%20help%20the%20detector%20distinguish%20targets%20with%20different%20scales%3A%0A2%29%20for%20location%20sensitivity%2C%20we%20introduce%20a%20penalty%20term%20based%20on%20the%20center%0Apoints%20of%20targets%20to%20help%20the%20detector%20localize%20targets%20more%20precisely.%20Then%2C%0Awe%20design%20a%20simple%20Multi-Scale%20Head%20to%20the%20plain%20U-Net%20%28MSHNet%29.%20By%20applying%0ASLS%20loss%20to%20each%20scale%20of%20the%20predictions%2C%20our%20MSHNet%20outperforms%20existing%0Astate-of-the-art%20methods%20by%20a%20large%20margin.%20In%20addition%2C%20the%20detection%0Aperformance%20of%20existing%20detectors%20can%20be%20further%20improved%20when%20trained%20with%20our%0ASLS%20loss%2C%20demonstrating%20the%20effectiveness%20and%20generalization%20of%20our%20SLS%20loss.%0AThe%20code%20is%20available%20at%20https%3A//github.com/ying-fu/MSHNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19366v1&entry.124074799=Read"},
{"title": "Manifold Constraint Regularization for Remote Sensing Image Generation", "author": "Xingzhe Su and Changwen Zheng and Wenwen Qiang and Fengge Wu and Junsuo Zhao and Fuchun Sun and Hui Xiong", "abstract": "  Generative Adversarial Networks (GANs) have shown notable accomplishments in\nremote sensing domain. However, this paper reveals that their performance on\nremote sensing images falls short when compared to their impressive results\nwith natural images. This study identifies a previously overlooked issue: GANs\nexhibit a heightened susceptibility to overfitting on remote sensing images.To\naddress this challenge, this paper analyzes the characteristics of remote\nsensing images and proposes manifold constraint regularization, a novel\napproach that tackles overfitting of GANs on remote sensing images for the\nfirst time. Our method includes a new measure for evaluating the structure of\nthe data manifold. Leveraging this measure, we propose the manifold constraint\nregularization term, which not only alleviates the overfitting problem, but\nalso promotes alignment between the generated and real data manifolds, leading\nto enhanced quality in the generated images. The effectiveness and versatility\nof this method have been corroborated through extensive validation on various\nremote sensing datasets and GAN models. The proposed method not only enhances\nthe quality of the generated images, reflected in a 3.13\\% improvement in\nFrechet Inception Distance (FID) score, but also boosts the performance of the\nGANs on downstream tasks, evidenced by a 3.76\\% increase in classification\naccuracy.\n", "link": "http://arxiv.org/abs/2305.19507v3", "date": "2024-03-28", "relevancy": 2.0723, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5249}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5173}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5115}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Manifold%20Constraint%20Regularization%20for%20Remote%20Sensing%20Image%20Generation&body=Title%3A%20Manifold%20Constraint%20Regularization%20for%20Remote%20Sensing%20Image%20Generation%0AAuthor%3A%20Xingzhe%20Su%20and%20Changwen%20Zheng%20and%20Wenwen%20Qiang%20and%20Fengge%20Wu%20and%20Junsuo%20Zhao%20and%20Fuchun%20Sun%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20shown%20notable%20accomplishments%20in%0Aremote%20sensing%20domain.%20However%2C%20this%20paper%20reveals%20that%20their%20performance%20on%0Aremote%20sensing%20images%20falls%20short%20when%20compared%20to%20their%20impressive%20results%0Awith%20natural%20images.%20This%20study%20identifies%20a%20previously%20overlooked%20issue%3A%20GANs%0Aexhibit%20a%20heightened%20susceptibility%20to%20overfitting%20on%20remote%20sensing%20images.To%0Aaddress%20this%20challenge%2C%20this%20paper%20analyzes%20the%20characteristics%20of%20remote%0Asensing%20images%20and%20proposes%20manifold%20constraint%20regularization%2C%20a%20novel%0Aapproach%20that%20tackles%20overfitting%20of%20GANs%20on%20remote%20sensing%20images%20for%20the%0Afirst%20time.%20Our%20method%20includes%20a%20new%20measure%20for%20evaluating%20the%20structure%20of%0Athe%20data%20manifold.%20Leveraging%20this%20measure%2C%20we%20propose%20the%20manifold%20constraint%0Aregularization%20term%2C%20which%20not%20only%20alleviates%20the%20overfitting%20problem%2C%20but%0Aalso%20promotes%20alignment%20between%20the%20generated%20and%20real%20data%20manifolds%2C%20leading%0Ato%20enhanced%20quality%20in%20the%20generated%20images.%20The%20effectiveness%20and%20versatility%0Aof%20this%20method%20have%20been%20corroborated%20through%20extensive%20validation%20on%20various%0Aremote%20sensing%20datasets%20and%20GAN%20models.%20The%20proposed%20method%20not%20only%20enhances%0Athe%20quality%20of%20the%20generated%20images%2C%20reflected%20in%20a%203.13%5C%25%20improvement%20in%0AFrechet%20Inception%20Distance%20%28FID%29%20score%2C%20but%20also%20boosts%20the%20performance%20of%20the%0AGANs%20on%20downstream%20tasks%2C%20evidenced%20by%20a%203.76%5C%25%20increase%20in%20classification%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.19507v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold%20Constraint%20Regularization%20for%20Remote%20Sensing%20Image%20Generation&entry.906535625=Xingzhe%20Su%20and%20Changwen%20Zheng%20and%20Wenwen%20Qiang%20and%20Fengge%20Wu%20and%20Junsuo%20Zhao%20and%20Fuchun%20Sun%20and%20Hui%20Xiong&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20shown%20notable%20accomplishments%20in%0Aremote%20sensing%20domain.%20However%2C%20this%20paper%20reveals%20that%20their%20performance%20on%0Aremote%20sensing%20images%20falls%20short%20when%20compared%20to%20their%20impressive%20results%0Awith%20natural%20images.%20This%20study%20identifies%20a%20previously%20overlooked%20issue%3A%20GANs%0Aexhibit%20a%20heightened%20susceptibility%20to%20overfitting%20on%20remote%20sensing%20images.To%0Aaddress%20this%20challenge%2C%20this%20paper%20analyzes%20the%20characteristics%20of%20remote%0Asensing%20images%20and%20proposes%20manifold%20constraint%20regularization%2C%20a%20novel%0Aapproach%20that%20tackles%20overfitting%20of%20GANs%20on%20remote%20sensing%20images%20for%20the%0Afirst%20time.%20Our%20method%20includes%20a%20new%20measure%20for%20evaluating%20the%20structure%20of%0Athe%20data%20manifold.%20Leveraging%20this%20measure%2C%20we%20propose%20the%20manifold%20constraint%0Aregularization%20term%2C%20which%20not%20only%20alleviates%20the%20overfitting%20problem%2C%20but%0Aalso%20promotes%20alignment%20between%20the%20generated%20and%20real%20data%20manifolds%2C%20leading%0Ato%20enhanced%20quality%20in%20the%20generated%20images.%20The%20effectiveness%20and%20versatility%0Aof%20this%20method%20have%20been%20corroborated%20through%20extensive%20validation%20on%20various%0Aremote%20sensing%20datasets%20and%20GAN%20models.%20The%20proposed%20method%20not%20only%20enhances%0Athe%20quality%20of%20the%20generated%20images%2C%20reflected%20in%20a%203.13%5C%25%20improvement%20in%0AFrechet%20Inception%20Distance%20%28FID%29%20score%2C%20but%20also%20boosts%20the%20performance%20of%20the%0AGANs%20on%20downstream%20tasks%2C%20evidenced%20by%20a%203.76%5C%25%20increase%20in%20classification%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.19507v3&entry.124074799=Read"},
{"title": "COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations", "author": "Vinicius G. Goecks and Nicholas Waytowich", "abstract": "  The development of Courses of Action (COAs) in military operations is\ntraditionally a time-consuming and intricate process. Addressing this\nchallenge, this study introduces COA-GPT, a novel algorithm employing Large\nLanguage Models (LLMs) for rapid and efficient generation of valid COAs.\nCOA-GPT incorporates military doctrine and domain expertise to LLMs through\nin-context learning, allowing commanders to input mission information - in both\ntext and image formats - and receive strategically aligned COAs for review and\napproval. Uniquely, COA-GPT not only accelerates COA development, producing\ninitial COAs within seconds, but also facilitates real-time refinement based on\ncommander feedback. This work evaluates COA-GPT in a military-relevant scenario\nwithin a militarized version of the StarCraft II game, comparing its\nperformance against state-of-the-art reinforcement learning algorithms. Our\nresults demonstrate COA-GPT's superiority in generating strategically sound\nCOAs more swiftly, with added benefits of enhanced adaptability and alignment\nwith commander intentions. COA-GPT's capability to rapidly adapt and update\nCOAs during missions presents a transformative potential for military planning,\nparticularly in addressing planning discrepancies and capitalizing on emergent\nwindows of opportunities.\n", "link": "http://arxiv.org/abs/2402.01786v2", "date": "2024-03-28", "relevancy": 2.0695, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5431}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5123}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5121}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20COA-GPT%3A%20Generative%20Pre-trained%20Transformers%20for%20Accelerated%20Course%20of%0A%20%20Action%20Development%20in%20Military%20Operations&body=Title%3A%20COA-GPT%3A%20Generative%20Pre-trained%20Transformers%20for%20Accelerated%20Course%20of%0A%20%20Action%20Development%20in%20Military%20Operations%0AAuthor%3A%20Vinicius%20G.%20Goecks%20and%20Nicholas%20Waytowich%0AAbstract%3A%20%20%20The%20development%20of%20Courses%20of%20Action%20%28COAs%29%20in%20military%20operations%20is%0Atraditionally%20a%20time-consuming%20and%20intricate%20process.%20Addressing%20this%0Achallenge%2C%20this%20study%20introduces%20COA-GPT%2C%20a%20novel%20algorithm%20employing%20Large%0ALanguage%20Models%20%28LLMs%29%20for%20rapid%20and%20efficient%20generation%20of%20valid%20COAs.%0ACOA-GPT%20incorporates%20military%20doctrine%20and%20domain%20expertise%20to%20LLMs%20through%0Ain-context%20learning%2C%20allowing%20commanders%20to%20input%20mission%20information%20-%20in%20both%0Atext%20and%20image%20formats%20-%20and%20receive%20strategically%20aligned%20COAs%20for%20review%20and%0Aapproval.%20Uniquely%2C%20COA-GPT%20not%20only%20accelerates%20COA%20development%2C%20producing%0Ainitial%20COAs%20within%20seconds%2C%20but%20also%20facilitates%20real-time%20refinement%20based%20on%0Acommander%20feedback.%20This%20work%20evaluates%20COA-GPT%20in%20a%20military-relevant%20scenario%0Awithin%20a%20militarized%20version%20of%20the%20StarCraft%20II%20game%2C%20comparing%20its%0Aperformance%20against%20state-of-the-art%20reinforcement%20learning%20algorithms.%20Our%0Aresults%20demonstrate%20COA-GPT%27s%20superiority%20in%20generating%20strategically%20sound%0ACOAs%20more%20swiftly%2C%20with%20added%20benefits%20of%20enhanced%20adaptability%20and%20alignment%0Awith%20commander%20intentions.%20COA-GPT%27s%20capability%20to%20rapidly%20adapt%20and%20update%0ACOAs%20during%20missions%20presents%20a%20transformative%20potential%20for%20military%20planning%2C%0Aparticularly%20in%20addressing%20planning%20discrepancies%20and%20capitalizing%20on%20emergent%0Awindows%20of%20opportunities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01786v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COA-GPT%3A%20Generative%20Pre-trained%20Transformers%20for%20Accelerated%20Course%20of%0A%20%20Action%20Development%20in%20Military%20Operations&entry.906535625=Vinicius%20G.%20Goecks%20and%20Nicholas%20Waytowich&entry.1292438233=%20%20The%20development%20of%20Courses%20of%20Action%20%28COAs%29%20in%20military%20operations%20is%0Atraditionally%20a%20time-consuming%20and%20intricate%20process.%20Addressing%20this%0Achallenge%2C%20this%20study%20introduces%20COA-GPT%2C%20a%20novel%20algorithm%20employing%20Large%0ALanguage%20Models%20%28LLMs%29%20for%20rapid%20and%20efficient%20generation%20of%20valid%20COAs.%0ACOA-GPT%20incorporates%20military%20doctrine%20and%20domain%20expertise%20to%20LLMs%20through%0Ain-context%20learning%2C%20allowing%20commanders%20to%20input%20mission%20information%20-%20in%20both%0Atext%20and%20image%20formats%20-%20and%20receive%20strategically%20aligned%20COAs%20for%20review%20and%0Aapproval.%20Uniquely%2C%20COA-GPT%20not%20only%20accelerates%20COA%20development%2C%20producing%0Ainitial%20COAs%20within%20seconds%2C%20but%20also%20facilitates%20real-time%20refinement%20based%20on%0Acommander%20feedback.%20This%20work%20evaluates%20COA-GPT%20in%20a%20military-relevant%20scenario%0Awithin%20a%20militarized%20version%20of%20the%20StarCraft%20II%20game%2C%20comparing%20its%0Aperformance%20against%20state-of-the-art%20reinforcement%20learning%20algorithms.%20Our%0Aresults%20demonstrate%20COA-GPT%27s%20superiority%20in%20generating%20strategically%20sound%0ACOAs%20more%20swiftly%2C%20with%20added%20benefits%20of%20enhanced%20adaptability%20and%20alignment%0Awith%20commander%20intentions.%20COA-GPT%27s%20capability%20to%20rapidly%20adapt%20and%20update%0ACOAs%20during%20missions%20presents%20a%20transformative%20potential%20for%20military%20planning%2C%0Aparticularly%20in%20addressing%20planning%20discrepancies%20and%20capitalizing%20on%20emergent%0Awindows%20of%20opportunities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01786v2&entry.124074799=Read"},
{"title": "Cross-Attention is Not Always Needed: Dynamic Cross-Attention for\n  Audio-Visual Dimensional Emotion Recognition", "author": "R. Gnana Praveen and Jahangir Alam", "abstract": "  In video-based emotion recognition, audio and visual modalities are often\nexpected to have a complementary relationship, which is widely explored using\ncross-attention. However, they may also exhibit weak complementary\nrelationships, resulting in poor representations of audio-visual features, thus\ndegrading the performance of the system. To address this issue, we propose\nDynamic Cross-Attention (DCA) that can dynamically select cross-attended or\nunattended features on the fly based on their strong or weak complementary\nrelationship with each other, respectively. Specifically, a simple yet\nefficient gating layer is designed to evaluate the contribution of the\ncross-attention mechanism and choose cross-attended features only when they\nexhibit a strong complementary relationship, otherwise unattended features. We\nevaluate the performance of the proposed approach on the challenging RECOLA and\nAff-Wild2 datasets. We also compare the proposed approach with other variants\nof cross-attention and show that the proposed model consistently improves the\nperformance on both datasets.\n", "link": "http://arxiv.org/abs/2403.19554v1", "date": "2024-03-28", "relevancy": 2.0691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5203}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5066}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Cross-Attention%20is%20Not%20Always%20Needed%3A%20Dynamic%20Cross-Attention%20for%0A%20%20Audio-Visual%20Dimensional%20Emotion%20Recognition&body=Title%3A%20Cross-Attention%20is%20Not%20Always%20Needed%3A%20Dynamic%20Cross-Attention%20for%0A%20%20Audio-Visual%20Dimensional%20Emotion%20Recognition%0AAuthor%3A%20R.%20Gnana%20Praveen%20and%20Jahangir%20Alam%0AAbstract%3A%20%20%20In%20video-based%20emotion%20recognition%2C%20audio%20and%20visual%20modalities%20are%20often%0Aexpected%20to%20have%20a%20complementary%20relationship%2C%20which%20is%20widely%20explored%20using%0Across-attention.%20However%2C%20they%20may%20also%20exhibit%20weak%20complementary%0Arelationships%2C%20resulting%20in%20poor%20representations%20of%20audio-visual%20features%2C%20thus%0Adegrading%20the%20performance%20of%20the%20system.%20To%20address%20this%20issue%2C%20we%20propose%0ADynamic%20Cross-Attention%20%28DCA%29%20that%20can%20dynamically%20select%20cross-attended%20or%0Aunattended%20features%20on%20the%20fly%20based%20on%20their%20strong%20or%20weak%20complementary%0Arelationship%20with%20each%20other%2C%20respectively.%20Specifically%2C%20a%20simple%20yet%0Aefficient%20gating%20layer%20is%20designed%20to%20evaluate%20the%20contribution%20of%20the%0Across-attention%20mechanism%20and%20choose%20cross-attended%20features%20only%20when%20they%0Aexhibit%20a%20strong%20complementary%20relationship%2C%20otherwise%20unattended%20features.%20We%0Aevaluate%20the%20performance%20of%20the%20proposed%20approach%20on%20the%20challenging%20RECOLA%20and%0AAff-Wild2%20datasets.%20We%20also%20compare%20the%20proposed%20approach%20with%20other%20variants%0Aof%20cross-attention%20and%20show%20that%20the%20proposed%20model%20consistently%20improves%20the%0Aperformance%20on%20both%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19554v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Attention%20is%20Not%20Always%20Needed%3A%20Dynamic%20Cross-Attention%20for%0A%20%20Audio-Visual%20Dimensional%20Emotion%20Recognition&entry.906535625=R.%20Gnana%20Praveen%20and%20Jahangir%20Alam&entry.1292438233=%20%20In%20video-based%20emotion%20recognition%2C%20audio%20and%20visual%20modalities%20are%20often%0Aexpected%20to%20have%20a%20complementary%20relationship%2C%20which%20is%20widely%20explored%20using%0Across-attention.%20However%2C%20they%20may%20also%20exhibit%20weak%20complementary%0Arelationships%2C%20resulting%20in%20poor%20representations%20of%20audio-visual%20features%2C%20thus%0Adegrading%20the%20performance%20of%20the%20system.%20To%20address%20this%20issue%2C%20we%20propose%0ADynamic%20Cross-Attention%20%28DCA%29%20that%20can%20dynamically%20select%20cross-attended%20or%0Aunattended%20features%20on%20the%20fly%20based%20on%20their%20strong%20or%20weak%20complementary%0Arelationship%20with%20each%20other%2C%20respectively.%20Specifically%2C%20a%20simple%20yet%0Aefficient%20gating%20layer%20is%20designed%20to%20evaluate%20the%20contribution%20of%20the%0Across-attention%20mechanism%20and%20choose%20cross-attended%20features%20only%20when%20they%0Aexhibit%20a%20strong%20complementary%20relationship%2C%20otherwise%20unattended%20features.%20We%0Aevaluate%20the%20performance%20of%20the%20proposed%20approach%20on%20the%20challenging%20RECOLA%20and%0AAff-Wild2%20datasets.%20We%20also%20compare%20the%20proposed%20approach%20with%20other%20variants%0Aof%20cross-attention%20and%20show%20that%20the%20proposed%20model%20consistently%20improves%20the%0Aperformance%20on%20both%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19554v1&entry.124074799=Read"},
{"title": "NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data", "author": "Matteo Caligiuri and Adriano Simonetto and Gianluca Agresti and Pietro Zanuttigh", "abstract": "  The acquisition of objects outside the Line-of-Sight of cameras is a very\nintriguing but also extremely challenging research topic. Recent works showed\nthe feasibility of this idea exploiting transient imaging data produced by\ncustom direct Time of Flight sensors. In this paper, for the first time, we\ntackle this problem using only data from an off-the-shelf indirect Time of\nFlight sensor without any further hardware requirement. We introduced a Deep\nLearning model able to re-frame the surfaces where light bounces happen as a\nvirtual mirror. This modeling makes the task easier to handle and also\nfacilitates the construction of annotated training data. From the obtained data\nit is possible to retrieve the depth information of the hidden scene. We also\nprovide a first-in-its-kind synthetic dataset for the task and demonstrate the\nfeasibility of the proposed idea over it.\n", "link": "http://arxiv.org/abs/2403.19376v1", "date": "2024-03-28", "relevancy": 2.0586, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5188}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5117}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5115}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NIGHT%20--%20Non-Line-of-Sight%20Imaging%20from%20Indirect%20Time%20of%20Flight%20Data&body=Title%3A%20NIGHT%20--%20Non-Line-of-Sight%20Imaging%20from%20Indirect%20Time%20of%20Flight%20Data%0AAuthor%3A%20Matteo%20Caligiuri%20and%20Adriano%20Simonetto%20and%20Gianluca%20Agresti%20and%20Pietro%20Zanuttigh%0AAbstract%3A%20%20%20The%20acquisition%20of%20objects%20outside%20the%20Line-of-Sight%20of%20cameras%20is%20a%20very%0Aintriguing%20but%20also%20extremely%20challenging%20research%20topic.%20Recent%20works%20showed%0Athe%20feasibility%20of%20this%20idea%20exploiting%20transient%20imaging%20data%20produced%20by%0Acustom%20direct%20Time%20of%20Flight%20sensors.%20In%20this%20paper%2C%20for%20the%20first%20time%2C%20we%0Atackle%20this%20problem%20using%20only%20data%20from%20an%20off-the-shelf%20indirect%20Time%20of%0AFlight%20sensor%20without%20any%20further%20hardware%20requirement.%20We%20introduced%20a%20Deep%0ALearning%20model%20able%20to%20re-frame%20the%20surfaces%20where%20light%20bounces%20happen%20as%20a%0Avirtual%20mirror.%20This%20modeling%20makes%20the%20task%20easier%20to%20handle%20and%20also%0Afacilitates%20the%20construction%20of%20annotated%20training%20data.%20From%20the%20obtained%20data%0Ait%20is%20possible%20to%20retrieve%20the%20depth%20information%20of%20the%20hidden%20scene.%20We%20also%0Aprovide%20a%20first-in-its-kind%20synthetic%20dataset%20for%20the%20task%20and%20demonstrate%20the%0Afeasibility%20of%20the%20proposed%20idea%20over%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19376v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NIGHT%20--%20Non-Line-of-Sight%20Imaging%20from%20Indirect%20Time%20of%20Flight%20Data&entry.906535625=Matteo%20Caligiuri%20and%20Adriano%20Simonetto%20and%20Gianluca%20Agresti%20and%20Pietro%20Zanuttigh&entry.1292438233=%20%20The%20acquisition%20of%20objects%20outside%20the%20Line-of-Sight%20of%20cameras%20is%20a%20very%0Aintriguing%20but%20also%20extremely%20challenging%20research%20topic.%20Recent%20works%20showed%0Athe%20feasibility%20of%20this%20idea%20exploiting%20transient%20imaging%20data%20produced%20by%0Acustom%20direct%20Time%20of%20Flight%20sensors.%20In%20this%20paper%2C%20for%20the%20first%20time%2C%20we%0Atackle%20this%20problem%20using%20only%20data%20from%20an%20off-the-shelf%20indirect%20Time%20of%0AFlight%20sensor%20without%20any%20further%20hardware%20requirement.%20We%20introduced%20a%20Deep%0ALearning%20model%20able%20to%20re-frame%20the%20surfaces%20where%20light%20bounces%20happen%20as%20a%0Avirtual%20mirror.%20This%20modeling%20makes%20the%20task%20easier%20to%20handle%20and%20also%0Afacilitates%20the%20construction%20of%20annotated%20training%20data.%20From%20the%20obtained%20data%0Ait%20is%20possible%20to%20retrieve%20the%20depth%20information%20of%20the%20hidden%20scene.%20We%20also%0Aprovide%20a%20first-in-its-kind%20synthetic%20dataset%20for%20the%20task%20and%20demonstrate%20the%0Afeasibility%20of%20the%20proposed%20idea%20over%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19376v1&entry.124074799=Read"},
{"title": "Model Stock: All we need is just a few fine-tuned models", "author": "Dong-Hwan Jang and Sangdoo Yun and Dongyoon Han", "abstract": "  This paper introduces an efficient fine-tuning method for large pre-trained\nmodels, offering strong in-distribution (ID) and out-of-distribution (OOD)\nperformance. Breaking away from traditional practices that need a multitude of\nfine-tuned models for averaging, our approach employs significantly fewer\nmodels to achieve final weights yet yield superior accuracy. Drawing from key\ninsights in the weight space of fine-tuned weights, we uncover a strong link\nbetween the performance and proximity to the center of weight space. Based on\nthis, we introduce a method that approximates a center-close weight using only\ntwo fine-tuned models, applicable during or after training. Our innovative\nlayer-wise weight averaging technique surpasses state-of-the-art model methods\nsuch as Model Soup, utilizing only two fine-tuned models. This strategy can be\naptly coined Model Stock, highlighting its reliance on selecting a minimal\nnumber of models to draw a more optimized-averaged model. We demonstrate the\nefficacy of Model Stock with fine-tuned models based upon pre-trained CLIP\narchitectures, achieving remarkable performance on both ID and OOD tasks on the\nstandard benchmarks, all while barely bringing extra computational demands. Our\ncode and pre-trained models are available at\nhttps://github.com/naver-ai/model-stock.\n", "link": "http://arxiv.org/abs/2403.19522v1", "date": "2024-03-28", "relevancy": 2.0493, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5216}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5068}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.503}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Model%20Stock%3A%20All%20we%20need%20is%20just%20a%20few%20fine-tuned%20models&body=Title%3A%20Model%20Stock%3A%20All%20we%20need%20is%20just%20a%20few%20fine-tuned%20models%0AAuthor%3A%20Dong-Hwan%20Jang%20and%20Sangdoo%20Yun%20and%20Dongyoon%20Han%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20efficient%20fine-tuning%20method%20for%20large%20pre-trained%0Amodels%2C%20offering%20strong%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%0Aperformance.%20Breaking%20away%20from%20traditional%20practices%20that%20need%20a%20multitude%20of%0Afine-tuned%20models%20for%20averaging%2C%20our%20approach%20employs%20significantly%20fewer%0Amodels%20to%20achieve%20final%20weights%20yet%20yield%20superior%20accuracy.%20Drawing%20from%20key%0Ainsights%20in%20the%20weight%20space%20of%20fine-tuned%20weights%2C%20we%20uncover%20a%20strong%20link%0Abetween%20the%20performance%20and%20proximity%20to%20the%20center%20of%20weight%20space.%20Based%20on%0Athis%2C%20we%20introduce%20a%20method%20that%20approximates%20a%20center-close%20weight%20using%20only%0Atwo%20fine-tuned%20models%2C%20applicable%20during%20or%20after%20training.%20Our%20innovative%0Alayer-wise%20weight%20averaging%20technique%20surpasses%20state-of-the-art%20model%20methods%0Asuch%20as%20Model%20Soup%2C%20utilizing%20only%20two%20fine-tuned%20models.%20This%20strategy%20can%20be%0Aaptly%20coined%20Model%20Stock%2C%20highlighting%20its%20reliance%20on%20selecting%20a%20minimal%0Anumber%20of%20models%20to%20draw%20a%20more%20optimized-averaged%20model.%20We%20demonstrate%20the%0Aefficacy%20of%20Model%20Stock%20with%20fine-tuned%20models%20based%20upon%20pre-trained%20CLIP%0Aarchitectures%2C%20achieving%20remarkable%20performance%20on%20both%20ID%20and%20OOD%20tasks%20on%20the%0Astandard%20benchmarks%2C%20all%20while%20barely%20bringing%20extra%20computational%20demands.%20Our%0Acode%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/naver-ai/model-stock.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19522v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Stock%3A%20All%20we%20need%20is%20just%20a%20few%20fine-tuned%20models&entry.906535625=Dong-Hwan%20Jang%20and%20Sangdoo%20Yun%20and%20Dongyoon%20Han&entry.1292438233=%20%20This%20paper%20introduces%20an%20efficient%20fine-tuning%20method%20for%20large%20pre-trained%0Amodels%2C%20offering%20strong%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28OOD%29%0Aperformance.%20Breaking%20away%20from%20traditional%20practices%20that%20need%20a%20multitude%20of%0Afine-tuned%20models%20for%20averaging%2C%20our%20approach%20employs%20significantly%20fewer%0Amodels%20to%20achieve%20final%20weights%20yet%20yield%20superior%20accuracy.%20Drawing%20from%20key%0Ainsights%20in%20the%20weight%20space%20of%20fine-tuned%20weights%2C%20we%20uncover%20a%20strong%20link%0Abetween%20the%20performance%20and%20proximity%20to%20the%20center%20of%20weight%20space.%20Based%20on%0Athis%2C%20we%20introduce%20a%20method%20that%20approximates%20a%20center-close%20weight%20using%20only%0Atwo%20fine-tuned%20models%2C%20applicable%20during%20or%20after%20training.%20Our%20innovative%0Alayer-wise%20weight%20averaging%20technique%20surpasses%20state-of-the-art%20model%20methods%0Asuch%20as%20Model%20Soup%2C%20utilizing%20only%20two%20fine-tuned%20models.%20This%20strategy%20can%20be%0Aaptly%20coined%20Model%20Stock%2C%20highlighting%20its%20reliance%20on%20selecting%20a%20minimal%0Anumber%20of%20models%20to%20draw%20a%20more%20optimized-averaged%20model.%20We%20demonstrate%20the%0Aefficacy%20of%20Model%20Stock%20with%20fine-tuned%20models%20based%20upon%20pre-trained%20CLIP%0Aarchitectures%2C%20achieving%20remarkable%20performance%20on%20both%20ID%20and%20OOD%20tasks%20on%20the%0Astandard%20benchmarks%2C%20all%20while%20barely%20bringing%20extra%20computational%20demands.%20Our%0Acode%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/naver-ai/model-stock.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19522v1&entry.124074799=Read"},
{"title": "Test-Time Domain Generalization for Face Anti-Spoofing", "author": "Qianyu Zhou and Ke-Yue Zhang and Taiping Yao and Xuequan Lu and Shouhong Ding and Lizhuang Ma", "abstract": "  Face Anti-Spoofing (FAS) is pivotal in safeguarding facial recognition\nsystems against presentation attacks. While domain generalization (DG) methods\nhave been developed to enhance FAS performance, they predominantly focus on\nlearning domain-invariant features during training, which may not guarantee\ngeneralizability to unseen data that differs largely from the source\ndistributions. Our insight is that testing data can serve as a valuable\nresource to enhance the generalizability beyond mere evaluation for DG FAS. In\nthis paper, we introduce a novel Test-Time Domain Generalization (TTDG)\nframework for FAS, which leverages the testing data to boost the model's\ngeneralizability. Our method, consisting of Test-Time Style Projection (TTSP)\nand Diverse Style Shifts Simulation (DSSS), effectively projects the unseen\ndata to the seen domain space. In particular, we first introduce the innovative\nTTSP to project the styles of the arbitrarily unseen samples of the testing\ndistribution to the known source space of the training distributions. We then\ndesign the efficient DSSS to synthesize diverse style shifts via learnable\nstyle bases with two specifically designed losses in a hyperspherical feature\nspace. Our method eliminates the need for model updates at the test time and\ncan be seamlessly integrated into not only the CNN but also ViT backbones.\nComprehensive experiments on widely used cross-domain FAS benchmarks\ndemonstrate our method's state-of-the-art performance and effectiveness.\n", "link": "http://arxiv.org/abs/2403.19334v1", "date": "2024-03-28", "relevancy": 2.0226, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5219}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4948}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.492}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Domain%20Generalization%20for%20Face%20Anti-Spoofing&body=Title%3A%20Test-Time%20Domain%20Generalization%20for%20Face%20Anti-Spoofing%0AAuthor%3A%20Qianyu%20Zhou%20and%20Ke-Yue%20Zhang%20and%20Taiping%20Yao%20and%20Xuequan%20Lu%20and%20Shouhong%20Ding%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Face%20Anti-Spoofing%20%28FAS%29%20is%20pivotal%20in%20safeguarding%20facial%20recognition%0Asystems%20against%20presentation%20attacks.%20While%20domain%20generalization%20%28DG%29%20methods%0Ahave%20been%20developed%20to%20enhance%20FAS%20performance%2C%20they%20predominantly%20focus%20on%0Alearning%20domain-invariant%20features%20during%20training%2C%20which%20may%20not%20guarantee%0Ageneralizability%20to%20unseen%20data%20that%20differs%20largely%20from%20the%20source%0Adistributions.%20Our%20insight%20is%20that%20testing%20data%20can%20serve%20as%20a%20valuable%0Aresource%20to%20enhance%20the%20generalizability%20beyond%20mere%20evaluation%20for%20DG%20FAS.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20Test-Time%20Domain%20Generalization%20%28TTDG%29%0Aframework%20for%20FAS%2C%20which%20leverages%20the%20testing%20data%20to%20boost%20the%20model%27s%0Ageneralizability.%20Our%20method%2C%20consisting%20of%20Test-Time%20Style%20Projection%20%28TTSP%29%0Aand%20Diverse%20Style%20Shifts%20Simulation%20%28DSSS%29%2C%20effectively%20projects%20the%20unseen%0Adata%20to%20the%20seen%20domain%20space.%20In%20particular%2C%20we%20first%20introduce%20the%20innovative%0ATTSP%20to%20project%20the%20styles%20of%20the%20arbitrarily%20unseen%20samples%20of%20the%20testing%0Adistribution%20to%20the%20known%20source%20space%20of%20the%20training%20distributions.%20We%20then%0Adesign%20the%20efficient%20DSSS%20to%20synthesize%20diverse%20style%20shifts%20via%20learnable%0Astyle%20bases%20with%20two%20specifically%20designed%20losses%20in%20a%20hyperspherical%20feature%0Aspace.%20Our%20method%20eliminates%20the%20need%20for%20model%20updates%20at%20the%20test%20time%20and%0Acan%20be%20seamlessly%20integrated%20into%20not%20only%20the%20CNN%20but%20also%20ViT%20backbones.%0AComprehensive%20experiments%20on%20widely%20used%20cross-domain%20FAS%20benchmarks%0Ademonstrate%20our%20method%27s%20state-of-the-art%20performance%20and%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19334v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Domain%20Generalization%20for%20Face%20Anti-Spoofing&entry.906535625=Qianyu%20Zhou%20and%20Ke-Yue%20Zhang%20and%20Taiping%20Yao%20and%20Xuequan%20Lu%20and%20Shouhong%20Ding%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Face%20Anti-Spoofing%20%28FAS%29%20is%20pivotal%20in%20safeguarding%20facial%20recognition%0Asystems%20against%20presentation%20attacks.%20While%20domain%20generalization%20%28DG%29%20methods%0Ahave%20been%20developed%20to%20enhance%20FAS%20performance%2C%20they%20predominantly%20focus%20on%0Alearning%20domain-invariant%20features%20during%20training%2C%20which%20may%20not%20guarantee%0Ageneralizability%20to%20unseen%20data%20that%20differs%20largely%20from%20the%20source%0Adistributions.%20Our%20insight%20is%20that%20testing%20data%20can%20serve%20as%20a%20valuable%0Aresource%20to%20enhance%20the%20generalizability%20beyond%20mere%20evaluation%20for%20DG%20FAS.%20In%0Athis%20paper%2C%20we%20introduce%20a%20novel%20Test-Time%20Domain%20Generalization%20%28TTDG%29%0Aframework%20for%20FAS%2C%20which%20leverages%20the%20testing%20data%20to%20boost%20the%20model%27s%0Ageneralizability.%20Our%20method%2C%20consisting%20of%20Test-Time%20Style%20Projection%20%28TTSP%29%0Aand%20Diverse%20Style%20Shifts%20Simulation%20%28DSSS%29%2C%20effectively%20projects%20the%20unseen%0Adata%20to%20the%20seen%20domain%20space.%20In%20particular%2C%20we%20first%20introduce%20the%20innovative%0ATTSP%20to%20project%20the%20styles%20of%20the%20arbitrarily%20unseen%20samples%20of%20the%20testing%0Adistribution%20to%20the%20known%20source%20space%20of%20the%20training%20distributions.%20We%20then%0Adesign%20the%20efficient%20DSSS%20to%20synthesize%20diverse%20style%20shifts%20via%20learnable%0Astyle%20bases%20with%20two%20specifically%20designed%20losses%20in%20a%20hyperspherical%20feature%0Aspace.%20Our%20method%20eliminates%20the%20need%20for%20model%20updates%20at%20the%20test%20time%20and%0Acan%20be%20seamlessly%20integrated%20into%20not%20only%20the%20CNN%20but%20also%20ViT%20backbones.%0AComprehensive%20experiments%20on%20widely%20used%20cross-domain%20FAS%20benchmarks%0Ademonstrate%20our%20method%27s%20state-of-the-art%20performance%20and%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19334v1&entry.124074799=Read"},
{"title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving\n  with VQ-VAE and Differentiable Optimization", "author": "Simon Idoko and Basant Sharma and Arun Kumar Singh", "abstract": "  Sampling trajectories from a distribution followed by ranking them based on a\nspecified cost function is a common approach in autonomous driving. Typically,\nthe sampling distribution is hand-crafted (e.g a Gaussian, or a grid).\nRecently, there have been efforts towards learning the sampling distribution\nthrough generative models such as Conditional Variational Autoencoder (CVAE).\nHowever, these approaches fail to capture the multi-modality of the driving\nbehaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we\nre-imagine the distribution learning through vector quantized variational\nautoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture\nmulti-modal sampling distribution. The VQ-VAE is trained with demonstration\ndata of optimal trajectories. We further propose a differentiable optimization\nbased safety filter to minimally correct the VQVAE sampled trajectories to\nensure collision avoidance. We use backpropagation through the optimization\nlayers in a self-supervised learning set-up to learn good initialization and\noptimal parameters of the safety filter. We perform extensive comparisons with\nstate-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios\nand show a reduction of up to 12 times in collision-rate while being\ncompetitive in driving speeds.\n", "link": "http://arxiv.org/abs/2403.19461v1", "date": "2024-03-28", "relevancy": 2.021, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5297}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5034}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4974}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Sampling%20Distribution%20and%20Safety%20Filter%20for%20Autonomous%20Driving%0A%20%20with%20VQ-VAE%20and%20Differentiable%20Optimization&body=Title%3A%20Learning%20Sampling%20Distribution%20and%20Safety%20Filter%20for%20Autonomous%20Driving%0A%20%20with%20VQ-VAE%20and%20Differentiable%20Optimization%0AAuthor%3A%20Simon%20Idoko%20and%20Basant%20Sharma%20and%20Arun%20Kumar%20Singh%0AAbstract%3A%20%20%20Sampling%20trajectories%20from%20a%20distribution%20followed%20by%20ranking%20them%20based%20on%20a%0Aspecified%20cost%20function%20is%20a%20common%20approach%20in%20autonomous%20driving.%20Typically%2C%0Athe%20sampling%20distribution%20is%20hand-crafted%20%28e.g%20a%20Gaussian%2C%20or%20a%20grid%29.%0ARecently%2C%20there%20have%20been%20efforts%20towards%20learning%20the%20sampling%20distribution%0Athrough%20generative%20models%20such%20as%20Conditional%20Variational%20Autoencoder%20%28CVAE%29.%0AHowever%2C%20these%20approaches%20fail%20to%20capture%20the%20multi-modality%20of%20the%20driving%0Abehaviour%20due%20to%20the%20Gaussian%20latent%20prior%20of%20the%20CVAE.%20Thus%2C%20in%20this%20paper%2C%20we%0Are-imagine%20the%20distribution%20learning%20through%20vector%20quantized%20variational%0Aautoencoder%20%28VQ-VAE%29%2C%20whose%20discrete%20latent-space%20is%20well%20equipped%20to%20capture%0Amulti-modal%20sampling%20distribution.%20The%20VQ-VAE%20is%20trained%20with%20demonstration%0Adata%20of%20optimal%20trajectories.%20We%20further%20propose%20a%20differentiable%20optimization%0Abased%20safety%20filter%20to%20minimally%20correct%20the%20VQVAE%20sampled%20trajectories%20to%0Aensure%20collision%20avoidance.%20We%20use%20backpropagation%20through%20the%20optimization%0Alayers%20in%20a%20self-supervised%20learning%20set-up%20to%20learn%20good%20initialization%20and%0Aoptimal%20parameters%20of%20the%20safety%20filter.%20We%20perform%20extensive%20comparisons%20with%0Astate-of-the-art%20CVAE-based%20baseline%20in%20dense%20and%20aggressive%20traffic%20scenarios%0Aand%20show%20a%20reduction%20of%20up%20to%2012%20times%20in%20collision-rate%20while%20being%0Acompetitive%20in%20driving%20speeds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19461v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Sampling%20Distribution%20and%20Safety%20Filter%20for%20Autonomous%20Driving%0A%20%20with%20VQ-VAE%20and%20Differentiable%20Optimization&entry.906535625=Simon%20Idoko%20and%20Basant%20Sharma%20and%20Arun%20Kumar%20Singh&entry.1292438233=%20%20Sampling%20trajectories%20from%20a%20distribution%20followed%20by%20ranking%20them%20based%20on%20a%0Aspecified%20cost%20function%20is%20a%20common%20approach%20in%20autonomous%20driving.%20Typically%2C%0Athe%20sampling%20distribution%20is%20hand-crafted%20%28e.g%20a%20Gaussian%2C%20or%20a%20grid%29.%0ARecently%2C%20there%20have%20been%20efforts%20towards%20learning%20the%20sampling%20distribution%0Athrough%20generative%20models%20such%20as%20Conditional%20Variational%20Autoencoder%20%28CVAE%29.%0AHowever%2C%20these%20approaches%20fail%20to%20capture%20the%20multi-modality%20of%20the%20driving%0Abehaviour%20due%20to%20the%20Gaussian%20latent%20prior%20of%20the%20CVAE.%20Thus%2C%20in%20this%20paper%2C%20we%0Are-imagine%20the%20distribution%20learning%20through%20vector%20quantized%20variational%0Aautoencoder%20%28VQ-VAE%29%2C%20whose%20discrete%20latent-space%20is%20well%20equipped%20to%20capture%0Amulti-modal%20sampling%20distribution.%20The%20VQ-VAE%20is%20trained%20with%20demonstration%0Adata%20of%20optimal%20trajectories.%20We%20further%20propose%20a%20differentiable%20optimization%0Abased%20safety%20filter%20to%20minimally%20correct%20the%20VQVAE%20sampled%20trajectories%20to%0Aensure%20collision%20avoidance.%20We%20use%20backpropagation%20through%20the%20optimization%0Alayers%20in%20a%20self-supervised%20learning%20set-up%20to%20learn%20good%20initialization%20and%0Aoptimal%20parameters%20of%20the%20safety%20filter.%20We%20perform%20extensive%20comparisons%20with%0Astate-of-the-art%20CVAE-based%20baseline%20in%20dense%20and%20aggressive%20traffic%20scenarios%0Aand%20show%20a%20reduction%20of%20up%20to%2012%20times%20in%20collision-rate%20while%20being%0Acompetitive%20in%20driving%20speeds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19461v1&entry.124074799=Read"},
{"title": "Genetic Quantization-Aware Approximation for Non-Linear Operations in\n  Transformers", "author": "Pingcheng Dong and Yonghao Tan and Dong Zhang and Tianwei Ni and Xuejiao Liu and Yu Liu and Peng Luo and Luhong Liang and Shih-Yang Liu and Xijie Huang and Huaiyu Zhu and Yun Pan and Fengwei An and Kwang-Ting Cheng", "abstract": "  Non-linear functions are prevalent in Transformers and their lightweight\nvariants, incurring substantial and frequently underestimated hardware costs.\nPrevious state-of-the-art works optimize these operations by piece-wise linear\napproximation and store the parameters in look-up tables (LUT), but most of\nthem require unfriendly high-precision arithmetics such as FP/INT 32 and lack\nconsideration of integer-only INT quantization. This paper proposed a genetic\nLUT-Approximation algorithm namely GQA-LUT that can automatically determine the\nparameters with quantization awareness. The results demonstrate that GQA-LUT\nachieves negligible degradation on the challenging semantic segmentation task\nfor both vanilla and linear Transformer models. Besides, proposed GQA-LUT\nenables the employment of INT8-based LUT-Approximation that achieves an area\nsavings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the\nhigh-precision FP/INT 32 alternatives. Code is available at https://\ngithub.com/PingchengDong/GQA-LUT.\n", "link": "http://arxiv.org/abs/2403.19591v1", "date": "2024-03-28", "relevancy": 2.0207, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5061}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4913}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Genetic%20Quantization-Aware%20Approximation%20for%20Non-Linear%20Operations%20in%0A%20%20Transformers&body=Title%3A%20Genetic%20Quantization-Aware%20Approximation%20for%20Non-Linear%20Operations%20in%0A%20%20Transformers%0AAuthor%3A%20Pingcheng%20Dong%20and%20Yonghao%20Tan%20and%20Dong%20Zhang%20and%20Tianwei%20Ni%20and%20Xuejiao%20Liu%20and%20Yu%20Liu%20and%20Peng%20Luo%20and%20Luhong%20Liang%20and%20Shih-Yang%20Liu%20and%20Xijie%20Huang%20and%20Huaiyu%20Zhu%20and%20Yun%20Pan%20and%20Fengwei%20An%20and%20Kwang-Ting%20Cheng%0AAbstract%3A%20%20%20Non-linear%20functions%20are%20prevalent%20in%20Transformers%20and%20their%20lightweight%0Avariants%2C%20incurring%20substantial%20and%20frequently%20underestimated%20hardware%20costs.%0APrevious%20state-of-the-art%20works%20optimize%20these%20operations%20by%20piece-wise%20linear%0Aapproximation%20and%20store%20the%20parameters%20in%20look-up%20tables%20%28LUT%29%2C%20but%20most%20of%0Athem%20require%20unfriendly%20high-precision%20arithmetics%20such%20as%20FP/INT%2032%20and%20lack%0Aconsideration%20of%20integer-only%20INT%20quantization.%20This%20paper%20proposed%20a%20genetic%0ALUT-Approximation%20algorithm%20namely%20GQA-LUT%20that%20can%20automatically%20determine%20the%0Aparameters%20with%20quantization%20awareness.%20The%20results%20demonstrate%20that%20GQA-LUT%0Aachieves%20negligible%20degradation%20on%20the%20challenging%20semantic%20segmentation%20task%0Afor%20both%20vanilla%20and%20linear%20Transformer%20models.%20Besides%2C%20proposed%20GQA-LUT%0Aenables%20the%20employment%20of%20INT8-based%20LUT-Approximation%20that%20achieves%20an%20area%0Asavings%20of%2081.3~81.7%25%20and%20a%20power%20reduction%20of%2079.3~80.2%25%20compared%20to%20the%0Ahigh-precision%20FP/INT%2032%20alternatives.%20Code%20is%20available%20at%20https%3A//%0Agithub.com/PingchengDong/GQA-LUT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19591v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Genetic%20Quantization-Aware%20Approximation%20for%20Non-Linear%20Operations%20in%0A%20%20Transformers&entry.906535625=Pingcheng%20Dong%20and%20Yonghao%20Tan%20and%20Dong%20Zhang%20and%20Tianwei%20Ni%20and%20Xuejiao%20Liu%20and%20Yu%20Liu%20and%20Peng%20Luo%20and%20Luhong%20Liang%20and%20Shih-Yang%20Liu%20and%20Xijie%20Huang%20and%20Huaiyu%20Zhu%20and%20Yun%20Pan%20and%20Fengwei%20An%20and%20Kwang-Ting%20Cheng&entry.1292438233=%20%20Non-linear%20functions%20are%20prevalent%20in%20Transformers%20and%20their%20lightweight%0Avariants%2C%20incurring%20substantial%20and%20frequently%20underestimated%20hardware%20costs.%0APrevious%20state-of-the-art%20works%20optimize%20these%20operations%20by%20piece-wise%20linear%0Aapproximation%20and%20store%20the%20parameters%20in%20look-up%20tables%20%28LUT%29%2C%20but%20most%20of%0Athem%20require%20unfriendly%20high-precision%20arithmetics%20such%20as%20FP/INT%2032%20and%20lack%0Aconsideration%20of%20integer-only%20INT%20quantization.%20This%20paper%20proposed%20a%20genetic%0ALUT-Approximation%20algorithm%20namely%20GQA-LUT%20that%20can%20automatically%20determine%20the%0Aparameters%20with%20quantization%20awareness.%20The%20results%20demonstrate%20that%20GQA-LUT%0Aachieves%20negligible%20degradation%20on%20the%20challenging%20semantic%20segmentation%20task%0Afor%20both%20vanilla%20and%20linear%20Transformer%20models.%20Besides%2C%20proposed%20GQA-LUT%0Aenables%20the%20employment%20of%20INT8-based%20LUT-Approximation%20that%20achieves%20an%20area%0Asavings%20of%2081.3~81.7%25%20and%20a%20power%20reduction%20of%2079.3~80.2%25%20compared%20to%20the%0Ahigh-precision%20FP/INT%2032%20alternatives.%20Code%20is%20available%20at%20https%3A//%0Agithub.com/PingchengDong/GQA-LUT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19591v1&entry.124074799=Read"},
{"title": "On the Two Sides of Redundancy in Graph Neural Networks", "author": "Franka Bause and Samir Moustafa and Johannes Langguth and Wilfried N. Gansterer and Nils M. Kriege", "abstract": "  Message passing neural networks iteratively generate node embeddings by\naggregating information from neighboring nodes. With increasing depth,\ninformation from more distant nodes is included. However, node embeddings may\nbe unable to represent the growing node neighborhoods accurately and the\ninfluence of distant nodes may vanish, a problem referred to as oversquashing.\nInformation redundancy in message passing, i.e., the repetitive exchange and\nencoding of identical information amplifies oversquashing. We develop a novel\naggregation scheme based on neighborhood trees, which allows for controlling\nredundancy by pruning redundant branches of unfolding trees underlying standard\nmessage passing. While the regular structure of unfolding trees allows the\nreuse of intermediate results in a straightforward way, the use of neighborhood\ntrees poses computational challenges. We propose compact representations of\nneighborhood trees and merge them, exploiting computational redundancy by\nidentifying isomorphic subtrees. From this, node and graph embeddings are\ncomputed via a neural architecture inspired by tree canonization techniques.\nOur method is less susceptible to oversquashing than traditional message\npassing neural networks and can improve the accuracy on widely used benchmark\ndatasets.\n", "link": "http://arxiv.org/abs/2310.04190v2", "date": "2024-03-28", "relevancy": 1.9985, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5045}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5026}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4946}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Two%20Sides%20of%20Redundancy%20in%20Graph%20Neural%20Networks&body=Title%3A%20On%20the%20Two%20Sides%20of%20Redundancy%20in%20Graph%20Neural%20Networks%0AAuthor%3A%20Franka%20Bause%20and%20Samir%20Moustafa%20and%20Johannes%20Langguth%20and%20Wilfried%20N.%20Gansterer%20and%20Nils%20M.%20Kriege%0AAbstract%3A%20%20%20Message%20passing%20neural%20networks%20iteratively%20generate%20node%20embeddings%20by%0Aaggregating%20information%20from%20neighboring%20nodes.%20With%20increasing%20depth%2C%0Ainformation%20from%20more%20distant%20nodes%20is%20included.%20However%2C%20node%20embeddings%20may%0Abe%20unable%20to%20represent%20the%20growing%20node%20neighborhoods%20accurately%20and%20the%0Ainfluence%20of%20distant%20nodes%20may%20vanish%2C%20a%20problem%20referred%20to%20as%20oversquashing.%0AInformation%20redundancy%20in%20message%20passing%2C%20i.e.%2C%20the%20repetitive%20exchange%20and%0Aencoding%20of%20identical%20information%20amplifies%20oversquashing.%20We%20develop%20a%20novel%0Aaggregation%20scheme%20based%20on%20neighborhood%20trees%2C%20which%20allows%20for%20controlling%0Aredundancy%20by%20pruning%20redundant%20branches%20of%20unfolding%20trees%20underlying%20standard%0Amessage%20passing.%20While%20the%20regular%20structure%20of%20unfolding%20trees%20allows%20the%0Areuse%20of%20intermediate%20results%20in%20a%20straightforward%20way%2C%20the%20use%20of%20neighborhood%0Atrees%20poses%20computational%20challenges.%20We%20propose%20compact%20representations%20of%0Aneighborhood%20trees%20and%20merge%20them%2C%20exploiting%20computational%20redundancy%20by%0Aidentifying%20isomorphic%20subtrees.%20From%20this%2C%20node%20and%20graph%20embeddings%20are%0Acomputed%20via%20a%20neural%20architecture%20inspired%20by%20tree%20canonization%20techniques.%0AOur%20method%20is%20less%20susceptible%20to%20oversquashing%20than%20traditional%20message%0Apassing%20neural%20networks%20and%20can%20improve%20the%20accuracy%20on%20widely%20used%20benchmark%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04190v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Two%20Sides%20of%20Redundancy%20in%20Graph%20Neural%20Networks&entry.906535625=Franka%20Bause%20and%20Samir%20Moustafa%20and%20Johannes%20Langguth%20and%20Wilfried%20N.%20Gansterer%20and%20Nils%20M.%20Kriege&entry.1292438233=%20%20Message%20passing%20neural%20networks%20iteratively%20generate%20node%20embeddings%20by%0Aaggregating%20information%20from%20neighboring%20nodes.%20With%20increasing%20depth%2C%0Ainformation%20from%20more%20distant%20nodes%20is%20included.%20However%2C%20node%20embeddings%20may%0Abe%20unable%20to%20represent%20the%20growing%20node%20neighborhoods%20accurately%20and%20the%0Ainfluence%20of%20distant%20nodes%20may%20vanish%2C%20a%20problem%20referred%20to%20as%20oversquashing.%0AInformation%20redundancy%20in%20message%20passing%2C%20i.e.%2C%20the%20repetitive%20exchange%20and%0Aencoding%20of%20identical%20information%20amplifies%20oversquashing.%20We%20develop%20a%20novel%0Aaggregation%20scheme%20based%20on%20neighborhood%20trees%2C%20which%20allows%20for%20controlling%0Aredundancy%20by%20pruning%20redundant%20branches%20of%20unfolding%20trees%20underlying%20standard%0Amessage%20passing.%20While%20the%20regular%20structure%20of%20unfolding%20trees%20allows%20the%0Areuse%20of%20intermediate%20results%20in%20a%20straightforward%20way%2C%20the%20use%20of%20neighborhood%0Atrees%20poses%20computational%20challenges.%20We%20propose%20compact%20representations%20of%0Aneighborhood%20trees%20and%20merge%20them%2C%20exploiting%20computational%20redundancy%20by%0Aidentifying%20isomorphic%20subtrees.%20From%20this%2C%20node%20and%20graph%20embeddings%20are%0Acomputed%20via%20a%20neural%20architecture%20inspired%20by%20tree%20canonization%20techniques.%0AOur%20method%20is%20less%20susceptible%20to%20oversquashing%20than%20traditional%20message%0Apassing%20neural%20networks%20and%20can%20improve%20the%20accuracy%20on%20widely%20used%20benchmark%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04190v2&entry.124074799=Read"},
{"title": "Kinodynamic Motion Planning for a Team of Multirotors Transporting a\n  Cable-Suspended Payload in Cluttered Environments", "author": "Khaled Wahba and Joaquim Ortiz-Haro and Marc Toussaint and Wolfgang H\u00f6nig", "abstract": "  We propose a motion planner for cable-driven payload transportation using\nmultiple unmanned aerial vehicles (UAVs) in an environment cluttered with\nobstacles. Our planner is kinodynamic, i.e., it considers the full dynamics\nmodel of the transporting system including actuation constraints. Due to the\nhigh dimensionality of the planning problem, we use a hierarchical approach\nwhere we first solve the geometric motion planning using a sampling-based\nmethod with a novel sampler, followed by constrained trajectory optimization\nthat considers the full dynamics of the system. Both planning stages consider\ninter-robot and robot/obstacle collisions. We demonstrate in a\nsoftware-in-the-loop simulation and real flight experiments that there is a\nsignificant benefit in kinodynamic motion planning for such payload transport\nsystems with respect to payload tracking error and energy consumption compared\nto the standard methods of planning for the payload alone. Notably, we observe\na significantly higher success rate in scenarios where the team formation\nchanges are needed to move through tight spaces.\n", "link": "http://arxiv.org/abs/2310.03394v2", "date": "2024-03-28", "relevancy": 1.9913, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5008}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Kinodynamic%20Motion%20Planning%20for%20a%20Team%20of%20Multirotors%20Transporting%20a%0A%20%20Cable-Suspended%20Payload%20in%20Cluttered%20Environments&body=Title%3A%20Kinodynamic%20Motion%20Planning%20for%20a%20Team%20of%20Multirotors%20Transporting%20a%0A%20%20Cable-Suspended%20Payload%20in%20Cluttered%20Environments%0AAuthor%3A%20Khaled%20Wahba%20and%20Joaquim%20Ortiz-Haro%20and%20Marc%20Toussaint%20and%20Wolfgang%20H%C3%B6nig%0AAbstract%3A%20%20%20We%20propose%20a%20motion%20planner%20for%20cable-driven%20payload%20transportation%20using%0Amultiple%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20an%20environment%20cluttered%20with%0Aobstacles.%20Our%20planner%20is%20kinodynamic%2C%20i.e.%2C%20it%20considers%20the%20full%20dynamics%0Amodel%20of%20the%20transporting%20system%20including%20actuation%20constraints.%20Due%20to%20the%0Ahigh%20dimensionality%20of%20the%20planning%20problem%2C%20we%20use%20a%20hierarchical%20approach%0Awhere%20we%20first%20solve%20the%20geometric%20motion%20planning%20using%20a%20sampling-based%0Amethod%20with%20a%20novel%20sampler%2C%20followed%20by%20constrained%20trajectory%20optimization%0Athat%20considers%20the%20full%20dynamics%20of%20the%20system.%20Both%20planning%20stages%20consider%0Ainter-robot%20and%20robot/obstacle%20collisions.%20We%20demonstrate%20in%20a%0Asoftware-in-the-loop%20simulation%20and%20real%20flight%20experiments%20that%20there%20is%20a%0Asignificant%20benefit%20in%20kinodynamic%20motion%20planning%20for%20such%20payload%20transport%0Asystems%20with%20respect%20to%20payload%20tracking%20error%20and%20energy%20consumption%20compared%0Ato%20the%20standard%20methods%20of%20planning%20for%20the%20payload%20alone.%20Notably%2C%20we%20observe%0Aa%20significantly%20higher%20success%20rate%20in%20scenarios%20where%20the%20team%20formation%0Achanges%20are%20needed%20to%20move%20through%20tight%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03394v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinodynamic%20Motion%20Planning%20for%20a%20Team%20of%20Multirotors%20Transporting%20a%0A%20%20Cable-Suspended%20Payload%20in%20Cluttered%20Environments&entry.906535625=Khaled%20Wahba%20and%20Joaquim%20Ortiz-Haro%20and%20Marc%20Toussaint%20and%20Wolfgang%20H%C3%B6nig&entry.1292438233=%20%20We%20propose%20a%20motion%20planner%20for%20cable-driven%20payload%20transportation%20using%0Amultiple%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20an%20environment%20cluttered%20with%0Aobstacles.%20Our%20planner%20is%20kinodynamic%2C%20i.e.%2C%20it%20considers%20the%20full%20dynamics%0Amodel%20of%20the%20transporting%20system%20including%20actuation%20constraints.%20Due%20to%20the%0Ahigh%20dimensionality%20of%20the%20planning%20problem%2C%20we%20use%20a%20hierarchical%20approach%0Awhere%20we%20first%20solve%20the%20geometric%20motion%20planning%20using%20a%20sampling-based%0Amethod%20with%20a%20novel%20sampler%2C%20followed%20by%20constrained%20trajectory%20optimization%0Athat%20considers%20the%20full%20dynamics%20of%20the%20system.%20Both%20planning%20stages%20consider%0Ainter-robot%20and%20robot/obstacle%20collisions.%20We%20demonstrate%20in%20a%0Asoftware-in-the-loop%20simulation%20and%20real%20flight%20experiments%20that%20there%20is%20a%0Asignificant%20benefit%20in%20kinodynamic%20motion%20planning%20for%20such%20payload%20transport%0Asystems%20with%20respect%20to%20payload%20tracking%20error%20and%20energy%20consumption%20compared%0Ato%20the%20standard%20methods%20of%20planning%20for%20the%20payload%20alone.%20Notably%2C%20we%20observe%0Aa%20significantly%20higher%20success%20rate%20in%20scenarios%20where%20the%20team%20formation%0Achanges%20are%20needed%20to%20move%20through%20tight%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03394v2&entry.124074799=Read"},
{"title": "Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators\n  for Reasoning-Based Chart VQA", "author": "Zhuowan Li and Bhavan Jasani and Peng Tang and Shabnam Ghadar", "abstract": "  Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.\n", "link": "http://arxiv.org/abs/2403.16385v2", "date": "2024-03-28", "relevancy": 1.9866, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5092}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4944}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4939}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Synthesize%20Step-by-Step%3A%20Tools%2C%20Templates%20and%20LLMs%20as%20Data%20Generators%0A%20%20for%20Reasoning-Based%20Chart%20VQA&body=Title%3A%20Synthesize%20Step-by-Step%3A%20Tools%2C%20Templates%20and%20LLMs%20as%20Data%20Generators%0A%20%20for%20Reasoning-Based%20Chart%20VQA%0AAuthor%3A%20Zhuowan%20Li%20and%20Bhavan%20Jasani%20and%20Peng%20Tang%20and%20Shabnam%20Ghadar%0AAbstract%3A%20%20%20Understanding%20data%20visualizations%20like%20charts%20and%20plots%20requires%20reasoning%0Aabout%20both%20visual%20elements%20and%20numerics.%20Although%20strong%20in%20extractive%0Aquestions%2C%20current%20chart%20visual%20question%20answering%20%28chart%20VQA%29%20models%20suffer%20on%0Acomplex%20reasoning%20questions.%20In%20this%20work%2C%20we%20address%20the%20lack%20of%20reasoning%0Aability%20by%20data%20augmentation.%20We%20leverage%20Large%20Language%20Models%20%28LLMs%29%2C%20which%0Ahave%20shown%20to%20have%20strong%20reasoning%20ability%2C%20as%20an%20automatic%20data%20annotator%0Athat%20generates%20question-answer%20annotations%20for%20chart%20images.%20The%20key%20innovation%0Ain%20our%20method%20lies%20in%20the%20Synthesize%20Step-by-Step%20strategy%3A%20our%20LLM-based%20data%0Agenerator%20learns%20to%20decompose%20the%20complex%20question%20into%20step-by-step%0Asub-questions%20%28rationales%29%2C%20which%20are%20then%20used%20to%20derive%20the%20final%20answer%0Ausing%20external%20tools%2C%20i.e.%20Python.%20This%20step-wise%20generation%20procedure%20is%0Atrained%20on%20synthetic%20data%20generated%20using%20a%20template-based%20QA%20generation%0Apipeline.%20Experimental%20results%20highlight%20the%20significance%20of%20the%20proposed%0Astep-by-step%20generation.%20By%20training%20with%20the%20LLM-augmented%20data%20%28LAMENDA%29%2C%20we%0Asignificantly%20enhance%20the%20chart%20VQA%20models%2C%20achieving%20the%20state-of-the-art%0Aaccuracy%20on%20the%20ChartQA%20and%20PlotQA%20datasets.%20In%20particular%2C%20our%20approach%0Aimproves%20the%20accuracy%20of%20the%20previous%20state-of-the-art%20approach%20from%2038%25%20to%2054%25%0Aon%20the%20human-written%20questions%20in%20the%20ChartQA%20dataset%2C%20which%20needs%20strong%0Areasoning.%20We%20hope%20our%20work%20underscores%20the%20potential%20of%20synthetic%20data%20and%0Aencourages%20further%20exploration%20of%20data%20augmentation%20using%20LLMs%20for%0Areasoning-heavy%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16385v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesize%20Step-by-Step%3A%20Tools%2C%20Templates%20and%20LLMs%20as%20Data%20Generators%0A%20%20for%20Reasoning-Based%20Chart%20VQA&entry.906535625=Zhuowan%20Li%20and%20Bhavan%20Jasani%20and%20Peng%20Tang%20and%20Shabnam%20Ghadar&entry.1292438233=%20%20Understanding%20data%20visualizations%20like%20charts%20and%20plots%20requires%20reasoning%0Aabout%20both%20visual%20elements%20and%20numerics.%20Although%20strong%20in%20extractive%0Aquestions%2C%20current%20chart%20visual%20question%20answering%20%28chart%20VQA%29%20models%20suffer%20on%0Acomplex%20reasoning%20questions.%20In%20this%20work%2C%20we%20address%20the%20lack%20of%20reasoning%0Aability%20by%20data%20augmentation.%20We%20leverage%20Large%20Language%20Models%20%28LLMs%29%2C%20which%0Ahave%20shown%20to%20have%20strong%20reasoning%20ability%2C%20as%20an%20automatic%20data%20annotator%0Athat%20generates%20question-answer%20annotations%20for%20chart%20images.%20The%20key%20innovation%0Ain%20our%20method%20lies%20in%20the%20Synthesize%20Step-by-Step%20strategy%3A%20our%20LLM-based%20data%0Agenerator%20learns%20to%20decompose%20the%20complex%20question%20into%20step-by-step%0Asub-questions%20%28rationales%29%2C%20which%20are%20then%20used%20to%20derive%20the%20final%20answer%0Ausing%20external%20tools%2C%20i.e.%20Python.%20This%20step-wise%20generation%20procedure%20is%0Atrained%20on%20synthetic%20data%20generated%20using%20a%20template-based%20QA%20generation%0Apipeline.%20Experimental%20results%20highlight%20the%20significance%20of%20the%20proposed%0Astep-by-step%20generation.%20By%20training%20with%20the%20LLM-augmented%20data%20%28LAMENDA%29%2C%20we%0Asignificantly%20enhance%20the%20chart%20VQA%20models%2C%20achieving%20the%20state-of-the-art%0Aaccuracy%20on%20the%20ChartQA%20and%20PlotQA%20datasets.%20In%20particular%2C%20our%20approach%0Aimproves%20the%20accuracy%20of%20the%20previous%20state-of-the-art%20approach%20from%2038%25%20to%2054%25%0Aon%20the%20human-written%20questions%20in%20the%20ChartQA%20dataset%2C%20which%20needs%20strong%0Areasoning.%20We%20hope%20our%20work%20underscores%20the%20potential%20of%20synthetic%20data%20and%0Aencourages%20further%20exploration%20of%20data%20augmentation%20using%20LLMs%20for%0Areasoning-heavy%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16385v2&entry.124074799=Read"},
{"title": "Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual\n  User Behaviors", "author": "Binzong Geng and Zhaoxin Huan and Xiaolu Zhang and Yong He and Liang Zhang and Fajie Yuan and Jun Zhou and Linjian Mo", "abstract": "  With the rise of large language models (LLMs), recent works have leveraged\nLLMs to improve the performance of click-through rate (CTR) prediction.\nHowever, we argue that a critical obstacle remains in deploying LLMs for\npractical use: the efficiency of LLMs when processing long textual user\nbehaviors. As user sequences grow longer, the current efficiency of LLMs is\ninadequate for training on billions of users and items. To break through the\nefficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical\nEncoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.\nSpecifically, BAHE proposes a novel hierarchical architecture that decouples\nthe encoding of user behaviors from inter-behavior interactions. Firstly, to\nprevent computational redundancy from repeated encoding of identical user\nbehaviors, BAHE employs the LLM's pre-trained shallow layers to extract\nembeddings of the most granular, atomic user behaviors from extensive user\nsequences and stores them in the offline database. Subsequently, the deeper,\ntrainable layers of the LLM facilitate intricate inter-behavior interactions,\nthereby generating comprehensive user embeddings. This separation allows the\nlearning of high-level user representations to be independent of low-level\nbehavior encoding, significantly reducing computational complexity. Finally,\nthese refined user embeddings, in conjunction with correspondingly processed\nitem embeddings, are incorporated into the CTR model to compute the CTR scores.\nExtensive experimental results show that BAHE reduces training time and memory\nby five times for CTR models using LLMs, especially with longer user sequences.\nBAHE has been deployed in a real-world system, allowing for daily updates of 50\nmillion CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR\nprediction.\n", "link": "http://arxiv.org/abs/2403.19347v1", "date": "2024-03-28", "relevancy": 1.9857, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5291}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4613}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Length%20Barrier%3A%20LLM-Enhanced%20CTR%20Prediction%20in%20Long%20Textual%0A%20%20User%20Behaviors&body=Title%3A%20Breaking%20the%20Length%20Barrier%3A%20LLM-Enhanced%20CTR%20Prediction%20in%20Long%20Textual%0A%20%20User%20Behaviors%0AAuthor%3A%20Binzong%20Geng%20and%20Zhaoxin%20Huan%20and%20Xiaolu%20Zhang%20and%20Yong%20He%20and%20Liang%20Zhang%20and%20Fajie%20Yuan%20and%20Jun%20Zhou%20and%20Linjian%20Mo%0AAbstract%3A%20%20%20With%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%2C%20recent%20works%20have%20leveraged%0ALLMs%20to%20improve%20the%20performance%20of%20click-through%20rate%20%28CTR%29%20prediction.%0AHowever%2C%20we%20argue%20that%20a%20critical%20obstacle%20remains%20in%20deploying%20LLMs%20for%0Apractical%20use%3A%20the%20efficiency%20of%20LLMs%20when%20processing%20long%20textual%20user%0Abehaviors.%20As%20user%20sequences%20grow%20longer%2C%20the%20current%20efficiency%20of%20LLMs%20is%0Ainadequate%20for%20training%20on%20billions%20of%20users%20and%20items.%20To%20break%20through%20the%0Aefficiency%20barrier%20of%20LLMs%2C%20we%20propose%20Behavior%20Aggregated%20Hierarchical%0AEncoding%20%28BAHE%29%20to%20enhance%20the%20efficiency%20of%20LLM-based%20CTR%20modeling.%0ASpecifically%2C%20BAHE%20proposes%20a%20novel%20hierarchical%20architecture%20that%20decouples%0Athe%20encoding%20of%20user%20behaviors%20from%20inter-behavior%20interactions.%20Firstly%2C%20to%0Aprevent%20computational%20redundancy%20from%20repeated%20encoding%20of%20identical%20user%0Abehaviors%2C%20BAHE%20employs%20the%20LLM%27s%20pre-trained%20shallow%20layers%20to%20extract%0Aembeddings%20of%20the%20most%20granular%2C%20atomic%20user%20behaviors%20from%20extensive%20user%0Asequences%20and%20stores%20them%20in%20the%20offline%20database.%20Subsequently%2C%20the%20deeper%2C%0Atrainable%20layers%20of%20the%20LLM%20facilitate%20intricate%20inter-behavior%20interactions%2C%0Athereby%20generating%20comprehensive%20user%20embeddings.%20This%20separation%20allows%20the%0Alearning%20of%20high-level%20user%20representations%20to%20be%20independent%20of%20low-level%0Abehavior%20encoding%2C%20significantly%20reducing%20computational%20complexity.%20Finally%2C%0Athese%20refined%20user%20embeddings%2C%20in%20conjunction%20with%20correspondingly%20processed%0Aitem%20embeddings%2C%20are%20incorporated%20into%20the%20CTR%20model%20to%20compute%20the%20CTR%20scores.%0AExtensive%20experimental%20results%20show%20that%20BAHE%20reduces%20training%20time%20and%20memory%0Aby%20five%20times%20for%20CTR%20models%20using%20LLMs%2C%20especially%20with%20longer%20user%20sequences.%0ABAHE%20has%20been%20deployed%20in%20a%20real-world%20system%2C%20allowing%20for%20daily%20updates%20of%2050%0Amillion%20CTR%20data%20on%208%20A100%20GPUs%2C%20making%20LLMs%20practical%20for%20industrial%20CTR%0Aprediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19347v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Length%20Barrier%3A%20LLM-Enhanced%20CTR%20Prediction%20in%20Long%20Textual%0A%20%20User%20Behaviors&entry.906535625=Binzong%20Geng%20and%20Zhaoxin%20Huan%20and%20Xiaolu%20Zhang%20and%20Yong%20He%20and%20Liang%20Zhang%20and%20Fajie%20Yuan%20and%20Jun%20Zhou%20and%20Linjian%20Mo&entry.1292438233=%20%20With%20the%20rise%20of%20large%20language%20models%20%28LLMs%29%2C%20recent%20works%20have%20leveraged%0ALLMs%20to%20improve%20the%20performance%20of%20click-through%20rate%20%28CTR%29%20prediction.%0AHowever%2C%20we%20argue%20that%20a%20critical%20obstacle%20remains%20in%20deploying%20LLMs%20for%0Apractical%20use%3A%20the%20efficiency%20of%20LLMs%20when%20processing%20long%20textual%20user%0Abehaviors.%20As%20user%20sequences%20grow%20longer%2C%20the%20current%20efficiency%20of%20LLMs%20is%0Ainadequate%20for%20training%20on%20billions%20of%20users%20and%20items.%20To%20break%20through%20the%0Aefficiency%20barrier%20of%20LLMs%2C%20we%20propose%20Behavior%20Aggregated%20Hierarchical%0AEncoding%20%28BAHE%29%20to%20enhance%20the%20efficiency%20of%20LLM-based%20CTR%20modeling.%0ASpecifically%2C%20BAHE%20proposes%20a%20novel%20hierarchical%20architecture%20that%20decouples%0Athe%20encoding%20of%20user%20behaviors%20from%20inter-behavior%20interactions.%20Firstly%2C%20to%0Aprevent%20computational%20redundancy%20from%20repeated%20encoding%20of%20identical%20user%0Abehaviors%2C%20BAHE%20employs%20the%20LLM%27s%20pre-trained%20shallow%20layers%20to%20extract%0Aembeddings%20of%20the%20most%20granular%2C%20atomic%20user%20behaviors%20from%20extensive%20user%0Asequences%20and%20stores%20them%20in%20the%20offline%20database.%20Subsequently%2C%20the%20deeper%2C%0Atrainable%20layers%20of%20the%20LLM%20facilitate%20intricate%20inter-behavior%20interactions%2C%0Athereby%20generating%20comprehensive%20user%20embeddings.%20This%20separation%20allows%20the%0Alearning%20of%20high-level%20user%20representations%20to%20be%20independent%20of%20low-level%0Abehavior%20encoding%2C%20significantly%20reducing%20computational%20complexity.%20Finally%2C%0Athese%20refined%20user%20embeddings%2C%20in%20conjunction%20with%20correspondingly%20processed%0Aitem%20embeddings%2C%20are%20incorporated%20into%20the%20CTR%20model%20to%20compute%20the%20CTR%20scores.%0AExtensive%20experimental%20results%20show%20that%20BAHE%20reduces%20training%20time%20and%20memory%0Aby%20five%20times%20for%20CTR%20models%20using%20LLMs%2C%20especially%20with%20longer%20user%20sequences.%0ABAHE%20has%20been%20deployed%20in%20a%20real-world%20system%2C%20allowing%20for%20daily%20updates%20of%2050%0Amillion%20CTR%20data%20on%208%20A100%20GPUs%2C%20making%20LLMs%20practical%20for%20industrial%20CTR%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19347v1&entry.124074799=Read"},
{"title": "Deep decomposition method for the limited aperture inverse obstacle\n  scattering problem", "author": "Yunwen Yin and Liang Yan", "abstract": "  In this paper, we consider a deep learning approach to the limited aperture\ninverse obstacle scattering problem. It is well known that traditional deep\nlearning relies solely on data, which may limit its performance for the inverse\nproblem when only indirect observation data and a physical model are available.\nA fundamental question arises in light of these limitations: is it possible to\nenable deep learning to work on inverse problems without labeled data and to be\naware of what it is learning? This work proposes a deep decomposition method\n(DDM) for such purposes, which does not require ground truth labels. It\naccomplishes this by providing physical operators associated with the\nscattering model to the neural network architecture. Additionally, a deep\nlearning based data completion scheme is implemented in DDM to prevent\ndistorting the solution of the inverse problem for limited aperture data.\nFurthermore, apart from addressing the ill-posedness imposed by the inverse\nproblem itself, DDM is a physics-aware machine learning technique that can have\ninterpretability property. The convergence result of DDM is theoretically\nproven. Numerical experiments are presented to demonstrate the validity of the\nproposed DDM even when the incident and observation apertures are extremely\nlimited.\n", "link": "http://arxiv.org/abs/2403.19470v1", "date": "2024-03-28", "relevancy": 1.9759, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5251}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4886}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4869}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20decomposition%20method%20for%20the%20limited%20aperture%20inverse%20obstacle%0A%20%20scattering%20problem&body=Title%3A%20Deep%20decomposition%20method%20for%20the%20limited%20aperture%20inverse%20obstacle%0A%20%20scattering%20problem%0AAuthor%3A%20Yunwen%20Yin%20and%20Liang%20Yan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20a%20deep%20learning%20approach%20to%20the%20limited%20aperture%0Ainverse%20obstacle%20scattering%20problem.%20It%20is%20well%20known%20that%20traditional%20deep%0Alearning%20relies%20solely%20on%20data%2C%20which%20may%20limit%20its%20performance%20for%20the%20inverse%0Aproblem%20when%20only%20indirect%20observation%20data%20and%20a%20physical%20model%20are%20available.%0AA%20fundamental%20question%20arises%20in%20light%20of%20these%20limitations%3A%20is%20it%20possible%20to%0Aenable%20deep%20learning%20to%20work%20on%20inverse%20problems%20without%20labeled%20data%20and%20to%20be%0Aaware%20of%20what%20it%20is%20learning%3F%20This%20work%20proposes%20a%20deep%20decomposition%20method%0A%28DDM%29%20for%20such%20purposes%2C%20which%20does%20not%20require%20ground%20truth%20labels.%20It%0Aaccomplishes%20this%20by%20providing%20physical%20operators%20associated%20with%20the%0Ascattering%20model%20to%20the%20neural%20network%20architecture.%20Additionally%2C%20a%20deep%0Alearning%20based%20data%20completion%20scheme%20is%20implemented%20in%20DDM%20to%20prevent%0Adistorting%20the%20solution%20of%20the%20inverse%20problem%20for%20limited%20aperture%20data.%0AFurthermore%2C%20apart%20from%20addressing%20the%20ill-posedness%20imposed%20by%20the%20inverse%0Aproblem%20itself%2C%20DDM%20is%20a%20physics-aware%20machine%20learning%20technique%20that%20can%20have%0Ainterpretability%20property.%20The%20convergence%20result%20of%20DDM%20is%20theoretically%0Aproven.%20Numerical%20experiments%20are%20presented%20to%20demonstrate%20the%20validity%20of%20the%0Aproposed%20DDM%20even%20when%20the%20incident%20and%20observation%20apertures%20are%20extremely%0Alimited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19470v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20decomposition%20method%20for%20the%20limited%20aperture%20inverse%20obstacle%0A%20%20scattering%20problem&entry.906535625=Yunwen%20Yin%20and%20Liang%20Yan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20a%20deep%20learning%20approach%20to%20the%20limited%20aperture%0Ainverse%20obstacle%20scattering%20problem.%20It%20is%20well%20known%20that%20traditional%20deep%0Alearning%20relies%20solely%20on%20data%2C%20which%20may%20limit%20its%20performance%20for%20the%20inverse%0Aproblem%20when%20only%20indirect%20observation%20data%20and%20a%20physical%20model%20are%20available.%0AA%20fundamental%20question%20arises%20in%20light%20of%20these%20limitations%3A%20is%20it%20possible%20to%0Aenable%20deep%20learning%20to%20work%20on%20inverse%20problems%20without%20labeled%20data%20and%20to%20be%0Aaware%20of%20what%20it%20is%20learning%3F%20This%20work%20proposes%20a%20deep%20decomposition%20method%0A%28DDM%29%20for%20such%20purposes%2C%20which%20does%20not%20require%20ground%20truth%20labels.%20It%0Aaccomplishes%20this%20by%20providing%20physical%20operators%20associated%20with%20the%0Ascattering%20model%20to%20the%20neural%20network%20architecture.%20Additionally%2C%20a%20deep%0Alearning%20based%20data%20completion%20scheme%20is%20implemented%20in%20DDM%20to%20prevent%0Adistorting%20the%20solution%20of%20the%20inverse%20problem%20for%20limited%20aperture%20data.%0AFurthermore%2C%20apart%20from%20addressing%20the%20ill-posedness%20imposed%20by%20the%20inverse%0Aproblem%20itself%2C%20DDM%20is%20a%20physics-aware%20machine%20learning%20technique%20that%20can%20have%0Ainterpretability%20property.%20The%20convergence%20result%20of%20DDM%20is%20theoretically%0Aproven.%20Numerical%20experiments%20are%20presented%20to%20demonstrate%20the%20validity%20of%20the%0Aproposed%20DDM%20even%20when%20the%20incident%20and%20observation%20apertures%20are%20extremely%0Alimited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19470v1&entry.124074799=Read"},
{"title": "Fine-Tuning Language Models with Reward Learning on Policy", "author": "Hao Lang and Fei Huang and Yongbin Li", "abstract": "  Reinforcement learning from human feedback (RLHF) has emerged as an effective\napproach to aligning large language models (LLMs) to human preferences. RLHF\ncontains three steps, i.e., human preference collecting, reward learning, and\npolicy optimization, which are usually performed serially. Despite its\npopularity, however, (fixed) reward models may suffer from inaccurate\noff-distribution, since policy optimization continuously shifts LLMs' data\ndistribution. Repeatedly collecting new preference data from the latest LLMs\nmay alleviate this issue, which unfortunately makes the resulting system more\ncomplicated and difficult to optimize. In this paper, we propose reward\nlearning on policy (RLP), an unsupervised framework that refines a reward model\nusing policy samples to keep it on-distribution. Specifically, an unsupervised\nmulti-view learning method is introduced to learn robust representations of\npolicy samples. Meanwhile, a synthetic preference generation approach is\ndeveloped to simulate high-quality preference data with policy outputs.\nExtensive experiments on three benchmark datasets show that RLP consistently\noutperforms the state-of-the-art. Our code is available at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.\n", "link": "http://arxiv.org/abs/2403.19279v1", "date": "2024-03-28", "relevancy": 1.9656, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4915}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4888}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fine-Tuning%20Language%20Models%20with%20Reward%20Learning%20on%20Policy&body=Title%3A%20Fine-Tuning%20Language%20Models%20with%20Reward%20Learning%20on%20Policy%0AAuthor%3A%20Hao%20Lang%20and%20Fei%20Huang%20and%20Yongbin%20Li%0AAbstract%3A%20%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20has%20emerged%20as%20an%20effective%0Aapproach%20to%20aligning%20large%20language%20models%20%28LLMs%29%20to%20human%20preferences.%20RLHF%0Acontains%20three%20steps%2C%20i.e.%2C%20human%20preference%20collecting%2C%20reward%20learning%2C%20and%0Apolicy%20optimization%2C%20which%20are%20usually%20performed%20serially.%20Despite%20its%0Apopularity%2C%20however%2C%20%28fixed%29%20reward%20models%20may%20suffer%20from%20inaccurate%0Aoff-distribution%2C%20since%20policy%20optimization%20continuously%20shifts%20LLMs%27%20data%0Adistribution.%20Repeatedly%20collecting%20new%20preference%20data%20from%20the%20latest%20LLMs%0Amay%20alleviate%20this%20issue%2C%20which%20unfortunately%20makes%20the%20resulting%20system%20more%0Acomplicated%20and%20difficult%20to%20optimize.%20In%20this%20paper%2C%20we%20propose%20reward%0Alearning%20on%20policy%20%28RLP%29%2C%20an%20unsupervised%20framework%20that%20refines%20a%20reward%20model%0Ausing%20policy%20samples%20to%20keep%20it%20on-distribution.%20Specifically%2C%20an%20unsupervised%0Amulti-view%20learning%20method%20is%20introduced%20to%20learn%20robust%20representations%20of%0Apolicy%20samples.%20Meanwhile%2C%20a%20synthetic%20preference%20generation%20approach%20is%0Adeveloped%20to%20simulate%20high-quality%20preference%20data%20with%20policy%20outputs.%0AExtensive%20experiments%20on%20three%20benchmark%20datasets%20show%20that%20RLP%20consistently%0Aoutperforms%20the%20state-of-the-art.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19279v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuning%20Language%20Models%20with%20Reward%20Learning%20on%20Policy&entry.906535625=Hao%20Lang%20and%20Fei%20Huang%20and%20Yongbin%20Li&entry.1292438233=%20%20Reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%20has%20emerged%20as%20an%20effective%0Aapproach%20to%20aligning%20large%20language%20models%20%28LLMs%29%20to%20human%20preferences.%20RLHF%0Acontains%20three%20steps%2C%20i.e.%2C%20human%20preference%20collecting%2C%20reward%20learning%2C%20and%0Apolicy%20optimization%2C%20which%20are%20usually%20performed%20serially.%20Despite%20its%0Apopularity%2C%20however%2C%20%28fixed%29%20reward%20models%20may%20suffer%20from%20inaccurate%0Aoff-distribution%2C%20since%20policy%20optimization%20continuously%20shifts%20LLMs%27%20data%0Adistribution.%20Repeatedly%20collecting%20new%20preference%20data%20from%20the%20latest%20LLMs%0Amay%20alleviate%20this%20issue%2C%20which%20unfortunately%20makes%20the%20resulting%20system%20more%0Acomplicated%20and%20difficult%20to%20optimize.%20In%20this%20paper%2C%20we%20propose%20reward%0Alearning%20on%20policy%20%28RLP%29%2C%20an%20unsupervised%20framework%20that%20refines%20a%20reward%20model%0Ausing%20policy%20samples%20to%20keep%20it%20on-distribution.%20Specifically%2C%20an%20unsupervised%0Amulti-view%20learning%20method%20is%20introduced%20to%20learn%20robust%20representations%20of%0Apolicy%20samples.%20Meanwhile%2C%20a%20synthetic%20preference%20generation%20approach%20is%0Adeveloped%20to%20simulate%20high-quality%20preference%20data%20with%20policy%20outputs.%0AExtensive%20experiments%20on%20three%20benchmark%20datasets%20show%20that%20RLP%20consistently%0Aoutperforms%20the%20state-of-the-art.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19279v1&entry.124074799=Read"},
{"title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning", "author": "Rui Pan and Xiang Liu and Shizhe Diao and Renjie Pi and Jipeng Zhang and Chi Han and Tong Zhang", "abstract": "  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n", "link": "http://arxiv.org/abs/2403.17919v2", "date": "2024-03-28", "relevancy": 1.9409, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4867}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LISA%3A%20Layerwise%20Importance%20Sampling%20for%20Memory-Efficient%20Large%20Language%0A%20%20Model%20Fine-Tuning&body=Title%3A%20LISA%3A%20Layerwise%20Importance%20Sampling%20for%20Memory-Efficient%20Large%20Language%0A%20%20Model%20Fine-Tuning%0AAuthor%3A%20Rui%20Pan%20and%20Xiang%20Liu%20and%20Shizhe%20Diao%20and%20Renjie%20Pi%20and%20Jipeng%20Zhang%20and%20Chi%20Han%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20The%20machine%20learning%20community%20has%20witnessed%20impressive%20advancements%20since%0Athe%20first%20appearance%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20their%20huge%20memory%0Aconsumption%20has%20become%20a%20major%20roadblock%20to%20large-scale%20training.%20Parameter%0AEfficient%20Fine-Tuning%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20have%20been%0Aproposed%20to%20alleviate%20this%20problem%2C%20but%20their%20performance%20still%20fails%20to%20match%0Afull%20parameter%20training%20in%20most%20large-scale%20fine-tuning%20settings.%20Attempting%20to%0Acomplement%20this%20deficiency%2C%20we%20investigate%20layerwise%20properties%20of%20LoRA%20on%0Afine-tuning%20tasks%20and%20observe%20an%20uncommon%20skewness%20of%20weight%20norms%20across%0Adifferent%20layers.%20Utilizing%20this%20key%20observation%2C%20a%20surprisingly%20simple%0Atraining%20strategy%20is%20discovered%2C%20which%20outperforms%20both%20LoRA%20and%20full%20parameter%0Atraining%20in%20a%20wide%20range%20of%20settings%20with%20memory%20costs%20as%20low%20as%20LoRA.%20We%20name%0Ait%20Layerwise%20Importance%20Sampled%20AdamW%20%28LISA%29%2C%20a%20promising%20alternative%20for%20LoRA%2C%0Awhich%20applies%20the%20idea%20of%20importance%20sampling%20to%20different%20layers%20in%20LLMs%20and%0Arandomly%20freeze%20most%20middle%20layers%20during%20optimization.%20Experimental%20results%0Ashow%20that%20with%20similar%20or%20less%20GPU%20memory%20consumption%2C%20LISA%20surpasses%20LoRA%20or%0Aeven%20full%20parameter%20tuning%20in%20downstream%20fine-tuning%20tasks%2C%20where%20LISA%0Aconsistently%20outperforms%20LoRA%20by%20over%20%2411%5C%25%24-%2437%5C%25%24%20in%20terms%20of%20MT-Bench%0Ascores.%20On%20large%20models%2C%20specifically%20LLaMA-2-70B%2C%20LISA%20achieves%20on-par%20or%0Abetter%20performance%20than%20LoRA%20on%20MT-Bench%2C%20GSM8K%2C%20and%20PubMedQA%2C%20demonstrating%0Aits%20effectiveness%20across%20different%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17919v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LISA%3A%20Layerwise%20Importance%20Sampling%20for%20Memory-Efficient%20Large%20Language%0A%20%20Model%20Fine-Tuning&entry.906535625=Rui%20Pan%20and%20Xiang%20Liu%20and%20Shizhe%20Diao%20and%20Renjie%20Pi%20and%20Jipeng%20Zhang%20and%20Chi%20Han%20and%20Tong%20Zhang&entry.1292438233=%20%20The%20machine%20learning%20community%20has%20witnessed%20impressive%20advancements%20since%0Athe%20first%20appearance%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20their%20huge%20memory%0Aconsumption%20has%20become%20a%20major%20roadblock%20to%20large-scale%20training.%20Parameter%0AEfficient%20Fine-Tuning%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%20have%20been%0Aproposed%20to%20alleviate%20this%20problem%2C%20but%20their%20performance%20still%20fails%20to%20match%0Afull%20parameter%20training%20in%20most%20large-scale%20fine-tuning%20settings.%20Attempting%20to%0Acomplement%20this%20deficiency%2C%20we%20investigate%20layerwise%20properties%20of%20LoRA%20on%0Afine-tuning%20tasks%20and%20observe%20an%20uncommon%20skewness%20of%20weight%20norms%20across%0Adifferent%20layers.%20Utilizing%20this%20key%20observation%2C%20a%20surprisingly%20simple%0Atraining%20strategy%20is%20discovered%2C%20which%20outperforms%20both%20LoRA%20and%20full%20parameter%0Atraining%20in%20a%20wide%20range%20of%20settings%20with%20memory%20costs%20as%20low%20as%20LoRA.%20We%20name%0Ait%20Layerwise%20Importance%20Sampled%20AdamW%20%28LISA%29%2C%20a%20promising%20alternative%20for%20LoRA%2C%0Awhich%20applies%20the%20idea%20of%20importance%20sampling%20to%20different%20layers%20in%20LLMs%20and%0Arandomly%20freeze%20most%20middle%20layers%20during%20optimization.%20Experimental%20results%0Ashow%20that%20with%20similar%20or%20less%20GPU%20memory%20consumption%2C%20LISA%20surpasses%20LoRA%20or%0Aeven%20full%20parameter%20tuning%20in%20downstream%20fine-tuning%20tasks%2C%20where%20LISA%0Aconsistently%20outperforms%20LoRA%20by%20over%20%2411%5C%25%24-%2437%5C%25%24%20in%20terms%20of%20MT-Bench%0Ascores.%20On%20large%20models%2C%20specifically%20LLaMA-2-70B%2C%20LISA%20achieves%20on-par%20or%0Abetter%20performance%20than%20LoRA%20on%20MT-Bench%2C%20GSM8K%2C%20and%20PubMedQA%2C%20demonstrating%0Aits%20effectiveness%20across%20different%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17919v2&entry.124074799=Read"},
{"title": "Boosting Latent Diffusion with Flow Matching", "author": "Johannes S. Fischer and Ming Gui and Pingchuan Ma and Nick Stracke and Stefan A. Baumann and Bj\u00f6rn Ommer", "abstract": "  Recently, there has been tremendous progress in visual synthesis and the\nunderlying generative models. Here, diffusion models (DMs) stand out\nparticularly, but lately, flow matching (FM) has also garnered considerable\ninterest. While DMs excel in providing diverse images, they suffer from long\ntraining and slow generation. With latent diffusion, these issues are only\npartially alleviated. Conversely, FM offers faster training and inference but\nexhibits less diversity in synthesis. We demonstrate that introducing FM\nbetween the Diffusion model and the convolutional decoder offers\nhigh-resolution image synthesis with reduced computational cost and model size.\nDiffusion can then efficiently provide the necessary generation diversity. FM\ncompensates for the lower resolution, mapping the small latent space to a\nhigh-dimensional one. Subsequently, the convolutional decoder of the LDM maps\nthese latents to high-resolution images. By combining the diversity of DMs, the\nefficiency of FMs, and the effectiveness of convolutional decoders, we achieve\nstate-of-the-art high-resolution image synthesis at $1024^2$ with minimal\ncomputational cost. Importantly, our approach is orthogonal to recent\napproximation and speed-up strategies for the underlying DMs, making it easily\nintegrable into various DM frameworks.\n", "link": "http://arxiv.org/abs/2312.07360v2", "date": "2024-03-28", "relevancy": 1.9388, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7022}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6883}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6071}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Boosting%20Latent%20Diffusion%20with%20Flow%20Matching&body=Title%3A%20Boosting%20Latent%20Diffusion%20with%20Flow%20Matching%0AAuthor%3A%20Johannes%20S.%20Fischer%20and%20Ming%20Gui%20and%20Pingchuan%20Ma%20and%20Nick%20Stracke%20and%20Stefan%20A.%20Baumann%20and%20Bj%C3%B6rn%20Ommer%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20tremendous%20progress%20in%20visual%20synthesis%20and%20the%0Aunderlying%20generative%20models.%20Here%2C%20diffusion%20models%20%28DMs%29%20stand%20out%0Aparticularly%2C%20but%20lately%2C%20flow%20matching%20%28FM%29%20has%20also%20garnered%20considerable%0Ainterest.%20While%20DMs%20excel%20in%20providing%20diverse%20images%2C%20they%20suffer%20from%20long%0Atraining%20and%20slow%20generation.%20With%20latent%20diffusion%2C%20these%20issues%20are%20only%0Apartially%20alleviated.%20Conversely%2C%20FM%20offers%20faster%20training%20and%20inference%20but%0Aexhibits%20less%20diversity%20in%20synthesis.%20We%20demonstrate%20that%20introducing%20FM%0Abetween%20the%20Diffusion%20model%20and%20the%20convolutional%20decoder%20offers%0Ahigh-resolution%20image%20synthesis%20with%20reduced%20computational%20cost%20and%20model%20size.%0ADiffusion%20can%20then%20efficiently%20provide%20the%20necessary%20generation%20diversity.%20FM%0Acompensates%20for%20the%20lower%20resolution%2C%20mapping%20the%20small%20latent%20space%20to%20a%0Ahigh-dimensional%20one.%20Subsequently%2C%20the%20convolutional%20decoder%20of%20the%20LDM%20maps%0Athese%20latents%20to%20high-resolution%20images.%20By%20combining%20the%20diversity%20of%20DMs%2C%20the%0Aefficiency%20of%20FMs%2C%20and%20the%20effectiveness%20of%20convolutional%20decoders%2C%20we%20achieve%0Astate-of-the-art%20high-resolution%20image%20synthesis%20at%20%241024%5E2%24%20with%20minimal%0Acomputational%20cost.%20Importantly%2C%20our%20approach%20is%20orthogonal%20to%20recent%0Aapproximation%20and%20speed-up%20strategies%20for%20the%20underlying%20DMs%2C%20making%20it%20easily%0Aintegrable%20into%20various%20DM%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07360v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Latent%20Diffusion%20with%20Flow%20Matching&entry.906535625=Johannes%20S.%20Fischer%20and%20Ming%20Gui%20and%20Pingchuan%20Ma%20and%20Nick%20Stracke%20and%20Stefan%20A.%20Baumann%20and%20Bj%C3%B6rn%20Ommer&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20tremendous%20progress%20in%20visual%20synthesis%20and%20the%0Aunderlying%20generative%20models.%20Here%2C%20diffusion%20models%20%28DMs%29%20stand%20out%0Aparticularly%2C%20but%20lately%2C%20flow%20matching%20%28FM%29%20has%20also%20garnered%20considerable%0Ainterest.%20While%20DMs%20excel%20in%20providing%20diverse%20images%2C%20they%20suffer%20from%20long%0Atraining%20and%20slow%20generation.%20With%20latent%20diffusion%2C%20these%20issues%20are%20only%0Apartially%20alleviated.%20Conversely%2C%20FM%20offers%20faster%20training%20and%20inference%20but%0Aexhibits%20less%20diversity%20in%20synthesis.%20We%20demonstrate%20that%20introducing%20FM%0Abetween%20the%20Diffusion%20model%20and%20the%20convolutional%20decoder%20offers%0Ahigh-resolution%20image%20synthesis%20with%20reduced%20computational%20cost%20and%20model%20size.%0ADiffusion%20can%20then%20efficiently%20provide%20the%20necessary%20generation%20diversity.%20FM%0Acompensates%20for%20the%20lower%20resolution%2C%20mapping%20the%20small%20latent%20space%20to%20a%0Ahigh-dimensional%20one.%20Subsequently%2C%20the%20convolutional%20decoder%20of%20the%20LDM%20maps%0Athese%20latents%20to%20high-resolution%20images.%20By%20combining%20the%20diversity%20of%20DMs%2C%20the%0Aefficiency%20of%20FMs%2C%20and%20the%20effectiveness%20of%20convolutional%20decoders%2C%20we%20achieve%0Astate-of-the-art%20high-resolution%20image%20synthesis%20at%20%241024%5E2%24%20with%20minimal%0Acomputational%20cost.%20Importantly%2C%20our%20approach%20is%20orthogonal%20to%20recent%0Aapproximation%20and%20speed-up%20strategies%20for%20the%20underlying%20DMs%2C%20making%20it%20easily%0Aintegrable%20into%20various%20DM%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07360v2&entry.124074799=Read"},
{"title": "sDPO: Don't Use Your Data All at Once", "author": "Dahyun Kim and Yungi Kim and Wonho Song and Hyeonwoo Kim and Yunsu Kim and Sanghoon Kim and Chanjun Park", "abstract": "  As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.\n", "link": "http://arxiv.org/abs/2403.19270v1", "date": "2024-03-28", "relevancy": 1.9338, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.494}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4718}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20sDPO%3A%20Don%27t%20Use%20Your%20Data%20All%20at%20Once&body=Title%3A%20sDPO%3A%20Don%27t%20Use%20Your%20Data%20All%20at%20Once%0AAuthor%3A%20Dahyun%20Kim%20and%20Yungi%20Kim%20and%20Wonho%20Song%20and%20Hyeonwoo%20Kim%20and%20Yunsu%20Kim%20and%20Sanghoon%20Kim%20and%20Chanjun%20Park%0AAbstract%3A%20%20%20As%20development%20of%20large%20language%20models%20%28LLM%29%20progresses%2C%20aligning%20them%20with%0Ahuman%20preferences%20has%20become%20increasingly%20important.%20We%20propose%20stepwise%20DPO%0A%28sDPO%29%2C%20an%20extension%20of%20the%20recently%20popularized%20direct%20preference%20optimization%0A%28DPO%29%20for%20alignment%20tuning.%20This%20approach%20involves%20dividing%20the%20available%0Apreference%20datasets%20and%20utilizing%20them%20in%20a%20stepwise%20manner%2C%20rather%20than%0Aemploying%20it%20all%20at%20once.%20We%20demonstrate%20that%20this%20method%20facilitates%20the%20use%0Aof%20more%20precisely%20aligned%20reference%20models%20within%20the%20DPO%20training%20framework.%0AFurthermore%2C%20sDPO%20trains%20the%20final%20model%20to%20be%20more%20performant%2C%20even%0Aoutperforming%20other%20popular%20LLMs%20with%20more%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19270v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=sDPO%3A%20Don%27t%20Use%20Your%20Data%20All%20at%20Once&entry.906535625=Dahyun%20Kim%20and%20Yungi%20Kim%20and%20Wonho%20Song%20and%20Hyeonwoo%20Kim%20and%20Yunsu%20Kim%20and%20Sanghoon%20Kim%20and%20Chanjun%20Park&entry.1292438233=%20%20As%20development%20of%20large%20language%20models%20%28LLM%29%20progresses%2C%20aligning%20them%20with%0Ahuman%20preferences%20has%20become%20increasingly%20important.%20We%20propose%20stepwise%20DPO%0A%28sDPO%29%2C%20an%20extension%20of%20the%20recently%20popularized%20direct%20preference%20optimization%0A%28DPO%29%20for%20alignment%20tuning.%20This%20approach%20involves%20dividing%20the%20available%0Apreference%20datasets%20and%20utilizing%20them%20in%20a%20stepwise%20manner%2C%20rather%20than%0Aemploying%20it%20all%20at%20once.%20We%20demonstrate%20that%20this%20method%20facilitates%20the%20use%0Aof%20more%20precisely%20aligned%20reference%20models%20within%20the%20DPO%20training%20framework.%0AFurthermore%2C%20sDPO%20trains%20the%20final%20model%20to%20be%20more%20performant%2C%20even%0Aoutperforming%20other%20popular%20LLMs%20with%20more%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19270v1&entry.124074799=Read"},
{"title": "GraspXL: Generating Grasping Motions for Diverse Objects at Scale", "author": "Hui Zhang and Sammy Christen and Zicong Fan and Otmar Hilliges and Jie Song", "abstract": "  Human hands possess the dexterity to interact with diverse objects such as\ngrasping specific parts of the objects and/or approaching them from desired\ndirections. More importantly, humans can grasp objects of any shape without\nobject-specific skills. Recent works synthesize grasping motions following\nsingle objectives such as a desired approach heading direction or a grasping\narea. Moreover, they usually rely on expensive 3D hand-object data during\ntraining and inference, which limits their capability to synthesize grasping\nmotions for unseen objects at scale. In this paper, we unify the generation of\nhand-object grasping motions across multiple motion objectives, diverse object\nshapes and dexterous hand morphologies in a policy learning framework GraspXL.\nThe objectives are composed of the graspable area, heading direction during\napproach, wrist rotation, and hand position. Without requiring any 3D\nhand-object interaction data, our policy trained with 58 objects can robustly\nsynthesize diverse grasping motions for more than 500k unseen objects with a\nsuccess rate of 82.2%. At the same time, the policy adheres to objectives,\nwhich enables the generation of diverse grasps per object. Moreover, we show\nthat our framework can be deployed to different dexterous hands and work with\nreconstructed or generated objects. We quantitatively and qualitatively\nevaluate our method to show the efficacy of our approach. Our model and code\nwill be available.\n", "link": "http://arxiv.org/abs/2403.19649v1", "date": "2024-03-28", "relevancy": 1.9331, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7241}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.574}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5154}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GraspXL%3A%20Generating%20Grasping%20Motions%20for%20Diverse%20Objects%20at%20Scale&body=Title%3A%20GraspXL%3A%20Generating%20Grasping%20Motions%20for%20Diverse%20Objects%20at%20Scale%0AAuthor%3A%20Hui%20Zhang%20and%20Sammy%20Christen%20and%20Zicong%20Fan%20and%20Otmar%20Hilliges%20and%20Jie%20Song%0AAbstract%3A%20%20%20Human%20hands%20possess%20the%20dexterity%20to%20interact%20with%20diverse%20objects%20such%20as%0Agrasping%20specific%20parts%20of%20the%20objects%20and/or%20approaching%20them%20from%20desired%0Adirections.%20More%20importantly%2C%20humans%20can%20grasp%20objects%20of%20any%20shape%20without%0Aobject-specific%20skills.%20Recent%20works%20synthesize%20grasping%20motions%20following%0Asingle%20objectives%20such%20as%20a%20desired%20approach%20heading%20direction%20or%20a%20grasping%0Aarea.%20Moreover%2C%20they%20usually%20rely%20on%20expensive%203D%20hand-object%20data%20during%0Atraining%20and%20inference%2C%20which%20limits%20their%20capability%20to%20synthesize%20grasping%0Amotions%20for%20unseen%20objects%20at%20scale.%20In%20this%20paper%2C%20we%20unify%20the%20generation%20of%0Ahand-object%20grasping%20motions%20across%20multiple%20motion%20objectives%2C%20diverse%20object%0Ashapes%20and%20dexterous%20hand%20morphologies%20in%20a%20policy%20learning%20framework%20GraspXL.%0AThe%20objectives%20are%20composed%20of%20the%20graspable%20area%2C%20heading%20direction%20during%0Aapproach%2C%20wrist%20rotation%2C%20and%20hand%20position.%20Without%20requiring%20any%203D%0Ahand-object%20interaction%20data%2C%20our%20policy%20trained%20with%2058%20objects%20can%20robustly%0Asynthesize%20diverse%20grasping%20motions%20for%20more%20than%20500k%20unseen%20objects%20with%20a%0Asuccess%20rate%20of%2082.2%25.%20At%20the%20same%20time%2C%20the%20policy%20adheres%20to%20objectives%2C%0Awhich%20enables%20the%20generation%20of%20diverse%20grasps%20per%20object.%20Moreover%2C%20we%20show%0Athat%20our%20framework%20can%20be%20deployed%20to%20different%20dexterous%20hands%20and%20work%20with%0Areconstructed%20or%20generated%20objects.%20We%20quantitatively%20and%20qualitatively%0Aevaluate%20our%20method%20to%20show%20the%20efficacy%20of%20our%20approach.%20Our%20model%20and%20code%0Awill%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19649v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraspXL%3A%20Generating%20Grasping%20Motions%20for%20Diverse%20Objects%20at%20Scale&entry.906535625=Hui%20Zhang%20and%20Sammy%20Christen%20and%20Zicong%20Fan%20and%20Otmar%20Hilliges%20and%20Jie%20Song&entry.1292438233=%20%20Human%20hands%20possess%20the%20dexterity%20to%20interact%20with%20diverse%20objects%20such%20as%0Agrasping%20specific%20parts%20of%20the%20objects%20and/or%20approaching%20them%20from%20desired%0Adirections.%20More%20importantly%2C%20humans%20can%20grasp%20objects%20of%20any%20shape%20without%0Aobject-specific%20skills.%20Recent%20works%20synthesize%20grasping%20motions%20following%0Asingle%20objectives%20such%20as%20a%20desired%20approach%20heading%20direction%20or%20a%20grasping%0Aarea.%20Moreover%2C%20they%20usually%20rely%20on%20expensive%203D%20hand-object%20data%20during%0Atraining%20and%20inference%2C%20which%20limits%20their%20capability%20to%20synthesize%20grasping%0Amotions%20for%20unseen%20objects%20at%20scale.%20In%20this%20paper%2C%20we%20unify%20the%20generation%20of%0Ahand-object%20grasping%20motions%20across%20multiple%20motion%20objectives%2C%20diverse%20object%0Ashapes%20and%20dexterous%20hand%20morphologies%20in%20a%20policy%20learning%20framework%20GraspXL.%0AThe%20objectives%20are%20composed%20of%20the%20graspable%20area%2C%20heading%20direction%20during%0Aapproach%2C%20wrist%20rotation%2C%20and%20hand%20position.%20Without%20requiring%20any%203D%0Ahand-object%20interaction%20data%2C%20our%20policy%20trained%20with%2058%20objects%20can%20robustly%0Asynthesize%20diverse%20grasping%20motions%20for%20more%20than%20500k%20unseen%20objects%20with%20a%0Asuccess%20rate%20of%2082.2%25.%20At%20the%20same%20time%2C%20the%20policy%20adheres%20to%20objectives%2C%0Awhich%20enables%20the%20generation%20of%20diverse%20grasps%20per%20object.%20Moreover%2C%20we%20show%0Athat%20our%20framework%20can%20be%20deployed%20to%20different%20dexterous%20hands%20and%20work%20with%0Areconstructed%20or%20generated%20objects.%20We%20quantitatively%20and%20qualitatively%0Aevaluate%20our%20method%20to%20show%20the%20efficacy%20of%20our%20approach.%20Our%20model%20and%20code%0Awill%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19649v1&entry.124074799=Read"},
{"title": "Towards Generalizable Tumor Synthesis", "author": "Qi Chen and Xiaoxi Chen and Haorui Song and Zhiwei Xiong and Alan Yuille and Chen Wei and Zongwei Zhou", "abstract": "  Tumor synthesis enables the creation of artificial tumors in medical images,\nfacilitating the training of AI models for tumor detection and segmentation.\nHowever, success in tumor synthesis hinges on creating visually realistic\ntumors that are generalizable across multiple organs and, furthermore, the\nresulting AI models being capable of detecting real tumors in images sourced\nfrom different domains (e.g., hospitals). This paper made a progressive stride\ntoward generalizable tumor synthesis by leveraging a critical observation:\nearly-stage tumors (< 2cm) tend to have similar imaging characteristics in\ncomputed tomography (CT), whether they originate in the liver, pancreas, or\nkidneys. We have ascertained that generative AI models, e.g., Diffusion Models,\ncan create realistic tumors generalized to a range of organs even when trained\non a limited number of tumor examples from only one organ. Moreover, we have\nshown that AI models trained on these synthetic tumors can be generalized to\ndetect and segment real tumors from CT volumes, encompassing a broad spectrum\nof patient demographics, imaging protocols, and healthcare facilities.\n", "link": "http://arxiv.org/abs/2402.19470v2", "date": "2024-03-28", "relevancy": 1.9299, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5074}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4782}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4768}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalizable%20Tumor%20Synthesis&body=Title%3A%20Towards%20Generalizable%20Tumor%20Synthesis%0AAuthor%3A%20Qi%20Chen%20and%20Xiaoxi%20Chen%20and%20Haorui%20Song%20and%20Zhiwei%20Xiong%20and%20Alan%20Yuille%20and%20Chen%20Wei%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20Tumor%20synthesis%20enables%20the%20creation%20of%20artificial%20tumors%20in%20medical%20images%2C%0Afacilitating%20the%20training%20of%20AI%20models%20for%20tumor%20detection%20and%20segmentation.%0AHowever%2C%20success%20in%20tumor%20synthesis%20hinges%20on%20creating%20visually%20realistic%0Atumors%20that%20are%20generalizable%20across%20multiple%20organs%20and%2C%20furthermore%2C%20the%0Aresulting%20AI%20models%20being%20capable%20of%20detecting%20real%20tumors%20in%20images%20sourced%0Afrom%20different%20domains%20%28e.g.%2C%20hospitals%29.%20This%20paper%20made%20a%20progressive%20stride%0Atoward%20generalizable%20tumor%20synthesis%20by%20leveraging%20a%20critical%20observation%3A%0Aearly-stage%20tumors%20%28%3C%202cm%29%20tend%20to%20have%20similar%20imaging%20characteristics%20in%0Acomputed%20tomography%20%28CT%29%2C%20whether%20they%20originate%20in%20the%20liver%2C%20pancreas%2C%20or%0Akidneys.%20We%20have%20ascertained%20that%20generative%20AI%20models%2C%20e.g.%2C%20Diffusion%20Models%2C%0Acan%20create%20realistic%20tumors%20generalized%20to%20a%20range%20of%20organs%20even%20when%20trained%0Aon%20a%20limited%20number%20of%20tumor%20examples%20from%20only%20one%20organ.%20Moreover%2C%20we%20have%0Ashown%20that%20AI%20models%20trained%20on%20these%20synthetic%20tumors%20can%20be%20generalized%20to%0Adetect%20and%20segment%20real%20tumors%20from%20CT%20volumes%2C%20encompassing%20a%20broad%20spectrum%0Aof%20patient%20demographics%2C%20imaging%20protocols%2C%20and%20healthcare%20facilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19470v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalizable%20Tumor%20Synthesis&entry.906535625=Qi%20Chen%20and%20Xiaoxi%20Chen%20and%20Haorui%20Song%20and%20Zhiwei%20Xiong%20and%20Alan%20Yuille%20and%20Chen%20Wei%20and%20Zongwei%20Zhou&entry.1292438233=%20%20Tumor%20synthesis%20enables%20the%20creation%20of%20artificial%20tumors%20in%20medical%20images%2C%0Afacilitating%20the%20training%20of%20AI%20models%20for%20tumor%20detection%20and%20segmentation.%0AHowever%2C%20success%20in%20tumor%20synthesis%20hinges%20on%20creating%20visually%20realistic%0Atumors%20that%20are%20generalizable%20across%20multiple%20organs%20and%2C%20furthermore%2C%20the%0Aresulting%20AI%20models%20being%20capable%20of%20detecting%20real%20tumors%20in%20images%20sourced%0Afrom%20different%20domains%20%28e.g.%2C%20hospitals%29.%20This%20paper%20made%20a%20progressive%20stride%0Atoward%20generalizable%20tumor%20synthesis%20by%20leveraging%20a%20critical%20observation%3A%0Aearly-stage%20tumors%20%28%3C%202cm%29%20tend%20to%20have%20similar%20imaging%20characteristics%20in%0Acomputed%20tomography%20%28CT%29%2C%20whether%20they%20originate%20in%20the%20liver%2C%20pancreas%2C%20or%0Akidneys.%20We%20have%20ascertained%20that%20generative%20AI%20models%2C%20e.g.%2C%20Diffusion%20Models%2C%0Acan%20create%20realistic%20tumors%20generalized%20to%20a%20range%20of%20organs%20even%20when%20trained%0Aon%20a%20limited%20number%20of%20tumor%20examples%20from%20only%20one%20organ.%20Moreover%2C%20we%20have%0Ashown%20that%20AI%20models%20trained%20on%20these%20synthetic%20tumors%20can%20be%20generalized%20to%0Adetect%20and%20segment%20real%20tumors%20from%20CT%20volumes%2C%20encompassing%20a%20broad%20spectrum%0Aof%20patient%20demographics%2C%20imaging%20protocols%2C%20and%20healthcare%20facilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19470v2&entry.124074799=Read"},
{"title": "NeuroLGP-SM: A Surrogate-assisted Neuroevolution Approach using Linear\n  Genetic Programming", "author": "Fergal Stapleton and Brendan Cody-Kenny and Edgar Galv\u00e1n", "abstract": "  Evolutionary algorithms are increasingly recognised as a viable computational\napproach for the automated optimisation of deep neural networks (DNNs) within\nartificial intelligence. This method extends to the training of DNNs, an\napproach known as neuroevolution. However, neuroevolution is an inherently\nresource-intensive process, with certain studies reporting the consumption of\nthousands of GPU days for refining and training a single DNN network. To\naddress the computational challenges associated with neuroevolution while still\nattaining good DNN accuracy, surrogate models emerge as a pragmatic solution.\nDespite their potential, the integration of surrogate models into\nneuroevolution is still in its early stages, hindered by factors such as the\neffective use of high-dimensional data and the representation employed in\nneuroevolution. In this context, we address these challenges by employing a\nsuitable representation based on Linear Genetic Programming, denoted as\nNeuroLGP, and leveraging Kriging Partial Least Squares. The amalgamation of\nthese two techniques culminates in our proposed methodology known as the\nNeuroLGP-Surrogate Model (NeuroLGP-SM). For comparison purposes, we also code\nand use a baseline approach incorporating a repair mechanism, a common practice\nin neuroevolution. Notably, the baseline approach surpasses the renowned VGG-16\nmodel in accuracy. Given the computational intensity inherent in DNN\noperations, a singular run is typically the norm. To evaluate the efficacy of\nour proposed approach, we conducted 96 independent runs. Significantly, our\nmethodologies consistently outperform the baseline, with the SM model\ndemonstrating superior accuracy or comparable results to the NeuroLGP approach.\nNoteworthy is the additional advantage that the SM approach exhibits a 25%\nreduction in computational requirements, further emphasising its efficiency for\nneuroevolution.\n", "link": "http://arxiv.org/abs/2403.19459v1", "date": "2024-03-28", "relevancy": 1.922, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4974}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4804}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4636}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NeuroLGP-SM%3A%20A%20Surrogate-assisted%20Neuroevolution%20Approach%20using%20Linear%0A%20%20Genetic%20Programming&body=Title%3A%20NeuroLGP-SM%3A%20A%20Surrogate-assisted%20Neuroevolution%20Approach%20using%20Linear%0A%20%20Genetic%20Programming%0AAuthor%3A%20Fergal%20Stapleton%20and%20Brendan%20Cody-Kenny%20and%20Edgar%20Galv%C3%A1n%0AAbstract%3A%20%20%20Evolutionary%20algorithms%20are%20increasingly%20recognised%20as%20a%20viable%20computational%0Aapproach%20for%20the%20automated%20optimisation%20of%20deep%20neural%20networks%20%28DNNs%29%20within%0Aartificial%20intelligence.%20This%20method%20extends%20to%20the%20training%20of%20DNNs%2C%20an%0Aapproach%20known%20as%20neuroevolution.%20However%2C%20neuroevolution%20is%20an%20inherently%0Aresource-intensive%20process%2C%20with%20certain%20studies%20reporting%20the%20consumption%20of%0Athousands%20of%20GPU%20days%20for%20refining%20and%20training%20a%20single%20DNN%20network.%20To%0Aaddress%20the%20computational%20challenges%20associated%20with%20neuroevolution%20while%20still%0Aattaining%20good%20DNN%20accuracy%2C%20surrogate%20models%20emerge%20as%20a%20pragmatic%20solution.%0ADespite%20their%20potential%2C%20the%20integration%20of%20surrogate%20models%20into%0Aneuroevolution%20is%20still%20in%20its%20early%20stages%2C%20hindered%20by%20factors%20such%20as%20the%0Aeffective%20use%20of%20high-dimensional%20data%20and%20the%20representation%20employed%20in%0Aneuroevolution.%20In%20this%20context%2C%20we%20address%20these%20challenges%20by%20employing%20a%0Asuitable%20representation%20based%20on%20Linear%20Genetic%20Programming%2C%20denoted%20as%0ANeuroLGP%2C%20and%20leveraging%20Kriging%20Partial%20Least%20Squares.%20The%20amalgamation%20of%0Athese%20two%20techniques%20culminates%20in%20our%20proposed%20methodology%20known%20as%20the%0ANeuroLGP-Surrogate%20Model%20%28NeuroLGP-SM%29.%20For%20comparison%20purposes%2C%20we%20also%20code%0Aand%20use%20a%20baseline%20approach%20incorporating%20a%20repair%20mechanism%2C%20a%20common%20practice%0Ain%20neuroevolution.%20Notably%2C%20the%20baseline%20approach%20surpasses%20the%20renowned%20VGG-16%0Amodel%20in%20accuracy.%20Given%20the%20computational%20intensity%20inherent%20in%20DNN%0Aoperations%2C%20a%20singular%20run%20is%20typically%20the%20norm.%20To%20evaluate%20the%20efficacy%20of%0Aour%20proposed%20approach%2C%20we%20conducted%2096%20independent%20runs.%20Significantly%2C%20our%0Amethodologies%20consistently%20outperform%20the%20baseline%2C%20with%20the%20SM%20model%0Ademonstrating%20superior%20accuracy%20or%20comparable%20results%20to%20the%20NeuroLGP%20approach.%0ANoteworthy%20is%20the%20additional%20advantage%20that%20the%20SM%20approach%20exhibits%20a%2025%25%0Areduction%20in%20computational%20requirements%2C%20further%20emphasising%20its%20efficiency%20for%0Aneuroevolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19459v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroLGP-SM%3A%20A%20Surrogate-assisted%20Neuroevolution%20Approach%20using%20Linear%0A%20%20Genetic%20Programming&entry.906535625=Fergal%20Stapleton%20and%20Brendan%20Cody-Kenny%20and%20Edgar%20Galv%C3%A1n&entry.1292438233=%20%20Evolutionary%20algorithms%20are%20increasingly%20recognised%20as%20a%20viable%20computational%0Aapproach%20for%20the%20automated%20optimisation%20of%20deep%20neural%20networks%20%28DNNs%29%20within%0Aartificial%20intelligence.%20This%20method%20extends%20to%20the%20training%20of%20DNNs%2C%20an%0Aapproach%20known%20as%20neuroevolution.%20However%2C%20neuroevolution%20is%20an%20inherently%0Aresource-intensive%20process%2C%20with%20certain%20studies%20reporting%20the%20consumption%20of%0Athousands%20of%20GPU%20days%20for%20refining%20and%20training%20a%20single%20DNN%20network.%20To%0Aaddress%20the%20computational%20challenges%20associated%20with%20neuroevolution%20while%20still%0Aattaining%20good%20DNN%20accuracy%2C%20surrogate%20models%20emerge%20as%20a%20pragmatic%20solution.%0ADespite%20their%20potential%2C%20the%20integration%20of%20surrogate%20models%20into%0Aneuroevolution%20is%20still%20in%20its%20early%20stages%2C%20hindered%20by%20factors%20such%20as%20the%0Aeffective%20use%20of%20high-dimensional%20data%20and%20the%20representation%20employed%20in%0Aneuroevolution.%20In%20this%20context%2C%20we%20address%20these%20challenges%20by%20employing%20a%0Asuitable%20representation%20based%20on%20Linear%20Genetic%20Programming%2C%20denoted%20as%0ANeuroLGP%2C%20and%20leveraging%20Kriging%20Partial%20Least%20Squares.%20The%20amalgamation%20of%0Athese%20two%20techniques%20culminates%20in%20our%20proposed%20methodology%20known%20as%20the%0ANeuroLGP-Surrogate%20Model%20%28NeuroLGP-SM%29.%20For%20comparison%20purposes%2C%20we%20also%20code%0Aand%20use%20a%20baseline%20approach%20incorporating%20a%20repair%20mechanism%2C%20a%20common%20practice%0Ain%20neuroevolution.%20Notably%2C%20the%20baseline%20approach%20surpasses%20the%20renowned%20VGG-16%0Amodel%20in%20accuracy.%20Given%20the%20computational%20intensity%20inherent%20in%20DNN%0Aoperations%2C%20a%20singular%20run%20is%20typically%20the%20norm.%20To%20evaluate%20the%20efficacy%20of%0Aour%20proposed%20approach%2C%20we%20conducted%2096%20independent%20runs.%20Significantly%2C%20our%0Amethodologies%20consistently%20outperform%20the%20baseline%2C%20with%20the%20SM%20model%0Ademonstrating%20superior%20accuracy%20or%20comparable%20results%20to%20the%20NeuroLGP%20approach.%0ANoteworthy%20is%20the%20additional%20advantage%20that%20the%20SM%20approach%20exhibits%20a%2025%25%0Areduction%20in%20computational%20requirements%2C%20further%20emphasising%20its%20efficiency%20for%0Aneuroevolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19459v1&entry.124074799=Read"},
{"title": "Parameter Efficient Fine-tuning via Cross Block Orchestration for\n  Segment Anything Model", "author": "Zelin Peng and Zhengqin Xu and Zhilin Zeng and Lingxi Xie and Qi Tian and Wei Shen", "abstract": "  Parameter-efficient fine-tuning (PEFT) is an effective methodology to unleash\nthe potential of large foundation models in novel scenarios with limited\ntraining data. In the computer vision community, PEFT has shown effectiveness\nin image classification, but little research has studied its ability for image\nsegmentation. Fine-tuning segmentation models usually require a heavier\nadjustment of parameters to align the proper projection directions in the\nparameter space for new scenarios. This raises a challenge to existing PEFT\nalgorithms, as they often inject a limited number of individual parameters into\neach block, which prevents substantial adjustment of the projection direction\nof the parameter space due to the limitation of Hidden Markov Chain along\nblocks. In this paper, we equip PEFT with a cross-block orchestration mechanism\nto enable the adaptation of the Segment Anything Model (SAM) to various\ndownstream scenarios. We introduce a novel inter-block communication module,\nwhich integrates a learnable relation matrix to facilitate communication among\ndifferent coefficient sets of each PEFT block's parameter space. Moreover, we\npropose an intra-block enhancement module, which introduces a linear projection\nhead whose weights are generated from a hyper-complex layer, further enhancing\nthe impact of the adjustment of projection directions on the entire parameter\nspace. Extensive experiments on diverse benchmarks demonstrate that our\nproposed approach consistently improves the segmentation performance\nsignificantly on novel scenarios with only around 1K additional parameters.\n", "link": "http://arxiv.org/abs/2311.17112v2", "date": "2024-03-28", "relevancy": 1.9193, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4817}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4794}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4763}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Parameter%20Efficient%20Fine-tuning%20via%20Cross%20Block%20Orchestration%20for%0A%20%20Segment%20Anything%20Model&body=Title%3A%20Parameter%20Efficient%20Fine-tuning%20via%20Cross%20Block%20Orchestration%20for%0A%20%20Segment%20Anything%20Model%0AAuthor%3A%20Zelin%20Peng%20and%20Zhengqin%20Xu%20and%20Zhilin%20Zeng%20and%20Lingxi%20Xie%20and%20Qi%20Tian%20and%20Wei%20Shen%0AAbstract%3A%20%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20is%20an%20effective%20methodology%20to%20unleash%0Athe%20potential%20of%20large%20foundation%20models%20in%20novel%20scenarios%20with%20limited%0Atraining%20data.%20In%20the%20computer%20vision%20community%2C%20PEFT%20has%20shown%20effectiveness%0Ain%20image%20classification%2C%20but%20little%20research%20has%20studied%20its%20ability%20for%20image%0Asegmentation.%20Fine-tuning%20segmentation%20models%20usually%20require%20a%20heavier%0Aadjustment%20of%20parameters%20to%20align%20the%20proper%20projection%20directions%20in%20the%0Aparameter%20space%20for%20new%20scenarios.%20This%20raises%20a%20challenge%20to%20existing%20PEFT%0Aalgorithms%2C%20as%20they%20often%20inject%20a%20limited%20number%20of%20individual%20parameters%20into%0Aeach%20block%2C%20which%20prevents%20substantial%20adjustment%20of%20the%20projection%20direction%0Aof%20the%20parameter%20space%20due%20to%20the%20limitation%20of%20Hidden%20Markov%20Chain%20along%0Ablocks.%20In%20this%20paper%2C%20we%20equip%20PEFT%20with%20a%20cross-block%20orchestration%20mechanism%0Ato%20enable%20the%20adaptation%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20various%0Adownstream%20scenarios.%20We%20introduce%20a%20novel%20inter-block%20communication%20module%2C%0Awhich%20integrates%20a%20learnable%20relation%20matrix%20to%20facilitate%20communication%20among%0Adifferent%20coefficient%20sets%20of%20each%20PEFT%20block%27s%20parameter%20space.%20Moreover%2C%20we%0Apropose%20an%20intra-block%20enhancement%20module%2C%20which%20introduces%20a%20linear%20projection%0Ahead%20whose%20weights%20are%20generated%20from%20a%20hyper-complex%20layer%2C%20further%20enhancing%0Athe%20impact%20of%20the%20adjustment%20of%20projection%20directions%20on%20the%20entire%20parameter%0Aspace.%20Extensive%20experiments%20on%20diverse%20benchmarks%20demonstrate%20that%20our%0Aproposed%20approach%20consistently%20improves%20the%20segmentation%20performance%0Asignificantly%20on%20novel%20scenarios%20with%20only%20around%201K%20additional%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17112v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter%20Efficient%20Fine-tuning%20via%20Cross%20Block%20Orchestration%20for%0A%20%20Segment%20Anything%20Model&entry.906535625=Zelin%20Peng%20and%20Zhengqin%20Xu%20and%20Zhilin%20Zeng%20and%20Lingxi%20Xie%20and%20Qi%20Tian%20and%20Wei%20Shen&entry.1292438233=%20%20Parameter-efficient%20fine-tuning%20%28PEFT%29%20is%20an%20effective%20methodology%20to%20unleash%0Athe%20potential%20of%20large%20foundation%20models%20in%20novel%20scenarios%20with%20limited%0Atraining%20data.%20In%20the%20computer%20vision%20community%2C%20PEFT%20has%20shown%20effectiveness%0Ain%20image%20classification%2C%20but%20little%20research%20has%20studied%20its%20ability%20for%20image%0Asegmentation.%20Fine-tuning%20segmentation%20models%20usually%20require%20a%20heavier%0Aadjustment%20of%20parameters%20to%20align%20the%20proper%20projection%20directions%20in%20the%0Aparameter%20space%20for%20new%20scenarios.%20This%20raises%20a%20challenge%20to%20existing%20PEFT%0Aalgorithms%2C%20as%20they%20often%20inject%20a%20limited%20number%20of%20individual%20parameters%20into%0Aeach%20block%2C%20which%20prevents%20substantial%20adjustment%20of%20the%20projection%20direction%0Aof%20the%20parameter%20space%20due%20to%20the%20limitation%20of%20Hidden%20Markov%20Chain%20along%0Ablocks.%20In%20this%20paper%2C%20we%20equip%20PEFT%20with%20a%20cross-block%20orchestration%20mechanism%0Ato%20enable%20the%20adaptation%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20various%0Adownstream%20scenarios.%20We%20introduce%20a%20novel%20inter-block%20communication%20module%2C%0Awhich%20integrates%20a%20learnable%20relation%20matrix%20to%20facilitate%20communication%20among%0Adifferent%20coefficient%20sets%20of%20each%20PEFT%20block%27s%20parameter%20space.%20Moreover%2C%20we%0Apropose%20an%20intra-block%20enhancement%20module%2C%20which%20introduces%20a%20linear%20projection%0Ahead%20whose%20weights%20are%20generated%20from%20a%20hyper-complex%20layer%2C%20further%20enhancing%0Athe%20impact%20of%20the%20adjustment%20of%20projection%20directions%20on%20the%20entire%20parameter%0Aspace.%20Extensive%20experiments%20on%20diverse%20benchmarks%20demonstrate%20that%20our%0Aproposed%20approach%20consistently%20improves%20the%20segmentation%20performance%0Asignificantly%20on%20novel%20scenarios%20with%20only%20around%201K%20additional%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17112v2&entry.124074799=Read"},
{"title": "Policy Bifurcation in Safe Reinforcement Learning", "author": "Wenjun Zou and Yao Lyu and Jie Li and Yujie Yang and Shengbo Eben Li and Jingliang Duan and Xianyuan Zhan and Jingjing Liu and Yaqin Zhang and Keqiang Li", "abstract": "  Safe reinforcement learning (RL) offers advanced solutions to constrained\noptimal control problems. Existing studies in safe RL implicitly assume\ncontinuity in policy functions, where policies map states to actions in a\nsmooth, uninterrupted manner; however, our research finds that in some\nscenarios, the feasible policy should be discontinuous or multi-valued,\ninterpolating between discontinuous local optima can inevitably lead to\nconstraint violations. We are the first to identify the generating mechanism of\nsuch a phenomenon, and employ topological analysis to rigorously prove the\nexistence of policy bifurcation in safe RL, which corresponds to the\ncontractibility of the reachable tuple. Our theorem reveals that in scenarios\nwhere the obstacle-free state space is non-simply connected, a feasible policy\nis required to be bifurcated, meaning its output action needs to change\nabruptly in response to the varying state. To train such a bifurcated policy,\nwe propose a safe RL algorithm called multimodal policy optimization (MUPO),\nwhich utilizes a Gaussian mixture distribution as the policy output. The\nbifurcated behavior can be achieved by selecting the Gaussian component with\nthe highest mixing coefficient. Besides, MUPO also integrates spectral\nnormalization and forward KL divergence to enhance the policy's capability of\nexploring different modes. Experiments with vehicle control tasks show that our\nalgorithm successfully learns the bifurcated policy and ensures satisfying\nsafety, while a continuous policy suffers from inevitable constraint\nviolations.\n", "link": "http://arxiv.org/abs/2403.12847v3", "date": "2024-03-28", "relevancy": 1.917, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5101}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4966}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4496}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Policy%20Bifurcation%20in%20Safe%20Reinforcement%20Learning&body=Title%3A%20Policy%20Bifurcation%20in%20Safe%20Reinforcement%20Learning%0AAuthor%3A%20Wenjun%20Zou%20and%20Yao%20Lyu%20and%20Jie%20Li%20and%20Yujie%20Yang%20and%20Shengbo%20Eben%20Li%20and%20Jingliang%20Duan%20and%20Xianyuan%20Zhan%20and%20Jingjing%20Liu%20and%20Yaqin%20Zhang%20and%20Keqiang%20Li%0AAbstract%3A%20%20%20Safe%20reinforcement%20learning%20%28RL%29%20offers%20advanced%20solutions%20to%20constrained%0Aoptimal%20control%20problems.%20Existing%20studies%20in%20safe%20RL%20implicitly%20assume%0Acontinuity%20in%20policy%20functions%2C%20where%20policies%20map%20states%20to%20actions%20in%20a%0Asmooth%2C%20uninterrupted%20manner%3B%20however%2C%20our%20research%20finds%20that%20in%20some%0Ascenarios%2C%20the%20feasible%20policy%20should%20be%20discontinuous%20or%20multi-valued%2C%0Ainterpolating%20between%20discontinuous%20local%20optima%20can%20inevitably%20lead%20to%0Aconstraint%20violations.%20We%20are%20the%20first%20to%20identify%20the%20generating%20mechanism%20of%0Asuch%20a%20phenomenon%2C%20and%20employ%20topological%20analysis%20to%20rigorously%20prove%20the%0Aexistence%20of%20policy%20bifurcation%20in%20safe%20RL%2C%20which%20corresponds%20to%20the%0Acontractibility%20of%20the%20reachable%20tuple.%20Our%20theorem%20reveals%20that%20in%20scenarios%0Awhere%20the%20obstacle-free%20state%20space%20is%20non-simply%20connected%2C%20a%20feasible%20policy%0Ais%20required%20to%20be%20bifurcated%2C%20meaning%20its%20output%20action%20needs%20to%20change%0Aabruptly%20in%20response%20to%20the%20varying%20state.%20To%20train%20such%20a%20bifurcated%20policy%2C%0Awe%20propose%20a%20safe%20RL%20algorithm%20called%20multimodal%20policy%20optimization%20%28MUPO%29%2C%0Awhich%20utilizes%20a%20Gaussian%20mixture%20distribution%20as%20the%20policy%20output.%20The%0Abifurcated%20behavior%20can%20be%20achieved%20by%20selecting%20the%20Gaussian%20component%20with%0Athe%20highest%20mixing%20coefficient.%20Besides%2C%20MUPO%20also%20integrates%20spectral%0Anormalization%20and%20forward%20KL%20divergence%20to%20enhance%20the%20policy%27s%20capability%20of%0Aexploring%20different%20modes.%20Experiments%20with%20vehicle%20control%20tasks%20show%20that%20our%0Aalgorithm%20successfully%20learns%20the%20bifurcated%20policy%20and%20ensures%20satisfying%0Asafety%2C%20while%20a%20continuous%20policy%20suffers%20from%20inevitable%20constraint%0Aviolations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12847v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Policy%20Bifurcation%20in%20Safe%20Reinforcement%20Learning&entry.906535625=Wenjun%20Zou%20and%20Yao%20Lyu%20and%20Jie%20Li%20and%20Yujie%20Yang%20and%20Shengbo%20Eben%20Li%20and%20Jingliang%20Duan%20and%20Xianyuan%20Zhan%20and%20Jingjing%20Liu%20and%20Yaqin%20Zhang%20and%20Keqiang%20Li&entry.1292438233=%20%20Safe%20reinforcement%20learning%20%28RL%29%20offers%20advanced%20solutions%20to%20constrained%0Aoptimal%20control%20problems.%20Existing%20studies%20in%20safe%20RL%20implicitly%20assume%0Acontinuity%20in%20policy%20functions%2C%20where%20policies%20map%20states%20to%20actions%20in%20a%0Asmooth%2C%20uninterrupted%20manner%3B%20however%2C%20our%20research%20finds%20that%20in%20some%0Ascenarios%2C%20the%20feasible%20policy%20should%20be%20discontinuous%20or%20multi-valued%2C%0Ainterpolating%20between%20discontinuous%20local%20optima%20can%20inevitably%20lead%20to%0Aconstraint%20violations.%20We%20are%20the%20first%20to%20identify%20the%20generating%20mechanism%20of%0Asuch%20a%20phenomenon%2C%20and%20employ%20topological%20analysis%20to%20rigorously%20prove%20the%0Aexistence%20of%20policy%20bifurcation%20in%20safe%20RL%2C%20which%20corresponds%20to%20the%0Acontractibility%20of%20the%20reachable%20tuple.%20Our%20theorem%20reveals%20that%20in%20scenarios%0Awhere%20the%20obstacle-free%20state%20space%20is%20non-simply%20connected%2C%20a%20feasible%20policy%0Ais%20required%20to%20be%20bifurcated%2C%20meaning%20its%20output%20action%20needs%20to%20change%0Aabruptly%20in%20response%20to%20the%20varying%20state.%20To%20train%20such%20a%20bifurcated%20policy%2C%0Awe%20propose%20a%20safe%20RL%20algorithm%20called%20multimodal%20policy%20optimization%20%28MUPO%29%2C%0Awhich%20utilizes%20a%20Gaussian%20mixture%20distribution%20as%20the%20policy%20output.%20The%0Abifurcated%20behavior%20can%20be%20achieved%20by%20selecting%20the%20Gaussian%20component%20with%0Athe%20highest%20mixing%20coefficient.%20Besides%2C%20MUPO%20also%20integrates%20spectral%0Anormalization%20and%20forward%20KL%20divergence%20to%20enhance%20the%20policy%27s%20capability%20of%0Aexploring%20different%20modes.%20Experiments%20with%20vehicle%20control%20tasks%20show%20that%20our%0Aalgorithm%20successfully%20learns%20the%20bifurcated%20policy%20and%20ensures%20satisfying%0Asafety%2C%20while%20a%20continuous%20policy%20suffers%20from%20inevitable%20constraint%0Aviolations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12847v3&entry.124074799=Read"},
{"title": "Random Vector Functional Link Networks for Function Approximation on\n  Manifolds", "author": "Deanna Needell and Aaron A. Nelson and Rayan Saab and Palina Salanevich and Olov Schavemaker", "abstract": "  The learning speed of feed-forward neural networks is notoriously slow and\nhas presented a bottleneck in deep learning applications for several decades.\nFor instance, gradient-based learning algorithms, which are used extensively to\ntrain neural networks, tend to work slowly when all of the network parameters\nmust be iteratively tuned. To counter this, both researchers and practitioners\nhave tried introducing randomness to reduce the learning requirement. Based on\nthe original construction of Igelnik and Pao, single layer neural-networks with\nrandom input-to-hidden layer weights and biases have seen success in practice,\nbut the necessary theoretical justification is lacking. In this paper, we begin\nto fill this theoretical gap. We provide a (corrected) rigorous proof that the\nIgelnik and Pao construction is a universal approximator for continuous\nfunctions on compact domains, with approximation error decaying asymptotically\nlike $O(1/\\sqrt{n})$ for the number $n$ of network nodes. We then extend this\nresult to the non-asymptotic setting, proving that one can achieve any desired\napproximation error with high probability provided $n$ is sufficiently large.\nWe further adapt this randomized neural network architecture to approximate\nfunctions on smooth, compact submanifolds of Euclidean space, providing\ntheoretical guarantees in both the asymptotic and non-asymptotic forms.\nFinally, we illustrate our results on manifolds with numerical experiments.\n", "link": "http://arxiv.org/abs/2007.15776v3", "date": "2024-03-28", "relevancy": 1.9111, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4828}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4815}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4712}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Random%20Vector%20Functional%20Link%20Networks%20for%20Function%20Approximation%20on%0A%20%20Manifolds&body=Title%3A%20Random%20Vector%20Functional%20Link%20Networks%20for%20Function%20Approximation%20on%0A%20%20Manifolds%0AAuthor%3A%20Deanna%20Needell%20and%20Aaron%20A.%20Nelson%20and%20Rayan%20Saab%20and%20Palina%20Salanevich%20and%20Olov%20Schavemaker%0AAbstract%3A%20%20%20The%20learning%20speed%20of%20feed-forward%20neural%20networks%20is%20notoriously%20slow%20and%0Ahas%20presented%20a%20bottleneck%20in%20deep%20learning%20applications%20for%20several%20decades.%0AFor%20instance%2C%20gradient-based%20learning%20algorithms%2C%20which%20are%20used%20extensively%20to%0Atrain%20neural%20networks%2C%20tend%20to%20work%20slowly%20when%20all%20of%20the%20network%20parameters%0Amust%20be%20iteratively%20tuned.%20To%20counter%20this%2C%20both%20researchers%20and%20practitioners%0Ahave%20tried%20introducing%20randomness%20to%20reduce%20the%20learning%20requirement.%20Based%20on%0Athe%20original%20construction%20of%20Igelnik%20and%20Pao%2C%20single%20layer%20neural-networks%20with%0Arandom%20input-to-hidden%20layer%20weights%20and%20biases%20have%20seen%20success%20in%20practice%2C%0Abut%20the%20necessary%20theoretical%20justification%20is%20lacking.%20In%20this%20paper%2C%20we%20begin%0Ato%20fill%20this%20theoretical%20gap.%20We%20provide%20a%20%28corrected%29%20rigorous%20proof%20that%20the%0AIgelnik%20and%20Pao%20construction%20is%20a%20universal%20approximator%20for%20continuous%0Afunctions%20on%20compact%20domains%2C%20with%20approximation%20error%20decaying%20asymptotically%0Alike%20%24O%281/%5Csqrt%7Bn%7D%29%24%20for%20the%20number%20%24n%24%20of%20network%20nodes.%20We%20then%20extend%20this%0Aresult%20to%20the%20non-asymptotic%20setting%2C%20proving%20that%20one%20can%20achieve%20any%20desired%0Aapproximation%20error%20with%20high%20probability%20provided%20%24n%24%20is%20sufficiently%20large.%0AWe%20further%20adapt%20this%20randomized%20neural%20network%20architecture%20to%20approximate%0Afunctions%20on%20smooth%2C%20compact%20submanifolds%20of%20Euclidean%20space%2C%20providing%0Atheoretical%20guarantees%20in%20both%20the%20asymptotic%20and%20non-asymptotic%20forms.%0AFinally%2C%20we%20illustrate%20our%20results%20on%20manifolds%20with%20numerical%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2007.15776v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Vector%20Functional%20Link%20Networks%20for%20Function%20Approximation%20on%0A%20%20Manifolds&entry.906535625=Deanna%20Needell%20and%20Aaron%20A.%20Nelson%20and%20Rayan%20Saab%20and%20Palina%20Salanevich%20and%20Olov%20Schavemaker&entry.1292438233=%20%20The%20learning%20speed%20of%20feed-forward%20neural%20networks%20is%20notoriously%20slow%20and%0Ahas%20presented%20a%20bottleneck%20in%20deep%20learning%20applications%20for%20several%20decades.%0AFor%20instance%2C%20gradient-based%20learning%20algorithms%2C%20which%20are%20used%20extensively%20to%0Atrain%20neural%20networks%2C%20tend%20to%20work%20slowly%20when%20all%20of%20the%20network%20parameters%0Amust%20be%20iteratively%20tuned.%20To%20counter%20this%2C%20both%20researchers%20and%20practitioners%0Ahave%20tried%20introducing%20randomness%20to%20reduce%20the%20learning%20requirement.%20Based%20on%0Athe%20original%20construction%20of%20Igelnik%20and%20Pao%2C%20single%20layer%20neural-networks%20with%0Arandom%20input-to-hidden%20layer%20weights%20and%20biases%20have%20seen%20success%20in%20practice%2C%0Abut%20the%20necessary%20theoretical%20justification%20is%20lacking.%20In%20this%20paper%2C%20we%20begin%0Ato%20fill%20this%20theoretical%20gap.%20We%20provide%20a%20%28corrected%29%20rigorous%20proof%20that%20the%0AIgelnik%20and%20Pao%20construction%20is%20a%20universal%20approximator%20for%20continuous%0Afunctions%20on%20compact%20domains%2C%20with%20approximation%20error%20decaying%20asymptotically%0Alike%20%24O%281/%5Csqrt%7Bn%7D%29%24%20for%20the%20number%20%24n%24%20of%20network%20nodes.%20We%20then%20extend%20this%0Aresult%20to%20the%20non-asymptotic%20setting%2C%20proving%20that%20one%20can%20achieve%20any%20desired%0Aapproximation%20error%20with%20high%20probability%20provided%20%24n%24%20is%20sufficiently%20large.%0AWe%20further%20adapt%20this%20randomized%20neural%20network%20architecture%20to%20approximate%0Afunctions%20on%20smooth%2C%20compact%20submanifolds%20of%20Euclidean%20space%2C%20providing%0Atheoretical%20guarantees%20in%20both%20the%20asymptotic%20and%20non-asymptotic%20forms.%0AFinally%2C%20we%20illustrate%20our%20results%20on%20manifolds%20with%20numerical%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2007.15776v3&entry.124074799=Read"},
{"title": "Learning a Formally Verified Control Barrier Function in Stochastic\n  Environment", "author": "Manan Tayal and Hongchao Zhang and Pushpak Jagtap and Andrew Clark and Shishir Kolathaya", "abstract": "  Safety is a fundamental requirement of control systems. Control Barrier\nFunctions (CBFs) are proposed to ensure the safety of the control system by\nconstructing safety filters or synthesizing control inputs. However, the safety\nguarantee and performance of safe controllers rely on the construction of valid\nCBFs. Inspired by universal approximatability, CBFs are represented by neural\nnetworks, known as neural CBFs (NCBFs). This paper presents an algorithm for\nsynthesizing formally verified continuous-time neural Control Barrier Functions\nin stochastic environments in a single step. The proposed training process\nensures efficacy across the entire state space with only a finite number of\ndata points by constructing a sample-based learning framework for Stochastic\nNeural CBFs (SNCBFs). Our methodology eliminates the need for post hoc\nverification by enforcing Lipschitz bounds on the neural network, its Jacobian,\nand Hessian terms. We demonstrate the effectiveness of our approach through\ncase studies on the inverted pendulum system and obstacle avoidance in\nautonomous driving, showcasing larger safe regions compared to baseline\nmethods.\n", "link": "http://arxiv.org/abs/2403.19332v1", "date": "2024-03-28", "relevancy": 1.9095, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5017}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4993}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4457}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20a%20Formally%20Verified%20Control%20Barrier%20Function%20in%20Stochastic%0A%20%20Environment&body=Title%3A%20Learning%20a%20Formally%20Verified%20Control%20Barrier%20Function%20in%20Stochastic%0A%20%20Environment%0AAuthor%3A%20Manan%20Tayal%20and%20Hongchao%20Zhang%20and%20Pushpak%20Jagtap%20and%20Andrew%20Clark%20and%20Shishir%20Kolathaya%0AAbstract%3A%20%20%20Safety%20is%20a%20fundamental%20requirement%20of%20control%20systems.%20Control%20Barrier%0AFunctions%20%28CBFs%29%20are%20proposed%20to%20ensure%20the%20safety%20of%20the%20control%20system%20by%0Aconstructing%20safety%20filters%20or%20synthesizing%20control%20inputs.%20However%2C%20the%20safety%0Aguarantee%20and%20performance%20of%20safe%20controllers%20rely%20on%20the%20construction%20of%20valid%0ACBFs.%20Inspired%20by%20universal%20approximatability%2C%20CBFs%20are%20represented%20by%20neural%0Anetworks%2C%20known%20as%20neural%20CBFs%20%28NCBFs%29.%20This%20paper%20presents%20an%20algorithm%20for%0Asynthesizing%20formally%20verified%20continuous-time%20neural%20Control%20Barrier%20Functions%0Ain%20stochastic%20environments%20in%20a%20single%20step.%20The%20proposed%20training%20process%0Aensures%20efficacy%20across%20the%20entire%20state%20space%20with%20only%20a%20finite%20number%20of%0Adata%20points%20by%20constructing%20a%20sample-based%20learning%20framework%20for%20Stochastic%0ANeural%20CBFs%20%28SNCBFs%29.%20Our%20methodology%20eliminates%20the%20need%20for%20post%20hoc%0Averification%20by%20enforcing%20Lipschitz%20bounds%20on%20the%20neural%20network%2C%20its%20Jacobian%2C%0Aand%20Hessian%20terms.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20through%0Acase%20studies%20on%20the%20inverted%20pendulum%20system%20and%20obstacle%20avoidance%20in%0Aautonomous%20driving%2C%20showcasing%20larger%20safe%20regions%20compared%20to%20baseline%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19332v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20a%20Formally%20Verified%20Control%20Barrier%20Function%20in%20Stochastic%0A%20%20Environment&entry.906535625=Manan%20Tayal%20and%20Hongchao%20Zhang%20and%20Pushpak%20Jagtap%20and%20Andrew%20Clark%20and%20Shishir%20Kolathaya&entry.1292438233=%20%20Safety%20is%20a%20fundamental%20requirement%20of%20control%20systems.%20Control%20Barrier%0AFunctions%20%28CBFs%29%20are%20proposed%20to%20ensure%20the%20safety%20of%20the%20control%20system%20by%0Aconstructing%20safety%20filters%20or%20synthesizing%20control%20inputs.%20However%2C%20the%20safety%0Aguarantee%20and%20performance%20of%20safe%20controllers%20rely%20on%20the%20construction%20of%20valid%0ACBFs.%20Inspired%20by%20universal%20approximatability%2C%20CBFs%20are%20represented%20by%20neural%0Anetworks%2C%20known%20as%20neural%20CBFs%20%28NCBFs%29.%20This%20paper%20presents%20an%20algorithm%20for%0Asynthesizing%20formally%20verified%20continuous-time%20neural%20Control%20Barrier%20Functions%0Ain%20stochastic%20environments%20in%20a%20single%20step.%20The%20proposed%20training%20process%0Aensures%20efficacy%20across%20the%20entire%20state%20space%20with%20only%20a%20finite%20number%20of%0Adata%20points%20by%20constructing%20a%20sample-based%20learning%20framework%20for%20Stochastic%0ANeural%20CBFs%20%28SNCBFs%29.%20Our%20methodology%20eliminates%20the%20need%20for%20post%20hoc%0Averification%20by%20enforcing%20Lipschitz%20bounds%20on%20the%20neural%20network%2C%20its%20Jacobian%2C%0Aand%20Hessian%20terms.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20through%0Acase%20studies%20on%20the%20inverted%20pendulum%20system%20and%20obstacle%20avoidance%20in%0Aautonomous%20driving%2C%20showcasing%20larger%20safe%20regions%20compared%20to%20baseline%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19332v1&entry.124074799=Read"},
{"title": "Exploiting Individual Graph Structures to Enhance Ecological Momentary\n  Assessment (EMA) Forecasting", "author": "Mandani Ntekouli and Gerasimos Spanakis and Lourens Waldorp and Anne Roefs", "abstract": "  In the evolving field of psychopathology, the accurate assessment and\nforecasting of data derived from Ecological Momentary Assessment (EMA) is\ncrucial. EMA offers contextually-rich psychopathological measurements over\ntime, that practically lead to Multivariate Time Series (MTS) data. Thus, many\nchallenges arise in analysis from the temporal complexities inherent in\nemotional, behavioral, and contextual EMA data as well as their\ninter-dependencies. To address both of these aspects, this research\ninvestigates the performance of Recurrent and Temporal Graph Neural Networks\n(GNNs). Overall, GNNs, by incorporating additional information from graphs\nreflecting the inner relationships between the variables, notably enhance the\nresults by decreasing the Mean Squared Error (MSE) to 0.84 compared to the\nbaseline LSTM model at 1.02. Therefore, the effect of constructing graphs with\ndifferent characteristics on GNN performance is also explored. Additionally,\nGNN-learned graphs, which are dynamically refined during the training process,\nwere evaluated. Using such graphs showed a similarly good performance. Thus,\ngraph learning proved also promising for other GNN methods, potentially\nrefining the pre-defined graphs.\n", "link": "http://arxiv.org/abs/2403.19442v1", "date": "2024-03-28", "relevancy": 1.8816, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.502}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.479}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4353}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Individual%20Graph%20Structures%20to%20Enhance%20Ecological%20Momentary%0A%20%20Assessment%20%28EMA%29%20Forecasting&body=Title%3A%20Exploiting%20Individual%20Graph%20Structures%20to%20Enhance%20Ecological%20Momentary%0A%20%20Assessment%20%28EMA%29%20Forecasting%0AAuthor%3A%20Mandani%20Ntekouli%20and%20Gerasimos%20Spanakis%20and%20Lourens%20Waldorp%20and%20Anne%20Roefs%0AAbstract%3A%20%20%20In%20the%20evolving%20field%20of%20psychopathology%2C%20the%20accurate%20assessment%20and%0Aforecasting%20of%20data%20derived%20from%20Ecological%20Momentary%20Assessment%20%28EMA%29%20is%0Acrucial.%20EMA%20offers%20contextually-rich%20psychopathological%20measurements%20over%0Atime%2C%20that%20practically%20lead%20to%20Multivariate%20Time%20Series%20%28MTS%29%20data.%20Thus%2C%20many%0Achallenges%20arise%20in%20analysis%20from%20the%20temporal%20complexities%20inherent%20in%0Aemotional%2C%20behavioral%2C%20and%20contextual%20EMA%20data%20as%20well%20as%20their%0Ainter-dependencies.%20To%20address%20both%20of%20these%20aspects%2C%20this%20research%0Ainvestigates%20the%20performance%20of%20Recurrent%20and%20Temporal%20Graph%20Neural%20Networks%0A%28GNNs%29.%20Overall%2C%20GNNs%2C%20by%20incorporating%20additional%20information%20from%20graphs%0Areflecting%20the%20inner%20relationships%20between%20the%20variables%2C%20notably%20enhance%20the%0Aresults%20by%20decreasing%20the%20Mean%20Squared%20Error%20%28MSE%29%20to%200.84%20compared%20to%20the%0Abaseline%20LSTM%20model%20at%201.02.%20Therefore%2C%20the%20effect%20of%20constructing%20graphs%20with%0Adifferent%20characteristics%20on%20GNN%20performance%20is%20also%20explored.%20Additionally%2C%0AGNN-learned%20graphs%2C%20which%20are%20dynamically%20refined%20during%20the%20training%20process%2C%0Awere%20evaluated.%20Using%20such%20graphs%20showed%20a%20similarly%20good%20performance.%20Thus%2C%0Agraph%20learning%20proved%20also%20promising%20for%20other%20GNN%20methods%2C%20potentially%0Arefining%20the%20pre-defined%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19442v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Individual%20Graph%20Structures%20to%20Enhance%20Ecological%20Momentary%0A%20%20Assessment%20%28EMA%29%20Forecasting&entry.906535625=Mandani%20Ntekouli%20and%20Gerasimos%20Spanakis%20and%20Lourens%20Waldorp%20and%20Anne%20Roefs&entry.1292438233=%20%20In%20the%20evolving%20field%20of%20psychopathology%2C%20the%20accurate%20assessment%20and%0Aforecasting%20of%20data%20derived%20from%20Ecological%20Momentary%20Assessment%20%28EMA%29%20is%0Acrucial.%20EMA%20offers%20contextually-rich%20psychopathological%20measurements%20over%0Atime%2C%20that%20practically%20lead%20to%20Multivariate%20Time%20Series%20%28MTS%29%20data.%20Thus%2C%20many%0Achallenges%20arise%20in%20analysis%20from%20the%20temporal%20complexities%20inherent%20in%0Aemotional%2C%20behavioral%2C%20and%20contextual%20EMA%20data%20as%20well%20as%20their%0Ainter-dependencies.%20To%20address%20both%20of%20these%20aspects%2C%20this%20research%0Ainvestigates%20the%20performance%20of%20Recurrent%20and%20Temporal%20Graph%20Neural%20Networks%0A%28GNNs%29.%20Overall%2C%20GNNs%2C%20by%20incorporating%20additional%20information%20from%20graphs%0Areflecting%20the%20inner%20relationships%20between%20the%20variables%2C%20notably%20enhance%20the%0Aresults%20by%20decreasing%20the%20Mean%20Squared%20Error%20%28MSE%29%20to%200.84%20compared%20to%20the%0Abaseline%20LSTM%20model%20at%201.02.%20Therefore%2C%20the%20effect%20of%20constructing%20graphs%20with%0Adifferent%20characteristics%20on%20GNN%20performance%20is%20also%20explored.%20Additionally%2C%0AGNN-learned%20graphs%2C%20which%20are%20dynamically%20refined%20during%20the%20training%20process%2C%0Awere%20evaluated.%20Using%20such%20graphs%20showed%20a%20similarly%20good%20performance.%20Thus%2C%0Agraph%20learning%20proved%20also%20promising%20for%20other%20GNN%20methods%2C%20potentially%0Arefining%20the%20pre-defined%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19442v1&entry.124074799=Read"},
{"title": "Client-supervised Federated Learning: Towards One-model-for-all\n  Personalization", "author": "Peng Yan and Guodong Long", "abstract": "  Personalized Federated Learning (PerFL) is a new machine learning paradigm\nthat delivers personalized models for diverse clients under federated learning\nsettings. Most PerFL methods require extra learning processes on a client to\nadapt a globally shared model to the client-specific personalized model using\nits own local data. However, the model adaptation process in PerFL is still an\nopen challenge in the stage of model deployment and test time. This work\ntackles the challenge by proposing a novel federated learning framework to\nlearn only one robust global model to achieve competitive performance to those\npersonalized models on unseen/test clients in the FL system. Specifically, we\ndesign a new Client-Supervised Federated Learning (FedCS) to unravel clients'\nbias on instances' latent representations so that the global model can learn\nboth client-specific and client-agnostic knowledge. Experimental study shows\nthat the FedCS can learn a robust FL global model for the changing data\ndistributions of unseen/test clients. The FedCS's global model can be directly\ndeployed to the test clients while achieving comparable performance to other\npersonalized FL methods that require model adaptation.\n", "link": "http://arxiv.org/abs/2403.19499v1", "date": "2024-03-28", "relevancy": 1.879, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.477}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4742}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4607}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Client-supervised%20Federated%20Learning%3A%20Towards%20One-model-for-all%0A%20%20Personalization&body=Title%3A%20Client-supervised%20Federated%20Learning%3A%20Towards%20One-model-for-all%0A%20%20Personalization%0AAuthor%3A%20Peng%20Yan%20and%20Guodong%20Long%0AAbstract%3A%20%20%20Personalized%20Federated%20Learning%20%28PerFL%29%20is%20a%20new%20machine%20learning%20paradigm%0Athat%20delivers%20personalized%20models%20for%20diverse%20clients%20under%20federated%20learning%0Asettings.%20Most%20PerFL%20methods%20require%20extra%20learning%20processes%20on%20a%20client%20to%0Aadapt%20a%20globally%20shared%20model%20to%20the%20client-specific%20personalized%20model%20using%0Aits%20own%20local%20data.%20However%2C%20the%20model%20adaptation%20process%20in%20PerFL%20is%20still%20an%0Aopen%20challenge%20in%20the%20stage%20of%20model%20deployment%20and%20test%20time.%20This%20work%0Atackles%20the%20challenge%20by%20proposing%20a%20novel%20federated%20learning%20framework%20to%0Alearn%20only%20one%20robust%20global%20model%20to%20achieve%20competitive%20performance%20to%20those%0Apersonalized%20models%20on%20unseen/test%20clients%20in%20the%20FL%20system.%20Specifically%2C%20we%0Adesign%20a%20new%20Client-Supervised%20Federated%20Learning%20%28FedCS%29%20to%20unravel%20clients%27%0Abias%20on%20instances%27%20latent%20representations%20so%20that%20the%20global%20model%20can%20learn%0Aboth%20client-specific%20and%20client-agnostic%20knowledge.%20Experimental%20study%20shows%0Athat%20the%20FedCS%20can%20learn%20a%20robust%20FL%20global%20model%20for%20the%20changing%20data%0Adistributions%20of%20unseen/test%20clients.%20The%20FedCS%27s%20global%20model%20can%20be%20directly%0Adeployed%20to%20the%20test%20clients%20while%20achieving%20comparable%20performance%20to%20other%0Apersonalized%20FL%20methods%20that%20require%20model%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19499v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Client-supervised%20Federated%20Learning%3A%20Towards%20One-model-for-all%0A%20%20Personalization&entry.906535625=Peng%20Yan%20and%20Guodong%20Long&entry.1292438233=%20%20Personalized%20Federated%20Learning%20%28PerFL%29%20is%20a%20new%20machine%20learning%20paradigm%0Athat%20delivers%20personalized%20models%20for%20diverse%20clients%20under%20federated%20learning%0Asettings.%20Most%20PerFL%20methods%20require%20extra%20learning%20processes%20on%20a%20client%20to%0Aadapt%20a%20globally%20shared%20model%20to%20the%20client-specific%20personalized%20model%20using%0Aits%20own%20local%20data.%20However%2C%20the%20model%20adaptation%20process%20in%20PerFL%20is%20still%20an%0Aopen%20challenge%20in%20the%20stage%20of%20model%20deployment%20and%20test%20time.%20This%20work%0Atackles%20the%20challenge%20by%20proposing%20a%20novel%20federated%20learning%20framework%20to%0Alearn%20only%20one%20robust%20global%20model%20to%20achieve%20competitive%20performance%20to%20those%0Apersonalized%20models%20on%20unseen/test%20clients%20in%20the%20FL%20system.%20Specifically%2C%20we%0Adesign%20a%20new%20Client-Supervised%20Federated%20Learning%20%28FedCS%29%20to%20unravel%20clients%27%0Abias%20on%20instances%27%20latent%20representations%20so%20that%20the%20global%20model%20can%20learn%0Aboth%20client-specific%20and%20client-agnostic%20knowledge.%20Experimental%20study%20shows%0Athat%20the%20FedCS%20can%20learn%20a%20robust%20FL%20global%20model%20for%20the%20changing%20data%0Adistributions%20of%20unseen/test%20clients.%20The%20FedCS%27s%20global%20model%20can%20be%20directly%0Adeployed%20to%20the%20test%20clients%20while%20achieving%20comparable%20performance%20to%20other%0Apersonalized%20FL%20methods%20that%20require%20model%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19499v1&entry.124074799=Read"},
{"title": "Learned representation-guided diffusion models for large-image\n  generation", "author": "Alexandros Graikos and Srikar Yellapragada and Minh-Quan Le and Saarthak Kapse and Prateek Prasanna and Joel Saltz and Dimitris Samaras", "abstract": "  To synthesize high-fidelity samples, diffusion models typically require\nauxiliary data to guide the generation process. However, it is impractical to\nprocure the painstaking patch-level annotation effort required in specialized\ndomains like histopathology and satellite imagery; it is often performed by\ndomain experts and involves hundreds of millions of patches. Modern-day\nself-supervised learning (SSL) representations encode rich semantic and visual\ninformation. In this paper, we posit that such representations are expressive\nenough to act as proxies to fine-grained human labels. We introduce a novel\napproach that trains diffusion models conditioned on embeddings from SSL. Our\ndiffusion models successfully project these features back to high-quality\nhistopathology and remote sensing images. In addition, we construct larger\nimages by assembling spatially consistent patches inferred from SSL embeddings,\npreserving long-range dependencies. Augmenting real data by generating\nvariations of real images improves downstream classifier accuracy for\npatch-level and larger, image-scale classification tasks. Our models are\neffective even on datasets not encountered during training, demonstrating their\nrobustness and generalizability. Generating images from learned embeddings is\nagnostic to the source of the embeddings. The SSL embeddings used to generate a\nlarge image can either be extracted from a reference image, or sampled from an\nauxiliary model conditioned on any related modality (e.g. class labels, text,\ngenomic data). As proof of concept, we introduce the text-to-large image\nsynthesis paradigm where we successfully synthesize large pathology and\nsatellite images out of text descriptions.\n", "link": "http://arxiv.org/abs/2312.07330v2", "date": "2024-03-28", "relevancy": 1.8654, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6446}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6431}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6041}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learned%20representation-guided%20diffusion%20models%20for%20large-image%0A%20%20generation&body=Title%3A%20Learned%20representation-guided%20diffusion%20models%20for%20large-image%0A%20%20generation%0AAuthor%3A%20Alexandros%20Graikos%20and%20Srikar%20Yellapragada%20and%20Minh-Quan%20Le%20and%20Saarthak%20Kapse%20and%20Prateek%20Prasanna%20and%20Joel%20Saltz%20and%20Dimitris%20Samaras%0AAbstract%3A%20%20%20To%20synthesize%20high-fidelity%20samples%2C%20diffusion%20models%20typically%20require%0Aauxiliary%20data%20to%20guide%20the%20generation%20process.%20However%2C%20it%20is%20impractical%20to%0Aprocure%20the%20painstaking%20patch-level%20annotation%20effort%20required%20in%20specialized%0Adomains%20like%20histopathology%20and%20satellite%20imagery%3B%20it%20is%20often%20performed%20by%0Adomain%20experts%20and%20involves%20hundreds%20of%20millions%20of%20patches.%20Modern-day%0Aself-supervised%20learning%20%28SSL%29%20representations%20encode%20rich%20semantic%20and%20visual%0Ainformation.%20In%20this%20paper%2C%20we%20posit%20that%20such%20representations%20are%20expressive%0Aenough%20to%20act%20as%20proxies%20to%20fine-grained%20human%20labels.%20We%20introduce%20a%20novel%0Aapproach%20that%20trains%20diffusion%20models%20conditioned%20on%20embeddings%20from%20SSL.%20Our%0Adiffusion%20models%20successfully%20project%20these%20features%20back%20to%20high-quality%0Ahistopathology%20and%20remote%20sensing%20images.%20In%20addition%2C%20we%20construct%20larger%0Aimages%20by%20assembling%20spatially%20consistent%20patches%20inferred%20from%20SSL%20embeddings%2C%0Apreserving%20long-range%20dependencies.%20Augmenting%20real%20data%20by%20generating%0Avariations%20of%20real%20images%20improves%20downstream%20classifier%20accuracy%20for%0Apatch-level%20and%20larger%2C%20image-scale%20classification%20tasks.%20Our%20models%20are%0Aeffective%20even%20on%20datasets%20not%20encountered%20during%20training%2C%20demonstrating%20their%0Arobustness%20and%20generalizability.%20Generating%20images%20from%20learned%20embeddings%20is%0Aagnostic%20to%20the%20source%20of%20the%20embeddings.%20The%20SSL%20embeddings%20used%20to%20generate%20a%0Alarge%20image%20can%20either%20be%20extracted%20from%20a%20reference%20image%2C%20or%20sampled%20from%20an%0Aauxiliary%20model%20conditioned%20on%20any%20related%20modality%20%28e.g.%20class%20labels%2C%20text%2C%0Agenomic%20data%29.%20As%20proof%20of%20concept%2C%20we%20introduce%20the%20text-to-large%20image%0Asynthesis%20paradigm%20where%20we%20successfully%20synthesize%20large%20pathology%20and%0Asatellite%20images%20out%20of%20text%20descriptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07330v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20representation-guided%20diffusion%20models%20for%20large-image%0A%20%20generation&entry.906535625=Alexandros%20Graikos%20and%20Srikar%20Yellapragada%20and%20Minh-Quan%20Le%20and%20Saarthak%20Kapse%20and%20Prateek%20Prasanna%20and%20Joel%20Saltz%20and%20Dimitris%20Samaras&entry.1292438233=%20%20To%20synthesize%20high-fidelity%20samples%2C%20diffusion%20models%20typically%20require%0Aauxiliary%20data%20to%20guide%20the%20generation%20process.%20However%2C%20it%20is%20impractical%20to%0Aprocure%20the%20painstaking%20patch-level%20annotation%20effort%20required%20in%20specialized%0Adomains%20like%20histopathology%20and%20satellite%20imagery%3B%20it%20is%20often%20performed%20by%0Adomain%20experts%20and%20involves%20hundreds%20of%20millions%20of%20patches.%20Modern-day%0Aself-supervised%20learning%20%28SSL%29%20representations%20encode%20rich%20semantic%20and%20visual%0Ainformation.%20In%20this%20paper%2C%20we%20posit%20that%20such%20representations%20are%20expressive%0Aenough%20to%20act%20as%20proxies%20to%20fine-grained%20human%20labels.%20We%20introduce%20a%20novel%0Aapproach%20that%20trains%20diffusion%20models%20conditioned%20on%20embeddings%20from%20SSL.%20Our%0Adiffusion%20models%20successfully%20project%20these%20features%20back%20to%20high-quality%0Ahistopathology%20and%20remote%20sensing%20images.%20In%20addition%2C%20we%20construct%20larger%0Aimages%20by%20assembling%20spatially%20consistent%20patches%20inferred%20from%20SSL%20embeddings%2C%0Apreserving%20long-range%20dependencies.%20Augmenting%20real%20data%20by%20generating%0Avariations%20of%20real%20images%20improves%20downstream%20classifier%20accuracy%20for%0Apatch-level%20and%20larger%2C%20image-scale%20classification%20tasks.%20Our%20models%20are%0Aeffective%20even%20on%20datasets%20not%20encountered%20during%20training%2C%20demonstrating%20their%0Arobustness%20and%20generalizability.%20Generating%20images%20from%20learned%20embeddings%20is%0Aagnostic%20to%20the%20source%20of%20the%20embeddings.%20The%20SSL%20embeddings%20used%20to%20generate%20a%0Alarge%20image%20can%20either%20be%20extracted%20from%20a%20reference%20image%2C%20or%20sampled%20from%20an%0Aauxiliary%20model%20conditioned%20on%20any%20related%20modality%20%28e.g.%20class%20labels%2C%20text%2C%0Agenomic%20data%29.%20As%20proof%20of%20concept%2C%20we%20introduce%20the%20text-to-large%20image%0Asynthesis%20paradigm%20where%20we%20successfully%20synthesize%20large%20pathology%20and%0Asatellite%20images%20out%20of%20text%20descriptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07330v2&entry.124074799=Read"},
{"title": "ACT-Diffusion: Efficient Adversarial Consistency Training for One-step\n  Diffusion Models", "author": "Fei Kong and Jinhao Duan and Lichao Sun and Hao Cheng and Renjing Xu and Hengtao Shen and Xiaofeng Zhu and Xiaoshuang Shi and Kaidi Xu", "abstract": "  Though diffusion models excel in image generation, their step-by-step\ndenoising leads to slow generation speeds. Consistency training addresses this\nissue with single-step sampling but often produces lower-quality generations\nand requires high training costs. In this paper, we show that optimizing\nconsistency training loss minimizes the Wasserstein distance between target and\ngenerated distributions. As timestep increases, the upper bound accumulates\nprevious consistency training losses. Therefore, larger batch sizes are needed\nto reduce both current and accumulated losses. We propose Adversarial\nConsistency Training (ACT), which directly minimizes the Jensen-Shannon (JS)\ndivergence between distributions at each timestep using a discriminator.\nTheoretically, ACT enhances generation quality, and convergence. By\nincorporating a discriminator into the consistency training framework, our\nmethod achieves improved FID scores on CIFAR10 and ImageNet 64$\\times$64 and\nLSUN Cat 256$\\times$256 datasets, retains zero-shot image inpainting\ncapabilities, and uses less than $1/6$ of the original batch size and fewer\nthan $1/2$ of the model parameters and training steps compared to the baseline\nmethod, this leads to a substantial reduction in resource consumption. Our code\nis available:https://github.com/kong13661/ACT\n", "link": "http://arxiv.org/abs/2311.14097v3", "date": "2024-03-28", "relevancy": 1.8649, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6747}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.625}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5601}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ACT-Diffusion%3A%20Efficient%20Adversarial%20Consistency%20Training%20for%20One-step%0A%20%20Diffusion%20Models&body=Title%3A%20ACT-Diffusion%3A%20Efficient%20Adversarial%20Consistency%20Training%20for%20One-step%0A%20%20Diffusion%20Models%0AAuthor%3A%20Fei%20Kong%20and%20Jinhao%20Duan%20and%20Lichao%20Sun%20and%20Hao%20Cheng%20and%20Renjing%20Xu%20and%20Hengtao%20Shen%20and%20Xiaofeng%20Zhu%20and%20Xiaoshuang%20Shi%20and%20Kaidi%20Xu%0AAbstract%3A%20%20%20Though%20diffusion%20models%20excel%20in%20image%20generation%2C%20their%20step-by-step%0Adenoising%20leads%20to%20slow%20generation%20speeds.%20Consistency%20training%20addresses%20this%0Aissue%20with%20single-step%20sampling%20but%20often%20produces%20lower-quality%20generations%0Aand%20requires%20high%20training%20costs.%20In%20this%20paper%2C%20we%20show%20that%20optimizing%0Aconsistency%20training%20loss%20minimizes%20the%20Wasserstein%20distance%20between%20target%20and%0Agenerated%20distributions.%20As%20timestep%20increases%2C%20the%20upper%20bound%20accumulates%0Aprevious%20consistency%20training%20losses.%20Therefore%2C%20larger%20batch%20sizes%20are%20needed%0Ato%20reduce%20both%20current%20and%20accumulated%20losses.%20We%20propose%20Adversarial%0AConsistency%20Training%20%28ACT%29%2C%20which%20directly%20minimizes%20the%20Jensen-Shannon%20%28JS%29%0Adivergence%20between%20distributions%20at%20each%20timestep%20using%20a%20discriminator.%0ATheoretically%2C%20ACT%20enhances%20generation%20quality%2C%20and%20convergence.%20By%0Aincorporating%20a%20discriminator%20into%20the%20consistency%20training%20framework%2C%20our%0Amethod%20achieves%20improved%20FID%20scores%20on%20CIFAR10%20and%20ImageNet%2064%24%5Ctimes%2464%20and%0ALSUN%20Cat%20256%24%5Ctimes%24256%20datasets%2C%20retains%20zero-shot%20image%20inpainting%0Acapabilities%2C%20and%20uses%20less%20than%20%241/6%24%20of%20the%20original%20batch%20size%20and%20fewer%0Athan%20%241/2%24%20of%20the%20model%20parameters%20and%20training%20steps%20compared%20to%20the%20baseline%0Amethod%2C%20this%20leads%20to%20a%20substantial%20reduction%20in%20resource%20consumption.%20Our%20code%0Ais%20available%3Ahttps%3A//github.com/kong13661/ACT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14097v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACT-Diffusion%3A%20Efficient%20Adversarial%20Consistency%20Training%20for%20One-step%0A%20%20Diffusion%20Models&entry.906535625=Fei%20Kong%20and%20Jinhao%20Duan%20and%20Lichao%20Sun%20and%20Hao%20Cheng%20and%20Renjing%20Xu%20and%20Hengtao%20Shen%20and%20Xiaofeng%20Zhu%20and%20Xiaoshuang%20Shi%20and%20Kaidi%20Xu&entry.1292438233=%20%20Though%20diffusion%20models%20excel%20in%20image%20generation%2C%20their%20step-by-step%0Adenoising%20leads%20to%20slow%20generation%20speeds.%20Consistency%20training%20addresses%20this%0Aissue%20with%20single-step%20sampling%20but%20often%20produces%20lower-quality%20generations%0Aand%20requires%20high%20training%20costs.%20In%20this%20paper%2C%20we%20show%20that%20optimizing%0Aconsistency%20training%20loss%20minimizes%20the%20Wasserstein%20distance%20between%20target%20and%0Agenerated%20distributions.%20As%20timestep%20increases%2C%20the%20upper%20bound%20accumulates%0Aprevious%20consistency%20training%20losses.%20Therefore%2C%20larger%20batch%20sizes%20are%20needed%0Ato%20reduce%20both%20current%20and%20accumulated%20losses.%20We%20propose%20Adversarial%0AConsistency%20Training%20%28ACT%29%2C%20which%20directly%20minimizes%20the%20Jensen-Shannon%20%28JS%29%0Adivergence%20between%20distributions%20at%20each%20timestep%20using%20a%20discriminator.%0ATheoretically%2C%20ACT%20enhances%20generation%20quality%2C%20and%20convergence.%20By%0Aincorporating%20a%20discriminator%20into%20the%20consistency%20training%20framework%2C%20our%0Amethod%20achieves%20improved%20FID%20scores%20on%20CIFAR10%20and%20ImageNet%2064%24%5Ctimes%2464%20and%0ALSUN%20Cat%20256%24%5Ctimes%24256%20datasets%2C%20retains%20zero-shot%20image%20inpainting%0Acapabilities%2C%20and%20uses%20less%20than%20%241/6%24%20of%20the%20original%20batch%20size%20and%20fewer%0Athan%20%241/2%24%20of%20the%20model%20parameters%20and%20training%20steps%20compared%20to%20the%20baseline%0Amethod%2C%20this%20leads%20to%20a%20substantial%20reduction%20in%20resource%20consumption.%20Our%20code%0Ais%20available%3Ahttps%3A//github.com/kong13661/ACT%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14097v3&entry.124074799=Read"},
{"title": "Few-Shot Learning Patterns in Financial Time-Series for Trend-Following\n  Strategies", "author": "Kieran Wood and Samuel Kessler and Stephen J. Roberts and Stefan Zohren", "abstract": "  Forecasting models for systematic trading strategies do not adapt quickly\nwhen financial market conditions rapidly change, as was seen in the advent of\nthe COVID-19 pandemic in 2020, causing many forecasting models to take\nloss-making positions. To deal with such situations, we propose a novel\ntime-series trend-following forecaster that can quickly adapt to new market\nconditions, referred to as regimes. We leverage recent developments from the\ndeep learning community and use few-shot learning. We propose the Cross\nAttentive Time-Series Trend Network -- X-Trend -- which takes positions\nattending over a context set of financial time-series regimes. X-Trend\ntransfers trends from similar patterns in the context set to make forecasts,\nthen subsequently takes positions for a new distinct target regime. By quickly\nadapting to new financial regimes, X-Trend increases Sharpe ratio by 18.9% over\na neural forecaster and 10-fold over a conventional Time-series Momentum\nstrategy during the turbulent market period from 2018 to 2023. Our strategy\nrecovers twice as quickly from the COVID-19 drawdown compared to the\nneural-forecaster. X-Trend can also take zero-shot positions on novel unseen\nfinancial assets obtaining a 5-fold Sharpe ratio increase versus a neural\ntime-series trend forecaster over the same period. Furthermore, the\ncross-attention mechanism allows us to interpret the relationship between\nforecasts and patterns in the context set.\n", "link": "http://arxiv.org/abs/2310.10500v2", "date": "2024-03-28", "relevancy": 1.8569, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4778}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4578}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4465}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Learning%20Patterns%20in%20Financial%20Time-Series%20for%20Trend-Following%0A%20%20Strategies&body=Title%3A%20Few-Shot%20Learning%20Patterns%20in%20Financial%20Time-Series%20for%20Trend-Following%0A%20%20Strategies%0AAuthor%3A%20Kieran%20Wood%20and%20Samuel%20Kessler%20and%20Stephen%20J.%20Roberts%20and%20Stefan%20Zohren%0AAbstract%3A%20%20%20Forecasting%20models%20for%20systematic%20trading%20strategies%20do%20not%20adapt%20quickly%0Awhen%20financial%20market%20conditions%20rapidly%20change%2C%20as%20was%20seen%20in%20the%20advent%20of%0Athe%20COVID-19%20pandemic%20in%202020%2C%20causing%20many%20forecasting%20models%20to%20take%0Aloss-making%20positions.%20To%20deal%20with%20such%20situations%2C%20we%20propose%20a%20novel%0Atime-series%20trend-following%20forecaster%20that%20can%20quickly%20adapt%20to%20new%20market%0Aconditions%2C%20referred%20to%20as%20regimes.%20We%20leverage%20recent%20developments%20from%20the%0Adeep%20learning%20community%20and%20use%20few-shot%20learning.%20We%20propose%20the%20Cross%0AAttentive%20Time-Series%20Trend%20Network%20--%20X-Trend%20--%20which%20takes%20positions%0Aattending%20over%20a%20context%20set%20of%20financial%20time-series%20regimes.%20X-Trend%0Atransfers%20trends%20from%20similar%20patterns%20in%20the%20context%20set%20to%20make%20forecasts%2C%0Athen%20subsequently%20takes%20positions%20for%20a%20new%20distinct%20target%20regime.%20By%20quickly%0Aadapting%20to%20new%20financial%20regimes%2C%20X-Trend%20increases%20Sharpe%20ratio%20by%2018.9%25%20over%0Aa%20neural%20forecaster%20and%2010-fold%20over%20a%20conventional%20Time-series%20Momentum%0Astrategy%20during%20the%20turbulent%20market%20period%20from%202018%20to%202023.%20Our%20strategy%0Arecovers%20twice%20as%20quickly%20from%20the%20COVID-19%20drawdown%20compared%20to%20the%0Aneural-forecaster.%20X-Trend%20can%20also%20take%20zero-shot%20positions%20on%20novel%20unseen%0Afinancial%20assets%20obtaining%20a%205-fold%20Sharpe%20ratio%20increase%20versus%20a%20neural%0Atime-series%20trend%20forecaster%20over%20the%20same%20period.%20Furthermore%2C%20the%0Across-attention%20mechanism%20allows%20us%20to%20interpret%20the%20relationship%20between%0Aforecasts%20and%20patterns%20in%20the%20context%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10500v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Learning%20Patterns%20in%20Financial%20Time-Series%20for%20Trend-Following%0A%20%20Strategies&entry.906535625=Kieran%20Wood%20and%20Samuel%20Kessler%20and%20Stephen%20J.%20Roberts%20and%20Stefan%20Zohren&entry.1292438233=%20%20Forecasting%20models%20for%20systematic%20trading%20strategies%20do%20not%20adapt%20quickly%0Awhen%20financial%20market%20conditions%20rapidly%20change%2C%20as%20was%20seen%20in%20the%20advent%20of%0Athe%20COVID-19%20pandemic%20in%202020%2C%20causing%20many%20forecasting%20models%20to%20take%0Aloss-making%20positions.%20To%20deal%20with%20such%20situations%2C%20we%20propose%20a%20novel%0Atime-series%20trend-following%20forecaster%20that%20can%20quickly%20adapt%20to%20new%20market%0Aconditions%2C%20referred%20to%20as%20regimes.%20We%20leverage%20recent%20developments%20from%20the%0Adeep%20learning%20community%20and%20use%20few-shot%20learning.%20We%20propose%20the%20Cross%0AAttentive%20Time-Series%20Trend%20Network%20--%20X-Trend%20--%20which%20takes%20positions%0Aattending%20over%20a%20context%20set%20of%20financial%20time-series%20regimes.%20X-Trend%0Atransfers%20trends%20from%20similar%20patterns%20in%20the%20context%20set%20to%20make%20forecasts%2C%0Athen%20subsequently%20takes%20positions%20for%20a%20new%20distinct%20target%20regime.%20By%20quickly%0Aadapting%20to%20new%20financial%20regimes%2C%20X-Trend%20increases%20Sharpe%20ratio%20by%2018.9%25%20over%0Aa%20neural%20forecaster%20and%2010-fold%20over%20a%20conventional%20Time-series%20Momentum%0Astrategy%20during%20the%20turbulent%20market%20period%20from%202018%20to%202023.%20Our%20strategy%0Arecovers%20twice%20as%20quickly%20from%20the%20COVID-19%20drawdown%20compared%20to%20the%0Aneural-forecaster.%20X-Trend%20can%20also%20take%20zero-shot%20positions%20on%20novel%20unseen%0Afinancial%20assets%20obtaining%20a%205-fold%20Sharpe%20ratio%20increase%20versus%20a%20neural%0Atime-series%20trend%20forecaster%20over%20the%20same%20period.%20Furthermore%2C%20the%0Across-attention%20mechanism%20allows%20us%20to%20interpret%20the%20relationship%20between%0Aforecasts%20and%20patterns%20in%20the%20context%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10500v2&entry.124074799=Read"},
{"title": "Continual Learning: Applications and the Road Forward", "author": "Eli Verwimp and Rahaf Aljundi and Shai Ben-David and Matthias Bethge and Andrea Cossu and Alexander Gepperth and Tyler L. Hayes and Eyke H\u00fcllermeier and Christopher Kanan and Dhireesha Kudithipudi and Christoph H. Lampert and Martin Mundt and Razvan Pascanu and Adrian Popescu and Andreas S. Tolias and Joost van de Weijer and Bing Liu and Vincenzo Lomonaco and Tinne Tuytelaars and Gido M. van de Ven", "abstract": "  Continual learning is a subfield of machine learning, which aims to allow\nmachine learning models to continuously learn on new data, by accumulating\nknowledge without forgetting what was learned in the past. In this work, we\ntake a step back, and ask: \"Why should one care about continual learning in the\nfirst place?\". We set the stage by examining recent continual learning papers\npublished at four major machine learning conferences, and show that\nmemory-constrained settings dominate the field. Then, we discuss five open\nproblems in machine learning, and even though they might seem unrelated to\ncontinual learning at first sight, we show that continual learning will\ninevitably be part of their solution. These problems are model editing,\npersonalization and specialization, on-device learning, faster (re-)training\nand reinforcement learning. Finally, by comparing the desiderata from these\nunsolved problems and the current assumptions in continual learning, we\nhighlight and discuss four future directions for continual learning research.\nWe hope that this work offers an interesting perspective on the future of\ncontinual learning, while displaying its potential value and the paths we have\nto pursue in order to make it successful. This work is the result of the many\ndiscussions the authors had at the Dagstuhl seminar on Deep Continual Learning,\nin March 2023.\n", "link": "http://arxiv.org/abs/2311.11908v3", "date": "2024-03-28", "relevancy": 1.8541, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4799}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4551}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4504}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Continual%20Learning%3A%20Applications%20and%20the%20Road%20Forward&body=Title%3A%20Continual%20Learning%3A%20Applications%20and%20the%20Road%20Forward%0AAuthor%3A%20Eli%20Verwimp%20and%20Rahaf%20Aljundi%20and%20Shai%20Ben-David%20and%20Matthias%20Bethge%20and%20Andrea%20Cossu%20and%20Alexander%20Gepperth%20and%20Tyler%20L.%20Hayes%20and%20Eyke%20H%C3%BCllermeier%20and%20Christopher%20Kanan%20and%20Dhireesha%20Kudithipudi%20and%20Christoph%20H.%20Lampert%20and%20Martin%20Mundt%20and%20Razvan%20Pascanu%20and%20Adrian%20Popescu%20and%20Andreas%20S.%20Tolias%20and%20Joost%20van%20de%20Weijer%20and%20Bing%20Liu%20and%20Vincenzo%20Lomonaco%20and%20Tinne%20Tuytelaars%20and%20Gido%20M.%20van%20de%20Ven%0AAbstract%3A%20%20%20Continual%20learning%20is%20a%20subfield%20of%20machine%20learning%2C%20which%20aims%20to%20allow%0Amachine%20learning%20models%20to%20continuously%20learn%20on%20new%20data%2C%20by%20accumulating%0Aknowledge%20without%20forgetting%20what%20was%20learned%20in%20the%20past.%20In%20this%20work%2C%20we%0Atake%20a%20step%20back%2C%20and%20ask%3A%20%22Why%20should%20one%20care%20about%20continual%20learning%20in%20the%0Afirst%20place%3F%22.%20We%20set%20the%20stage%20by%20examining%20recent%20continual%20learning%20papers%0Apublished%20at%20four%20major%20machine%20learning%20conferences%2C%20and%20show%20that%0Amemory-constrained%20settings%20dominate%20the%20field.%20Then%2C%20we%20discuss%20five%20open%0Aproblems%20in%20machine%20learning%2C%20and%20even%20though%20they%20might%20seem%20unrelated%20to%0Acontinual%20learning%20at%20first%20sight%2C%20we%20show%20that%20continual%20learning%20will%0Ainevitably%20be%20part%20of%20their%20solution.%20These%20problems%20are%20model%20editing%2C%0Apersonalization%20and%20specialization%2C%20on-device%20learning%2C%20faster%20%28re-%29training%0Aand%20reinforcement%20learning.%20Finally%2C%20by%20comparing%20the%20desiderata%20from%20these%0Aunsolved%20problems%20and%20the%20current%20assumptions%20in%20continual%20learning%2C%20we%0Ahighlight%20and%20discuss%20four%20future%20directions%20for%20continual%20learning%20research.%0AWe%20hope%20that%20this%20work%20offers%20an%20interesting%20perspective%20on%20the%20future%20of%0Acontinual%20learning%2C%20while%20displaying%20its%20potential%20value%20and%20the%20paths%20we%20have%0Ato%20pursue%20in%20order%20to%20make%20it%20successful.%20This%20work%20is%20the%20result%20of%20the%20many%0Adiscussions%20the%20authors%20had%20at%20the%20Dagstuhl%20seminar%20on%20Deep%20Continual%20Learning%2C%0Ain%20March%202023.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11908v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Learning%3A%20Applications%20and%20the%20Road%20Forward&entry.906535625=Eli%20Verwimp%20and%20Rahaf%20Aljundi%20and%20Shai%20Ben-David%20and%20Matthias%20Bethge%20and%20Andrea%20Cossu%20and%20Alexander%20Gepperth%20and%20Tyler%20L.%20Hayes%20and%20Eyke%20H%C3%BCllermeier%20and%20Christopher%20Kanan%20and%20Dhireesha%20Kudithipudi%20and%20Christoph%20H.%20Lampert%20and%20Martin%20Mundt%20and%20Razvan%20Pascanu%20and%20Adrian%20Popescu%20and%20Andreas%20S.%20Tolias%20and%20Joost%20van%20de%20Weijer%20and%20Bing%20Liu%20and%20Vincenzo%20Lomonaco%20and%20Tinne%20Tuytelaars%20and%20Gido%20M.%20van%20de%20Ven&entry.1292438233=%20%20Continual%20learning%20is%20a%20subfield%20of%20machine%20learning%2C%20which%20aims%20to%20allow%0Amachine%20learning%20models%20to%20continuously%20learn%20on%20new%20data%2C%20by%20accumulating%0Aknowledge%20without%20forgetting%20what%20was%20learned%20in%20the%20past.%20In%20this%20work%2C%20we%0Atake%20a%20step%20back%2C%20and%20ask%3A%20%22Why%20should%20one%20care%20about%20continual%20learning%20in%20the%0Afirst%20place%3F%22.%20We%20set%20the%20stage%20by%20examining%20recent%20continual%20learning%20papers%0Apublished%20at%20four%20major%20machine%20learning%20conferences%2C%20and%20show%20that%0Amemory-constrained%20settings%20dominate%20the%20field.%20Then%2C%20we%20discuss%20five%20open%0Aproblems%20in%20machine%20learning%2C%20and%20even%20though%20they%20might%20seem%20unrelated%20to%0Acontinual%20learning%20at%20first%20sight%2C%20we%20show%20that%20continual%20learning%20will%0Ainevitably%20be%20part%20of%20their%20solution.%20These%20problems%20are%20model%20editing%2C%0Apersonalization%20and%20specialization%2C%20on-device%20learning%2C%20faster%20%28re-%29training%0Aand%20reinforcement%20learning.%20Finally%2C%20by%20comparing%20the%20desiderata%20from%20these%0Aunsolved%20problems%20and%20the%20current%20assumptions%20in%20continual%20learning%2C%20we%0Ahighlight%20and%20discuss%20four%20future%20directions%20for%20continual%20learning%20research.%0AWe%20hope%20that%20this%20work%20offers%20an%20interesting%20perspective%20on%20the%20future%20of%0Acontinual%20learning%2C%20while%20displaying%20its%20potential%20value%20and%20the%20paths%20we%20have%0Ato%20pursue%20in%20order%20to%20make%20it%20successful.%20This%20work%20is%20the%20result%20of%20the%20many%0Adiscussions%20the%20authors%20had%20at%20the%20Dagstuhl%20seminar%20on%20Deep%20Continual%20Learning%2C%0Ain%20March%202023.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11908v3&entry.124074799=Read"},
{"title": "Surface-based parcellation and vertex-wise analysis of ultra\n  high-resolution ex vivo 7 tesla MRI in neurodegenerative diseases", "author": "Pulkit Khandelwal and Michael Tran Duong and Constanza Fuentes and Amanda Denning and Winifred Trotman and Ranjit Ittyerah and Alejandra Bahena and Theresa Schuck and Marianna Gabrielyan and Karthik Prabhakaran and Daniel Ohm and Gabor Mizsei and John Robinson and Monica Munoz and John Detre and Edward Lee and David Irwin and Corey McMillan and M. Dylan Tisdall and Sandhitsu Das and David Wolk and Paul A. Yushkevich", "abstract": "  Magnetic resonance imaging (MRI) is the standard modality to understand human\nbrain structure and function in vivo (antemortem). Decades of research in human\nneuroimaging has led to the widespread development of methods and tools to\nprovide automated volume-based segmentations and surface-based parcellations\nwhich help localize brain functions to specialized anatomical regions. Recently\nex vivo (postmortem) imaging of the brain has opened-up avenues to study brain\nstructure at sub-millimeter ultra high-resolution revealing details not\npossible to observe with in vivo MRI. Unfortunately, there has been limited\nmethodological development in ex vivo MRI primarily due to lack of datasets and\nlimited centers with such imaging resources. Therefore, in this work, we\npresent one-of-its-kind dataset of 82 ex vivo T2w whole brain hemispheres MRI\nat 0.3 mm isotropic resolution spanning Alzheimer's disease and related\ndementias. We adapted and developed a fast and easy-to-use automated\nsurface-based pipeline to parcellate, for the first time, ultra high-resolution\nex vivo brain tissue at the native subject space resolution using the\nDesikan-Killiany-Tourville (DKT) brain atlas. This allows us to perform\nvertex-wise analysis in the template space and thereby link morphometry\nmeasures with pathology measurements derived from histology. We will\nopen-source our dataset docker container, Jupyter notebooks for ready-to-use\nout-of-the-box set of tools and command line options to advance ex vivo MRI\nclinical brain imaging research on the project webpage.\n", "link": "http://arxiv.org/abs/2403.19497v1", "date": "2024-03-28", "relevancy": 1.8149, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4593}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4519}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4443}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Surface-based%20parcellation%20and%20vertex-wise%20analysis%20of%20ultra%0A%20%20high-resolution%20ex%20vivo%207%20tesla%20MRI%20in%20neurodegenerative%20diseases&body=Title%3A%20Surface-based%20parcellation%20and%20vertex-wise%20analysis%20of%20ultra%0A%20%20high-resolution%20ex%20vivo%207%20tesla%20MRI%20in%20neurodegenerative%20diseases%0AAuthor%3A%20Pulkit%20Khandelwal%20and%20Michael%20Tran%20Duong%20and%20Constanza%20Fuentes%20and%20Amanda%20Denning%20and%20Winifred%20Trotman%20and%20Ranjit%20Ittyerah%20and%20Alejandra%20Bahena%20and%20Theresa%20Schuck%20and%20Marianna%20Gabrielyan%20and%20Karthik%20Prabhakaran%20and%20Daniel%20Ohm%20and%20Gabor%20Mizsei%20and%20John%20Robinson%20and%20Monica%20Munoz%20and%20John%20Detre%20and%20Edward%20Lee%20and%20David%20Irwin%20and%20Corey%20McMillan%20and%20M.%20Dylan%20Tisdall%20and%20Sandhitsu%20Das%20and%20David%20Wolk%20and%20Paul%20A.%20Yushkevich%0AAbstract%3A%20%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20the%20standard%20modality%20to%20understand%20human%0Abrain%20structure%20and%20function%20in%20vivo%20%28antemortem%29.%20Decades%20of%20research%20in%20human%0Aneuroimaging%20has%20led%20to%20the%20widespread%20development%20of%20methods%20and%20tools%20to%0Aprovide%20automated%20volume-based%20segmentations%20and%20surface-based%20parcellations%0Awhich%20help%20localize%20brain%20functions%20to%20specialized%20anatomical%20regions.%20Recently%0Aex%20vivo%20%28postmortem%29%20imaging%20of%20the%20brain%20has%20opened-up%20avenues%20to%20study%20brain%0Astructure%20at%20sub-millimeter%20ultra%20high-resolution%20revealing%20details%20not%0Apossible%20to%20observe%20with%20in%20vivo%20MRI.%20Unfortunately%2C%20there%20has%20been%20limited%0Amethodological%20development%20in%20ex%20vivo%20MRI%20primarily%20due%20to%20lack%20of%20datasets%20and%0Alimited%20centers%20with%20such%20imaging%20resources.%20Therefore%2C%20in%20this%20work%2C%20we%0Apresent%20one-of-its-kind%20dataset%20of%2082%20ex%20vivo%20T2w%20whole%20brain%20hemispheres%20MRI%0Aat%200.3%20mm%20isotropic%20resolution%20spanning%20Alzheimer%27s%20disease%20and%20related%0Adementias.%20We%20adapted%20and%20developed%20a%20fast%20and%20easy-to-use%20automated%0Asurface-based%20pipeline%20to%20parcellate%2C%20for%20the%20first%20time%2C%20ultra%20high-resolution%0Aex%20vivo%20brain%20tissue%20at%20the%20native%20subject%20space%20resolution%20using%20the%0ADesikan-Killiany-Tourville%20%28DKT%29%20brain%20atlas.%20This%20allows%20us%20to%20perform%0Avertex-wise%20analysis%20in%20the%20template%20space%20and%20thereby%20link%20morphometry%0Ameasures%20with%20pathology%20measurements%20derived%20from%20histology.%20We%20will%0Aopen-source%20our%20dataset%20docker%20container%2C%20Jupyter%20notebooks%20for%20ready-to-use%0Aout-of-the-box%20set%20of%20tools%20and%20command%20line%20options%20to%20advance%20ex%20vivo%20MRI%0Aclinical%20brain%20imaging%20research%20on%20the%20project%20webpage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19497v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surface-based%20parcellation%20and%20vertex-wise%20analysis%20of%20ultra%0A%20%20high-resolution%20ex%20vivo%207%20tesla%20MRI%20in%20neurodegenerative%20diseases&entry.906535625=Pulkit%20Khandelwal%20and%20Michael%20Tran%20Duong%20and%20Constanza%20Fuentes%20and%20Amanda%20Denning%20and%20Winifred%20Trotman%20and%20Ranjit%20Ittyerah%20and%20Alejandra%20Bahena%20and%20Theresa%20Schuck%20and%20Marianna%20Gabrielyan%20and%20Karthik%20Prabhakaran%20and%20Daniel%20Ohm%20and%20Gabor%20Mizsei%20and%20John%20Robinson%20and%20Monica%20Munoz%20and%20John%20Detre%20and%20Edward%20Lee%20and%20David%20Irwin%20and%20Corey%20McMillan%20and%20M.%20Dylan%20Tisdall%20and%20Sandhitsu%20Das%20and%20David%20Wolk%20and%20Paul%20A.%20Yushkevich&entry.1292438233=%20%20Magnetic%20resonance%20imaging%20%28MRI%29%20is%20the%20standard%20modality%20to%20understand%20human%0Abrain%20structure%20and%20function%20in%20vivo%20%28antemortem%29.%20Decades%20of%20research%20in%20human%0Aneuroimaging%20has%20led%20to%20the%20widespread%20development%20of%20methods%20and%20tools%20to%0Aprovide%20automated%20volume-based%20segmentations%20and%20surface-based%20parcellations%0Awhich%20help%20localize%20brain%20functions%20to%20specialized%20anatomical%20regions.%20Recently%0Aex%20vivo%20%28postmortem%29%20imaging%20of%20the%20brain%20has%20opened-up%20avenues%20to%20study%20brain%0Astructure%20at%20sub-millimeter%20ultra%20high-resolution%20revealing%20details%20not%0Apossible%20to%20observe%20with%20in%20vivo%20MRI.%20Unfortunately%2C%20there%20has%20been%20limited%0Amethodological%20development%20in%20ex%20vivo%20MRI%20primarily%20due%20to%20lack%20of%20datasets%20and%0Alimited%20centers%20with%20such%20imaging%20resources.%20Therefore%2C%20in%20this%20work%2C%20we%0Apresent%20one-of-its-kind%20dataset%20of%2082%20ex%20vivo%20T2w%20whole%20brain%20hemispheres%20MRI%0Aat%200.3%20mm%20isotropic%20resolution%20spanning%20Alzheimer%27s%20disease%20and%20related%0Adementias.%20We%20adapted%20and%20developed%20a%20fast%20and%20easy-to-use%20automated%0Asurface-based%20pipeline%20to%20parcellate%2C%20for%20the%20first%20time%2C%20ultra%20high-resolution%0Aex%20vivo%20brain%20tissue%20at%20the%20native%20subject%20space%20resolution%20using%20the%0ADesikan-Killiany-Tourville%20%28DKT%29%20brain%20atlas.%20This%20allows%20us%20to%20perform%0Avertex-wise%20analysis%20in%20the%20template%20space%20and%20thereby%20link%20morphometry%0Ameasures%20with%20pathology%20measurements%20derived%20from%20histology.%20We%20will%0Aopen-source%20our%20dataset%20docker%20container%2C%20Jupyter%20notebooks%20for%20ready-to-use%0Aout-of-the-box%20set%20of%20tools%20and%20command%20line%20options%20to%20advance%20ex%20vivo%20MRI%0Aclinical%20brain%20imaging%20research%20on%20the%20project%20webpage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19497v1&entry.124074799=Read"},
{"title": "Self-Discovering Interpretable Diffusion Latent Directions for\n  Responsible Text-to-Image Generation", "author": "Hang Li and Chengzhi Shen and Philip Torr and Volker Tresp and Jindong Gu", "abstract": "  Diffusion-based models have gained significant popularity for text-to-image\ngeneration due to their exceptional image-generation capabilities. A risk with\nthese models is the potential generation of inappropriate content, such as\nbiased or harmful images. However, the underlying reasons for generating such\nundesired content from the perspective of the diffusion model's internal\nrepresentation remain unclear. Previous work interprets vectors in an\ninterpretable latent space of diffusion models as semantic concepts. However,\nexisting approaches cannot discover directions for arbitrary concepts, such as\nthose related to inappropriate concepts. In this work, we propose a novel\nself-supervised approach to find interpretable latent directions for a given\nconcept. With the discovered vectors, we further propose a simple approach to\nmitigate inappropriate generation. Extensive experiments have been conducted to\nverify the effectiveness of our mitigation approach, namely, for fair\ngeneration, safe generation, and responsible text-enhancing generation. Project\npage: \\url{https://interpretdiffusion.github.io}.\n", "link": "http://arxiv.org/abs/2311.17216v2", "date": "2024-03-28", "relevancy": 1.8506, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6264}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6172}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6065}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-Discovering%20Interpretable%20Diffusion%20Latent%20Directions%20for%0A%20%20Responsible%20Text-to-Image%20Generation&body=Title%3A%20Self-Discovering%20Interpretable%20Diffusion%20Latent%20Directions%20for%0A%20%20Responsible%20Text-to-Image%20Generation%0AAuthor%3A%20Hang%20Li%20and%20Chengzhi%20Shen%20and%20Philip%20Torr%20and%20Volker%20Tresp%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Diffusion-based%20models%20have%20gained%20significant%20popularity%20for%20text-to-image%0Ageneration%20due%20to%20their%20exceptional%20image-generation%20capabilities.%20A%20risk%20with%0Athese%20models%20is%20the%20potential%20generation%20of%20inappropriate%20content%2C%20such%20as%0Abiased%20or%20harmful%20images.%20However%2C%20the%20underlying%20reasons%20for%20generating%20such%0Aundesired%20content%20from%20the%20perspective%20of%20the%20diffusion%20model%27s%20internal%0Arepresentation%20remain%20unclear.%20Previous%20work%20interprets%20vectors%20in%20an%0Ainterpretable%20latent%20space%20of%20diffusion%20models%20as%20semantic%20concepts.%20However%2C%0Aexisting%20approaches%20cannot%20discover%20directions%20for%20arbitrary%20concepts%2C%20such%20as%0Athose%20related%20to%20inappropriate%20concepts.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aself-supervised%20approach%20to%20find%20interpretable%20latent%20directions%20for%20a%20given%0Aconcept.%20With%20the%20discovered%20vectors%2C%20we%20further%20propose%20a%20simple%20approach%20to%0Amitigate%20inappropriate%20generation.%20Extensive%20experiments%20have%20been%20conducted%20to%0Averify%20the%20effectiveness%20of%20our%20mitigation%20approach%2C%20namely%2C%20for%20fair%0Ageneration%2C%20safe%20generation%2C%20and%20responsible%20text-enhancing%20generation.%20Project%0Apage%3A%20%5Curl%7Bhttps%3A//interpretdiffusion.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17216v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Discovering%20Interpretable%20Diffusion%20Latent%20Directions%20for%0A%20%20Responsible%20Text-to-Image%20Generation&entry.906535625=Hang%20Li%20and%20Chengzhi%20Shen%20and%20Philip%20Torr%20and%20Volker%20Tresp%20and%20Jindong%20Gu&entry.1292438233=%20%20Diffusion-based%20models%20have%20gained%20significant%20popularity%20for%20text-to-image%0Ageneration%20due%20to%20their%20exceptional%20image-generation%20capabilities.%20A%20risk%20with%0Athese%20models%20is%20the%20potential%20generation%20of%20inappropriate%20content%2C%20such%20as%0Abiased%20or%20harmful%20images.%20However%2C%20the%20underlying%20reasons%20for%20generating%20such%0Aundesired%20content%20from%20the%20perspective%20of%20the%20diffusion%20model%27s%20internal%0Arepresentation%20remain%20unclear.%20Previous%20work%20interprets%20vectors%20in%20an%0Ainterpretable%20latent%20space%20of%20diffusion%20models%20as%20semantic%20concepts.%20However%2C%0Aexisting%20approaches%20cannot%20discover%20directions%20for%20arbitrary%20concepts%2C%20such%20as%0Athose%20related%20to%20inappropriate%20concepts.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aself-supervised%20approach%20to%20find%20interpretable%20latent%20directions%20for%20a%20given%0Aconcept.%20With%20the%20discovered%20vectors%2C%20we%20further%20propose%20a%20simple%20approach%20to%0Amitigate%20inappropriate%20generation.%20Extensive%20experiments%20have%20been%20conducted%20to%0Averify%20the%20effectiveness%20of%20our%20mitigation%20approach%2C%20namely%2C%20for%20fair%0Ageneration%2C%20safe%20generation%2C%20and%20responsible%20text-enhancing%20generation.%20Project%0Apage%3A%20%5Curl%7Bhttps%3A//interpretdiffusion.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17216v2&entry.124074799=Read"},
{"title": "Multi-Agent Team Access Monitoring: Environments that Benefit from\n  Target Information Sharing", "author": "Andrew Dudash and Scott James and Ryan Rubel", "abstract": "  Robotic access monitoring of multiple target areas has applications including\ncheckpoint enforcement, surveillance and containment of fire and flood hazards.\nMonitoring access for a single target region has been successfully modeled as a\nminimum-cut problem. We generalize this model to support multiple target areas\nusing two approaches: iterating on individual targets and examining the\ncollections of targets holistically. Through simulation we measure the\nperformance of each approach on different scenarios.\n", "link": "http://arxiv.org/abs/2403.19375v1", "date": "2024-03-28", "relevancy": 1.5542, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5209}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5016}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Team%20Access%20Monitoring%3A%20Environments%20that%20Benefit%20from%0A%20%20Target%20Information%20Sharing&body=Title%3A%20Multi-Agent%20Team%20Access%20Monitoring%3A%20Environments%20that%20Benefit%20from%0A%20%20Target%20Information%20Sharing%0AAuthor%3A%20Andrew%20Dudash%20and%20Scott%20James%20and%20Ryan%20Rubel%0AAbstract%3A%20%20%20Robotic%20access%20monitoring%20of%20multiple%20target%20areas%20has%20applications%20including%0Acheckpoint%20enforcement%2C%20surveillance%20and%20containment%20of%20fire%20and%20flood%20hazards.%0AMonitoring%20access%20for%20a%20single%20target%20region%20has%20been%20successfully%20modeled%20as%20a%0Aminimum-cut%20problem.%20We%20generalize%20this%20model%20to%20support%20multiple%20target%20areas%0Ausing%20two%20approaches%3A%20iterating%20on%20individual%20targets%20and%20examining%20the%0Acollections%20of%20targets%20holistically.%20Through%20simulation%20we%20measure%20the%0Aperformance%20of%20each%20approach%20on%20different%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19375v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Team%20Access%20Monitoring%3A%20Environments%20that%20Benefit%20from%0A%20%20Target%20Information%20Sharing&entry.906535625=Andrew%20Dudash%20and%20Scott%20James%20and%20Ryan%20Rubel&entry.1292438233=%20%20Robotic%20access%20monitoring%20of%20multiple%20target%20areas%20has%20applications%20including%0Acheckpoint%20enforcement%2C%20surveillance%20and%20containment%20of%20fire%20and%20flood%20hazards.%0AMonitoring%20access%20for%20a%20single%20target%20region%20has%20been%20successfully%20modeled%20as%20a%0Aminimum-cut%20problem.%20We%20generalize%20this%20model%20to%20support%20multiple%20target%20areas%0Ausing%20two%20approaches%3A%20iterating%20on%20individual%20targets%20and%20examining%20the%0Acollections%20of%20targets%20holistically.%20Through%20simulation%20we%20measure%20the%0Aperformance%20of%20each%20approach%20on%20different%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19375v1&entry.124074799=Read"},
{"title": "XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized\n  Manifold", "author": "Guangyu Wang and Jinzhi Zhang and Fan Wang and Ruqi Huang and Lu Fang", "abstract": "  We propose XScale-NVS for high-fidelity cross-scale novel view synthesis of\nreal-world large-scale scenes. Existing representations based on explicit\nsurface suffer from discretization resolution or UV distortion, while implicit\nvolumetric representations lack scalability for large scenes due to the\ndispersed weight distribution and surface ambiguity. In light of the above\nchallenges, we introduce hash featurized manifold, a novel hash-based\nfeaturization coupled with a deferred neural rendering framework. This approach\nfully unlocks the expressivity of the representation by explicitly\nconcentrating the hash entries on the 2D manifold, thus effectively\nrepresenting highly detailed contents independent of the discretization\nresolution. We also introduce a novel dataset, namely GigaNVS, to benchmark\ncross-scale, high-resolution novel view synthesis of realworld large-scale\nscenes. Our method significantly outperforms competing baselines on various\nreal-world scenes, yielding an average LPIPS that is 40% lower than prior\nstate-of-the-art on the challenging GigaNVS benchmark. Please see our project\npage at: xscalenvs.github.io.\n", "link": "http://arxiv.org/abs/2403.19517v1", "date": "2024-03-28", "relevancy": 1.5786, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5483}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5249}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5179}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20XScale-NVS%3A%20Cross-Scale%20Novel%20View%20Synthesis%20with%20Hash%20Featurized%0A%20%20Manifold&body=Title%3A%20XScale-NVS%3A%20Cross-Scale%20Novel%20View%20Synthesis%20with%20Hash%20Featurized%0A%20%20Manifold%0AAuthor%3A%20Guangyu%20Wang%20and%20Jinzhi%20Zhang%20and%20Fan%20Wang%20and%20Ruqi%20Huang%20and%20Lu%20Fang%0AAbstract%3A%20%20%20We%20propose%20XScale-NVS%20for%20high-fidelity%20cross-scale%20novel%20view%20synthesis%20of%0Areal-world%20large-scale%20scenes.%20Existing%20representations%20based%20on%20explicit%0Asurface%20suffer%20from%20discretization%20resolution%20or%20UV%20distortion%2C%20while%20implicit%0Avolumetric%20representations%20lack%20scalability%20for%20large%20scenes%20due%20to%20the%0Adispersed%20weight%20distribution%20and%20surface%20ambiguity.%20In%20light%20of%20the%20above%0Achallenges%2C%20we%20introduce%20hash%20featurized%20manifold%2C%20a%20novel%20hash-based%0Afeaturization%20coupled%20with%20a%20deferred%20neural%20rendering%20framework.%20This%20approach%0Afully%20unlocks%20the%20expressivity%20of%20the%20representation%20by%20explicitly%0Aconcentrating%20the%20hash%20entries%20on%20the%202D%20manifold%2C%20thus%20effectively%0Arepresenting%20highly%20detailed%20contents%20independent%20of%20the%20discretization%0Aresolution.%20We%20also%20introduce%20a%20novel%20dataset%2C%20namely%20GigaNVS%2C%20to%20benchmark%0Across-scale%2C%20high-resolution%20novel%20view%20synthesis%20of%20realworld%20large-scale%0Ascenes.%20Our%20method%20significantly%20outperforms%20competing%20baselines%20on%20various%0Areal-world%20scenes%2C%20yielding%20an%20average%20LPIPS%20that%20is%2040%25%20lower%20than%20prior%0Astate-of-the-art%20on%20the%20challenging%20GigaNVS%20benchmark.%20Please%20see%20our%20project%0Apage%20at%3A%20xscalenvs.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19517v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XScale-NVS%3A%20Cross-Scale%20Novel%20View%20Synthesis%20with%20Hash%20Featurized%0A%20%20Manifold&entry.906535625=Guangyu%20Wang%20and%20Jinzhi%20Zhang%20and%20Fan%20Wang%20and%20Ruqi%20Huang%20and%20Lu%20Fang&entry.1292438233=%20%20We%20propose%20XScale-NVS%20for%20high-fidelity%20cross-scale%20novel%20view%20synthesis%20of%0Areal-world%20large-scale%20scenes.%20Existing%20representations%20based%20on%20explicit%0Asurface%20suffer%20from%20discretization%20resolution%20or%20UV%20distortion%2C%20while%20implicit%0Avolumetric%20representations%20lack%20scalability%20for%20large%20scenes%20due%20to%20the%0Adispersed%20weight%20distribution%20and%20surface%20ambiguity.%20In%20light%20of%20the%20above%0Achallenges%2C%20we%20introduce%20hash%20featurized%20manifold%2C%20a%20novel%20hash-based%0Afeaturization%20coupled%20with%20a%20deferred%20neural%20rendering%20framework.%20This%20approach%0Afully%20unlocks%20the%20expressivity%20of%20the%20representation%20by%20explicitly%0Aconcentrating%20the%20hash%20entries%20on%20the%202D%20manifold%2C%20thus%20effectively%0Arepresenting%20highly%20detailed%20contents%20independent%20of%20the%20discretization%0Aresolution.%20We%20also%20introduce%20a%20novel%20dataset%2C%20namely%20GigaNVS%2C%20to%20benchmark%0Across-scale%2C%20high-resolution%20novel%20view%20synthesis%20of%20realworld%20large-scale%0Ascenes.%20Our%20method%20significantly%20outperforms%20competing%20baselines%20on%20various%0Areal-world%20scenes%2C%20yielding%20an%20average%20LPIPS%20that%20is%2040%25%20lower%20than%20prior%0Astate-of-the-art%20on%20the%20challenging%20GigaNVS%20benchmark.%20Please%20see%20our%20project%0Apage%20at%3A%20xscalenvs.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19517v1&entry.124074799=Read"},
{"title": "Top-$k$ Classification and Cardinality-Aware Prediction", "author": "Anqi Mao and Mehryar Mohri and Yutao Zhong", "abstract": "  We present a detailed study of top-$k$ classification, the task of predicting\nthe $k$ most probable classes for an input, extending beyond single-class\nprediction. We demonstrate that several prevalent surrogate loss functions in\nmulti-class classification, such as comp-sum and constrained losses, are\nsupported by $H$-consistency bounds with respect to the top-$k$ loss. These\nbounds guarantee consistency in relation to the hypothesis set $H$, providing\nstronger guarantees than Bayes-consistency due to their non-asymptotic and\nhypothesis-set specific nature. To address the trade-off between accuracy and\ncardinality $k$, we further introduce cardinality-aware loss functions through\ninstance-dependent cost-sensitive learning. For these functions, we derive\ncost-sensitive comp-sum and constrained surrogate losses, establishing their\n$H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to\nnew cardinality-aware algorithms for top-$k$ classification. We report the\nresults of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN\ndatasets demonstrating the effectiveness and benefit of these algorithms.\n", "link": "http://arxiv.org/abs/2403.19625v1", "date": "2024-03-28", "relevancy": 1.8385, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4785}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4667}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.445}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Top-%24k%24%20Classification%20and%20Cardinality-Aware%20Prediction&body=Title%3A%20Top-%24k%24%20Classification%20and%20Cardinality-Aware%20Prediction%0AAuthor%3A%20Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong%0AAbstract%3A%20%20%20We%20present%20a%20detailed%20study%20of%20top-%24k%24%20classification%2C%20the%20task%20of%20predicting%0Athe%20%24k%24%20most%20probable%20classes%20for%20an%20input%2C%20extending%20beyond%20single-class%0Aprediction.%20We%20demonstrate%20that%20several%20prevalent%20surrogate%20loss%20functions%20in%0Amulti-class%20classification%2C%20such%20as%20comp-sum%20and%20constrained%20losses%2C%20are%0Asupported%20by%20%24H%24-consistency%20bounds%20with%20respect%20to%20the%20top-%24k%24%20loss.%20These%0Abounds%20guarantee%20consistency%20in%20relation%20to%20the%20hypothesis%20set%20%24H%24%2C%20providing%0Astronger%20guarantees%20than%20Bayes-consistency%20due%20to%20their%20non-asymptotic%20and%0Ahypothesis-set%20specific%20nature.%20To%20address%20the%20trade-off%20between%20accuracy%20and%0Acardinality%20%24k%24%2C%20we%20further%20introduce%20cardinality-aware%20loss%20functions%20through%0Ainstance-dependent%20cost-sensitive%20learning.%20For%20these%20functions%2C%20we%20derive%0Acost-sensitive%20comp-sum%20and%20constrained%20surrogate%20losses%2C%20establishing%20their%0A%24H%24-consistency%20bounds%20and%20Bayes-consistency.%20Minimizing%20these%20losses%20leads%20to%0Anew%20cardinality-aware%20algorithms%20for%20top-%24k%24%20classification.%20We%20report%20the%0Aresults%20of%20extensive%20experiments%20on%20CIFAR-100%2C%20ImageNet%2C%20CIFAR-10%2C%20and%20SVHN%0Adatasets%20demonstrating%20the%20effectiveness%20and%20benefit%20of%20these%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19625v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Top-%24k%24%20Classification%20and%20Cardinality-Aware%20Prediction&entry.906535625=Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong&entry.1292438233=%20%20We%20present%20a%20detailed%20study%20of%20top-%24k%24%20classification%2C%20the%20task%20of%20predicting%0Athe%20%24k%24%20most%20probable%20classes%20for%20an%20input%2C%20extending%20beyond%20single-class%0Aprediction.%20We%20demonstrate%20that%20several%20prevalent%20surrogate%20loss%20functions%20in%0Amulti-class%20classification%2C%20such%20as%20comp-sum%20and%20constrained%20losses%2C%20are%0Asupported%20by%20%24H%24-consistency%20bounds%20with%20respect%20to%20the%20top-%24k%24%20loss.%20These%0Abounds%20guarantee%20consistency%20in%20relation%20to%20the%20hypothesis%20set%20%24H%24%2C%20providing%0Astronger%20guarantees%20than%20Bayes-consistency%20due%20to%20their%20non-asymptotic%20and%0Ahypothesis-set%20specific%20nature.%20To%20address%20the%20trade-off%20between%20accuracy%20and%0Acardinality%20%24k%24%2C%20we%20further%20introduce%20cardinality-aware%20loss%20functions%20through%0Ainstance-dependent%20cost-sensitive%20learning.%20For%20these%20functions%2C%20we%20derive%0Acost-sensitive%20comp-sum%20and%20constrained%20surrogate%20losses%2C%20establishing%20their%0A%24H%24-consistency%20bounds%20and%20Bayes-consistency.%20Minimizing%20these%20losses%20leads%20to%0Anew%20cardinality-aware%20algorithms%20for%20top-%24k%24%20classification.%20We%20report%20the%0Aresults%20of%20extensive%20experiments%20on%20CIFAR-100%2C%20ImageNet%2C%20CIFAR-10%2C%20and%20SVHN%0Adatasets%20demonstrating%20the%20effectiveness%20and%20benefit%20of%20these%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19625v1&entry.124074799=Read"},
{"title": "The Role of Syntactic Span Preferences in Post-Hoc Explanation\n  Disagreement", "author": "Jonathan Kamp and Lisa Beinborn and Antske Fokkens", "abstract": "  Post-hoc explanation methods are an important tool for increasing model\ntransparency for users. Unfortunately, the currently used methods for\nattributing token importance often yield diverging patterns. In this work, we\nstudy potential sources of disagreement across methods from a linguistic\nperspective. We find that different methods systematically select different\nclasses of words and that methods that agree most with other methods and with\nhumans display similar linguistic preferences. Token-level differences between\nmethods are smoothed out if we compare them on the syntactic span level. We\nalso find higher agreement across methods by estimating the most important\nspans dynamically instead of relying on a fixed subset of size $k$. We\nsystematically investigate the interaction between $k$ and spans and propose an\nimproved configuration for selecting important tokens.\n", "link": "http://arxiv.org/abs/2403.19424v1", "date": "2024-03-28", "relevancy": 1.1708, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3958}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3903}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.388}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Syntactic%20Span%20Preferences%20in%20Post-Hoc%20Explanation%0A%20%20Disagreement&body=Title%3A%20The%20Role%20of%20Syntactic%20Span%20Preferences%20in%20Post-Hoc%20Explanation%0A%20%20Disagreement%0AAuthor%3A%20Jonathan%20Kamp%20and%20Lisa%20Beinborn%20and%20Antske%20Fokkens%0AAbstract%3A%20%20%20Post-hoc%20explanation%20methods%20are%20an%20important%20tool%20for%20increasing%20model%0Atransparency%20for%20users.%20Unfortunately%2C%20the%20currently%20used%20methods%20for%0Aattributing%20token%20importance%20often%20yield%20diverging%20patterns.%20In%20this%20work%2C%20we%0Astudy%20potential%20sources%20of%20disagreement%20across%20methods%20from%20a%20linguistic%0Aperspective.%20We%20find%20that%20different%20methods%20systematically%20select%20different%0Aclasses%20of%20words%20and%20that%20methods%20that%20agree%20most%20with%20other%20methods%20and%20with%0Ahumans%20display%20similar%20linguistic%20preferences.%20Token-level%20differences%20between%0Amethods%20are%20smoothed%20out%20if%20we%20compare%20them%20on%20the%20syntactic%20span%20level.%20We%0Aalso%20find%20higher%20agreement%20across%20methods%20by%20estimating%20the%20most%20important%0Aspans%20dynamically%20instead%20of%20relying%20on%20a%20fixed%20subset%20of%20size%20%24k%24.%20We%0Asystematically%20investigate%20the%20interaction%20between%20%24k%24%20and%20spans%20and%20propose%20an%0Aimproved%20configuration%20for%20selecting%20important%20tokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19424v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Syntactic%20Span%20Preferences%20in%20Post-Hoc%20Explanation%0A%20%20Disagreement&entry.906535625=Jonathan%20Kamp%20and%20Lisa%20Beinborn%20and%20Antske%20Fokkens&entry.1292438233=%20%20Post-hoc%20explanation%20methods%20are%20an%20important%20tool%20for%20increasing%20model%0Atransparency%20for%20users.%20Unfortunately%2C%20the%20currently%20used%20methods%20for%0Aattributing%20token%20importance%20often%20yield%20diverging%20patterns.%20In%20this%20work%2C%20we%0Astudy%20potential%20sources%20of%20disagreement%20across%20methods%20from%20a%20linguistic%0Aperspective.%20We%20find%20that%20different%20methods%20systematically%20select%20different%0Aclasses%20of%20words%20and%20that%20methods%20that%20agree%20most%20with%20other%20methods%20and%20with%0Ahumans%20display%20similar%20linguistic%20preferences.%20Token-level%20differences%20between%0Amethods%20are%20smoothed%20out%20if%20we%20compare%20them%20on%20the%20syntactic%20span%20level.%20We%0Aalso%20find%20higher%20agreement%20across%20methods%20by%20estimating%20the%20most%20important%0Aspans%20dynamically%20instead%20of%20relying%20on%20a%20fixed%20subset%20of%20size%20%24k%24.%20We%0Asystematically%20investigate%20the%20interaction%20between%20%24k%24%20and%20spans%20and%20propose%20an%0Aimproved%20configuration%20for%20selecting%20important%20tokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19424v1&entry.124074799=Read"},
{"title": "MRNaB: Mixed Reality-based Robot Navigation Interface using\n  Optical-see-through MR-beacon", "author": "Eduardo Iglesius and Masato Kobayashi and Yuki Uranishi and Haruo Takemura", "abstract": "  Recent advancements in robotics have led to the development of numerous\ninterfaces to enhance the intuitiveness of robot navigation. However, the\nreliance on traditional 2D displays imposes limitations on the simultaneous\nvisualization of information. Mixed Reality (MR) technology addresses this\nissue by enhancing the dimensionality of information visualization, allowing\nusers to perceive multiple pieces of information concurrently. This paper\nproposes Mixed reality-based robot navigation interface using an\noptical-see-through MR-beacon (MRNaB), a novel approach that incorporates an\nMR-beacon, situated atop the real-world environment, to function as a signal\ntransmitter for robot navigation. This MR-beacon is designed to be persistent,\neliminating the need for repeated navigation inputs for the same location. Our\nsystem is mainly constructed into four primary functions: \"Add\", \"Move\",\n\"Delete\", and \"Select\". These allow for the addition of a MR-beacon, location\nmovement, its deletion, and the selection of MR-beacon for navigation purposes,\nrespectively. The effectiveness of the proposed method was then validated\nthrough experiments by comparing it with the traditional 2D system. As the\nresult, MRNaB was proven to increase the performance of the user when doing\nnavigation to a certain place subjectively and objectively. For additional\nmaterial, please check: https://mertcookimg.github.io/mrnab\n", "link": "http://arxiv.org/abs/2403.19310v1", "date": "2024-03-28", "relevancy": 1.7324, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5983}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5689}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5339}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MRNaB%3A%20Mixed%20Reality-based%20Robot%20Navigation%20Interface%20using%0A%20%20Optical-see-through%20MR-beacon&body=Title%3A%20MRNaB%3A%20Mixed%20Reality-based%20Robot%20Navigation%20Interface%20using%0A%20%20Optical-see-through%20MR-beacon%0AAuthor%3A%20Eduardo%20Iglesius%20and%20Masato%20Kobayashi%20and%20Yuki%20Uranishi%20and%20Haruo%20Takemura%0AAbstract%3A%20%20%20Recent%20advancements%20in%20robotics%20have%20led%20to%20the%20development%20of%20numerous%0Ainterfaces%20to%20enhance%20the%20intuitiveness%20of%20robot%20navigation.%20However%2C%20the%0Areliance%20on%20traditional%202D%20displays%20imposes%20limitations%20on%20the%20simultaneous%0Avisualization%20of%20information.%20Mixed%20Reality%20%28MR%29%20technology%20addresses%20this%0Aissue%20by%20enhancing%20the%20dimensionality%20of%20information%20visualization%2C%20allowing%0Ausers%20to%20perceive%20multiple%20pieces%20of%20information%20concurrently.%20This%20paper%0Aproposes%20Mixed%20reality-based%20robot%20navigation%20interface%20using%20an%0Aoptical-see-through%20MR-beacon%20%28MRNaB%29%2C%20a%20novel%20approach%20that%20incorporates%20an%0AMR-beacon%2C%20situated%20atop%20the%20real-world%20environment%2C%20to%20function%20as%20a%20signal%0Atransmitter%20for%20robot%20navigation.%20This%20MR-beacon%20is%20designed%20to%20be%20persistent%2C%0Aeliminating%20the%20need%20for%20repeated%20navigation%20inputs%20for%20the%20same%20location.%20Our%0Asystem%20is%20mainly%20constructed%20into%20four%20primary%20functions%3A%20%22Add%22%2C%20%22Move%22%2C%0A%22Delete%22%2C%20and%20%22Select%22.%20These%20allow%20for%20the%20addition%20of%20a%20MR-beacon%2C%20location%0Amovement%2C%20its%20deletion%2C%20and%20the%20selection%20of%20MR-beacon%20for%20navigation%20purposes%2C%0Arespectively.%20The%20effectiveness%20of%20the%20proposed%20method%20was%20then%20validated%0Athrough%20experiments%20by%20comparing%20it%20with%20the%20traditional%202D%20system.%20As%20the%0Aresult%2C%20MRNaB%20was%20proven%20to%20increase%20the%20performance%20of%20the%20user%20when%20doing%0Anavigation%20to%20a%20certain%20place%20subjectively%20and%20objectively.%20For%20additional%0Amaterial%2C%20please%20check%3A%20https%3A//mertcookimg.github.io/mrnab%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19310v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRNaB%3A%20Mixed%20Reality-based%20Robot%20Navigation%20Interface%20using%0A%20%20Optical-see-through%20MR-beacon&entry.906535625=Eduardo%20Iglesius%20and%20Masato%20Kobayashi%20and%20Yuki%20Uranishi%20and%20Haruo%20Takemura&entry.1292438233=%20%20Recent%20advancements%20in%20robotics%20have%20led%20to%20the%20development%20of%20numerous%0Ainterfaces%20to%20enhance%20the%20intuitiveness%20of%20robot%20navigation.%20However%2C%20the%0Areliance%20on%20traditional%202D%20displays%20imposes%20limitations%20on%20the%20simultaneous%0Avisualization%20of%20information.%20Mixed%20Reality%20%28MR%29%20technology%20addresses%20this%0Aissue%20by%20enhancing%20the%20dimensionality%20of%20information%20visualization%2C%20allowing%0Ausers%20to%20perceive%20multiple%20pieces%20of%20information%20concurrently.%20This%20paper%0Aproposes%20Mixed%20reality-based%20robot%20navigation%20interface%20using%20an%0Aoptical-see-through%20MR-beacon%20%28MRNaB%29%2C%20a%20novel%20approach%20that%20incorporates%20an%0AMR-beacon%2C%20situated%20atop%20the%20real-world%20environment%2C%20to%20function%20as%20a%20signal%0Atransmitter%20for%20robot%20navigation.%20This%20MR-beacon%20is%20designed%20to%20be%20persistent%2C%0Aeliminating%20the%20need%20for%20repeated%20navigation%20inputs%20for%20the%20same%20location.%20Our%0Asystem%20is%20mainly%20constructed%20into%20four%20primary%20functions%3A%20%22Add%22%2C%20%22Move%22%2C%0A%22Delete%22%2C%20and%20%22Select%22.%20These%20allow%20for%20the%20addition%20of%20a%20MR-beacon%2C%20location%0Amovement%2C%20its%20deletion%2C%20and%20the%20selection%20of%20MR-beacon%20for%20navigation%20purposes%2C%0Arespectively.%20The%20effectiveness%20of%20the%20proposed%20method%20was%20then%20validated%0Athrough%20experiments%20by%20comparing%20it%20with%20the%20traditional%202D%20system.%20As%20the%0Aresult%2C%20MRNaB%20was%20proven%20to%20increase%20the%20performance%20of%20the%20user%20when%20doing%0Anavigation%20to%20a%20certain%20place%20subjectively%20and%20objectively.%20For%20additional%0Amaterial%2C%20please%20check%3A%20https%3A//mertcookimg.github.io/mrnab%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19310v1&entry.124074799=Read"},
{"title": "On Uncertainty Quantification for Near-Bayes Optimal Algorithms", "author": "Ziyu Wang and Chris Holmes", "abstract": "  Bayesian modelling allows for the quantification of predictive uncertainty\nwhich is crucial in safety-critical applications. Yet for many machine learning\n(ML) algorithms, it is difficult to construct or implement their Bayesian\ncounterpart. In this work we present a promising approach to address this\nchallenge, based on the hypothesis that commonly used ML algorithms are\nefficient across a wide variety of tasks and may thus be near Bayes-optimal\nw.r.t. an unknown task distribution. We prove that it is possible to recover\nthe Bayesian posterior defined by the task distribution, which is unknown but\noptimal in this setting, by building a martingale posterior using the\nalgorithm. We further propose a practical uncertainty quantification method\nthat apply to general ML algorithms. Experiments based on a variety of non-NN\nand NN algorithms demonstrate the efficacy of our method.\n", "link": "http://arxiv.org/abs/2403.19381v1", "date": "2024-03-28", "relevancy": 1.6231, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5359}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20Uncertainty%20Quantification%20for%20Near-Bayes%20Optimal%20Algorithms&body=Title%3A%20On%20Uncertainty%20Quantification%20for%20Near-Bayes%20Optimal%20Algorithms%0AAuthor%3A%20Ziyu%20Wang%20and%20Chris%20Holmes%0AAbstract%3A%20%20%20Bayesian%20modelling%20allows%20for%20the%20quantification%20of%20predictive%20uncertainty%0Awhich%20is%20crucial%20in%20safety-critical%20applications.%20Yet%20for%20many%20machine%20learning%0A%28ML%29%20algorithms%2C%20it%20is%20difficult%20to%20construct%20or%20implement%20their%20Bayesian%0Acounterpart.%20In%20this%20work%20we%20present%20a%20promising%20approach%20to%20address%20this%0Achallenge%2C%20based%20on%20the%20hypothesis%20that%20commonly%20used%20ML%20algorithms%20are%0Aefficient%20across%20a%20wide%20variety%20of%20tasks%20and%20may%20thus%20be%20near%20Bayes-optimal%0Aw.r.t.%20an%20unknown%20task%20distribution.%20We%20prove%20that%20it%20is%20possible%20to%20recover%0Athe%20Bayesian%20posterior%20defined%20by%20the%20task%20distribution%2C%20which%20is%20unknown%20but%0Aoptimal%20in%20this%20setting%2C%20by%20building%20a%20martingale%20posterior%20using%20the%0Aalgorithm.%20We%20further%20propose%20a%20practical%20uncertainty%20quantification%20method%0Athat%20apply%20to%20general%20ML%20algorithms.%20Experiments%20based%20on%20a%20variety%20of%20non-NN%0Aand%20NN%20algorithms%20demonstrate%20the%20efficacy%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19381v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Uncertainty%20Quantification%20for%20Near-Bayes%20Optimal%20Algorithms&entry.906535625=Ziyu%20Wang%20and%20Chris%20Holmes&entry.1292438233=%20%20Bayesian%20modelling%20allows%20for%20the%20quantification%20of%20predictive%20uncertainty%0Awhich%20is%20crucial%20in%20safety-critical%20applications.%20Yet%20for%20many%20machine%20learning%0A%28ML%29%20algorithms%2C%20it%20is%20difficult%20to%20construct%20or%20implement%20their%20Bayesian%0Acounterpart.%20In%20this%20work%20we%20present%20a%20promising%20approach%20to%20address%20this%0Achallenge%2C%20based%20on%20the%20hypothesis%20that%20commonly%20used%20ML%20algorithms%20are%0Aefficient%20across%20a%20wide%20variety%20of%20tasks%20and%20may%20thus%20be%20near%20Bayes-optimal%0Aw.r.t.%20an%20unknown%20task%20distribution.%20We%20prove%20that%20it%20is%20possible%20to%20recover%0Athe%20Bayesian%20posterior%20defined%20by%20the%20task%20distribution%2C%20which%20is%20unknown%20but%0Aoptimal%20in%20this%20setting%2C%20by%20building%20a%20martingale%20posterior%20using%20the%0Aalgorithm.%20We%20further%20propose%20a%20practical%20uncertainty%20quantification%20method%0Athat%20apply%20to%20general%20ML%20algorithms.%20Experiments%20based%20on%20a%20variety%20of%20non-NN%0Aand%20NN%20algorithms%20demonstrate%20the%20efficacy%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19381v1&entry.124074799=Read"},
{"title": "Uncovering Misattributed Suicide Causes through Annotation Inconsistency\n  Detection in Death Investigation Notes", "author": "Song Wang and Yiliang Zhou and Ziqiang Han and Cui Tao and Yunyu Xiao and Ying Ding and Joydeep Ghosh and Yifan Peng", "abstract": "  Data accuracy is essential for scientific research and policy development.\nThe National Violent Death Reporting System (NVDRS) data is widely used for\ndiscovering the patterns and causes of death. Recent studies suggested the\nannotation inconsistencies within the NVDRS and the potential impact on\nerroneous suicide-cause attributions. We present an empirical Natural Language\nProcessing (NLP) approach to detect annotation inconsistencies and adopt a\ncross-validation-like paradigm to identify problematic instances. We analyzed\n267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our\nresults showed that incorporating the target state's data into training the\nsuicide-crisis classifier brought an increase of 5.4% to the F-1 score on the\ntarget state's test set and a decrease of 1.1% on other states' test set. To\nconclude, we demonstrated the annotation inconsistencies in NVDRS's death\ninvestigation notes, identified problematic instances, evaluated the\neffectiveness of correcting problematic instances, and eventually proposed an\nNLP improvement solution.\n", "link": "http://arxiv.org/abs/2403.19432v1", "date": "2024-03-28", "relevancy": 1.2315, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3951}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Misattributed%20Suicide%20Causes%20through%20Annotation%20Inconsistency%0A%20%20Detection%20in%20Death%20Investigation%20Notes&body=Title%3A%20Uncovering%20Misattributed%20Suicide%20Causes%20through%20Annotation%20Inconsistency%0A%20%20Detection%20in%20Death%20Investigation%20Notes%0AAuthor%3A%20Song%20Wang%20and%20Yiliang%20Zhou%20and%20Ziqiang%20Han%20and%20Cui%20Tao%20and%20Yunyu%20Xiao%20and%20Ying%20Ding%20and%20Joydeep%20Ghosh%20and%20Yifan%20Peng%0AAbstract%3A%20%20%20Data%20accuracy%20is%20essential%20for%20scientific%20research%20and%20policy%20development.%0AThe%20National%20Violent%20Death%20Reporting%20System%20%28NVDRS%29%20data%20is%20widely%20used%20for%0Adiscovering%20the%20patterns%20and%20causes%20of%20death.%20Recent%20studies%20suggested%20the%0Aannotation%20inconsistencies%20within%20the%20NVDRS%20and%20the%20potential%20impact%20on%0Aerroneous%20suicide-cause%20attributions.%20We%20present%20an%20empirical%20Natural%20Language%0AProcessing%20%28NLP%29%20approach%20to%20detect%20annotation%20inconsistencies%20and%20adopt%20a%0Across-validation-like%20paradigm%20to%20identify%20problematic%20instances.%20We%20analyzed%0A267%2C804%20suicide%20death%20incidents%20between%202003%20and%202020%20from%20the%20NVDRS.%20Our%0Aresults%20showed%20that%20incorporating%20the%20target%20state%27s%20data%20into%20training%20the%0Asuicide-crisis%20classifier%20brought%20an%20increase%20of%205.4%25%20to%20the%20F-1%20score%20on%20the%0Atarget%20state%27s%20test%20set%20and%20a%20decrease%20of%201.1%25%20on%20other%20states%27%20test%20set.%20To%0Aconclude%2C%20we%20demonstrated%20the%20annotation%20inconsistencies%20in%20NVDRS%27s%20death%0Ainvestigation%20notes%2C%20identified%20problematic%20instances%2C%20evaluated%20the%0Aeffectiveness%20of%20correcting%20problematic%20instances%2C%20and%20eventually%20proposed%20an%0ANLP%20improvement%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19432v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Misattributed%20Suicide%20Causes%20through%20Annotation%20Inconsistency%0A%20%20Detection%20in%20Death%20Investigation%20Notes&entry.906535625=Song%20Wang%20and%20Yiliang%20Zhou%20and%20Ziqiang%20Han%20and%20Cui%20Tao%20and%20Yunyu%20Xiao%20and%20Ying%20Ding%20and%20Joydeep%20Ghosh%20and%20Yifan%20Peng&entry.1292438233=%20%20Data%20accuracy%20is%20essential%20for%20scientific%20research%20and%20policy%20development.%0AThe%20National%20Violent%20Death%20Reporting%20System%20%28NVDRS%29%20data%20is%20widely%20used%20for%0Adiscovering%20the%20patterns%20and%20causes%20of%20death.%20Recent%20studies%20suggested%20the%0Aannotation%20inconsistencies%20within%20the%20NVDRS%20and%20the%20potential%20impact%20on%0Aerroneous%20suicide-cause%20attributions.%20We%20present%20an%20empirical%20Natural%20Language%0AProcessing%20%28NLP%29%20approach%20to%20detect%20annotation%20inconsistencies%20and%20adopt%20a%0Across-validation-like%20paradigm%20to%20identify%20problematic%20instances.%20We%20analyzed%0A267%2C804%20suicide%20death%20incidents%20between%202003%20and%202020%20from%20the%20NVDRS.%20Our%0Aresults%20showed%20that%20incorporating%20the%20target%20state%27s%20data%20into%20training%20the%0Asuicide-crisis%20classifier%20brought%20an%20increase%20of%205.4%25%20to%20the%20F-1%20score%20on%20the%0Atarget%20state%27s%20test%20set%20and%20a%20decrease%20of%201.1%25%20on%20other%20states%27%20test%20set.%20To%0Aconclude%2C%20we%20demonstrated%20the%20annotation%20inconsistencies%20in%20NVDRS%27s%20death%0Ainvestigation%20notes%2C%20identified%20problematic%20instances%2C%20evaluated%20the%0Aeffectiveness%20of%20correcting%20problematic%20instances%2C%20and%20eventually%20proposed%20an%0ANLP%20improvement%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19432v1&entry.124074799=Read"},
{"title": "Scalable Interactive Machine Learning for Future Command and Control", "author": "Anna Madison and Ellen Novoseller and Vinicius G. Goecks and Benjamin T. Files and Nicholas Waytowich and Alfred Yu and Vernon J. Lawhern and Steven Thurman and Christopher Kelshaw and Kaleb McDowell", "abstract": "  Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.\n", "link": "http://arxiv.org/abs/2402.06501v2", "date": "2024-03-28", "relevancy": 1.5364, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5179}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5104}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4994}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Scalable%20Interactive%20Machine%20Learning%20for%20Future%20Command%20and%20Control&body=Title%3A%20Scalable%20Interactive%20Machine%20Learning%20for%20Future%20Command%20and%20Control%0AAuthor%3A%20Anna%20Madison%20and%20Ellen%20Novoseller%20and%20Vinicius%20G.%20Goecks%20and%20Benjamin%20T.%20Files%20and%20Nicholas%20Waytowich%20and%20Alfred%20Yu%20and%20Vernon%20J.%20Lawhern%20and%20Steven%20Thurman%20and%20Christopher%20Kelshaw%20and%20Kaleb%20McDowell%0AAbstract%3A%20%20%20Future%20warfare%20will%20require%20Command%20and%20Control%20%28C2%29%20personnel%20to%20make%0Adecisions%20at%20shrinking%20timescales%20in%20complex%20and%20potentially%20ill-defined%0Asituations.%20Given%20the%20need%20for%20robust%20decision-making%20processes%20and%0Adecision-support%20tools%2C%20integration%20of%20artificial%20and%20human%20intelligence%20holds%0Athe%20potential%20to%20revolutionize%20the%20C2%20operations%20process%20to%20ensure%20adaptability%0Aand%20efficiency%20in%20rapidly%20changing%20operational%20environments.%20We%20propose%20to%0Aleverage%20recent%20promising%20breakthroughs%20in%20interactive%20machine%20learning%2C%20in%0Awhich%20humans%20can%20cooperate%20with%20machine%20learning%20algorithms%20to%20guide%20machine%0Alearning%20algorithm%20behavior.%20This%20paper%20identifies%20several%20gaps%20in%0Astate-of-the-art%20science%20and%20technology%20that%20future%20work%20should%20address%20to%0Aextend%20these%20approaches%20to%20function%20in%20complex%20C2%20contexts.%20In%20particular%2C%20we%0Adescribe%20three%20research%20focus%20areas%20that%20together%2C%20aim%20to%20enable%20scalable%0Ainteractive%20machine%20learning%20%28SIML%29%3A%201%29%20developing%20human-AI%20interaction%0Aalgorithms%20to%20enable%20planning%20in%20complex%2C%20dynamic%20situations%3B%202%29%20fostering%0Aresilient%20human-AI%20teams%20through%20optimizing%20roles%2C%20configurations%2C%20and%20trust%3B%0Aand%203%29%20scaling%20algorithms%20and%20human-AI%20teams%20for%20flexibility%20across%20a%20range%20of%0Apotential%20contexts%20and%20situations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06501v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Interactive%20Machine%20Learning%20for%20Future%20Command%20and%20Control&entry.906535625=Anna%20Madison%20and%20Ellen%20Novoseller%20and%20Vinicius%20G.%20Goecks%20and%20Benjamin%20T.%20Files%20and%20Nicholas%20Waytowich%20and%20Alfred%20Yu%20and%20Vernon%20J.%20Lawhern%20and%20Steven%20Thurman%20and%20Christopher%20Kelshaw%20and%20Kaleb%20McDowell&entry.1292438233=%20%20Future%20warfare%20will%20require%20Command%20and%20Control%20%28C2%29%20personnel%20to%20make%0Adecisions%20at%20shrinking%20timescales%20in%20complex%20and%20potentially%20ill-defined%0Asituations.%20Given%20the%20need%20for%20robust%20decision-making%20processes%20and%0Adecision-support%20tools%2C%20integration%20of%20artificial%20and%20human%20intelligence%20holds%0Athe%20potential%20to%20revolutionize%20the%20C2%20operations%20process%20to%20ensure%20adaptability%0Aand%20efficiency%20in%20rapidly%20changing%20operational%20environments.%20We%20propose%20to%0Aleverage%20recent%20promising%20breakthroughs%20in%20interactive%20machine%20learning%2C%20in%0Awhich%20humans%20can%20cooperate%20with%20machine%20learning%20algorithms%20to%20guide%20machine%0Alearning%20algorithm%20behavior.%20This%20paper%20identifies%20several%20gaps%20in%0Astate-of-the-art%20science%20and%20technology%20that%20future%20work%20should%20address%20to%0Aextend%20these%20approaches%20to%20function%20in%20complex%20C2%20contexts.%20In%20particular%2C%20we%0Adescribe%20three%20research%20focus%20areas%20that%20together%2C%20aim%20to%20enable%20scalable%0Ainteractive%20machine%20learning%20%28SIML%29%3A%201%29%20developing%20human-AI%20interaction%0Aalgorithms%20to%20enable%20planning%20in%20complex%2C%20dynamic%20situations%3B%202%29%20fostering%0Aresilient%20human-AI%20teams%20through%20optimizing%20roles%2C%20configurations%2C%20and%20trust%3B%0Aand%203%29%20scaling%20algorithms%20and%20human-AI%20teams%20for%20flexibility%20across%20a%20range%20of%0Apotential%20contexts%20and%20situations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06501v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


