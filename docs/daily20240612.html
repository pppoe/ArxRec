<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240611.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion\n  Models", "author": "Heng Yu and Chaoyang Wang and Peiye Zhuang and Willi Menapace and Aliaksandr Siarohin and Junli Cao and Laszlo A Jeni and Sergey Tulyakov and Hsin-Ying Lee", "abstract": "  Existing dynamic scene generation methods mostly rely on distilling knowledge\nfrom pre-trained 3D generative models, which are typically fine-tuned on\nsynthetic object datasets. As a result, the generated scenes are often\nobject-centric and lack photorealism. To address these limitations, we\nintroduce a novel pipeline designed for photorealistic text-to-4D scene\ngeneration, discarding the dependency on multi-view generative models and\ninstead fully utilizing video generative models trained on diverse real-world\ndatasets. Our method begins by generating a reference video using the video\ngeneration model. We then learn the canonical 3D representation of the video\nusing a freeze-time video, delicately generated from the reference video. To\nhandle inconsistencies in the freeze-time video, we jointly learn a per-frame\ndeformation to model these imperfections. We then learn the temporal\ndeformation based on the canonical representation to capture dynamic\ninteractions in the reference video. The pipeline facilitates the generation of\ndynamic scenes with enhanced photorealism and structural integrity, viewable\nfrom multiple perspectives, thereby setting a new standard in 4D scene\ngeneration.\n", "link": "http://arxiv.org/abs/2406.07472v1", "date": "2024-06-11", "relevancy": 3.2925, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6767}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6767}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204Real%3A%20Towards%20Photorealistic%204D%20Scene%20Generation%20via%20Video%20Diffusion%0A%20%20Models&body=Title%3A%204Real%3A%20Towards%20Photorealistic%204D%20Scene%20Generation%20via%20Video%20Diffusion%0A%20%20Models%0AAuthor%3A%20Heng%20Yu%20and%20Chaoyang%20Wang%20and%20Peiye%20Zhuang%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Junli%20Cao%20and%20Laszlo%20A%20Jeni%20and%20Sergey%20Tulyakov%20and%20Hsin-Ying%20Lee%0AAbstract%3A%20%20%20Existing%20dynamic%20scene%20generation%20methods%20mostly%20rely%20on%20distilling%20knowledge%0Afrom%20pre-trained%203D%20generative%20models%2C%20which%20are%20typically%20fine-tuned%20on%0Asynthetic%20object%20datasets.%20As%20a%20result%2C%20the%20generated%20scenes%20are%20often%0Aobject-centric%20and%20lack%20photorealism.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20a%20novel%20pipeline%20designed%20for%20photorealistic%20text-to-4D%20scene%0Ageneration%2C%20discarding%20the%20dependency%20on%20multi-view%20generative%20models%20and%0Ainstead%20fully%20utilizing%20video%20generative%20models%20trained%20on%20diverse%20real-world%0Adatasets.%20Our%20method%20begins%20by%20generating%20a%20reference%20video%20using%20the%20video%0Ageneration%20model.%20We%20then%20learn%20the%20canonical%203D%20representation%20of%20the%20video%0Ausing%20a%20freeze-time%20video%2C%20delicately%20generated%20from%20the%20reference%20video.%20To%0Ahandle%20inconsistencies%20in%20the%20freeze-time%20video%2C%20we%20jointly%20learn%20a%20per-frame%0Adeformation%20to%20model%20these%20imperfections.%20We%20then%20learn%20the%20temporal%0Adeformation%20based%20on%20the%20canonical%20representation%20to%20capture%20dynamic%0Ainteractions%20in%20the%20reference%20video.%20The%20pipeline%20facilitates%20the%20generation%20of%0Adynamic%20scenes%20with%20enhanced%20photorealism%20and%20structural%20integrity%2C%20viewable%0Afrom%20multiple%20perspectives%2C%20thereby%20setting%20a%20new%20standard%20in%204D%20scene%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4Real%253A%2520Towards%2520Photorealistic%25204D%2520Scene%2520Generation%2520via%2520Video%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DHeng%2520Yu%2520and%2520Chaoyang%2520Wang%2520and%2520Peiye%2520Zhuang%2520and%2520Willi%2520Menapace%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Junli%2520Cao%2520and%2520Laszlo%2520A%2520Jeni%2520and%2520Sergey%2520Tulyakov%2520and%2520Hsin-Ying%2520Lee%26entry.1292438233%3D%2520%2520Existing%2520dynamic%2520scene%2520generation%2520methods%2520mostly%2520rely%2520on%2520distilling%2520knowledge%250Afrom%2520pre-trained%25203D%2520generative%2520models%252C%2520which%2520are%2520typically%2520fine-tuned%2520on%250Asynthetic%2520object%2520datasets.%2520As%2520a%2520result%252C%2520the%2520generated%2520scenes%2520are%2520often%250Aobject-centric%2520and%2520lack%2520photorealism.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aintroduce%2520a%2520novel%2520pipeline%2520designed%2520for%2520photorealistic%2520text-to-4D%2520scene%250Ageneration%252C%2520discarding%2520the%2520dependency%2520on%2520multi-view%2520generative%2520models%2520and%250Ainstead%2520fully%2520utilizing%2520video%2520generative%2520models%2520trained%2520on%2520diverse%2520real-world%250Adatasets.%2520Our%2520method%2520begins%2520by%2520generating%2520a%2520reference%2520video%2520using%2520the%2520video%250Ageneration%2520model.%2520We%2520then%2520learn%2520the%2520canonical%25203D%2520representation%2520of%2520the%2520video%250Ausing%2520a%2520freeze-time%2520video%252C%2520delicately%2520generated%2520from%2520the%2520reference%2520video.%2520To%250Ahandle%2520inconsistencies%2520in%2520the%2520freeze-time%2520video%252C%2520we%2520jointly%2520learn%2520a%2520per-frame%250Adeformation%2520to%2520model%2520these%2520imperfections.%2520We%2520then%2520learn%2520the%2520temporal%250Adeformation%2520based%2520on%2520the%2520canonical%2520representation%2520to%2520capture%2520dynamic%250Ainteractions%2520in%2520the%2520reference%2520video.%2520The%2520pipeline%2520facilitates%2520the%2520generation%2520of%250Adynamic%2520scenes%2520with%2520enhanced%2520photorealism%2520and%2520structural%2520integrity%252C%2520viewable%250Afrom%2520multiple%2520perspectives%252C%2520thereby%2520setting%2520a%2520new%2520standard%2520in%25204D%2520scene%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4Real%3A%20Towards%20Photorealistic%204D%20Scene%20Generation%20via%20Video%20Diffusion%0A%20%20Models&entry.906535625=Heng%20Yu%20and%20Chaoyang%20Wang%20and%20Peiye%20Zhuang%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Junli%20Cao%20and%20Laszlo%20A%20Jeni%20and%20Sergey%20Tulyakov%20and%20Hsin-Ying%20Lee&entry.1292438233=%20%20Existing%20dynamic%20scene%20generation%20methods%20mostly%20rely%20on%20distilling%20knowledge%0Afrom%20pre-trained%203D%20generative%20models%2C%20which%20are%20typically%20fine-tuned%20on%0Asynthetic%20object%20datasets.%20As%20a%20result%2C%20the%20generated%20scenes%20are%20often%0Aobject-centric%20and%20lack%20photorealism.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20a%20novel%20pipeline%20designed%20for%20photorealistic%20text-to-4D%20scene%0Ageneration%2C%20discarding%20the%20dependency%20on%20multi-view%20generative%20models%20and%0Ainstead%20fully%20utilizing%20video%20generative%20models%20trained%20on%20diverse%20real-world%0Adatasets.%20Our%20method%20begins%20by%20generating%20a%20reference%20video%20using%20the%20video%0Ageneration%20model.%20We%20then%20learn%20the%20canonical%203D%20representation%20of%20the%20video%0Ausing%20a%20freeze-time%20video%2C%20delicately%20generated%20from%20the%20reference%20video.%20To%0Ahandle%20inconsistencies%20in%20the%20freeze-time%20video%2C%20we%20jointly%20learn%20a%20per-frame%0Adeformation%20to%20model%20these%20imperfections.%20We%20then%20learn%20the%20temporal%0Adeformation%20based%20on%20the%20canonical%20representation%20to%20capture%20dynamic%0Ainteractions%20in%20the%20reference%20video.%20The%20pipeline%20facilitates%20the%20generation%20of%0Adynamic%20scenes%20with%20enhanced%20photorealism%20and%20structural%20integrity%2C%20viewable%0Afrom%20multiple%20perspectives%2C%20thereby%20setting%20a%20new%20standard%20in%204D%20scene%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07472v1&entry.124074799=Read"},
{"title": "3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming\n  of Photo-Realistic Free-Viewpoint Videos", "author": "Jiakai Sun and Han Jiao and Guangyuan Li and Zhanjie Zhang and Lei Zhao and Wei Xing", "abstract": "  Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes\nfrom multi-view videos remains a challenging endeavor. Despite the remarkable\nadvancements achieved by current neural rendering techniques, these methods\ngenerally require complete video sequences for offline training and are not\ncapable of real-time rendering. To address these constraints, we introduce\n3DGStream, a method designed for efficient FVV streaming of real-world dynamic\nscenes. Our method achieves fast on-the-fly per-frame reconstruction within 12\nseconds and real-time rendering at 200 FPS. Specifically, we utilize 3D\nGaussians (3DGs) to represent the scene. Instead of the na\\\"ive approach of\ndirectly optimizing 3DGs per-frame, we employ a compact Neural Transformation\nCache (NTC) to model the translations and rotations of 3DGs, markedly reducing\nthe training time and storage required for each FVV frame. Furthermore, we\npropose an adaptive 3DG addition strategy to handle emerging objects in dynamic\nscenes. Experiments demonstrate that 3DGStream achieves competitive performance\nin terms of rendering speed, image quality, training time, and model storage\nwhen compared with state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.01444v4", "date": "2024-06-11", "relevancy": 3.2383, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.677}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.653}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGStream%3A%20On-the-Fly%20Training%20of%203D%20Gaussians%20for%20Efficient%20Streaming%0A%20%20of%20Photo-Realistic%20Free-Viewpoint%20Videos&body=Title%3A%203DGStream%3A%20On-the-Fly%20Training%20of%203D%20Gaussians%20for%20Efficient%20Streaming%0A%20%20of%20Photo-Realistic%20Free-Viewpoint%20Videos%0AAuthor%3A%20Jiakai%20Sun%20and%20Han%20Jiao%20and%20Guangyuan%20Li%20and%20Zhanjie%20Zhang%20and%20Lei%20Zhao%20and%20Wei%20Xing%0AAbstract%3A%20%20%20Constructing%20photo-realistic%20Free-Viewpoint%20Videos%20%28FVVs%29%20of%20dynamic%20scenes%0Afrom%20multi-view%20videos%20remains%20a%20challenging%20endeavor.%20Despite%20the%20remarkable%0Aadvancements%20achieved%20by%20current%20neural%20rendering%20techniques%2C%20these%20methods%0Agenerally%20require%20complete%20video%20sequences%20for%20offline%20training%20and%20are%20not%0Acapable%20of%20real-time%20rendering.%20To%20address%20these%20constraints%2C%20we%20introduce%0A3DGStream%2C%20a%20method%20designed%20for%20efficient%20FVV%20streaming%20of%20real-world%20dynamic%0Ascenes.%20Our%20method%20achieves%20fast%20on-the-fly%20per-frame%20reconstruction%20within%2012%0Aseconds%20and%20real-time%20rendering%20at%20200%20FPS.%20Specifically%2C%20we%20utilize%203D%0AGaussians%20%283DGs%29%20to%20represent%20the%20scene.%20Instead%20of%20the%20na%5C%22ive%20approach%20of%0Adirectly%20optimizing%203DGs%20per-frame%2C%20we%20employ%20a%20compact%20Neural%20Transformation%0ACache%20%28NTC%29%20to%20model%20the%20translations%20and%20rotations%20of%203DGs%2C%20markedly%20reducing%0Athe%20training%20time%20and%20storage%20required%20for%20each%20FVV%20frame.%20Furthermore%2C%20we%0Apropose%20an%20adaptive%203DG%20addition%20strategy%20to%20handle%20emerging%20objects%20in%20dynamic%0Ascenes.%20Experiments%20demonstrate%20that%203DGStream%20achieves%20competitive%20performance%0Ain%20terms%20of%20rendering%20speed%2C%20image%20quality%2C%20training%20time%2C%20and%20model%20storage%0Awhen%20compared%20with%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01444v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGStream%253A%2520On-the-Fly%2520Training%2520of%25203D%2520Gaussians%2520for%2520Efficient%2520Streaming%250A%2520%2520of%2520Photo-Realistic%2520Free-Viewpoint%2520Videos%26entry.906535625%3DJiakai%2520Sun%2520and%2520Han%2520Jiao%2520and%2520Guangyuan%2520Li%2520and%2520Zhanjie%2520Zhang%2520and%2520Lei%2520Zhao%2520and%2520Wei%2520Xing%26entry.1292438233%3D%2520%2520Constructing%2520photo-realistic%2520Free-Viewpoint%2520Videos%2520%2528FVVs%2529%2520of%2520dynamic%2520scenes%250Afrom%2520multi-view%2520videos%2520remains%2520a%2520challenging%2520endeavor.%2520Despite%2520the%2520remarkable%250Aadvancements%2520achieved%2520by%2520current%2520neural%2520rendering%2520techniques%252C%2520these%2520methods%250Agenerally%2520require%2520complete%2520video%2520sequences%2520for%2520offline%2520training%2520and%2520are%2520not%250Acapable%2520of%2520real-time%2520rendering.%2520To%2520address%2520these%2520constraints%252C%2520we%2520introduce%250A3DGStream%252C%2520a%2520method%2520designed%2520for%2520efficient%2520FVV%2520streaming%2520of%2520real-world%2520dynamic%250Ascenes.%2520Our%2520method%2520achieves%2520fast%2520on-the-fly%2520per-frame%2520reconstruction%2520within%252012%250Aseconds%2520and%2520real-time%2520rendering%2520at%2520200%2520FPS.%2520Specifically%252C%2520we%2520utilize%25203D%250AGaussians%2520%25283DGs%2529%2520to%2520represent%2520the%2520scene.%2520Instead%2520of%2520the%2520na%255C%2522ive%2520approach%2520of%250Adirectly%2520optimizing%25203DGs%2520per-frame%252C%2520we%2520employ%2520a%2520compact%2520Neural%2520Transformation%250ACache%2520%2528NTC%2529%2520to%2520model%2520the%2520translations%2520and%2520rotations%2520of%25203DGs%252C%2520markedly%2520reducing%250Athe%2520training%2520time%2520and%2520storage%2520required%2520for%2520each%2520FVV%2520frame.%2520Furthermore%252C%2520we%250Apropose%2520an%2520adaptive%25203DG%2520addition%2520strategy%2520to%2520handle%2520emerging%2520objects%2520in%2520dynamic%250Ascenes.%2520Experiments%2520demonstrate%2520that%25203DGStream%2520achieves%2520competitive%2520performance%250Ain%2520terms%2520of%2520rendering%2520speed%252C%2520image%2520quality%252C%2520training%2520time%252C%2520and%2520model%2520storage%250Awhen%2520compared%2520with%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01444v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGStream%3A%20On-the-Fly%20Training%20of%203D%20Gaussians%20for%20Efficient%20Streaming%0A%20%20of%20Photo-Realistic%20Free-Viewpoint%20Videos&entry.906535625=Jiakai%20Sun%20and%20Han%20Jiao%20and%20Guangyuan%20Li%20and%20Zhanjie%20Zhang%20and%20Lei%20Zhao%20and%20Wei%20Xing&entry.1292438233=%20%20Constructing%20photo-realistic%20Free-Viewpoint%20Videos%20%28FVVs%29%20of%20dynamic%20scenes%0Afrom%20multi-view%20videos%20remains%20a%20challenging%20endeavor.%20Despite%20the%20remarkable%0Aadvancements%20achieved%20by%20current%20neural%20rendering%20techniques%2C%20these%20methods%0Agenerally%20require%20complete%20video%20sequences%20for%20offline%20training%20and%20are%20not%0Acapable%20of%20real-time%20rendering.%20To%20address%20these%20constraints%2C%20we%20introduce%0A3DGStream%2C%20a%20method%20designed%20for%20efficient%20FVV%20streaming%20of%20real-world%20dynamic%0Ascenes.%20Our%20method%20achieves%20fast%20on-the-fly%20per-frame%20reconstruction%20within%2012%0Aseconds%20and%20real-time%20rendering%20at%20200%20FPS.%20Specifically%2C%20we%20utilize%203D%0AGaussians%20%283DGs%29%20to%20represent%20the%20scene.%20Instead%20of%20the%20na%5C%22ive%20approach%20of%0Adirectly%20optimizing%203DGs%20per-frame%2C%20we%20employ%20a%20compact%20Neural%20Transformation%0ACache%20%28NTC%29%20to%20model%20the%20translations%20and%20rotations%20of%203DGs%2C%20markedly%20reducing%0Athe%20training%20time%20and%20storage%20required%20for%20each%20FVV%20frame.%20Furthermore%2C%20we%0Apropose%20an%20adaptive%203DG%20addition%20strategy%20to%20handle%20emerging%20objects%20in%20dynamic%0Ascenes.%20Experiments%20demonstrate%20that%203DGStream%20achieves%20competitive%20performance%0Ain%20terms%20of%20rendering%20speed%2C%20image%20quality%2C%20training%20time%2C%20and%20model%20storage%0Awhen%20compared%20with%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01444v4&entry.124074799=Read"},
{"title": "Instant 3D Human Avatar Generation using Image Diffusion Models", "author": "Nikos Kolotouros and Thiemo Alldieck and Enric Corona and Eduard Gabriel Bazavan and Cristian Sminchisescu", "abstract": "  We present AvatarPopUp, a method for fast, high quality 3D human avatar\ngeneration from different input modalities, such as images and text prompts and\nwith control over the generated pose and shape. The common theme is the use of\ndiffusion-based image generation networks that are specialized for each\nparticular task, followed by a 3D lifting network. We purposefully decouple the\ngeneration from the 3D modeling which allow us to leverage powerful image\nsynthesis priors, trained on billions of text-image pairs. We fine-tune latent\ndiffusion networks with additional image conditioning to solve tasks such as\nimage generation and back-view prediction, and to support qualitatively\ndifferent multiple 3D hypotheses. Our partial fine-tuning approach allows to\nadapt the networks for each task without inducing catastrophic forgetting. In\nour experiments, we demonstrate that our method produces accurate, high-quality\n3D avatars with diverse appearance that respect the multimodal text, image, and\nbody control signals. Our approach can produce a 3D model in as few as 2\nseconds, a four orders of magnitude speedup w.r.t. the vast majority of\nexisting methods, most of which solve only a subset of our tasks, and with\nfewer controls, thus enabling applications that require the controlled 3D\ngeneration of human avatars at scale. The project website can be found at\nhttps://www.nikoskolot.com/avatarpopup/.\n", "link": "http://arxiv.org/abs/2406.07516v1", "date": "2024-06-11", "relevancy": 3.1858, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6381}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6381}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instant%203D%20Human%20Avatar%20Generation%20using%20Image%20Diffusion%20Models&body=Title%3A%20Instant%203D%20Human%20Avatar%20Generation%20using%20Image%20Diffusion%20Models%0AAuthor%3A%20Nikos%20Kolotouros%20and%20Thiemo%20Alldieck%20and%20Enric%20Corona%20and%20Eduard%20Gabriel%20Bazavan%20and%20Cristian%20Sminchisescu%0AAbstract%3A%20%20%20We%20present%20AvatarPopUp%2C%20a%20method%20for%20fast%2C%20high%20quality%203D%20human%20avatar%0Ageneration%20from%20different%20input%20modalities%2C%20such%20as%20images%20and%20text%20prompts%20and%0Awith%20control%20over%20the%20generated%20pose%20and%20shape.%20The%20common%20theme%20is%20the%20use%20of%0Adiffusion-based%20image%20generation%20networks%20that%20are%20specialized%20for%20each%0Aparticular%20task%2C%20followed%20by%20a%203D%20lifting%20network.%20We%20purposefully%20decouple%20the%0Ageneration%20from%20the%203D%20modeling%20which%20allow%20us%20to%20leverage%20powerful%20image%0Asynthesis%20priors%2C%20trained%20on%20billions%20of%20text-image%20pairs.%20We%20fine-tune%20latent%0Adiffusion%20networks%20with%20additional%20image%20conditioning%20to%20solve%20tasks%20such%20as%0Aimage%20generation%20and%20back-view%20prediction%2C%20and%20to%20support%20qualitatively%0Adifferent%20multiple%203D%20hypotheses.%20Our%20partial%20fine-tuning%20approach%20allows%20to%0Aadapt%20the%20networks%20for%20each%20task%20without%20inducing%20catastrophic%20forgetting.%20In%0Aour%20experiments%2C%20we%20demonstrate%20that%20our%20method%20produces%20accurate%2C%20high-quality%0A3D%20avatars%20with%20diverse%20appearance%20that%20respect%20the%20multimodal%20text%2C%20image%2C%20and%0Abody%20control%20signals.%20Our%20approach%20can%20produce%20a%203D%20model%20in%20as%20few%20as%202%0Aseconds%2C%20a%20four%20orders%20of%20magnitude%20speedup%20w.r.t.%20the%20vast%20majority%20of%0Aexisting%20methods%2C%20most%20of%20which%20solve%20only%20a%20subset%20of%20our%20tasks%2C%20and%20with%0Afewer%20controls%2C%20thus%20enabling%20applications%20that%20require%20the%20controlled%203D%0Ageneration%20of%20human%20avatars%20at%20scale.%20The%20project%20website%20can%20be%20found%20at%0Ahttps%3A//www.nikoskolot.com/avatarpopup/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstant%25203D%2520Human%2520Avatar%2520Generation%2520using%2520Image%2520Diffusion%2520Models%26entry.906535625%3DNikos%2520Kolotouros%2520and%2520Thiemo%2520Alldieck%2520and%2520Enric%2520Corona%2520and%2520Eduard%2520Gabriel%2520Bazavan%2520and%2520Cristian%2520Sminchisescu%26entry.1292438233%3D%2520%2520We%2520present%2520AvatarPopUp%252C%2520a%2520method%2520for%2520fast%252C%2520high%2520quality%25203D%2520human%2520avatar%250Ageneration%2520from%2520different%2520input%2520modalities%252C%2520such%2520as%2520images%2520and%2520text%2520prompts%2520and%250Awith%2520control%2520over%2520the%2520generated%2520pose%2520and%2520shape.%2520The%2520common%2520theme%2520is%2520the%2520use%2520of%250Adiffusion-based%2520image%2520generation%2520networks%2520that%2520are%2520specialized%2520for%2520each%250Aparticular%2520task%252C%2520followed%2520by%2520a%25203D%2520lifting%2520network.%2520We%2520purposefully%2520decouple%2520the%250Ageneration%2520from%2520the%25203D%2520modeling%2520which%2520allow%2520us%2520to%2520leverage%2520powerful%2520image%250Asynthesis%2520priors%252C%2520trained%2520on%2520billions%2520of%2520text-image%2520pairs.%2520We%2520fine-tune%2520latent%250Adiffusion%2520networks%2520with%2520additional%2520image%2520conditioning%2520to%2520solve%2520tasks%2520such%2520as%250Aimage%2520generation%2520and%2520back-view%2520prediction%252C%2520and%2520to%2520support%2520qualitatively%250Adifferent%2520multiple%25203D%2520hypotheses.%2520Our%2520partial%2520fine-tuning%2520approach%2520allows%2520to%250Aadapt%2520the%2520networks%2520for%2520each%2520task%2520without%2520inducing%2520catastrophic%2520forgetting.%2520In%250Aour%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520produces%2520accurate%252C%2520high-quality%250A3D%2520avatars%2520with%2520diverse%2520appearance%2520that%2520respect%2520the%2520multimodal%2520text%252C%2520image%252C%2520and%250Abody%2520control%2520signals.%2520Our%2520approach%2520can%2520produce%2520a%25203D%2520model%2520in%2520as%2520few%2520as%25202%250Aseconds%252C%2520a%2520four%2520orders%2520of%2520magnitude%2520speedup%2520w.r.t.%2520the%2520vast%2520majority%2520of%250Aexisting%2520methods%252C%2520most%2520of%2520which%2520solve%2520only%2520a%2520subset%2520of%2520our%2520tasks%252C%2520and%2520with%250Afewer%2520controls%252C%2520thus%2520enabling%2520applications%2520that%2520require%2520the%2520controlled%25203D%250Ageneration%2520of%2520human%2520avatars%2520at%2520scale.%2520The%2520project%2520website%2520can%2520be%2520found%2520at%250Ahttps%253A//www.nikoskolot.com/avatarpopup/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instant%203D%20Human%20Avatar%20Generation%20using%20Image%20Diffusion%20Models&entry.906535625=Nikos%20Kolotouros%20and%20Thiemo%20Alldieck%20and%20Enric%20Corona%20and%20Eduard%20Gabriel%20Bazavan%20and%20Cristian%20Sminchisescu&entry.1292438233=%20%20We%20present%20AvatarPopUp%2C%20a%20method%20for%20fast%2C%20high%20quality%203D%20human%20avatar%0Ageneration%20from%20different%20input%20modalities%2C%20such%20as%20images%20and%20text%20prompts%20and%0Awith%20control%20over%20the%20generated%20pose%20and%20shape.%20The%20common%20theme%20is%20the%20use%20of%0Adiffusion-based%20image%20generation%20networks%20that%20are%20specialized%20for%20each%0Aparticular%20task%2C%20followed%20by%20a%203D%20lifting%20network.%20We%20purposefully%20decouple%20the%0Ageneration%20from%20the%203D%20modeling%20which%20allow%20us%20to%20leverage%20powerful%20image%0Asynthesis%20priors%2C%20trained%20on%20billions%20of%20text-image%20pairs.%20We%20fine-tune%20latent%0Adiffusion%20networks%20with%20additional%20image%20conditioning%20to%20solve%20tasks%20such%20as%0Aimage%20generation%20and%20back-view%20prediction%2C%20and%20to%20support%20qualitatively%0Adifferent%20multiple%203D%20hypotheses.%20Our%20partial%20fine-tuning%20approach%20allows%20to%0Aadapt%20the%20networks%20for%20each%20task%20without%20inducing%20catastrophic%20forgetting.%20In%0Aour%20experiments%2C%20we%20demonstrate%20that%20our%20method%20produces%20accurate%2C%20high-quality%0A3D%20avatars%20with%20diverse%20appearance%20that%20respect%20the%20multimodal%20text%2C%20image%2C%20and%0Abody%20control%20signals.%20Our%20approach%20can%20produce%20a%203D%20model%20in%20as%20few%20as%202%0Aseconds%2C%20a%20four%20orders%20of%20magnitude%20speedup%20w.r.t.%20the%20vast%20majority%20of%0Aexisting%20methods%2C%20most%20of%20which%20solve%20only%20a%20subset%20of%20our%20tasks%2C%20and%20with%0Afewer%20controls%2C%20thus%20enabling%20applications%20that%20require%20the%20controlled%203D%0Ageneration%20of%20human%20avatars%20at%20scale.%20The%20project%20website%20can%20be%20found%20at%0Ahttps%3A//www.nikoskolot.com/avatarpopup/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07516v1&entry.124074799=Read"},
{"title": "Trim 3D Gaussian Splatting for Accurate Geometry Representation", "author": "Lue Fan and Yuxue Yang and Minxing Li and Hongsheng Li and Zhaoxiang Zhang", "abstract": "  In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to\nreconstruct accurate 3D geometry from images. Previous arts for geometry\nreconstruction from 3D Gaussians mainly focus on exploring strong geometry\nregularization. Instead, from a fresh perspective, we propose to obtain\naccurate 3D geometry of a scene by Gaussian trimming, which selectively removes\nthe inaccurate geometry while preserving accurate structures. To achieve this,\nwe analyze the contributions of individual 3D Gaussians and propose a\ncontribution-based trimming strategy to remove the redundant or inaccurate\nGaussians. Furthermore, our experimental and theoretical analyses reveal that a\nrelatively small Gaussian scale is a non-negligible factor in representing and\noptimizing the intricate details. Therefore the proposed TrimGS maintains\nrelatively small Gaussian scales. In addition, TrimGS is also compatible with\nthe effective geometry regularization strategies in previous arts. When\ncombined with the original 3DGS and the state-of-the-art 2DGS, TrimGS\nconsistently yields more accurate geometry and higher perceptual quality. Our\nproject page is https://trimgs.github.io\n", "link": "http://arxiv.org/abs/2406.07499v1", "date": "2024-06-11", "relevancy": 3.0976, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6992}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6249}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trim%203D%20Gaussian%20Splatting%20for%20Accurate%20Geometry%20Representation&body=Title%3A%20Trim%203D%20Gaussian%20Splatting%20for%20Accurate%20Geometry%20Representation%0AAuthor%3A%20Lue%20Fan%20and%20Yuxue%20Yang%20and%20Minxing%20Li%20and%20Hongsheng%20Li%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Trim%203D%20Gaussian%20Splatting%20%28TrimGS%29%20to%0Areconstruct%20accurate%203D%20geometry%20from%20images.%20Previous%20arts%20for%20geometry%0Areconstruction%20from%203D%20Gaussians%20mainly%20focus%20on%20exploring%20strong%20geometry%0Aregularization.%20Instead%2C%20from%20a%20fresh%20perspective%2C%20we%20propose%20to%20obtain%0Aaccurate%203D%20geometry%20of%20a%20scene%20by%20Gaussian%20trimming%2C%20which%20selectively%20removes%0Athe%20inaccurate%20geometry%20while%20preserving%20accurate%20structures.%20To%20achieve%20this%2C%0Awe%20analyze%20the%20contributions%20of%20individual%203D%20Gaussians%20and%20propose%20a%0Acontribution-based%20trimming%20strategy%20to%20remove%20the%20redundant%20or%20inaccurate%0AGaussians.%20Furthermore%2C%20our%20experimental%20and%20theoretical%20analyses%20reveal%20that%20a%0Arelatively%20small%20Gaussian%20scale%20is%20a%20non-negligible%20factor%20in%20representing%20and%0Aoptimizing%20the%20intricate%20details.%20Therefore%20the%20proposed%20TrimGS%20maintains%0Arelatively%20small%20Gaussian%20scales.%20In%20addition%2C%20TrimGS%20is%20also%20compatible%20with%0Athe%20effective%20geometry%20regularization%20strategies%20in%20previous%20arts.%20When%0Acombined%20with%20the%20original%203DGS%20and%20the%20state-of-the-art%202DGS%2C%20TrimGS%0Aconsistently%20yields%20more%20accurate%20geometry%20and%20higher%20perceptual%20quality.%20Our%0Aproject%20page%20is%20https%3A//trimgs.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrim%25203D%2520Gaussian%2520Splatting%2520for%2520Accurate%2520Geometry%2520Representation%26entry.906535625%3DLue%2520Fan%2520and%2520Yuxue%2520Yang%2520and%2520Minxing%2520Li%2520and%2520Hongsheng%2520Li%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Trim%25203D%2520Gaussian%2520Splatting%2520%2528TrimGS%2529%2520to%250Areconstruct%2520accurate%25203D%2520geometry%2520from%2520images.%2520Previous%2520arts%2520for%2520geometry%250Areconstruction%2520from%25203D%2520Gaussians%2520mainly%2520focus%2520on%2520exploring%2520strong%2520geometry%250Aregularization.%2520Instead%252C%2520from%2520a%2520fresh%2520perspective%252C%2520we%2520propose%2520to%2520obtain%250Aaccurate%25203D%2520geometry%2520of%2520a%2520scene%2520by%2520Gaussian%2520trimming%252C%2520which%2520selectively%2520removes%250Athe%2520inaccurate%2520geometry%2520while%2520preserving%2520accurate%2520structures.%2520To%2520achieve%2520this%252C%250Awe%2520analyze%2520the%2520contributions%2520of%2520individual%25203D%2520Gaussians%2520and%2520propose%2520a%250Acontribution-based%2520trimming%2520strategy%2520to%2520remove%2520the%2520redundant%2520or%2520inaccurate%250AGaussians.%2520Furthermore%252C%2520our%2520experimental%2520and%2520theoretical%2520analyses%2520reveal%2520that%2520a%250Arelatively%2520small%2520Gaussian%2520scale%2520is%2520a%2520non-negligible%2520factor%2520in%2520representing%2520and%250Aoptimizing%2520the%2520intricate%2520details.%2520Therefore%2520the%2520proposed%2520TrimGS%2520maintains%250Arelatively%2520small%2520Gaussian%2520scales.%2520In%2520addition%252C%2520TrimGS%2520is%2520also%2520compatible%2520with%250Athe%2520effective%2520geometry%2520regularization%2520strategies%2520in%2520previous%2520arts.%2520When%250Acombined%2520with%2520the%2520original%25203DGS%2520and%2520the%2520state-of-the-art%25202DGS%252C%2520TrimGS%250Aconsistently%2520yields%2520more%2520accurate%2520geometry%2520and%2520higher%2520perceptual%2520quality.%2520Our%250Aproject%2520page%2520is%2520https%253A//trimgs.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trim%203D%20Gaussian%20Splatting%20for%20Accurate%20Geometry%20Representation&entry.906535625=Lue%20Fan%20and%20Yuxue%20Yang%20and%20Minxing%20Li%20and%20Hongsheng%20Li%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Trim%203D%20Gaussian%20Splatting%20%28TrimGS%29%20to%0Areconstruct%20accurate%203D%20geometry%20from%20images.%20Previous%20arts%20for%20geometry%0Areconstruction%20from%203D%20Gaussians%20mainly%20focus%20on%20exploring%20strong%20geometry%0Aregularization.%20Instead%2C%20from%20a%20fresh%20perspective%2C%20we%20propose%20to%20obtain%0Aaccurate%203D%20geometry%20of%20a%20scene%20by%20Gaussian%20trimming%2C%20which%20selectively%20removes%0Athe%20inaccurate%20geometry%20while%20preserving%20accurate%20structures.%20To%20achieve%20this%2C%0Awe%20analyze%20the%20contributions%20of%20individual%203D%20Gaussians%20and%20propose%20a%0Acontribution-based%20trimming%20strategy%20to%20remove%20the%20redundant%20or%20inaccurate%0AGaussians.%20Furthermore%2C%20our%20experimental%20and%20theoretical%20analyses%20reveal%20that%20a%0Arelatively%20small%20Gaussian%20scale%20is%20a%20non-negligible%20factor%20in%20representing%20and%0Aoptimizing%20the%20intricate%20details.%20Therefore%20the%20proposed%20TrimGS%20maintains%0Arelatively%20small%20Gaussian%20scales.%20In%20addition%2C%20TrimGS%20is%20also%20compatible%20with%0Athe%20effective%20geometry%20regularization%20strategies%20in%20previous%20arts.%20When%0Acombined%20with%20the%20original%203DGS%20and%20the%20state-of-the-art%202DGS%2C%20TrimGS%0Aconsistently%20yields%20more%20accurate%20geometry%20and%20higher%20perceptual%20quality.%20Our%0Aproject%20page%20is%20https%3A//trimgs.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07499v1&entry.124074799=Read"},
{"title": "Gaussian Splatting with NeRF-based Color and Opacity", "author": "Dawid Malarz and Weronika Smolak and Jacek Tabor and S\u0142awomir Tadeja and Przemys\u0142aw Spurek", "abstract": "  Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of\nneural networks to capture the intricacies of 3D objects. By encoding the shape\nand color information within neural network weights, NeRFs excel at producing\nstrikingly sharp novel views of 3D objects. Recently, numerous generalizations\nof NeRFs utilizing generative models have emerged, expanding its versatility.\nIn contrast, Gaussian Splatting (GS) offers a similar render quality with\nfaster training and inference as it does not need neural networks to work. It\nencodes information about the 3D objects in the set of Gaussian distributions\nthat can be rendered in 3D similarly to classical meshes. Unfortunately, GS are\ndifficult to condition since they usually require circa hundred thousand\nGaussian components. To mitigate the caveats of both models, we propose a\nhybrid model Viewing Direction Gaussian Splatting (VDGS) that uses GS\nrepresentation of the 3D object's shape and NeRF-based encoding of color and\nopacity. Our model uses Gaussian distributions with trainable positions (i.e.\nmeans of Gaussian), shape (i.e. covariance of Gaussian), color and opacity, and\na neural network that takes Gaussian parameters and viewing direction to\nproduce changes in the said color and opacity. As a result, our model better\ndescribes shadows, light reflections, and the transparency of 3D objects\nwithout adding additional texture and light components.\n", "link": "http://arxiv.org/abs/2312.13729v4", "date": "2024-06-11", "relevancy": 3.0684, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6847}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6028}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20with%20NeRF-based%20Color%20and%20Opacity&body=Title%3A%20Gaussian%20Splatting%20with%20NeRF-based%20Color%20and%20Opacity%0AAuthor%3A%20Dawid%20Malarz%20and%20Weronika%20Smolak%20and%20Jacek%20Tabor%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20demonstrated%20the%20remarkable%20potential%20of%0Aneural%20networks%20to%20capture%20the%20intricacies%20of%203D%20objects.%20By%20encoding%20the%20shape%0Aand%20color%20information%20within%20neural%20network%20weights%2C%20NeRFs%20excel%20at%20producing%0Astrikingly%20sharp%20novel%20views%20of%203D%20objects.%20Recently%2C%20numerous%20generalizations%0Aof%20NeRFs%20utilizing%20generative%20models%20have%20emerged%2C%20expanding%20its%20versatility.%0AIn%20contrast%2C%20Gaussian%20Splatting%20%28GS%29%20offers%20a%20similar%20render%20quality%20with%0Afaster%20training%20and%20inference%20as%20it%20does%20not%20need%20neural%20networks%20to%20work.%20It%0Aencodes%20information%20about%20the%203D%20objects%20in%20the%20set%20of%20Gaussian%20distributions%0Athat%20can%20be%20rendered%20in%203D%20similarly%20to%20classical%20meshes.%20Unfortunately%2C%20GS%20are%0Adifficult%20to%20condition%20since%20they%20usually%20require%20circa%20hundred%20thousand%0AGaussian%20components.%20To%20mitigate%20the%20caveats%20of%20both%20models%2C%20we%20propose%20a%0Ahybrid%20model%20Viewing%20Direction%20Gaussian%20Splatting%20%28VDGS%29%20that%20uses%20GS%0Arepresentation%20of%20the%203D%20object%27s%20shape%20and%20NeRF-based%20encoding%20of%20color%20and%0Aopacity.%20Our%20model%20uses%20Gaussian%20distributions%20with%20trainable%20positions%20%28i.e.%0Ameans%20of%20Gaussian%29%2C%20shape%20%28i.e.%20covariance%20of%20Gaussian%29%2C%20color%20and%20opacity%2C%20and%0Aa%20neural%20network%20that%20takes%20Gaussian%20parameters%20and%20viewing%20direction%20to%0Aproduce%20changes%20in%20the%20said%20color%20and%20opacity.%20As%20a%20result%2C%20our%20model%20better%0Adescribes%20shadows%2C%20light%20reflections%2C%20and%20the%20transparency%20of%203D%20objects%0Awithout%20adding%20additional%20texture%20and%20light%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13729v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520with%2520NeRF-based%2520Color%2520and%2520Opacity%26entry.906535625%3DDawid%2520Malarz%2520and%2520Weronika%2520Smolak%2520and%2520Jacek%2520Tabor%2520and%2520S%25C5%2582awomir%2520Tadeja%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520demonstrated%2520the%2520remarkable%2520potential%2520of%250Aneural%2520networks%2520to%2520capture%2520the%2520intricacies%2520of%25203D%2520objects.%2520By%2520encoding%2520the%2520shape%250Aand%2520color%2520information%2520within%2520neural%2520network%2520weights%252C%2520NeRFs%2520excel%2520at%2520producing%250Astrikingly%2520sharp%2520novel%2520views%2520of%25203D%2520objects.%2520Recently%252C%2520numerous%2520generalizations%250Aof%2520NeRFs%2520utilizing%2520generative%2520models%2520have%2520emerged%252C%2520expanding%2520its%2520versatility.%250AIn%2520contrast%252C%2520Gaussian%2520Splatting%2520%2528GS%2529%2520offers%2520a%2520similar%2520render%2520quality%2520with%250Afaster%2520training%2520and%2520inference%2520as%2520it%2520does%2520not%2520need%2520neural%2520networks%2520to%2520work.%2520It%250Aencodes%2520information%2520about%2520the%25203D%2520objects%2520in%2520the%2520set%2520of%2520Gaussian%2520distributions%250Athat%2520can%2520be%2520rendered%2520in%25203D%2520similarly%2520to%2520classical%2520meshes.%2520Unfortunately%252C%2520GS%2520are%250Adifficult%2520to%2520condition%2520since%2520they%2520usually%2520require%2520circa%2520hundred%2520thousand%250AGaussian%2520components.%2520To%2520mitigate%2520the%2520caveats%2520of%2520both%2520models%252C%2520we%2520propose%2520a%250Ahybrid%2520model%2520Viewing%2520Direction%2520Gaussian%2520Splatting%2520%2528VDGS%2529%2520that%2520uses%2520GS%250Arepresentation%2520of%2520the%25203D%2520object%2527s%2520shape%2520and%2520NeRF-based%2520encoding%2520of%2520color%2520and%250Aopacity.%2520Our%2520model%2520uses%2520Gaussian%2520distributions%2520with%2520trainable%2520positions%2520%2528i.e.%250Ameans%2520of%2520Gaussian%2529%252C%2520shape%2520%2528i.e.%2520covariance%2520of%2520Gaussian%2529%252C%2520color%2520and%2520opacity%252C%2520and%250Aa%2520neural%2520network%2520that%2520takes%2520Gaussian%2520parameters%2520and%2520viewing%2520direction%2520to%250Aproduce%2520changes%2520in%2520the%2520said%2520color%2520and%2520opacity.%2520As%2520a%2520result%252C%2520our%2520model%2520better%250Adescribes%2520shadows%252C%2520light%2520reflections%252C%2520and%2520the%2520transparency%2520of%25203D%2520objects%250Awithout%2520adding%2520additional%2520texture%2520and%2520light%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13729v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20with%20NeRF-based%20Color%20and%20Opacity&entry.906535625=Dawid%20Malarz%20and%20Weronika%20Smolak%20and%20Jacek%20Tabor%20and%20S%C5%82awomir%20Tadeja%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20demonstrated%20the%20remarkable%20potential%20of%0Aneural%20networks%20to%20capture%20the%20intricacies%20of%203D%20objects.%20By%20encoding%20the%20shape%0Aand%20color%20information%20within%20neural%20network%20weights%2C%20NeRFs%20excel%20at%20producing%0Astrikingly%20sharp%20novel%20views%20of%203D%20objects.%20Recently%2C%20numerous%20generalizations%0Aof%20NeRFs%20utilizing%20generative%20models%20have%20emerged%2C%20expanding%20its%20versatility.%0AIn%20contrast%2C%20Gaussian%20Splatting%20%28GS%29%20offers%20a%20similar%20render%20quality%20with%0Afaster%20training%20and%20inference%20as%20it%20does%20not%20need%20neural%20networks%20to%20work.%20It%0Aencodes%20information%20about%20the%203D%20objects%20in%20the%20set%20of%20Gaussian%20distributions%0Athat%20can%20be%20rendered%20in%203D%20similarly%20to%20classical%20meshes.%20Unfortunately%2C%20GS%20are%0Adifficult%20to%20condition%20since%20they%20usually%20require%20circa%20hundred%20thousand%0AGaussian%20components.%20To%20mitigate%20the%20caveats%20of%20both%20models%2C%20we%20propose%20a%0Ahybrid%20model%20Viewing%20Direction%20Gaussian%20Splatting%20%28VDGS%29%20that%20uses%20GS%0Arepresentation%20of%20the%203D%20object%27s%20shape%20and%20NeRF-based%20encoding%20of%20color%20and%0Aopacity.%20Our%20model%20uses%20Gaussian%20distributions%20with%20trainable%20positions%20%28i.e.%0Ameans%20of%20Gaussian%29%2C%20shape%20%28i.e.%20covariance%20of%20Gaussian%29%2C%20color%20and%20opacity%2C%20and%0Aa%20neural%20network%20that%20takes%20Gaussian%20parameters%20and%20viewing%20direction%20to%0Aproduce%20changes%20in%20the%20said%20color%20and%20opacity.%20As%20a%20result%2C%20our%20model%20better%0Adescribes%20shadows%2C%20light%20reflections%2C%20and%20the%20transparency%20of%203D%20objects%0Awithout%20adding%20additional%20texture%20and%20light%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13729v4&entry.124074799=Read"},
{"title": "Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field", "author": "Chao Wang and Krzysztof Wolski and Bernhard Kerbl and Ana Serrano and Mojtaba Bemana and Hans-Peter Seidel and Karol Myszkowski and Thomas Leimk\u00fchler", "abstract": "  Radiance field methods represent the state of the art in reconstructing\ncomplex scenes from multi-view photos. However, these reconstructions often\nsuffer from one or both of the following limitations: First, they typically\nrepresent scenes in low dynamic range (LDR), which restricts their use to\nevenly lit environments and hinders immersive viewing experiences. Secondly,\ntheir reliance on a pinhole camera model, assuming all scene elements are in\nfocus in the input images, presents practical challenges and complicates\nrefocusing during novel-view synthesis. Addressing these limitations, we\npresent a lightweight method based on 3D Gaussian Splatting that utilizes\nmulti-view LDR images of a scene with varying exposure times, apertures, and\nfocus distances as input to reconstruct a high-dynamic-range (HDR) radiance\nfield. By incorporating analytical convolutions of Gaussians based on a\nthin-lens camera model as well as a tonemapping module, our reconstructions\nenable the rendering of HDR content with flexible refocusing capabilities. We\ndemonstrate that our combined treatment of HDR and depth of field facilitates\nreal-time cinematic rendering, outperforming the state of the art.\n", "link": "http://arxiv.org/abs/2406.07329v1", "date": "2024-06-11", "relevancy": 3.0546, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6639}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6134}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cinematic%20Gaussians%3A%20Real-Time%20HDR%20Radiance%20Fields%20with%20Depth%20of%20Field&body=Title%3A%20Cinematic%20Gaussians%3A%20Real-Time%20HDR%20Radiance%20Fields%20with%20Depth%20of%20Field%0AAuthor%3A%20Chao%20Wang%20and%20Krzysztof%20Wolski%20and%20Bernhard%20Kerbl%20and%20Ana%20Serrano%20and%20Mojtaba%20Bemana%20and%20Hans-Peter%20Seidel%20and%20Karol%20Myszkowski%20and%20Thomas%20Leimk%C3%BChler%0AAbstract%3A%20%20%20Radiance%20field%20methods%20represent%20the%20state%20of%20the%20art%20in%20reconstructing%0Acomplex%20scenes%20from%20multi-view%20photos.%20However%2C%20these%20reconstructions%20often%0Asuffer%20from%20one%20or%20both%20of%20the%20following%20limitations%3A%20First%2C%20they%20typically%0Arepresent%20scenes%20in%20low%20dynamic%20range%20%28LDR%29%2C%20which%20restricts%20their%20use%20to%0Aevenly%20lit%20environments%20and%20hinders%20immersive%20viewing%20experiences.%20Secondly%2C%0Atheir%20reliance%20on%20a%20pinhole%20camera%20model%2C%20assuming%20all%20scene%20elements%20are%20in%0Afocus%20in%20the%20input%20images%2C%20presents%20practical%20challenges%20and%20complicates%0Arefocusing%20during%20novel-view%20synthesis.%20Addressing%20these%20limitations%2C%20we%0Apresent%20a%20lightweight%20method%20based%20on%203D%20Gaussian%20Splatting%20that%20utilizes%0Amulti-view%20LDR%20images%20of%20a%20scene%20with%20varying%20exposure%20times%2C%20apertures%2C%20and%0Afocus%20distances%20as%20input%20to%20reconstruct%20a%20high-dynamic-range%20%28HDR%29%20radiance%0Afield.%20By%20incorporating%20analytical%20convolutions%20of%20Gaussians%20based%20on%20a%0Athin-lens%20camera%20model%20as%20well%20as%20a%20tonemapping%20module%2C%20our%20reconstructions%0Aenable%20the%20rendering%20of%20HDR%20content%20with%20flexible%20refocusing%20capabilities.%20We%0Ademonstrate%20that%20our%20combined%20treatment%20of%20HDR%20and%20depth%20of%20field%20facilitates%0Areal-time%20cinematic%20rendering%2C%20outperforming%20the%20state%20of%20the%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCinematic%2520Gaussians%253A%2520Real-Time%2520HDR%2520Radiance%2520Fields%2520with%2520Depth%2520of%2520Field%26entry.906535625%3DChao%2520Wang%2520and%2520Krzysztof%2520Wolski%2520and%2520Bernhard%2520Kerbl%2520and%2520Ana%2520Serrano%2520and%2520Mojtaba%2520Bemana%2520and%2520Hans-Peter%2520Seidel%2520and%2520Karol%2520Myszkowski%2520and%2520Thomas%2520Leimk%25C3%25BChler%26entry.1292438233%3D%2520%2520Radiance%2520field%2520methods%2520represent%2520the%2520state%2520of%2520the%2520art%2520in%2520reconstructing%250Acomplex%2520scenes%2520from%2520multi-view%2520photos.%2520However%252C%2520these%2520reconstructions%2520often%250Asuffer%2520from%2520one%2520or%2520both%2520of%2520the%2520following%2520limitations%253A%2520First%252C%2520they%2520typically%250Arepresent%2520scenes%2520in%2520low%2520dynamic%2520range%2520%2528LDR%2529%252C%2520which%2520restricts%2520their%2520use%2520to%250Aevenly%2520lit%2520environments%2520and%2520hinders%2520immersive%2520viewing%2520experiences.%2520Secondly%252C%250Atheir%2520reliance%2520on%2520a%2520pinhole%2520camera%2520model%252C%2520assuming%2520all%2520scene%2520elements%2520are%2520in%250Afocus%2520in%2520the%2520input%2520images%252C%2520presents%2520practical%2520challenges%2520and%2520complicates%250Arefocusing%2520during%2520novel-view%2520synthesis.%2520Addressing%2520these%2520limitations%252C%2520we%250Apresent%2520a%2520lightweight%2520method%2520based%2520on%25203D%2520Gaussian%2520Splatting%2520that%2520utilizes%250Amulti-view%2520LDR%2520images%2520of%2520a%2520scene%2520with%2520varying%2520exposure%2520times%252C%2520apertures%252C%2520and%250Afocus%2520distances%2520as%2520input%2520to%2520reconstruct%2520a%2520high-dynamic-range%2520%2528HDR%2529%2520radiance%250Afield.%2520By%2520incorporating%2520analytical%2520convolutions%2520of%2520Gaussians%2520based%2520on%2520a%250Athin-lens%2520camera%2520model%2520as%2520well%2520as%2520a%2520tonemapping%2520module%252C%2520our%2520reconstructions%250Aenable%2520the%2520rendering%2520of%2520HDR%2520content%2520with%2520flexible%2520refocusing%2520capabilities.%2520We%250Ademonstrate%2520that%2520our%2520combined%2520treatment%2520of%2520HDR%2520and%2520depth%2520of%2520field%2520facilitates%250Areal-time%2520cinematic%2520rendering%252C%2520outperforming%2520the%2520state%2520of%2520the%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cinematic%20Gaussians%3A%20Real-Time%20HDR%20Radiance%20Fields%20with%20Depth%20of%20Field&entry.906535625=Chao%20Wang%20and%20Krzysztof%20Wolski%20and%20Bernhard%20Kerbl%20and%20Ana%20Serrano%20and%20Mojtaba%20Bemana%20and%20Hans-Peter%20Seidel%20and%20Karol%20Myszkowski%20and%20Thomas%20Leimk%C3%BChler&entry.1292438233=%20%20Radiance%20field%20methods%20represent%20the%20state%20of%20the%20art%20in%20reconstructing%0Acomplex%20scenes%20from%20multi-view%20photos.%20However%2C%20these%20reconstructions%20often%0Asuffer%20from%20one%20or%20both%20of%20the%20following%20limitations%3A%20First%2C%20they%20typically%0Arepresent%20scenes%20in%20low%20dynamic%20range%20%28LDR%29%2C%20which%20restricts%20their%20use%20to%0Aevenly%20lit%20environments%20and%20hinders%20immersive%20viewing%20experiences.%20Secondly%2C%0Atheir%20reliance%20on%20a%20pinhole%20camera%20model%2C%20assuming%20all%20scene%20elements%20are%20in%0Afocus%20in%20the%20input%20images%2C%20presents%20practical%20challenges%20and%20complicates%0Arefocusing%20during%20novel-view%20synthesis.%20Addressing%20these%20limitations%2C%20we%0Apresent%20a%20lightweight%20method%20based%20on%203D%20Gaussian%20Splatting%20that%20utilizes%0Amulti-view%20LDR%20images%20of%20a%20scene%20with%20varying%20exposure%20times%2C%20apertures%2C%20and%0Afocus%20distances%20as%20input%20to%20reconstruct%20a%20high-dynamic-range%20%28HDR%29%20radiance%0Afield.%20By%20incorporating%20analytical%20convolutions%20of%20Gaussians%20based%20on%20a%0Athin-lens%20camera%20model%20as%20well%20as%20a%20tonemapping%20module%2C%20our%20reconstructions%0Aenable%20the%20rendering%20of%20HDR%20content%20with%20flexible%20refocusing%20capabilities.%20We%0Ademonstrate%20that%20our%20combined%20treatment%20of%20HDR%20and%20depth%20of%20field%20facilitates%0Areal-time%20cinematic%20rendering%2C%20outperforming%20the%20state%20of%20the%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07329v1&entry.124074799=Read"},
{"title": "GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided\n  Generative Gaussian Splatting", "author": "Xiaoyu Zhou and Xingjian Ran and Yajiao Xiong and Jinlin He and Zhiwei Lin and Yongtao Wang and Deqing Sun and Ming-Hsuan Yang", "abstract": "  We present GALA3D, generative 3D GAussians with LAyout-guided control, for\neffective compositional text-to-3D generation. We first utilize large language\nmodels (LLMs) to generate the initial layout and introduce a layout-guided 3D\nGaussian representation for 3D content generation with adaptive geometric\nconstraints. We then propose an instance-scene compositional optimization\nmechanism with conditioned diffusion to collaboratively generate realistic 3D\nscenes with consistent geometry, texture, scale, and accurate interactions\namong multiple objects while simultaneously adjusting the coarse layout priors\nextracted from the LLMs to align with the generated scene. Experiments show\nthat GALA3D is a user-friendly, end-to-end framework for state-of-the-art\nscene-level 3D content generation and controllable editing while ensuring the\nhigh fidelity of object-level entities within the scene. The source codes and\nmodels will be available at gala3d.github.io.\n", "link": "http://arxiv.org/abs/2402.07207v2", "date": "2024-06-11", "relevancy": 3.0113, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6209}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5935}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GALA3D%3A%20Towards%20Text-to-3D%20Complex%20Scene%20Generation%20via%20Layout-guided%0A%20%20Generative%20Gaussian%20Splatting&body=Title%3A%20GALA3D%3A%20Towards%20Text-to-3D%20Complex%20Scene%20Generation%20via%20Layout-guided%0A%20%20Generative%20Gaussian%20Splatting%0AAuthor%3A%20Xiaoyu%20Zhou%20and%20Xingjian%20Ran%20and%20Yajiao%20Xiong%20and%20Jinlin%20He%20and%20Zhiwei%20Lin%20and%20Yongtao%20Wang%20and%20Deqing%20Sun%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20We%20present%20GALA3D%2C%20generative%203D%20GAussians%20with%20LAyout-guided%20control%2C%20for%0Aeffective%20compositional%20text-to-3D%20generation.%20We%20first%20utilize%20large%20language%0Amodels%20%28LLMs%29%20to%20generate%20the%20initial%20layout%20and%20introduce%20a%20layout-guided%203D%0AGaussian%20representation%20for%203D%20content%20generation%20with%20adaptive%20geometric%0Aconstraints.%20We%20then%20propose%20an%20instance-scene%20compositional%20optimization%0Amechanism%20with%20conditioned%20diffusion%20to%20collaboratively%20generate%20realistic%203D%0Ascenes%20with%20consistent%20geometry%2C%20texture%2C%20scale%2C%20and%20accurate%20interactions%0Aamong%20multiple%20objects%20while%20simultaneously%20adjusting%20the%20coarse%20layout%20priors%0Aextracted%20from%20the%20LLMs%20to%20align%20with%20the%20generated%20scene.%20Experiments%20show%0Athat%20GALA3D%20is%20a%20user-friendly%2C%20end-to-end%20framework%20for%20state-of-the-art%0Ascene-level%203D%20content%20generation%20and%20controllable%20editing%20while%20ensuring%20the%0Ahigh%20fidelity%20of%20object-level%20entities%20within%20the%20scene.%20The%20source%20codes%20and%0Amodels%20will%20be%20available%20at%20gala3d.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07207v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGALA3D%253A%2520Towards%2520Text-to-3D%2520Complex%2520Scene%2520Generation%2520via%2520Layout-guided%250A%2520%2520Generative%2520Gaussian%2520Splatting%26entry.906535625%3DXiaoyu%2520Zhou%2520and%2520Xingjian%2520Ran%2520and%2520Yajiao%2520Xiong%2520and%2520Jinlin%2520He%2520and%2520Zhiwei%2520Lin%2520and%2520Yongtao%2520Wang%2520and%2520Deqing%2520Sun%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520We%2520present%2520GALA3D%252C%2520generative%25203D%2520GAussians%2520with%2520LAyout-guided%2520control%252C%2520for%250Aeffective%2520compositional%2520text-to-3D%2520generation.%2520We%2520first%2520utilize%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520generate%2520the%2520initial%2520layout%2520and%2520introduce%2520a%2520layout-guided%25203D%250AGaussian%2520representation%2520for%25203D%2520content%2520generation%2520with%2520adaptive%2520geometric%250Aconstraints.%2520We%2520then%2520propose%2520an%2520instance-scene%2520compositional%2520optimization%250Amechanism%2520with%2520conditioned%2520diffusion%2520to%2520collaboratively%2520generate%2520realistic%25203D%250Ascenes%2520with%2520consistent%2520geometry%252C%2520texture%252C%2520scale%252C%2520and%2520accurate%2520interactions%250Aamong%2520multiple%2520objects%2520while%2520simultaneously%2520adjusting%2520the%2520coarse%2520layout%2520priors%250Aextracted%2520from%2520the%2520LLMs%2520to%2520align%2520with%2520the%2520generated%2520scene.%2520Experiments%2520show%250Athat%2520GALA3D%2520is%2520a%2520user-friendly%252C%2520end-to-end%2520framework%2520for%2520state-of-the-art%250Ascene-level%25203D%2520content%2520generation%2520and%2520controllable%2520editing%2520while%2520ensuring%2520the%250Ahigh%2520fidelity%2520of%2520object-level%2520entities%2520within%2520the%2520scene.%2520The%2520source%2520codes%2520and%250Amodels%2520will%2520be%2520available%2520at%2520gala3d.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07207v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GALA3D%3A%20Towards%20Text-to-3D%20Complex%20Scene%20Generation%20via%20Layout-guided%0A%20%20Generative%20Gaussian%20Splatting&entry.906535625=Xiaoyu%20Zhou%20and%20Xingjian%20Ran%20and%20Yajiao%20Xiong%20and%20Jinlin%20He%20and%20Zhiwei%20Lin%20and%20Yongtao%20Wang%20and%20Deqing%20Sun%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20We%20present%20GALA3D%2C%20generative%203D%20GAussians%20with%20LAyout-guided%20control%2C%20for%0Aeffective%20compositional%20text-to-3D%20generation.%20We%20first%20utilize%20large%20language%0Amodels%20%28LLMs%29%20to%20generate%20the%20initial%20layout%20and%20introduce%20a%20layout-guided%203D%0AGaussian%20representation%20for%203D%20content%20generation%20with%20adaptive%20geometric%0Aconstraints.%20We%20then%20propose%20an%20instance-scene%20compositional%20optimization%0Amechanism%20with%20conditioned%20diffusion%20to%20collaboratively%20generate%20realistic%203D%0Ascenes%20with%20consistent%20geometry%2C%20texture%2C%20scale%2C%20and%20accurate%20interactions%0Aamong%20multiple%20objects%20while%20simultaneously%20adjusting%20the%20coarse%20layout%20priors%0Aextracted%20from%20the%20LLMs%20to%20align%20with%20the%20generated%20scene.%20Experiments%20show%0Athat%20GALA3D%20is%20a%20user-friendly%2C%20end-to-end%20framework%20for%20state-of-the-art%0Ascene-level%203D%20content%20generation%20and%20controllable%20editing%20while%20ensuring%20the%0Ahigh%20fidelity%20of%20object-level%20entities%20within%20the%20scene.%20The%20source%20codes%20and%0Amodels%20will%20be%20available%20at%20gala3d.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07207v2&entry.124074799=Read"},
{"title": "Generated Contents Enrichment", "author": "Mahdi Naseri and Jiayan Qiu and Zhou Wang", "abstract": "  In this paper, we investigate a novel artificial intelligence generation\ntask, termed as generated contents enrichment (GCE). Different from\nconventional artificial intelligence contents generation task that enriches the\ngiven textual description implicitly with limited semantics for generating\nvisually real content, our proposed GCE strives to perform content enrichment\nexplicitly on both the visual and textual domain, from which the enriched\ncontents are visually real, structurally reasonable, and semantically abundant.\nTowards to solve GCE, we propose a deep end-to-end method that explicitly\nexplores the semantics and inter-semantic relationships during the enrichment.\nSpecifically, we first model the input description as a semantic graph, wherein\neach node represents an object and each edge corresponds to the inter-object\nrelationship. We then adopt Graph Convolutional Networks on top of the input\nscene description to predict the enriching objects and their relationships with\nthe input objects. Finally, the enriched description is fed into an image\nsynthesis model to carry out the visual contents generation. Our experiments\nconducted on the Visual Genome dataset exhibit promising and visually plausible\nresults.\n", "link": "http://arxiv.org/abs/2405.03650v2", "date": "2024-06-11", "relevancy": 2.9375, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6498}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5575}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generated%20Contents%20Enrichment&body=Title%3A%20Generated%20Contents%20Enrichment%0AAuthor%3A%20Mahdi%20Naseri%20and%20Jiayan%20Qiu%20and%20Zhou%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20a%20novel%20artificial%20intelligence%20generation%0Atask%2C%20termed%20as%20generated%20contents%20enrichment%20%28GCE%29.%20Different%20from%0Aconventional%20artificial%20intelligence%20contents%20generation%20task%20that%20enriches%20the%0Agiven%20textual%20description%20implicitly%20with%20limited%20semantics%20for%20generating%0Avisually%20real%20content%2C%20our%20proposed%20GCE%20strives%20to%20perform%20content%20enrichment%0Aexplicitly%20on%20both%20the%20visual%20and%20textual%20domain%2C%20from%20which%20the%20enriched%0Acontents%20are%20visually%20real%2C%20structurally%20reasonable%2C%20and%20semantically%20abundant.%0ATowards%20to%20solve%20GCE%2C%20we%20propose%20a%20deep%20end-to-end%20method%20that%20explicitly%0Aexplores%20the%20semantics%20and%20inter-semantic%20relationships%20during%20the%20enrichment.%0ASpecifically%2C%20we%20first%20model%20the%20input%20description%20as%20a%20semantic%20graph%2C%20wherein%0Aeach%20node%20represents%20an%20object%20and%20each%20edge%20corresponds%20to%20the%20inter-object%0Arelationship.%20We%20then%20adopt%20Graph%20Convolutional%20Networks%20on%20top%20of%20the%20input%0Ascene%20description%20to%20predict%20the%20enriching%20objects%20and%20their%20relationships%20with%0Athe%20input%20objects.%20Finally%2C%20the%20enriched%20description%20is%20fed%20into%20an%20image%0Asynthesis%20model%20to%20carry%20out%20the%20visual%20contents%20generation.%20Our%20experiments%0Aconducted%20on%20the%20Visual%20Genome%20dataset%20exhibit%20promising%20and%20visually%20plausible%0Aresults.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03650v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerated%2520Contents%2520Enrichment%26entry.906535625%3DMahdi%2520Naseri%2520and%2520Jiayan%2520Qiu%2520and%2520Zhou%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520a%2520novel%2520artificial%2520intelligence%2520generation%250Atask%252C%2520termed%2520as%2520generated%2520contents%2520enrichment%2520%2528GCE%2529.%2520Different%2520from%250Aconventional%2520artificial%2520intelligence%2520contents%2520generation%2520task%2520that%2520enriches%2520the%250Agiven%2520textual%2520description%2520implicitly%2520with%2520limited%2520semantics%2520for%2520generating%250Avisually%2520real%2520content%252C%2520our%2520proposed%2520GCE%2520strives%2520to%2520perform%2520content%2520enrichment%250Aexplicitly%2520on%2520both%2520the%2520visual%2520and%2520textual%2520domain%252C%2520from%2520which%2520the%2520enriched%250Acontents%2520are%2520visually%2520real%252C%2520structurally%2520reasonable%252C%2520and%2520semantically%2520abundant.%250ATowards%2520to%2520solve%2520GCE%252C%2520we%2520propose%2520a%2520deep%2520end-to-end%2520method%2520that%2520explicitly%250Aexplores%2520the%2520semantics%2520and%2520inter-semantic%2520relationships%2520during%2520the%2520enrichment.%250ASpecifically%252C%2520we%2520first%2520model%2520the%2520input%2520description%2520as%2520a%2520semantic%2520graph%252C%2520wherein%250Aeach%2520node%2520represents%2520an%2520object%2520and%2520each%2520edge%2520corresponds%2520to%2520the%2520inter-object%250Arelationship.%2520We%2520then%2520adopt%2520Graph%2520Convolutional%2520Networks%2520on%2520top%2520of%2520the%2520input%250Ascene%2520description%2520to%2520predict%2520the%2520enriching%2520objects%2520and%2520their%2520relationships%2520with%250Athe%2520input%2520objects.%2520Finally%252C%2520the%2520enriched%2520description%2520is%2520fed%2520into%2520an%2520image%250Asynthesis%2520model%2520to%2520carry%2520out%2520the%2520visual%2520contents%2520generation.%2520Our%2520experiments%250Aconducted%2520on%2520the%2520Visual%2520Genome%2520dataset%2520exhibit%2520promising%2520and%2520visually%2520plausible%250Aresults.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03650v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generated%20Contents%20Enrichment&entry.906535625=Mahdi%20Naseri%20and%20Jiayan%20Qiu%20and%20Zhou%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20a%20novel%20artificial%20intelligence%20generation%0Atask%2C%20termed%20as%20generated%20contents%20enrichment%20%28GCE%29.%20Different%20from%0Aconventional%20artificial%20intelligence%20contents%20generation%20task%20that%20enriches%20the%0Agiven%20textual%20description%20implicitly%20with%20limited%20semantics%20for%20generating%0Avisually%20real%20content%2C%20our%20proposed%20GCE%20strives%20to%20perform%20content%20enrichment%0Aexplicitly%20on%20both%20the%20visual%20and%20textual%20domain%2C%20from%20which%20the%20enriched%0Acontents%20are%20visually%20real%2C%20structurally%20reasonable%2C%20and%20semantically%20abundant.%0ATowards%20to%20solve%20GCE%2C%20we%20propose%20a%20deep%20end-to-end%20method%20that%20explicitly%0Aexplores%20the%20semantics%20and%20inter-semantic%20relationships%20during%20the%20enrichment.%0ASpecifically%2C%20we%20first%20model%20the%20input%20description%20as%20a%20semantic%20graph%2C%20wherein%0Aeach%20node%20represents%20an%20object%20and%20each%20edge%20corresponds%20to%20the%20inter-object%0Arelationship.%20We%20then%20adopt%20Graph%20Convolutional%20Networks%20on%20top%20of%20the%20input%0Ascene%20description%20to%20predict%20the%20enriching%20objects%20and%20their%20relationships%20with%0Athe%20input%20objects.%20Finally%2C%20the%20enriched%20description%20is%20fed%20into%20an%20image%0Asynthesis%20model%20to%20carry%20out%20the%20visual%20contents%20generation.%20Our%20experiments%0Aconducted%20on%20the%20Visual%20Genome%20dataset%20exhibit%20promising%20and%20visually%20plausible%0Aresults.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03650v2&entry.124074799=Read"},
{"title": "Benchmarking and Boosting Radiology Report Generation for 3D\n  High-Resolution Medical Images", "author": "Che Liu and Zhongwei Wan and Yuqi Wang and Hui Shen and Haozhe Wang and Kangyu Zheng and Mi Zhang and Rossella Arcucci", "abstract": "  Automatic radiology report generation can significantly benefit the\nlabor-intensive process of report writing by radiologists, especially for 3D\nradiographs like CT scans, which are crucial for broad clinical diagnostics yet\nunderexplored compared to 2D radiographs. Existing methods often handle 3D\nvolumes either slice-wise or with aggressive downsampling due to current GPU\nmemory limitations, which results in a loss of the inherent 3D nature and\ncritical details. To overcome these issues, we introduce a novel framework that\nefficiently and effectively generates radiology reports for high-resolution\n(HR) 3D volumes, based on large language models (LLMs). Specifically, our\nframework utilizes low-resolution (LR) visual tokens as queries to mine\ninformation from HR tokens, preserving detailed HR information while reducing\ncomputational costs by only processing HR informed LR visual queries. Further\nbenefiting the field, we curate and release BIMCV-RG, a new dataset with 5,328\nHR 3D volumes and paired reports, establishing the first benchmarks for report\ngeneration from 3D HR medical images. Our method consistently surpasses\nexisting methods on this benchmark across three different settings:\nnormal-resolution, high-resolution inputs, and zero-shot domain transfer, all\nat an acceptable computational cost, trainable on a single A100-80G.\n", "link": "http://arxiv.org/abs/2406.07146v1", "date": "2024-06-11", "relevancy": 2.8785, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5869}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5869}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20and%20Boosting%20Radiology%20Report%20Generation%20for%203D%0A%20%20High-Resolution%20Medical%20Images&body=Title%3A%20Benchmarking%20and%20Boosting%20Radiology%20Report%20Generation%20for%203D%0A%20%20High-Resolution%20Medical%20Images%0AAuthor%3A%20Che%20Liu%20and%20Zhongwei%20Wan%20and%20Yuqi%20Wang%20and%20Hui%20Shen%20and%20Haozhe%20Wang%20and%20Kangyu%20Zheng%20and%20Mi%20Zhang%20and%20Rossella%20Arcucci%0AAbstract%3A%20%20%20Automatic%20radiology%20report%20generation%20can%20significantly%20benefit%20the%0Alabor-intensive%20process%20of%20report%20writing%20by%20radiologists%2C%20especially%20for%203D%0Aradiographs%20like%20CT%20scans%2C%20which%20are%20crucial%20for%20broad%20clinical%20diagnostics%20yet%0Aunderexplored%20compared%20to%202D%20radiographs.%20Existing%20methods%20often%20handle%203D%0Avolumes%20either%20slice-wise%20or%20with%20aggressive%20downsampling%20due%20to%20current%20GPU%0Amemory%20limitations%2C%20which%20results%20in%20a%20loss%20of%20the%20inherent%203D%20nature%20and%0Acritical%20details.%20To%20overcome%20these%20issues%2C%20we%20introduce%20a%20novel%20framework%20that%0Aefficiently%20and%20effectively%20generates%20radiology%20reports%20for%20high-resolution%0A%28HR%29%203D%20volumes%2C%20based%20on%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20our%0Aframework%20utilizes%20low-resolution%20%28LR%29%20visual%20tokens%20as%20queries%20to%20mine%0Ainformation%20from%20HR%20tokens%2C%20preserving%20detailed%20HR%20information%20while%20reducing%0Acomputational%20costs%20by%20only%20processing%20HR%20informed%20LR%20visual%20queries.%20Further%0Abenefiting%20the%20field%2C%20we%20curate%20and%20release%20BIMCV-RG%2C%20a%20new%20dataset%20with%205%2C328%0AHR%203D%20volumes%20and%20paired%20reports%2C%20establishing%20the%20first%20benchmarks%20for%20report%0Ageneration%20from%203D%20HR%20medical%20images.%20Our%20method%20consistently%20surpasses%0Aexisting%20methods%20on%20this%20benchmark%20across%20three%20different%20settings%3A%0Anormal-resolution%2C%20high-resolution%20inputs%2C%20and%20zero-shot%20domain%20transfer%2C%20all%0Aat%20an%20acceptable%20computational%20cost%2C%20trainable%20on%20a%20single%20A100-80G.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520and%2520Boosting%2520Radiology%2520Report%2520Generation%2520for%25203D%250A%2520%2520High-Resolution%2520Medical%2520Images%26entry.906535625%3DChe%2520Liu%2520and%2520Zhongwei%2520Wan%2520and%2520Yuqi%2520Wang%2520and%2520Hui%2520Shen%2520and%2520Haozhe%2520Wang%2520and%2520Kangyu%2520Zheng%2520and%2520Mi%2520Zhang%2520and%2520Rossella%2520Arcucci%26entry.1292438233%3D%2520%2520Automatic%2520radiology%2520report%2520generation%2520can%2520significantly%2520benefit%2520the%250Alabor-intensive%2520process%2520of%2520report%2520writing%2520by%2520radiologists%252C%2520especially%2520for%25203D%250Aradiographs%2520like%2520CT%2520scans%252C%2520which%2520are%2520crucial%2520for%2520broad%2520clinical%2520diagnostics%2520yet%250Aunderexplored%2520compared%2520to%25202D%2520radiographs.%2520Existing%2520methods%2520often%2520handle%25203D%250Avolumes%2520either%2520slice-wise%2520or%2520with%2520aggressive%2520downsampling%2520due%2520to%2520current%2520GPU%250Amemory%2520limitations%252C%2520which%2520results%2520in%2520a%2520loss%2520of%2520the%2520inherent%25203D%2520nature%2520and%250Acritical%2520details.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%250Aefficiently%2520and%2520effectively%2520generates%2520radiology%2520reports%2520for%2520high-resolution%250A%2528HR%2529%25203D%2520volumes%252C%2520based%2520on%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Specifically%252C%2520our%250Aframework%2520utilizes%2520low-resolution%2520%2528LR%2529%2520visual%2520tokens%2520as%2520queries%2520to%2520mine%250Ainformation%2520from%2520HR%2520tokens%252C%2520preserving%2520detailed%2520HR%2520information%2520while%2520reducing%250Acomputational%2520costs%2520by%2520only%2520processing%2520HR%2520informed%2520LR%2520visual%2520queries.%2520Further%250Abenefiting%2520the%2520field%252C%2520we%2520curate%2520and%2520release%2520BIMCV-RG%252C%2520a%2520new%2520dataset%2520with%25205%252C328%250AHR%25203D%2520volumes%2520and%2520paired%2520reports%252C%2520establishing%2520the%2520first%2520benchmarks%2520for%2520report%250Ageneration%2520from%25203D%2520HR%2520medical%2520images.%2520Our%2520method%2520consistently%2520surpasses%250Aexisting%2520methods%2520on%2520this%2520benchmark%2520across%2520three%2520different%2520settings%253A%250Anormal-resolution%252C%2520high-resolution%2520inputs%252C%2520and%2520zero-shot%2520domain%2520transfer%252C%2520all%250Aat%2520an%2520acceptable%2520computational%2520cost%252C%2520trainable%2520on%2520a%2520single%2520A100-80G.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20and%20Boosting%20Radiology%20Report%20Generation%20for%203D%0A%20%20High-Resolution%20Medical%20Images&entry.906535625=Che%20Liu%20and%20Zhongwei%20Wan%20and%20Yuqi%20Wang%20and%20Hui%20Shen%20and%20Haozhe%20Wang%20and%20Kangyu%20Zheng%20and%20Mi%20Zhang%20and%20Rossella%20Arcucci&entry.1292438233=%20%20Automatic%20radiology%20report%20generation%20can%20significantly%20benefit%20the%0Alabor-intensive%20process%20of%20report%20writing%20by%20radiologists%2C%20especially%20for%203D%0Aradiographs%20like%20CT%20scans%2C%20which%20are%20crucial%20for%20broad%20clinical%20diagnostics%20yet%0Aunderexplored%20compared%20to%202D%20radiographs.%20Existing%20methods%20often%20handle%203D%0Avolumes%20either%20slice-wise%20or%20with%20aggressive%20downsampling%20due%20to%20current%20GPU%0Amemory%20limitations%2C%20which%20results%20in%20a%20loss%20of%20the%20inherent%203D%20nature%20and%0Acritical%20details.%20To%20overcome%20these%20issues%2C%20we%20introduce%20a%20novel%20framework%20that%0Aefficiently%20and%20effectively%20generates%20radiology%20reports%20for%20high-resolution%0A%28HR%29%203D%20volumes%2C%20based%20on%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20our%0Aframework%20utilizes%20low-resolution%20%28LR%29%20visual%20tokens%20as%20queries%20to%20mine%0Ainformation%20from%20HR%20tokens%2C%20preserving%20detailed%20HR%20information%20while%20reducing%0Acomputational%20costs%20by%20only%20processing%20HR%20informed%20LR%20visual%20queries.%20Further%0Abenefiting%20the%20field%2C%20we%20curate%20and%20release%20BIMCV-RG%2C%20a%20new%20dataset%20with%205%2C328%0AHR%203D%20volumes%20and%20paired%20reports%2C%20establishing%20the%20first%20benchmarks%20for%20report%0Ageneration%20from%203D%20HR%20medical%20images.%20Our%20method%20consistently%20surpasses%0Aexisting%20methods%20on%20this%20benchmark%20across%20three%20different%20settings%3A%0Anormal-resolution%2C%20high-resolution%20inputs%2C%20and%20zero-shot%20domain%20transfer%2C%20all%0Aat%20an%20acceptable%20computational%20cost%2C%20trainable%20on%20a%20single%20A100-80G.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07146v1&entry.124074799=Read"},
{"title": "VoxNeuS: Enhancing Voxel-Based Neural Surface Reconstruction via\n  Gradient Interpolation", "author": "Sidun Liu and Peng Qiao and Zongxin Ye and Wenyu Li and Yong Dou", "abstract": "  Neural Surface Reconstruction learns a Signed Distance Field~(SDF) to\nreconstruct the 3D model from multi-view images. Previous works adopt\nvoxel-based explicit representation to improve efficiency. However, they\nignored the gradient instability of interpolation in the voxel grid, leading to\ndegradation on convergence and smoothness. Besides, previous works entangled\nthe optimization of geometry and radiance, which leads to the deformation of\ngeometry to explain radiance, causing artifacts when reconstructing textured\nplanes.\n  In this work, we reveal that the instability of gradient comes from its\ndiscontinuity during trilinear interpolation, and propose to use the\ninterpolated gradient instead of the original analytical gradient to eliminate\nthe discontinuity. Based on gradient interpolation, we propose VoxNeuS, a\nlightweight surface reconstruction method for computational and memory\nefficient neural surface reconstruction. Thanks to the explicit representation,\nthe gradient of regularization terms, i.e. Eikonal and curvature loss, are\ndirectly solved, avoiding computation and memory-access overhead.\n  Further, VoxNeuS adopts a geometry-radiance disentangled architecture to\nhandle the geometry deformation from radiance optimization.\n  The experimental results show that VoxNeuS achieves better reconstruction\nquality than previous works. The entire training process takes 15 minutes and\nless than 3 GB of memory on a single 2080ti GPU.\n", "link": "http://arxiv.org/abs/2406.07170v1", "date": "2024-06-11", "relevancy": 2.8591, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5956}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5793}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoxNeuS%3A%20Enhancing%20Voxel-Based%20Neural%20Surface%20Reconstruction%20via%0A%20%20Gradient%20Interpolation&body=Title%3A%20VoxNeuS%3A%20Enhancing%20Voxel-Based%20Neural%20Surface%20Reconstruction%20via%0A%20%20Gradient%20Interpolation%0AAuthor%3A%20Sidun%20Liu%20and%20Peng%20Qiao%20and%20Zongxin%20Ye%20and%20Wenyu%20Li%20and%20Yong%20Dou%0AAbstract%3A%20%20%20Neural%20Surface%20Reconstruction%20learns%20a%20Signed%20Distance%20Field~%28SDF%29%20to%0Areconstruct%20the%203D%20model%20from%20multi-view%20images.%20Previous%20works%20adopt%0Avoxel-based%20explicit%20representation%20to%20improve%20efficiency.%20However%2C%20they%0Aignored%20the%20gradient%20instability%20of%20interpolation%20in%20the%20voxel%20grid%2C%20leading%20to%0Adegradation%20on%20convergence%20and%20smoothness.%20Besides%2C%20previous%20works%20entangled%0Athe%20optimization%20of%20geometry%20and%20radiance%2C%20which%20leads%20to%20the%20deformation%20of%0Ageometry%20to%20explain%20radiance%2C%20causing%20artifacts%20when%20reconstructing%20textured%0Aplanes.%0A%20%20In%20this%20work%2C%20we%20reveal%20that%20the%20instability%20of%20gradient%20comes%20from%20its%0Adiscontinuity%20during%20trilinear%20interpolation%2C%20and%20propose%20to%20use%20the%0Ainterpolated%20gradient%20instead%20of%20the%20original%20analytical%20gradient%20to%20eliminate%0Athe%20discontinuity.%20Based%20on%20gradient%20interpolation%2C%20we%20propose%20VoxNeuS%2C%20a%0Alightweight%20surface%20reconstruction%20method%20for%20computational%20and%20memory%0Aefficient%20neural%20surface%20reconstruction.%20Thanks%20to%20the%20explicit%20representation%2C%0Athe%20gradient%20of%20regularization%20terms%2C%20i.e.%20Eikonal%20and%20curvature%20loss%2C%20are%0Adirectly%20solved%2C%20avoiding%20computation%20and%20memory-access%20overhead.%0A%20%20Further%2C%20VoxNeuS%20adopts%20a%20geometry-radiance%20disentangled%20architecture%20to%0Ahandle%20the%20geometry%20deformation%20from%20radiance%20optimization.%0A%20%20The%20experimental%20results%20show%20that%20VoxNeuS%20achieves%20better%20reconstruction%0Aquality%20than%20previous%20works.%20The%20entire%20training%20process%20takes%2015%20minutes%20and%0Aless%20than%203%20GB%20of%20memory%20on%20a%20single%202080ti%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxNeuS%253A%2520Enhancing%2520Voxel-Based%2520Neural%2520Surface%2520Reconstruction%2520via%250A%2520%2520Gradient%2520Interpolation%26entry.906535625%3DSidun%2520Liu%2520and%2520Peng%2520Qiao%2520and%2520Zongxin%2520Ye%2520and%2520Wenyu%2520Li%2520and%2520Yong%2520Dou%26entry.1292438233%3D%2520%2520Neural%2520Surface%2520Reconstruction%2520learns%2520a%2520Signed%2520Distance%2520Field~%2528SDF%2529%2520to%250Areconstruct%2520the%25203D%2520model%2520from%2520multi-view%2520images.%2520Previous%2520works%2520adopt%250Avoxel-based%2520explicit%2520representation%2520to%2520improve%2520efficiency.%2520However%252C%2520they%250Aignored%2520the%2520gradient%2520instability%2520of%2520interpolation%2520in%2520the%2520voxel%2520grid%252C%2520leading%2520to%250Adegradation%2520on%2520convergence%2520and%2520smoothness.%2520Besides%252C%2520previous%2520works%2520entangled%250Athe%2520optimization%2520of%2520geometry%2520and%2520radiance%252C%2520which%2520leads%2520to%2520the%2520deformation%2520of%250Ageometry%2520to%2520explain%2520radiance%252C%2520causing%2520artifacts%2520when%2520reconstructing%2520textured%250Aplanes.%250A%2520%2520In%2520this%2520work%252C%2520we%2520reveal%2520that%2520the%2520instability%2520of%2520gradient%2520comes%2520from%2520its%250Adiscontinuity%2520during%2520trilinear%2520interpolation%252C%2520and%2520propose%2520to%2520use%2520the%250Ainterpolated%2520gradient%2520instead%2520of%2520the%2520original%2520analytical%2520gradient%2520to%2520eliminate%250Athe%2520discontinuity.%2520Based%2520on%2520gradient%2520interpolation%252C%2520we%2520propose%2520VoxNeuS%252C%2520a%250Alightweight%2520surface%2520reconstruction%2520method%2520for%2520computational%2520and%2520memory%250Aefficient%2520neural%2520surface%2520reconstruction.%2520Thanks%2520to%2520the%2520explicit%2520representation%252C%250Athe%2520gradient%2520of%2520regularization%2520terms%252C%2520i.e.%2520Eikonal%2520and%2520curvature%2520loss%252C%2520are%250Adirectly%2520solved%252C%2520avoiding%2520computation%2520and%2520memory-access%2520overhead.%250A%2520%2520Further%252C%2520VoxNeuS%2520adopts%2520a%2520geometry-radiance%2520disentangled%2520architecture%2520to%250Ahandle%2520the%2520geometry%2520deformation%2520from%2520radiance%2520optimization.%250A%2520%2520The%2520experimental%2520results%2520show%2520that%2520VoxNeuS%2520achieves%2520better%2520reconstruction%250Aquality%2520than%2520previous%2520works.%2520The%2520entire%2520training%2520process%2520takes%252015%2520minutes%2520and%250Aless%2520than%25203%2520GB%2520of%2520memory%2520on%2520a%2520single%25202080ti%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoxNeuS%3A%20Enhancing%20Voxel-Based%20Neural%20Surface%20Reconstruction%20via%0A%20%20Gradient%20Interpolation&entry.906535625=Sidun%20Liu%20and%20Peng%20Qiao%20and%20Zongxin%20Ye%20and%20Wenyu%20Li%20and%20Yong%20Dou&entry.1292438233=%20%20Neural%20Surface%20Reconstruction%20learns%20a%20Signed%20Distance%20Field~%28SDF%29%20to%0Areconstruct%20the%203D%20model%20from%20multi-view%20images.%20Previous%20works%20adopt%0Avoxel-based%20explicit%20representation%20to%20improve%20efficiency.%20However%2C%20they%0Aignored%20the%20gradient%20instability%20of%20interpolation%20in%20the%20voxel%20grid%2C%20leading%20to%0Adegradation%20on%20convergence%20and%20smoothness.%20Besides%2C%20previous%20works%20entangled%0Athe%20optimization%20of%20geometry%20and%20radiance%2C%20which%20leads%20to%20the%20deformation%20of%0Ageometry%20to%20explain%20radiance%2C%20causing%20artifacts%20when%20reconstructing%20textured%0Aplanes.%0A%20%20In%20this%20work%2C%20we%20reveal%20that%20the%20instability%20of%20gradient%20comes%20from%20its%0Adiscontinuity%20during%20trilinear%20interpolation%2C%20and%20propose%20to%20use%20the%0Ainterpolated%20gradient%20instead%20of%20the%20original%20analytical%20gradient%20to%20eliminate%0Athe%20discontinuity.%20Based%20on%20gradient%20interpolation%2C%20we%20propose%20VoxNeuS%2C%20a%0Alightweight%20surface%20reconstruction%20method%20for%20computational%20and%20memory%0Aefficient%20neural%20surface%20reconstruction.%20Thanks%20to%20the%20explicit%20representation%2C%0Athe%20gradient%20of%20regularization%20terms%2C%20i.e.%20Eikonal%20and%20curvature%20loss%2C%20are%0Adirectly%20solved%2C%20avoiding%20computation%20and%20memory-access%20overhead.%0A%20%20Further%2C%20VoxNeuS%20adopts%20a%20geometry-radiance%20disentangled%20architecture%20to%0Ahandle%20the%20geometry%20deformation%20from%20radiance%20optimization.%0A%20%20The%20experimental%20results%20show%20that%20VoxNeuS%20achieves%20better%20reconstruction%0Aquality%20than%20previous%20works.%20The%20entire%20training%20process%20takes%2015%20minutes%20and%0Aless%20than%203%20GB%20of%20memory%20on%20a%20single%202080ti%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07170v1&entry.124074799=Read"},
{"title": "FaceGPT: Self-supervised Learning to Chat about 3D Human Faces", "author": "Haoran Wang and Mohit Mendiratta and Christian Theobalt and Adam Kortylewski", "abstract": "  We introduce FaceGPT, a self-supervised learning framework for Large\nVision-Language Models (VLMs) to reason about 3D human faces from images and\ntext. Typical 3D face reconstruction methods are specialized algorithms that\nlack semantic reasoning capabilities. FaceGPT overcomes this limitation by\nembedding the parameters of a 3D morphable face model (3DMM) into the token\nspace of a VLM, enabling the generation of 3D faces from both textual and\nvisual inputs. FaceGPT is trained in a self-supervised manner as a model-based\nautoencoder from in-the-wild images. In particular, the hidden state of LLM is\nprojected into 3DMM parameters and subsequently rendered as 2D face image to\nguide the self-supervised learning process via image-based reconstruction.\nWithout relying on expensive 3D annotations of human faces, FaceGPT obtains a\ndetailed understanding about 3D human faces, while preserving the capacity to\nunderstand general user instructions. Our experiments demonstrate that FaceGPT\nnot only achieves high-quality 3D face reconstructions but also retains the\nability for general-purpose visual instruction following. Furthermore, FaceGPT\nlearns fully self-supervised to generate 3D faces based on complex textual\ninputs, which opens a new direction in human face analysis.\n", "link": "http://arxiv.org/abs/2406.07163v1", "date": "2024-06-11", "relevancy": 2.8589, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5976}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5614}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceGPT%3A%20Self-supervised%20Learning%20to%20Chat%20about%203D%20Human%20Faces&body=Title%3A%20FaceGPT%3A%20Self-supervised%20Learning%20to%20Chat%20about%203D%20Human%20Faces%0AAuthor%3A%20Haoran%20Wang%20and%20Mohit%20Mendiratta%20and%20Christian%20Theobalt%20and%20Adam%20Kortylewski%0AAbstract%3A%20%20%20We%20introduce%20FaceGPT%2C%20a%20self-supervised%20learning%20framework%20for%20Large%0AVision-Language%20Models%20%28VLMs%29%20to%20reason%20about%203D%20human%20faces%20from%20images%20and%0Atext.%20Typical%203D%20face%20reconstruction%20methods%20are%20specialized%20algorithms%20that%0Alack%20semantic%20reasoning%20capabilities.%20FaceGPT%20overcomes%20this%20limitation%20by%0Aembedding%20the%20parameters%20of%20a%203D%20morphable%20face%20model%20%283DMM%29%20into%20the%20token%0Aspace%20of%20a%20VLM%2C%20enabling%20the%20generation%20of%203D%20faces%20from%20both%20textual%20and%0Avisual%20inputs.%20FaceGPT%20is%20trained%20in%20a%20self-supervised%20manner%20as%20a%20model-based%0Aautoencoder%20from%20in-the-wild%20images.%20In%20particular%2C%20the%20hidden%20state%20of%20LLM%20is%0Aprojected%20into%203DMM%20parameters%20and%20subsequently%20rendered%20as%202D%20face%20image%20to%0Aguide%20the%20self-supervised%20learning%20process%20via%20image-based%20reconstruction.%0AWithout%20relying%20on%20expensive%203D%20annotations%20of%20human%20faces%2C%20FaceGPT%20obtains%20a%0Adetailed%20understanding%20about%203D%20human%20faces%2C%20while%20preserving%20the%20capacity%20to%0Aunderstand%20general%20user%20instructions.%20Our%20experiments%20demonstrate%20that%20FaceGPT%0Anot%20only%20achieves%20high-quality%203D%20face%20reconstructions%20but%20also%20retains%20the%0Aability%20for%20general-purpose%20visual%20instruction%20following.%20Furthermore%2C%20FaceGPT%0Alearns%20fully%20self-supervised%20to%20generate%203D%20faces%20based%20on%20complex%20textual%0Ainputs%2C%20which%20opens%20a%20new%20direction%20in%20human%20face%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceGPT%253A%2520Self-supervised%2520Learning%2520to%2520Chat%2520about%25203D%2520Human%2520Faces%26entry.906535625%3DHaoran%2520Wang%2520and%2520Mohit%2520Mendiratta%2520and%2520Christian%2520Theobalt%2520and%2520Adam%2520Kortylewski%26entry.1292438233%3D%2520%2520We%2520introduce%2520FaceGPT%252C%2520a%2520self-supervised%2520learning%2520framework%2520for%2520Large%250AVision-Language%2520Models%2520%2528VLMs%2529%2520to%2520reason%2520about%25203D%2520human%2520faces%2520from%2520images%2520and%250Atext.%2520Typical%25203D%2520face%2520reconstruction%2520methods%2520are%2520specialized%2520algorithms%2520that%250Alack%2520semantic%2520reasoning%2520capabilities.%2520FaceGPT%2520overcomes%2520this%2520limitation%2520by%250Aembedding%2520the%2520parameters%2520of%2520a%25203D%2520morphable%2520face%2520model%2520%25283DMM%2529%2520into%2520the%2520token%250Aspace%2520of%2520a%2520VLM%252C%2520enabling%2520the%2520generation%2520of%25203D%2520faces%2520from%2520both%2520textual%2520and%250Avisual%2520inputs.%2520FaceGPT%2520is%2520trained%2520in%2520a%2520self-supervised%2520manner%2520as%2520a%2520model-based%250Aautoencoder%2520from%2520in-the-wild%2520images.%2520In%2520particular%252C%2520the%2520hidden%2520state%2520of%2520LLM%2520is%250Aprojected%2520into%25203DMM%2520parameters%2520and%2520subsequently%2520rendered%2520as%25202D%2520face%2520image%2520to%250Aguide%2520the%2520self-supervised%2520learning%2520process%2520via%2520image-based%2520reconstruction.%250AWithout%2520relying%2520on%2520expensive%25203D%2520annotations%2520of%2520human%2520faces%252C%2520FaceGPT%2520obtains%2520a%250Adetailed%2520understanding%2520about%25203D%2520human%2520faces%252C%2520while%2520preserving%2520the%2520capacity%2520to%250Aunderstand%2520general%2520user%2520instructions.%2520Our%2520experiments%2520demonstrate%2520that%2520FaceGPT%250Anot%2520only%2520achieves%2520high-quality%25203D%2520face%2520reconstructions%2520but%2520also%2520retains%2520the%250Aability%2520for%2520general-purpose%2520visual%2520instruction%2520following.%2520Furthermore%252C%2520FaceGPT%250Alearns%2520fully%2520self-supervised%2520to%2520generate%25203D%2520faces%2520based%2520on%2520complex%2520textual%250Ainputs%252C%2520which%2520opens%2520a%2520new%2520direction%2520in%2520human%2520face%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceGPT%3A%20Self-supervised%20Learning%20to%20Chat%20about%203D%20Human%20Faces&entry.906535625=Haoran%20Wang%20and%20Mohit%20Mendiratta%20and%20Christian%20Theobalt%20and%20Adam%20Kortylewski&entry.1292438233=%20%20We%20introduce%20FaceGPT%2C%20a%20self-supervised%20learning%20framework%20for%20Large%0AVision-Language%20Models%20%28VLMs%29%20to%20reason%20about%203D%20human%20faces%20from%20images%20and%0Atext.%20Typical%203D%20face%20reconstruction%20methods%20are%20specialized%20algorithms%20that%0Alack%20semantic%20reasoning%20capabilities.%20FaceGPT%20overcomes%20this%20limitation%20by%0Aembedding%20the%20parameters%20of%20a%203D%20morphable%20face%20model%20%283DMM%29%20into%20the%20token%0Aspace%20of%20a%20VLM%2C%20enabling%20the%20generation%20of%203D%20faces%20from%20both%20textual%20and%0Avisual%20inputs.%20FaceGPT%20is%20trained%20in%20a%20self-supervised%20manner%20as%20a%20model-based%0Aautoencoder%20from%20in-the-wild%20images.%20In%20particular%2C%20the%20hidden%20state%20of%20LLM%20is%0Aprojected%20into%203DMM%20parameters%20and%20subsequently%20rendered%20as%202D%20face%20image%20to%0Aguide%20the%20self-supervised%20learning%20process%20via%20image-based%20reconstruction.%0AWithout%20relying%20on%20expensive%203D%20annotations%20of%20human%20faces%2C%20FaceGPT%20obtains%20a%0Adetailed%20understanding%20about%203D%20human%20faces%2C%20while%20preserving%20the%20capacity%20to%0Aunderstand%20general%20user%20instructions.%20Our%20experiments%20demonstrate%20that%20FaceGPT%0Anot%20only%20achieves%20high-quality%203D%20face%20reconstructions%20but%20also%20retains%20the%0Aability%20for%20general-purpose%20visual%20instruction%20following.%20Furthermore%2C%20FaceGPT%0Alearns%20fully%20self-supervised%20to%20generate%203D%20faces%20based%20on%20complex%20textual%0Ainputs%2C%20which%20opens%20a%20new%20direction%20in%20human%20face%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07163v1&entry.124074799=Read"},
{"title": "CalibNet: Dual-branch Cross-modal Calibration for RGB-D Salient Instance\n  Segmentation", "author": "Jialun Pei and Tao Jiang and He Tang and Nian Liu and Yueming Jin and Deng-Ping Fan and Pheng-Ann Heng", "abstract": "  We propose a novel approach for RGB-D salient instance segmentation using a\ndual-branch cross-modal feature calibration architecture called CalibNet. Our\nmethod simultaneously calibrates depth and RGB features in the kernel and mask\nbranches to generate instance-aware kernels and mask features. CalibNet\nconsists of three simple modules, a dynamic interactive kernel (DIK) and a\nweight-sharing fusion (WSF), which work together to generate effective\ninstance-aware kernels and integrate cross-modal features. To improve the\nquality of depth features, we incorporate a depth similarity assessment (DSA)\nmodule prior to DIK and WSF. In addition, we further contribute a new DSIS\ndataset, which contains 1,940 images with elaborate instance-level annotations.\nExtensive experiments on three challenging benchmarks show that CalibNet yields\na promising result, i.e., 58.0% AP with 320*480 input size on the COME15K-N\ntest set, which significantly surpasses the alternative frameworks. Our code\nand dataset are available at: https://github.com/PJLallen/CalibNet.\n", "link": "http://arxiv.org/abs/2307.08098v2", "date": "2024-06-11", "relevancy": 2.7884, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5867}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CalibNet%3A%20Dual-branch%20Cross-modal%20Calibration%20for%20RGB-D%20Salient%20Instance%0A%20%20Segmentation&body=Title%3A%20CalibNet%3A%20Dual-branch%20Cross-modal%20Calibration%20for%20RGB-D%20Salient%20Instance%0A%20%20Segmentation%0AAuthor%3A%20Jialun%20Pei%20and%20Tao%20Jiang%20and%20He%20Tang%20and%20Nian%20Liu%20and%20Yueming%20Jin%20and%20Deng-Ping%20Fan%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20approach%20for%20RGB-D%20salient%20instance%20segmentation%20using%20a%0Adual-branch%20cross-modal%20feature%20calibration%20architecture%20called%20CalibNet.%20Our%0Amethod%20simultaneously%20calibrates%20depth%20and%20RGB%20features%20in%20the%20kernel%20and%20mask%0Abranches%20to%20generate%20instance-aware%20kernels%20and%20mask%20features.%20CalibNet%0Aconsists%20of%20three%20simple%20modules%2C%20a%20dynamic%20interactive%20kernel%20%28DIK%29%20and%20a%0Aweight-sharing%20fusion%20%28WSF%29%2C%20which%20work%20together%20to%20generate%20effective%0Ainstance-aware%20kernels%20and%20integrate%20cross-modal%20features.%20To%20improve%20the%0Aquality%20of%20depth%20features%2C%20we%20incorporate%20a%20depth%20similarity%20assessment%20%28DSA%29%0Amodule%20prior%20to%20DIK%20and%20WSF.%20In%20addition%2C%20we%20further%20contribute%20a%20new%20DSIS%0Adataset%2C%20which%20contains%201%2C940%20images%20with%20elaborate%20instance-level%20annotations.%0AExtensive%20experiments%20on%20three%20challenging%20benchmarks%20show%20that%20CalibNet%20yields%0Aa%20promising%20result%2C%20i.e.%2C%2058.0%25%20AP%20with%20320%2A480%20input%20size%20on%20the%20COME15K-N%0Atest%20set%2C%20which%20significantly%20surpasses%20the%20alternative%20frameworks.%20Our%20code%0Aand%20dataset%20are%20available%20at%3A%20https%3A//github.com/PJLallen/CalibNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibNet%253A%2520Dual-branch%2520Cross-modal%2520Calibration%2520for%2520RGB-D%2520Salient%2520Instance%250A%2520%2520Segmentation%26entry.906535625%3DJialun%2520Pei%2520and%2520Tao%2520Jiang%2520and%2520He%2520Tang%2520and%2520Nian%2520Liu%2520and%2520Yueming%2520Jin%2520and%2520Deng-Ping%2520Fan%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520approach%2520for%2520RGB-D%2520salient%2520instance%2520segmentation%2520using%2520a%250Adual-branch%2520cross-modal%2520feature%2520calibration%2520architecture%2520called%2520CalibNet.%2520Our%250Amethod%2520simultaneously%2520calibrates%2520depth%2520and%2520RGB%2520features%2520in%2520the%2520kernel%2520and%2520mask%250Abranches%2520to%2520generate%2520instance-aware%2520kernels%2520and%2520mask%2520features.%2520CalibNet%250Aconsists%2520of%2520three%2520simple%2520modules%252C%2520a%2520dynamic%2520interactive%2520kernel%2520%2528DIK%2529%2520and%2520a%250Aweight-sharing%2520fusion%2520%2528WSF%2529%252C%2520which%2520work%2520together%2520to%2520generate%2520effective%250Ainstance-aware%2520kernels%2520and%2520integrate%2520cross-modal%2520features.%2520To%2520improve%2520the%250Aquality%2520of%2520depth%2520features%252C%2520we%2520incorporate%2520a%2520depth%2520similarity%2520assessment%2520%2528DSA%2529%250Amodule%2520prior%2520to%2520DIK%2520and%2520WSF.%2520In%2520addition%252C%2520we%2520further%2520contribute%2520a%2520new%2520DSIS%250Adataset%252C%2520which%2520contains%25201%252C940%2520images%2520with%2520elaborate%2520instance-level%2520annotations.%250AExtensive%2520experiments%2520on%2520three%2520challenging%2520benchmarks%2520show%2520that%2520CalibNet%2520yields%250Aa%2520promising%2520result%252C%2520i.e.%252C%252058.0%2525%2520AP%2520with%2520320%252A480%2520input%2520size%2520on%2520the%2520COME15K-N%250Atest%2520set%252C%2520which%2520significantly%2520surpasses%2520the%2520alternative%2520frameworks.%2520Our%2520code%250Aand%2520dataset%2520are%2520available%2520at%253A%2520https%253A//github.com/PJLallen/CalibNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.08098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CalibNet%3A%20Dual-branch%20Cross-modal%20Calibration%20for%20RGB-D%20Salient%20Instance%0A%20%20Segmentation&entry.906535625=Jialun%20Pei%20and%20Tao%20Jiang%20and%20He%20Tang%20and%20Nian%20Liu%20and%20Yueming%20Jin%20and%20Deng-Ping%20Fan%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%20We%20propose%20a%20novel%20approach%20for%20RGB-D%20salient%20instance%20segmentation%20using%20a%0Adual-branch%20cross-modal%20feature%20calibration%20architecture%20called%20CalibNet.%20Our%0Amethod%20simultaneously%20calibrates%20depth%20and%20RGB%20features%20in%20the%20kernel%20and%20mask%0Abranches%20to%20generate%20instance-aware%20kernels%20and%20mask%20features.%20CalibNet%0Aconsists%20of%20three%20simple%20modules%2C%20a%20dynamic%20interactive%20kernel%20%28DIK%29%20and%20a%0Aweight-sharing%20fusion%20%28WSF%29%2C%20which%20work%20together%20to%20generate%20effective%0Ainstance-aware%20kernels%20and%20integrate%20cross-modal%20features.%20To%20improve%20the%0Aquality%20of%20depth%20features%2C%20we%20incorporate%20a%20depth%20similarity%20assessment%20%28DSA%29%0Amodule%20prior%20to%20DIK%20and%20WSF.%20In%20addition%2C%20we%20further%20contribute%20a%20new%20DSIS%0Adataset%2C%20which%20contains%201%2C940%20images%20with%20elaborate%20instance-level%20annotations.%0AExtensive%20experiments%20on%20three%20challenging%20benchmarks%20show%20that%20CalibNet%20yields%0Aa%20promising%20result%2C%20i.e.%2C%2058.0%25%20AP%20with%20320%2A480%20input%20size%20on%20the%20COME15K-N%0Atest%20set%2C%20which%20significantly%20surpasses%20the%20alternative%20frameworks.%20Our%20code%0Aand%20dataset%20are%20available%20at%3A%20https%3A//github.com/PJLallen/CalibNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08098v2&entry.124074799=Read"},
{"title": "Spatio-Temporal Encoding of Brain Dynamics with Surface Masked\n  Autoencoders", "author": "Simon Dahan and Logan Z. J. Williams and Yourong Guo and Daniel Rueckert and Emma C. Robinson", "abstract": "  The development of robust and generalisable models for encoding the\nspatio-temporal dynamics of human brain activity is crucial for advancing\nneuroscientific discoveries. However, significant individual variation in the\norganisation of the human cerebral cortex makes it difficult to identify\npopulation-level trends in these signals. Recently, Surface Vision Transformers\n(SiTs) have emerged as a promising approach for modelling cortical signals, yet\nthey face some limitations in low-data scenarios due to the lack of inductive\nbiases in their architecture. To address these challenges, this paper proposes\nthe surface Masked AutoEncoder (sMAE) and video surface Masked AutoEncoder\n(vsMAE) - for multivariate and spatio-temporal pre-training of cortical signals\nover regular icosahedral grids. These models are trained to reconstruct\ncortical feature maps from masked versions of the input by learning strong\nlatent representations of cortical structure and function. Such representations\ntranslate into better modelling of individual phenotypes and enhanced\nperformance in downstream tasks. The proposed approach was evaluated on\ncortical phenotype regression using data from the young adult Human Connectome\nProject (HCP) and developing HCP (dHCP). Results show that (v)sMAE pre-trained\nmodels improve phenotyping prediction performance on multiple tasks by $\\ge\n26\\%$, and offer faster convergence relative to models trained from scratch.\nFinally, we show that pre-training vision transformers on large datasets, such\nas the UK Biobank (UKB), supports transfer learning to low-data regimes. Our\ncode and pre-trained models are publicly available at\nhttps://github.com/metrics-lab/surface-masked-autoencoders .\n", "link": "http://arxiv.org/abs/2308.05474v3", "date": "2024-06-11", "relevancy": 2.7788, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5687}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5568}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20Encoding%20of%20Brain%20Dynamics%20with%20Surface%20Masked%0A%20%20Autoencoders&body=Title%3A%20Spatio-Temporal%20Encoding%20of%20Brain%20Dynamics%20with%20Surface%20Masked%0A%20%20Autoencoders%0AAuthor%3A%20Simon%20Dahan%20and%20Logan%20Z.%20J.%20Williams%20and%20Yourong%20Guo%20and%20Daniel%20Rueckert%20and%20Emma%20C.%20Robinson%0AAbstract%3A%20%20%20The%20development%20of%20robust%20and%20generalisable%20models%20for%20encoding%20the%0Aspatio-temporal%20dynamics%20of%20human%20brain%20activity%20is%20crucial%20for%20advancing%0Aneuroscientific%20discoveries.%20However%2C%20significant%20individual%20variation%20in%20the%0Aorganisation%20of%20the%20human%20cerebral%20cortex%20makes%20it%20difficult%20to%20identify%0Apopulation-level%20trends%20in%20these%20signals.%20Recently%2C%20Surface%20Vision%20Transformers%0A%28SiTs%29%20have%20emerged%20as%20a%20promising%20approach%20for%20modelling%20cortical%20signals%2C%20yet%0Athey%20face%20some%20limitations%20in%20low-data%20scenarios%20due%20to%20the%20lack%20of%20inductive%0Abiases%20in%20their%20architecture.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%0Athe%20surface%20Masked%20AutoEncoder%20%28sMAE%29%20and%20video%20surface%20Masked%20AutoEncoder%0A%28vsMAE%29%20-%20for%20multivariate%20and%20spatio-temporal%20pre-training%20of%20cortical%20signals%0Aover%20regular%20icosahedral%20grids.%20These%20models%20are%20trained%20to%20reconstruct%0Acortical%20feature%20maps%20from%20masked%20versions%20of%20the%20input%20by%20learning%20strong%0Alatent%20representations%20of%20cortical%20structure%20and%20function.%20Such%20representations%0Atranslate%20into%20better%20modelling%20of%20individual%20phenotypes%20and%20enhanced%0Aperformance%20in%20downstream%20tasks.%20The%20proposed%20approach%20was%20evaluated%20on%0Acortical%20phenotype%20regression%20using%20data%20from%20the%20young%20adult%20Human%20Connectome%0AProject%20%28HCP%29%20and%20developing%20HCP%20%28dHCP%29.%20Results%20show%20that%20%28v%29sMAE%20pre-trained%0Amodels%20improve%20phenotyping%20prediction%20performance%20on%20multiple%20tasks%20by%20%24%5Cge%0A26%5C%25%24%2C%20and%20offer%20faster%20convergence%20relative%20to%20models%20trained%20from%20scratch.%0AFinally%2C%20we%20show%20that%20pre-training%20vision%20transformers%20on%20large%20datasets%2C%20such%0Aas%20the%20UK%20Biobank%20%28UKB%29%2C%20supports%20transfer%20learning%20to%20low-data%20regimes.%20Our%0Acode%20and%20pre-trained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/metrics-lab/surface-masked-autoencoders%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05474v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Temporal%2520Encoding%2520of%2520Brain%2520Dynamics%2520with%2520Surface%2520Masked%250A%2520%2520Autoencoders%26entry.906535625%3DSimon%2520Dahan%2520and%2520Logan%2520Z.%2520J.%2520Williams%2520and%2520Yourong%2520Guo%2520and%2520Daniel%2520Rueckert%2520and%2520Emma%2520C.%2520Robinson%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520robust%2520and%2520generalisable%2520models%2520for%2520encoding%2520the%250Aspatio-temporal%2520dynamics%2520of%2520human%2520brain%2520activity%2520is%2520crucial%2520for%2520advancing%250Aneuroscientific%2520discoveries.%2520However%252C%2520significant%2520individual%2520variation%2520in%2520the%250Aorganisation%2520of%2520the%2520human%2520cerebral%2520cortex%2520makes%2520it%2520difficult%2520to%2520identify%250Apopulation-level%2520trends%2520in%2520these%2520signals.%2520Recently%252C%2520Surface%2520Vision%2520Transformers%250A%2528SiTs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520modelling%2520cortical%2520signals%252C%2520yet%250Athey%2520face%2520some%2520limitations%2520in%2520low-data%2520scenarios%2520due%2520to%2520the%2520lack%2520of%2520inductive%250Abiases%2520in%2520their%2520architecture.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%250Athe%2520surface%2520Masked%2520AutoEncoder%2520%2528sMAE%2529%2520and%2520video%2520surface%2520Masked%2520AutoEncoder%250A%2528vsMAE%2529%2520-%2520for%2520multivariate%2520and%2520spatio-temporal%2520pre-training%2520of%2520cortical%2520signals%250Aover%2520regular%2520icosahedral%2520grids.%2520These%2520models%2520are%2520trained%2520to%2520reconstruct%250Acortical%2520feature%2520maps%2520from%2520masked%2520versions%2520of%2520the%2520input%2520by%2520learning%2520strong%250Alatent%2520representations%2520of%2520cortical%2520structure%2520and%2520function.%2520Such%2520representations%250Atranslate%2520into%2520better%2520modelling%2520of%2520individual%2520phenotypes%2520and%2520enhanced%250Aperformance%2520in%2520downstream%2520tasks.%2520The%2520proposed%2520approach%2520was%2520evaluated%2520on%250Acortical%2520phenotype%2520regression%2520using%2520data%2520from%2520the%2520young%2520adult%2520Human%2520Connectome%250AProject%2520%2528HCP%2529%2520and%2520developing%2520HCP%2520%2528dHCP%2529.%2520Results%2520show%2520that%2520%2528v%2529sMAE%2520pre-trained%250Amodels%2520improve%2520phenotyping%2520prediction%2520performance%2520on%2520multiple%2520tasks%2520by%2520%2524%255Cge%250A26%255C%2525%2524%252C%2520and%2520offer%2520faster%2520convergence%2520relative%2520to%2520models%2520trained%2520from%2520scratch.%250AFinally%252C%2520we%2520show%2520that%2520pre-training%2520vision%2520transformers%2520on%2520large%2520datasets%252C%2520such%250Aas%2520the%2520UK%2520Biobank%2520%2528UKB%2529%252C%2520supports%2520transfer%2520learning%2520to%2520low-data%2520regimes.%2520Our%250Acode%2520and%2520pre-trained%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/metrics-lab/surface-masked-autoencoders%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05474v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20Encoding%20of%20Brain%20Dynamics%20with%20Surface%20Masked%0A%20%20Autoencoders&entry.906535625=Simon%20Dahan%20and%20Logan%20Z.%20J.%20Williams%20and%20Yourong%20Guo%20and%20Daniel%20Rueckert%20and%20Emma%20C.%20Robinson&entry.1292438233=%20%20The%20development%20of%20robust%20and%20generalisable%20models%20for%20encoding%20the%0Aspatio-temporal%20dynamics%20of%20human%20brain%20activity%20is%20crucial%20for%20advancing%0Aneuroscientific%20discoveries.%20However%2C%20significant%20individual%20variation%20in%20the%0Aorganisation%20of%20the%20human%20cerebral%20cortex%20makes%20it%20difficult%20to%20identify%0Apopulation-level%20trends%20in%20these%20signals.%20Recently%2C%20Surface%20Vision%20Transformers%0A%28SiTs%29%20have%20emerged%20as%20a%20promising%20approach%20for%20modelling%20cortical%20signals%2C%20yet%0Athey%20face%20some%20limitations%20in%20low-data%20scenarios%20due%20to%20the%20lack%20of%20inductive%0Abiases%20in%20their%20architecture.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%0Athe%20surface%20Masked%20AutoEncoder%20%28sMAE%29%20and%20video%20surface%20Masked%20AutoEncoder%0A%28vsMAE%29%20-%20for%20multivariate%20and%20spatio-temporal%20pre-training%20of%20cortical%20signals%0Aover%20regular%20icosahedral%20grids.%20These%20models%20are%20trained%20to%20reconstruct%0Acortical%20feature%20maps%20from%20masked%20versions%20of%20the%20input%20by%20learning%20strong%0Alatent%20representations%20of%20cortical%20structure%20and%20function.%20Such%20representations%0Atranslate%20into%20better%20modelling%20of%20individual%20phenotypes%20and%20enhanced%0Aperformance%20in%20downstream%20tasks.%20The%20proposed%20approach%20was%20evaluated%20on%0Acortical%20phenotype%20regression%20using%20data%20from%20the%20young%20adult%20Human%20Connectome%0AProject%20%28HCP%29%20and%20developing%20HCP%20%28dHCP%29.%20Results%20show%20that%20%28v%29sMAE%20pre-trained%0Amodels%20improve%20phenotyping%20prediction%20performance%20on%20multiple%20tasks%20by%20%24%5Cge%0A26%5C%25%24%2C%20and%20offer%20faster%20convergence%20relative%20to%20models%20trained%20from%20scratch.%0AFinally%2C%20we%20show%20that%20pre-training%20vision%20transformers%20on%20large%20datasets%2C%20such%0Aas%20the%20UK%20Biobank%20%28UKB%29%2C%20supports%20transfer%20learning%20to%20low-data%20regimes.%20Our%0Acode%20and%20pre-trained%20models%20are%20publicly%20available%20at%0Ahttps%3A//github.com/metrics-lab/surface-masked-autoencoders%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05474v3&entry.124074799=Read"},
{"title": "Hearing Anything Anywhere", "author": "Mason Wang and Ryosuke Sawata and Samuel Clarke and Ruohan Gao and Shangzhe Wu and Jiajun Wu", "abstract": "  Recent years have seen immense progress in 3D computer vision and computer\ngraphics, with emerging tools that can virtualize real-world 3D environments\nfor numerous Mixed Reality (XR) applications. However, alongside immersive\nvisual experiences, immersive auditory experiences are equally vital to our\nholistic perception of an environment. In this paper, we aim to reconstruct the\nspatial acoustic characteristics of an arbitrary environment given only a\nsparse set of (roughly 12) room impulse response (RIR) recordings and a planar\nreconstruction of the scene, a setup that is easily achievable by ordinary\nusers. To this end, we introduce DiffRIR, a differentiable RIR rendering\nframework with interpretable parametric models of salient acoustic features of\nthe scene, including sound source directivity and surface reflectivity. This\nallows us to synthesize novel auditory experiences through the space with any\nsource audio. To evaluate our method, we collect a dataset of RIR recordings\nand music in four diverse, real environments. We show that our model\noutperforms state-ofthe-art baselines on rendering monaural and binaural RIRs\nand music at unseen locations, and learns physically interpretable parameters\ncharacterizing acoustic properties of the sound source and surfaces in the\nscene.\n", "link": "http://arxiv.org/abs/2406.07532v1", "date": "2024-06-11", "relevancy": 2.7704, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5735}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5735}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hearing%20Anything%20Anywhere&body=Title%3A%20Hearing%20Anything%20Anywhere%0AAuthor%3A%20Mason%20Wang%20and%20Ryosuke%20Sawata%20and%20Samuel%20Clarke%20and%20Ruohan%20Gao%20and%20Shangzhe%20Wu%20and%20Jiajun%20Wu%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20immense%20progress%20in%203D%20computer%20vision%20and%20computer%0Agraphics%2C%20with%20emerging%20tools%20that%20can%20virtualize%20real-world%203D%20environments%0Afor%20numerous%20Mixed%20Reality%20%28XR%29%20applications.%20However%2C%20alongside%20immersive%0Avisual%20experiences%2C%20immersive%20auditory%20experiences%20are%20equally%20vital%20to%20our%0Aholistic%20perception%20of%20an%20environment.%20In%20this%20paper%2C%20we%20aim%20to%20reconstruct%20the%0Aspatial%20acoustic%20characteristics%20of%20an%20arbitrary%20environment%20given%20only%20a%0Asparse%20set%20of%20%28roughly%2012%29%20room%20impulse%20response%20%28RIR%29%20recordings%20and%20a%20planar%0Areconstruction%20of%20the%20scene%2C%20a%20setup%20that%20is%20easily%20achievable%20by%20ordinary%0Ausers.%20To%20this%20end%2C%20we%20introduce%20DiffRIR%2C%20a%20differentiable%20RIR%20rendering%0Aframework%20with%20interpretable%20parametric%20models%20of%20salient%20acoustic%20features%20of%0Athe%20scene%2C%20including%20sound%20source%20directivity%20and%20surface%20reflectivity.%20This%0Aallows%20us%20to%20synthesize%20novel%20auditory%20experiences%20through%20the%20space%20with%20any%0Asource%20audio.%20To%20evaluate%20our%20method%2C%20we%20collect%20a%20dataset%20of%20RIR%20recordings%0Aand%20music%20in%20four%20diverse%2C%20real%20environments.%20We%20show%20that%20our%20model%0Aoutperforms%20state-ofthe-art%20baselines%20on%20rendering%20monaural%20and%20binaural%20RIRs%0Aand%20music%20at%20unseen%20locations%2C%20and%20learns%20physically%20interpretable%20parameters%0Acharacterizing%20acoustic%20properties%20of%20the%20sound%20source%20and%20surfaces%20in%20the%0Ascene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHearing%2520Anything%2520Anywhere%26entry.906535625%3DMason%2520Wang%2520and%2520Ryosuke%2520Sawata%2520and%2520Samuel%2520Clarke%2520and%2520Ruohan%2520Gao%2520and%2520Shangzhe%2520Wu%2520and%2520Jiajun%2520Wu%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520immense%2520progress%2520in%25203D%2520computer%2520vision%2520and%2520computer%250Agraphics%252C%2520with%2520emerging%2520tools%2520that%2520can%2520virtualize%2520real-world%25203D%2520environments%250Afor%2520numerous%2520Mixed%2520Reality%2520%2528XR%2529%2520applications.%2520However%252C%2520alongside%2520immersive%250Avisual%2520experiences%252C%2520immersive%2520auditory%2520experiences%2520are%2520equally%2520vital%2520to%2520our%250Aholistic%2520perception%2520of%2520an%2520environment.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520reconstruct%2520the%250Aspatial%2520acoustic%2520characteristics%2520of%2520an%2520arbitrary%2520environment%2520given%2520only%2520a%250Asparse%2520set%2520of%2520%2528roughly%252012%2529%2520room%2520impulse%2520response%2520%2528RIR%2529%2520recordings%2520and%2520a%2520planar%250Areconstruction%2520of%2520the%2520scene%252C%2520a%2520setup%2520that%2520is%2520easily%2520achievable%2520by%2520ordinary%250Ausers.%2520To%2520this%2520end%252C%2520we%2520introduce%2520DiffRIR%252C%2520a%2520differentiable%2520RIR%2520rendering%250Aframework%2520with%2520interpretable%2520parametric%2520models%2520of%2520salient%2520acoustic%2520features%2520of%250Athe%2520scene%252C%2520including%2520sound%2520source%2520directivity%2520and%2520surface%2520reflectivity.%2520This%250Aallows%2520us%2520to%2520synthesize%2520novel%2520auditory%2520experiences%2520through%2520the%2520space%2520with%2520any%250Asource%2520audio.%2520To%2520evaluate%2520our%2520method%252C%2520we%2520collect%2520a%2520dataset%2520of%2520RIR%2520recordings%250Aand%2520music%2520in%2520four%2520diverse%252C%2520real%2520environments.%2520We%2520show%2520that%2520our%2520model%250Aoutperforms%2520state-ofthe-art%2520baselines%2520on%2520rendering%2520monaural%2520and%2520binaural%2520RIRs%250Aand%2520music%2520at%2520unseen%2520locations%252C%2520and%2520learns%2520physically%2520interpretable%2520parameters%250Acharacterizing%2520acoustic%2520properties%2520of%2520the%2520sound%2520source%2520and%2520surfaces%2520in%2520the%250Ascene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hearing%20Anything%20Anywhere&entry.906535625=Mason%20Wang%20and%20Ryosuke%20Sawata%20and%20Samuel%20Clarke%20and%20Ruohan%20Gao%20and%20Shangzhe%20Wu%20and%20Jiajun%20Wu&entry.1292438233=%20%20Recent%20years%20have%20seen%20immense%20progress%20in%203D%20computer%20vision%20and%20computer%0Agraphics%2C%20with%20emerging%20tools%20that%20can%20virtualize%20real-world%203D%20environments%0Afor%20numerous%20Mixed%20Reality%20%28XR%29%20applications.%20However%2C%20alongside%20immersive%0Avisual%20experiences%2C%20immersive%20auditory%20experiences%20are%20equally%20vital%20to%20our%0Aholistic%20perception%20of%20an%20environment.%20In%20this%20paper%2C%20we%20aim%20to%20reconstruct%20the%0Aspatial%20acoustic%20characteristics%20of%20an%20arbitrary%20environment%20given%20only%20a%0Asparse%20set%20of%20%28roughly%2012%29%20room%20impulse%20response%20%28RIR%29%20recordings%20and%20a%20planar%0Areconstruction%20of%20the%20scene%2C%20a%20setup%20that%20is%20easily%20achievable%20by%20ordinary%0Ausers.%20To%20this%20end%2C%20we%20introduce%20DiffRIR%2C%20a%20differentiable%20RIR%20rendering%0Aframework%20with%20interpretable%20parametric%20models%20of%20salient%20acoustic%20features%20of%0Athe%20scene%2C%20including%20sound%20source%20directivity%20and%20surface%20reflectivity.%20This%0Aallows%20us%20to%20synthesize%20novel%20auditory%20experiences%20through%20the%20space%20with%20any%0Asource%20audio.%20To%20evaluate%20our%20method%2C%20we%20collect%20a%20dataset%20of%20RIR%20recordings%0Aand%20music%20in%20four%20diverse%2C%20real%20environments.%20We%20show%20that%20our%20model%0Aoutperforms%20state-ofthe-art%20baselines%20on%20rendering%20monaural%20and%20binaural%20RIRs%0Aand%20music%20at%20unseen%20locations%2C%20and%20learns%20physically%20interpretable%20parameters%0Acharacterizing%20acoustic%20properties%20of%20the%20sound%20source%20and%20surfaces%20in%20the%0Ascene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07532v1&entry.124074799=Read"},
{"title": "NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse\n  Polarized Images", "author": "Yufei Han and Heng Guo and Koki Fukai and Hiroaki Santo and Boxin Shi and Fumio Okura and Zhanyu Ma and Yunpeng Jia", "abstract": "  We present NeRSP, a Neural 3D reconstruction technique for Reflective\nsurfaces with Sparse Polarized images. Reflective surface reconstruction is\nextremely challenging as specular reflections are view-dependent and thus\nviolate the multiview consistency for multiview stereo. On the other hand,\nsparse image inputs, as a practical capture setting, commonly cause incomplete\nor distorted results due to the lack of correspondence matching. This paper\njointly handles the challenges from sparse inputs and reflective surfaces by\nleveraging polarized images. We derive photometric and geometric cues from the\npolarimetric image formation model and multiview azimuth consistency, which\njointly optimize the surface geometry modeled via implicit neural\nrepresentation. Based on the experiments on our synthetic and real datasets, we\nachieve the state-of-the-art surface reconstruction results with only 6 views\nas input.\n", "link": "http://arxiv.org/abs/2406.07111v1", "date": "2024-06-11", "relevancy": 2.7667, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6067}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5267}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRSP%3A%20Neural%203D%20Reconstruction%20for%20Reflective%20Objects%20with%20Sparse%0A%20%20Polarized%20Images&body=Title%3A%20NeRSP%3A%20Neural%203D%20Reconstruction%20for%20Reflective%20Objects%20with%20Sparse%0A%20%20Polarized%20Images%0AAuthor%3A%20Yufei%20Han%20and%20Heng%20Guo%20and%20Koki%20Fukai%20and%20Hiroaki%20Santo%20and%20Boxin%20Shi%20and%20Fumio%20Okura%20and%20Zhanyu%20Ma%20and%20Yunpeng%20Jia%0AAbstract%3A%20%20%20We%20present%20NeRSP%2C%20a%20Neural%203D%20reconstruction%20technique%20for%20Reflective%0Asurfaces%20with%20Sparse%20Polarized%20images.%20Reflective%20surface%20reconstruction%20is%0Aextremely%20challenging%20as%20specular%20reflections%20are%20view-dependent%20and%20thus%0Aviolate%20the%20multiview%20consistency%20for%20multiview%20stereo.%20On%20the%20other%20hand%2C%0Asparse%20image%20inputs%2C%20as%20a%20practical%20capture%20setting%2C%20commonly%20cause%20incomplete%0Aor%20distorted%20results%20due%20to%20the%20lack%20of%20correspondence%20matching.%20This%20paper%0Ajointly%20handles%20the%20challenges%20from%20sparse%20inputs%20and%20reflective%20surfaces%20by%0Aleveraging%20polarized%20images.%20We%20derive%20photometric%20and%20geometric%20cues%20from%20the%0Apolarimetric%20image%20formation%20model%20and%20multiview%20azimuth%20consistency%2C%20which%0Ajointly%20optimize%20the%20surface%20geometry%20modeled%20via%20implicit%20neural%0Arepresentation.%20Based%20on%20the%20experiments%20on%20our%20synthetic%20and%20real%20datasets%2C%20we%0Aachieve%20the%20state-of-the-art%20surface%20reconstruction%20results%20with%20only%206%20views%0Aas%20input.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRSP%253A%2520Neural%25203D%2520Reconstruction%2520for%2520Reflective%2520Objects%2520with%2520Sparse%250A%2520%2520Polarized%2520Images%26entry.906535625%3DYufei%2520Han%2520and%2520Heng%2520Guo%2520and%2520Koki%2520Fukai%2520and%2520Hiroaki%2520Santo%2520and%2520Boxin%2520Shi%2520and%2520Fumio%2520Okura%2520and%2520Zhanyu%2520Ma%2520and%2520Yunpeng%2520Jia%26entry.1292438233%3D%2520%2520We%2520present%2520NeRSP%252C%2520a%2520Neural%25203D%2520reconstruction%2520technique%2520for%2520Reflective%250Asurfaces%2520with%2520Sparse%2520Polarized%2520images.%2520Reflective%2520surface%2520reconstruction%2520is%250Aextremely%2520challenging%2520as%2520specular%2520reflections%2520are%2520view-dependent%2520and%2520thus%250Aviolate%2520the%2520multiview%2520consistency%2520for%2520multiview%2520stereo.%2520On%2520the%2520other%2520hand%252C%250Asparse%2520image%2520inputs%252C%2520as%2520a%2520practical%2520capture%2520setting%252C%2520commonly%2520cause%2520incomplete%250Aor%2520distorted%2520results%2520due%2520to%2520the%2520lack%2520of%2520correspondence%2520matching.%2520This%2520paper%250Ajointly%2520handles%2520the%2520challenges%2520from%2520sparse%2520inputs%2520and%2520reflective%2520surfaces%2520by%250Aleveraging%2520polarized%2520images.%2520We%2520derive%2520photometric%2520and%2520geometric%2520cues%2520from%2520the%250Apolarimetric%2520image%2520formation%2520model%2520and%2520multiview%2520azimuth%2520consistency%252C%2520which%250Ajointly%2520optimize%2520the%2520surface%2520geometry%2520modeled%2520via%2520implicit%2520neural%250Arepresentation.%2520Based%2520on%2520the%2520experiments%2520on%2520our%2520synthetic%2520and%2520real%2520datasets%252C%2520we%250Aachieve%2520the%2520state-of-the-art%2520surface%2520reconstruction%2520results%2520with%2520only%25206%2520views%250Aas%2520input.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRSP%3A%20Neural%203D%20Reconstruction%20for%20Reflective%20Objects%20with%20Sparse%0A%20%20Polarized%20Images&entry.906535625=Yufei%20Han%20and%20Heng%20Guo%20and%20Koki%20Fukai%20and%20Hiroaki%20Santo%20and%20Boxin%20Shi%20and%20Fumio%20Okura%20and%20Zhanyu%20Ma%20and%20Yunpeng%20Jia&entry.1292438233=%20%20We%20present%20NeRSP%2C%20a%20Neural%203D%20reconstruction%20technique%20for%20Reflective%0Asurfaces%20with%20Sparse%20Polarized%20images.%20Reflective%20surface%20reconstruction%20is%0Aextremely%20challenging%20as%20specular%20reflections%20are%20view-dependent%20and%20thus%0Aviolate%20the%20multiview%20consistency%20for%20multiview%20stereo.%20On%20the%20other%20hand%2C%0Asparse%20image%20inputs%2C%20as%20a%20practical%20capture%20setting%2C%20commonly%20cause%20incomplete%0Aor%20distorted%20results%20due%20to%20the%20lack%20of%20correspondence%20matching.%20This%20paper%0Ajointly%20handles%20the%20challenges%20from%20sparse%20inputs%20and%20reflective%20surfaces%20by%0Aleveraging%20polarized%20images.%20We%20derive%20photometric%20and%20geometric%20cues%20from%20the%0Apolarimetric%20image%20formation%20model%20and%20multiview%20azimuth%20consistency%2C%20which%0Ajointly%20optimize%20the%20surface%20geometry%20modeled%20via%20implicit%20neural%0Arepresentation.%20Based%20on%20the%20experiments%20on%20our%20synthetic%20and%20real%20datasets%2C%20we%0Aachieve%20the%20state-of-the-art%20surface%20reconstruction%20results%20with%20only%206%20views%0Aas%20input.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07111v1&entry.124074799=Read"},
{"title": "RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer\n  Tracker", "author": "Yunfeng Li and Bo Wang and Jiuran Sun and Xueyi Wu and Ye Li", "abstract": "  Vision camera and sonar are naturally complementary in the underwater\nenvironment. Combining the information from two modalities will promote better\nobservation of underwater targets. However, this problem has not received\nsufficient attention in previous research. Therefore, this paper introduces a\nnew challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve\nefficient tracking of an underwater target through the interaction of RGB and\nsonar modalities. Specifically, we first propose an RGBS50 benchmark dataset\ncontaining 50 sequences and more than 87000 high-quality annotated bounding\nboxes. Experimental results show that the RGBS50 benchmark poses a challenge to\ncurrently popular SOT trackers. Second, we propose an RGB-S tracker called\nSCANet, which includes a spatial cross-attention module (SCAM) consisting of a\nnovel spatial cross-attention layer and two independent global integration\nmodules. The spatial cross-attention is used to overcome the problem of spatial\nmisalignment of between RGB and sonar images. Third, we propose a SOT\ndata-based RGB-S simulation training method (SRST) to overcome the lack of\nRGB-S training datasets. It converts RGB images into sonar-like saliency images\nto construct pseudo-data pairs, enabling the model to learn the semantic\nstructure of RGB-S-like data. Comprehensive experiments show that the proposed\nspatial cross-attention effectively achieves the interaction between RGB and\nsonar modalities and SCANet achieves state-of-the-art performance on the\nproposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/RGBS50.\n", "link": "http://arxiv.org/abs/2406.07189v1", "date": "2024-06-11", "relevancy": 2.758, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&body=Title%3A%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker%0AAuthor%3A%20Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li%0AAbstract%3A%20%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Sonar%2520Tracking%2520Benchmark%2520and%2520Spatial%2520Cross-Attention%2520Transformer%250A%2520%2520Tracker%26entry.906535625%3DYunfeng%2520Li%2520and%2520Bo%2520Wang%2520and%2520Jiuran%2520Sun%2520and%2520Xueyi%2520Wu%2520and%2520Ye%2520Li%26entry.1292438233%3D%2520%2520Vision%2520camera%2520and%2520sonar%2520are%2520naturally%2520complementary%2520in%2520the%2520underwater%250Aenvironment.%2520Combining%2520the%2520information%2520from%2520two%2520modalities%2520will%2520promote%2520better%250Aobservation%2520of%2520underwater%2520targets.%2520However%252C%2520this%2520problem%2520has%2520not%2520received%250Asufficient%2520attention%2520in%2520previous%2520research.%2520Therefore%252C%2520this%2520paper%2520introduces%2520a%250Anew%2520challenging%2520RGB-Sonar%2520%2528RGB-S%2529%2520tracking%2520task%2520and%2520investigates%2520how%2520to%2520achieve%250Aefficient%2520tracking%2520of%2520an%2520underwater%2520target%2520through%2520the%2520interaction%2520of%2520RGB%2520and%250Asonar%2520modalities.%2520Specifically%252C%2520we%2520first%2520propose%2520an%2520RGBS50%2520benchmark%2520dataset%250Acontaining%252050%2520sequences%2520and%2520more%2520than%252087000%2520high-quality%2520annotated%2520bounding%250Aboxes.%2520Experimental%2520results%2520show%2520that%2520the%2520RGBS50%2520benchmark%2520poses%2520a%2520challenge%2520to%250Acurrently%2520popular%2520SOT%2520trackers.%2520Second%252C%2520we%2520propose%2520an%2520RGB-S%2520tracker%2520called%250ASCANet%252C%2520which%2520includes%2520a%2520spatial%2520cross-attention%2520module%2520%2528SCAM%2529%2520consisting%2520of%2520a%250Anovel%2520spatial%2520cross-attention%2520layer%2520and%2520two%2520independent%2520global%2520integration%250Amodules.%2520The%2520spatial%2520cross-attention%2520is%2520used%2520to%2520overcome%2520the%2520problem%2520of%2520spatial%250Amisalignment%2520of%2520between%2520RGB%2520and%2520sonar%2520images.%2520Third%252C%2520we%2520propose%2520a%2520SOT%250Adata-based%2520RGB-S%2520simulation%2520training%2520method%2520%2528SRST%2529%2520to%2520overcome%2520the%2520lack%2520of%250ARGB-S%2520training%2520datasets.%2520It%2520converts%2520RGB%2520images%2520into%2520sonar-like%2520saliency%2520images%250Ato%2520construct%2520pseudo-data%2520pairs%252C%2520enabling%2520the%2520model%2520to%2520learn%2520the%2520semantic%250Astructure%2520of%2520RGB-S-like%2520data.%2520Comprehensive%2520experiments%2520show%2520that%2520the%2520proposed%250Aspatial%2520cross-attention%2520effectively%2520achieves%2520the%2520interaction%2520between%2520RGB%2520and%250Asonar%2520modalities%2520and%2520SCANet%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250Aproposed%2520benchmark.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiYunfengLYF/RGBS50.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&entry.906535625=Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li&entry.1292438233=%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07189v1&entry.124074799=Read"},
{"title": "An Optimism-based Approach to Online Evaluation of Generative Models", "author": "Xiaoyan Hu and Ho-fung Leung and Farzan Farnia", "abstract": "  Existing frameworks for evaluating and comparing generative models typically\ntarget an offline setting, where the evaluator has access to full batches of\ndata produced by the models. However, in many practical scenarios, the goal is\nto identify the best model using the fewest generated samples to minimize the\ncosts of querying data from the models. Such an online comparison is\nchallenging with current offline assessment methods. In this work, we propose\nan online evaluation framework to find the generative model that maximizes a\nstandard assessment score among a group of available models. Our method uses an\noptimism-based multi-armed bandit framework to identify the model producing\ndata with the highest evaluation score, quantifying the quality and diversity\nof generated data. Specifically, we study the online assessment of generative\nmodels based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS)\nmetrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper\nconfidence bound approach in online learning. We prove sub-linear regret bounds\nfor these algorithms and present numerical results on standard image datasets,\ndemonstrating their effectiveness in identifying the score-maximizing\ngenerative model.\n", "link": "http://arxiv.org/abs/2406.07451v1", "date": "2024-06-11", "relevancy": 2.7365, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5613}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5482}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Optimism-based%20Approach%20to%20Online%20Evaluation%20of%20Generative%20Models&body=Title%3A%20An%20Optimism-based%20Approach%20to%20Online%20Evaluation%20of%20Generative%20Models%0AAuthor%3A%20Xiaoyan%20Hu%20and%20Ho-fung%20Leung%20and%20Farzan%20Farnia%0AAbstract%3A%20%20%20Existing%20frameworks%20for%20evaluating%20and%20comparing%20generative%20models%20typically%0Atarget%20an%20offline%20setting%2C%20where%20the%20evaluator%20has%20access%20to%20full%20batches%20of%0Adata%20produced%20by%20the%20models.%20However%2C%20in%20many%20practical%20scenarios%2C%20the%20goal%20is%0Ato%20identify%20the%20best%20model%20using%20the%20fewest%20generated%20samples%20to%20minimize%20the%0Acosts%20of%20querying%20data%20from%20the%20models.%20Such%20an%20online%20comparison%20is%0Achallenging%20with%20current%20offline%20assessment%20methods.%20In%20this%20work%2C%20we%20propose%0Aan%20online%20evaluation%20framework%20to%20find%20the%20generative%20model%20that%20maximizes%20a%0Astandard%20assessment%20score%20among%20a%20group%20of%20available%20models.%20Our%20method%20uses%20an%0Aoptimism-based%20multi-armed%20bandit%20framework%20to%20identify%20the%20model%20producing%0Adata%20with%20the%20highest%20evaluation%20score%2C%20quantifying%20the%20quality%20and%20diversity%0Aof%20generated%20data.%20Specifically%2C%20we%20study%20the%20online%20assessment%20of%20generative%0Amodels%20based%20on%20the%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20and%20Inception%20Score%20%28IS%29%0Ametrics%20and%20propose%20the%20FID-UCB%20and%20IS-UCB%20algorithms%20leveraging%20the%20upper%0Aconfidence%20bound%20approach%20in%20online%20learning.%20We%20prove%20sub-linear%20regret%20bounds%0Afor%20these%20algorithms%20and%20present%20numerical%20results%20on%20standard%20image%20datasets%2C%0Ademonstrating%20their%20effectiveness%20in%20identifying%20the%20score-maximizing%0Agenerative%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Optimism-based%2520Approach%2520to%2520Online%2520Evaluation%2520of%2520Generative%2520Models%26entry.906535625%3DXiaoyan%2520Hu%2520and%2520Ho-fung%2520Leung%2520and%2520Farzan%2520Farnia%26entry.1292438233%3D%2520%2520Existing%2520frameworks%2520for%2520evaluating%2520and%2520comparing%2520generative%2520models%2520typically%250Atarget%2520an%2520offline%2520setting%252C%2520where%2520the%2520evaluator%2520has%2520access%2520to%2520full%2520batches%2520of%250Adata%2520produced%2520by%2520the%2520models.%2520However%252C%2520in%2520many%2520practical%2520scenarios%252C%2520the%2520goal%2520is%250Ato%2520identify%2520the%2520best%2520model%2520using%2520the%2520fewest%2520generated%2520samples%2520to%2520minimize%2520the%250Acosts%2520of%2520querying%2520data%2520from%2520the%2520models.%2520Such%2520an%2520online%2520comparison%2520is%250Achallenging%2520with%2520current%2520offline%2520assessment%2520methods.%2520In%2520this%2520work%252C%2520we%2520propose%250Aan%2520online%2520evaluation%2520framework%2520to%2520find%2520the%2520generative%2520model%2520that%2520maximizes%2520a%250Astandard%2520assessment%2520score%2520among%2520a%2520group%2520of%2520available%2520models.%2520Our%2520method%2520uses%2520an%250Aoptimism-based%2520multi-armed%2520bandit%2520framework%2520to%2520identify%2520the%2520model%2520producing%250Adata%2520with%2520the%2520highest%2520evaluation%2520score%252C%2520quantifying%2520the%2520quality%2520and%2520diversity%250Aof%2520generated%2520data.%2520Specifically%252C%2520we%2520study%2520the%2520online%2520assessment%2520of%2520generative%250Amodels%2520based%2520on%2520the%2520Fr%255C%2527echet%2520Inception%2520Distance%2520%2528FID%2529%2520and%2520Inception%2520Score%2520%2528IS%2529%250Ametrics%2520and%2520propose%2520the%2520FID-UCB%2520and%2520IS-UCB%2520algorithms%2520leveraging%2520the%2520upper%250Aconfidence%2520bound%2520approach%2520in%2520online%2520learning.%2520We%2520prove%2520sub-linear%2520regret%2520bounds%250Afor%2520these%2520algorithms%2520and%2520present%2520numerical%2520results%2520on%2520standard%2520image%2520datasets%252C%250Ademonstrating%2520their%2520effectiveness%2520in%2520identifying%2520the%2520score-maximizing%250Agenerative%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Optimism-based%20Approach%20to%20Online%20Evaluation%20of%20Generative%20Models&entry.906535625=Xiaoyan%20Hu%20and%20Ho-fung%20Leung%20and%20Farzan%20Farnia&entry.1292438233=%20%20Existing%20frameworks%20for%20evaluating%20and%20comparing%20generative%20models%20typically%0Atarget%20an%20offline%20setting%2C%20where%20the%20evaluator%20has%20access%20to%20full%20batches%20of%0Adata%20produced%20by%20the%20models.%20However%2C%20in%20many%20practical%20scenarios%2C%20the%20goal%20is%0Ato%20identify%20the%20best%20model%20using%20the%20fewest%20generated%20samples%20to%20minimize%20the%0Acosts%20of%20querying%20data%20from%20the%20models.%20Such%20an%20online%20comparison%20is%0Achallenging%20with%20current%20offline%20assessment%20methods.%20In%20this%20work%2C%20we%20propose%0Aan%20online%20evaluation%20framework%20to%20find%20the%20generative%20model%20that%20maximizes%20a%0Astandard%20assessment%20score%20among%20a%20group%20of%20available%20models.%20Our%20method%20uses%20an%0Aoptimism-based%20multi-armed%20bandit%20framework%20to%20identify%20the%20model%20producing%0Adata%20with%20the%20highest%20evaluation%20score%2C%20quantifying%20the%20quality%20and%20diversity%0Aof%20generated%20data.%20Specifically%2C%20we%20study%20the%20online%20assessment%20of%20generative%0Amodels%20based%20on%20the%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20and%20Inception%20Score%20%28IS%29%0Ametrics%20and%20propose%20the%20FID-UCB%20and%20IS-UCB%20algorithms%20leveraging%20the%20upper%0Aconfidence%20bound%20approach%20in%20online%20learning.%20We%20prove%20sub-linear%20regret%20bounds%0Afor%20these%20algorithms%20and%20present%20numerical%20results%20on%20standard%20image%20datasets%2C%0Ademonstrating%20their%20effectiveness%20in%20identifying%20the%20score-maximizing%0Agenerative%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07451v1&entry.124074799=Read"},
{"title": "Polarization Wavefront Lidar: Learning Large Scene Reconstruction from\n  Polarized Wavefronts", "author": "Dominik Scheuble and Chenyang Lei and Seung-Hwan Baek and Mario Bijelic and Felix Heide", "abstract": "  Lidar has become a cornerstone sensing modality for 3D vision, especially for\nlarge outdoor scenarios and autonomous driving. Conventional lidar sensors are\ncapable of providing centimeter-accurate distance information by emitting laser\npulses into a scene and measuring the time-of-flight (ToF) of the reflection.\nHowever, the polarization of the received light that depends on the surface\norientation and material properties is usually not considered. As such, the\npolarization modality has the potential to improve scene reconstruction beyond\ndistance measurements. In this work, we introduce a novel long-range\npolarization wavefront lidar sensor (PolLidar) that modulates the polarization\nof the emitted and received light. Departing from conventional lidar sensors,\nPolLidar allows access to the raw time-resolved polarimetric wavefronts. We\nleverage polarimetric wavefronts to estimate normals, distance, and material\nproperties in outdoor scenarios with a novel learned reconstruction method. To\ntrain and evaluate the method, we introduce a simulated and real-world\nlong-range dataset with paired raw lidar data, ground truth distance, and\nnormal maps. We find that the proposed method improves normal and distance\nreconstruction by 53\\% mean angular error and 41\\% mean absolute error compared\nto existing shape-from-polarization (SfP) and ToF methods. Code and data are\nopen-sourced at https://light.princeton.edu/pollidar.\n", "link": "http://arxiv.org/abs/2406.03461v2", "date": "2024-06-11", "relevancy": 2.6457, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5451}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5418}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polarization%20Wavefront%20Lidar%3A%20Learning%20Large%20Scene%20Reconstruction%20from%0A%20%20Polarized%20Wavefronts&body=Title%3A%20Polarization%20Wavefront%20Lidar%3A%20Learning%20Large%20Scene%20Reconstruction%20from%0A%20%20Polarized%20Wavefronts%0AAuthor%3A%20Dominik%20Scheuble%20and%20Chenyang%20Lei%20and%20Seung-Hwan%20Baek%20and%20Mario%20Bijelic%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Lidar%20has%20become%20a%20cornerstone%20sensing%20modality%20for%203D%20vision%2C%20especially%20for%0Alarge%20outdoor%20scenarios%20and%20autonomous%20driving.%20Conventional%20lidar%20sensors%20are%0Acapable%20of%20providing%20centimeter-accurate%20distance%20information%20by%20emitting%20laser%0Apulses%20into%20a%20scene%20and%20measuring%20the%20time-of-flight%20%28ToF%29%20of%20the%20reflection.%0AHowever%2C%20the%20polarization%20of%20the%20received%20light%20that%20depends%20on%20the%20surface%0Aorientation%20and%20material%20properties%20is%20usually%20not%20considered.%20As%20such%2C%20the%0Apolarization%20modality%20has%20the%20potential%20to%20improve%20scene%20reconstruction%20beyond%0Adistance%20measurements.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20long-range%0Apolarization%20wavefront%20lidar%20sensor%20%28PolLidar%29%20that%20modulates%20the%20polarization%0Aof%20the%20emitted%20and%20received%20light.%20Departing%20from%20conventional%20lidar%20sensors%2C%0APolLidar%20allows%20access%20to%20the%20raw%20time-resolved%20polarimetric%20wavefronts.%20We%0Aleverage%20polarimetric%20wavefronts%20to%20estimate%20normals%2C%20distance%2C%20and%20material%0Aproperties%20in%20outdoor%20scenarios%20with%20a%20novel%20learned%20reconstruction%20method.%20To%0Atrain%20and%20evaluate%20the%20method%2C%20we%20introduce%20a%20simulated%20and%20real-world%0Along-range%20dataset%20with%20paired%20raw%20lidar%20data%2C%20ground%20truth%20distance%2C%20and%0Anormal%20maps.%20We%20find%20that%20the%20proposed%20method%20improves%20normal%20and%20distance%0Areconstruction%20by%2053%5C%25%20mean%20angular%20error%20and%2041%5C%25%20mean%20absolute%20error%20compared%0Ato%20existing%20shape-from-polarization%20%28SfP%29%20and%20ToF%20methods.%20Code%20and%20data%20are%0Aopen-sourced%20at%20https%3A//light.princeton.edu/pollidar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolarization%2520Wavefront%2520Lidar%253A%2520Learning%2520Large%2520Scene%2520Reconstruction%2520from%250A%2520%2520Polarized%2520Wavefronts%26entry.906535625%3DDominik%2520Scheuble%2520and%2520Chenyang%2520Lei%2520and%2520Seung-Hwan%2520Baek%2520and%2520Mario%2520Bijelic%2520and%2520Felix%2520Heide%26entry.1292438233%3D%2520%2520Lidar%2520has%2520become%2520a%2520cornerstone%2520sensing%2520modality%2520for%25203D%2520vision%252C%2520especially%2520for%250Alarge%2520outdoor%2520scenarios%2520and%2520autonomous%2520driving.%2520Conventional%2520lidar%2520sensors%2520are%250Acapable%2520of%2520providing%2520centimeter-accurate%2520distance%2520information%2520by%2520emitting%2520laser%250Apulses%2520into%2520a%2520scene%2520and%2520measuring%2520the%2520time-of-flight%2520%2528ToF%2529%2520of%2520the%2520reflection.%250AHowever%252C%2520the%2520polarization%2520of%2520the%2520received%2520light%2520that%2520depends%2520on%2520the%2520surface%250Aorientation%2520and%2520material%2520properties%2520is%2520usually%2520not%2520considered.%2520As%2520such%252C%2520the%250Apolarization%2520modality%2520has%2520the%2520potential%2520to%2520improve%2520scene%2520reconstruction%2520beyond%250Adistance%2520measurements.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520long-range%250Apolarization%2520wavefront%2520lidar%2520sensor%2520%2528PolLidar%2529%2520that%2520modulates%2520the%2520polarization%250Aof%2520the%2520emitted%2520and%2520received%2520light.%2520Departing%2520from%2520conventional%2520lidar%2520sensors%252C%250APolLidar%2520allows%2520access%2520to%2520the%2520raw%2520time-resolved%2520polarimetric%2520wavefronts.%2520We%250Aleverage%2520polarimetric%2520wavefronts%2520to%2520estimate%2520normals%252C%2520distance%252C%2520and%2520material%250Aproperties%2520in%2520outdoor%2520scenarios%2520with%2520a%2520novel%2520learned%2520reconstruction%2520method.%2520To%250Atrain%2520and%2520evaluate%2520the%2520method%252C%2520we%2520introduce%2520a%2520simulated%2520and%2520real-world%250Along-range%2520dataset%2520with%2520paired%2520raw%2520lidar%2520data%252C%2520ground%2520truth%2520distance%252C%2520and%250Anormal%2520maps.%2520We%2520find%2520that%2520the%2520proposed%2520method%2520improves%2520normal%2520and%2520distance%250Areconstruction%2520by%252053%255C%2525%2520mean%2520angular%2520error%2520and%252041%255C%2525%2520mean%2520absolute%2520error%2520compared%250Ato%2520existing%2520shape-from-polarization%2520%2528SfP%2529%2520and%2520ToF%2520methods.%2520Code%2520and%2520data%2520are%250Aopen-sourced%2520at%2520https%253A//light.princeton.edu/pollidar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polarization%20Wavefront%20Lidar%3A%20Learning%20Large%20Scene%20Reconstruction%20from%0A%20%20Polarized%20Wavefronts&entry.906535625=Dominik%20Scheuble%20and%20Chenyang%20Lei%20and%20Seung-Hwan%20Baek%20and%20Mario%20Bijelic%20and%20Felix%20Heide&entry.1292438233=%20%20Lidar%20has%20become%20a%20cornerstone%20sensing%20modality%20for%203D%20vision%2C%20especially%20for%0Alarge%20outdoor%20scenarios%20and%20autonomous%20driving.%20Conventional%20lidar%20sensors%20are%0Acapable%20of%20providing%20centimeter-accurate%20distance%20information%20by%20emitting%20laser%0Apulses%20into%20a%20scene%20and%20measuring%20the%20time-of-flight%20%28ToF%29%20of%20the%20reflection.%0AHowever%2C%20the%20polarization%20of%20the%20received%20light%20that%20depends%20on%20the%20surface%0Aorientation%20and%20material%20properties%20is%20usually%20not%20considered.%20As%20such%2C%20the%0Apolarization%20modality%20has%20the%20potential%20to%20improve%20scene%20reconstruction%20beyond%0Adistance%20measurements.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20long-range%0Apolarization%20wavefront%20lidar%20sensor%20%28PolLidar%29%20that%20modulates%20the%20polarization%0Aof%20the%20emitted%20and%20received%20light.%20Departing%20from%20conventional%20lidar%20sensors%2C%0APolLidar%20allows%20access%20to%20the%20raw%20time-resolved%20polarimetric%20wavefronts.%20We%0Aleverage%20polarimetric%20wavefronts%20to%20estimate%20normals%2C%20distance%2C%20and%20material%0Aproperties%20in%20outdoor%20scenarios%20with%20a%20novel%20learned%20reconstruction%20method.%20To%0Atrain%20and%20evaluate%20the%20method%2C%20we%20introduce%20a%20simulated%20and%20real-world%0Along-range%20dataset%20with%20paired%20raw%20lidar%20data%2C%20ground%20truth%20distance%2C%20and%0Anormal%20maps.%20We%20find%20that%20the%20proposed%20method%20improves%20normal%20and%20distance%0Areconstruction%20by%2053%5C%25%20mean%20angular%20error%20and%2041%5C%25%20mean%20absolute%20error%20compared%0Ato%20existing%20shape-from-polarization%20%28SfP%29%20and%20ToF%20methods.%20Code%20and%20data%20are%0Aopen-sourced%20at%20https%3A//light.princeton.edu/pollidar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03461v2&entry.124074799=Read"},
{"title": "Global-Regularized Neighborhood Regression for Efficient Zero-Shot\n  Texture Anomaly Detection", "author": "Haiming Yao and Wei Luo and Yunkang Cao and Yiheng Zhang and Wenyong Yu and Weiming Shen", "abstract": "  Texture surface anomaly detection finds widespread applications in industrial\nsettings. However, existing methods often necessitate gathering numerous\nsamples for model training. Moreover, they predominantly operate within a\nclose-set detection framework, limiting their ability to identify anomalies\nbeyond the training dataset. To tackle these challenges, this paper introduces\na novel zero-shot texture anomaly detection method named Global-Regularized\nNeighborhood Regression (GRNR). Unlike conventional approaches, GRNR can detect\nanomalies on arbitrary textured surfaces without any training data or cost.\nDrawing from human visual cognition, GRNR derives two intrinsic prior supports\ndirectly from the test texture image: local neighborhood priors characterized\nby coherent similarities and global normality priors featuring typical normal\npatterns. The fundamental principle of GRNR involves utilizing the two\nextracted intrinsic support priors for self-reconstructive regression of the\nquery sample. This process employs the transformation facilitated by local\nneighbor support while being regularized by global normality support, aiming to\nnot only achieve visually consistent reconstruction results but also preserve\nnormality properties. We validate the effectiveness of GRNR across various\nindustrial scenarios using eight benchmark datasets, demonstrating its superior\ndetection performance without the need for training data. Remarkably, our\nmethod is applicable for open-set texture defect detection and can even surpass\nexisting vanilla approaches that require extensive training.\n", "link": "http://arxiv.org/abs/2406.07333v1", "date": "2024-06-11", "relevancy": 2.6386, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5579}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5191}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global-Regularized%20Neighborhood%20Regression%20for%20Efficient%20Zero-Shot%0A%20%20Texture%20Anomaly%20Detection&body=Title%3A%20Global-Regularized%20Neighborhood%20Regression%20for%20Efficient%20Zero-Shot%0A%20%20Texture%20Anomaly%20Detection%0AAuthor%3A%20Haiming%20Yao%20and%20Wei%20Luo%20and%20Yunkang%20Cao%20and%20Yiheng%20Zhang%20and%20Wenyong%20Yu%20and%20Weiming%20Shen%0AAbstract%3A%20%20%20Texture%20surface%20anomaly%20detection%20finds%20widespread%20applications%20in%20industrial%0Asettings.%20However%2C%20existing%20methods%20often%20necessitate%20gathering%20numerous%0Asamples%20for%20model%20training.%20Moreover%2C%20they%20predominantly%20operate%20within%20a%0Aclose-set%20detection%20framework%2C%20limiting%20their%20ability%20to%20identify%20anomalies%0Abeyond%20the%20training%20dataset.%20To%20tackle%20these%20challenges%2C%20this%20paper%20introduces%0Aa%20novel%20zero-shot%20texture%20anomaly%20detection%20method%20named%20Global-Regularized%0ANeighborhood%20Regression%20%28GRNR%29.%20Unlike%20conventional%20approaches%2C%20GRNR%20can%20detect%0Aanomalies%20on%20arbitrary%20textured%20surfaces%20without%20any%20training%20data%20or%20cost.%0ADrawing%20from%20human%20visual%20cognition%2C%20GRNR%20derives%20two%20intrinsic%20prior%20supports%0Adirectly%20from%20the%20test%20texture%20image%3A%20local%20neighborhood%20priors%20characterized%0Aby%20coherent%20similarities%20and%20global%20normality%20priors%20featuring%20typical%20normal%0Apatterns.%20The%20fundamental%20principle%20of%20GRNR%20involves%20utilizing%20the%20two%0Aextracted%20intrinsic%20support%20priors%20for%20self-reconstructive%20regression%20of%20the%0Aquery%20sample.%20This%20process%20employs%20the%20transformation%20facilitated%20by%20local%0Aneighbor%20support%20while%20being%20regularized%20by%20global%20normality%20support%2C%20aiming%20to%0Anot%20only%20achieve%20visually%20consistent%20reconstruction%20results%20but%20also%20preserve%0Anormality%20properties.%20We%20validate%20the%20effectiveness%20of%20GRNR%20across%20various%0Aindustrial%20scenarios%20using%20eight%20benchmark%20datasets%2C%20demonstrating%20its%20superior%0Adetection%20performance%20without%20the%20need%20for%20training%20data.%20Remarkably%2C%20our%0Amethod%20is%20applicable%20for%20open-set%20texture%20defect%20detection%20and%20can%20even%20surpass%0Aexisting%20vanilla%20approaches%20that%20require%20extensive%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal-Regularized%2520Neighborhood%2520Regression%2520for%2520Efficient%2520Zero-Shot%250A%2520%2520Texture%2520Anomaly%2520Detection%26entry.906535625%3DHaiming%2520Yao%2520and%2520Wei%2520Luo%2520and%2520Yunkang%2520Cao%2520and%2520Yiheng%2520Zhang%2520and%2520Wenyong%2520Yu%2520and%2520Weiming%2520Shen%26entry.1292438233%3D%2520%2520Texture%2520surface%2520anomaly%2520detection%2520finds%2520widespread%2520applications%2520in%2520industrial%250Asettings.%2520However%252C%2520existing%2520methods%2520often%2520necessitate%2520gathering%2520numerous%250Asamples%2520for%2520model%2520training.%2520Moreover%252C%2520they%2520predominantly%2520operate%2520within%2520a%250Aclose-set%2520detection%2520framework%252C%2520limiting%2520their%2520ability%2520to%2520identify%2520anomalies%250Abeyond%2520the%2520training%2520dataset.%2520To%2520tackle%2520these%2520challenges%252C%2520this%2520paper%2520introduces%250Aa%2520novel%2520zero-shot%2520texture%2520anomaly%2520detection%2520method%2520named%2520Global-Regularized%250ANeighborhood%2520Regression%2520%2528GRNR%2529.%2520Unlike%2520conventional%2520approaches%252C%2520GRNR%2520can%2520detect%250Aanomalies%2520on%2520arbitrary%2520textured%2520surfaces%2520without%2520any%2520training%2520data%2520or%2520cost.%250ADrawing%2520from%2520human%2520visual%2520cognition%252C%2520GRNR%2520derives%2520two%2520intrinsic%2520prior%2520supports%250Adirectly%2520from%2520the%2520test%2520texture%2520image%253A%2520local%2520neighborhood%2520priors%2520characterized%250Aby%2520coherent%2520similarities%2520and%2520global%2520normality%2520priors%2520featuring%2520typical%2520normal%250Apatterns.%2520The%2520fundamental%2520principle%2520of%2520GRNR%2520involves%2520utilizing%2520the%2520two%250Aextracted%2520intrinsic%2520support%2520priors%2520for%2520self-reconstructive%2520regression%2520of%2520the%250Aquery%2520sample.%2520This%2520process%2520employs%2520the%2520transformation%2520facilitated%2520by%2520local%250Aneighbor%2520support%2520while%2520being%2520regularized%2520by%2520global%2520normality%2520support%252C%2520aiming%2520to%250Anot%2520only%2520achieve%2520visually%2520consistent%2520reconstruction%2520results%2520but%2520also%2520preserve%250Anormality%2520properties.%2520We%2520validate%2520the%2520effectiveness%2520of%2520GRNR%2520across%2520various%250Aindustrial%2520scenarios%2520using%2520eight%2520benchmark%2520datasets%252C%2520demonstrating%2520its%2520superior%250Adetection%2520performance%2520without%2520the%2520need%2520for%2520training%2520data.%2520Remarkably%252C%2520our%250Amethod%2520is%2520applicable%2520for%2520open-set%2520texture%2520defect%2520detection%2520and%2520can%2520even%2520surpass%250Aexisting%2520vanilla%2520approaches%2520that%2520require%2520extensive%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global-Regularized%20Neighborhood%20Regression%20for%20Efficient%20Zero-Shot%0A%20%20Texture%20Anomaly%20Detection&entry.906535625=Haiming%20Yao%20and%20Wei%20Luo%20and%20Yunkang%20Cao%20and%20Yiheng%20Zhang%20and%20Wenyong%20Yu%20and%20Weiming%20Shen&entry.1292438233=%20%20Texture%20surface%20anomaly%20detection%20finds%20widespread%20applications%20in%20industrial%0Asettings.%20However%2C%20existing%20methods%20often%20necessitate%20gathering%20numerous%0Asamples%20for%20model%20training.%20Moreover%2C%20they%20predominantly%20operate%20within%20a%0Aclose-set%20detection%20framework%2C%20limiting%20their%20ability%20to%20identify%20anomalies%0Abeyond%20the%20training%20dataset.%20To%20tackle%20these%20challenges%2C%20this%20paper%20introduces%0Aa%20novel%20zero-shot%20texture%20anomaly%20detection%20method%20named%20Global-Regularized%0ANeighborhood%20Regression%20%28GRNR%29.%20Unlike%20conventional%20approaches%2C%20GRNR%20can%20detect%0Aanomalies%20on%20arbitrary%20textured%20surfaces%20without%20any%20training%20data%20or%20cost.%0ADrawing%20from%20human%20visual%20cognition%2C%20GRNR%20derives%20two%20intrinsic%20prior%20supports%0Adirectly%20from%20the%20test%20texture%20image%3A%20local%20neighborhood%20priors%20characterized%0Aby%20coherent%20similarities%20and%20global%20normality%20priors%20featuring%20typical%20normal%0Apatterns.%20The%20fundamental%20principle%20of%20GRNR%20involves%20utilizing%20the%20two%0Aextracted%20intrinsic%20support%20priors%20for%20self-reconstructive%20regression%20of%20the%0Aquery%20sample.%20This%20process%20employs%20the%20transformation%20facilitated%20by%20local%0Aneighbor%20support%20while%20being%20regularized%20by%20global%20normality%20support%2C%20aiming%20to%0Anot%20only%20achieve%20visually%20consistent%20reconstruction%20results%20but%20also%20preserve%0Anormality%20properties.%20We%20validate%20the%20effectiveness%20of%20GRNR%20across%20various%0Aindustrial%20scenarios%20using%20eight%20benchmark%20datasets%2C%20demonstrating%20its%20superior%0Adetection%20performance%20without%20the%20need%20for%20training%20data.%20Remarkably%2C%20our%0Amethod%20is%20applicable%20for%20open-set%20texture%20defect%20detection%20and%20can%20even%20surpass%0Aexisting%20vanilla%20approaches%20that%20require%20extensive%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07333v1&entry.124074799=Read"},
{"title": "Holistic Memory Diversification for Incremental Learning in Growing\n  Graphs", "author": "Ziyue Qiao and Junren Xiao and Qingqiang Sun and Meng Xiao and Hui Xiong", "abstract": "  This paper addresses the challenge of incremental learning in growing graphs\nwith increasingly complex tasks. The goal is to continually train a graph model\nto handle new tasks while retaining its inference ability on previous tasks.\nExisting methods usually neglect the importance of memory diversity, limiting\nin effectively selecting high-quality memory from previous tasks and\nremembering broad previous knowledge within the scarce memory on graphs. To\naddress that, we introduce a novel holistic Diversified Memory Selection and\nGeneration (DMSG) framework for incremental learning in graphs, which first\nintroduces a buffer selection strategy that considers both intra-class and\ninter-class diversities, employing an efficient greedy algorithm for sampling\nrepresentative training nodes from graphs into memory buffers after learning\neach new task. Then, to adequately rememorize the knowledge preserved in the\nmemory buffer when learning new tasks, we propose a diversified memory\ngeneration replay method. This method first utilizes a variational layer to\ngenerate the distribution of buffer node embeddings and sample synthesized ones\nfor replaying. Furthermore, an adversarial variational embedding learning\nmethod and a reconstruction-based decoder are proposed to maintain the\nintegrity and consolidate the generalization of the synthesized node\nembeddings, respectively. Finally, we evaluate our model on node classification\ntasks involving increasing class numbers. Extensive experimental results on\npublicly accessible datasets demonstrate the superiority of DMSG over\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.07413v1", "date": "2024-06-11", "relevancy": 2.599, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5267}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.518}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holistic%20Memory%20Diversification%20for%20Incremental%20Learning%20in%20Growing%0A%20%20Graphs&body=Title%3A%20Holistic%20Memory%20Diversification%20for%20Incremental%20Learning%20in%20Growing%0A%20%20Graphs%0AAuthor%3A%20Ziyue%20Qiao%20and%20Junren%20Xiao%20and%20Qingqiang%20Sun%20and%20Meng%20Xiao%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20incremental%20learning%20in%20growing%20graphs%0Awith%20increasingly%20complex%20tasks.%20The%20goal%20is%20to%20continually%20train%20a%20graph%20model%0Ato%20handle%20new%20tasks%20while%20retaining%20its%20inference%20ability%20on%20previous%20tasks.%0AExisting%20methods%20usually%20neglect%20the%20importance%20of%20memory%20diversity%2C%20limiting%0Ain%20effectively%20selecting%20high-quality%20memory%20from%20previous%20tasks%20and%0Aremembering%20broad%20previous%20knowledge%20within%20the%20scarce%20memory%20on%20graphs.%20To%0Aaddress%20that%2C%20we%20introduce%20a%20novel%20holistic%20Diversified%20Memory%20Selection%20and%0AGeneration%20%28DMSG%29%20framework%20for%20incremental%20learning%20in%20graphs%2C%20which%20first%0Aintroduces%20a%20buffer%20selection%20strategy%20that%20considers%20both%20intra-class%20and%0Ainter-class%20diversities%2C%20employing%20an%20efficient%20greedy%20algorithm%20for%20sampling%0Arepresentative%20training%20nodes%20from%20graphs%20into%20memory%20buffers%20after%20learning%0Aeach%20new%20task.%20Then%2C%20to%20adequately%20rememorize%20the%20knowledge%20preserved%20in%20the%0Amemory%20buffer%20when%20learning%20new%20tasks%2C%20we%20propose%20a%20diversified%20memory%0Ageneration%20replay%20method.%20This%20method%20first%20utilizes%20a%20variational%20layer%20to%0Agenerate%20the%20distribution%20of%20buffer%20node%20embeddings%20and%20sample%20synthesized%20ones%0Afor%20replaying.%20Furthermore%2C%20an%20adversarial%20variational%20embedding%20learning%0Amethod%20and%20a%20reconstruction-based%20decoder%20are%20proposed%20to%20maintain%20the%0Aintegrity%20and%20consolidate%20the%20generalization%20of%20the%20synthesized%20node%0Aembeddings%2C%20respectively.%20Finally%2C%20we%20evaluate%20our%20model%20on%20node%20classification%0Atasks%20involving%20increasing%20class%20numbers.%20Extensive%20experimental%20results%20on%0Apublicly%20accessible%20datasets%20demonstrate%20the%20superiority%20of%20DMSG%20over%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHolistic%2520Memory%2520Diversification%2520for%2520Incremental%2520Learning%2520in%2520Growing%250A%2520%2520Graphs%26entry.906535625%3DZiyue%2520Qiao%2520and%2520Junren%2520Xiao%2520and%2520Qingqiang%2520Sun%2520and%2520Meng%2520Xiao%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520incremental%2520learning%2520in%2520growing%2520graphs%250Awith%2520increasingly%2520complex%2520tasks.%2520The%2520goal%2520is%2520to%2520continually%2520train%2520a%2520graph%2520model%250Ato%2520handle%2520new%2520tasks%2520while%2520retaining%2520its%2520inference%2520ability%2520on%2520previous%2520tasks.%250AExisting%2520methods%2520usually%2520neglect%2520the%2520importance%2520of%2520memory%2520diversity%252C%2520limiting%250Ain%2520effectively%2520selecting%2520high-quality%2520memory%2520from%2520previous%2520tasks%2520and%250Aremembering%2520broad%2520previous%2520knowledge%2520within%2520the%2520scarce%2520memory%2520on%2520graphs.%2520To%250Aaddress%2520that%252C%2520we%2520introduce%2520a%2520novel%2520holistic%2520Diversified%2520Memory%2520Selection%2520and%250AGeneration%2520%2528DMSG%2529%2520framework%2520for%2520incremental%2520learning%2520in%2520graphs%252C%2520which%2520first%250Aintroduces%2520a%2520buffer%2520selection%2520strategy%2520that%2520considers%2520both%2520intra-class%2520and%250Ainter-class%2520diversities%252C%2520employing%2520an%2520efficient%2520greedy%2520algorithm%2520for%2520sampling%250Arepresentative%2520training%2520nodes%2520from%2520graphs%2520into%2520memory%2520buffers%2520after%2520learning%250Aeach%2520new%2520task.%2520Then%252C%2520to%2520adequately%2520rememorize%2520the%2520knowledge%2520preserved%2520in%2520the%250Amemory%2520buffer%2520when%2520learning%2520new%2520tasks%252C%2520we%2520propose%2520a%2520diversified%2520memory%250Ageneration%2520replay%2520method.%2520This%2520method%2520first%2520utilizes%2520a%2520variational%2520layer%2520to%250Agenerate%2520the%2520distribution%2520of%2520buffer%2520node%2520embeddings%2520and%2520sample%2520synthesized%2520ones%250Afor%2520replaying.%2520Furthermore%252C%2520an%2520adversarial%2520variational%2520embedding%2520learning%250Amethod%2520and%2520a%2520reconstruction-based%2520decoder%2520are%2520proposed%2520to%2520maintain%2520the%250Aintegrity%2520and%2520consolidate%2520the%2520generalization%2520of%2520the%2520synthesized%2520node%250Aembeddings%252C%2520respectively.%2520Finally%252C%2520we%2520evaluate%2520our%2520model%2520on%2520node%2520classification%250Atasks%2520involving%2520increasing%2520class%2520numbers.%2520Extensive%2520experimental%2520results%2520on%250Apublicly%2520accessible%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520DMSG%2520over%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holistic%20Memory%20Diversification%20for%20Incremental%20Learning%20in%20Growing%0A%20%20Graphs&entry.906535625=Ziyue%20Qiao%20and%20Junren%20Xiao%20and%20Qingqiang%20Sun%20and%20Meng%20Xiao%20and%20Hui%20Xiong&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20incremental%20learning%20in%20growing%20graphs%0Awith%20increasingly%20complex%20tasks.%20The%20goal%20is%20to%20continually%20train%20a%20graph%20model%0Ato%20handle%20new%20tasks%20while%20retaining%20its%20inference%20ability%20on%20previous%20tasks.%0AExisting%20methods%20usually%20neglect%20the%20importance%20of%20memory%20diversity%2C%20limiting%0Ain%20effectively%20selecting%20high-quality%20memory%20from%20previous%20tasks%20and%0Aremembering%20broad%20previous%20knowledge%20within%20the%20scarce%20memory%20on%20graphs.%20To%0Aaddress%20that%2C%20we%20introduce%20a%20novel%20holistic%20Diversified%20Memory%20Selection%20and%0AGeneration%20%28DMSG%29%20framework%20for%20incremental%20learning%20in%20graphs%2C%20which%20first%0Aintroduces%20a%20buffer%20selection%20strategy%20that%20considers%20both%20intra-class%20and%0Ainter-class%20diversities%2C%20employing%20an%20efficient%20greedy%20algorithm%20for%20sampling%0Arepresentative%20training%20nodes%20from%20graphs%20into%20memory%20buffers%20after%20learning%0Aeach%20new%20task.%20Then%2C%20to%20adequately%20rememorize%20the%20knowledge%20preserved%20in%20the%0Amemory%20buffer%20when%20learning%20new%20tasks%2C%20we%20propose%20a%20diversified%20memory%0Ageneration%20replay%20method.%20This%20method%20first%20utilizes%20a%20variational%20layer%20to%0Agenerate%20the%20distribution%20of%20buffer%20node%20embeddings%20and%20sample%20synthesized%20ones%0Afor%20replaying.%20Furthermore%2C%20an%20adversarial%20variational%20embedding%20learning%0Amethod%20and%20a%20reconstruction-based%20decoder%20are%20proposed%20to%20maintain%20the%0Aintegrity%20and%20consolidate%20the%20generalization%20of%20the%20synthesized%20node%0Aembeddings%2C%20respectively.%20Finally%2C%20we%20evaluate%20our%20model%20on%20node%20classification%0Atasks%20involving%20increasing%20class%20numbers.%20Extensive%20experimental%20results%20on%0Apublicly%20accessible%20datasets%20demonstrate%20the%20superiority%20of%20DMSG%20over%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07413v1&entry.124074799=Read"},
{"title": "SPIN: Spacecraft Imagery for Navigation", "author": "Javier Montalvo and Juan Ignacio Bravo P\u00e9rez-Villar and \u00c1lvaro Garc\u00eda-Mart\u00edn and Pablo Carballeira and Jes\u00fas Besc'os", "abstract": "  Data acquired in space operational conditions is scarce due to the costs and\ncomplexity of space operations. This poses a challenge to learning-based\nvisual-based navigation algorithms employed in autonomous spacecraft\nnavigation. Existing datasets, which largely depend on computer-simulated data,\nhave partially filled this gap. However, the image generation tools they use\nare proprietary, which limits the evaluation of methods to unseen scenarios.\nFurthermore, these datasets provide limited ground-truth data, primarily\nfocusing on the spacecraft's translation and rotation relative to the camera.\nTo address these limitations, we present SPIN (SPacecraft Imagery for\nNavigation), an open-source realistic spacecraft image generation tool for\nrelative navigation between two spacecrafts. SPIN provides a wide variety of\nground-truth data and allows researchers to employ custom 3D models of\nsatellites, define specific camera-relative poses, and adjust various settings\nsuch as camera parameters and environmental illumination conditions. For the\ntask of spacecraft pose estimation, we compare the results of training with a\nSPIN-generated dataset against existing synthetic datasets. We show a %50\naverage error reduction in common testbed data (that simulates realistic space\nconditions). Both the SPIN tool (and source code) and our enhanced version of\nthe synthetic datasets will be publicly released upon paper acceptance on\nGitHub https://github.com/vpulab/SPIN.\n", "link": "http://arxiv.org/abs/2406.07500v1", "date": "2024-06-11", "relevancy": 2.5751, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5141}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPIN%3A%20Spacecraft%20Imagery%20for%20Navigation&body=Title%3A%20SPIN%3A%20Spacecraft%20Imagery%20for%20Navigation%0AAuthor%3A%20Javier%20Montalvo%20and%20Juan%20Ignacio%20Bravo%20P%C3%A9rez-Villar%20and%20%C3%81lvaro%20Garc%C3%ADa-Mart%C3%ADn%20and%20Pablo%20Carballeira%20and%20Jes%C3%BAs%20Besc%27os%0AAbstract%3A%20%20%20Data%20acquired%20in%20space%20operational%20conditions%20is%20scarce%20due%20to%20the%20costs%20and%0Acomplexity%20of%20space%20operations.%20This%20poses%20a%20challenge%20to%20learning-based%0Avisual-based%20navigation%20algorithms%20employed%20in%20autonomous%20spacecraft%0Anavigation.%20Existing%20datasets%2C%20which%20largely%20depend%20on%20computer-simulated%20data%2C%0Ahave%20partially%20filled%20this%20gap.%20However%2C%20the%20image%20generation%20tools%20they%20use%0Aare%20proprietary%2C%20which%20limits%20the%20evaluation%20of%20methods%20to%20unseen%20scenarios.%0AFurthermore%2C%20these%20datasets%20provide%20limited%20ground-truth%20data%2C%20primarily%0Afocusing%20on%20the%20spacecraft%27s%20translation%20and%20rotation%20relative%20to%20the%20camera.%0ATo%20address%20these%20limitations%2C%20we%20present%20SPIN%20%28SPacecraft%20Imagery%20for%0ANavigation%29%2C%20an%20open-source%20realistic%20spacecraft%20image%20generation%20tool%20for%0Arelative%20navigation%20between%20two%20spacecrafts.%20SPIN%20provides%20a%20wide%20variety%20of%0Aground-truth%20data%20and%20allows%20researchers%20to%20employ%20custom%203D%20models%20of%0Asatellites%2C%20define%20specific%20camera-relative%20poses%2C%20and%20adjust%20various%20settings%0Asuch%20as%20camera%20parameters%20and%20environmental%20illumination%20conditions.%20For%20the%0Atask%20of%20spacecraft%20pose%20estimation%2C%20we%20compare%20the%20results%20of%20training%20with%20a%0ASPIN-generated%20dataset%20against%20existing%20synthetic%20datasets.%20We%20show%20a%20%2550%0Aaverage%20error%20reduction%20in%20common%20testbed%20data%20%28that%20simulates%20realistic%20space%0Aconditions%29.%20Both%20the%20SPIN%20tool%20%28and%20source%20code%29%20and%20our%20enhanced%20version%20of%0Athe%20synthetic%20datasets%20will%20be%20publicly%20released%20upon%20paper%20acceptance%20on%0AGitHub%20https%3A//github.com/vpulab/SPIN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPIN%253A%2520Spacecraft%2520Imagery%2520for%2520Navigation%26entry.906535625%3DJavier%2520Montalvo%2520and%2520Juan%2520Ignacio%2520Bravo%2520P%25C3%25A9rez-Villar%2520and%2520%25C3%2581lvaro%2520Garc%25C3%25ADa-Mart%25C3%25ADn%2520and%2520Pablo%2520Carballeira%2520and%2520Jes%25C3%25BAs%2520Besc%2527os%26entry.1292438233%3D%2520%2520Data%2520acquired%2520in%2520space%2520operational%2520conditions%2520is%2520scarce%2520due%2520to%2520the%2520costs%2520and%250Acomplexity%2520of%2520space%2520operations.%2520This%2520poses%2520a%2520challenge%2520to%2520learning-based%250Avisual-based%2520navigation%2520algorithms%2520employed%2520in%2520autonomous%2520spacecraft%250Anavigation.%2520Existing%2520datasets%252C%2520which%2520largely%2520depend%2520on%2520computer-simulated%2520data%252C%250Ahave%2520partially%2520filled%2520this%2520gap.%2520However%252C%2520the%2520image%2520generation%2520tools%2520they%2520use%250Aare%2520proprietary%252C%2520which%2520limits%2520the%2520evaluation%2520of%2520methods%2520to%2520unseen%2520scenarios.%250AFurthermore%252C%2520these%2520datasets%2520provide%2520limited%2520ground-truth%2520data%252C%2520primarily%250Afocusing%2520on%2520the%2520spacecraft%2527s%2520translation%2520and%2520rotation%2520relative%2520to%2520the%2520camera.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520present%2520SPIN%2520%2528SPacecraft%2520Imagery%2520for%250ANavigation%2529%252C%2520an%2520open-source%2520realistic%2520spacecraft%2520image%2520generation%2520tool%2520for%250Arelative%2520navigation%2520between%2520two%2520spacecrafts.%2520SPIN%2520provides%2520a%2520wide%2520variety%2520of%250Aground-truth%2520data%2520and%2520allows%2520researchers%2520to%2520employ%2520custom%25203D%2520models%2520of%250Asatellites%252C%2520define%2520specific%2520camera-relative%2520poses%252C%2520and%2520adjust%2520various%2520settings%250Asuch%2520as%2520camera%2520parameters%2520and%2520environmental%2520illumination%2520conditions.%2520For%2520the%250Atask%2520of%2520spacecraft%2520pose%2520estimation%252C%2520we%2520compare%2520the%2520results%2520of%2520training%2520with%2520a%250ASPIN-generated%2520dataset%2520against%2520existing%2520synthetic%2520datasets.%2520We%2520show%2520a%2520%252550%250Aaverage%2520error%2520reduction%2520in%2520common%2520testbed%2520data%2520%2528that%2520simulates%2520realistic%2520space%250Aconditions%2529.%2520Both%2520the%2520SPIN%2520tool%2520%2528and%2520source%2520code%2529%2520and%2520our%2520enhanced%2520version%2520of%250Athe%2520synthetic%2520datasets%2520will%2520be%2520publicly%2520released%2520upon%2520paper%2520acceptance%2520on%250AGitHub%2520https%253A//github.com/vpulab/SPIN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPIN%3A%20Spacecraft%20Imagery%20for%20Navigation&entry.906535625=Javier%20Montalvo%20and%20Juan%20Ignacio%20Bravo%20P%C3%A9rez-Villar%20and%20%C3%81lvaro%20Garc%C3%ADa-Mart%C3%ADn%20and%20Pablo%20Carballeira%20and%20Jes%C3%BAs%20Besc%27os&entry.1292438233=%20%20Data%20acquired%20in%20space%20operational%20conditions%20is%20scarce%20due%20to%20the%20costs%20and%0Acomplexity%20of%20space%20operations.%20This%20poses%20a%20challenge%20to%20learning-based%0Avisual-based%20navigation%20algorithms%20employed%20in%20autonomous%20spacecraft%0Anavigation.%20Existing%20datasets%2C%20which%20largely%20depend%20on%20computer-simulated%20data%2C%0Ahave%20partially%20filled%20this%20gap.%20However%2C%20the%20image%20generation%20tools%20they%20use%0Aare%20proprietary%2C%20which%20limits%20the%20evaluation%20of%20methods%20to%20unseen%20scenarios.%0AFurthermore%2C%20these%20datasets%20provide%20limited%20ground-truth%20data%2C%20primarily%0Afocusing%20on%20the%20spacecraft%27s%20translation%20and%20rotation%20relative%20to%20the%20camera.%0ATo%20address%20these%20limitations%2C%20we%20present%20SPIN%20%28SPacecraft%20Imagery%20for%0ANavigation%29%2C%20an%20open-source%20realistic%20spacecraft%20image%20generation%20tool%20for%0Arelative%20navigation%20between%20two%20spacecrafts.%20SPIN%20provides%20a%20wide%20variety%20of%0Aground-truth%20data%20and%20allows%20researchers%20to%20employ%20custom%203D%20models%20of%0Asatellites%2C%20define%20specific%20camera-relative%20poses%2C%20and%20adjust%20various%20settings%0Asuch%20as%20camera%20parameters%20and%20environmental%20illumination%20conditions.%20For%20the%0Atask%20of%20spacecraft%20pose%20estimation%2C%20we%20compare%20the%20results%20of%20training%20with%20a%0ASPIN-generated%20dataset%20against%20existing%20synthetic%20datasets.%20We%20show%20a%20%2550%0Aaverage%20error%20reduction%20in%20common%20testbed%20data%20%28that%20simulates%20realistic%20space%0Aconditions%29.%20Both%20the%20SPIN%20tool%20%28and%20source%20code%29%20and%20our%20enhanced%20version%20of%0Athe%20synthetic%20datasets%20will%20be%20publicly%20released%20upon%20paper%20acceptance%20on%0AGitHub%20https%3A//github.com/vpulab/SPIN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07500v1&entry.124074799=Read"},
{"title": "Frame Interpolation with Consecutive Brownian Bridge Diffusion", "author": "Zonglin Lyu and Ming Li and Jianbo Jiao and Chen Chen", "abstract": "  Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a\ndiffusion-based conditional image generation problem, synthesizing the\nintermediate frame given a random noise and neighboring frames. Due to the\nrelatively high resolution of videos, Latent Diffusion Models (LDMs) are\nemployed as the conditional generation model, where the autoencoder compresses\nimages into latent representations for diffusion and then reconstructs images\nfrom these latent representations. Such a formulation poses a crucial\nchallenge: VFI expects that the output is deterministically equal to the ground\ntruth intermediate frame, but LDMs randomly generate a diverse set of different\nimages when the model runs multiple times. The reason for the diverse\ngeneration is that the cumulative variance (variance accumulated at each step\nof generation) of generated latent representations in LDMs is large. This makes\nthe sampling trajectory random, resulting in diverse rather than deterministic\ngenerations. To address this problem, we propose our unique solution: Frame\nInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, we\npropose consecutive Brownian Bridge diffusion that takes a deterministic\ninitial value as input, resulting in a much smaller cumulative variance of\ngenerated latent representations. Our experiments suggest that our method can\nimprove together with the improvement of the autoencoder and achieve\nstate-of-the-art performance in VFI, leaving strong potential for further\nenhancement.\n", "link": "http://arxiv.org/abs/2405.05953v2", "date": "2024-06-11", "relevancy": 2.501, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6468}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6161}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion&body=Title%3A%20Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion%0AAuthor%3A%20Zonglin%20Lyu%20and%20Ming%20Li%20and%20Jianbo%20Jiao%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Recent%20work%20in%20Video%20Frame%20Interpolation%20%28VFI%29%20tries%20to%20formulate%20VFI%20as%20a%0Adiffusion-based%20conditional%20image%20generation%20problem%2C%20synthesizing%20the%0Aintermediate%20frame%20given%20a%20random%20noise%20and%20neighboring%20frames.%20Due%20to%20the%0Arelatively%20high%20resolution%20of%20videos%2C%20Latent%20Diffusion%20Models%20%28LDMs%29%20are%0Aemployed%20as%20the%20conditional%20generation%20model%2C%20where%20the%20autoencoder%20compresses%0Aimages%20into%20latent%20representations%20for%20diffusion%20and%20then%20reconstructs%20images%0Afrom%20these%20latent%20representations.%20Such%20a%20formulation%20poses%20a%20crucial%0Achallenge%3A%20VFI%20expects%20that%20the%20output%20is%20deterministically%20equal%20to%20the%20ground%0Atruth%20intermediate%20frame%2C%20but%20LDMs%20randomly%20generate%20a%20diverse%20set%20of%20different%0Aimages%20when%20the%20model%20runs%20multiple%20times.%20The%20reason%20for%20the%20diverse%0Ageneration%20is%20that%20the%20cumulative%20variance%20%28variance%20accumulated%20at%20each%20step%0Aof%20generation%29%20of%20generated%20latent%20representations%20in%20LDMs%20is%20large.%20This%20makes%0Athe%20sampling%20trajectory%20random%2C%20resulting%20in%20diverse%20rather%20than%20deterministic%0Agenerations.%20To%20address%20this%20problem%2C%20we%20propose%20our%20unique%20solution%3A%20Frame%0AInterpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion.%20Specifically%2C%20we%0Apropose%20consecutive%20Brownian%20Bridge%20diffusion%20that%20takes%20a%20deterministic%0Ainitial%20value%20as%20input%2C%20resulting%20in%20a%20much%20smaller%20cumulative%20variance%20of%0Agenerated%20latent%20representations.%20Our%20experiments%20suggest%20that%20our%20method%20can%0Aimprove%20together%20with%20the%20improvement%20of%20the%20autoencoder%20and%20achieve%0Astate-of-the-art%20performance%20in%20VFI%2C%20leaving%20strong%20potential%20for%20further%0Aenhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrame%2520Interpolation%2520with%2520Consecutive%2520Brownian%2520Bridge%2520Diffusion%26entry.906535625%3DZonglin%2520Lyu%2520and%2520Ming%2520Li%2520and%2520Jianbo%2520Jiao%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520work%2520in%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520tries%2520to%2520formulate%2520VFI%2520as%2520a%250Adiffusion-based%2520conditional%2520image%2520generation%2520problem%252C%2520synthesizing%2520the%250Aintermediate%2520frame%2520given%2520a%2520random%2520noise%2520and%2520neighboring%2520frames.%2520Due%2520to%2520the%250Arelatively%2520high%2520resolution%2520of%2520videos%252C%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529%2520are%250Aemployed%2520as%2520the%2520conditional%2520generation%2520model%252C%2520where%2520the%2520autoencoder%2520compresses%250Aimages%2520into%2520latent%2520representations%2520for%2520diffusion%2520and%2520then%2520reconstructs%2520images%250Afrom%2520these%2520latent%2520representations.%2520Such%2520a%2520formulation%2520poses%2520a%2520crucial%250Achallenge%253A%2520VFI%2520expects%2520that%2520the%2520output%2520is%2520deterministically%2520equal%2520to%2520the%2520ground%250Atruth%2520intermediate%2520frame%252C%2520but%2520LDMs%2520randomly%2520generate%2520a%2520diverse%2520set%2520of%2520different%250Aimages%2520when%2520the%2520model%2520runs%2520multiple%2520times.%2520The%2520reason%2520for%2520the%2520diverse%250Ageneration%2520is%2520that%2520the%2520cumulative%2520variance%2520%2528variance%2520accumulated%2520at%2520each%2520step%250Aof%2520generation%2529%2520of%2520generated%2520latent%2520representations%2520in%2520LDMs%2520is%2520large.%2520This%2520makes%250Athe%2520sampling%2520trajectory%2520random%252C%2520resulting%2520in%2520diverse%2520rather%2520than%2520deterministic%250Agenerations.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520our%2520unique%2520solution%253A%2520Frame%250AInterpolation%2520with%2520Consecutive%2520Brownian%2520Bridge%2520Diffusion.%2520Specifically%252C%2520we%250Apropose%2520consecutive%2520Brownian%2520Bridge%2520diffusion%2520that%2520takes%2520a%2520deterministic%250Ainitial%2520value%2520as%2520input%252C%2520resulting%2520in%2520a%2520much%2520smaller%2520cumulative%2520variance%2520of%250Agenerated%2520latent%2520representations.%2520Our%2520experiments%2520suggest%2520that%2520our%2520method%2520can%250Aimprove%2520together%2520with%2520the%2520improvement%2520of%2520the%2520autoencoder%2520and%2520achieve%250Astate-of-the-art%2520performance%2520in%2520VFI%252C%2520leaving%2520strong%2520potential%2520for%2520further%250Aenhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion&entry.906535625=Zonglin%20Lyu%20and%20Ming%20Li%20and%20Jianbo%20Jiao%20and%20Chen%20Chen&entry.1292438233=%20%20Recent%20work%20in%20Video%20Frame%20Interpolation%20%28VFI%29%20tries%20to%20formulate%20VFI%20as%20a%0Adiffusion-based%20conditional%20image%20generation%20problem%2C%20synthesizing%20the%0Aintermediate%20frame%20given%20a%20random%20noise%20and%20neighboring%20frames.%20Due%20to%20the%0Arelatively%20high%20resolution%20of%20videos%2C%20Latent%20Diffusion%20Models%20%28LDMs%29%20are%0Aemployed%20as%20the%20conditional%20generation%20model%2C%20where%20the%20autoencoder%20compresses%0Aimages%20into%20latent%20representations%20for%20diffusion%20and%20then%20reconstructs%20images%0Afrom%20these%20latent%20representations.%20Such%20a%20formulation%20poses%20a%20crucial%0Achallenge%3A%20VFI%20expects%20that%20the%20output%20is%20deterministically%20equal%20to%20the%20ground%0Atruth%20intermediate%20frame%2C%20but%20LDMs%20randomly%20generate%20a%20diverse%20set%20of%20different%0Aimages%20when%20the%20model%20runs%20multiple%20times.%20The%20reason%20for%20the%20diverse%0Ageneration%20is%20that%20the%20cumulative%20variance%20%28variance%20accumulated%20at%20each%20step%0Aof%20generation%29%20of%20generated%20latent%20representations%20in%20LDMs%20is%20large.%20This%20makes%0Athe%20sampling%20trajectory%20random%2C%20resulting%20in%20diverse%20rather%20than%20deterministic%0Agenerations.%20To%20address%20this%20problem%2C%20we%20propose%20our%20unique%20solution%3A%20Frame%0AInterpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion.%20Specifically%2C%20we%0Apropose%20consecutive%20Brownian%20Bridge%20diffusion%20that%20takes%20a%20deterministic%0Ainitial%20value%20as%20input%2C%20resulting%20in%20a%20much%20smaller%20cumulative%20variance%20of%0Agenerated%20latent%20representations.%20Our%20experiments%20suggest%20that%20our%20method%20can%0Aimprove%20together%20with%20the%20improvement%20of%20the%20autoencoder%20and%20achieve%0Astate-of-the-art%20performance%20in%20VFI%2C%20leaving%20strong%20potential%20for%20further%0Aenhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05953v2&entry.124074799=Read"},
{"title": "MeGA: Merging Multiple Independently Trained Neural Networks Based on\n  Genetic Algorithm", "author": "Daniel Yun", "abstract": "  In this paper, we introduce a novel method for merging the weights of\nmultiple pre-trained neural networks using a genetic algorithm called MeGA.\nTraditional techniques, such as weight averaging and ensemble methods, often\nfail to fully harness the capabilities of pre-trained networks. Our approach\nleverages a genetic algorithm with tournament selection, crossover, and\nmutation to optimize weight combinations, creating a more effective fusion.\nThis technique allows the merged model to inherit advantageous features from\nboth parent models, resulting in enhanced accuracy and robustness. Through\nexperiments on the CIFAR-10 dataset, we demonstrate that our genetic\nalgorithm-based weight merging method improves test accuracy compared to\nindividual models and conventional methods. This approach provides a scalable\nsolution for integrating multiple pre-trained networks across various deep\nlearning applications. Github is available at:\nhttps://github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm\n", "link": "http://arxiv.org/abs/2406.04607v2", "date": "2024-06-11", "relevancy": 2.474, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5305}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5042}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeGA%3A%20Merging%20Multiple%20Independently%20Trained%20Neural%20Networks%20Based%20on%0A%20%20Genetic%20Algorithm&body=Title%3A%20MeGA%3A%20Merging%20Multiple%20Independently%20Trained%20Neural%20Networks%20Based%20on%0A%20%20Genetic%20Algorithm%0AAuthor%3A%20Daniel%20Yun%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20for%20merging%20the%20weights%20of%0Amultiple%20pre-trained%20neural%20networks%20using%20a%20genetic%20algorithm%20called%20MeGA.%0ATraditional%20techniques%2C%20such%20as%20weight%20averaging%20and%20ensemble%20methods%2C%20often%0Afail%20to%20fully%20harness%20the%20capabilities%20of%20pre-trained%20networks.%20Our%20approach%0Aleverages%20a%20genetic%20algorithm%20with%20tournament%20selection%2C%20crossover%2C%20and%0Amutation%20to%20optimize%20weight%20combinations%2C%20creating%20a%20more%20effective%20fusion.%0AThis%20technique%20allows%20the%20merged%20model%20to%20inherit%20advantageous%20features%20from%0Aboth%20parent%20models%2C%20resulting%20in%20enhanced%20accuracy%20and%20robustness.%20Through%0Aexperiments%20on%20the%20CIFAR-10%20dataset%2C%20we%20demonstrate%20that%20our%20genetic%0Aalgorithm-based%20weight%20merging%20method%20improves%20test%20accuracy%20compared%20to%0Aindividual%20models%20and%20conventional%20methods.%20This%20approach%20provides%20a%20scalable%0Asolution%20for%20integrating%20multiple%20pre-trained%20networks%20across%20various%20deep%0Alearning%20applications.%20Github%20is%20available%20at%3A%0Ahttps%3A//github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04607v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeGA%253A%2520Merging%2520Multiple%2520Independently%2520Trained%2520Neural%2520Networks%2520Based%2520on%250A%2520%2520Genetic%2520Algorithm%26entry.906535625%3DDaniel%2520Yun%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520method%2520for%2520merging%2520the%2520weights%2520of%250Amultiple%2520pre-trained%2520neural%2520networks%2520using%2520a%2520genetic%2520algorithm%2520called%2520MeGA.%250ATraditional%2520techniques%252C%2520such%2520as%2520weight%2520averaging%2520and%2520ensemble%2520methods%252C%2520often%250Afail%2520to%2520fully%2520harness%2520the%2520capabilities%2520of%2520pre-trained%2520networks.%2520Our%2520approach%250Aleverages%2520a%2520genetic%2520algorithm%2520with%2520tournament%2520selection%252C%2520crossover%252C%2520and%250Amutation%2520to%2520optimize%2520weight%2520combinations%252C%2520creating%2520a%2520more%2520effective%2520fusion.%250AThis%2520technique%2520allows%2520the%2520merged%2520model%2520to%2520inherit%2520advantageous%2520features%2520from%250Aboth%2520parent%2520models%252C%2520resulting%2520in%2520enhanced%2520accuracy%2520and%2520robustness.%2520Through%250Aexperiments%2520on%2520the%2520CIFAR-10%2520dataset%252C%2520we%2520demonstrate%2520that%2520our%2520genetic%250Aalgorithm-based%2520weight%2520merging%2520method%2520improves%2520test%2520accuracy%2520compared%2520to%250Aindividual%2520models%2520and%2520conventional%2520methods.%2520This%2520approach%2520provides%2520a%2520scalable%250Asolution%2520for%2520integrating%2520multiple%2520pre-trained%2520networks%2520across%2520various%2520deep%250Alearning%2520applications.%2520Github%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04607v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeGA%3A%20Merging%20Multiple%20Independently%20Trained%20Neural%20Networks%20Based%20on%0A%20%20Genetic%20Algorithm&entry.906535625=Daniel%20Yun&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20for%20merging%20the%20weights%20of%0Amultiple%20pre-trained%20neural%20networks%20using%20a%20genetic%20algorithm%20called%20MeGA.%0ATraditional%20techniques%2C%20such%20as%20weight%20averaging%20and%20ensemble%20methods%2C%20often%0Afail%20to%20fully%20harness%20the%20capabilities%20of%20pre-trained%20networks.%20Our%20approach%0Aleverages%20a%20genetic%20algorithm%20with%20tournament%20selection%2C%20crossover%2C%20and%0Amutation%20to%20optimize%20weight%20combinations%2C%20creating%20a%20more%20effective%20fusion.%0AThis%20technique%20allows%20the%20merged%20model%20to%20inherit%20advantageous%20features%20from%0Aboth%20parent%20models%2C%20resulting%20in%20enhanced%20accuracy%20and%20robustness.%20Through%0Aexperiments%20on%20the%20CIFAR-10%20dataset%2C%20we%20demonstrate%20that%20our%20genetic%0Aalgorithm-based%20weight%20merging%20method%20improves%20test%20accuracy%20compared%20to%0Aindividual%20models%20and%20conventional%20methods.%20This%20approach%20provides%20a%20scalable%0Asolution%20for%20integrating%20multiple%20pre-trained%20networks%20across%20various%20deep%0Alearning%20applications.%20Github%20is%20available%20at%3A%0Ahttps%3A//github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04607v2&entry.124074799=Read"},
{"title": "LOP-Field: Brain-inspired Layout-Object-Position Fields for Robotic\n  Scene Understanding", "author": "Jiawei Hou and Wenhao Guan and Xiangyang Xue and Taiping Zeng", "abstract": "  Spatial cognition empowers animals with remarkably efficient navigation\nabilities, largely depending on the scene-level understanding of spatial\nenvironments. Recently, it has been found that a neural population in the\npostrhinal cortex of rat brains is more strongly tuned to the spatial layout\nrather than objects in a scene. Inspired by the representations of spatial\nlayout in local scenes to encode different regions separately, we proposed\nLOP-Field that realizes the Layout-Object-Position(LOP) association to model\nthe hierarchical representations for robotic scene understanding. Powered by\nfoundation models and implicit scene representation, a neural field is\nimplemented as a scene memory for robots, storing a queryable representation of\nscenes with position-wise, object-wise, and layout-wise information. To\nvalidate the built LOP association, the model is tested to infer region\ninformation from 3D positions with quantitative metrics, achieving an average\naccuracy of more than 88\\%. It is also shown that the proposed method using\nregion information can achieve improved object and view localization results\nwith text and RGB input compared to state-of-the-art localization methods.\n", "link": "http://arxiv.org/abs/2406.05985v2", "date": "2024-06-11", "relevancy": 2.4493, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6343}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6221}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOP-Field%3A%20Brain-inspired%20Layout-Object-Position%20Fields%20for%20Robotic%0A%20%20Scene%20Understanding&body=Title%3A%20LOP-Field%3A%20Brain-inspired%20Layout-Object-Position%20Fields%20for%20Robotic%0A%20%20Scene%20Understanding%0AAuthor%3A%20Jiawei%20Hou%20and%20Wenhao%20Guan%20and%20Xiangyang%20Xue%20and%20Taiping%20Zeng%0AAbstract%3A%20%20%20Spatial%20cognition%20empowers%20animals%20with%20remarkably%20efficient%20navigation%0Aabilities%2C%20largely%20depending%20on%20the%20scene-level%20understanding%20of%20spatial%0Aenvironments.%20Recently%2C%20it%20has%20been%20found%20that%20a%20neural%20population%20in%20the%0Apostrhinal%20cortex%20of%20rat%20brains%20is%20more%20strongly%20tuned%20to%20the%20spatial%20layout%0Arather%20than%20objects%20in%20a%20scene.%20Inspired%20by%20the%20representations%20of%20spatial%0Alayout%20in%20local%20scenes%20to%20encode%20different%20regions%20separately%2C%20we%20proposed%0ALOP-Field%20that%20realizes%20the%20Layout-Object-Position%28LOP%29%20association%20to%20model%0Athe%20hierarchical%20representations%20for%20robotic%20scene%20understanding.%20Powered%20by%0Afoundation%20models%20and%20implicit%20scene%20representation%2C%20a%20neural%20field%20is%0Aimplemented%20as%20a%20scene%20memory%20for%20robots%2C%20storing%20a%20queryable%20representation%20of%0Ascenes%20with%20position-wise%2C%20object-wise%2C%20and%20layout-wise%20information.%20To%0Avalidate%20the%20built%20LOP%20association%2C%20the%20model%20is%20tested%20to%20infer%20region%0Ainformation%20from%203D%20positions%20with%20quantitative%20metrics%2C%20achieving%20an%20average%0Aaccuracy%20of%20more%20than%2088%5C%25.%20It%20is%20also%20shown%20that%20the%20proposed%20method%20using%0Aregion%20information%20can%20achieve%20improved%20object%20and%20view%20localization%20results%0Awith%20text%20and%20RGB%20input%20compared%20to%20state-of-the-art%20localization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05985v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOP-Field%253A%2520Brain-inspired%2520Layout-Object-Position%2520Fields%2520for%2520Robotic%250A%2520%2520Scene%2520Understanding%26entry.906535625%3DJiawei%2520Hou%2520and%2520Wenhao%2520Guan%2520and%2520Xiangyang%2520Xue%2520and%2520Taiping%2520Zeng%26entry.1292438233%3D%2520%2520Spatial%2520cognition%2520empowers%2520animals%2520with%2520remarkably%2520efficient%2520navigation%250Aabilities%252C%2520largely%2520depending%2520on%2520the%2520scene-level%2520understanding%2520of%2520spatial%250Aenvironments.%2520Recently%252C%2520it%2520has%2520been%2520found%2520that%2520a%2520neural%2520population%2520in%2520the%250Apostrhinal%2520cortex%2520of%2520rat%2520brains%2520is%2520more%2520strongly%2520tuned%2520to%2520the%2520spatial%2520layout%250Arather%2520than%2520objects%2520in%2520a%2520scene.%2520Inspired%2520by%2520the%2520representations%2520of%2520spatial%250Alayout%2520in%2520local%2520scenes%2520to%2520encode%2520different%2520regions%2520separately%252C%2520we%2520proposed%250ALOP-Field%2520that%2520realizes%2520the%2520Layout-Object-Position%2528LOP%2529%2520association%2520to%2520model%250Athe%2520hierarchical%2520representations%2520for%2520robotic%2520scene%2520understanding.%2520Powered%2520by%250Afoundation%2520models%2520and%2520implicit%2520scene%2520representation%252C%2520a%2520neural%2520field%2520is%250Aimplemented%2520as%2520a%2520scene%2520memory%2520for%2520robots%252C%2520storing%2520a%2520queryable%2520representation%2520of%250Ascenes%2520with%2520position-wise%252C%2520object-wise%252C%2520and%2520layout-wise%2520information.%2520To%250Avalidate%2520the%2520built%2520LOP%2520association%252C%2520the%2520model%2520is%2520tested%2520to%2520infer%2520region%250Ainformation%2520from%25203D%2520positions%2520with%2520quantitative%2520metrics%252C%2520achieving%2520an%2520average%250Aaccuracy%2520of%2520more%2520than%252088%255C%2525.%2520It%2520is%2520also%2520shown%2520that%2520the%2520proposed%2520method%2520using%250Aregion%2520information%2520can%2520achieve%2520improved%2520object%2520and%2520view%2520localization%2520results%250Awith%2520text%2520and%2520RGB%2520input%2520compared%2520to%2520state-of-the-art%2520localization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05985v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOP-Field%3A%20Brain-inspired%20Layout-Object-Position%20Fields%20for%20Robotic%0A%20%20Scene%20Understanding&entry.906535625=Jiawei%20Hou%20and%20Wenhao%20Guan%20and%20Xiangyang%20Xue%20and%20Taiping%20Zeng&entry.1292438233=%20%20Spatial%20cognition%20empowers%20animals%20with%20remarkably%20efficient%20navigation%0Aabilities%2C%20largely%20depending%20on%20the%20scene-level%20understanding%20of%20spatial%0Aenvironments.%20Recently%2C%20it%20has%20been%20found%20that%20a%20neural%20population%20in%20the%0Apostrhinal%20cortex%20of%20rat%20brains%20is%20more%20strongly%20tuned%20to%20the%20spatial%20layout%0Arather%20than%20objects%20in%20a%20scene.%20Inspired%20by%20the%20representations%20of%20spatial%0Alayout%20in%20local%20scenes%20to%20encode%20different%20regions%20separately%2C%20we%20proposed%0ALOP-Field%20that%20realizes%20the%20Layout-Object-Position%28LOP%29%20association%20to%20model%0Athe%20hierarchical%20representations%20for%20robotic%20scene%20understanding.%20Powered%20by%0Afoundation%20models%20and%20implicit%20scene%20representation%2C%20a%20neural%20field%20is%0Aimplemented%20as%20a%20scene%20memory%20for%20robots%2C%20storing%20a%20queryable%20representation%20of%0Ascenes%20with%20position-wise%2C%20object-wise%2C%20and%20layout-wise%20information.%20To%0Avalidate%20the%20built%20LOP%20association%2C%20the%20model%20is%20tested%20to%20infer%20region%0Ainformation%20from%203D%20positions%20with%20quantitative%20metrics%2C%20achieving%20an%20average%0Aaccuracy%20of%20more%20than%2088%5C%25.%20It%20is%20also%20shown%20that%20the%20proposed%20method%20using%0Aregion%20information%20can%20achieve%20improved%20object%20and%20view%20localization%20results%0Awith%20text%20and%20RGB%20input%20compared%20to%20state-of-the-art%20localization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05985v2&entry.124074799=Read"},
{"title": "CT Reconstruction using Diffusion Posterior Sampling conditioned on a\n  Nonlinear Measurement Model", "author": "Shudong Li and Xiao Jiang and Matthew Tivnan and Grace J. Gang and Yuan Shen and J. Webster Stayman", "abstract": "  Diffusion models have been demonstrated as powerful deep learning tools for\nimage generation in CT reconstruction and restoration. Recently, diffusion\nposterior sampling, where a score-based diffusion prior is combined with a\nlikelihood model, has been used to produce high quality CT images given\nlow-quality measurements. This technique is attractive since it permits a\none-time, unsupervised training of a CT prior; which can then be incorporated\nwith an arbitrary data model. However, current methods rely on a linear model\nof x-ray CT physics to reconstruct or restore images. While it is common to\nlinearize the transmission tomography reconstruction problem, this is an\napproximation to the true and inherently nonlinear forward model. We propose a\nnew method that solves the inverse problem of nonlinear CT image reconstruction\nvia diffusion posterior sampling. We implement a traditional unconditional\ndiffusion model by training a prior score function estimator, and apply Bayes\nrule to combine this prior with a measurement likelihood score function derived\nfrom the nonlinear physical model to arrive at a posterior score function that\ncan be used to sample the reverse-time diffusion process. This plug-and-play\nmethod allows incorporation of a diffusion-based prior with generalized\nnonlinear CT image reconstruction into multiple CT system designs with\ndifferent forward models, without the need for any additional training. We\ndevelop the algorithm that performs this reconstruction, including an\nordered-subsets variant for accelerated processing and demonstrate the\ntechnique in both fully sampled low dose data and sparse-view geometries using\na single unsupervised training of the prior.\n", "link": "http://arxiv.org/abs/2312.01464v2", "date": "2024-06-11", "relevancy": 2.4274, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6475}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5987}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT%20Reconstruction%20using%20Diffusion%20Posterior%20Sampling%20conditioned%20on%20a%0A%20%20Nonlinear%20Measurement%20Model&body=Title%3A%20CT%20Reconstruction%20using%20Diffusion%20Posterior%20Sampling%20conditioned%20on%20a%0A%20%20Nonlinear%20Measurement%20Model%0AAuthor%3A%20Shudong%20Li%20and%20Xiao%20Jiang%20and%20Matthew%20Tivnan%20and%20Grace%20J.%20Gang%20and%20Yuan%20Shen%20and%20J.%20Webster%20Stayman%0AAbstract%3A%20%20%20Diffusion%20models%20have%20been%20demonstrated%20as%20powerful%20deep%20learning%20tools%20for%0Aimage%20generation%20in%20CT%20reconstruction%20and%20restoration.%20Recently%2C%20diffusion%0Aposterior%20sampling%2C%20where%20a%20score-based%20diffusion%20prior%20is%20combined%20with%20a%0Alikelihood%20model%2C%20has%20been%20used%20to%20produce%20high%20quality%20CT%20images%20given%0Alow-quality%20measurements.%20This%20technique%20is%20attractive%20since%20it%20permits%20a%0Aone-time%2C%20unsupervised%20training%20of%20a%20CT%20prior%3B%20which%20can%20then%20be%20incorporated%0Awith%20an%20arbitrary%20data%20model.%20However%2C%20current%20methods%20rely%20on%20a%20linear%20model%0Aof%20x-ray%20CT%20physics%20to%20reconstruct%20or%20restore%20images.%20While%20it%20is%20common%20to%0Alinearize%20the%20transmission%20tomography%20reconstruction%20problem%2C%20this%20is%20an%0Aapproximation%20to%20the%20true%20and%20inherently%20nonlinear%20forward%20model.%20We%20propose%20a%0Anew%20method%20that%20solves%20the%20inverse%20problem%20of%20nonlinear%20CT%20image%20reconstruction%0Avia%20diffusion%20posterior%20sampling.%20We%20implement%20a%20traditional%20unconditional%0Adiffusion%20model%20by%20training%20a%20prior%20score%20function%20estimator%2C%20and%20apply%20Bayes%0Arule%20to%20combine%20this%20prior%20with%20a%20measurement%20likelihood%20score%20function%20derived%0Afrom%20the%20nonlinear%20physical%20model%20to%20arrive%20at%20a%20posterior%20score%20function%20that%0Acan%20be%20used%20to%20sample%20the%20reverse-time%20diffusion%20process.%20This%20plug-and-play%0Amethod%20allows%20incorporation%20of%20a%20diffusion-based%20prior%20with%20generalized%0Anonlinear%20CT%20image%20reconstruction%20into%20multiple%20CT%20system%20designs%20with%0Adifferent%20forward%20models%2C%20without%20the%20need%20for%20any%20additional%20training.%20We%0Adevelop%20the%20algorithm%20that%20performs%20this%20reconstruction%2C%20including%20an%0Aordered-subsets%20variant%20for%20accelerated%20processing%20and%20demonstrate%20the%0Atechnique%20in%20both%20fully%20sampled%20low%20dose%20data%20and%20sparse-view%20geometries%20using%0Aa%20single%20unsupervised%20training%20of%20the%20prior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01464v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT%2520Reconstruction%2520using%2520Diffusion%2520Posterior%2520Sampling%2520conditioned%2520on%2520a%250A%2520%2520Nonlinear%2520Measurement%2520Model%26entry.906535625%3DShudong%2520Li%2520and%2520Xiao%2520Jiang%2520and%2520Matthew%2520Tivnan%2520and%2520Grace%2520J.%2520Gang%2520and%2520Yuan%2520Shen%2520and%2520J.%2520Webster%2520Stayman%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520been%2520demonstrated%2520as%2520powerful%2520deep%2520learning%2520tools%2520for%250Aimage%2520generation%2520in%2520CT%2520reconstruction%2520and%2520restoration.%2520Recently%252C%2520diffusion%250Aposterior%2520sampling%252C%2520where%2520a%2520score-based%2520diffusion%2520prior%2520is%2520combined%2520with%2520a%250Alikelihood%2520model%252C%2520has%2520been%2520used%2520to%2520produce%2520high%2520quality%2520CT%2520images%2520given%250Alow-quality%2520measurements.%2520This%2520technique%2520is%2520attractive%2520since%2520it%2520permits%2520a%250Aone-time%252C%2520unsupervised%2520training%2520of%2520a%2520CT%2520prior%253B%2520which%2520can%2520then%2520be%2520incorporated%250Awith%2520an%2520arbitrary%2520data%2520model.%2520However%252C%2520current%2520methods%2520rely%2520on%2520a%2520linear%2520model%250Aof%2520x-ray%2520CT%2520physics%2520to%2520reconstruct%2520or%2520restore%2520images.%2520While%2520it%2520is%2520common%2520to%250Alinearize%2520the%2520transmission%2520tomography%2520reconstruction%2520problem%252C%2520this%2520is%2520an%250Aapproximation%2520to%2520the%2520true%2520and%2520inherently%2520nonlinear%2520forward%2520model.%2520We%2520propose%2520a%250Anew%2520method%2520that%2520solves%2520the%2520inverse%2520problem%2520of%2520nonlinear%2520CT%2520image%2520reconstruction%250Avia%2520diffusion%2520posterior%2520sampling.%2520We%2520implement%2520a%2520traditional%2520unconditional%250Adiffusion%2520model%2520by%2520training%2520a%2520prior%2520score%2520function%2520estimator%252C%2520and%2520apply%2520Bayes%250Arule%2520to%2520combine%2520this%2520prior%2520with%2520a%2520measurement%2520likelihood%2520score%2520function%2520derived%250Afrom%2520the%2520nonlinear%2520physical%2520model%2520to%2520arrive%2520at%2520a%2520posterior%2520score%2520function%2520that%250Acan%2520be%2520used%2520to%2520sample%2520the%2520reverse-time%2520diffusion%2520process.%2520This%2520plug-and-play%250Amethod%2520allows%2520incorporation%2520of%2520a%2520diffusion-based%2520prior%2520with%2520generalized%250Anonlinear%2520CT%2520image%2520reconstruction%2520into%2520multiple%2520CT%2520system%2520designs%2520with%250Adifferent%2520forward%2520models%252C%2520without%2520the%2520need%2520for%2520any%2520additional%2520training.%2520We%250Adevelop%2520the%2520algorithm%2520that%2520performs%2520this%2520reconstruction%252C%2520including%2520an%250Aordered-subsets%2520variant%2520for%2520accelerated%2520processing%2520and%2520demonstrate%2520the%250Atechnique%2520in%2520both%2520fully%2520sampled%2520low%2520dose%2520data%2520and%2520sparse-view%2520geometries%2520using%250Aa%2520single%2520unsupervised%2520training%2520of%2520the%2520prior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01464v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT%20Reconstruction%20using%20Diffusion%20Posterior%20Sampling%20conditioned%20on%20a%0A%20%20Nonlinear%20Measurement%20Model&entry.906535625=Shudong%20Li%20and%20Xiao%20Jiang%20and%20Matthew%20Tivnan%20and%20Grace%20J.%20Gang%20and%20Yuan%20Shen%20and%20J.%20Webster%20Stayman&entry.1292438233=%20%20Diffusion%20models%20have%20been%20demonstrated%20as%20powerful%20deep%20learning%20tools%20for%0Aimage%20generation%20in%20CT%20reconstruction%20and%20restoration.%20Recently%2C%20diffusion%0Aposterior%20sampling%2C%20where%20a%20score-based%20diffusion%20prior%20is%20combined%20with%20a%0Alikelihood%20model%2C%20has%20been%20used%20to%20produce%20high%20quality%20CT%20images%20given%0Alow-quality%20measurements.%20This%20technique%20is%20attractive%20since%20it%20permits%20a%0Aone-time%2C%20unsupervised%20training%20of%20a%20CT%20prior%3B%20which%20can%20then%20be%20incorporated%0Awith%20an%20arbitrary%20data%20model.%20However%2C%20current%20methods%20rely%20on%20a%20linear%20model%0Aof%20x-ray%20CT%20physics%20to%20reconstruct%20or%20restore%20images.%20While%20it%20is%20common%20to%0Alinearize%20the%20transmission%20tomography%20reconstruction%20problem%2C%20this%20is%20an%0Aapproximation%20to%20the%20true%20and%20inherently%20nonlinear%20forward%20model.%20We%20propose%20a%0Anew%20method%20that%20solves%20the%20inverse%20problem%20of%20nonlinear%20CT%20image%20reconstruction%0Avia%20diffusion%20posterior%20sampling.%20We%20implement%20a%20traditional%20unconditional%0Adiffusion%20model%20by%20training%20a%20prior%20score%20function%20estimator%2C%20and%20apply%20Bayes%0Arule%20to%20combine%20this%20prior%20with%20a%20measurement%20likelihood%20score%20function%20derived%0Afrom%20the%20nonlinear%20physical%20model%20to%20arrive%20at%20a%20posterior%20score%20function%20that%0Acan%20be%20used%20to%20sample%20the%20reverse-time%20diffusion%20process.%20This%20plug-and-play%0Amethod%20allows%20incorporation%20of%20a%20diffusion-based%20prior%20with%20generalized%0Anonlinear%20CT%20image%20reconstruction%20into%20multiple%20CT%20system%20designs%20with%0Adifferent%20forward%20models%2C%20without%20the%20need%20for%20any%20additional%20training.%20We%0Adevelop%20the%20algorithm%20that%20performs%20this%20reconstruction%2C%20including%20an%0Aordered-subsets%20variant%20for%20accelerated%20processing%20and%20demonstrate%20the%0Atechnique%20in%20both%20fully%20sampled%20low%20dose%20data%20and%20sparse-view%20geometries%20using%0Aa%20single%20unsupervised%20training%20of%20the%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01464v2&entry.124074799=Read"},
{"title": "Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene\n  Graph", "author": "Sergey Linok and Tatiana Zemskova and Svetlana Ladanova and Roman Titkov and Dmitry Yudin", "abstract": "  Locating objects referred to in natural language poses a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object retrieval with simple (bare) queries but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene spatial graph representation\nwith metric edges and utilizes a large language model as a human-to-agent\ninterface through our deductive scene reasoning algorithm. BBQ employs robust\nDINO-powered associations to form 3D objects, an advanced raycasting algorithm\nto project them to 2D, and a vision-language model to describe them as graph\nnodes. On Replica and ScanNet datasets, we show that the designed method\naccurately constructs 3D object-centric maps. We have demonstrated that their\nquality takes a leading place for open-vocabulary 3D semantic segmentation\nagainst other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach\ndemonstrates a significant improvement, enabling retrieving objects by complex\nqueries compared to other state-of-the-art methods. Considering our design\nsolutions, we achieved a processing speed approximately x3 times faster than\nthe closest analog. This promising performance enables our approach for usage\nin applied intelligent robotics projects. We make the code publicly available\nat linukc.github.io/bbq/.\n", "link": "http://arxiv.org/abs/2406.07113v1", "date": "2024-06-11", "relevancy": 2.4245, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6439}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5986}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Retrieval%20with%203D%20Scene%0A%20%20Graph&body=Title%3A%20Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Retrieval%20with%203D%20Scene%0A%20%20Graph%0AAuthor%3A%20Sergey%20Linok%20and%20Tatiana%20Zemskova%20and%20Svetlana%20Ladanova%20and%20Roman%20Titkov%20and%20Dmitry%20Yudin%0AAbstract%3A%20%20%20Locating%20objects%20referred%20to%20in%20natural%20language%20poses%20a%20significant%0Achallenge%20for%20autonomous%20agents.%20Existing%20CLIP-based%20open-vocabulary%20methods%0Asuccessfully%20perform%203D%20object%20retrieval%20with%20simple%20%28bare%29%20queries%20but%20cannot%0Acope%20with%20ambiguous%20descriptions%20that%20demand%20an%20understanding%20of%20object%0Arelations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20modular%20approach%20called%20BBQ%0A%28Beyond%20Bare%20Queries%29%2C%20which%20constructs%203D%20scene%20spatial%20graph%20representation%0Awith%20metric%20edges%20and%20utilizes%20a%20large%20language%20model%20as%20a%20human-to-agent%0Ainterface%20through%20our%20deductive%20scene%20reasoning%20algorithm.%20BBQ%20employs%20robust%0ADINO-powered%20associations%20to%20form%203D%20objects%2C%20an%20advanced%20raycasting%20algorithm%0Ato%20project%20them%20to%202D%2C%20and%20a%20vision-language%20model%20to%20describe%20them%20as%20graph%0Anodes.%20On%20Replica%20and%20ScanNet%20datasets%2C%20we%20show%20that%20the%20designed%20method%0Aaccurately%20constructs%203D%20object-centric%20maps.%20We%20have%20demonstrated%20that%20their%0Aquality%20takes%20a%20leading%20place%20for%20open-vocabulary%203D%20semantic%20segmentation%0Aagainst%20other%20zero-shot%20methods.%20Also%2C%20we%20show%20that%20leveraging%20spatial%0Arelations%20is%20especially%20effective%20for%20scenes%20containing%20multiple%20entities%20of%0Athe%20same%20semantic%20class.%20On%20Sr3D%20and%20Nr3D%20benchmarks%2C%20our%20deductive%20approach%0Ademonstrates%20a%20significant%20improvement%2C%20enabling%20retrieving%20objects%20by%20complex%0Aqueries%20compared%20to%20other%20state-of-the-art%20methods.%20Considering%20our%20design%0Asolutions%2C%20we%20achieved%20a%20processing%20speed%20approximately%20x3%20times%20faster%20than%0Athe%20closest%20analog.%20This%20promising%20performance%20enables%20our%20approach%20for%20usage%0Ain%20applied%20intelligent%20robotics%20projects.%20We%20make%20the%20code%20publicly%20available%0Aat%20linukc.github.io/bbq/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Bare%2520Queries%253A%2520Open-Vocabulary%2520Object%2520Retrieval%2520with%25203D%2520Scene%250A%2520%2520Graph%26entry.906535625%3DSergey%2520Linok%2520and%2520Tatiana%2520Zemskova%2520and%2520Svetlana%2520Ladanova%2520and%2520Roman%2520Titkov%2520and%2520Dmitry%2520Yudin%26entry.1292438233%3D%2520%2520Locating%2520objects%2520referred%2520to%2520in%2520natural%2520language%2520poses%2520a%2520significant%250Achallenge%2520for%2520autonomous%2520agents.%2520Existing%2520CLIP-based%2520open-vocabulary%2520methods%250Asuccessfully%2520perform%25203D%2520object%2520retrieval%2520with%2520simple%2520%2528bare%2529%2520queries%2520but%2520cannot%250Acope%2520with%2520ambiguous%2520descriptions%2520that%2520demand%2520an%2520understanding%2520of%2520object%250Arelations.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520modular%2520approach%2520called%2520BBQ%250A%2528Beyond%2520Bare%2520Queries%2529%252C%2520which%2520constructs%25203D%2520scene%2520spatial%2520graph%2520representation%250Awith%2520metric%2520edges%2520and%2520utilizes%2520a%2520large%2520language%2520model%2520as%2520a%2520human-to-agent%250Ainterface%2520through%2520our%2520deductive%2520scene%2520reasoning%2520algorithm.%2520BBQ%2520employs%2520robust%250ADINO-powered%2520associations%2520to%2520form%25203D%2520objects%252C%2520an%2520advanced%2520raycasting%2520algorithm%250Ato%2520project%2520them%2520to%25202D%252C%2520and%2520a%2520vision-language%2520model%2520to%2520describe%2520them%2520as%2520graph%250Anodes.%2520On%2520Replica%2520and%2520ScanNet%2520datasets%252C%2520we%2520show%2520that%2520the%2520designed%2520method%250Aaccurately%2520constructs%25203D%2520object-centric%2520maps.%2520We%2520have%2520demonstrated%2520that%2520their%250Aquality%2520takes%2520a%2520leading%2520place%2520for%2520open-vocabulary%25203D%2520semantic%2520segmentation%250Aagainst%2520other%2520zero-shot%2520methods.%2520Also%252C%2520we%2520show%2520that%2520leveraging%2520spatial%250Arelations%2520is%2520especially%2520effective%2520for%2520scenes%2520containing%2520multiple%2520entities%2520of%250Athe%2520same%2520semantic%2520class.%2520On%2520Sr3D%2520and%2520Nr3D%2520benchmarks%252C%2520our%2520deductive%2520approach%250Ademonstrates%2520a%2520significant%2520improvement%252C%2520enabling%2520retrieving%2520objects%2520by%2520complex%250Aqueries%2520compared%2520to%2520other%2520state-of-the-art%2520methods.%2520Considering%2520our%2520design%250Asolutions%252C%2520we%2520achieved%2520a%2520processing%2520speed%2520approximately%2520x3%2520times%2520faster%2520than%250Athe%2520closest%2520analog.%2520This%2520promising%2520performance%2520enables%2520our%2520approach%2520for%2520usage%250Ain%2520applied%2520intelligent%2520robotics%2520projects.%2520We%2520make%2520the%2520code%2520publicly%2520available%250Aat%2520linukc.github.io/bbq/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Retrieval%20with%203D%20Scene%0A%20%20Graph&entry.906535625=Sergey%20Linok%20and%20Tatiana%20Zemskova%20and%20Svetlana%20Ladanova%20and%20Roman%20Titkov%20and%20Dmitry%20Yudin&entry.1292438233=%20%20Locating%20objects%20referred%20to%20in%20natural%20language%20poses%20a%20significant%0Achallenge%20for%20autonomous%20agents.%20Existing%20CLIP-based%20open-vocabulary%20methods%0Asuccessfully%20perform%203D%20object%20retrieval%20with%20simple%20%28bare%29%20queries%20but%20cannot%0Acope%20with%20ambiguous%20descriptions%20that%20demand%20an%20understanding%20of%20object%0Arelations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20modular%20approach%20called%20BBQ%0A%28Beyond%20Bare%20Queries%29%2C%20which%20constructs%203D%20scene%20spatial%20graph%20representation%0Awith%20metric%20edges%20and%20utilizes%20a%20large%20language%20model%20as%20a%20human-to-agent%0Ainterface%20through%20our%20deductive%20scene%20reasoning%20algorithm.%20BBQ%20employs%20robust%0ADINO-powered%20associations%20to%20form%203D%20objects%2C%20an%20advanced%20raycasting%20algorithm%0Ato%20project%20them%20to%202D%2C%20and%20a%20vision-language%20model%20to%20describe%20them%20as%20graph%0Anodes.%20On%20Replica%20and%20ScanNet%20datasets%2C%20we%20show%20that%20the%20designed%20method%0Aaccurately%20constructs%203D%20object-centric%20maps.%20We%20have%20demonstrated%20that%20their%0Aquality%20takes%20a%20leading%20place%20for%20open-vocabulary%203D%20semantic%20segmentation%0Aagainst%20other%20zero-shot%20methods.%20Also%2C%20we%20show%20that%20leveraging%20spatial%0Arelations%20is%20especially%20effective%20for%20scenes%20containing%20multiple%20entities%20of%0Athe%20same%20semantic%20class.%20On%20Sr3D%20and%20Nr3D%20benchmarks%2C%20our%20deductive%20approach%0Ademonstrates%20a%20significant%20improvement%2C%20enabling%20retrieving%20objects%20by%20complex%0Aqueries%20compared%20to%20other%20state-of-the-art%20methods.%20Considering%20our%20design%0Asolutions%2C%20we%20achieved%20a%20processing%20speed%20approximately%20x3%20times%20faster%20than%0Athe%20closest%20analog.%20This%20promising%20performance%20enables%20our%20approach%20for%20usage%0Ain%20applied%20intelligent%20robotics%20projects.%20We%20make%20the%20code%20publicly%20available%0Aat%20linukc.github.io/bbq/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07113v1&entry.124074799=Read"},
{"title": "EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image\n  Visual Stimuli of Multi-Granularity Labels", "author": "Shuqi Zhu and Ziyi Ye and Qingyao Ai and Yiqun Liu", "abstract": "  Identifying and reconstructing what we see from brain activity gives us a\nspecial insight into investigating how the biological visual system represents\nthe world. While recent efforts have achieved high-performance image\nclassification and high-quality image reconstruction from brain signals\ncollected by Functional Magnetic Resonance Imaging (fMRI) or\nmagnetoencephalogram (MEG), the expensiveness and bulkiness of these devices\nmake relevant applications difficult to generalize to practical applications.\nOn the other hand, Electroencephalography (EEG), despite its advantages of ease\nof use, cost-efficiency, high temporal resolution, and non-invasive nature, has\nnot been fully explored in relevant studies due to the lack of comprehensive\ndatasets. To address this gap, we introduce EEG-ImageNet, a novel EEG dataset\ncomprising recordings from 16 subjects exposed to 4000 images selected from the\nImageNet dataset. EEG-ImageNet consists of 5 times EEG-image pairs larger than\nexisting similar EEG benchmarks. EEG-ImageNet is collected with image stimuli\nof multi-granularity labels, i.e., 40 images with coarse-grained labels and 40\nwith fine-grained labels. Based on it, we establish benchmarks for object\nclassification and image reconstruction. Experiments with several commonly used\nmodels show that the best models can achieve object classification with\naccuracy around 60% and image reconstruction with two-way identification around\n64%. These results demonstrate the dataset's potential to advance EEG-based\nvisual brain-computer interfaces, understand the visual perception of\nbiological systems, and provide potential applications in improving machine\nvisual models.\n", "link": "http://arxiv.org/abs/2406.07151v1", "date": "2024-06-11", "relevancy": 2.3985, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4834}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4806}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG-ImageNet%3A%20An%20Electroencephalogram%20Dataset%20and%20Benchmarks%20with%20Image%0A%20%20Visual%20Stimuli%20of%20Multi-Granularity%20Labels&body=Title%3A%20EEG-ImageNet%3A%20An%20Electroencephalogram%20Dataset%20and%20Benchmarks%20with%20Image%0A%20%20Visual%20Stimuli%20of%20Multi-Granularity%20Labels%0AAuthor%3A%20Shuqi%20Zhu%20and%20Ziyi%20Ye%20and%20Qingyao%20Ai%20and%20Yiqun%20Liu%0AAbstract%3A%20%20%20Identifying%20and%20reconstructing%20what%20we%20see%20from%20brain%20activity%20gives%20us%20a%0Aspecial%20insight%20into%20investigating%20how%20the%20biological%20visual%20system%20represents%0Athe%20world.%20While%20recent%20efforts%20have%20achieved%20high-performance%20image%0Aclassification%20and%20high-quality%20image%20reconstruction%20from%20brain%20signals%0Acollected%20by%20Functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20or%0Amagnetoencephalogram%20%28MEG%29%2C%20the%20expensiveness%20and%20bulkiness%20of%20these%20devices%0Amake%20relevant%20applications%20difficult%20to%20generalize%20to%20practical%20applications.%0AOn%20the%20other%20hand%2C%20Electroencephalography%20%28EEG%29%2C%20despite%20its%20advantages%20of%20ease%0Aof%20use%2C%20cost-efficiency%2C%20high%20temporal%20resolution%2C%20and%20non-invasive%20nature%2C%20has%0Anot%20been%20fully%20explored%20in%20relevant%20studies%20due%20to%20the%20lack%20of%20comprehensive%0Adatasets.%20To%20address%20this%20gap%2C%20we%20introduce%20EEG-ImageNet%2C%20a%20novel%20EEG%20dataset%0Acomprising%20recordings%20from%2016%20subjects%20exposed%20to%204000%20images%20selected%20from%20the%0AImageNet%20dataset.%20EEG-ImageNet%20consists%20of%205%20times%20EEG-image%20pairs%20larger%20than%0Aexisting%20similar%20EEG%20benchmarks.%20EEG-ImageNet%20is%20collected%20with%20image%20stimuli%0Aof%20multi-granularity%20labels%2C%20i.e.%2C%2040%20images%20with%20coarse-grained%20labels%20and%2040%0Awith%20fine-grained%20labels.%20Based%20on%20it%2C%20we%20establish%20benchmarks%20for%20object%0Aclassification%20and%20image%20reconstruction.%20Experiments%20with%20several%20commonly%20used%0Amodels%20show%20that%20the%20best%20models%20can%20achieve%20object%20classification%20with%0Aaccuracy%20around%2060%25%20and%20image%20reconstruction%20with%20two-way%20identification%20around%0A64%25.%20These%20results%20demonstrate%20the%20dataset%27s%20potential%20to%20advance%20EEG-based%0Avisual%20brain-computer%20interfaces%2C%20understand%20the%20visual%20perception%20of%0Abiological%20systems%2C%20and%20provide%20potential%20applications%20in%20improving%20machine%0Avisual%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG-ImageNet%253A%2520An%2520Electroencephalogram%2520Dataset%2520and%2520Benchmarks%2520with%2520Image%250A%2520%2520Visual%2520Stimuli%2520of%2520Multi-Granularity%2520Labels%26entry.906535625%3DShuqi%2520Zhu%2520and%2520Ziyi%2520Ye%2520and%2520Qingyao%2520Ai%2520and%2520Yiqun%2520Liu%26entry.1292438233%3D%2520%2520Identifying%2520and%2520reconstructing%2520what%2520we%2520see%2520from%2520brain%2520activity%2520gives%2520us%2520a%250Aspecial%2520insight%2520into%2520investigating%2520how%2520the%2520biological%2520visual%2520system%2520represents%250Athe%2520world.%2520While%2520recent%2520efforts%2520have%2520achieved%2520high-performance%2520image%250Aclassification%2520and%2520high-quality%2520image%2520reconstruction%2520from%2520brain%2520signals%250Acollected%2520by%2520Functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%2520or%250Amagnetoencephalogram%2520%2528MEG%2529%252C%2520the%2520expensiveness%2520and%2520bulkiness%2520of%2520these%2520devices%250Amake%2520relevant%2520applications%2520difficult%2520to%2520generalize%2520to%2520practical%2520applications.%250AOn%2520the%2520other%2520hand%252C%2520Electroencephalography%2520%2528EEG%2529%252C%2520despite%2520its%2520advantages%2520of%2520ease%250Aof%2520use%252C%2520cost-efficiency%252C%2520high%2520temporal%2520resolution%252C%2520and%2520non-invasive%2520nature%252C%2520has%250Anot%2520been%2520fully%2520explored%2520in%2520relevant%2520studies%2520due%2520to%2520the%2520lack%2520of%2520comprehensive%250Adatasets.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520EEG-ImageNet%252C%2520a%2520novel%2520EEG%2520dataset%250Acomprising%2520recordings%2520from%252016%2520subjects%2520exposed%2520to%25204000%2520images%2520selected%2520from%2520the%250AImageNet%2520dataset.%2520EEG-ImageNet%2520consists%2520of%25205%2520times%2520EEG-image%2520pairs%2520larger%2520than%250Aexisting%2520similar%2520EEG%2520benchmarks.%2520EEG-ImageNet%2520is%2520collected%2520with%2520image%2520stimuli%250Aof%2520multi-granularity%2520labels%252C%2520i.e.%252C%252040%2520images%2520with%2520coarse-grained%2520labels%2520and%252040%250Awith%2520fine-grained%2520labels.%2520Based%2520on%2520it%252C%2520we%2520establish%2520benchmarks%2520for%2520object%250Aclassification%2520and%2520image%2520reconstruction.%2520Experiments%2520with%2520several%2520commonly%2520used%250Amodels%2520show%2520that%2520the%2520best%2520models%2520can%2520achieve%2520object%2520classification%2520with%250Aaccuracy%2520around%252060%2525%2520and%2520image%2520reconstruction%2520with%2520two-way%2520identification%2520around%250A64%2525.%2520These%2520results%2520demonstrate%2520the%2520dataset%2527s%2520potential%2520to%2520advance%2520EEG-based%250Avisual%2520brain-computer%2520interfaces%252C%2520understand%2520the%2520visual%2520perception%2520of%250Abiological%2520systems%252C%2520and%2520provide%2520potential%2520applications%2520in%2520improving%2520machine%250Avisual%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG-ImageNet%3A%20An%20Electroencephalogram%20Dataset%20and%20Benchmarks%20with%20Image%0A%20%20Visual%20Stimuli%20of%20Multi-Granularity%20Labels&entry.906535625=Shuqi%20Zhu%20and%20Ziyi%20Ye%20and%20Qingyao%20Ai%20and%20Yiqun%20Liu&entry.1292438233=%20%20Identifying%20and%20reconstructing%20what%20we%20see%20from%20brain%20activity%20gives%20us%20a%0Aspecial%20insight%20into%20investigating%20how%20the%20biological%20visual%20system%20represents%0Athe%20world.%20While%20recent%20efforts%20have%20achieved%20high-performance%20image%0Aclassification%20and%20high-quality%20image%20reconstruction%20from%20brain%20signals%0Acollected%20by%20Functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20or%0Amagnetoencephalogram%20%28MEG%29%2C%20the%20expensiveness%20and%20bulkiness%20of%20these%20devices%0Amake%20relevant%20applications%20difficult%20to%20generalize%20to%20practical%20applications.%0AOn%20the%20other%20hand%2C%20Electroencephalography%20%28EEG%29%2C%20despite%20its%20advantages%20of%20ease%0Aof%20use%2C%20cost-efficiency%2C%20high%20temporal%20resolution%2C%20and%20non-invasive%20nature%2C%20has%0Anot%20been%20fully%20explored%20in%20relevant%20studies%20due%20to%20the%20lack%20of%20comprehensive%0Adatasets.%20To%20address%20this%20gap%2C%20we%20introduce%20EEG-ImageNet%2C%20a%20novel%20EEG%20dataset%0Acomprising%20recordings%20from%2016%20subjects%20exposed%20to%204000%20images%20selected%20from%20the%0AImageNet%20dataset.%20EEG-ImageNet%20consists%20of%205%20times%20EEG-image%20pairs%20larger%20than%0Aexisting%20similar%20EEG%20benchmarks.%20EEG-ImageNet%20is%20collected%20with%20image%20stimuli%0Aof%20multi-granularity%20labels%2C%20i.e.%2C%2040%20images%20with%20coarse-grained%20labels%20and%2040%0Awith%20fine-grained%20labels.%20Based%20on%20it%2C%20we%20establish%20benchmarks%20for%20object%0Aclassification%20and%20image%20reconstruction.%20Experiments%20with%20several%20commonly%20used%0Amodels%20show%20that%20the%20best%20models%20can%20achieve%20object%20classification%20with%0Aaccuracy%20around%2060%25%20and%20image%20reconstruction%20with%20two-way%20identification%20around%0A64%25.%20These%20results%20demonstrate%20the%20dataset%27s%20potential%20to%20advance%20EEG-based%0Avisual%20brain-computer%20interfaces%2C%20understand%20the%20visual%20perception%20of%0Abiological%20systems%2C%20and%20provide%20potential%20applications%20in%20improving%20machine%0Avisual%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07151v1&entry.124074799=Read"},
{"title": "Test-Driven Development for Code Generation", "author": "Noble Saji Mathews and Meiyappan Nagappan", "abstract": "  Recent Large Language Models (LLMs) have demonstrated significant\ncapabilities in generating code snippets directly from problem statements. This\nincreasingly automated process mirrors traditional human-led software\ndevelopment, where code is often written in response to a requirement.\nHistorically, Test-Driven Development (TDD) has proven its merit, requiring\ndevelopers to write tests before the functional code, ensuring alignment with\nthe initial problem statements. Applying TDD principles to LLM-based code\ngeneration offers one distinct benefit: it enables developers to verify the\ncorrectness of generated code against predefined tests. This paper investigates\nif and how TDD can be incorporated into AI-assisted code-generation processes.\nWe experimentally evaluate our hypothesis that providing LLMs like GPT-4 and\nLlama 3 with tests in addition to the problem statements enhances code\ngeneration outcomes. We experimented with established function-level code\ngeneration benchmarks such as MBPP and HumanEval. Our results consistently\ndemonstrate that including test cases leads to higher success in solving\nprogramming challenges. We assert that TDD is a promising paradigm for helping\nensure that the code generated by LLMs effectively captures the requirements.\n", "link": "http://arxiv.org/abs/2402.13521v2", "date": "2024-06-11", "relevancy": 2.388, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5107}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4681}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Driven%20Development%20for%20Code%20Generation&body=Title%3A%20Test-Driven%20Development%20for%20Code%20Generation%0AAuthor%3A%20Noble%20Saji%20Mathews%20and%20Meiyappan%20Nagappan%0AAbstract%3A%20%20%20Recent%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%0Acapabilities%20in%20generating%20code%20snippets%20directly%20from%20problem%20statements.%20This%0Aincreasingly%20automated%20process%20mirrors%20traditional%20human-led%20software%0Adevelopment%2C%20where%20code%20is%20often%20written%20in%20response%20to%20a%20requirement.%0AHistorically%2C%20Test-Driven%20Development%20%28TDD%29%20has%20proven%20its%20merit%2C%20requiring%0Adevelopers%20to%20write%20tests%20before%20the%20functional%20code%2C%20ensuring%20alignment%20with%0Athe%20initial%20problem%20statements.%20Applying%20TDD%20principles%20to%20LLM-based%20code%0Ageneration%20offers%20one%20distinct%20benefit%3A%20it%20enables%20developers%20to%20verify%20the%0Acorrectness%20of%20generated%20code%20against%20predefined%20tests.%20This%20paper%20investigates%0Aif%20and%20how%20TDD%20can%20be%20incorporated%20into%20AI-assisted%20code-generation%20processes.%0AWe%20experimentally%20evaluate%20our%20hypothesis%20that%20providing%20LLMs%20like%20GPT-4%20and%0ALlama%203%20with%20tests%20in%20addition%20to%20the%20problem%20statements%20enhances%20code%0Ageneration%20outcomes.%20We%20experimented%20with%20established%20function-level%20code%0Ageneration%20benchmarks%20such%20as%20MBPP%20and%20HumanEval.%20Our%20results%20consistently%0Ademonstrate%20that%20including%20test%20cases%20leads%20to%20higher%20success%20in%20solving%0Aprogramming%20challenges.%20We%20assert%20that%20TDD%20is%20a%20promising%20paradigm%20for%20helping%0Aensure%20that%20the%20code%20generated%20by%20LLMs%20effectively%20captures%20the%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Driven%2520Development%2520for%2520Code%2520Generation%26entry.906535625%3DNoble%2520Saji%2520Mathews%2520and%2520Meiyappan%2520Nagappan%26entry.1292438233%3D%2520%2520Recent%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520significant%250Acapabilities%2520in%2520generating%2520code%2520snippets%2520directly%2520from%2520problem%2520statements.%2520This%250Aincreasingly%2520automated%2520process%2520mirrors%2520traditional%2520human-led%2520software%250Adevelopment%252C%2520where%2520code%2520is%2520often%2520written%2520in%2520response%2520to%2520a%2520requirement.%250AHistorically%252C%2520Test-Driven%2520Development%2520%2528TDD%2529%2520has%2520proven%2520its%2520merit%252C%2520requiring%250Adevelopers%2520to%2520write%2520tests%2520before%2520the%2520functional%2520code%252C%2520ensuring%2520alignment%2520with%250Athe%2520initial%2520problem%2520statements.%2520Applying%2520TDD%2520principles%2520to%2520LLM-based%2520code%250Ageneration%2520offers%2520one%2520distinct%2520benefit%253A%2520it%2520enables%2520developers%2520to%2520verify%2520the%250Acorrectness%2520of%2520generated%2520code%2520against%2520predefined%2520tests.%2520This%2520paper%2520investigates%250Aif%2520and%2520how%2520TDD%2520can%2520be%2520incorporated%2520into%2520AI-assisted%2520code-generation%2520processes.%250AWe%2520experimentally%2520evaluate%2520our%2520hypothesis%2520that%2520providing%2520LLMs%2520like%2520GPT-4%2520and%250ALlama%25203%2520with%2520tests%2520in%2520addition%2520to%2520the%2520problem%2520statements%2520enhances%2520code%250Ageneration%2520outcomes.%2520We%2520experimented%2520with%2520established%2520function-level%2520code%250Ageneration%2520benchmarks%2520such%2520as%2520MBPP%2520and%2520HumanEval.%2520Our%2520results%2520consistently%250Ademonstrate%2520that%2520including%2520test%2520cases%2520leads%2520to%2520higher%2520success%2520in%2520solving%250Aprogramming%2520challenges.%2520We%2520assert%2520that%2520TDD%2520is%2520a%2520promising%2520paradigm%2520for%2520helping%250Aensure%2520that%2520the%2520code%2520generated%2520by%2520LLMs%2520effectively%2520captures%2520the%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Driven%20Development%20for%20Code%20Generation&entry.906535625=Noble%20Saji%20Mathews%20and%20Meiyappan%20Nagappan&entry.1292438233=%20%20Recent%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20significant%0Acapabilities%20in%20generating%20code%20snippets%20directly%20from%20problem%20statements.%20This%0Aincreasingly%20automated%20process%20mirrors%20traditional%20human-led%20software%0Adevelopment%2C%20where%20code%20is%20often%20written%20in%20response%20to%20a%20requirement.%0AHistorically%2C%20Test-Driven%20Development%20%28TDD%29%20has%20proven%20its%20merit%2C%20requiring%0Adevelopers%20to%20write%20tests%20before%20the%20functional%20code%2C%20ensuring%20alignment%20with%0Athe%20initial%20problem%20statements.%20Applying%20TDD%20principles%20to%20LLM-based%20code%0Ageneration%20offers%20one%20distinct%20benefit%3A%20it%20enables%20developers%20to%20verify%20the%0Acorrectness%20of%20generated%20code%20against%20predefined%20tests.%20This%20paper%20investigates%0Aif%20and%20how%20TDD%20can%20be%20incorporated%20into%20AI-assisted%20code-generation%20processes.%0AWe%20experimentally%20evaluate%20our%20hypothesis%20that%20providing%20LLMs%20like%20GPT-4%20and%0ALlama%203%20with%20tests%20in%20addition%20to%20the%20problem%20statements%20enhances%20code%0Ageneration%20outcomes.%20We%20experimented%20with%20established%20function-level%20code%0Ageneration%20benchmarks%20such%20as%20MBPP%20and%20HumanEval.%20Our%20results%20consistently%0Ademonstrate%20that%20including%20test%20cases%20leads%20to%20higher%20success%20in%20solving%0Aprogramming%20challenges.%20We%20assert%20that%20TDD%20is%20a%20promising%20paradigm%20for%20helping%0Aensure%20that%20the%20code%20generated%20by%20LLMs%20effectively%20captures%20the%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13521v2&entry.124074799=Read"},
{"title": "Fun with Flags: Robust Principal Directions via Flag Manifolds", "author": "Nathan Mankovich and Gustau Camps-Valls and Tolga Birdal", "abstract": "  Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.\n", "link": "http://arxiv.org/abs/2401.04071v3", "date": "2024-06-11", "relevancy": 2.3698, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4922}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4683}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fun%20with%20Flags%3A%20Robust%20Principal%20Directions%20via%20Flag%20Manifolds&body=Title%3A%20Fun%20with%20Flags%3A%20Robust%20Principal%20Directions%20via%20Flag%20Manifolds%0AAuthor%3A%20Nathan%20Mankovich%20and%20Gustau%20Camps-Valls%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Principal%20component%20analysis%20%28PCA%29%2C%20along%20with%20its%20extensions%20to%20manifolds%0Aand%20outlier%20contaminated%20data%2C%20have%20been%20indispensable%20in%20computer%20vision%20and%0Amachine%20learning.%20In%20this%20work%2C%20we%20present%20a%20unifying%20formalism%20for%20PCA%20and%20its%0Avariants%2C%20and%20introduce%20a%20framework%20based%20on%20the%20flags%20of%20linear%20subspaces%2C%20ie%0Aa%20hierarchy%20of%20nested%20linear%20subspaces%20of%20increasing%20dimension%2C%20which%20not%20only%0Aallows%20for%20a%20common%20implementation%20but%20also%20yields%20novel%20variants%2C%20not%20explored%0Apreviously.%20We%20begin%20by%20generalizing%20traditional%20PCA%20methods%20that%20either%0Amaximize%20variance%20or%20minimize%20reconstruction%20error.%20We%20expand%20these%0Ainterpretations%20to%20develop%20a%20wide%20array%20of%20new%20dimensionality%20reduction%0Aalgorithms%20by%20accounting%20for%20outliers%20and%20the%20data%20manifold.%20To%20devise%20a%20common%0Acomputational%20approach%2C%20we%20recast%20robust%20and%20dual%20forms%20of%20PCA%20as%20optimization%0Aproblems%20on%20flag%20manifolds.%20We%20then%20integrate%20tangent%20space%20approximations%20of%0Aprincipal%20geodesic%20analysis%20%28tangent-PCA%29%20into%20this%20flag-based%20framework%2C%0Acreating%20novel%20robust%20and%20dual%20geodesic%20PCA%20variations.%20The%20remarkable%0Aflexibility%20offered%20by%20the%20%27flagification%27%20introduced%20here%20enables%20even%20more%0Aalgorithmic%20variants%20identified%20by%20specific%20flag%20types.%20Last%20but%20not%20least%2C%20we%0Apropose%20an%20effective%20convergent%20solver%20for%20these%20flag-formulations%20employing%0Athe%20Stiefel%20manifold.%20Our%20empirical%20results%20on%20both%20real-world%20and%20synthetic%0Ascenarios%2C%20demonstrate%20the%20superiority%20of%20our%20novel%20algorithms%2C%20especially%20in%0Aterms%20of%20robustness%20to%20outliers%20on%20manifolds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04071v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFun%2520with%2520Flags%253A%2520Robust%2520Principal%2520Directions%2520via%2520Flag%2520Manifolds%26entry.906535625%3DNathan%2520Mankovich%2520and%2520Gustau%2520Camps-Valls%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Principal%2520component%2520analysis%2520%2528PCA%2529%252C%2520along%2520with%2520its%2520extensions%2520to%2520manifolds%250Aand%2520outlier%2520contaminated%2520data%252C%2520have%2520been%2520indispensable%2520in%2520computer%2520vision%2520and%250Amachine%2520learning.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520unifying%2520formalism%2520for%2520PCA%2520and%2520its%250Avariants%252C%2520and%2520introduce%2520a%2520framework%2520based%2520on%2520the%2520flags%2520of%2520linear%2520subspaces%252C%2520ie%250Aa%2520hierarchy%2520of%2520nested%2520linear%2520subspaces%2520of%2520increasing%2520dimension%252C%2520which%2520not%2520only%250Aallows%2520for%2520a%2520common%2520implementation%2520but%2520also%2520yields%2520novel%2520variants%252C%2520not%2520explored%250Apreviously.%2520We%2520begin%2520by%2520generalizing%2520traditional%2520PCA%2520methods%2520that%2520either%250Amaximize%2520variance%2520or%2520minimize%2520reconstruction%2520error.%2520We%2520expand%2520these%250Ainterpretations%2520to%2520develop%2520a%2520wide%2520array%2520of%2520new%2520dimensionality%2520reduction%250Aalgorithms%2520by%2520accounting%2520for%2520outliers%2520and%2520the%2520data%2520manifold.%2520To%2520devise%2520a%2520common%250Acomputational%2520approach%252C%2520we%2520recast%2520robust%2520and%2520dual%2520forms%2520of%2520PCA%2520as%2520optimization%250Aproblems%2520on%2520flag%2520manifolds.%2520We%2520then%2520integrate%2520tangent%2520space%2520approximations%2520of%250Aprincipal%2520geodesic%2520analysis%2520%2528tangent-PCA%2529%2520into%2520this%2520flag-based%2520framework%252C%250Acreating%2520novel%2520robust%2520and%2520dual%2520geodesic%2520PCA%2520variations.%2520The%2520remarkable%250Aflexibility%2520offered%2520by%2520the%2520%2527flagification%2527%2520introduced%2520here%2520enables%2520even%2520more%250Aalgorithmic%2520variants%2520identified%2520by%2520specific%2520flag%2520types.%2520Last%2520but%2520not%2520least%252C%2520we%250Apropose%2520an%2520effective%2520convergent%2520solver%2520for%2520these%2520flag-formulations%2520employing%250Athe%2520Stiefel%2520manifold.%2520Our%2520empirical%2520results%2520on%2520both%2520real-world%2520and%2520synthetic%250Ascenarios%252C%2520demonstrate%2520the%2520superiority%2520of%2520our%2520novel%2520algorithms%252C%2520especially%2520in%250Aterms%2520of%2520robustness%2520to%2520outliers%2520on%2520manifolds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04071v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fun%20with%20Flags%3A%20Robust%20Principal%20Directions%20via%20Flag%20Manifolds&entry.906535625=Nathan%20Mankovich%20and%20Gustau%20Camps-Valls%20and%20Tolga%20Birdal&entry.1292438233=%20%20Principal%20component%20analysis%20%28PCA%29%2C%20along%20with%20its%20extensions%20to%20manifolds%0Aand%20outlier%20contaminated%20data%2C%20have%20been%20indispensable%20in%20computer%20vision%20and%0Amachine%20learning.%20In%20this%20work%2C%20we%20present%20a%20unifying%20formalism%20for%20PCA%20and%20its%0Avariants%2C%20and%20introduce%20a%20framework%20based%20on%20the%20flags%20of%20linear%20subspaces%2C%20ie%0Aa%20hierarchy%20of%20nested%20linear%20subspaces%20of%20increasing%20dimension%2C%20which%20not%20only%0Aallows%20for%20a%20common%20implementation%20but%20also%20yields%20novel%20variants%2C%20not%20explored%0Apreviously.%20We%20begin%20by%20generalizing%20traditional%20PCA%20methods%20that%20either%0Amaximize%20variance%20or%20minimize%20reconstruction%20error.%20We%20expand%20these%0Ainterpretations%20to%20develop%20a%20wide%20array%20of%20new%20dimensionality%20reduction%0Aalgorithms%20by%20accounting%20for%20outliers%20and%20the%20data%20manifold.%20To%20devise%20a%20common%0Acomputational%20approach%2C%20we%20recast%20robust%20and%20dual%20forms%20of%20PCA%20as%20optimization%0Aproblems%20on%20flag%20manifolds.%20We%20then%20integrate%20tangent%20space%20approximations%20of%0Aprincipal%20geodesic%20analysis%20%28tangent-PCA%29%20into%20this%20flag-based%20framework%2C%0Acreating%20novel%20robust%20and%20dual%20geodesic%20PCA%20variations.%20The%20remarkable%0Aflexibility%20offered%20by%20the%20%27flagification%27%20introduced%20here%20enables%20even%20more%0Aalgorithmic%20variants%20identified%20by%20specific%20flag%20types.%20Last%20but%20not%20least%2C%20we%0Apropose%20an%20effective%20convergent%20solver%20for%20these%20flag-formulations%20employing%0Athe%20Stiefel%20manifold.%20Our%20empirical%20results%20on%20both%20real-world%20and%20synthetic%0Ascenarios%2C%20demonstrate%20the%20superiority%20of%20our%20novel%20algorithms%2C%20especially%20in%0Aterms%20of%20robustness%20to%20outliers%20on%20manifolds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04071v3&entry.124074799=Read"},
{"title": "A benchmark dataset for deep learning-based airplane detection: HRPlanes", "author": "Tolga Bakirman and Elif Sertel", "abstract": "  Airplane detection from satellite imagery is a challenging task due to the\ncomplex backgrounds in the images and differences in data acquisition\nconditions caused by the sensor geometry and atmospheric effects. Deep learning\nmethods provide reliable and accurate solutions for automatic detection of\nairplanes; however, huge amount of training data is required to obtain\npromising results. In this study, we create a novel airplane detection dataset\ncalled High Resolution Planes (HRPlanes) by using images from Google Earth (GE)\nand labeling the bounding box of each plane on the images. HRPlanes include GE\nimages of several different airports across the world to represent a variety of\nlandscape, seasonal and satellite geometry conditions obtained from different\nsatellites. We evaluated our dataset with two widely used object detection\nmethods namely YOLOv4 and Faster R-CNN. Our preliminary results show that the\nproposed dataset can be a valuable data source and benchmark data set for\nfuture applications. Moreover, proposed architectures and results of this study\ncould be used for transfer learning of different datasets and models for\nairplane detection.\n", "link": "http://arxiv.org/abs/2204.10959v2", "date": "2024-06-11", "relevancy": 2.3686, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4992}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20benchmark%20dataset%20for%20deep%20learning-based%20airplane%20detection%3A%20HRPlanes&body=Title%3A%20A%20benchmark%20dataset%20for%20deep%20learning-based%20airplane%20detection%3A%20HRPlanes%0AAuthor%3A%20Tolga%20Bakirman%20and%20Elif%20Sertel%0AAbstract%3A%20%20%20Airplane%20detection%20from%20satellite%20imagery%20is%20a%20challenging%20task%20due%20to%20the%0Acomplex%20backgrounds%20in%20the%20images%20and%20differences%20in%20data%20acquisition%0Aconditions%20caused%20by%20the%20sensor%20geometry%20and%20atmospheric%20effects.%20Deep%20learning%0Amethods%20provide%20reliable%20and%20accurate%20solutions%20for%20automatic%20detection%20of%0Aairplanes%3B%20however%2C%20huge%20amount%20of%20training%20data%20is%20required%20to%20obtain%0Apromising%20results.%20In%20this%20study%2C%20we%20create%20a%20novel%20airplane%20detection%20dataset%0Acalled%20High%20Resolution%20Planes%20%28HRPlanes%29%20by%20using%20images%20from%20Google%20Earth%20%28GE%29%0Aand%20labeling%20the%20bounding%20box%20of%20each%20plane%20on%20the%20images.%20HRPlanes%20include%20GE%0Aimages%20of%20several%20different%20airports%20across%20the%20world%20to%20represent%20a%20variety%20of%0Alandscape%2C%20seasonal%20and%20satellite%20geometry%20conditions%20obtained%20from%20different%0Asatellites.%20We%20evaluated%20our%20dataset%20with%20two%20widely%20used%20object%20detection%0Amethods%20namely%20YOLOv4%20and%20Faster%20R-CNN.%20Our%20preliminary%20results%20show%20that%20the%0Aproposed%20dataset%20can%20be%20a%20valuable%20data%20source%20and%20benchmark%20data%20set%20for%0Afuture%20applications.%20Moreover%2C%20proposed%20architectures%20and%20results%20of%20this%20study%0Acould%20be%20used%20for%20transfer%20learning%20of%20different%20datasets%20and%20models%20for%0Aairplane%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.10959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520benchmark%2520dataset%2520for%2520deep%2520learning-based%2520airplane%2520detection%253A%2520HRPlanes%26entry.906535625%3DTolga%2520Bakirman%2520and%2520Elif%2520Sertel%26entry.1292438233%3D%2520%2520Airplane%2520detection%2520from%2520satellite%2520imagery%2520is%2520a%2520challenging%2520task%2520due%2520to%2520the%250Acomplex%2520backgrounds%2520in%2520the%2520images%2520and%2520differences%2520in%2520data%2520acquisition%250Aconditions%2520caused%2520by%2520the%2520sensor%2520geometry%2520and%2520atmospheric%2520effects.%2520Deep%2520learning%250Amethods%2520provide%2520reliable%2520and%2520accurate%2520solutions%2520for%2520automatic%2520detection%2520of%250Aairplanes%253B%2520however%252C%2520huge%2520amount%2520of%2520training%2520data%2520is%2520required%2520to%2520obtain%250Apromising%2520results.%2520In%2520this%2520study%252C%2520we%2520create%2520a%2520novel%2520airplane%2520detection%2520dataset%250Acalled%2520High%2520Resolution%2520Planes%2520%2528HRPlanes%2529%2520by%2520using%2520images%2520from%2520Google%2520Earth%2520%2528GE%2529%250Aand%2520labeling%2520the%2520bounding%2520box%2520of%2520each%2520plane%2520on%2520the%2520images.%2520HRPlanes%2520include%2520GE%250Aimages%2520of%2520several%2520different%2520airports%2520across%2520the%2520world%2520to%2520represent%2520a%2520variety%2520of%250Alandscape%252C%2520seasonal%2520and%2520satellite%2520geometry%2520conditions%2520obtained%2520from%2520different%250Asatellites.%2520We%2520evaluated%2520our%2520dataset%2520with%2520two%2520widely%2520used%2520object%2520detection%250Amethods%2520namely%2520YOLOv4%2520and%2520Faster%2520R-CNN.%2520Our%2520preliminary%2520results%2520show%2520that%2520the%250Aproposed%2520dataset%2520can%2520be%2520a%2520valuable%2520data%2520source%2520and%2520benchmark%2520data%2520set%2520for%250Afuture%2520applications.%2520Moreover%252C%2520proposed%2520architectures%2520and%2520results%2520of%2520this%2520study%250Acould%2520be%2520used%2520for%2520transfer%2520learning%2520of%2520different%2520datasets%2520and%2520models%2520for%250Aairplane%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.10959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20benchmark%20dataset%20for%20deep%20learning-based%20airplane%20detection%3A%20HRPlanes&entry.906535625=Tolga%20Bakirman%20and%20Elif%20Sertel&entry.1292438233=%20%20Airplane%20detection%20from%20satellite%20imagery%20is%20a%20challenging%20task%20due%20to%20the%0Acomplex%20backgrounds%20in%20the%20images%20and%20differences%20in%20data%20acquisition%0Aconditions%20caused%20by%20the%20sensor%20geometry%20and%20atmospheric%20effects.%20Deep%20learning%0Amethods%20provide%20reliable%20and%20accurate%20solutions%20for%20automatic%20detection%20of%0Aairplanes%3B%20however%2C%20huge%20amount%20of%20training%20data%20is%20required%20to%20obtain%0Apromising%20results.%20In%20this%20study%2C%20we%20create%20a%20novel%20airplane%20detection%20dataset%0Acalled%20High%20Resolution%20Planes%20%28HRPlanes%29%20by%20using%20images%20from%20Google%20Earth%20%28GE%29%0Aand%20labeling%20the%20bounding%20box%20of%20each%20plane%20on%20the%20images.%20HRPlanes%20include%20GE%0Aimages%20of%20several%20different%20airports%20across%20the%20world%20to%20represent%20a%20variety%20of%0Alandscape%2C%20seasonal%20and%20satellite%20geometry%20conditions%20obtained%20from%20different%0Asatellites.%20We%20evaluated%20our%20dataset%20with%20two%20widely%20used%20object%20detection%0Amethods%20namely%20YOLOv4%20and%20Faster%20R-CNN.%20Our%20preliminary%20results%20show%20that%20the%0Aproposed%20dataset%20can%20be%20a%20valuable%20data%20source%20and%20benchmark%20data%20set%20for%0Afuture%20applications.%20Moreover%2C%20proposed%20architectures%20and%20results%20of%20this%20study%0Acould%20be%20used%20for%20transfer%20learning%20of%20different%20datasets%20and%20models%20for%0Aairplane%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.10959v2&entry.124074799=Read"},
{"title": "Vision Model Pre-training on Interleaved Image-Text Data via Latent\n  Compression Learning", "author": "Chenyu Yang and Xizhou Zhu and Jinguo Zhu and Weijie Su and Junjie Wang and Xuan Dong and Wenhai Wang and Lewei Lu and Bin Li and Jie Zhou and Yu Qiao and Jifeng Dai", "abstract": "  Recently, vision model pre-training has evolved from relying on manually\nannotated datasets to leveraging large-scale, web-crawled image-text data.\nDespite these advances, there is no pre-training method that effectively\nexploits the interleaved image-text data, which is very prevalent on the\nInternet. Inspired by the recent success of compression learning in natural\nlanguage processing, we propose a novel vision model pre-training method called\nLatent Compression Learning (LCL) for interleaved image-text data. This method\nperforms latent compression learning by maximizing the mutual information\nbetween the inputs and outputs of a causal attention model. The training\nobjective can be decomposed into two basic tasks: 1) contrastive learning\nbetween visual representation and preceding context, and 2) generating\nsubsequent text based on visual representation. Our experiments demonstrate\nthat our method not only matches the performance of CLIP on paired pre-training\ndatasets (e.g., LAION), but can also leverage interleaved pre-training data\n(e.g., MMC4) to learn robust visual representation from scratch, showcasing the\npotential of vision model pre-training with interleaved image-text data. Code\nis released at https://github.com/OpenGVLab/LCL.\n", "link": "http://arxiv.org/abs/2406.07543v1", "date": "2024-06-11", "relevancy": 2.3305, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6172}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5595}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Model%20Pre-training%20on%20Interleaved%20Image-Text%20Data%20via%20Latent%0A%20%20Compression%20Learning&body=Title%3A%20Vision%20Model%20Pre-training%20on%20Interleaved%20Image-Text%20Data%20via%20Latent%0A%20%20Compression%20Learning%0AAuthor%3A%20Chenyu%20Yang%20and%20Xizhou%20Zhu%20and%20Jinguo%20Zhu%20and%20Weijie%20Su%20and%20Junjie%20Wang%20and%20Xuan%20Dong%20and%20Wenhai%20Wang%20and%20Lewei%20Lu%20and%20Bin%20Li%20and%20Jie%20Zhou%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20Recently%2C%20vision%20model%20pre-training%20has%20evolved%20from%20relying%20on%20manually%0Aannotated%20datasets%20to%20leveraging%20large-scale%2C%20web-crawled%20image-text%20data.%0ADespite%20these%20advances%2C%20there%20is%20no%20pre-training%20method%20that%20effectively%0Aexploits%20the%20interleaved%20image-text%20data%2C%20which%20is%20very%20prevalent%20on%20the%0AInternet.%20Inspired%20by%20the%20recent%20success%20of%20compression%20learning%20in%20natural%0Alanguage%20processing%2C%20we%20propose%20a%20novel%20vision%20model%20pre-training%20method%20called%0ALatent%20Compression%20Learning%20%28LCL%29%20for%20interleaved%20image-text%20data.%20This%20method%0Aperforms%20latent%20compression%20learning%20by%20maximizing%20the%20mutual%20information%0Abetween%20the%20inputs%20and%20outputs%20of%20a%20causal%20attention%20model.%20The%20training%0Aobjective%20can%20be%20decomposed%20into%20two%20basic%20tasks%3A%201%29%20contrastive%20learning%0Abetween%20visual%20representation%20and%20preceding%20context%2C%20and%202%29%20generating%0Asubsequent%20text%20based%20on%20visual%20representation.%20Our%20experiments%20demonstrate%0Athat%20our%20method%20not%20only%20matches%20the%20performance%20of%20CLIP%20on%20paired%20pre-training%0Adatasets%20%28e.g.%2C%20LAION%29%2C%20but%20can%20also%20leverage%20interleaved%20pre-training%20data%0A%28e.g.%2C%20MMC4%29%20to%20learn%20robust%20visual%20representation%20from%20scratch%2C%20showcasing%20the%0Apotential%20of%20vision%20model%20pre-training%20with%20interleaved%20image-text%20data.%20Code%0Ais%20released%20at%20https%3A//github.com/OpenGVLab/LCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Model%2520Pre-training%2520on%2520Interleaved%2520Image-Text%2520Data%2520via%2520Latent%250A%2520%2520Compression%2520Learning%26entry.906535625%3DChenyu%2520Yang%2520and%2520Xizhou%2520Zhu%2520and%2520Jinguo%2520Zhu%2520and%2520Weijie%2520Su%2520and%2520Junjie%2520Wang%2520and%2520Xuan%2520Dong%2520and%2520Wenhai%2520Wang%2520and%2520Lewei%2520Lu%2520and%2520Bin%2520Li%2520and%2520Jie%2520Zhou%2520and%2520Yu%2520Qiao%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520Recently%252C%2520vision%2520model%2520pre-training%2520has%2520evolved%2520from%2520relying%2520on%2520manually%250Aannotated%2520datasets%2520to%2520leveraging%2520large-scale%252C%2520web-crawled%2520image-text%2520data.%250ADespite%2520these%2520advances%252C%2520there%2520is%2520no%2520pre-training%2520method%2520that%2520effectively%250Aexploits%2520the%2520interleaved%2520image-text%2520data%252C%2520which%2520is%2520very%2520prevalent%2520on%2520the%250AInternet.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520compression%2520learning%2520in%2520natural%250Alanguage%2520processing%252C%2520we%2520propose%2520a%2520novel%2520vision%2520model%2520pre-training%2520method%2520called%250ALatent%2520Compression%2520Learning%2520%2528LCL%2529%2520for%2520interleaved%2520image-text%2520data.%2520This%2520method%250Aperforms%2520latent%2520compression%2520learning%2520by%2520maximizing%2520the%2520mutual%2520information%250Abetween%2520the%2520inputs%2520and%2520outputs%2520of%2520a%2520causal%2520attention%2520model.%2520The%2520training%250Aobjective%2520can%2520be%2520decomposed%2520into%2520two%2520basic%2520tasks%253A%25201%2529%2520contrastive%2520learning%250Abetween%2520visual%2520representation%2520and%2520preceding%2520context%252C%2520and%25202%2529%2520generating%250Asubsequent%2520text%2520based%2520on%2520visual%2520representation.%2520Our%2520experiments%2520demonstrate%250Athat%2520our%2520method%2520not%2520only%2520matches%2520the%2520performance%2520of%2520CLIP%2520on%2520paired%2520pre-training%250Adatasets%2520%2528e.g.%252C%2520LAION%2529%252C%2520but%2520can%2520also%2520leverage%2520interleaved%2520pre-training%2520data%250A%2528e.g.%252C%2520MMC4%2529%2520to%2520learn%2520robust%2520visual%2520representation%2520from%2520scratch%252C%2520showcasing%2520the%250Apotential%2520of%2520vision%2520model%2520pre-training%2520with%2520interleaved%2520image-text%2520data.%2520Code%250Ais%2520released%2520at%2520https%253A//github.com/OpenGVLab/LCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Model%20Pre-training%20on%20Interleaved%20Image-Text%20Data%20via%20Latent%0A%20%20Compression%20Learning&entry.906535625=Chenyu%20Yang%20and%20Xizhou%20Zhu%20and%20Jinguo%20Zhu%20and%20Weijie%20Su%20and%20Junjie%20Wang%20and%20Xuan%20Dong%20and%20Wenhai%20Wang%20and%20Lewei%20Lu%20and%20Bin%20Li%20and%20Jie%20Zhou%20and%20Yu%20Qiao%20and%20Jifeng%20Dai&entry.1292438233=%20%20Recently%2C%20vision%20model%20pre-training%20has%20evolved%20from%20relying%20on%20manually%0Aannotated%20datasets%20to%20leveraging%20large-scale%2C%20web-crawled%20image-text%20data.%0ADespite%20these%20advances%2C%20there%20is%20no%20pre-training%20method%20that%20effectively%0Aexploits%20the%20interleaved%20image-text%20data%2C%20which%20is%20very%20prevalent%20on%20the%0AInternet.%20Inspired%20by%20the%20recent%20success%20of%20compression%20learning%20in%20natural%0Alanguage%20processing%2C%20we%20propose%20a%20novel%20vision%20model%20pre-training%20method%20called%0ALatent%20Compression%20Learning%20%28LCL%29%20for%20interleaved%20image-text%20data.%20This%20method%0Aperforms%20latent%20compression%20learning%20by%20maximizing%20the%20mutual%20information%0Abetween%20the%20inputs%20and%20outputs%20of%20a%20causal%20attention%20model.%20The%20training%0Aobjective%20can%20be%20decomposed%20into%20two%20basic%20tasks%3A%201%29%20contrastive%20learning%0Abetween%20visual%20representation%20and%20preceding%20context%2C%20and%202%29%20generating%0Asubsequent%20text%20based%20on%20visual%20representation.%20Our%20experiments%20demonstrate%0Athat%20our%20method%20not%20only%20matches%20the%20performance%20of%20CLIP%20on%20paired%20pre-training%0Adatasets%20%28e.g.%2C%20LAION%29%2C%20but%20can%20also%20leverage%20interleaved%20pre-training%20data%0A%28e.g.%2C%20MMC4%29%20to%20learn%20robust%20visual%20representation%20from%20scratch%2C%20showcasing%20the%0Apotential%20of%20vision%20model%20pre-training%20with%20interleaved%20image-text%20data.%20Code%0Ais%20released%20at%20https%3A//github.com/OpenGVLab/LCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07543v1&entry.124074799=Read"},
{"title": "Clifford-Steerable Convolutional Neural Networks", "author": "Maksim Zhdanov and David Ruhe and Maurice Weiler and Ana Lucic and Johannes Brandstetter and Patrick Forr\u00e9", "abstract": "  We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a\nnovel class of $\\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector\nfields on pseudo-Euclidean spaces $\\mathbb{R}^{p,q}$. They cover, for instance,\n$\\mathrm{E}(3)$-equivariance on $\\mathbb{R}^3$ and Poincar\\'e-equivariance on\nMinkowski spacetime $\\mathbb{R}^{1,3}$. Our approach is based on an implicit\nparametrization of $\\mathrm{O}(p,q)$-steerable kernels via Clifford group\nequivariant neural networks. We significantly and consistently outperform\nbaseline methods on fluid dynamics as well as relativistic electrodynamics\nforecasting tasks.\n", "link": "http://arxiv.org/abs/2402.14730v2", "date": "2024-06-11", "relevancy": 2.3242, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4709}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4694}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clifford-Steerable%20Convolutional%20Neural%20Networks&body=Title%3A%20Clifford-Steerable%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Maksim%20Zhdanov%20and%20David%20Ruhe%20and%20Maurice%20Weiler%20and%20Ana%20Lucic%20and%20Johannes%20Brandstetter%20and%20Patrick%20Forr%C3%A9%0AAbstract%3A%20%20%20We%20present%20Clifford-Steerable%20Convolutional%20Neural%20Networks%20%28CS-CNNs%29%2C%20a%0Anovel%20class%20of%20%24%5Cmathrm%7BE%7D%28p%2C%20q%29%24-equivariant%20CNNs.%20CS-CNNs%20process%20multivector%0Afields%20on%20pseudo-Euclidean%20spaces%20%24%5Cmathbb%7BR%7D%5E%7Bp%2Cq%7D%24.%20They%20cover%2C%20for%20instance%2C%0A%24%5Cmathrm%7BE%7D%283%29%24-equivariance%20on%20%24%5Cmathbb%7BR%7D%5E3%24%20and%20Poincar%5C%27e-equivariance%20on%0AMinkowski%20spacetime%20%24%5Cmathbb%7BR%7D%5E%7B1%2C3%7D%24.%20Our%20approach%20is%20based%20on%20an%20implicit%0Aparametrization%20of%20%24%5Cmathrm%7BO%7D%28p%2Cq%29%24-steerable%20kernels%20via%20Clifford%20group%0Aequivariant%20neural%20networks.%20We%20significantly%20and%20consistently%20outperform%0Abaseline%20methods%20on%20fluid%20dynamics%20as%20well%20as%20relativistic%20electrodynamics%0Aforecasting%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClifford-Steerable%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DMaksim%2520Zhdanov%2520and%2520David%2520Ruhe%2520and%2520Maurice%2520Weiler%2520and%2520Ana%2520Lucic%2520and%2520Johannes%2520Brandstetter%2520and%2520Patrick%2520Forr%25C3%25A9%26entry.1292438233%3D%2520%2520We%2520present%2520Clifford-Steerable%2520Convolutional%2520Neural%2520Networks%2520%2528CS-CNNs%2529%252C%2520a%250Anovel%2520class%2520of%2520%2524%255Cmathrm%257BE%257D%2528p%252C%2520q%2529%2524-equivariant%2520CNNs.%2520CS-CNNs%2520process%2520multivector%250Afields%2520on%2520pseudo-Euclidean%2520spaces%2520%2524%255Cmathbb%257BR%257D%255E%257Bp%252Cq%257D%2524.%2520They%2520cover%252C%2520for%2520instance%252C%250A%2524%255Cmathrm%257BE%257D%25283%2529%2524-equivariance%2520on%2520%2524%255Cmathbb%257BR%257D%255E3%2524%2520and%2520Poincar%255C%2527e-equivariance%2520on%250AMinkowski%2520spacetime%2520%2524%255Cmathbb%257BR%257D%255E%257B1%252C3%257D%2524.%2520Our%2520approach%2520is%2520based%2520on%2520an%2520implicit%250Aparametrization%2520of%2520%2524%255Cmathrm%257BO%257D%2528p%252Cq%2529%2524-steerable%2520kernels%2520via%2520Clifford%2520group%250Aequivariant%2520neural%2520networks.%2520We%2520significantly%2520and%2520consistently%2520outperform%250Abaseline%2520methods%2520on%2520fluid%2520dynamics%2520as%2520well%2520as%2520relativistic%2520electrodynamics%250Aforecasting%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clifford-Steerable%20Convolutional%20Neural%20Networks&entry.906535625=Maksim%20Zhdanov%20and%20David%20Ruhe%20and%20Maurice%20Weiler%20and%20Ana%20Lucic%20and%20Johannes%20Brandstetter%20and%20Patrick%20Forr%C3%A9&entry.1292438233=%20%20We%20present%20Clifford-Steerable%20Convolutional%20Neural%20Networks%20%28CS-CNNs%29%2C%20a%0Anovel%20class%20of%20%24%5Cmathrm%7BE%7D%28p%2C%20q%29%24-equivariant%20CNNs.%20CS-CNNs%20process%20multivector%0Afields%20on%20pseudo-Euclidean%20spaces%20%24%5Cmathbb%7BR%7D%5E%7Bp%2Cq%7D%24.%20They%20cover%2C%20for%20instance%2C%0A%24%5Cmathrm%7BE%7D%283%29%24-equivariance%20on%20%24%5Cmathbb%7BR%7D%5E3%24%20and%20Poincar%5C%27e-equivariance%20on%0AMinkowski%20spacetime%20%24%5Cmathbb%7BR%7D%5E%7B1%2C3%7D%24.%20Our%20approach%20is%20based%20on%20an%20implicit%0Aparametrization%20of%20%24%5Cmathrm%7BO%7D%28p%2Cq%29%24-steerable%20kernels%20via%20Clifford%20group%0Aequivariant%20neural%20networks.%20We%20significantly%20and%20consistently%20outperform%0Abaseline%20methods%20on%20fluid%20dynamics%20as%20well%20as%20relativistic%20electrodynamics%0Aforecasting%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14730v2&entry.124074799=Read"},
{"title": "Closing the Computational-Query Depth Gap in Parallel Stochastic Convex\n  Optimization", "author": "Arun Jambulapati and Aaron Sidford and Kevin Tian", "abstract": "  We develop a new parallel algorithm for minimizing Lipschitz, convex\nfunctions with a stochastic subgradient oracle. The total number of queries\nmade and the query depth, i.e., the number of parallel rounds of queries, match\nthe prior state-of-the-art, [CJJLLST23], while improving upon the computational\ndepth by a polynomial factor for sufficiently small accuracy. When combined\nwith previous state-of-the-art methods our result closes a gap between the\nbest-known query depth and the best-known computational depth of parallel\nalgorithms.\n  Our method starts with a ball acceleration framework of previous parallel\nmethods, i.e., [CJJJLST20, ACJJS21], which reduce the problem to minimizing a\nregularized Gaussian convolution of the function constrained to Euclidean\nballs. By developing and leveraging new stability properties of the Hessian of\nthis induced function, we depart from prior parallel algorithms and reduce\nthese ball-constrained optimization problems to stochastic unconstrained\nquadratic minimization problems. Although we are unable to prove concentration\nof the asymmetric matrices that we use to approximate this Hessian, we\nnevertheless develop an efficient parallel method for solving these quadratics.\nInterestingly, our algorithms can be improved using fast matrix multiplication\nand use nearly-linear work if the matrix multiplication exponent is 2.\n", "link": "http://arxiv.org/abs/2406.07373v1", "date": "2024-06-11", "relevancy": 2.2924, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4596}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Computational-Query%20Depth%20Gap%20in%20Parallel%20Stochastic%20Convex%0A%20%20Optimization&body=Title%3A%20Closing%20the%20Computational-Query%20Depth%20Gap%20in%20Parallel%20Stochastic%20Convex%0A%20%20Optimization%0AAuthor%3A%20Arun%20Jambulapati%20and%20Aaron%20Sidford%20and%20Kevin%20Tian%0AAbstract%3A%20%20%20We%20develop%20a%20new%20parallel%20algorithm%20for%20minimizing%20Lipschitz%2C%20convex%0Afunctions%20with%20a%20stochastic%20subgradient%20oracle.%20The%20total%20number%20of%20queries%0Amade%20and%20the%20query%20depth%2C%20i.e.%2C%20the%20number%20of%20parallel%20rounds%20of%20queries%2C%20match%0Athe%20prior%20state-of-the-art%2C%20%5BCJJLLST23%5D%2C%20while%20improving%20upon%20the%20computational%0Adepth%20by%20a%20polynomial%20factor%20for%20sufficiently%20small%20accuracy.%20When%20combined%0Awith%20previous%20state-of-the-art%20methods%20our%20result%20closes%20a%20gap%20between%20the%0Abest-known%20query%20depth%20and%20the%20best-known%20computational%20depth%20of%20parallel%0Aalgorithms.%0A%20%20Our%20method%20starts%20with%20a%20ball%20acceleration%20framework%20of%20previous%20parallel%0Amethods%2C%20i.e.%2C%20%5BCJJJLST20%2C%20ACJJS21%5D%2C%20which%20reduce%20the%20problem%20to%20minimizing%20a%0Aregularized%20Gaussian%20convolution%20of%20the%20function%20constrained%20to%20Euclidean%0Aballs.%20By%20developing%20and%20leveraging%20new%20stability%20properties%20of%20the%20Hessian%20of%0Athis%20induced%20function%2C%20we%20depart%20from%20prior%20parallel%20algorithms%20and%20reduce%0Athese%20ball-constrained%20optimization%20problems%20to%20stochastic%20unconstrained%0Aquadratic%20minimization%20problems.%20Although%20we%20are%20unable%20to%20prove%20concentration%0Aof%20the%20asymmetric%20matrices%20that%20we%20use%20to%20approximate%20this%20Hessian%2C%20we%0Anevertheless%20develop%20an%20efficient%20parallel%20method%20for%20solving%20these%20quadratics.%0AInterestingly%2C%20our%20algorithms%20can%20be%20improved%20using%20fast%20matrix%20multiplication%0Aand%20use%20nearly-linear%20work%20if%20the%20matrix%20multiplication%20exponent%20is%202.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520Computational-Query%2520Depth%2520Gap%2520in%2520Parallel%2520Stochastic%2520Convex%250A%2520%2520Optimization%26entry.906535625%3DArun%2520Jambulapati%2520and%2520Aaron%2520Sidford%2520and%2520Kevin%2520Tian%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520new%2520parallel%2520algorithm%2520for%2520minimizing%2520Lipschitz%252C%2520convex%250Afunctions%2520with%2520a%2520stochastic%2520subgradient%2520oracle.%2520The%2520total%2520number%2520of%2520queries%250Amade%2520and%2520the%2520query%2520depth%252C%2520i.e.%252C%2520the%2520number%2520of%2520parallel%2520rounds%2520of%2520queries%252C%2520match%250Athe%2520prior%2520state-of-the-art%252C%2520%255BCJJLLST23%255D%252C%2520while%2520improving%2520upon%2520the%2520computational%250Adepth%2520by%2520a%2520polynomial%2520factor%2520for%2520sufficiently%2520small%2520accuracy.%2520When%2520combined%250Awith%2520previous%2520state-of-the-art%2520methods%2520our%2520result%2520closes%2520a%2520gap%2520between%2520the%250Abest-known%2520query%2520depth%2520and%2520the%2520best-known%2520computational%2520depth%2520of%2520parallel%250Aalgorithms.%250A%2520%2520Our%2520method%2520starts%2520with%2520a%2520ball%2520acceleration%2520framework%2520of%2520previous%2520parallel%250Amethods%252C%2520i.e.%252C%2520%255BCJJJLST20%252C%2520ACJJS21%255D%252C%2520which%2520reduce%2520the%2520problem%2520to%2520minimizing%2520a%250Aregularized%2520Gaussian%2520convolution%2520of%2520the%2520function%2520constrained%2520to%2520Euclidean%250Aballs.%2520By%2520developing%2520and%2520leveraging%2520new%2520stability%2520properties%2520of%2520the%2520Hessian%2520of%250Athis%2520induced%2520function%252C%2520we%2520depart%2520from%2520prior%2520parallel%2520algorithms%2520and%2520reduce%250Athese%2520ball-constrained%2520optimization%2520problems%2520to%2520stochastic%2520unconstrained%250Aquadratic%2520minimization%2520problems.%2520Although%2520we%2520are%2520unable%2520to%2520prove%2520concentration%250Aof%2520the%2520asymmetric%2520matrices%2520that%2520we%2520use%2520to%2520approximate%2520this%2520Hessian%252C%2520we%250Anevertheless%2520develop%2520an%2520efficient%2520parallel%2520method%2520for%2520solving%2520these%2520quadratics.%250AInterestingly%252C%2520our%2520algorithms%2520can%2520be%2520improved%2520using%2520fast%2520matrix%2520multiplication%250Aand%2520use%2520nearly-linear%2520work%2520if%2520the%2520matrix%2520multiplication%2520exponent%2520is%25202.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Computational-Query%20Depth%20Gap%20in%20Parallel%20Stochastic%20Convex%0A%20%20Optimization&entry.906535625=Arun%20Jambulapati%20and%20Aaron%20Sidford%20and%20Kevin%20Tian&entry.1292438233=%20%20We%20develop%20a%20new%20parallel%20algorithm%20for%20minimizing%20Lipschitz%2C%20convex%0Afunctions%20with%20a%20stochastic%20subgradient%20oracle.%20The%20total%20number%20of%20queries%0Amade%20and%20the%20query%20depth%2C%20i.e.%2C%20the%20number%20of%20parallel%20rounds%20of%20queries%2C%20match%0Athe%20prior%20state-of-the-art%2C%20%5BCJJLLST23%5D%2C%20while%20improving%20upon%20the%20computational%0Adepth%20by%20a%20polynomial%20factor%20for%20sufficiently%20small%20accuracy.%20When%20combined%0Awith%20previous%20state-of-the-art%20methods%20our%20result%20closes%20a%20gap%20between%20the%0Abest-known%20query%20depth%20and%20the%20best-known%20computational%20depth%20of%20parallel%0Aalgorithms.%0A%20%20Our%20method%20starts%20with%20a%20ball%20acceleration%20framework%20of%20previous%20parallel%0Amethods%2C%20i.e.%2C%20%5BCJJJLST20%2C%20ACJJS21%5D%2C%20which%20reduce%20the%20problem%20to%20minimizing%20a%0Aregularized%20Gaussian%20convolution%20of%20the%20function%20constrained%20to%20Euclidean%0Aballs.%20By%20developing%20and%20leveraging%20new%20stability%20properties%20of%20the%20Hessian%20of%0Athis%20induced%20function%2C%20we%20depart%20from%20prior%20parallel%20algorithms%20and%20reduce%0Athese%20ball-constrained%20optimization%20problems%20to%20stochastic%20unconstrained%0Aquadratic%20minimization%20problems.%20Although%20we%20are%20unable%20to%20prove%20concentration%0Aof%20the%20asymmetric%20matrices%20that%20we%20use%20to%20approximate%20this%20Hessian%2C%20we%0Anevertheless%20develop%20an%20efficient%20parallel%20method%20for%20solving%20these%20quadratics.%0AInterestingly%2C%20our%20algorithms%20can%20be%20improved%20using%20fast%20matrix%20multiplication%0Aand%20use%20nearly-linear%20work%20if%20the%20matrix%20multiplication%20exponent%20is%202.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07373v1&entry.124074799=Read"},
{"title": "Deep Implicit Optimization for Robust and Flexible Image Registration", "author": "Rohit Jena and Pratik Chaudhari and James C. Gee", "abstract": "  Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.\n", "link": "http://arxiv.org/abs/2406.07361v1", "date": "2024-06-11", "relevancy": 2.2829, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6022}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5526}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Implicit%20Optimization%20for%20Robust%20and%20Flexible%20Image%20Registration&body=Title%3A%20Deep%20Implicit%20Optimization%20for%20Robust%20and%20Flexible%20Image%20Registration%0AAuthor%3A%20Rohit%20Jena%20and%20Pratik%20Chaudhari%20and%20James%20C.%20Gee%0AAbstract%3A%20%20%20Deep%20Learning%20in%20Image%20Registration%20%28DLIR%29%20methods%20have%20been%20tremendously%0Asuccessful%20in%20image%20registration%20due%20to%20their%20speed%20and%20ability%20to%20incorporate%0Aweak%20label%20supervision%20at%20training%20time.%20However%2C%20DLIR%20methods%20forego%20many%20of%0Athe%20benefits%20of%20classical%20optimization-based%20methods.%20The%20functional%20nature%20of%0Adeep%20networks%20do%20not%20guarantee%20that%20the%20predicted%20transformation%20is%20a%20local%0Aminima%20of%20the%20registration%20objective%2C%20the%20representation%20of%20the%20transformation%0A%28displacement/velocity%20field/affine%29%20is%20fixed%2C%20and%20the%20networks%20are%20not%20robust%0Ato%20domain%20shift.%20Our%20method%20aims%20to%20bridge%20this%20gap%20between%20classical%20and%0Alearning%20methods%20by%20incorporating%20optimization%20as%20a%20layer%20in%20a%20deep%20network.%20A%0Adeep%20network%20is%20trained%20to%20predict%20multi-scale%20dense%20feature%20images%20that%20are%0Aregistered%20using%20a%20black%20box%20iterative%20optimization%20solver.%20This%20optimal%20warp%0Ais%20then%20used%20to%20minimize%20image%20and%20label%20alignment%20errors.%20By%20implicitly%0Adifferentiating%20end-to-end%20through%20an%20iterative%20optimization%20solver%2C%20our%0Alearned%20features%20are%20registration%20and%20label-aware%2C%20and%20the%20warp%20functions%20are%0Aguaranteed%20to%20be%20local%20minima%20of%20the%20registration%20objective%20in%20the%20feature%0Aspace.%20Our%20framework%20shows%20excellent%20performance%20on%20in-domain%20datasets%2C%20and%20is%0Aagnostic%20to%20domain%20shift%20such%20as%20anisotropy%20and%20varying%20intensity%20profiles.%20For%0Athe%20first%20time%2C%20our%20method%20allows%20switching%20between%20arbitrary%20transformation%0Arepresentations%20%28free-form%20to%20diffeomorphic%29%20at%20test%20time%20with%20zero%20retraining.%0AEnd-to-end%20feature%20learning%20also%20facilitates%20interpretability%20of%20features%2C%20and%0Aout-of-the-box%20promptability%20using%20additional%20label-fidelity%20terms%20at%0Ainference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Implicit%2520Optimization%2520for%2520Robust%2520and%2520Flexible%2520Image%2520Registration%26entry.906535625%3DRohit%2520Jena%2520and%2520Pratik%2520Chaudhari%2520and%2520James%2520C.%2520Gee%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520in%2520Image%2520Registration%2520%2528DLIR%2529%2520methods%2520have%2520been%2520tremendously%250Asuccessful%2520in%2520image%2520registration%2520due%2520to%2520their%2520speed%2520and%2520ability%2520to%2520incorporate%250Aweak%2520label%2520supervision%2520at%2520training%2520time.%2520However%252C%2520DLIR%2520methods%2520forego%2520many%2520of%250Athe%2520benefits%2520of%2520classical%2520optimization-based%2520methods.%2520The%2520functional%2520nature%2520of%250Adeep%2520networks%2520do%2520not%2520guarantee%2520that%2520the%2520predicted%2520transformation%2520is%2520a%2520local%250Aminima%2520of%2520the%2520registration%2520objective%252C%2520the%2520representation%2520of%2520the%2520transformation%250A%2528displacement/velocity%2520field/affine%2529%2520is%2520fixed%252C%2520and%2520the%2520networks%2520are%2520not%2520robust%250Ato%2520domain%2520shift.%2520Our%2520method%2520aims%2520to%2520bridge%2520this%2520gap%2520between%2520classical%2520and%250Alearning%2520methods%2520by%2520incorporating%2520optimization%2520as%2520a%2520layer%2520in%2520a%2520deep%2520network.%2520A%250Adeep%2520network%2520is%2520trained%2520to%2520predict%2520multi-scale%2520dense%2520feature%2520images%2520that%2520are%250Aregistered%2520using%2520a%2520black%2520box%2520iterative%2520optimization%2520solver.%2520This%2520optimal%2520warp%250Ais%2520then%2520used%2520to%2520minimize%2520image%2520and%2520label%2520alignment%2520errors.%2520By%2520implicitly%250Adifferentiating%2520end-to-end%2520through%2520an%2520iterative%2520optimization%2520solver%252C%2520our%250Alearned%2520features%2520are%2520registration%2520and%2520label-aware%252C%2520and%2520the%2520warp%2520functions%2520are%250Aguaranteed%2520to%2520be%2520local%2520minima%2520of%2520the%2520registration%2520objective%2520in%2520the%2520feature%250Aspace.%2520Our%2520framework%2520shows%2520excellent%2520performance%2520on%2520in-domain%2520datasets%252C%2520and%2520is%250Aagnostic%2520to%2520domain%2520shift%2520such%2520as%2520anisotropy%2520and%2520varying%2520intensity%2520profiles.%2520For%250Athe%2520first%2520time%252C%2520our%2520method%2520allows%2520switching%2520between%2520arbitrary%2520transformation%250Arepresentations%2520%2528free-form%2520to%2520diffeomorphic%2529%2520at%2520test%2520time%2520with%2520zero%2520retraining.%250AEnd-to-end%2520feature%2520learning%2520also%2520facilitates%2520interpretability%2520of%2520features%252C%2520and%250Aout-of-the-box%2520promptability%2520using%2520additional%2520label-fidelity%2520terms%2520at%250Ainference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Implicit%20Optimization%20for%20Robust%20and%20Flexible%20Image%20Registration&entry.906535625=Rohit%20Jena%20and%20Pratik%20Chaudhari%20and%20James%20C.%20Gee&entry.1292438233=%20%20Deep%20Learning%20in%20Image%20Registration%20%28DLIR%29%20methods%20have%20been%20tremendously%0Asuccessful%20in%20image%20registration%20due%20to%20their%20speed%20and%20ability%20to%20incorporate%0Aweak%20label%20supervision%20at%20training%20time.%20However%2C%20DLIR%20methods%20forego%20many%20of%0Athe%20benefits%20of%20classical%20optimization-based%20methods.%20The%20functional%20nature%20of%0Adeep%20networks%20do%20not%20guarantee%20that%20the%20predicted%20transformation%20is%20a%20local%0Aminima%20of%20the%20registration%20objective%2C%20the%20representation%20of%20the%20transformation%0A%28displacement/velocity%20field/affine%29%20is%20fixed%2C%20and%20the%20networks%20are%20not%20robust%0Ato%20domain%20shift.%20Our%20method%20aims%20to%20bridge%20this%20gap%20between%20classical%20and%0Alearning%20methods%20by%20incorporating%20optimization%20as%20a%20layer%20in%20a%20deep%20network.%20A%0Adeep%20network%20is%20trained%20to%20predict%20multi-scale%20dense%20feature%20images%20that%20are%0Aregistered%20using%20a%20black%20box%20iterative%20optimization%20solver.%20This%20optimal%20warp%0Ais%20then%20used%20to%20minimize%20image%20and%20label%20alignment%20errors.%20By%20implicitly%0Adifferentiating%20end-to-end%20through%20an%20iterative%20optimization%20solver%2C%20our%0Alearned%20features%20are%20registration%20and%20label-aware%2C%20and%20the%20warp%20functions%20are%0Aguaranteed%20to%20be%20local%20minima%20of%20the%20registration%20objective%20in%20the%20feature%0Aspace.%20Our%20framework%20shows%20excellent%20performance%20on%20in-domain%20datasets%2C%20and%20is%0Aagnostic%20to%20domain%20shift%20such%20as%20anisotropy%20and%20varying%20intensity%20profiles.%20For%0Athe%20first%20time%2C%20our%20method%20allows%20switching%20between%20arbitrary%20transformation%0Arepresentations%20%28free-form%20to%20diffeomorphic%29%20at%20test%20time%20with%20zero%20retraining.%0AEnd-to-end%20feature%20learning%20also%20facilitates%20interpretability%20of%20features%2C%20and%0Aout-of-the-box%20promptability%20using%20additional%20label-fidelity%20terms%20at%0Ainference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07361v1&entry.124074799=Read"},
{"title": "The Multiscale Surface Vision Transformer", "author": "Simon Dahan and Logan Z. J. Williams and Daniel Rueckert and Emma C. Robinson", "abstract": "  Surface meshes are a favoured domain for representing structural and\nfunctional information on the human cortex, but their complex topology and\ngeometry pose significant challenges for deep learning analysis. While\nTransformers have excelled as domain-agnostic architectures for\nsequence-to-sequence learning, the quadratic cost of the self-attention\noperation remains an obstacle for many dense prediction tasks. Inspired by some\nof the latest advances in hierarchical modelling with vision transformers, we\nintroduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone\narchitecture for surface deep learning. The self-attention mechanism is applied\nwithin local-mesh-windows to allow for high-resolution sampling of the\nunderlying data, while a shifted-window strategy improves the sharing of\ninformation between windows. Neighbouring patches are successively merged,\nallowing the MS-SiT to learn hierarchical representations suitable for any\nprediction task. Results demonstrate that the MS-SiT outperforms existing\nsurface deep learning methods for neonatal phenotyping prediction tasks using\nthe Developing Human Connectome Project (dHCP) dataset. Furthermore, building\nthe MS-SiT backbone into a U-shaped architecture for surface segmentation\ndemonstrates competitive results on cortical parcellation using the UK Biobank\n(UKB) and manually-annotated MindBoggle datasets. Code and trained models are\npublicly available at\nhttps://github.com/metrics-lab/surface-vision-transformers.\n", "link": "http://arxiv.org/abs/2303.11909v3", "date": "2024-06-11", "relevancy": 2.2783, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.593}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5858}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Multiscale%20Surface%20Vision%20Transformer&body=Title%3A%20The%20Multiscale%20Surface%20Vision%20Transformer%0AAuthor%3A%20Simon%20Dahan%20and%20Logan%20Z.%20J.%20Williams%20and%20Daniel%20Rueckert%20and%20Emma%20C.%20Robinson%0AAbstract%3A%20%20%20Surface%20meshes%20are%20a%20favoured%20domain%20for%20representing%20structural%20and%0Afunctional%20information%20on%20the%20human%20cortex%2C%20but%20their%20complex%20topology%20and%0Ageometry%20pose%20significant%20challenges%20for%20deep%20learning%20analysis.%20While%0ATransformers%20have%20excelled%20as%20domain-agnostic%20architectures%20for%0Asequence-to-sequence%20learning%2C%20the%20quadratic%20cost%20of%20the%20self-attention%0Aoperation%20remains%20an%20obstacle%20for%20many%20dense%20prediction%20tasks.%20Inspired%20by%20some%0Aof%20the%20latest%20advances%20in%20hierarchical%20modelling%20with%20vision%20transformers%2C%20we%0Aintroduce%20the%20Multiscale%20Surface%20Vision%20Transformer%20%28MS-SiT%29%20as%20a%20backbone%0Aarchitecture%20for%20surface%20deep%20learning.%20The%20self-attention%20mechanism%20is%20applied%0Awithin%20local-mesh-windows%20to%20allow%20for%20high-resolution%20sampling%20of%20the%0Aunderlying%20data%2C%20while%20a%20shifted-window%20strategy%20improves%20the%20sharing%20of%0Ainformation%20between%20windows.%20Neighbouring%20patches%20are%20successively%20merged%2C%0Aallowing%20the%20MS-SiT%20to%20learn%20hierarchical%20representations%20suitable%20for%20any%0Aprediction%20task.%20Results%20demonstrate%20that%20the%20MS-SiT%20outperforms%20existing%0Asurface%20deep%20learning%20methods%20for%20neonatal%20phenotyping%20prediction%20tasks%20using%0Athe%20Developing%20Human%20Connectome%20Project%20%28dHCP%29%20dataset.%20Furthermore%2C%20building%0Athe%20MS-SiT%20backbone%20into%20a%20U-shaped%20architecture%20for%20surface%20segmentation%0Ademonstrates%20competitive%20results%20on%20cortical%20parcellation%20using%20the%20UK%20Biobank%0A%28UKB%29%20and%20manually-annotated%20MindBoggle%20datasets.%20Code%20and%20trained%20models%20are%0Apublicly%20available%20at%0Ahttps%3A//github.com/metrics-lab/surface-vision-transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.11909v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Multiscale%2520Surface%2520Vision%2520Transformer%26entry.906535625%3DSimon%2520Dahan%2520and%2520Logan%2520Z.%2520J.%2520Williams%2520and%2520Daniel%2520Rueckert%2520and%2520Emma%2520C.%2520Robinson%26entry.1292438233%3D%2520%2520Surface%2520meshes%2520are%2520a%2520favoured%2520domain%2520for%2520representing%2520structural%2520and%250Afunctional%2520information%2520on%2520the%2520human%2520cortex%252C%2520but%2520their%2520complex%2520topology%2520and%250Ageometry%2520pose%2520significant%2520challenges%2520for%2520deep%2520learning%2520analysis.%2520While%250ATransformers%2520have%2520excelled%2520as%2520domain-agnostic%2520architectures%2520for%250Asequence-to-sequence%2520learning%252C%2520the%2520quadratic%2520cost%2520of%2520the%2520self-attention%250Aoperation%2520remains%2520an%2520obstacle%2520for%2520many%2520dense%2520prediction%2520tasks.%2520Inspired%2520by%2520some%250Aof%2520the%2520latest%2520advances%2520in%2520hierarchical%2520modelling%2520with%2520vision%2520transformers%252C%2520we%250Aintroduce%2520the%2520Multiscale%2520Surface%2520Vision%2520Transformer%2520%2528MS-SiT%2529%2520as%2520a%2520backbone%250Aarchitecture%2520for%2520surface%2520deep%2520learning.%2520The%2520self-attention%2520mechanism%2520is%2520applied%250Awithin%2520local-mesh-windows%2520to%2520allow%2520for%2520high-resolution%2520sampling%2520of%2520the%250Aunderlying%2520data%252C%2520while%2520a%2520shifted-window%2520strategy%2520improves%2520the%2520sharing%2520of%250Ainformation%2520between%2520windows.%2520Neighbouring%2520patches%2520are%2520successively%2520merged%252C%250Aallowing%2520the%2520MS-SiT%2520to%2520learn%2520hierarchical%2520representations%2520suitable%2520for%2520any%250Aprediction%2520task.%2520Results%2520demonstrate%2520that%2520the%2520MS-SiT%2520outperforms%2520existing%250Asurface%2520deep%2520learning%2520methods%2520for%2520neonatal%2520phenotyping%2520prediction%2520tasks%2520using%250Athe%2520Developing%2520Human%2520Connectome%2520Project%2520%2528dHCP%2529%2520dataset.%2520Furthermore%252C%2520building%250Athe%2520MS-SiT%2520backbone%2520into%2520a%2520U-shaped%2520architecture%2520for%2520surface%2520segmentation%250Ademonstrates%2520competitive%2520results%2520on%2520cortical%2520parcellation%2520using%2520the%2520UK%2520Biobank%250A%2528UKB%2529%2520and%2520manually-annotated%2520MindBoggle%2520datasets.%2520Code%2520and%2520trained%2520models%2520are%250Apublicly%2520available%2520at%250Ahttps%253A//github.com/metrics-lab/surface-vision-transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.11909v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Multiscale%20Surface%20Vision%20Transformer&entry.906535625=Simon%20Dahan%20and%20Logan%20Z.%20J.%20Williams%20and%20Daniel%20Rueckert%20and%20Emma%20C.%20Robinson&entry.1292438233=%20%20Surface%20meshes%20are%20a%20favoured%20domain%20for%20representing%20structural%20and%0Afunctional%20information%20on%20the%20human%20cortex%2C%20but%20their%20complex%20topology%20and%0Ageometry%20pose%20significant%20challenges%20for%20deep%20learning%20analysis.%20While%0ATransformers%20have%20excelled%20as%20domain-agnostic%20architectures%20for%0Asequence-to-sequence%20learning%2C%20the%20quadratic%20cost%20of%20the%20self-attention%0Aoperation%20remains%20an%20obstacle%20for%20many%20dense%20prediction%20tasks.%20Inspired%20by%20some%0Aof%20the%20latest%20advances%20in%20hierarchical%20modelling%20with%20vision%20transformers%2C%20we%0Aintroduce%20the%20Multiscale%20Surface%20Vision%20Transformer%20%28MS-SiT%29%20as%20a%20backbone%0Aarchitecture%20for%20surface%20deep%20learning.%20The%20self-attention%20mechanism%20is%20applied%0Awithin%20local-mesh-windows%20to%20allow%20for%20high-resolution%20sampling%20of%20the%0Aunderlying%20data%2C%20while%20a%20shifted-window%20strategy%20improves%20the%20sharing%20of%0Ainformation%20between%20windows.%20Neighbouring%20patches%20are%20successively%20merged%2C%0Aallowing%20the%20MS-SiT%20to%20learn%20hierarchical%20representations%20suitable%20for%20any%0Aprediction%20task.%20Results%20demonstrate%20that%20the%20MS-SiT%20outperforms%20existing%0Asurface%20deep%20learning%20methods%20for%20neonatal%20phenotyping%20prediction%20tasks%20using%0Athe%20Developing%20Human%20Connectome%20Project%20%28dHCP%29%20dataset.%20Furthermore%2C%20building%0Athe%20MS-SiT%20backbone%20into%20a%20U-shaped%20architecture%20for%20surface%20segmentation%0Ademonstrates%20competitive%20results%20on%20cortical%20parcellation%20using%20the%20UK%20Biobank%0A%28UKB%29%20and%20manually-annotated%20MindBoggle%20datasets.%20Code%20and%20trained%20models%20are%0Apublicly%20available%20at%0Ahttps%3A//github.com/metrics-lab/surface-vision-transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.11909v3&entry.124074799=Read"},
{"title": "3D Voxel Maps to 2D Occupancy Maps for Efficient Path Planning for\n  Aerial and Ground Robots", "author": "Scott Fredriksson and Akshit Saradagi and George Nikolakopoulos", "abstract": "  This article introduces a novel method for converting 3D voxel maps, commonly\nutilized by robots for localization and navigation, into 2D occupancy maps that\ncan be used for more computationally efficient large-scale navigation, both in\nthe sense of computation time and memory usage. The main aim is to effectively\nintegrate the distinct mapping advantages of 2D and 3D maps to enable efficient\npath planning for both unmanned aerial vehicles (UAVs) and unmanned ground\nvehicles (UGVs). The proposed method uses the free space representation in the\nUFOMap mapping solution to generate 2D occupancy maps with height and slope\ninformation. In the process of 3D to 2D map conversion, the proposed method\nconducts safety checks and eliminates free spaces in the map with dimensions\n(in the height axis) lower than the robot's safety margins. This allows an\naerial or ground robot to navigate safely, relying primarily on the 2D map\ngenerated by the method. Additionally, the method extracts height and slope\ndata from the 3D voxel map. The slope data identifies areas too steep for a\nground robot to traverse, marking them as occupied, thus enabling a more\naccurate representation of the terrain for ground robots. The height data is\nutilized to convert paths generated using the 2D map into paths in 3D space for\nboth UAVs and UGVs. The effectiveness of the proposed method is evaluated in\ntwo different environments.\n", "link": "http://arxiv.org/abs/2406.07270v1", "date": "2024-06-11", "relevancy": 2.2742, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5455}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Voxel%20Maps%20to%202D%20Occupancy%20Maps%20for%20Efficient%20Path%20Planning%20for%0A%20%20Aerial%20and%20Ground%20Robots&body=Title%3A%203D%20Voxel%20Maps%20to%202D%20Occupancy%20Maps%20for%20Efficient%20Path%20Planning%20for%0A%20%20Aerial%20and%20Ground%20Robots%0AAuthor%3A%20Scott%20Fredriksson%20and%20Akshit%20Saradagi%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20This%20article%20introduces%20a%20novel%20method%20for%20converting%203D%20voxel%20maps%2C%20commonly%0Autilized%20by%20robots%20for%20localization%20and%20navigation%2C%20into%202D%20occupancy%20maps%20that%0Acan%20be%20used%20for%20more%20computationally%20efficient%20large-scale%20navigation%2C%20both%20in%0Athe%20sense%20of%20computation%20time%20and%20memory%20usage.%20The%20main%20aim%20is%20to%20effectively%0Aintegrate%20the%20distinct%20mapping%20advantages%20of%202D%20and%203D%20maps%20to%20enable%20efficient%0Apath%20planning%20for%20both%20unmanned%20aerial%20vehicles%20%28UAVs%29%20and%20unmanned%20ground%0Avehicles%20%28UGVs%29.%20The%20proposed%20method%20uses%20the%20free%20space%20representation%20in%20the%0AUFOMap%20mapping%20solution%20to%20generate%202D%20occupancy%20maps%20with%20height%20and%20slope%0Ainformation.%20In%20the%20process%20of%203D%20to%202D%20map%20conversion%2C%20the%20proposed%20method%0Aconducts%20safety%20checks%20and%20eliminates%20free%20spaces%20in%20the%20map%20with%20dimensions%0A%28in%20the%20height%20axis%29%20lower%20than%20the%20robot%27s%20safety%20margins.%20This%20allows%20an%0Aaerial%20or%20ground%20robot%20to%20navigate%20safely%2C%20relying%20primarily%20on%20the%202D%20map%0Agenerated%20by%20the%20method.%20Additionally%2C%20the%20method%20extracts%20height%20and%20slope%0Adata%20from%20the%203D%20voxel%20map.%20The%20slope%20data%20identifies%20areas%20too%20steep%20for%20a%0Aground%20robot%20to%20traverse%2C%20marking%20them%20as%20occupied%2C%20thus%20enabling%20a%20more%0Aaccurate%20representation%20of%20the%20terrain%20for%20ground%20robots.%20The%20height%20data%20is%0Autilized%20to%20convert%20paths%20generated%20using%20the%202D%20map%20into%20paths%20in%203D%20space%20for%0Aboth%20UAVs%20and%20UGVs.%20The%20effectiveness%20of%20the%20proposed%20method%20is%20evaluated%20in%0Atwo%20different%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Voxel%2520Maps%2520to%25202D%2520Occupancy%2520Maps%2520for%2520Efficient%2520Path%2520Planning%2520for%250A%2520%2520Aerial%2520and%2520Ground%2520Robots%26entry.906535625%3DScott%2520Fredriksson%2520and%2520Akshit%2520Saradagi%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520a%2520novel%2520method%2520for%2520converting%25203D%2520voxel%2520maps%252C%2520commonly%250Autilized%2520by%2520robots%2520for%2520localization%2520and%2520navigation%252C%2520into%25202D%2520occupancy%2520maps%2520that%250Acan%2520be%2520used%2520for%2520more%2520computationally%2520efficient%2520large-scale%2520navigation%252C%2520both%2520in%250Athe%2520sense%2520of%2520computation%2520time%2520and%2520memory%2520usage.%2520The%2520main%2520aim%2520is%2520to%2520effectively%250Aintegrate%2520the%2520distinct%2520mapping%2520advantages%2520of%25202D%2520and%25203D%2520maps%2520to%2520enable%2520efficient%250Apath%2520planning%2520for%2520both%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520and%2520unmanned%2520ground%250Avehicles%2520%2528UGVs%2529.%2520The%2520proposed%2520method%2520uses%2520the%2520free%2520space%2520representation%2520in%2520the%250AUFOMap%2520mapping%2520solution%2520to%2520generate%25202D%2520occupancy%2520maps%2520with%2520height%2520and%2520slope%250Ainformation.%2520In%2520the%2520process%2520of%25203D%2520to%25202D%2520map%2520conversion%252C%2520the%2520proposed%2520method%250Aconducts%2520safety%2520checks%2520and%2520eliminates%2520free%2520spaces%2520in%2520the%2520map%2520with%2520dimensions%250A%2528in%2520the%2520height%2520axis%2529%2520lower%2520than%2520the%2520robot%2527s%2520safety%2520margins.%2520This%2520allows%2520an%250Aaerial%2520or%2520ground%2520robot%2520to%2520navigate%2520safely%252C%2520relying%2520primarily%2520on%2520the%25202D%2520map%250Agenerated%2520by%2520the%2520method.%2520Additionally%252C%2520the%2520method%2520extracts%2520height%2520and%2520slope%250Adata%2520from%2520the%25203D%2520voxel%2520map.%2520The%2520slope%2520data%2520identifies%2520areas%2520too%2520steep%2520for%2520a%250Aground%2520robot%2520to%2520traverse%252C%2520marking%2520them%2520as%2520occupied%252C%2520thus%2520enabling%2520a%2520more%250Aaccurate%2520representation%2520of%2520the%2520terrain%2520for%2520ground%2520robots.%2520The%2520height%2520data%2520is%250Autilized%2520to%2520convert%2520paths%2520generated%2520using%2520the%25202D%2520map%2520into%2520paths%2520in%25203D%2520space%2520for%250Aboth%2520UAVs%2520and%2520UGVs.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520method%2520is%2520evaluated%2520in%250Atwo%2520different%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Voxel%20Maps%20to%202D%20Occupancy%20Maps%20for%20Efficient%20Path%20Planning%20for%0A%20%20Aerial%20and%20Ground%20Robots&entry.906535625=Scott%20Fredriksson%20and%20Akshit%20Saradagi%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20This%20article%20introduces%20a%20novel%20method%20for%20converting%203D%20voxel%20maps%2C%20commonly%0Autilized%20by%20robots%20for%20localization%20and%20navigation%2C%20into%202D%20occupancy%20maps%20that%0Acan%20be%20used%20for%20more%20computationally%20efficient%20large-scale%20navigation%2C%20both%20in%0Athe%20sense%20of%20computation%20time%20and%20memory%20usage.%20The%20main%20aim%20is%20to%20effectively%0Aintegrate%20the%20distinct%20mapping%20advantages%20of%202D%20and%203D%20maps%20to%20enable%20efficient%0Apath%20planning%20for%20both%20unmanned%20aerial%20vehicles%20%28UAVs%29%20and%20unmanned%20ground%0Avehicles%20%28UGVs%29.%20The%20proposed%20method%20uses%20the%20free%20space%20representation%20in%20the%0AUFOMap%20mapping%20solution%20to%20generate%202D%20occupancy%20maps%20with%20height%20and%20slope%0Ainformation.%20In%20the%20process%20of%203D%20to%202D%20map%20conversion%2C%20the%20proposed%20method%0Aconducts%20safety%20checks%20and%20eliminates%20free%20spaces%20in%20the%20map%20with%20dimensions%0A%28in%20the%20height%20axis%29%20lower%20than%20the%20robot%27s%20safety%20margins.%20This%20allows%20an%0Aaerial%20or%20ground%20robot%20to%20navigate%20safely%2C%20relying%20primarily%20on%20the%202D%20map%0Agenerated%20by%20the%20method.%20Additionally%2C%20the%20method%20extracts%20height%20and%20slope%0Adata%20from%20the%203D%20voxel%20map.%20The%20slope%20data%20identifies%20areas%20too%20steep%20for%20a%0Aground%20robot%20to%20traverse%2C%20marking%20them%20as%20occupied%2C%20thus%20enabling%20a%20more%0Aaccurate%20representation%20of%20the%20terrain%20for%20ground%20robots.%20The%20height%20data%20is%0Autilized%20to%20convert%20paths%20generated%20using%20the%202D%20map%20into%20paths%20in%203D%20space%20for%0Aboth%20UAVs%20and%20UGVs.%20The%20effectiveness%20of%20the%20proposed%20method%20is%20evaluated%20in%0Atwo%20different%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07270v1&entry.124074799=Read"},
{"title": "ToNER: Type-oriented Named Entity Recognition with Generative Language\n  Model", "author": "Guochao Jiang and Ziqin Luo and Yuchen Shi and Dixuan Wang and Jiaqing Liang and Deqing Yang", "abstract": "  In recent years, the fine-tuned generative models have been proven more\npowerful than the previous tagging-based or span-based models on named entity\nrecognition (NER) task. It has also been found that the information related to\nentities, such as entity types, can prompt a model to achieve NER better.\nHowever, it is not easy to determine the entity types indeed existing in the\ngiven sentence in advance, and inputting too many potential entity types would\ndistract the model inevitably. To exploit entity types' merit on promoting NER\ntask, in this paper we propose a novel NER framework, namely ToNER based on a\ngenerative model. In ToNER, a type matching model is proposed at first to\nidentify the entity types most likely to appear in the sentence. Then, we\nappend a multiple binary classification task to fine-tune the generative\nmodel's encoder, so as to generate the refined representation of the input\nsentence. Moreover, we add an auxiliary task for the model to discover the\nentity types which further fine-tunes the model to output more accurate\nresults. Our extensive experiments on some NER benchmarks verify the\neffectiveness of our proposed strategies in ToNER that are oriented towards\nentity types' exploitation.\n", "link": "http://arxiv.org/abs/2404.09145v2", "date": "2024-06-11", "relevancy": 2.2672, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4955}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4447}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToNER%3A%20Type-oriented%20Named%20Entity%20Recognition%20with%20Generative%20Language%0A%20%20Model&body=Title%3A%20ToNER%3A%20Type-oriented%20Named%20Entity%20Recognition%20with%20Generative%20Language%0A%20%20Model%0AAuthor%3A%20Guochao%20Jiang%20and%20Ziqin%20Luo%20and%20Yuchen%20Shi%20and%20Dixuan%20Wang%20and%20Jiaqing%20Liang%20and%20Deqing%20Yang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20fine-tuned%20generative%20models%20have%20been%20proven%20more%0Apowerful%20than%20the%20previous%20tagging-based%20or%20span-based%20models%20on%20named%20entity%0Arecognition%20%28NER%29%20task.%20It%20has%20also%20been%20found%20that%20the%20information%20related%20to%0Aentities%2C%20such%20as%20entity%20types%2C%20can%20prompt%20a%20model%20to%20achieve%20NER%20better.%0AHowever%2C%20it%20is%20not%20easy%20to%20determine%20the%20entity%20types%20indeed%20existing%20in%20the%0Agiven%20sentence%20in%20advance%2C%20and%20inputting%20too%20many%20potential%20entity%20types%20would%0Adistract%20the%20model%20inevitably.%20To%20exploit%20entity%20types%27%20merit%20on%20promoting%20NER%0Atask%2C%20in%20this%20paper%20we%20propose%20a%20novel%20NER%20framework%2C%20namely%20ToNER%20based%20on%20a%0Agenerative%20model.%20In%20ToNER%2C%20a%20type%20matching%20model%20is%20proposed%20at%20first%20to%0Aidentify%20the%20entity%20types%20most%20likely%20to%20appear%20in%20the%20sentence.%20Then%2C%20we%0Aappend%20a%20multiple%20binary%20classification%20task%20to%20fine-tune%20the%20generative%0Amodel%27s%20encoder%2C%20so%20as%20to%20generate%20the%20refined%20representation%20of%20the%20input%0Asentence.%20Moreover%2C%20we%20add%20an%20auxiliary%20task%20for%20the%20model%20to%20discover%20the%0Aentity%20types%20which%20further%20fine-tunes%20the%20model%20to%20output%20more%20accurate%0Aresults.%20Our%20extensive%20experiments%20on%20some%20NER%20benchmarks%20verify%20the%0Aeffectiveness%20of%20our%20proposed%20strategies%20in%20ToNER%20that%20are%20oriented%20towards%0Aentity%20types%27%20exploitation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09145v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToNER%253A%2520Type-oriented%2520Named%2520Entity%2520Recognition%2520with%2520Generative%2520Language%250A%2520%2520Model%26entry.906535625%3DGuochao%2520Jiang%2520and%2520Ziqin%2520Luo%2520and%2520Yuchen%2520Shi%2520and%2520Dixuan%2520Wang%2520and%2520Jiaqing%2520Liang%2520and%2520Deqing%2520Yang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520fine-tuned%2520generative%2520models%2520have%2520been%2520proven%2520more%250Apowerful%2520than%2520the%2520previous%2520tagging-based%2520or%2520span-based%2520models%2520on%2520named%2520entity%250Arecognition%2520%2528NER%2529%2520task.%2520It%2520has%2520also%2520been%2520found%2520that%2520the%2520information%2520related%2520to%250Aentities%252C%2520such%2520as%2520entity%2520types%252C%2520can%2520prompt%2520a%2520model%2520to%2520achieve%2520NER%2520better.%250AHowever%252C%2520it%2520is%2520not%2520easy%2520to%2520determine%2520the%2520entity%2520types%2520indeed%2520existing%2520in%2520the%250Agiven%2520sentence%2520in%2520advance%252C%2520and%2520inputting%2520too%2520many%2520potential%2520entity%2520types%2520would%250Adistract%2520the%2520model%2520inevitably.%2520To%2520exploit%2520entity%2520types%2527%2520merit%2520on%2520promoting%2520NER%250Atask%252C%2520in%2520this%2520paper%2520we%2520propose%2520a%2520novel%2520NER%2520framework%252C%2520namely%2520ToNER%2520based%2520on%2520a%250Agenerative%2520model.%2520In%2520ToNER%252C%2520a%2520type%2520matching%2520model%2520is%2520proposed%2520at%2520first%2520to%250Aidentify%2520the%2520entity%2520types%2520most%2520likely%2520to%2520appear%2520in%2520the%2520sentence.%2520Then%252C%2520we%250Aappend%2520a%2520multiple%2520binary%2520classification%2520task%2520to%2520fine-tune%2520the%2520generative%250Amodel%2527s%2520encoder%252C%2520so%2520as%2520to%2520generate%2520the%2520refined%2520representation%2520of%2520the%2520input%250Asentence.%2520Moreover%252C%2520we%2520add%2520an%2520auxiliary%2520task%2520for%2520the%2520model%2520to%2520discover%2520the%250Aentity%2520types%2520which%2520further%2520fine-tunes%2520the%2520model%2520to%2520output%2520more%2520accurate%250Aresults.%2520Our%2520extensive%2520experiments%2520on%2520some%2520NER%2520benchmarks%2520verify%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520strategies%2520in%2520ToNER%2520that%2520are%2520oriented%2520towards%250Aentity%2520types%2527%2520exploitation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09145v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToNER%3A%20Type-oriented%20Named%20Entity%20Recognition%20with%20Generative%20Language%0A%20%20Model&entry.906535625=Guochao%20Jiang%20and%20Ziqin%20Luo%20and%20Yuchen%20Shi%20and%20Dixuan%20Wang%20and%20Jiaqing%20Liang%20and%20Deqing%20Yang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20fine-tuned%20generative%20models%20have%20been%20proven%20more%0Apowerful%20than%20the%20previous%20tagging-based%20or%20span-based%20models%20on%20named%20entity%0Arecognition%20%28NER%29%20task.%20It%20has%20also%20been%20found%20that%20the%20information%20related%20to%0Aentities%2C%20such%20as%20entity%20types%2C%20can%20prompt%20a%20model%20to%20achieve%20NER%20better.%0AHowever%2C%20it%20is%20not%20easy%20to%20determine%20the%20entity%20types%20indeed%20existing%20in%20the%0Agiven%20sentence%20in%20advance%2C%20and%20inputting%20too%20many%20potential%20entity%20types%20would%0Adistract%20the%20model%20inevitably.%20To%20exploit%20entity%20types%27%20merit%20on%20promoting%20NER%0Atask%2C%20in%20this%20paper%20we%20propose%20a%20novel%20NER%20framework%2C%20namely%20ToNER%20based%20on%20a%0Agenerative%20model.%20In%20ToNER%2C%20a%20type%20matching%20model%20is%20proposed%20at%20first%20to%0Aidentify%20the%20entity%20types%20most%20likely%20to%20appear%20in%20the%20sentence.%20Then%2C%20we%0Aappend%20a%20multiple%20binary%20classification%20task%20to%20fine-tune%20the%20generative%0Amodel%27s%20encoder%2C%20so%20as%20to%20generate%20the%20refined%20representation%20of%20the%20input%0Asentence.%20Moreover%2C%20we%20add%20an%20auxiliary%20task%20for%20the%20model%20to%20discover%20the%0Aentity%20types%20which%20further%20fine-tunes%20the%20model%20to%20output%20more%20accurate%0Aresults.%20Our%20extensive%20experiments%20on%20some%20NER%20benchmarks%20verify%20the%0Aeffectiveness%20of%20our%20proposed%20strategies%20in%20ToNER%20that%20are%20oriented%20towards%0Aentity%20types%27%20exploitation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09145v2&entry.124074799=Read"},
{"title": "Private Geometric Median", "author": "Mahdi Haghifam and Thomas Steinke and Jonathan Ullman", "abstract": "  In this paper, we study differentially private (DP) algorithms for computing\nthe geometric median (GM) of a dataset: Given $n$ points, $x_1,\\dots,x_n$ in\n$\\mathbb{R}^d$, the goal is to find a point $\\theta$ that minimizes the sum of\nthe Euclidean distances to these points, i.e., $\\sum_{i=1}^{n} \\|\\theta -\nx_i\\|_2$. Off-the-shelf methods, such as DP-GD, require strong a priori\nknowledge locating the data within a ball of radius $R$, and the excess risk of\nthe algorithm depends linearly on $R$. In this paper, we ask: can we design an\nefficient and private algorithm with an excess error guarantee that scales with\nthe (unknown) radius containing the majority of the datapoints? Our main\ncontribution is a pair of polynomial-time DP algorithms for the task of private\nGM with an excess error guarantee that scales with the effective diameter of\nthe datapoints. Additionally, we propose an inefficient algorithm based on the\ninverse smooth sensitivity mechanism, which satisfies the more restrictive\nnotion of pure DP. We complement our results with a lower bound and demonstrate\nthe optimality of our polynomial-time algorithms in terms of sample complexity.\n", "link": "http://arxiv.org/abs/2406.07407v1", "date": "2024-06-11", "relevancy": 2.2635, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5015}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4299}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Private%20Geometric%20Median&body=Title%3A%20Private%20Geometric%20Median%0AAuthor%3A%20Mahdi%20Haghifam%20and%20Thomas%20Steinke%20and%20Jonathan%20Ullman%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20differentially%20private%20%28DP%29%20algorithms%20for%20computing%0Athe%20geometric%20median%20%28GM%29%20of%20a%20dataset%3A%20Given%20%24n%24%20points%2C%20%24x_1%2C%5Cdots%2Cx_n%24%20in%0A%24%5Cmathbb%7BR%7D%5Ed%24%2C%20the%20goal%20is%20to%20find%20a%20point%20%24%5Ctheta%24%20that%20minimizes%20the%20sum%20of%0Athe%20Euclidean%20distances%20to%20these%20points%2C%20i.e.%2C%20%24%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5C%7C%5Ctheta%20-%0Ax_i%5C%7C_2%24.%20Off-the-shelf%20methods%2C%20such%20as%20DP-GD%2C%20require%20strong%20a%20priori%0Aknowledge%20locating%20the%20data%20within%20a%20ball%20of%20radius%20%24R%24%2C%20and%20the%20excess%20risk%20of%0Athe%20algorithm%20depends%20linearly%20on%20%24R%24.%20In%20this%20paper%2C%20we%20ask%3A%20can%20we%20design%20an%0Aefficient%20and%20private%20algorithm%20with%20an%20excess%20error%20guarantee%20that%20scales%20with%0Athe%20%28unknown%29%20radius%20containing%20the%20majority%20of%20the%20datapoints%3F%20Our%20main%0Acontribution%20is%20a%20pair%20of%20polynomial-time%20DP%20algorithms%20for%20the%20task%20of%20private%0AGM%20with%20an%20excess%20error%20guarantee%20that%20scales%20with%20the%20effective%20diameter%20of%0Athe%20datapoints.%20Additionally%2C%20we%20propose%20an%20inefficient%20algorithm%20based%20on%20the%0Ainverse%20smooth%20sensitivity%20mechanism%2C%20which%20satisfies%20the%20more%20restrictive%0Anotion%20of%20pure%20DP.%20We%20complement%20our%20results%20with%20a%20lower%20bound%20and%20demonstrate%0Athe%20optimality%20of%20our%20polynomial-time%20algorithms%20in%20terms%20of%20sample%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivate%2520Geometric%2520Median%26entry.906535625%3DMahdi%2520Haghifam%2520and%2520Thomas%2520Steinke%2520and%2520Jonathan%2520Ullman%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520differentially%2520private%2520%2528DP%2529%2520algorithms%2520for%2520computing%250Athe%2520geometric%2520median%2520%2528GM%2529%2520of%2520a%2520dataset%253A%2520Given%2520%2524n%2524%2520points%252C%2520%2524x_1%252C%255Cdots%252Cx_n%2524%2520in%250A%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520the%2520goal%2520is%2520to%2520find%2520a%2520point%2520%2524%255Ctheta%2524%2520that%2520minimizes%2520the%2520sum%2520of%250Athe%2520Euclidean%2520distances%2520to%2520these%2520points%252C%2520i.e.%252C%2520%2524%255Csum_%257Bi%253D1%257D%255E%257Bn%257D%2520%255C%257C%255Ctheta%2520-%250Ax_i%255C%257C_2%2524.%2520Off-the-shelf%2520methods%252C%2520such%2520as%2520DP-GD%252C%2520require%2520strong%2520a%2520priori%250Aknowledge%2520locating%2520the%2520data%2520within%2520a%2520ball%2520of%2520radius%2520%2524R%2524%252C%2520and%2520the%2520excess%2520risk%2520of%250Athe%2520algorithm%2520depends%2520linearly%2520on%2520%2524R%2524.%2520In%2520this%2520paper%252C%2520we%2520ask%253A%2520can%2520we%2520design%2520an%250Aefficient%2520and%2520private%2520algorithm%2520with%2520an%2520excess%2520error%2520guarantee%2520that%2520scales%2520with%250Athe%2520%2528unknown%2529%2520radius%2520containing%2520the%2520majority%2520of%2520the%2520datapoints%253F%2520Our%2520main%250Acontribution%2520is%2520a%2520pair%2520of%2520polynomial-time%2520DP%2520algorithms%2520for%2520the%2520task%2520of%2520private%250AGM%2520with%2520an%2520excess%2520error%2520guarantee%2520that%2520scales%2520with%2520the%2520effective%2520diameter%2520of%250Athe%2520datapoints.%2520Additionally%252C%2520we%2520propose%2520an%2520inefficient%2520algorithm%2520based%2520on%2520the%250Ainverse%2520smooth%2520sensitivity%2520mechanism%252C%2520which%2520satisfies%2520the%2520more%2520restrictive%250Anotion%2520of%2520pure%2520DP.%2520We%2520complement%2520our%2520results%2520with%2520a%2520lower%2520bound%2520and%2520demonstrate%250Athe%2520optimality%2520of%2520our%2520polynomial-time%2520algorithms%2520in%2520terms%2520of%2520sample%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Private%20Geometric%20Median&entry.906535625=Mahdi%20Haghifam%20and%20Thomas%20Steinke%20and%20Jonathan%20Ullman&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20differentially%20private%20%28DP%29%20algorithms%20for%20computing%0Athe%20geometric%20median%20%28GM%29%20of%20a%20dataset%3A%20Given%20%24n%24%20points%2C%20%24x_1%2C%5Cdots%2Cx_n%24%20in%0A%24%5Cmathbb%7BR%7D%5Ed%24%2C%20the%20goal%20is%20to%20find%20a%20point%20%24%5Ctheta%24%20that%20minimizes%20the%20sum%20of%0Athe%20Euclidean%20distances%20to%20these%20points%2C%20i.e.%2C%20%24%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%5C%7C%5Ctheta%20-%0Ax_i%5C%7C_2%24.%20Off-the-shelf%20methods%2C%20such%20as%20DP-GD%2C%20require%20strong%20a%20priori%0Aknowledge%20locating%20the%20data%20within%20a%20ball%20of%20radius%20%24R%24%2C%20and%20the%20excess%20risk%20of%0Athe%20algorithm%20depends%20linearly%20on%20%24R%24.%20In%20this%20paper%2C%20we%20ask%3A%20can%20we%20design%20an%0Aefficient%20and%20private%20algorithm%20with%20an%20excess%20error%20guarantee%20that%20scales%20with%0Athe%20%28unknown%29%20radius%20containing%20the%20majority%20of%20the%20datapoints%3F%20Our%20main%0Acontribution%20is%20a%20pair%20of%20polynomial-time%20DP%20algorithms%20for%20the%20task%20of%20private%0AGM%20with%20an%20excess%20error%20guarantee%20that%20scales%20with%20the%20effective%20diameter%20of%0Athe%20datapoints.%20Additionally%2C%20we%20propose%20an%20inefficient%20algorithm%20based%20on%20the%0Ainverse%20smooth%20sensitivity%20mechanism%2C%20which%20satisfies%20the%20more%20restrictive%0Anotion%20of%20pure%20DP.%20We%20complement%20our%20results%20with%20a%20lower%20bound%20and%20demonstrate%0Athe%20optimality%20of%20our%20polynomial-time%20algorithms%20in%20terms%20of%20sample%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07407v1&entry.124074799=Read"},
{"title": "Hate Speech Detection with Generalizable Target-aware Fairness", "author": "Tong Chen and Danny Wang and Xurong Liang and Marten Risius and Gianluca Demartini and Hongzhi Yin", "abstract": "  To counter the side effect brought by the proliferation of social media\nplatforms, hate speech detection (HSD) plays a vital role in halting the\ndissemination of toxic online posts at an early stage. However, given the\nubiquitous topical communities on social media, a trained HSD classifier easily\nbecomes biased towards specific targeted groups (e.g., female and black\npeople), where a high rate of false positive/negative results can significantly\nimpair public trust in the fairness of content moderation mechanisms, and\neventually harm the diversity of online society. Although existing\nfairness-aware HSD methods can smooth out some discrepancies across targeted\ngroups, they are mostly specific to a narrow selection of targets that are\nassumed to be known and fixed. This inevitably prevents those methods from\ngeneralizing to real-world use cases where new targeted groups constantly\nemerge over time. To tackle this defect, we propose Generalizable target-aware\nFairness (GetFair), a new method for fairly classifying each post that contains\ndiverse and even unseen targets during inference. To remove the HSD\nclassifier's spurious dependence on target-related features, GetFair trains a\nseries of filter functions in an adversarial pipeline, so as to deceive the\ndiscriminator that recovers the targeted group from filtered post embeddings.\nTo maintain scalability and generalizability, we innovatively parameterize all\nfilter functions via a hypernetwork that is regularized by the semantic\naffinity among targets. Taking a target's pretrained word embedding as input,\nthe hypernetwork generates the weights used by each target-specific filter\non-the-fly without storing dedicated filter parameters. Finally, comparative\nexperiments on two HSD datasets have shown advantageous performance of GetFair\non out-of-sample targets.\n", "link": "http://arxiv.org/abs/2406.00046v2", "date": "2024-06-11", "relevancy": 2.2608, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4544}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hate%20Speech%20Detection%20with%20Generalizable%20Target-aware%20Fairness&body=Title%3A%20Hate%20Speech%20Detection%20with%20Generalizable%20Target-aware%20Fairness%0AAuthor%3A%20Tong%20Chen%20and%20Danny%20Wang%20and%20Xurong%20Liang%20and%20Marten%20Risius%20and%20Gianluca%20Demartini%20and%20Hongzhi%20Yin%0AAbstract%3A%20%20%20To%20counter%20the%20side%20effect%20brought%20by%20the%20proliferation%20of%20social%20media%0Aplatforms%2C%20hate%20speech%20detection%20%28HSD%29%20plays%20a%20vital%20role%20in%20halting%20the%0Adissemination%20of%20toxic%20online%20posts%20at%20an%20early%20stage.%20However%2C%20given%20the%0Aubiquitous%20topical%20communities%20on%20social%20media%2C%20a%20trained%20HSD%20classifier%20easily%0Abecomes%20biased%20towards%20specific%20targeted%20groups%20%28e.g.%2C%20female%20and%20black%0Apeople%29%2C%20where%20a%20high%20rate%20of%20false%20positive/negative%20results%20can%20significantly%0Aimpair%20public%20trust%20in%20the%20fairness%20of%20content%20moderation%20mechanisms%2C%20and%0Aeventually%20harm%20the%20diversity%20of%20online%20society.%20Although%20existing%0Afairness-aware%20HSD%20methods%20can%20smooth%20out%20some%20discrepancies%20across%20targeted%0Agroups%2C%20they%20are%20mostly%20specific%20to%20a%20narrow%20selection%20of%20targets%20that%20are%0Aassumed%20to%20be%20known%20and%20fixed.%20This%20inevitably%20prevents%20those%20methods%20from%0Ageneralizing%20to%20real-world%20use%20cases%20where%20new%20targeted%20groups%20constantly%0Aemerge%20over%20time.%20To%20tackle%20this%20defect%2C%20we%20propose%20Generalizable%20target-aware%0AFairness%20%28GetFair%29%2C%20a%20new%20method%20for%20fairly%20classifying%20each%20post%20that%20contains%0Adiverse%20and%20even%20unseen%20targets%20during%20inference.%20To%20remove%20the%20HSD%0Aclassifier%27s%20spurious%20dependence%20on%20target-related%20features%2C%20GetFair%20trains%20a%0Aseries%20of%20filter%20functions%20in%20an%20adversarial%20pipeline%2C%20so%20as%20to%20deceive%20the%0Adiscriminator%20that%20recovers%20the%20targeted%20group%20from%20filtered%20post%20embeddings.%0ATo%20maintain%20scalability%20and%20generalizability%2C%20we%20innovatively%20parameterize%20all%0Afilter%20functions%20via%20a%20hypernetwork%20that%20is%20regularized%20by%20the%20semantic%0Aaffinity%20among%20targets.%20Taking%20a%20target%27s%20pretrained%20word%20embedding%20as%20input%2C%0Athe%20hypernetwork%20generates%20the%20weights%20used%20by%20each%20target-specific%20filter%0Aon-the-fly%20without%20storing%20dedicated%20filter%20parameters.%20Finally%2C%20comparative%0Aexperiments%20on%20two%20HSD%20datasets%20have%20shown%20advantageous%20performance%20of%20GetFair%0Aon%20out-of-sample%20targets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHate%2520Speech%2520Detection%2520with%2520Generalizable%2520Target-aware%2520Fairness%26entry.906535625%3DTong%2520Chen%2520and%2520Danny%2520Wang%2520and%2520Xurong%2520Liang%2520and%2520Marten%2520Risius%2520and%2520Gianluca%2520Demartini%2520and%2520Hongzhi%2520Yin%26entry.1292438233%3D%2520%2520To%2520counter%2520the%2520side%2520effect%2520brought%2520by%2520the%2520proliferation%2520of%2520social%2520media%250Aplatforms%252C%2520hate%2520speech%2520detection%2520%2528HSD%2529%2520plays%2520a%2520vital%2520role%2520in%2520halting%2520the%250Adissemination%2520of%2520toxic%2520online%2520posts%2520at%2520an%2520early%2520stage.%2520However%252C%2520given%2520the%250Aubiquitous%2520topical%2520communities%2520on%2520social%2520media%252C%2520a%2520trained%2520HSD%2520classifier%2520easily%250Abecomes%2520biased%2520towards%2520specific%2520targeted%2520groups%2520%2528e.g.%252C%2520female%2520and%2520black%250Apeople%2529%252C%2520where%2520a%2520high%2520rate%2520of%2520false%2520positive/negative%2520results%2520can%2520significantly%250Aimpair%2520public%2520trust%2520in%2520the%2520fairness%2520of%2520content%2520moderation%2520mechanisms%252C%2520and%250Aeventually%2520harm%2520the%2520diversity%2520of%2520online%2520society.%2520Although%2520existing%250Afairness-aware%2520HSD%2520methods%2520can%2520smooth%2520out%2520some%2520discrepancies%2520across%2520targeted%250Agroups%252C%2520they%2520are%2520mostly%2520specific%2520to%2520a%2520narrow%2520selection%2520of%2520targets%2520that%2520are%250Aassumed%2520to%2520be%2520known%2520and%2520fixed.%2520This%2520inevitably%2520prevents%2520those%2520methods%2520from%250Ageneralizing%2520to%2520real-world%2520use%2520cases%2520where%2520new%2520targeted%2520groups%2520constantly%250Aemerge%2520over%2520time.%2520To%2520tackle%2520this%2520defect%252C%2520we%2520propose%2520Generalizable%2520target-aware%250AFairness%2520%2528GetFair%2529%252C%2520a%2520new%2520method%2520for%2520fairly%2520classifying%2520each%2520post%2520that%2520contains%250Adiverse%2520and%2520even%2520unseen%2520targets%2520during%2520inference.%2520To%2520remove%2520the%2520HSD%250Aclassifier%2527s%2520spurious%2520dependence%2520on%2520target-related%2520features%252C%2520GetFair%2520trains%2520a%250Aseries%2520of%2520filter%2520functions%2520in%2520an%2520adversarial%2520pipeline%252C%2520so%2520as%2520to%2520deceive%2520the%250Adiscriminator%2520that%2520recovers%2520the%2520targeted%2520group%2520from%2520filtered%2520post%2520embeddings.%250ATo%2520maintain%2520scalability%2520and%2520generalizability%252C%2520we%2520innovatively%2520parameterize%2520all%250Afilter%2520functions%2520via%2520a%2520hypernetwork%2520that%2520is%2520regularized%2520by%2520the%2520semantic%250Aaffinity%2520among%2520targets.%2520Taking%2520a%2520target%2527s%2520pretrained%2520word%2520embedding%2520as%2520input%252C%250Athe%2520hypernetwork%2520generates%2520the%2520weights%2520used%2520by%2520each%2520target-specific%2520filter%250Aon-the-fly%2520without%2520storing%2520dedicated%2520filter%2520parameters.%2520Finally%252C%2520comparative%250Aexperiments%2520on%2520two%2520HSD%2520datasets%2520have%2520shown%2520advantageous%2520performance%2520of%2520GetFair%250Aon%2520out-of-sample%2520targets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hate%20Speech%20Detection%20with%20Generalizable%20Target-aware%20Fairness&entry.906535625=Tong%20Chen%20and%20Danny%20Wang%20and%20Xurong%20Liang%20and%20Marten%20Risius%20and%20Gianluca%20Demartini%20and%20Hongzhi%20Yin&entry.1292438233=%20%20To%20counter%20the%20side%20effect%20brought%20by%20the%20proliferation%20of%20social%20media%0Aplatforms%2C%20hate%20speech%20detection%20%28HSD%29%20plays%20a%20vital%20role%20in%20halting%20the%0Adissemination%20of%20toxic%20online%20posts%20at%20an%20early%20stage.%20However%2C%20given%20the%0Aubiquitous%20topical%20communities%20on%20social%20media%2C%20a%20trained%20HSD%20classifier%20easily%0Abecomes%20biased%20towards%20specific%20targeted%20groups%20%28e.g.%2C%20female%20and%20black%0Apeople%29%2C%20where%20a%20high%20rate%20of%20false%20positive/negative%20results%20can%20significantly%0Aimpair%20public%20trust%20in%20the%20fairness%20of%20content%20moderation%20mechanisms%2C%20and%0Aeventually%20harm%20the%20diversity%20of%20online%20society.%20Although%20existing%0Afairness-aware%20HSD%20methods%20can%20smooth%20out%20some%20discrepancies%20across%20targeted%0Agroups%2C%20they%20are%20mostly%20specific%20to%20a%20narrow%20selection%20of%20targets%20that%20are%0Aassumed%20to%20be%20known%20and%20fixed.%20This%20inevitably%20prevents%20those%20methods%20from%0Ageneralizing%20to%20real-world%20use%20cases%20where%20new%20targeted%20groups%20constantly%0Aemerge%20over%20time.%20To%20tackle%20this%20defect%2C%20we%20propose%20Generalizable%20target-aware%0AFairness%20%28GetFair%29%2C%20a%20new%20method%20for%20fairly%20classifying%20each%20post%20that%20contains%0Adiverse%20and%20even%20unseen%20targets%20during%20inference.%20To%20remove%20the%20HSD%0Aclassifier%27s%20spurious%20dependence%20on%20target-related%20features%2C%20GetFair%20trains%20a%0Aseries%20of%20filter%20functions%20in%20an%20adversarial%20pipeline%2C%20so%20as%20to%20deceive%20the%0Adiscriminator%20that%20recovers%20the%20targeted%20group%20from%20filtered%20post%20embeddings.%0ATo%20maintain%20scalability%20and%20generalizability%2C%20we%20innovatively%20parameterize%20all%0Afilter%20functions%20via%20a%20hypernetwork%20that%20is%20regularized%20by%20the%20semantic%0Aaffinity%20among%20targets.%20Taking%20a%20target%27s%20pretrained%20word%20embedding%20as%20input%2C%0Athe%20hypernetwork%20generates%20the%20weights%20used%20by%20each%20target-specific%20filter%0Aon-the-fly%20without%20storing%20dedicated%20filter%20parameters.%20Finally%2C%20comparative%0Aexperiments%20on%20two%20HSD%20datasets%20have%20shown%20advantageous%20performance%20of%20GetFair%0Aon%20out-of-sample%20targets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00046v2&entry.124074799=Read"},
{"title": "iMESA: Incremental Distributed Optimization for Collaborative\n  Simultaneous Localization and Mapping", "author": "Daniel McGann and Michael Kaess", "abstract": "  This paper introduces a novel incremental distributed back-end algorithm for\nCollaborative Simultaneous Localization and Mapping (C-SLAM). For real-world\ndeployments, robotic teams require algorithms to compute a consistent state\nestimate accurately, within online runtime constraints, and with potentially\nlimited communication. Existing centralized, decentralized, and distributed\napproaches to solving C-SLAM problems struggle to achieve all of these goals.\nTo address this capability gap, we present Incremental Manifold Edge-based\nSeparable ADMM (iMESA) a fully distributed C-SLAM back-end algorithm that can\nprovide a multi-robot team with accurate state estimates in real-time with only\nsparse pair-wise communication between robots. Extensive evaluation on real and\nsynthetic data demonstrates that iMESA is able to outperform comparable\nstate-of-the-art C-SLAM back-ends.\n", "link": "http://arxiv.org/abs/2406.07371v1", "date": "2024-06-11", "relevancy": 2.2478, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5829}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5518}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iMESA%3A%20Incremental%20Distributed%20Optimization%20for%20Collaborative%0A%20%20Simultaneous%20Localization%20and%20Mapping&body=Title%3A%20iMESA%3A%20Incremental%20Distributed%20Optimization%20for%20Collaborative%0A%20%20Simultaneous%20Localization%20and%20Mapping%0AAuthor%3A%20Daniel%20McGann%20and%20Michael%20Kaess%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20incremental%20distributed%20back-end%20algorithm%20for%0ACollaborative%20Simultaneous%20Localization%20and%20Mapping%20%28C-SLAM%29.%20For%20real-world%0Adeployments%2C%20robotic%20teams%20require%20algorithms%20to%20compute%20a%20consistent%20state%0Aestimate%20accurately%2C%20within%20online%20runtime%20constraints%2C%20and%20with%20potentially%0Alimited%20communication.%20Existing%20centralized%2C%20decentralized%2C%20and%20distributed%0Aapproaches%20to%20solving%20C-SLAM%20problems%20struggle%20to%20achieve%20all%20of%20these%20goals.%0ATo%20address%20this%20capability%20gap%2C%20we%20present%20Incremental%20Manifold%20Edge-based%0ASeparable%20ADMM%20%28iMESA%29%20a%20fully%20distributed%20C-SLAM%20back-end%20algorithm%20that%20can%0Aprovide%20a%20multi-robot%20team%20with%20accurate%20state%20estimates%20in%20real-time%20with%20only%0Asparse%20pair-wise%20communication%20between%20robots.%20Extensive%20evaluation%20on%20real%20and%0Asynthetic%20data%20demonstrates%20that%20iMESA%20is%20able%20to%20outperform%20comparable%0Astate-of-the-art%20C-SLAM%20back-ends.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiMESA%253A%2520Incremental%2520Distributed%2520Optimization%2520for%2520Collaborative%250A%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%26entry.906535625%3DDaniel%2520McGann%2520and%2520Michael%2520Kaess%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520incremental%2520distributed%2520back-end%2520algorithm%2520for%250ACollaborative%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528C-SLAM%2529.%2520For%2520real-world%250Adeployments%252C%2520robotic%2520teams%2520require%2520algorithms%2520to%2520compute%2520a%2520consistent%2520state%250Aestimate%2520accurately%252C%2520within%2520online%2520runtime%2520constraints%252C%2520and%2520with%2520potentially%250Alimited%2520communication.%2520Existing%2520centralized%252C%2520decentralized%252C%2520and%2520distributed%250Aapproaches%2520to%2520solving%2520C-SLAM%2520problems%2520struggle%2520to%2520achieve%2520all%2520of%2520these%2520goals.%250ATo%2520address%2520this%2520capability%2520gap%252C%2520we%2520present%2520Incremental%2520Manifold%2520Edge-based%250ASeparable%2520ADMM%2520%2528iMESA%2529%2520a%2520fully%2520distributed%2520C-SLAM%2520back-end%2520algorithm%2520that%2520can%250Aprovide%2520a%2520multi-robot%2520team%2520with%2520accurate%2520state%2520estimates%2520in%2520real-time%2520with%2520only%250Asparse%2520pair-wise%2520communication%2520between%2520robots.%2520Extensive%2520evaluation%2520on%2520real%2520and%250Asynthetic%2520data%2520demonstrates%2520that%2520iMESA%2520is%2520able%2520to%2520outperform%2520comparable%250Astate-of-the-art%2520C-SLAM%2520back-ends.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iMESA%3A%20Incremental%20Distributed%20Optimization%20for%20Collaborative%0A%20%20Simultaneous%20Localization%20and%20Mapping&entry.906535625=Daniel%20McGann%20and%20Michael%20Kaess&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20incremental%20distributed%20back-end%20algorithm%20for%0ACollaborative%20Simultaneous%20Localization%20and%20Mapping%20%28C-SLAM%29.%20For%20real-world%0Adeployments%2C%20robotic%20teams%20require%20algorithms%20to%20compute%20a%20consistent%20state%0Aestimate%20accurately%2C%20within%20online%20runtime%20constraints%2C%20and%20with%20potentially%0Alimited%20communication.%20Existing%20centralized%2C%20decentralized%2C%20and%20distributed%0Aapproaches%20to%20solving%20C-SLAM%20problems%20struggle%20to%20achieve%20all%20of%20these%20goals.%0ATo%20address%20this%20capability%20gap%2C%20we%20present%20Incremental%20Manifold%20Edge-based%0ASeparable%20ADMM%20%28iMESA%29%20a%20fully%20distributed%20C-SLAM%20back-end%20algorithm%20that%20can%0Aprovide%20a%20multi-robot%20team%20with%20accurate%20state%20estimates%20in%20real-time%20with%20only%0Asparse%20pair-wise%20communication%20between%20robots.%20Extensive%20evaluation%20on%20real%20and%0Asynthetic%20data%20demonstrates%20that%20iMESA%20is%20able%20to%20outperform%20comparable%0Astate-of-the-art%20C-SLAM%20back-ends.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07371v1&entry.124074799=Read"},
{"title": "Haptic Repurposing with GenAI", "author": "Haoyu Wang", "abstract": "  Mixed Reality aims to merge the digital and physical worlds to create\nimmersive human-computer interactions. Despite notable advancements, the\nabsence of realistic haptic feedback often breaks the immersive experience by\ncreating a disconnect between visual and tactile perceptions. This paper\nintroduces Haptic Repurposing with GenAI, an innovative approach to enhance MR\ninteractions by transforming any physical objects into adaptive haptic\ninterfaces for AI-generated virtual assets. Utilizing state-of-the-art\ngenerative AI models, this system captures both 2D and 3D features of physical\nobjects and, through user-directed prompts, generates corresponding virtual\nobjects that maintain the physical form of the original objects. Through\nmodel-based object tracking, the system dynamically anchors virtual assets to\nphysical props in real time, allowing objects to visually morph into any\nuser-specified virtual object. This paper details the system's development,\npresents findings from usability studies that validate its effectiveness, and\nexplores its potential to significantly enhance interactive MR environments.\nThe hope is this work can lay a foundation for further research into AI-driven\nspatial transformation in immersive and haptic technologies.\n", "link": "http://arxiv.org/abs/2406.07228v1", "date": "2024-06-11", "relevancy": 2.2427, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5797}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5475}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Haptic%20Repurposing%20with%20GenAI&body=Title%3A%20Haptic%20Repurposing%20with%20GenAI%0AAuthor%3A%20Haoyu%20Wang%0AAbstract%3A%20%20%20Mixed%20Reality%20aims%20to%20merge%20the%20digital%20and%20physical%20worlds%20to%20create%0Aimmersive%20human-computer%20interactions.%20Despite%20notable%20advancements%2C%20the%0Aabsence%20of%20realistic%20haptic%20feedback%20often%20breaks%20the%20immersive%20experience%20by%0Acreating%20a%20disconnect%20between%20visual%20and%20tactile%20perceptions.%20This%20paper%0Aintroduces%20Haptic%20Repurposing%20with%20GenAI%2C%20an%20innovative%20approach%20to%20enhance%20MR%0Ainteractions%20by%20transforming%20any%20physical%20objects%20into%20adaptive%20haptic%0Ainterfaces%20for%20AI-generated%20virtual%20assets.%20Utilizing%20state-of-the-art%0Agenerative%20AI%20models%2C%20this%20system%20captures%20both%202D%20and%203D%20features%20of%20physical%0Aobjects%20and%2C%20through%20user-directed%20prompts%2C%20generates%20corresponding%20virtual%0Aobjects%20that%20maintain%20the%20physical%20form%20of%20the%20original%20objects.%20Through%0Amodel-based%20object%20tracking%2C%20the%20system%20dynamically%20anchors%20virtual%20assets%20to%0Aphysical%20props%20in%20real%20time%2C%20allowing%20objects%20to%20visually%20morph%20into%20any%0Auser-specified%20virtual%20object.%20This%20paper%20details%20the%20system%27s%20development%2C%0Apresents%20findings%20from%20usability%20studies%20that%20validate%20its%20effectiveness%2C%20and%0Aexplores%20its%20potential%20to%20significantly%20enhance%20interactive%20MR%20environments.%0AThe%20hope%20is%20this%20work%20can%20lay%20a%20foundation%20for%20further%20research%20into%20AI-driven%0Aspatial%20transformation%20in%20immersive%20and%20haptic%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHaptic%2520Repurposing%2520with%2520GenAI%26entry.906535625%3DHaoyu%2520Wang%26entry.1292438233%3D%2520%2520Mixed%2520Reality%2520aims%2520to%2520merge%2520the%2520digital%2520and%2520physical%2520worlds%2520to%2520create%250Aimmersive%2520human-computer%2520interactions.%2520Despite%2520notable%2520advancements%252C%2520the%250Aabsence%2520of%2520realistic%2520haptic%2520feedback%2520often%2520breaks%2520the%2520immersive%2520experience%2520by%250Acreating%2520a%2520disconnect%2520between%2520visual%2520and%2520tactile%2520perceptions.%2520This%2520paper%250Aintroduces%2520Haptic%2520Repurposing%2520with%2520GenAI%252C%2520an%2520innovative%2520approach%2520to%2520enhance%2520MR%250Ainteractions%2520by%2520transforming%2520any%2520physical%2520objects%2520into%2520adaptive%2520haptic%250Ainterfaces%2520for%2520AI-generated%2520virtual%2520assets.%2520Utilizing%2520state-of-the-art%250Agenerative%2520AI%2520models%252C%2520this%2520system%2520captures%2520both%25202D%2520and%25203D%2520features%2520of%2520physical%250Aobjects%2520and%252C%2520through%2520user-directed%2520prompts%252C%2520generates%2520corresponding%2520virtual%250Aobjects%2520that%2520maintain%2520the%2520physical%2520form%2520of%2520the%2520original%2520objects.%2520Through%250Amodel-based%2520object%2520tracking%252C%2520the%2520system%2520dynamically%2520anchors%2520virtual%2520assets%2520to%250Aphysical%2520props%2520in%2520real%2520time%252C%2520allowing%2520objects%2520to%2520visually%2520morph%2520into%2520any%250Auser-specified%2520virtual%2520object.%2520This%2520paper%2520details%2520the%2520system%2527s%2520development%252C%250Apresents%2520findings%2520from%2520usability%2520studies%2520that%2520validate%2520its%2520effectiveness%252C%2520and%250Aexplores%2520its%2520potential%2520to%2520significantly%2520enhance%2520interactive%2520MR%2520environments.%250AThe%2520hope%2520is%2520this%2520work%2520can%2520lay%2520a%2520foundation%2520for%2520further%2520research%2520into%2520AI-driven%250Aspatial%2520transformation%2520in%2520immersive%2520and%2520haptic%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Haptic%20Repurposing%20with%20GenAI&entry.906535625=Haoyu%20Wang&entry.1292438233=%20%20Mixed%20Reality%20aims%20to%20merge%20the%20digital%20and%20physical%20worlds%20to%20create%0Aimmersive%20human-computer%20interactions.%20Despite%20notable%20advancements%2C%20the%0Aabsence%20of%20realistic%20haptic%20feedback%20often%20breaks%20the%20immersive%20experience%20by%0Acreating%20a%20disconnect%20between%20visual%20and%20tactile%20perceptions.%20This%20paper%0Aintroduces%20Haptic%20Repurposing%20with%20GenAI%2C%20an%20innovative%20approach%20to%20enhance%20MR%0Ainteractions%20by%20transforming%20any%20physical%20objects%20into%20adaptive%20haptic%0Ainterfaces%20for%20AI-generated%20virtual%20assets.%20Utilizing%20state-of-the-art%0Agenerative%20AI%20models%2C%20this%20system%20captures%20both%202D%20and%203D%20features%20of%20physical%0Aobjects%20and%2C%20through%20user-directed%20prompts%2C%20generates%20corresponding%20virtual%0Aobjects%20that%20maintain%20the%20physical%20form%20of%20the%20original%20objects.%20Through%0Amodel-based%20object%20tracking%2C%20the%20system%20dynamically%20anchors%20virtual%20assets%20to%0Aphysical%20props%20in%20real%20time%2C%20allowing%20objects%20to%20visually%20morph%20into%20any%0Auser-specified%20virtual%20object.%20This%20paper%20details%20the%20system%27s%20development%2C%0Apresents%20findings%20from%20usability%20studies%20that%20validate%20its%20effectiveness%2C%20and%0Aexplores%20its%20potential%20to%20significantly%20enhance%20interactive%20MR%20environments.%0AThe%20hope%20is%20this%20work%20can%20lay%20a%20foundation%20for%20further%20research%20into%20AI-driven%0Aspatial%20transformation%20in%20immersive%20and%20haptic%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07228v1&entry.124074799=Read"},
{"title": "Image Textualization: An Automatic Framework for Creating Accurate and\n  Detailed Image Descriptions", "author": "Renjie Pi and Jianshu Zhang and Jipeng Zhang and Rui Pan and Zhekai Chen and Tong Zhang", "abstract": "  Image description datasets play a crucial role in the advancement of various\napplications such as image understanding, text-to-image generation, and\ntext-image retrieval. Currently, image description datasets primarily originate\nfrom two sources. One source is the scraping of image-text pairs from the web.\nDespite their abundance, these descriptions are often of low quality and noisy.\nAnother is through human labeling. Datasets such as COCO are generally very\nshort and lack details. Although detailed image descriptions can be annotated\nby humans, the high annotation cost limits the feasibility. These limitations\nunderscore the need for more efficient and scalable methods to generate\naccurate and detailed image descriptions. In this paper, we propose an\ninnovative framework termed Image Textualization (IT), which automatically\nproduces high-quality image descriptions by leveraging existing multi-modal\nlarge language models (MLLMs) and multiple vision expert models in a\ncollaborative manner, which maximally convert the visual information into text.\nTo address the current lack of benchmarks for detailed descriptions, we propose\nseveral benchmarks for comprehensive evaluation, which verifies the quality of\nimage descriptions created by our framework. Furthermore, we show that\nLLaVA-7B, benefiting from training on IT-curated descriptions, acquire improved\ncapability to generate richer image descriptions, substantially increasing the\nlength and detail of their output with less hallucination.\n", "link": "http://arxiv.org/abs/2406.07502v1", "date": "2024-06-11", "relevancy": 2.2246, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5577}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5551}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Textualization%3A%20An%20Automatic%20Framework%20for%20Creating%20Accurate%20and%0A%20%20Detailed%20Image%20Descriptions&body=Title%3A%20Image%20Textualization%3A%20An%20Automatic%20Framework%20for%20Creating%20Accurate%20and%0A%20%20Detailed%20Image%20Descriptions%0AAuthor%3A%20Renjie%20Pi%20and%20Jianshu%20Zhang%20and%20Jipeng%20Zhang%20and%20Rui%20Pan%20and%20Zhekai%20Chen%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Image%20description%20datasets%20play%20a%20crucial%20role%20in%20the%20advancement%20of%20various%0Aapplications%20such%20as%20image%20understanding%2C%20text-to-image%20generation%2C%20and%0Atext-image%20retrieval.%20Currently%2C%20image%20description%20datasets%20primarily%20originate%0Afrom%20two%20sources.%20One%20source%20is%20the%20scraping%20of%20image-text%20pairs%20from%20the%20web.%0ADespite%20their%20abundance%2C%20these%20descriptions%20are%20often%20of%20low%20quality%20and%20noisy.%0AAnother%20is%20through%20human%20labeling.%20Datasets%20such%20as%20COCO%20are%20generally%20very%0Ashort%20and%20lack%20details.%20Although%20detailed%20image%20descriptions%20can%20be%20annotated%0Aby%20humans%2C%20the%20high%20annotation%20cost%20limits%20the%20feasibility.%20These%20limitations%0Aunderscore%20the%20need%20for%20more%20efficient%20and%20scalable%20methods%20to%20generate%0Aaccurate%20and%20detailed%20image%20descriptions.%20In%20this%20paper%2C%20we%20propose%20an%0Ainnovative%20framework%20termed%20Image%20Textualization%20%28IT%29%2C%20which%20automatically%0Aproduces%20high-quality%20image%20descriptions%20by%20leveraging%20existing%20multi-modal%0Alarge%20language%20models%20%28MLLMs%29%20and%20multiple%20vision%20expert%20models%20in%20a%0Acollaborative%20manner%2C%20which%20maximally%20convert%20the%20visual%20information%20into%20text.%0ATo%20address%20the%20current%20lack%20of%20benchmarks%20for%20detailed%20descriptions%2C%20we%20propose%0Aseveral%20benchmarks%20for%20comprehensive%20evaluation%2C%20which%20verifies%20the%20quality%20of%0Aimage%20descriptions%20created%20by%20our%20framework.%20Furthermore%2C%20we%20show%20that%0ALLaVA-7B%2C%20benefiting%20from%20training%20on%20IT-curated%20descriptions%2C%20acquire%20improved%0Acapability%20to%20generate%20richer%20image%20descriptions%2C%20substantially%20increasing%20the%0Alength%20and%20detail%20of%20their%20output%20with%20less%20hallucination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Textualization%253A%2520An%2520Automatic%2520Framework%2520for%2520Creating%2520Accurate%2520and%250A%2520%2520Detailed%2520Image%2520Descriptions%26entry.906535625%3DRenjie%2520Pi%2520and%2520Jianshu%2520Zhang%2520and%2520Jipeng%2520Zhang%2520and%2520Rui%2520Pan%2520and%2520Zhekai%2520Chen%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Image%2520description%2520datasets%2520play%2520a%2520crucial%2520role%2520in%2520the%2520advancement%2520of%2520various%250Aapplications%2520such%2520as%2520image%2520understanding%252C%2520text-to-image%2520generation%252C%2520and%250Atext-image%2520retrieval.%2520Currently%252C%2520image%2520description%2520datasets%2520primarily%2520originate%250Afrom%2520two%2520sources.%2520One%2520source%2520is%2520the%2520scraping%2520of%2520image-text%2520pairs%2520from%2520the%2520web.%250ADespite%2520their%2520abundance%252C%2520these%2520descriptions%2520are%2520often%2520of%2520low%2520quality%2520and%2520noisy.%250AAnother%2520is%2520through%2520human%2520labeling.%2520Datasets%2520such%2520as%2520COCO%2520are%2520generally%2520very%250Ashort%2520and%2520lack%2520details.%2520Although%2520detailed%2520image%2520descriptions%2520can%2520be%2520annotated%250Aby%2520humans%252C%2520the%2520high%2520annotation%2520cost%2520limits%2520the%2520feasibility.%2520These%2520limitations%250Aunderscore%2520the%2520need%2520for%2520more%2520efficient%2520and%2520scalable%2520methods%2520to%2520generate%250Aaccurate%2520and%2520detailed%2520image%2520descriptions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Ainnovative%2520framework%2520termed%2520Image%2520Textualization%2520%2528IT%2529%252C%2520which%2520automatically%250Aproduces%2520high-quality%2520image%2520descriptions%2520by%2520leveraging%2520existing%2520multi-modal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520and%2520multiple%2520vision%2520expert%2520models%2520in%2520a%250Acollaborative%2520manner%252C%2520which%2520maximally%2520convert%2520the%2520visual%2520information%2520into%2520text.%250ATo%2520address%2520the%2520current%2520lack%2520of%2520benchmarks%2520for%2520detailed%2520descriptions%252C%2520we%2520propose%250Aseveral%2520benchmarks%2520for%2520comprehensive%2520evaluation%252C%2520which%2520verifies%2520the%2520quality%2520of%250Aimage%2520descriptions%2520created%2520by%2520our%2520framework.%2520Furthermore%252C%2520we%2520show%2520that%250ALLaVA-7B%252C%2520benefiting%2520from%2520training%2520on%2520IT-curated%2520descriptions%252C%2520acquire%2520improved%250Acapability%2520to%2520generate%2520richer%2520image%2520descriptions%252C%2520substantially%2520increasing%2520the%250Alength%2520and%2520detail%2520of%2520their%2520output%2520with%2520less%2520hallucination.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Textualization%3A%20An%20Automatic%20Framework%20for%20Creating%20Accurate%20and%0A%20%20Detailed%20Image%20Descriptions&entry.906535625=Renjie%20Pi%20and%20Jianshu%20Zhang%20and%20Jipeng%20Zhang%20and%20Rui%20Pan%20and%20Zhekai%20Chen%20and%20Tong%20Zhang&entry.1292438233=%20%20Image%20description%20datasets%20play%20a%20crucial%20role%20in%20the%20advancement%20of%20various%0Aapplications%20such%20as%20image%20understanding%2C%20text-to-image%20generation%2C%20and%0Atext-image%20retrieval.%20Currently%2C%20image%20description%20datasets%20primarily%20originate%0Afrom%20two%20sources.%20One%20source%20is%20the%20scraping%20of%20image-text%20pairs%20from%20the%20web.%0ADespite%20their%20abundance%2C%20these%20descriptions%20are%20often%20of%20low%20quality%20and%20noisy.%0AAnother%20is%20through%20human%20labeling.%20Datasets%20such%20as%20COCO%20are%20generally%20very%0Ashort%20and%20lack%20details.%20Although%20detailed%20image%20descriptions%20can%20be%20annotated%0Aby%20humans%2C%20the%20high%20annotation%20cost%20limits%20the%20feasibility.%20These%20limitations%0Aunderscore%20the%20need%20for%20more%20efficient%20and%20scalable%20methods%20to%20generate%0Aaccurate%20and%20detailed%20image%20descriptions.%20In%20this%20paper%2C%20we%20propose%20an%0Ainnovative%20framework%20termed%20Image%20Textualization%20%28IT%29%2C%20which%20automatically%0Aproduces%20high-quality%20image%20descriptions%20by%20leveraging%20existing%20multi-modal%0Alarge%20language%20models%20%28MLLMs%29%20and%20multiple%20vision%20expert%20models%20in%20a%0Acollaborative%20manner%2C%20which%20maximally%20convert%20the%20visual%20information%20into%20text.%0ATo%20address%20the%20current%20lack%20of%20benchmarks%20for%20detailed%20descriptions%2C%20we%20propose%0Aseveral%20benchmarks%20for%20comprehensive%20evaluation%2C%20which%20verifies%20the%20quality%20of%0Aimage%20descriptions%20created%20by%20our%20framework.%20Furthermore%2C%20we%20show%20that%0ALLaVA-7B%2C%20benefiting%20from%20training%20on%20IT-curated%20descriptions%2C%20acquire%20improved%0Acapability%20to%20generate%20richer%20image%20descriptions%2C%20substantially%20increasing%20the%0Alength%20and%20detail%20of%20their%20output%20with%20less%20hallucination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07502v1&entry.124074799=Read"},
{"title": "MeMSVD: Long-Range Temporal Structure Capturing Using Incremental SVD", "author": "Ioanna Ntinou and Enrique Sanchez and Georgios Tzimiropoulos", "abstract": "  This paper is on long-term video understanding where the goal is to recognise\nhuman actions over long temporal windows (up to minutes long). In prior work,\nlong temporal context is captured by constructing a long-term memory bank\nconsisting of past and future video features which are then integrated into\nstandard (short-term) video recognition backbones through the use of attention\nmechanisms. Two well-known problems related to this approach are the quadratic\ncomplexity of the attention operation and the fact that the whole feature bank\nmust be stored in memory for inference. To address both issues, we propose an\nalternative to attention-based schemes which is based on a low-rank\napproximation of the memory obtained using Singular Value Decomposition. Our\nscheme has two advantages: (a) it reduces complexity by more than an order of\nmagnitude, and (b) it is amenable to an efficient implementation for the\ncalculation of the memory bases in an incremental fashion which does not\nrequire the storage of the whole feature bank in memory. The proposed scheme\nmatches or surpasses the accuracy achieved by attention-based mechanisms while\nbeing memory-efficient. Through extensive experiments, we demonstrate that our\nframework generalises to different architectures and tasks, outperforming the\nstate-of-the-art in three datasets.\n", "link": "http://arxiv.org/abs/2406.07191v1", "date": "2024-06-11", "relevancy": 2.2189, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5674}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5462}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeMSVD%3A%20Long-Range%20Temporal%20Structure%20Capturing%20Using%20Incremental%20SVD&body=Title%3A%20MeMSVD%3A%20Long-Range%20Temporal%20Structure%20Capturing%20Using%20Incremental%20SVD%0AAuthor%3A%20Ioanna%20Ntinou%20and%20Enrique%20Sanchez%20and%20Georgios%20Tzimiropoulos%0AAbstract%3A%20%20%20This%20paper%20is%20on%20long-term%20video%20understanding%20where%20the%20goal%20is%20to%20recognise%0Ahuman%20actions%20over%20long%20temporal%20windows%20%28up%20to%20minutes%20long%29.%20In%20prior%20work%2C%0Along%20temporal%20context%20is%20captured%20by%20constructing%20a%20long-term%20memory%20bank%0Aconsisting%20of%20past%20and%20future%20video%20features%20which%20are%20then%20integrated%20into%0Astandard%20%28short-term%29%20video%20recognition%20backbones%20through%20the%20use%20of%20attention%0Amechanisms.%20Two%20well-known%20problems%20related%20to%20this%20approach%20are%20the%20quadratic%0Acomplexity%20of%20the%20attention%20operation%20and%20the%20fact%20that%20the%20whole%20feature%20bank%0Amust%20be%20stored%20in%20memory%20for%20inference.%20To%20address%20both%20issues%2C%20we%20propose%20an%0Aalternative%20to%20attention-based%20schemes%20which%20is%20based%20on%20a%20low-rank%0Aapproximation%20of%20the%20memory%20obtained%20using%20Singular%20Value%20Decomposition.%20Our%0Ascheme%20has%20two%20advantages%3A%20%28a%29%20it%20reduces%20complexity%20by%20more%20than%20an%20order%20of%0Amagnitude%2C%20and%20%28b%29%20it%20is%20amenable%20to%20an%20efficient%20implementation%20for%20the%0Acalculation%20of%20the%20memory%20bases%20in%20an%20incremental%20fashion%20which%20does%20not%0Arequire%20the%20storage%20of%20the%20whole%20feature%20bank%20in%20memory.%20The%20proposed%20scheme%0Amatches%20or%20surpasses%20the%20accuracy%20achieved%20by%20attention-based%20mechanisms%20while%0Abeing%20memory-efficient.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%0Aframework%20generalises%20to%20different%20architectures%20and%20tasks%2C%20outperforming%20the%0Astate-of-the-art%20in%20three%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeMSVD%253A%2520Long-Range%2520Temporal%2520Structure%2520Capturing%2520Using%2520Incremental%2520SVD%26entry.906535625%3DIoanna%2520Ntinou%2520and%2520Enrique%2520Sanchez%2520and%2520Georgios%2520Tzimiropoulos%26entry.1292438233%3D%2520%2520This%2520paper%2520is%2520on%2520long-term%2520video%2520understanding%2520where%2520the%2520goal%2520is%2520to%2520recognise%250Ahuman%2520actions%2520over%2520long%2520temporal%2520windows%2520%2528up%2520to%2520minutes%2520long%2529.%2520In%2520prior%2520work%252C%250Along%2520temporal%2520context%2520is%2520captured%2520by%2520constructing%2520a%2520long-term%2520memory%2520bank%250Aconsisting%2520of%2520past%2520and%2520future%2520video%2520features%2520which%2520are%2520then%2520integrated%2520into%250Astandard%2520%2528short-term%2529%2520video%2520recognition%2520backbones%2520through%2520the%2520use%2520of%2520attention%250Amechanisms.%2520Two%2520well-known%2520problems%2520related%2520to%2520this%2520approach%2520are%2520the%2520quadratic%250Acomplexity%2520of%2520the%2520attention%2520operation%2520and%2520the%2520fact%2520that%2520the%2520whole%2520feature%2520bank%250Amust%2520be%2520stored%2520in%2520memory%2520for%2520inference.%2520To%2520address%2520both%2520issues%252C%2520we%2520propose%2520an%250Aalternative%2520to%2520attention-based%2520schemes%2520which%2520is%2520based%2520on%2520a%2520low-rank%250Aapproximation%2520of%2520the%2520memory%2520obtained%2520using%2520Singular%2520Value%2520Decomposition.%2520Our%250Ascheme%2520has%2520two%2520advantages%253A%2520%2528a%2529%2520it%2520reduces%2520complexity%2520by%2520more%2520than%2520an%2520order%2520of%250Amagnitude%252C%2520and%2520%2528b%2529%2520it%2520is%2520amenable%2520to%2520an%2520efficient%2520implementation%2520for%2520the%250Acalculation%2520of%2520the%2520memory%2520bases%2520in%2520an%2520incremental%2520fashion%2520which%2520does%2520not%250Arequire%2520the%2520storage%2520of%2520the%2520whole%2520feature%2520bank%2520in%2520memory.%2520The%2520proposed%2520scheme%250Amatches%2520or%2520surpasses%2520the%2520accuracy%2520achieved%2520by%2520attention-based%2520mechanisms%2520while%250Abeing%2520memory-efficient.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%250Aframework%2520generalises%2520to%2520different%2520architectures%2520and%2520tasks%252C%2520outperforming%2520the%250Astate-of-the-art%2520in%2520three%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeMSVD%3A%20Long-Range%20Temporal%20Structure%20Capturing%20Using%20Incremental%20SVD&entry.906535625=Ioanna%20Ntinou%20and%20Enrique%20Sanchez%20and%20Georgios%20Tzimiropoulos&entry.1292438233=%20%20This%20paper%20is%20on%20long-term%20video%20understanding%20where%20the%20goal%20is%20to%20recognise%0Ahuman%20actions%20over%20long%20temporal%20windows%20%28up%20to%20minutes%20long%29.%20In%20prior%20work%2C%0Along%20temporal%20context%20is%20captured%20by%20constructing%20a%20long-term%20memory%20bank%0Aconsisting%20of%20past%20and%20future%20video%20features%20which%20are%20then%20integrated%20into%0Astandard%20%28short-term%29%20video%20recognition%20backbones%20through%20the%20use%20of%20attention%0Amechanisms.%20Two%20well-known%20problems%20related%20to%20this%20approach%20are%20the%20quadratic%0Acomplexity%20of%20the%20attention%20operation%20and%20the%20fact%20that%20the%20whole%20feature%20bank%0Amust%20be%20stored%20in%20memory%20for%20inference.%20To%20address%20both%20issues%2C%20we%20propose%20an%0Aalternative%20to%20attention-based%20schemes%20which%20is%20based%20on%20a%20low-rank%0Aapproximation%20of%20the%20memory%20obtained%20using%20Singular%20Value%20Decomposition.%20Our%0Ascheme%20has%20two%20advantages%3A%20%28a%29%20it%20reduces%20complexity%20by%20more%20than%20an%20order%20of%0Amagnitude%2C%20and%20%28b%29%20it%20is%20amenable%20to%20an%20efficient%20implementation%20for%20the%0Acalculation%20of%20the%20memory%20bases%20in%20an%20incremental%20fashion%20which%20does%20not%0Arequire%20the%20storage%20of%20the%20whole%20feature%20bank%20in%20memory.%20The%20proposed%20scheme%0Amatches%20or%20surpasses%20the%20accuracy%20achieved%20by%20attention-based%20mechanisms%20while%0Abeing%20memory-efficient.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%0Aframework%20generalises%20to%20different%20architectures%20and%20tasks%2C%20outperforming%20the%0Astate-of-the-art%20in%20three%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07191v1&entry.124074799=Read"},
{"title": "Post-hoc Orthogonalization for Mitigation of Protected Feature Bias in\n  CXR Embeddings", "author": "Tobias Weber and Michael Ingrisch and Bernd Bischl and David R\u00fcgamer", "abstract": "  Purpose: To analyze and remove protected feature effects in chest radiograph\nembeddings of deep learning models. Methods: An orthogonalization is utilized\nto remove the influence of protected features (e.g., age, sex, race) in CXR\nembeddings, ensuring feature-independent results. To validate the efficacy of\nthe approach, we retrospectively study the MIMIC and CheXpert datasets using\nthree pre-trained models, namely a supervised contrastive, a self-supervised\ncontrastive, and a baseline classifier model. Our statistical analysis involves\ncomparing the original versus the orthogonalized embeddings by estimating\nprotected feature influences and evaluating the ability to predict race, age,\nor sex using the two types of embeddings. Results: Our experiments reveal a\nsignificant influence of protected features on predictions of pathologies.\nApplying orthogonalization removes these feature effects. Apart from removing\nany influence on pathology classification, while maintaining competitive\npredictive performance, orthogonalized embeddings further make it infeasible to\ndirectly predict protected attributes and mitigate subgroup disparities.\nConclusion: The presented work demonstrates the successful application and\nevaluation of the orthogonalization technique in the domain of chest X-ray\nimage classification.\n", "link": "http://arxiv.org/abs/2311.01349v2", "date": "2024-06-11", "relevancy": 2.2085, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4487}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4398}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Post-hoc%20Orthogonalization%20for%20Mitigation%20of%20Protected%20Feature%20Bias%20in%0A%20%20CXR%20Embeddings&body=Title%3A%20Post-hoc%20Orthogonalization%20for%20Mitigation%20of%20Protected%20Feature%20Bias%20in%0A%20%20CXR%20Embeddings%0AAuthor%3A%20Tobias%20Weber%20and%20Michael%20Ingrisch%20and%20Bernd%20Bischl%20and%20David%20R%C3%BCgamer%0AAbstract%3A%20%20%20Purpose%3A%20To%20analyze%20and%20remove%20protected%20feature%20effects%20in%20chest%20radiograph%0Aembeddings%20of%20deep%20learning%20models.%20Methods%3A%20An%20orthogonalization%20is%20utilized%0Ato%20remove%20the%20influence%20of%20protected%20features%20%28e.g.%2C%20age%2C%20sex%2C%20race%29%20in%20CXR%0Aembeddings%2C%20ensuring%20feature-independent%20results.%20To%20validate%20the%20efficacy%20of%0Athe%20approach%2C%20we%20retrospectively%20study%20the%20MIMIC%20and%20CheXpert%20datasets%20using%0Athree%20pre-trained%20models%2C%20namely%20a%20supervised%20contrastive%2C%20a%20self-supervised%0Acontrastive%2C%20and%20a%20baseline%20classifier%20model.%20Our%20statistical%20analysis%20involves%0Acomparing%20the%20original%20versus%20the%20orthogonalized%20embeddings%20by%20estimating%0Aprotected%20feature%20influences%20and%20evaluating%20the%20ability%20to%20predict%20race%2C%20age%2C%0Aor%20sex%20using%20the%20two%20types%20of%20embeddings.%20Results%3A%20Our%20experiments%20reveal%20a%0Asignificant%20influence%20of%20protected%20features%20on%20predictions%20of%20pathologies.%0AApplying%20orthogonalization%20removes%20these%20feature%20effects.%20Apart%20from%20removing%0Aany%20influence%20on%20pathology%20classification%2C%20while%20maintaining%20competitive%0Apredictive%20performance%2C%20orthogonalized%20embeddings%20further%20make%20it%20infeasible%20to%0Adirectly%20predict%20protected%20attributes%20and%20mitigate%20subgroup%20disparities.%0AConclusion%3A%20The%20presented%20work%20demonstrates%20the%20successful%20application%20and%0Aevaluation%20of%20the%20orthogonalization%20technique%20in%20the%20domain%20of%20chest%20X-ray%0Aimage%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPost-hoc%2520Orthogonalization%2520for%2520Mitigation%2520of%2520Protected%2520Feature%2520Bias%2520in%250A%2520%2520CXR%2520Embeddings%26entry.906535625%3DTobias%2520Weber%2520and%2520Michael%2520Ingrisch%2520and%2520Bernd%2520Bischl%2520and%2520David%2520R%25C3%25BCgamer%26entry.1292438233%3D%2520%2520Purpose%253A%2520To%2520analyze%2520and%2520remove%2520protected%2520feature%2520effects%2520in%2520chest%2520radiograph%250Aembeddings%2520of%2520deep%2520learning%2520models.%2520Methods%253A%2520An%2520orthogonalization%2520is%2520utilized%250Ato%2520remove%2520the%2520influence%2520of%2520protected%2520features%2520%2528e.g.%252C%2520age%252C%2520sex%252C%2520race%2529%2520in%2520CXR%250Aembeddings%252C%2520ensuring%2520feature-independent%2520results.%2520To%2520validate%2520the%2520efficacy%2520of%250Athe%2520approach%252C%2520we%2520retrospectively%2520study%2520the%2520MIMIC%2520and%2520CheXpert%2520datasets%2520using%250Athree%2520pre-trained%2520models%252C%2520namely%2520a%2520supervised%2520contrastive%252C%2520a%2520self-supervised%250Acontrastive%252C%2520and%2520a%2520baseline%2520classifier%2520model.%2520Our%2520statistical%2520analysis%2520involves%250Acomparing%2520the%2520original%2520versus%2520the%2520orthogonalized%2520embeddings%2520by%2520estimating%250Aprotected%2520feature%2520influences%2520and%2520evaluating%2520the%2520ability%2520to%2520predict%2520race%252C%2520age%252C%250Aor%2520sex%2520using%2520the%2520two%2520types%2520of%2520embeddings.%2520Results%253A%2520Our%2520experiments%2520reveal%2520a%250Asignificant%2520influence%2520of%2520protected%2520features%2520on%2520predictions%2520of%2520pathologies.%250AApplying%2520orthogonalization%2520removes%2520these%2520feature%2520effects.%2520Apart%2520from%2520removing%250Aany%2520influence%2520on%2520pathology%2520classification%252C%2520while%2520maintaining%2520competitive%250Apredictive%2520performance%252C%2520orthogonalized%2520embeddings%2520further%2520make%2520it%2520infeasible%2520to%250Adirectly%2520predict%2520protected%2520attributes%2520and%2520mitigate%2520subgroup%2520disparities.%250AConclusion%253A%2520The%2520presented%2520work%2520demonstrates%2520the%2520successful%2520application%2520and%250Aevaluation%2520of%2520the%2520orthogonalization%2520technique%2520in%2520the%2520domain%2520of%2520chest%2520X-ray%250Aimage%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Post-hoc%20Orthogonalization%20for%20Mitigation%20of%20Protected%20Feature%20Bias%20in%0A%20%20CXR%20Embeddings&entry.906535625=Tobias%20Weber%20and%20Michael%20Ingrisch%20and%20Bernd%20Bischl%20and%20David%20R%C3%BCgamer&entry.1292438233=%20%20Purpose%3A%20To%20analyze%20and%20remove%20protected%20feature%20effects%20in%20chest%20radiograph%0Aembeddings%20of%20deep%20learning%20models.%20Methods%3A%20An%20orthogonalization%20is%20utilized%0Ato%20remove%20the%20influence%20of%20protected%20features%20%28e.g.%2C%20age%2C%20sex%2C%20race%29%20in%20CXR%0Aembeddings%2C%20ensuring%20feature-independent%20results.%20To%20validate%20the%20efficacy%20of%0Athe%20approach%2C%20we%20retrospectively%20study%20the%20MIMIC%20and%20CheXpert%20datasets%20using%0Athree%20pre-trained%20models%2C%20namely%20a%20supervised%20contrastive%2C%20a%20self-supervised%0Acontrastive%2C%20and%20a%20baseline%20classifier%20model.%20Our%20statistical%20analysis%20involves%0Acomparing%20the%20original%20versus%20the%20orthogonalized%20embeddings%20by%20estimating%0Aprotected%20feature%20influences%20and%20evaluating%20the%20ability%20to%20predict%20race%2C%20age%2C%0Aor%20sex%20using%20the%20two%20types%20of%20embeddings.%20Results%3A%20Our%20experiments%20reveal%20a%0Asignificant%20influence%20of%20protected%20features%20on%20predictions%20of%20pathologies.%0AApplying%20orthogonalization%20removes%20these%20feature%20effects.%20Apart%20from%20removing%0Aany%20influence%20on%20pathology%20classification%2C%20while%20maintaining%20competitive%0Apredictive%20performance%2C%20orthogonalized%20embeddings%20further%20make%20it%20infeasible%20to%0Adirectly%20predict%20protected%20attributes%20and%20mitigate%20subgroup%20disparities.%0AConclusion%3A%20The%20presented%20work%20demonstrates%20the%20successful%20application%20and%0Aevaluation%20of%20the%20orthogonalization%20technique%20in%20the%20domain%20of%20chest%20X-ray%0Aimage%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01349v2&entry.124074799=Read"},
{"title": "Active Scout: Multi-Target Tracking Using Neural Radiance Fields in\n  Dense Urban Environments", "author": "Christopher D. Hsu and Pratik Chaudhari", "abstract": "  We study pursuit-evasion games in highly occluded urban environments, e.g.\ntall buildings in a city, where a scout (quadrotor) tracks multiple dynamic\ntargets on the ground. We show that we can build a neural radiance field (NeRF)\nrepresentation of the city -- online -- using RGB and depth images from\ndifferent vantage points. This representation is used to calculate the\ninformation gain to both explore unknown parts of the city and track the\ntargets -- thereby giving a completely first-principles approach to actively\ntracking dynamic targets. We demonstrate, using a custom-built simulator using\nOpen Street Maps data of Philadelphia and New York City, that we can explore\nand locate 20 stationary targets within 300 steps. This is slower than a greedy\nbaseline which which does not use active perception. But for dynamic targets\nthat actively hide behind occlusions, we show that our approach maintains, at\nworst, a tracking error of 200m; the greedy baseline can have a tracking error\nas large as 600m. We observe a number of interesting properties in the scout's\npolicies, e.g., it switches its attention to track a different target\nperiodically, as the quality of the NeRF representation improves over time, the\nscout also becomes better in terms of target tracking.\n", "link": "http://arxiv.org/abs/2406.07431v1", "date": "2024-06-11", "relevancy": 2.2065, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5453}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Scout%3A%20Multi-Target%20Tracking%20Using%20Neural%20Radiance%20Fields%20in%0A%20%20Dense%20Urban%20Environments&body=Title%3A%20Active%20Scout%3A%20Multi-Target%20Tracking%20Using%20Neural%20Radiance%20Fields%20in%0A%20%20Dense%20Urban%20Environments%0AAuthor%3A%20Christopher%20D.%20Hsu%20and%20Pratik%20Chaudhari%0AAbstract%3A%20%20%20We%20study%20pursuit-evasion%20games%20in%20highly%20occluded%20urban%20environments%2C%20e.g.%0Atall%20buildings%20in%20a%20city%2C%20where%20a%20scout%20%28quadrotor%29%20tracks%20multiple%20dynamic%0Atargets%20on%20the%20ground.%20We%20show%20that%20we%20can%20build%20a%20neural%20radiance%20field%20%28NeRF%29%0Arepresentation%20of%20the%20city%20--%20online%20--%20using%20RGB%20and%20depth%20images%20from%0Adifferent%20vantage%20points.%20This%20representation%20is%20used%20to%20calculate%20the%0Ainformation%20gain%20to%20both%20explore%20unknown%20parts%20of%20the%20city%20and%20track%20the%0Atargets%20--%20thereby%20giving%20a%20completely%20first-principles%20approach%20to%20actively%0Atracking%20dynamic%20targets.%20We%20demonstrate%2C%20using%20a%20custom-built%20simulator%20using%0AOpen%20Street%20Maps%20data%20of%20Philadelphia%20and%20New%20York%20City%2C%20that%20we%20can%20explore%0Aand%20locate%2020%20stationary%20targets%20within%20300%20steps.%20This%20is%20slower%20than%20a%20greedy%0Abaseline%20which%20which%20does%20not%20use%20active%20perception.%20But%20for%20dynamic%20targets%0Athat%20actively%20hide%20behind%20occlusions%2C%20we%20show%20that%20our%20approach%20maintains%2C%20at%0Aworst%2C%20a%20tracking%20error%20of%20200m%3B%20the%20greedy%20baseline%20can%20have%20a%20tracking%20error%0Aas%20large%20as%20600m.%20We%20observe%20a%20number%20of%20interesting%20properties%20in%20the%20scout%27s%0Apolicies%2C%20e.g.%2C%20it%20switches%20its%20attention%20to%20track%20a%20different%20target%0Aperiodically%2C%20as%20the%20quality%20of%20the%20NeRF%20representation%20improves%20over%20time%2C%20the%0Ascout%20also%20becomes%20better%20in%20terms%20of%20target%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Scout%253A%2520Multi-Target%2520Tracking%2520Using%2520Neural%2520Radiance%2520Fields%2520in%250A%2520%2520Dense%2520Urban%2520Environments%26entry.906535625%3DChristopher%2520D.%2520Hsu%2520and%2520Pratik%2520Chaudhari%26entry.1292438233%3D%2520%2520We%2520study%2520pursuit-evasion%2520games%2520in%2520highly%2520occluded%2520urban%2520environments%252C%2520e.g.%250Atall%2520buildings%2520in%2520a%2520city%252C%2520where%2520a%2520scout%2520%2528quadrotor%2529%2520tracks%2520multiple%2520dynamic%250Atargets%2520on%2520the%2520ground.%2520We%2520show%2520that%2520we%2520can%2520build%2520a%2520neural%2520radiance%2520field%2520%2528NeRF%2529%250Arepresentation%2520of%2520the%2520city%2520--%2520online%2520--%2520using%2520RGB%2520and%2520depth%2520images%2520from%250Adifferent%2520vantage%2520points.%2520This%2520representation%2520is%2520used%2520to%2520calculate%2520the%250Ainformation%2520gain%2520to%2520both%2520explore%2520unknown%2520parts%2520of%2520the%2520city%2520and%2520track%2520the%250Atargets%2520--%2520thereby%2520giving%2520a%2520completely%2520first-principles%2520approach%2520to%2520actively%250Atracking%2520dynamic%2520targets.%2520We%2520demonstrate%252C%2520using%2520a%2520custom-built%2520simulator%2520using%250AOpen%2520Street%2520Maps%2520data%2520of%2520Philadelphia%2520and%2520New%2520York%2520City%252C%2520that%2520we%2520can%2520explore%250Aand%2520locate%252020%2520stationary%2520targets%2520within%2520300%2520steps.%2520This%2520is%2520slower%2520than%2520a%2520greedy%250Abaseline%2520which%2520which%2520does%2520not%2520use%2520active%2520perception.%2520But%2520for%2520dynamic%2520targets%250Athat%2520actively%2520hide%2520behind%2520occlusions%252C%2520we%2520show%2520that%2520our%2520approach%2520maintains%252C%2520at%250Aworst%252C%2520a%2520tracking%2520error%2520of%2520200m%253B%2520the%2520greedy%2520baseline%2520can%2520have%2520a%2520tracking%2520error%250Aas%2520large%2520as%2520600m.%2520We%2520observe%2520a%2520number%2520of%2520interesting%2520properties%2520in%2520the%2520scout%2527s%250Apolicies%252C%2520e.g.%252C%2520it%2520switches%2520its%2520attention%2520to%2520track%2520a%2520different%2520target%250Aperiodically%252C%2520as%2520the%2520quality%2520of%2520the%2520NeRF%2520representation%2520improves%2520over%2520time%252C%2520the%250Ascout%2520also%2520becomes%2520better%2520in%2520terms%2520of%2520target%2520tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Scout%3A%20Multi-Target%20Tracking%20Using%20Neural%20Radiance%20Fields%20in%0A%20%20Dense%20Urban%20Environments&entry.906535625=Christopher%20D.%20Hsu%20and%20Pratik%20Chaudhari&entry.1292438233=%20%20We%20study%20pursuit-evasion%20games%20in%20highly%20occluded%20urban%20environments%2C%20e.g.%0Atall%20buildings%20in%20a%20city%2C%20where%20a%20scout%20%28quadrotor%29%20tracks%20multiple%20dynamic%0Atargets%20on%20the%20ground.%20We%20show%20that%20we%20can%20build%20a%20neural%20radiance%20field%20%28NeRF%29%0Arepresentation%20of%20the%20city%20--%20online%20--%20using%20RGB%20and%20depth%20images%20from%0Adifferent%20vantage%20points.%20This%20representation%20is%20used%20to%20calculate%20the%0Ainformation%20gain%20to%20both%20explore%20unknown%20parts%20of%20the%20city%20and%20track%20the%0Atargets%20--%20thereby%20giving%20a%20completely%20first-principles%20approach%20to%20actively%0Atracking%20dynamic%20targets.%20We%20demonstrate%2C%20using%20a%20custom-built%20simulator%20using%0AOpen%20Street%20Maps%20data%20of%20Philadelphia%20and%20New%20York%20City%2C%20that%20we%20can%20explore%0Aand%20locate%2020%20stationary%20targets%20within%20300%20steps.%20This%20is%20slower%20than%20a%20greedy%0Abaseline%20which%20which%20does%20not%20use%20active%20perception.%20But%20for%20dynamic%20targets%0Athat%20actively%20hide%20behind%20occlusions%2C%20we%20show%20that%20our%20approach%20maintains%2C%20at%0Aworst%2C%20a%20tracking%20error%20of%20200m%3B%20the%20greedy%20baseline%20can%20have%20a%20tracking%20error%0Aas%20large%20as%20600m.%20We%20observe%20a%20number%20of%20interesting%20properties%20in%20the%20scout%27s%0Apolicies%2C%20e.g.%2C%20it%20switches%20its%20attention%20to%20track%20a%20different%20target%0Aperiodically%2C%20as%20the%20quality%20of%20the%20NeRF%20representation%20improves%20over%20time%2C%20the%0Ascout%20also%20becomes%20better%20in%20terms%20of%20target%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07431v1&entry.124074799=Read"},
{"title": "Redefining Automotive Radar Imaging: A Domain-Informed 1D Deep Learning\n  Approach for High-Resolution and Efficient Performance", "author": "Ruxin Zheng and Shunqiao Sun and Holger Caesar and Honglei Chen and Jian Li", "abstract": "  Millimeter-wave (mmWave) radars are indispensable for perception tasks of\nautonomous vehicles, thanks to their resilience in challenging weather\nconditions. Yet, their deployment is often limited by insufficient spatial\nresolution for precise semantic scene interpretation. Classical\nsuper-resolution techniques adapted from optical imaging inadequately address\nthe distinct characteristics of radar signal data. In response, our study\nredefines radar imaging super-resolution as a one-dimensional (1D) signal\nsuper-resolution spectra estimation problem by harnessing the radar signal\nprocessing domain knowledge, introducing innovative data normalization and a\ndomain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailored\ndeep learning network for automotive radar imaging exhibits remarkable\nscalability, parameter efficiency and fast inference speed, alongside enhanced\nperformance in terms of radar imaging quality and resolution. Extensive testing\nconfirms that our SR-SPECNet sets a new benchmark in producing high-resolution\nradar range-azimuth images, outperforming existing methods across varied\nantenna configurations and dataset sizes. Source code and new radar dataset\nwill be made publicly available online.\n", "link": "http://arxiv.org/abs/2406.07399v1", "date": "2024-06-11", "relevancy": 2.1978, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5712}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5507}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Redefining%20Automotive%20Radar%20Imaging%3A%20A%20Domain-Informed%201D%20Deep%20Learning%0A%20%20Approach%20for%20High-Resolution%20and%20Efficient%20Performance&body=Title%3A%20Redefining%20Automotive%20Radar%20Imaging%3A%20A%20Domain-Informed%201D%20Deep%20Learning%0A%20%20Approach%20for%20High-Resolution%20and%20Efficient%20Performance%0AAuthor%3A%20Ruxin%20Zheng%20and%20Shunqiao%20Sun%20and%20Holger%20Caesar%20and%20Honglei%20Chen%20and%20Jian%20Li%0AAbstract%3A%20%20%20Millimeter-wave%20%28mmWave%29%20radars%20are%20indispensable%20for%20perception%20tasks%20of%0Aautonomous%20vehicles%2C%20thanks%20to%20their%20resilience%20in%20challenging%20weather%0Aconditions.%20Yet%2C%20their%20deployment%20is%20often%20limited%20by%20insufficient%20spatial%0Aresolution%20for%20precise%20semantic%20scene%20interpretation.%20Classical%0Asuper-resolution%20techniques%20adapted%20from%20optical%20imaging%20inadequately%20address%0Athe%20distinct%20characteristics%20of%20radar%20signal%20data.%20In%20response%2C%20our%20study%0Aredefines%20radar%20imaging%20super-resolution%20as%20a%20one-dimensional%20%281D%29%20signal%0Asuper-resolution%20spectra%20estimation%20problem%20by%20harnessing%20the%20radar%20signal%0Aprocessing%20domain%20knowledge%2C%20introducing%20innovative%20data%20normalization%20and%20a%0Adomain-informed%20signal-to-noise%20ratio%20%28SNR%29-guided%20loss%20function.%20Our%20tailored%0Adeep%20learning%20network%20for%20automotive%20radar%20imaging%20exhibits%20remarkable%0Ascalability%2C%20parameter%20efficiency%20and%20fast%20inference%20speed%2C%20alongside%20enhanced%0Aperformance%20in%20terms%20of%20radar%20imaging%20quality%20and%20resolution.%20Extensive%20testing%0Aconfirms%20that%20our%20SR-SPECNet%20sets%20a%20new%20benchmark%20in%20producing%20high-resolution%0Aradar%20range-azimuth%20images%2C%20outperforming%20existing%20methods%20across%20varied%0Aantenna%20configurations%20and%20dataset%20sizes.%20Source%20code%20and%20new%20radar%20dataset%0Awill%20be%20made%20publicly%20available%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRedefining%2520Automotive%2520Radar%2520Imaging%253A%2520A%2520Domain-Informed%25201D%2520Deep%2520Learning%250A%2520%2520Approach%2520for%2520High-Resolution%2520and%2520Efficient%2520Performance%26entry.906535625%3DRuxin%2520Zheng%2520and%2520Shunqiao%2520Sun%2520and%2520Holger%2520Caesar%2520and%2520Honglei%2520Chen%2520and%2520Jian%2520Li%26entry.1292438233%3D%2520%2520Millimeter-wave%2520%2528mmWave%2529%2520radars%2520are%2520indispensable%2520for%2520perception%2520tasks%2520of%250Aautonomous%2520vehicles%252C%2520thanks%2520to%2520their%2520resilience%2520in%2520challenging%2520weather%250Aconditions.%2520Yet%252C%2520their%2520deployment%2520is%2520often%2520limited%2520by%2520insufficient%2520spatial%250Aresolution%2520for%2520precise%2520semantic%2520scene%2520interpretation.%2520Classical%250Asuper-resolution%2520techniques%2520adapted%2520from%2520optical%2520imaging%2520inadequately%2520address%250Athe%2520distinct%2520characteristics%2520of%2520radar%2520signal%2520data.%2520In%2520response%252C%2520our%2520study%250Aredefines%2520radar%2520imaging%2520super-resolution%2520as%2520a%2520one-dimensional%2520%25281D%2529%2520signal%250Asuper-resolution%2520spectra%2520estimation%2520problem%2520by%2520harnessing%2520the%2520radar%2520signal%250Aprocessing%2520domain%2520knowledge%252C%2520introducing%2520innovative%2520data%2520normalization%2520and%2520a%250Adomain-informed%2520signal-to-noise%2520ratio%2520%2528SNR%2529-guided%2520loss%2520function.%2520Our%2520tailored%250Adeep%2520learning%2520network%2520for%2520automotive%2520radar%2520imaging%2520exhibits%2520remarkable%250Ascalability%252C%2520parameter%2520efficiency%2520and%2520fast%2520inference%2520speed%252C%2520alongside%2520enhanced%250Aperformance%2520in%2520terms%2520of%2520radar%2520imaging%2520quality%2520and%2520resolution.%2520Extensive%2520testing%250Aconfirms%2520that%2520our%2520SR-SPECNet%2520sets%2520a%2520new%2520benchmark%2520in%2520producing%2520high-resolution%250Aradar%2520range-azimuth%2520images%252C%2520outperforming%2520existing%2520methods%2520across%2520varied%250Aantenna%2520configurations%2520and%2520dataset%2520sizes.%2520Source%2520code%2520and%2520new%2520radar%2520dataset%250Awill%2520be%2520made%2520publicly%2520available%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Redefining%20Automotive%20Radar%20Imaging%3A%20A%20Domain-Informed%201D%20Deep%20Learning%0A%20%20Approach%20for%20High-Resolution%20and%20Efficient%20Performance&entry.906535625=Ruxin%20Zheng%20and%20Shunqiao%20Sun%20and%20Holger%20Caesar%20and%20Honglei%20Chen%20and%20Jian%20Li&entry.1292438233=%20%20Millimeter-wave%20%28mmWave%29%20radars%20are%20indispensable%20for%20perception%20tasks%20of%0Aautonomous%20vehicles%2C%20thanks%20to%20their%20resilience%20in%20challenging%20weather%0Aconditions.%20Yet%2C%20their%20deployment%20is%20often%20limited%20by%20insufficient%20spatial%0Aresolution%20for%20precise%20semantic%20scene%20interpretation.%20Classical%0Asuper-resolution%20techniques%20adapted%20from%20optical%20imaging%20inadequately%20address%0Athe%20distinct%20characteristics%20of%20radar%20signal%20data.%20In%20response%2C%20our%20study%0Aredefines%20radar%20imaging%20super-resolution%20as%20a%20one-dimensional%20%281D%29%20signal%0Asuper-resolution%20spectra%20estimation%20problem%20by%20harnessing%20the%20radar%20signal%0Aprocessing%20domain%20knowledge%2C%20introducing%20innovative%20data%20normalization%20and%20a%0Adomain-informed%20signal-to-noise%20ratio%20%28SNR%29-guided%20loss%20function.%20Our%20tailored%0Adeep%20learning%20network%20for%20automotive%20radar%20imaging%20exhibits%20remarkable%0Ascalability%2C%20parameter%20efficiency%20and%20fast%20inference%20speed%2C%20alongside%20enhanced%0Aperformance%20in%20terms%20of%20radar%20imaging%20quality%20and%20resolution.%20Extensive%20testing%0Aconfirms%20that%20our%20SR-SPECNet%20sets%20a%20new%20benchmark%20in%20producing%20high-resolution%0Aradar%20range-azimuth%20images%2C%20outperforming%20existing%20methods%20across%20varied%0Aantenna%20configurations%20and%20dataset%20sizes.%20Source%20code%20and%20new%20radar%20dataset%0Awill%20be%20made%20publicly%20available%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07399v1&entry.124074799=Read"},
{"title": "Enhanced Gene Selection in Single-Cell Genomics: Pre-Filtering Synergy\n  and Reinforced Optimization", "author": "Weiliang Zhang and Zhen Meng and Dongjie Wang and Min Wu and Kunpeng Liu and Yuanchun Zhou and Meng Xiao", "abstract": "  Recent advancements in single-cell genomics necessitate precision in gene\npanel selection to interpret complex biological data effectively. Those methods\naim to streamline the analysis of scRNA-seq data by focusing on the most\ninformative genes that contribute significantly to the specific analysis task.\nTraditional selection methods, which often rely on expert domain knowledge,\nembedded machine learning models, or heuristic-based iterative optimization,\nare prone to biases and inefficiencies that may obscure critical genomic\nsignals. Recognizing the limitations of traditional methods, we aim to\ntranscend these constraints with a refined strategy. In this study, we\nintroduce an iterative gene panel selection strategy that is applicable to\nclustering tasks in single-cell genomics. Our method uniquely integrates\nresults from other gene selection algorithms, providing valuable preliminary\nboundaries or prior knowledge as initial guides in the search space to enhance\nthe efficiency of our framework. Furthermore, we incorporate the stochastic\nnature of the exploration process in reinforcement learning (RL) and its\ncapability for continuous optimization through reward-based feedback. This\ncombination mitigates the biases inherent in the initial boundaries and\nharnesses RL's adaptability to refine and target gene panel selection\ndynamically. To illustrate the effectiveness of our method, we conducted\ndetailed comparative experiments, case studies, and visualization analysis.\n", "link": "http://arxiv.org/abs/2406.07418v1", "date": "2024-06-11", "relevancy": 2.1937, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4407}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4401}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Gene%20Selection%20in%20Single-Cell%20Genomics%3A%20Pre-Filtering%20Synergy%0A%20%20and%20Reinforced%20Optimization&body=Title%3A%20Enhanced%20Gene%20Selection%20in%20Single-Cell%20Genomics%3A%20Pre-Filtering%20Synergy%0A%20%20and%20Reinforced%20Optimization%0AAuthor%3A%20Weiliang%20Zhang%20and%20Zhen%20Meng%20and%20Dongjie%20Wang%20and%20Min%20Wu%20and%20Kunpeng%20Liu%20and%20Yuanchun%20Zhou%20and%20Meng%20Xiao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20single-cell%20genomics%20necessitate%20precision%20in%20gene%0Apanel%20selection%20to%20interpret%20complex%20biological%20data%20effectively.%20Those%20methods%0Aaim%20to%20streamline%20the%20analysis%20of%20scRNA-seq%20data%20by%20focusing%20on%20the%20most%0Ainformative%20genes%20that%20contribute%20significantly%20to%20the%20specific%20analysis%20task.%0ATraditional%20selection%20methods%2C%20which%20often%20rely%20on%20expert%20domain%20knowledge%2C%0Aembedded%20machine%20learning%20models%2C%20or%20heuristic-based%20iterative%20optimization%2C%0Aare%20prone%20to%20biases%20and%20inefficiencies%20that%20may%20obscure%20critical%20genomic%0Asignals.%20Recognizing%20the%20limitations%20of%20traditional%20methods%2C%20we%20aim%20to%0Atranscend%20these%20constraints%20with%20a%20refined%20strategy.%20In%20this%20study%2C%20we%0Aintroduce%20an%20iterative%20gene%20panel%20selection%20strategy%20that%20is%20applicable%20to%0Aclustering%20tasks%20in%20single-cell%20genomics.%20Our%20method%20uniquely%20integrates%0Aresults%20from%20other%20gene%20selection%20algorithms%2C%20providing%20valuable%20preliminary%0Aboundaries%20or%20prior%20knowledge%20as%20initial%20guides%20in%20the%20search%20space%20to%20enhance%0Athe%20efficiency%20of%20our%20framework.%20Furthermore%2C%20we%20incorporate%20the%20stochastic%0Anature%20of%20the%20exploration%20process%20in%20reinforcement%20learning%20%28RL%29%20and%20its%0Acapability%20for%20continuous%20optimization%20through%20reward-based%20feedback.%20This%0Acombination%20mitigates%20the%20biases%20inherent%20in%20the%20initial%20boundaries%20and%0Aharnesses%20RL%27s%20adaptability%20to%20refine%20and%20target%20gene%20panel%20selection%0Adynamically.%20To%20illustrate%20the%20effectiveness%20of%20our%20method%2C%20we%20conducted%0Adetailed%20comparative%20experiments%2C%20case%20studies%2C%20and%20visualization%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Gene%2520Selection%2520in%2520Single-Cell%2520Genomics%253A%2520Pre-Filtering%2520Synergy%250A%2520%2520and%2520Reinforced%2520Optimization%26entry.906535625%3DWeiliang%2520Zhang%2520and%2520Zhen%2520Meng%2520and%2520Dongjie%2520Wang%2520and%2520Min%2520Wu%2520and%2520Kunpeng%2520Liu%2520and%2520Yuanchun%2520Zhou%2520and%2520Meng%2520Xiao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520single-cell%2520genomics%2520necessitate%2520precision%2520in%2520gene%250Apanel%2520selection%2520to%2520interpret%2520complex%2520biological%2520data%2520effectively.%2520Those%2520methods%250Aaim%2520to%2520streamline%2520the%2520analysis%2520of%2520scRNA-seq%2520data%2520by%2520focusing%2520on%2520the%2520most%250Ainformative%2520genes%2520that%2520contribute%2520significantly%2520to%2520the%2520specific%2520analysis%2520task.%250ATraditional%2520selection%2520methods%252C%2520which%2520often%2520rely%2520on%2520expert%2520domain%2520knowledge%252C%250Aembedded%2520machine%2520learning%2520models%252C%2520or%2520heuristic-based%2520iterative%2520optimization%252C%250Aare%2520prone%2520to%2520biases%2520and%2520inefficiencies%2520that%2520may%2520obscure%2520critical%2520genomic%250Asignals.%2520Recognizing%2520the%2520limitations%2520of%2520traditional%2520methods%252C%2520we%2520aim%2520to%250Atranscend%2520these%2520constraints%2520with%2520a%2520refined%2520strategy.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520an%2520iterative%2520gene%2520panel%2520selection%2520strategy%2520that%2520is%2520applicable%2520to%250Aclustering%2520tasks%2520in%2520single-cell%2520genomics.%2520Our%2520method%2520uniquely%2520integrates%250Aresults%2520from%2520other%2520gene%2520selection%2520algorithms%252C%2520providing%2520valuable%2520preliminary%250Aboundaries%2520or%2520prior%2520knowledge%2520as%2520initial%2520guides%2520in%2520the%2520search%2520space%2520to%2520enhance%250Athe%2520efficiency%2520of%2520our%2520framework.%2520Furthermore%252C%2520we%2520incorporate%2520the%2520stochastic%250Anature%2520of%2520the%2520exploration%2520process%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520and%2520its%250Acapability%2520for%2520continuous%2520optimization%2520through%2520reward-based%2520feedback.%2520This%250Acombination%2520mitigates%2520the%2520biases%2520inherent%2520in%2520the%2520initial%2520boundaries%2520and%250Aharnesses%2520RL%2527s%2520adaptability%2520to%2520refine%2520and%2520target%2520gene%2520panel%2520selection%250Adynamically.%2520To%2520illustrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520we%2520conducted%250Adetailed%2520comparative%2520experiments%252C%2520case%2520studies%252C%2520and%2520visualization%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Gene%20Selection%20in%20Single-Cell%20Genomics%3A%20Pre-Filtering%20Synergy%0A%20%20and%20Reinforced%20Optimization&entry.906535625=Weiliang%20Zhang%20and%20Zhen%20Meng%20and%20Dongjie%20Wang%20and%20Min%20Wu%20and%20Kunpeng%20Liu%20and%20Yuanchun%20Zhou%20and%20Meng%20Xiao&entry.1292438233=%20%20Recent%20advancements%20in%20single-cell%20genomics%20necessitate%20precision%20in%20gene%0Apanel%20selection%20to%20interpret%20complex%20biological%20data%20effectively.%20Those%20methods%0Aaim%20to%20streamline%20the%20analysis%20of%20scRNA-seq%20data%20by%20focusing%20on%20the%20most%0Ainformative%20genes%20that%20contribute%20significantly%20to%20the%20specific%20analysis%20task.%0ATraditional%20selection%20methods%2C%20which%20often%20rely%20on%20expert%20domain%20knowledge%2C%0Aembedded%20machine%20learning%20models%2C%20or%20heuristic-based%20iterative%20optimization%2C%0Aare%20prone%20to%20biases%20and%20inefficiencies%20that%20may%20obscure%20critical%20genomic%0Asignals.%20Recognizing%20the%20limitations%20of%20traditional%20methods%2C%20we%20aim%20to%0Atranscend%20these%20constraints%20with%20a%20refined%20strategy.%20In%20this%20study%2C%20we%0Aintroduce%20an%20iterative%20gene%20panel%20selection%20strategy%20that%20is%20applicable%20to%0Aclustering%20tasks%20in%20single-cell%20genomics.%20Our%20method%20uniquely%20integrates%0Aresults%20from%20other%20gene%20selection%20algorithms%2C%20providing%20valuable%20preliminary%0Aboundaries%20or%20prior%20knowledge%20as%20initial%20guides%20in%20the%20search%20space%20to%20enhance%0Athe%20efficiency%20of%20our%20framework.%20Furthermore%2C%20we%20incorporate%20the%20stochastic%0Anature%20of%20the%20exploration%20process%20in%20reinforcement%20learning%20%28RL%29%20and%20its%0Acapability%20for%20continuous%20optimization%20through%20reward-based%20feedback.%20This%0Acombination%20mitigates%20the%20biases%20inherent%20in%20the%20initial%20boundaries%20and%0Aharnesses%20RL%27s%20adaptability%20to%20refine%20and%20target%20gene%20panel%20selection%0Adynamically.%20To%20illustrate%20the%20effectiveness%20of%20our%20method%2C%20we%20conducted%0Adetailed%20comparative%20experiments%2C%20case%20studies%2C%20and%20visualization%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07418v1&entry.124074799=Read"},
{"title": "Image and Video Tokenization with Binary Spherical Quantization", "author": "Yue Zhao and Yuanjun Xiong and Philipp Kr\u00e4henb\u00fchl", "abstract": "  We propose a new transformer-based image and video tokenizer with Binary\nSpherical Quantization (BSQ). BSQ projects the high-dimensional visual\nembedding to a lower-dimensional hypersphere and then applies binary\nquantization. BSQ is (1) parameter-efficient without an explicit codebook, (2)\nscalable to arbitrary token dimensions, and (3) compact: compressing visual\ndata by up to 100$\\times$ with minimal distortion. Our tokenizer uses a\ntransformer encoder and decoder with simple block-wise causal masking to\nsupport variable-length videos as input. The resulting BSQ-ViT achieves\nstate-of-the-art visual reconstruction quality on image and video\nreconstruction benchmarks with 2.4$\\times$ throughput compared to the best\nprior methods. Furthermore, by learning an autoregressive prior for adaptive\narithmetic coding, BSQ-ViT achieves comparable results on video compression\nwith state-of-the-art video compression standards. BSQ-ViT also enables masked\nlanguage models to achieve competitive image synthesis quality to GAN- and\ndiffusion-based methods.\n", "link": "http://arxiv.org/abs/2406.07548v1", "date": "2024-06-11", "relevancy": 2.1768, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5461}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5432}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20and%20Video%20Tokenization%20with%20Binary%20Spherical%20Quantization&body=Title%3A%20Image%20and%20Video%20Tokenization%20with%20Binary%20Spherical%20Quantization%0AAuthor%3A%20Yue%20Zhao%20and%20Yuanjun%20Xiong%20and%20Philipp%20Kr%C3%A4henb%C3%BChl%0AAbstract%3A%20%20%20We%20propose%20a%20new%20transformer-based%20image%20and%20video%20tokenizer%20with%20Binary%0ASpherical%20Quantization%20%28BSQ%29.%20BSQ%20projects%20the%20high-dimensional%20visual%0Aembedding%20to%20a%20lower-dimensional%20hypersphere%20and%20then%20applies%20binary%0Aquantization.%20BSQ%20is%20%281%29%20parameter-efficient%20without%20an%20explicit%20codebook%2C%20%282%29%0Ascalable%20to%20arbitrary%20token%20dimensions%2C%20and%20%283%29%20compact%3A%20compressing%20visual%0Adata%20by%20up%20to%20100%24%5Ctimes%24%20with%20minimal%20distortion.%20Our%20tokenizer%20uses%20a%0Atransformer%20encoder%20and%20decoder%20with%20simple%20block-wise%20causal%20masking%20to%0Asupport%20variable-length%20videos%20as%20input.%20The%20resulting%20BSQ-ViT%20achieves%0Astate-of-the-art%20visual%20reconstruction%20quality%20on%20image%20and%20video%0Areconstruction%20benchmarks%20with%202.4%24%5Ctimes%24%20throughput%20compared%20to%20the%20best%0Aprior%20methods.%20Furthermore%2C%20by%20learning%20an%20autoregressive%20prior%20for%20adaptive%0Aarithmetic%20coding%2C%20BSQ-ViT%20achieves%20comparable%20results%20on%20video%20compression%0Awith%20state-of-the-art%20video%20compression%20standards.%20BSQ-ViT%20also%20enables%20masked%0Alanguage%20models%20to%20achieve%20competitive%20image%20synthesis%20quality%20to%20GAN-%20and%0Adiffusion-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520and%2520Video%2520Tokenization%2520with%2520Binary%2520Spherical%2520Quantization%26entry.906535625%3DYue%2520Zhao%2520and%2520Yuanjun%2520Xiong%2520and%2520Philipp%2520Kr%25C3%25A4henb%25C3%25BChl%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520transformer-based%2520image%2520and%2520video%2520tokenizer%2520with%2520Binary%250ASpherical%2520Quantization%2520%2528BSQ%2529.%2520BSQ%2520projects%2520the%2520high-dimensional%2520visual%250Aembedding%2520to%2520a%2520lower-dimensional%2520hypersphere%2520and%2520then%2520applies%2520binary%250Aquantization.%2520BSQ%2520is%2520%25281%2529%2520parameter-efficient%2520without%2520an%2520explicit%2520codebook%252C%2520%25282%2529%250Ascalable%2520to%2520arbitrary%2520token%2520dimensions%252C%2520and%2520%25283%2529%2520compact%253A%2520compressing%2520visual%250Adata%2520by%2520up%2520to%2520100%2524%255Ctimes%2524%2520with%2520minimal%2520distortion.%2520Our%2520tokenizer%2520uses%2520a%250Atransformer%2520encoder%2520and%2520decoder%2520with%2520simple%2520block-wise%2520causal%2520masking%2520to%250Asupport%2520variable-length%2520videos%2520as%2520input.%2520The%2520resulting%2520BSQ-ViT%2520achieves%250Astate-of-the-art%2520visual%2520reconstruction%2520quality%2520on%2520image%2520and%2520video%250Areconstruction%2520benchmarks%2520with%25202.4%2524%255Ctimes%2524%2520throughput%2520compared%2520to%2520the%2520best%250Aprior%2520methods.%2520Furthermore%252C%2520by%2520learning%2520an%2520autoregressive%2520prior%2520for%2520adaptive%250Aarithmetic%2520coding%252C%2520BSQ-ViT%2520achieves%2520comparable%2520results%2520on%2520video%2520compression%250Awith%2520state-of-the-art%2520video%2520compression%2520standards.%2520BSQ-ViT%2520also%2520enables%2520masked%250Alanguage%2520models%2520to%2520achieve%2520competitive%2520image%2520synthesis%2520quality%2520to%2520GAN-%2520and%250Adiffusion-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20and%20Video%20Tokenization%20with%20Binary%20Spherical%20Quantization&entry.906535625=Yue%20Zhao%20and%20Yuanjun%20Xiong%20and%20Philipp%20Kr%C3%A4henb%C3%BChl&entry.1292438233=%20%20We%20propose%20a%20new%20transformer-based%20image%20and%20video%20tokenizer%20with%20Binary%0ASpherical%20Quantization%20%28BSQ%29.%20BSQ%20projects%20the%20high-dimensional%20visual%0Aembedding%20to%20a%20lower-dimensional%20hypersphere%20and%20then%20applies%20binary%0Aquantization.%20BSQ%20is%20%281%29%20parameter-efficient%20without%20an%20explicit%20codebook%2C%20%282%29%0Ascalable%20to%20arbitrary%20token%20dimensions%2C%20and%20%283%29%20compact%3A%20compressing%20visual%0Adata%20by%20up%20to%20100%24%5Ctimes%24%20with%20minimal%20distortion.%20Our%20tokenizer%20uses%20a%0Atransformer%20encoder%20and%20decoder%20with%20simple%20block-wise%20causal%20masking%20to%0Asupport%20variable-length%20videos%20as%20input.%20The%20resulting%20BSQ-ViT%20achieves%0Astate-of-the-art%20visual%20reconstruction%20quality%20on%20image%20and%20video%0Areconstruction%20benchmarks%20with%202.4%24%5Ctimes%24%20throughput%20compared%20to%20the%20best%0Aprior%20methods.%20Furthermore%2C%20by%20learning%20an%20autoregressive%20prior%20for%20adaptive%0Aarithmetic%20coding%2C%20BSQ-ViT%20achieves%20comparable%20results%20on%20video%20compression%0Awith%20state-of-the-art%20video%20compression%20standards.%20BSQ-ViT%20also%20enables%20masked%0Alanguage%20models%20to%20achieve%20competitive%20image%20synthesis%20quality%20to%20GAN-%20and%0Adiffusion-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07548v1&entry.124074799=Read"},
{"title": "Faster Spectral Density Estimation and Sparsification in the Nuclear\n  Norm", "author": "Yujia Jin and Ishani Karmarkar and Christopher Musco and Aaron Sidford and Apoorv Vikram Singh", "abstract": "  We consider the problem of estimating the spectral density of the normalized\nadjacency matrix of an $n$-node undirected graph. We provide a randomized\nalgorithm that, with $O(n\\epsilon^{-2})$ queries to a degree and neighbor\noracle and in $O(n\\epsilon^{-3})$ time, estimates the spectrum up to $\\epsilon$\naccuracy in the Wasserstein-1 metric. This improves on previous\nstate-of-the-art methods, including an $O(n\\epsilon^{-7})$ time algorithm from\n[Braverman et al., STOC 2022] and, for sufficiently small $\\epsilon$, a\n$2^{O(\\epsilon^{-1})}$ time method from [Cohen-Steiner et al., KDD 2018]. To\nachieve this result, we introduce a new notion of graph sparsification, which\nwe call nuclear sparsification. We provide an $O(n\\epsilon^{-2})$-query and\n$O(n\\epsilon^{-2})$-time algorithm for computing $O(n\\epsilon^{-2})$-sparse\nnuclear sparsifiers. We show that this bound is optimal in both its sparsity\nand query complexity, and we separate our results from the related notion of\nadditive spectral sparsification. Of independent interest, we show that our\nsparsification method also yields the first deterministic algorithm for\nspectral density estimation that scales linearly with $n$ (sublinear in the\nrepresentation size of the graph).\n", "link": "http://arxiv.org/abs/2406.07521v1", "date": "2024-06-11", "relevancy": 2.1766, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4373}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4368}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Spectral%20Density%20Estimation%20and%20Sparsification%20in%20the%20Nuclear%0A%20%20Norm&body=Title%3A%20Faster%20Spectral%20Density%20Estimation%20and%20Sparsification%20in%20the%20Nuclear%0A%20%20Norm%0AAuthor%3A%20Yujia%20Jin%20and%20Ishani%20Karmarkar%20and%20Christopher%20Musco%20and%20Aaron%20Sidford%20and%20Apoorv%20Vikram%20Singh%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20estimating%20the%20spectral%20density%20of%20the%20normalized%0Aadjacency%20matrix%20of%20an%20%24n%24-node%20undirected%20graph.%20We%20provide%20a%20randomized%0Aalgorithm%20that%2C%20with%20%24O%28n%5Cepsilon%5E%7B-2%7D%29%24%20queries%20to%20a%20degree%20and%20neighbor%0Aoracle%20and%20in%20%24O%28n%5Cepsilon%5E%7B-3%7D%29%24%20time%2C%20estimates%20the%20spectrum%20up%20to%20%24%5Cepsilon%24%0Aaccuracy%20in%20the%20Wasserstein-1%20metric.%20This%20improves%20on%20previous%0Astate-of-the-art%20methods%2C%20including%20an%20%24O%28n%5Cepsilon%5E%7B-7%7D%29%24%20time%20algorithm%20from%0A%5BBraverman%20et%20al.%2C%20STOC%202022%5D%20and%2C%20for%20sufficiently%20small%20%24%5Cepsilon%24%2C%20a%0A%242%5E%7BO%28%5Cepsilon%5E%7B-1%7D%29%7D%24%20time%20method%20from%20%5BCohen-Steiner%20et%20al.%2C%20KDD%202018%5D.%20To%0Aachieve%20this%20result%2C%20we%20introduce%20a%20new%20notion%20of%20graph%20sparsification%2C%20which%0Awe%20call%20nuclear%20sparsification.%20We%20provide%20an%20%24O%28n%5Cepsilon%5E%7B-2%7D%29%24-query%20and%0A%24O%28n%5Cepsilon%5E%7B-2%7D%29%24-time%20algorithm%20for%20computing%20%24O%28n%5Cepsilon%5E%7B-2%7D%29%24-sparse%0Anuclear%20sparsifiers.%20We%20show%20that%20this%20bound%20is%20optimal%20in%20both%20its%20sparsity%0Aand%20query%20complexity%2C%20and%20we%20separate%20our%20results%20from%20the%20related%20notion%20of%0Aadditive%20spectral%20sparsification.%20Of%20independent%20interest%2C%20we%20show%20that%20our%0Asparsification%20method%20also%20yields%20the%20first%20deterministic%20algorithm%20for%0Aspectral%20density%20estimation%20that%20scales%20linearly%20with%20%24n%24%20%28sublinear%20in%20the%0Arepresentation%20size%20of%20the%20graph%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Spectral%2520Density%2520Estimation%2520and%2520Sparsification%2520in%2520the%2520Nuclear%250A%2520%2520Norm%26entry.906535625%3DYujia%2520Jin%2520and%2520Ishani%2520Karmarkar%2520and%2520Christopher%2520Musco%2520and%2520Aaron%2520Sidford%2520and%2520Apoorv%2520Vikram%2520Singh%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520estimating%2520the%2520spectral%2520density%2520of%2520the%2520normalized%250Aadjacency%2520matrix%2520of%2520an%2520%2524n%2524-node%2520undirected%2520graph.%2520We%2520provide%2520a%2520randomized%250Aalgorithm%2520that%252C%2520with%2520%2524O%2528n%255Cepsilon%255E%257B-2%257D%2529%2524%2520queries%2520to%2520a%2520degree%2520and%2520neighbor%250Aoracle%2520and%2520in%2520%2524O%2528n%255Cepsilon%255E%257B-3%257D%2529%2524%2520time%252C%2520estimates%2520the%2520spectrum%2520up%2520to%2520%2524%255Cepsilon%2524%250Aaccuracy%2520in%2520the%2520Wasserstein-1%2520metric.%2520This%2520improves%2520on%2520previous%250Astate-of-the-art%2520methods%252C%2520including%2520an%2520%2524O%2528n%255Cepsilon%255E%257B-7%257D%2529%2524%2520time%2520algorithm%2520from%250A%255BBraverman%2520et%2520al.%252C%2520STOC%25202022%255D%2520and%252C%2520for%2520sufficiently%2520small%2520%2524%255Cepsilon%2524%252C%2520a%250A%25242%255E%257BO%2528%255Cepsilon%255E%257B-1%257D%2529%257D%2524%2520time%2520method%2520from%2520%255BCohen-Steiner%2520et%2520al.%252C%2520KDD%25202018%255D.%2520To%250Aachieve%2520this%2520result%252C%2520we%2520introduce%2520a%2520new%2520notion%2520of%2520graph%2520sparsification%252C%2520which%250Awe%2520call%2520nuclear%2520sparsification.%2520We%2520provide%2520an%2520%2524O%2528n%255Cepsilon%255E%257B-2%257D%2529%2524-query%2520and%250A%2524O%2528n%255Cepsilon%255E%257B-2%257D%2529%2524-time%2520algorithm%2520for%2520computing%2520%2524O%2528n%255Cepsilon%255E%257B-2%257D%2529%2524-sparse%250Anuclear%2520sparsifiers.%2520We%2520show%2520that%2520this%2520bound%2520is%2520optimal%2520in%2520both%2520its%2520sparsity%250Aand%2520query%2520complexity%252C%2520and%2520we%2520separate%2520our%2520results%2520from%2520the%2520related%2520notion%2520of%250Aadditive%2520spectral%2520sparsification.%2520Of%2520independent%2520interest%252C%2520we%2520show%2520that%2520our%250Asparsification%2520method%2520also%2520yields%2520the%2520first%2520deterministic%2520algorithm%2520for%250Aspectral%2520density%2520estimation%2520that%2520scales%2520linearly%2520with%2520%2524n%2524%2520%2528sublinear%2520in%2520the%250Arepresentation%2520size%2520of%2520the%2520graph%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Spectral%20Density%20Estimation%20and%20Sparsification%20in%20the%20Nuclear%0A%20%20Norm&entry.906535625=Yujia%20Jin%20and%20Ishani%20Karmarkar%20and%20Christopher%20Musco%20and%20Aaron%20Sidford%20and%20Apoorv%20Vikram%20Singh&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20estimating%20the%20spectral%20density%20of%20the%20normalized%0Aadjacency%20matrix%20of%20an%20%24n%24-node%20undirected%20graph.%20We%20provide%20a%20randomized%0Aalgorithm%20that%2C%20with%20%24O%28n%5Cepsilon%5E%7B-2%7D%29%24%20queries%20to%20a%20degree%20and%20neighbor%0Aoracle%20and%20in%20%24O%28n%5Cepsilon%5E%7B-3%7D%29%24%20time%2C%20estimates%20the%20spectrum%20up%20to%20%24%5Cepsilon%24%0Aaccuracy%20in%20the%20Wasserstein-1%20metric.%20This%20improves%20on%20previous%0Astate-of-the-art%20methods%2C%20including%20an%20%24O%28n%5Cepsilon%5E%7B-7%7D%29%24%20time%20algorithm%20from%0A%5BBraverman%20et%20al.%2C%20STOC%202022%5D%20and%2C%20for%20sufficiently%20small%20%24%5Cepsilon%24%2C%20a%0A%242%5E%7BO%28%5Cepsilon%5E%7B-1%7D%29%7D%24%20time%20method%20from%20%5BCohen-Steiner%20et%20al.%2C%20KDD%202018%5D.%20To%0Aachieve%20this%20result%2C%20we%20introduce%20a%20new%20notion%20of%20graph%20sparsification%2C%20which%0Awe%20call%20nuclear%20sparsification.%20We%20provide%20an%20%24O%28n%5Cepsilon%5E%7B-2%7D%29%24-query%20and%0A%24O%28n%5Cepsilon%5E%7B-2%7D%29%24-time%20algorithm%20for%20computing%20%24O%28n%5Cepsilon%5E%7B-2%7D%29%24-sparse%0Anuclear%20sparsifiers.%20We%20show%20that%20this%20bound%20is%20optimal%20in%20both%20its%20sparsity%0Aand%20query%20complexity%2C%20and%20we%20separate%20our%20results%20from%20the%20related%20notion%20of%0Aadditive%20spectral%20sparsification.%20Of%20independent%20interest%2C%20we%20show%20that%20our%0Asparsification%20method%20also%20yields%20the%20first%20deterministic%20algorithm%20for%0Aspectral%20density%20estimation%20that%20scales%20linearly%20with%20%24n%24%20%28sublinear%20in%20the%0Arepresentation%20size%20of%20the%20graph%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07521v1&entry.124074799=Read"},
{"title": "LM4LV: A Frozen Large Language Model for Low-level Vision Tasks", "author": "Boyang Zheng and Jinjin Gu and Shijun Li and Chao Dong", "abstract": "  The success of large language models (LLMs) has fostered a new research trend\nof multi-modality large language models (MLLMs), which changes the paradigm of\nvarious fields in computer vision. Though MLLMs have shown promising results in\nnumerous high-level vision and vision-language tasks such as VQA and\ntext-to-image, no works have demonstrated how low-level vision tasks can\nbenefit from MLLMs. We find that most current MLLMs are blind to low-level\nfeatures due to their design of vision modules, thus are inherently incapable\nfor solving low-level vision tasks. In this work, we purpose $\\textbf{LM4LV}$,\na framework that enables a FROZEN LLM to solve a range of low-level vision\ntasks without any multi-modal data or prior. This showcases the LLM's strong\npotential in low-level vision and bridges the gap between MLLMs and low-level\nvision tasks. We hope this work can inspire new perspectives on LLMs and deeper\nunderstanding of their mechanisms. Code is available at\nhttps://github.com/bytetriper/LM4LV.\n", "link": "http://arxiv.org/abs/2405.15734v2", "date": "2024-06-11", "relevancy": 2.1727, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5657}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.534}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LM4LV%3A%20A%20Frozen%20Large%20Language%20Model%20for%20Low-level%20Vision%20Tasks&body=Title%3A%20LM4LV%3A%20A%20Frozen%20Large%20Language%20Model%20for%20Low-level%20Vision%20Tasks%0AAuthor%3A%20Boyang%20Zheng%20and%20Jinjin%20Gu%20and%20Shijun%20Li%20and%20Chao%20Dong%0AAbstract%3A%20%20%20The%20success%20of%20large%20language%20models%20%28LLMs%29%20has%20fostered%20a%20new%20research%20trend%0Aof%20multi-modality%20large%20language%20models%20%28MLLMs%29%2C%20which%20changes%20the%20paradigm%20of%0Avarious%20fields%20in%20computer%20vision.%20Though%20MLLMs%20have%20shown%20promising%20results%20in%0Anumerous%20high-level%20vision%20and%20vision-language%20tasks%20such%20as%20VQA%20and%0Atext-to-image%2C%20no%20works%20have%20demonstrated%20how%20low-level%20vision%20tasks%20can%0Abenefit%20from%20MLLMs.%20We%20find%20that%20most%20current%20MLLMs%20are%20blind%20to%20low-level%0Afeatures%20due%20to%20their%20design%20of%20vision%20modules%2C%20thus%20are%20inherently%20incapable%0Afor%20solving%20low-level%20vision%20tasks.%20In%20this%20work%2C%20we%20purpose%20%24%5Ctextbf%7BLM4LV%7D%24%2C%0Aa%20framework%20that%20enables%20a%20FROZEN%20LLM%20to%20solve%20a%20range%20of%20low-level%20vision%0Atasks%20without%20any%20multi-modal%20data%20or%20prior.%20This%20showcases%20the%20LLM%27s%20strong%0Apotential%20in%20low-level%20vision%20and%20bridges%20the%20gap%20between%20MLLMs%20and%20low-level%0Avision%20tasks.%20We%20hope%20this%20work%20can%20inspire%20new%20perspectives%20on%20LLMs%20and%20deeper%0Aunderstanding%20of%20their%20mechanisms.%20Code%20is%20available%20at%0Ahttps%3A//github.com/bytetriper/LM4LV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLM4LV%253A%2520A%2520Frozen%2520Large%2520Language%2520Model%2520for%2520Low-level%2520Vision%2520Tasks%26entry.906535625%3DBoyang%2520Zheng%2520and%2520Jinjin%2520Gu%2520and%2520Shijun%2520Li%2520and%2520Chao%2520Dong%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520fostered%2520a%2520new%2520research%2520trend%250Aof%2520multi-modality%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520which%2520changes%2520the%2520paradigm%2520of%250Avarious%2520fields%2520in%2520computer%2520vision.%2520Though%2520MLLMs%2520have%2520shown%2520promising%2520results%2520in%250Anumerous%2520high-level%2520vision%2520and%2520vision-language%2520tasks%2520such%2520as%2520VQA%2520and%250Atext-to-image%252C%2520no%2520works%2520have%2520demonstrated%2520how%2520low-level%2520vision%2520tasks%2520can%250Abenefit%2520from%2520MLLMs.%2520We%2520find%2520that%2520most%2520current%2520MLLMs%2520are%2520blind%2520to%2520low-level%250Afeatures%2520due%2520to%2520their%2520design%2520of%2520vision%2520modules%252C%2520thus%2520are%2520inherently%2520incapable%250Afor%2520solving%2520low-level%2520vision%2520tasks.%2520In%2520this%2520work%252C%2520we%2520purpose%2520%2524%255Ctextbf%257BLM4LV%257D%2524%252C%250Aa%2520framework%2520that%2520enables%2520a%2520FROZEN%2520LLM%2520to%2520solve%2520a%2520range%2520of%2520low-level%2520vision%250Atasks%2520without%2520any%2520multi-modal%2520data%2520or%2520prior.%2520This%2520showcases%2520the%2520LLM%2527s%2520strong%250Apotential%2520in%2520low-level%2520vision%2520and%2520bridges%2520the%2520gap%2520between%2520MLLMs%2520and%2520low-level%250Avision%2520tasks.%2520We%2520hope%2520this%2520work%2520can%2520inspire%2520new%2520perspectives%2520on%2520LLMs%2520and%2520deeper%250Aunderstanding%2520of%2520their%2520mechanisms.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/bytetriper/LM4LV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LM4LV%3A%20A%20Frozen%20Large%20Language%20Model%20for%20Low-level%20Vision%20Tasks&entry.906535625=Boyang%20Zheng%20and%20Jinjin%20Gu%20and%20Shijun%20Li%20and%20Chao%20Dong&entry.1292438233=%20%20The%20success%20of%20large%20language%20models%20%28LLMs%29%20has%20fostered%20a%20new%20research%20trend%0Aof%20multi-modality%20large%20language%20models%20%28MLLMs%29%2C%20which%20changes%20the%20paradigm%20of%0Avarious%20fields%20in%20computer%20vision.%20Though%20MLLMs%20have%20shown%20promising%20results%20in%0Anumerous%20high-level%20vision%20and%20vision-language%20tasks%20such%20as%20VQA%20and%0Atext-to-image%2C%20no%20works%20have%20demonstrated%20how%20low-level%20vision%20tasks%20can%0Abenefit%20from%20MLLMs.%20We%20find%20that%20most%20current%20MLLMs%20are%20blind%20to%20low-level%0Afeatures%20due%20to%20their%20design%20of%20vision%20modules%2C%20thus%20are%20inherently%20incapable%0Afor%20solving%20low-level%20vision%20tasks.%20In%20this%20work%2C%20we%20purpose%20%24%5Ctextbf%7BLM4LV%7D%24%2C%0Aa%20framework%20that%20enables%20a%20FROZEN%20LLM%20to%20solve%20a%20range%20of%20low-level%20vision%0Atasks%20without%20any%20multi-modal%20data%20or%20prior.%20This%20showcases%20the%20LLM%27s%20strong%0Apotential%20in%20low-level%20vision%20and%20bridges%20the%20gap%20between%20MLLMs%20and%20low-level%0Avision%20tasks.%20We%20hope%20this%20work%20can%20inspire%20new%20perspectives%20on%20LLMs%20and%20deeper%0Aunderstanding%20of%20their%20mechanisms.%20Code%20is%20available%20at%0Ahttps%3A//github.com/bytetriper/LM4LV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15734v2&entry.124074799=Read"},
{"title": "Robust Inverse Graphics via Probabilistic Inference", "author": "Tuan Anh Le and Pavel Sountsov and Matthew D. Hoffman and Ben Lee and Brian Patton and Rif A. Saurous", "abstract": "  How do we infer a 3D scene from a single image in the presence of corruptions\nlike rain, snow or fog? Straightforward domain randomization relies on knowing\nthe family of corruptions ahead of time. Here, we propose a Bayesian\napproach-dubbed robust inverse graphics (RIG)-that relies on a strong scene\nprior and an uninformative uniform corruption prior, making it applicable to a\nwide range of corruptions. Given a single image, RIG performs posterior\ninference jointly over the scene and the corruption. We demonstrate this idea\nby training a neural radiance field (NeRF) scene prior and using a secondary\nNeRF to represent the corruptions over which we place an uninformative prior.\nRIG, trained only on clean data, outperforms depth estimators and alternative\nNeRF approaches that perform point estimation instead of full inference. The\nresults hold for a number of scene prior architectures based on normalizing\nflows and diffusion models. For the latter, we develop reconstruction-guidance\nwith auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is\napplicable in the presence of auxiliary latent variables such as the\ncorruption. RIG demonstrates how scene priors can be used beyond generation\ntasks.\n", "link": "http://arxiv.org/abs/2402.01915v2", "date": "2024-06-11", "relevancy": 2.168, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5531}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5357}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Inverse%20Graphics%20via%20Probabilistic%20Inference&body=Title%3A%20Robust%20Inverse%20Graphics%20via%20Probabilistic%20Inference%0AAuthor%3A%20Tuan%20Anh%20Le%20and%20Pavel%20Sountsov%20and%20Matthew%20D.%20Hoffman%20and%20Ben%20Lee%20and%20Brian%20Patton%20and%20Rif%20A.%20Saurous%0AAbstract%3A%20%20%20How%20do%20we%20infer%20a%203D%20scene%20from%20a%20single%20image%20in%20the%20presence%20of%20corruptions%0Alike%20rain%2C%20snow%20or%20fog%3F%20Straightforward%20domain%20randomization%20relies%20on%20knowing%0Athe%20family%20of%20corruptions%20ahead%20of%20time.%20Here%2C%20we%20propose%20a%20Bayesian%0Aapproach-dubbed%20robust%20inverse%20graphics%20%28RIG%29-that%20relies%20on%20a%20strong%20scene%0Aprior%20and%20an%20uninformative%20uniform%20corruption%20prior%2C%20making%20it%20applicable%20to%20a%0Awide%20range%20of%20corruptions.%20Given%20a%20single%20image%2C%20RIG%20performs%20posterior%0Ainference%20jointly%20over%20the%20scene%20and%20the%20corruption.%20We%20demonstrate%20this%20idea%0Aby%20training%20a%20neural%20radiance%20field%20%28NeRF%29%20scene%20prior%20and%20using%20a%20secondary%0ANeRF%20to%20represent%20the%20corruptions%20over%20which%20we%20place%20an%20uninformative%20prior.%0ARIG%2C%20trained%20only%20on%20clean%20data%2C%20outperforms%20depth%20estimators%20and%20alternative%0ANeRF%20approaches%20that%20perform%20point%20estimation%20instead%20of%20full%20inference.%20The%0Aresults%20hold%20for%20a%20number%20of%20scene%20prior%20architectures%20based%20on%20normalizing%0Aflows%20and%20diffusion%20models.%20For%20the%20latter%2C%20we%20develop%20reconstruction-guidance%0Awith%20auxiliary%20latents%20%28ReGAL%29-a%20diffusion%20conditioning%20algorithm%20that%20is%0Aapplicable%20in%20the%20presence%20of%20auxiliary%20latent%20variables%20such%20as%20the%0Acorruption.%20RIG%20demonstrates%20how%20scene%20priors%20can%20be%20used%20beyond%20generation%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Inverse%2520Graphics%2520via%2520Probabilistic%2520Inference%26entry.906535625%3DTuan%2520Anh%2520Le%2520and%2520Pavel%2520Sountsov%2520and%2520Matthew%2520D.%2520Hoffman%2520and%2520Ben%2520Lee%2520and%2520Brian%2520Patton%2520and%2520Rif%2520A.%2520Saurous%26entry.1292438233%3D%2520%2520How%2520do%2520we%2520infer%2520a%25203D%2520scene%2520from%2520a%2520single%2520image%2520in%2520the%2520presence%2520of%2520corruptions%250Alike%2520rain%252C%2520snow%2520or%2520fog%253F%2520Straightforward%2520domain%2520randomization%2520relies%2520on%2520knowing%250Athe%2520family%2520of%2520corruptions%2520ahead%2520of%2520time.%2520Here%252C%2520we%2520propose%2520a%2520Bayesian%250Aapproach-dubbed%2520robust%2520inverse%2520graphics%2520%2528RIG%2529-that%2520relies%2520on%2520a%2520strong%2520scene%250Aprior%2520and%2520an%2520uninformative%2520uniform%2520corruption%2520prior%252C%2520making%2520it%2520applicable%2520to%2520a%250Awide%2520range%2520of%2520corruptions.%2520Given%2520a%2520single%2520image%252C%2520RIG%2520performs%2520posterior%250Ainference%2520jointly%2520over%2520the%2520scene%2520and%2520the%2520corruption.%2520We%2520demonstrate%2520this%2520idea%250Aby%2520training%2520a%2520neural%2520radiance%2520field%2520%2528NeRF%2529%2520scene%2520prior%2520and%2520using%2520a%2520secondary%250ANeRF%2520to%2520represent%2520the%2520corruptions%2520over%2520which%2520we%2520place%2520an%2520uninformative%2520prior.%250ARIG%252C%2520trained%2520only%2520on%2520clean%2520data%252C%2520outperforms%2520depth%2520estimators%2520and%2520alternative%250ANeRF%2520approaches%2520that%2520perform%2520point%2520estimation%2520instead%2520of%2520full%2520inference.%2520The%250Aresults%2520hold%2520for%2520a%2520number%2520of%2520scene%2520prior%2520architectures%2520based%2520on%2520normalizing%250Aflows%2520and%2520diffusion%2520models.%2520For%2520the%2520latter%252C%2520we%2520develop%2520reconstruction-guidance%250Awith%2520auxiliary%2520latents%2520%2528ReGAL%2529-a%2520diffusion%2520conditioning%2520algorithm%2520that%2520is%250Aapplicable%2520in%2520the%2520presence%2520of%2520auxiliary%2520latent%2520variables%2520such%2520as%2520the%250Acorruption.%2520RIG%2520demonstrates%2520how%2520scene%2520priors%2520can%2520be%2520used%2520beyond%2520generation%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Inverse%20Graphics%20via%20Probabilistic%20Inference&entry.906535625=Tuan%20Anh%20Le%20and%20Pavel%20Sountsov%20and%20Matthew%20D.%20Hoffman%20and%20Ben%20Lee%20and%20Brian%20Patton%20and%20Rif%20A.%20Saurous&entry.1292438233=%20%20How%20do%20we%20infer%20a%203D%20scene%20from%20a%20single%20image%20in%20the%20presence%20of%20corruptions%0Alike%20rain%2C%20snow%20or%20fog%3F%20Straightforward%20domain%20randomization%20relies%20on%20knowing%0Athe%20family%20of%20corruptions%20ahead%20of%20time.%20Here%2C%20we%20propose%20a%20Bayesian%0Aapproach-dubbed%20robust%20inverse%20graphics%20%28RIG%29-that%20relies%20on%20a%20strong%20scene%0Aprior%20and%20an%20uninformative%20uniform%20corruption%20prior%2C%20making%20it%20applicable%20to%20a%0Awide%20range%20of%20corruptions.%20Given%20a%20single%20image%2C%20RIG%20performs%20posterior%0Ainference%20jointly%20over%20the%20scene%20and%20the%20corruption.%20We%20demonstrate%20this%20idea%0Aby%20training%20a%20neural%20radiance%20field%20%28NeRF%29%20scene%20prior%20and%20using%20a%20secondary%0ANeRF%20to%20represent%20the%20corruptions%20over%20which%20we%20place%20an%20uninformative%20prior.%0ARIG%2C%20trained%20only%20on%20clean%20data%2C%20outperforms%20depth%20estimators%20and%20alternative%0ANeRF%20approaches%20that%20perform%20point%20estimation%20instead%20of%20full%20inference.%20The%0Aresults%20hold%20for%20a%20number%20of%20scene%20prior%20architectures%20based%20on%20normalizing%0Aflows%20and%20diffusion%20models.%20For%20the%20latter%2C%20we%20develop%20reconstruction-guidance%0Awith%20auxiliary%20latents%20%28ReGAL%29-a%20diffusion%20conditioning%20algorithm%20that%20is%0Aapplicable%20in%20the%20presence%20of%20auxiliary%20latent%20variables%20such%20as%20the%0Acorruption.%20RIG%20demonstrates%20how%20scene%20priors%20can%20be%20used%20beyond%20generation%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01915v2&entry.124074799=Read"},
{"title": "Benchmarking Vision-Language Contrastive Methods for Medical\n  Representation Learning", "author": "Shuvendu Roy and Yasaman Parhizkar and Franklin Ogidi and Vahid Reza Khazaie and Michael Colacci and Ali Etemad and Elham Dolatabadi and Arash Afkanpour", "abstract": "  We perform a comprehensive benchmarking of contrastive frameworks for\nlearning multimodal representations in the medical domain. Through this study,\nwe aim to answer the following research questions: (i) How transferable are\ngeneral-domain representations to the medical domain? (ii) Is multimodal\ncontrastive training sufficient, or does it benefit from unimodal training as\nwell? (iii) What is the impact of feature granularity on the effectiveness of\nmultimodal medical representation learning? To answer these questions, we\ninvestigate eight contrastive learning approaches under identical training\nsetups, and train them on 2.8 million image-text pairs from four datasets, and\nevaluate them on 25 downstream tasks, including classification (zero-shot and\nlinear probing), image-to-text and text-to-image retrieval, and visual\nquestion-answering. Our findings suggest a positive answer to the first\nquestion, a negative answer to the second question, and the benefit of learning\nfine-grained features. Finally, we make our code publicly available.\n", "link": "http://arxiv.org/abs/2406.07450v1", "date": "2024-06-11", "relevancy": 2.1678, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5671}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5371}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Vision-Language%20Contrastive%20Methods%20for%20Medical%0A%20%20Representation%20Learning&body=Title%3A%20Benchmarking%20Vision-Language%20Contrastive%20Methods%20for%20Medical%0A%20%20Representation%20Learning%0AAuthor%3A%20Shuvendu%20Roy%20and%20Yasaman%20Parhizkar%20and%20Franklin%20Ogidi%20and%20Vahid%20Reza%20Khazaie%20and%20Michael%20Colacci%20and%20Ali%20Etemad%20and%20Elham%20Dolatabadi%20and%20Arash%20Afkanpour%0AAbstract%3A%20%20%20We%20perform%20a%20comprehensive%20benchmarking%20of%20contrastive%20frameworks%20for%0Alearning%20multimodal%20representations%20in%20the%20medical%20domain.%20Through%20this%20study%2C%0Awe%20aim%20to%20answer%20the%20following%20research%20questions%3A%20%28i%29%20How%20transferable%20are%0Ageneral-domain%20representations%20to%20the%20medical%20domain%3F%20%28ii%29%20Is%20multimodal%0Acontrastive%20training%20sufficient%2C%20or%20does%20it%20benefit%20from%20unimodal%20training%20as%0Awell%3F%20%28iii%29%20What%20is%20the%20impact%20of%20feature%20granularity%20on%20the%20effectiveness%20of%0Amultimodal%20medical%20representation%20learning%3F%20To%20answer%20these%20questions%2C%20we%0Ainvestigate%20eight%20contrastive%20learning%20approaches%20under%20identical%20training%0Asetups%2C%20and%20train%20them%20on%202.8%20million%20image-text%20pairs%20from%20four%20datasets%2C%20and%0Aevaluate%20them%20on%2025%20downstream%20tasks%2C%20including%20classification%20%28zero-shot%20and%0Alinear%20probing%29%2C%20image-to-text%20and%20text-to-image%20retrieval%2C%20and%20visual%0Aquestion-answering.%20Our%20findings%20suggest%20a%20positive%20answer%20to%20the%20first%0Aquestion%2C%20a%20negative%20answer%20to%20the%20second%20question%2C%20and%20the%20benefit%20of%20learning%0Afine-grained%20features.%20Finally%2C%20we%20make%20our%20code%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Vision-Language%2520Contrastive%2520Methods%2520for%2520Medical%250A%2520%2520Representation%2520Learning%26entry.906535625%3DShuvendu%2520Roy%2520and%2520Yasaman%2520Parhizkar%2520and%2520Franklin%2520Ogidi%2520and%2520Vahid%2520Reza%2520Khazaie%2520and%2520Michael%2520Colacci%2520and%2520Ali%2520Etemad%2520and%2520Elham%2520Dolatabadi%2520and%2520Arash%2520Afkanpour%26entry.1292438233%3D%2520%2520We%2520perform%2520a%2520comprehensive%2520benchmarking%2520of%2520contrastive%2520frameworks%2520for%250Alearning%2520multimodal%2520representations%2520in%2520the%2520medical%2520domain.%2520Through%2520this%2520study%252C%250Awe%2520aim%2520to%2520answer%2520the%2520following%2520research%2520questions%253A%2520%2528i%2529%2520How%2520transferable%2520are%250Ageneral-domain%2520representations%2520to%2520the%2520medical%2520domain%253F%2520%2528ii%2529%2520Is%2520multimodal%250Acontrastive%2520training%2520sufficient%252C%2520or%2520does%2520it%2520benefit%2520from%2520unimodal%2520training%2520as%250Awell%253F%2520%2528iii%2529%2520What%2520is%2520the%2520impact%2520of%2520feature%2520granularity%2520on%2520the%2520effectiveness%2520of%250Amultimodal%2520medical%2520representation%2520learning%253F%2520To%2520answer%2520these%2520questions%252C%2520we%250Ainvestigate%2520eight%2520contrastive%2520learning%2520approaches%2520under%2520identical%2520training%250Asetups%252C%2520and%2520train%2520them%2520on%25202.8%2520million%2520image-text%2520pairs%2520from%2520four%2520datasets%252C%2520and%250Aevaluate%2520them%2520on%252025%2520downstream%2520tasks%252C%2520including%2520classification%2520%2528zero-shot%2520and%250Alinear%2520probing%2529%252C%2520image-to-text%2520and%2520text-to-image%2520retrieval%252C%2520and%2520visual%250Aquestion-answering.%2520Our%2520findings%2520suggest%2520a%2520positive%2520answer%2520to%2520the%2520first%250Aquestion%252C%2520a%2520negative%2520answer%2520to%2520the%2520second%2520question%252C%2520and%2520the%2520benefit%2520of%2520learning%250Afine-grained%2520features.%2520Finally%252C%2520we%2520make%2520our%2520code%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Vision-Language%20Contrastive%20Methods%20for%20Medical%0A%20%20Representation%20Learning&entry.906535625=Shuvendu%20Roy%20and%20Yasaman%20Parhizkar%20and%20Franklin%20Ogidi%20and%20Vahid%20Reza%20Khazaie%20and%20Michael%20Colacci%20and%20Ali%20Etemad%20and%20Elham%20Dolatabadi%20and%20Arash%20Afkanpour&entry.1292438233=%20%20We%20perform%20a%20comprehensive%20benchmarking%20of%20contrastive%20frameworks%20for%0Alearning%20multimodal%20representations%20in%20the%20medical%20domain.%20Through%20this%20study%2C%0Awe%20aim%20to%20answer%20the%20following%20research%20questions%3A%20%28i%29%20How%20transferable%20are%0Ageneral-domain%20representations%20to%20the%20medical%20domain%3F%20%28ii%29%20Is%20multimodal%0Acontrastive%20training%20sufficient%2C%20or%20does%20it%20benefit%20from%20unimodal%20training%20as%0Awell%3F%20%28iii%29%20What%20is%20the%20impact%20of%20feature%20granularity%20on%20the%20effectiveness%20of%0Amultimodal%20medical%20representation%20learning%3F%20To%20answer%20these%20questions%2C%20we%0Ainvestigate%20eight%20contrastive%20learning%20approaches%20under%20identical%20training%0Asetups%2C%20and%20train%20them%20on%202.8%20million%20image-text%20pairs%20from%20four%20datasets%2C%20and%0Aevaluate%20them%20on%2025%20downstream%20tasks%2C%20including%20classification%20%28zero-shot%20and%0Alinear%20probing%29%2C%20image-to-text%20and%20text-to-image%20retrieval%2C%20and%20visual%0Aquestion-answering.%20Our%20findings%20suggest%20a%20positive%20answer%20to%20the%20first%0Aquestion%2C%20a%20negative%20answer%20to%20the%20second%20question%2C%20and%20the%20benefit%20of%20learning%0Afine-grained%20features.%20Finally%2C%20we%20make%20our%20code%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07450v1&entry.124074799=Read"},
{"title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models\n  Understand Commonsense?", "author": "Xingyu Fu and Muyu He and Yujie Lu and William Yang Wang and Dan Roth", "abstract": "  We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that fit commonsense in\nreal life, which we call Commonsense-T2I. Given two adversarial text prompts\ncontaining an identical set of action words with minor differences, such as \"a\nlightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate\nwhether T2I models can conduct visual-commonsense reasoning, e.g. produce\nimages that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\"\ncorrespondingly. Commonsense-T2I presents an adversarial challenge, providing\npairwise text prompts along with expected outputs. The dataset is carefully\nhand-curated by experts and annotated with fine-grained labels, such as\ncommonsense type and likelihood of the expected outputs, to assist analyzing\nmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I models\nand surprisingly find that, there is still a large gap between image synthesis\nand real life photos--even the DALL-E 3 model could only achieve 48.92% on\nCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%\naccuracy. Our experiments show that GPT-enriched prompts cannot solve this\nchallenge, and we include a detailed analysis about possible reasons for such\ndeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation\nbenchmark for T2I commonsense checking, fostering advancements in real life\nimage generation.\n", "link": "http://arxiv.org/abs/2406.07546v1", "date": "2024-06-11", "relevancy": 2.1619, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5454}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5448}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Commonsense-T2I%20Challenge%3A%20Can%20Text-to-Image%20Generation%20Models%0A%20%20Understand%20Commonsense%3F&body=Title%3A%20Commonsense-T2I%20Challenge%3A%20Can%20Text-to-Image%20Generation%20Models%0A%20%20Understand%20Commonsense%3F%0AAuthor%3A%20Xingyu%20Fu%20and%20Muyu%20He%20and%20Yujie%20Lu%20and%20William%20Yang%20Wang%20and%20Dan%20Roth%0AAbstract%3A%20%20%20We%20present%20a%20novel%20task%20and%20benchmark%20for%20evaluating%20the%20ability%20of%0Atext-to-image%28T2I%29%20generation%20models%20to%20produce%20images%20that%20fit%20commonsense%20in%0Areal%20life%2C%20which%20we%20call%20Commonsense-T2I.%20Given%20two%20adversarial%20text%20prompts%0Acontaining%20an%20identical%20set%20of%20action%20words%20with%20minor%20differences%2C%20such%20as%20%22a%0Alightbulb%20without%20electricity%22%20v.s.%20%22a%20lightbulb%20with%20electricity%22%2C%20we%20evaluate%0Awhether%20T2I%20models%20can%20conduct%20visual-commonsense%20reasoning%2C%20e.g.%20produce%0Aimages%20that%20fit%20%22the%20lightbulb%20is%20unlit%22%20vs.%20%22the%20lightbulb%20is%20lit%22%0Acorrespondingly.%20Commonsense-T2I%20presents%20an%20adversarial%20challenge%2C%20providing%0Apairwise%20text%20prompts%20along%20with%20expected%20outputs.%20The%20dataset%20is%20carefully%0Ahand-curated%20by%20experts%20and%20annotated%20with%20fine-grained%20labels%2C%20such%20as%0Acommonsense%20type%20and%20likelihood%20of%20the%20expected%20outputs%2C%20to%20assist%20analyzing%0Amodel%20behavior.%20We%20benchmark%20a%20variety%20of%20state-of-the-art%20%28sota%29%20T2I%20models%0Aand%20surprisingly%20find%20that%2C%20there%20is%20still%20a%20large%20gap%20between%20image%20synthesis%0Aand%20real%20life%20photos--even%20the%20DALL-E%203%20model%20could%20only%20achieve%2048.92%25%20on%0ACommonsense-T2I%2C%20and%20the%20stable%20diffusion%20XL%20model%20only%20achieves%2024.92%25%0Aaccuracy.%20Our%20experiments%20show%20that%20GPT-enriched%20prompts%20cannot%20solve%20this%0Achallenge%2C%20and%20we%20include%20a%20detailed%20analysis%20about%20possible%20reasons%20for%20such%0Adeficiency.%20We%20aim%20for%20Commonsense-T2I%20to%20serve%20as%20a%20high-quality%20evaluation%0Abenchmark%20for%20T2I%20commonsense%20checking%2C%20fostering%20advancements%20in%20real%20life%0Aimage%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommonsense-T2I%2520Challenge%253A%2520Can%2520Text-to-Image%2520Generation%2520Models%250A%2520%2520Understand%2520Commonsense%253F%26entry.906535625%3DXingyu%2520Fu%2520and%2520Muyu%2520He%2520and%2520Yujie%2520Lu%2520and%2520William%2520Yang%2520Wang%2520and%2520Dan%2520Roth%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520task%2520and%2520benchmark%2520for%2520evaluating%2520the%2520ability%2520of%250Atext-to-image%2528T2I%2529%2520generation%2520models%2520to%2520produce%2520images%2520that%2520fit%2520commonsense%2520in%250Areal%2520life%252C%2520which%2520we%2520call%2520Commonsense-T2I.%2520Given%2520two%2520adversarial%2520text%2520prompts%250Acontaining%2520an%2520identical%2520set%2520of%2520action%2520words%2520with%2520minor%2520differences%252C%2520such%2520as%2520%2522a%250Alightbulb%2520without%2520electricity%2522%2520v.s.%2520%2522a%2520lightbulb%2520with%2520electricity%2522%252C%2520we%2520evaluate%250Awhether%2520T2I%2520models%2520can%2520conduct%2520visual-commonsense%2520reasoning%252C%2520e.g.%2520produce%250Aimages%2520that%2520fit%2520%2522the%2520lightbulb%2520is%2520unlit%2522%2520vs.%2520%2522the%2520lightbulb%2520is%2520lit%2522%250Acorrespondingly.%2520Commonsense-T2I%2520presents%2520an%2520adversarial%2520challenge%252C%2520providing%250Apairwise%2520text%2520prompts%2520along%2520with%2520expected%2520outputs.%2520The%2520dataset%2520is%2520carefully%250Ahand-curated%2520by%2520experts%2520and%2520annotated%2520with%2520fine-grained%2520labels%252C%2520such%2520as%250Acommonsense%2520type%2520and%2520likelihood%2520of%2520the%2520expected%2520outputs%252C%2520to%2520assist%2520analyzing%250Amodel%2520behavior.%2520We%2520benchmark%2520a%2520variety%2520of%2520state-of-the-art%2520%2528sota%2529%2520T2I%2520models%250Aand%2520surprisingly%2520find%2520that%252C%2520there%2520is%2520still%2520a%2520large%2520gap%2520between%2520image%2520synthesis%250Aand%2520real%2520life%2520photos--even%2520the%2520DALL-E%25203%2520model%2520could%2520only%2520achieve%252048.92%2525%2520on%250ACommonsense-T2I%252C%2520and%2520the%2520stable%2520diffusion%2520XL%2520model%2520only%2520achieves%252024.92%2525%250Aaccuracy.%2520Our%2520experiments%2520show%2520that%2520GPT-enriched%2520prompts%2520cannot%2520solve%2520this%250Achallenge%252C%2520and%2520we%2520include%2520a%2520detailed%2520analysis%2520about%2520possible%2520reasons%2520for%2520such%250Adeficiency.%2520We%2520aim%2520for%2520Commonsense-T2I%2520to%2520serve%2520as%2520a%2520high-quality%2520evaluation%250Abenchmark%2520for%2520T2I%2520commonsense%2520checking%252C%2520fostering%2520advancements%2520in%2520real%2520life%250Aimage%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Commonsense-T2I%20Challenge%3A%20Can%20Text-to-Image%20Generation%20Models%0A%20%20Understand%20Commonsense%3F&entry.906535625=Xingyu%20Fu%20and%20Muyu%20He%20and%20Yujie%20Lu%20and%20William%20Yang%20Wang%20and%20Dan%20Roth&entry.1292438233=%20%20We%20present%20a%20novel%20task%20and%20benchmark%20for%20evaluating%20the%20ability%20of%0Atext-to-image%28T2I%29%20generation%20models%20to%20produce%20images%20that%20fit%20commonsense%20in%0Areal%20life%2C%20which%20we%20call%20Commonsense-T2I.%20Given%20two%20adversarial%20text%20prompts%0Acontaining%20an%20identical%20set%20of%20action%20words%20with%20minor%20differences%2C%20such%20as%20%22a%0Alightbulb%20without%20electricity%22%20v.s.%20%22a%20lightbulb%20with%20electricity%22%2C%20we%20evaluate%0Awhether%20T2I%20models%20can%20conduct%20visual-commonsense%20reasoning%2C%20e.g.%20produce%0Aimages%20that%20fit%20%22the%20lightbulb%20is%20unlit%22%20vs.%20%22the%20lightbulb%20is%20lit%22%0Acorrespondingly.%20Commonsense-T2I%20presents%20an%20adversarial%20challenge%2C%20providing%0Apairwise%20text%20prompts%20along%20with%20expected%20outputs.%20The%20dataset%20is%20carefully%0Ahand-curated%20by%20experts%20and%20annotated%20with%20fine-grained%20labels%2C%20such%20as%0Acommonsense%20type%20and%20likelihood%20of%20the%20expected%20outputs%2C%20to%20assist%20analyzing%0Amodel%20behavior.%20We%20benchmark%20a%20variety%20of%20state-of-the-art%20%28sota%29%20T2I%20models%0Aand%20surprisingly%20find%20that%2C%20there%20is%20still%20a%20large%20gap%20between%20image%20synthesis%0Aand%20real%20life%20photos--even%20the%20DALL-E%203%20model%20could%20only%20achieve%2048.92%25%20on%0ACommonsense-T2I%2C%20and%20the%20stable%20diffusion%20XL%20model%20only%20achieves%2024.92%25%0Aaccuracy.%20Our%20experiments%20show%20that%20GPT-enriched%20prompts%20cannot%20solve%20this%0Achallenge%2C%20and%20we%20include%20a%20detailed%20analysis%20about%20possible%20reasons%20for%20such%0Adeficiency.%20We%20aim%20for%20Commonsense-T2I%20to%20serve%20as%20a%20high-quality%20evaluation%0Abenchmark%20for%20T2I%20commonsense%20checking%2C%20fostering%20advancements%20in%20real%20life%0Aimage%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07546v1&entry.124074799=Read"},
{"title": "Let Go of Your Labels with Unsupervised Transfer", "author": "Artyom Gadetsky and Yulun Jiang and Maria Brbic", "abstract": "  Foundation vision-language models have enabled remarkable zero-shot\ntransferability of the pre-trained representations to a wide range of\ndownstream tasks. However, to solve a new task, zero-shot transfer still\nnecessitates human guidance to define visual categories that appear in the\ndata. Here, we show that fully unsupervised transfer emerges when searching for\nthe labeling of a dataset that induces maximal margin classifiers in\nrepresentation spaces of different foundation models. We present TURTLE, a\nfully unsupervised method that effectively employs this guiding principle to\nuncover the underlying labeling of a downstream dataset without any supervision\nand task-specific representation learning. We evaluate TURTLE on a diverse\nbenchmark suite of 26 datasets and show that it achieves new state-of-the-art\nunsupervised performance. Furthermore, TURTLE, although being fully\nunsupervised, outperforms zero-shot transfer baselines on a wide range of\ndatasets. In particular, TURTLE matches the average performance of CLIP\nzero-shot on 26 datasets by employing the same representation space, spanning a\nwide range of architectures and model sizes. By guiding the search for the\nunderlying labeling using the representation spaces of two foundation models,\nTURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines,\ndemonstrating the surprising power and effectiveness of unsupervised transfer.\n", "link": "http://arxiv.org/abs/2406.07236v1", "date": "2024-06-11", "relevancy": 2.1617, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5568}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5318}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%20Go%20of%20Your%20Labels%20with%20Unsupervised%20Transfer&body=Title%3A%20Let%20Go%20of%20Your%20Labels%20with%20Unsupervised%20Transfer%0AAuthor%3A%20Artyom%20Gadetsky%20and%20Yulun%20Jiang%20and%20Maria%20Brbic%0AAbstract%3A%20%20%20Foundation%20vision-language%20models%20have%20enabled%20remarkable%20zero-shot%0Atransferability%20of%20the%20pre-trained%20representations%20to%20a%20wide%20range%20of%0Adownstream%20tasks.%20However%2C%20to%20solve%20a%20new%20task%2C%20zero-shot%20transfer%20still%0Anecessitates%20human%20guidance%20to%20define%20visual%20categories%20that%20appear%20in%20the%0Adata.%20Here%2C%20we%20show%20that%20fully%20unsupervised%20transfer%20emerges%20when%20searching%20for%0Athe%20labeling%20of%20a%20dataset%20that%20induces%20maximal%20margin%20classifiers%20in%0Arepresentation%20spaces%20of%20different%20foundation%20models.%20We%20present%20TURTLE%2C%20a%0Afully%20unsupervised%20method%20that%20effectively%20employs%20this%20guiding%20principle%20to%0Auncover%20the%20underlying%20labeling%20of%20a%20downstream%20dataset%20without%20any%20supervision%0Aand%20task-specific%20representation%20learning.%20We%20evaluate%20TURTLE%20on%20a%20diverse%0Abenchmark%20suite%20of%2026%20datasets%20and%20show%20that%20it%20achieves%20new%20state-of-the-art%0Aunsupervised%20performance.%20Furthermore%2C%20TURTLE%2C%20although%20being%20fully%0Aunsupervised%2C%20outperforms%20zero-shot%20transfer%20baselines%20on%20a%20wide%20range%20of%0Adatasets.%20In%20particular%2C%20TURTLE%20matches%20the%20average%20performance%20of%20CLIP%0Azero-shot%20on%2026%20datasets%20by%20employing%20the%20same%20representation%20space%2C%20spanning%20a%0Awide%20range%20of%20architectures%20and%20model%20sizes.%20By%20guiding%20the%20search%20for%20the%0Aunderlying%20labeling%20using%20the%20representation%20spaces%20of%20two%20foundation%20models%2C%0ATURTLE%20surpasses%20zero-shot%20transfer%20and%20unsupervised%20prompt%20tuning%20baselines%2C%0Ademonstrating%20the%20surprising%20power%20and%20effectiveness%20of%20unsupervised%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2520Go%2520of%2520Your%2520Labels%2520with%2520Unsupervised%2520Transfer%26entry.906535625%3DArtyom%2520Gadetsky%2520and%2520Yulun%2520Jiang%2520and%2520Maria%2520Brbic%26entry.1292438233%3D%2520%2520Foundation%2520vision-language%2520models%2520have%2520enabled%2520remarkable%2520zero-shot%250Atransferability%2520of%2520the%2520pre-trained%2520representations%2520to%2520a%2520wide%2520range%2520of%250Adownstream%2520tasks.%2520However%252C%2520to%2520solve%2520a%2520new%2520task%252C%2520zero-shot%2520transfer%2520still%250Anecessitates%2520human%2520guidance%2520to%2520define%2520visual%2520categories%2520that%2520appear%2520in%2520the%250Adata.%2520Here%252C%2520we%2520show%2520that%2520fully%2520unsupervised%2520transfer%2520emerges%2520when%2520searching%2520for%250Athe%2520labeling%2520of%2520a%2520dataset%2520that%2520induces%2520maximal%2520margin%2520classifiers%2520in%250Arepresentation%2520spaces%2520of%2520different%2520foundation%2520models.%2520We%2520present%2520TURTLE%252C%2520a%250Afully%2520unsupervised%2520method%2520that%2520effectively%2520employs%2520this%2520guiding%2520principle%2520to%250Auncover%2520the%2520underlying%2520labeling%2520of%2520a%2520downstream%2520dataset%2520without%2520any%2520supervision%250Aand%2520task-specific%2520representation%2520learning.%2520We%2520evaluate%2520TURTLE%2520on%2520a%2520diverse%250Abenchmark%2520suite%2520of%252026%2520datasets%2520and%2520show%2520that%2520it%2520achieves%2520new%2520state-of-the-art%250Aunsupervised%2520performance.%2520Furthermore%252C%2520TURTLE%252C%2520although%2520being%2520fully%250Aunsupervised%252C%2520outperforms%2520zero-shot%2520transfer%2520baselines%2520on%2520a%2520wide%2520range%2520of%250Adatasets.%2520In%2520particular%252C%2520TURTLE%2520matches%2520the%2520average%2520performance%2520of%2520CLIP%250Azero-shot%2520on%252026%2520datasets%2520by%2520employing%2520the%2520same%2520representation%2520space%252C%2520spanning%2520a%250Awide%2520range%2520of%2520architectures%2520and%2520model%2520sizes.%2520By%2520guiding%2520the%2520search%2520for%2520the%250Aunderlying%2520labeling%2520using%2520the%2520representation%2520spaces%2520of%2520two%2520foundation%2520models%252C%250ATURTLE%2520surpasses%2520zero-shot%2520transfer%2520and%2520unsupervised%2520prompt%2520tuning%2520baselines%252C%250Ademonstrating%2520the%2520surprising%2520power%2520and%2520effectiveness%2520of%2520unsupervised%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%20Go%20of%20Your%20Labels%20with%20Unsupervised%20Transfer&entry.906535625=Artyom%20Gadetsky%20and%20Yulun%20Jiang%20and%20Maria%20Brbic&entry.1292438233=%20%20Foundation%20vision-language%20models%20have%20enabled%20remarkable%20zero-shot%0Atransferability%20of%20the%20pre-trained%20representations%20to%20a%20wide%20range%20of%0Adownstream%20tasks.%20However%2C%20to%20solve%20a%20new%20task%2C%20zero-shot%20transfer%20still%0Anecessitates%20human%20guidance%20to%20define%20visual%20categories%20that%20appear%20in%20the%0Adata.%20Here%2C%20we%20show%20that%20fully%20unsupervised%20transfer%20emerges%20when%20searching%20for%0Athe%20labeling%20of%20a%20dataset%20that%20induces%20maximal%20margin%20classifiers%20in%0Arepresentation%20spaces%20of%20different%20foundation%20models.%20We%20present%20TURTLE%2C%20a%0Afully%20unsupervised%20method%20that%20effectively%20employs%20this%20guiding%20principle%20to%0Auncover%20the%20underlying%20labeling%20of%20a%20downstream%20dataset%20without%20any%20supervision%0Aand%20task-specific%20representation%20learning.%20We%20evaluate%20TURTLE%20on%20a%20diverse%0Abenchmark%20suite%20of%2026%20datasets%20and%20show%20that%20it%20achieves%20new%20state-of-the-art%0Aunsupervised%20performance.%20Furthermore%2C%20TURTLE%2C%20although%20being%20fully%0Aunsupervised%2C%20outperforms%20zero-shot%20transfer%20baselines%20on%20a%20wide%20range%20of%0Adatasets.%20In%20particular%2C%20TURTLE%20matches%20the%20average%20performance%20of%20CLIP%0Azero-shot%20on%2026%20datasets%20by%20employing%20the%20same%20representation%20space%2C%20spanning%20a%0Awide%20range%20of%20architectures%20and%20model%20sizes.%20By%20guiding%20the%20search%20for%20the%0Aunderlying%20labeling%20using%20the%20representation%20spaces%20of%20two%20foundation%20models%2C%0ATURTLE%20surpasses%20zero-shot%20transfer%20and%20unsupervised%20prompt%20tuning%20baselines%2C%0Ademonstrating%20the%20surprising%20power%20and%20effectiveness%20of%20unsupervised%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07236v1&entry.124074799=Read"},
{"title": "Unsupervised Object Detection with Theoretical Guarantees", "author": "Marian Longa and Jo\u00e3o F. Henriques", "abstract": "  Unsupervised object detection using deep neural networks is typically a\ndifficult problem with few to no guarantees about the learned representation.\nIn this work we present the first unsupervised object detection method that is\ntheoretically guaranteed to recover the true object positions up to\nquantifiable small shifts. We develop an unsupervised object detection\narchitecture and prove that the learned variables correspond to the true object\npositions up to small shifts related to the encoder and decoder receptive field\nsizes, the object sizes, and the widths of the Gaussians used in the rendering\nprocess. We perform detailed analysis of how the error depends on each of these\nvariables and perform synthetic experiments validating our theoretical\npredictions up to a precision of individual pixels. We also perform experiments\non CLEVR-based data and show that, unlike current SOTA object detection methods\n(SAM, CutLER), our method's prediction errors always lie within our theoretical\nbounds. We hope that this work helps open up an avenue of research into object\ndetection methods with theoretical guarantees.\n", "link": "http://arxiv.org/abs/2406.07284v1", "date": "2024-06-11", "relevancy": 2.158, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5673}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Object%20Detection%20with%20Theoretical%20Guarantees&body=Title%3A%20Unsupervised%20Object%20Detection%20with%20Theoretical%20Guarantees%0AAuthor%3A%20Marian%20Longa%20and%20Jo%C3%A3o%20F.%20Henriques%0AAbstract%3A%20%20%20Unsupervised%20object%20detection%20using%20deep%20neural%20networks%20is%20typically%20a%0Adifficult%20problem%20with%20few%20to%20no%20guarantees%20about%20the%20learned%20representation.%0AIn%20this%20work%20we%20present%20the%20first%20unsupervised%20object%20detection%20method%20that%20is%0Atheoretically%20guaranteed%20to%20recover%20the%20true%20object%20positions%20up%20to%0Aquantifiable%20small%20shifts.%20We%20develop%20an%20unsupervised%20object%20detection%0Aarchitecture%20and%20prove%20that%20the%20learned%20variables%20correspond%20to%20the%20true%20object%0Apositions%20up%20to%20small%20shifts%20related%20to%20the%20encoder%20and%20decoder%20receptive%20field%0Asizes%2C%20the%20object%20sizes%2C%20and%20the%20widths%20of%20the%20Gaussians%20used%20in%20the%20rendering%0Aprocess.%20We%20perform%20detailed%20analysis%20of%20how%20the%20error%20depends%20on%20each%20of%20these%0Avariables%20and%20perform%20synthetic%20experiments%20validating%20our%20theoretical%0Apredictions%20up%20to%20a%20precision%20of%20individual%20pixels.%20We%20also%20perform%20experiments%0Aon%20CLEVR-based%20data%20and%20show%20that%2C%20unlike%20current%20SOTA%20object%20detection%20methods%0A%28SAM%2C%20CutLER%29%2C%20our%20method%27s%20prediction%20errors%20always%20lie%20within%20our%20theoretical%0Abounds.%20We%20hope%20that%20this%20work%20helps%20open%20up%20an%20avenue%20of%20research%20into%20object%0Adetection%20methods%20with%20theoretical%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Object%2520Detection%2520with%2520Theoretical%2520Guarantees%26entry.906535625%3DMarian%2520Longa%2520and%2520Jo%25C3%25A3o%2520F.%2520Henriques%26entry.1292438233%3D%2520%2520Unsupervised%2520object%2520detection%2520using%2520deep%2520neural%2520networks%2520is%2520typically%2520a%250Adifficult%2520problem%2520with%2520few%2520to%2520no%2520guarantees%2520about%2520the%2520learned%2520representation.%250AIn%2520this%2520work%2520we%2520present%2520the%2520first%2520unsupervised%2520object%2520detection%2520method%2520that%2520is%250Atheoretically%2520guaranteed%2520to%2520recover%2520the%2520true%2520object%2520positions%2520up%2520to%250Aquantifiable%2520small%2520shifts.%2520We%2520develop%2520an%2520unsupervised%2520object%2520detection%250Aarchitecture%2520and%2520prove%2520that%2520the%2520learned%2520variables%2520correspond%2520to%2520the%2520true%2520object%250Apositions%2520up%2520to%2520small%2520shifts%2520related%2520to%2520the%2520encoder%2520and%2520decoder%2520receptive%2520field%250Asizes%252C%2520the%2520object%2520sizes%252C%2520and%2520the%2520widths%2520of%2520the%2520Gaussians%2520used%2520in%2520the%2520rendering%250Aprocess.%2520We%2520perform%2520detailed%2520analysis%2520of%2520how%2520the%2520error%2520depends%2520on%2520each%2520of%2520these%250Avariables%2520and%2520perform%2520synthetic%2520experiments%2520validating%2520our%2520theoretical%250Apredictions%2520up%2520to%2520a%2520precision%2520of%2520individual%2520pixels.%2520We%2520also%2520perform%2520experiments%250Aon%2520CLEVR-based%2520data%2520and%2520show%2520that%252C%2520unlike%2520current%2520SOTA%2520object%2520detection%2520methods%250A%2528SAM%252C%2520CutLER%2529%252C%2520our%2520method%2527s%2520prediction%2520errors%2520always%2520lie%2520within%2520our%2520theoretical%250Abounds.%2520We%2520hope%2520that%2520this%2520work%2520helps%2520open%2520up%2520an%2520avenue%2520of%2520research%2520into%2520object%250Adetection%2520methods%2520with%2520theoretical%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Object%20Detection%20with%20Theoretical%20Guarantees&entry.906535625=Marian%20Longa%20and%20Jo%C3%A3o%20F.%20Henriques&entry.1292438233=%20%20Unsupervised%20object%20detection%20using%20deep%20neural%20networks%20is%20typically%20a%0Adifficult%20problem%20with%20few%20to%20no%20guarantees%20about%20the%20learned%20representation.%0AIn%20this%20work%20we%20present%20the%20first%20unsupervised%20object%20detection%20method%20that%20is%0Atheoretically%20guaranteed%20to%20recover%20the%20true%20object%20positions%20up%20to%0Aquantifiable%20small%20shifts.%20We%20develop%20an%20unsupervised%20object%20detection%0Aarchitecture%20and%20prove%20that%20the%20learned%20variables%20correspond%20to%20the%20true%20object%0Apositions%20up%20to%20small%20shifts%20related%20to%20the%20encoder%20and%20decoder%20receptive%20field%0Asizes%2C%20the%20object%20sizes%2C%20and%20the%20widths%20of%20the%20Gaussians%20used%20in%20the%20rendering%0Aprocess.%20We%20perform%20detailed%20analysis%20of%20how%20the%20error%20depends%20on%20each%20of%20these%0Avariables%20and%20perform%20synthetic%20experiments%20validating%20our%20theoretical%0Apredictions%20up%20to%20a%20precision%20of%20individual%20pixels.%20We%20also%20perform%20experiments%0Aon%20CLEVR-based%20data%20and%20show%20that%2C%20unlike%20current%20SOTA%20object%20detection%20methods%0A%28SAM%2C%20CutLER%29%2C%20our%20method%27s%20prediction%20errors%20always%20lie%20within%20our%20theoretical%0Abounds.%20We%20hope%20that%20this%20work%20helps%20open%20up%20an%20avenue%20of%20research%20into%20object%0Adetection%20methods%20with%20theoretical%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07284v1&entry.124074799=Read"},
{"title": "Identifiable Object-Centric Representation Learning via Probabilistic\n  Slot Attention", "author": "Avinash Kori and Francesco Locatello and Ainkaran Santhirasekaram and Francesca Toni and Ben Glocker and Fabio De Sousa Ribeiro", "abstract": "  Learning modular object-centric representations is crucial for systematic\ngeneralization. Existing methods show promising object-binding capabilities\nempirically, but theoretical identifiability guarantees remain relatively\nunderdeveloped. Understanding when object-centric representations can\ntheoretically be identified is crucial for scaling slot-based methods to\nhigh-dimensional images with correctness guarantees. To that end, we propose a\nprobabilistic slot-attention algorithm that imposes an aggregate mixture prior\nover object-centric slot representations, thereby providing slot\nidentifiability guarantees without supervision, up to an equivalence relation.\nWe provide empirical verification of our theoretical identifiability result\nusing both simple 2-dimensional data and high-resolution imaging datasets.\n", "link": "http://arxiv.org/abs/2406.07141v1", "date": "2024-06-11", "relevancy": 2.1572, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5431}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifiable%20Object-Centric%20Representation%20Learning%20via%20Probabilistic%0A%20%20Slot%20Attention&body=Title%3A%20Identifiable%20Object-Centric%20Representation%20Learning%20via%20Probabilistic%0A%20%20Slot%20Attention%0AAuthor%3A%20Avinash%20Kori%20and%20Francesco%20Locatello%20and%20Ainkaran%20Santhirasekaram%20and%20Francesca%20Toni%20and%20Ben%20Glocker%20and%20Fabio%20De%20Sousa%20Ribeiro%0AAbstract%3A%20%20%20Learning%20modular%20object-centric%20representations%20is%20crucial%20for%20systematic%0Ageneralization.%20Existing%20methods%20show%20promising%20object-binding%20capabilities%0Aempirically%2C%20but%20theoretical%20identifiability%20guarantees%20remain%20relatively%0Aunderdeveloped.%20Understanding%20when%20object-centric%20representations%20can%0Atheoretically%20be%20identified%20is%20crucial%20for%20scaling%20slot-based%20methods%20to%0Ahigh-dimensional%20images%20with%20correctness%20guarantees.%20To%20that%20end%2C%20we%20propose%20a%0Aprobabilistic%20slot-attention%20algorithm%20that%20imposes%20an%20aggregate%20mixture%20prior%0Aover%20object-centric%20slot%20representations%2C%20thereby%20providing%20slot%0Aidentifiability%20guarantees%20without%20supervision%2C%20up%20to%20an%20equivalence%20relation.%0AWe%20provide%20empirical%20verification%20of%20our%20theoretical%20identifiability%20result%0Ausing%20both%20simple%202-dimensional%20data%20and%20high-resolution%20imaging%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifiable%2520Object-Centric%2520Representation%2520Learning%2520via%2520Probabilistic%250A%2520%2520Slot%2520Attention%26entry.906535625%3DAvinash%2520Kori%2520and%2520Francesco%2520Locatello%2520and%2520Ainkaran%2520Santhirasekaram%2520and%2520Francesca%2520Toni%2520and%2520Ben%2520Glocker%2520and%2520Fabio%2520De%2520Sousa%2520Ribeiro%26entry.1292438233%3D%2520%2520Learning%2520modular%2520object-centric%2520representations%2520is%2520crucial%2520for%2520systematic%250Ageneralization.%2520Existing%2520methods%2520show%2520promising%2520object-binding%2520capabilities%250Aempirically%252C%2520but%2520theoretical%2520identifiability%2520guarantees%2520remain%2520relatively%250Aunderdeveloped.%2520Understanding%2520when%2520object-centric%2520representations%2520can%250Atheoretically%2520be%2520identified%2520is%2520crucial%2520for%2520scaling%2520slot-based%2520methods%2520to%250Ahigh-dimensional%2520images%2520with%2520correctness%2520guarantees.%2520To%2520that%2520end%252C%2520we%2520propose%2520a%250Aprobabilistic%2520slot-attention%2520algorithm%2520that%2520imposes%2520an%2520aggregate%2520mixture%2520prior%250Aover%2520object-centric%2520slot%2520representations%252C%2520thereby%2520providing%2520slot%250Aidentifiability%2520guarantees%2520without%2520supervision%252C%2520up%2520to%2520an%2520equivalence%2520relation.%250AWe%2520provide%2520empirical%2520verification%2520of%2520our%2520theoretical%2520identifiability%2520result%250Ausing%2520both%2520simple%25202-dimensional%2520data%2520and%2520high-resolution%2520imaging%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifiable%20Object-Centric%20Representation%20Learning%20via%20Probabilistic%0A%20%20Slot%20Attention&entry.906535625=Avinash%20Kori%20and%20Francesco%20Locatello%20and%20Ainkaran%20Santhirasekaram%20and%20Francesca%20Toni%20and%20Ben%20Glocker%20and%20Fabio%20De%20Sousa%20Ribeiro&entry.1292438233=%20%20Learning%20modular%20object-centric%20representations%20is%20crucial%20for%20systematic%0Ageneralization.%20Existing%20methods%20show%20promising%20object-binding%20capabilities%0Aempirically%2C%20but%20theoretical%20identifiability%20guarantees%20remain%20relatively%0Aunderdeveloped.%20Understanding%20when%20object-centric%20representations%20can%0Atheoretically%20be%20identified%20is%20crucial%20for%20scaling%20slot-based%20methods%20to%0Ahigh-dimensional%20images%20with%20correctness%20guarantees.%20To%20that%20end%2C%20we%20propose%20a%0Aprobabilistic%20slot-attention%20algorithm%20that%20imposes%20an%20aggregate%20mixture%20prior%0Aover%20object-centric%20slot%20representations%2C%20thereby%20providing%20slot%0Aidentifiability%20guarantees%20without%20supervision%2C%20up%20to%20an%20equivalence%20relation.%0AWe%20provide%20empirical%20verification%20of%20our%20theoretical%20identifiability%20result%0Ausing%20both%20simple%202-dimensional%20data%20and%20high-resolution%20imaging%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07141v1&entry.124074799=Read"},
{"title": "Open-World Human-Object Interaction Detection via Multi-modal Prompts", "author": "Jie Yang and Bingliang Li and Ailing Zeng and Lei Zhang and Ruimao Zhang", "abstract": "  In this paper, we develop \\textbf{MP-HOI}, a powerful Multi-modal\nPrompt-based HOI detector designed to leverage both textual descriptions for\nopen-set generalization and visual exemplars for handling high ambiguity in\ndescriptions, realizing HOI detection in the open world. Specifically, it\nintegrates visual prompts into existing language-guided-only HOI detectors to\nhandle situations where textual descriptions face difficulties in\ngeneralization and to address complex scenarios with high interaction\nambiguity. To facilitate MP-HOI training, we build a large-scale HOI dataset\nnamed Magic-HOI, which gathers six existing datasets into a unified label\nspace, forming over 186K images with 2.4K objects, 1.2K actions, and 20K HOI\ninteractions. Furthermore, to tackle the long-tail issue within the Magic-HOI\ndataset, we introduce an automated pipeline for generating realistically\nannotated HOI images and present SynHOI, a high-quality synthetic HOI dataset\ncontaining 100K images. Leveraging these two datasets, MP-HOI optimizes the HOI\ntask as a similarity learning process between multi-modal prompts and\nobjects/interactions via a unified contrastive loss, to learn generalizable and\ntransferable objects/interactions representations from large-scale data. MP-HOI\ncould serve as a generalist HOI detector, surpassing the HOI vocabulary of\nexisting expert models by more than 30 times. Concurrently, our results\ndemonstrate that MP-HOI exhibits remarkable zero-shot capability in real-world\nscenarios and consistently achieves a new state-of-the-art performance across\nvarious benchmarks.\n", "link": "http://arxiv.org/abs/2406.07221v1", "date": "2024-06-11", "relevancy": 2.1564, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5667}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5363}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-World%20Human-Object%20Interaction%20Detection%20via%20Multi-modal%20Prompts&body=Title%3A%20Open-World%20Human-Object%20Interaction%20Detection%20via%20Multi-modal%20Prompts%0AAuthor%3A%20Jie%20Yang%20and%20Bingliang%20Li%20and%20Ailing%20Zeng%20and%20Lei%20Zhang%20and%20Ruimao%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20develop%20%5Ctextbf%7BMP-HOI%7D%2C%20a%20powerful%20Multi-modal%0APrompt-based%20HOI%20detector%20designed%20to%20leverage%20both%20textual%20descriptions%20for%0Aopen-set%20generalization%20and%20visual%20exemplars%20for%20handling%20high%20ambiguity%20in%0Adescriptions%2C%20realizing%20HOI%20detection%20in%20the%20open%20world.%20Specifically%2C%20it%0Aintegrates%20visual%20prompts%20into%20existing%20language-guided-only%20HOI%20detectors%20to%0Ahandle%20situations%20where%20textual%20descriptions%20face%20difficulties%20in%0Ageneralization%20and%20to%20address%20complex%20scenarios%20with%20high%20interaction%0Aambiguity.%20To%20facilitate%20MP-HOI%20training%2C%20we%20build%20a%20large-scale%20HOI%20dataset%0Anamed%20Magic-HOI%2C%20which%20gathers%20six%20existing%20datasets%20into%20a%20unified%20label%0Aspace%2C%20forming%20over%20186K%20images%20with%202.4K%20objects%2C%201.2K%20actions%2C%20and%2020K%20HOI%0Ainteractions.%20Furthermore%2C%20to%20tackle%20the%20long-tail%20issue%20within%20the%20Magic-HOI%0Adataset%2C%20we%20introduce%20an%20automated%20pipeline%20for%20generating%20realistically%0Aannotated%20HOI%20images%20and%20present%20SynHOI%2C%20a%20high-quality%20synthetic%20HOI%20dataset%0Acontaining%20100K%20images.%20Leveraging%20these%20two%20datasets%2C%20MP-HOI%20optimizes%20the%20HOI%0Atask%20as%20a%20similarity%20learning%20process%20between%20multi-modal%20prompts%20and%0Aobjects/interactions%20via%20a%20unified%20contrastive%20loss%2C%20to%20learn%20generalizable%20and%0Atransferable%20objects/interactions%20representations%20from%20large-scale%20data.%20MP-HOI%0Acould%20serve%20as%20a%20generalist%20HOI%20detector%2C%20surpassing%20the%20HOI%20vocabulary%20of%0Aexisting%20expert%20models%20by%20more%20than%2030%20times.%20Concurrently%2C%20our%20results%0Ademonstrate%20that%20MP-HOI%20exhibits%20remarkable%20zero-shot%20capability%20in%20real-world%0Ascenarios%20and%20consistently%20achieves%20a%20new%20state-of-the-art%20performance%20across%0Avarious%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-World%2520Human-Object%2520Interaction%2520Detection%2520via%2520Multi-modal%2520Prompts%26entry.906535625%3DJie%2520Yang%2520and%2520Bingliang%2520Li%2520and%2520Ailing%2520Zeng%2520and%2520Lei%2520Zhang%2520and%2520Ruimao%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520%255Ctextbf%257BMP-HOI%257D%252C%2520a%2520powerful%2520Multi-modal%250APrompt-based%2520HOI%2520detector%2520designed%2520to%2520leverage%2520both%2520textual%2520descriptions%2520for%250Aopen-set%2520generalization%2520and%2520visual%2520exemplars%2520for%2520handling%2520high%2520ambiguity%2520in%250Adescriptions%252C%2520realizing%2520HOI%2520detection%2520in%2520the%2520open%2520world.%2520Specifically%252C%2520it%250Aintegrates%2520visual%2520prompts%2520into%2520existing%2520language-guided-only%2520HOI%2520detectors%2520to%250Ahandle%2520situations%2520where%2520textual%2520descriptions%2520face%2520difficulties%2520in%250Ageneralization%2520and%2520to%2520address%2520complex%2520scenarios%2520with%2520high%2520interaction%250Aambiguity.%2520To%2520facilitate%2520MP-HOI%2520training%252C%2520we%2520build%2520a%2520large-scale%2520HOI%2520dataset%250Anamed%2520Magic-HOI%252C%2520which%2520gathers%2520six%2520existing%2520datasets%2520into%2520a%2520unified%2520label%250Aspace%252C%2520forming%2520over%2520186K%2520images%2520with%25202.4K%2520objects%252C%25201.2K%2520actions%252C%2520and%252020K%2520HOI%250Ainteractions.%2520Furthermore%252C%2520to%2520tackle%2520the%2520long-tail%2520issue%2520within%2520the%2520Magic-HOI%250Adataset%252C%2520we%2520introduce%2520an%2520automated%2520pipeline%2520for%2520generating%2520realistically%250Aannotated%2520HOI%2520images%2520and%2520present%2520SynHOI%252C%2520a%2520high-quality%2520synthetic%2520HOI%2520dataset%250Acontaining%2520100K%2520images.%2520Leveraging%2520these%2520two%2520datasets%252C%2520MP-HOI%2520optimizes%2520the%2520HOI%250Atask%2520as%2520a%2520similarity%2520learning%2520process%2520between%2520multi-modal%2520prompts%2520and%250Aobjects/interactions%2520via%2520a%2520unified%2520contrastive%2520loss%252C%2520to%2520learn%2520generalizable%2520and%250Atransferable%2520objects/interactions%2520representations%2520from%2520large-scale%2520data.%2520MP-HOI%250Acould%2520serve%2520as%2520a%2520generalist%2520HOI%2520detector%252C%2520surpassing%2520the%2520HOI%2520vocabulary%2520of%250Aexisting%2520expert%2520models%2520by%2520more%2520than%252030%2520times.%2520Concurrently%252C%2520our%2520results%250Ademonstrate%2520that%2520MP-HOI%2520exhibits%2520remarkable%2520zero-shot%2520capability%2520in%2520real-world%250Ascenarios%2520and%2520consistently%2520achieves%2520a%2520new%2520state-of-the-art%2520performance%2520across%250Avarious%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-World%20Human-Object%20Interaction%20Detection%20via%20Multi-modal%20Prompts&entry.906535625=Jie%20Yang%20and%20Bingliang%20Li%20and%20Ailing%20Zeng%20and%20Lei%20Zhang%20and%20Ruimao%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20develop%20%5Ctextbf%7BMP-HOI%7D%2C%20a%20powerful%20Multi-modal%0APrompt-based%20HOI%20detector%20designed%20to%20leverage%20both%20textual%20descriptions%20for%0Aopen-set%20generalization%20and%20visual%20exemplars%20for%20handling%20high%20ambiguity%20in%0Adescriptions%2C%20realizing%20HOI%20detection%20in%20the%20open%20world.%20Specifically%2C%20it%0Aintegrates%20visual%20prompts%20into%20existing%20language-guided-only%20HOI%20detectors%20to%0Ahandle%20situations%20where%20textual%20descriptions%20face%20difficulties%20in%0Ageneralization%20and%20to%20address%20complex%20scenarios%20with%20high%20interaction%0Aambiguity.%20To%20facilitate%20MP-HOI%20training%2C%20we%20build%20a%20large-scale%20HOI%20dataset%0Anamed%20Magic-HOI%2C%20which%20gathers%20six%20existing%20datasets%20into%20a%20unified%20label%0Aspace%2C%20forming%20over%20186K%20images%20with%202.4K%20objects%2C%201.2K%20actions%2C%20and%2020K%20HOI%0Ainteractions.%20Furthermore%2C%20to%20tackle%20the%20long-tail%20issue%20within%20the%20Magic-HOI%0Adataset%2C%20we%20introduce%20an%20automated%20pipeline%20for%20generating%20realistically%0Aannotated%20HOI%20images%20and%20present%20SynHOI%2C%20a%20high-quality%20synthetic%20HOI%20dataset%0Acontaining%20100K%20images.%20Leveraging%20these%20two%20datasets%2C%20MP-HOI%20optimizes%20the%20HOI%0Atask%20as%20a%20similarity%20learning%20process%20between%20multi-modal%20prompts%20and%0Aobjects/interactions%20via%20a%20unified%20contrastive%20loss%2C%20to%20learn%20generalizable%20and%0Atransferable%20objects/interactions%20representations%20from%20large-scale%20data.%20MP-HOI%0Acould%20serve%20as%20a%20generalist%20HOI%20detector%2C%20surpassing%20the%20HOI%20vocabulary%20of%0Aexisting%20expert%20models%20by%20more%20than%2030%20times.%20Concurrently%2C%20our%20results%0Ademonstrate%20that%20MP-HOI%20exhibits%20remarkable%20zero-shot%20capability%20in%20real-world%0Ascenarios%20and%20consistently%20achieves%20a%20new%20state-of-the-art%20performance%20across%0Avarious%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07221v1&entry.124074799=Read"},
{"title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio\n  Understanding in Video-LLMs", "author": "Zesen Cheng and Sicong Leng and Hang Zhang and Yifei Xin and Xin Li and Guanzheng Chen and Yongxin Zhu and Wenqi Zhang and Ziyang Luo and Deli Zhao and Lidong Bing", "abstract": "  In this paper, we present the VideoLLaMA 2, a set of Video Large Language\nModels (Video-LLMs) designed to enhance spatial-temporal modeling and audio\nunderstanding in video and audio-oriented tasks. Building upon its predecessor,\nVideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)\nconnector, which effectively captures the intricate spatial and temporal\ndynamics of video data. Additionally, we integrate an Audio Branch into the\nmodel through joint training, thereby enriching the multimodal understanding\ncapabilities of the model by seamlessly incorporating audio cues. Comprehensive\nevaluations on multiple-choice video question answering (MC-VQA), open-ended\nvideo question answering (OE-VQA), and video captioning (VC) tasks demonstrate\nthat VideoLLaMA 2 consistently achieves competitive results among open-source\nmodels and even gets close to some proprietary models on several benchmarks.\nFurthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and\naudio-video question-answering (AQA & OE-AVQA) benchmarks over existing models.\nThese advancements underline VideoLLaMA 2's superior performance in multimodal\ncomprehension, setting a new standard for intelligent video analysis systems.\nAll models are public to facilitate further research.\n", "link": "http://arxiv.org/abs/2406.07476v1", "date": "2024-06-11", "relevancy": 2.1523, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5564}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5541}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoLLaMA%202%3A%20Advancing%20Spatial-Temporal%20Modeling%20and%20Audio%0A%20%20Understanding%20in%20Video-LLMs&body=Title%3A%20VideoLLaMA%202%3A%20Advancing%20Spatial-Temporal%20Modeling%20and%20Audio%0A%20%20Understanding%20in%20Video-LLMs%0AAuthor%3A%20Zesen%20Cheng%20and%20Sicong%20Leng%20and%20Hang%20Zhang%20and%20Yifei%20Xin%20and%20Xin%20Li%20and%20Guanzheng%20Chen%20and%20Yongxin%20Zhu%20and%20Wenqi%20Zhang%20and%20Ziyang%20Luo%20and%20Deli%20Zhao%20and%20Lidong%20Bing%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20the%20VideoLLaMA%202%2C%20a%20set%20of%20Video%20Large%20Language%0AModels%20%28Video-LLMs%29%20designed%20to%20enhance%20spatial-temporal%20modeling%20and%20audio%0Aunderstanding%20in%20video%20and%20audio-oriented%20tasks.%20Building%20upon%20its%20predecessor%2C%0AVideoLLaMA%202%20incorporates%20a%20tailor-made%20Spatial-Temporal%20Convolution%20%28STC%29%0Aconnector%2C%20which%20effectively%20captures%20the%20intricate%20spatial%20and%20temporal%0Adynamics%20of%20video%20data.%20Additionally%2C%20we%20integrate%20an%20Audio%20Branch%20into%20the%0Amodel%20through%20joint%20training%2C%20thereby%20enriching%20the%20multimodal%20understanding%0Acapabilities%20of%20the%20model%20by%20seamlessly%20incorporating%20audio%20cues.%20Comprehensive%0Aevaluations%20on%20multiple-choice%20video%20question%20answering%20%28MC-VQA%29%2C%20open-ended%0Avideo%20question%20answering%20%28OE-VQA%29%2C%20and%20video%20captioning%20%28VC%29%20tasks%20demonstrate%0Athat%20VideoLLaMA%202%20consistently%20achieves%20competitive%20results%20among%20open-source%0Amodels%20and%20even%20gets%20close%20to%20some%20proprietary%20models%20on%20several%20benchmarks.%0AFurthermore%2C%20VideoLLaMA%202%20exhibits%20reasonable%20improvements%20in%20audio-only%20and%0Aaudio-video%20question-answering%20%28AQA%20%26%20OE-AVQA%29%20benchmarks%20over%20existing%20models.%0AThese%20advancements%20underline%20VideoLLaMA%202%27s%20superior%20performance%20in%20multimodal%0Acomprehension%2C%20setting%20a%20new%20standard%20for%20intelligent%20video%20analysis%20systems.%0AAll%20models%20are%20public%20to%20facilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoLLaMA%25202%253A%2520Advancing%2520Spatial-Temporal%2520Modeling%2520and%2520Audio%250A%2520%2520Understanding%2520in%2520Video-LLMs%26entry.906535625%3DZesen%2520Cheng%2520and%2520Sicong%2520Leng%2520and%2520Hang%2520Zhang%2520and%2520Yifei%2520Xin%2520and%2520Xin%2520Li%2520and%2520Guanzheng%2520Chen%2520and%2520Yongxin%2520Zhu%2520and%2520Wenqi%2520Zhang%2520and%2520Ziyang%2520Luo%2520and%2520Deli%2520Zhao%2520and%2520Lidong%2520Bing%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520VideoLLaMA%25202%252C%2520a%2520set%2520of%2520Video%2520Large%2520Language%250AModels%2520%2528Video-LLMs%2529%2520designed%2520to%2520enhance%2520spatial-temporal%2520modeling%2520and%2520audio%250Aunderstanding%2520in%2520video%2520and%2520audio-oriented%2520tasks.%2520Building%2520upon%2520its%2520predecessor%252C%250AVideoLLaMA%25202%2520incorporates%2520a%2520tailor-made%2520Spatial-Temporal%2520Convolution%2520%2528STC%2529%250Aconnector%252C%2520which%2520effectively%2520captures%2520the%2520intricate%2520spatial%2520and%2520temporal%250Adynamics%2520of%2520video%2520data.%2520Additionally%252C%2520we%2520integrate%2520an%2520Audio%2520Branch%2520into%2520the%250Amodel%2520through%2520joint%2520training%252C%2520thereby%2520enriching%2520the%2520multimodal%2520understanding%250Acapabilities%2520of%2520the%2520model%2520by%2520seamlessly%2520incorporating%2520audio%2520cues.%2520Comprehensive%250Aevaluations%2520on%2520multiple-choice%2520video%2520question%2520answering%2520%2528MC-VQA%2529%252C%2520open-ended%250Avideo%2520question%2520answering%2520%2528OE-VQA%2529%252C%2520and%2520video%2520captioning%2520%2528VC%2529%2520tasks%2520demonstrate%250Athat%2520VideoLLaMA%25202%2520consistently%2520achieves%2520competitive%2520results%2520among%2520open-source%250Amodels%2520and%2520even%2520gets%2520close%2520to%2520some%2520proprietary%2520models%2520on%2520several%2520benchmarks.%250AFurthermore%252C%2520VideoLLaMA%25202%2520exhibits%2520reasonable%2520improvements%2520in%2520audio-only%2520and%250Aaudio-video%2520question-answering%2520%2528AQA%2520%2526%2520OE-AVQA%2529%2520benchmarks%2520over%2520existing%2520models.%250AThese%2520advancements%2520underline%2520VideoLLaMA%25202%2527s%2520superior%2520performance%2520in%2520multimodal%250Acomprehension%252C%2520setting%2520a%2520new%2520standard%2520for%2520intelligent%2520video%2520analysis%2520systems.%250AAll%2520models%2520are%2520public%2520to%2520facilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoLLaMA%202%3A%20Advancing%20Spatial-Temporal%20Modeling%20and%20Audio%0A%20%20Understanding%20in%20Video-LLMs&entry.906535625=Zesen%20Cheng%20and%20Sicong%20Leng%20and%20Hang%20Zhang%20and%20Yifei%20Xin%20and%20Xin%20Li%20and%20Guanzheng%20Chen%20and%20Yongxin%20Zhu%20and%20Wenqi%20Zhang%20and%20Ziyang%20Luo%20and%20Deli%20Zhao%20and%20Lidong%20Bing&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20the%20VideoLLaMA%202%2C%20a%20set%20of%20Video%20Large%20Language%0AModels%20%28Video-LLMs%29%20designed%20to%20enhance%20spatial-temporal%20modeling%20and%20audio%0Aunderstanding%20in%20video%20and%20audio-oriented%20tasks.%20Building%20upon%20its%20predecessor%2C%0AVideoLLaMA%202%20incorporates%20a%20tailor-made%20Spatial-Temporal%20Convolution%20%28STC%29%0Aconnector%2C%20which%20effectively%20captures%20the%20intricate%20spatial%20and%20temporal%0Adynamics%20of%20video%20data.%20Additionally%2C%20we%20integrate%20an%20Audio%20Branch%20into%20the%0Amodel%20through%20joint%20training%2C%20thereby%20enriching%20the%20multimodal%20understanding%0Acapabilities%20of%20the%20model%20by%20seamlessly%20incorporating%20audio%20cues.%20Comprehensive%0Aevaluations%20on%20multiple-choice%20video%20question%20answering%20%28MC-VQA%29%2C%20open-ended%0Avideo%20question%20answering%20%28OE-VQA%29%2C%20and%20video%20captioning%20%28VC%29%20tasks%20demonstrate%0Athat%20VideoLLaMA%202%20consistently%20achieves%20competitive%20results%20among%20open-source%0Amodels%20and%20even%20gets%20close%20to%20some%20proprietary%20models%20on%20several%20benchmarks.%0AFurthermore%2C%20VideoLLaMA%202%20exhibits%20reasonable%20improvements%20in%20audio-only%20and%0Aaudio-video%20question-answering%20%28AQA%20%26%20OE-AVQA%29%20benchmarks%20over%20existing%20models.%0AThese%20advancements%20underline%20VideoLLaMA%202%27s%20superior%20performance%20in%20multimodal%0Acomprehension%2C%20setting%20a%20new%20standard%20for%20intelligent%20video%20analysis%20systems.%0AAll%20models%20are%20public%20to%20facilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07476v1&entry.124074799=Read"},
{"title": "Dynamical Mean-Field Theory of Self-Attention Neural Networks", "author": "\u00c1ngel Poc-L\u00f3pez and Miguel Aguilera", "abstract": "  Transformer-based models have demonstrated exceptional performance across\ndiverse domains, becoming the state-of-the-art solution for addressing\nsequential machine learning problems. Even though we have a general\nunderstanding of the fundamental components in the transformer architecture,\nlittle is known about how they operate or what are their expected dynamics.\nRecently, there has been an increasing interest in exploring the relationship\nbetween attention mechanisms and Hopfield networks, promising to shed light on\nthe statistical physics of transformer networks. However, to date, the\ndynamical regimes of transformer-like models have not been studied in depth. In\nthis paper, we address this gap by using methods for the study of asymmetric\nHopfield networks in nonequilibrium regimes --namely path integral methods over\ngenerating functionals, yielding dynamics governed by concurrent mean-field\nvariables. Assuming 1-bit tokens and weights, we derive analytical\napproximations for the behavior of large self-attention neural networks coupled\nto a softmax output, which become exact in the large limit size. Our findings\nreveal nontrivial dynamical phenomena, including nonequilibrium phase\ntransitions associated with chaotic bifurcations, even for very simple\nconfigurations with a few encoded features and a very short context window.\nFinally, we discuss the potential of our analytic approach to improve our\nunderstanding of the inner workings of transformer models, potentially reducing\ncomputational training costs and enhancing model interpretability.\n", "link": "http://arxiv.org/abs/2406.07247v1", "date": "2024-06-11", "relevancy": 2.1516, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5835}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.533}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamical%20Mean-Field%20Theory%20of%20Self-Attention%20Neural%20Networks&body=Title%3A%20Dynamical%20Mean-Field%20Theory%20of%20Self-Attention%20Neural%20Networks%0AAuthor%3A%20%C3%81ngel%20Poc-L%C3%B3pez%20and%20Miguel%20Aguilera%0AAbstract%3A%20%20%20Transformer-based%20models%20have%20demonstrated%20exceptional%20performance%20across%0Adiverse%20domains%2C%20becoming%20the%20state-of-the-art%20solution%20for%20addressing%0Asequential%20machine%20learning%20problems.%20Even%20though%20we%20have%20a%20general%0Aunderstanding%20of%20the%20fundamental%20components%20in%20the%20transformer%20architecture%2C%0Alittle%20is%20known%20about%20how%20they%20operate%20or%20what%20are%20their%20expected%20dynamics.%0ARecently%2C%20there%20has%20been%20an%20increasing%20interest%20in%20exploring%20the%20relationship%0Abetween%20attention%20mechanisms%20and%20Hopfield%20networks%2C%20promising%20to%20shed%20light%20on%0Athe%20statistical%20physics%20of%20transformer%20networks.%20However%2C%20to%20date%2C%20the%0Adynamical%20regimes%20of%20transformer-like%20models%20have%20not%20been%20studied%20in%20depth.%20In%0Athis%20paper%2C%20we%20address%20this%20gap%20by%20using%20methods%20for%20the%20study%20of%20asymmetric%0AHopfield%20networks%20in%20nonequilibrium%20regimes%20--namely%20path%20integral%20methods%20over%0Agenerating%20functionals%2C%20yielding%20dynamics%20governed%20by%20concurrent%20mean-field%0Avariables.%20Assuming%201-bit%20tokens%20and%20weights%2C%20we%20derive%20analytical%0Aapproximations%20for%20the%20behavior%20of%20large%20self-attention%20neural%20networks%20coupled%0Ato%20a%20softmax%20output%2C%20which%20become%20exact%20in%20the%20large%20limit%20size.%20Our%20findings%0Areveal%20nontrivial%20dynamical%20phenomena%2C%20including%20nonequilibrium%20phase%0Atransitions%20associated%20with%20chaotic%20bifurcations%2C%20even%20for%20very%20simple%0Aconfigurations%20with%20a%20few%20encoded%20features%20and%20a%20very%20short%20context%20window.%0AFinally%2C%20we%20discuss%20the%20potential%20of%20our%20analytic%20approach%20to%20improve%20our%0Aunderstanding%20of%20the%20inner%20workings%20of%20transformer%20models%2C%20potentially%20reducing%0Acomputational%20training%20costs%20and%20enhancing%20model%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07247v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamical%2520Mean-Field%2520Theory%2520of%2520Self-Attention%2520Neural%2520Networks%26entry.906535625%3D%25C3%2581ngel%2520Poc-L%25C3%25B3pez%2520and%2520Miguel%2520Aguilera%26entry.1292438233%3D%2520%2520Transformer-based%2520models%2520have%2520demonstrated%2520exceptional%2520performance%2520across%250Adiverse%2520domains%252C%2520becoming%2520the%2520state-of-the-art%2520solution%2520for%2520addressing%250Asequential%2520machine%2520learning%2520problems.%2520Even%2520though%2520we%2520have%2520a%2520general%250Aunderstanding%2520of%2520the%2520fundamental%2520components%2520in%2520the%2520transformer%2520architecture%252C%250Alittle%2520is%2520known%2520about%2520how%2520they%2520operate%2520or%2520what%2520are%2520their%2520expected%2520dynamics.%250ARecently%252C%2520there%2520has%2520been%2520an%2520increasing%2520interest%2520in%2520exploring%2520the%2520relationship%250Abetween%2520attention%2520mechanisms%2520and%2520Hopfield%2520networks%252C%2520promising%2520to%2520shed%2520light%2520on%250Athe%2520statistical%2520physics%2520of%2520transformer%2520networks.%2520However%252C%2520to%2520date%252C%2520the%250Adynamical%2520regimes%2520of%2520transformer-like%2520models%2520have%2520not%2520been%2520studied%2520in%2520depth.%2520In%250Athis%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520using%2520methods%2520for%2520the%2520study%2520of%2520asymmetric%250AHopfield%2520networks%2520in%2520nonequilibrium%2520regimes%2520--namely%2520path%2520integral%2520methods%2520over%250Agenerating%2520functionals%252C%2520yielding%2520dynamics%2520governed%2520by%2520concurrent%2520mean-field%250Avariables.%2520Assuming%25201-bit%2520tokens%2520and%2520weights%252C%2520we%2520derive%2520analytical%250Aapproximations%2520for%2520the%2520behavior%2520of%2520large%2520self-attention%2520neural%2520networks%2520coupled%250Ato%2520a%2520softmax%2520output%252C%2520which%2520become%2520exact%2520in%2520the%2520large%2520limit%2520size.%2520Our%2520findings%250Areveal%2520nontrivial%2520dynamical%2520phenomena%252C%2520including%2520nonequilibrium%2520phase%250Atransitions%2520associated%2520with%2520chaotic%2520bifurcations%252C%2520even%2520for%2520very%2520simple%250Aconfigurations%2520with%2520a%2520few%2520encoded%2520features%2520and%2520a%2520very%2520short%2520context%2520window.%250AFinally%252C%2520we%2520discuss%2520the%2520potential%2520of%2520our%2520analytic%2520approach%2520to%2520improve%2520our%250Aunderstanding%2520of%2520the%2520inner%2520workings%2520of%2520transformer%2520models%252C%2520potentially%2520reducing%250Acomputational%2520training%2520costs%2520and%2520enhancing%2520model%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07247v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamical%20Mean-Field%20Theory%20of%20Self-Attention%20Neural%20Networks&entry.906535625=%C3%81ngel%20Poc-L%C3%B3pez%20and%20Miguel%20Aguilera&entry.1292438233=%20%20Transformer-based%20models%20have%20demonstrated%20exceptional%20performance%20across%0Adiverse%20domains%2C%20becoming%20the%20state-of-the-art%20solution%20for%20addressing%0Asequential%20machine%20learning%20problems.%20Even%20though%20we%20have%20a%20general%0Aunderstanding%20of%20the%20fundamental%20components%20in%20the%20transformer%20architecture%2C%0Alittle%20is%20known%20about%20how%20they%20operate%20or%20what%20are%20their%20expected%20dynamics.%0ARecently%2C%20there%20has%20been%20an%20increasing%20interest%20in%20exploring%20the%20relationship%0Abetween%20attention%20mechanisms%20and%20Hopfield%20networks%2C%20promising%20to%20shed%20light%20on%0Athe%20statistical%20physics%20of%20transformer%20networks.%20However%2C%20to%20date%2C%20the%0Adynamical%20regimes%20of%20transformer-like%20models%20have%20not%20been%20studied%20in%20depth.%20In%0Athis%20paper%2C%20we%20address%20this%20gap%20by%20using%20methods%20for%20the%20study%20of%20asymmetric%0AHopfield%20networks%20in%20nonequilibrium%20regimes%20--namely%20path%20integral%20methods%20over%0Agenerating%20functionals%2C%20yielding%20dynamics%20governed%20by%20concurrent%20mean-field%0Avariables.%20Assuming%201-bit%20tokens%20and%20weights%2C%20we%20derive%20analytical%0Aapproximations%20for%20the%20behavior%20of%20large%20self-attention%20neural%20networks%20coupled%0Ato%20a%20softmax%20output%2C%20which%20become%20exact%20in%20the%20large%20limit%20size.%20Our%20findings%0Areveal%20nontrivial%20dynamical%20phenomena%2C%20including%20nonequilibrium%20phase%0Atransitions%20associated%20with%20chaotic%20bifurcations%2C%20even%20for%20very%20simple%0Aconfigurations%20with%20a%20few%20encoded%20features%20and%20a%20very%20short%20context%20window.%0AFinally%2C%20we%20discuss%20the%20potential%20of%20our%20analytic%20approach%20to%20improve%20our%0Aunderstanding%20of%20the%20inner%20workings%20of%20transformer%20models%2C%20potentially%20reducing%0Acomputational%20training%20costs%20and%20enhancing%20model%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07247v1&entry.124074799=Read"},
{"title": "Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision", "author": "Zhaoqing Wang and Xiaobo Xia and Ziye Chen and Xiao He and Yandong Guo and Mingming Gong and Tongliang Liu", "abstract": "  Current state-of-the-art open-vocabulary segmentation methods typically rely\non image-mask-text triplet annotations for supervision. However, acquiring such\ndetailed annotations is labour-intensive and poses scalability challenges in\ncomplex real-world scenarios. While existing weakly-supervised approaches\nleverage image-text pairs to reduce the expansive annotation cost, the lack of\nmask supervision makes it difficult for the model to locate multiple instances\nand accurately group pixels with similar semantics, significantly hampering\nversatility and performance. In this paper, we introduce Unpair-Seg, a novel\nweakly-supervised open-vocabulary segmentation framework that learns from\nunpaired image-mask and image-text pairs, which can be independently and\nefficiently collected. Unpair-Seg initially predicts a set of binary masks and\ngenerates pseudo labels by identifying confident pairs of masks and text\nentities. We then train a feature adapter to align region embeddings with text\nembeddings based on these pseudo labels, achieving open-vocabulary\nsegmentation. However, the inherent noise in the mask-entity correspondence\nposes a challenge to obtaining reliable pairs. To address this, we employ a\nvision-language large model to re-caption the input images and extract precise\nentities, and we design a multi-scale matching strategy to reduce noisy\nmask-entity pairs. Our Unpair-Seg framework demonstrates impressive\nperformance, achieving 14.6\\% and 19.5\\% mIoU on the ADE-847 and PASCAL\nContext-459 datasets, significantly narrowing the gap between fully-supervised\nand weakly-supervised methods.\n", "link": "http://arxiv.org/abs/2402.08960v2", "date": "2024-06-11", "relevancy": 2.1417, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5356}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Vocabulary%20Segmentation%20with%20Unpaired%20Mask-Text%20Supervision&body=Title%3A%20Open-Vocabulary%20Segmentation%20with%20Unpaired%20Mask-Text%20Supervision%0AAuthor%3A%20Zhaoqing%20Wang%20and%20Xiaobo%20Xia%20and%20Ziye%20Chen%20and%20Xiao%20He%20and%20Yandong%20Guo%20and%20Mingming%20Gong%20and%20Tongliang%20Liu%0AAbstract%3A%20%20%20Current%20state-of-the-art%20open-vocabulary%20segmentation%20methods%20typically%20rely%0Aon%20image-mask-text%20triplet%20annotations%20for%20supervision.%20However%2C%20acquiring%20such%0Adetailed%20annotations%20is%20labour-intensive%20and%20poses%20scalability%20challenges%20in%0Acomplex%20real-world%20scenarios.%20While%20existing%20weakly-supervised%20approaches%0Aleverage%20image-text%20pairs%20to%20reduce%20the%20expansive%20annotation%20cost%2C%20the%20lack%20of%0Amask%20supervision%20makes%20it%20difficult%20for%20the%20model%20to%20locate%20multiple%20instances%0Aand%20accurately%20group%20pixels%20with%20similar%20semantics%2C%20significantly%20hampering%0Aversatility%20and%20performance.%20In%20this%20paper%2C%20we%20introduce%20Unpair-Seg%2C%20a%20novel%0Aweakly-supervised%20open-vocabulary%20segmentation%20framework%20that%20learns%20from%0Aunpaired%20image-mask%20and%20image-text%20pairs%2C%20which%20can%20be%20independently%20and%0Aefficiently%20collected.%20Unpair-Seg%20initially%20predicts%20a%20set%20of%20binary%20masks%20and%0Agenerates%20pseudo%20labels%20by%20identifying%20confident%20pairs%20of%20masks%20and%20text%0Aentities.%20We%20then%20train%20a%20feature%20adapter%20to%20align%20region%20embeddings%20with%20text%0Aembeddings%20based%20on%20these%20pseudo%20labels%2C%20achieving%20open-vocabulary%0Asegmentation.%20However%2C%20the%20inherent%20noise%20in%20the%20mask-entity%20correspondence%0Aposes%20a%20challenge%20to%20obtaining%20reliable%20pairs.%20To%20address%20this%2C%20we%20employ%20a%0Avision-language%20large%20model%20to%20re-caption%20the%20input%20images%20and%20extract%20precise%0Aentities%2C%20and%20we%20design%20a%20multi-scale%20matching%20strategy%20to%20reduce%20noisy%0Amask-entity%20pairs.%20Our%20Unpair-Seg%20framework%20demonstrates%20impressive%0Aperformance%2C%20achieving%2014.6%5C%25%20and%2019.5%5C%25%20mIoU%20on%20the%20ADE-847%20and%20PASCAL%0AContext-459%20datasets%2C%20significantly%20narrowing%20the%20gap%20between%20fully-supervised%0Aand%20weakly-supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Vocabulary%2520Segmentation%2520with%2520Unpaired%2520Mask-Text%2520Supervision%26entry.906535625%3DZhaoqing%2520Wang%2520and%2520Xiaobo%2520Xia%2520and%2520Ziye%2520Chen%2520and%2520Xiao%2520He%2520and%2520Yandong%2520Guo%2520and%2520Mingming%2520Gong%2520and%2520Tongliang%2520Liu%26entry.1292438233%3D%2520%2520Current%2520state-of-the-art%2520open-vocabulary%2520segmentation%2520methods%2520typically%2520rely%250Aon%2520image-mask-text%2520triplet%2520annotations%2520for%2520supervision.%2520However%252C%2520acquiring%2520such%250Adetailed%2520annotations%2520is%2520labour-intensive%2520and%2520poses%2520scalability%2520challenges%2520in%250Acomplex%2520real-world%2520scenarios.%2520While%2520existing%2520weakly-supervised%2520approaches%250Aleverage%2520image-text%2520pairs%2520to%2520reduce%2520the%2520expansive%2520annotation%2520cost%252C%2520the%2520lack%2520of%250Amask%2520supervision%2520makes%2520it%2520difficult%2520for%2520the%2520model%2520to%2520locate%2520multiple%2520instances%250Aand%2520accurately%2520group%2520pixels%2520with%2520similar%2520semantics%252C%2520significantly%2520hampering%250Aversatility%2520and%2520performance.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Unpair-Seg%252C%2520a%2520novel%250Aweakly-supervised%2520open-vocabulary%2520segmentation%2520framework%2520that%2520learns%2520from%250Aunpaired%2520image-mask%2520and%2520image-text%2520pairs%252C%2520which%2520can%2520be%2520independently%2520and%250Aefficiently%2520collected.%2520Unpair-Seg%2520initially%2520predicts%2520a%2520set%2520of%2520binary%2520masks%2520and%250Agenerates%2520pseudo%2520labels%2520by%2520identifying%2520confident%2520pairs%2520of%2520masks%2520and%2520text%250Aentities.%2520We%2520then%2520train%2520a%2520feature%2520adapter%2520to%2520align%2520region%2520embeddings%2520with%2520text%250Aembeddings%2520based%2520on%2520these%2520pseudo%2520labels%252C%2520achieving%2520open-vocabulary%250Asegmentation.%2520However%252C%2520the%2520inherent%2520noise%2520in%2520the%2520mask-entity%2520correspondence%250Aposes%2520a%2520challenge%2520to%2520obtaining%2520reliable%2520pairs.%2520To%2520address%2520this%252C%2520we%2520employ%2520a%250Avision-language%2520large%2520model%2520to%2520re-caption%2520the%2520input%2520images%2520and%2520extract%2520precise%250Aentities%252C%2520and%2520we%2520design%2520a%2520multi-scale%2520matching%2520strategy%2520to%2520reduce%2520noisy%250Amask-entity%2520pairs.%2520Our%2520Unpair-Seg%2520framework%2520demonstrates%2520impressive%250Aperformance%252C%2520achieving%252014.6%255C%2525%2520and%252019.5%255C%2525%2520mIoU%2520on%2520the%2520ADE-847%2520and%2520PASCAL%250AContext-459%2520datasets%252C%2520significantly%2520narrowing%2520the%2520gap%2520between%2520fully-supervised%250Aand%2520weakly-supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Vocabulary%20Segmentation%20with%20Unpaired%20Mask-Text%20Supervision&entry.906535625=Zhaoqing%20Wang%20and%20Xiaobo%20Xia%20and%20Ziye%20Chen%20and%20Xiao%20He%20and%20Yandong%20Guo%20and%20Mingming%20Gong%20and%20Tongliang%20Liu&entry.1292438233=%20%20Current%20state-of-the-art%20open-vocabulary%20segmentation%20methods%20typically%20rely%0Aon%20image-mask-text%20triplet%20annotations%20for%20supervision.%20However%2C%20acquiring%20such%0Adetailed%20annotations%20is%20labour-intensive%20and%20poses%20scalability%20challenges%20in%0Acomplex%20real-world%20scenarios.%20While%20existing%20weakly-supervised%20approaches%0Aleverage%20image-text%20pairs%20to%20reduce%20the%20expansive%20annotation%20cost%2C%20the%20lack%20of%0Amask%20supervision%20makes%20it%20difficult%20for%20the%20model%20to%20locate%20multiple%20instances%0Aand%20accurately%20group%20pixels%20with%20similar%20semantics%2C%20significantly%20hampering%0Aversatility%20and%20performance.%20In%20this%20paper%2C%20we%20introduce%20Unpair-Seg%2C%20a%20novel%0Aweakly-supervised%20open-vocabulary%20segmentation%20framework%20that%20learns%20from%0Aunpaired%20image-mask%20and%20image-text%20pairs%2C%20which%20can%20be%20independently%20and%0Aefficiently%20collected.%20Unpair-Seg%20initially%20predicts%20a%20set%20of%20binary%20masks%20and%0Agenerates%20pseudo%20labels%20by%20identifying%20confident%20pairs%20of%20masks%20and%20text%0Aentities.%20We%20then%20train%20a%20feature%20adapter%20to%20align%20region%20embeddings%20with%20text%0Aembeddings%20based%20on%20these%20pseudo%20labels%2C%20achieving%20open-vocabulary%0Asegmentation.%20However%2C%20the%20inherent%20noise%20in%20the%20mask-entity%20correspondence%0Aposes%20a%20challenge%20to%20obtaining%20reliable%20pairs.%20To%20address%20this%2C%20we%20employ%20a%0Avision-language%20large%20model%20to%20re-caption%20the%20input%20images%20and%20extract%20precise%0Aentities%2C%20and%20we%20design%20a%20multi-scale%20matching%20strategy%20to%20reduce%20noisy%0Amask-entity%20pairs.%20Our%20Unpair-Seg%20framework%20demonstrates%20impressive%0Aperformance%2C%20achieving%2014.6%5C%25%20and%2019.5%5C%25%20mIoU%20on%20the%20ADE-847%20and%20PASCAL%0AContext-459%20datasets%2C%20significantly%20narrowing%20the%20gap%20between%20fully-supervised%0Aand%20weakly-supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08960v2&entry.124074799=Read"},
{"title": "TextGrad: Automatic \"Differentiation\" via Text", "author": "Mert Yuksekgonul and Federico Bianchi and Joseph Boen and Sheng Liu and Zhi Huang and Carlos Guestrin and James Zou", "abstract": "  AI is undergoing a paradigm shift, with breakthroughs achieved by systems\norchestrating multiple large language models (LLMs) and other complex\ncomponents. As a result, developing principled and automated optimization\nmethods for compound AI systems is one of the most important new challenges.\nNeural networks faced a similar challenge in its early days until\nbackpropagation and automatic differentiation transformed the field by making\noptimization turn-key. Inspired by this, we introduce TextGrad, a powerful\nframework performing automatic ``differentiation'' via text. TextGrad\nbackpropagates textual feedback provided by LLMs to improve individual\ncomponents of a compound AI system. In our framework, LLMs provide rich,\ngeneral, natural language suggestions to optimize variables in computation\ngraphs, ranging from code snippets to molecular structures. TextGrad follows\nPyTorch's syntax and abstraction and is flexible and easy-to-use. It works\nout-of-the-box for a variety of tasks, where the users only provide the\nobjective function without tuning components or prompts of the framework. We\nshowcase TextGrad's effectiveness and generality across a diverse range of\napplications, from question answering and molecule optimization to radiotherapy\ntreatment planning. Without modifying the framework, TextGrad improves the\nzero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\\%$ to\n$55\\%$, yields $20\\%$ relative performance gain in optimizing LeetCode-Hard\ncoding problem solutions, improves prompts for reasoning, designs new druglike\nsmall molecules with desirable in silico binding, and designs radiation\noncology treatment plans with high specificity. TextGrad lays a foundation to\naccelerate the development of the next-generation of AI systems.\n", "link": "http://arxiv.org/abs/2406.07496v1", "date": "2024-06-11", "relevancy": 2.1329, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.547}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5261}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextGrad%3A%20Automatic%20%22Differentiation%22%20via%20Text&body=Title%3A%20TextGrad%3A%20Automatic%20%22Differentiation%22%20via%20Text%0AAuthor%3A%20Mert%20Yuksekgonul%20and%20Federico%20Bianchi%20and%20Joseph%20Boen%20and%20Sheng%20Liu%20and%20Zhi%20Huang%20and%20Carlos%20Guestrin%20and%20James%20Zou%0AAbstract%3A%20%20%20AI%20is%20undergoing%20a%20paradigm%20shift%2C%20with%20breakthroughs%20achieved%20by%20systems%0Aorchestrating%20multiple%20large%20language%20models%20%28LLMs%29%20and%20other%20complex%0Acomponents.%20As%20a%20result%2C%20developing%20principled%20and%20automated%20optimization%0Amethods%20for%20compound%20AI%20systems%20is%20one%20of%20the%20most%20important%20new%20challenges.%0ANeural%20networks%20faced%20a%20similar%20challenge%20in%20its%20early%20days%20until%0Abackpropagation%20and%20automatic%20differentiation%20transformed%20the%20field%20by%20making%0Aoptimization%20turn-key.%20Inspired%20by%20this%2C%20we%20introduce%20TextGrad%2C%20a%20powerful%0Aframework%20performing%20automatic%20%60%60differentiation%27%27%20via%20text.%20TextGrad%0Abackpropagates%20textual%20feedback%20provided%20by%20LLMs%20to%20improve%20individual%0Acomponents%20of%20a%20compound%20AI%20system.%20In%20our%20framework%2C%20LLMs%20provide%20rich%2C%0Ageneral%2C%20natural%20language%20suggestions%20to%20optimize%20variables%20in%20computation%0Agraphs%2C%20ranging%20from%20code%20snippets%20to%20molecular%20structures.%20TextGrad%20follows%0APyTorch%27s%20syntax%20and%20abstraction%20and%20is%20flexible%20and%20easy-to-use.%20It%20works%0Aout-of-the-box%20for%20a%20variety%20of%20tasks%2C%20where%20the%20users%20only%20provide%20the%0Aobjective%20function%20without%20tuning%20components%20or%20prompts%20of%20the%20framework.%20We%0Ashowcase%20TextGrad%27s%20effectiveness%20and%20generality%20across%20a%20diverse%20range%20of%0Aapplications%2C%20from%20question%20answering%20and%20molecule%20optimization%20to%20radiotherapy%0Atreatment%20planning.%20Without%20modifying%20the%20framework%2C%20TextGrad%20improves%20the%0Azero-shot%20accuracy%20of%20GPT-4o%20in%20Google-Proof%20Question%20Answering%20from%20%2451%5C%25%24%20to%0A%2455%5C%25%24%2C%20yields%20%2420%5C%25%24%20relative%20performance%20gain%20in%20optimizing%20LeetCode-Hard%0Acoding%20problem%20solutions%2C%20improves%20prompts%20for%20reasoning%2C%20designs%20new%20druglike%0Asmall%20molecules%20with%20desirable%20in%20silico%20binding%2C%20and%20designs%20radiation%0Aoncology%20treatment%20plans%20with%20high%20specificity.%20TextGrad%20lays%20a%20foundation%20to%0Aaccelerate%20the%20development%20of%20the%20next-generation%20of%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextGrad%253A%2520Automatic%2520%2522Differentiation%2522%2520via%2520Text%26entry.906535625%3DMert%2520Yuksekgonul%2520and%2520Federico%2520Bianchi%2520and%2520Joseph%2520Boen%2520and%2520Sheng%2520Liu%2520and%2520Zhi%2520Huang%2520and%2520Carlos%2520Guestrin%2520and%2520James%2520Zou%26entry.1292438233%3D%2520%2520AI%2520is%2520undergoing%2520a%2520paradigm%2520shift%252C%2520with%2520breakthroughs%2520achieved%2520by%2520systems%250Aorchestrating%2520multiple%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520other%2520complex%250Acomponents.%2520As%2520a%2520result%252C%2520developing%2520principled%2520and%2520automated%2520optimization%250Amethods%2520for%2520compound%2520AI%2520systems%2520is%2520one%2520of%2520the%2520most%2520important%2520new%2520challenges.%250ANeural%2520networks%2520faced%2520a%2520similar%2520challenge%2520in%2520its%2520early%2520days%2520until%250Abackpropagation%2520and%2520automatic%2520differentiation%2520transformed%2520the%2520field%2520by%2520making%250Aoptimization%2520turn-key.%2520Inspired%2520by%2520this%252C%2520we%2520introduce%2520TextGrad%252C%2520a%2520powerful%250Aframework%2520performing%2520automatic%2520%2560%2560differentiation%2527%2527%2520via%2520text.%2520TextGrad%250Abackpropagates%2520textual%2520feedback%2520provided%2520by%2520LLMs%2520to%2520improve%2520individual%250Acomponents%2520of%2520a%2520compound%2520AI%2520system.%2520In%2520our%2520framework%252C%2520LLMs%2520provide%2520rich%252C%250Ageneral%252C%2520natural%2520language%2520suggestions%2520to%2520optimize%2520variables%2520in%2520computation%250Agraphs%252C%2520ranging%2520from%2520code%2520snippets%2520to%2520molecular%2520structures.%2520TextGrad%2520follows%250APyTorch%2527s%2520syntax%2520and%2520abstraction%2520and%2520is%2520flexible%2520and%2520easy-to-use.%2520It%2520works%250Aout-of-the-box%2520for%2520a%2520variety%2520of%2520tasks%252C%2520where%2520the%2520users%2520only%2520provide%2520the%250Aobjective%2520function%2520without%2520tuning%2520components%2520or%2520prompts%2520of%2520the%2520framework.%2520We%250Ashowcase%2520TextGrad%2527s%2520effectiveness%2520and%2520generality%2520across%2520a%2520diverse%2520range%2520of%250Aapplications%252C%2520from%2520question%2520answering%2520and%2520molecule%2520optimization%2520to%2520radiotherapy%250Atreatment%2520planning.%2520Without%2520modifying%2520the%2520framework%252C%2520TextGrad%2520improves%2520the%250Azero-shot%2520accuracy%2520of%2520GPT-4o%2520in%2520Google-Proof%2520Question%2520Answering%2520from%2520%252451%255C%2525%2524%2520to%250A%252455%255C%2525%2524%252C%2520yields%2520%252420%255C%2525%2524%2520relative%2520performance%2520gain%2520in%2520optimizing%2520LeetCode-Hard%250Acoding%2520problem%2520solutions%252C%2520improves%2520prompts%2520for%2520reasoning%252C%2520designs%2520new%2520druglike%250Asmall%2520molecules%2520with%2520desirable%2520in%2520silico%2520binding%252C%2520and%2520designs%2520radiation%250Aoncology%2520treatment%2520plans%2520with%2520high%2520specificity.%2520TextGrad%2520lays%2520a%2520foundation%2520to%250Aaccelerate%2520the%2520development%2520of%2520the%2520next-generation%2520of%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextGrad%3A%20Automatic%20%22Differentiation%22%20via%20Text&entry.906535625=Mert%20Yuksekgonul%20and%20Federico%20Bianchi%20and%20Joseph%20Boen%20and%20Sheng%20Liu%20and%20Zhi%20Huang%20and%20Carlos%20Guestrin%20and%20James%20Zou&entry.1292438233=%20%20AI%20is%20undergoing%20a%20paradigm%20shift%2C%20with%20breakthroughs%20achieved%20by%20systems%0Aorchestrating%20multiple%20large%20language%20models%20%28LLMs%29%20and%20other%20complex%0Acomponents.%20As%20a%20result%2C%20developing%20principled%20and%20automated%20optimization%0Amethods%20for%20compound%20AI%20systems%20is%20one%20of%20the%20most%20important%20new%20challenges.%0ANeural%20networks%20faced%20a%20similar%20challenge%20in%20its%20early%20days%20until%0Abackpropagation%20and%20automatic%20differentiation%20transformed%20the%20field%20by%20making%0Aoptimization%20turn-key.%20Inspired%20by%20this%2C%20we%20introduce%20TextGrad%2C%20a%20powerful%0Aframework%20performing%20automatic%20%60%60differentiation%27%27%20via%20text.%20TextGrad%0Abackpropagates%20textual%20feedback%20provided%20by%20LLMs%20to%20improve%20individual%0Acomponents%20of%20a%20compound%20AI%20system.%20In%20our%20framework%2C%20LLMs%20provide%20rich%2C%0Ageneral%2C%20natural%20language%20suggestions%20to%20optimize%20variables%20in%20computation%0Agraphs%2C%20ranging%20from%20code%20snippets%20to%20molecular%20structures.%20TextGrad%20follows%0APyTorch%27s%20syntax%20and%20abstraction%20and%20is%20flexible%20and%20easy-to-use.%20It%20works%0Aout-of-the-box%20for%20a%20variety%20of%20tasks%2C%20where%20the%20users%20only%20provide%20the%0Aobjective%20function%20without%20tuning%20components%20or%20prompts%20of%20the%20framework.%20We%0Ashowcase%20TextGrad%27s%20effectiveness%20and%20generality%20across%20a%20diverse%20range%20of%0Aapplications%2C%20from%20question%20answering%20and%20molecule%20optimization%20to%20radiotherapy%0Atreatment%20planning.%20Without%20modifying%20the%20framework%2C%20TextGrad%20improves%20the%0Azero-shot%20accuracy%20of%20GPT-4o%20in%20Google-Proof%20Question%20Answering%20from%20%2451%5C%25%24%20to%0A%2455%5C%25%24%2C%20yields%20%2420%5C%25%24%20relative%20performance%20gain%20in%20optimizing%20LeetCode-Hard%0Acoding%20problem%20solutions%2C%20improves%20prompts%20for%20reasoning%2C%20designs%20new%20druglike%0Asmall%20molecules%20with%20desirable%20in%20silico%20binding%2C%20and%20designs%20radiation%0Aoncology%20treatment%20plans%20with%20high%20specificity.%20TextGrad%20lays%20a%20foundation%20to%0Aaccelerate%20the%20development%20of%20the%20next-generation%20of%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07496v1&entry.124074799=Read"},
{"title": "Spatio-temporal Early Prediction based on Multi-objective Reinforcement\n  Learning", "author": "Wei Shao and Yufan Kang and Ziyan Peng and Xiao Xiao and Lei Wang and Yuhui Yang and Flora D Salim", "abstract": "  Accuracy and timeliness are indeed often conflicting goals in prediction\ntasks. Premature predictions may yield a higher rate of false alarms, whereas\ndelaying predictions to gather more information can render them too late to be\nuseful. In applications such as wildfires, crimes, and traffic jams, timely\npredictions are vital for safeguarding human life and property. Consequently,\nfinding a balance between accuracy and timeliness is crucial. In this paper, we\npropose a spatio-temporal early prediction model based on Multi-Objective\nreinforcement learning that can either implement an optimal policy given a\npreference or infer the preference based on a small number of samples. The\nmodel addresses two primary challenges: 1) enhancing the accuracy of early\npredictions and 2) providing the optimal policy for determining the most\nsuitable prediction time for each area. Our method demonstrates superior\nperformance on three large-scale real-world datasets, surpassing existing\nmethods in early spatio-temporal prediction tasks.\n", "link": "http://arxiv.org/abs/2406.04035v2", "date": "2024-06-11", "relevancy": 2.1309, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5762}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.547}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-temporal%20Early%20Prediction%20based%20on%20Multi-objective%20Reinforcement%0A%20%20Learning&body=Title%3A%20Spatio-temporal%20Early%20Prediction%20based%20on%20Multi-objective%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Wei%20Shao%20and%20Yufan%20Kang%20and%20Ziyan%20Peng%20and%20Xiao%20Xiao%20and%20Lei%20Wang%20and%20Yuhui%20Yang%20and%20Flora%20D%20Salim%0AAbstract%3A%20%20%20Accuracy%20and%20timeliness%20are%20indeed%20often%20conflicting%20goals%20in%20prediction%0Atasks.%20Premature%20predictions%20may%20yield%20a%20higher%20rate%20of%20false%20alarms%2C%20whereas%0Adelaying%20predictions%20to%20gather%20more%20information%20can%20render%20them%20too%20late%20to%20be%0Auseful.%20In%20applications%20such%20as%20wildfires%2C%20crimes%2C%20and%20traffic%20jams%2C%20timely%0Apredictions%20are%20vital%20for%20safeguarding%20human%20life%20and%20property.%20Consequently%2C%0Afinding%20a%20balance%20between%20accuracy%20and%20timeliness%20is%20crucial.%20In%20this%20paper%2C%20we%0Apropose%20a%20spatio-temporal%20early%20prediction%20model%20based%20on%20Multi-Objective%0Areinforcement%20learning%20that%20can%20either%20implement%20an%20optimal%20policy%20given%20a%0Apreference%20or%20infer%20the%20preference%20based%20on%20a%20small%20number%20of%20samples.%20The%0Amodel%20addresses%20two%20primary%20challenges%3A%201%29%20enhancing%20the%20accuracy%20of%20early%0Apredictions%20and%202%29%20providing%20the%20optimal%20policy%20for%20determining%20the%20most%0Asuitable%20prediction%20time%20for%20each%20area.%20Our%20method%20demonstrates%20superior%0Aperformance%20on%20three%20large-scale%20real-world%20datasets%2C%20surpassing%20existing%0Amethods%20in%20early%20spatio-temporal%20prediction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-temporal%2520Early%2520Prediction%2520based%2520on%2520Multi-objective%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DWei%2520Shao%2520and%2520Yufan%2520Kang%2520and%2520Ziyan%2520Peng%2520and%2520Xiao%2520Xiao%2520and%2520Lei%2520Wang%2520and%2520Yuhui%2520Yang%2520and%2520Flora%2520D%2520Salim%26entry.1292438233%3D%2520%2520Accuracy%2520and%2520timeliness%2520are%2520indeed%2520often%2520conflicting%2520goals%2520in%2520prediction%250Atasks.%2520Premature%2520predictions%2520may%2520yield%2520a%2520higher%2520rate%2520of%2520false%2520alarms%252C%2520whereas%250Adelaying%2520predictions%2520to%2520gather%2520more%2520information%2520can%2520render%2520them%2520too%2520late%2520to%2520be%250Auseful.%2520In%2520applications%2520such%2520as%2520wildfires%252C%2520crimes%252C%2520and%2520traffic%2520jams%252C%2520timely%250Apredictions%2520are%2520vital%2520for%2520safeguarding%2520human%2520life%2520and%2520property.%2520Consequently%252C%250Afinding%2520a%2520balance%2520between%2520accuracy%2520and%2520timeliness%2520is%2520crucial.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520spatio-temporal%2520early%2520prediction%2520model%2520based%2520on%2520Multi-Objective%250Areinforcement%2520learning%2520that%2520can%2520either%2520implement%2520an%2520optimal%2520policy%2520given%2520a%250Apreference%2520or%2520infer%2520the%2520preference%2520based%2520on%2520a%2520small%2520number%2520of%2520samples.%2520The%250Amodel%2520addresses%2520two%2520primary%2520challenges%253A%25201%2529%2520enhancing%2520the%2520accuracy%2520of%2520early%250Apredictions%2520and%25202%2529%2520providing%2520the%2520optimal%2520policy%2520for%2520determining%2520the%2520most%250Asuitable%2520prediction%2520time%2520for%2520each%2520area.%2520Our%2520method%2520demonstrates%2520superior%250Aperformance%2520on%2520three%2520large-scale%2520real-world%2520datasets%252C%2520surpassing%2520existing%250Amethods%2520in%2520early%2520spatio-temporal%2520prediction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-temporal%20Early%20Prediction%20based%20on%20Multi-objective%20Reinforcement%0A%20%20Learning&entry.906535625=Wei%20Shao%20and%20Yufan%20Kang%20and%20Ziyan%20Peng%20and%20Xiao%20Xiao%20and%20Lei%20Wang%20and%20Yuhui%20Yang%20and%20Flora%20D%20Salim&entry.1292438233=%20%20Accuracy%20and%20timeliness%20are%20indeed%20often%20conflicting%20goals%20in%20prediction%0Atasks.%20Premature%20predictions%20may%20yield%20a%20higher%20rate%20of%20false%20alarms%2C%20whereas%0Adelaying%20predictions%20to%20gather%20more%20information%20can%20render%20them%20too%20late%20to%20be%0Auseful.%20In%20applications%20such%20as%20wildfires%2C%20crimes%2C%20and%20traffic%20jams%2C%20timely%0Apredictions%20are%20vital%20for%20safeguarding%20human%20life%20and%20property.%20Consequently%2C%0Afinding%20a%20balance%20between%20accuracy%20and%20timeliness%20is%20crucial.%20In%20this%20paper%2C%20we%0Apropose%20a%20spatio-temporal%20early%20prediction%20model%20based%20on%20Multi-Objective%0Areinforcement%20learning%20that%20can%20either%20implement%20an%20optimal%20policy%20given%20a%0Apreference%20or%20infer%20the%20preference%20based%20on%20a%20small%20number%20of%20samples.%20The%0Amodel%20addresses%20two%20primary%20challenges%3A%201%29%20enhancing%20the%20accuracy%20of%20early%0Apredictions%20and%202%29%20providing%20the%20optimal%20policy%20for%20determining%20the%20most%0Asuitable%20prediction%20time%20for%20each%20area.%20Our%20method%20demonstrates%20superior%0Aperformance%20on%20three%20large-scale%20real-world%20datasets%2C%20surpassing%20existing%0Amethods%20in%20early%20spatio-temporal%20prediction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04035v2&entry.124074799=Read"},
{"title": "Realistic Data Generation for 6D Pose Estimation of Surgical Instruments", "author": "Juan Antonio Barragan and Jintan Zhang and Haoying Zhou and Adnan Munawar and Peter Kazanzides", "abstract": "  Automation in surgical robotics has the potential to improve patient safety\nand surgical efficiency, but it is difficult to achieve due to the need for\nrobust perception algorithms. In particular, 6D pose estimation of surgical\ninstruments is critical to enable the automatic execution of surgical maneuvers\nbased on visual feedback. In recent years, supervised deep learning algorithms\nhave shown increasingly better performance at 6D pose estimation tasks; yet,\ntheir success depends on the availability of large amounts of annotated data.\nIn household and industrial settings, synthetic data, generated with 3D\ncomputer graphics software, has been shown as an alternative to minimize\nannotation costs of 6D pose datasets. However, this strategy does not translate\nwell to surgical domains as commercial graphics software have limited tools to\ngenerate images depicting realistic instrument-tissue interactions. To address\nthese limitations, we propose an improved simulation environment for surgical\nrobotics that enables the automatic generation of large and diverse datasets\nfor 6D pose estimation of surgical instruments. Among the improvements, we\ndeveloped an automated data generation pipeline and an improved surgical scene.\nTo show the applicability of our system, we generated a dataset of 7.5k images\nwith pose annotations of a surgical needle that was used to evaluate a\nstate-of-the-art pose estimation network. The trained model obtained a mean\ntranslational error of 2.59mm on a challenging dataset that presented varying\nlevels of occlusion. These results highlight our pipeline's success in training\nand evaluating novel vision algorithms for surgical robotics applications.\n", "link": "http://arxiv.org/abs/2406.07328v1", "date": "2024-06-11", "relevancy": 2.1219, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5474}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5283}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Realistic%20Data%20Generation%20for%206D%20Pose%20Estimation%20of%20Surgical%20Instruments&body=Title%3A%20Realistic%20Data%20Generation%20for%206D%20Pose%20Estimation%20of%20Surgical%20Instruments%0AAuthor%3A%20Juan%20Antonio%20Barragan%20and%20Jintan%20Zhang%20and%20Haoying%20Zhou%20and%20Adnan%20Munawar%20and%20Peter%20Kazanzides%0AAbstract%3A%20%20%20Automation%20in%20surgical%20robotics%20has%20the%20potential%20to%20improve%20patient%20safety%0Aand%20surgical%20efficiency%2C%20but%20it%20is%20difficult%20to%20achieve%20due%20to%20the%20need%20for%0Arobust%20perception%20algorithms.%20In%20particular%2C%206D%20pose%20estimation%20of%20surgical%0Ainstruments%20is%20critical%20to%20enable%20the%20automatic%20execution%20of%20surgical%20maneuvers%0Abased%20on%20visual%20feedback.%20In%20recent%20years%2C%20supervised%20deep%20learning%20algorithms%0Ahave%20shown%20increasingly%20better%20performance%20at%206D%20pose%20estimation%20tasks%3B%20yet%2C%0Atheir%20success%20depends%20on%20the%20availability%20of%20large%20amounts%20of%20annotated%20data.%0AIn%20household%20and%20industrial%20settings%2C%20synthetic%20data%2C%20generated%20with%203D%0Acomputer%20graphics%20software%2C%20has%20been%20shown%20as%20an%20alternative%20to%20minimize%0Aannotation%20costs%20of%206D%20pose%20datasets.%20However%2C%20this%20strategy%20does%20not%20translate%0Awell%20to%20surgical%20domains%20as%20commercial%20graphics%20software%20have%20limited%20tools%20to%0Agenerate%20images%20depicting%20realistic%20instrument-tissue%20interactions.%20To%20address%0Athese%20limitations%2C%20we%20propose%20an%20improved%20simulation%20environment%20for%20surgical%0Arobotics%20that%20enables%20the%20automatic%20generation%20of%20large%20and%20diverse%20datasets%0Afor%206D%20pose%20estimation%20of%20surgical%20instruments.%20Among%20the%20improvements%2C%20we%0Adeveloped%20an%20automated%20data%20generation%20pipeline%20and%20an%20improved%20surgical%20scene.%0ATo%20show%20the%20applicability%20of%20our%20system%2C%20we%20generated%20a%20dataset%20of%207.5k%20images%0Awith%20pose%20annotations%20of%20a%20surgical%20needle%20that%20was%20used%20to%20evaluate%20a%0Astate-of-the-art%20pose%20estimation%20network.%20The%20trained%20model%20obtained%20a%20mean%0Atranslational%20error%20of%202.59mm%20on%20a%20challenging%20dataset%20that%20presented%20varying%0Alevels%20of%20occlusion.%20These%20results%20highlight%20our%20pipeline%27s%20success%20in%20training%0Aand%20evaluating%20novel%20vision%20algorithms%20for%20surgical%20robotics%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealistic%2520Data%2520Generation%2520for%25206D%2520Pose%2520Estimation%2520of%2520Surgical%2520Instruments%26entry.906535625%3DJuan%2520Antonio%2520Barragan%2520and%2520Jintan%2520Zhang%2520and%2520Haoying%2520Zhou%2520and%2520Adnan%2520Munawar%2520and%2520Peter%2520Kazanzides%26entry.1292438233%3D%2520%2520Automation%2520in%2520surgical%2520robotics%2520has%2520the%2520potential%2520to%2520improve%2520patient%2520safety%250Aand%2520surgical%2520efficiency%252C%2520but%2520it%2520is%2520difficult%2520to%2520achieve%2520due%2520to%2520the%2520need%2520for%250Arobust%2520perception%2520algorithms.%2520In%2520particular%252C%25206D%2520pose%2520estimation%2520of%2520surgical%250Ainstruments%2520is%2520critical%2520to%2520enable%2520the%2520automatic%2520execution%2520of%2520surgical%2520maneuvers%250Abased%2520on%2520visual%2520feedback.%2520In%2520recent%2520years%252C%2520supervised%2520deep%2520learning%2520algorithms%250Ahave%2520shown%2520increasingly%2520better%2520performance%2520at%25206D%2520pose%2520estimation%2520tasks%253B%2520yet%252C%250Atheir%2520success%2520depends%2520on%2520the%2520availability%2520of%2520large%2520amounts%2520of%2520annotated%2520data.%250AIn%2520household%2520and%2520industrial%2520settings%252C%2520synthetic%2520data%252C%2520generated%2520with%25203D%250Acomputer%2520graphics%2520software%252C%2520has%2520been%2520shown%2520as%2520an%2520alternative%2520to%2520minimize%250Aannotation%2520costs%2520of%25206D%2520pose%2520datasets.%2520However%252C%2520this%2520strategy%2520does%2520not%2520translate%250Awell%2520to%2520surgical%2520domains%2520as%2520commercial%2520graphics%2520software%2520have%2520limited%2520tools%2520to%250Agenerate%2520images%2520depicting%2520realistic%2520instrument-tissue%2520interactions.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520an%2520improved%2520simulation%2520environment%2520for%2520surgical%250Arobotics%2520that%2520enables%2520the%2520automatic%2520generation%2520of%2520large%2520and%2520diverse%2520datasets%250Afor%25206D%2520pose%2520estimation%2520of%2520surgical%2520instruments.%2520Among%2520the%2520improvements%252C%2520we%250Adeveloped%2520an%2520automated%2520data%2520generation%2520pipeline%2520and%2520an%2520improved%2520surgical%2520scene.%250ATo%2520show%2520the%2520applicability%2520of%2520our%2520system%252C%2520we%2520generated%2520a%2520dataset%2520of%25207.5k%2520images%250Awith%2520pose%2520annotations%2520of%2520a%2520surgical%2520needle%2520that%2520was%2520used%2520to%2520evaluate%2520a%250Astate-of-the-art%2520pose%2520estimation%2520network.%2520The%2520trained%2520model%2520obtained%2520a%2520mean%250Atranslational%2520error%2520of%25202.59mm%2520on%2520a%2520challenging%2520dataset%2520that%2520presented%2520varying%250Alevels%2520of%2520occlusion.%2520These%2520results%2520highlight%2520our%2520pipeline%2527s%2520success%2520in%2520training%250Aand%2520evaluating%2520novel%2520vision%2520algorithms%2520for%2520surgical%2520robotics%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Realistic%20Data%20Generation%20for%206D%20Pose%20Estimation%20of%20Surgical%20Instruments&entry.906535625=Juan%20Antonio%20Barragan%20and%20Jintan%20Zhang%20and%20Haoying%20Zhou%20and%20Adnan%20Munawar%20and%20Peter%20Kazanzides&entry.1292438233=%20%20Automation%20in%20surgical%20robotics%20has%20the%20potential%20to%20improve%20patient%20safety%0Aand%20surgical%20efficiency%2C%20but%20it%20is%20difficult%20to%20achieve%20due%20to%20the%20need%20for%0Arobust%20perception%20algorithms.%20In%20particular%2C%206D%20pose%20estimation%20of%20surgical%0Ainstruments%20is%20critical%20to%20enable%20the%20automatic%20execution%20of%20surgical%20maneuvers%0Abased%20on%20visual%20feedback.%20In%20recent%20years%2C%20supervised%20deep%20learning%20algorithms%0Ahave%20shown%20increasingly%20better%20performance%20at%206D%20pose%20estimation%20tasks%3B%20yet%2C%0Atheir%20success%20depends%20on%20the%20availability%20of%20large%20amounts%20of%20annotated%20data.%0AIn%20household%20and%20industrial%20settings%2C%20synthetic%20data%2C%20generated%20with%203D%0Acomputer%20graphics%20software%2C%20has%20been%20shown%20as%20an%20alternative%20to%20minimize%0Aannotation%20costs%20of%206D%20pose%20datasets.%20However%2C%20this%20strategy%20does%20not%20translate%0Awell%20to%20surgical%20domains%20as%20commercial%20graphics%20software%20have%20limited%20tools%20to%0Agenerate%20images%20depicting%20realistic%20instrument-tissue%20interactions.%20To%20address%0Athese%20limitations%2C%20we%20propose%20an%20improved%20simulation%20environment%20for%20surgical%0Arobotics%20that%20enables%20the%20automatic%20generation%20of%20large%20and%20diverse%20datasets%0Afor%206D%20pose%20estimation%20of%20surgical%20instruments.%20Among%20the%20improvements%2C%20we%0Adeveloped%20an%20automated%20data%20generation%20pipeline%20and%20an%20improved%20surgical%20scene.%0ATo%20show%20the%20applicability%20of%20our%20system%2C%20we%20generated%20a%20dataset%20of%207.5k%20images%0Awith%20pose%20annotations%20of%20a%20surgical%20needle%20that%20was%20used%20to%20evaluate%20a%0Astate-of-the-art%20pose%20estimation%20network.%20The%20trained%20model%20obtained%20a%20mean%0Atranslational%20error%20of%202.59mm%20on%20a%20challenging%20dataset%20that%20presented%20varying%0Alevels%20of%20occlusion.%20These%20results%20highlight%20our%20pipeline%27s%20success%20in%20training%0Aand%20evaluating%20novel%20vision%20algorithms%20for%20surgical%20robotics%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07328v1&entry.124074799=Read"},
{"title": "Beware of Aliases -- Signal Preservation is Crucial for Robust Image\n  Restoration", "author": "Shashank Agnihotri and Julia Grabinski and Janis Keuper and Margret Keuper", "abstract": "  Image restoration networks are usually comprised of an encoder and a decoder,\nresponsible for aggregating image content from noisy, distorted data and to\nrestore clean, undistorted images, respectively. Data aggregation as well as\nhigh-resolution image generation both usually come at the risk of involving\naliases, i.e.~standard architectures put their ability to reconstruct the model\ninput in jeopardy to reach high PSNR values on validation data. The price to be\npaid is low model robustness. In this work, we show that simply providing\nalias-free paths in state-of-the-art reconstruction transformers supports\nimproved model robustness at low costs on the restoration performance. We do so\nby proposing BOA-Restormer, a transformer-based image restoration model that\nexecutes downsampling and upsampling operations partly in the frequency domain\nto ensure alias-free paths along the entire model while potentially preserving\nall relevant high-frequency information.\n", "link": "http://arxiv.org/abs/2406.07435v1", "date": "2024-06-11", "relevancy": 2.1204, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5641}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5245}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beware%20of%20Aliases%20--%20Signal%20Preservation%20is%20Crucial%20for%20Robust%20Image%0A%20%20Restoration&body=Title%3A%20Beware%20of%20Aliases%20--%20Signal%20Preservation%20is%20Crucial%20for%20Robust%20Image%0A%20%20Restoration%0AAuthor%3A%20Shashank%20Agnihotri%20and%20Julia%20Grabinski%20and%20Janis%20Keuper%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Image%20restoration%20networks%20are%20usually%20comprised%20of%20an%20encoder%20and%20a%20decoder%2C%0Aresponsible%20for%20aggregating%20image%20content%20from%20noisy%2C%20distorted%20data%20and%20to%0Arestore%20clean%2C%20undistorted%20images%2C%20respectively.%20Data%20aggregation%20as%20well%20as%0Ahigh-resolution%20image%20generation%20both%20usually%20come%20at%20the%20risk%20of%20involving%0Aaliases%2C%20i.e.~standard%20architectures%20put%20their%20ability%20to%20reconstruct%20the%20model%0Ainput%20in%20jeopardy%20to%20reach%20high%20PSNR%20values%20on%20validation%20data.%20The%20price%20to%20be%0Apaid%20is%20low%20model%20robustness.%20In%20this%20work%2C%20we%20show%20that%20simply%20providing%0Aalias-free%20paths%20in%20state-of-the-art%20reconstruction%20transformers%20supports%0Aimproved%20model%20robustness%20at%20low%20costs%20on%20the%20restoration%20performance.%20We%20do%20so%0Aby%20proposing%20BOA-Restormer%2C%20a%20transformer-based%20image%20restoration%20model%20that%0Aexecutes%20downsampling%20and%20upsampling%20operations%20partly%20in%20the%20frequency%20domain%0Ato%20ensure%20alias-free%20paths%20along%20the%20entire%20model%20while%20potentially%20preserving%0Aall%20relevant%20high-frequency%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeware%2520of%2520Aliases%2520--%2520Signal%2520Preservation%2520is%2520Crucial%2520for%2520Robust%2520Image%250A%2520%2520Restoration%26entry.906535625%3DShashank%2520Agnihotri%2520and%2520Julia%2520Grabinski%2520and%2520Janis%2520Keuper%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Image%2520restoration%2520networks%2520are%2520usually%2520comprised%2520of%2520an%2520encoder%2520and%2520a%2520decoder%252C%250Aresponsible%2520for%2520aggregating%2520image%2520content%2520from%2520noisy%252C%2520distorted%2520data%2520and%2520to%250Arestore%2520clean%252C%2520undistorted%2520images%252C%2520respectively.%2520Data%2520aggregation%2520as%2520well%2520as%250Ahigh-resolution%2520image%2520generation%2520both%2520usually%2520come%2520at%2520the%2520risk%2520of%2520involving%250Aaliases%252C%2520i.e.~standard%2520architectures%2520put%2520their%2520ability%2520to%2520reconstruct%2520the%2520model%250Ainput%2520in%2520jeopardy%2520to%2520reach%2520high%2520PSNR%2520values%2520on%2520validation%2520data.%2520The%2520price%2520to%2520be%250Apaid%2520is%2520low%2520model%2520robustness.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520simply%2520providing%250Aalias-free%2520paths%2520in%2520state-of-the-art%2520reconstruction%2520transformers%2520supports%250Aimproved%2520model%2520robustness%2520at%2520low%2520costs%2520on%2520the%2520restoration%2520performance.%2520We%2520do%2520so%250Aby%2520proposing%2520BOA-Restormer%252C%2520a%2520transformer-based%2520image%2520restoration%2520model%2520that%250Aexecutes%2520downsampling%2520and%2520upsampling%2520operations%2520partly%2520in%2520the%2520frequency%2520domain%250Ato%2520ensure%2520alias-free%2520paths%2520along%2520the%2520entire%2520model%2520while%2520potentially%2520preserving%250Aall%2520relevant%2520high-frequency%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beware%20of%20Aliases%20--%20Signal%20Preservation%20is%20Crucial%20for%20Robust%20Image%0A%20%20Restoration&entry.906535625=Shashank%20Agnihotri%20and%20Julia%20Grabinski%20and%20Janis%20Keuper%20and%20Margret%20Keuper&entry.1292438233=%20%20Image%20restoration%20networks%20are%20usually%20comprised%20of%20an%20encoder%20and%20a%20decoder%2C%0Aresponsible%20for%20aggregating%20image%20content%20from%20noisy%2C%20distorted%20data%20and%20to%0Arestore%20clean%2C%20undistorted%20images%2C%20respectively.%20Data%20aggregation%20as%20well%20as%0Ahigh-resolution%20image%20generation%20both%20usually%20come%20at%20the%20risk%20of%20involving%0Aaliases%2C%20i.e.~standard%20architectures%20put%20their%20ability%20to%20reconstruct%20the%20model%0Ainput%20in%20jeopardy%20to%20reach%20high%20PSNR%20values%20on%20validation%20data.%20The%20price%20to%20be%0Apaid%20is%20low%20model%20robustness.%20In%20this%20work%2C%20we%20show%20that%20simply%20providing%0Aalias-free%20paths%20in%20state-of-the-art%20reconstruction%20transformers%20supports%0Aimproved%20model%20robustness%20at%20low%20costs%20on%20the%20restoration%20performance.%20We%20do%20so%0Aby%20proposing%20BOA-Restormer%2C%20a%20transformer-based%20image%20restoration%20model%20that%0Aexecutes%20downsampling%20and%20upsampling%20operations%20partly%20in%20the%20frequency%20domain%0Ato%20ensure%20alias-free%20paths%20along%20the%20entire%20model%20while%20potentially%20preserving%0Aall%20relevant%20high-frequency%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07435v1&entry.124074799=Read"},
{"title": "Transferring Knowledge from Large Foundation Models to Small Downstream\n  Models", "author": "Shikai Qiu and Boran Han and Danielle C. Maddix and Shuai Zhang and Yuyang Wang and Andrew Gordon Wilson", "abstract": "  How do we transfer the relevant knowledge from ever larger foundation models\ninto small, task-specific downstream models that can run at much lower costs?\nStandard transfer learning using pre-trained weights as the initialization\ntransfers limited information and commits us to often massive pre-trained\narchitectures. This procedure also precludes combining multiple pre-trained\nmodels that learn complementary information. To address these shortcomings, we\nintroduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT\noperates purely on features, thereby decoupling the choice of the pre-trained\nmodel from the smaller downstream model. Rather than indiscriminately\ncompressing all pre-trained features, AFT adaptively transfers pre-trained\nfeatures that are most useful for performing the downstream task, using a\nsimple regularization that adds minimal overhead. Across multiple vision,\nlanguage, and multi-modal datasets, AFT achieves significantly better\ndownstream performance compared to alternatives with a similar computational\ncost. Furthermore, AFT reliably translates improvement in pre-trained models\ninto improvement in downstream performance, even if the downstream model is\nover $50\\times$ smaller, and can effectively transfer complementary information\nlearned by multiple pre-trained models.\n", "link": "http://arxiv.org/abs/2406.07337v1", "date": "2024-06-11", "relevancy": 2.1114, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5514}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5268}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferring%20Knowledge%20from%20Large%20Foundation%20Models%20to%20Small%20Downstream%0A%20%20Models&body=Title%3A%20Transferring%20Knowledge%20from%20Large%20Foundation%20Models%20to%20Small%20Downstream%0A%20%20Models%0AAuthor%3A%20Shikai%20Qiu%20and%20Boran%20Han%20and%20Danielle%20C.%20Maddix%20and%20Shuai%20Zhang%20and%20Yuyang%20Wang%20and%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20How%20do%20we%20transfer%20the%20relevant%20knowledge%20from%20ever%20larger%20foundation%20models%0Ainto%20small%2C%20task-specific%20downstream%20models%20that%20can%20run%20at%20much%20lower%20costs%3F%0AStandard%20transfer%20learning%20using%20pre-trained%20weights%20as%20the%20initialization%0Atransfers%20limited%20information%20and%20commits%20us%20to%20often%20massive%20pre-trained%0Aarchitectures.%20This%20procedure%20also%20precludes%20combining%20multiple%20pre-trained%0Amodels%20that%20learn%20complementary%20information.%20To%20address%20these%20shortcomings%2C%20we%0Aintroduce%20Adaptive%20Feature%20Transfer%20%28AFT%29.%20Instead%20of%20transferring%20weights%2C%20AFT%0Aoperates%20purely%20on%20features%2C%20thereby%20decoupling%20the%20choice%20of%20the%20pre-trained%0Amodel%20from%20the%20smaller%20downstream%20model.%20Rather%20than%20indiscriminately%0Acompressing%20all%20pre-trained%20features%2C%20AFT%20adaptively%20transfers%20pre-trained%0Afeatures%20that%20are%20most%20useful%20for%20performing%20the%20downstream%20task%2C%20using%20a%0Asimple%20regularization%20that%20adds%20minimal%20overhead.%20Across%20multiple%20vision%2C%0Alanguage%2C%20and%20multi-modal%20datasets%2C%20AFT%20achieves%20significantly%20better%0Adownstream%20performance%20compared%20to%20alternatives%20with%20a%20similar%20computational%0Acost.%20Furthermore%2C%20AFT%20reliably%20translates%20improvement%20in%20pre-trained%20models%0Ainto%20improvement%20in%20downstream%20performance%2C%20even%20if%20the%20downstream%20model%20is%0Aover%20%2450%5Ctimes%24%20smaller%2C%20and%20can%20effectively%20transfer%20complementary%20information%0Alearned%20by%20multiple%20pre-trained%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferring%2520Knowledge%2520from%2520Large%2520Foundation%2520Models%2520to%2520Small%2520Downstream%250A%2520%2520Models%26entry.906535625%3DShikai%2520Qiu%2520and%2520Boran%2520Han%2520and%2520Danielle%2520C.%2520Maddix%2520and%2520Shuai%2520Zhang%2520and%2520Yuyang%2520Wang%2520and%2520Andrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520How%2520do%2520we%2520transfer%2520the%2520relevant%2520knowledge%2520from%2520ever%2520larger%2520foundation%2520models%250Ainto%2520small%252C%2520task-specific%2520downstream%2520models%2520that%2520can%2520run%2520at%2520much%2520lower%2520costs%253F%250AStandard%2520transfer%2520learning%2520using%2520pre-trained%2520weights%2520as%2520the%2520initialization%250Atransfers%2520limited%2520information%2520and%2520commits%2520us%2520to%2520often%2520massive%2520pre-trained%250Aarchitectures.%2520This%2520procedure%2520also%2520precludes%2520combining%2520multiple%2520pre-trained%250Amodels%2520that%2520learn%2520complementary%2520information.%2520To%2520address%2520these%2520shortcomings%252C%2520we%250Aintroduce%2520Adaptive%2520Feature%2520Transfer%2520%2528AFT%2529.%2520Instead%2520of%2520transferring%2520weights%252C%2520AFT%250Aoperates%2520purely%2520on%2520features%252C%2520thereby%2520decoupling%2520the%2520choice%2520of%2520the%2520pre-trained%250Amodel%2520from%2520the%2520smaller%2520downstream%2520model.%2520Rather%2520than%2520indiscriminately%250Acompressing%2520all%2520pre-trained%2520features%252C%2520AFT%2520adaptively%2520transfers%2520pre-trained%250Afeatures%2520that%2520are%2520most%2520useful%2520for%2520performing%2520the%2520downstream%2520task%252C%2520using%2520a%250Asimple%2520regularization%2520that%2520adds%2520minimal%2520overhead.%2520Across%2520multiple%2520vision%252C%250Alanguage%252C%2520and%2520multi-modal%2520datasets%252C%2520AFT%2520achieves%2520significantly%2520better%250Adownstream%2520performance%2520compared%2520to%2520alternatives%2520with%2520a%2520similar%2520computational%250Acost.%2520Furthermore%252C%2520AFT%2520reliably%2520translates%2520improvement%2520in%2520pre-trained%2520models%250Ainto%2520improvement%2520in%2520downstream%2520performance%252C%2520even%2520if%2520the%2520downstream%2520model%2520is%250Aover%2520%252450%255Ctimes%2524%2520smaller%252C%2520and%2520can%2520effectively%2520transfer%2520complementary%2520information%250Alearned%2520by%2520multiple%2520pre-trained%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferring%20Knowledge%20from%20Large%20Foundation%20Models%20to%20Small%20Downstream%0A%20%20Models&entry.906535625=Shikai%20Qiu%20and%20Boran%20Han%20and%20Danielle%20C.%20Maddix%20and%20Shuai%20Zhang%20and%20Yuyang%20Wang%20and%20Andrew%20Gordon%20Wilson&entry.1292438233=%20%20How%20do%20we%20transfer%20the%20relevant%20knowledge%20from%20ever%20larger%20foundation%20models%0Ainto%20small%2C%20task-specific%20downstream%20models%20that%20can%20run%20at%20much%20lower%20costs%3F%0AStandard%20transfer%20learning%20using%20pre-trained%20weights%20as%20the%20initialization%0Atransfers%20limited%20information%20and%20commits%20us%20to%20often%20massive%20pre-trained%0Aarchitectures.%20This%20procedure%20also%20precludes%20combining%20multiple%20pre-trained%0Amodels%20that%20learn%20complementary%20information.%20To%20address%20these%20shortcomings%2C%20we%0Aintroduce%20Adaptive%20Feature%20Transfer%20%28AFT%29.%20Instead%20of%20transferring%20weights%2C%20AFT%0Aoperates%20purely%20on%20features%2C%20thereby%20decoupling%20the%20choice%20of%20the%20pre-trained%0Amodel%20from%20the%20smaller%20downstream%20model.%20Rather%20than%20indiscriminately%0Acompressing%20all%20pre-trained%20features%2C%20AFT%20adaptively%20transfers%20pre-trained%0Afeatures%20that%20are%20most%20useful%20for%20performing%20the%20downstream%20task%2C%20using%20a%0Asimple%20regularization%20that%20adds%20minimal%20overhead.%20Across%20multiple%20vision%2C%0Alanguage%2C%20and%20multi-modal%20datasets%2C%20AFT%20achieves%20significantly%20better%0Adownstream%20performance%20compared%20to%20alternatives%20with%20a%20similar%20computational%0Acost.%20Furthermore%2C%20AFT%20reliably%20translates%20improvement%20in%20pre-trained%20models%0Ainto%20improvement%20in%20downstream%20performance%2C%20even%20if%20the%20downstream%20model%20is%0Aover%20%2450%5Ctimes%24%20smaller%2C%20and%20can%20effectively%20transfer%20complementary%20information%0Alearned%20by%20multiple%20pre-trained%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07337v1&entry.124074799=Read"},
{"title": "Embedded Graph Convolutional Networks for Real-Time Event Data\n  Processing on SoC FPGAs", "author": "Kamil Jeziorek and Piotr Wzorek and Krzysztof Blachut and Andrea Pinna and Tomasz Kryjak", "abstract": "  The utilisation of event cameras represents an important and swiftly evolving\ntrend aimed at addressing the constraints of traditional video systems.\nParticularly within the automotive domain, these cameras find significant\nrelevance for their integration into embedded real-time systems due to lower\nlatency and energy consumption. One effective approach to ensure the necessary\nthroughput and latency for event processing systems is through the utilisation\nof graph convolutional networks (GCNs). In this study, we introduce a series of\nhardware-aware optimisations tailored for PointNet++, a GCN architecture\ndesigned for point cloud processing. The proposed techniques result in more\nthan a 100-fold reduction in model size compared to Asynchronous Event-based\nGNN (AEGNN), one of the most recent works in the field, with a relatively small\ndecrease in accuracy (2.3% for N-Caltech101 classification, 1.7% for N-Cars\nclassification), thus following the TinyML trend. Based on software research,\nwe designed a custom EFGCN (Event-Based FPGA-accelerated Graph Convolutional\nNetwork) and we implemented it on ZCU104 SoC FPGA platform, achieving a\nthroughput of 13.3 million events per second (MEPS) and real-time partially\nasynchronous processing with a latency of 4.47 ms. We also address the\nscalability of the proposed hardware model to improve the obtained accuracy\nscore. To the best of our knowledge, this study marks the first endeavour in\naccelerating PointNet++ networks on SoC FPGAs, as well as the first hardware\narchitecture exploration of graph convolutional networks implementation for\nreal-time continuous event data processing. We publish both software and\nhardware source code in an open repository: https://github.com/vision-agh/***\n(will be published upon acceptance).\n", "link": "http://arxiv.org/abs/2406.07318v1", "date": "2024-06-11", "relevancy": 2.1095, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5426}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5198}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedded%20Graph%20Convolutional%20Networks%20for%20Real-Time%20Event%20Data%0A%20%20Processing%20on%20SoC%20FPGAs&body=Title%3A%20Embedded%20Graph%20Convolutional%20Networks%20for%20Real-Time%20Event%20Data%0A%20%20Processing%20on%20SoC%20FPGAs%0AAuthor%3A%20Kamil%20Jeziorek%20and%20Piotr%20Wzorek%20and%20Krzysztof%20Blachut%20and%20Andrea%20Pinna%20and%20Tomasz%20Kryjak%0AAbstract%3A%20%20%20The%20utilisation%20of%20event%20cameras%20represents%20an%20important%20and%20swiftly%20evolving%0Atrend%20aimed%20at%20addressing%20the%20constraints%20of%20traditional%20video%20systems.%0AParticularly%20within%20the%20automotive%20domain%2C%20these%20cameras%20find%20significant%0Arelevance%20for%20their%20integration%20into%20embedded%20real-time%20systems%20due%20to%20lower%0Alatency%20and%20energy%20consumption.%20One%20effective%20approach%20to%20ensure%20the%20necessary%0Athroughput%20and%20latency%20for%20event%20processing%20systems%20is%20through%20the%20utilisation%0Aof%20graph%20convolutional%20networks%20%28GCNs%29.%20In%20this%20study%2C%20we%20introduce%20a%20series%20of%0Ahardware-aware%20optimisations%20tailored%20for%20PointNet%2B%2B%2C%20a%20GCN%20architecture%0Adesigned%20for%20point%20cloud%20processing.%20The%20proposed%20techniques%20result%20in%20more%0Athan%20a%20100-fold%20reduction%20in%20model%20size%20compared%20to%20Asynchronous%20Event-based%0AGNN%20%28AEGNN%29%2C%20one%20of%20the%20most%20recent%20works%20in%20the%20field%2C%20with%20a%20relatively%20small%0Adecrease%20in%20accuracy%20%282.3%25%20for%20N-Caltech101%20classification%2C%201.7%25%20for%20N-Cars%0Aclassification%29%2C%20thus%20following%20the%20TinyML%20trend.%20Based%20on%20software%20research%2C%0Awe%20designed%20a%20custom%20EFGCN%20%28Event-Based%20FPGA-accelerated%20Graph%20Convolutional%0ANetwork%29%20and%20we%20implemented%20it%20on%20ZCU104%20SoC%20FPGA%20platform%2C%20achieving%20a%0Athroughput%20of%2013.3%20million%20events%20per%20second%20%28MEPS%29%20and%20real-time%20partially%0Aasynchronous%20processing%20with%20a%20latency%20of%204.47%20ms.%20We%20also%20address%20the%0Ascalability%20of%20the%20proposed%20hardware%20model%20to%20improve%20the%20obtained%20accuracy%0Ascore.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20study%20marks%20the%20first%20endeavour%20in%0Aaccelerating%20PointNet%2B%2B%20networks%20on%20SoC%20FPGAs%2C%20as%20well%20as%20the%20first%20hardware%0Aarchitecture%20exploration%20of%20graph%20convolutional%20networks%20implementation%20for%0Areal-time%20continuous%20event%20data%20processing.%20We%20publish%20both%20software%20and%0Ahardware%20source%20code%20in%20an%20open%20repository%3A%20https%3A//github.com/vision-agh/%2A%2A%2A%0A%28will%20be%20published%20upon%20acceptance%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedded%2520Graph%2520Convolutional%2520Networks%2520for%2520Real-Time%2520Event%2520Data%250A%2520%2520Processing%2520on%2520SoC%2520FPGAs%26entry.906535625%3DKamil%2520Jeziorek%2520and%2520Piotr%2520Wzorek%2520and%2520Krzysztof%2520Blachut%2520and%2520Andrea%2520Pinna%2520and%2520Tomasz%2520Kryjak%26entry.1292438233%3D%2520%2520The%2520utilisation%2520of%2520event%2520cameras%2520represents%2520an%2520important%2520and%2520swiftly%2520evolving%250Atrend%2520aimed%2520at%2520addressing%2520the%2520constraints%2520of%2520traditional%2520video%2520systems.%250AParticularly%2520within%2520the%2520automotive%2520domain%252C%2520these%2520cameras%2520find%2520significant%250Arelevance%2520for%2520their%2520integration%2520into%2520embedded%2520real-time%2520systems%2520due%2520to%2520lower%250Alatency%2520and%2520energy%2520consumption.%2520One%2520effective%2520approach%2520to%2520ensure%2520the%2520necessary%250Athroughput%2520and%2520latency%2520for%2520event%2520processing%2520systems%2520is%2520through%2520the%2520utilisation%250Aof%2520graph%2520convolutional%2520networks%2520%2528GCNs%2529.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520series%2520of%250Ahardware-aware%2520optimisations%2520tailored%2520for%2520PointNet%252B%252B%252C%2520a%2520GCN%2520architecture%250Adesigned%2520for%2520point%2520cloud%2520processing.%2520The%2520proposed%2520techniques%2520result%2520in%2520more%250Athan%2520a%2520100-fold%2520reduction%2520in%2520model%2520size%2520compared%2520to%2520Asynchronous%2520Event-based%250AGNN%2520%2528AEGNN%2529%252C%2520one%2520of%2520the%2520most%2520recent%2520works%2520in%2520the%2520field%252C%2520with%2520a%2520relatively%2520small%250Adecrease%2520in%2520accuracy%2520%25282.3%2525%2520for%2520N-Caltech101%2520classification%252C%25201.7%2525%2520for%2520N-Cars%250Aclassification%2529%252C%2520thus%2520following%2520the%2520TinyML%2520trend.%2520Based%2520on%2520software%2520research%252C%250Awe%2520designed%2520a%2520custom%2520EFGCN%2520%2528Event-Based%2520FPGA-accelerated%2520Graph%2520Convolutional%250ANetwork%2529%2520and%2520we%2520implemented%2520it%2520on%2520ZCU104%2520SoC%2520FPGA%2520platform%252C%2520achieving%2520a%250Athroughput%2520of%252013.3%2520million%2520events%2520per%2520second%2520%2528MEPS%2529%2520and%2520real-time%2520partially%250Aasynchronous%2520processing%2520with%2520a%2520latency%2520of%25204.47%2520ms.%2520We%2520also%2520address%2520the%250Ascalability%2520of%2520the%2520proposed%2520hardware%2520model%2520to%2520improve%2520the%2520obtained%2520accuracy%250Ascore.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520study%2520marks%2520the%2520first%2520endeavour%2520in%250Aaccelerating%2520PointNet%252B%252B%2520networks%2520on%2520SoC%2520FPGAs%252C%2520as%2520well%2520as%2520the%2520first%2520hardware%250Aarchitecture%2520exploration%2520of%2520graph%2520convolutional%2520networks%2520implementation%2520for%250Areal-time%2520continuous%2520event%2520data%2520processing.%2520We%2520publish%2520both%2520software%2520and%250Ahardware%2520source%2520code%2520in%2520an%2520open%2520repository%253A%2520https%253A//github.com/vision-agh/%252A%252A%252A%250A%2528will%2520be%2520published%2520upon%2520acceptance%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20Graph%20Convolutional%20Networks%20for%20Real-Time%20Event%20Data%0A%20%20Processing%20on%20SoC%20FPGAs&entry.906535625=Kamil%20Jeziorek%20and%20Piotr%20Wzorek%20and%20Krzysztof%20Blachut%20and%20Andrea%20Pinna%20and%20Tomasz%20Kryjak&entry.1292438233=%20%20The%20utilisation%20of%20event%20cameras%20represents%20an%20important%20and%20swiftly%20evolving%0Atrend%20aimed%20at%20addressing%20the%20constraints%20of%20traditional%20video%20systems.%0AParticularly%20within%20the%20automotive%20domain%2C%20these%20cameras%20find%20significant%0Arelevance%20for%20their%20integration%20into%20embedded%20real-time%20systems%20due%20to%20lower%0Alatency%20and%20energy%20consumption.%20One%20effective%20approach%20to%20ensure%20the%20necessary%0Athroughput%20and%20latency%20for%20event%20processing%20systems%20is%20through%20the%20utilisation%0Aof%20graph%20convolutional%20networks%20%28GCNs%29.%20In%20this%20study%2C%20we%20introduce%20a%20series%20of%0Ahardware-aware%20optimisations%20tailored%20for%20PointNet%2B%2B%2C%20a%20GCN%20architecture%0Adesigned%20for%20point%20cloud%20processing.%20The%20proposed%20techniques%20result%20in%20more%0Athan%20a%20100-fold%20reduction%20in%20model%20size%20compared%20to%20Asynchronous%20Event-based%0AGNN%20%28AEGNN%29%2C%20one%20of%20the%20most%20recent%20works%20in%20the%20field%2C%20with%20a%20relatively%20small%0Adecrease%20in%20accuracy%20%282.3%25%20for%20N-Caltech101%20classification%2C%201.7%25%20for%20N-Cars%0Aclassification%29%2C%20thus%20following%20the%20TinyML%20trend.%20Based%20on%20software%20research%2C%0Awe%20designed%20a%20custom%20EFGCN%20%28Event-Based%20FPGA-accelerated%20Graph%20Convolutional%0ANetwork%29%20and%20we%20implemented%20it%20on%20ZCU104%20SoC%20FPGA%20platform%2C%20achieving%20a%0Athroughput%20of%2013.3%20million%20events%20per%20second%20%28MEPS%29%20and%20real-time%20partially%0Aasynchronous%20processing%20with%20a%20latency%20of%204.47%20ms.%20We%20also%20address%20the%0Ascalability%20of%20the%20proposed%20hardware%20model%20to%20improve%20the%20obtained%20accuracy%0Ascore.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20study%20marks%20the%20first%20endeavour%20in%0Aaccelerating%20PointNet%2B%2B%20networks%20on%20SoC%20FPGAs%2C%20as%20well%20as%20the%20first%20hardware%0Aarchitecture%20exploration%20of%20graph%20convolutional%20networks%20implementation%20for%0Areal-time%20continuous%20event%20data%20processing.%20We%20publish%20both%20software%20and%0Ahardware%20source%20code%20in%20an%20open%20repository%3A%20https%3A//github.com/vision-agh/%2A%2A%2A%0A%28will%20be%20published%20upon%20acceptance%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07318v1&entry.124074799=Read"},
{"title": "NNG-Mix: Improving Semi-supervised Anomaly Detection with Pseudo-anomaly\n  Generation", "author": "Hao Dong and Ga\u00ebtan Frusque and Yue Zhao and Eleni Chatzi and Olga Fink", "abstract": "  Anomaly detection (AD) is essential in identifying rare and often critical\nevents in complex systems, finding applications in fields such as network\nintrusion detection, financial fraud detection, and fault detection in\ninfrastructure and industrial systems. While AD is typically treated as an\nunsupervised learning task due to the high cost of label annotation, it is more\npractical to assume access to a small set of labeled anomaly samples from\ndomain experts, as is the case for semi-supervised anomaly detection.\nSemi-supervised and supervised approaches can leverage such labeled data,\nresulting in improved performance. In this paper, rather than proposing a new\nsemi-supervised or supervised approach for AD, we introduce a novel algorithm\nfor generating additional pseudo-anomalies on the basis of the limited labeled\nanomalies and a large volume of unlabeled data. This serves as an augmentation\nto facilitate the detection of new anomalies. Our proposed algorithm, named\nNearest Neighbor Gaussian Mixup (NNG-Mix), efficiently integrates information\nfrom both labeled and unlabeled data to generate pseudo-anomalies. We compare\nthe performance of this novel algorithm with commonly applied augmentation\ntechniques, such as Mixup and Cutout. We evaluate NNG-Mix by training various\nexisting semi-supervised and supervised anomaly detection algorithms on the\noriginal training data along with the generated pseudo-anomalies. Through\nextensive experiments on 57 benchmark datasets in ADBench, reflecting different\ndata types, we demonstrate that NNG-Mix outperforms other data augmentation\nmethods. It yields significant performance improvements compared to the\nbaselines trained exclusively on the original training data. Notably, NNG-Mix\nyields up to 16.4%, 8.8%, and 8.0% improvements on Classical, CV, and NLP\ndatasets in ADBench. Our source code is available at\nhttps://github.com/donghao51/NNG-Mix.\n", "link": "http://arxiv.org/abs/2311.11961v2", "date": "2024-06-11", "relevancy": 2.1078, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.564}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NNG-Mix%3A%20Improving%20Semi-supervised%20Anomaly%20Detection%20with%20Pseudo-anomaly%0A%20%20Generation&body=Title%3A%20NNG-Mix%3A%20Improving%20Semi-supervised%20Anomaly%20Detection%20with%20Pseudo-anomaly%0A%20%20Generation%0AAuthor%3A%20Hao%20Dong%20and%20Ga%C3%ABtan%20Frusque%20and%20Yue%20Zhao%20and%20Eleni%20Chatzi%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Anomaly%20detection%20%28AD%29%20is%20essential%20in%20identifying%20rare%20and%20often%20critical%0Aevents%20in%20complex%20systems%2C%20finding%20applications%20in%20fields%20such%20as%20network%0Aintrusion%20detection%2C%20financial%20fraud%20detection%2C%20and%20fault%20detection%20in%0Ainfrastructure%20and%20industrial%20systems.%20While%20AD%20is%20typically%20treated%20as%20an%0Aunsupervised%20learning%20task%20due%20to%20the%20high%20cost%20of%20label%20annotation%2C%20it%20is%20more%0Apractical%20to%20assume%20access%20to%20a%20small%20set%20of%20labeled%20anomaly%20samples%20from%0Adomain%20experts%2C%20as%20is%20the%20case%20for%20semi-supervised%20anomaly%20detection.%0ASemi-supervised%20and%20supervised%20approaches%20can%20leverage%20such%20labeled%20data%2C%0Aresulting%20in%20improved%20performance.%20In%20this%20paper%2C%20rather%20than%20proposing%20a%20new%0Asemi-supervised%20or%20supervised%20approach%20for%20AD%2C%20we%20introduce%20a%20novel%20algorithm%0Afor%20generating%20additional%20pseudo-anomalies%20on%20the%20basis%20of%20the%20limited%20labeled%0Aanomalies%20and%20a%20large%20volume%20of%20unlabeled%20data.%20This%20serves%20as%20an%20augmentation%0Ato%20facilitate%20the%20detection%20of%20new%20anomalies.%20Our%20proposed%20algorithm%2C%20named%0ANearest%20Neighbor%20Gaussian%20Mixup%20%28NNG-Mix%29%2C%20efficiently%20integrates%20information%0Afrom%20both%20labeled%20and%20unlabeled%20data%20to%20generate%20pseudo-anomalies.%20We%20compare%0Athe%20performance%20of%20this%20novel%20algorithm%20with%20commonly%20applied%20augmentation%0Atechniques%2C%20such%20as%20Mixup%20and%20Cutout.%20We%20evaluate%20NNG-Mix%20by%20training%20various%0Aexisting%20semi-supervised%20and%20supervised%20anomaly%20detection%20algorithms%20on%20the%0Aoriginal%20training%20data%20along%20with%20the%20generated%20pseudo-anomalies.%20Through%0Aextensive%20experiments%20on%2057%20benchmark%20datasets%20in%20ADBench%2C%20reflecting%20different%0Adata%20types%2C%20we%20demonstrate%20that%20NNG-Mix%20outperforms%20other%20data%20augmentation%0Amethods.%20It%20yields%20significant%20performance%20improvements%20compared%20to%20the%0Abaselines%20trained%20exclusively%20on%20the%20original%20training%20data.%20Notably%2C%20NNG-Mix%0Ayields%20up%20to%2016.4%25%2C%208.8%25%2C%20and%208.0%25%20improvements%20on%20Classical%2C%20CV%2C%20and%20NLP%0Adatasets%20in%20ADBench.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/donghao51/NNG-Mix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNNG-Mix%253A%2520Improving%2520Semi-supervised%2520Anomaly%2520Detection%2520with%2520Pseudo-anomaly%250A%2520%2520Generation%26entry.906535625%3DHao%2520Dong%2520and%2520Ga%25C3%25ABtan%2520Frusque%2520and%2520Yue%2520Zhao%2520and%2520Eleni%2520Chatzi%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520%2528AD%2529%2520is%2520essential%2520in%2520identifying%2520rare%2520and%2520often%2520critical%250Aevents%2520in%2520complex%2520systems%252C%2520finding%2520applications%2520in%2520fields%2520such%2520as%2520network%250Aintrusion%2520detection%252C%2520financial%2520fraud%2520detection%252C%2520and%2520fault%2520detection%2520in%250Ainfrastructure%2520and%2520industrial%2520systems.%2520While%2520AD%2520is%2520typically%2520treated%2520as%2520an%250Aunsupervised%2520learning%2520task%2520due%2520to%2520the%2520high%2520cost%2520of%2520label%2520annotation%252C%2520it%2520is%2520more%250Apractical%2520to%2520assume%2520access%2520to%2520a%2520small%2520set%2520of%2520labeled%2520anomaly%2520samples%2520from%250Adomain%2520experts%252C%2520as%2520is%2520the%2520case%2520for%2520semi-supervised%2520anomaly%2520detection.%250ASemi-supervised%2520and%2520supervised%2520approaches%2520can%2520leverage%2520such%2520labeled%2520data%252C%250Aresulting%2520in%2520improved%2520performance.%2520In%2520this%2520paper%252C%2520rather%2520than%2520proposing%2520a%2520new%250Asemi-supervised%2520or%2520supervised%2520approach%2520for%2520AD%252C%2520we%2520introduce%2520a%2520novel%2520algorithm%250Afor%2520generating%2520additional%2520pseudo-anomalies%2520on%2520the%2520basis%2520of%2520the%2520limited%2520labeled%250Aanomalies%2520and%2520a%2520large%2520volume%2520of%2520unlabeled%2520data.%2520This%2520serves%2520as%2520an%2520augmentation%250Ato%2520facilitate%2520the%2520detection%2520of%2520new%2520anomalies.%2520Our%2520proposed%2520algorithm%252C%2520named%250ANearest%2520Neighbor%2520Gaussian%2520Mixup%2520%2528NNG-Mix%2529%252C%2520efficiently%2520integrates%2520information%250Afrom%2520both%2520labeled%2520and%2520unlabeled%2520data%2520to%2520generate%2520pseudo-anomalies.%2520We%2520compare%250Athe%2520performance%2520of%2520this%2520novel%2520algorithm%2520with%2520commonly%2520applied%2520augmentation%250Atechniques%252C%2520such%2520as%2520Mixup%2520and%2520Cutout.%2520We%2520evaluate%2520NNG-Mix%2520by%2520training%2520various%250Aexisting%2520semi-supervised%2520and%2520supervised%2520anomaly%2520detection%2520algorithms%2520on%2520the%250Aoriginal%2520training%2520data%2520along%2520with%2520the%2520generated%2520pseudo-anomalies.%2520Through%250Aextensive%2520experiments%2520on%252057%2520benchmark%2520datasets%2520in%2520ADBench%252C%2520reflecting%2520different%250Adata%2520types%252C%2520we%2520demonstrate%2520that%2520NNG-Mix%2520outperforms%2520other%2520data%2520augmentation%250Amethods.%2520It%2520yields%2520significant%2520performance%2520improvements%2520compared%2520to%2520the%250Abaselines%2520trained%2520exclusively%2520on%2520the%2520original%2520training%2520data.%2520Notably%252C%2520NNG-Mix%250Ayields%2520up%2520to%252016.4%2525%252C%25208.8%2525%252C%2520and%25208.0%2525%2520improvements%2520on%2520Classical%252C%2520CV%252C%2520and%2520NLP%250Adatasets%2520in%2520ADBench.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/donghao51/NNG-Mix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NNG-Mix%3A%20Improving%20Semi-supervised%20Anomaly%20Detection%20with%20Pseudo-anomaly%0A%20%20Generation&entry.906535625=Hao%20Dong%20and%20Ga%C3%ABtan%20Frusque%20and%20Yue%20Zhao%20and%20Eleni%20Chatzi%20and%20Olga%20Fink&entry.1292438233=%20%20Anomaly%20detection%20%28AD%29%20is%20essential%20in%20identifying%20rare%20and%20often%20critical%0Aevents%20in%20complex%20systems%2C%20finding%20applications%20in%20fields%20such%20as%20network%0Aintrusion%20detection%2C%20financial%20fraud%20detection%2C%20and%20fault%20detection%20in%0Ainfrastructure%20and%20industrial%20systems.%20While%20AD%20is%20typically%20treated%20as%20an%0Aunsupervised%20learning%20task%20due%20to%20the%20high%20cost%20of%20label%20annotation%2C%20it%20is%20more%0Apractical%20to%20assume%20access%20to%20a%20small%20set%20of%20labeled%20anomaly%20samples%20from%0Adomain%20experts%2C%20as%20is%20the%20case%20for%20semi-supervised%20anomaly%20detection.%0ASemi-supervised%20and%20supervised%20approaches%20can%20leverage%20such%20labeled%20data%2C%0Aresulting%20in%20improved%20performance.%20In%20this%20paper%2C%20rather%20than%20proposing%20a%20new%0Asemi-supervised%20or%20supervised%20approach%20for%20AD%2C%20we%20introduce%20a%20novel%20algorithm%0Afor%20generating%20additional%20pseudo-anomalies%20on%20the%20basis%20of%20the%20limited%20labeled%0Aanomalies%20and%20a%20large%20volume%20of%20unlabeled%20data.%20This%20serves%20as%20an%20augmentation%0Ato%20facilitate%20the%20detection%20of%20new%20anomalies.%20Our%20proposed%20algorithm%2C%20named%0ANearest%20Neighbor%20Gaussian%20Mixup%20%28NNG-Mix%29%2C%20efficiently%20integrates%20information%0Afrom%20both%20labeled%20and%20unlabeled%20data%20to%20generate%20pseudo-anomalies.%20We%20compare%0Athe%20performance%20of%20this%20novel%20algorithm%20with%20commonly%20applied%20augmentation%0Atechniques%2C%20such%20as%20Mixup%20and%20Cutout.%20We%20evaluate%20NNG-Mix%20by%20training%20various%0Aexisting%20semi-supervised%20and%20supervised%20anomaly%20detection%20algorithms%20on%20the%0Aoriginal%20training%20data%20along%20with%20the%20generated%20pseudo-anomalies.%20Through%0Aextensive%20experiments%20on%2057%20benchmark%20datasets%20in%20ADBench%2C%20reflecting%20different%0Adata%20types%2C%20we%20demonstrate%20that%20NNG-Mix%20outperforms%20other%20data%20augmentation%0Amethods.%20It%20yields%20significant%20performance%20improvements%20compared%20to%20the%0Abaselines%20trained%20exclusively%20on%20the%20original%20training%20data.%20Notably%2C%20NNG-Mix%0Ayields%20up%20to%2016.4%25%2C%208.8%25%2C%20and%208.0%25%20improvements%20on%20Classical%2C%20CV%2C%20and%20NLP%0Adatasets%20in%20ADBench.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/donghao51/NNG-Mix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11961v2&entry.124074799=Read"},
{"title": "Beyond Model Collapse: Scaling Up with Synthesized Data Requires\n  Reinforcement", "author": "Yunzhen Feng and Elvis Dohmatob and Pu Yang and Francois Charton and Julia Kempe", "abstract": "  Synthesized data from generative models is increasingly considered as an\nalternative to human-annotated data for fine-tuning Large Language Models. This\nraises concerns about model collapse: a drop in performance of models\nfine-tuned on generated data. Considering that it is easier for both humans and\nmachines to tell between good and bad examples than to generate high-quality\nsamples, we investigate the use of feedback on synthesized data to prevent\nmodel collapse. We derive theoretical conditions under which a Gaussian mixture\nclassification model can achieve asymptotically optimal performance when\ntrained on feedback-augmented synthesized data, and provide supporting\nsimulations for finite regimes. We illustrate our theoretical predictions on\ntwo practical problems: computing matrix eigenvalues with transformers and news\nsummarization with large language models, which both undergo model collapse\nwhen trained on model-generated data. We show that training from\nfeedback-augmented synthesized data, either by pruning incorrect predictions or\nby selecting the best of several guesses, can prevent model collapse,\nvalidating popular approaches like RLHF.\n", "link": "http://arxiv.org/abs/2406.07515v1", "date": "2024-06-11", "relevancy": 2.0994, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6292}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5159}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Model%20Collapse%3A%20Scaling%20Up%20with%20Synthesized%20Data%20Requires%0A%20%20Reinforcement&body=Title%3A%20Beyond%20Model%20Collapse%3A%20Scaling%20Up%20with%20Synthesized%20Data%20Requires%0A%20%20Reinforcement%0AAuthor%3A%20Yunzhen%20Feng%20and%20Elvis%20Dohmatob%20and%20Pu%20Yang%20and%20Francois%20Charton%20and%20Julia%20Kempe%0AAbstract%3A%20%20%20Synthesized%20data%20from%20generative%20models%20is%20increasingly%20considered%20as%20an%0Aalternative%20to%20human-annotated%20data%20for%20fine-tuning%20Large%20Language%20Models.%20This%0Araises%20concerns%20about%20model%20collapse%3A%20a%20drop%20in%20performance%20of%20models%0Afine-tuned%20on%20generated%20data.%20Considering%20that%20it%20is%20easier%20for%20both%20humans%20and%0Amachines%20to%20tell%20between%20good%20and%20bad%20examples%20than%20to%20generate%20high-quality%0Asamples%2C%20we%20investigate%20the%20use%20of%20feedback%20on%20synthesized%20data%20to%20prevent%0Amodel%20collapse.%20We%20derive%20theoretical%20conditions%20under%20which%20a%20Gaussian%20mixture%0Aclassification%20model%20can%20achieve%20asymptotically%20optimal%20performance%20when%0Atrained%20on%20feedback-augmented%20synthesized%20data%2C%20and%20provide%20supporting%0Asimulations%20for%20finite%20regimes.%20We%20illustrate%20our%20theoretical%20predictions%20on%0Atwo%20practical%20problems%3A%20computing%20matrix%20eigenvalues%20with%20transformers%20and%20news%0Asummarization%20with%20large%20language%20models%2C%20which%20both%20undergo%20model%20collapse%0Awhen%20trained%20on%20model-generated%20data.%20We%20show%20that%20training%20from%0Afeedback-augmented%20synthesized%20data%2C%20either%20by%20pruning%20incorrect%20predictions%20or%0Aby%20selecting%20the%20best%20of%20several%20guesses%2C%20can%20prevent%20model%20collapse%2C%0Avalidating%20popular%20approaches%20like%20RLHF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Model%2520Collapse%253A%2520Scaling%2520Up%2520with%2520Synthesized%2520Data%2520Requires%250A%2520%2520Reinforcement%26entry.906535625%3DYunzhen%2520Feng%2520and%2520Elvis%2520Dohmatob%2520and%2520Pu%2520Yang%2520and%2520Francois%2520Charton%2520and%2520Julia%2520Kempe%26entry.1292438233%3D%2520%2520Synthesized%2520data%2520from%2520generative%2520models%2520is%2520increasingly%2520considered%2520as%2520an%250Aalternative%2520to%2520human-annotated%2520data%2520for%2520fine-tuning%2520Large%2520Language%2520Models.%2520This%250Araises%2520concerns%2520about%2520model%2520collapse%253A%2520a%2520drop%2520in%2520performance%2520of%2520models%250Afine-tuned%2520on%2520generated%2520data.%2520Considering%2520that%2520it%2520is%2520easier%2520for%2520both%2520humans%2520and%250Amachines%2520to%2520tell%2520between%2520good%2520and%2520bad%2520examples%2520than%2520to%2520generate%2520high-quality%250Asamples%252C%2520we%2520investigate%2520the%2520use%2520of%2520feedback%2520on%2520synthesized%2520data%2520to%2520prevent%250Amodel%2520collapse.%2520We%2520derive%2520theoretical%2520conditions%2520under%2520which%2520a%2520Gaussian%2520mixture%250Aclassification%2520model%2520can%2520achieve%2520asymptotically%2520optimal%2520performance%2520when%250Atrained%2520on%2520feedback-augmented%2520synthesized%2520data%252C%2520and%2520provide%2520supporting%250Asimulations%2520for%2520finite%2520regimes.%2520We%2520illustrate%2520our%2520theoretical%2520predictions%2520on%250Atwo%2520practical%2520problems%253A%2520computing%2520matrix%2520eigenvalues%2520with%2520transformers%2520and%2520news%250Asummarization%2520with%2520large%2520language%2520models%252C%2520which%2520both%2520undergo%2520model%2520collapse%250Awhen%2520trained%2520on%2520model-generated%2520data.%2520We%2520show%2520that%2520training%2520from%250Afeedback-augmented%2520synthesized%2520data%252C%2520either%2520by%2520pruning%2520incorrect%2520predictions%2520or%250Aby%2520selecting%2520the%2520best%2520of%2520several%2520guesses%252C%2520can%2520prevent%2520model%2520collapse%252C%250Avalidating%2520popular%2520approaches%2520like%2520RLHF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Model%20Collapse%3A%20Scaling%20Up%20with%20Synthesized%20Data%20Requires%0A%20%20Reinforcement&entry.906535625=Yunzhen%20Feng%20and%20Elvis%20Dohmatob%20and%20Pu%20Yang%20and%20Francois%20Charton%20and%20Julia%20Kempe&entry.1292438233=%20%20Synthesized%20data%20from%20generative%20models%20is%20increasingly%20considered%20as%20an%0Aalternative%20to%20human-annotated%20data%20for%20fine-tuning%20Large%20Language%20Models.%20This%0Araises%20concerns%20about%20model%20collapse%3A%20a%20drop%20in%20performance%20of%20models%0Afine-tuned%20on%20generated%20data.%20Considering%20that%20it%20is%20easier%20for%20both%20humans%20and%0Amachines%20to%20tell%20between%20good%20and%20bad%20examples%20than%20to%20generate%20high-quality%0Asamples%2C%20we%20investigate%20the%20use%20of%20feedback%20on%20synthesized%20data%20to%20prevent%0Amodel%20collapse.%20We%20derive%20theoretical%20conditions%20under%20which%20a%20Gaussian%20mixture%0Aclassification%20model%20can%20achieve%20asymptotically%20optimal%20performance%20when%0Atrained%20on%20feedback-augmented%20synthesized%20data%2C%20and%20provide%20supporting%0Asimulations%20for%20finite%20regimes.%20We%20illustrate%20our%20theoretical%20predictions%20on%0Atwo%20practical%20problems%3A%20computing%20matrix%20eigenvalues%20with%20transformers%20and%20news%0Asummarization%20with%20large%20language%20models%2C%20which%20both%20undergo%20model%20collapse%0Awhen%20trained%20on%20model-generated%20data.%20We%20show%20that%20training%20from%0Afeedback-augmented%20synthesized%20data%2C%20either%20by%20pruning%20incorrect%20predictions%20or%0Aby%20selecting%20the%20best%20of%20several%20guesses%2C%20can%20prevent%20model%20collapse%2C%0Avalidating%20popular%20approaches%20like%20RLHF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07515v1&entry.124074799=Read"},
{"title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language\n  Production from Text", "author": "Aoxiong Yin and Haoyuan Li and Kai Shen and Siliang Tang and Yueting Zhuang", "abstract": "  In this work, we propose a two-stage sign language production (SLP) paradigm\nthat first encodes sign language sequences into discrete codes and then\nautoregressively generates sign language from text based on the learned\ncodebook. However, existing vector quantization (VQ) methods are fixed-length\nencodings, overlooking the uneven information density in sign language, which\nleads to under-encoding of important regions and over-encoding of unimportant\nregions. To address this issue, we propose a novel dynamic vector quantization\n(DVA-VAE) model that can dynamically adjust the encoding length based on the\ninformation density in sign language to achieve accurate and compact encoding.\nThen, a GPT-like model learns to generate code sequences and their\ncorresponding durations from spoken language text. Extensive experiments\nconducted on the PHOENIX14T dataset demonstrate the effectiveness of our\nproposed method. To promote sign language research, we propose a new large\nGerman sign language dataset, PHOENIX-News, which contains 486 hours of sign\nlanguage videos, audio, and transcription texts.Experimental analysis on\nPHOENIX-News shows that the performance of our model can be further improved by\nincreasing the size of the training data. Our project homepage is\nhttps://t2sgpt-demo.yinaoxiong.cn.\n", "link": "http://arxiv.org/abs/2406.07119v1", "date": "2024-06-11", "relevancy": 2.0946, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.543}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5105}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2S-GPT%3A%20Dynamic%20Vector%20Quantization%20for%20Autoregressive%20Sign%20Language%0A%20%20Production%20from%20Text&body=Title%3A%20T2S-GPT%3A%20Dynamic%20Vector%20Quantization%20for%20Autoregressive%20Sign%20Language%0A%20%20Production%20from%20Text%0AAuthor%3A%20Aoxiong%20Yin%20and%20Haoyuan%20Li%20and%20Kai%20Shen%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20two-stage%20sign%20language%20production%20%28SLP%29%20paradigm%0Athat%20first%20encodes%20sign%20language%20sequences%20into%20discrete%20codes%20and%20then%0Aautoregressively%20generates%20sign%20language%20from%20text%20based%20on%20the%20learned%0Acodebook.%20However%2C%20existing%20vector%20quantization%20%28VQ%29%20methods%20are%20fixed-length%0Aencodings%2C%20overlooking%20the%20uneven%20information%20density%20in%20sign%20language%2C%20which%0Aleads%20to%20under-encoding%20of%20important%20regions%20and%20over-encoding%20of%20unimportant%0Aregions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20dynamic%20vector%20quantization%0A%28DVA-VAE%29%20model%20that%20can%20dynamically%20adjust%20the%20encoding%20length%20based%20on%20the%0Ainformation%20density%20in%20sign%20language%20to%20achieve%20accurate%20and%20compact%20encoding.%0AThen%2C%20a%20GPT-like%20model%20learns%20to%20generate%20code%20sequences%20and%20their%0Acorresponding%20durations%20from%20spoken%20language%20text.%20Extensive%20experiments%0Aconducted%20on%20the%20PHOENIX14T%20dataset%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20method.%20To%20promote%20sign%20language%20research%2C%20we%20propose%20a%20new%20large%0AGerman%20sign%20language%20dataset%2C%20PHOENIX-News%2C%20which%20contains%20486%20hours%20of%20sign%0Alanguage%20videos%2C%20audio%2C%20and%20transcription%20texts.Experimental%20analysis%20on%0APHOENIX-News%20shows%20that%20the%20performance%20of%20our%20model%20can%20be%20further%20improved%20by%0Aincreasing%20the%20size%20of%20the%20training%20data.%20Our%20project%20homepage%20is%0Ahttps%3A//t2sgpt-demo.yinaoxiong.cn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2S-GPT%253A%2520Dynamic%2520Vector%2520Quantization%2520for%2520Autoregressive%2520Sign%2520Language%250A%2520%2520Production%2520from%2520Text%26entry.906535625%3DAoxiong%2520Yin%2520and%2520Haoyuan%2520Li%2520and%2520Kai%2520Shen%2520and%2520Siliang%2520Tang%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520two-stage%2520sign%2520language%2520production%2520%2528SLP%2529%2520paradigm%250Athat%2520first%2520encodes%2520sign%2520language%2520sequences%2520into%2520discrete%2520codes%2520and%2520then%250Aautoregressively%2520generates%2520sign%2520language%2520from%2520text%2520based%2520on%2520the%2520learned%250Acodebook.%2520However%252C%2520existing%2520vector%2520quantization%2520%2528VQ%2529%2520methods%2520are%2520fixed-length%250Aencodings%252C%2520overlooking%2520the%2520uneven%2520information%2520density%2520in%2520sign%2520language%252C%2520which%250Aleads%2520to%2520under-encoding%2520of%2520important%2520regions%2520and%2520over-encoding%2520of%2520unimportant%250Aregions.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520dynamic%2520vector%2520quantization%250A%2528DVA-VAE%2529%2520model%2520that%2520can%2520dynamically%2520adjust%2520the%2520encoding%2520length%2520based%2520on%2520the%250Ainformation%2520density%2520in%2520sign%2520language%2520to%2520achieve%2520accurate%2520and%2520compact%2520encoding.%250AThen%252C%2520a%2520GPT-like%2520model%2520learns%2520to%2520generate%2520code%2520sequences%2520and%2520their%250Acorresponding%2520durations%2520from%2520spoken%2520language%2520text.%2520Extensive%2520experiments%250Aconducted%2520on%2520the%2520PHOENIX14T%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520method.%2520To%2520promote%2520sign%2520language%2520research%252C%2520we%2520propose%2520a%2520new%2520large%250AGerman%2520sign%2520language%2520dataset%252C%2520PHOENIX-News%252C%2520which%2520contains%2520486%2520hours%2520of%2520sign%250Alanguage%2520videos%252C%2520audio%252C%2520and%2520transcription%2520texts.Experimental%2520analysis%2520on%250APHOENIX-News%2520shows%2520that%2520the%2520performance%2520of%2520our%2520model%2520can%2520be%2520further%2520improved%2520by%250Aincreasing%2520the%2520size%2520of%2520the%2520training%2520data.%2520Our%2520project%2520homepage%2520is%250Ahttps%253A//t2sgpt-demo.yinaoxiong.cn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2S-GPT%3A%20Dynamic%20Vector%20Quantization%20for%20Autoregressive%20Sign%20Language%0A%20%20Production%20from%20Text&entry.906535625=Aoxiong%20Yin%20and%20Haoyuan%20Li%20and%20Kai%20Shen%20and%20Siliang%20Tang%20and%20Yueting%20Zhuang&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20two-stage%20sign%20language%20production%20%28SLP%29%20paradigm%0Athat%20first%20encodes%20sign%20language%20sequences%20into%20discrete%20codes%20and%20then%0Aautoregressively%20generates%20sign%20language%20from%20text%20based%20on%20the%20learned%0Acodebook.%20However%2C%20existing%20vector%20quantization%20%28VQ%29%20methods%20are%20fixed-length%0Aencodings%2C%20overlooking%20the%20uneven%20information%20density%20in%20sign%20language%2C%20which%0Aleads%20to%20under-encoding%20of%20important%20regions%20and%20over-encoding%20of%20unimportant%0Aregions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20dynamic%20vector%20quantization%0A%28DVA-VAE%29%20model%20that%20can%20dynamically%20adjust%20the%20encoding%20length%20based%20on%20the%0Ainformation%20density%20in%20sign%20language%20to%20achieve%20accurate%20and%20compact%20encoding.%0AThen%2C%20a%20GPT-like%20model%20learns%20to%20generate%20code%20sequences%20and%20their%0Acorresponding%20durations%20from%20spoken%20language%20text.%20Extensive%20experiments%0Aconducted%20on%20the%20PHOENIX14T%20dataset%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20method.%20To%20promote%20sign%20language%20research%2C%20we%20propose%20a%20new%20large%0AGerman%20sign%20language%20dataset%2C%20PHOENIX-News%2C%20which%20contains%20486%20hours%20of%20sign%0Alanguage%20videos%2C%20audio%2C%20and%20transcription%20texts.Experimental%20analysis%20on%0APHOENIX-News%20shows%20that%20the%20performance%20of%20our%20model%20can%20be%20further%20improved%20by%0Aincreasing%20the%20size%20of%20the%20training%20data.%20Our%20project%20homepage%20is%0Ahttps%3A//t2sgpt-demo.yinaoxiong.cn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07119v1&entry.124074799=Read"},
{"title": "Label Alignment Regularization for Distribution Shift", "author": "Ehsan Imani and Guojun Zhang and Runjia Li and Jun Luo and Pascal Poupart and Philip H. S. Torr and Yangchen Pan", "abstract": "  Recent work has highlighted the label alignment property (LAP) in supervised\nlearning, where the vector of all labels in the dataset is mostly in the span\nof the top few singular vectors of the data matrix. Drawing inspiration from\nthis observation, we propose a regularization method for unsupervised domain\nadaptation that encourages alignment between the predictions in the target\ndomain and its top singular vectors. Unlike conventional domain adaptation\napproaches that focus on regularizing representations, we instead regularize\nthe classifier to align with the unsupervised target data, guided by the LAP in\nboth the source and target domains. Theoretical analysis demonstrates that,\nunder certain assumptions, our solution resides within the span of the top\nright singular vectors of the target domain data and aligns with the optimal\nsolution. By removing the reliance on the commonly used optimal joint risk\nassumption found in classic domain adaptation theory, we showcase the\neffectiveness of our method on addressing problems where traditional domain\nadaptation methods often fall short due to high joint error. Additionally, we\nreport improved performance over domain adaptation baselines in well-known\ntasks such as MNIST-USPS domain adaptation and cross-lingual sentiment\nanalysis.\n", "link": "http://arxiv.org/abs/2211.14960v4", "date": "2024-06-11", "relevancy": 2.0945, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5413}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5125}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label%20Alignment%20Regularization%20for%20Distribution%20Shift&body=Title%3A%20Label%20Alignment%20Regularization%20for%20Distribution%20Shift%0AAuthor%3A%20Ehsan%20Imani%20and%20Guojun%20Zhang%20and%20Runjia%20Li%20and%20Jun%20Luo%20and%20Pascal%20Poupart%20and%20Philip%20H.%20S.%20Torr%20and%20Yangchen%20Pan%0AAbstract%3A%20%20%20Recent%20work%20has%20highlighted%20the%20label%20alignment%20property%20%28LAP%29%20in%20supervised%0Alearning%2C%20where%20the%20vector%20of%20all%20labels%20in%20the%20dataset%20is%20mostly%20in%20the%20span%0Aof%20the%20top%20few%20singular%20vectors%20of%20the%20data%20matrix.%20Drawing%20inspiration%20from%0Athis%20observation%2C%20we%20propose%20a%20regularization%20method%20for%20unsupervised%20domain%0Aadaptation%20that%20encourages%20alignment%20between%20the%20predictions%20in%20the%20target%0Adomain%20and%20its%20top%20singular%20vectors.%20Unlike%20conventional%20domain%20adaptation%0Aapproaches%20that%20focus%20on%20regularizing%20representations%2C%20we%20instead%20regularize%0Athe%20classifier%20to%20align%20with%20the%20unsupervised%20target%20data%2C%20guided%20by%20the%20LAP%20in%0Aboth%20the%20source%20and%20target%20domains.%20Theoretical%20analysis%20demonstrates%20that%2C%0Aunder%20certain%20assumptions%2C%20our%20solution%20resides%20within%20the%20span%20of%20the%20top%0Aright%20singular%20vectors%20of%20the%20target%20domain%20data%20and%20aligns%20with%20the%20optimal%0Asolution.%20By%20removing%20the%20reliance%20on%20the%20commonly%20used%20optimal%20joint%20risk%0Aassumption%20found%20in%20classic%20domain%20adaptation%20theory%2C%20we%20showcase%20the%0Aeffectiveness%20of%20our%20method%20on%20addressing%20problems%20where%20traditional%20domain%0Aadaptation%20methods%20often%20fall%20short%20due%20to%20high%20joint%20error.%20Additionally%2C%20we%0Areport%20improved%20performance%20over%20domain%20adaptation%20baselines%20in%20well-known%0Atasks%20such%20as%20MNIST-USPS%20domain%20adaptation%20and%20cross-lingual%20sentiment%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.14960v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel%2520Alignment%2520Regularization%2520for%2520Distribution%2520Shift%26entry.906535625%3DEhsan%2520Imani%2520and%2520Guojun%2520Zhang%2520and%2520Runjia%2520Li%2520and%2520Jun%2520Luo%2520and%2520Pascal%2520Poupart%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Yangchen%2520Pan%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520highlighted%2520the%2520label%2520alignment%2520property%2520%2528LAP%2529%2520in%2520supervised%250Alearning%252C%2520where%2520the%2520vector%2520of%2520all%2520labels%2520in%2520the%2520dataset%2520is%2520mostly%2520in%2520the%2520span%250Aof%2520the%2520top%2520few%2520singular%2520vectors%2520of%2520the%2520data%2520matrix.%2520Drawing%2520inspiration%2520from%250Athis%2520observation%252C%2520we%2520propose%2520a%2520regularization%2520method%2520for%2520unsupervised%2520domain%250Aadaptation%2520that%2520encourages%2520alignment%2520between%2520the%2520predictions%2520in%2520the%2520target%250Adomain%2520and%2520its%2520top%2520singular%2520vectors.%2520Unlike%2520conventional%2520domain%2520adaptation%250Aapproaches%2520that%2520focus%2520on%2520regularizing%2520representations%252C%2520we%2520instead%2520regularize%250Athe%2520classifier%2520to%2520align%2520with%2520the%2520unsupervised%2520target%2520data%252C%2520guided%2520by%2520the%2520LAP%2520in%250Aboth%2520the%2520source%2520and%2520target%2520domains.%2520Theoretical%2520analysis%2520demonstrates%2520that%252C%250Aunder%2520certain%2520assumptions%252C%2520our%2520solution%2520resides%2520within%2520the%2520span%2520of%2520the%2520top%250Aright%2520singular%2520vectors%2520of%2520the%2520target%2520domain%2520data%2520and%2520aligns%2520with%2520the%2520optimal%250Asolution.%2520By%2520removing%2520the%2520reliance%2520on%2520the%2520commonly%2520used%2520optimal%2520joint%2520risk%250Aassumption%2520found%2520in%2520classic%2520domain%2520adaptation%2520theory%252C%2520we%2520showcase%2520the%250Aeffectiveness%2520of%2520our%2520method%2520on%2520addressing%2520problems%2520where%2520traditional%2520domain%250Aadaptation%2520methods%2520often%2520fall%2520short%2520due%2520to%2520high%2520joint%2520error.%2520Additionally%252C%2520we%250Areport%2520improved%2520performance%2520over%2520domain%2520adaptation%2520baselines%2520in%2520well-known%250Atasks%2520such%2520as%2520MNIST-USPS%2520domain%2520adaptation%2520and%2520cross-lingual%2520sentiment%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.14960v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Alignment%20Regularization%20for%20Distribution%20Shift&entry.906535625=Ehsan%20Imani%20and%20Guojun%20Zhang%20and%20Runjia%20Li%20and%20Jun%20Luo%20and%20Pascal%20Poupart%20and%20Philip%20H.%20S.%20Torr%20and%20Yangchen%20Pan&entry.1292438233=%20%20Recent%20work%20has%20highlighted%20the%20label%20alignment%20property%20%28LAP%29%20in%20supervised%0Alearning%2C%20where%20the%20vector%20of%20all%20labels%20in%20the%20dataset%20is%20mostly%20in%20the%20span%0Aof%20the%20top%20few%20singular%20vectors%20of%20the%20data%20matrix.%20Drawing%20inspiration%20from%0Athis%20observation%2C%20we%20propose%20a%20regularization%20method%20for%20unsupervised%20domain%0Aadaptation%20that%20encourages%20alignment%20between%20the%20predictions%20in%20the%20target%0Adomain%20and%20its%20top%20singular%20vectors.%20Unlike%20conventional%20domain%20adaptation%0Aapproaches%20that%20focus%20on%20regularizing%20representations%2C%20we%20instead%20regularize%0Athe%20classifier%20to%20align%20with%20the%20unsupervised%20target%20data%2C%20guided%20by%20the%20LAP%20in%0Aboth%20the%20source%20and%20target%20domains.%20Theoretical%20analysis%20demonstrates%20that%2C%0Aunder%20certain%20assumptions%2C%20our%20solution%20resides%20within%20the%20span%20of%20the%20top%0Aright%20singular%20vectors%20of%20the%20target%20domain%20data%20and%20aligns%20with%20the%20optimal%0Asolution.%20By%20removing%20the%20reliance%20on%20the%20commonly%20used%20optimal%20joint%20risk%0Aassumption%20found%20in%20classic%20domain%20adaptation%20theory%2C%20we%20showcase%20the%0Aeffectiveness%20of%20our%20method%20on%20addressing%20problems%20where%20traditional%20domain%0Aadaptation%20methods%20often%20fall%20short%20due%20to%20high%20joint%20error.%20Additionally%2C%20we%0Areport%20improved%20performance%20over%20domain%20adaptation%20baselines%20in%20well-known%0Atasks%20such%20as%20MNIST-USPS%20domain%20adaptation%20and%20cross-lingual%20sentiment%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.14960v4&entry.124074799=Read"},
{"title": "iMotion-LLM: Motion Prediction Instruction Tuning", "author": "Abdulwahab Felemban and Eslam Mohamed Bakr and Xiaoqian Shen and Jian Ding and Abduallah Mohamed and Mohamed Elhoseiny", "abstract": "  We introduce iMotion-LLM: a Multimodal Large Language Models (LLMs) with\ntrajectory prediction, tailored to guide interactive multi-agent scenarios.\nDifferent from conventional motion prediction approaches, iMotion-LLM\ncapitalizes on textual instructions as key inputs for generating contextually\nrelevant trajectories. By enriching the real-world driving scenarios in the\nWaymo Open Dataset with textual motion instructions, we created InstructWaymo.\nLeveraging this dataset, iMotion-LLM integrates a pretrained LLM, fine-tuned\nwith LoRA, to translate scene features into the LLM input space. iMotion-LLM\noffers significant advantages over conventional motion prediction models.\nFirst, it can generate trajectories that align with the provided instructions\nif it is a feasible direction. Second, when given an infeasible direction, it\ncan reject the instruction, thereby enhancing safety. These findings act as\nmilestones in empowering autonomous navigation systems to interpret and predict\nthe dynamics of multi-agent environments, laying the groundwork for future\nadvancements in this field.\n", "link": "http://arxiv.org/abs/2406.06211v2", "date": "2024-06-11", "relevancy": 2.0903, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5899}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5118}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iMotion-LLM%3A%20Motion%20Prediction%20Instruction%20Tuning&body=Title%3A%20iMotion-LLM%3A%20Motion%20Prediction%20Instruction%20Tuning%0AAuthor%3A%20Abdulwahab%20Felemban%20and%20Eslam%20Mohamed%20Bakr%20and%20Xiaoqian%20Shen%20and%20Jian%20Ding%20and%20Abduallah%20Mohamed%20and%20Mohamed%20Elhoseiny%0AAbstract%3A%20%20%20We%20introduce%20iMotion-LLM%3A%20a%20Multimodal%20Large%20Language%20Models%20%28LLMs%29%20with%0Atrajectory%20prediction%2C%20tailored%20to%20guide%20interactive%20multi-agent%20scenarios.%0ADifferent%20from%20conventional%20motion%20prediction%20approaches%2C%20iMotion-LLM%0Acapitalizes%20on%20textual%20instructions%20as%20key%20inputs%20for%20generating%20contextually%0Arelevant%20trajectories.%20By%20enriching%20the%20real-world%20driving%20scenarios%20in%20the%0AWaymo%20Open%20Dataset%20with%20textual%20motion%20instructions%2C%20we%20created%20InstructWaymo.%0ALeveraging%20this%20dataset%2C%20iMotion-LLM%20integrates%20a%20pretrained%20LLM%2C%20fine-tuned%0Awith%20LoRA%2C%20to%20translate%20scene%20features%20into%20the%20LLM%20input%20space.%20iMotion-LLM%0Aoffers%20significant%20advantages%20over%20conventional%20motion%20prediction%20models.%0AFirst%2C%20it%20can%20generate%20trajectories%20that%20align%20with%20the%20provided%20instructions%0Aif%20it%20is%20a%20feasible%20direction.%20Second%2C%20when%20given%20an%20infeasible%20direction%2C%20it%0Acan%20reject%20the%20instruction%2C%20thereby%20enhancing%20safety.%20These%20findings%20act%20as%0Amilestones%20in%20empowering%20autonomous%20navigation%20systems%20to%20interpret%20and%20predict%0Athe%20dynamics%20of%20multi-agent%20environments%2C%20laying%20the%20groundwork%20for%20future%0Aadvancements%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06211v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiMotion-LLM%253A%2520Motion%2520Prediction%2520Instruction%2520Tuning%26entry.906535625%3DAbdulwahab%2520Felemban%2520and%2520Eslam%2520Mohamed%2520Bakr%2520and%2520Xiaoqian%2520Shen%2520and%2520Jian%2520Ding%2520and%2520Abduallah%2520Mohamed%2520and%2520Mohamed%2520Elhoseiny%26entry.1292438233%3D%2520%2520We%2520introduce%2520iMotion-LLM%253A%2520a%2520Multimodal%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%250Atrajectory%2520prediction%252C%2520tailored%2520to%2520guide%2520interactive%2520multi-agent%2520scenarios.%250ADifferent%2520from%2520conventional%2520motion%2520prediction%2520approaches%252C%2520iMotion-LLM%250Acapitalizes%2520on%2520textual%2520instructions%2520as%2520key%2520inputs%2520for%2520generating%2520contextually%250Arelevant%2520trajectories.%2520By%2520enriching%2520the%2520real-world%2520driving%2520scenarios%2520in%2520the%250AWaymo%2520Open%2520Dataset%2520with%2520textual%2520motion%2520instructions%252C%2520we%2520created%2520InstructWaymo.%250ALeveraging%2520this%2520dataset%252C%2520iMotion-LLM%2520integrates%2520a%2520pretrained%2520LLM%252C%2520fine-tuned%250Awith%2520LoRA%252C%2520to%2520translate%2520scene%2520features%2520into%2520the%2520LLM%2520input%2520space.%2520iMotion-LLM%250Aoffers%2520significant%2520advantages%2520over%2520conventional%2520motion%2520prediction%2520models.%250AFirst%252C%2520it%2520can%2520generate%2520trajectories%2520that%2520align%2520with%2520the%2520provided%2520instructions%250Aif%2520it%2520is%2520a%2520feasible%2520direction.%2520Second%252C%2520when%2520given%2520an%2520infeasible%2520direction%252C%2520it%250Acan%2520reject%2520the%2520instruction%252C%2520thereby%2520enhancing%2520safety.%2520These%2520findings%2520act%2520as%250Amilestones%2520in%2520empowering%2520autonomous%2520navigation%2520systems%2520to%2520interpret%2520and%2520predict%250Athe%2520dynamics%2520of%2520multi-agent%2520environments%252C%2520laying%2520the%2520groundwork%2520for%2520future%250Aadvancements%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06211v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iMotion-LLM%3A%20Motion%20Prediction%20Instruction%20Tuning&entry.906535625=Abdulwahab%20Felemban%20and%20Eslam%20Mohamed%20Bakr%20and%20Xiaoqian%20Shen%20and%20Jian%20Ding%20and%20Abduallah%20Mohamed%20and%20Mohamed%20Elhoseiny&entry.1292438233=%20%20We%20introduce%20iMotion-LLM%3A%20a%20Multimodal%20Large%20Language%20Models%20%28LLMs%29%20with%0Atrajectory%20prediction%2C%20tailored%20to%20guide%20interactive%20multi-agent%20scenarios.%0ADifferent%20from%20conventional%20motion%20prediction%20approaches%2C%20iMotion-LLM%0Acapitalizes%20on%20textual%20instructions%20as%20key%20inputs%20for%20generating%20contextually%0Arelevant%20trajectories.%20By%20enriching%20the%20real-world%20driving%20scenarios%20in%20the%0AWaymo%20Open%20Dataset%20with%20textual%20motion%20instructions%2C%20we%20created%20InstructWaymo.%0ALeveraging%20this%20dataset%2C%20iMotion-LLM%20integrates%20a%20pretrained%20LLM%2C%20fine-tuned%0Awith%20LoRA%2C%20to%20translate%20scene%20features%20into%20the%20LLM%20input%20space.%20iMotion-LLM%0Aoffers%20significant%20advantages%20over%20conventional%20motion%20prediction%20models.%0AFirst%2C%20it%20can%20generate%20trajectories%20that%20align%20with%20the%20provided%20instructions%0Aif%20it%20is%20a%20feasible%20direction.%20Second%2C%20when%20given%20an%20infeasible%20direction%2C%20it%0Acan%20reject%20the%20instruction%2C%20thereby%20enhancing%20safety.%20These%20findings%20act%20as%0Amilestones%20in%20empowering%20autonomous%20navigation%20systems%20to%20interpret%20and%20predict%0Athe%20dynamics%20of%20multi-agent%20environments%2C%20laying%20the%20groundwork%20for%20future%0Aadvancements%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06211v2&entry.124074799=Read"},
{"title": "Efficient 3D Molecular Generation with Flow Matching and Scale Optimal\n  Transport", "author": "Ross Irwin and Alessandro Tibo and Jon-Paul Janet and Simon Olsson", "abstract": "  Generative models for 3D drug design have gained prominence recently for\ntheir potential to design ligands directly within protein pockets. Current\napproaches, however, often suffer from very slow sampling times or generate\nmolecules with poor chemical validity. Addressing these limitations, we propose\nSemla, a scalable E(3)-equivariant message passing architecture. We further\nintroduce a molecular generation model, MolFlow, which is trained using flow\nmatching along with scale optimal transport, a novel extension of equivariant\noptimal transport. Our model produces state-of-the-art results on benchmark\ndatasets with just 100 sampling steps. Crucially, MolFlow samples high quality\nmolecules with as few as 20 steps, corresponding to a two order-of-magnitude\nspeed-up compared to state-of-the-art, without sacrificing performance.\nFurthermore, we highlight limitations of current evaluation methods for 3D\ngeneration and propose new benchmark metrics for unconditional molecular\ngenerators. Finally, using these new metrics, we compare our model's ability to\ngenerate high quality samples against current approaches and further\ndemonstrate MolFlow's strong performance.\n", "link": "http://arxiv.org/abs/2406.07266v1", "date": "2024-06-11", "relevancy": 2.0782, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5754}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5084}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%203D%20Molecular%20Generation%20with%20Flow%20Matching%20and%20Scale%20Optimal%0A%20%20Transport&body=Title%3A%20Efficient%203D%20Molecular%20Generation%20with%20Flow%20Matching%20and%20Scale%20Optimal%0A%20%20Transport%0AAuthor%3A%20Ross%20Irwin%20and%20Alessandro%20Tibo%20and%20Jon-Paul%20Janet%20and%20Simon%20Olsson%0AAbstract%3A%20%20%20Generative%20models%20for%203D%20drug%20design%20have%20gained%20prominence%20recently%20for%0Atheir%20potential%20to%20design%20ligands%20directly%20within%20protein%20pockets.%20Current%0Aapproaches%2C%20however%2C%20often%20suffer%20from%20very%20slow%20sampling%20times%20or%20generate%0Amolecules%20with%20poor%20chemical%20validity.%20Addressing%20these%20limitations%2C%20we%20propose%0ASemla%2C%20a%20scalable%20E%283%29-equivariant%20message%20passing%20architecture.%20We%20further%0Aintroduce%20a%20molecular%20generation%20model%2C%20MolFlow%2C%20which%20is%20trained%20using%20flow%0Amatching%20along%20with%20scale%20optimal%20transport%2C%20a%20novel%20extension%20of%20equivariant%0Aoptimal%20transport.%20Our%20model%20produces%20state-of-the-art%20results%20on%20benchmark%0Adatasets%20with%20just%20100%20sampling%20steps.%20Crucially%2C%20MolFlow%20samples%20high%20quality%0Amolecules%20with%20as%20few%20as%2020%20steps%2C%20corresponding%20to%20a%20two%20order-of-magnitude%0Aspeed-up%20compared%20to%20state-of-the-art%2C%20without%20sacrificing%20performance.%0AFurthermore%2C%20we%20highlight%20limitations%20of%20current%20evaluation%20methods%20for%203D%0Ageneration%20and%20propose%20new%20benchmark%20metrics%20for%20unconditional%20molecular%0Agenerators.%20Finally%2C%20using%20these%20new%20metrics%2C%20we%20compare%20our%20model%27s%20ability%20to%0Agenerate%20high%20quality%20samples%20against%20current%20approaches%20and%20further%0Ademonstrate%20MolFlow%27s%20strong%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%25203D%2520Molecular%2520Generation%2520with%2520Flow%2520Matching%2520and%2520Scale%2520Optimal%250A%2520%2520Transport%26entry.906535625%3DRoss%2520Irwin%2520and%2520Alessandro%2520Tibo%2520and%2520Jon-Paul%2520Janet%2520and%2520Simon%2520Olsson%26entry.1292438233%3D%2520%2520Generative%2520models%2520for%25203D%2520drug%2520design%2520have%2520gained%2520prominence%2520recently%2520for%250Atheir%2520potential%2520to%2520design%2520ligands%2520directly%2520within%2520protein%2520pockets.%2520Current%250Aapproaches%252C%2520however%252C%2520often%2520suffer%2520from%2520very%2520slow%2520sampling%2520times%2520or%2520generate%250Amolecules%2520with%2520poor%2520chemical%2520validity.%2520Addressing%2520these%2520limitations%252C%2520we%2520propose%250ASemla%252C%2520a%2520scalable%2520E%25283%2529-equivariant%2520message%2520passing%2520architecture.%2520We%2520further%250Aintroduce%2520a%2520molecular%2520generation%2520model%252C%2520MolFlow%252C%2520which%2520is%2520trained%2520using%2520flow%250Amatching%2520along%2520with%2520scale%2520optimal%2520transport%252C%2520a%2520novel%2520extension%2520of%2520equivariant%250Aoptimal%2520transport.%2520Our%2520model%2520produces%2520state-of-the-art%2520results%2520on%2520benchmark%250Adatasets%2520with%2520just%2520100%2520sampling%2520steps.%2520Crucially%252C%2520MolFlow%2520samples%2520high%2520quality%250Amolecules%2520with%2520as%2520few%2520as%252020%2520steps%252C%2520corresponding%2520to%2520a%2520two%2520order-of-magnitude%250Aspeed-up%2520compared%2520to%2520state-of-the-art%252C%2520without%2520sacrificing%2520performance.%250AFurthermore%252C%2520we%2520highlight%2520limitations%2520of%2520current%2520evaluation%2520methods%2520for%25203D%250Ageneration%2520and%2520propose%2520new%2520benchmark%2520metrics%2520for%2520unconditional%2520molecular%250Agenerators.%2520Finally%252C%2520using%2520these%2520new%2520metrics%252C%2520we%2520compare%2520our%2520model%2527s%2520ability%2520to%250Agenerate%2520high%2520quality%2520samples%2520against%2520current%2520approaches%2520and%2520further%250Ademonstrate%2520MolFlow%2527s%2520strong%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%203D%20Molecular%20Generation%20with%20Flow%20Matching%20and%20Scale%20Optimal%0A%20%20Transport&entry.906535625=Ross%20Irwin%20and%20Alessandro%20Tibo%20and%20Jon-Paul%20Janet%20and%20Simon%20Olsson&entry.1292438233=%20%20Generative%20models%20for%203D%20drug%20design%20have%20gained%20prominence%20recently%20for%0Atheir%20potential%20to%20design%20ligands%20directly%20within%20protein%20pockets.%20Current%0Aapproaches%2C%20however%2C%20often%20suffer%20from%20very%20slow%20sampling%20times%20or%20generate%0Amolecules%20with%20poor%20chemical%20validity.%20Addressing%20these%20limitations%2C%20we%20propose%0ASemla%2C%20a%20scalable%20E%283%29-equivariant%20message%20passing%20architecture.%20We%20further%0Aintroduce%20a%20molecular%20generation%20model%2C%20MolFlow%2C%20which%20is%20trained%20using%20flow%0Amatching%20along%20with%20scale%20optimal%20transport%2C%20a%20novel%20extension%20of%20equivariant%0Aoptimal%20transport.%20Our%20model%20produces%20state-of-the-art%20results%20on%20benchmark%0Adatasets%20with%20just%20100%20sampling%20steps.%20Crucially%2C%20MolFlow%20samples%20high%20quality%0Amolecules%20with%20as%20few%20as%2020%20steps%2C%20corresponding%20to%20a%20two%20order-of-magnitude%0Aspeed-up%20compared%20to%20state-of-the-art%2C%20without%20sacrificing%20performance.%0AFurthermore%2C%20we%20highlight%20limitations%20of%20current%20evaluation%20methods%20for%203D%0Ageneration%20and%20propose%20new%20benchmark%20metrics%20for%20unconditional%20molecular%0Agenerators.%20Finally%2C%20using%20these%20new%20metrics%2C%20we%20compare%20our%20model%27s%20ability%20to%0Agenerate%20high%20quality%20samples%20against%20current%20approaches%20and%20further%0Ademonstrate%20MolFlow%27s%20strong%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07266v1&entry.124074799=Read"},
{"title": "Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of\n  Agents", "author": "Marco Pleines and Matthias Pallasch and Frank Zimmer and Mike Preuss", "abstract": "  Memory Gym presents a suite of 2D partially observable environments, namely\nMortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark\nmemory capabilities in decision-making agents. These environments, originally\nwith finite tasks, are expanded into innovative, endless formats, mirroring the\nescalating challenges of cumulative memory games such as ``I packed my bag''.\nThis progression in task design shifts the focus from merely assessing sample\nefficiency to also probing the levels of memory effectiveness in dynamic,\nprolonged scenarios. To address the gap in available memory-based Deep\nReinforcement Learning baselines, we introduce an implementation that\nintegrates Transformer-XL (TrXL) with Proximal Policy Optimization. This\napproach utilizes TrXL as a form of episodic memory, employing a sliding window\ntechnique. Our comparative study between the Gated Recurrent Unit (GRU) and\nTrXL reveals varied performances across different settings. TrXL, on the finite\nenvironments, demonstrates superior sample efficiency in Mystery Path and\noutperforms in Mortar Mayhem. However, GRU is more efficient on Searing\nSpotlights. Most notably, in all endless tasks, GRU makes a remarkable\nresurgence, consistently outperforming TrXL by significant margins. Website and\nSource Code: https://github.com/MarcoMeter/endless-memory-gym/\n", "link": "http://arxiv.org/abs/2309.17207v4", "date": "2024-06-11", "relevancy": 2.0746, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.554}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5251}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Gym%3A%20Towards%20Endless%20Tasks%20to%20Benchmark%20Memory%20Capabilities%20of%0A%20%20Agents&body=Title%3A%20Memory%20Gym%3A%20Towards%20Endless%20Tasks%20to%20Benchmark%20Memory%20Capabilities%20of%0A%20%20Agents%0AAuthor%3A%20Marco%20Pleines%20and%20Matthias%20Pallasch%20and%20Frank%20Zimmer%20and%20Mike%20Preuss%0AAbstract%3A%20%20%20Memory%20Gym%20presents%20a%20suite%20of%202D%20partially%20observable%20environments%2C%20namely%0AMortar%20Mayhem%2C%20Mystery%20Path%2C%20and%20Searing%20Spotlights%2C%20designed%20to%20benchmark%0Amemory%20capabilities%20in%20decision-making%20agents.%20These%20environments%2C%20originally%0Awith%20finite%20tasks%2C%20are%20expanded%20into%20innovative%2C%20endless%20formats%2C%20mirroring%20the%0Aescalating%20challenges%20of%20cumulative%20memory%20games%20such%20as%20%60%60I%20packed%20my%20bag%27%27.%0AThis%20progression%20in%20task%20design%20shifts%20the%20focus%20from%20merely%20assessing%20sample%0Aefficiency%20to%20also%20probing%20the%20levels%20of%20memory%20effectiveness%20in%20dynamic%2C%0Aprolonged%20scenarios.%20To%20address%20the%20gap%20in%20available%20memory-based%20Deep%0AReinforcement%20Learning%20baselines%2C%20we%20introduce%20an%20implementation%20that%0Aintegrates%20Transformer-XL%20%28TrXL%29%20with%20Proximal%20Policy%20Optimization.%20This%0Aapproach%20utilizes%20TrXL%20as%20a%20form%20of%20episodic%20memory%2C%20employing%20a%20sliding%20window%0Atechnique.%20Our%20comparative%20study%20between%20the%20Gated%20Recurrent%20Unit%20%28GRU%29%20and%0ATrXL%20reveals%20varied%20performances%20across%20different%20settings.%20TrXL%2C%20on%20the%20finite%0Aenvironments%2C%20demonstrates%20superior%20sample%20efficiency%20in%20Mystery%20Path%20and%0Aoutperforms%20in%20Mortar%20Mayhem.%20However%2C%20GRU%20is%20more%20efficient%20on%20Searing%0ASpotlights.%20Most%20notably%2C%20in%20all%20endless%20tasks%2C%20GRU%20makes%20a%20remarkable%0Aresurgence%2C%20consistently%20outperforming%20TrXL%20by%20significant%20margins.%20Website%20and%0ASource%20Code%3A%20https%3A//github.com/MarcoMeter/endless-memory-gym/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17207v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Gym%253A%2520Towards%2520Endless%2520Tasks%2520to%2520Benchmark%2520Memory%2520Capabilities%2520of%250A%2520%2520Agents%26entry.906535625%3DMarco%2520Pleines%2520and%2520Matthias%2520Pallasch%2520and%2520Frank%2520Zimmer%2520and%2520Mike%2520Preuss%26entry.1292438233%3D%2520%2520Memory%2520Gym%2520presents%2520a%2520suite%2520of%25202D%2520partially%2520observable%2520environments%252C%2520namely%250AMortar%2520Mayhem%252C%2520Mystery%2520Path%252C%2520and%2520Searing%2520Spotlights%252C%2520designed%2520to%2520benchmark%250Amemory%2520capabilities%2520in%2520decision-making%2520agents.%2520These%2520environments%252C%2520originally%250Awith%2520finite%2520tasks%252C%2520are%2520expanded%2520into%2520innovative%252C%2520endless%2520formats%252C%2520mirroring%2520the%250Aescalating%2520challenges%2520of%2520cumulative%2520memory%2520games%2520such%2520as%2520%2560%2560I%2520packed%2520my%2520bag%2527%2527.%250AThis%2520progression%2520in%2520task%2520design%2520shifts%2520the%2520focus%2520from%2520merely%2520assessing%2520sample%250Aefficiency%2520to%2520also%2520probing%2520the%2520levels%2520of%2520memory%2520effectiveness%2520in%2520dynamic%252C%250Aprolonged%2520scenarios.%2520To%2520address%2520the%2520gap%2520in%2520available%2520memory-based%2520Deep%250AReinforcement%2520Learning%2520baselines%252C%2520we%2520introduce%2520an%2520implementation%2520that%250Aintegrates%2520Transformer-XL%2520%2528TrXL%2529%2520with%2520Proximal%2520Policy%2520Optimization.%2520This%250Aapproach%2520utilizes%2520TrXL%2520as%2520a%2520form%2520of%2520episodic%2520memory%252C%2520employing%2520a%2520sliding%2520window%250Atechnique.%2520Our%2520comparative%2520study%2520between%2520the%2520Gated%2520Recurrent%2520Unit%2520%2528GRU%2529%2520and%250ATrXL%2520reveals%2520varied%2520performances%2520across%2520different%2520settings.%2520TrXL%252C%2520on%2520the%2520finite%250Aenvironments%252C%2520demonstrates%2520superior%2520sample%2520efficiency%2520in%2520Mystery%2520Path%2520and%250Aoutperforms%2520in%2520Mortar%2520Mayhem.%2520However%252C%2520GRU%2520is%2520more%2520efficient%2520on%2520Searing%250ASpotlights.%2520Most%2520notably%252C%2520in%2520all%2520endless%2520tasks%252C%2520GRU%2520makes%2520a%2520remarkable%250Aresurgence%252C%2520consistently%2520outperforming%2520TrXL%2520by%2520significant%2520margins.%2520Website%2520and%250ASource%2520Code%253A%2520https%253A//github.com/MarcoMeter/endless-memory-gym/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17207v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Gym%3A%20Towards%20Endless%20Tasks%20to%20Benchmark%20Memory%20Capabilities%20of%0A%20%20Agents&entry.906535625=Marco%20Pleines%20and%20Matthias%20Pallasch%20and%20Frank%20Zimmer%20and%20Mike%20Preuss&entry.1292438233=%20%20Memory%20Gym%20presents%20a%20suite%20of%202D%20partially%20observable%20environments%2C%20namely%0AMortar%20Mayhem%2C%20Mystery%20Path%2C%20and%20Searing%20Spotlights%2C%20designed%20to%20benchmark%0Amemory%20capabilities%20in%20decision-making%20agents.%20These%20environments%2C%20originally%0Awith%20finite%20tasks%2C%20are%20expanded%20into%20innovative%2C%20endless%20formats%2C%20mirroring%20the%0Aescalating%20challenges%20of%20cumulative%20memory%20games%20such%20as%20%60%60I%20packed%20my%20bag%27%27.%0AThis%20progression%20in%20task%20design%20shifts%20the%20focus%20from%20merely%20assessing%20sample%0Aefficiency%20to%20also%20probing%20the%20levels%20of%20memory%20effectiveness%20in%20dynamic%2C%0Aprolonged%20scenarios.%20To%20address%20the%20gap%20in%20available%20memory-based%20Deep%0AReinforcement%20Learning%20baselines%2C%20we%20introduce%20an%20implementation%20that%0Aintegrates%20Transformer-XL%20%28TrXL%29%20with%20Proximal%20Policy%20Optimization.%20This%0Aapproach%20utilizes%20TrXL%20as%20a%20form%20of%20episodic%20memory%2C%20employing%20a%20sliding%20window%0Atechnique.%20Our%20comparative%20study%20between%20the%20Gated%20Recurrent%20Unit%20%28GRU%29%20and%0ATrXL%20reveals%20varied%20performances%20across%20different%20settings.%20TrXL%2C%20on%20the%20finite%0Aenvironments%2C%20demonstrates%20superior%20sample%20efficiency%20in%20Mystery%20Path%20and%0Aoutperforms%20in%20Mortar%20Mayhem.%20However%2C%20GRU%20is%20more%20efficient%20on%20Searing%0ASpotlights.%20Most%20notably%2C%20in%20all%20endless%20tasks%2C%20GRU%20makes%20a%20remarkable%0Aresurgence%2C%20consistently%20outperforming%20TrXL%20by%20significant%20margins.%20Website%20and%0ASource%20Code%3A%20https%3A//github.com/MarcoMeter/endless-memory-gym/%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17207v4&entry.124074799=Read"},
{"title": "Improving the realism of robotic surgery simulation through injection of\n  learning-based estimated errors", "author": "Juan Antonio Barragan and Hisashi Ishida and Adnan Munawar and Peter Kazanzides", "abstract": "  The development of algorithms for automation of subtasks during robotic\nsurgery can be accelerated by the availability of realistic simulation\nenvironments. In this work, we focus on one aspect of the realism of a surgical\nsimulator, which is the positional accuracy of the robot. In current\nsimulators, robots have perfect or near-perfect accuracy, which is not\nrepresentative of their physical counterparts. We therefore propose a pair of\nneural networks, trained by data collected from a physical robot, to estimate\nboth the controller error and the kinematic and non-kinematic error. These\nerror estimates are then injected within the simulator to produce a simulated\nrobot that has the characteristic performance of the physical robot. In this\nscenario, we believe it is sufficient for the estimated error used in the\nsimulation to have a statistically similar distribution to the actual error of\nthe physical robot. This is less stringent, and therefore more tenable, than\nthe requirement for error compensation of a physical robot, where the estimated\nerror should equal the actual error. Our results demonstrate that error\ninjection reduces the mean position and orientation differences between the\nsimulated and physical robots from 5.0 mm / 3.6 deg to 1.3 mm / 1.7 deg,\nrespectively, which represents reductions by factors of 3.8 and 2.1.\n", "link": "http://arxiv.org/abs/2406.07375v1", "date": "2024-06-11", "relevancy": 2.0693, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5744}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5091}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20realism%20of%20robotic%20surgery%20simulation%20through%20injection%20of%0A%20%20learning-based%20estimated%20errors&body=Title%3A%20Improving%20the%20realism%20of%20robotic%20surgery%20simulation%20through%20injection%20of%0A%20%20learning-based%20estimated%20errors%0AAuthor%3A%20Juan%20Antonio%20Barragan%20and%20Hisashi%20Ishida%20and%20Adnan%20Munawar%20and%20Peter%20Kazanzides%0AAbstract%3A%20%20%20The%20development%20of%20algorithms%20for%20automation%20of%20subtasks%20during%20robotic%0Asurgery%20can%20be%20accelerated%20by%20the%20availability%20of%20realistic%20simulation%0Aenvironments.%20In%20this%20work%2C%20we%20focus%20on%20one%20aspect%20of%20the%20realism%20of%20a%20surgical%0Asimulator%2C%20which%20is%20the%20positional%20accuracy%20of%20the%20robot.%20In%20current%0Asimulators%2C%20robots%20have%20perfect%20or%20near-perfect%20accuracy%2C%20which%20is%20not%0Arepresentative%20of%20their%20physical%20counterparts.%20We%20therefore%20propose%20a%20pair%20of%0Aneural%20networks%2C%20trained%20by%20data%20collected%20from%20a%20physical%20robot%2C%20to%20estimate%0Aboth%20the%20controller%20error%20and%20the%20kinematic%20and%20non-kinematic%20error.%20These%0Aerror%20estimates%20are%20then%20injected%20within%20the%20simulator%20to%20produce%20a%20simulated%0Arobot%20that%20has%20the%20characteristic%20performance%20of%20the%20physical%20robot.%20In%20this%0Ascenario%2C%20we%20believe%20it%20is%20sufficient%20for%20the%20estimated%20error%20used%20in%20the%0Asimulation%20to%20have%20a%20statistically%20similar%20distribution%20to%20the%20actual%20error%20of%0Athe%20physical%20robot.%20This%20is%20less%20stringent%2C%20and%20therefore%20more%20tenable%2C%20than%0Athe%20requirement%20for%20error%20compensation%20of%20a%20physical%20robot%2C%20where%20the%20estimated%0Aerror%20should%20equal%20the%20actual%20error.%20Our%20results%20demonstrate%20that%20error%0Ainjection%20reduces%20the%20mean%20position%20and%20orientation%20differences%20between%20the%0Asimulated%20and%20physical%20robots%20from%205.0%20mm%20/%203.6%20deg%20to%201.3%20mm%20/%201.7%20deg%2C%0Arespectively%2C%20which%20represents%20reductions%20by%20factors%20of%203.8%20and%202.1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520realism%2520of%2520robotic%2520surgery%2520simulation%2520through%2520injection%2520of%250A%2520%2520learning-based%2520estimated%2520errors%26entry.906535625%3DJuan%2520Antonio%2520Barragan%2520and%2520Hisashi%2520Ishida%2520and%2520Adnan%2520Munawar%2520and%2520Peter%2520Kazanzides%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520algorithms%2520for%2520automation%2520of%2520subtasks%2520during%2520robotic%250Asurgery%2520can%2520be%2520accelerated%2520by%2520the%2520availability%2520of%2520realistic%2520simulation%250Aenvironments.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520one%2520aspect%2520of%2520the%2520realism%2520of%2520a%2520surgical%250Asimulator%252C%2520which%2520is%2520the%2520positional%2520accuracy%2520of%2520the%2520robot.%2520In%2520current%250Asimulators%252C%2520robots%2520have%2520perfect%2520or%2520near-perfect%2520accuracy%252C%2520which%2520is%2520not%250Arepresentative%2520of%2520their%2520physical%2520counterparts.%2520We%2520therefore%2520propose%2520a%2520pair%2520of%250Aneural%2520networks%252C%2520trained%2520by%2520data%2520collected%2520from%2520a%2520physical%2520robot%252C%2520to%2520estimate%250Aboth%2520the%2520controller%2520error%2520and%2520the%2520kinematic%2520and%2520non-kinematic%2520error.%2520These%250Aerror%2520estimates%2520are%2520then%2520injected%2520within%2520the%2520simulator%2520to%2520produce%2520a%2520simulated%250Arobot%2520that%2520has%2520the%2520characteristic%2520performance%2520of%2520the%2520physical%2520robot.%2520In%2520this%250Ascenario%252C%2520we%2520believe%2520it%2520is%2520sufficient%2520for%2520the%2520estimated%2520error%2520used%2520in%2520the%250Asimulation%2520to%2520have%2520a%2520statistically%2520similar%2520distribution%2520to%2520the%2520actual%2520error%2520of%250Athe%2520physical%2520robot.%2520This%2520is%2520less%2520stringent%252C%2520and%2520therefore%2520more%2520tenable%252C%2520than%250Athe%2520requirement%2520for%2520error%2520compensation%2520of%2520a%2520physical%2520robot%252C%2520where%2520the%2520estimated%250Aerror%2520should%2520equal%2520the%2520actual%2520error.%2520Our%2520results%2520demonstrate%2520that%2520error%250Ainjection%2520reduces%2520the%2520mean%2520position%2520and%2520orientation%2520differences%2520between%2520the%250Asimulated%2520and%2520physical%2520robots%2520from%25205.0%2520mm%2520/%25203.6%2520deg%2520to%25201.3%2520mm%2520/%25201.7%2520deg%252C%250Arespectively%252C%2520which%2520represents%2520reductions%2520by%2520factors%2520of%25203.8%2520and%25202.1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20realism%20of%20robotic%20surgery%20simulation%20through%20injection%20of%0A%20%20learning-based%20estimated%20errors&entry.906535625=Juan%20Antonio%20Barragan%20and%20Hisashi%20Ishida%20and%20Adnan%20Munawar%20and%20Peter%20Kazanzides&entry.1292438233=%20%20The%20development%20of%20algorithms%20for%20automation%20of%20subtasks%20during%20robotic%0Asurgery%20can%20be%20accelerated%20by%20the%20availability%20of%20realistic%20simulation%0Aenvironments.%20In%20this%20work%2C%20we%20focus%20on%20one%20aspect%20of%20the%20realism%20of%20a%20surgical%0Asimulator%2C%20which%20is%20the%20positional%20accuracy%20of%20the%20robot.%20In%20current%0Asimulators%2C%20robots%20have%20perfect%20or%20near-perfect%20accuracy%2C%20which%20is%20not%0Arepresentative%20of%20their%20physical%20counterparts.%20We%20therefore%20propose%20a%20pair%20of%0Aneural%20networks%2C%20trained%20by%20data%20collected%20from%20a%20physical%20robot%2C%20to%20estimate%0Aboth%20the%20controller%20error%20and%20the%20kinematic%20and%20non-kinematic%20error.%20These%0Aerror%20estimates%20are%20then%20injected%20within%20the%20simulator%20to%20produce%20a%20simulated%0Arobot%20that%20has%20the%20characteristic%20performance%20of%20the%20physical%20robot.%20In%20this%0Ascenario%2C%20we%20believe%20it%20is%20sufficient%20for%20the%20estimated%20error%20used%20in%20the%0Asimulation%20to%20have%20a%20statistically%20similar%20distribution%20to%20the%20actual%20error%20of%0Athe%20physical%20robot.%20This%20is%20less%20stringent%2C%20and%20therefore%20more%20tenable%2C%20than%0Athe%20requirement%20for%20error%20compensation%20of%20a%20physical%20robot%2C%20where%20the%20estimated%0Aerror%20should%20equal%20the%20actual%20error.%20Our%20results%20demonstrate%20that%20error%0Ainjection%20reduces%20the%20mean%20position%20and%20orientation%20differences%20between%20the%0Asimulated%20and%20physical%20robots%20from%205.0%20mm%20/%203.6%20deg%20to%201.3%20mm%20/%201.7%20deg%2C%0Arespectively%2C%20which%20represents%20reductions%20by%20factors%20of%203.8%20and%202.1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07375v1&entry.124074799=Read"},
{"title": "AIGB: Generative Auto-bidding via Diffusion Modeling", "author": "Jiayan Guo and Yusen Huo and Zhilin Zhang and Tianyu Wang and Chuan Yu and Jian Xu and Yan Zhang and Bo Zheng", "abstract": "  Auto-bidding plays a crucial role in facilitating online advertising by\nautomatically providing bids for advertisers. Reinforcement learning (RL) has\ngained popularity for auto-bidding. However, most current RL auto-bidding\nmethods are modeled through the Markovian Decision Process (MDP), which assumes\nthe Markovian state transition. This assumption restricts the ability to\nperform in long horizon scenarios and makes the model unstable when dealing\nwith highly random online advertising environments. To tackle this issue, this\npaper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding\nthrough generative modeling. In this paradigm, we propose DiffBid, a\nconditional diffusion modeling approach for bid generation. DiffBid directly\nmodels the correlation between the return and the entire trajectory,\neffectively avoiding error propagation across time steps in long horizons.\nAdditionally, DiffBid offers a versatile approach for generating trajectories\nthat maximize given targets while adhering to specific constraints. Extensive\nexperiments conducted on the real-world dataset and online A/B test on Alibaba\nadvertising platform demonstrate the effectiveness of DiffBid, achieving 2.81%\nincrease in GMV and 3.36% increase in ROI.\n", "link": "http://arxiv.org/abs/2405.16141v2", "date": "2024-06-11", "relevancy": 2.0589, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5182}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5126}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIGB%3A%20Generative%20Auto-bidding%20via%20Diffusion%20Modeling&body=Title%3A%20AIGB%3A%20Generative%20Auto-bidding%20via%20Diffusion%20Modeling%0AAuthor%3A%20Jiayan%20Guo%20and%20Yusen%20Huo%20and%20Zhilin%20Zhang%20and%20Tianyu%20Wang%20and%20Chuan%20Yu%20and%20Jian%20Xu%20and%20Yan%20Zhang%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Auto-bidding%20plays%20a%20crucial%20role%20in%20facilitating%20online%20advertising%20by%0Aautomatically%20providing%20bids%20for%20advertisers.%20Reinforcement%20learning%20%28RL%29%20has%0Agained%20popularity%20for%20auto-bidding.%20However%2C%20most%20current%20RL%20auto-bidding%0Amethods%20are%20modeled%20through%20the%20Markovian%20Decision%20Process%20%28MDP%29%2C%20which%20assumes%0Athe%20Markovian%20state%20transition.%20This%20assumption%20restricts%20the%20ability%20to%0Aperform%20in%20long%20horizon%20scenarios%20and%20makes%20the%20model%20unstable%20when%20dealing%0Awith%20highly%20random%20online%20advertising%20environments.%20To%20tackle%20this%20issue%2C%20this%0Apaper%20introduces%20AI-Generated%20Bidding%20%28AIGB%29%2C%20a%20novel%20paradigm%20for%20auto-bidding%0Athrough%20generative%20modeling.%20In%20this%20paradigm%2C%20we%20propose%20DiffBid%2C%20a%0Aconditional%20diffusion%20modeling%20approach%20for%20bid%20generation.%20DiffBid%20directly%0Amodels%20the%20correlation%20between%20the%20return%20and%20the%20entire%20trajectory%2C%0Aeffectively%20avoiding%20error%20propagation%20across%20time%20steps%20in%20long%20horizons.%0AAdditionally%2C%20DiffBid%20offers%20a%20versatile%20approach%20for%20generating%20trajectories%0Athat%20maximize%20given%20targets%20while%20adhering%20to%20specific%20constraints.%20Extensive%0Aexperiments%20conducted%20on%20the%20real-world%20dataset%20and%20online%20A/B%20test%20on%20Alibaba%0Aadvertising%20platform%20demonstrate%20the%20effectiveness%20of%20DiffBid%2C%20achieving%202.81%25%0Aincrease%20in%20GMV%20and%203.36%25%20increase%20in%20ROI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIGB%253A%2520Generative%2520Auto-bidding%2520via%2520Diffusion%2520Modeling%26entry.906535625%3DJiayan%2520Guo%2520and%2520Yusen%2520Huo%2520and%2520Zhilin%2520Zhang%2520and%2520Tianyu%2520Wang%2520and%2520Chuan%2520Yu%2520and%2520Jian%2520Xu%2520and%2520Yan%2520Zhang%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Auto-bidding%2520plays%2520a%2520crucial%2520role%2520in%2520facilitating%2520online%2520advertising%2520by%250Aautomatically%2520providing%2520bids%2520for%2520advertisers.%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%250Agained%2520popularity%2520for%2520auto-bidding.%2520However%252C%2520most%2520current%2520RL%2520auto-bidding%250Amethods%2520are%2520modeled%2520through%2520the%2520Markovian%2520Decision%2520Process%2520%2528MDP%2529%252C%2520which%2520assumes%250Athe%2520Markovian%2520state%2520transition.%2520This%2520assumption%2520restricts%2520the%2520ability%2520to%250Aperform%2520in%2520long%2520horizon%2520scenarios%2520and%2520makes%2520the%2520model%2520unstable%2520when%2520dealing%250Awith%2520highly%2520random%2520online%2520advertising%2520environments.%2520To%2520tackle%2520this%2520issue%252C%2520this%250Apaper%2520introduces%2520AI-Generated%2520Bidding%2520%2528AIGB%2529%252C%2520a%2520novel%2520paradigm%2520for%2520auto-bidding%250Athrough%2520generative%2520modeling.%2520In%2520this%2520paradigm%252C%2520we%2520propose%2520DiffBid%252C%2520a%250Aconditional%2520diffusion%2520modeling%2520approach%2520for%2520bid%2520generation.%2520DiffBid%2520directly%250Amodels%2520the%2520correlation%2520between%2520the%2520return%2520and%2520the%2520entire%2520trajectory%252C%250Aeffectively%2520avoiding%2520error%2520propagation%2520across%2520time%2520steps%2520in%2520long%2520horizons.%250AAdditionally%252C%2520DiffBid%2520offers%2520a%2520versatile%2520approach%2520for%2520generating%2520trajectories%250Athat%2520maximize%2520given%2520targets%2520while%2520adhering%2520to%2520specific%2520constraints.%2520Extensive%250Aexperiments%2520conducted%2520on%2520the%2520real-world%2520dataset%2520and%2520online%2520A/B%2520test%2520on%2520Alibaba%250Aadvertising%2520platform%2520demonstrate%2520the%2520effectiveness%2520of%2520DiffBid%252C%2520achieving%25202.81%2525%250Aincrease%2520in%2520GMV%2520and%25203.36%2525%2520increase%2520in%2520ROI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIGB%3A%20Generative%20Auto-bidding%20via%20Diffusion%20Modeling&entry.906535625=Jiayan%20Guo%20and%20Yusen%20Huo%20and%20Zhilin%20Zhang%20and%20Tianyu%20Wang%20and%20Chuan%20Yu%20and%20Jian%20Xu%20and%20Yan%20Zhang%20and%20Bo%20Zheng&entry.1292438233=%20%20Auto-bidding%20plays%20a%20crucial%20role%20in%20facilitating%20online%20advertising%20by%0Aautomatically%20providing%20bids%20for%20advertisers.%20Reinforcement%20learning%20%28RL%29%20has%0Agained%20popularity%20for%20auto-bidding.%20However%2C%20most%20current%20RL%20auto-bidding%0Amethods%20are%20modeled%20through%20the%20Markovian%20Decision%20Process%20%28MDP%29%2C%20which%20assumes%0Athe%20Markovian%20state%20transition.%20This%20assumption%20restricts%20the%20ability%20to%0Aperform%20in%20long%20horizon%20scenarios%20and%20makes%20the%20model%20unstable%20when%20dealing%0Awith%20highly%20random%20online%20advertising%20environments.%20To%20tackle%20this%20issue%2C%20this%0Apaper%20introduces%20AI-Generated%20Bidding%20%28AIGB%29%2C%20a%20novel%20paradigm%20for%20auto-bidding%0Athrough%20generative%20modeling.%20In%20this%20paradigm%2C%20we%20propose%20DiffBid%2C%20a%0Aconditional%20diffusion%20modeling%20approach%20for%20bid%20generation.%20DiffBid%20directly%0Amodels%20the%20correlation%20between%20the%20return%20and%20the%20entire%20trajectory%2C%0Aeffectively%20avoiding%20error%20propagation%20across%20time%20steps%20in%20long%20horizons.%0AAdditionally%2C%20DiffBid%20offers%20a%20versatile%20approach%20for%20generating%20trajectories%0Athat%20maximize%20given%20targets%20while%20adhering%20to%20specific%20constraints.%20Extensive%0Aexperiments%20conducted%20on%20the%20real-world%20dataset%20and%20online%20A/B%20test%20on%20Alibaba%0Aadvertising%20platform%20demonstrate%20the%20effectiveness%20of%20DiffBid%2C%20achieving%202.81%25%0Aincrease%20in%20GMV%20and%203.36%25%20increase%20in%20ROI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16141v2&entry.124074799=Read"},
{"title": "Advancing Grounded Multimodal Named Entity Recognition via LLM-Based\n  Reformulation and Box-Based Segmentation", "author": "Jinyuan Li and Ziyan Li and Han Li and Jianfei Yu and Rui Xia and Di Sun and Gang Pan", "abstract": "  Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify\nnamed entities, entity types and their corresponding visual regions. GMNER task\nexhibits two challenging attributes: 1) The tenuous correlation between images\nand text on social media contributes to a notable proportion of named entities\nbeing ungroundable. 2) There exists a distinction between coarse-grained noun\nphrases used in similar tasks (e.g., phrase localization) and fine-grained\nnamed entities. In this paper, we propose RiVEG, a unified framework that\nreformulates GMNER into a joint MNER-VE-VG task by leveraging large language\nmodels (LLMs) as connecting bridges. This reformulation brings two benefits: 1)\nIt enables us to optimize the MNER module for optimal MNER performance and\neliminates the need to pre-extract region features using object detection\nmethods, thus naturally addressing the two major limitations of existing GMNER\nmethods. 2) The introduction of Entity Expansion Expression module and Visual\nEntailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).\nThis endows the proposed framework with unlimited data and model scalability.\nFurthermore, to address the potential ambiguity stemming from the\ncoarse-grained bounding box output in GMNER, we further construct the new\nSegmented Multimodal Named Entity Recognition (SMNER) task and corresponding\nTwitter-SMNER dataset aimed at generating fine-grained segmentation masks, and\nexperimentally demonstrate the feasibility and effectiveness of using box\nprompt-based Segment Anything Model (SAM) to empower any GMNER model with the\nability to accomplish the SMNER task. Extensive experiments demonstrate that\nRiVEG significantly outperforms SoTA methods on four datasets across the MNER,\nGMNER, and SMNER tasks.\n", "link": "http://arxiv.org/abs/2406.07268v1", "date": "2024-06-11", "relevancy": 2.0564, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5207}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20via%20LLM-Based%0A%20%20Reformulation%20and%20Box-Based%20Segmentation&body=Title%3A%20Advancing%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20via%20LLM-Based%0A%20%20Reformulation%20and%20Box-Based%20Segmentation%0AAuthor%3A%20Jinyuan%20Li%20and%20Ziyan%20Li%20and%20Han%20Li%20and%20Jianfei%20Yu%20and%20Rui%20Xia%20and%20Di%20Sun%20and%20Gang%20Pan%0AAbstract%3A%20%20%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20%28GMNER%29%20task%20aims%20to%20identify%0Anamed%20entities%2C%20entity%20types%20and%20their%20corresponding%20visual%20regions.%20GMNER%20task%0Aexhibits%20two%20challenging%20attributes%3A%201%29%20The%20tenuous%20correlation%20between%20images%0Aand%20text%20on%20social%20media%20contributes%20to%20a%20notable%20proportion%20of%20named%20entities%0Abeing%20ungroundable.%202%29%20There%20exists%20a%20distinction%20between%20coarse-grained%20noun%0Aphrases%20used%20in%20similar%20tasks%20%28e.g.%2C%20phrase%20localization%29%20and%20fine-grained%0Anamed%20entities.%20In%20this%20paper%2C%20we%20propose%20RiVEG%2C%20a%20unified%20framework%20that%0Areformulates%20GMNER%20into%20a%20joint%20MNER-VE-VG%20task%20by%20leveraging%20large%20language%0Amodels%20%28LLMs%29%20as%20connecting%20bridges.%20This%20reformulation%20brings%20two%20benefits%3A%201%29%0AIt%20enables%20us%20to%20optimize%20the%20MNER%20module%20for%20optimal%20MNER%20performance%20and%0Aeliminates%20the%20need%20to%20pre-extract%20region%20features%20using%20object%20detection%0Amethods%2C%20thus%20naturally%20addressing%20the%20two%20major%20limitations%20of%20existing%20GMNER%0Amethods.%202%29%20The%20introduction%20of%20Entity%20Expansion%20Expression%20module%20and%20Visual%0AEntailment%20%28VE%29%20module%20unifies%20Visual%20Grounding%20%28VG%29%20and%20Entity%20Grounding%20%28EG%29.%0AThis%20endows%20the%20proposed%20framework%20with%20unlimited%20data%20and%20model%20scalability.%0AFurthermore%2C%20to%20address%20the%20potential%20ambiguity%20stemming%20from%20the%0Acoarse-grained%20bounding%20box%20output%20in%20GMNER%2C%20we%20further%20construct%20the%20new%0ASegmented%20Multimodal%20Named%20Entity%20Recognition%20%28SMNER%29%20task%20and%20corresponding%0ATwitter-SMNER%20dataset%20aimed%20at%20generating%20fine-grained%20segmentation%20masks%2C%20and%0Aexperimentally%20demonstrate%20the%20feasibility%20and%20effectiveness%20of%20using%20box%0Aprompt-based%20Segment%20Anything%20Model%20%28SAM%29%20to%20empower%20any%20GMNER%20model%20with%20the%0Aability%20to%20accomplish%20the%20SMNER%20task.%20Extensive%20experiments%20demonstrate%20that%0ARiVEG%20significantly%20outperforms%20SoTA%20methods%20on%20four%20datasets%20across%20the%20MNER%2C%0AGMNER%2C%20and%20SMNER%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Grounded%2520Multimodal%2520Named%2520Entity%2520Recognition%2520via%2520LLM-Based%250A%2520%2520Reformulation%2520and%2520Box-Based%2520Segmentation%26entry.906535625%3DJinyuan%2520Li%2520and%2520Ziyan%2520Li%2520and%2520Han%2520Li%2520and%2520Jianfei%2520Yu%2520and%2520Rui%2520Xia%2520and%2520Di%2520Sun%2520and%2520Gang%2520Pan%26entry.1292438233%3D%2520%2520Grounded%2520Multimodal%2520Named%2520Entity%2520Recognition%2520%2528GMNER%2529%2520task%2520aims%2520to%2520identify%250Anamed%2520entities%252C%2520entity%2520types%2520and%2520their%2520corresponding%2520visual%2520regions.%2520GMNER%2520task%250Aexhibits%2520two%2520challenging%2520attributes%253A%25201%2529%2520The%2520tenuous%2520correlation%2520between%2520images%250Aand%2520text%2520on%2520social%2520media%2520contributes%2520to%2520a%2520notable%2520proportion%2520of%2520named%2520entities%250Abeing%2520ungroundable.%25202%2529%2520There%2520exists%2520a%2520distinction%2520between%2520coarse-grained%2520noun%250Aphrases%2520used%2520in%2520similar%2520tasks%2520%2528e.g.%252C%2520phrase%2520localization%2529%2520and%2520fine-grained%250Anamed%2520entities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520RiVEG%252C%2520a%2520unified%2520framework%2520that%250Areformulates%2520GMNER%2520into%2520a%2520joint%2520MNER-VE-VG%2520task%2520by%2520leveraging%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520as%2520connecting%2520bridges.%2520This%2520reformulation%2520brings%2520two%2520benefits%253A%25201%2529%250AIt%2520enables%2520us%2520to%2520optimize%2520the%2520MNER%2520module%2520for%2520optimal%2520MNER%2520performance%2520and%250Aeliminates%2520the%2520need%2520to%2520pre-extract%2520region%2520features%2520using%2520object%2520detection%250Amethods%252C%2520thus%2520naturally%2520addressing%2520the%2520two%2520major%2520limitations%2520of%2520existing%2520GMNER%250Amethods.%25202%2529%2520The%2520introduction%2520of%2520Entity%2520Expansion%2520Expression%2520module%2520and%2520Visual%250AEntailment%2520%2528VE%2529%2520module%2520unifies%2520Visual%2520Grounding%2520%2528VG%2529%2520and%2520Entity%2520Grounding%2520%2528EG%2529.%250AThis%2520endows%2520the%2520proposed%2520framework%2520with%2520unlimited%2520data%2520and%2520model%2520scalability.%250AFurthermore%252C%2520to%2520address%2520the%2520potential%2520ambiguity%2520stemming%2520from%2520the%250Acoarse-grained%2520bounding%2520box%2520output%2520in%2520GMNER%252C%2520we%2520further%2520construct%2520the%2520new%250ASegmented%2520Multimodal%2520Named%2520Entity%2520Recognition%2520%2528SMNER%2529%2520task%2520and%2520corresponding%250ATwitter-SMNER%2520dataset%2520aimed%2520at%2520generating%2520fine-grained%2520segmentation%2520masks%252C%2520and%250Aexperimentally%2520demonstrate%2520the%2520feasibility%2520and%2520effectiveness%2520of%2520using%2520box%250Aprompt-based%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520to%2520empower%2520any%2520GMNER%2520model%2520with%2520the%250Aability%2520to%2520accomplish%2520the%2520SMNER%2520task.%2520Extensive%2520experiments%2520demonstrate%2520that%250ARiVEG%2520significantly%2520outperforms%2520SoTA%2520methods%2520on%2520four%2520datasets%2520across%2520the%2520MNER%252C%250AGMNER%252C%2520and%2520SMNER%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20via%20LLM-Based%0A%20%20Reformulation%20and%20Box-Based%20Segmentation&entry.906535625=Jinyuan%20Li%20and%20Ziyan%20Li%20and%20Han%20Li%20and%20Jianfei%20Yu%20and%20Rui%20Xia%20and%20Di%20Sun%20and%20Gang%20Pan&entry.1292438233=%20%20Grounded%20Multimodal%20Named%20Entity%20Recognition%20%28GMNER%29%20task%20aims%20to%20identify%0Anamed%20entities%2C%20entity%20types%20and%20their%20corresponding%20visual%20regions.%20GMNER%20task%0Aexhibits%20two%20challenging%20attributes%3A%201%29%20The%20tenuous%20correlation%20between%20images%0Aand%20text%20on%20social%20media%20contributes%20to%20a%20notable%20proportion%20of%20named%20entities%0Abeing%20ungroundable.%202%29%20There%20exists%20a%20distinction%20between%20coarse-grained%20noun%0Aphrases%20used%20in%20similar%20tasks%20%28e.g.%2C%20phrase%20localization%29%20and%20fine-grained%0Anamed%20entities.%20In%20this%20paper%2C%20we%20propose%20RiVEG%2C%20a%20unified%20framework%20that%0Areformulates%20GMNER%20into%20a%20joint%20MNER-VE-VG%20task%20by%20leveraging%20large%20language%0Amodels%20%28LLMs%29%20as%20connecting%20bridges.%20This%20reformulation%20brings%20two%20benefits%3A%201%29%0AIt%20enables%20us%20to%20optimize%20the%20MNER%20module%20for%20optimal%20MNER%20performance%20and%0Aeliminates%20the%20need%20to%20pre-extract%20region%20features%20using%20object%20detection%0Amethods%2C%20thus%20naturally%20addressing%20the%20two%20major%20limitations%20of%20existing%20GMNER%0Amethods.%202%29%20The%20introduction%20of%20Entity%20Expansion%20Expression%20module%20and%20Visual%0AEntailment%20%28VE%29%20module%20unifies%20Visual%20Grounding%20%28VG%29%20and%20Entity%20Grounding%20%28EG%29.%0AThis%20endows%20the%20proposed%20framework%20with%20unlimited%20data%20and%20model%20scalability.%0AFurthermore%2C%20to%20address%20the%20potential%20ambiguity%20stemming%20from%20the%0Acoarse-grained%20bounding%20box%20output%20in%20GMNER%2C%20we%20further%20construct%20the%20new%0ASegmented%20Multimodal%20Named%20Entity%20Recognition%20%28SMNER%29%20task%20and%20corresponding%0ATwitter-SMNER%20dataset%20aimed%20at%20generating%20fine-grained%20segmentation%20masks%2C%20and%0Aexperimentally%20demonstrate%20the%20feasibility%20and%20effectiveness%20of%20using%20box%0Aprompt-based%20Segment%20Anything%20Model%20%28SAM%29%20to%20empower%20any%20GMNER%20model%20with%20the%0Aability%20to%20accomplish%20the%20SMNER%20task.%20Extensive%20experiments%20demonstrate%20that%0ARiVEG%20significantly%20outperforms%20SoTA%20methods%20on%20four%20datasets%20across%20the%20MNER%2C%0AGMNER%2C%20and%20SMNER%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07268v1&entry.124074799=Read"},
{"title": "Trainwreck: A damaging adversarial attack on image classifiers", "author": "Jan Zah\u00e1lka", "abstract": "  Adversarial attacks are an important security concern for computer vision\n(CV). As CV models are becoming increasingly valuable assets in applied\npractice, disrupting them is emerging as a form of economic sabotage. This\npaper opens up the exploration of damaging adversarial attacks (DAAs) that seek\nto damage target CV models. DAAs are formalized by defining the threat model,\nthe cost function DAAs maximize, and setting three requirements for success:\npotency, stealth, and customizability. As a pioneer DAA, this paper proposes\nTrainwreck, a train-time attack that conflates the data of similar classes in\nthe training data using stealthy ($\\epsilon \\leq 8/255$) class-pair universal\nperturbations obtained from a surrogate model. Trainwreck is a black-box,\ntransferable attack: it requires no knowledge of the target architecture, and a\nsingle poisoned dataset degrades the performance of any model trained on it.\nThe experimental evaluation on CIFAR-10 and CIFAR-100 and various model\narchitectures (EfficientNetV2, ResNeXt-101, and a finetuned ViT-L-16)\ndemonstrates Trainwreck's efficiency. Trainwreck achieves similar or better\npotency compared to the data poisoning state of the art and is fully\ncustomizable by the poison rate parameter. Finally, data redundancy with\nhashing is identified as a reliable defense against Trainwreck or similar DAAs.\nThe code is available at https://github.com/JanZahalka/trainwreck.\n", "link": "http://arxiv.org/abs/2311.14772v2", "date": "2024-06-11", "relevancy": 2.0564, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5204}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5117}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trainwreck%3A%20A%20damaging%20adversarial%20attack%20on%20image%20classifiers&body=Title%3A%20Trainwreck%3A%20A%20damaging%20adversarial%20attack%20on%20image%20classifiers%0AAuthor%3A%20Jan%20Zah%C3%A1lka%0AAbstract%3A%20%20%20Adversarial%20attacks%20are%20an%20important%20security%20concern%20for%20computer%20vision%0A%28CV%29.%20As%20CV%20models%20are%20becoming%20increasingly%20valuable%20assets%20in%20applied%0Apractice%2C%20disrupting%20them%20is%20emerging%20as%20a%20form%20of%20economic%20sabotage.%20This%0Apaper%20opens%20up%20the%20exploration%20of%20damaging%20adversarial%20attacks%20%28DAAs%29%20that%20seek%0Ato%20damage%20target%20CV%20models.%20DAAs%20are%20formalized%20by%20defining%20the%20threat%20model%2C%0Athe%20cost%20function%20DAAs%20maximize%2C%20and%20setting%20three%20requirements%20for%20success%3A%0Apotency%2C%20stealth%2C%20and%20customizability.%20As%20a%20pioneer%20DAA%2C%20this%20paper%20proposes%0ATrainwreck%2C%20a%20train-time%20attack%20that%20conflates%20the%20data%20of%20similar%20classes%20in%0Athe%20training%20data%20using%20stealthy%20%28%24%5Cepsilon%20%5Cleq%208/255%24%29%20class-pair%20universal%0Aperturbations%20obtained%20from%20a%20surrogate%20model.%20Trainwreck%20is%20a%20black-box%2C%0Atransferable%20attack%3A%20it%20requires%20no%20knowledge%20of%20the%20target%20architecture%2C%20and%20a%0Asingle%20poisoned%20dataset%20degrades%20the%20performance%20of%20any%20model%20trained%20on%20it.%0AThe%20experimental%20evaluation%20on%20CIFAR-10%20and%20CIFAR-100%20and%20various%20model%0Aarchitectures%20%28EfficientNetV2%2C%20ResNeXt-101%2C%20and%20a%20finetuned%20ViT-L-16%29%0Ademonstrates%20Trainwreck%27s%20efficiency.%20Trainwreck%20achieves%20similar%20or%20better%0Apotency%20compared%20to%20the%20data%20poisoning%20state%20of%20the%20art%20and%20is%20fully%0Acustomizable%20by%20the%20poison%20rate%20parameter.%20Finally%2C%20data%20redundancy%20with%0Ahashing%20is%20identified%20as%20a%20reliable%20defense%20against%20Trainwreck%20or%20similar%20DAAs.%0AThe%20code%20is%20available%20at%20https%3A//github.com/JanZahalka/trainwreck.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrainwreck%253A%2520A%2520damaging%2520adversarial%2520attack%2520on%2520image%2520classifiers%26entry.906535625%3DJan%2520Zah%25C3%25A1lka%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520are%2520an%2520important%2520security%2520concern%2520for%2520computer%2520vision%250A%2528CV%2529.%2520As%2520CV%2520models%2520are%2520becoming%2520increasingly%2520valuable%2520assets%2520in%2520applied%250Apractice%252C%2520disrupting%2520them%2520is%2520emerging%2520as%2520a%2520form%2520of%2520economic%2520sabotage.%2520This%250Apaper%2520opens%2520up%2520the%2520exploration%2520of%2520damaging%2520adversarial%2520attacks%2520%2528DAAs%2529%2520that%2520seek%250Ato%2520damage%2520target%2520CV%2520models.%2520DAAs%2520are%2520formalized%2520by%2520defining%2520the%2520threat%2520model%252C%250Athe%2520cost%2520function%2520DAAs%2520maximize%252C%2520and%2520setting%2520three%2520requirements%2520for%2520success%253A%250Apotency%252C%2520stealth%252C%2520and%2520customizability.%2520As%2520a%2520pioneer%2520DAA%252C%2520this%2520paper%2520proposes%250ATrainwreck%252C%2520a%2520train-time%2520attack%2520that%2520conflates%2520the%2520data%2520of%2520similar%2520classes%2520in%250Athe%2520training%2520data%2520using%2520stealthy%2520%2528%2524%255Cepsilon%2520%255Cleq%25208/255%2524%2529%2520class-pair%2520universal%250Aperturbations%2520obtained%2520from%2520a%2520surrogate%2520model.%2520Trainwreck%2520is%2520a%2520black-box%252C%250Atransferable%2520attack%253A%2520it%2520requires%2520no%2520knowledge%2520of%2520the%2520target%2520architecture%252C%2520and%2520a%250Asingle%2520poisoned%2520dataset%2520degrades%2520the%2520performance%2520of%2520any%2520model%2520trained%2520on%2520it.%250AThe%2520experimental%2520evaluation%2520on%2520CIFAR-10%2520and%2520CIFAR-100%2520and%2520various%2520model%250Aarchitectures%2520%2528EfficientNetV2%252C%2520ResNeXt-101%252C%2520and%2520a%2520finetuned%2520ViT-L-16%2529%250Ademonstrates%2520Trainwreck%2527s%2520efficiency.%2520Trainwreck%2520achieves%2520similar%2520or%2520better%250Apotency%2520compared%2520to%2520the%2520data%2520poisoning%2520state%2520of%2520the%2520art%2520and%2520is%2520fully%250Acustomizable%2520by%2520the%2520poison%2520rate%2520parameter.%2520Finally%252C%2520data%2520redundancy%2520with%250Ahashing%2520is%2520identified%2520as%2520a%2520reliable%2520defense%2520against%2520Trainwreck%2520or%2520similar%2520DAAs.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/JanZahalka/trainwreck.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trainwreck%3A%20A%20damaging%20adversarial%20attack%20on%20image%20classifiers&entry.906535625=Jan%20Zah%C3%A1lka&entry.1292438233=%20%20Adversarial%20attacks%20are%20an%20important%20security%20concern%20for%20computer%20vision%0A%28CV%29.%20As%20CV%20models%20are%20becoming%20increasingly%20valuable%20assets%20in%20applied%0Apractice%2C%20disrupting%20them%20is%20emerging%20as%20a%20form%20of%20economic%20sabotage.%20This%0Apaper%20opens%20up%20the%20exploration%20of%20damaging%20adversarial%20attacks%20%28DAAs%29%20that%20seek%0Ato%20damage%20target%20CV%20models.%20DAAs%20are%20formalized%20by%20defining%20the%20threat%20model%2C%0Athe%20cost%20function%20DAAs%20maximize%2C%20and%20setting%20three%20requirements%20for%20success%3A%0Apotency%2C%20stealth%2C%20and%20customizability.%20As%20a%20pioneer%20DAA%2C%20this%20paper%20proposes%0ATrainwreck%2C%20a%20train-time%20attack%20that%20conflates%20the%20data%20of%20similar%20classes%20in%0Athe%20training%20data%20using%20stealthy%20%28%24%5Cepsilon%20%5Cleq%208/255%24%29%20class-pair%20universal%0Aperturbations%20obtained%20from%20a%20surrogate%20model.%20Trainwreck%20is%20a%20black-box%2C%0Atransferable%20attack%3A%20it%20requires%20no%20knowledge%20of%20the%20target%20architecture%2C%20and%20a%0Asingle%20poisoned%20dataset%20degrades%20the%20performance%20of%20any%20model%20trained%20on%20it.%0AThe%20experimental%20evaluation%20on%20CIFAR-10%20and%20CIFAR-100%20and%20various%20model%0Aarchitectures%20%28EfficientNetV2%2C%20ResNeXt-101%2C%20and%20a%20finetuned%20ViT-L-16%29%0Ademonstrates%20Trainwreck%27s%20efficiency.%20Trainwreck%20achieves%20similar%20or%20better%0Apotency%20compared%20to%20the%20data%20poisoning%20state%20of%20the%20art%20and%20is%20fully%0Acustomizable%20by%20the%20poison%20rate%20parameter.%20Finally%2C%20data%20redundancy%20with%0Ahashing%20is%20identified%20as%20a%20reliable%20defense%20against%20Trainwreck%20or%20similar%20DAAs.%0AThe%20code%20is%20available%20at%20https%3A//github.com/JanZahalka/trainwreck.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14772v2&entry.124074799=Read"},
{"title": "Autoregressive Pretraining with Mamba in Vision", "author": "Sucheng Ren and Xianhang Li and Haoqin Tu and Feng Wang and Fangxun Shu and Lei Zhang and Jieru Mei and Linjie Yang and Peng Wang and Heng Wang and Alan Yuille and Cihang Xie", "abstract": "  The vision community has started to build with the recently developed state\nspace model, Mamba, as the new backbone for a range of tasks. This paper shows\nthat Mamba's visual capability can be significantly enhanced through\nautoregressive pretraining, a direction not previously explored.\nEfficiency-wise, the autoregressive nature can well capitalize on the Mamba's\nunidirectional recurrent structure, enabling faster overall training speed\ncompared to other training strategies like mask modeling. Performance-wise,\nautoregressive pretraining equips the Mamba architecture with markedly higher\naccuracy over its supervised-trained counterparts and, more importantly,\nsuccessfully unlocks its scaling potential to large and even huge model sizes.\nFor example, with autoregressive pretraining, a base-size Mamba attains 83.2\\%\nImageNet accuracy, outperforming its supervised counterpart by 2.0\\%; our\nhuge-size Mamba, the largest Vision Mamba to date, attains 85.0\\% ImageNet\naccuracy (85.5\\% when finetuned with $384\\times384$ inputs), notably surpassing\nall other Mamba variants in vision. The code is available at\n\\url{https://github.com/OliverRensu/ARM}.\n", "link": "http://arxiv.org/abs/2406.07537v1", "date": "2024-06-11", "relevancy": 2.0542, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5196}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Pretraining%20with%20Mamba%20in%20Vision&body=Title%3A%20Autoregressive%20Pretraining%20with%20Mamba%20in%20Vision%0AAuthor%3A%20Sucheng%20Ren%20and%20Xianhang%20Li%20and%20Haoqin%20Tu%20and%20Feng%20Wang%20and%20Fangxun%20Shu%20and%20Lei%20Zhang%20and%20Jieru%20Mei%20and%20Linjie%20Yang%20and%20Peng%20Wang%20and%20Heng%20Wang%20and%20Alan%20Yuille%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20The%20vision%20community%20has%20started%20to%20build%20with%20the%20recently%20developed%20state%0Aspace%20model%2C%20Mamba%2C%20as%20the%20new%20backbone%20for%20a%20range%20of%20tasks.%20This%20paper%20shows%0Athat%20Mamba%27s%20visual%20capability%20can%20be%20significantly%20enhanced%20through%0Aautoregressive%20pretraining%2C%20a%20direction%20not%20previously%20explored.%0AEfficiency-wise%2C%20the%20autoregressive%20nature%20can%20well%20capitalize%20on%20the%20Mamba%27s%0Aunidirectional%20recurrent%20structure%2C%20enabling%20faster%20overall%20training%20speed%0Acompared%20to%20other%20training%20strategies%20like%20mask%20modeling.%20Performance-wise%2C%0Aautoregressive%20pretraining%20equips%20the%20Mamba%20architecture%20with%20markedly%20higher%0Aaccuracy%20over%20its%20supervised-trained%20counterparts%20and%2C%20more%20importantly%2C%0Asuccessfully%20unlocks%20its%20scaling%20potential%20to%20large%20and%20even%20huge%20model%20sizes.%0AFor%20example%2C%20with%20autoregressive%20pretraining%2C%20a%20base-size%20Mamba%20attains%2083.2%5C%25%0AImageNet%20accuracy%2C%20outperforming%20its%20supervised%20counterpart%20by%202.0%5C%25%3B%20our%0Ahuge-size%20Mamba%2C%20the%20largest%20Vision%20Mamba%20to%20date%2C%20attains%2085.0%5C%25%20ImageNet%0Aaccuracy%20%2885.5%5C%25%20when%20finetuned%20with%20%24384%5Ctimes384%24%20inputs%29%2C%20notably%20surpassing%0Aall%20other%20Mamba%20variants%20in%20vision.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/OliverRensu/ARM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Pretraining%2520with%2520Mamba%2520in%2520Vision%26entry.906535625%3DSucheng%2520Ren%2520and%2520Xianhang%2520Li%2520and%2520Haoqin%2520Tu%2520and%2520Feng%2520Wang%2520and%2520Fangxun%2520Shu%2520and%2520Lei%2520Zhang%2520and%2520Jieru%2520Mei%2520and%2520Linjie%2520Yang%2520and%2520Peng%2520Wang%2520and%2520Heng%2520Wang%2520and%2520Alan%2520Yuille%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520The%2520vision%2520community%2520has%2520started%2520to%2520build%2520with%2520the%2520recently%2520developed%2520state%250Aspace%2520model%252C%2520Mamba%252C%2520as%2520the%2520new%2520backbone%2520for%2520a%2520range%2520of%2520tasks.%2520This%2520paper%2520shows%250Athat%2520Mamba%2527s%2520visual%2520capability%2520can%2520be%2520significantly%2520enhanced%2520through%250Aautoregressive%2520pretraining%252C%2520a%2520direction%2520not%2520previously%2520explored.%250AEfficiency-wise%252C%2520the%2520autoregressive%2520nature%2520can%2520well%2520capitalize%2520on%2520the%2520Mamba%2527s%250Aunidirectional%2520recurrent%2520structure%252C%2520enabling%2520faster%2520overall%2520training%2520speed%250Acompared%2520to%2520other%2520training%2520strategies%2520like%2520mask%2520modeling.%2520Performance-wise%252C%250Aautoregressive%2520pretraining%2520equips%2520the%2520Mamba%2520architecture%2520with%2520markedly%2520higher%250Aaccuracy%2520over%2520its%2520supervised-trained%2520counterparts%2520and%252C%2520more%2520importantly%252C%250Asuccessfully%2520unlocks%2520its%2520scaling%2520potential%2520to%2520large%2520and%2520even%2520huge%2520model%2520sizes.%250AFor%2520example%252C%2520with%2520autoregressive%2520pretraining%252C%2520a%2520base-size%2520Mamba%2520attains%252083.2%255C%2525%250AImageNet%2520accuracy%252C%2520outperforming%2520its%2520supervised%2520counterpart%2520by%25202.0%255C%2525%253B%2520our%250Ahuge-size%2520Mamba%252C%2520the%2520largest%2520Vision%2520Mamba%2520to%2520date%252C%2520attains%252085.0%255C%2525%2520ImageNet%250Aaccuracy%2520%252885.5%255C%2525%2520when%2520finetuned%2520with%2520%2524384%255Ctimes384%2524%2520inputs%2529%252C%2520notably%2520surpassing%250Aall%2520other%2520Mamba%2520variants%2520in%2520vision.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/OliverRensu/ARM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Pretraining%20with%20Mamba%20in%20Vision&entry.906535625=Sucheng%20Ren%20and%20Xianhang%20Li%20and%20Haoqin%20Tu%20and%20Feng%20Wang%20and%20Fangxun%20Shu%20and%20Lei%20Zhang%20and%20Jieru%20Mei%20and%20Linjie%20Yang%20and%20Peng%20Wang%20and%20Heng%20Wang%20and%20Alan%20Yuille%20and%20Cihang%20Xie&entry.1292438233=%20%20The%20vision%20community%20has%20started%20to%20build%20with%20the%20recently%20developed%20state%0Aspace%20model%2C%20Mamba%2C%20as%20the%20new%20backbone%20for%20a%20range%20of%20tasks.%20This%20paper%20shows%0Athat%20Mamba%27s%20visual%20capability%20can%20be%20significantly%20enhanced%20through%0Aautoregressive%20pretraining%2C%20a%20direction%20not%20previously%20explored.%0AEfficiency-wise%2C%20the%20autoregressive%20nature%20can%20well%20capitalize%20on%20the%20Mamba%27s%0Aunidirectional%20recurrent%20structure%2C%20enabling%20faster%20overall%20training%20speed%0Acompared%20to%20other%20training%20strategies%20like%20mask%20modeling.%20Performance-wise%2C%0Aautoregressive%20pretraining%20equips%20the%20Mamba%20architecture%20with%20markedly%20higher%0Aaccuracy%20over%20its%20supervised-trained%20counterparts%20and%2C%20more%20importantly%2C%0Asuccessfully%20unlocks%20its%20scaling%20potential%20to%20large%20and%20even%20huge%20model%20sizes.%0AFor%20example%2C%20with%20autoregressive%20pretraining%2C%20a%20base-size%20Mamba%20attains%2083.2%5C%25%0AImageNet%20accuracy%2C%20outperforming%20its%20supervised%20counterpart%20by%202.0%5C%25%3B%20our%0Ahuge-size%20Mamba%2C%20the%20largest%20Vision%20Mamba%20to%20date%2C%20attains%2085.0%5C%25%20ImageNet%0Aaccuracy%20%2885.5%5C%25%20when%20finetuned%20with%20%24384%5Ctimes384%24%20inputs%29%2C%20notably%20surpassing%0Aall%20other%20Mamba%20variants%20in%20vision.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/OliverRensu/ARM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07537v1&entry.124074799=Read"},
{"title": "MAP: Low-compute Model Merging with Amortized Pareto Fronts via\n  Quadratic Approximation", "author": "Lu Li and Tianyu Zhang and Zhiqi Bu and Suyuchen Wang and Huan He and Jie Fu and Yonghui Wu and Jiang Bian and Yong Chen and Yoshua Bengio", "abstract": "  Model merging has emerged as an effective approach to combine multiple\nsingle-task models, fine-tuned from the same pre-trained model, into a\nmultitask model. This process typically involves computing a weighted average\nof the model parameters without any additional training. Existing model-merging\nmethods focus on enhancing average task accuracy. However, interference and\nconflicts between the objectives of different tasks can lead to trade-offs\nduring model merging. In real-world applications, a set of solutions with\nvarious trade-offs can be more informative, helping practitioners make\ndecisions based on diverse preferences. In this paper, we introduce a novel\nlow-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP\nidentifies a Pareto set of scaling coefficients for merging multiple models to\nreflect the trade-offs. The core component of MAP is approximating the\nevaluation metrics of the various tasks using a quadratic approximation\nsurrogate model derived from a pre-selected set of scaling coefficients,\nenabling amortized inference. Experimental results on vision and natural\nlanguage processing tasks show that MAP can accurately identify the Pareto\nfront. To further reduce the required computation of MAP, we propose (1) a\nBayesian adaptive sampling algorithm and (2) a nested merging scheme with\nmultiple stages.\n", "link": "http://arxiv.org/abs/2406.07529v1", "date": "2024-06-11", "relevancy": 2.0431, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5189}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5102}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAP%3A%20Low-compute%20Model%20Merging%20with%20Amortized%20Pareto%20Fronts%20via%0A%20%20Quadratic%20Approximation&body=Title%3A%20MAP%3A%20Low-compute%20Model%20Merging%20with%20Amortized%20Pareto%20Fronts%20via%0A%20%20Quadratic%20Approximation%0AAuthor%3A%20Lu%20Li%20and%20Tianyu%20Zhang%20and%20Zhiqi%20Bu%20and%20Suyuchen%20Wang%20and%20Huan%20He%20and%20Jie%20Fu%20and%20Yonghui%20Wu%20and%20Jiang%20Bian%20and%20Yong%20Chen%20and%20Yoshua%20Bengio%0AAbstract%3A%20%20%20Model%20merging%20has%20emerged%20as%20an%20effective%20approach%20to%20combine%20multiple%0Asingle-task%20models%2C%20fine-tuned%20from%20the%20same%20pre-trained%20model%2C%20into%20a%0Amultitask%20model.%20This%20process%20typically%20involves%20computing%20a%20weighted%20average%0Aof%20the%20model%20parameters%20without%20any%20additional%20training.%20Existing%20model-merging%0Amethods%20focus%20on%20enhancing%20average%20task%20accuracy.%20However%2C%20interference%20and%0Aconflicts%20between%20the%20objectives%20of%20different%20tasks%20can%20lead%20to%20trade-offs%0Aduring%20model%20merging.%20In%20real-world%20applications%2C%20a%20set%20of%20solutions%20with%0Avarious%20trade-offs%20can%20be%20more%20informative%2C%20helping%20practitioners%20make%0Adecisions%20based%20on%20diverse%20preferences.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Alow-compute%20algorithm%2C%20Model%20Merging%20with%20Amortized%20Pareto%20Front%20%28MAP%29.%20MAP%0Aidentifies%20a%20Pareto%20set%20of%20scaling%20coefficients%20for%20merging%20multiple%20models%20to%0Areflect%20the%20trade-offs.%20The%20core%20component%20of%20MAP%20is%20approximating%20the%0Aevaluation%20metrics%20of%20the%20various%20tasks%20using%20a%20quadratic%20approximation%0Asurrogate%20model%20derived%20from%20a%20pre-selected%20set%20of%20scaling%20coefficients%2C%0Aenabling%20amortized%20inference.%20Experimental%20results%20on%20vision%20and%20natural%0Alanguage%20processing%20tasks%20show%20that%20MAP%20can%20accurately%20identify%20the%20Pareto%0Afront.%20To%20further%20reduce%20the%20required%20computation%20of%20MAP%2C%20we%20propose%20%281%29%20a%0ABayesian%20adaptive%20sampling%20algorithm%20and%20%282%29%20a%20nested%20merging%20scheme%20with%0Amultiple%20stages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAP%253A%2520Low-compute%2520Model%2520Merging%2520with%2520Amortized%2520Pareto%2520Fronts%2520via%250A%2520%2520Quadratic%2520Approximation%26entry.906535625%3DLu%2520Li%2520and%2520Tianyu%2520Zhang%2520and%2520Zhiqi%2520Bu%2520and%2520Suyuchen%2520Wang%2520and%2520Huan%2520He%2520and%2520Jie%2520Fu%2520and%2520Yonghui%2520Wu%2520and%2520Jiang%2520Bian%2520and%2520Yong%2520Chen%2520and%2520Yoshua%2520Bengio%26entry.1292438233%3D%2520%2520Model%2520merging%2520has%2520emerged%2520as%2520an%2520effective%2520approach%2520to%2520combine%2520multiple%250Asingle-task%2520models%252C%2520fine-tuned%2520from%2520the%2520same%2520pre-trained%2520model%252C%2520into%2520a%250Amultitask%2520model.%2520This%2520process%2520typically%2520involves%2520computing%2520a%2520weighted%2520average%250Aof%2520the%2520model%2520parameters%2520without%2520any%2520additional%2520training.%2520Existing%2520model-merging%250Amethods%2520focus%2520on%2520enhancing%2520average%2520task%2520accuracy.%2520However%252C%2520interference%2520and%250Aconflicts%2520between%2520the%2520objectives%2520of%2520different%2520tasks%2520can%2520lead%2520to%2520trade-offs%250Aduring%2520model%2520merging.%2520In%2520real-world%2520applications%252C%2520a%2520set%2520of%2520solutions%2520with%250Avarious%2520trade-offs%2520can%2520be%2520more%2520informative%252C%2520helping%2520practitioners%2520make%250Adecisions%2520based%2520on%2520diverse%2520preferences.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Alow-compute%2520algorithm%252C%2520Model%2520Merging%2520with%2520Amortized%2520Pareto%2520Front%2520%2528MAP%2529.%2520MAP%250Aidentifies%2520a%2520Pareto%2520set%2520of%2520scaling%2520coefficients%2520for%2520merging%2520multiple%2520models%2520to%250Areflect%2520the%2520trade-offs.%2520The%2520core%2520component%2520of%2520MAP%2520is%2520approximating%2520the%250Aevaluation%2520metrics%2520of%2520the%2520various%2520tasks%2520using%2520a%2520quadratic%2520approximation%250Asurrogate%2520model%2520derived%2520from%2520a%2520pre-selected%2520set%2520of%2520scaling%2520coefficients%252C%250Aenabling%2520amortized%2520inference.%2520Experimental%2520results%2520on%2520vision%2520and%2520natural%250Alanguage%2520processing%2520tasks%2520show%2520that%2520MAP%2520can%2520accurately%2520identify%2520the%2520Pareto%250Afront.%2520To%2520further%2520reduce%2520the%2520required%2520computation%2520of%2520MAP%252C%2520we%2520propose%2520%25281%2529%2520a%250ABayesian%2520adaptive%2520sampling%2520algorithm%2520and%2520%25282%2529%2520a%2520nested%2520merging%2520scheme%2520with%250Amultiple%2520stages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAP%3A%20Low-compute%20Model%20Merging%20with%20Amortized%20Pareto%20Fronts%20via%0A%20%20Quadratic%20Approximation&entry.906535625=Lu%20Li%20and%20Tianyu%20Zhang%20and%20Zhiqi%20Bu%20and%20Suyuchen%20Wang%20and%20Huan%20He%20and%20Jie%20Fu%20and%20Yonghui%20Wu%20and%20Jiang%20Bian%20and%20Yong%20Chen%20and%20Yoshua%20Bengio&entry.1292438233=%20%20Model%20merging%20has%20emerged%20as%20an%20effective%20approach%20to%20combine%20multiple%0Asingle-task%20models%2C%20fine-tuned%20from%20the%20same%20pre-trained%20model%2C%20into%20a%0Amultitask%20model.%20This%20process%20typically%20involves%20computing%20a%20weighted%20average%0Aof%20the%20model%20parameters%20without%20any%20additional%20training.%20Existing%20model-merging%0Amethods%20focus%20on%20enhancing%20average%20task%20accuracy.%20However%2C%20interference%20and%0Aconflicts%20between%20the%20objectives%20of%20different%20tasks%20can%20lead%20to%20trade-offs%0Aduring%20model%20merging.%20In%20real-world%20applications%2C%20a%20set%20of%20solutions%20with%0Avarious%20trade-offs%20can%20be%20more%20informative%2C%20helping%20practitioners%20make%0Adecisions%20based%20on%20diverse%20preferences.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Alow-compute%20algorithm%2C%20Model%20Merging%20with%20Amortized%20Pareto%20Front%20%28MAP%29.%20MAP%0Aidentifies%20a%20Pareto%20set%20of%20scaling%20coefficients%20for%20merging%20multiple%20models%20to%0Areflect%20the%20trade-offs.%20The%20core%20component%20of%20MAP%20is%20approximating%20the%0Aevaluation%20metrics%20of%20the%20various%20tasks%20using%20a%20quadratic%20approximation%0Asurrogate%20model%20derived%20from%20a%20pre-selected%20set%20of%20scaling%20coefficients%2C%0Aenabling%20amortized%20inference.%20Experimental%20results%20on%20vision%20and%20natural%0Alanguage%20processing%20tasks%20show%20that%20MAP%20can%20accurately%20identify%20the%20Pareto%0Afront.%20To%20further%20reduce%20the%20required%20computation%20of%20MAP%2C%20we%20propose%20%281%29%20a%0ABayesian%20adaptive%20sampling%20algorithm%20and%20%282%29%20a%20nested%20merging%20scheme%20with%0Amultiple%20stages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07529v1&entry.124074799=Read"},
{"title": "Improving Logits-based Detector without Logits from Black-box LLMs", "author": "Cong Zeng and Shengkun Tang and Xianjun Yang and Yuanzhou Chen and Yiyou Sun and zhiqiang xu and Yao Li and Haifeng Chen and Wei Cheng and Dongkuan Xu", "abstract": "  The advent of Large Language Models (LLMs) has revolutionized text\ngeneration, producing outputs that closely mimic human writing. This blurring\nof lines between machine- and human-written text presents new challenges in\ndistinguishing one from the other a task further complicated by the frequent\nupdates and closed nature of leading proprietary LLMs. Traditional logits-based\ndetection methods leverage surrogate models for identifying LLM-generated\ncontent when the exact logits are unavailable from black-box LLMs. However,\nthese methods grapple with the misalignment between the distributions of the\nsurrogate and the often undisclosed target models, leading to performance\ndegradation, particularly with the introduction of new, closed-source models.\nFurthermore, while current methodologies are generally effective when the\nsource model is identified, they falter in scenarios where the model version\nremains unknown, or the test set comprises outputs from various source models.\nTo address these limitations, we present Distribution-Aligned LLMs Detection\n(DALD), an innovative framework that redefines the state-of-the-art performance\nin black-box text detection even without logits from source LLMs. DALD is\ndesigned to align the surrogate model's distribution with that of unknown\ntarget LLMs, ensuring enhanced detection capability and resilience against\nrapid model iterations with minimal training investment. By leveraging corpus\nsamples from publicly accessible outputs of advanced models such as ChatGPT,\nGPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with\nunknown source model distributions effectively.\n", "link": "http://arxiv.org/abs/2406.05232v2", "date": "2024-06-11", "relevancy": 2.034, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5344}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5165}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Logits-based%20Detector%20without%20Logits%20from%20Black-box%20LLMs&body=Title%3A%20Improving%20Logits-based%20Detector%20without%20Logits%20from%20Black-box%20LLMs%0AAuthor%3A%20Cong%20Zeng%20and%20Shengkun%20Tang%20and%20Xianjun%20Yang%20and%20Yuanzhou%20Chen%20and%20Yiyou%20Sun%20and%20zhiqiang%20xu%20and%20Yao%20Li%20and%20Haifeng%20Chen%20and%20Wei%20Cheng%20and%20Dongkuan%20Xu%0AAbstract%3A%20%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20revolutionized%20text%0Ageneration%2C%20producing%20outputs%20that%20closely%20mimic%20human%20writing.%20This%20blurring%0Aof%20lines%20between%20machine-%20and%20human-written%20text%20presents%20new%20challenges%20in%0Adistinguishing%20one%20from%20the%20other%20a%20task%20further%20complicated%20by%20the%20frequent%0Aupdates%20and%20closed%20nature%20of%20leading%20proprietary%20LLMs.%20Traditional%20logits-based%0Adetection%20methods%20leverage%20surrogate%20models%20for%20identifying%20LLM-generated%0Acontent%20when%20the%20exact%20logits%20are%20unavailable%20from%20black-box%20LLMs.%20However%2C%0Athese%20methods%20grapple%20with%20the%20misalignment%20between%20the%20distributions%20of%20the%0Asurrogate%20and%20the%20often%20undisclosed%20target%20models%2C%20leading%20to%20performance%0Adegradation%2C%20particularly%20with%20the%20introduction%20of%20new%2C%20closed-source%20models.%0AFurthermore%2C%20while%20current%20methodologies%20are%20generally%20effective%20when%20the%0Asource%20model%20is%20identified%2C%20they%20falter%20in%20scenarios%20where%20the%20model%20version%0Aremains%20unknown%2C%20or%20the%20test%20set%20comprises%20outputs%20from%20various%20source%20models.%0ATo%20address%20these%20limitations%2C%20we%20present%20Distribution-Aligned%20LLMs%20Detection%0A%28DALD%29%2C%20an%20innovative%20framework%20that%20redefines%20the%20state-of-the-art%20performance%0Ain%20black-box%20text%20detection%20even%20without%20logits%20from%20source%20LLMs.%20DALD%20is%0Adesigned%20to%20align%20the%20surrogate%20model%27s%20distribution%20with%20that%20of%20unknown%0Atarget%20LLMs%2C%20ensuring%20enhanced%20detection%20capability%20and%20resilience%20against%0Arapid%20model%20iterations%20with%20minimal%20training%20investment.%20By%20leveraging%20corpus%0Asamples%20from%20publicly%20accessible%20outputs%20of%20advanced%20models%20such%20as%20ChatGPT%2C%0AGPT-4%20and%20Claude-3%2C%20DALD%20fine-tunes%20surrogate%20models%20to%20synchronize%20with%0Aunknown%20source%20model%20distributions%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05232v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Logits-based%2520Detector%2520without%2520Logits%2520from%2520Black-box%2520LLMs%26entry.906535625%3DCong%2520Zeng%2520and%2520Shengkun%2520Tang%2520and%2520Xianjun%2520Yang%2520and%2520Yuanzhou%2520Chen%2520and%2520Yiyou%2520Sun%2520and%2520zhiqiang%2520xu%2520and%2520Yao%2520Li%2520and%2520Haifeng%2520Chen%2520and%2520Wei%2520Cheng%2520and%2520Dongkuan%2520Xu%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520revolutionized%2520text%250Ageneration%252C%2520producing%2520outputs%2520that%2520closely%2520mimic%2520human%2520writing.%2520This%2520blurring%250Aof%2520lines%2520between%2520machine-%2520and%2520human-written%2520text%2520presents%2520new%2520challenges%2520in%250Adistinguishing%2520one%2520from%2520the%2520other%2520a%2520task%2520further%2520complicated%2520by%2520the%2520frequent%250Aupdates%2520and%2520closed%2520nature%2520of%2520leading%2520proprietary%2520LLMs.%2520Traditional%2520logits-based%250Adetection%2520methods%2520leverage%2520surrogate%2520models%2520for%2520identifying%2520LLM-generated%250Acontent%2520when%2520the%2520exact%2520logits%2520are%2520unavailable%2520from%2520black-box%2520LLMs.%2520However%252C%250Athese%2520methods%2520grapple%2520with%2520the%2520misalignment%2520between%2520the%2520distributions%2520of%2520the%250Asurrogate%2520and%2520the%2520often%2520undisclosed%2520target%2520models%252C%2520leading%2520to%2520performance%250Adegradation%252C%2520particularly%2520with%2520the%2520introduction%2520of%2520new%252C%2520closed-source%2520models.%250AFurthermore%252C%2520while%2520current%2520methodologies%2520are%2520generally%2520effective%2520when%2520the%250Asource%2520model%2520is%2520identified%252C%2520they%2520falter%2520in%2520scenarios%2520where%2520the%2520model%2520version%250Aremains%2520unknown%252C%2520or%2520the%2520test%2520set%2520comprises%2520outputs%2520from%2520various%2520source%2520models.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520present%2520Distribution-Aligned%2520LLMs%2520Detection%250A%2528DALD%2529%252C%2520an%2520innovative%2520framework%2520that%2520redefines%2520the%2520state-of-the-art%2520performance%250Ain%2520black-box%2520text%2520detection%2520even%2520without%2520logits%2520from%2520source%2520LLMs.%2520DALD%2520is%250Adesigned%2520to%2520align%2520the%2520surrogate%2520model%2527s%2520distribution%2520with%2520that%2520of%2520unknown%250Atarget%2520LLMs%252C%2520ensuring%2520enhanced%2520detection%2520capability%2520and%2520resilience%2520against%250Arapid%2520model%2520iterations%2520with%2520minimal%2520training%2520investment.%2520By%2520leveraging%2520corpus%250Asamples%2520from%2520publicly%2520accessible%2520outputs%2520of%2520advanced%2520models%2520such%2520as%2520ChatGPT%252C%250AGPT-4%2520and%2520Claude-3%252C%2520DALD%2520fine-tunes%2520surrogate%2520models%2520to%2520synchronize%2520with%250Aunknown%2520source%2520model%2520distributions%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05232v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Logits-based%20Detector%20without%20Logits%20from%20Black-box%20LLMs&entry.906535625=Cong%20Zeng%20and%20Shengkun%20Tang%20and%20Xianjun%20Yang%20and%20Yuanzhou%20Chen%20and%20Yiyou%20Sun%20and%20zhiqiang%20xu%20and%20Yao%20Li%20and%20Haifeng%20Chen%20and%20Wei%20Cheng%20and%20Dongkuan%20Xu&entry.1292438233=%20%20The%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20revolutionized%20text%0Ageneration%2C%20producing%20outputs%20that%20closely%20mimic%20human%20writing.%20This%20blurring%0Aof%20lines%20between%20machine-%20and%20human-written%20text%20presents%20new%20challenges%20in%0Adistinguishing%20one%20from%20the%20other%20a%20task%20further%20complicated%20by%20the%20frequent%0Aupdates%20and%20closed%20nature%20of%20leading%20proprietary%20LLMs.%20Traditional%20logits-based%0Adetection%20methods%20leverage%20surrogate%20models%20for%20identifying%20LLM-generated%0Acontent%20when%20the%20exact%20logits%20are%20unavailable%20from%20black-box%20LLMs.%20However%2C%0Athese%20methods%20grapple%20with%20the%20misalignment%20between%20the%20distributions%20of%20the%0Asurrogate%20and%20the%20often%20undisclosed%20target%20models%2C%20leading%20to%20performance%0Adegradation%2C%20particularly%20with%20the%20introduction%20of%20new%2C%20closed-source%20models.%0AFurthermore%2C%20while%20current%20methodologies%20are%20generally%20effective%20when%20the%0Asource%20model%20is%20identified%2C%20they%20falter%20in%20scenarios%20where%20the%20model%20version%0Aremains%20unknown%2C%20or%20the%20test%20set%20comprises%20outputs%20from%20various%20source%20models.%0ATo%20address%20these%20limitations%2C%20we%20present%20Distribution-Aligned%20LLMs%20Detection%0A%28DALD%29%2C%20an%20innovative%20framework%20that%20redefines%20the%20state-of-the-art%20performance%0Ain%20black-box%20text%20detection%20even%20without%20logits%20from%20source%20LLMs.%20DALD%20is%0Adesigned%20to%20align%20the%20surrogate%20model%27s%20distribution%20with%20that%20of%20unknown%0Atarget%20LLMs%2C%20ensuring%20enhanced%20detection%20capability%20and%20resilience%20against%0Arapid%20model%20iterations%20with%20minimal%20training%20investment.%20By%20leveraging%20corpus%0Asamples%20from%20publicly%20accessible%20outputs%20of%20advanced%20models%20such%20as%20ChatGPT%2C%0AGPT-4%20and%20Claude-3%2C%20DALD%20fine-tunes%20surrogate%20models%20to%20synchronize%20with%0Aunknown%20source%20model%20distributions%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05232v2&entry.124074799=Read"},
{"title": "On the Convergence of Loss and Uncertainty-based Active Learning\n  Algorithms", "author": "Daniel Haimovich and Dima Karamshuk and Fridolin Linder and Niek Tax and Milan Vojnovic", "abstract": "  We investigate the convergence rates and data sample sizes required for\ntraining a machine learning model using a stochastic gradient descent (SGD)\nalgorithm, where data points are sampled based on either their loss value or\nuncertainty value. These training methods are particularly relevant for active\nlearning and data subset selection problems. For SGD with a constant step size\nupdate, we present convergence results for linear classifiers and linearly\nseparable datasets using squared hinge loss and similar training loss\nfunctions. Additionally, we extend our analysis to more general classifiers and\ndatasets, considering a wide range of loss-based sampling strategies and smooth\nconvex training loss functions. We propose a novel algorithm called\nAdaptive-Weight Sampling (AWS) that utilizes SGD with an adaptive step size\nthat achieves stochastic Polyak's step size in expectation. We establish\nconvergence rate results for AWS for smooth convex training loss functions. Our\nnumerical experiments demonstrate the efficiency of AWS on various datasets by\nusing either exact or estimated loss values.\n", "link": "http://arxiv.org/abs/2312.13927v3", "date": "2024-06-11", "relevancy": 2.0292, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5036}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20of%20Loss%20and%20Uncertainty-based%20Active%20Learning%0A%20%20Algorithms&body=Title%3A%20On%20the%20Convergence%20of%20Loss%20and%20Uncertainty-based%20Active%20Learning%0A%20%20Algorithms%0AAuthor%3A%20Daniel%20Haimovich%20and%20Dima%20Karamshuk%20and%20Fridolin%20Linder%20and%20Niek%20Tax%20and%20Milan%20Vojnovic%0AAbstract%3A%20%20%20We%20investigate%20the%20convergence%20rates%20and%20data%20sample%20sizes%20required%20for%0Atraining%20a%20machine%20learning%20model%20using%20a%20stochastic%20gradient%20descent%20%28SGD%29%0Aalgorithm%2C%20where%20data%20points%20are%20sampled%20based%20on%20either%20their%20loss%20value%20or%0Auncertainty%20value.%20These%20training%20methods%20are%20particularly%20relevant%20for%20active%0Alearning%20and%20data%20subset%20selection%20problems.%20For%20SGD%20with%20a%20constant%20step%20size%0Aupdate%2C%20we%20present%20convergence%20results%20for%20linear%20classifiers%20and%20linearly%0Aseparable%20datasets%20using%20squared%20hinge%20loss%20and%20similar%20training%20loss%0Afunctions.%20Additionally%2C%20we%20extend%20our%20analysis%20to%20more%20general%20classifiers%20and%0Adatasets%2C%20considering%20a%20wide%20range%20of%20loss-based%20sampling%20strategies%20and%20smooth%0Aconvex%20training%20loss%20functions.%20We%20propose%20a%20novel%20algorithm%20called%0AAdaptive-Weight%20Sampling%20%28AWS%29%20that%20utilizes%20SGD%20with%20an%20adaptive%20step%20size%0Athat%20achieves%20stochastic%20Polyak%27s%20step%20size%20in%20expectation.%20We%20establish%0Aconvergence%20rate%20results%20for%20AWS%20for%20smooth%20convex%20training%20loss%20functions.%20Our%0Anumerical%20experiments%20demonstrate%20the%20efficiency%20of%20AWS%20on%20various%20datasets%20by%0Ausing%20either%20exact%20or%20estimated%20loss%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Convergence%2520of%2520Loss%2520and%2520Uncertainty-based%2520Active%2520Learning%250A%2520%2520Algorithms%26entry.906535625%3DDaniel%2520Haimovich%2520and%2520Dima%2520Karamshuk%2520and%2520Fridolin%2520Linder%2520and%2520Niek%2520Tax%2520and%2520Milan%2520Vojnovic%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520convergence%2520rates%2520and%2520data%2520sample%2520sizes%2520required%2520for%250Atraining%2520a%2520machine%2520learning%2520model%2520using%2520a%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%250Aalgorithm%252C%2520where%2520data%2520points%2520are%2520sampled%2520based%2520on%2520either%2520their%2520loss%2520value%2520or%250Auncertainty%2520value.%2520These%2520training%2520methods%2520are%2520particularly%2520relevant%2520for%2520active%250Alearning%2520and%2520data%2520subset%2520selection%2520problems.%2520For%2520SGD%2520with%2520a%2520constant%2520step%2520size%250Aupdate%252C%2520we%2520present%2520convergence%2520results%2520for%2520linear%2520classifiers%2520and%2520linearly%250Aseparable%2520datasets%2520using%2520squared%2520hinge%2520loss%2520and%2520similar%2520training%2520loss%250Afunctions.%2520Additionally%252C%2520we%2520extend%2520our%2520analysis%2520to%2520more%2520general%2520classifiers%2520and%250Adatasets%252C%2520considering%2520a%2520wide%2520range%2520of%2520loss-based%2520sampling%2520strategies%2520and%2520smooth%250Aconvex%2520training%2520loss%2520functions.%2520We%2520propose%2520a%2520novel%2520algorithm%2520called%250AAdaptive-Weight%2520Sampling%2520%2528AWS%2529%2520that%2520utilizes%2520SGD%2520with%2520an%2520adaptive%2520step%2520size%250Athat%2520achieves%2520stochastic%2520Polyak%2527s%2520step%2520size%2520in%2520expectation.%2520We%2520establish%250Aconvergence%2520rate%2520results%2520for%2520AWS%2520for%2520smooth%2520convex%2520training%2520loss%2520functions.%2520Our%250Anumerical%2520experiments%2520demonstrate%2520the%2520efficiency%2520of%2520AWS%2520on%2520various%2520datasets%2520by%250Ausing%2520either%2520exact%2520or%2520estimated%2520loss%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20of%20Loss%20and%20Uncertainty-based%20Active%20Learning%0A%20%20Algorithms&entry.906535625=Daniel%20Haimovich%20and%20Dima%20Karamshuk%20and%20Fridolin%20Linder%20and%20Niek%20Tax%20and%20Milan%20Vojnovic&entry.1292438233=%20%20We%20investigate%20the%20convergence%20rates%20and%20data%20sample%20sizes%20required%20for%0Atraining%20a%20machine%20learning%20model%20using%20a%20stochastic%20gradient%20descent%20%28SGD%29%0Aalgorithm%2C%20where%20data%20points%20are%20sampled%20based%20on%20either%20their%20loss%20value%20or%0Auncertainty%20value.%20These%20training%20methods%20are%20particularly%20relevant%20for%20active%0Alearning%20and%20data%20subset%20selection%20problems.%20For%20SGD%20with%20a%20constant%20step%20size%0Aupdate%2C%20we%20present%20convergence%20results%20for%20linear%20classifiers%20and%20linearly%0Aseparable%20datasets%20using%20squared%20hinge%20loss%20and%20similar%20training%20loss%0Afunctions.%20Additionally%2C%20we%20extend%20our%20analysis%20to%20more%20general%20classifiers%20and%0Adatasets%2C%20considering%20a%20wide%20range%20of%20loss-based%20sampling%20strategies%20and%20smooth%0Aconvex%20training%20loss%20functions.%20We%20propose%20a%20novel%20algorithm%20called%0AAdaptive-Weight%20Sampling%20%28AWS%29%20that%20utilizes%20SGD%20with%20an%20adaptive%20step%20size%0Athat%20achieves%20stochastic%20Polyak%27s%20step%20size%20in%20expectation.%20We%20establish%0Aconvergence%20rate%20results%20for%20AWS%20for%20smooth%20convex%20training%20loss%20functions.%20Our%0Anumerical%20experiments%20demonstrate%20the%20efficiency%20of%20AWS%20on%20various%20datasets%20by%0Ausing%20either%20exact%20or%20estimated%20loss%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13927v3&entry.124074799=Read"},
{"title": "Enhancing CTC-based speech recognition with diverse modeling units", "author": "Shiyi Han and Zhihong Lei and Mingbin Xu and Xingyu Na and Zhen Huang", "abstract": "  In recent years, the evolution of end-to-end (E2E) automatic speech\nrecognition (ASR) models has been remarkable, largely due to advances in deep\nlearning architectures like transformer. On top of E2E systems, researchers\nhave achieved substantial accuracy improvement by rescoring E2E model's N-best\nhypotheses with a phoneme-based model. This raises an interesting question\nabout where the improvements come from other than the system combination\neffect. We examine the underlying mechanisms driving these gains and propose an\nefficient joint training approach, where E2E models are trained jointly with\ndiverse modeling units. This methodology does not only align the strengths of\nboth phoneme and grapheme-based models but also reveals that using these\ndiverse modeling units in a synergistic way can significantly enhance model\naccuracy. Our findings offer new insights into the optimal integration of\nheterogeneous modeling units in the development of more robust and accurate ASR\nsystems.\n", "link": "http://arxiv.org/abs/2406.03274v2", "date": "2024-06-11", "relevancy": 2.0264, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20CTC-based%20speech%20recognition%20with%20diverse%20modeling%20units&body=Title%3A%20Enhancing%20CTC-based%20speech%20recognition%20with%20diverse%20modeling%20units%0AAuthor%3A%20Shiyi%20Han%20and%20Zhihong%20Lei%20and%20Mingbin%20Xu%20and%20Xingyu%20Na%20and%20Zhen%20Huang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20evolution%20of%20end-to-end%20%28E2E%29%20automatic%20speech%0Arecognition%20%28ASR%29%20models%20has%20been%20remarkable%2C%20largely%20due%20to%20advances%20in%20deep%0Alearning%20architectures%20like%20transformer.%20On%20top%20of%20E2E%20systems%2C%20researchers%0Ahave%20achieved%20substantial%20accuracy%20improvement%20by%20rescoring%20E2E%20model%27s%20N-best%0Ahypotheses%20with%20a%20phoneme-based%20model.%20This%20raises%20an%20interesting%20question%0Aabout%20where%20the%20improvements%20come%20from%20other%20than%20the%20system%20combination%0Aeffect.%20We%20examine%20the%20underlying%20mechanisms%20driving%20these%20gains%20and%20propose%20an%0Aefficient%20joint%20training%20approach%2C%20where%20E2E%20models%20are%20trained%20jointly%20with%0Adiverse%20modeling%20units.%20This%20methodology%20does%20not%20only%20align%20the%20strengths%20of%0Aboth%20phoneme%20and%20grapheme-based%20models%20but%20also%20reveals%20that%20using%20these%0Adiverse%20modeling%20units%20in%20a%20synergistic%20way%20can%20significantly%20enhance%20model%0Aaccuracy.%20Our%20findings%20offer%20new%20insights%20into%20the%20optimal%20integration%20of%0Aheterogeneous%20modeling%20units%20in%20the%20development%20of%20more%20robust%20and%20accurate%20ASR%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520CTC-based%2520speech%2520recognition%2520with%2520diverse%2520modeling%2520units%26entry.906535625%3DShiyi%2520Han%2520and%2520Zhihong%2520Lei%2520and%2520Mingbin%2520Xu%2520and%2520Xingyu%2520Na%2520and%2520Zhen%2520Huang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520evolution%2520of%2520end-to-end%2520%2528E2E%2529%2520automatic%2520speech%250Arecognition%2520%2528ASR%2529%2520models%2520has%2520been%2520remarkable%252C%2520largely%2520due%2520to%2520advances%2520in%2520deep%250Alearning%2520architectures%2520like%2520transformer.%2520On%2520top%2520of%2520E2E%2520systems%252C%2520researchers%250Ahave%2520achieved%2520substantial%2520accuracy%2520improvement%2520by%2520rescoring%2520E2E%2520model%2527s%2520N-best%250Ahypotheses%2520with%2520a%2520phoneme-based%2520model.%2520This%2520raises%2520an%2520interesting%2520question%250Aabout%2520where%2520the%2520improvements%2520come%2520from%2520other%2520than%2520the%2520system%2520combination%250Aeffect.%2520We%2520examine%2520the%2520underlying%2520mechanisms%2520driving%2520these%2520gains%2520and%2520propose%2520an%250Aefficient%2520joint%2520training%2520approach%252C%2520where%2520E2E%2520models%2520are%2520trained%2520jointly%2520with%250Adiverse%2520modeling%2520units.%2520This%2520methodology%2520does%2520not%2520only%2520align%2520the%2520strengths%2520of%250Aboth%2520phoneme%2520and%2520grapheme-based%2520models%2520but%2520also%2520reveals%2520that%2520using%2520these%250Adiverse%2520modeling%2520units%2520in%2520a%2520synergistic%2520way%2520can%2520significantly%2520enhance%2520model%250Aaccuracy.%2520Our%2520findings%2520offer%2520new%2520insights%2520into%2520the%2520optimal%2520integration%2520of%250Aheterogeneous%2520modeling%2520units%2520in%2520the%2520development%2520of%2520more%2520robust%2520and%2520accurate%2520ASR%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20CTC-based%20speech%20recognition%20with%20diverse%20modeling%20units&entry.906535625=Shiyi%20Han%20and%20Zhihong%20Lei%20and%20Mingbin%20Xu%20and%20Xingyu%20Na%20and%20Zhen%20Huang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20evolution%20of%20end-to-end%20%28E2E%29%20automatic%20speech%0Arecognition%20%28ASR%29%20models%20has%20been%20remarkable%2C%20largely%20due%20to%20advances%20in%20deep%0Alearning%20architectures%20like%20transformer.%20On%20top%20of%20E2E%20systems%2C%20researchers%0Ahave%20achieved%20substantial%20accuracy%20improvement%20by%20rescoring%20E2E%20model%27s%20N-best%0Ahypotheses%20with%20a%20phoneme-based%20model.%20This%20raises%20an%20interesting%20question%0Aabout%20where%20the%20improvements%20come%20from%20other%20than%20the%20system%20combination%0Aeffect.%20We%20examine%20the%20underlying%20mechanisms%20driving%20these%20gains%20and%20propose%20an%0Aefficient%20joint%20training%20approach%2C%20where%20E2E%20models%20are%20trained%20jointly%20with%0Adiverse%20modeling%20units.%20This%20methodology%20does%20not%20only%20align%20the%20strengths%20of%0Aboth%20phoneme%20and%20grapheme-based%20models%20but%20also%20reveals%20that%20using%20these%0Adiverse%20modeling%20units%20in%20a%20synergistic%20way%20can%20significantly%20enhance%20model%0Aaccuracy.%20Our%20findings%20offer%20new%20insights%20into%20the%20optimal%20integration%20of%0Aheterogeneous%20modeling%20units%20in%20the%20development%20of%20more%20robust%20and%20accurate%20ASR%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03274v2&entry.124074799=Read"},
{"title": "Large Language Model Meets Graph Neural Network in Knowledge\n  Distillation", "author": "Shengxiang Hu and Guobing Zou and Song Yang and Yanglan Gan and Bofeng Zhang and Yixin Chen", "abstract": "  In service-oriented architectures, accurately predicting the Quality of\nService (QoS) is crucial for maintaining reliability and enhancing user\nsatisfaction. However, significant challenges remain due to existing methods\nalways overlooking high-order latent collaborative relationships between users\nand services and failing to dynamically adjust feature learning for every\nspecific user-service invocation, which are critical for learning accurate\nfeatures. Additionally, reliance on RNNs for capturing QoS evolution hampers\nmodels' ability to detect long-term trends due to difficulties in managing\nlong-range dependencies. To address these challenges, we propose the\n\\underline{T}arget-Prompt \\underline{O}nline \\underline{G}raph\n\\underline{C}ollaborative \\underline{L}earning (TOGCL) framework for\ntemporal-aware QoS prediction. TOGCL leverages a dynamic user-service\ninvocation graph to model historical interactions, providing a comprehensive\nrepresentation of user-service relationships. Building on this graph, it\ndevelops a target-prompt graph attention network to extract online deep latent\nfeatures of users and services at each time slice, simultaneously considering\nimplicit collaborative relationships between target users/services and their\nneighbors, as well as relevant historical QoS values. Additionally, a\nmulti-layer Transformer encoder is employed to uncover temporal feature\nevolution patterns of users and services, leading to temporal-aware QoS\nprediction. Extensive experiments conducted on the WS-DREAM dataset demonstrate\nthat our proposed TOGCL framework significantly outperforms state-of-the-art\nmethods across multiple metrics, achieving improvements of up to 38.80\\%. These\nresults underscore the effectiveness of the TOGCL framework for precise\ntemporal QoS prediction.\n", "link": "http://arxiv.org/abs/2402.05894v4", "date": "2024-06-11", "relevancy": 2.025, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5254}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5029}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Meets%20Graph%20Neural%20Network%20in%20Knowledge%0A%20%20Distillation&body=Title%3A%20Large%20Language%20Model%20Meets%20Graph%20Neural%20Network%20in%20Knowledge%0A%20%20Distillation%0AAuthor%3A%20Shengxiang%20Hu%20and%20Guobing%20Zou%20and%20Song%20Yang%20and%20Yanglan%20Gan%20and%20Bofeng%20Zhang%20and%20Yixin%20Chen%0AAbstract%3A%20%20%20In%20service-oriented%20architectures%2C%20accurately%20predicting%20the%20Quality%20of%0AService%20%28QoS%29%20is%20crucial%20for%20maintaining%20reliability%20and%20enhancing%20user%0Asatisfaction.%20However%2C%20significant%20challenges%20remain%20due%20to%20existing%20methods%0Aalways%20overlooking%20high-order%20latent%20collaborative%20relationships%20between%20users%0Aand%20services%20and%20failing%20to%20dynamically%20adjust%20feature%20learning%20for%20every%0Aspecific%20user-service%20invocation%2C%20which%20are%20critical%20for%20learning%20accurate%0Afeatures.%20Additionally%2C%20reliance%20on%20RNNs%20for%20capturing%20QoS%20evolution%20hampers%0Amodels%27%20ability%20to%20detect%20long-term%20trends%20due%20to%20difficulties%20in%20managing%0Along-range%20dependencies.%20To%20address%20these%20challenges%2C%20we%20propose%20the%0A%5Cunderline%7BT%7Darget-Prompt%20%5Cunderline%7BO%7Dnline%20%5Cunderline%7BG%7Draph%0A%5Cunderline%7BC%7Dollaborative%20%5Cunderline%7BL%7Dearning%20%28TOGCL%29%20framework%20for%0Atemporal-aware%20QoS%20prediction.%20TOGCL%20leverages%20a%20dynamic%20user-service%0Ainvocation%20graph%20to%20model%20historical%20interactions%2C%20providing%20a%20comprehensive%0Arepresentation%20of%20user-service%20relationships.%20Building%20on%20this%20graph%2C%20it%0Adevelops%20a%20target-prompt%20graph%20attention%20network%20to%20extract%20online%20deep%20latent%0Afeatures%20of%20users%20and%20services%20at%20each%20time%20slice%2C%20simultaneously%20considering%0Aimplicit%20collaborative%20relationships%20between%20target%20users/services%20and%20their%0Aneighbors%2C%20as%20well%20as%20relevant%20historical%20QoS%20values.%20Additionally%2C%20a%0Amulti-layer%20Transformer%20encoder%20is%20employed%20to%20uncover%20temporal%20feature%0Aevolution%20patterns%20of%20users%20and%20services%2C%20leading%20to%20temporal-aware%20QoS%0Aprediction.%20Extensive%20experiments%20conducted%20on%20the%20WS-DREAM%20dataset%20demonstrate%0Athat%20our%20proposed%20TOGCL%20framework%20significantly%20outperforms%20state-of-the-art%0Amethods%20across%20multiple%20metrics%2C%20achieving%20improvements%20of%20up%20to%2038.80%5C%25.%20These%0Aresults%20underscore%20the%20effectiveness%20of%20the%20TOGCL%20framework%20for%20precise%0Atemporal%20QoS%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05894v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Meets%2520Graph%2520Neural%2520Network%2520in%2520Knowledge%250A%2520%2520Distillation%26entry.906535625%3DShengxiang%2520Hu%2520and%2520Guobing%2520Zou%2520and%2520Song%2520Yang%2520and%2520Yanglan%2520Gan%2520and%2520Bofeng%2520Zhang%2520and%2520Yixin%2520Chen%26entry.1292438233%3D%2520%2520In%2520service-oriented%2520architectures%252C%2520accurately%2520predicting%2520the%2520Quality%2520of%250AService%2520%2528QoS%2529%2520is%2520crucial%2520for%2520maintaining%2520reliability%2520and%2520enhancing%2520user%250Asatisfaction.%2520However%252C%2520significant%2520challenges%2520remain%2520due%2520to%2520existing%2520methods%250Aalways%2520overlooking%2520high-order%2520latent%2520collaborative%2520relationships%2520between%2520users%250Aand%2520services%2520and%2520failing%2520to%2520dynamically%2520adjust%2520feature%2520learning%2520for%2520every%250Aspecific%2520user-service%2520invocation%252C%2520which%2520are%2520critical%2520for%2520learning%2520accurate%250Afeatures.%2520Additionally%252C%2520reliance%2520on%2520RNNs%2520for%2520capturing%2520QoS%2520evolution%2520hampers%250Amodels%2527%2520ability%2520to%2520detect%2520long-term%2520trends%2520due%2520to%2520difficulties%2520in%2520managing%250Along-range%2520dependencies.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%250A%255Cunderline%257BT%257Darget-Prompt%2520%255Cunderline%257BO%257Dnline%2520%255Cunderline%257BG%257Draph%250A%255Cunderline%257BC%257Dollaborative%2520%255Cunderline%257BL%257Dearning%2520%2528TOGCL%2529%2520framework%2520for%250Atemporal-aware%2520QoS%2520prediction.%2520TOGCL%2520leverages%2520a%2520dynamic%2520user-service%250Ainvocation%2520graph%2520to%2520model%2520historical%2520interactions%252C%2520providing%2520a%2520comprehensive%250Arepresentation%2520of%2520user-service%2520relationships.%2520Building%2520on%2520this%2520graph%252C%2520it%250Adevelops%2520a%2520target-prompt%2520graph%2520attention%2520network%2520to%2520extract%2520online%2520deep%2520latent%250Afeatures%2520of%2520users%2520and%2520services%2520at%2520each%2520time%2520slice%252C%2520simultaneously%2520considering%250Aimplicit%2520collaborative%2520relationships%2520between%2520target%2520users/services%2520and%2520their%250Aneighbors%252C%2520as%2520well%2520as%2520relevant%2520historical%2520QoS%2520values.%2520Additionally%252C%2520a%250Amulti-layer%2520Transformer%2520encoder%2520is%2520employed%2520to%2520uncover%2520temporal%2520feature%250Aevolution%2520patterns%2520of%2520users%2520and%2520services%252C%2520leading%2520to%2520temporal-aware%2520QoS%250Aprediction.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520WS-DREAM%2520dataset%2520demonstrate%250Athat%2520our%2520proposed%2520TOGCL%2520framework%2520significantly%2520outperforms%2520state-of-the-art%250Amethods%2520across%2520multiple%2520metrics%252C%2520achieving%2520improvements%2520of%2520up%2520to%252038.80%255C%2525.%2520These%250Aresults%2520underscore%2520the%2520effectiveness%2520of%2520the%2520TOGCL%2520framework%2520for%2520precise%250Atemporal%2520QoS%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05894v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Meets%20Graph%20Neural%20Network%20in%20Knowledge%0A%20%20Distillation&entry.906535625=Shengxiang%20Hu%20and%20Guobing%20Zou%20and%20Song%20Yang%20and%20Yanglan%20Gan%20and%20Bofeng%20Zhang%20and%20Yixin%20Chen&entry.1292438233=%20%20In%20service-oriented%20architectures%2C%20accurately%20predicting%20the%20Quality%20of%0AService%20%28QoS%29%20is%20crucial%20for%20maintaining%20reliability%20and%20enhancing%20user%0Asatisfaction.%20However%2C%20significant%20challenges%20remain%20due%20to%20existing%20methods%0Aalways%20overlooking%20high-order%20latent%20collaborative%20relationships%20between%20users%0Aand%20services%20and%20failing%20to%20dynamically%20adjust%20feature%20learning%20for%20every%0Aspecific%20user-service%20invocation%2C%20which%20are%20critical%20for%20learning%20accurate%0Afeatures.%20Additionally%2C%20reliance%20on%20RNNs%20for%20capturing%20QoS%20evolution%20hampers%0Amodels%27%20ability%20to%20detect%20long-term%20trends%20due%20to%20difficulties%20in%20managing%0Along-range%20dependencies.%20To%20address%20these%20challenges%2C%20we%20propose%20the%0A%5Cunderline%7BT%7Darget-Prompt%20%5Cunderline%7BO%7Dnline%20%5Cunderline%7BG%7Draph%0A%5Cunderline%7BC%7Dollaborative%20%5Cunderline%7BL%7Dearning%20%28TOGCL%29%20framework%20for%0Atemporal-aware%20QoS%20prediction.%20TOGCL%20leverages%20a%20dynamic%20user-service%0Ainvocation%20graph%20to%20model%20historical%20interactions%2C%20providing%20a%20comprehensive%0Arepresentation%20of%20user-service%20relationships.%20Building%20on%20this%20graph%2C%20it%0Adevelops%20a%20target-prompt%20graph%20attention%20network%20to%20extract%20online%20deep%20latent%0Afeatures%20of%20users%20and%20services%20at%20each%20time%20slice%2C%20simultaneously%20considering%0Aimplicit%20collaborative%20relationships%20between%20target%20users/services%20and%20their%0Aneighbors%2C%20as%20well%20as%20relevant%20historical%20QoS%20values.%20Additionally%2C%20a%0Amulti-layer%20Transformer%20encoder%20is%20employed%20to%20uncover%20temporal%20feature%0Aevolution%20patterns%20of%20users%20and%20services%2C%20leading%20to%20temporal-aware%20QoS%0Aprediction.%20Extensive%20experiments%20conducted%20on%20the%20WS-DREAM%20dataset%20demonstrate%0Athat%20our%20proposed%20TOGCL%20framework%20significantly%20outperforms%20state-of-the-art%0Amethods%20across%20multiple%20metrics%2C%20achieving%20improvements%20of%20up%20to%2038.80%5C%25.%20These%0Aresults%20underscore%20the%20effectiveness%20of%20the%20TOGCL%20framework%20for%20precise%0Atemporal%20QoS%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05894v4&entry.124074799=Read"},
{"title": "Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language\n  Models for Media Forensics", "author": "Shan Jia and Reilin Lyu and Kangran Zhao and Yize Chen and Zhiyuan Yan and Yan Ju and Chuanbo Hu and Xin Li and Baoyuan Wu and Siwei Lyu", "abstract": "  DeepFakes, which refer to AI-generated media content, have become an\nincreasing concern due to their use as a means for disinformation. Detecting\nDeepFakes is currently solved with programmed machine learning algorithms. In\nthis work, we investigate the capabilities of multimodal large language models\n(LLMs) in DeepFake detection. We conducted qualitative and quantitative\nexperiments to demonstrate multimodal LLMs and show that they can expose\nAI-generated images through careful experimental design and prompt engineering.\nThis is interesting, considering that LLMs are not inherently tailored for\nmedia forensic tasks, and the process does not require programming. We discuss\nthe limitations of multimodal LLMs for these tasks and suggest possible\nimprovements.\n", "link": "http://arxiv.org/abs/2403.14077v4", "date": "2024-06-11", "relevancy": 2.0181, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5166}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5056}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20ChatGPT%20Detect%20DeepFakes%3F%20A%20Study%20of%20Using%20Multimodal%20Large%20Language%0A%20%20Models%20for%20Media%20Forensics&body=Title%3A%20Can%20ChatGPT%20Detect%20DeepFakes%3F%20A%20Study%20of%20Using%20Multimodal%20Large%20Language%0A%20%20Models%20for%20Media%20Forensics%0AAuthor%3A%20Shan%20Jia%20and%20Reilin%20Lyu%20and%20Kangran%20Zhao%20and%20Yize%20Chen%20and%20Zhiyuan%20Yan%20and%20Yan%20Ju%20and%20Chuanbo%20Hu%20and%20Xin%20Li%20and%20Baoyuan%20Wu%20and%20Siwei%20Lyu%0AAbstract%3A%20%20%20DeepFakes%2C%20which%20refer%20to%20AI-generated%20media%20content%2C%20have%20become%20an%0Aincreasing%20concern%20due%20to%20their%20use%20as%20a%20means%20for%20disinformation.%20Detecting%0ADeepFakes%20is%20currently%20solved%20with%20programmed%20machine%20learning%20algorithms.%20In%0Athis%20work%2C%20we%20investigate%20the%20capabilities%20of%20multimodal%20large%20language%20models%0A%28LLMs%29%20in%20DeepFake%20detection.%20We%20conducted%20qualitative%20and%20quantitative%0Aexperiments%20to%20demonstrate%20multimodal%20LLMs%20and%20show%20that%20they%20can%20expose%0AAI-generated%20images%20through%20careful%20experimental%20design%20and%20prompt%20engineering.%0AThis%20is%20interesting%2C%20considering%20that%20LLMs%20are%20not%20inherently%20tailored%20for%0Amedia%20forensic%20tasks%2C%20and%20the%20process%20does%20not%20require%20programming.%20We%20discuss%0Athe%20limitations%20of%20multimodal%20LLMs%20for%20these%20tasks%20and%20suggest%20possible%0Aimprovements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14077v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520ChatGPT%2520Detect%2520DeepFakes%253F%2520A%2520Study%2520of%2520Using%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%2520for%2520Media%2520Forensics%26entry.906535625%3DShan%2520Jia%2520and%2520Reilin%2520Lyu%2520and%2520Kangran%2520Zhao%2520and%2520Yize%2520Chen%2520and%2520Zhiyuan%2520Yan%2520and%2520Yan%2520Ju%2520and%2520Chuanbo%2520Hu%2520and%2520Xin%2520Li%2520and%2520Baoyuan%2520Wu%2520and%2520Siwei%2520Lyu%26entry.1292438233%3D%2520%2520DeepFakes%252C%2520which%2520refer%2520to%2520AI-generated%2520media%2520content%252C%2520have%2520become%2520an%250Aincreasing%2520concern%2520due%2520to%2520their%2520use%2520as%2520a%2520means%2520for%2520disinformation.%2520Detecting%250ADeepFakes%2520is%2520currently%2520solved%2520with%2520programmed%2520machine%2520learning%2520algorithms.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520the%2520capabilities%2520of%2520multimodal%2520large%2520language%2520models%250A%2528LLMs%2529%2520in%2520DeepFake%2520detection.%2520We%2520conducted%2520qualitative%2520and%2520quantitative%250Aexperiments%2520to%2520demonstrate%2520multimodal%2520LLMs%2520and%2520show%2520that%2520they%2520can%2520expose%250AAI-generated%2520images%2520through%2520careful%2520experimental%2520design%2520and%2520prompt%2520engineering.%250AThis%2520is%2520interesting%252C%2520considering%2520that%2520LLMs%2520are%2520not%2520inherently%2520tailored%2520for%250Amedia%2520forensic%2520tasks%252C%2520and%2520the%2520process%2520does%2520not%2520require%2520programming.%2520We%2520discuss%250Athe%2520limitations%2520of%2520multimodal%2520LLMs%2520for%2520these%2520tasks%2520and%2520suggest%2520possible%250Aimprovements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14077v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20ChatGPT%20Detect%20DeepFakes%3F%20A%20Study%20of%20Using%20Multimodal%20Large%20Language%0A%20%20Models%20for%20Media%20Forensics&entry.906535625=Shan%20Jia%20and%20Reilin%20Lyu%20and%20Kangran%20Zhao%20and%20Yize%20Chen%20and%20Zhiyuan%20Yan%20and%20Yan%20Ju%20and%20Chuanbo%20Hu%20and%20Xin%20Li%20and%20Baoyuan%20Wu%20and%20Siwei%20Lyu&entry.1292438233=%20%20DeepFakes%2C%20which%20refer%20to%20AI-generated%20media%20content%2C%20have%20become%20an%0Aincreasing%20concern%20due%20to%20their%20use%20as%20a%20means%20for%20disinformation.%20Detecting%0ADeepFakes%20is%20currently%20solved%20with%20programmed%20machine%20learning%20algorithms.%20In%0Athis%20work%2C%20we%20investigate%20the%20capabilities%20of%20multimodal%20large%20language%20models%0A%28LLMs%29%20in%20DeepFake%20detection.%20We%20conducted%20qualitative%20and%20quantitative%0Aexperiments%20to%20demonstrate%20multimodal%20LLMs%20and%20show%20that%20they%20can%20expose%0AAI-generated%20images%20through%20careful%20experimental%20design%20and%20prompt%20engineering.%0AThis%20is%20interesting%2C%20considering%20that%20LLMs%20are%20not%20inherently%20tailored%20for%0Amedia%20forensic%20tasks%2C%20and%20the%20process%20does%20not%20require%20programming.%20We%20discuss%0Athe%20limitations%20of%20multimodal%20LLMs%20for%20these%20tasks%20and%20suggest%20possible%0Aimprovements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14077v4&entry.124074799=Read"},
{"title": "Learning Disentangled Semantic Spaces of Explanations via Invertible\n  Neural Networks", "author": "Yingji Zhang and Danilo S. Carvalho and Andr\u00e9 Freitas", "abstract": "  Disentangled latent spaces usually have better semantic separability and\ngeometrical properties, which leads to better interpretability and more\ncontrollable data generation. While this has been well investigated in Computer\nVision, in tasks such as image disentanglement, in the NLP domain sentence\ndisentanglement is still comparatively under-investigated. Most previous work\nhave concentrated on disentangling task-specific generative factors, such as\nsentiment, within the context of style transfer. In this work, we focus on a\nmore general form of sentence disentanglement, targeting the localised\nmodification and control of more general sentence semantic features. To achieve\nthis, we contribute to a novel notion of sentence semantic disentanglement and\nintroduce a flow-based invertible neural network (INN) mechanism integrated\nwith a transformer-based language Autoencoder (AE) in order to deliver latent\nspaces with better separability properties. Experimental results demonstrate\nthat the model can conform the distributed latent space into a better\nsemantically disentangled sentence space, leading to improved language\ninterpretability and controlled generation when compared to the recent\nstate-of-the-art language VAE models.\n", "link": "http://arxiv.org/abs/2305.01713v3", "date": "2024-06-11", "relevancy": 2.0167, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5141}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.511}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Disentangled%20Semantic%20Spaces%20of%20Explanations%20via%20Invertible%0A%20%20Neural%20Networks&body=Title%3A%20Learning%20Disentangled%20Semantic%20Spaces%20of%20Explanations%20via%20Invertible%0A%20%20Neural%20Networks%0AAuthor%3A%20Yingji%20Zhang%20and%20Danilo%20S.%20Carvalho%20and%20Andr%C3%A9%20Freitas%0AAbstract%3A%20%20%20Disentangled%20latent%20spaces%20usually%20have%20better%20semantic%20separability%20and%0Ageometrical%20properties%2C%20which%20leads%20to%20better%20interpretability%20and%20more%0Acontrollable%20data%20generation.%20While%20this%20has%20been%20well%20investigated%20in%20Computer%0AVision%2C%20in%20tasks%20such%20as%20image%20disentanglement%2C%20in%20the%20NLP%20domain%20sentence%0Adisentanglement%20is%20still%20comparatively%20under-investigated.%20Most%20previous%20work%0Ahave%20concentrated%20on%20disentangling%20task-specific%20generative%20factors%2C%20such%20as%0Asentiment%2C%20within%20the%20context%20of%20style%20transfer.%20In%20this%20work%2C%20we%20focus%20on%20a%0Amore%20general%20form%20of%20sentence%20disentanglement%2C%20targeting%20the%20localised%0Amodification%20and%20control%20of%20more%20general%20sentence%20semantic%20features.%20To%20achieve%0Athis%2C%20we%20contribute%20to%20a%20novel%20notion%20of%20sentence%20semantic%20disentanglement%20and%0Aintroduce%20a%20flow-based%20invertible%20neural%20network%20%28INN%29%20mechanism%20integrated%0Awith%20a%20transformer-based%20language%20Autoencoder%20%28AE%29%20in%20order%20to%20deliver%20latent%0Aspaces%20with%20better%20separability%20properties.%20Experimental%20results%20demonstrate%0Athat%20the%20model%20can%20conform%20the%20distributed%20latent%20space%20into%20a%20better%0Asemantically%20disentangled%20sentence%20space%2C%20leading%20to%20improved%20language%0Ainterpretability%20and%20controlled%20generation%20when%20compared%20to%20the%20recent%0Astate-of-the-art%20language%20VAE%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.01713v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Disentangled%2520Semantic%2520Spaces%2520of%2520Explanations%2520via%2520Invertible%250A%2520%2520Neural%2520Networks%26entry.906535625%3DYingji%2520Zhang%2520and%2520Danilo%2520S.%2520Carvalho%2520and%2520Andr%25C3%25A9%2520Freitas%26entry.1292438233%3D%2520%2520Disentangled%2520latent%2520spaces%2520usually%2520have%2520better%2520semantic%2520separability%2520and%250Ageometrical%2520properties%252C%2520which%2520leads%2520to%2520better%2520interpretability%2520and%2520more%250Acontrollable%2520data%2520generation.%2520While%2520this%2520has%2520been%2520well%2520investigated%2520in%2520Computer%250AVision%252C%2520in%2520tasks%2520such%2520as%2520image%2520disentanglement%252C%2520in%2520the%2520NLP%2520domain%2520sentence%250Adisentanglement%2520is%2520still%2520comparatively%2520under-investigated.%2520Most%2520previous%2520work%250Ahave%2520concentrated%2520on%2520disentangling%2520task-specific%2520generative%2520factors%252C%2520such%2520as%250Asentiment%252C%2520within%2520the%2520context%2520of%2520style%2520transfer.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520a%250Amore%2520general%2520form%2520of%2520sentence%2520disentanglement%252C%2520targeting%2520the%2520localised%250Amodification%2520and%2520control%2520of%2520more%2520general%2520sentence%2520semantic%2520features.%2520To%2520achieve%250Athis%252C%2520we%2520contribute%2520to%2520a%2520novel%2520notion%2520of%2520sentence%2520semantic%2520disentanglement%2520and%250Aintroduce%2520a%2520flow-based%2520invertible%2520neural%2520network%2520%2528INN%2529%2520mechanism%2520integrated%250Awith%2520a%2520transformer-based%2520language%2520Autoencoder%2520%2528AE%2529%2520in%2520order%2520to%2520deliver%2520latent%250Aspaces%2520with%2520better%2520separability%2520properties.%2520Experimental%2520results%2520demonstrate%250Athat%2520the%2520model%2520can%2520conform%2520the%2520distributed%2520latent%2520space%2520into%2520a%2520better%250Asemantically%2520disentangled%2520sentence%2520space%252C%2520leading%2520to%2520improved%2520language%250Ainterpretability%2520and%2520controlled%2520generation%2520when%2520compared%2520to%2520the%2520recent%250Astate-of-the-art%2520language%2520VAE%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.01713v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Disentangled%20Semantic%20Spaces%20of%20Explanations%20via%20Invertible%0A%20%20Neural%20Networks&entry.906535625=Yingji%20Zhang%20and%20Danilo%20S.%20Carvalho%20and%20Andr%C3%A9%20Freitas&entry.1292438233=%20%20Disentangled%20latent%20spaces%20usually%20have%20better%20semantic%20separability%20and%0Ageometrical%20properties%2C%20which%20leads%20to%20better%20interpretability%20and%20more%0Acontrollable%20data%20generation.%20While%20this%20has%20been%20well%20investigated%20in%20Computer%0AVision%2C%20in%20tasks%20such%20as%20image%20disentanglement%2C%20in%20the%20NLP%20domain%20sentence%0Adisentanglement%20is%20still%20comparatively%20under-investigated.%20Most%20previous%20work%0Ahave%20concentrated%20on%20disentangling%20task-specific%20generative%20factors%2C%20such%20as%0Asentiment%2C%20within%20the%20context%20of%20style%20transfer.%20In%20this%20work%2C%20we%20focus%20on%20a%0Amore%20general%20form%20of%20sentence%20disentanglement%2C%20targeting%20the%20localised%0Amodification%20and%20control%20of%20more%20general%20sentence%20semantic%20features.%20To%20achieve%0Athis%2C%20we%20contribute%20to%20a%20novel%20notion%20of%20sentence%20semantic%20disentanglement%20and%0Aintroduce%20a%20flow-based%20invertible%20neural%20network%20%28INN%29%20mechanism%20integrated%0Awith%20a%20transformer-based%20language%20Autoencoder%20%28AE%29%20in%20order%20to%20deliver%20latent%0Aspaces%20with%20better%20separability%20properties.%20Experimental%20results%20demonstrate%0Athat%20the%20model%20can%20conform%20the%20distributed%20latent%20space%20into%20a%20better%0Asemantically%20disentangled%20sentence%20space%2C%20leading%20to%20improved%20language%0Ainterpretability%20and%20controlled%20generation%20when%20compared%20to%20the%20recent%0Astate-of-the-art%20language%20VAE%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.01713v3&entry.124074799=Read"},
{"title": "RAD: A Comprehensive Dataset for Benchmarking the Robustness of Image\n  Anomaly Detection", "author": "Yuqi Cheng and Yunkang Cao and Rui Chen and Weiming Shen", "abstract": "  Robustness against noisy imaging is crucial for practical image anomaly\ndetection systems. This study introduces a Robust Anomaly Detection (RAD)\ndataset with free views, uneven illuminations, and blurry collections to\nsystematically evaluate the robustness of current anomaly detection methods.\nSpecifically, RAD aims to identify foreign objects on working platforms as\nanomalies. The collection process incorporates various sources of imaging\nnoise, such as viewpoint changes, uneven illuminations, and blurry collections,\nto replicate real-world inspection scenarios. Subsequently, we assess and\nanalyze 11 state-of-the-art unsupervised and zero-shot methods on RAD. Our\nfindings indicate that: 1) Variations in viewpoint, illumination, and blurring\naffect anomaly detection methods to varying degrees; 2) Methods relying on\nmemory banks and assisted by synthetic anomalies demonstrate stronger\nrobustness; 3) Effectively leveraging the general knowledge of foundational\nmodels is a promising avenue for enhancing the robustness of anomaly detection\nmethods.\n", "link": "http://arxiv.org/abs/2406.07176v1", "date": "2024-06-11", "relevancy": 2.0039, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5154}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5027}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAD%3A%20A%20Comprehensive%20Dataset%20for%20Benchmarking%20the%20Robustness%20of%20Image%0A%20%20Anomaly%20Detection&body=Title%3A%20RAD%3A%20A%20Comprehensive%20Dataset%20for%20Benchmarking%20the%20Robustness%20of%20Image%0A%20%20Anomaly%20Detection%0AAuthor%3A%20Yuqi%20Cheng%20and%20Yunkang%20Cao%20and%20Rui%20Chen%20and%20Weiming%20Shen%0AAbstract%3A%20%20%20Robustness%20against%20noisy%20imaging%20is%20crucial%20for%20practical%20image%20anomaly%0Adetection%20systems.%20This%20study%20introduces%20a%20Robust%20Anomaly%20Detection%20%28RAD%29%0Adataset%20with%20free%20views%2C%20uneven%20illuminations%2C%20and%20blurry%20collections%20to%0Asystematically%20evaluate%20the%20robustness%20of%20current%20anomaly%20detection%20methods.%0ASpecifically%2C%20RAD%20aims%20to%20identify%20foreign%20objects%20on%20working%20platforms%20as%0Aanomalies.%20The%20collection%20process%20incorporates%20various%20sources%20of%20imaging%0Anoise%2C%20such%20as%20viewpoint%20changes%2C%20uneven%20illuminations%2C%20and%20blurry%20collections%2C%0Ato%20replicate%20real-world%20inspection%20scenarios.%20Subsequently%2C%20we%20assess%20and%0Aanalyze%2011%20state-of-the-art%20unsupervised%20and%20zero-shot%20methods%20on%20RAD.%20Our%0Afindings%20indicate%20that%3A%201%29%20Variations%20in%20viewpoint%2C%20illumination%2C%20and%20blurring%0Aaffect%20anomaly%20detection%20methods%20to%20varying%20degrees%3B%202%29%20Methods%20relying%20on%0Amemory%20banks%20and%20assisted%20by%20synthetic%20anomalies%20demonstrate%20stronger%0Arobustness%3B%203%29%20Effectively%20leveraging%20the%20general%20knowledge%20of%20foundational%0Amodels%20is%20a%20promising%20avenue%20for%20enhancing%20the%20robustness%20of%20anomaly%20detection%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAD%253A%2520A%2520Comprehensive%2520Dataset%2520for%2520Benchmarking%2520the%2520Robustness%2520of%2520Image%250A%2520%2520Anomaly%2520Detection%26entry.906535625%3DYuqi%2520Cheng%2520and%2520Yunkang%2520Cao%2520and%2520Rui%2520Chen%2520and%2520Weiming%2520Shen%26entry.1292438233%3D%2520%2520Robustness%2520against%2520noisy%2520imaging%2520is%2520crucial%2520for%2520practical%2520image%2520anomaly%250Adetection%2520systems.%2520This%2520study%2520introduces%2520a%2520Robust%2520Anomaly%2520Detection%2520%2528RAD%2529%250Adataset%2520with%2520free%2520views%252C%2520uneven%2520illuminations%252C%2520and%2520blurry%2520collections%2520to%250Asystematically%2520evaluate%2520the%2520robustness%2520of%2520current%2520anomaly%2520detection%2520methods.%250ASpecifically%252C%2520RAD%2520aims%2520to%2520identify%2520foreign%2520objects%2520on%2520working%2520platforms%2520as%250Aanomalies.%2520The%2520collection%2520process%2520incorporates%2520various%2520sources%2520of%2520imaging%250Anoise%252C%2520such%2520as%2520viewpoint%2520changes%252C%2520uneven%2520illuminations%252C%2520and%2520blurry%2520collections%252C%250Ato%2520replicate%2520real-world%2520inspection%2520scenarios.%2520Subsequently%252C%2520we%2520assess%2520and%250Aanalyze%252011%2520state-of-the-art%2520unsupervised%2520and%2520zero-shot%2520methods%2520on%2520RAD.%2520Our%250Afindings%2520indicate%2520that%253A%25201%2529%2520Variations%2520in%2520viewpoint%252C%2520illumination%252C%2520and%2520blurring%250Aaffect%2520anomaly%2520detection%2520methods%2520to%2520varying%2520degrees%253B%25202%2529%2520Methods%2520relying%2520on%250Amemory%2520banks%2520and%2520assisted%2520by%2520synthetic%2520anomalies%2520demonstrate%2520stronger%250Arobustness%253B%25203%2529%2520Effectively%2520leveraging%2520the%2520general%2520knowledge%2520of%2520foundational%250Amodels%2520is%2520a%2520promising%2520avenue%2520for%2520enhancing%2520the%2520robustness%2520of%2520anomaly%2520detection%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAD%3A%20A%20Comprehensive%20Dataset%20for%20Benchmarking%20the%20Robustness%20of%20Image%0A%20%20Anomaly%20Detection&entry.906535625=Yuqi%20Cheng%20and%20Yunkang%20Cao%20and%20Rui%20Chen%20and%20Weiming%20Shen&entry.1292438233=%20%20Robustness%20against%20noisy%20imaging%20is%20crucial%20for%20practical%20image%20anomaly%0Adetection%20systems.%20This%20study%20introduces%20a%20Robust%20Anomaly%20Detection%20%28RAD%29%0Adataset%20with%20free%20views%2C%20uneven%20illuminations%2C%20and%20blurry%20collections%20to%0Asystematically%20evaluate%20the%20robustness%20of%20current%20anomaly%20detection%20methods.%0ASpecifically%2C%20RAD%20aims%20to%20identify%20foreign%20objects%20on%20working%20platforms%20as%0Aanomalies.%20The%20collection%20process%20incorporates%20various%20sources%20of%20imaging%0Anoise%2C%20such%20as%20viewpoint%20changes%2C%20uneven%20illuminations%2C%20and%20blurry%20collections%2C%0Ato%20replicate%20real-world%20inspection%20scenarios.%20Subsequently%2C%20we%20assess%20and%0Aanalyze%2011%20state-of-the-art%20unsupervised%20and%20zero-shot%20methods%20on%20RAD.%20Our%0Afindings%20indicate%20that%3A%201%29%20Variations%20in%20viewpoint%2C%20illumination%2C%20and%20blurring%0Aaffect%20anomaly%20detection%20methods%20to%20varying%20degrees%3B%202%29%20Methods%20relying%20on%0Amemory%20banks%20and%20assisted%20by%20synthetic%20anomalies%20demonstrate%20stronger%0Arobustness%3B%203%29%20Effectively%20leveraging%20the%20general%20knowledge%20of%20foundational%0Amodels%20is%20a%20promising%20avenue%20for%20enhancing%20the%20robustness%20of%20anomaly%20detection%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07176v1&entry.124074799=Read"},
{"title": "When Linear Attention Meets Autoregressive Decoding: Towards More\n  Effective and Efficient Linearized Large Language Models", "author": "Haoran You and Yichao Fu and Zheng Wang and Amir Yazdanbakhsh and  Yingyan and  Lin", "abstract": "  Autoregressive Large Language Models (LLMs) have achieved impressive\nperformance in language tasks but face two significant bottlenecks: (1)\nquadratic complexity in the attention module as the number of tokens increases,\nand (2) limited efficiency due to the sequential processing nature of\nautoregressive LLMs during generation. While linear attention and speculative\ndecoding offer potential solutions, their applicability and synergistic\npotential for enhancing autoregressive LLMs remain uncertain. We conduct the\nfirst comprehensive study on the efficacy of existing linear attention methods\nfor autoregressive LLMs, integrating them with speculative decoding. We\nintroduce an augmentation technique for linear attention that ensures\ncompatibility with speculative decoding, enabling more efficient training and\nserving of LLMs. Extensive experiments and ablation studies involving seven\nexisting linear attention models and five encoder/decoder-based LLMs\nconsistently validate the effectiveness of our augmented linearized LLMs.\nNotably, our approach achieves up to a 6.67 reduction in perplexity on the\nLLaMA model and up to a 2$\\times$ speedup during generation compared to prior\nlinear attention methods. Codes and models are available at\nhttps://github.com/GATECH-EIC/Linearized-LLM.\n", "link": "http://arxiv.org/abs/2406.07368v1", "date": "2024-06-11", "relevancy": 1.9971, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5063}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5052}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Linear%20Attention%20Meets%20Autoregressive%20Decoding%3A%20Towards%20More%0A%20%20Effective%20and%20Efficient%20Linearized%20Large%20Language%20Models&body=Title%3A%20When%20Linear%20Attention%20Meets%20Autoregressive%20Decoding%3A%20Towards%20More%0A%20%20Effective%20and%20Efficient%20Linearized%20Large%20Language%20Models%0AAuthor%3A%20Haoran%20You%20and%20Yichao%20Fu%20and%20Zheng%20Wang%20and%20Amir%20Yazdanbakhsh%20and%20%20Yingyan%20and%20%20Lin%0AAbstract%3A%20%20%20Autoregressive%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%0Aperformance%20in%20language%20tasks%20but%20face%20two%20significant%20bottlenecks%3A%20%281%29%0Aquadratic%20complexity%20in%20the%20attention%20module%20as%20the%20number%20of%20tokens%20increases%2C%0Aand%20%282%29%20limited%20efficiency%20due%20to%20the%20sequential%20processing%20nature%20of%0Aautoregressive%20LLMs%20during%20generation.%20While%20linear%20attention%20and%20speculative%0Adecoding%20offer%20potential%20solutions%2C%20their%20applicability%20and%20synergistic%0Apotential%20for%20enhancing%20autoregressive%20LLMs%20remain%20uncertain.%20We%20conduct%20the%0Afirst%20comprehensive%20study%20on%20the%20efficacy%20of%20existing%20linear%20attention%20methods%0Afor%20autoregressive%20LLMs%2C%20integrating%20them%20with%20speculative%20decoding.%20We%0Aintroduce%20an%20augmentation%20technique%20for%20linear%20attention%20that%20ensures%0Acompatibility%20with%20speculative%20decoding%2C%20enabling%20more%20efficient%20training%20and%0Aserving%20of%20LLMs.%20Extensive%20experiments%20and%20ablation%20studies%20involving%20seven%0Aexisting%20linear%20attention%20models%20and%20five%20encoder/decoder-based%20LLMs%0Aconsistently%20validate%20the%20effectiveness%20of%20our%20augmented%20linearized%20LLMs.%0ANotably%2C%20our%20approach%20achieves%20up%20to%20a%206.67%20reduction%20in%20perplexity%20on%20the%0ALLaMA%20model%20and%20up%20to%20a%202%24%5Ctimes%24%20speedup%20during%20generation%20compared%20to%20prior%0Alinear%20attention%20methods.%20Codes%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/GATECH-EIC/Linearized-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Linear%2520Attention%2520Meets%2520Autoregressive%2520Decoding%253A%2520Towards%2520More%250A%2520%2520Effective%2520and%2520Efficient%2520Linearized%2520Large%2520Language%2520Models%26entry.906535625%3DHaoran%2520You%2520and%2520Yichao%2520Fu%2520and%2520Zheng%2520Wang%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520%2520Yingyan%2520and%2520%2520Lin%26entry.1292438233%3D%2520%2520Autoregressive%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%250Aperformance%2520in%2520language%2520tasks%2520but%2520face%2520two%2520significant%2520bottlenecks%253A%2520%25281%2529%250Aquadratic%2520complexity%2520in%2520the%2520attention%2520module%2520as%2520the%2520number%2520of%2520tokens%2520increases%252C%250Aand%2520%25282%2529%2520limited%2520efficiency%2520due%2520to%2520the%2520sequential%2520processing%2520nature%2520of%250Aautoregressive%2520LLMs%2520during%2520generation.%2520While%2520linear%2520attention%2520and%2520speculative%250Adecoding%2520offer%2520potential%2520solutions%252C%2520their%2520applicability%2520and%2520synergistic%250Apotential%2520for%2520enhancing%2520autoregressive%2520LLMs%2520remain%2520uncertain.%2520We%2520conduct%2520the%250Afirst%2520comprehensive%2520study%2520on%2520the%2520efficacy%2520of%2520existing%2520linear%2520attention%2520methods%250Afor%2520autoregressive%2520LLMs%252C%2520integrating%2520them%2520with%2520speculative%2520decoding.%2520We%250Aintroduce%2520an%2520augmentation%2520technique%2520for%2520linear%2520attention%2520that%2520ensures%250Acompatibility%2520with%2520speculative%2520decoding%252C%2520enabling%2520more%2520efficient%2520training%2520and%250Aserving%2520of%2520LLMs.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%2520involving%2520seven%250Aexisting%2520linear%2520attention%2520models%2520and%2520five%2520encoder/decoder-based%2520LLMs%250Aconsistently%2520validate%2520the%2520effectiveness%2520of%2520our%2520augmented%2520linearized%2520LLMs.%250ANotably%252C%2520our%2520approach%2520achieves%2520up%2520to%2520a%25206.67%2520reduction%2520in%2520perplexity%2520on%2520the%250ALLaMA%2520model%2520and%2520up%2520to%2520a%25202%2524%255Ctimes%2524%2520speedup%2520during%2520generation%2520compared%2520to%2520prior%250Alinear%2520attention%2520methods.%2520Codes%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/GATECH-EIC/Linearized-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Linear%20Attention%20Meets%20Autoregressive%20Decoding%3A%20Towards%20More%0A%20%20Effective%20and%20Efficient%20Linearized%20Large%20Language%20Models&entry.906535625=Haoran%20You%20and%20Yichao%20Fu%20and%20Zheng%20Wang%20and%20Amir%20Yazdanbakhsh%20and%20%20Yingyan%20and%20%20Lin&entry.1292438233=%20%20Autoregressive%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%0Aperformance%20in%20language%20tasks%20but%20face%20two%20significant%20bottlenecks%3A%20%281%29%0Aquadratic%20complexity%20in%20the%20attention%20module%20as%20the%20number%20of%20tokens%20increases%2C%0Aand%20%282%29%20limited%20efficiency%20due%20to%20the%20sequential%20processing%20nature%20of%0Aautoregressive%20LLMs%20during%20generation.%20While%20linear%20attention%20and%20speculative%0Adecoding%20offer%20potential%20solutions%2C%20their%20applicability%20and%20synergistic%0Apotential%20for%20enhancing%20autoregressive%20LLMs%20remain%20uncertain.%20We%20conduct%20the%0Afirst%20comprehensive%20study%20on%20the%20efficacy%20of%20existing%20linear%20attention%20methods%0Afor%20autoregressive%20LLMs%2C%20integrating%20them%20with%20speculative%20decoding.%20We%0Aintroduce%20an%20augmentation%20technique%20for%20linear%20attention%20that%20ensures%0Acompatibility%20with%20speculative%20decoding%2C%20enabling%20more%20efficient%20training%20and%0Aserving%20of%20LLMs.%20Extensive%20experiments%20and%20ablation%20studies%20involving%20seven%0Aexisting%20linear%20attention%20models%20and%20five%20encoder/decoder-based%20LLMs%0Aconsistently%20validate%20the%20effectiveness%20of%20our%20augmented%20linearized%20LLMs.%0ANotably%2C%20our%20approach%20achieves%20up%20to%20a%206.67%20reduction%20in%20perplexity%20on%20the%0ALLaMA%20model%20and%20up%20to%20a%202%24%5Ctimes%24%20speedup%20during%20generation%20compared%20to%20prior%0Alinear%20attention%20methods.%20Codes%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/GATECH-EIC/Linearized-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07368v1&entry.124074799=Read"},
{"title": "Neuromorphic quadratic programming for efficient and scalable model\n  predictive control", "author": "Ashish Rao Mangalore and Gabriel Andreas Fonseca Guerra and Sumedh R. Risbud and Philipp Stratmann and Andreas Wild", "abstract": "  Applications in robotics or other size-, weight- and power-constrained\nautonomous systems at the edge often require real-time and low-energy solutions\nto large optimization problems. Event-based and memory-integrated neuromorphic\narchitectures promise to solve such optimization problems with superior energy\nefficiency and performance compared to conventional von Neumann architectures.\nHere, we present a method to solve convex continuous optimization problems with\nquadratic cost functions and linear constraints on Intel's scalable\nneuromorphic research chip Loihi 2. When applied to model predictive control\n(MPC) problems for the quadruped robotic platform ANYmal, this method achieves\nover two orders of magnitude reduction in combined energy-delay product\ncompared to the state-of-the-art solver, OSQP, on (edge) CPUs and GPUs with\nsolution times under ten milliseconds for various problem sizes. These results\ndemonstrate the benefit of non-von-Neumann architectures for robotic control\napplications.\n", "link": "http://arxiv.org/abs/2401.14885v2", "date": "2024-06-11", "relevancy": 1.9917, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5488}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4909}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuromorphic%20quadratic%20programming%20for%20efficient%20and%20scalable%20model%0A%20%20predictive%20control&body=Title%3A%20Neuromorphic%20quadratic%20programming%20for%20efficient%20and%20scalable%20model%0A%20%20predictive%20control%0AAuthor%3A%20Ashish%20Rao%20Mangalore%20and%20Gabriel%20Andreas%20Fonseca%20Guerra%20and%20Sumedh%20R.%20Risbud%20and%20Philipp%20Stratmann%20and%20Andreas%20Wild%0AAbstract%3A%20%20%20Applications%20in%20robotics%20or%20other%20size-%2C%20weight-%20and%20power-constrained%0Aautonomous%20systems%20at%20the%20edge%20often%20require%20real-time%20and%20low-energy%20solutions%0Ato%20large%20optimization%20problems.%20Event-based%20and%20memory-integrated%20neuromorphic%0Aarchitectures%20promise%20to%20solve%20such%20optimization%20problems%20with%20superior%20energy%0Aefficiency%20and%20performance%20compared%20to%20conventional%20von%20Neumann%20architectures.%0AHere%2C%20we%20present%20a%20method%20to%20solve%20convex%20continuous%20optimization%20problems%20with%0Aquadratic%20cost%20functions%20and%20linear%20constraints%20on%20Intel%27s%20scalable%0Aneuromorphic%20research%20chip%20Loihi%202.%20When%20applied%20to%20model%20predictive%20control%0A%28MPC%29%20problems%20for%20the%20quadruped%20robotic%20platform%20ANYmal%2C%20this%20method%20achieves%0Aover%20two%20orders%20of%20magnitude%20reduction%20in%20combined%20energy-delay%20product%0Acompared%20to%20the%20state-of-the-art%20solver%2C%20OSQP%2C%20on%20%28edge%29%20CPUs%20and%20GPUs%20with%0Asolution%20times%20under%20ten%20milliseconds%20for%20various%20problem%20sizes.%20These%20results%0Ademonstrate%20the%20benefit%20of%20non-von-Neumann%20architectures%20for%20robotic%20control%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14885v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuromorphic%2520quadratic%2520programming%2520for%2520efficient%2520and%2520scalable%2520model%250A%2520%2520predictive%2520control%26entry.906535625%3DAshish%2520Rao%2520Mangalore%2520and%2520Gabriel%2520Andreas%2520Fonseca%2520Guerra%2520and%2520Sumedh%2520R.%2520Risbud%2520and%2520Philipp%2520Stratmann%2520and%2520Andreas%2520Wild%26entry.1292438233%3D%2520%2520Applications%2520in%2520robotics%2520or%2520other%2520size-%252C%2520weight-%2520and%2520power-constrained%250Aautonomous%2520systems%2520at%2520the%2520edge%2520often%2520require%2520real-time%2520and%2520low-energy%2520solutions%250Ato%2520large%2520optimization%2520problems.%2520Event-based%2520and%2520memory-integrated%2520neuromorphic%250Aarchitectures%2520promise%2520to%2520solve%2520such%2520optimization%2520problems%2520with%2520superior%2520energy%250Aefficiency%2520and%2520performance%2520compared%2520to%2520conventional%2520von%2520Neumann%2520architectures.%250AHere%252C%2520we%2520present%2520a%2520method%2520to%2520solve%2520convex%2520continuous%2520optimization%2520problems%2520with%250Aquadratic%2520cost%2520functions%2520and%2520linear%2520constraints%2520on%2520Intel%2527s%2520scalable%250Aneuromorphic%2520research%2520chip%2520Loihi%25202.%2520When%2520applied%2520to%2520model%2520predictive%2520control%250A%2528MPC%2529%2520problems%2520for%2520the%2520quadruped%2520robotic%2520platform%2520ANYmal%252C%2520this%2520method%2520achieves%250Aover%2520two%2520orders%2520of%2520magnitude%2520reduction%2520in%2520combined%2520energy-delay%2520product%250Acompared%2520to%2520the%2520state-of-the-art%2520solver%252C%2520OSQP%252C%2520on%2520%2528edge%2529%2520CPUs%2520and%2520GPUs%2520with%250Asolution%2520times%2520under%2520ten%2520milliseconds%2520for%2520various%2520problem%2520sizes.%2520These%2520results%250Ademonstrate%2520the%2520benefit%2520of%2520non-von-Neumann%2520architectures%2520for%2520robotic%2520control%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14885v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuromorphic%20quadratic%20programming%20for%20efficient%20and%20scalable%20model%0A%20%20predictive%20control&entry.906535625=Ashish%20Rao%20Mangalore%20and%20Gabriel%20Andreas%20Fonseca%20Guerra%20and%20Sumedh%20R.%20Risbud%20and%20Philipp%20Stratmann%20and%20Andreas%20Wild&entry.1292438233=%20%20Applications%20in%20robotics%20or%20other%20size-%2C%20weight-%20and%20power-constrained%0Aautonomous%20systems%20at%20the%20edge%20often%20require%20real-time%20and%20low-energy%20solutions%0Ato%20large%20optimization%20problems.%20Event-based%20and%20memory-integrated%20neuromorphic%0Aarchitectures%20promise%20to%20solve%20such%20optimization%20problems%20with%20superior%20energy%0Aefficiency%20and%20performance%20compared%20to%20conventional%20von%20Neumann%20architectures.%0AHere%2C%20we%20present%20a%20method%20to%20solve%20convex%20continuous%20optimization%20problems%20with%0Aquadratic%20cost%20functions%20and%20linear%20constraints%20on%20Intel%27s%20scalable%0Aneuromorphic%20research%20chip%20Loihi%202.%20When%20applied%20to%20model%20predictive%20control%0A%28MPC%29%20problems%20for%20the%20quadruped%20robotic%20platform%20ANYmal%2C%20this%20method%20achieves%0Aover%20two%20orders%20of%20magnitude%20reduction%20in%20combined%20energy-delay%20product%0Acompared%20to%20the%20state-of-the-art%20solver%2C%20OSQP%2C%20on%20%28edge%29%20CPUs%20and%20GPUs%20with%0Asolution%20times%20under%20ten%20milliseconds%20for%20various%20problem%20sizes.%20These%20results%0Ademonstrate%20the%20benefit%20of%20non-von-Neumann%20architectures%20for%20robotic%20control%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14885v2&entry.124074799=Read"},
{"title": "Comparing Deep Learning Models for Rice Mapping in Bhutan Using High\n  Resolution Satellite Imagery", "author": "Biplov Bhandari and Timothy Mayer", "abstract": "  The Bhutanese government is increasing its utilization of technological\napproaches such as including Remote Sensing-based knowledge in their\ndecision-making process. This study focuses on crop type and crop extent in\nParo, one of the top rice-yielding districts in Bhutan, and employs publicly\navailable NICFI high-resolution satellite imagery from Planet. Two Deep\nLearning (DL) approaches, point-based (DNN) and patch-based (U-Net), models\nwere used in conjunction with cloud-computing platforms. Three different models\nper DL approaches (DNN and U-Net) were trained: 1) RGBN channels from Planet;\n2) RGBN and elevation data (RGBNE); 3) RGBN and Sentinel-1 (S1) data (RGBNS),\nand RGBN with E and S1 data (RGBNES). From this comprehensive analysis, the\nU-Net displayed higher performance metrics across both model training and model\nvalidation efforts. Among the U-Net model sets, the RGBN, RGBNE, RGBNS, and\nRGBNES models had an F1-score of 0.8546, 0.8563, 0.8467, and 0.8500\nrespectively. An independent model evaluation was performed and found a high\nlevel of performance variation across all the metrics. For this independent\nmodel evaluation, the U-Net RGBN, RGBNE, RGBNES, and RGBN models displayed the\nF1-scores of 0.5935, 0.6154, 0.5882, and 0.6582, suggesting U-Net RGBNES as the\nbest model. The study shows that the DL approaches can predict rice. Also, DL\nmethods can be used with the survey-based approaches currently utilized by the\nBhutan Department of Agriculture. Further, this study demonstrated the usage of\nregional land cover products such as SERVIR's RLCMS as a weak label approach to\ncapture different strata addressing the class imbalance problem and improving\nthe sampling design for DL application. Finally, through preliminary model\ntesting and comparisons outlined it was shown that using additional features\nsuch as NDVI, EVI, and NDWI did not drastically improve model performance.\n", "link": "http://arxiv.org/abs/2406.07482v1", "date": "2024-06-11", "relevancy": 1.9881, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5252}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4975}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Deep%20Learning%20Models%20for%20Rice%20Mapping%20in%20Bhutan%20Using%20High%0A%20%20Resolution%20Satellite%20Imagery&body=Title%3A%20Comparing%20Deep%20Learning%20Models%20for%20Rice%20Mapping%20in%20Bhutan%20Using%20High%0A%20%20Resolution%20Satellite%20Imagery%0AAuthor%3A%20Biplov%20Bhandari%20and%20Timothy%20Mayer%0AAbstract%3A%20%20%20The%20Bhutanese%20government%20is%20increasing%20its%20utilization%20of%20technological%0Aapproaches%20such%20as%20including%20Remote%20Sensing-based%20knowledge%20in%20their%0Adecision-making%20process.%20This%20study%20focuses%20on%20crop%20type%20and%20crop%20extent%20in%0AParo%2C%20one%20of%20the%20top%20rice-yielding%20districts%20in%20Bhutan%2C%20and%20employs%20publicly%0Aavailable%20NICFI%20high-resolution%20satellite%20imagery%20from%20Planet.%20Two%20Deep%0ALearning%20%28DL%29%20approaches%2C%20point-based%20%28DNN%29%20and%20patch-based%20%28U-Net%29%2C%20models%0Awere%20used%20in%20conjunction%20with%20cloud-computing%20platforms.%20Three%20different%20models%0Aper%20DL%20approaches%20%28DNN%20and%20U-Net%29%20were%20trained%3A%201%29%20RGBN%20channels%20from%20Planet%3B%0A2%29%20RGBN%20and%20elevation%20data%20%28RGBNE%29%3B%203%29%20RGBN%20and%20Sentinel-1%20%28S1%29%20data%20%28RGBNS%29%2C%0Aand%20RGBN%20with%20E%20and%20S1%20data%20%28RGBNES%29.%20From%20this%20comprehensive%20analysis%2C%20the%0AU-Net%20displayed%20higher%20performance%20metrics%20across%20both%20model%20training%20and%20model%0Avalidation%20efforts.%20Among%20the%20U-Net%20model%20sets%2C%20the%20RGBN%2C%20RGBNE%2C%20RGBNS%2C%20and%0ARGBNES%20models%20had%20an%20F1-score%20of%200.8546%2C%200.8563%2C%200.8467%2C%20and%200.8500%0Arespectively.%20An%20independent%20model%20evaluation%20was%20performed%20and%20found%20a%20high%0Alevel%20of%20performance%20variation%20across%20all%20the%20metrics.%20For%20this%20independent%0Amodel%20evaluation%2C%20the%20U-Net%20RGBN%2C%20RGBNE%2C%20RGBNES%2C%20and%20RGBN%20models%20displayed%20the%0AF1-scores%20of%200.5935%2C%200.6154%2C%200.5882%2C%20and%200.6582%2C%20suggesting%20U-Net%20RGBNES%20as%20the%0Abest%20model.%20The%20study%20shows%20that%20the%20DL%20approaches%20can%20predict%20rice.%20Also%2C%20DL%0Amethods%20can%20be%20used%20with%20the%20survey-based%20approaches%20currently%20utilized%20by%20the%0ABhutan%20Department%20of%20Agriculture.%20Further%2C%20this%20study%20demonstrated%20the%20usage%20of%0Aregional%20land%20cover%20products%20such%20as%20SERVIR%27s%20RLCMS%20as%20a%20weak%20label%20approach%20to%0Acapture%20different%20strata%20addressing%20the%20class%20imbalance%20problem%20and%20improving%0Athe%20sampling%20design%20for%20DL%20application.%20Finally%2C%20through%20preliminary%20model%0Atesting%20and%20comparisons%20outlined%20it%20was%20shown%20that%20using%20additional%20features%0Asuch%20as%20NDVI%2C%20EVI%2C%20and%20NDWI%20did%20not%20drastically%20improve%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Deep%2520Learning%2520Models%2520for%2520Rice%2520Mapping%2520in%2520Bhutan%2520Using%2520High%250A%2520%2520Resolution%2520Satellite%2520Imagery%26entry.906535625%3DBiplov%2520Bhandari%2520and%2520Timothy%2520Mayer%26entry.1292438233%3D%2520%2520The%2520Bhutanese%2520government%2520is%2520increasing%2520its%2520utilization%2520of%2520technological%250Aapproaches%2520such%2520as%2520including%2520Remote%2520Sensing-based%2520knowledge%2520in%2520their%250Adecision-making%2520process.%2520This%2520study%2520focuses%2520on%2520crop%2520type%2520and%2520crop%2520extent%2520in%250AParo%252C%2520one%2520of%2520the%2520top%2520rice-yielding%2520districts%2520in%2520Bhutan%252C%2520and%2520employs%2520publicly%250Aavailable%2520NICFI%2520high-resolution%2520satellite%2520imagery%2520from%2520Planet.%2520Two%2520Deep%250ALearning%2520%2528DL%2529%2520approaches%252C%2520point-based%2520%2528DNN%2529%2520and%2520patch-based%2520%2528U-Net%2529%252C%2520models%250Awere%2520used%2520in%2520conjunction%2520with%2520cloud-computing%2520platforms.%2520Three%2520different%2520models%250Aper%2520DL%2520approaches%2520%2528DNN%2520and%2520U-Net%2529%2520were%2520trained%253A%25201%2529%2520RGBN%2520channels%2520from%2520Planet%253B%250A2%2529%2520RGBN%2520and%2520elevation%2520data%2520%2528RGBNE%2529%253B%25203%2529%2520RGBN%2520and%2520Sentinel-1%2520%2528S1%2529%2520data%2520%2528RGBNS%2529%252C%250Aand%2520RGBN%2520with%2520E%2520and%2520S1%2520data%2520%2528RGBNES%2529.%2520From%2520this%2520comprehensive%2520analysis%252C%2520the%250AU-Net%2520displayed%2520higher%2520performance%2520metrics%2520across%2520both%2520model%2520training%2520and%2520model%250Avalidation%2520efforts.%2520Among%2520the%2520U-Net%2520model%2520sets%252C%2520the%2520RGBN%252C%2520RGBNE%252C%2520RGBNS%252C%2520and%250ARGBNES%2520models%2520had%2520an%2520F1-score%2520of%25200.8546%252C%25200.8563%252C%25200.8467%252C%2520and%25200.8500%250Arespectively.%2520An%2520independent%2520model%2520evaluation%2520was%2520performed%2520and%2520found%2520a%2520high%250Alevel%2520of%2520performance%2520variation%2520across%2520all%2520the%2520metrics.%2520For%2520this%2520independent%250Amodel%2520evaluation%252C%2520the%2520U-Net%2520RGBN%252C%2520RGBNE%252C%2520RGBNES%252C%2520and%2520RGBN%2520models%2520displayed%2520the%250AF1-scores%2520of%25200.5935%252C%25200.6154%252C%25200.5882%252C%2520and%25200.6582%252C%2520suggesting%2520U-Net%2520RGBNES%2520as%2520the%250Abest%2520model.%2520The%2520study%2520shows%2520that%2520the%2520DL%2520approaches%2520can%2520predict%2520rice.%2520Also%252C%2520DL%250Amethods%2520can%2520be%2520used%2520with%2520the%2520survey-based%2520approaches%2520currently%2520utilized%2520by%2520the%250ABhutan%2520Department%2520of%2520Agriculture.%2520Further%252C%2520this%2520study%2520demonstrated%2520the%2520usage%2520of%250Aregional%2520land%2520cover%2520products%2520such%2520as%2520SERVIR%2527s%2520RLCMS%2520as%2520a%2520weak%2520label%2520approach%2520to%250Acapture%2520different%2520strata%2520addressing%2520the%2520class%2520imbalance%2520problem%2520and%2520improving%250Athe%2520sampling%2520design%2520for%2520DL%2520application.%2520Finally%252C%2520through%2520preliminary%2520model%250Atesting%2520and%2520comparisons%2520outlined%2520it%2520was%2520shown%2520that%2520using%2520additional%2520features%250Asuch%2520as%2520NDVI%252C%2520EVI%252C%2520and%2520NDWI%2520did%2520not%2520drastically%2520improve%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Deep%20Learning%20Models%20for%20Rice%20Mapping%20in%20Bhutan%20Using%20High%0A%20%20Resolution%20Satellite%20Imagery&entry.906535625=Biplov%20Bhandari%20and%20Timothy%20Mayer&entry.1292438233=%20%20The%20Bhutanese%20government%20is%20increasing%20its%20utilization%20of%20technological%0Aapproaches%20such%20as%20including%20Remote%20Sensing-based%20knowledge%20in%20their%0Adecision-making%20process.%20This%20study%20focuses%20on%20crop%20type%20and%20crop%20extent%20in%0AParo%2C%20one%20of%20the%20top%20rice-yielding%20districts%20in%20Bhutan%2C%20and%20employs%20publicly%0Aavailable%20NICFI%20high-resolution%20satellite%20imagery%20from%20Planet.%20Two%20Deep%0ALearning%20%28DL%29%20approaches%2C%20point-based%20%28DNN%29%20and%20patch-based%20%28U-Net%29%2C%20models%0Awere%20used%20in%20conjunction%20with%20cloud-computing%20platforms.%20Three%20different%20models%0Aper%20DL%20approaches%20%28DNN%20and%20U-Net%29%20were%20trained%3A%201%29%20RGBN%20channels%20from%20Planet%3B%0A2%29%20RGBN%20and%20elevation%20data%20%28RGBNE%29%3B%203%29%20RGBN%20and%20Sentinel-1%20%28S1%29%20data%20%28RGBNS%29%2C%0Aand%20RGBN%20with%20E%20and%20S1%20data%20%28RGBNES%29.%20From%20this%20comprehensive%20analysis%2C%20the%0AU-Net%20displayed%20higher%20performance%20metrics%20across%20both%20model%20training%20and%20model%0Avalidation%20efforts.%20Among%20the%20U-Net%20model%20sets%2C%20the%20RGBN%2C%20RGBNE%2C%20RGBNS%2C%20and%0ARGBNES%20models%20had%20an%20F1-score%20of%200.8546%2C%200.8563%2C%200.8467%2C%20and%200.8500%0Arespectively.%20An%20independent%20model%20evaluation%20was%20performed%20and%20found%20a%20high%0Alevel%20of%20performance%20variation%20across%20all%20the%20metrics.%20For%20this%20independent%0Amodel%20evaluation%2C%20the%20U-Net%20RGBN%2C%20RGBNE%2C%20RGBNES%2C%20and%20RGBN%20models%20displayed%20the%0AF1-scores%20of%200.5935%2C%200.6154%2C%200.5882%2C%20and%200.6582%2C%20suggesting%20U-Net%20RGBNES%20as%20the%0Abest%20model.%20The%20study%20shows%20that%20the%20DL%20approaches%20can%20predict%20rice.%20Also%2C%20DL%0Amethods%20can%20be%20used%20with%20the%20survey-based%20approaches%20currently%20utilized%20by%20the%0ABhutan%20Department%20of%20Agriculture.%20Further%2C%20this%20study%20demonstrated%20the%20usage%20of%0Aregional%20land%20cover%20products%20such%20as%20SERVIR%27s%20RLCMS%20as%20a%20weak%20label%20approach%20to%0Acapture%20different%20strata%20addressing%20the%20class%20imbalance%20problem%20and%20improving%0Athe%20sampling%20design%20for%20DL%20application.%20Finally%2C%20through%20preliminary%20model%0Atesting%20and%20comparisons%20outlined%20it%20was%20shown%20that%20using%20additional%20features%0Asuch%20as%20NDVI%2C%20EVI%2C%20and%20NDWI%20did%20not%20drastically%20improve%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07482v1&entry.124074799=Read"},
{"title": "Mitigating Oversmoothing Through Reverse Process of GNNs for\n  Heterophilic Graphs", "author": "MoonJeong Park and Jaeseung Heo and Dongwoo Kim", "abstract": "  Graph Neural Network (GNN) resembles the diffusion process, leading to the\nover-smoothing of learned representations when stacking many layers. Hence, the\nreverse process of message passing can produce the distinguishable node\nrepresentations by inverting the forward message propagation. The\ndistinguishable representations can help us to better classify neighboring\nnodes with different labels, such as in heterophilic graphs. In this work, we\napply the design principle of the reverse process to the three variants of the\nGNNs. Through the experiments on heterophilic graph data, where adjacent nodes\nneed to have different representations for successful classification, we show\nthat the reverse process significantly improves the prediction performance in\nmany cases. Additional analysis reveals that the reverse mechanism can mitigate\nthe over-smoothing over hundreds of layers. Our code is available at\nhttps://github.com/ml-postech/reverse-gnn.\n", "link": "http://arxiv.org/abs/2403.10543v2", "date": "2024-06-11", "relevancy": 1.9861, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5013}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4942}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Oversmoothing%20Through%20Reverse%20Process%20of%20GNNs%20for%0A%20%20Heterophilic%20Graphs&body=Title%3A%20Mitigating%20Oversmoothing%20Through%20Reverse%20Process%20of%20GNNs%20for%0A%20%20Heterophilic%20Graphs%0AAuthor%3A%20MoonJeong%20Park%20and%20Jaeseung%20Heo%20and%20Dongwoo%20Kim%0AAbstract%3A%20%20%20Graph%20Neural%20Network%20%28GNN%29%20resembles%20the%20diffusion%20process%2C%20leading%20to%20the%0Aover-smoothing%20of%20learned%20representations%20when%20stacking%20many%20layers.%20Hence%2C%20the%0Areverse%20process%20of%20message%20passing%20can%20produce%20the%20distinguishable%20node%0Arepresentations%20by%20inverting%20the%20forward%20message%20propagation.%20The%0Adistinguishable%20representations%20can%20help%20us%20to%20better%20classify%20neighboring%0Anodes%20with%20different%20labels%2C%20such%20as%20in%20heterophilic%20graphs.%20In%20this%20work%2C%20we%0Aapply%20the%20design%20principle%20of%20the%20reverse%20process%20to%20the%20three%20variants%20of%20the%0AGNNs.%20Through%20the%20experiments%20on%20heterophilic%20graph%20data%2C%20where%20adjacent%20nodes%0Aneed%20to%20have%20different%20representations%20for%20successful%20classification%2C%20we%20show%0Athat%20the%20reverse%20process%20significantly%20improves%20the%20prediction%20performance%20in%0Amany%20cases.%20Additional%20analysis%20reveals%20that%20the%20reverse%20mechanism%20can%20mitigate%0Athe%20over-smoothing%20over%20hundreds%20of%20layers.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ml-postech/reverse-gnn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Oversmoothing%2520Through%2520Reverse%2520Process%2520of%2520GNNs%2520for%250A%2520%2520Heterophilic%2520Graphs%26entry.906535625%3DMoonJeong%2520Park%2520and%2520Jaeseung%2520Heo%2520and%2520Dongwoo%2520Kim%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520resembles%2520the%2520diffusion%2520process%252C%2520leading%2520to%2520the%250Aover-smoothing%2520of%2520learned%2520representations%2520when%2520stacking%2520many%2520layers.%2520Hence%252C%2520the%250Areverse%2520process%2520of%2520message%2520passing%2520can%2520produce%2520the%2520distinguishable%2520node%250Arepresentations%2520by%2520inverting%2520the%2520forward%2520message%2520propagation.%2520The%250Adistinguishable%2520representations%2520can%2520help%2520us%2520to%2520better%2520classify%2520neighboring%250Anodes%2520with%2520different%2520labels%252C%2520such%2520as%2520in%2520heterophilic%2520graphs.%2520In%2520this%2520work%252C%2520we%250Aapply%2520the%2520design%2520principle%2520of%2520the%2520reverse%2520process%2520to%2520the%2520three%2520variants%2520of%2520the%250AGNNs.%2520Through%2520the%2520experiments%2520on%2520heterophilic%2520graph%2520data%252C%2520where%2520adjacent%2520nodes%250Aneed%2520to%2520have%2520different%2520representations%2520for%2520successful%2520classification%252C%2520we%2520show%250Athat%2520the%2520reverse%2520process%2520significantly%2520improves%2520the%2520prediction%2520performance%2520in%250Amany%2520cases.%2520Additional%2520analysis%2520reveals%2520that%2520the%2520reverse%2520mechanism%2520can%2520mitigate%250Athe%2520over-smoothing%2520over%2520hundreds%2520of%2520layers.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ml-postech/reverse-gnn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Oversmoothing%20Through%20Reverse%20Process%20of%20GNNs%20for%0A%20%20Heterophilic%20Graphs&entry.906535625=MoonJeong%20Park%20and%20Jaeseung%20Heo%20and%20Dongwoo%20Kim&entry.1292438233=%20%20Graph%20Neural%20Network%20%28GNN%29%20resembles%20the%20diffusion%20process%2C%20leading%20to%20the%0Aover-smoothing%20of%20learned%20representations%20when%20stacking%20many%20layers.%20Hence%2C%20the%0Areverse%20process%20of%20message%20passing%20can%20produce%20the%20distinguishable%20node%0Arepresentations%20by%20inverting%20the%20forward%20message%20propagation.%20The%0Adistinguishable%20representations%20can%20help%20us%20to%20better%20classify%20neighboring%0Anodes%20with%20different%20labels%2C%20such%20as%20in%20heterophilic%20graphs.%20In%20this%20work%2C%20we%0Aapply%20the%20design%20principle%20of%20the%20reverse%20process%20to%20the%20three%20variants%20of%20the%0AGNNs.%20Through%20the%20experiments%20on%20heterophilic%20graph%20data%2C%20where%20adjacent%20nodes%0Aneed%20to%20have%20different%20representations%20for%20successful%20classification%2C%20we%20show%0Athat%20the%20reverse%20process%20significantly%20improves%20the%20prediction%20performance%20in%0Amany%20cases.%20Additional%20analysis%20reveals%20that%20the%20reverse%20mechanism%20can%20mitigate%0Athe%20over-smoothing%20over%20hundreds%20of%20layers.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/ml-postech/reverse-gnn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10543v2&entry.124074799=Read"},
{"title": "G-Transformer: Counterfactual Outcome Prediction under Dynamic and\n  Time-varying Treatment Regimes", "author": "Hong Xiong and Feng Wu and Leon Deng and Megan Su and Li-wei H Lehman", "abstract": "  In the context of medical decision making, counterfactual prediction enables\nclinicians to predict treatment outcomes of interest under alternative courses\nof therapeutic actions given observed patient history. Prior machine learning\napproaches for counterfactual predictions under time-varying treatments focus\non static time-varying treatment regimes where treatments do not depend on\nprevious covariate history. In this work, we present G-Transformer, a\nTransformer-based framework supporting g-computation for counterfactual\nprediction under dynamic and time-varying treatment strategies. G-Transfomer\ncaptures complex, long-range dependencies in time-varying covariates using a\nTransformer architecture. G-Transformer estimates the conditional distribution\nof relevant covariates given covariate and treatment history at each time point\nusing an encoder architecture, then produces Monte Carlo estimates of\ncounterfactual outcomes by simulating forward patient trajectories under\ntreatment strategies of interest. We evaluate G-Transformer extensively using\ntwo simulated longitudinal datasets from mechanistic models, and a real-world\nsepsis ICU dataset from MIMIC-IV. G-Transformer outperforms both classical and\nstate-of-the-art counterfactual prediction models in these settings. To the\nbest of our knowledge, this is the first Transformer-based architecture for\ncounterfactual outcome prediction under dynamic and time-varying treatment\nstrategies. Code will be released upon publication of the paper.\n", "link": "http://arxiv.org/abs/2406.05504v2", "date": "2024-06-11", "relevancy": 1.4212, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4685}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-Transformer%3A%20Counterfactual%20Outcome%20Prediction%20under%20Dynamic%20and%0A%20%20Time-varying%20Treatment%20Regimes&body=Title%3A%20G-Transformer%3A%20Counterfactual%20Outcome%20Prediction%20under%20Dynamic%20and%0A%20%20Time-varying%20Treatment%20Regimes%0AAuthor%3A%20Hong%20Xiong%20and%20Feng%20Wu%20and%20Leon%20Deng%20and%20Megan%20Su%20and%20Li-wei%20H%20Lehman%0AAbstract%3A%20%20%20In%20the%20context%20of%20medical%20decision%20making%2C%20counterfactual%20prediction%20enables%0Aclinicians%20to%20predict%20treatment%20outcomes%20of%20interest%20under%20alternative%20courses%0Aof%20therapeutic%20actions%20given%20observed%20patient%20history.%20Prior%20machine%20learning%0Aapproaches%20for%20counterfactual%20predictions%20under%20time-varying%20treatments%20focus%0Aon%20static%20time-varying%20treatment%20regimes%20where%20treatments%20do%20not%20depend%20on%0Aprevious%20covariate%20history.%20In%20this%20work%2C%20we%20present%20G-Transformer%2C%20a%0ATransformer-based%20framework%20supporting%20g-computation%20for%20counterfactual%0Aprediction%20under%20dynamic%20and%20time-varying%20treatment%20strategies.%20G-Transfomer%0Acaptures%20complex%2C%20long-range%20dependencies%20in%20time-varying%20covariates%20using%20a%0ATransformer%20architecture.%20G-Transformer%20estimates%20the%20conditional%20distribution%0Aof%20relevant%20covariates%20given%20covariate%20and%20treatment%20history%20at%20each%20time%20point%0Ausing%20an%20encoder%20architecture%2C%20then%20produces%20Monte%20Carlo%20estimates%20of%0Acounterfactual%20outcomes%20by%20simulating%20forward%20patient%20trajectories%20under%0Atreatment%20strategies%20of%20interest.%20We%20evaluate%20G-Transformer%20extensively%20using%0Atwo%20simulated%20longitudinal%20datasets%20from%20mechanistic%20models%2C%20and%20a%20real-world%0Asepsis%20ICU%20dataset%20from%20MIMIC-IV.%20G-Transformer%20outperforms%20both%20classical%20and%0Astate-of-the-art%20counterfactual%20prediction%20models%20in%20these%20settings.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20is%20the%20first%20Transformer-based%20architecture%20for%0Acounterfactual%20outcome%20prediction%20under%20dynamic%20and%20time-varying%20treatment%0Astrategies.%20Code%20will%20be%20released%20upon%20publication%20of%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05504v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-Transformer%253A%2520Counterfactual%2520Outcome%2520Prediction%2520under%2520Dynamic%2520and%250A%2520%2520Time-varying%2520Treatment%2520Regimes%26entry.906535625%3DHong%2520Xiong%2520and%2520Feng%2520Wu%2520and%2520Leon%2520Deng%2520and%2520Megan%2520Su%2520and%2520Li-wei%2520H%2520Lehman%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520medical%2520decision%2520making%252C%2520counterfactual%2520prediction%2520enables%250Aclinicians%2520to%2520predict%2520treatment%2520outcomes%2520of%2520interest%2520under%2520alternative%2520courses%250Aof%2520therapeutic%2520actions%2520given%2520observed%2520patient%2520history.%2520Prior%2520machine%2520learning%250Aapproaches%2520for%2520counterfactual%2520predictions%2520under%2520time-varying%2520treatments%2520focus%250Aon%2520static%2520time-varying%2520treatment%2520regimes%2520where%2520treatments%2520do%2520not%2520depend%2520on%250Aprevious%2520covariate%2520history.%2520In%2520this%2520work%252C%2520we%2520present%2520G-Transformer%252C%2520a%250ATransformer-based%2520framework%2520supporting%2520g-computation%2520for%2520counterfactual%250Aprediction%2520under%2520dynamic%2520and%2520time-varying%2520treatment%2520strategies.%2520G-Transfomer%250Acaptures%2520complex%252C%2520long-range%2520dependencies%2520in%2520time-varying%2520covariates%2520using%2520a%250ATransformer%2520architecture.%2520G-Transformer%2520estimates%2520the%2520conditional%2520distribution%250Aof%2520relevant%2520covariates%2520given%2520covariate%2520and%2520treatment%2520history%2520at%2520each%2520time%2520point%250Ausing%2520an%2520encoder%2520architecture%252C%2520then%2520produces%2520Monte%2520Carlo%2520estimates%2520of%250Acounterfactual%2520outcomes%2520by%2520simulating%2520forward%2520patient%2520trajectories%2520under%250Atreatment%2520strategies%2520of%2520interest.%2520We%2520evaluate%2520G-Transformer%2520extensively%2520using%250Atwo%2520simulated%2520longitudinal%2520datasets%2520from%2520mechanistic%2520models%252C%2520and%2520a%2520real-world%250Asepsis%2520ICU%2520dataset%2520from%2520MIMIC-IV.%2520G-Transformer%2520outperforms%2520both%2520classical%2520and%250Astate-of-the-art%2520counterfactual%2520prediction%2520models%2520in%2520these%2520settings.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520Transformer-based%2520architecture%2520for%250Acounterfactual%2520outcome%2520prediction%2520under%2520dynamic%2520and%2520time-varying%2520treatment%250Astrategies.%2520Code%2520will%2520be%2520released%2520upon%2520publication%2520of%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05504v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-Transformer%3A%20Counterfactual%20Outcome%20Prediction%20under%20Dynamic%20and%0A%20%20Time-varying%20Treatment%20Regimes&entry.906535625=Hong%20Xiong%20and%20Feng%20Wu%20and%20Leon%20Deng%20and%20Megan%20Su%20and%20Li-wei%20H%20Lehman&entry.1292438233=%20%20In%20the%20context%20of%20medical%20decision%20making%2C%20counterfactual%20prediction%20enables%0Aclinicians%20to%20predict%20treatment%20outcomes%20of%20interest%20under%20alternative%20courses%0Aof%20therapeutic%20actions%20given%20observed%20patient%20history.%20Prior%20machine%20learning%0Aapproaches%20for%20counterfactual%20predictions%20under%20time-varying%20treatments%20focus%0Aon%20static%20time-varying%20treatment%20regimes%20where%20treatments%20do%20not%20depend%20on%0Aprevious%20covariate%20history.%20In%20this%20work%2C%20we%20present%20G-Transformer%2C%20a%0ATransformer-based%20framework%20supporting%20g-computation%20for%20counterfactual%0Aprediction%20under%20dynamic%20and%20time-varying%20treatment%20strategies.%20G-Transfomer%0Acaptures%20complex%2C%20long-range%20dependencies%20in%20time-varying%20covariates%20using%20a%0ATransformer%20architecture.%20G-Transformer%20estimates%20the%20conditional%20distribution%0Aof%20relevant%20covariates%20given%20covariate%20and%20treatment%20history%20at%20each%20time%20point%0Ausing%20an%20encoder%20architecture%2C%20then%20produces%20Monte%20Carlo%20estimates%20of%0Acounterfactual%20outcomes%20by%20simulating%20forward%20patient%20trajectories%20under%0Atreatment%20strategies%20of%20interest.%20We%20evaluate%20G-Transformer%20extensively%20using%0Atwo%20simulated%20longitudinal%20datasets%20from%20mechanistic%20models%2C%20and%20a%20real-world%0Asepsis%20ICU%20dataset%20from%20MIMIC-IV.%20G-Transformer%20outperforms%20both%20classical%20and%0Astate-of-the-art%20counterfactual%20prediction%20models%20in%20these%20settings.%20To%20the%0Abest%20of%20our%20knowledge%2C%20this%20is%20the%20first%20Transformer-based%20architecture%20for%0Acounterfactual%20outcome%20prediction%20under%20dynamic%20and%20time-varying%20treatment%0Astrategies.%20Code%20will%20be%20released%20upon%20publication%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05504v2&entry.124074799=Read"},
{"title": "Speaking Your Language: Spatial Relationships in Interpretable Emergent\n  Communication", "author": "Olaf Lipinski and Adam J. Sobey and Federico Cerutti and Timothy J. Norman", "abstract": "  Effective communication requires the ability to refer to specific parts of an\nobservation in relation to others. While emergent communication literature\nshows success in developing various language properties, no research has shown\nthe emergence of such positional references. This paper demonstrates how agents\ncan communicate about spatial relationships within their observations. The\nresults indicate that agents can develop a language capable of expressing the\nrelationships between parts of their observation, achieving over 90% accuracy\nwhen trained in a referential game which requires such communication. Using a\ncollocation measure, we demonstrate how the agents create such references. This\nanalysis suggests that agents use a mixture of non-compositional and\ncompositional messages to convey spatial relationships. We also show that the\nemergent language is interpretable by humans. The translation accuracy is\ntested by communicating with the receiver agent, where the receiver achieves\nover 78% accuracy using parts of this lexicon, confirming that the\ninterpretation of the emergent language was successful.\n", "link": "http://arxiv.org/abs/2406.07277v1", "date": "2024-06-11", "relevancy": 1.4677, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.507}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4939}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speaking%20Your%20Language%3A%20Spatial%20Relationships%20in%20Interpretable%20Emergent%0A%20%20Communication&body=Title%3A%20Speaking%20Your%20Language%3A%20Spatial%20Relationships%20in%20Interpretable%20Emergent%0A%20%20Communication%0AAuthor%3A%20Olaf%20Lipinski%20and%20Adam%20J.%20Sobey%20and%20Federico%20Cerutti%20and%20Timothy%20J.%20Norman%0AAbstract%3A%20%20%20Effective%20communication%20requires%20the%20ability%20to%20refer%20to%20specific%20parts%20of%20an%0Aobservation%20in%20relation%20to%20others.%20While%20emergent%20communication%20literature%0Ashows%20success%20in%20developing%20various%20language%20properties%2C%20no%20research%20has%20shown%0Athe%20emergence%20of%20such%20positional%20references.%20This%20paper%20demonstrates%20how%20agents%0Acan%20communicate%20about%20spatial%20relationships%20within%20their%20observations.%20The%0Aresults%20indicate%20that%20agents%20can%20develop%20a%20language%20capable%20of%20expressing%20the%0Arelationships%20between%20parts%20of%20their%20observation%2C%20achieving%20over%2090%25%20accuracy%0Awhen%20trained%20in%20a%20referential%20game%20which%20requires%20such%20communication.%20Using%20a%0Acollocation%20measure%2C%20we%20demonstrate%20how%20the%20agents%20create%20such%20references.%20This%0Aanalysis%20suggests%20that%20agents%20use%20a%20mixture%20of%20non-compositional%20and%0Acompositional%20messages%20to%20convey%20spatial%20relationships.%20We%20also%20show%20that%20the%0Aemergent%20language%20is%20interpretable%20by%20humans.%20The%20translation%20accuracy%20is%0Atested%20by%20communicating%20with%20the%20receiver%20agent%2C%20where%20the%20receiver%20achieves%0Aover%2078%25%20accuracy%20using%20parts%20of%20this%20lexicon%2C%20confirming%20that%20the%0Ainterpretation%20of%20the%20emergent%20language%20was%20successful.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeaking%2520Your%2520Language%253A%2520Spatial%2520Relationships%2520in%2520Interpretable%2520Emergent%250A%2520%2520Communication%26entry.906535625%3DOlaf%2520Lipinski%2520and%2520Adam%2520J.%2520Sobey%2520and%2520Federico%2520Cerutti%2520and%2520Timothy%2520J.%2520Norman%26entry.1292438233%3D%2520%2520Effective%2520communication%2520requires%2520the%2520ability%2520to%2520refer%2520to%2520specific%2520parts%2520of%2520an%250Aobservation%2520in%2520relation%2520to%2520others.%2520While%2520emergent%2520communication%2520literature%250Ashows%2520success%2520in%2520developing%2520various%2520language%2520properties%252C%2520no%2520research%2520has%2520shown%250Athe%2520emergence%2520of%2520such%2520positional%2520references.%2520This%2520paper%2520demonstrates%2520how%2520agents%250Acan%2520communicate%2520about%2520spatial%2520relationships%2520within%2520their%2520observations.%2520The%250Aresults%2520indicate%2520that%2520agents%2520can%2520develop%2520a%2520language%2520capable%2520of%2520expressing%2520the%250Arelationships%2520between%2520parts%2520of%2520their%2520observation%252C%2520achieving%2520over%252090%2525%2520accuracy%250Awhen%2520trained%2520in%2520a%2520referential%2520game%2520which%2520requires%2520such%2520communication.%2520Using%2520a%250Acollocation%2520measure%252C%2520we%2520demonstrate%2520how%2520the%2520agents%2520create%2520such%2520references.%2520This%250Aanalysis%2520suggests%2520that%2520agents%2520use%2520a%2520mixture%2520of%2520non-compositional%2520and%250Acompositional%2520messages%2520to%2520convey%2520spatial%2520relationships.%2520We%2520also%2520show%2520that%2520the%250Aemergent%2520language%2520is%2520interpretable%2520by%2520humans.%2520The%2520translation%2520accuracy%2520is%250Atested%2520by%2520communicating%2520with%2520the%2520receiver%2520agent%252C%2520where%2520the%2520receiver%2520achieves%250Aover%252078%2525%2520accuracy%2520using%2520parts%2520of%2520this%2520lexicon%252C%2520confirming%2520that%2520the%250Ainterpretation%2520of%2520the%2520emergent%2520language%2520was%2520successful.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speaking%20Your%20Language%3A%20Spatial%20Relationships%20in%20Interpretable%20Emergent%0A%20%20Communication&entry.906535625=Olaf%20Lipinski%20and%20Adam%20J.%20Sobey%20and%20Federico%20Cerutti%20and%20Timothy%20J.%20Norman&entry.1292438233=%20%20Effective%20communication%20requires%20the%20ability%20to%20refer%20to%20specific%20parts%20of%20an%0Aobservation%20in%20relation%20to%20others.%20While%20emergent%20communication%20literature%0Ashows%20success%20in%20developing%20various%20language%20properties%2C%20no%20research%20has%20shown%0Athe%20emergence%20of%20such%20positional%20references.%20This%20paper%20demonstrates%20how%20agents%0Acan%20communicate%20about%20spatial%20relationships%20within%20their%20observations.%20The%0Aresults%20indicate%20that%20agents%20can%20develop%20a%20language%20capable%20of%20expressing%20the%0Arelationships%20between%20parts%20of%20their%20observation%2C%20achieving%20over%2090%25%20accuracy%0Awhen%20trained%20in%20a%20referential%20game%20which%20requires%20such%20communication.%20Using%20a%0Acollocation%20measure%2C%20we%20demonstrate%20how%20the%20agents%20create%20such%20references.%20This%0Aanalysis%20suggests%20that%20agents%20use%20a%20mixture%20of%20non-compositional%20and%0Acompositional%20messages%20to%20convey%20spatial%20relationships.%20We%20also%20show%20that%20the%0Aemergent%20language%20is%20interpretable%20by%20humans.%20The%20translation%20accuracy%20is%0Atested%20by%20communicating%20with%20the%20receiver%20agent%2C%20where%20the%20receiver%20achieves%0Aover%2078%25%20accuracy%20using%20parts%20of%20this%20lexicon%2C%20confirming%20that%20the%0Ainterpretation%20of%20the%20emergent%20language%20was%20successful.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07277v1&entry.124074799=Read"},
{"title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization", "author": "Haoran You and Yipin Guo and Yichao Fu and Wei Zhou and Huihong Shi and Xiaofan Zhang and Souvik Kundu and Amir Yazdanbakhsh and  Yingyan and  Lin", "abstract": "  Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.\n", "link": "http://arxiv.org/abs/2406.05981v2", "date": "2024-06-11", "relevancy": 1.53, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5205}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5064}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShiftAddLLM%3A%20Accelerating%20Pretrained%20LLMs%20via%20Post-Training%0A%20%20Multiplication-Less%20Reparameterization&body=Title%3A%20ShiftAddLLM%3A%20Accelerating%20Pretrained%20LLMs%20via%20Post-Training%0A%20%20Multiplication-Less%20Reparameterization%0AAuthor%3A%20Haoran%20You%20and%20Yipin%20Guo%20and%20Yichao%20Fu%20and%20Wei%20Zhou%20and%20Huihong%20Shi%20and%20Xiaofan%20Zhang%20and%20Souvik%20Kundu%20and%20Amir%20Yazdanbakhsh%20and%20%20Yingyan%20and%20%20Lin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20performance%20on%20language%0Atasks%20but%20face%20challenges%20when%20deployed%20on%20resource-constrained%20devices%20due%20to%0Atheir%20extensive%20parameters%20and%20reliance%20on%20dense%20multiplications%2C%20resulting%20in%0Ahigh%20memory%20demands%20and%20latency%20bottlenecks.%20Shift-and-add%20reparameterization%0Aoffers%20a%20promising%20solution%20by%20replacing%20costly%20multiplications%20with%0Ahardware-friendly%20primitives%20in%20both%20the%20attention%20and%20multi-layer%20perceptron%0A%28MLP%29%20layers%20of%20an%20LLM.%20However%2C%20current%20reparameterization%20techniques%20require%0Atraining%20from%20scratch%20or%20full%20parameter%20fine-tuning%20to%20restore%20accuracy%2C%20which%0Ais%20resource-intensive%20for%20LLMs.%20To%20address%20this%2C%20we%20propose%20accelerating%0Apretrained%20LLMs%20through%20post-training%20shift-and-add%20reparameterization%2C%0Acreating%20efficient%20multiplication-free%20models%2C%20dubbed%20ShiftAddLLM.%0ASpecifically%2C%20we%20quantize%20each%20weight%20matrix%20into%20binary%20matrices%20paired%20with%0Agroup-wise%20scaling%20factors.%20The%20associated%20multiplications%20are%20reparameterized%0Ainto%20%281%29%20shifts%20between%20activations%20and%20scaling%20factors%20and%20%282%29%20queries%20and%0Aadds%20according%20to%20the%20binary%20matrices.%20To%20reduce%20accuracy%20loss%2C%20we%20present%20a%0Amulti-objective%20optimization%20method%20to%20minimize%20both%20weight%20and%20output%0Aactivation%20reparameterization%20errors.%20Additionally%2C%20based%20on%20varying%0Asensitivity%20across%20layers%20to%20reparameterization%2C%20we%20develop%20an%20automated%20bit%0Aallocation%20strategy%20to%20further%20reduce%20memory%20usage%20and%20latency.%20Experiments%20on%0Afive%20LLM%20families%20and%20eight%20tasks%20consistently%20validate%20the%20effectiveness%20of%0AShiftAddLLM%2C%20achieving%20average%20perplexity%20improvements%20of%205.6%20and%2022.7%20points%0Aat%20comparable%20or%20lower%20latency%20compared%20to%20the%20most%20competitive%20quantized%20LLMs%0Aat%203%20and%202%20bits%2C%20respectively%2C%20and%20more%20than%2080%25%20memory%20and%20energy%20reductions%0Aover%20the%20original%20LLMs.%20Codes%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/GATECH-EIC/ShiftAddLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05981v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShiftAddLLM%253A%2520Accelerating%2520Pretrained%2520LLMs%2520via%2520Post-Training%250A%2520%2520Multiplication-Less%2520Reparameterization%26entry.906535625%3DHaoran%2520You%2520and%2520Yipin%2520Guo%2520and%2520Yichao%2520Fu%2520and%2520Wei%2520Zhou%2520and%2520Huihong%2520Shi%2520and%2520Xiaofan%2520Zhang%2520and%2520Souvik%2520Kundu%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520%2520Yingyan%2520and%2520%2520Lin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520performance%2520on%2520language%250Atasks%2520but%2520face%2520challenges%2520when%2520deployed%2520on%2520resource-constrained%2520devices%2520due%2520to%250Atheir%2520extensive%2520parameters%2520and%2520reliance%2520on%2520dense%2520multiplications%252C%2520resulting%2520in%250Ahigh%2520memory%2520demands%2520and%2520latency%2520bottlenecks.%2520Shift-and-add%2520reparameterization%250Aoffers%2520a%2520promising%2520solution%2520by%2520replacing%2520costly%2520multiplications%2520with%250Ahardware-friendly%2520primitives%2520in%2520both%2520the%2520attention%2520and%2520multi-layer%2520perceptron%250A%2528MLP%2529%2520layers%2520of%2520an%2520LLM.%2520However%252C%2520current%2520reparameterization%2520techniques%2520require%250Atraining%2520from%2520scratch%2520or%2520full%2520parameter%2520fine-tuning%2520to%2520restore%2520accuracy%252C%2520which%250Ais%2520resource-intensive%2520for%2520LLMs.%2520To%2520address%2520this%252C%2520we%2520propose%2520accelerating%250Apretrained%2520LLMs%2520through%2520post-training%2520shift-and-add%2520reparameterization%252C%250Acreating%2520efficient%2520multiplication-free%2520models%252C%2520dubbed%2520ShiftAddLLM.%250ASpecifically%252C%2520we%2520quantize%2520each%2520weight%2520matrix%2520into%2520binary%2520matrices%2520paired%2520with%250Agroup-wise%2520scaling%2520factors.%2520The%2520associated%2520multiplications%2520are%2520reparameterized%250Ainto%2520%25281%2529%2520shifts%2520between%2520activations%2520and%2520scaling%2520factors%2520and%2520%25282%2529%2520queries%2520and%250Aadds%2520according%2520to%2520the%2520binary%2520matrices.%2520To%2520reduce%2520accuracy%2520loss%252C%2520we%2520present%2520a%250Amulti-objective%2520optimization%2520method%2520to%2520minimize%2520both%2520weight%2520and%2520output%250Aactivation%2520reparameterization%2520errors.%2520Additionally%252C%2520based%2520on%2520varying%250Asensitivity%2520across%2520layers%2520to%2520reparameterization%252C%2520we%2520develop%2520an%2520automated%2520bit%250Aallocation%2520strategy%2520to%2520further%2520reduce%2520memory%2520usage%2520and%2520latency.%2520Experiments%2520on%250Afive%2520LLM%2520families%2520and%2520eight%2520tasks%2520consistently%2520validate%2520the%2520effectiveness%2520of%250AShiftAddLLM%252C%2520achieving%2520average%2520perplexity%2520improvements%2520of%25205.6%2520and%252022.7%2520points%250Aat%2520comparable%2520or%2520lower%2520latency%2520compared%2520to%2520the%2520most%2520competitive%2520quantized%2520LLMs%250Aat%25203%2520and%25202%2520bits%252C%2520respectively%252C%2520and%2520more%2520than%252080%2525%2520memory%2520and%2520energy%2520reductions%250Aover%2520the%2520original%2520LLMs.%2520Codes%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/GATECH-EIC/ShiftAddLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05981v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShiftAddLLM%3A%20Accelerating%20Pretrained%20LLMs%20via%20Post-Training%0A%20%20Multiplication-Less%20Reparameterization&entry.906535625=Haoran%20You%20and%20Yipin%20Guo%20and%20Yichao%20Fu%20and%20Wei%20Zhou%20and%20Huihong%20Shi%20and%20Xiaofan%20Zhang%20and%20Souvik%20Kundu%20and%20Amir%20Yazdanbakhsh%20and%20%20Yingyan%20and%20%20Lin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20impressive%20performance%20on%20language%0Atasks%20but%20face%20challenges%20when%20deployed%20on%20resource-constrained%20devices%20due%20to%0Atheir%20extensive%20parameters%20and%20reliance%20on%20dense%20multiplications%2C%20resulting%20in%0Ahigh%20memory%20demands%20and%20latency%20bottlenecks.%20Shift-and-add%20reparameterization%0Aoffers%20a%20promising%20solution%20by%20replacing%20costly%20multiplications%20with%0Ahardware-friendly%20primitives%20in%20both%20the%20attention%20and%20multi-layer%20perceptron%0A%28MLP%29%20layers%20of%20an%20LLM.%20However%2C%20current%20reparameterization%20techniques%20require%0Atraining%20from%20scratch%20or%20full%20parameter%20fine-tuning%20to%20restore%20accuracy%2C%20which%0Ais%20resource-intensive%20for%20LLMs.%20To%20address%20this%2C%20we%20propose%20accelerating%0Apretrained%20LLMs%20through%20post-training%20shift-and-add%20reparameterization%2C%0Acreating%20efficient%20multiplication-free%20models%2C%20dubbed%20ShiftAddLLM.%0ASpecifically%2C%20we%20quantize%20each%20weight%20matrix%20into%20binary%20matrices%20paired%20with%0Agroup-wise%20scaling%20factors.%20The%20associated%20multiplications%20are%20reparameterized%0Ainto%20%281%29%20shifts%20between%20activations%20and%20scaling%20factors%20and%20%282%29%20queries%20and%0Aadds%20according%20to%20the%20binary%20matrices.%20To%20reduce%20accuracy%20loss%2C%20we%20present%20a%0Amulti-objective%20optimization%20method%20to%20minimize%20both%20weight%20and%20output%0Aactivation%20reparameterization%20errors.%20Additionally%2C%20based%20on%20varying%0Asensitivity%20across%20layers%20to%20reparameterization%2C%20we%20develop%20an%20automated%20bit%0Aallocation%20strategy%20to%20further%20reduce%20memory%20usage%20and%20latency.%20Experiments%20on%0Afive%20LLM%20families%20and%20eight%20tasks%20consistently%20validate%20the%20effectiveness%20of%0AShiftAddLLM%2C%20achieving%20average%20perplexity%20improvements%20of%205.6%20and%2022.7%20points%0Aat%20comparable%20or%20lower%20latency%20compared%20to%20the%20most%20competitive%20quantized%20LLMs%0Aat%203%20and%202%20bits%2C%20respectively%2C%20and%20more%20than%2080%25%20memory%20and%20energy%20reductions%0Aover%20the%20original%20LLMs.%20Codes%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/GATECH-EIC/ShiftAddLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05981v2&entry.124074799=Read"},
{"title": "Merging Improves Self-Critique Against Jailbreak Attacks", "author": "Victor Gallego", "abstract": "  The robustness of large language models (LLMs) against adversarial\nmanipulations, such as jailbreak attacks, remains a significant challenge. In\nthis work, we propose an approach that enhances the self-critique capability of\nthe LLM and further fine-tunes it over sanitized synthetic data. This is done\nwith the addition of an external critic model that can be merged with the\noriginal, thus bolstering self-critique capabilities and improving the\nrobustness of the LLMs response to adversarial prompts. Our results demonstrate\nthat the combination of merging and self-critique can reduce the attack success\nrate of adversaries significantly, thus offering a promising defense mechanism\nagainst jailbreak attacks. Code, data and models released at\nhttps://github.com/vicgalle/merging-self-critique-jailbreaks .\n", "link": "http://arxiv.org/abs/2406.07188v1", "date": "2024-06-11", "relevancy": 1.8027, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4535}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4503}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Merging%20Improves%20Self-Critique%20Against%20Jailbreak%20Attacks&body=Title%3A%20Merging%20Improves%20Self-Critique%20Against%20Jailbreak%20Attacks%0AAuthor%3A%20Victor%20Gallego%0AAbstract%3A%20%20%20The%20robustness%20of%20large%20language%20models%20%28LLMs%29%20against%20adversarial%0Amanipulations%2C%20such%20as%20jailbreak%20attacks%2C%20remains%20a%20significant%20challenge.%20In%0Athis%20work%2C%20we%20propose%20an%20approach%20that%20enhances%20the%20self-critique%20capability%20of%0Athe%20LLM%20and%20further%20fine-tunes%20it%20over%20sanitized%20synthetic%20data.%20This%20is%20done%0Awith%20the%20addition%20of%20an%20external%20critic%20model%20that%20can%20be%20merged%20with%20the%0Aoriginal%2C%20thus%20bolstering%20self-critique%20capabilities%20and%20improving%20the%0Arobustness%20of%20the%20LLMs%20response%20to%20adversarial%20prompts.%20Our%20results%20demonstrate%0Athat%20the%20combination%20of%20merging%20and%20self-critique%20can%20reduce%20the%20attack%20success%0Arate%20of%20adversaries%20significantly%2C%20thus%20offering%20a%20promising%20defense%20mechanism%0Aagainst%20jailbreak%20attacks.%20Code%2C%20data%20and%20models%20released%20at%0Ahttps%3A//github.com/vicgalle/merging-self-critique-jailbreaks%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMerging%2520Improves%2520Self-Critique%2520Against%2520Jailbreak%2520Attacks%26entry.906535625%3DVictor%2520Gallego%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520against%2520adversarial%250Amanipulations%252C%2520such%2520as%2520jailbreak%2520attacks%252C%2520remains%2520a%2520significant%2520challenge.%2520In%250Athis%2520work%252C%2520we%2520propose%2520an%2520approach%2520that%2520enhances%2520the%2520self-critique%2520capability%2520of%250Athe%2520LLM%2520and%2520further%2520fine-tunes%2520it%2520over%2520sanitized%2520synthetic%2520data.%2520This%2520is%2520done%250Awith%2520the%2520addition%2520of%2520an%2520external%2520critic%2520model%2520that%2520can%2520be%2520merged%2520with%2520the%250Aoriginal%252C%2520thus%2520bolstering%2520self-critique%2520capabilities%2520and%2520improving%2520the%250Arobustness%2520of%2520the%2520LLMs%2520response%2520to%2520adversarial%2520prompts.%2520Our%2520results%2520demonstrate%250Athat%2520the%2520combination%2520of%2520merging%2520and%2520self-critique%2520can%2520reduce%2520the%2520attack%2520success%250Arate%2520of%2520adversaries%2520significantly%252C%2520thus%2520offering%2520a%2520promising%2520defense%2520mechanism%250Aagainst%2520jailbreak%2520attacks.%2520Code%252C%2520data%2520and%2520models%2520released%2520at%250Ahttps%253A//github.com/vicgalle/merging-self-critique-jailbreaks%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Merging%20Improves%20Self-Critique%20Against%20Jailbreak%20Attacks&entry.906535625=Victor%20Gallego&entry.1292438233=%20%20The%20robustness%20of%20large%20language%20models%20%28LLMs%29%20against%20adversarial%0Amanipulations%2C%20such%20as%20jailbreak%20attacks%2C%20remains%20a%20significant%20challenge.%20In%0Athis%20work%2C%20we%20propose%20an%20approach%20that%20enhances%20the%20self-critique%20capability%20of%0Athe%20LLM%20and%20further%20fine-tunes%20it%20over%20sanitized%20synthetic%20data.%20This%20is%20done%0Awith%20the%20addition%20of%20an%20external%20critic%20model%20that%20can%20be%20merged%20with%20the%0Aoriginal%2C%20thus%20bolstering%20self-critique%20capabilities%20and%20improving%20the%0Arobustness%20of%20the%20LLMs%20response%20to%20adversarial%20prompts.%20Our%20results%20demonstrate%0Athat%20the%20combination%20of%20merging%20and%20self-critique%20can%20reduce%20the%20attack%20success%0Arate%20of%20adversaries%20significantly%2C%20thus%20offering%20a%20promising%20defense%20mechanism%0Aagainst%20jailbreak%20attacks.%20Code%2C%20data%20and%20models%20released%20at%0Ahttps%3A//github.com/vicgalle/merging-self-critique-jailbreaks%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07188v1&entry.124074799=Read"},
{"title": "DeformTime: Capturing Variable Dependencies with Deformable Attention\n  for Time Series Forecasting", "author": "Yuxuan Shu and Vasileios Lampos", "abstract": "  In multivariate time series (MTS) forecasting, existing state-of-the-art deep\nlearning approaches tend to focus on autoregressive formulations and overlook\nthe information within exogenous indicators. To address this limitation, we\npresent DeformTime, a neural network architecture that attempts to capture\ncorrelated temporal patterns from the input space, and hence, improve\nforecasting accuracy. It deploys two core operations performed by deformable\nattention blocks (DABs): learning dependencies across variables from different\ntime steps (variable DAB), and preserving temporal dependencies in data from\nprevious time steps (temporal DAB). Input data transformation is explicitly\ndesigned to enhance learning from the deformed series of information while\npassing through a DAB. We conduct extensive experiments on 6 MTS data sets,\nusing previously established benchmarks as well as challenging infectious\ndisease modelling tasks with more exogenous variables. The results demonstrate\nthat DeformTime improves accuracy against previous competitive methods across\nthe vast majority of MTS forecasting tasks, reducing the mean absolute error by\n10% on average. Notably, performance gains remain consistent across longer\nforecasting horizons.\n", "link": "http://arxiv.org/abs/2406.07438v1", "date": "2024-06-11", "relevancy": 1.9612, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4983}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeformTime%3A%20Capturing%20Variable%20Dependencies%20with%20Deformable%20Attention%0A%20%20for%20Time%20Series%20Forecasting&body=Title%3A%20DeformTime%3A%20Capturing%20Variable%20Dependencies%20with%20Deformable%20Attention%0A%20%20for%20Time%20Series%20Forecasting%0AAuthor%3A%20Yuxuan%20Shu%20and%20Vasileios%20Lampos%0AAbstract%3A%20%20%20In%20multivariate%20time%20series%20%28MTS%29%20forecasting%2C%20existing%20state-of-the-art%20deep%0Alearning%20approaches%20tend%20to%20focus%20on%20autoregressive%20formulations%20and%20overlook%0Athe%20information%20within%20exogenous%20indicators.%20To%20address%20this%20limitation%2C%20we%0Apresent%20DeformTime%2C%20a%20neural%20network%20architecture%20that%20attempts%20to%20capture%0Acorrelated%20temporal%20patterns%20from%20the%20input%20space%2C%20and%20hence%2C%20improve%0Aforecasting%20accuracy.%20It%20deploys%20two%20core%20operations%20performed%20by%20deformable%0Aattention%20blocks%20%28DABs%29%3A%20learning%20dependencies%20across%20variables%20from%20different%0Atime%20steps%20%28variable%20DAB%29%2C%20and%20preserving%20temporal%20dependencies%20in%20data%20from%0Aprevious%20time%20steps%20%28temporal%20DAB%29.%20Input%20data%20transformation%20is%20explicitly%0Adesigned%20to%20enhance%20learning%20from%20the%20deformed%20series%20of%20information%20while%0Apassing%20through%20a%20DAB.%20We%20conduct%20extensive%20experiments%20on%206%20MTS%20data%20sets%2C%0Ausing%20previously%20established%20benchmarks%20as%20well%20as%20challenging%20infectious%0Adisease%20modelling%20tasks%20with%20more%20exogenous%20variables.%20The%20results%20demonstrate%0Athat%20DeformTime%20improves%20accuracy%20against%20previous%20competitive%20methods%20across%0Athe%20vast%20majority%20of%20MTS%20forecasting%20tasks%2C%20reducing%20the%20mean%20absolute%20error%20by%0A10%25%20on%20average.%20Notably%2C%20performance%20gains%20remain%20consistent%20across%20longer%0Aforecasting%20horizons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeformTime%253A%2520Capturing%2520Variable%2520Dependencies%2520with%2520Deformable%2520Attention%250A%2520%2520for%2520Time%2520Series%2520Forecasting%26entry.906535625%3DYuxuan%2520Shu%2520and%2520Vasileios%2520Lampos%26entry.1292438233%3D%2520%2520In%2520multivariate%2520time%2520series%2520%2528MTS%2529%2520forecasting%252C%2520existing%2520state-of-the-art%2520deep%250Alearning%2520approaches%2520tend%2520to%2520focus%2520on%2520autoregressive%2520formulations%2520and%2520overlook%250Athe%2520information%2520within%2520exogenous%2520indicators.%2520To%2520address%2520this%2520limitation%252C%2520we%250Apresent%2520DeformTime%252C%2520a%2520neural%2520network%2520architecture%2520that%2520attempts%2520to%2520capture%250Acorrelated%2520temporal%2520patterns%2520from%2520the%2520input%2520space%252C%2520and%2520hence%252C%2520improve%250Aforecasting%2520accuracy.%2520It%2520deploys%2520two%2520core%2520operations%2520performed%2520by%2520deformable%250Aattention%2520blocks%2520%2528DABs%2529%253A%2520learning%2520dependencies%2520across%2520variables%2520from%2520different%250Atime%2520steps%2520%2528variable%2520DAB%2529%252C%2520and%2520preserving%2520temporal%2520dependencies%2520in%2520data%2520from%250Aprevious%2520time%2520steps%2520%2528temporal%2520DAB%2529.%2520Input%2520data%2520transformation%2520is%2520explicitly%250Adesigned%2520to%2520enhance%2520learning%2520from%2520the%2520deformed%2520series%2520of%2520information%2520while%250Apassing%2520through%2520a%2520DAB.%2520We%2520conduct%2520extensive%2520experiments%2520on%25206%2520MTS%2520data%2520sets%252C%250Ausing%2520previously%2520established%2520benchmarks%2520as%2520well%2520as%2520challenging%2520infectious%250Adisease%2520modelling%2520tasks%2520with%2520more%2520exogenous%2520variables.%2520The%2520results%2520demonstrate%250Athat%2520DeformTime%2520improves%2520accuracy%2520against%2520previous%2520competitive%2520methods%2520across%250Athe%2520vast%2520majority%2520of%2520MTS%2520forecasting%2520tasks%252C%2520reducing%2520the%2520mean%2520absolute%2520error%2520by%250A10%2525%2520on%2520average.%2520Notably%252C%2520performance%2520gains%2520remain%2520consistent%2520across%2520longer%250Aforecasting%2520horizons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeformTime%3A%20Capturing%20Variable%20Dependencies%20with%20Deformable%20Attention%0A%20%20for%20Time%20Series%20Forecasting&entry.906535625=Yuxuan%20Shu%20and%20Vasileios%20Lampos&entry.1292438233=%20%20In%20multivariate%20time%20series%20%28MTS%29%20forecasting%2C%20existing%20state-of-the-art%20deep%0Alearning%20approaches%20tend%20to%20focus%20on%20autoregressive%20formulations%20and%20overlook%0Athe%20information%20within%20exogenous%20indicators.%20To%20address%20this%20limitation%2C%20we%0Apresent%20DeformTime%2C%20a%20neural%20network%20architecture%20that%20attempts%20to%20capture%0Acorrelated%20temporal%20patterns%20from%20the%20input%20space%2C%20and%20hence%2C%20improve%0Aforecasting%20accuracy.%20It%20deploys%20two%20core%20operations%20performed%20by%20deformable%0Aattention%20blocks%20%28DABs%29%3A%20learning%20dependencies%20across%20variables%20from%20different%0Atime%20steps%20%28variable%20DAB%29%2C%20and%20preserving%20temporal%20dependencies%20in%20data%20from%0Aprevious%20time%20steps%20%28temporal%20DAB%29.%20Input%20data%20transformation%20is%20explicitly%0Adesigned%20to%20enhance%20learning%20from%20the%20deformed%20series%20of%20information%20while%0Apassing%20through%20a%20DAB.%20We%20conduct%20extensive%20experiments%20on%206%20MTS%20data%20sets%2C%0Ausing%20previously%20established%20benchmarks%20as%20well%20as%20challenging%20infectious%0Adisease%20modelling%20tasks%20with%20more%20exogenous%20variables.%20The%20results%20demonstrate%0Athat%20DeformTime%20improves%20accuracy%20against%20previous%20competitive%20methods%20across%0Athe%20vast%20majority%20of%20MTS%20forecasting%20tasks%2C%20reducing%20the%20mean%20absolute%20error%20by%0A10%25%20on%20average.%20Notably%2C%20performance%20gains%20remain%20consistent%20across%20longer%0Aforecasting%20horizons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07438v1&entry.124074799=Read"},
{"title": "Trusting Fair Data: Leveraging Quality in Fairness-Driven Data Removal\n  Techniques", "author": "Manh Khoi Duong and Stefan Conrad", "abstract": "  In this paper, we deal with bias mitigation techniques that remove specific\ndata points from the training set to aim for a fair representation of the\npopulation in that set. Machine learning models are trained on these\npre-processed datasets, and their predictions are expected to be fair. However,\nsuch approaches may exclude relevant data, making the attained subsets less\ntrustworthy for further usage. To enhance the trustworthiness of prior methods,\nwe propose additional requirements and objectives that the subsets must fulfill\nin addition to fairness: (1) group coverage, and (2) minimal data loss. While\nremoving entire groups may improve the measured fairness, this practice is very\nproblematic as failing to represent every group cannot be considered fair. In\nour second concern, we advocate for the retention of data while minimizing\ndiscrimination. By introducing a multi-objective optimization problem that\nconsiders fairness and data loss, we propose a methodology to find\nPareto-optimal solutions that balance these objectives. By identifying such\nsolutions, users can make informed decisions about the trade-off between\nfairness and data quality and select the most suitable subset for their\napplication.\n", "link": "http://arxiv.org/abs/2405.12926v2", "date": "2024-06-11", "relevancy": 1.7699, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4569}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4405}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trusting%20Fair%20Data%3A%20Leveraging%20Quality%20in%20Fairness-Driven%20Data%20Removal%0A%20%20Techniques&body=Title%3A%20Trusting%20Fair%20Data%3A%20Leveraging%20Quality%20in%20Fairness-Driven%20Data%20Removal%0A%20%20Techniques%0AAuthor%3A%20Manh%20Khoi%20Duong%20and%20Stefan%20Conrad%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20deal%20with%20bias%20mitigation%20techniques%20that%20remove%20specific%0Adata%20points%20from%20the%20training%20set%20to%20aim%20for%20a%20fair%20representation%20of%20the%0Apopulation%20in%20that%20set.%20Machine%20learning%20models%20are%20trained%20on%20these%0Apre-processed%20datasets%2C%20and%20their%20predictions%20are%20expected%20to%20be%20fair.%20However%2C%0Asuch%20approaches%20may%20exclude%20relevant%20data%2C%20making%20the%20attained%20subsets%20less%0Atrustworthy%20for%20further%20usage.%20To%20enhance%20the%20trustworthiness%20of%20prior%20methods%2C%0Awe%20propose%20additional%20requirements%20and%20objectives%20that%20the%20subsets%20must%20fulfill%0Ain%20addition%20to%20fairness%3A%20%281%29%20group%20coverage%2C%20and%20%282%29%20minimal%20data%20loss.%20While%0Aremoving%20entire%20groups%20may%20improve%20the%20measured%20fairness%2C%20this%20practice%20is%20very%0Aproblematic%20as%20failing%20to%20represent%20every%20group%20cannot%20be%20considered%20fair.%20In%0Aour%20second%20concern%2C%20we%20advocate%20for%20the%20retention%20of%20data%20while%20minimizing%0Adiscrimination.%20By%20introducing%20a%20multi-objective%20optimization%20problem%20that%0Aconsiders%20fairness%20and%20data%20loss%2C%20we%20propose%20a%20methodology%20to%20find%0APareto-optimal%20solutions%20that%20balance%20these%20objectives.%20By%20identifying%20such%0Asolutions%2C%20users%20can%20make%20informed%20decisions%20about%20the%20trade-off%20between%0Afairness%20and%20data%20quality%20and%20select%20the%20most%20suitable%20subset%20for%20their%0Aapplication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrusting%2520Fair%2520Data%253A%2520Leveraging%2520Quality%2520in%2520Fairness-Driven%2520Data%2520Removal%250A%2520%2520Techniques%26entry.906535625%3DManh%2520Khoi%2520Duong%2520and%2520Stefan%2520Conrad%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520deal%2520with%2520bias%2520mitigation%2520techniques%2520that%2520remove%2520specific%250Adata%2520points%2520from%2520the%2520training%2520set%2520to%2520aim%2520for%2520a%2520fair%2520representation%2520of%2520the%250Apopulation%2520in%2520that%2520set.%2520Machine%2520learning%2520models%2520are%2520trained%2520on%2520these%250Apre-processed%2520datasets%252C%2520and%2520their%2520predictions%2520are%2520expected%2520to%2520be%2520fair.%2520However%252C%250Asuch%2520approaches%2520may%2520exclude%2520relevant%2520data%252C%2520making%2520the%2520attained%2520subsets%2520less%250Atrustworthy%2520for%2520further%2520usage.%2520To%2520enhance%2520the%2520trustworthiness%2520of%2520prior%2520methods%252C%250Awe%2520propose%2520additional%2520requirements%2520and%2520objectives%2520that%2520the%2520subsets%2520must%2520fulfill%250Ain%2520addition%2520to%2520fairness%253A%2520%25281%2529%2520group%2520coverage%252C%2520and%2520%25282%2529%2520minimal%2520data%2520loss.%2520While%250Aremoving%2520entire%2520groups%2520may%2520improve%2520the%2520measured%2520fairness%252C%2520this%2520practice%2520is%2520very%250Aproblematic%2520as%2520failing%2520to%2520represent%2520every%2520group%2520cannot%2520be%2520considered%2520fair.%2520In%250Aour%2520second%2520concern%252C%2520we%2520advocate%2520for%2520the%2520retention%2520of%2520data%2520while%2520minimizing%250Adiscrimination.%2520By%2520introducing%2520a%2520multi-objective%2520optimization%2520problem%2520that%250Aconsiders%2520fairness%2520and%2520data%2520loss%252C%2520we%2520propose%2520a%2520methodology%2520to%2520find%250APareto-optimal%2520solutions%2520that%2520balance%2520these%2520objectives.%2520By%2520identifying%2520such%250Asolutions%252C%2520users%2520can%2520make%2520informed%2520decisions%2520about%2520the%2520trade-off%2520between%250Afairness%2520and%2520data%2520quality%2520and%2520select%2520the%2520most%2520suitable%2520subset%2520for%2520their%250Aapplication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trusting%20Fair%20Data%3A%20Leveraging%20Quality%20in%20Fairness-Driven%20Data%20Removal%0A%20%20Techniques&entry.906535625=Manh%20Khoi%20Duong%20and%20Stefan%20Conrad&entry.1292438233=%20%20In%20this%20paper%2C%20we%20deal%20with%20bias%20mitigation%20techniques%20that%20remove%20specific%0Adata%20points%20from%20the%20training%20set%20to%20aim%20for%20a%20fair%20representation%20of%20the%0Apopulation%20in%20that%20set.%20Machine%20learning%20models%20are%20trained%20on%20these%0Apre-processed%20datasets%2C%20and%20their%20predictions%20are%20expected%20to%20be%20fair.%20However%2C%0Asuch%20approaches%20may%20exclude%20relevant%20data%2C%20making%20the%20attained%20subsets%20less%0Atrustworthy%20for%20further%20usage.%20To%20enhance%20the%20trustworthiness%20of%20prior%20methods%2C%0Awe%20propose%20additional%20requirements%20and%20objectives%20that%20the%20subsets%20must%20fulfill%0Ain%20addition%20to%20fairness%3A%20%281%29%20group%20coverage%2C%20and%20%282%29%20minimal%20data%20loss.%20While%0Aremoving%20entire%20groups%20may%20improve%20the%20measured%20fairness%2C%20this%20practice%20is%20very%0Aproblematic%20as%20failing%20to%20represent%20every%20group%20cannot%20be%20considered%20fair.%20In%0Aour%20second%20concern%2C%20we%20advocate%20for%20the%20retention%20of%20data%20while%20minimizing%0Adiscrimination.%20By%20introducing%20a%20multi-objective%20optimization%20problem%20that%0Aconsiders%20fairness%20and%20data%20loss%2C%20we%20propose%20a%20methodology%20to%20find%0APareto-optimal%20solutions%20that%20balance%20these%20objectives.%20By%20identifying%20such%0Asolutions%2C%20users%20can%20make%20informed%20decisions%20about%20the%20trade-off%20between%0Afairness%20and%20data%20quality%20and%20select%20the%20most%20suitable%20subset%20for%20their%0Aapplication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12926v2&entry.124074799=Read"},
{"title": "TernaryLLM: Ternarized Large Language Model", "author": "Tianqi Chen and Zhe Li and Weixiang Xu and Zeyu Zhu and Dong Li and Lu Tian and Emad Barsoum and Peisong Wang and Jian Cheng", "abstract": "  Large language models (LLMs) have achieved remarkable performance on Natural\nLanguage Processing (NLP) tasks, but they are hindered by high computational\ncosts and memory requirements. Ternarization, an extreme form of quantization,\noffers a solution by reducing memory usage and enabling energy-efficient\nfloating-point additions. However, applying ternarization to LLMs faces\nchallenges stemming from outliers in both weights and activations. In this\nwork, observing asymmetric outliers and non-zero means in weights, we introduce\nDual Learnable Ternarization (DLT), which enables both scales and shifts to be\nlearnable. We also propose Outlier-Friendly Feature Knowledge Distillation\n(OFF) to recover the information lost in extremely low-bit quantization. The\nproposed OFF can incorporate semantic information and is insensitive to\noutliers. At the core of OFF is maximizing the mutual information between\nfeatures in ternarized and floating-point models using cosine similarity.\nExtensive experiments demonstrate that our TernaryLLM surpasses previous\nlow-bit quantization methods on the standard text generation and zero-shot\nbenchmarks for different LLM families. Specifically, for one of the most\npowerful open-source models, LLaMA-3, our approach (W1.58A16) outperforms the\nprevious state-of-the-art method (W2A16) by 5.8 in terms of perplexity on C4\nand by 8.2% in terms of average accuracy on zero-shot tasks.\n", "link": "http://arxiv.org/abs/2406.07177v1", "date": "2024-06-11", "relevancy": 1.9797, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5136}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5015}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TernaryLLM%3A%20Ternarized%20Large%20Language%20Model&body=Title%3A%20TernaryLLM%3A%20Ternarized%20Large%20Language%20Model%0AAuthor%3A%20Tianqi%20Chen%20and%20Zhe%20Li%20and%20Weixiang%20Xu%20and%20Zeyu%20Zhu%20and%20Dong%20Li%20and%20Lu%20Tian%20and%20Emad%20Barsoum%20and%20Peisong%20Wang%20and%20Jian%20Cheng%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20on%20Natural%0ALanguage%20Processing%20%28NLP%29%20tasks%2C%20but%20they%20are%20hindered%20by%20high%20computational%0Acosts%20and%20memory%20requirements.%20Ternarization%2C%20an%20extreme%20form%20of%20quantization%2C%0Aoffers%20a%20solution%20by%20reducing%20memory%20usage%20and%20enabling%20energy-efficient%0Afloating-point%20additions.%20However%2C%20applying%20ternarization%20to%20LLMs%20faces%0Achallenges%20stemming%20from%20outliers%20in%20both%20weights%20and%20activations.%20In%20this%0Awork%2C%20observing%20asymmetric%20outliers%20and%20non-zero%20means%20in%20weights%2C%20we%20introduce%0ADual%20Learnable%20Ternarization%20%28DLT%29%2C%20which%20enables%20both%20scales%20and%20shifts%20to%20be%0Alearnable.%20We%20also%20propose%20Outlier-Friendly%20Feature%20Knowledge%20Distillation%0A%28OFF%29%20to%20recover%20the%20information%20lost%20in%20extremely%20low-bit%20quantization.%20The%0Aproposed%20OFF%20can%20incorporate%20semantic%20information%20and%20is%20insensitive%20to%0Aoutliers.%20At%20the%20core%20of%20OFF%20is%20maximizing%20the%20mutual%20information%20between%0Afeatures%20in%20ternarized%20and%20floating-point%20models%20using%20cosine%20similarity.%0AExtensive%20experiments%20demonstrate%20that%20our%20TernaryLLM%20surpasses%20previous%0Alow-bit%20quantization%20methods%20on%20the%20standard%20text%20generation%20and%20zero-shot%0Abenchmarks%20for%20different%20LLM%20families.%20Specifically%2C%20for%20one%20of%20the%20most%0Apowerful%20open-source%20models%2C%20LLaMA-3%2C%20our%20approach%20%28W1.58A16%29%20outperforms%20the%0Aprevious%20state-of-the-art%20method%20%28W2A16%29%20by%205.8%20in%20terms%20of%20perplexity%20on%20C4%0Aand%20by%208.2%25%20in%20terms%20of%20average%20accuracy%20on%20zero-shot%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTernaryLLM%253A%2520Ternarized%2520Large%2520Language%2520Model%26entry.906535625%3DTianqi%2520Chen%2520and%2520Zhe%2520Li%2520and%2520Weixiang%2520Xu%2520and%2520Zeyu%2520Zhu%2520and%2520Dong%2520Li%2520and%2520Lu%2520Tian%2520and%2520Emad%2520Barsoum%2520and%2520Peisong%2520Wang%2520and%2520Jian%2520Cheng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520performance%2520on%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%2520tasks%252C%2520but%2520they%2520are%2520hindered%2520by%2520high%2520computational%250Acosts%2520and%2520memory%2520requirements.%2520Ternarization%252C%2520an%2520extreme%2520form%2520of%2520quantization%252C%250Aoffers%2520a%2520solution%2520by%2520reducing%2520memory%2520usage%2520and%2520enabling%2520energy-efficient%250Afloating-point%2520additions.%2520However%252C%2520applying%2520ternarization%2520to%2520LLMs%2520faces%250Achallenges%2520stemming%2520from%2520outliers%2520in%2520both%2520weights%2520and%2520activations.%2520In%2520this%250Awork%252C%2520observing%2520asymmetric%2520outliers%2520and%2520non-zero%2520means%2520in%2520weights%252C%2520we%2520introduce%250ADual%2520Learnable%2520Ternarization%2520%2528DLT%2529%252C%2520which%2520enables%2520both%2520scales%2520and%2520shifts%2520to%2520be%250Alearnable.%2520We%2520also%2520propose%2520Outlier-Friendly%2520Feature%2520Knowledge%2520Distillation%250A%2528OFF%2529%2520to%2520recover%2520the%2520information%2520lost%2520in%2520extremely%2520low-bit%2520quantization.%2520The%250Aproposed%2520OFF%2520can%2520incorporate%2520semantic%2520information%2520and%2520is%2520insensitive%2520to%250Aoutliers.%2520At%2520the%2520core%2520of%2520OFF%2520is%2520maximizing%2520the%2520mutual%2520information%2520between%250Afeatures%2520in%2520ternarized%2520and%2520floating-point%2520models%2520using%2520cosine%2520similarity.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520TernaryLLM%2520surpasses%2520previous%250Alow-bit%2520quantization%2520methods%2520on%2520the%2520standard%2520text%2520generation%2520and%2520zero-shot%250Abenchmarks%2520for%2520different%2520LLM%2520families.%2520Specifically%252C%2520for%2520one%2520of%2520the%2520most%250Apowerful%2520open-source%2520models%252C%2520LLaMA-3%252C%2520our%2520approach%2520%2528W1.58A16%2529%2520outperforms%2520the%250Aprevious%2520state-of-the-art%2520method%2520%2528W2A16%2529%2520by%25205.8%2520in%2520terms%2520of%2520perplexity%2520on%2520C4%250Aand%2520by%25208.2%2525%2520in%2520terms%2520of%2520average%2520accuracy%2520on%2520zero-shot%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TernaryLLM%3A%20Ternarized%20Large%20Language%20Model&entry.906535625=Tianqi%20Chen%20and%20Zhe%20Li%20and%20Weixiang%20Xu%20and%20Zeyu%20Zhu%20and%20Dong%20Li%20and%20Lu%20Tian%20and%20Emad%20Barsoum%20and%20Peisong%20Wang%20and%20Jian%20Cheng&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20performance%20on%20Natural%0ALanguage%20Processing%20%28NLP%29%20tasks%2C%20but%20they%20are%20hindered%20by%20high%20computational%0Acosts%20and%20memory%20requirements.%20Ternarization%2C%20an%20extreme%20form%20of%20quantization%2C%0Aoffers%20a%20solution%20by%20reducing%20memory%20usage%20and%20enabling%20energy-efficient%0Afloating-point%20additions.%20However%2C%20applying%20ternarization%20to%20LLMs%20faces%0Achallenges%20stemming%20from%20outliers%20in%20both%20weights%20and%20activations.%20In%20this%0Awork%2C%20observing%20asymmetric%20outliers%20and%20non-zero%20means%20in%20weights%2C%20we%20introduce%0ADual%20Learnable%20Ternarization%20%28DLT%29%2C%20which%20enables%20both%20scales%20and%20shifts%20to%20be%0Alearnable.%20We%20also%20propose%20Outlier-Friendly%20Feature%20Knowledge%20Distillation%0A%28OFF%29%20to%20recover%20the%20information%20lost%20in%20extremely%20low-bit%20quantization.%20The%0Aproposed%20OFF%20can%20incorporate%20semantic%20information%20and%20is%20insensitive%20to%0Aoutliers.%20At%20the%20core%20of%20OFF%20is%20maximizing%20the%20mutual%20information%20between%0Afeatures%20in%20ternarized%20and%20floating-point%20models%20using%20cosine%20similarity.%0AExtensive%20experiments%20demonstrate%20that%20our%20TernaryLLM%20surpasses%20previous%0Alow-bit%20quantization%20methods%20on%20the%20standard%20text%20generation%20and%20zero-shot%0Abenchmarks%20for%20different%20LLM%20families.%20Specifically%2C%20for%20one%20of%20the%20most%0Apowerful%20open-source%20models%2C%20LLaMA-3%2C%20our%20approach%20%28W1.58A16%29%20outperforms%20the%0Aprevious%20state-of-the-art%20method%20%28W2A16%29%20by%205.8%20in%20terms%20of%20perplexity%20on%20C4%0Aand%20by%208.2%25%20in%20terms%20of%20average%20accuracy%20on%20zero-shot%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07177v1&entry.124074799=Read"},
{"title": "EnQuery: Ensemble Policies for Diverse Query-Generation in Preference\n  Alignment of Robot Navigation", "author": "Jorge de Heuvel and Florian Seiler and Maren Bennewitz", "abstract": "  To align mobile robot navigation policies with user preferences through\nreinforcement learning from human feedback (RLHF), reliable and\nbehavior-diverse user queries are required. However, deterministic policies\nfail to generate a variety of navigation trajectory suggestions for a given\nnavigation task. In this paper, we introduce EnQuery, a query generation\napproach using an ensemble of policies that achieve behavioral diversity\nthrough a regularization term. For a given navigation task, EnQuery produces\nmultiple navigation trajectory suggestions, thereby optimizing the efficiency\nof preference data collection with fewer queries. Our methodology demonstrates\nsuperior performance in aligning navigation policies with user preferences in\nlow-query regimes, offering enhanced policy convergence from sparse preference\nqueries. The evaluation is complemented with a novel explainability\nrepresentation, capturing full scene navigation behavior of the mobile robot in\na single plot. Our code is available online at\nhttps://github.com/hrl-bonn/EnQuery.\n", "link": "http://arxiv.org/abs/2404.04852v2", "date": "2024-06-11", "relevancy": 1.6103, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5412}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EnQuery%3A%20Ensemble%20Policies%20for%20Diverse%20Query-Generation%20in%20Preference%0A%20%20Alignment%20of%20Robot%20Navigation&body=Title%3A%20EnQuery%3A%20Ensemble%20Policies%20for%20Diverse%20Query-Generation%20in%20Preference%0A%20%20Alignment%20of%20Robot%20Navigation%0AAuthor%3A%20Jorge%20de%20Heuvel%20and%20Florian%20Seiler%20and%20Maren%20Bennewitz%0AAbstract%3A%20%20%20To%20align%20mobile%20robot%20navigation%20policies%20with%20user%20preferences%20through%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%2C%20reliable%20and%0Abehavior-diverse%20user%20queries%20are%20required.%20However%2C%20deterministic%20policies%0Afail%20to%20generate%20a%20variety%20of%20navigation%20trajectory%20suggestions%20for%20a%20given%0Anavigation%20task.%20In%20this%20paper%2C%20we%20introduce%20EnQuery%2C%20a%20query%20generation%0Aapproach%20using%20an%20ensemble%20of%20policies%20that%20achieve%20behavioral%20diversity%0Athrough%20a%20regularization%20term.%20For%20a%20given%20navigation%20task%2C%20EnQuery%20produces%0Amultiple%20navigation%20trajectory%20suggestions%2C%20thereby%20optimizing%20the%20efficiency%0Aof%20preference%20data%20collection%20with%20fewer%20queries.%20Our%20methodology%20demonstrates%0Asuperior%20performance%20in%20aligning%20navigation%20policies%20with%20user%20preferences%20in%0Alow-query%20regimes%2C%20offering%20enhanced%20policy%20convergence%20from%20sparse%20preference%0Aqueries.%20The%20evaluation%20is%20complemented%20with%20a%20novel%20explainability%0Arepresentation%2C%20capturing%20full%20scene%20navigation%20behavior%20of%20the%20mobile%20robot%20in%0Aa%20single%20plot.%20Our%20code%20is%20available%20online%20at%0Ahttps%3A//github.com/hrl-bonn/EnQuery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04852v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnQuery%253A%2520Ensemble%2520Policies%2520for%2520Diverse%2520Query-Generation%2520in%2520Preference%250A%2520%2520Alignment%2520of%2520Robot%2520Navigation%26entry.906535625%3DJorge%2520de%2520Heuvel%2520and%2520Florian%2520Seiler%2520and%2520Maren%2520Bennewitz%26entry.1292438233%3D%2520%2520To%2520align%2520mobile%2520robot%2520navigation%2520policies%2520with%2520user%2520preferences%2520through%250Areinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%252C%2520reliable%2520and%250Abehavior-diverse%2520user%2520queries%2520are%2520required.%2520However%252C%2520deterministic%2520policies%250Afail%2520to%2520generate%2520a%2520variety%2520of%2520navigation%2520trajectory%2520suggestions%2520for%2520a%2520given%250Anavigation%2520task.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520EnQuery%252C%2520a%2520query%2520generation%250Aapproach%2520using%2520an%2520ensemble%2520of%2520policies%2520that%2520achieve%2520behavioral%2520diversity%250Athrough%2520a%2520regularization%2520term.%2520For%2520a%2520given%2520navigation%2520task%252C%2520EnQuery%2520produces%250Amultiple%2520navigation%2520trajectory%2520suggestions%252C%2520thereby%2520optimizing%2520the%2520efficiency%250Aof%2520preference%2520data%2520collection%2520with%2520fewer%2520queries.%2520Our%2520methodology%2520demonstrates%250Asuperior%2520performance%2520in%2520aligning%2520navigation%2520policies%2520with%2520user%2520preferences%2520in%250Alow-query%2520regimes%252C%2520offering%2520enhanced%2520policy%2520convergence%2520from%2520sparse%2520preference%250Aqueries.%2520The%2520evaluation%2520is%2520complemented%2520with%2520a%2520novel%2520explainability%250Arepresentation%252C%2520capturing%2520full%2520scene%2520navigation%2520behavior%2520of%2520the%2520mobile%2520robot%2520in%250Aa%2520single%2520plot.%2520Our%2520code%2520is%2520available%2520online%2520at%250Ahttps%253A//github.com/hrl-bonn/EnQuery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04852v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EnQuery%3A%20Ensemble%20Policies%20for%20Diverse%20Query-Generation%20in%20Preference%0A%20%20Alignment%20of%20Robot%20Navigation&entry.906535625=Jorge%20de%20Heuvel%20and%20Florian%20Seiler%20and%20Maren%20Bennewitz&entry.1292438233=%20%20To%20align%20mobile%20robot%20navigation%20policies%20with%20user%20preferences%20through%0Areinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%2C%20reliable%20and%0Abehavior-diverse%20user%20queries%20are%20required.%20However%2C%20deterministic%20policies%0Afail%20to%20generate%20a%20variety%20of%20navigation%20trajectory%20suggestions%20for%20a%20given%0Anavigation%20task.%20In%20this%20paper%2C%20we%20introduce%20EnQuery%2C%20a%20query%20generation%0Aapproach%20using%20an%20ensemble%20of%20policies%20that%20achieve%20behavioral%20diversity%0Athrough%20a%20regularization%20term.%20For%20a%20given%20navigation%20task%2C%20EnQuery%20produces%0Amultiple%20navigation%20trajectory%20suggestions%2C%20thereby%20optimizing%20the%20efficiency%0Aof%20preference%20data%20collection%20with%20fewer%20queries.%20Our%20methodology%20demonstrates%0Asuperior%20performance%20in%20aligning%20navigation%20policies%20with%20user%20preferences%20in%0Alow-query%20regimes%2C%20offering%20enhanced%20policy%20convergence%20from%20sparse%20preference%0Aqueries.%20The%20evaluation%20is%20complemented%20with%20a%20novel%20explainability%0Arepresentation%2C%20capturing%20full%20scene%20navigation%20behavior%20of%20the%20mobile%20robot%20in%0Aa%20single%20plot.%20Our%20code%20is%20available%20online%20at%0Ahttps%3A//github.com/hrl-bonn/EnQuery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04852v2&entry.124074799=Read"},
{"title": "Improving Commonsense Bias Classification by Mitigating the Influence of\n  Demographic Terms", "author": "JinKyu Lee and Jihie Kim", "abstract": "  Understanding commonsense knowledge is crucial in the field of Natural\nLanguage Processing (NLP). However, the presence of demographic terms in\ncommonsense knowledge poses a potential risk of compromising the performance of\nNLP models. This study aims to investigate and propose methods for enhancing\nthe performance and effectiveness of a commonsense polarization classifier by\nmitigating the influence of demographic terms. Three methods are introduced in\nthis paper: (1) hierarchical generalization of demographic terms (2)\nthreshold-based augmentation and (3) integration of hierarchical generalization\nand threshold-based augmentation methods (IHTA). The first method involves\nreplacing demographic terms with more general ones based on a term hierarchy\nontology, aiming to mitigate the influence of specific terms. To address the\nlimited bias-related information, the second method measures the polarization\nof demographic terms by comparing the changes in the model's predictions when\nthese terms are masked versus unmasked. This method augments commonsense\nsentences containing terms with high polarization values by replacing their\npredicates with synonyms generated by ChatGPT. The third method combines the\ntwo approaches, starting with threshold-based augmentation followed by\nhierarchical generalization. The experiments show that the first method\nincreases the accuracy over the baseline by 2.33%, and the second one by 0.96%\nover standard augmentation methods. The IHTA techniques yielded an 8.82% and\n9.96% higher accuracy than threshold-based and standard augmentation methods,\nrespectively.\n", "link": "http://arxiv.org/abs/2406.07229v1", "date": "2024-06-11", "relevancy": 1.8726, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4839}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.467}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Commonsense%20Bias%20Classification%20by%20Mitigating%20the%20Influence%20of%0A%20%20Demographic%20Terms&body=Title%3A%20Improving%20Commonsense%20Bias%20Classification%20by%20Mitigating%20the%20Influence%20of%0A%20%20Demographic%20Terms%0AAuthor%3A%20JinKyu%20Lee%20and%20Jihie%20Kim%0AAbstract%3A%20%20%20Understanding%20commonsense%20knowledge%20is%20crucial%20in%20the%20field%20of%20Natural%0ALanguage%20Processing%20%28NLP%29.%20However%2C%20the%20presence%20of%20demographic%20terms%20in%0Acommonsense%20knowledge%20poses%20a%20potential%20risk%20of%20compromising%20the%20performance%20of%0ANLP%20models.%20This%20study%20aims%20to%20investigate%20and%20propose%20methods%20for%20enhancing%0Athe%20performance%20and%20effectiveness%20of%20a%20commonsense%20polarization%20classifier%20by%0Amitigating%20the%20influence%20of%20demographic%20terms.%20Three%20methods%20are%20introduced%20in%0Athis%20paper%3A%20%281%29%20hierarchical%20generalization%20of%20demographic%20terms%20%282%29%0Athreshold-based%20augmentation%20and%20%283%29%20integration%20of%20hierarchical%20generalization%0Aand%20threshold-based%20augmentation%20methods%20%28IHTA%29.%20The%20first%20method%20involves%0Areplacing%20demographic%20terms%20with%20more%20general%20ones%20based%20on%20a%20term%20hierarchy%0Aontology%2C%20aiming%20to%20mitigate%20the%20influence%20of%20specific%20terms.%20To%20address%20the%0Alimited%20bias-related%20information%2C%20the%20second%20method%20measures%20the%20polarization%0Aof%20demographic%20terms%20by%20comparing%20the%20changes%20in%20the%20model%27s%20predictions%20when%0Athese%20terms%20are%20masked%20versus%20unmasked.%20This%20method%20augments%20commonsense%0Asentences%20containing%20terms%20with%20high%20polarization%20values%20by%20replacing%20their%0Apredicates%20with%20synonyms%20generated%20by%20ChatGPT.%20The%20third%20method%20combines%20the%0Atwo%20approaches%2C%20starting%20with%20threshold-based%20augmentation%20followed%20by%0Ahierarchical%20generalization.%20The%20experiments%20show%20that%20the%20first%20method%0Aincreases%20the%20accuracy%20over%20the%20baseline%20by%202.33%25%2C%20and%20the%20second%20one%20by%200.96%25%0Aover%20standard%20augmentation%20methods.%20The%20IHTA%20techniques%20yielded%20an%208.82%25%20and%0A9.96%25%20higher%20accuracy%20than%20threshold-based%20and%20standard%20augmentation%20methods%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Commonsense%2520Bias%2520Classification%2520by%2520Mitigating%2520the%2520Influence%2520of%250A%2520%2520Demographic%2520Terms%26entry.906535625%3DJinKyu%2520Lee%2520and%2520Jihie%2520Kim%26entry.1292438233%3D%2520%2520Understanding%2520commonsense%2520knowledge%2520is%2520crucial%2520in%2520the%2520field%2520of%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529.%2520However%252C%2520the%2520presence%2520of%2520demographic%2520terms%2520in%250Acommonsense%2520knowledge%2520poses%2520a%2520potential%2520risk%2520of%2520compromising%2520the%2520performance%2520of%250ANLP%2520models.%2520This%2520study%2520aims%2520to%2520investigate%2520and%2520propose%2520methods%2520for%2520enhancing%250Athe%2520performance%2520and%2520effectiveness%2520of%2520a%2520commonsense%2520polarization%2520classifier%2520by%250Amitigating%2520the%2520influence%2520of%2520demographic%2520terms.%2520Three%2520methods%2520are%2520introduced%2520in%250Athis%2520paper%253A%2520%25281%2529%2520hierarchical%2520generalization%2520of%2520demographic%2520terms%2520%25282%2529%250Athreshold-based%2520augmentation%2520and%2520%25283%2529%2520integration%2520of%2520hierarchical%2520generalization%250Aand%2520threshold-based%2520augmentation%2520methods%2520%2528IHTA%2529.%2520The%2520first%2520method%2520involves%250Areplacing%2520demographic%2520terms%2520with%2520more%2520general%2520ones%2520based%2520on%2520a%2520term%2520hierarchy%250Aontology%252C%2520aiming%2520to%2520mitigate%2520the%2520influence%2520of%2520specific%2520terms.%2520To%2520address%2520the%250Alimited%2520bias-related%2520information%252C%2520the%2520second%2520method%2520measures%2520the%2520polarization%250Aof%2520demographic%2520terms%2520by%2520comparing%2520the%2520changes%2520in%2520the%2520model%2527s%2520predictions%2520when%250Athese%2520terms%2520are%2520masked%2520versus%2520unmasked.%2520This%2520method%2520augments%2520commonsense%250Asentences%2520containing%2520terms%2520with%2520high%2520polarization%2520values%2520by%2520replacing%2520their%250Apredicates%2520with%2520synonyms%2520generated%2520by%2520ChatGPT.%2520The%2520third%2520method%2520combines%2520the%250Atwo%2520approaches%252C%2520starting%2520with%2520threshold-based%2520augmentation%2520followed%2520by%250Ahierarchical%2520generalization.%2520The%2520experiments%2520show%2520that%2520the%2520first%2520method%250Aincreases%2520the%2520accuracy%2520over%2520the%2520baseline%2520by%25202.33%2525%252C%2520and%2520the%2520second%2520one%2520by%25200.96%2525%250Aover%2520standard%2520augmentation%2520methods.%2520The%2520IHTA%2520techniques%2520yielded%2520an%25208.82%2525%2520and%250A9.96%2525%2520higher%2520accuracy%2520than%2520threshold-based%2520and%2520standard%2520augmentation%2520methods%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Commonsense%20Bias%20Classification%20by%20Mitigating%20the%20Influence%20of%0A%20%20Demographic%20Terms&entry.906535625=JinKyu%20Lee%20and%20Jihie%20Kim&entry.1292438233=%20%20Understanding%20commonsense%20knowledge%20is%20crucial%20in%20the%20field%20of%20Natural%0ALanguage%20Processing%20%28NLP%29.%20However%2C%20the%20presence%20of%20demographic%20terms%20in%0Acommonsense%20knowledge%20poses%20a%20potential%20risk%20of%20compromising%20the%20performance%20of%0ANLP%20models.%20This%20study%20aims%20to%20investigate%20and%20propose%20methods%20for%20enhancing%0Athe%20performance%20and%20effectiveness%20of%20a%20commonsense%20polarization%20classifier%20by%0Amitigating%20the%20influence%20of%20demographic%20terms.%20Three%20methods%20are%20introduced%20in%0Athis%20paper%3A%20%281%29%20hierarchical%20generalization%20of%20demographic%20terms%20%282%29%0Athreshold-based%20augmentation%20and%20%283%29%20integration%20of%20hierarchical%20generalization%0Aand%20threshold-based%20augmentation%20methods%20%28IHTA%29.%20The%20first%20method%20involves%0Areplacing%20demographic%20terms%20with%20more%20general%20ones%20based%20on%20a%20term%20hierarchy%0Aontology%2C%20aiming%20to%20mitigate%20the%20influence%20of%20specific%20terms.%20To%20address%20the%0Alimited%20bias-related%20information%2C%20the%20second%20method%20measures%20the%20polarization%0Aof%20demographic%20terms%20by%20comparing%20the%20changes%20in%20the%20model%27s%20predictions%20when%0Athese%20terms%20are%20masked%20versus%20unmasked.%20This%20method%20augments%20commonsense%0Asentences%20containing%20terms%20with%20high%20polarization%20values%20by%20replacing%20their%0Apredicates%20with%20synonyms%20generated%20by%20ChatGPT.%20The%20third%20method%20combines%20the%0Atwo%20approaches%2C%20starting%20with%20threshold-based%20augmentation%20followed%20by%0Ahierarchical%20generalization.%20The%20experiments%20show%20that%20the%20first%20method%0Aincreases%20the%20accuracy%20over%20the%20baseline%20by%202.33%25%2C%20and%20the%20second%20one%20by%200.96%25%0Aover%20standard%20augmentation%20methods.%20The%20IHTA%20techniques%20yielded%20an%208.82%25%20and%0A9.96%25%20higher%20accuracy%20than%20threshold-based%20and%20standard%20augmentation%20methods%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07229v1&entry.124074799=Read"},
{"title": "Refined Sample Complexity for Markov Games with Independent Linear\n  Function Approximation", "author": "Yan Dai and Qiwen Cui and Simon S. Du", "abstract": "  Markov Games (MG) is an important model for Multi-Agent Reinforcement\nLearning (MARL). It was long believed that the \"curse of multi-agents\" (i.e.,\nthe algorithmic performance drops exponentially with the number of agents) is\nunavoidable until several recent works (Daskalakis et al., 2023; Cui et al.,\n2023; Wang et al., 2023). While these works resolved the curse of multi-agents,\nwhen the state spaces are prohibitively large and (linear) function\napproximations are deployed, they either had a slower convergence rate of\n$O(T^{-1/4})$ or brought a polynomial dependency on the number of actions\n$A_{\\max}$ -- which is avoidable in single-agent cases even when the loss\nfunctions can arbitrarily vary with time. This paper first refines the AVLPR\nframework by Wang et al. (2023), with an insight of designing *data-dependent*\n(i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a\nbroader choice of plug-in algorithms. When specialized to MGs with independent\nlinear function approximations, we propose novel *action-dependent bonuses* to\ncover occasionally extreme estimation errors. With the help of state-of-the-art\ntechniques from the single-agent RL literature, we give the first algorithm\nthat tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$\nconvergence rate, and avoids $\\text{poly}(A_{\\max})$ dependency simultaneously.\n", "link": "http://arxiv.org/abs/2402.07082v2", "date": "2024-06-11", "relevancy": 1.3617, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5162}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4522}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refined%20Sample%20Complexity%20for%20Markov%20Games%20with%20Independent%20Linear%0A%20%20Function%20Approximation&body=Title%3A%20Refined%20Sample%20Complexity%20for%20Markov%20Games%20with%20Independent%20Linear%0A%20%20Function%20Approximation%0AAuthor%3A%20Yan%20Dai%20and%20Qiwen%20Cui%20and%20Simon%20S.%20Du%0AAbstract%3A%20%20%20Markov%20Games%20%28MG%29%20is%20an%20important%20model%20for%20Multi-Agent%20Reinforcement%0ALearning%20%28MARL%29.%20It%20was%20long%20believed%20that%20the%20%22curse%20of%20multi-agents%22%20%28i.e.%2C%0Athe%20algorithmic%20performance%20drops%20exponentially%20with%20the%20number%20of%20agents%29%20is%0Aunavoidable%20until%20several%20recent%20works%20%28Daskalakis%20et%20al.%2C%202023%3B%20Cui%20et%20al.%2C%0A2023%3B%20Wang%20et%20al.%2C%202023%29.%20While%20these%20works%20resolved%20the%20curse%20of%20multi-agents%2C%0Awhen%20the%20state%20spaces%20are%20prohibitively%20large%20and%20%28linear%29%20function%0Aapproximations%20are%20deployed%2C%20they%20either%20had%20a%20slower%20convergence%20rate%20of%0A%24O%28T%5E%7B-1/4%7D%29%24%20or%20brought%20a%20polynomial%20dependency%20on%20the%20number%20of%20actions%0A%24A_%7B%5Cmax%7D%24%20--%20which%20is%20avoidable%20in%20single-agent%20cases%20even%20when%20the%20loss%0Afunctions%20can%20arbitrarily%20vary%20with%20time.%20This%20paper%20first%20refines%20the%20AVLPR%0Aframework%20by%20Wang%20et%20al.%20%282023%29%2C%20with%20an%20insight%20of%20designing%20%2Adata-dependent%2A%0A%28i.e.%2C%20stochastic%29%20pessimistic%20estimation%20of%20the%20sub-optimality%20gap%2C%20allowing%20a%0Abroader%20choice%20of%20plug-in%20algorithms.%20When%20specialized%20to%20MGs%20with%20independent%0Alinear%20function%20approximations%2C%20we%20propose%20novel%20%2Aaction-dependent%20bonuses%2A%20to%0Acover%20occasionally%20extreme%20estimation%20errors.%20With%20the%20help%20of%20state-of-the-art%0Atechniques%20from%20the%20single-agent%20RL%20literature%2C%20we%20give%20the%20first%20algorithm%0Athat%20tackles%20the%20curse%20of%20multi-agents%2C%20attains%20the%20optimal%20%24O%28T%5E%7B-1/2%7D%29%24%0Aconvergence%20rate%2C%20and%20avoids%20%24%5Ctext%7Bpoly%7D%28A_%7B%5Cmax%7D%29%24%20dependency%20simultaneously.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07082v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefined%2520Sample%2520Complexity%2520for%2520Markov%2520Games%2520with%2520Independent%2520Linear%250A%2520%2520Function%2520Approximation%26entry.906535625%3DYan%2520Dai%2520and%2520Qiwen%2520Cui%2520and%2520Simon%2520S.%2520Du%26entry.1292438233%3D%2520%2520Markov%2520Games%2520%2528MG%2529%2520is%2520an%2520important%2520model%2520for%2520Multi-Agent%2520Reinforcement%250ALearning%2520%2528MARL%2529.%2520It%2520was%2520long%2520believed%2520that%2520the%2520%2522curse%2520of%2520multi-agents%2522%2520%2528i.e.%252C%250Athe%2520algorithmic%2520performance%2520drops%2520exponentially%2520with%2520the%2520number%2520of%2520agents%2529%2520is%250Aunavoidable%2520until%2520several%2520recent%2520works%2520%2528Daskalakis%2520et%2520al.%252C%25202023%253B%2520Cui%2520et%2520al.%252C%250A2023%253B%2520Wang%2520et%2520al.%252C%25202023%2529.%2520While%2520these%2520works%2520resolved%2520the%2520curse%2520of%2520multi-agents%252C%250Awhen%2520the%2520state%2520spaces%2520are%2520prohibitively%2520large%2520and%2520%2528linear%2529%2520function%250Aapproximations%2520are%2520deployed%252C%2520they%2520either%2520had%2520a%2520slower%2520convergence%2520rate%2520of%250A%2524O%2528T%255E%257B-1/4%257D%2529%2524%2520or%2520brought%2520a%2520polynomial%2520dependency%2520on%2520the%2520number%2520of%2520actions%250A%2524A_%257B%255Cmax%257D%2524%2520--%2520which%2520is%2520avoidable%2520in%2520single-agent%2520cases%2520even%2520when%2520the%2520loss%250Afunctions%2520can%2520arbitrarily%2520vary%2520with%2520time.%2520This%2520paper%2520first%2520refines%2520the%2520AVLPR%250Aframework%2520by%2520Wang%2520et%2520al.%2520%25282023%2529%252C%2520with%2520an%2520insight%2520of%2520designing%2520%252Adata-dependent%252A%250A%2528i.e.%252C%2520stochastic%2529%2520pessimistic%2520estimation%2520of%2520the%2520sub-optimality%2520gap%252C%2520allowing%2520a%250Abroader%2520choice%2520of%2520plug-in%2520algorithms.%2520When%2520specialized%2520to%2520MGs%2520with%2520independent%250Alinear%2520function%2520approximations%252C%2520we%2520propose%2520novel%2520%252Aaction-dependent%2520bonuses%252A%2520to%250Acover%2520occasionally%2520extreme%2520estimation%2520errors.%2520With%2520the%2520help%2520of%2520state-of-the-art%250Atechniques%2520from%2520the%2520single-agent%2520RL%2520literature%252C%2520we%2520give%2520the%2520first%2520algorithm%250Athat%2520tackles%2520the%2520curse%2520of%2520multi-agents%252C%2520attains%2520the%2520optimal%2520%2524O%2528T%255E%257B-1/2%257D%2529%2524%250Aconvergence%2520rate%252C%2520and%2520avoids%2520%2524%255Ctext%257Bpoly%257D%2528A_%257B%255Cmax%257D%2529%2524%2520dependency%2520simultaneously.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07082v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refined%20Sample%20Complexity%20for%20Markov%20Games%20with%20Independent%20Linear%0A%20%20Function%20Approximation&entry.906535625=Yan%20Dai%20and%20Qiwen%20Cui%20and%20Simon%20S.%20Du&entry.1292438233=%20%20Markov%20Games%20%28MG%29%20is%20an%20important%20model%20for%20Multi-Agent%20Reinforcement%0ALearning%20%28MARL%29.%20It%20was%20long%20believed%20that%20the%20%22curse%20of%20multi-agents%22%20%28i.e.%2C%0Athe%20algorithmic%20performance%20drops%20exponentially%20with%20the%20number%20of%20agents%29%20is%0Aunavoidable%20until%20several%20recent%20works%20%28Daskalakis%20et%20al.%2C%202023%3B%20Cui%20et%20al.%2C%0A2023%3B%20Wang%20et%20al.%2C%202023%29.%20While%20these%20works%20resolved%20the%20curse%20of%20multi-agents%2C%0Awhen%20the%20state%20spaces%20are%20prohibitively%20large%20and%20%28linear%29%20function%0Aapproximations%20are%20deployed%2C%20they%20either%20had%20a%20slower%20convergence%20rate%20of%0A%24O%28T%5E%7B-1/4%7D%29%24%20or%20brought%20a%20polynomial%20dependency%20on%20the%20number%20of%20actions%0A%24A_%7B%5Cmax%7D%24%20--%20which%20is%20avoidable%20in%20single-agent%20cases%20even%20when%20the%20loss%0Afunctions%20can%20arbitrarily%20vary%20with%20time.%20This%20paper%20first%20refines%20the%20AVLPR%0Aframework%20by%20Wang%20et%20al.%20%282023%29%2C%20with%20an%20insight%20of%20designing%20%2Adata-dependent%2A%0A%28i.e.%2C%20stochastic%29%20pessimistic%20estimation%20of%20the%20sub-optimality%20gap%2C%20allowing%20a%0Abroader%20choice%20of%20plug-in%20algorithms.%20When%20specialized%20to%20MGs%20with%20independent%0Alinear%20function%20approximations%2C%20we%20propose%20novel%20%2Aaction-dependent%20bonuses%2A%20to%0Acover%20occasionally%20extreme%20estimation%20errors.%20With%20the%20help%20of%20state-of-the-art%0Atechniques%20from%20the%20single-agent%20RL%20literature%2C%20we%20give%20the%20first%20algorithm%0Athat%20tackles%20the%20curse%20of%20multi-agents%2C%20attains%20the%20optimal%20%24O%28T%5E%7B-1/2%7D%29%24%0Aconvergence%20rate%2C%20and%20avoids%20%24%5Ctext%7Bpoly%7D%28A_%7B%5Cmax%7D%29%24%20dependency%20simultaneously.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07082v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


